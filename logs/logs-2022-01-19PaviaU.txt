creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f76ded6f940>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.050, val_acc:0.097]
Epoch [2/120    avg_loss:1.626, val_acc:0.255]
Epoch [3/120    avg_loss:1.417, val_acc:0.309]
Epoch [4/120    avg_loss:1.246, val_acc:0.405]
Epoch [5/120    avg_loss:1.117, val_acc:0.461]
Epoch [6/120    avg_loss:1.002, val_acc:0.469]
Epoch [7/120    avg_loss:0.875, val_acc:0.587]
Epoch [8/120    avg_loss:0.772, val_acc:0.706]
Epoch [9/120    avg_loss:0.664, val_acc:0.747]
Epoch [10/120    avg_loss:0.599, val_acc:0.734]
Epoch [11/120    avg_loss:0.481, val_acc:0.753]
Epoch [12/120    avg_loss:0.431, val_acc:0.783]
Epoch [13/120    avg_loss:0.393, val_acc:0.839]
Epoch [14/120    avg_loss:0.342, val_acc:0.883]
Epoch [15/120    avg_loss:0.311, val_acc:0.901]
Epoch [16/120    avg_loss:0.272, val_acc:0.843]
Epoch [17/120    avg_loss:0.255, val_acc:0.863]
Epoch [18/120    avg_loss:0.205, val_acc:0.897]
Epoch [19/120    avg_loss:0.269, val_acc:0.815]
Epoch [20/120    avg_loss:0.216, val_acc:0.858]
Epoch [21/120    avg_loss:0.287, val_acc:0.865]
Epoch [22/120    avg_loss:0.216, val_acc:0.883]
Epoch [23/120    avg_loss:0.168, val_acc:0.947]
Epoch [24/120    avg_loss:0.151, val_acc:0.932]
Epoch [25/120    avg_loss:0.137, val_acc:0.935]
Epoch [26/120    avg_loss:0.137, val_acc:0.930]
Epoch [27/120    avg_loss:0.127, val_acc:0.961]
Epoch [28/120    avg_loss:0.115, val_acc:0.945]
Epoch [29/120    avg_loss:0.106, val_acc:0.942]
Epoch [30/120    avg_loss:0.083, val_acc:0.972]
Epoch [31/120    avg_loss:0.064, val_acc:0.966]
Epoch [32/120    avg_loss:0.070, val_acc:0.976]
Epoch [33/120    avg_loss:0.069, val_acc:0.921]
Epoch [34/120    avg_loss:0.071, val_acc:0.970]
Epoch [35/120    avg_loss:0.093, val_acc:0.960]
Epoch [36/120    avg_loss:0.065, val_acc:0.959]
Epoch [37/120    avg_loss:0.070, val_acc:0.968]
Epoch [38/120    avg_loss:0.053, val_acc:0.964]
Epoch [39/120    avg_loss:0.046, val_acc:0.955]
Epoch [40/120    avg_loss:0.070, val_acc:0.948]
Epoch [41/120    avg_loss:0.057, val_acc:0.970]
Epoch [42/120    avg_loss:0.063, val_acc:0.968]
Epoch [43/120    avg_loss:0.043, val_acc:0.974]
Epoch [44/120    avg_loss:0.049, val_acc:0.952]
Epoch [45/120    avg_loss:0.039, val_acc:0.981]
Epoch [46/120    avg_loss:0.029, val_acc:0.982]
Epoch [47/120    avg_loss:0.043, val_acc:0.969]
Epoch [48/120    avg_loss:0.051, val_acc:0.967]
Epoch [49/120    avg_loss:0.036, val_acc:0.979]
Epoch [50/120    avg_loss:0.036, val_acc:0.982]
Epoch [51/120    avg_loss:0.043, val_acc:0.978]
Epoch [52/120    avg_loss:0.037, val_acc:0.982]
Epoch [53/120    avg_loss:0.035, val_acc:0.976]
Epoch [54/120    avg_loss:0.020, val_acc:0.975]
Epoch [55/120    avg_loss:0.022, val_acc:0.982]
Epoch [56/120    avg_loss:0.020, val_acc:0.972]
Epoch [57/120    avg_loss:0.027, val_acc:0.984]
Epoch [58/120    avg_loss:0.024, val_acc:0.974]
Epoch [59/120    avg_loss:0.015, val_acc:0.982]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.018, val_acc:0.977]
Epoch [62/120    avg_loss:0.012, val_acc:0.982]
Epoch [63/120    avg_loss:0.019, val_acc:0.982]
Epoch [64/120    avg_loss:0.024, val_acc:0.978]
Epoch [65/120    avg_loss:0.018, val_acc:0.968]
Epoch [66/120    avg_loss:0.018, val_acc:0.981]
Epoch [67/120    avg_loss:0.020, val_acc:0.972]
Epoch [68/120    avg_loss:0.019, val_acc:0.983]
Epoch [69/120    avg_loss:0.014, val_acc:0.982]
Epoch [70/120    avg_loss:0.022, val_acc:0.971]
Epoch [71/120    avg_loss:0.021, val_acc:0.984]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.016, val_acc:0.984]
Epoch [74/120    avg_loss:0.016, val_acc:0.983]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.984]
Epoch [78/120    avg_loss:0.011, val_acc:0.978]
Epoch [79/120    avg_loss:0.019, val_acc:0.982]
Epoch [80/120    avg_loss:0.011, val_acc:0.982]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.975]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.006, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     0     1     0     0     0    40    12]
 [    0     0 18064     0    13     0    12     0     1     0]
 [    0     5     0  2004     0     0     0     0    23     4]
 [    0    19    14     6  2916     0     2     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3    12     0     0  4859     0     4     0]
 [    0     3     0     0     0     0     0  1280     1     6]
 [    0    24     0     7    74     0     0     0  3466     0]
 [    0     0     0     0    12     9     0     0     0   898]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.99191417 0.9988112  0.98597786 0.9739479  0.99656357
 0.99661573 0.99610895 0.97345878 0.97661773]

Kappa:
0.9897183303720302
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10dc02a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.011, val_acc:0.180]
Epoch [2/120    avg_loss:1.669, val_acc:0.265]
Epoch [3/120    avg_loss:1.466, val_acc:0.322]
Epoch [4/120    avg_loss:1.269, val_acc:0.380]
Epoch [5/120    avg_loss:1.144, val_acc:0.430]
Epoch [6/120    avg_loss:1.015, val_acc:0.429]
Epoch [7/120    avg_loss:0.896, val_acc:0.552]
Epoch [8/120    avg_loss:0.806, val_acc:0.636]
Epoch [9/120    avg_loss:0.731, val_acc:0.597]
Epoch [10/120    avg_loss:0.642, val_acc:0.649]
Epoch [11/120    avg_loss:0.575, val_acc:0.703]
Epoch [12/120    avg_loss:0.495, val_acc:0.741]
Epoch [13/120    avg_loss:0.412, val_acc:0.759]
Epoch [14/120    avg_loss:0.385, val_acc:0.774]
Epoch [15/120    avg_loss:0.366, val_acc:0.766]
Epoch [16/120    avg_loss:0.312, val_acc:0.840]
Epoch [17/120    avg_loss:0.349, val_acc:0.789]
Epoch [18/120    avg_loss:0.282, val_acc:0.873]
Epoch [19/120    avg_loss:0.228, val_acc:0.867]
Epoch [20/120    avg_loss:0.212, val_acc:0.895]
Epoch [21/120    avg_loss:0.205, val_acc:0.909]
Epoch [22/120    avg_loss:0.191, val_acc:0.892]
Epoch [23/120    avg_loss:0.190, val_acc:0.894]
Epoch [24/120    avg_loss:0.148, val_acc:0.917]
Epoch [25/120    avg_loss:0.184, val_acc:0.922]
Epoch [26/120    avg_loss:0.151, val_acc:0.939]
Epoch [27/120    avg_loss:0.121, val_acc:0.941]
Epoch [28/120    avg_loss:0.130, val_acc:0.950]
Epoch [29/120    avg_loss:0.115, val_acc:0.933]
Epoch [30/120    avg_loss:0.105, val_acc:0.947]
Epoch [31/120    avg_loss:0.103, val_acc:0.966]
Epoch [32/120    avg_loss:0.074, val_acc:0.943]
Epoch [33/120    avg_loss:0.090, val_acc:0.959]
Epoch [34/120    avg_loss:0.068, val_acc:0.944]
Epoch [35/120    avg_loss:0.069, val_acc:0.961]
Epoch [36/120    avg_loss:0.062, val_acc:0.966]
Epoch [37/120    avg_loss:0.055, val_acc:0.965]
Epoch [38/120    avg_loss:0.051, val_acc:0.953]
Epoch [39/120    avg_loss:0.061, val_acc:0.962]
Epoch [40/120    avg_loss:0.049, val_acc:0.967]
Epoch [41/120    avg_loss:0.043, val_acc:0.969]
Epoch [42/120    avg_loss:0.036, val_acc:0.909]
Epoch [43/120    avg_loss:0.041, val_acc:0.957]
Epoch [44/120    avg_loss:0.068, val_acc:0.936]
Epoch [45/120    avg_loss:0.051, val_acc:0.963]
Epoch [46/120    avg_loss:0.033, val_acc:0.976]
Epoch [47/120    avg_loss:0.043, val_acc:0.975]
Epoch [48/120    avg_loss:0.045, val_acc:0.973]
Epoch [49/120    avg_loss:0.042, val_acc:0.974]
Epoch [50/120    avg_loss:0.042, val_acc:0.972]
Epoch [51/120    avg_loss:0.035, val_acc:0.966]
Epoch [52/120    avg_loss:0.034, val_acc:0.970]
Epoch [53/120    avg_loss:0.042, val_acc:0.974]
Epoch [54/120    avg_loss:0.022, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.972]
Epoch [56/120    avg_loss:0.019, val_acc:0.977]
Epoch [57/120    avg_loss:0.020, val_acc:0.981]
Epoch [58/120    avg_loss:0.027, val_acc:0.980]
Epoch [59/120    avg_loss:0.020, val_acc:0.970]
Epoch [60/120    avg_loss:0.015, val_acc:0.977]
Epoch [61/120    avg_loss:0.026, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.975]
Epoch [63/120    avg_loss:0.020, val_acc:0.980]
Epoch [64/120    avg_loss:0.025, val_acc:0.982]
Epoch [65/120    avg_loss:0.014, val_acc:0.982]
Epoch [66/120    avg_loss:0.014, val_acc:0.984]
Epoch [67/120    avg_loss:0.025, val_acc:0.978]
Epoch [68/120    avg_loss:0.032, val_acc:0.969]
Epoch [69/120    avg_loss:0.020, val_acc:0.981]
Epoch [70/120    avg_loss:0.014, val_acc:0.980]
Epoch [71/120    avg_loss:0.012, val_acc:0.978]
Epoch [72/120    avg_loss:0.015, val_acc:0.981]
Epoch [73/120    avg_loss:0.036, val_acc:0.978]
Epoch [74/120    avg_loss:0.033, val_acc:0.959]
Epoch [75/120    avg_loss:0.028, val_acc:0.974]
Epoch [76/120    avg_loss:0.015, val_acc:0.982]
Epoch [77/120    avg_loss:0.016, val_acc:0.981]
Epoch [78/120    avg_loss:0.013, val_acc:0.979]
Epoch [79/120    avg_loss:0.012, val_acc:0.984]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.014, val_acc:0.984]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.007, val_acc:0.982]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.982]
Epoch [88/120    avg_loss:0.015, val_acc:0.983]
Epoch [89/120    avg_loss:0.010, val_acc:0.977]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.982]
Epoch [92/120    avg_loss:0.010, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.017, val_acc:0.980]
Epoch [101/120    avg_loss:0.016, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.976]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.014, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     0     0     0     7     6    72     0]
 [    0     0 18041     0    41     0     6     0     2     0]
 [    0     5     0  2002     0     0     0     0    26     3]
 [    0    25    13     0  2911     0     2     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     0     0     0  4864     0     2     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0    20     0    10    51     0     0     0  3471    19]
 [    0     0     0     0    13    15     0     0     0   891]]

Accuracy:
99.0986431446268

F1 scores:
[       nan 0.98947697 0.99795331 0.98913043 0.97227789 0.99428571
 0.99682345 0.99651568 0.96887648 0.97217676]

Kappa:
0.9880607202724603
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d3d2bf908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.028, val_acc:0.196]
Epoch [2/120    avg_loss:1.693, val_acc:0.532]
Epoch [3/120    avg_loss:1.494, val_acc:0.602]
Epoch [4/120    avg_loss:1.283, val_acc:0.643]
Epoch [5/120    avg_loss:1.118, val_acc:0.691]
Epoch [6/120    avg_loss:0.987, val_acc:0.642]
Epoch [7/120    avg_loss:0.872, val_acc:0.652]
Epoch [8/120    avg_loss:0.791, val_acc:0.697]
Epoch [9/120    avg_loss:0.680, val_acc:0.614]
Epoch [10/120    avg_loss:0.561, val_acc:0.719]
Epoch [11/120    avg_loss:0.518, val_acc:0.751]
Epoch [12/120    avg_loss:0.438, val_acc:0.797]
Epoch [13/120    avg_loss:0.384, val_acc:0.860]
Epoch [14/120    avg_loss:0.359, val_acc:0.832]
Epoch [15/120    avg_loss:0.293, val_acc:0.905]
Epoch [16/120    avg_loss:0.255, val_acc:0.887]
Epoch [17/120    avg_loss:0.243, val_acc:0.935]
Epoch [18/120    avg_loss:0.206, val_acc:0.953]
Epoch [19/120    avg_loss:0.177, val_acc:0.946]
Epoch [20/120    avg_loss:0.196, val_acc:0.934]
Epoch [21/120    avg_loss:0.173, val_acc:0.922]
Epoch [22/120    avg_loss:0.146, val_acc:0.940]
Epoch [23/120    avg_loss:0.133, val_acc:0.953]
Epoch [24/120    avg_loss:0.128, val_acc:0.960]
Epoch [25/120    avg_loss:0.137, val_acc:0.954]
Epoch [26/120    avg_loss:0.128, val_acc:0.913]
Epoch [27/120    avg_loss:0.122, val_acc:0.970]
Epoch [28/120    avg_loss:0.095, val_acc:0.970]
Epoch [29/120    avg_loss:0.086, val_acc:0.947]
Epoch [30/120    avg_loss:0.094, val_acc:0.948]
Epoch [31/120    avg_loss:0.112, val_acc:0.903]
Epoch [32/120    avg_loss:0.108, val_acc:0.962]
Epoch [33/120    avg_loss:0.103, val_acc:0.917]
Epoch [34/120    avg_loss:0.110, val_acc:0.960]
Epoch [35/120    avg_loss:0.083, val_acc:0.962]
Epoch [36/120    avg_loss:0.067, val_acc:0.964]
Epoch [37/120    avg_loss:0.057, val_acc:0.973]
Epoch [38/120    avg_loss:0.069, val_acc:0.953]
Epoch [39/120    avg_loss:0.058, val_acc:0.966]
Epoch [40/120    avg_loss:0.058, val_acc:0.973]
Epoch [41/120    avg_loss:0.051, val_acc:0.959]
Epoch [42/120    avg_loss:0.065, val_acc:0.966]
Epoch [43/120    avg_loss:0.034, val_acc:0.975]
Epoch [44/120    avg_loss:0.029, val_acc:0.973]
Epoch [45/120    avg_loss:0.028, val_acc:0.971]
Epoch [46/120    avg_loss:0.054, val_acc:0.957]
Epoch [47/120    avg_loss:0.066, val_acc:0.945]
Epoch [48/120    avg_loss:0.068, val_acc:0.965]
Epoch [49/120    avg_loss:0.046, val_acc:0.976]
Epoch [50/120    avg_loss:0.227, val_acc:0.959]
Epoch [51/120    avg_loss:0.112, val_acc:0.941]
Epoch [52/120    avg_loss:0.086, val_acc:0.958]
Epoch [53/120    avg_loss:0.045, val_acc:0.970]
Epoch [54/120    avg_loss:0.027, val_acc:0.966]
Epoch [55/120    avg_loss:0.033, val_acc:0.975]
Epoch [56/120    avg_loss:0.032, val_acc:0.977]
Epoch [57/120    avg_loss:0.034, val_acc:0.967]
Epoch [58/120    avg_loss:0.027, val_acc:0.972]
Epoch [59/120    avg_loss:0.021, val_acc:0.976]
Epoch [60/120    avg_loss:0.019, val_acc:0.981]
Epoch [61/120    avg_loss:0.018, val_acc:0.980]
Epoch [62/120    avg_loss:0.016, val_acc:0.985]
Epoch [63/120    avg_loss:0.018, val_acc:0.981]
Epoch [64/120    avg_loss:0.026, val_acc:0.981]
Epoch [65/120    avg_loss:0.023, val_acc:0.975]
Epoch [66/120    avg_loss:0.028, val_acc:0.977]
Epoch [67/120    avg_loss:0.037, val_acc:0.978]
Epoch [68/120    avg_loss:0.028, val_acc:0.979]
Epoch [69/120    avg_loss:0.031, val_acc:0.980]
Epoch [70/120    avg_loss:0.019, val_acc:0.975]
Epoch [71/120    avg_loss:0.018, val_acc:0.984]
Epoch [72/120    avg_loss:0.017, val_acc:0.978]
Epoch [73/120    avg_loss:0.020, val_acc:0.978]
Epoch [74/120    avg_loss:0.021, val_acc:0.979]
Epoch [75/120    avg_loss:0.052, val_acc:0.966]
Epoch [76/120    avg_loss:0.036, val_acc:0.973]
Epoch [77/120    avg_loss:0.026, val_acc:0.978]
Epoch [78/120    avg_loss:0.024, val_acc:0.978]
Epoch [79/120    avg_loss:0.021, val_acc:0.979]
Epoch [80/120    avg_loss:0.018, val_acc:0.978]
Epoch [81/120    avg_loss:0.024, val_acc:0.979]
Epoch [82/120    avg_loss:0.019, val_acc:0.984]
Epoch [83/120    avg_loss:0.018, val_acc:0.983]
Epoch [84/120    avg_loss:0.017, val_acc:0.981]
Epoch [85/120    avg_loss:0.015, val_acc:0.984]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.015, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.016, val_acc:0.984]
Epoch [92/120    avg_loss:0.012, val_acc:0.984]
Epoch [93/120    avg_loss:0.013, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.984]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.984]
Epoch [100/120    avg_loss:0.012, val_acc:0.984]
Epoch [101/120    avg_loss:0.012, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.984]
Epoch [103/120    avg_loss:0.013, val_acc:0.984]
Epoch [104/120    avg_loss:0.012, val_acc:0.984]
Epoch [105/120    avg_loss:0.017, val_acc:0.984]
Epoch [106/120    avg_loss:0.013, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.016, val_acc:0.984]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.014, val_acc:0.984]
Epoch [113/120    avg_loss:0.013, val_acc:0.984]
Epoch [114/120    avg_loss:0.015, val_acc:0.984]
Epoch [115/120    avg_loss:0.013, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.984]
Epoch [118/120    avg_loss:0.012, val_acc:0.984]
Epoch [119/120    avg_loss:0.013, val_acc:0.984]
Epoch [120/120    avg_loss:0.012, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6296     0     8     4     0    12     2   100    10]
 [    0     2 18066     0    19     0     3     0     0     0]
 [    0     2     0  1945     2     0     0     0    83     4]
 [    0    38     2     0  2901     0     4     0    24     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    34     0     0     0  4841     0     3     0]
 [    0     0     0     0     0     0     3  1280     0     7]
 [    0    28     0    42    53     0     0     0  3440     8]
 [    0     0     0     0     7    46     0     0     0   866]]

Accuracy:
98.66724507748295

F1 scores:
[       nan 0.98390373 0.99834218 0.96502109 0.97381672 0.98268072
 0.99394313 0.99533437 0.95277662 0.95321959]

Kappa:
0.9823403274730369
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72b0afe908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.029, val_acc:0.125]
Epoch [2/120    avg_loss:1.686, val_acc:0.143]
Epoch [3/120    avg_loss:1.467, val_acc:0.269]
Epoch [4/120    avg_loss:1.312, val_acc:0.366]
Epoch [5/120    avg_loss:1.152, val_acc:0.432]
Epoch [6/120    avg_loss:0.991, val_acc:0.522]
Epoch [7/120    avg_loss:0.855, val_acc:0.670]
Epoch [8/120    avg_loss:0.722, val_acc:0.771]
Epoch [9/120    avg_loss:0.629, val_acc:0.714]
Epoch [10/120    avg_loss:0.566, val_acc:0.816]
Epoch [11/120    avg_loss:0.508, val_acc:0.793]
Epoch [12/120    avg_loss:0.460, val_acc:0.826]
Epoch [13/120    avg_loss:0.397, val_acc:0.831]
Epoch [14/120    avg_loss:0.369, val_acc:0.824]
Epoch [15/120    avg_loss:0.323, val_acc:0.854]
Epoch [16/120    avg_loss:0.296, val_acc:0.882]
Epoch [17/120    avg_loss:0.287, val_acc:0.891]
Epoch [18/120    avg_loss:0.262, val_acc:0.898]
Epoch [19/120    avg_loss:0.227, val_acc:0.881]
Epoch [20/120    avg_loss:0.215, val_acc:0.915]
Epoch [21/120    avg_loss:0.192, val_acc:0.923]
Epoch [22/120    avg_loss:0.178, val_acc:0.942]
Epoch [23/120    avg_loss:0.155, val_acc:0.934]
Epoch [24/120    avg_loss:0.137, val_acc:0.959]
Epoch [25/120    avg_loss:0.143, val_acc:0.942]
Epoch [26/120    avg_loss:0.166, val_acc:0.932]
Epoch [27/120    avg_loss:0.109, val_acc:0.966]
Epoch [28/120    avg_loss:0.098, val_acc:0.919]
Epoch [29/120    avg_loss:0.099, val_acc:0.970]
Epoch [30/120    avg_loss:0.070, val_acc:0.970]
Epoch [31/120    avg_loss:0.076, val_acc:0.966]
Epoch [32/120    avg_loss:0.083, val_acc:0.952]
Epoch [33/120    avg_loss:0.068, val_acc:0.953]
Epoch [34/120    avg_loss:0.066, val_acc:0.959]
Epoch [35/120    avg_loss:0.068, val_acc:0.966]
Epoch [36/120    avg_loss:0.048, val_acc:0.966]
Epoch [37/120    avg_loss:0.070, val_acc:0.956]
Epoch [38/120    avg_loss:0.060, val_acc:0.969]
Epoch [39/120    avg_loss:0.048, val_acc:0.970]
Epoch [40/120    avg_loss:0.035, val_acc:0.973]
Epoch [41/120    avg_loss:0.048, val_acc:0.971]
Epoch [42/120    avg_loss:0.033, val_acc:0.977]
Epoch [43/120    avg_loss:0.037, val_acc:0.966]
Epoch [44/120    avg_loss:0.035, val_acc:0.977]
Epoch [45/120    avg_loss:0.045, val_acc:0.972]
Epoch [46/120    avg_loss:0.038, val_acc:0.984]
Epoch [47/120    avg_loss:0.028, val_acc:0.952]
Epoch [48/120    avg_loss:0.029, val_acc:0.978]
Epoch [49/120    avg_loss:0.025, val_acc:0.982]
Epoch [50/120    avg_loss:0.022, val_acc:0.978]
Epoch [51/120    avg_loss:0.020, val_acc:0.989]
Epoch [52/120    avg_loss:0.022, val_acc:0.977]
Epoch [53/120    avg_loss:0.044, val_acc:0.969]
Epoch [54/120    avg_loss:0.022, val_acc:0.988]
Epoch [55/120    avg_loss:0.038, val_acc:0.970]
Epoch [56/120    avg_loss:0.020, val_acc:0.975]
Epoch [57/120    avg_loss:0.022, val_acc:0.982]
Epoch [58/120    avg_loss:0.031, val_acc:0.973]
Epoch [59/120    avg_loss:0.031, val_acc:0.987]
Epoch [60/120    avg_loss:0.026, val_acc:0.976]
Epoch [61/120    avg_loss:0.029, val_acc:0.961]
Epoch [62/120    avg_loss:0.041, val_acc:0.981]
Epoch [63/120    avg_loss:0.050, val_acc:0.975]
Epoch [64/120    avg_loss:0.049, val_acc:0.974]
Epoch [65/120    avg_loss:0.029, val_acc:0.978]
Epoch [66/120    avg_loss:0.021, val_acc:0.977]
Epoch [67/120    avg_loss:0.017, val_acc:0.979]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.017, val_acc:0.983]
Epoch [70/120    avg_loss:0.014, val_acc:0.984]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.016, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.987]
Epoch [74/120    avg_loss:0.012, val_acc:0.987]
Epoch [75/120    avg_loss:0.014, val_acc:0.987]
Epoch [76/120    avg_loss:0.015, val_acc:0.986]
Epoch [77/120    avg_loss:0.014, val_acc:0.987]
Epoch [78/120    avg_loss:0.014, val_acc:0.988]
Epoch [79/120    avg_loss:0.011, val_acc:0.988]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.014, val_acc:0.987]
Epoch [82/120    avg_loss:0.014, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.987]
Epoch [84/120    avg_loss:0.012, val_acc:0.987]
Epoch [85/120    avg_loss:0.013, val_acc:0.987]
Epoch [86/120    avg_loss:0.012, val_acc:0.987]
Epoch [87/120    avg_loss:0.014, val_acc:0.987]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.012, val_acc:0.987]
Epoch [90/120    avg_loss:0.017, val_acc:0.987]
Epoch [91/120    avg_loss:0.015, val_acc:0.987]
Epoch [92/120    avg_loss:0.017, val_acc:0.987]
Epoch [93/120    avg_loss:0.014, val_acc:0.987]
Epoch [94/120    avg_loss:0.012, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.987]
Epoch [96/120    avg_loss:0.011, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.012, val_acc:0.987]
Epoch [102/120    avg_loss:0.014, val_acc:0.987]
Epoch [103/120    avg_loss:0.010, val_acc:0.987]
Epoch [104/120    avg_loss:0.014, val_acc:0.987]
Epoch [105/120    avg_loss:0.011, val_acc:0.987]
Epoch [106/120    avg_loss:0.013, val_acc:0.987]
Epoch [107/120    avg_loss:0.010, val_acc:0.987]
Epoch [108/120    avg_loss:0.012, val_acc:0.987]
Epoch [109/120    avg_loss:0.013, val_acc:0.987]
Epoch [110/120    avg_loss:0.013, val_acc:0.987]
Epoch [111/120    avg_loss:0.016, val_acc:0.987]
Epoch [112/120    avg_loss:0.012, val_acc:0.987]
Epoch [113/120    avg_loss:0.014, val_acc:0.987]
Epoch [114/120    avg_loss:0.012, val_acc:0.987]
Epoch [115/120    avg_loss:0.012, val_acc:0.987]
Epoch [116/120    avg_loss:0.011, val_acc:0.987]
Epoch [117/120    avg_loss:0.012, val_acc:0.987]
Epoch [118/120    avg_loss:0.012, val_acc:0.987]
Epoch [119/120    avg_loss:0.014, val_acc:0.987]
Epoch [120/120    avg_loss:0.011, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     0     1     0     0    35    15     2]
 [    0     0 17912     0    72     0   106     0     0     0]
 [    0     8     0  2006     0     0     0     0    19     3]
 [    0    25    20     0  2892     0     8     0    22     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5    10     0     0  4862     0     1     0]
 [    0     3     0     0     0     0     0  1286     0     1]
 [    0    34     0    10    51     0     0     0  3463    13]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
98.76605692526451

F1 scores:
[       nan 0.99045105 0.99436534 0.98769079 0.96367877 0.98901099
 0.98680739 0.98506319 0.97673107 0.96316658]

Kappa:
0.9836765211101958
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0ef305860>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.151, val_acc:0.100]
Epoch [2/120    avg_loss:1.758, val_acc:0.205]
Epoch [3/120    avg_loss:1.516, val_acc:0.246]
Epoch [4/120    avg_loss:1.327, val_acc:0.391]
Epoch [5/120    avg_loss:1.166, val_acc:0.468]
Epoch [6/120    avg_loss:1.066, val_acc:0.611]
Epoch [7/120    avg_loss:0.942, val_acc:0.613]
Epoch [8/120    avg_loss:0.854, val_acc:0.676]
Epoch [9/120    avg_loss:0.748, val_acc:0.690]
Epoch [10/120    avg_loss:0.633, val_acc:0.635]
Epoch [11/120    avg_loss:0.589, val_acc:0.707]
Epoch [12/120    avg_loss:0.586, val_acc:0.684]
Epoch [13/120    avg_loss:0.481, val_acc:0.760]
Epoch [14/120    avg_loss:0.416, val_acc:0.772]
Epoch [15/120    avg_loss:0.378, val_acc:0.740]
Epoch [16/120    avg_loss:0.344, val_acc:0.771]
Epoch [17/120    avg_loss:0.312, val_acc:0.831]
Epoch [18/120    avg_loss:0.314, val_acc:0.805]
Epoch [19/120    avg_loss:0.277, val_acc:0.848]
Epoch [20/120    avg_loss:0.244, val_acc:0.826]
Epoch [21/120    avg_loss:0.225, val_acc:0.891]
Epoch [22/120    avg_loss:0.208, val_acc:0.923]
Epoch [23/120    avg_loss:0.176, val_acc:0.935]
Epoch [24/120    avg_loss:0.163, val_acc:0.933]
Epoch [25/120    avg_loss:0.153, val_acc:0.906]
Epoch [26/120    avg_loss:0.128, val_acc:0.946]
Epoch [27/120    avg_loss:0.120, val_acc:0.924]
Epoch [28/120    avg_loss:0.102, val_acc:0.949]
Epoch [29/120    avg_loss:0.109, val_acc:0.964]
Epoch [30/120    avg_loss:0.124, val_acc:0.955]
Epoch [31/120    avg_loss:0.088, val_acc:0.923]
Epoch [32/120    avg_loss:0.092, val_acc:0.948]
Epoch [33/120    avg_loss:0.075, val_acc:0.954]
Epoch [34/120    avg_loss:0.069, val_acc:0.954]
Epoch [35/120    avg_loss:0.103, val_acc:0.960]
Epoch [36/120    avg_loss:0.074, val_acc:0.966]
Epoch [37/120    avg_loss:0.061, val_acc:0.960]
Epoch [38/120    avg_loss:0.077, val_acc:0.955]
Epoch [39/120    avg_loss:0.055, val_acc:0.977]
Epoch [40/120    avg_loss:0.096, val_acc:0.929]
Epoch [41/120    avg_loss:0.066, val_acc:0.966]
Epoch [42/120    avg_loss:0.053, val_acc:0.974]
Epoch [43/120    avg_loss:0.043, val_acc:0.970]
Epoch [44/120    avg_loss:0.033, val_acc:0.976]
Epoch [45/120    avg_loss:0.038, val_acc:0.973]
Epoch [46/120    avg_loss:0.053, val_acc:0.947]
Epoch [47/120    avg_loss:0.061, val_acc:0.963]
Epoch [48/120    avg_loss:0.048, val_acc:0.959]
Epoch [49/120    avg_loss:0.038, val_acc:0.978]
Epoch [50/120    avg_loss:0.028, val_acc:0.973]
Epoch [51/120    avg_loss:0.034, val_acc:0.978]
Epoch [52/120    avg_loss:0.044, val_acc:0.969]
Epoch [53/120    avg_loss:0.026, val_acc:0.979]
Epoch [54/120    avg_loss:0.024, val_acc:0.980]
Epoch [55/120    avg_loss:0.026, val_acc:0.985]
Epoch [56/120    avg_loss:0.023, val_acc:0.985]
Epoch [57/120    avg_loss:0.021, val_acc:0.983]
Epoch [58/120    avg_loss:0.016, val_acc:0.985]
Epoch [59/120    avg_loss:0.025, val_acc:0.975]
Epoch [60/120    avg_loss:0.025, val_acc:0.947]
Epoch [61/120    avg_loss:0.021, val_acc:0.978]
Epoch [62/120    avg_loss:0.026, val_acc:0.978]
Epoch [63/120    avg_loss:0.024, val_acc:0.984]
Epoch [64/120    avg_loss:0.028, val_acc:0.972]
Epoch [65/120    avg_loss:0.027, val_acc:0.974]
Epoch [66/120    avg_loss:0.024, val_acc:0.984]
Epoch [67/120    avg_loss:0.017, val_acc:0.986]
Epoch [68/120    avg_loss:0.017, val_acc:0.969]
Epoch [69/120    avg_loss:0.015, val_acc:0.985]
Epoch [70/120    avg_loss:0.014, val_acc:0.972]
Epoch [71/120    avg_loss:0.026, val_acc:0.977]
Epoch [72/120    avg_loss:0.018, val_acc:0.983]
Epoch [73/120    avg_loss:0.022, val_acc:0.972]
Epoch [74/120    avg_loss:0.019, val_acc:0.983]
Epoch [75/120    avg_loss:0.013, val_acc:0.985]
Epoch [76/120    avg_loss:0.016, val_acc:0.986]
Epoch [77/120    avg_loss:0.019, val_acc:0.981]
Epoch [78/120    avg_loss:0.018, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.986]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.015, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.978]
Epoch [84/120    avg_loss:0.017, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.987]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.017, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.014, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.973]
Epoch [96/120    avg_loss:0.019, val_acc:0.948]
Epoch [97/120    avg_loss:0.016, val_acc:0.987]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.010, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.004, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0     0     8    28     0]
 [    0     5 18007     0    61     0    13     0     4     0]
 [    0     0     0  2033     0     0     0     0     1     2]
 [    0    31    26     0  2888     0    13     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     2     0     0  4847     0     0     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0    13     0    16    34     0     0     0  3501     7]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.15166413611935

F1 scores:
[       nan 0.99332194 0.99618278 0.99486176 0.96766628 0.98863636
 0.99415445 0.99652107 0.98356511 0.97060455]

Kappa:
0.9887625153959236
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c33a5e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.075, val_acc:0.087]
Epoch [2/120    avg_loss:1.683, val_acc:0.133]
Epoch [3/120    avg_loss:1.425, val_acc:0.356]
Epoch [4/120    avg_loss:1.250, val_acc:0.462]
Epoch [5/120    avg_loss:1.121, val_acc:0.498]
Epoch [6/120    avg_loss:0.973, val_acc:0.564]
Epoch [7/120    avg_loss:0.845, val_acc:0.578]
Epoch [8/120    avg_loss:0.752, val_acc:0.655]
Epoch [9/120    avg_loss:0.673, val_acc:0.688]
Epoch [10/120    avg_loss:0.599, val_acc:0.707]
Epoch [11/120    avg_loss:0.528, val_acc:0.748]
Epoch [12/120    avg_loss:0.490, val_acc:0.755]
Epoch [13/120    avg_loss:0.434, val_acc:0.760]
Epoch [14/120    avg_loss:0.386, val_acc:0.787]
Epoch [15/120    avg_loss:0.330, val_acc:0.799]
Epoch [16/120    avg_loss:0.300, val_acc:0.857]
Epoch [17/120    avg_loss:0.271, val_acc:0.877]
Epoch [18/120    avg_loss:0.269, val_acc:0.893]
Epoch [19/120    avg_loss:0.242, val_acc:0.897]
Epoch [20/120    avg_loss:0.235, val_acc:0.917]
Epoch [21/120    avg_loss:0.245, val_acc:0.900]
Epoch [22/120    avg_loss:0.208, val_acc:0.927]
Epoch [23/120    avg_loss:0.241, val_acc:0.867]
Epoch [24/120    avg_loss:0.195, val_acc:0.925]
Epoch [25/120    avg_loss:0.169, val_acc:0.903]
Epoch [26/120    avg_loss:0.134, val_acc:0.950]
Epoch [27/120    avg_loss:0.146, val_acc:0.947]
Epoch [28/120    avg_loss:0.109, val_acc:0.939]
Epoch [29/120    avg_loss:0.129, val_acc:0.965]
Epoch [30/120    avg_loss:0.093, val_acc:0.963]
Epoch [31/120    avg_loss:0.093, val_acc:0.943]
Epoch [32/120    avg_loss:0.090, val_acc:0.968]
Epoch [33/120    avg_loss:0.098, val_acc:0.958]
Epoch [34/120    avg_loss:0.077, val_acc:0.967]
Epoch [35/120    avg_loss:0.059, val_acc:0.968]
Epoch [36/120    avg_loss:0.058, val_acc:0.972]
Epoch [37/120    avg_loss:0.072, val_acc:0.958]
Epoch [38/120    avg_loss:0.074, val_acc:0.965]
Epoch [39/120    avg_loss:0.082, val_acc:0.963]
Epoch [40/120    avg_loss:0.070, val_acc:0.924]
Epoch [41/120    avg_loss:0.071, val_acc:0.966]
Epoch [42/120    avg_loss:0.057, val_acc:0.978]
Epoch [43/120    avg_loss:0.040, val_acc:0.977]
Epoch [44/120    avg_loss:0.041, val_acc:0.978]
Epoch [45/120    avg_loss:0.039, val_acc:0.976]
Epoch [46/120    avg_loss:0.036, val_acc:0.963]
Epoch [47/120    avg_loss:0.041, val_acc:0.958]
Epoch [48/120    avg_loss:0.053, val_acc:0.978]
Epoch [49/120    avg_loss:0.033, val_acc:0.976]
Epoch [50/120    avg_loss:0.043, val_acc:0.968]
Epoch [51/120    avg_loss:0.031, val_acc:0.977]
Epoch [52/120    avg_loss:0.023, val_acc:0.981]
Epoch [53/120    avg_loss:0.030, val_acc:0.977]
Epoch [54/120    avg_loss:0.054, val_acc:0.960]
Epoch [55/120    avg_loss:0.058, val_acc:0.968]
Epoch [56/120    avg_loss:0.041, val_acc:0.980]
Epoch [57/120    avg_loss:0.056, val_acc:0.972]
Epoch [58/120    avg_loss:0.050, val_acc:0.978]
Epoch [59/120    avg_loss:0.028, val_acc:0.978]
Epoch [60/120    avg_loss:0.035, val_acc:0.970]
Epoch [61/120    avg_loss:0.035, val_acc:0.979]
Epoch [62/120    avg_loss:0.039, val_acc:0.963]
Epoch [63/120    avg_loss:0.046, val_acc:0.975]
Epoch [64/120    avg_loss:0.045, val_acc:0.977]
Epoch [65/120    avg_loss:0.028, val_acc:0.969]
Epoch [66/120    avg_loss:0.018, val_acc:0.977]
Epoch [67/120    avg_loss:0.019, val_acc:0.978]
Epoch [68/120    avg_loss:0.013, val_acc:0.979]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.017, val_acc:0.981]
Epoch [71/120    avg_loss:0.014, val_acc:0.982]
Epoch [72/120    avg_loss:0.016, val_acc:0.983]
Epoch [73/120    avg_loss:0.012, val_acc:0.983]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.013, val_acc:0.983]
Epoch [77/120    avg_loss:0.013, val_acc:0.985]
Epoch [78/120    avg_loss:0.014, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.015, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.014, val_acc:0.983]
Epoch [89/120    avg_loss:0.010, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.012, val_acc:0.983]
Epoch [93/120    avg_loss:0.015, val_acc:0.984]
Epoch [94/120    avg_loss:0.013, val_acc:0.985]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.985]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.013, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.013, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.010, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.013, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.010, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6345     0     0     8     0    21     0    54     4]
 [    0     1 18049     0    37     0     2     0     1     0]
 [    0     4     0  1941     0     0     0     0    83     8]
 [    0    16    18     1  2904     0    11     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     3     0     0     0     0     0  1281     0     6]
 [    0    25     0    21    52     0     0     0  3467     6]
 [    0     0     0     0    14    37     0     0     0   868]]

Accuracy:
98.90342949413154

F1 scores:
[       nan 0.98939654 0.99836823 0.97074269 0.97010189 0.98602191
 0.99652707 0.99649942 0.96332315 0.95858642]

Kappa:
0.9854731731010203
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d45bcc8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.089, val_acc:0.191]
Epoch [2/120    avg_loss:1.649, val_acc:0.403]
Epoch [3/120    avg_loss:1.408, val_acc:0.511]
Epoch [4/120    avg_loss:1.219, val_acc:0.579]
Epoch [5/120    avg_loss:1.044, val_acc:0.629]
Epoch [6/120    avg_loss:0.906, val_acc:0.672]
Epoch [7/120    avg_loss:0.776, val_acc:0.660]
Epoch [8/120    avg_loss:0.673, val_acc:0.631]
Epoch [9/120    avg_loss:0.586, val_acc:0.650]
Epoch [10/120    avg_loss:0.525, val_acc:0.690]
Epoch [11/120    avg_loss:0.448, val_acc:0.707]
Epoch [12/120    avg_loss:0.393, val_acc:0.756]
Epoch [13/120    avg_loss:0.359, val_acc:0.730]
Epoch [14/120    avg_loss:0.336, val_acc:0.777]
Epoch [15/120    avg_loss:0.311, val_acc:0.788]
Epoch [16/120    avg_loss:0.278, val_acc:0.777]
Epoch [17/120    avg_loss:0.276, val_acc:0.807]
Epoch [18/120    avg_loss:0.253, val_acc:0.795]
Epoch [19/120    avg_loss:0.234, val_acc:0.833]
Epoch [20/120    avg_loss:0.207, val_acc:0.870]
Epoch [21/120    avg_loss:0.191, val_acc:0.881]
Epoch [22/120    avg_loss:0.178, val_acc:0.853]
Epoch [23/120    avg_loss:0.157, val_acc:0.868]
Epoch [24/120    avg_loss:0.159, val_acc:0.933]
Epoch [25/120    avg_loss:0.139, val_acc:0.959]
Epoch [26/120    avg_loss:0.123, val_acc:0.958]
Epoch [27/120    avg_loss:0.108, val_acc:0.939]
Epoch [28/120    avg_loss:0.121, val_acc:0.966]
Epoch [29/120    avg_loss:0.098, val_acc:0.972]
Epoch [30/120    avg_loss:0.094, val_acc:0.956]
Epoch [31/120    avg_loss:0.077, val_acc:0.926]
Epoch [32/120    avg_loss:0.088, val_acc:0.962]
Epoch [33/120    avg_loss:0.075, val_acc:0.962]
Epoch [34/120    avg_loss:0.072, val_acc:0.969]
Epoch [35/120    avg_loss:0.087, val_acc:0.940]
Epoch [36/120    avg_loss:0.079, val_acc:0.980]
Epoch [37/120    avg_loss:0.058, val_acc:0.989]
Epoch [38/120    avg_loss:0.043, val_acc:0.984]
Epoch [39/120    avg_loss:0.054, val_acc:0.979]
Epoch [40/120    avg_loss:0.041, val_acc:0.984]
Epoch [41/120    avg_loss:0.036, val_acc:0.986]
Epoch [42/120    avg_loss:0.036, val_acc:0.973]
Epoch [43/120    avg_loss:0.036, val_acc:0.983]
Epoch [44/120    avg_loss:0.027, val_acc:0.987]
Epoch [45/120    avg_loss:0.026, val_acc:0.984]
Epoch [46/120    avg_loss:0.024, val_acc:0.985]
Epoch [47/120    avg_loss:0.026, val_acc:0.989]
Epoch [48/120    avg_loss:0.024, val_acc:0.984]
Epoch [49/120    avg_loss:0.020, val_acc:0.989]
Epoch [50/120    avg_loss:0.021, val_acc:0.984]
Epoch [51/120    avg_loss:0.025, val_acc:0.984]
Epoch [52/120    avg_loss:0.025, val_acc:0.989]
Epoch [53/120    avg_loss:0.021, val_acc:0.988]
Epoch [54/120    avg_loss:0.018, val_acc:0.987]
Epoch [55/120    avg_loss:0.015, val_acc:0.990]
Epoch [56/120    avg_loss:0.015, val_acc:0.982]
Epoch [57/120    avg_loss:0.017, val_acc:0.991]
Epoch [58/120    avg_loss:0.014, val_acc:0.991]
Epoch [59/120    avg_loss:0.018, val_acc:0.978]
Epoch [60/120    avg_loss:0.030, val_acc:0.967]
Epoch [61/120    avg_loss:0.016, val_acc:0.987]
Epoch [62/120    avg_loss:0.011, val_acc:0.989]
Epoch [63/120    avg_loss:0.011, val_acc:0.990]
Epoch [64/120    avg_loss:0.016, val_acc:0.986]
Epoch [65/120    avg_loss:0.022, val_acc:0.989]
Epoch [66/120    avg_loss:0.030, val_acc:0.980]
Epoch [67/120    avg_loss:0.026, val_acc:0.979]
Epoch [68/120    avg_loss:0.035, val_acc:0.979]
Epoch [69/120    avg_loss:0.029, val_acc:0.981]
Epoch [70/120    avg_loss:0.032, val_acc:0.979]
Epoch [71/120    avg_loss:0.035, val_acc:0.986]
Epoch [72/120    avg_loss:0.024, val_acc:0.990]
Epoch [73/120    avg_loss:0.015, val_acc:0.993]
Epoch [74/120    avg_loss:0.016, val_acc:0.993]
Epoch [75/120    avg_loss:0.014, val_acc:0.993]
Epoch [76/120    avg_loss:0.012, val_acc:0.994]
Epoch [77/120    avg_loss:0.012, val_acc:0.994]
Epoch [78/120    avg_loss:0.010, val_acc:0.993]
Epoch [79/120    avg_loss:0.011, val_acc:0.994]
Epoch [80/120    avg_loss:0.012, val_acc:0.992]
Epoch [81/120    avg_loss:0.011, val_acc:0.992]
Epoch [82/120    avg_loss:0.011, val_acc:0.992]
Epoch [83/120    avg_loss:0.011, val_acc:0.992]
Epoch [84/120    avg_loss:0.011, val_acc:0.992]
Epoch [85/120    avg_loss:0.009, val_acc:0.992]
Epoch [86/120    avg_loss:0.010, val_acc:0.992]
Epoch [87/120    avg_loss:0.009, val_acc:0.993]
Epoch [88/120    avg_loss:0.013, val_acc:0.991]
Epoch [89/120    avg_loss:0.010, val_acc:0.991]
Epoch [90/120    avg_loss:0.010, val_acc:0.991]
Epoch [91/120    avg_loss:0.009, val_acc:0.991]
Epoch [92/120    avg_loss:0.009, val_acc:0.992]
Epoch [93/120    avg_loss:0.010, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.991]
Epoch [95/120    avg_loss:0.008, val_acc:0.991]
Epoch [96/120    avg_loss:0.008, val_acc:0.991]
Epoch [97/120    avg_loss:0.009, val_acc:0.991]
Epoch [98/120    avg_loss:0.009, val_acc:0.991]
Epoch [99/120    avg_loss:0.008, val_acc:0.991]
Epoch [100/120    avg_loss:0.008, val_acc:0.991]
Epoch [101/120    avg_loss:0.009, val_acc:0.991]
Epoch [102/120    avg_loss:0.010, val_acc:0.991]
Epoch [103/120    avg_loss:0.008, val_acc:0.991]
Epoch [104/120    avg_loss:0.010, val_acc:0.991]
Epoch [105/120    avg_loss:0.009, val_acc:0.991]
Epoch [106/120    avg_loss:0.009, val_acc:0.991]
Epoch [107/120    avg_loss:0.008, val_acc:0.991]
Epoch [108/120    avg_loss:0.010, val_acc:0.991]
Epoch [109/120    avg_loss:0.008, val_acc:0.991]
Epoch [110/120    avg_loss:0.009, val_acc:0.991]
Epoch [111/120    avg_loss:0.009, val_acc:0.991]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.008, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.991]
Epoch [116/120    avg_loss:0.008, val_acc:0.991]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.008, val_acc:0.991]
Epoch [119/120    avg_loss:0.008, val_acc:0.991]
Epoch [120/120    avg_loss:0.009, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     0     1     0     0    14    24     3]
 [    0     3 18063     0    18     0     3     0     3     0]
 [    0     8     0  1989     0     0     0     0    38     1]
 [    0    31    22     0  2896     0     0     0    22     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24    11     0     0  4842     0     1     0]
 [    0     2     0     0     0     0     1  1276     0    11]
 [    0    20     0    12    53     0     0     0  3451    35]
 [    0     0     0     0    15    54     0     0     0   850]]

Accuracy:
98.96127057575977

F1 scores:
[       nan 0.99177402 0.99798337 0.98270751 0.97262804 0.97972973
 0.99588647 0.98914729 0.97074543 0.93406593]

Kappa:
0.9862335313280338
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc8f6aaa908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.101, val_acc:0.195]
Epoch [2/120    avg_loss:1.729, val_acc:0.228]
Epoch [3/120    avg_loss:1.486, val_acc:0.278]
Epoch [4/120    avg_loss:1.269, val_acc:0.345]
Epoch [5/120    avg_loss:1.115, val_acc:0.388]
Epoch [6/120    avg_loss:0.991, val_acc:0.554]
Epoch [7/120    avg_loss:0.856, val_acc:0.672]
Epoch [8/120    avg_loss:0.724, val_acc:0.716]
Epoch [9/120    avg_loss:0.609, val_acc:0.769]
Epoch [10/120    avg_loss:0.555, val_acc:0.765]
Epoch [11/120    avg_loss:0.456, val_acc:0.803]
Epoch [12/120    avg_loss:0.399, val_acc:0.855]
Epoch [13/120    avg_loss:0.400, val_acc:0.867]
Epoch [14/120    avg_loss:0.356, val_acc:0.902]
Epoch [15/120    avg_loss:0.307, val_acc:0.896]
Epoch [16/120    avg_loss:0.270, val_acc:0.887]
Epoch [17/120    avg_loss:0.248, val_acc:0.928]
Epoch [18/120    avg_loss:0.221, val_acc:0.889]
Epoch [19/120    avg_loss:0.226, val_acc:0.932]
Epoch [20/120    avg_loss:0.177, val_acc:0.940]
Epoch [21/120    avg_loss:0.173, val_acc:0.940]
Epoch [22/120    avg_loss:0.146, val_acc:0.963]
Epoch [23/120    avg_loss:0.166, val_acc:0.912]
Epoch [24/120    avg_loss:0.145, val_acc:0.892]
Epoch [25/120    avg_loss:0.118, val_acc:0.952]
Epoch [26/120    avg_loss:0.102, val_acc:0.974]
Epoch [27/120    avg_loss:0.093, val_acc:0.957]
Epoch [28/120    avg_loss:0.114, val_acc:0.972]
Epoch [29/120    avg_loss:0.068, val_acc:0.967]
Epoch [30/120    avg_loss:0.065, val_acc:0.967]
Epoch [31/120    avg_loss:0.057, val_acc:0.962]
Epoch [32/120    avg_loss:0.072, val_acc:0.945]
Epoch [33/120    avg_loss:0.081, val_acc:0.963]
Epoch [34/120    avg_loss:0.077, val_acc:0.959]
Epoch [35/120    avg_loss:0.055, val_acc:0.969]
Epoch [36/120    avg_loss:0.059, val_acc:0.969]
Epoch [37/120    avg_loss:0.042, val_acc:0.962]
Epoch [38/120    avg_loss:0.044, val_acc:0.976]
Epoch [39/120    avg_loss:0.040, val_acc:0.977]
Epoch [40/120    avg_loss:0.051, val_acc:0.974]
Epoch [41/120    avg_loss:0.038, val_acc:0.977]
Epoch [42/120    avg_loss:0.048, val_acc:0.972]
Epoch [43/120    avg_loss:0.043, val_acc:0.972]
Epoch [44/120    avg_loss:0.048, val_acc:0.961]
Epoch [45/120    avg_loss:0.054, val_acc:0.967]
Epoch [46/120    avg_loss:0.071, val_acc:0.953]
Epoch [47/120    avg_loss:0.079, val_acc:0.970]
Epoch [48/120    avg_loss:0.051, val_acc:0.972]
Epoch [49/120    avg_loss:0.034, val_acc:0.982]
Epoch [50/120    avg_loss:0.038, val_acc:0.973]
Epoch [51/120    avg_loss:0.045, val_acc:0.978]
Epoch [52/120    avg_loss:0.043, val_acc:0.970]
Epoch [53/120    avg_loss:0.027, val_acc:0.983]
Epoch [54/120    avg_loss:0.025, val_acc:0.983]
Epoch [55/120    avg_loss:0.037, val_acc:0.972]
Epoch [56/120    avg_loss:0.035, val_acc:0.982]
Epoch [57/120    avg_loss:0.026, val_acc:0.982]
Epoch [58/120    avg_loss:0.015, val_acc:0.983]
Epoch [59/120    avg_loss:0.032, val_acc:0.978]
Epoch [60/120    avg_loss:0.023, val_acc:0.984]
Epoch [61/120    avg_loss:0.018, val_acc:0.975]
Epoch [62/120    avg_loss:0.017, val_acc:0.947]
Epoch [63/120    avg_loss:0.016, val_acc:0.986]
Epoch [64/120    avg_loss:0.019, val_acc:0.983]
Epoch [65/120    avg_loss:0.022, val_acc:0.980]
Epoch [66/120    avg_loss:0.036, val_acc:0.984]
Epoch [67/120    avg_loss:0.019, val_acc:0.983]
Epoch [68/120    avg_loss:0.020, val_acc:0.986]
Epoch [69/120    avg_loss:0.015, val_acc:0.980]
Epoch [70/120    avg_loss:0.024, val_acc:0.985]
Epoch [71/120    avg_loss:0.012, val_acc:0.973]
Epoch [72/120    avg_loss:0.017, val_acc:0.988]
Epoch [73/120    avg_loss:0.016, val_acc:0.990]
Epoch [74/120    avg_loss:0.019, val_acc:0.991]
Epoch [75/120    avg_loss:0.014, val_acc:0.989]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.038, val_acc:0.974]
Epoch [78/120    avg_loss:0.038, val_acc:0.968]
Epoch [79/120    avg_loss:0.027, val_acc:0.979]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.019, val_acc:0.983]
Epoch [82/120    avg_loss:0.024, val_acc:0.980]
Epoch [83/120    avg_loss:0.021, val_acc:0.978]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.013, val_acc:0.983]
Epoch [86/120    avg_loss:0.012, val_acc:0.978]
Epoch [87/120    avg_loss:0.017, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.990]
Epoch [90/120    avg_loss:0.008, val_acc:0.990]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.011, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.993]
Epoch [99/120    avg_loss:0.007, val_acc:0.992]
Epoch [100/120    avg_loss:0.007, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.993]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.008, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.991]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.007, val_acc:0.991]
Epoch [113/120    avg_loss:0.007, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.993]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.993]
Epoch [117/120    avg_loss:0.007, val_acc:0.995]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.993]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     1     0     0     2     5     1]
 [    0     0 18033     0    16     0    41     0     0     0]
 [    0    10     0  2010     0     0     0     0    13     3]
 [    0    38     2     0  2909     0     5     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     7     4     0  4866     0     1     0]
 [    0     0     0     0     0     0     2  1282     0     6]
 [    0    26     0    15    54     0     0     0  3476     0]
 [    0     0     0     0     7    45     0     0     0   867]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.99358032 0.99836678 0.98820059 0.97568338 0.98305085
 0.99387255 0.996115   0.98136646 0.96547884]

Kappa:
0.9897214064537778
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f983a967748>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.083, val_acc:0.176]
Epoch [2/120    avg_loss:1.737, val_acc:0.217]
Epoch [3/120    avg_loss:1.515, val_acc:0.292]
Epoch [4/120    avg_loss:1.353, val_acc:0.340]
Epoch [5/120    avg_loss:1.198, val_acc:0.409]
Epoch [6/120    avg_loss:1.073, val_acc:0.490]
Epoch [7/120    avg_loss:0.939, val_acc:0.488]
Epoch [8/120    avg_loss:0.831, val_acc:0.496]
Epoch [9/120    avg_loss:0.736, val_acc:0.561]
Epoch [10/120    avg_loss:0.607, val_acc:0.641]
Epoch [11/120    avg_loss:0.518, val_acc:0.730]
Epoch [12/120    avg_loss:0.451, val_acc:0.738]
Epoch [13/120    avg_loss:0.410, val_acc:0.830]
Epoch [14/120    avg_loss:0.375, val_acc:0.854]
Epoch [15/120    avg_loss:0.332, val_acc:0.827]
Epoch [16/120    avg_loss:0.308, val_acc:0.840]
Epoch [17/120    avg_loss:0.278, val_acc:0.876]
Epoch [18/120    avg_loss:0.257, val_acc:0.870]
Epoch [19/120    avg_loss:0.231, val_acc:0.897]
Epoch [20/120    avg_loss:0.215, val_acc:0.918]
Epoch [21/120    avg_loss:0.199, val_acc:0.917]
Epoch [22/120    avg_loss:0.214, val_acc:0.911]
Epoch [23/120    avg_loss:0.201, val_acc:0.830]
Epoch [24/120    avg_loss:0.169, val_acc:0.937]
Epoch [25/120    avg_loss:0.148, val_acc:0.928]
Epoch [26/120    avg_loss:0.120, val_acc:0.953]
Epoch [27/120    avg_loss:0.141, val_acc:0.923]
Epoch [28/120    avg_loss:0.144, val_acc:0.914]
Epoch [29/120    avg_loss:0.142, val_acc:0.918]
Epoch [30/120    avg_loss:0.150, val_acc:0.952]
Epoch [31/120    avg_loss:0.118, val_acc:0.934]
Epoch [32/120    avg_loss:0.101, val_acc:0.936]
Epoch [33/120    avg_loss:0.112, val_acc:0.951]
Epoch [34/120    avg_loss:0.078, val_acc:0.961]
Epoch [35/120    avg_loss:0.066, val_acc:0.953]
Epoch [36/120    avg_loss:0.074, val_acc:0.956]
Epoch [37/120    avg_loss:0.059, val_acc:0.963]
Epoch [38/120    avg_loss:0.074, val_acc:0.936]
Epoch [39/120    avg_loss:0.056, val_acc:0.963]
Epoch [40/120    avg_loss:0.052, val_acc:0.963]
Epoch [41/120    avg_loss:0.063, val_acc:0.970]
Epoch [42/120    avg_loss:0.059, val_acc:0.960]
Epoch [43/120    avg_loss:0.071, val_acc:0.962]
Epoch [44/120    avg_loss:0.044, val_acc:0.972]
Epoch [45/120    avg_loss:0.045, val_acc:0.957]
Epoch [46/120    avg_loss:0.043, val_acc:0.973]
Epoch [47/120    avg_loss:0.033, val_acc:0.972]
Epoch [48/120    avg_loss:0.027, val_acc:0.971]
Epoch [49/120    avg_loss:0.024, val_acc:0.975]
Epoch [50/120    avg_loss:0.021, val_acc:0.984]
Epoch [51/120    avg_loss:0.021, val_acc:0.972]
Epoch [52/120    avg_loss:0.035, val_acc:0.963]
Epoch [53/120    avg_loss:0.044, val_acc:0.972]
Epoch [54/120    avg_loss:0.028, val_acc:0.971]
Epoch [55/120    avg_loss:0.031, val_acc:0.974]
Epoch [56/120    avg_loss:0.022, val_acc:0.975]
Epoch [57/120    avg_loss:0.019, val_acc:0.980]
Epoch [58/120    avg_loss:0.026, val_acc:0.958]
Epoch [59/120    avg_loss:0.055, val_acc:0.952]
Epoch [60/120    avg_loss:0.050, val_acc:0.977]
Epoch [61/120    avg_loss:0.026, val_acc:0.973]
Epoch [62/120    avg_loss:0.034, val_acc:0.973]
Epoch [63/120    avg_loss:0.027, val_acc:0.972]
Epoch [64/120    avg_loss:0.020, val_acc:0.983]
Epoch [65/120    avg_loss:0.016, val_acc:0.982]
Epoch [66/120    avg_loss:0.014, val_acc:0.983]
Epoch [67/120    avg_loss:0.012, val_acc:0.984]
Epoch [68/120    avg_loss:0.011, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.983]
Epoch [70/120    avg_loss:0.015, val_acc:0.982]
Epoch [71/120    avg_loss:0.012, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.982]
Epoch [73/120    avg_loss:0.011, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.984]
Epoch [78/120    avg_loss:0.011, val_acc:0.984]
Epoch [79/120    avg_loss:0.017, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.009, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.012, val_acc:0.981]
Epoch [89/120    avg_loss:0.010, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.013, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.010, val_acc:0.984]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6325     0     0     3     0     9    37    58     0]
 [    0     5 18044     0    36     0     4     0     1     0]
 [    0     1     0  1928     5     0     0     0    96     6]
 [    0    51    15     0  2880     0     3     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    18     0     0     0  4858     0     2     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    39     0    45    45     0     0     0  3438     4]
 [    0     0     0     0    14    25     0     0     0   880]]

Accuracy:
98.6865254380257

F1 scores:
[       nan 0.98420602 0.99781569 0.96183587 0.96725441 0.99051233
 0.99630845 0.98586167 0.95646126 0.97291321]

Kappa:
0.9825983401415539
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14c57be978>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.064, val_acc:0.137]
Epoch [2/120    avg_loss:1.714, val_acc:0.148]
Epoch [3/120    avg_loss:1.474, val_acc:0.298]
Epoch [4/120    avg_loss:1.295, val_acc:0.370]
Epoch [5/120    avg_loss:1.140, val_acc:0.547]
Epoch [6/120    avg_loss:1.020, val_acc:0.538]
Epoch [7/120    avg_loss:0.918, val_acc:0.692]
Epoch [8/120    avg_loss:0.818, val_acc:0.653]
Epoch [9/120    avg_loss:0.700, val_acc:0.690]
Epoch [10/120    avg_loss:0.618, val_acc:0.633]
Epoch [11/120    avg_loss:0.514, val_acc:0.678]
Epoch [12/120    avg_loss:0.456, val_acc:0.750]
Epoch [13/120    avg_loss:0.418, val_acc:0.792]
Epoch [14/120    avg_loss:0.360, val_acc:0.788]
Epoch [15/120    avg_loss:0.315, val_acc:0.782]
Epoch [16/120    avg_loss:0.323, val_acc:0.777]
Epoch [17/120    avg_loss:0.449, val_acc:0.748]
Epoch [18/120    avg_loss:0.380, val_acc:0.864]
Epoch [19/120    avg_loss:0.288, val_acc:0.893]
Epoch [20/120    avg_loss:0.232, val_acc:0.879]
Epoch [21/120    avg_loss:0.242, val_acc:0.923]
Epoch [22/120    avg_loss:0.223, val_acc:0.921]
Epoch [23/120    avg_loss:0.175, val_acc:0.938]
Epoch [24/120    avg_loss:0.253, val_acc:0.870]
Epoch [25/120    avg_loss:0.196, val_acc:0.929]
Epoch [26/120    avg_loss:0.171, val_acc:0.938]
Epoch [27/120    avg_loss:0.153, val_acc:0.923]
Epoch [28/120    avg_loss:0.160, val_acc:0.928]
Epoch [29/120    avg_loss:0.141, val_acc:0.943]
Epoch [30/120    avg_loss:0.112, val_acc:0.953]
Epoch [31/120    avg_loss:0.113, val_acc:0.932]
Epoch [32/120    avg_loss:0.091, val_acc:0.950]
Epoch [33/120    avg_loss:0.103, val_acc:0.959]
Epoch [34/120    avg_loss:0.096, val_acc:0.955]
Epoch [35/120    avg_loss:0.080, val_acc:0.961]
Epoch [36/120    avg_loss:0.079, val_acc:0.944]
Epoch [37/120    avg_loss:0.088, val_acc:0.958]
Epoch [38/120    avg_loss:0.069, val_acc:0.958]
Epoch [39/120    avg_loss:0.064, val_acc:0.971]
Epoch [40/120    avg_loss:0.074, val_acc:0.957]
Epoch [41/120    avg_loss:0.065, val_acc:0.959]
Epoch [42/120    avg_loss:0.052, val_acc:0.967]
Epoch [43/120    avg_loss:0.066, val_acc:0.965]
Epoch [44/120    avg_loss:0.049, val_acc:0.972]
Epoch [45/120    avg_loss:0.034, val_acc:0.978]
Epoch [46/120    avg_loss:0.087, val_acc:0.947]
Epoch [47/120    avg_loss:0.058, val_acc:0.968]
Epoch [48/120    avg_loss:0.073, val_acc:0.951]
Epoch [49/120    avg_loss:0.049, val_acc:0.965]
Epoch [50/120    avg_loss:0.050, val_acc:0.962]
Epoch [51/120    avg_loss:0.053, val_acc:0.959]
Epoch [52/120    avg_loss:0.036, val_acc:0.971]
Epoch [53/120    avg_loss:0.033, val_acc:0.972]
Epoch [54/120    avg_loss:0.029, val_acc:0.979]
Epoch [55/120    avg_loss:0.033, val_acc:0.973]
Epoch [56/120    avg_loss:0.073, val_acc:0.961]
Epoch [57/120    avg_loss:0.061, val_acc:0.970]
Epoch [58/120    avg_loss:0.042, val_acc:0.972]
Epoch [59/120    avg_loss:0.033, val_acc:0.979]
Epoch [60/120    avg_loss:0.060, val_acc:0.958]
Epoch [61/120    avg_loss:0.054, val_acc:0.971]
Epoch [62/120    avg_loss:0.038, val_acc:0.977]
Epoch [63/120    avg_loss:0.023, val_acc:0.976]
Epoch [64/120    avg_loss:0.030, val_acc:0.968]
Epoch [65/120    avg_loss:0.039, val_acc:0.980]
Epoch [66/120    avg_loss:0.032, val_acc:0.972]
Epoch [67/120    avg_loss:0.026, val_acc:0.978]
Epoch [68/120    avg_loss:0.024, val_acc:0.984]
Epoch [69/120    avg_loss:0.023, val_acc:0.981]
Epoch [70/120    avg_loss:0.016, val_acc:0.984]
Epoch [71/120    avg_loss:0.018, val_acc:0.977]
Epoch [72/120    avg_loss:0.019, val_acc:0.978]
Epoch [73/120    avg_loss:0.021, val_acc:0.982]
Epoch [74/120    avg_loss:0.018, val_acc:0.980]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.014, val_acc:0.978]
Epoch [78/120    avg_loss:0.019, val_acc:0.980]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.015, val_acc:0.978]
Epoch [82/120    avg_loss:0.015, val_acc:0.974]
Epoch [83/120    avg_loss:0.021, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.013, val_acc:0.973]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.012, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.990]
Epoch [91/120    avg_loss:0.013, val_acc:0.969]
Epoch [92/120    avg_loss:0.019, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.018, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.989]
Epoch [99/120    avg_loss:0.008, val_acc:0.987]
Epoch [100/120    avg_loss:0.011, val_acc:0.984]
Epoch [101/120    avg_loss:0.017, val_acc:0.981]
Epoch [102/120    avg_loss:0.009, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.990]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     0     0     1     0]
 [    0     3 18029     0    19     0    37     0     2     0]
 [    0     7     0  2017     0     0     0     0    10     2]
 [    0    54     6     0  2898     0     2     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     1     0     0  4867     0     2     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     2     0     5    63     0     0     0  3483    18]
 [    0     0     0     0    14    15     0     0     0   890]]

Accuracy:
99.31554720073265

F1 scores:
[       nan 0.99481785 0.99792434 0.99384085 0.9715052  0.99428571
 0.99488962 0.99961225 0.98375936 0.9726776 ]

Kappa:
0.9909338214555609
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff03415d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.032, val_acc:0.086]
Epoch [2/120    avg_loss:1.702, val_acc:0.141]
Epoch [3/120    avg_loss:1.525, val_acc:0.214]
Epoch [4/120    avg_loss:1.365, val_acc:0.341]
Epoch [5/120    avg_loss:1.272, val_acc:0.437]
Epoch [6/120    avg_loss:1.132, val_acc:0.539]
Epoch [7/120    avg_loss:1.000, val_acc:0.705]
Epoch [8/120    avg_loss:0.899, val_acc:0.671]
Epoch [9/120    avg_loss:0.803, val_acc:0.716]
Epoch [10/120    avg_loss:0.701, val_acc:0.660]
Epoch [11/120    avg_loss:0.648, val_acc:0.702]
Epoch [12/120    avg_loss:0.520, val_acc:0.737]
Epoch [13/120    avg_loss:0.436, val_acc:0.842]
Epoch [14/120    avg_loss:0.366, val_acc:0.878]
Epoch [15/120    avg_loss:0.359, val_acc:0.862]
Epoch [16/120    avg_loss:0.317, val_acc:0.907]
Epoch [17/120    avg_loss:0.241, val_acc:0.928]
Epoch [18/120    avg_loss:0.212, val_acc:0.920]
Epoch [19/120    avg_loss:0.218, val_acc:0.920]
Epoch [20/120    avg_loss:0.209, val_acc:0.874]
Epoch [21/120    avg_loss:0.181, val_acc:0.916]
Epoch [22/120    avg_loss:0.156, val_acc:0.942]
Epoch [23/120    avg_loss:0.172, val_acc:0.930]
Epoch [24/120    avg_loss:0.179, val_acc:0.954]
Epoch [25/120    avg_loss:0.127, val_acc:0.947]
Epoch [26/120    avg_loss:0.109, val_acc:0.956]
Epoch [27/120    avg_loss:0.133, val_acc:0.879]
Epoch [28/120    avg_loss:0.152, val_acc:0.913]
Epoch [29/120    avg_loss:0.337, val_acc:0.522]
Epoch [30/120    avg_loss:1.046, val_acc:0.505]
Epoch [31/120    avg_loss:0.871, val_acc:0.515]
Epoch [32/120    avg_loss:0.668, val_acc:0.686]
Epoch [33/120    avg_loss:0.633, val_acc:0.732]
Epoch [34/120    avg_loss:0.587, val_acc:0.728]
Epoch [35/120    avg_loss:0.476, val_acc:0.833]
Epoch [36/120    avg_loss:0.436, val_acc:0.801]
Epoch [37/120    avg_loss:0.444, val_acc:0.807]
Epoch [38/120    avg_loss:0.352, val_acc:0.856]
Epoch [39/120    avg_loss:0.281, val_acc:0.913]
Epoch [40/120    avg_loss:0.233, val_acc:0.893]
Epoch [41/120    avg_loss:0.207, val_acc:0.924]
Epoch [42/120    avg_loss:0.207, val_acc:0.918]
Epoch [43/120    avg_loss:0.197, val_acc:0.919]
Epoch [44/120    avg_loss:0.209, val_acc:0.918]
Epoch [45/120    avg_loss:0.199, val_acc:0.930]
Epoch [46/120    avg_loss:0.189, val_acc:0.935]
Epoch [47/120    avg_loss:0.199, val_acc:0.934]
Epoch [48/120    avg_loss:0.185, val_acc:0.940]
Epoch [49/120    avg_loss:0.162, val_acc:0.928]
Epoch [50/120    avg_loss:0.165, val_acc:0.942]
Epoch [51/120    avg_loss:0.175, val_acc:0.924]
Epoch [52/120    avg_loss:0.184, val_acc:0.923]
Epoch [53/120    avg_loss:0.165, val_acc:0.934]
Epoch [54/120    avg_loss:0.166, val_acc:0.936]
Epoch [55/120    avg_loss:0.151, val_acc:0.940]
Epoch [56/120    avg_loss:0.165, val_acc:0.941]
Epoch [57/120    avg_loss:0.158, val_acc:0.942]
Epoch [58/120    avg_loss:0.165, val_acc:0.944]
Epoch [59/120    avg_loss:0.167, val_acc:0.945]
Epoch [60/120    avg_loss:0.162, val_acc:0.945]
Epoch [61/120    avg_loss:0.146, val_acc:0.944]
Epoch [62/120    avg_loss:0.163, val_acc:0.947]
Epoch [63/120    avg_loss:0.154, val_acc:0.946]
Epoch [64/120    avg_loss:0.172, val_acc:0.943]
Epoch [65/120    avg_loss:0.161, val_acc:0.943]
Epoch [66/120    avg_loss:0.155, val_acc:0.943]
Epoch [67/120    avg_loss:0.161, val_acc:0.943]
Epoch [68/120    avg_loss:0.159, val_acc:0.943]
Epoch [69/120    avg_loss:0.150, val_acc:0.943]
Epoch [70/120    avg_loss:0.154, val_acc:0.943]
Epoch [71/120    avg_loss:0.149, val_acc:0.943]
Epoch [72/120    avg_loss:0.153, val_acc:0.944]
Epoch [73/120    avg_loss:0.158, val_acc:0.944]
Epoch [74/120    avg_loss:0.163, val_acc:0.944]
Epoch [75/120    avg_loss:0.163, val_acc:0.944]
Epoch [76/120    avg_loss:0.143, val_acc:0.944]
Epoch [77/120    avg_loss:0.165, val_acc:0.944]
Epoch [78/120    avg_loss:0.150, val_acc:0.944]
Epoch [79/120    avg_loss:0.156, val_acc:0.944]
Epoch [80/120    avg_loss:0.149, val_acc:0.944]
Epoch [81/120    avg_loss:0.157, val_acc:0.944]
Epoch [82/120    avg_loss:0.147, val_acc:0.944]
Epoch [83/120    avg_loss:0.147, val_acc:0.944]
Epoch [84/120    avg_loss:0.158, val_acc:0.944]
Epoch [85/120    avg_loss:0.159, val_acc:0.944]
Epoch [86/120    avg_loss:0.157, val_acc:0.944]
Epoch [87/120    avg_loss:0.168, val_acc:0.944]
Epoch [88/120    avg_loss:0.147, val_acc:0.944]
Epoch [89/120    avg_loss:0.151, val_acc:0.944]
Epoch [90/120    avg_loss:0.141, val_acc:0.944]
Epoch [91/120    avg_loss:0.150, val_acc:0.944]
Epoch [92/120    avg_loss:0.149, val_acc:0.944]
Epoch [93/120    avg_loss:0.160, val_acc:0.944]
Epoch [94/120    avg_loss:0.154, val_acc:0.944]
Epoch [95/120    avg_loss:0.160, val_acc:0.944]
Epoch [96/120    avg_loss:0.155, val_acc:0.944]
Epoch [97/120    avg_loss:0.160, val_acc:0.944]
Epoch [98/120    avg_loss:0.166, val_acc:0.944]
Epoch [99/120    avg_loss:0.156, val_acc:0.944]
Epoch [100/120    avg_loss:0.153, val_acc:0.944]
Epoch [101/120    avg_loss:0.154, val_acc:0.944]
Epoch [102/120    avg_loss:0.159, val_acc:0.944]
Epoch [103/120    avg_loss:0.164, val_acc:0.944]
Epoch [104/120    avg_loss:0.166, val_acc:0.944]
Epoch [105/120    avg_loss:0.154, val_acc:0.944]
Epoch [106/120    avg_loss:0.161, val_acc:0.944]
Epoch [107/120    avg_loss:0.154, val_acc:0.944]
Epoch [108/120    avg_loss:0.153, val_acc:0.944]
Epoch [109/120    avg_loss:0.162, val_acc:0.944]
Epoch [110/120    avg_loss:0.170, val_acc:0.944]
Epoch [111/120    avg_loss:0.155, val_acc:0.944]
Epoch [112/120    avg_loss:0.171, val_acc:0.944]
Epoch [113/120    avg_loss:0.158, val_acc:0.944]
Epoch [114/120    avg_loss:0.161, val_acc:0.944]
Epoch [115/120    avg_loss:0.168, val_acc:0.944]
Epoch [116/120    avg_loss:0.154, val_acc:0.944]
Epoch [117/120    avg_loss:0.155, val_acc:0.944]
Epoch [118/120    avg_loss:0.161, val_acc:0.944]
Epoch [119/120    avg_loss:0.157, val_acc:0.944]
Epoch [120/120    avg_loss:0.158, val_acc:0.944]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5621     0   196   190     0    56     7   308    54]
 [    0     5 17716     0   113     0   256     0     0     0]
 [    0    11     0  1956     0     0     0     0    40    29]
 [    0   108    27     0  2759     0    23     0    30    25]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     2     6     4     0     0  4851     0    15     0]
 [    0     4     0     0     0     0     9  1271     0     6]
 [    0    91     0    21    78     0     0     0  3370    11]
 [    0    21     0     0    13    65     0     0     0   820]]

Accuracy:
95.60166775118695

F1 scores:
[       nan 0.91435543 0.98864366 0.92855447 0.90089796 0.97531788
 0.96316887 0.98987539 0.91900736 0.87935657]

Kappa:
0.9420174803126885
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd35d931940>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.132, val_acc:0.173]
Epoch [2/120    avg_loss:1.772, val_acc:0.266]
Epoch [3/120    avg_loss:1.554, val_acc:0.332]
Epoch [4/120    avg_loss:1.400, val_acc:0.353]
Epoch [5/120    avg_loss:1.224, val_acc:0.409]
Epoch [6/120    avg_loss:1.090, val_acc:0.451]
Epoch [7/120    avg_loss:0.967, val_acc:0.527]
Epoch [8/120    avg_loss:0.853, val_acc:0.527]
Epoch [9/120    avg_loss:0.739, val_acc:0.583]
Epoch [10/120    avg_loss:0.656, val_acc:0.651]
Epoch [11/120    avg_loss:0.580, val_acc:0.680]
Epoch [12/120    avg_loss:0.537, val_acc:0.718]
Epoch [13/120    avg_loss:0.475, val_acc:0.774]
Epoch [14/120    avg_loss:0.410, val_acc:0.784]
Epoch [15/120    avg_loss:0.385, val_acc:0.763]
Epoch [16/120    avg_loss:0.369, val_acc:0.810]
Epoch [17/120    avg_loss:0.373, val_acc:0.784]
Epoch [18/120    avg_loss:0.345, val_acc:0.834]
Epoch [19/120    avg_loss:0.323, val_acc:0.824]
Epoch [20/120    avg_loss:0.285, val_acc:0.834]
Epoch [21/120    avg_loss:0.232, val_acc:0.894]
Epoch [22/120    avg_loss:0.255, val_acc:0.892]
Epoch [23/120    avg_loss:0.206, val_acc:0.916]
Epoch [24/120    avg_loss:0.200, val_acc:0.917]
Epoch [25/120    avg_loss:0.186, val_acc:0.843]
Epoch [26/120    avg_loss:0.180, val_acc:0.927]
Epoch [27/120    avg_loss:0.159, val_acc:0.947]
Epoch [28/120    avg_loss:0.129, val_acc:0.949]
Epoch [29/120    avg_loss:0.104, val_acc:0.965]
Epoch [30/120    avg_loss:0.111, val_acc:0.935]
Epoch [31/120    avg_loss:0.157, val_acc:0.946]
Epoch [32/120    avg_loss:0.114, val_acc:0.954]
Epoch [33/120    avg_loss:0.096, val_acc:0.950]
Epoch [34/120    avg_loss:0.100, val_acc:0.957]
Epoch [35/120    avg_loss:0.090, val_acc:0.950]
Epoch [36/120    avg_loss:0.089, val_acc:0.965]
Epoch [37/120    avg_loss:0.071, val_acc:0.971]
Epoch [38/120    avg_loss:0.064, val_acc:0.975]
Epoch [39/120    avg_loss:0.052, val_acc:0.974]
Epoch [40/120    avg_loss:0.062, val_acc:0.978]
Epoch [41/120    avg_loss:0.105, val_acc:0.953]
Epoch [42/120    avg_loss:0.092, val_acc:0.973]
Epoch [43/120    avg_loss:0.111, val_acc:0.963]
Epoch [44/120    avg_loss:0.080, val_acc:0.955]
Epoch [45/120    avg_loss:0.061, val_acc:0.965]
Epoch [46/120    avg_loss:0.056, val_acc:0.971]
Epoch [47/120    avg_loss:0.051, val_acc:0.984]
Epoch [48/120    avg_loss:0.043, val_acc:0.970]
Epoch [49/120    avg_loss:0.040, val_acc:0.981]
Epoch [50/120    avg_loss:0.043, val_acc:0.974]
Epoch [51/120    avg_loss:0.059, val_acc:0.981]
Epoch [52/120    avg_loss:0.028, val_acc:0.982]
Epoch [53/120    avg_loss:0.035, val_acc:0.978]
Epoch [54/120    avg_loss:0.026, val_acc:0.981]
Epoch [55/120    avg_loss:0.029, val_acc:0.970]
Epoch [56/120    avg_loss:0.025, val_acc:0.985]
Epoch [57/120    avg_loss:0.025, val_acc:0.959]
Epoch [58/120    avg_loss:0.040, val_acc:0.967]
Epoch [59/120    avg_loss:0.028, val_acc:0.980]
Epoch [60/120    avg_loss:0.028, val_acc:0.981]
Epoch [61/120    avg_loss:0.032, val_acc:0.984]
Epoch [62/120    avg_loss:0.027, val_acc:0.983]
Epoch [63/120    avg_loss:0.027, val_acc:0.974]
Epoch [64/120    avg_loss:0.024, val_acc:0.981]
Epoch [65/120    avg_loss:0.026, val_acc:0.979]
Epoch [66/120    avg_loss:0.026, val_acc:0.979]
Epoch [67/120    avg_loss:0.037, val_acc:0.982]
Epoch [68/120    avg_loss:0.043, val_acc:0.984]
Epoch [69/120    avg_loss:0.031, val_acc:0.980]
Epoch [70/120    avg_loss:0.032, val_acc:0.984]
Epoch [71/120    avg_loss:0.020, val_acc:0.989]
Epoch [72/120    avg_loss:0.018, val_acc:0.988]
Epoch [73/120    avg_loss:0.018, val_acc:0.989]
Epoch [74/120    avg_loss:0.016, val_acc:0.986]
Epoch [75/120    avg_loss:0.012, val_acc:0.988]
Epoch [76/120    avg_loss:0.016, val_acc:0.987]
Epoch [77/120    avg_loss:0.014, val_acc:0.987]
Epoch [78/120    avg_loss:0.014, val_acc:0.988]
Epoch [79/120    avg_loss:0.011, val_acc:0.988]
Epoch [80/120    avg_loss:0.013, val_acc:0.989]
Epoch [81/120    avg_loss:0.013, val_acc:0.989]
Epoch [82/120    avg_loss:0.014, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.014, val_acc:0.989]
Epoch [86/120    avg_loss:0.012, val_acc:0.989]
Epoch [87/120    avg_loss:0.011, val_acc:0.988]
Epoch [88/120    avg_loss:0.012, val_acc:0.989]
Epoch [89/120    avg_loss:0.013, val_acc:0.989]
Epoch [90/120    avg_loss:0.013, val_acc:0.990]
Epoch [91/120    avg_loss:0.011, val_acc:0.989]
Epoch [92/120    avg_loss:0.011, val_acc:0.989]
Epoch [93/120    avg_loss:0.011, val_acc:0.990]
Epoch [94/120    avg_loss:0.011, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.989]
Epoch [96/120    avg_loss:0.011, val_acc:0.989]
Epoch [97/120    avg_loss:0.011, val_acc:0.990]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.013, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.990]
Epoch [102/120    avg_loss:0.013, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.990]
Epoch [104/120    avg_loss:0.013, val_acc:0.989]
Epoch [105/120    avg_loss:0.010, val_acc:0.989]
Epoch [106/120    avg_loss:0.010, val_acc:0.989]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.011, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.989]
Epoch [110/120    avg_loss:0.013, val_acc:0.988]
Epoch [111/120    avg_loss:0.011, val_acc:0.989]
Epoch [112/120    avg_loss:0.011, val_acc:0.989]
Epoch [113/120    avg_loss:0.011, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.990]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.989]
Epoch [118/120    avg_loss:0.010, val_acc:0.989]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.010, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     0     8     0     0     4     8     0]
 [    0     0 18024     0    62     0     4     0     0     0]
 [    0     7     0  1999     2     0     0     0    27     1]
 [    0    53    25     0  2863     0     3     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0    21     0     7    58     0     0     0  3483     2]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.11792350516954

F1 scores:
[       nan 0.99218569 0.99745434 0.9891143  0.95768523 0.9893859
 0.9991805  0.99728787 0.9787832  0.97336293]

Kappa:
0.9883150434091812
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2700a58908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.047, val_acc:0.103]
Epoch [2/120    avg_loss:1.676, val_acc:0.225]
Epoch [3/120    avg_loss:1.491, val_acc:0.356]
Epoch [4/120    avg_loss:1.329, val_acc:0.405]
Epoch [5/120    avg_loss:1.162, val_acc:0.426]
Epoch [6/120    avg_loss:1.029, val_acc:0.478]
Epoch [7/120    avg_loss:0.907, val_acc:0.493]
Epoch [8/120    avg_loss:0.819, val_acc:0.509]
Epoch [9/120    avg_loss:0.695, val_acc:0.663]
Epoch [10/120    avg_loss:0.616, val_acc:0.608]
Epoch [11/120    avg_loss:0.552, val_acc:0.655]
Epoch [12/120    avg_loss:0.488, val_acc:0.679]
Epoch [13/120    avg_loss:0.439, val_acc:0.759]
Epoch [14/120    avg_loss:0.411, val_acc:0.752]
Epoch [15/120    avg_loss:0.438, val_acc:0.795]
Epoch [16/120    avg_loss:0.349, val_acc:0.815]
Epoch [17/120    avg_loss:0.288, val_acc:0.830]
Epoch [18/120    avg_loss:0.260, val_acc:0.876]
Epoch [19/120    avg_loss:0.275, val_acc:0.875]
Epoch [20/120    avg_loss:0.252, val_acc:0.878]
Epoch [21/120    avg_loss:0.229, val_acc:0.880]
Epoch [22/120    avg_loss:0.219, val_acc:0.932]
Epoch [23/120    avg_loss:0.174, val_acc:0.939]
Epoch [24/120    avg_loss:0.150, val_acc:0.933]
Epoch [25/120    avg_loss:0.138, val_acc:0.946]
Epoch [26/120    avg_loss:0.109, val_acc:0.922]
Epoch [27/120    avg_loss:0.107, val_acc:0.960]
Epoch [28/120    avg_loss:0.097, val_acc:0.962]
Epoch [29/120    avg_loss:0.110, val_acc:0.957]
Epoch [30/120    avg_loss:0.091, val_acc:0.966]
Epoch [31/120    avg_loss:0.118, val_acc:0.950]
Epoch [32/120    avg_loss:0.110, val_acc:0.955]
Epoch [33/120    avg_loss:0.085, val_acc:0.963]
Epoch [34/120    avg_loss:0.081, val_acc:0.955]
Epoch [35/120    avg_loss:0.059, val_acc:0.963]
Epoch [36/120    avg_loss:0.066, val_acc:0.968]
Epoch [37/120    avg_loss:0.073, val_acc:0.964]
Epoch [38/120    avg_loss:0.061, val_acc:0.972]
Epoch [39/120    avg_loss:0.074, val_acc:0.963]
Epoch [40/120    avg_loss:0.060, val_acc:0.968]
Epoch [41/120    avg_loss:0.050, val_acc:0.959]
Epoch [42/120    avg_loss:0.044, val_acc:0.960]
Epoch [43/120    avg_loss:0.071, val_acc:0.941]
Epoch [44/120    avg_loss:0.075, val_acc:0.976]
Epoch [45/120    avg_loss:0.057, val_acc:0.970]
Epoch [46/120    avg_loss:0.038, val_acc:0.959]
Epoch [47/120    avg_loss:0.069, val_acc:0.965]
Epoch [48/120    avg_loss:0.066, val_acc:0.972]
Epoch [49/120    avg_loss:0.040, val_acc:0.977]
Epoch [50/120    avg_loss:0.028, val_acc:0.978]
Epoch [51/120    avg_loss:0.029, val_acc:0.981]
Epoch [52/120    avg_loss:0.032, val_acc:0.960]
Epoch [53/120    avg_loss:0.039, val_acc:0.966]
Epoch [54/120    avg_loss:0.034, val_acc:0.974]
Epoch [55/120    avg_loss:0.045, val_acc:0.965]
Epoch [56/120    avg_loss:0.202, val_acc:0.894]
Epoch [57/120    avg_loss:0.117, val_acc:0.957]
Epoch [58/120    avg_loss:0.054, val_acc:0.974]
Epoch [59/120    avg_loss:0.041, val_acc:0.972]
Epoch [60/120    avg_loss:0.028, val_acc:0.966]
Epoch [61/120    avg_loss:0.027, val_acc:0.974]
Epoch [62/120    avg_loss:0.035, val_acc:0.973]
Epoch [63/120    avg_loss:0.021, val_acc:0.977]
Epoch [64/120    avg_loss:0.023, val_acc:0.983]
Epoch [65/120    avg_loss:0.048, val_acc:0.962]
Epoch [66/120    avg_loss:0.030, val_acc:0.983]
Epoch [67/120    avg_loss:0.018, val_acc:0.986]
Epoch [68/120    avg_loss:0.027, val_acc:0.977]
Epoch [69/120    avg_loss:0.016, val_acc:0.984]
Epoch [70/120    avg_loss:0.029, val_acc:0.973]
Epoch [71/120    avg_loss:0.025, val_acc:0.976]
Epoch [72/120    avg_loss:0.014, val_acc:0.987]
Epoch [73/120    avg_loss:0.012, val_acc:0.989]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.013, val_acc:0.985]
Epoch [79/120    avg_loss:0.017, val_acc:0.978]
Epoch [80/120    avg_loss:0.018, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.009, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.011, val_acc:0.989]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.028, val_acc:0.984]
Epoch [87/120    avg_loss:0.058, val_acc:0.976]
Epoch [88/120    avg_loss:0.021, val_acc:0.981]
Epoch [89/120    avg_loss:0.018, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.991]
Epoch [92/120    avg_loss:0.012, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.987]
Epoch [97/120    avg_loss:0.010, val_acc:0.989]
Epoch [98/120    avg_loss:0.008, val_acc:0.984]
Epoch [99/120    avg_loss:0.013, val_acc:0.985]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.011, val_acc:0.991]
Epoch [102/120    avg_loss:0.012, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.991]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.991]
Epoch [113/120    avg_loss:0.007, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.009, val_acc:0.989]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.993]
Epoch [119/120    avg_loss:0.005, val_acc:0.993]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6388     0     0     0     0    24     0    20     0]
 [    0     0 18011     0    58     0    16     0     5     0]
 [    0    14     0  2013     2     0     0     0     5     2]
 [    0    34    18     0  2880     0    14     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     0     0     0    52     0     0     0  3507    12]
 [    0     0     0     0    14    47     0     0     0   858]]

Accuracy:
99.12033355023739

F1 scores:
[       nan 0.99285048 0.99731443 0.99431959 0.96353295 0.98231088
 0.9942927  0.9992242  0.98317914 0.95812395]

Kappa:
0.9883507324282252
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f756481e908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.080, val_acc:0.067]
Epoch [2/120    avg_loss:1.744, val_acc:0.238]
Epoch [3/120    avg_loss:1.523, val_acc:0.300]
Epoch [4/120    avg_loss:1.357, val_acc:0.376]
Epoch [5/120    avg_loss:1.210, val_acc:0.400]
Epoch [6/120    avg_loss:1.092, val_acc:0.459]
Epoch [7/120    avg_loss:0.983, val_acc:0.504]
Epoch [8/120    avg_loss:0.804, val_acc:0.557]
Epoch [9/120    avg_loss:0.692, val_acc:0.621]
Epoch [10/120    avg_loss:0.602, val_acc:0.686]
Epoch [11/120    avg_loss:0.522, val_acc:0.726]
Epoch [12/120    avg_loss:0.489, val_acc:0.772]
Epoch [13/120    avg_loss:0.446, val_acc:0.793]
Epoch [14/120    avg_loss:0.387, val_acc:0.756]
Epoch [15/120    avg_loss:0.368, val_acc:0.828]
Epoch [16/120    avg_loss:0.371, val_acc:0.701]
Epoch [17/120    avg_loss:0.340, val_acc:0.830]
Epoch [18/120    avg_loss:0.305, val_acc:0.859]
Epoch [19/120    avg_loss:0.266, val_acc:0.883]
Epoch [20/120    avg_loss:0.240, val_acc:0.872]
Epoch [21/120    avg_loss:0.266, val_acc:0.895]
Epoch [22/120    avg_loss:0.236, val_acc:0.888]
Epoch [23/120    avg_loss:0.220, val_acc:0.918]
Epoch [24/120    avg_loss:0.209, val_acc:0.922]
Epoch [25/120    avg_loss:0.224, val_acc:0.936]
Epoch [26/120    avg_loss:0.139, val_acc:0.932]
Epoch [27/120    avg_loss:0.136, val_acc:0.930]
Epoch [28/120    avg_loss:0.122, val_acc:0.947]
Epoch [29/120    avg_loss:0.123, val_acc:0.937]
Epoch [30/120    avg_loss:0.109, val_acc:0.948]
Epoch [31/120    avg_loss:0.146, val_acc:0.942]
Epoch [32/120    avg_loss:0.112, val_acc:0.934]
Epoch [33/120    avg_loss:0.109, val_acc:0.916]
Epoch [34/120    avg_loss:0.085, val_acc:0.945]
Epoch [35/120    avg_loss:0.066, val_acc:0.965]
Epoch [36/120    avg_loss:0.087, val_acc:0.934]
Epoch [37/120    avg_loss:0.099, val_acc:0.945]
Epoch [38/120    avg_loss:0.060, val_acc:0.961]
Epoch [39/120    avg_loss:0.101, val_acc:0.906]
Epoch [40/120    avg_loss:0.086, val_acc:0.965]
Epoch [41/120    avg_loss:0.054, val_acc:0.955]
Epoch [42/120    avg_loss:0.053, val_acc:0.957]
Epoch [43/120    avg_loss:0.072, val_acc:0.970]
Epoch [44/120    avg_loss:0.046, val_acc:0.967]
Epoch [45/120    avg_loss:0.042, val_acc:0.972]
Epoch [46/120    avg_loss:0.040, val_acc:0.971]
Epoch [47/120    avg_loss:0.079, val_acc:0.972]
Epoch [48/120    avg_loss:0.050, val_acc:0.969]
Epoch [49/120    avg_loss:0.046, val_acc:0.969]
Epoch [50/120    avg_loss:0.036, val_acc:0.977]
Epoch [51/120    avg_loss:0.040, val_acc:0.978]
Epoch [52/120    avg_loss:0.039, val_acc:0.959]
Epoch [53/120    avg_loss:0.051, val_acc:0.972]
Epoch [54/120    avg_loss:0.053, val_acc:0.965]
Epoch [55/120    avg_loss:0.036, val_acc:0.978]
Epoch [56/120    avg_loss:0.033, val_acc:0.976]
Epoch [57/120    avg_loss:0.026, val_acc:0.982]
Epoch [58/120    avg_loss:0.028, val_acc:0.972]
Epoch [59/120    avg_loss:0.023, val_acc:0.981]
Epoch [60/120    avg_loss:0.024, val_acc:0.973]
Epoch [61/120    avg_loss:0.028, val_acc:0.983]
Epoch [62/120    avg_loss:0.030, val_acc:0.978]
Epoch [63/120    avg_loss:0.019, val_acc:0.981]
Epoch [64/120    avg_loss:0.022, val_acc:0.978]
Epoch [65/120    avg_loss:0.017, val_acc:0.981]
Epoch [66/120    avg_loss:0.020, val_acc:0.975]
Epoch [67/120    avg_loss:0.047, val_acc:0.975]
Epoch [68/120    avg_loss:0.058, val_acc:0.970]
Epoch [69/120    avg_loss:0.025, val_acc:0.977]
Epoch [70/120    avg_loss:0.020, val_acc:0.981]
Epoch [71/120    avg_loss:0.017, val_acc:0.975]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.017, val_acc:0.972]
Epoch [74/120    avg_loss:0.034, val_acc:0.958]
Epoch [75/120    avg_loss:0.025, val_acc:0.980]
Epoch [76/120    avg_loss:0.019, val_acc:0.981]
Epoch [77/120    avg_loss:0.015, val_acc:0.983]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.984]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.016, val_acc:0.985]
Epoch [82/120    avg_loss:0.016, val_acc:0.985]
Epoch [83/120    avg_loss:0.018, val_acc:0.987]
Epoch [84/120    avg_loss:0.015, val_acc:0.984]
Epoch [85/120    avg_loss:0.012, val_acc:0.986]
Epoch [86/120    avg_loss:0.013, val_acc:0.984]
Epoch [87/120    avg_loss:0.014, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.986]
Epoch [91/120    avg_loss:0.013, val_acc:0.986]
Epoch [92/120    avg_loss:0.012, val_acc:0.987]
Epoch [93/120    avg_loss:0.011, val_acc:0.987]
Epoch [94/120    avg_loss:0.012, val_acc:0.985]
Epoch [95/120    avg_loss:0.013, val_acc:0.986]
Epoch [96/120    avg_loss:0.012, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.985]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.011, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.016, val_acc:0.986]
Epoch [111/120    avg_loss:0.013, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     0     0     0     0    50     0]
 [    0     0 17987     0    84     0    19     0     0     0]
 [    0     5     0  2017     2     0     0     0    12     0]
 [    0    55    15     0  2867     0     7     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     1     0     0  4863     0     0     0]
 [    0     4     0     0     0     0     1  1284     0     1]
 [    0    55     0     1    71     0     0     0  3442     2]
 [    0     0     0     0    14    47     0     0     0   858]]

Accuracy:
98.82389800689273

F1 scores:
[       nan 0.98693265 0.9963441  0.99482121 0.95407654 0.98231088
 0.99570025 0.997669   0.96916796 0.96404494]

Kappa:
0.9844256922682806
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f2e0ec908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.101, val_acc:0.086]
Epoch [2/120    avg_loss:1.764, val_acc:0.128]
Epoch [3/120    avg_loss:1.556, val_acc:0.308]
Epoch [4/120    avg_loss:1.394, val_acc:0.417]
Epoch [5/120    avg_loss:1.200, val_acc:0.474]
Epoch [6/120    avg_loss:1.050, val_acc:0.460]
Epoch [7/120    avg_loss:0.915, val_acc:0.483]
Epoch [8/120    avg_loss:0.792, val_acc:0.602]
Epoch [9/120    avg_loss:0.705, val_acc:0.673]
Epoch [10/120    avg_loss:0.640, val_acc:0.765]
Epoch [11/120    avg_loss:0.512, val_acc:0.768]
Epoch [12/120    avg_loss:0.454, val_acc:0.797]
Epoch [13/120    avg_loss:0.404, val_acc:0.819]
Epoch [14/120    avg_loss:0.374, val_acc:0.834]
Epoch [15/120    avg_loss:0.350, val_acc:0.834]
Epoch [16/120    avg_loss:0.293, val_acc:0.846]
Epoch [17/120    avg_loss:0.275, val_acc:0.880]
Epoch [18/120    avg_loss:0.247, val_acc:0.909]
Epoch [19/120    avg_loss:0.236, val_acc:0.890]
Epoch [20/120    avg_loss:0.227, val_acc:0.914]
Epoch [21/120    avg_loss:0.182, val_acc:0.965]
Epoch [22/120    avg_loss:0.154, val_acc:0.956]
Epoch [23/120    avg_loss:0.145, val_acc:0.944]
Epoch [24/120    avg_loss:0.128, val_acc:0.928]
Epoch [25/120    avg_loss:0.115, val_acc:0.972]
Epoch [26/120    avg_loss:0.107, val_acc:0.964]
Epoch [27/120    avg_loss:0.094, val_acc:0.944]
Epoch [28/120    avg_loss:0.107, val_acc:0.960]
Epoch [29/120    avg_loss:0.090, val_acc:0.928]
Epoch [30/120    avg_loss:0.079, val_acc:0.962]
Epoch [31/120    avg_loss:0.077, val_acc:0.951]
Epoch [32/120    avg_loss:0.082, val_acc:0.977]
Epoch [33/120    avg_loss:0.083, val_acc:0.974]
Epoch [34/120    avg_loss:0.085, val_acc:0.965]
Epoch [35/120    avg_loss:0.085, val_acc:0.925]
Epoch [36/120    avg_loss:0.074, val_acc:0.964]
Epoch [37/120    avg_loss:0.051, val_acc:0.983]
Epoch [38/120    avg_loss:0.050, val_acc:0.973]
Epoch [39/120    avg_loss:0.045, val_acc:0.972]
Epoch [40/120    avg_loss:0.039, val_acc:0.977]
Epoch [41/120    avg_loss:0.049, val_acc:0.971]
Epoch [42/120    avg_loss:0.059, val_acc:0.954]
Epoch [43/120    avg_loss:0.042, val_acc:0.980]
Epoch [44/120    avg_loss:0.043, val_acc:0.978]
Epoch [45/120    avg_loss:0.035, val_acc:0.978]
Epoch [46/120    avg_loss:0.034, val_acc:0.949]
Epoch [47/120    avg_loss:0.038, val_acc:0.984]
Epoch [48/120    avg_loss:0.030, val_acc:0.984]
Epoch [49/120    avg_loss:0.038, val_acc:0.972]
Epoch [50/120    avg_loss:0.038, val_acc:0.972]
Epoch [51/120    avg_loss:0.039, val_acc:0.984]
Epoch [52/120    avg_loss:0.032, val_acc:0.980]
Epoch [53/120    avg_loss:0.035, val_acc:0.983]
Epoch [54/120    avg_loss:0.025, val_acc:0.981]
Epoch [55/120    avg_loss:0.040, val_acc:0.950]
Epoch [56/120    avg_loss:0.053, val_acc:0.978]
Epoch [57/120    avg_loss:0.045, val_acc:0.969]
Epoch [58/120    avg_loss:0.038, val_acc:0.967]
Epoch [59/120    avg_loss:0.039, val_acc:0.978]
Epoch [60/120    avg_loss:0.040, val_acc:0.983]
Epoch [61/120    avg_loss:0.040, val_acc:0.983]
Epoch [62/120    avg_loss:0.030, val_acc:0.976]
Epoch [63/120    avg_loss:0.020, val_acc:0.984]
Epoch [64/120    avg_loss:0.017, val_acc:0.979]
Epoch [65/120    avg_loss:0.017, val_acc:0.985]
Epoch [66/120    avg_loss:0.016, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.986]
Epoch [68/120    avg_loss:0.016, val_acc:0.987]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.016, val_acc:0.986]
Epoch [71/120    avg_loss:0.014, val_acc:0.986]
Epoch [72/120    avg_loss:0.013, val_acc:0.987]
Epoch [73/120    avg_loss:0.012, val_acc:0.986]
Epoch [74/120    avg_loss:0.012, val_acc:0.987]
Epoch [75/120    avg_loss:0.013, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.013, val_acc:0.986]
Epoch [78/120    avg_loss:0.014, val_acc:0.987]
Epoch [79/120    avg_loss:0.014, val_acc:0.986]
Epoch [80/120    avg_loss:0.016, val_acc:0.986]
Epoch [81/120    avg_loss:0.012, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.013, val_acc:0.986]
Epoch [87/120    avg_loss:0.014, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.986]
Epoch [89/120    avg_loss:0.012, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.012, val_acc:0.986]
Epoch [97/120    avg_loss:0.014, val_acc:0.987]
Epoch [98/120    avg_loss:0.014, val_acc:0.985]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.012, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.985]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.985]
Epoch [108/120    avg_loss:0.012, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.014, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.013, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6383     0     0     0     0     0     0    49     0]
 [    0     0 18031     0    39     0    14     0     4     2]
 [    0     6     0  2021     0     0     0     0     8     1]
 [    0    45    25     0  2875     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7    21     0     0  4842     0     8     0]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     2     0     0    57     0     0     0  3495    17]
 [    0     0     0     1    14    33     0     0     0   871]]

Accuracy:
99.07695273901622

F1 scores:
[       nan 0.99207336 0.99748292 0.99092915 0.96525097 0.98751419
 0.99486337 0.99883586 0.97598436 0.96083839]

Kappa:
0.9877731078498863
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb45dddb978>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.056, val_acc:0.243]
Epoch [2/120    avg_loss:1.725, val_acc:0.245]
Epoch [3/120    avg_loss:1.547, val_acc:0.330]
Epoch [4/120    avg_loss:1.357, val_acc:0.361]
Epoch [5/120    avg_loss:1.192, val_acc:0.432]
Epoch [6/120    avg_loss:1.067, val_acc:0.491]
Epoch [7/120    avg_loss:0.957, val_acc:0.506]
Epoch [8/120    avg_loss:0.833, val_acc:0.521]
Epoch [9/120    avg_loss:0.735, val_acc:0.597]
Epoch [10/120    avg_loss:0.611, val_acc:0.618]
Epoch [11/120    avg_loss:0.560, val_acc:0.694]
Epoch [12/120    avg_loss:0.500, val_acc:0.750]
Epoch [13/120    avg_loss:0.418, val_acc:0.808]
Epoch [14/120    avg_loss:0.393, val_acc:0.785]
Epoch [15/120    avg_loss:0.340, val_acc:0.856]
Epoch [16/120    avg_loss:0.357, val_acc:0.829]
Epoch [17/120    avg_loss:0.307, val_acc:0.934]
Epoch [18/120    avg_loss:0.264, val_acc:0.905]
Epoch [19/120    avg_loss:0.237, val_acc:0.855]
Epoch [20/120    avg_loss:0.224, val_acc:0.938]
Epoch [21/120    avg_loss:0.213, val_acc:0.941]
Epoch [22/120    avg_loss:0.166, val_acc:0.934]
Epoch [23/120    avg_loss:0.172, val_acc:0.934]
Epoch [24/120    avg_loss:0.157, val_acc:0.936]
Epoch [25/120    avg_loss:0.147, val_acc:0.943]
Epoch [26/120    avg_loss:0.143, val_acc:0.954]
Epoch [27/120    avg_loss:0.125, val_acc:0.966]
Epoch [28/120    avg_loss:0.121, val_acc:0.962]
Epoch [29/120    avg_loss:0.145, val_acc:0.948]
Epoch [30/120    avg_loss:0.133, val_acc:0.961]
Epoch [31/120    avg_loss:0.101, val_acc:0.965]
Epoch [32/120    avg_loss:0.083, val_acc:0.964]
Epoch [33/120    avg_loss:0.076, val_acc:0.962]
Epoch [34/120    avg_loss:0.063, val_acc:0.965]
Epoch [35/120    avg_loss:0.068, val_acc:0.957]
Epoch [36/120    avg_loss:0.071, val_acc:0.954]
Epoch [37/120    avg_loss:0.058, val_acc:0.953]
Epoch [38/120    avg_loss:0.061, val_acc:0.966]
Epoch [39/120    avg_loss:0.053, val_acc:0.965]
Epoch [40/120    avg_loss:0.046, val_acc:0.969]
Epoch [41/120    avg_loss:0.040, val_acc:0.975]
Epoch [42/120    avg_loss:0.033, val_acc:0.974]
Epoch [43/120    avg_loss:0.027, val_acc:0.973]
Epoch [44/120    avg_loss:0.035, val_acc:0.972]
Epoch [45/120    avg_loss:0.045, val_acc:0.954]
Epoch [46/120    avg_loss:0.046, val_acc:0.965]
Epoch [47/120    avg_loss:0.033, val_acc:0.968]
Epoch [48/120    avg_loss:0.045, val_acc:0.970]
Epoch [49/120    avg_loss:0.035, val_acc:0.966]
Epoch [50/120    avg_loss:0.024, val_acc:0.975]
Epoch [51/120    avg_loss:0.026, val_acc:0.975]
Epoch [52/120    avg_loss:0.025, val_acc:0.972]
Epoch [53/120    avg_loss:0.028, val_acc:0.978]
Epoch [54/120    avg_loss:0.029, val_acc:0.972]
Epoch [55/120    avg_loss:0.036, val_acc:0.968]
Epoch [56/120    avg_loss:0.033, val_acc:0.978]
Epoch [57/120    avg_loss:0.016, val_acc:0.977]
Epoch [58/120    avg_loss:0.016, val_acc:0.980]
Epoch [59/120    avg_loss:0.015, val_acc:0.974]
Epoch [60/120    avg_loss:0.015, val_acc:0.977]
Epoch [61/120    avg_loss:0.029, val_acc:0.963]
Epoch [62/120    avg_loss:0.032, val_acc:0.972]
Epoch [63/120    avg_loss:0.046, val_acc:0.972]
Epoch [64/120    avg_loss:0.022, val_acc:0.972]
Epoch [65/120    avg_loss:0.020, val_acc:0.978]
Epoch [66/120    avg_loss:0.016, val_acc:0.966]
Epoch [67/120    avg_loss:0.018, val_acc:0.976]
Epoch [68/120    avg_loss:0.016, val_acc:0.978]
Epoch [69/120    avg_loss:0.014, val_acc:0.978]
Epoch [70/120    avg_loss:0.012, val_acc:0.980]
Epoch [71/120    avg_loss:0.014, val_acc:0.975]
Epoch [72/120    avg_loss:0.014, val_acc:0.977]
Epoch [73/120    avg_loss:0.020, val_acc:0.981]
Epoch [74/120    avg_loss:0.018, val_acc:0.978]
Epoch [75/120    avg_loss:0.014, val_acc:0.980]
Epoch [76/120    avg_loss:0.019, val_acc:0.972]
Epoch [77/120    avg_loss:0.026, val_acc:0.977]
Epoch [78/120    avg_loss:0.014, val_acc:0.978]
Epoch [79/120    avg_loss:0.018, val_acc:0.970]
Epoch [80/120    avg_loss:0.039, val_acc:0.972]
Epoch [81/120    avg_loss:0.021, val_acc:0.979]
Epoch [82/120    avg_loss:0.017, val_acc:0.978]
Epoch [83/120    avg_loss:0.015, val_acc:0.982]
Epoch [84/120    avg_loss:0.015, val_acc:0.976]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.009, val_acc:0.979]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.976]
Epoch [89/120    avg_loss:0.014, val_acc:0.972]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.006, val_acc:0.982]
Epoch [92/120    avg_loss:0.011, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.976]
Epoch [94/120    avg_loss:0.014, val_acc:0.977]
Epoch [95/120    avg_loss:0.022, val_acc:0.967]
Epoch [96/120    avg_loss:0.025, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.980]
Epoch [98/120    avg_loss:0.008, val_acc:0.977]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.982]
Epoch [102/120    avg_loss:0.004, val_acc:0.982]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.982]
Epoch [105/120    avg_loss:0.005, val_acc:0.982]
Epoch [106/120    avg_loss:0.005, val_acc:0.982]
Epoch [107/120    avg_loss:0.004, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.004, val_acc:0.982]
Epoch [116/120    avg_loss:0.005, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.006, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     0     1     0     0    25    16     3]
 [    0     0 18060     0    26     0     4     0     0     0]
 [    0     0     0  2014     2     0     0     5    11     4]
 [    0    27    22     0  2886     0     7     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     9    23     0     0     0  4843     0     0     3]
 [    0     0     0     0     0     0     0  1283     0     7]
 [    0    13     0     0    46     0     0     0  3505     7]
 [    0     0     0     1    14    40     0     0     0   864]]

Accuracy:
99.16612440652641

F1 scores:
[       nan 0.99269506 0.99792789 0.99432239 0.9705734  0.98490566
 0.99527333 0.98578563 0.9827562  0.95628113]

Kappa:
0.9889492208708461
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f366d886908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.053, val_acc:0.130]
Epoch [2/120    avg_loss:1.674, val_acc:0.245]
Epoch [3/120    avg_loss:1.435, val_acc:0.328]
Epoch [4/120    avg_loss:1.301, val_acc:0.355]
Epoch [5/120    avg_loss:1.169, val_acc:0.667]
Epoch [6/120    avg_loss:1.068, val_acc:0.718]
Epoch [7/120    avg_loss:0.957, val_acc:0.694]
Epoch [8/120    avg_loss:0.848, val_acc:0.783]
Epoch [9/120    avg_loss:0.770, val_acc:0.790]
Epoch [10/120    avg_loss:0.671, val_acc:0.768]
Epoch [11/120    avg_loss:0.612, val_acc:0.763]
Epoch [12/120    avg_loss:0.528, val_acc:0.699]
Epoch [13/120    avg_loss:0.447, val_acc:0.758]
Epoch [14/120    avg_loss:0.427, val_acc:0.778]
Epoch [15/120    avg_loss:0.371, val_acc:0.807]
Epoch [16/120    avg_loss:0.315, val_acc:0.791]
Epoch [17/120    avg_loss:0.317, val_acc:0.820]
Epoch [18/120    avg_loss:0.284, val_acc:0.879]
Epoch [19/120    avg_loss:0.262, val_acc:0.867]
Epoch [20/120    avg_loss:0.218, val_acc:0.931]
Epoch [21/120    avg_loss:0.160, val_acc:0.928]
Epoch [22/120    avg_loss:0.155, val_acc:0.902]
Epoch [23/120    avg_loss:0.156, val_acc:0.906]
Epoch [24/120    avg_loss:0.154, val_acc:0.938]
Epoch [25/120    avg_loss:0.153, val_acc:0.927]
Epoch [26/120    avg_loss:0.113, val_acc:0.952]
Epoch [27/120    avg_loss:0.109, val_acc:0.948]
Epoch [28/120    avg_loss:0.243, val_acc:0.838]
Epoch [29/120    avg_loss:0.180, val_acc:0.929]
Epoch [30/120    avg_loss:0.101, val_acc:0.952]
Epoch [31/120    avg_loss:0.101, val_acc:0.919]
Epoch [32/120    avg_loss:0.107, val_acc:0.952]
Epoch [33/120    avg_loss:0.091, val_acc:0.959]
Epoch [34/120    avg_loss:0.073, val_acc:0.961]
Epoch [35/120    avg_loss:0.075, val_acc:0.953]
Epoch [36/120    avg_loss:0.068, val_acc:0.946]
Epoch [37/120    avg_loss:0.067, val_acc:0.961]
Epoch [38/120    avg_loss:0.067, val_acc:0.938]
Epoch [39/120    avg_loss:0.045, val_acc:0.966]
Epoch [40/120    avg_loss:0.047, val_acc:0.948]
Epoch [41/120    avg_loss:0.036, val_acc:0.970]
Epoch [42/120    avg_loss:0.036, val_acc:0.966]
Epoch [43/120    avg_loss:0.032, val_acc:0.963]
Epoch [44/120    avg_loss:0.030, val_acc:0.974]
Epoch [45/120    avg_loss:0.024, val_acc:0.959]
Epoch [46/120    avg_loss:0.027, val_acc:0.968]
Epoch [47/120    avg_loss:0.040, val_acc:0.973]
Epoch [48/120    avg_loss:0.028, val_acc:0.967]
Epoch [49/120    avg_loss:0.029, val_acc:0.968]
Epoch [50/120    avg_loss:0.027, val_acc:0.976]
Epoch [51/120    avg_loss:0.028, val_acc:0.974]
Epoch [52/120    avg_loss:0.024, val_acc:0.977]
Epoch [53/120    avg_loss:0.017, val_acc:0.975]
Epoch [54/120    avg_loss:0.020, val_acc:0.973]
Epoch [55/120    avg_loss:0.021, val_acc:0.976]
Epoch [56/120    avg_loss:0.015, val_acc:0.977]
Epoch [57/120    avg_loss:0.014, val_acc:0.972]
Epoch [58/120    avg_loss:0.020, val_acc:0.973]
Epoch [59/120    avg_loss:0.026, val_acc:0.975]
Epoch [60/120    avg_loss:0.034, val_acc:0.976]
Epoch [61/120    avg_loss:0.084, val_acc:0.943]
Epoch [62/120    avg_loss:0.062, val_acc:0.959]
Epoch [63/120    avg_loss:0.046, val_acc:0.973]
Epoch [64/120    avg_loss:0.027, val_acc:0.959]
Epoch [65/120    avg_loss:0.028, val_acc:0.965]
Epoch [66/120    avg_loss:0.023, val_acc:0.976]
Epoch [67/120    avg_loss:0.014, val_acc:0.978]
Epoch [68/120    avg_loss:0.015, val_acc:0.966]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.013, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.980]
Epoch [72/120    avg_loss:0.011, val_acc:0.969]
Epoch [73/120    avg_loss:0.018, val_acc:0.978]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.011, val_acc:0.973]
Epoch [76/120    avg_loss:0.013, val_acc:0.976]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.010, val_acc:0.978]
Epoch [79/120    avg_loss:0.008, val_acc:0.980]
Epoch [80/120    avg_loss:0.011, val_acc:0.976]
Epoch [81/120    avg_loss:0.009, val_acc:0.979]
Epoch [82/120    avg_loss:0.014, val_acc:0.975]
Epoch [83/120    avg_loss:0.011, val_acc:0.957]
Epoch [84/120    avg_loss:0.013, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.976]
Epoch [86/120    avg_loss:0.013, val_acc:0.974]
Epoch [87/120    avg_loss:0.011, val_acc:0.977]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.008, val_acc:0.979]
Epoch [90/120    avg_loss:0.007, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.006, val_acc:0.976]
Epoch [93/120    avg_loss:0.008, val_acc:0.974]
Epoch [94/120    avg_loss:0.010, val_acc:0.974]
Epoch [95/120    avg_loss:0.009, val_acc:0.963]
Epoch [96/120    avg_loss:0.041, val_acc:0.955]
Epoch [97/120    avg_loss:0.015, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.979]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.007, val_acc:0.980]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.980]
Epoch [103/120    avg_loss:0.008, val_acc:0.981]
Epoch [104/120    avg_loss:0.009, val_acc:0.981]
Epoch [105/120    avg_loss:0.008, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.007, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.007, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6262     0     1    13     0     3     0    67    86]
 [    0     1 18059     0    30     0     0     0     0     0]
 [    0     0     0  2015     3     0     0     0    15     3]
 [    0    48    19     0  2872     0     8     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12    19     0     0  4838     0     4     5]
 [    0     2     0     0     0     0     1  1277     0    10]
 [    0    20     0     8    67     0     0     0  3460    16]
 [    0     0     0     1    14    44     0     0     0   860]]

Accuracy:
98.6865254380257

F1 scores:
[       nan 0.98112025 0.99828635 0.9877451  0.96198292 0.98342125
 0.99465461 0.99493572 0.96891627 0.90573986]

Kappa:
0.9826034357908231
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f31fdc8d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.040, val_acc:0.141]
Epoch [2/120    avg_loss:1.718, val_acc:0.180]
Epoch [3/120    avg_loss:1.506, val_acc:0.294]
Epoch [4/120    avg_loss:1.351, val_acc:0.306]
Epoch [5/120    avg_loss:1.185, val_acc:0.408]
Epoch [6/120    avg_loss:1.095, val_acc:0.445]
Epoch [7/120    avg_loss:0.966, val_acc:0.653]
Epoch [8/120    avg_loss:0.849, val_acc:0.661]
Epoch [9/120    avg_loss:0.703, val_acc:0.769]
Epoch [10/120    avg_loss:0.620, val_acc:0.764]
Epoch [11/120    avg_loss:0.533, val_acc:0.798]
Epoch [12/120    avg_loss:0.465, val_acc:0.812]
Epoch [13/120    avg_loss:0.430, val_acc:0.807]
Epoch [14/120    avg_loss:0.381, val_acc:0.861]
Epoch [15/120    avg_loss:0.297, val_acc:0.901]
Epoch [16/120    avg_loss:0.283, val_acc:0.917]
Epoch [17/120    avg_loss:0.256, val_acc:0.942]
Epoch [18/120    avg_loss:0.207, val_acc:0.944]
Epoch [19/120    avg_loss:0.186, val_acc:0.918]
Epoch [20/120    avg_loss:0.227, val_acc:0.949]
Epoch [21/120    avg_loss:0.185, val_acc:0.943]
Epoch [22/120    avg_loss:0.156, val_acc:0.945]
Epoch [23/120    avg_loss:0.135, val_acc:0.958]
Epoch [24/120    avg_loss:0.125, val_acc:0.960]
Epoch [25/120    avg_loss:0.127, val_acc:0.964]
Epoch [26/120    avg_loss:0.105, val_acc:0.953]
Epoch [27/120    avg_loss:0.123, val_acc:0.967]
Epoch [28/120    avg_loss:0.083, val_acc:0.962]
Epoch [29/120    avg_loss:0.087, val_acc:0.966]
Epoch [30/120    avg_loss:0.089, val_acc:0.950]
Epoch [31/120    avg_loss:0.080, val_acc:0.953]
Epoch [32/120    avg_loss:0.063, val_acc:0.966]
Epoch [33/120    avg_loss:0.056, val_acc:0.972]
Epoch [34/120    avg_loss:0.048, val_acc:0.977]
Epoch [35/120    avg_loss:0.052, val_acc:0.965]
Epoch [36/120    avg_loss:0.056, val_acc:0.970]
Epoch [37/120    avg_loss:0.071, val_acc:0.969]
Epoch [38/120    avg_loss:0.048, val_acc:0.971]
Epoch [39/120    avg_loss:0.057, val_acc:0.972]
Epoch [40/120    avg_loss:0.045, val_acc:0.972]
Epoch [41/120    avg_loss:0.040, val_acc:0.978]
Epoch [42/120    avg_loss:0.063, val_acc:0.972]
Epoch [43/120    avg_loss:0.032, val_acc:0.973]
Epoch [44/120    avg_loss:0.026, val_acc:0.974]
Epoch [45/120    avg_loss:0.041, val_acc:0.972]
Epoch [46/120    avg_loss:0.044, val_acc:0.982]
Epoch [47/120    avg_loss:0.037, val_acc:0.979]
Epoch [48/120    avg_loss:0.040, val_acc:0.959]
Epoch [49/120    avg_loss:0.029, val_acc:0.977]
Epoch [50/120    avg_loss:0.032, val_acc:0.979]
Epoch [51/120    avg_loss:0.021, val_acc:0.976]
Epoch [52/120    avg_loss:0.019, val_acc:0.978]
Epoch [53/120    avg_loss:0.020, val_acc:0.979]
Epoch [54/120    avg_loss:0.019, val_acc:0.978]
Epoch [55/120    avg_loss:0.017, val_acc:0.982]
Epoch [56/120    avg_loss:0.019, val_acc:0.981]
Epoch [57/120    avg_loss:0.040, val_acc:0.978]
Epoch [58/120    avg_loss:0.021, val_acc:0.981]
Epoch [59/120    avg_loss:0.018, val_acc:0.981]
Epoch [60/120    avg_loss:0.032, val_acc:0.978]
Epoch [61/120    avg_loss:0.018, val_acc:0.980]
Epoch [62/120    avg_loss:0.016, val_acc:0.978]
Epoch [63/120    avg_loss:0.022, val_acc:0.973]
Epoch [64/120    avg_loss:0.017, val_acc:0.982]
Epoch [65/120    avg_loss:0.015, val_acc:0.980]
Epoch [66/120    avg_loss:0.012, val_acc:0.980]
Epoch [67/120    avg_loss:0.021, val_acc:0.980]
Epoch [68/120    avg_loss:0.022, val_acc:0.974]
Epoch [69/120    avg_loss:0.023, val_acc:0.971]
Epoch [70/120    avg_loss:0.020, val_acc:0.978]
Epoch [71/120    avg_loss:0.015, val_acc:0.978]
Epoch [72/120    avg_loss:0.019, val_acc:0.977]
Epoch [73/120    avg_loss:0.011, val_acc:0.981]
Epoch [74/120    avg_loss:0.022, val_acc:0.974]
Epoch [75/120    avg_loss:0.019, val_acc:0.982]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.982]
Epoch [78/120    avg_loss:0.014, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.976]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.014, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.026, val_acc:0.984]
Epoch [92/120    avg_loss:0.032, val_acc:0.976]
Epoch [93/120    avg_loss:0.019, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.020, val_acc:0.972]
Epoch [104/120    avg_loss:0.016, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.013, val_acc:0.984]
Epoch [108/120    avg_loss:0.013, val_acc:0.984]
Epoch [109/120    avg_loss:0.013, val_acc:0.969]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.979]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.975]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     0     0     0     0     4    60     9]
 [    0     0 18039     0    44     0     7     0     0     0]
 [    0     3     0  2011     1     0     0     0    14     7]
 [    0    34    19     0  2884     0     9     0    21     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     7     0     0  4868     0     0     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    23     0     1    49     0     0     0  3493     5]
 [    0     0     0     0    14    76     0     0     0   829]]

Accuracy:
98.99742125177741

F1 scores:
[       nan 0.98965061 0.99798069 0.9918619  0.96713615 0.97170514
 0.99733661 0.99806427 0.97583461 0.93408451]

Kappa:
0.9867198077225688
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff186f13908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.019, val_acc:0.153]
Epoch [2/120    avg_loss:1.752, val_acc:0.235]
Epoch [3/120    avg_loss:1.521, val_acc:0.317]
Epoch [4/120    avg_loss:1.333, val_acc:0.353]
Epoch [5/120    avg_loss:1.167, val_acc:0.429]
Epoch [6/120    avg_loss:1.026, val_acc:0.442]
Epoch [7/120    avg_loss:0.885, val_acc:0.541]
Epoch [8/120    avg_loss:0.808, val_acc:0.615]
Epoch [9/120    avg_loss:0.699, val_acc:0.618]
Epoch [10/120    avg_loss:0.631, val_acc:0.694]
Epoch [11/120    avg_loss:0.550, val_acc:0.749]
Epoch [12/120    avg_loss:0.482, val_acc:0.794]
Epoch [13/120    avg_loss:0.455, val_acc:0.788]
Epoch [14/120    avg_loss:0.375, val_acc:0.805]
Epoch [15/120    avg_loss:0.333, val_acc:0.788]
Epoch [16/120    avg_loss:0.301, val_acc:0.819]
Epoch [17/120    avg_loss:0.302, val_acc:0.828]
Epoch [18/120    avg_loss:0.263, val_acc:0.842]
Epoch [19/120    avg_loss:0.240, val_acc:0.871]
Epoch [20/120    avg_loss:0.253, val_acc:0.836]
Epoch [21/120    avg_loss:0.225, val_acc:0.830]
Epoch [22/120    avg_loss:0.264, val_acc:0.910]
Epoch [23/120    avg_loss:0.204, val_acc:0.909]
Epoch [24/120    avg_loss:0.189, val_acc:0.903]
Epoch [25/120    avg_loss:0.159, val_acc:0.937]
Epoch [26/120    avg_loss:0.131, val_acc:0.947]
Epoch [27/120    avg_loss:0.146, val_acc:0.915]
Epoch [28/120    avg_loss:0.152, val_acc:0.933]
Epoch [29/120    avg_loss:0.128, val_acc:0.941]
Epoch [30/120    avg_loss:0.144, val_acc:0.952]
Epoch [31/120    avg_loss:0.127, val_acc:0.949]
Epoch [32/120    avg_loss:0.116, val_acc:0.953]
Epoch [33/120    avg_loss:0.094, val_acc:0.963]
Epoch [34/120    avg_loss:0.094, val_acc:0.954]
Epoch [35/120    avg_loss:0.076, val_acc:0.948]
Epoch [36/120    avg_loss:0.059, val_acc:0.966]
Epoch [37/120    avg_loss:0.083, val_acc:0.958]
Epoch [38/120    avg_loss:0.072, val_acc:0.966]
Epoch [39/120    avg_loss:0.058, val_acc:0.969]
Epoch [40/120    avg_loss:0.073, val_acc:0.948]
Epoch [41/120    avg_loss:0.067, val_acc:0.965]
Epoch [42/120    avg_loss:0.054, val_acc:0.968]
Epoch [43/120    avg_loss:0.044, val_acc:0.965]
Epoch [44/120    avg_loss:0.052, val_acc:0.974]
Epoch [45/120    avg_loss:0.049, val_acc:0.968]
Epoch [46/120    avg_loss:0.039, val_acc:0.977]
Epoch [47/120    avg_loss:0.041, val_acc:0.968]
Epoch [48/120    avg_loss:0.041, val_acc:0.969]
Epoch [49/120    avg_loss:0.034, val_acc:0.962]
Epoch [50/120    avg_loss:0.041, val_acc:0.969]
Epoch [51/120    avg_loss:0.047, val_acc:0.976]
Epoch [52/120    avg_loss:0.039, val_acc:0.954]
Epoch [53/120    avg_loss:0.054, val_acc:0.976]
Epoch [54/120    avg_loss:0.043, val_acc:0.963]
Epoch [55/120    avg_loss:0.036, val_acc:0.966]
Epoch [56/120    avg_loss:0.048, val_acc:0.978]
Epoch [57/120    avg_loss:0.042, val_acc:0.972]
Epoch [58/120    avg_loss:0.028, val_acc:0.978]
Epoch [59/120    avg_loss:0.025, val_acc:0.978]
Epoch [60/120    avg_loss:0.027, val_acc:0.962]
Epoch [61/120    avg_loss:0.038, val_acc:0.977]
Epoch [62/120    avg_loss:0.027, val_acc:0.978]
Epoch [63/120    avg_loss:0.031, val_acc:0.969]
Epoch [64/120    avg_loss:0.026, val_acc:0.979]
Epoch [65/120    avg_loss:0.024, val_acc:0.967]
Epoch [66/120    avg_loss:0.027, val_acc:0.981]
Epoch [67/120    avg_loss:0.032, val_acc:0.972]
Epoch [68/120    avg_loss:0.027, val_acc:0.980]
Epoch [69/120    avg_loss:0.027, val_acc:0.966]
Epoch [70/120    avg_loss:0.019, val_acc:0.981]
Epoch [71/120    avg_loss:0.019, val_acc:0.978]
Epoch [72/120    avg_loss:0.021, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.022, val_acc:0.987]
Epoch [75/120    avg_loss:0.017, val_acc:0.978]
Epoch [76/120    avg_loss:0.022, val_acc:0.984]
Epoch [77/120    avg_loss:0.040, val_acc:0.967]
Epoch [78/120    avg_loss:0.047, val_acc:0.974]
Epoch [79/120    avg_loss:0.035, val_acc:0.980]
Epoch [80/120    avg_loss:0.027, val_acc:0.975]
Epoch [81/120    avg_loss:0.016, val_acc:0.978]
Epoch [82/120    avg_loss:0.017, val_acc:0.982]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.016, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.981]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.010, val_acc:0.982]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.008, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6319     0    22     1     0     0     0    79    11]
 [    0     0 17999     0    44     0    47     0     0     0]
 [    0     1     0  2017     0     0     0     0    18     0]
 [    0    31    19     0  2888     0    10     0    24     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0    14     0     0     0  4864     0     0     0]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     4     0     0    59     0     0     0  3501     7]
 [    0     0     0     0    14    44     0     1     0   860]]

Accuracy:
98.90824958426722

F1 scores:
[       nan 0.98834754 0.99656719 0.98993865 0.96620943 0.9826546
 0.99265306 0.99922481 0.97344641 0.95608671]

Kappa:
0.9855450129921881
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f26abf30828>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.065, val_acc:0.129]
Epoch [2/120    avg_loss:1.697, val_acc:0.224]
Epoch [3/120    avg_loss:1.453, val_acc:0.250]
Epoch [4/120    avg_loss:1.358, val_acc:0.371]
Epoch [5/120    avg_loss:1.215, val_acc:0.436]
Epoch [6/120    avg_loss:1.065, val_acc:0.584]
Epoch [7/120    avg_loss:0.979, val_acc:0.621]
Epoch [8/120    avg_loss:0.894, val_acc:0.765]
Epoch [9/120    avg_loss:0.798, val_acc:0.684]
Epoch [10/120    avg_loss:0.702, val_acc:0.703]
Epoch [11/120    avg_loss:0.619, val_acc:0.580]
Epoch [12/120    avg_loss:0.549, val_acc:0.728]
Epoch [13/120    avg_loss:0.436, val_acc:0.782]
Epoch [14/120    avg_loss:0.382, val_acc:0.861]
Epoch [15/120    avg_loss:0.357, val_acc:0.879]
Epoch [16/120    avg_loss:0.308, val_acc:0.875]
Epoch [17/120    avg_loss:0.296, val_acc:0.897]
Epoch [18/120    avg_loss:0.242, val_acc:0.945]
Epoch [19/120    avg_loss:0.203, val_acc:0.953]
Epoch [20/120    avg_loss:0.178, val_acc:0.956]
Epoch [21/120    avg_loss:0.213, val_acc:0.929]
Epoch [22/120    avg_loss:0.160, val_acc:0.941]
Epoch [23/120    avg_loss:0.143, val_acc:0.915]
Epoch [24/120    avg_loss:0.129, val_acc:0.965]
Epoch [25/120    avg_loss:0.131, val_acc:0.901]
Epoch [26/120    avg_loss:0.127, val_acc:0.973]
Epoch [27/120    avg_loss:0.100, val_acc:0.974]
Epoch [28/120    avg_loss:0.090, val_acc:0.979]
Epoch [29/120    avg_loss:0.080, val_acc:0.978]
Epoch [30/120    avg_loss:0.091, val_acc:0.930]
Epoch [31/120    avg_loss:0.106, val_acc:0.951]
Epoch [32/120    avg_loss:0.088, val_acc:0.976]
Epoch [33/120    avg_loss:0.068, val_acc:0.934]
Epoch [34/120    avg_loss:0.075, val_acc:0.971]
Epoch [35/120    avg_loss:0.067, val_acc:0.984]
Epoch [36/120    avg_loss:0.060, val_acc:0.968]
Epoch [37/120    avg_loss:0.058, val_acc:0.978]
Epoch [38/120    avg_loss:0.079, val_acc:0.975]
Epoch [39/120    avg_loss:0.072, val_acc:0.959]
Epoch [40/120    avg_loss:0.068, val_acc:0.966]
Epoch [41/120    avg_loss:0.062, val_acc:0.966]
Epoch [42/120    avg_loss:0.076, val_acc:0.970]
Epoch [43/120    avg_loss:0.044, val_acc:0.982]
Epoch [44/120    avg_loss:0.037, val_acc:0.980]
Epoch [45/120    avg_loss:0.028, val_acc:0.979]
Epoch [46/120    avg_loss:0.024, val_acc:0.984]
Epoch [47/120    avg_loss:0.037, val_acc:0.957]
Epoch [48/120    avg_loss:0.030, val_acc:0.981]
Epoch [49/120    avg_loss:0.055, val_acc:0.974]
Epoch [50/120    avg_loss:0.046, val_acc:0.970]
Epoch [51/120    avg_loss:0.058, val_acc:0.965]
Epoch [52/120    avg_loss:0.121, val_acc:0.962]
Epoch [53/120    avg_loss:0.033, val_acc:0.980]
Epoch [54/120    avg_loss:0.030, val_acc:0.950]
Epoch [55/120    avg_loss:0.026, val_acc:0.984]
Epoch [56/120    avg_loss:0.022, val_acc:0.978]
Epoch [57/120    avg_loss:0.032, val_acc:0.978]
Epoch [58/120    avg_loss:0.021, val_acc:0.990]
Epoch [59/120    avg_loss:0.015, val_acc:0.990]
Epoch [60/120    avg_loss:0.018, val_acc:0.978]
Epoch [61/120    avg_loss:0.037, val_acc:0.971]
Epoch [62/120    avg_loss:0.072, val_acc:0.985]
Epoch [63/120    avg_loss:0.042, val_acc:0.982]
Epoch [64/120    avg_loss:0.030, val_acc:0.991]
Epoch [65/120    avg_loss:0.021, val_acc:0.988]
Epoch [66/120    avg_loss:0.023, val_acc:0.968]
Epoch [67/120    avg_loss:0.021, val_acc:0.991]
Epoch [68/120    avg_loss:0.017, val_acc:0.991]
Epoch [69/120    avg_loss:0.021, val_acc:0.981]
Epoch [70/120    avg_loss:0.015, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.989]
Epoch [72/120    avg_loss:0.012, val_acc:0.993]
Epoch [73/120    avg_loss:0.014, val_acc:0.981]
Epoch [74/120    avg_loss:0.013, val_acc:0.991]
Epoch [75/120    avg_loss:0.019, val_acc:0.989]
Epoch [76/120    avg_loss:0.018, val_acc:0.969]
Epoch [77/120    avg_loss:0.014, val_acc:0.987]
Epoch [78/120    avg_loss:0.010, val_acc:0.987]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.013, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.990]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.007, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.008, val_acc:0.990]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.009, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.007, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.989]
Epoch [105/120    avg_loss:0.007, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0    10     0    26     0]
 [    0     0 18073     0     6     0    10     0     1     0]
 [    0     0     0  2030     0     0     0     0     5     1]
 [    0    46    26     0  2873     0     1     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     9     0     0  4858     0     0     0]
 [    0     5     0     0     0     0     0  1278     0     7]
 [    0     1     0     5    61     0     0     0  3491    13]
 [    0     0     0     0    14    49     0     0     0   856]]

Accuracy:
99.19745499240835

F1 scores:
[       nan 0.9931677  0.99850829 0.99509804 0.96962538 0.98157202
 0.99579789 0.9953271  0.98061798 0.9532294 ]

Kappa:
0.9893628164565452
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff11b9c9908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.101, val_acc:0.124]
Epoch [2/120    avg_loss:1.763, val_acc:0.178]
Epoch [3/120    avg_loss:1.533, val_acc:0.242]
Epoch [4/120    avg_loss:1.359, val_acc:0.374]
Epoch [5/120    avg_loss:1.205, val_acc:0.478]
Epoch [6/120    avg_loss:1.081, val_acc:0.459]
Epoch [7/120    avg_loss:0.985, val_acc:0.528]
Epoch [8/120    avg_loss:0.879, val_acc:0.586]
Epoch [9/120    avg_loss:0.770, val_acc:0.648]
Epoch [10/120    avg_loss:0.681, val_acc:0.650]
Epoch [11/120    avg_loss:0.591, val_acc:0.687]
Epoch [12/120    avg_loss:0.510, val_acc:0.716]
Epoch [13/120    avg_loss:0.463, val_acc:0.769]
Epoch [14/120    avg_loss:0.393, val_acc:0.822]
Epoch [15/120    avg_loss:0.378, val_acc:0.785]
Epoch [16/120    avg_loss:0.342, val_acc:0.908]
Epoch [17/120    avg_loss:0.476, val_acc:0.791]
Epoch [18/120    avg_loss:0.344, val_acc:0.887]
Epoch [19/120    avg_loss:0.283, val_acc:0.880]
Epoch [20/120    avg_loss:0.230, val_acc:0.833]
Epoch [21/120    avg_loss:0.213, val_acc:0.906]
Epoch [22/120    avg_loss:0.202, val_acc:0.891]
Epoch [23/120    avg_loss:0.211, val_acc:0.909]
Epoch [24/120    avg_loss:0.193, val_acc:0.947]
Epoch [25/120    avg_loss:0.157, val_acc:0.947]
Epoch [26/120    avg_loss:0.148, val_acc:0.941]
Epoch [27/120    avg_loss:0.127, val_acc:0.950]
Epoch [28/120    avg_loss:0.141, val_acc:0.930]
Epoch [29/120    avg_loss:0.111, val_acc:0.955]
Epoch [30/120    avg_loss:0.119, val_acc:0.949]
Epoch [31/120    avg_loss:0.116, val_acc:0.934]
Epoch [32/120    avg_loss:0.160, val_acc:0.951]
Epoch [33/120    avg_loss:0.117, val_acc:0.953]
Epoch [34/120    avg_loss:0.095, val_acc:0.950]
Epoch [35/120    avg_loss:0.075, val_acc:0.954]
Epoch [36/120    avg_loss:0.076, val_acc:0.958]
Epoch [37/120    avg_loss:0.081, val_acc:0.964]
Epoch [38/120    avg_loss:0.065, val_acc:0.948]
Epoch [39/120    avg_loss:0.058, val_acc:0.964]
Epoch [40/120    avg_loss:0.057, val_acc:0.971]
Epoch [41/120    avg_loss:0.057, val_acc:0.949]
Epoch [42/120    avg_loss:0.062, val_acc:0.959]
Epoch [43/120    avg_loss:0.050, val_acc:0.970]
Epoch [44/120    avg_loss:0.085, val_acc:0.963]
Epoch [45/120    avg_loss:0.055, val_acc:0.971]
Epoch [46/120    avg_loss:0.051, val_acc:0.960]
Epoch [47/120    avg_loss:0.057, val_acc:0.972]
Epoch [48/120    avg_loss:0.049, val_acc:0.974]
Epoch [49/120    avg_loss:0.063, val_acc:0.965]
Epoch [50/120    avg_loss:0.048, val_acc:0.943]
Epoch [51/120    avg_loss:0.031, val_acc:0.970]
Epoch [52/120    avg_loss:0.029, val_acc:0.981]
Epoch [53/120    avg_loss:0.027, val_acc:0.970]
Epoch [54/120    avg_loss:0.028, val_acc:0.978]
Epoch [55/120    avg_loss:0.027, val_acc:0.978]
Epoch [56/120    avg_loss:0.023, val_acc:0.977]
Epoch [57/120    avg_loss:0.024, val_acc:0.973]
Epoch [58/120    avg_loss:0.031, val_acc:0.966]
Epoch [59/120    avg_loss:0.033, val_acc:0.968]
Epoch [60/120    avg_loss:0.024, val_acc:0.979]
Epoch [61/120    avg_loss:0.021, val_acc:0.975]
Epoch [62/120    avg_loss:0.018, val_acc:0.983]
Epoch [63/120    avg_loss:0.018, val_acc:0.981]
Epoch [64/120    avg_loss:0.019, val_acc:0.981]
Epoch [65/120    avg_loss:0.017, val_acc:0.981]
Epoch [66/120    avg_loss:0.017, val_acc:0.978]
Epoch [67/120    avg_loss:0.028, val_acc:0.981]
Epoch [68/120    avg_loss:0.018, val_acc:0.985]
Epoch [69/120    avg_loss:0.018, val_acc:0.978]
Epoch [70/120    avg_loss:0.016, val_acc:0.975]
Epoch [71/120    avg_loss:0.018, val_acc:0.978]
Epoch [72/120    avg_loss:0.011, val_acc:0.981]
Epoch [73/120    avg_loss:0.012, val_acc:0.979]
Epoch [74/120    avg_loss:0.026, val_acc:0.978]
Epoch [75/120    avg_loss:0.015, val_acc:0.980]
Epoch [76/120    avg_loss:0.012, val_acc:0.981]
Epoch [77/120    avg_loss:0.015, val_acc:0.973]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.016, val_acc:0.978]
Epoch [80/120    avg_loss:0.025, val_acc:0.976]
Epoch [81/120    avg_loss:0.025, val_acc:0.976]
Epoch [82/120    avg_loss:0.015, val_acc:0.977]
Epoch [83/120    avg_loss:0.012, val_acc:0.978]
Epoch [84/120    avg_loss:0.009, val_acc:0.978]
Epoch [85/120    avg_loss:0.013, val_acc:0.979]
Epoch [86/120    avg_loss:0.010, val_acc:0.978]
Epoch [87/120    avg_loss:0.011, val_acc:0.978]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.979]
Epoch [90/120    avg_loss:0.009, val_acc:0.979]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.979]
Epoch [93/120    avg_loss:0.009, val_acc:0.979]
Epoch [94/120    avg_loss:0.009, val_acc:0.979]
Epoch [95/120    avg_loss:0.008, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.007, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.009, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.979]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.009, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.979]
Epoch [108/120    avg_loss:0.009, val_acc:0.979]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.007, val_acc:0.979]
Epoch [111/120    avg_loss:0.008, val_acc:0.979]
Epoch [112/120    avg_loss:0.008, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.007, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.979]
Epoch [116/120    avg_loss:0.009, val_acc:0.979]
Epoch [117/120    avg_loss:0.012, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.010, val_acc:0.979]
Epoch [120/120    avg_loss:0.013, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6378     0     0     0     0    16     0    38     0]
 [    0     2 18067     0    12     0     6     0     3     0]
 [    0    11     0  2015     0     0     0     0     8     2]
 [    0    42    19     0  2870     0     7     0    34     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2    16     0     0  4851     0     4     5]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     0     0     3    60     0     0     0  3505     3]
 [    0     0     0     0    18    59     0     0     0   842]]

Accuracy:
99.10587327983033

F1 scores:
[       nan 0.9915274  0.99878379 0.99017199 0.96763318 0.97789434
 0.99426112 0.99961225 0.97864023 0.9503386 ]

Kappa:
0.9881520044450351
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f1f5cf940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.130, val_acc:0.171]
Epoch [2/120    avg_loss:1.787, val_acc:0.260]
Epoch [3/120    avg_loss:1.572, val_acc:0.257]
Epoch [4/120    avg_loss:1.377, val_acc:0.320]
Epoch [5/120    avg_loss:1.254, val_acc:0.405]
Epoch [6/120    avg_loss:1.115, val_acc:0.440]
Epoch [7/120    avg_loss:0.953, val_acc:0.491]
Epoch [8/120    avg_loss:0.850, val_acc:0.536]
Epoch [9/120    avg_loss:0.786, val_acc:0.630]
Epoch [10/120    avg_loss:0.664, val_acc:0.703]
Epoch [11/120    avg_loss:0.600, val_acc:0.719]
Epoch [12/120    avg_loss:0.502, val_acc:0.726]
Epoch [13/120    avg_loss:0.450, val_acc:0.822]
Epoch [14/120    avg_loss:0.398, val_acc:0.802]
Epoch [15/120    avg_loss:0.372, val_acc:0.829]
Epoch [16/120    avg_loss:0.313, val_acc:0.841]
Epoch [17/120    avg_loss:0.290, val_acc:0.873]
Epoch [18/120    avg_loss:0.291, val_acc:0.858]
Epoch [19/120    avg_loss:0.253, val_acc:0.883]
Epoch [20/120    avg_loss:0.246, val_acc:0.934]
Epoch [21/120    avg_loss:0.225, val_acc:0.904]
Epoch [22/120    avg_loss:0.208, val_acc:0.932]
Epoch [23/120    avg_loss:0.153, val_acc:0.943]
Epoch [24/120    avg_loss:0.152, val_acc:0.937]
Epoch [25/120    avg_loss:0.151, val_acc:0.950]
Epoch [26/120    avg_loss:0.139, val_acc:0.954]
Epoch [27/120    avg_loss:0.140, val_acc:0.935]
Epoch [28/120    avg_loss:0.139, val_acc:0.941]
Epoch [29/120    avg_loss:0.101, val_acc:0.955]
Epoch [30/120    avg_loss:0.098, val_acc:0.954]
Epoch [31/120    avg_loss:0.103, val_acc:0.966]
Epoch [32/120    avg_loss:0.081, val_acc:0.967]
Epoch [33/120    avg_loss:0.067, val_acc:0.973]
Epoch [34/120    avg_loss:0.099, val_acc:0.966]
Epoch [35/120    avg_loss:0.065, val_acc:0.973]
Epoch [36/120    avg_loss:0.062, val_acc:0.968]
Epoch [37/120    avg_loss:0.063, val_acc:0.959]
Epoch [38/120    avg_loss:0.119, val_acc:0.953]
Epoch [39/120    avg_loss:0.115, val_acc:0.951]
Epoch [40/120    avg_loss:0.072, val_acc:0.959]
Epoch [41/120    avg_loss:0.056, val_acc:0.977]
Epoch [42/120    avg_loss:0.058, val_acc:0.960]
Epoch [43/120    avg_loss:0.061, val_acc:0.974]
Epoch [44/120    avg_loss:0.047, val_acc:0.979]
Epoch [45/120    avg_loss:0.050, val_acc:0.977]
Epoch [46/120    avg_loss:0.030, val_acc:0.984]
Epoch [47/120    avg_loss:0.040, val_acc:0.979]
Epoch [48/120    avg_loss:0.033, val_acc:0.984]
Epoch [49/120    avg_loss:0.028, val_acc:0.979]
Epoch [50/120    avg_loss:0.030, val_acc:0.976]
Epoch [51/120    avg_loss:0.030, val_acc:0.979]
Epoch [52/120    avg_loss:0.033, val_acc:0.984]
Epoch [53/120    avg_loss:0.031, val_acc:0.984]
Epoch [54/120    avg_loss:0.037, val_acc:0.980]
Epoch [55/120    avg_loss:0.023, val_acc:0.984]
Epoch [56/120    avg_loss:0.032, val_acc:0.974]
Epoch [57/120    avg_loss:0.030, val_acc:0.983]
Epoch [58/120    avg_loss:0.033, val_acc:0.915]
Epoch [59/120    avg_loss:0.031, val_acc:0.974]
Epoch [60/120    avg_loss:0.029, val_acc:0.979]
Epoch [61/120    avg_loss:0.024, val_acc:0.987]
Epoch [62/120    avg_loss:0.019, val_acc:0.987]
Epoch [63/120    avg_loss:0.022, val_acc:0.988]
Epoch [64/120    avg_loss:0.017, val_acc:0.987]
Epoch [65/120    avg_loss:0.021, val_acc:0.970]
Epoch [66/120    avg_loss:0.020, val_acc:0.986]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.018, val_acc:0.987]
Epoch [69/120    avg_loss:0.014, val_acc:0.986]
Epoch [70/120    avg_loss:0.015, val_acc:0.989]
Epoch [71/120    avg_loss:0.019, val_acc:0.987]
Epoch [72/120    avg_loss:0.023, val_acc:0.977]
Epoch [73/120    avg_loss:0.015, val_acc:0.988]
Epoch [74/120    avg_loss:0.025, val_acc:0.966]
Epoch [75/120    avg_loss:0.021, val_acc:0.980]
Epoch [76/120    avg_loss:0.019, val_acc:0.986]
Epoch [77/120    avg_loss:0.013, val_acc:0.984]
Epoch [78/120    avg_loss:0.015, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.185, val_acc:0.924]
Epoch [81/120    avg_loss:0.157, val_acc:0.965]
Epoch [82/120    avg_loss:0.085, val_acc:0.961]
Epoch [83/120    avg_loss:0.076, val_acc:0.857]
Epoch [84/120    avg_loss:0.078, val_acc:0.972]
Epoch [85/120    avg_loss:0.037, val_acc:0.973]
Epoch [86/120    avg_loss:0.035, val_acc:0.977]
Epoch [87/120    avg_loss:0.033, val_acc:0.977]
Epoch [88/120    avg_loss:0.033, val_acc:0.979]
Epoch [89/120    avg_loss:0.031, val_acc:0.979]
Epoch [90/120    avg_loss:0.027, val_acc:0.979]
Epoch [91/120    avg_loss:0.030, val_acc:0.978]
Epoch [92/120    avg_loss:0.034, val_acc:0.979]
Epoch [93/120    avg_loss:0.030, val_acc:0.980]
Epoch [94/120    avg_loss:0.026, val_acc:0.982]
Epoch [95/120    avg_loss:0.028, val_acc:0.983]
Epoch [96/120    avg_loss:0.028, val_acc:0.983]
Epoch [97/120    avg_loss:0.024, val_acc:0.983]
Epoch [98/120    avg_loss:0.023, val_acc:0.983]
Epoch [99/120    avg_loss:0.021, val_acc:0.983]
Epoch [100/120    avg_loss:0.026, val_acc:0.984]
Epoch [101/120    avg_loss:0.019, val_acc:0.983]
Epoch [102/120    avg_loss:0.024, val_acc:0.983]
Epoch [103/120    avg_loss:0.020, val_acc:0.983]
Epoch [104/120    avg_loss:0.022, val_acc:0.983]
Epoch [105/120    avg_loss:0.024, val_acc:0.983]
Epoch [106/120    avg_loss:0.024, val_acc:0.983]
Epoch [107/120    avg_loss:0.025, val_acc:0.983]
Epoch [108/120    avg_loss:0.024, val_acc:0.983]
Epoch [109/120    avg_loss:0.027, val_acc:0.984]
Epoch [110/120    avg_loss:0.025, val_acc:0.984]
Epoch [111/120    avg_loss:0.021, val_acc:0.984]
Epoch [112/120    avg_loss:0.022, val_acc:0.984]
Epoch [113/120    avg_loss:0.024, val_acc:0.984]
Epoch [114/120    avg_loss:0.025, val_acc:0.984]
Epoch [115/120    avg_loss:0.025, val_acc:0.984]
Epoch [116/120    avg_loss:0.024, val_acc:0.984]
Epoch [117/120    avg_loss:0.024, val_acc:0.984]
Epoch [118/120    avg_loss:0.027, val_acc:0.984]
Epoch [119/120    avg_loss:0.025, val_acc:0.984]
Epoch [120/120    avg_loss:0.022, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6226     0    64     9     0     0     6    69    58]
 [    0     0 17975     0    70     0    45     0     0     0]
 [    0     0     0  1980     1     0     0     0    54     1]
 [    0    52    19     0  2862     0    13     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4863     0     0     0]
 [    0     0     0     0     0     0     0  1284     0     6]
 [    0    33     0    31    87     0     0     0  3416     4]
 [    0     0     0     1    17    55     0     0     0   846]]

Accuracy:
98.22620683006772

F1 scores:
[       nan 0.97716393 0.99587246 0.96303502 0.95114656 0.9793621
 0.99255026 0.99534884 0.9573991  0.92257361]

Kappa:
0.9765266240887377
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3d765ca908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.086, val_acc:0.118]
Epoch [2/120    avg_loss:1.752, val_acc:0.308]
Epoch [3/120    avg_loss:1.539, val_acc:0.328]
Epoch [4/120    avg_loss:1.404, val_acc:0.408]
Epoch [5/120    avg_loss:1.223, val_acc:0.429]
Epoch [6/120    avg_loss:1.115, val_acc:0.478]
Epoch [7/120    avg_loss:0.970, val_acc:0.477]
Epoch [8/120    avg_loss:0.881, val_acc:0.550]
Epoch [9/120    avg_loss:0.793, val_acc:0.573]
Epoch [10/120    avg_loss:0.683, val_acc:0.664]
Epoch [11/120    avg_loss:0.583, val_acc:0.651]
Epoch [12/120    avg_loss:0.497, val_acc:0.703]
Epoch [13/120    avg_loss:0.478, val_acc:0.695]
Epoch [14/120    avg_loss:0.512, val_acc:0.744]
Epoch [15/120    avg_loss:0.405, val_acc:0.758]
Epoch [16/120    avg_loss:0.417, val_acc:0.784]
Epoch [17/120    avg_loss:0.354, val_acc:0.782]
Epoch [18/120    avg_loss:0.311, val_acc:0.790]
Epoch [19/120    avg_loss:0.280, val_acc:0.860]
Epoch [20/120    avg_loss:0.232, val_acc:0.858]
Epoch [21/120    avg_loss:0.257, val_acc:0.921]
Epoch [22/120    avg_loss:0.222, val_acc:0.898]
Epoch [23/120    avg_loss:0.201, val_acc:0.928]
Epoch [24/120    avg_loss:0.197, val_acc:0.874]
Epoch [25/120    avg_loss:0.888, val_acc:0.605]
Epoch [26/120    avg_loss:0.665, val_acc:0.687]
Epoch [27/120    avg_loss:0.577, val_acc:0.826]
Epoch [28/120    avg_loss:0.491, val_acc:0.828]
Epoch [29/120    avg_loss:0.390, val_acc:0.878]
Epoch [30/120    avg_loss:0.354, val_acc:0.892]
Epoch [31/120    avg_loss:0.321, val_acc:0.843]
Epoch [32/120    avg_loss:0.285, val_acc:0.925]
Epoch [33/120    avg_loss:0.254, val_acc:0.917]
Epoch [34/120    avg_loss:0.217, val_acc:0.913]
Epoch [35/120    avg_loss:0.242, val_acc:0.910]
Epoch [36/120    avg_loss:0.237, val_acc:0.936]
Epoch [37/120    avg_loss:0.239, val_acc:0.880]
Epoch [38/120    avg_loss:0.247, val_acc:0.936]
Epoch [39/120    avg_loss:0.203, val_acc:0.931]
Epoch [40/120    avg_loss:0.212, val_acc:0.926]
Epoch [41/120    avg_loss:0.204, val_acc:0.905]
Epoch [42/120    avg_loss:0.162, val_acc:0.934]
Epoch [43/120    avg_loss:0.156, val_acc:0.922]
Epoch [44/120    avg_loss:0.130, val_acc:0.940]
Epoch [45/120    avg_loss:0.123, val_acc:0.953]
Epoch [46/120    avg_loss:0.135, val_acc:0.922]
Epoch [47/120    avg_loss:0.112, val_acc:0.962]
Epoch [48/120    avg_loss:0.085, val_acc:0.947]
Epoch [49/120    avg_loss:0.099, val_acc:0.944]
Epoch [50/120    avg_loss:0.118, val_acc:0.932]
Epoch [51/120    avg_loss:0.092, val_acc:0.963]
Epoch [52/120    avg_loss:0.082, val_acc:0.961]
Epoch [53/120    avg_loss:0.117, val_acc:0.911]
Epoch [54/120    avg_loss:0.164, val_acc:0.947]
Epoch [55/120    avg_loss:0.111, val_acc:0.946]
Epoch [56/120    avg_loss:0.121, val_acc:0.956]
Epoch [57/120    avg_loss:0.087, val_acc:0.941]
Epoch [58/120    avg_loss:0.079, val_acc:0.956]
Epoch [59/120    avg_loss:0.066, val_acc:0.966]
Epoch [60/120    avg_loss:0.053, val_acc:0.963]
Epoch [61/120    avg_loss:0.068, val_acc:0.954]
Epoch [62/120    avg_loss:0.061, val_acc:0.947]
Epoch [63/120    avg_loss:0.062, val_acc:0.966]
Epoch [64/120    avg_loss:0.055, val_acc:0.964]
Epoch [65/120    avg_loss:0.044, val_acc:0.978]
Epoch [66/120    avg_loss:0.034, val_acc:0.973]
Epoch [67/120    avg_loss:0.040, val_acc:0.972]
Epoch [68/120    avg_loss:0.042, val_acc:0.958]
Epoch [69/120    avg_loss:0.039, val_acc:0.970]
Epoch [70/120    avg_loss:0.040, val_acc:0.975]
Epoch [71/120    avg_loss:0.031, val_acc:0.972]
Epoch [72/120    avg_loss:0.039, val_acc:0.981]
Epoch [73/120    avg_loss:0.038, val_acc:0.973]
Epoch [74/120    avg_loss:0.029, val_acc:0.978]
Epoch [75/120    avg_loss:0.026, val_acc:0.983]
Epoch [76/120    avg_loss:0.018, val_acc:0.983]
Epoch [77/120    avg_loss:0.021, val_acc:0.981]
Epoch [78/120    avg_loss:0.020, val_acc:0.982]
Epoch [79/120    avg_loss:0.015, val_acc:0.981]
Epoch [80/120    avg_loss:0.024, val_acc:0.974]
Epoch [81/120    avg_loss:0.031, val_acc:0.976]
Epoch [82/120    avg_loss:0.039, val_acc:0.971]
Epoch [83/120    avg_loss:0.028, val_acc:0.975]
Epoch [84/120    avg_loss:0.044, val_acc:0.968]
Epoch [85/120    avg_loss:0.056, val_acc:0.974]
Epoch [86/120    avg_loss:0.038, val_acc:0.976]
Epoch [87/120    avg_loss:0.028, val_acc:0.978]
Epoch [88/120    avg_loss:0.018, val_acc:0.982]
Epoch [89/120    avg_loss:0.016, val_acc:0.975]
Epoch [90/120    avg_loss:0.014, val_acc:0.978]
Epoch [91/120    avg_loss:0.016, val_acc:0.981]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.013, val_acc:0.984]
Epoch [94/120    avg_loss:0.012, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.984]
Epoch [96/120    avg_loss:0.014, val_acc:0.982]
Epoch [97/120    avg_loss:0.010, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.011, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.012, val_acc:0.982]
Epoch [105/120    avg_loss:0.013, val_acc:0.984]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.014, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.013, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.012, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0     4     0    28     0]
 [    0    11 17993     0    65     0    21     0     0     0]
 [    0     7     0  2019     0     0     0     0     9     1]
 [    0    30    21     1  2880     0    12     0    22     6]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     2  1282     0     6]
 [    0     5     0     0    52     0     0     0  3487    27]
 [    0     0     0     7    19    54     0     0     0   839]]

Accuracy:
99.00706143204879

F1 scores:
[       nan 0.99340318 0.99673166 0.99384691 0.96192385 0.97896319
 0.99601838 0.99688958 0.97990726 0.93222222]

Kappa:
0.9868531203574099
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9824f79860>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.087, val_acc:0.074]
Epoch [2/120    avg_loss:1.737, val_acc:0.206]
Epoch [3/120    avg_loss:1.567, val_acc:0.282]
Epoch [4/120    avg_loss:1.394, val_acc:0.325]
Epoch [5/120    avg_loss:1.291, val_acc:0.372]
Epoch [6/120    avg_loss:1.182, val_acc:0.412]
Epoch [7/120    avg_loss:1.111, val_acc:0.428]
Epoch [8/120    avg_loss:1.000, val_acc:0.506]
Epoch [9/120    avg_loss:0.922, val_acc:0.535]
Epoch [10/120    avg_loss:0.809, val_acc:0.626]
Epoch [11/120    avg_loss:0.725, val_acc:0.628]
Epoch [12/120    avg_loss:0.634, val_acc:0.636]
Epoch [13/120    avg_loss:0.584, val_acc:0.676]
Epoch [14/120    avg_loss:0.523, val_acc:0.733]
Epoch [15/120    avg_loss:0.479, val_acc:0.757]
Epoch [16/120    avg_loss:0.405, val_acc:0.762]
Epoch [17/120    avg_loss:0.400, val_acc:0.787]
Epoch [18/120    avg_loss:0.381, val_acc:0.819]
Epoch [19/120    avg_loss:0.385, val_acc:0.795]
Epoch [20/120    avg_loss:0.307, val_acc:0.853]
Epoch [21/120    avg_loss:0.324, val_acc:0.896]
Epoch [22/120    avg_loss:0.256, val_acc:0.912]
Epoch [23/120    avg_loss:0.345, val_acc:0.466]
Epoch [24/120    avg_loss:1.276, val_acc:0.497]
Epoch [25/120    avg_loss:1.212, val_acc:0.520]
Epoch [26/120    avg_loss:1.131, val_acc:0.409]
Epoch [27/120    avg_loss:1.028, val_acc:0.516]
Epoch [28/120    avg_loss:1.021, val_acc:0.592]
Epoch [29/120    avg_loss:1.020, val_acc:0.609]
Epoch [30/120    avg_loss:0.989, val_acc:0.597]
Epoch [31/120    avg_loss:0.920, val_acc:0.609]
Epoch [32/120    avg_loss:0.916, val_acc:0.668]
Epoch [33/120    avg_loss:0.936, val_acc:0.544]
Epoch [34/120    avg_loss:0.852, val_acc:0.674]
Epoch [35/120    avg_loss:0.841, val_acc:0.665]
Epoch [36/120    avg_loss:0.746, val_acc:0.663]
Epoch [37/120    avg_loss:0.727, val_acc:0.685]
Epoch [38/120    avg_loss:0.719, val_acc:0.688]
Epoch [39/120    avg_loss:0.755, val_acc:0.692]
Epoch [40/120    avg_loss:0.720, val_acc:0.696]
Epoch [41/120    avg_loss:0.709, val_acc:0.691]
Epoch [42/120    avg_loss:0.691, val_acc:0.688]
Epoch [43/120    avg_loss:0.734, val_acc:0.692]
Epoch [44/120    avg_loss:0.687, val_acc:0.691]
Epoch [45/120    avg_loss:0.682, val_acc:0.697]
Epoch [46/120    avg_loss:0.703, val_acc:0.691]
Epoch [47/120    avg_loss:0.691, val_acc:0.693]
Epoch [48/120    avg_loss:0.674, val_acc:0.693]
Epoch [49/120    avg_loss:0.675, val_acc:0.697]
Epoch [50/120    avg_loss:0.691, val_acc:0.701]
Epoch [51/120    avg_loss:0.682, val_acc:0.695]
Epoch [52/120    avg_loss:0.671, val_acc:0.694]
Epoch [53/120    avg_loss:0.677, val_acc:0.694]
Epoch [54/120    avg_loss:0.682, val_acc:0.696]
Epoch [55/120    avg_loss:0.708, val_acc:0.693]
Epoch [56/120    avg_loss:0.667, val_acc:0.694]
Epoch [57/120    avg_loss:0.689, val_acc:0.694]
Epoch [58/120    avg_loss:0.672, val_acc:0.696]
Epoch [59/120    avg_loss:0.668, val_acc:0.695]
Epoch [60/120    avg_loss:0.702, val_acc:0.701]
Epoch [61/120    avg_loss:0.686, val_acc:0.695]
Epoch [62/120    avg_loss:0.681, val_acc:0.695]
Epoch [63/120    avg_loss:0.676, val_acc:0.695]
Epoch [64/120    avg_loss:0.693, val_acc:0.695]
Epoch [65/120    avg_loss:0.675, val_acc:0.694]
Epoch [66/120    avg_loss:0.685, val_acc:0.694]
Epoch [67/120    avg_loss:0.682, val_acc:0.694]
Epoch [68/120    avg_loss:0.670, val_acc:0.694]
Epoch [69/120    avg_loss:0.692, val_acc:0.694]
Epoch [70/120    avg_loss:0.661, val_acc:0.693]
Epoch [71/120    avg_loss:0.693, val_acc:0.693]
Epoch [72/120    avg_loss:0.666, val_acc:0.694]
Epoch [73/120    avg_loss:0.684, val_acc:0.694]
Epoch [74/120    avg_loss:0.681, val_acc:0.695]
Epoch [75/120    avg_loss:0.685, val_acc:0.695]
Epoch [76/120    avg_loss:0.679, val_acc:0.695]
Epoch [77/120    avg_loss:0.664, val_acc:0.695]
Epoch [78/120    avg_loss:0.686, val_acc:0.695]
Epoch [79/120    avg_loss:0.691, val_acc:0.695]
Epoch [80/120    avg_loss:0.696, val_acc:0.695]
Epoch [81/120    avg_loss:0.679, val_acc:0.695]
Epoch [82/120    avg_loss:0.671, val_acc:0.695]
Epoch [83/120    avg_loss:0.684, val_acc:0.695]
Epoch [84/120    avg_loss:0.650, val_acc:0.695]
Epoch [85/120    avg_loss:0.696, val_acc:0.695]
Epoch [86/120    avg_loss:0.670, val_acc:0.695]
Epoch [87/120    avg_loss:0.673, val_acc:0.695]
Epoch [88/120    avg_loss:0.681, val_acc:0.695]
Epoch [89/120    avg_loss:0.684, val_acc:0.695]
Epoch [90/120    avg_loss:0.686, val_acc:0.695]
Epoch [91/120    avg_loss:0.675, val_acc:0.695]
Epoch [92/120    avg_loss:0.683, val_acc:0.695]
Epoch [93/120    avg_loss:0.670, val_acc:0.695]
Epoch [94/120    avg_loss:0.701, val_acc:0.695]
Epoch [95/120    avg_loss:0.686, val_acc:0.695]
Epoch [96/120    avg_loss:0.673, val_acc:0.695]
Epoch [97/120    avg_loss:0.666, val_acc:0.695]
Epoch [98/120    avg_loss:0.688, val_acc:0.695]
Epoch [99/120    avg_loss:0.666, val_acc:0.695]
Epoch [100/120    avg_loss:0.673, val_acc:0.695]
Epoch [101/120    avg_loss:0.670, val_acc:0.695]
Epoch [102/120    avg_loss:0.698, val_acc:0.695]
Epoch [103/120    avg_loss:0.685, val_acc:0.695]
Epoch [104/120    avg_loss:0.677, val_acc:0.695]
Epoch [105/120    avg_loss:0.680, val_acc:0.695]
Epoch [106/120    avg_loss:0.669, val_acc:0.695]
Epoch [107/120    avg_loss:0.668, val_acc:0.695]
Epoch [108/120    avg_loss:0.684, val_acc:0.695]
Epoch [109/120    avg_loss:0.666, val_acc:0.695]
Epoch [110/120    avg_loss:0.675, val_acc:0.695]
Epoch [111/120    avg_loss:0.674, val_acc:0.695]
Epoch [112/120    avg_loss:0.683, val_acc:0.695]
Epoch [113/120    avg_loss:0.689, val_acc:0.695]
Epoch [114/120    avg_loss:0.686, val_acc:0.695]
Epoch [115/120    avg_loss:0.682, val_acc:0.695]
Epoch [116/120    avg_loss:0.678, val_acc:0.695]
Epoch [117/120    avg_loss:0.676, val_acc:0.695]
Epoch [118/120    avg_loss:0.702, val_acc:0.695]
Epoch [119/120    avg_loss:0.676, val_acc:0.695]
Epoch [120/120    avg_loss:0.669, val_acc:0.695]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4499    60   178   404     0   400   124   597   170]
 [    0   693 11296     0   978     0  4343     0   780     0]
 [    0    12     0  1755     0     0    16     0   161    92]
 [    0   149    54     0  2466     0   227     0    57    19]
 [    0     0     0     0     0  1300     0     5     0     0]
 [    0     0   162     0   229     0  4461     0    26     0]
 [    0    34     0     0    20     0     9  1197     0    30]
 [    0   330    69   149    96     0   138     0  2786     3]
 [    0    30     0    13    15   138    12     0     6   705]]

Accuracy:
73.42202299182995

F1 scores:
[       nan 0.73881271 0.75988026 0.8496732  0.68690808 0.9478673
 0.61599006 0.91513761 0.69789579 0.72755418]

Kappa:
0.6708512162361896
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f405cf6d978>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.118, val_acc:0.073]
Epoch [2/120    avg_loss:1.765, val_acc:0.124]
Epoch [3/120    avg_loss:1.532, val_acc:0.276]
Epoch [4/120    avg_loss:1.353, val_acc:0.379]
Epoch [5/120    avg_loss:1.209, val_acc:0.426]
Epoch [6/120    avg_loss:1.110, val_acc:0.485]
Epoch [7/120    avg_loss:0.982, val_acc:0.448]
Epoch [8/120    avg_loss:0.900, val_acc:0.549]
Epoch [9/120    avg_loss:0.814, val_acc:0.706]
Epoch [10/120    avg_loss:0.702, val_acc:0.727]
Epoch [11/120    avg_loss:0.635, val_acc:0.808]
Epoch [12/120    avg_loss:0.584, val_acc:0.866]
Epoch [13/120    avg_loss:0.455, val_acc:0.859]
Epoch [14/120    avg_loss:0.395, val_acc:0.878]
Epoch [15/120    avg_loss:0.368, val_acc:0.911]
Epoch [16/120    avg_loss:0.318, val_acc:0.910]
Epoch [17/120    avg_loss:0.275, val_acc:0.889]
Epoch [18/120    avg_loss:0.322, val_acc:0.884]
Epoch [19/120    avg_loss:0.291, val_acc:0.926]
Epoch [20/120    avg_loss:0.237, val_acc:0.938]
Epoch [21/120    avg_loss:0.199, val_acc:0.896]
Epoch [22/120    avg_loss:0.203, val_acc:0.936]
Epoch [23/120    avg_loss:0.148, val_acc:0.953]
Epoch [24/120    avg_loss:0.138, val_acc:0.943]
Epoch [25/120    avg_loss:0.157, val_acc:0.916]
Epoch [26/120    avg_loss:0.196, val_acc:0.933]
Epoch [27/120    avg_loss:0.140, val_acc:0.947]
Epoch [28/120    avg_loss:0.263, val_acc:0.870]
Epoch [29/120    avg_loss:0.309, val_acc:0.839]
Epoch [30/120    avg_loss:0.257, val_acc:0.939]
Epoch [31/120    avg_loss:0.143, val_acc:0.953]
Epoch [32/120    avg_loss:0.109, val_acc:0.959]
Epoch [33/120    avg_loss:0.104, val_acc:0.952]
Epoch [34/120    avg_loss:0.099, val_acc:0.956]
Epoch [35/120    avg_loss:0.104, val_acc:0.947]
Epoch [36/120    avg_loss:0.101, val_acc:0.945]
Epoch [37/120    avg_loss:0.093, val_acc:0.944]
Epoch [38/120    avg_loss:0.091, val_acc:0.921]
Epoch [39/120    avg_loss:0.086, val_acc:0.966]
Epoch [40/120    avg_loss:0.075, val_acc:0.975]
Epoch [41/120    avg_loss:0.069, val_acc:0.963]
Epoch [42/120    avg_loss:0.072, val_acc:0.968]
Epoch [43/120    avg_loss:0.061, val_acc:0.972]
Epoch [44/120    avg_loss:0.046, val_acc:0.972]
Epoch [45/120    avg_loss:0.056, val_acc:0.955]
Epoch [46/120    avg_loss:0.055, val_acc:0.948]
Epoch [47/120    avg_loss:0.067, val_acc:0.971]
Epoch [48/120    avg_loss:0.057, val_acc:0.977]
Epoch [49/120    avg_loss:0.047, val_acc:0.975]
Epoch [50/120    avg_loss:0.037, val_acc:0.977]
Epoch [51/120    avg_loss:0.052, val_acc:0.973]
Epoch [52/120    avg_loss:0.041, val_acc:0.979]
Epoch [53/120    avg_loss:0.047, val_acc:0.962]
Epoch [54/120    avg_loss:0.041, val_acc:0.976]
Epoch [55/120    avg_loss:0.033, val_acc:0.975]
Epoch [56/120    avg_loss:0.036, val_acc:0.975]
Epoch [57/120    avg_loss:0.030, val_acc:0.978]
Epoch [58/120    avg_loss:0.042, val_acc:0.982]
Epoch [59/120    avg_loss:0.034, val_acc:0.981]
Epoch [60/120    avg_loss:0.036, val_acc:0.973]
Epoch [61/120    avg_loss:0.041, val_acc:0.978]
Epoch [62/120    avg_loss:0.027, val_acc:0.981]
Epoch [63/120    avg_loss:0.039, val_acc:0.973]
Epoch [64/120    avg_loss:0.046, val_acc:0.976]
Epoch [65/120    avg_loss:0.038, val_acc:0.976]
Epoch [66/120    avg_loss:0.031, val_acc:0.977]
Epoch [67/120    avg_loss:0.029, val_acc:0.984]
Epoch [68/120    avg_loss:0.029, val_acc:0.982]
Epoch [69/120    avg_loss:0.022, val_acc:0.981]
Epoch [70/120    avg_loss:0.017, val_acc:0.983]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.021, val_acc:0.979]
Epoch [73/120    avg_loss:0.017, val_acc:0.982]
Epoch [74/120    avg_loss:0.019, val_acc:0.986]
Epoch [75/120    avg_loss:0.015, val_acc:0.989]
Epoch [76/120    avg_loss:0.020, val_acc:0.987]
Epoch [77/120    avg_loss:0.018, val_acc:0.985]
Epoch [78/120    avg_loss:0.013, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.991]
Epoch [81/120    avg_loss:0.012, val_acc:0.985]
Epoch [82/120    avg_loss:0.014, val_acc:0.985]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.012, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.985]
Epoch [87/120    avg_loss:0.013, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.989]
Epoch [89/120    avg_loss:0.010, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.011, val_acc:0.991]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.012, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.979]
Epoch [96/120    avg_loss:0.019, val_acc:0.978]
Epoch [97/120    avg_loss:0.024, val_acc:0.964]
Epoch [98/120    avg_loss:0.022, val_acc:0.984]
Epoch [99/120    avg_loss:0.015, val_acc:0.988]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.015, val_acc:0.949]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.013, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     3     0    11     0    37     4]
 [    0     0 18050     0    23     0    16     0     1     0]
 [    0     0     0  2023     4     0     0     0     8     1]
 [    0    50    20     0  2867     0    10     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    20     0     0  4858     0     0     0]
 [    0     0     0     0     0     0     1  1286     0     3]
 [    0    13     0     0    64     0     0     0  3485     9]
 [    0     0     0     2    14    42     0     0     0   861]]

Accuracy:
99.0817728291519

F1 scores:
[       nan 0.99083282 0.99834071 0.99142367 0.96418362 0.9841629
 0.99406589 0.9984472  0.9779711  0.95826377]

Kappa:
0.9878351532630827
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f062666b8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.095, val_acc:0.096]
Epoch [2/120    avg_loss:1.785, val_acc:0.234]
Epoch [3/120    avg_loss:1.594, val_acc:0.272]
Epoch [4/120    avg_loss:1.427, val_acc:0.337]
Epoch [5/120    avg_loss:1.294, val_acc:0.412]
Epoch [6/120    avg_loss:1.162, val_acc:0.453]
Epoch [7/120    avg_loss:1.020, val_acc:0.459]
Epoch [8/120    avg_loss:0.887, val_acc:0.630]
Epoch [9/120    avg_loss:0.801, val_acc:0.712]
Epoch [10/120    avg_loss:0.708, val_acc:0.748]
Epoch [11/120    avg_loss:0.584, val_acc:0.739]
Epoch [12/120    avg_loss:0.532, val_acc:0.786]
Epoch [13/120    avg_loss:0.484, val_acc:0.834]
Epoch [14/120    avg_loss:0.422, val_acc:0.860]
Epoch [15/120    avg_loss:0.387, val_acc:0.901]
Epoch [16/120    avg_loss:0.352, val_acc:0.911]
Epoch [17/120    avg_loss:0.268, val_acc:0.919]
Epoch [18/120    avg_loss:0.220, val_acc:0.922]
Epoch [19/120    avg_loss:0.234, val_acc:0.939]
Epoch [20/120    avg_loss:0.220, val_acc:0.920]
Epoch [21/120    avg_loss:0.167, val_acc:0.965]
Epoch [22/120    avg_loss:0.140, val_acc:0.956]
Epoch [23/120    avg_loss:0.145, val_acc:0.947]
Epoch [24/120    avg_loss:0.160, val_acc:0.946]
Epoch [25/120    avg_loss:0.157, val_acc:0.956]
Epoch [26/120    avg_loss:0.131, val_acc:0.917]
Epoch [27/120    avg_loss:0.116, val_acc:0.965]
Epoch [28/120    avg_loss:0.134, val_acc:0.965]
Epoch [29/120    avg_loss:0.097, val_acc:0.974]
Epoch [30/120    avg_loss:0.075, val_acc:0.971]
Epoch [31/120    avg_loss:0.080, val_acc:0.964]
Epoch [32/120    avg_loss:0.068, val_acc:0.963]
Epoch [33/120    avg_loss:0.071, val_acc:0.961]
Epoch [34/120    avg_loss:0.061, val_acc:0.959]
Epoch [35/120    avg_loss:0.077, val_acc:0.949]
Epoch [36/120    avg_loss:0.079, val_acc:0.970]
Epoch [37/120    avg_loss:0.116, val_acc:0.941]
Epoch [38/120    avg_loss:0.111, val_acc:0.953]
Epoch [39/120    avg_loss:0.089, val_acc:0.960]
Epoch [40/120    avg_loss:0.076, val_acc:0.968]
Epoch [41/120    avg_loss:0.065, val_acc:0.968]
Epoch [42/120    avg_loss:0.048, val_acc:0.973]
Epoch [43/120    avg_loss:0.044, val_acc:0.975]
Epoch [44/120    avg_loss:0.034, val_acc:0.978]
Epoch [45/120    avg_loss:0.033, val_acc:0.978]
Epoch [46/120    avg_loss:0.035, val_acc:0.978]
Epoch [47/120    avg_loss:0.034, val_acc:0.980]
Epoch [48/120    avg_loss:0.030, val_acc:0.980]
Epoch [49/120    avg_loss:0.031, val_acc:0.980]
Epoch [50/120    avg_loss:0.030, val_acc:0.981]
Epoch [51/120    avg_loss:0.029, val_acc:0.978]
Epoch [52/120    avg_loss:0.030, val_acc:0.981]
Epoch [53/120    avg_loss:0.033, val_acc:0.982]
Epoch [54/120    avg_loss:0.025, val_acc:0.982]
Epoch [55/120    avg_loss:0.024, val_acc:0.981]
Epoch [56/120    avg_loss:0.024, val_acc:0.982]
Epoch [57/120    avg_loss:0.025, val_acc:0.981]
Epoch [58/120    avg_loss:0.026, val_acc:0.978]
Epoch [59/120    avg_loss:0.035, val_acc:0.978]
Epoch [60/120    avg_loss:0.025, val_acc:0.978]
Epoch [61/120    avg_loss:0.031, val_acc:0.981]
Epoch [62/120    avg_loss:0.024, val_acc:0.982]
Epoch [63/120    avg_loss:0.025, val_acc:0.981]
Epoch [64/120    avg_loss:0.025, val_acc:0.979]
Epoch [65/120    avg_loss:0.023, val_acc:0.980]
Epoch [66/120    avg_loss:0.026, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.981]
Epoch [68/120    avg_loss:0.024, val_acc:0.981]
Epoch [69/120    avg_loss:0.024, val_acc:0.983]
Epoch [70/120    avg_loss:0.025, val_acc:0.981]
Epoch [71/120    avg_loss:0.026, val_acc:0.982]
Epoch [72/120    avg_loss:0.031, val_acc:0.980]
Epoch [73/120    avg_loss:0.023, val_acc:0.981]
Epoch [74/120    avg_loss:0.022, val_acc:0.979]
Epoch [75/120    avg_loss:0.025, val_acc:0.983]
Epoch [76/120    avg_loss:0.025, val_acc:0.982]
Epoch [77/120    avg_loss:0.020, val_acc:0.982]
Epoch [78/120    avg_loss:0.020, val_acc:0.981]
Epoch [79/120    avg_loss:0.024, val_acc:0.980]
Epoch [80/120    avg_loss:0.025, val_acc:0.982]
Epoch [81/120    avg_loss:0.023, val_acc:0.981]
Epoch [82/120    avg_loss:0.021, val_acc:0.981]
Epoch [83/120    avg_loss:0.021, val_acc:0.981]
Epoch [84/120    avg_loss:0.022, val_acc:0.981]
Epoch [85/120    avg_loss:0.027, val_acc:0.983]
Epoch [86/120    avg_loss:0.023, val_acc:0.981]
Epoch [87/120    avg_loss:0.023, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.980]
Epoch [89/120    avg_loss:0.020, val_acc:0.980]
Epoch [90/120    avg_loss:0.026, val_acc:0.979]
Epoch [91/120    avg_loss:0.023, val_acc:0.980]
Epoch [92/120    avg_loss:0.021, val_acc:0.980]
Epoch [93/120    avg_loss:0.021, val_acc:0.981]
Epoch [94/120    avg_loss:0.020, val_acc:0.981]
Epoch [95/120    avg_loss:0.022, val_acc:0.978]
Epoch [96/120    avg_loss:0.019, val_acc:0.979]
Epoch [97/120    avg_loss:0.021, val_acc:0.980]
Epoch [98/120    avg_loss:0.022, val_acc:0.979]
Epoch [99/120    avg_loss:0.017, val_acc:0.979]
Epoch [100/120    avg_loss:0.019, val_acc:0.979]
Epoch [101/120    avg_loss:0.019, val_acc:0.980]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.018, val_acc:0.979]
Epoch [104/120    avg_loss:0.021, val_acc:0.979]
Epoch [105/120    avg_loss:0.017, val_acc:0.979]
Epoch [106/120    avg_loss:0.019, val_acc:0.979]
Epoch [107/120    avg_loss:0.019, val_acc:0.979]
Epoch [108/120    avg_loss:0.021, val_acc:0.979]
Epoch [109/120    avg_loss:0.018, val_acc:0.979]
Epoch [110/120    avg_loss:0.019, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.018, val_acc:0.979]
Epoch [113/120    avg_loss:0.016, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.019, val_acc:0.979]
Epoch [116/120    avg_loss:0.024, val_acc:0.979]
Epoch [117/120    avg_loss:0.024, val_acc:0.979]
Epoch [118/120    avg_loss:0.018, val_acc:0.979]
Epoch [119/120    avg_loss:0.019, val_acc:0.979]
Epoch [120/120    avg_loss:0.019, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     0    12     0     0     0    40     5]
 [    0     5 18015     0    60     0    10     0     0     0]
 [    0     0     0  2022     0     0     0     0    12     2]
 [    0    43    20     0  2861     0    13     0    35     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     3     0     0  4863     0     5     7]
 [    0     0     0     0     0     0     0  1286     0     4]
 [    0     2     0     1    41     0     0     0  3527     0]
 [    0     0     0     1    19    71     0     3     0   825]]

Accuracy:
99.00224134191309

F1 scores:
[       nan 0.99167769 0.99737024 0.99532365 0.95926236 0.97351734
 0.99610815 0.99728577 0.98108484 0.93643587]

Kappa:
0.9867868637717983
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb402bd5898>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.046, val_acc:0.369]
Epoch [2/120    avg_loss:1.703, val_acc:0.233]
Epoch [3/120    avg_loss:1.498, val_acc:0.321]
Epoch [4/120    avg_loss:1.333, val_acc:0.380]
Epoch [5/120    avg_loss:1.199, val_acc:0.437]
Epoch [6/120    avg_loss:1.082, val_acc:0.524]
Epoch [7/120    avg_loss:0.965, val_acc:0.643]
Epoch [8/120    avg_loss:0.859, val_acc:0.703]
Epoch [9/120    avg_loss:0.752, val_acc:0.716]
Epoch [10/120    avg_loss:0.689, val_acc:0.763]
Epoch [11/120    avg_loss:0.568, val_acc:0.772]
Epoch [12/120    avg_loss:0.496, val_acc:0.790]
Epoch [13/120    avg_loss:0.420, val_acc:0.802]
Epoch [14/120    avg_loss:0.380, val_acc:0.837]
Epoch [15/120    avg_loss:0.353, val_acc:0.860]
Epoch [16/120    avg_loss:0.390, val_acc:0.827]
Epoch [17/120    avg_loss:0.371, val_acc:0.872]
Epoch [18/120    avg_loss:0.261, val_acc:0.897]
Epoch [19/120    avg_loss:0.239, val_acc:0.915]
Epoch [20/120    avg_loss:0.192, val_acc:0.934]
Epoch [21/120    avg_loss:0.172, val_acc:0.880]
Epoch [22/120    avg_loss:0.189, val_acc:0.901]
Epoch [23/120    avg_loss:0.171, val_acc:0.944]
Epoch [24/120    avg_loss:0.139, val_acc:0.941]
Epoch [25/120    avg_loss:0.130, val_acc:0.942]
Epoch [26/120    avg_loss:0.113, val_acc:0.959]
Epoch [27/120    avg_loss:0.143, val_acc:0.947]
Epoch [28/120    avg_loss:0.137, val_acc:0.923]
Epoch [29/120    avg_loss:0.104, val_acc:0.957]
Epoch [30/120    avg_loss:0.094, val_acc:0.966]
Epoch [31/120    avg_loss:0.078, val_acc:0.958]
Epoch [32/120    avg_loss:0.072, val_acc:0.978]
Epoch [33/120    avg_loss:0.085, val_acc:0.920]
Epoch [34/120    avg_loss:0.088, val_acc:0.953]
Epoch [35/120    avg_loss:0.079, val_acc:0.977]
Epoch [36/120    avg_loss:0.066, val_acc:0.969]
Epoch [37/120    avg_loss:0.072, val_acc:0.973]
Epoch [38/120    avg_loss:0.053, val_acc:0.977]
Epoch [39/120    avg_loss:0.048, val_acc:0.975]
Epoch [40/120    avg_loss:0.053, val_acc:0.957]
Epoch [41/120    avg_loss:0.053, val_acc:0.979]
Epoch [42/120    avg_loss:0.071, val_acc:0.970]
Epoch [43/120    avg_loss:0.092, val_acc:0.953]
Epoch [44/120    avg_loss:0.078, val_acc:0.979]
Epoch [45/120    avg_loss:0.062, val_acc:0.977]
Epoch [46/120    avg_loss:0.038, val_acc:0.982]
Epoch [47/120    avg_loss:0.033, val_acc:0.985]
Epoch [48/120    avg_loss:0.031, val_acc:0.983]
Epoch [49/120    avg_loss:0.034, val_acc:0.981]
Epoch [50/120    avg_loss:0.040, val_acc:0.964]
Epoch [51/120    avg_loss:0.054, val_acc:0.963]
Epoch [52/120    avg_loss:0.051, val_acc:0.972]
Epoch [53/120    avg_loss:0.062, val_acc:0.934]
Epoch [54/120    avg_loss:0.051, val_acc:0.978]
Epoch [55/120    avg_loss:0.049, val_acc:0.979]
Epoch [56/120    avg_loss:0.030, val_acc:0.979]
Epoch [57/120    avg_loss:0.029, val_acc:0.972]
Epoch [58/120    avg_loss:0.022, val_acc:0.980]
Epoch [59/120    avg_loss:0.023, val_acc:0.987]
Epoch [60/120    avg_loss:0.033, val_acc:0.983]
Epoch [61/120    avg_loss:0.029, val_acc:0.963]
Epoch [62/120    avg_loss:0.027, val_acc:0.965]
Epoch [63/120    avg_loss:0.021, val_acc:0.972]
Epoch [64/120    avg_loss:0.016, val_acc:0.985]
Epoch [65/120    avg_loss:0.015, val_acc:0.988]
Epoch [66/120    avg_loss:0.016, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.987]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.012, val_acc:0.986]
Epoch [71/120    avg_loss:0.010, val_acc:0.988]
Epoch [72/120    avg_loss:0.016, val_acc:0.986]
Epoch [73/120    avg_loss:0.018, val_acc:0.988]
Epoch [74/120    avg_loss:0.019, val_acc:0.984]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.019, val_acc:0.976]
Epoch [77/120    avg_loss:0.019, val_acc:0.989]
Epoch [78/120    avg_loss:0.022, val_acc:0.968]
Epoch [79/120    avg_loss:0.018, val_acc:0.984]
Epoch [80/120    avg_loss:0.012, val_acc:0.991]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.010, val_acc:0.991]
Epoch [83/120    avg_loss:0.011, val_acc:0.989]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.017, val_acc:0.984]
Epoch [86/120    avg_loss:0.016, val_acc:0.985]
Epoch [87/120    avg_loss:0.013, val_acc:0.987]
Epoch [88/120    avg_loss:0.011, val_acc:0.991]
Epoch [89/120    avg_loss:0.011, val_acc:0.989]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.987]
Epoch [92/120    avg_loss:0.021, val_acc:0.975]
Epoch [93/120    avg_loss:0.016, val_acc:0.986]
Epoch [94/120    avg_loss:0.012, val_acc:0.987]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.991]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.991]
Epoch [101/120    avg_loss:0.011, val_acc:0.950]
Epoch [102/120    avg_loss:0.016, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.993]
Epoch [105/120    avg_loss:0.009, val_acc:0.993]
Epoch [106/120    avg_loss:0.006, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.993]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.991]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     4     0     0     0     0    25     0]
 [    0     0 18067     0    19     0     4     0     0     0]
 [    0    11     0  2020     0     0     0     0     3     2]
 [    0    44    20     0  2871     0     0     0    28     9]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4864     0     0    12]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     0     0     1    58     0     0     0  3489    23]
 [    0     0     0     5    13    56     0     0     0   845]]

Accuracy:
99.17576458679777

F1 scores:
[       nan 0.99348332 0.99875618 0.99360551 0.96780718 0.97899475
 0.99815309 0.99883586 0.98060708 0.93215665]

Kappa:
0.9890783085706332
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1af3b6978>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.105, val_acc:0.078]
Epoch [2/120    avg_loss:1.746, val_acc:0.154]
Epoch [3/120    avg_loss:1.566, val_acc:0.237]
Epoch [4/120    avg_loss:1.401, val_acc:0.361]
Epoch [5/120    avg_loss:1.288, val_acc:0.399]
Epoch [6/120    avg_loss:1.174, val_acc:0.424]
Epoch [7/120    avg_loss:1.089, val_acc:0.440]
Epoch [8/120    avg_loss:0.989, val_acc:0.501]
Epoch [9/120    avg_loss:0.888, val_acc:0.603]
Epoch [10/120    avg_loss:0.841, val_acc:0.526]
Epoch [11/120    avg_loss:0.720, val_acc:0.663]
Epoch [12/120    avg_loss:0.642, val_acc:0.689]
Epoch [13/120    avg_loss:0.571, val_acc:0.695]
Epoch [14/120    avg_loss:0.516, val_acc:0.762]
Epoch [15/120    avg_loss:0.417, val_acc:0.819]
Epoch [16/120    avg_loss:0.375, val_acc:0.815]
Epoch [17/120    avg_loss:0.364, val_acc:0.791]
Epoch [18/120    avg_loss:0.292, val_acc:0.886]
Epoch [19/120    avg_loss:0.306, val_acc:0.912]
Epoch [20/120    avg_loss:0.268, val_acc:0.908]
Epoch [21/120    avg_loss:0.286, val_acc:0.906]
Epoch [22/120    avg_loss:0.212, val_acc:0.929]
Epoch [23/120    avg_loss:0.170, val_acc:0.849]
Epoch [24/120    avg_loss:0.181, val_acc:0.908]
Epoch [25/120    avg_loss:0.155, val_acc:0.949]
Epoch [26/120    avg_loss:0.220, val_acc:0.945]
Epoch [27/120    avg_loss:0.176, val_acc:0.936]
Epoch [28/120    avg_loss:0.147, val_acc:0.890]
Epoch [29/120    avg_loss:0.126, val_acc:0.960]
Epoch [30/120    avg_loss:0.121, val_acc:0.946]
Epoch [31/120    avg_loss:0.103, val_acc:0.954]
Epoch [32/120    avg_loss:0.102, val_acc:0.967]
Epoch [33/120    avg_loss:0.072, val_acc:0.965]
Epoch [34/120    avg_loss:0.135, val_acc:0.957]
Epoch [35/120    avg_loss:0.095, val_acc:0.958]
Epoch [36/120    avg_loss:0.086, val_acc:0.961]
Epoch [37/120    avg_loss:0.060, val_acc:0.970]
Epoch [38/120    avg_loss:0.068, val_acc:0.967]
Epoch [39/120    avg_loss:0.058, val_acc:0.971]
Epoch [40/120    avg_loss:0.072, val_acc:0.962]
Epoch [41/120    avg_loss:0.058, val_acc:0.968]
Epoch [42/120    avg_loss:0.060, val_acc:0.973]
Epoch [43/120    avg_loss:0.047, val_acc:0.971]
Epoch [44/120    avg_loss:0.058, val_acc:0.971]
Epoch [45/120    avg_loss:0.045, val_acc:0.968]
Epoch [46/120    avg_loss:0.050, val_acc:0.961]
Epoch [47/120    avg_loss:0.055, val_acc:0.969]
Epoch [48/120    avg_loss:0.067, val_acc:0.970]
Epoch [49/120    avg_loss:0.056, val_acc:0.964]
Epoch [50/120    avg_loss:0.054, val_acc:0.966]
Epoch [51/120    avg_loss:0.037, val_acc:0.975]
Epoch [52/120    avg_loss:0.043, val_acc:0.973]
Epoch [53/120    avg_loss:0.032, val_acc:0.979]
Epoch [54/120    avg_loss:0.023, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.978]
Epoch [56/120    avg_loss:0.020, val_acc:0.978]
Epoch [57/120    avg_loss:0.036, val_acc:0.971]
Epoch [58/120    avg_loss:0.041, val_acc:0.959]
Epoch [59/120    avg_loss:0.038, val_acc:0.977]
Epoch [60/120    avg_loss:0.027, val_acc:0.978]
Epoch [61/120    avg_loss:0.020, val_acc:0.981]
Epoch [62/120    avg_loss:0.027, val_acc:0.968]
Epoch [63/120    avg_loss:0.036, val_acc:0.968]
Epoch [64/120    avg_loss:0.030, val_acc:0.974]
Epoch [65/120    avg_loss:0.026, val_acc:0.973]
Epoch [66/120    avg_loss:0.028, val_acc:0.977]
Epoch [67/120    avg_loss:0.019, val_acc:0.981]
Epoch [68/120    avg_loss:0.018, val_acc:0.976]
Epoch [69/120    avg_loss:0.020, val_acc:0.971]
Epoch [70/120    avg_loss:0.017, val_acc:0.979]
Epoch [71/120    avg_loss:0.016, val_acc:0.981]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.020, val_acc:0.970]
Epoch [74/120    avg_loss:0.018, val_acc:0.982]
Epoch [75/120    avg_loss:0.016, val_acc:0.973]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.010, val_acc:0.982]
Epoch [80/120    avg_loss:0.026, val_acc:0.974]
Epoch [81/120    avg_loss:0.025, val_acc:0.975]
Epoch [82/120    avg_loss:0.023, val_acc:0.972]
Epoch [83/120    avg_loss:0.017, val_acc:0.976]
Epoch [84/120    avg_loss:0.011, val_acc:0.980]
Epoch [85/120    avg_loss:0.011, val_acc:0.979]
Epoch [86/120    avg_loss:0.012, val_acc:0.978]
Epoch [87/120    avg_loss:0.104, val_acc:0.812]
Epoch [88/120    avg_loss:0.277, val_acc:0.898]
Epoch [89/120    avg_loss:0.164, val_acc:0.952]
Epoch [90/120    avg_loss:0.094, val_acc:0.963]
Epoch [91/120    avg_loss:0.061, val_acc:0.971]
Epoch [92/120    avg_loss:0.053, val_acc:0.975]
Epoch [93/120    avg_loss:0.040, val_acc:0.971]
Epoch [94/120    avg_loss:0.039, val_acc:0.971]
Epoch [95/120    avg_loss:0.032, val_acc:0.973]
Epoch [96/120    avg_loss:0.032, val_acc:0.974]
Epoch [97/120    avg_loss:0.038, val_acc:0.973]
Epoch [98/120    avg_loss:0.034, val_acc:0.974]
Epoch [99/120    avg_loss:0.035, val_acc:0.977]
Epoch [100/120    avg_loss:0.029, val_acc:0.972]
Epoch [101/120    avg_loss:0.030, val_acc:0.973]
Epoch [102/120    avg_loss:0.032, val_acc:0.976]
Epoch [103/120    avg_loss:0.035, val_acc:0.975]
Epoch [104/120    avg_loss:0.031, val_acc:0.975]
Epoch [105/120    avg_loss:0.031, val_acc:0.976]
Epoch [106/120    avg_loss:0.024, val_acc:0.976]
Epoch [107/120    avg_loss:0.034, val_acc:0.977]
Epoch [108/120    avg_loss:0.027, val_acc:0.976]
Epoch [109/120    avg_loss:0.036, val_acc:0.976]
Epoch [110/120    avg_loss:0.029, val_acc:0.976]
Epoch [111/120    avg_loss:0.034, val_acc:0.976]
Epoch [112/120    avg_loss:0.030, val_acc:0.976]
Epoch [113/120    avg_loss:0.025, val_acc:0.976]
Epoch [114/120    avg_loss:0.025, val_acc:0.976]
Epoch [115/120    avg_loss:0.029, val_acc:0.976]
Epoch [116/120    avg_loss:0.024, val_acc:0.976]
Epoch [117/120    avg_loss:0.028, val_acc:0.976]
Epoch [118/120    avg_loss:0.027, val_acc:0.977]
Epoch [119/120    avg_loss:0.027, val_acc:0.977]
Epoch [120/120    avg_loss:0.034, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6296     0     7    33     0    33     0    62     1]
 [    0     0 17941     0    87     0    62     0     0     0]
 [    0     4     0  1989     0     0     0     0    43     0]
 [    0    68    18     0  2848     0     7     0    29     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     0     2]
 [    0     6     0     0     0     0     3  1278     0     3]
 [    0    12     0     5    63     0     0     0  3461    30]
 [    0     0     0     1    21    48     0     0     1   848]]

Accuracy:
98.43106066083436

F1 scores:
[       nan 0.98236854 0.99536742 0.98514116 0.94555113 0.98194131
 0.98914697 0.9953271  0.96581554 0.93961219]

Kappa:
0.9792407800829617
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc89b619908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.056, val_acc:0.160]
Epoch [2/120    avg_loss:1.739, val_acc:0.222]
Epoch [3/120    avg_loss:1.559, val_acc:0.261]
Epoch [4/120    avg_loss:1.392, val_acc:0.297]
Epoch [5/120    avg_loss:1.272, val_acc:0.359]
Epoch [6/120    avg_loss:1.123, val_acc:0.462]
Epoch [7/120    avg_loss:1.007, val_acc:0.641]
Epoch [8/120    avg_loss:0.866, val_acc:0.693]
Epoch [9/120    avg_loss:0.812, val_acc:0.739]
Epoch [10/120    avg_loss:0.716, val_acc:0.721]
Epoch [11/120    avg_loss:0.597, val_acc:0.709]
Epoch [12/120    avg_loss:0.545, val_acc:0.767]
Epoch [13/120    avg_loss:0.498, val_acc:0.782]
Epoch [14/120    avg_loss:0.439, val_acc:0.834]
Epoch [15/120    avg_loss:0.357, val_acc:0.894]
Epoch [16/120    avg_loss:0.350, val_acc:0.852]
Epoch [17/120    avg_loss:0.285, val_acc:0.913]
Epoch [18/120    avg_loss:0.280, val_acc:0.893]
Epoch [19/120    avg_loss:0.283, val_acc:0.901]
Epoch [20/120    avg_loss:0.233, val_acc:0.927]
Epoch [21/120    avg_loss:0.207, val_acc:0.916]
Epoch [22/120    avg_loss:0.193, val_acc:0.909]
Epoch [23/120    avg_loss:0.271, val_acc:0.932]
Epoch [24/120    avg_loss:0.615, val_acc:0.165]
Epoch [25/120    avg_loss:1.571, val_acc:0.556]
Epoch [26/120    avg_loss:1.399, val_acc:0.609]
Epoch [27/120    avg_loss:1.288, val_acc:0.641]
Epoch [28/120    avg_loss:1.210, val_acc:0.496]
Epoch [29/120    avg_loss:1.126, val_acc:0.655]
Epoch [30/120    avg_loss:1.056, val_acc:0.597]
Epoch [31/120    avg_loss:1.006, val_acc:0.648]
Epoch [32/120    avg_loss:0.961, val_acc:0.751]
Epoch [33/120    avg_loss:0.928, val_acc:0.691]
Epoch [34/120    avg_loss:0.889, val_acc:0.753]
Epoch [35/120    avg_loss:0.829, val_acc:0.490]
Epoch [36/120    avg_loss:0.834, val_acc:0.785]
Epoch [37/120    avg_loss:0.823, val_acc:0.764]
Epoch [38/120    avg_loss:0.785, val_acc:0.778]
Epoch [39/120    avg_loss:0.804, val_acc:0.775]
Epoch [40/120    avg_loss:0.786, val_acc:0.766]
Epoch [41/120    avg_loss:0.784, val_acc:0.762]
Epoch [42/120    avg_loss:0.787, val_acc:0.771]
Epoch [43/120    avg_loss:0.761, val_acc:0.747]
Epoch [44/120    avg_loss:0.788, val_acc:0.767]
Epoch [45/120    avg_loss:0.756, val_acc:0.754]
Epoch [46/120    avg_loss:0.765, val_acc:0.772]
Epoch [47/120    avg_loss:0.770, val_acc:0.736]
Epoch [48/120    avg_loss:0.743, val_acc:0.784]
Epoch [49/120    avg_loss:0.748, val_acc:0.743]
Epoch [50/120    avg_loss:0.743, val_acc:0.753]
Epoch [51/120    avg_loss:0.770, val_acc:0.759]
Epoch [52/120    avg_loss:0.760, val_acc:0.762]
Epoch [53/120    avg_loss:0.756, val_acc:0.758]
Epoch [54/120    avg_loss:0.753, val_acc:0.759]
Epoch [55/120    avg_loss:0.750, val_acc:0.761]
Epoch [56/120    avg_loss:0.747, val_acc:0.762]
Epoch [57/120    avg_loss:0.744, val_acc:0.759]
Epoch [58/120    avg_loss:0.750, val_acc:0.762]
Epoch [59/120    avg_loss:0.735, val_acc:0.766]
Epoch [60/120    avg_loss:0.742, val_acc:0.766]
Epoch [61/120    avg_loss:0.746, val_acc:0.766]
Epoch [62/120    avg_loss:0.763, val_acc:0.766]
Epoch [63/120    avg_loss:0.756, val_acc:0.766]
Epoch [64/120    avg_loss:0.760, val_acc:0.766]
Epoch [65/120    avg_loss:0.737, val_acc:0.766]
Epoch [66/120    avg_loss:0.735, val_acc:0.766]
Epoch [67/120    avg_loss:0.770, val_acc:0.765]
Epoch [68/120    avg_loss:0.738, val_acc:0.764]
Epoch [69/120    avg_loss:0.769, val_acc:0.764]
Epoch [70/120    avg_loss:0.760, val_acc:0.764]
Epoch [71/120    avg_loss:0.740, val_acc:0.764]
Epoch [72/120    avg_loss:0.748, val_acc:0.763]
Epoch [73/120    avg_loss:0.776, val_acc:0.763]
Epoch [74/120    avg_loss:0.742, val_acc:0.762]
Epoch [75/120    avg_loss:0.724, val_acc:0.762]
Epoch [76/120    avg_loss:0.760, val_acc:0.762]
Epoch [77/120    avg_loss:0.741, val_acc:0.762]
Epoch [78/120    avg_loss:0.772, val_acc:0.762]
Epoch [79/120    avg_loss:0.750, val_acc:0.762]
Epoch [80/120    avg_loss:0.759, val_acc:0.762]
Epoch [81/120    avg_loss:0.757, val_acc:0.762]
Epoch [82/120    avg_loss:0.735, val_acc:0.763]
Epoch [83/120    avg_loss:0.745, val_acc:0.763]
Epoch [84/120    avg_loss:0.739, val_acc:0.763]
Epoch [85/120    avg_loss:0.741, val_acc:0.764]
Epoch [86/120    avg_loss:0.761, val_acc:0.764]
Epoch [87/120    avg_loss:0.763, val_acc:0.764]
Epoch [88/120    avg_loss:0.757, val_acc:0.763]
Epoch [89/120    avg_loss:0.751, val_acc:0.763]
Epoch [90/120    avg_loss:0.741, val_acc:0.763]
Epoch [91/120    avg_loss:0.761, val_acc:0.763]
Epoch [92/120    avg_loss:0.766, val_acc:0.763]
Epoch [93/120    avg_loss:0.764, val_acc:0.763]
Epoch [94/120    avg_loss:0.741, val_acc:0.763]
Epoch [95/120    avg_loss:0.743, val_acc:0.763]
Epoch [96/120    avg_loss:0.756, val_acc:0.763]
Epoch [97/120    avg_loss:0.759, val_acc:0.763]
Epoch [98/120    avg_loss:0.738, val_acc:0.763]
Epoch [99/120    avg_loss:0.754, val_acc:0.763]
Epoch [100/120    avg_loss:0.755, val_acc:0.763]
Epoch [101/120    avg_loss:0.752, val_acc:0.763]
Epoch [102/120    avg_loss:0.761, val_acc:0.763]
Epoch [103/120    avg_loss:0.759, val_acc:0.763]
Epoch [104/120    avg_loss:0.760, val_acc:0.763]
Epoch [105/120    avg_loss:0.772, val_acc:0.763]
Epoch [106/120    avg_loss:0.742, val_acc:0.763]
Epoch [107/120    avg_loss:0.731, val_acc:0.763]
Epoch [108/120    avg_loss:0.741, val_acc:0.763]
Epoch [109/120    avg_loss:0.745, val_acc:0.763]
Epoch [110/120    avg_loss:0.753, val_acc:0.763]
Epoch [111/120    avg_loss:0.736, val_acc:0.763]
Epoch [112/120    avg_loss:0.731, val_acc:0.763]
Epoch [113/120    avg_loss:0.748, val_acc:0.763]
Epoch [114/120    avg_loss:0.745, val_acc:0.763]
Epoch [115/120    avg_loss:0.759, val_acc:0.763]
Epoch [116/120    avg_loss:0.762, val_acc:0.763]
Epoch [117/120    avg_loss:0.749, val_acc:0.763]
Epoch [118/120    avg_loss:0.743, val_acc:0.763]
Epoch [119/120    avg_loss:0.760, val_acc:0.763]
Epoch [120/120    avg_loss:0.735, val_acc:0.763]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4545    56   103   455     0   224   215   552   282]
 [    0     0 13490     0   148     0  4449     0     3     0]
 [    0     3     0  1795    11     0     0     0   174    53]
 [    0   137   466    14  1989     0   187     0   159    20]
 [    0     0     0     5     0  1300     0     0     0     0]
 [    0     0   368   115   223     0  3993     0   179     0]
 [    0    69     0     0     5     0     0  1193     0    23]
 [    0    90     0    83   156     0    65     0  3137    40]
 [    0    17     0    18    21   181     0     4     0   678]]

Accuracy:
77.41064757910974

F1 scores:
[       nan 0.8049234  0.83092085 0.86111777 0.66521739 0.93323762
 0.57886344 0.88304959 0.80694534 0.67295285]

Kappa:
0.7127052590735881
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7924de2978>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.066, val_acc:0.121]
Epoch [2/120    avg_loss:1.689, val_acc:0.156]
Epoch [3/120    avg_loss:1.474, val_acc:0.333]
Epoch [4/120    avg_loss:1.336, val_acc:0.389]
Epoch [5/120    avg_loss:1.208, val_acc:0.419]
Epoch [6/120    avg_loss:1.097, val_acc:0.452]
Epoch [7/120    avg_loss:0.961, val_acc:0.467]
Epoch [8/120    avg_loss:0.866, val_acc:0.503]
Epoch [9/120    avg_loss:0.780, val_acc:0.544]
Epoch [10/120    avg_loss:0.671, val_acc:0.578]
Epoch [11/120    avg_loss:0.592, val_acc:0.654]
Epoch [12/120    avg_loss:0.517, val_acc:0.695]
Epoch [13/120    avg_loss:0.478, val_acc:0.767]
Epoch [14/120    avg_loss:0.429, val_acc:0.784]
Epoch [15/120    avg_loss:0.377, val_acc:0.823]
Epoch [16/120    avg_loss:0.352, val_acc:0.857]
Epoch [17/120    avg_loss:0.339, val_acc:0.885]
Epoch [18/120    avg_loss:0.292, val_acc:0.903]
Epoch [19/120    avg_loss:0.256, val_acc:0.893]
Epoch [20/120    avg_loss:0.235, val_acc:0.895]
Epoch [21/120    avg_loss:0.211, val_acc:0.912]
Epoch [22/120    avg_loss:0.206, val_acc:0.919]
Epoch [23/120    avg_loss:0.167, val_acc:0.941]
Epoch [24/120    avg_loss:0.184, val_acc:0.915]
Epoch [25/120    avg_loss:0.154, val_acc:0.952]
Epoch [26/120    avg_loss:0.161, val_acc:0.963]
Epoch [27/120    avg_loss:0.115, val_acc:0.936]
Epoch [28/120    avg_loss:0.107, val_acc:0.966]
Epoch [29/120    avg_loss:0.120, val_acc:0.961]
Epoch [30/120    avg_loss:0.104, val_acc:0.941]
Epoch [31/120    avg_loss:0.107, val_acc:0.963]
Epoch [32/120    avg_loss:0.147, val_acc:0.956]
Epoch [33/120    avg_loss:0.141, val_acc:0.936]
Epoch [34/120    avg_loss:0.129, val_acc:0.950]
Epoch [35/120    avg_loss:0.082, val_acc:0.969]
Epoch [36/120    avg_loss:0.082, val_acc:0.974]
Epoch [37/120    avg_loss:0.080, val_acc:0.966]
Epoch [38/120    avg_loss:0.086, val_acc:0.939]
Epoch [39/120    avg_loss:0.132, val_acc:0.926]
Epoch [40/120    avg_loss:0.106, val_acc:0.950]
Epoch [41/120    avg_loss:0.999, val_acc:0.566]
Epoch [42/120    avg_loss:1.295, val_acc:0.647]
Epoch [43/120    avg_loss:1.109, val_acc:0.709]
Epoch [44/120    avg_loss:0.934, val_acc:0.749]
Epoch [45/120    avg_loss:0.820, val_acc:0.695]
Epoch [46/120    avg_loss:0.721, val_acc:0.772]
Epoch [47/120    avg_loss:0.656, val_acc:0.759]
Epoch [48/120    avg_loss:0.618, val_acc:0.637]
Epoch [49/120    avg_loss:0.588, val_acc:0.791]
Epoch [50/120    avg_loss:0.528, val_acc:0.786]
Epoch [51/120    avg_loss:0.515, val_acc:0.781]
Epoch [52/120    avg_loss:0.486, val_acc:0.788]
Epoch [53/120    avg_loss:0.497, val_acc:0.792]
Epoch [54/120    avg_loss:0.491, val_acc:0.803]
Epoch [55/120    avg_loss:0.502, val_acc:0.796]
Epoch [56/120    avg_loss:0.488, val_acc:0.800]
Epoch [57/120    avg_loss:0.477, val_acc:0.808]
Epoch [58/120    avg_loss:0.471, val_acc:0.800]
Epoch [59/120    avg_loss:0.479, val_acc:0.796]
Epoch [60/120    avg_loss:0.479, val_acc:0.808]
Epoch [61/120    avg_loss:0.458, val_acc:0.805]
Epoch [62/120    avg_loss:0.461, val_acc:0.800]
Epoch [63/120    avg_loss:0.449, val_acc:0.801]
Epoch [64/120    avg_loss:0.456, val_acc:0.802]
Epoch [65/120    avg_loss:0.459, val_acc:0.802]
Epoch [66/120    avg_loss:0.449, val_acc:0.806]
Epoch [67/120    avg_loss:0.443, val_acc:0.803]
Epoch [68/120    avg_loss:0.446, val_acc:0.803]
Epoch [69/120    avg_loss:0.440, val_acc:0.804]
Epoch [70/120    avg_loss:0.446, val_acc:0.806]
Epoch [71/120    avg_loss:0.439, val_acc:0.804]
Epoch [72/120    avg_loss:0.442, val_acc:0.803]
Epoch [73/120    avg_loss:0.441, val_acc:0.806]
Epoch [74/120    avg_loss:0.453, val_acc:0.806]
Epoch [75/120    avg_loss:0.455, val_acc:0.807]
Epoch [76/120    avg_loss:0.446, val_acc:0.807]
Epoch [77/120    avg_loss:0.449, val_acc:0.808]
Epoch [78/120    avg_loss:0.452, val_acc:0.807]
Epoch [79/120    avg_loss:0.439, val_acc:0.807]
Epoch [80/120    avg_loss:0.440, val_acc:0.807]
Epoch [81/120    avg_loss:0.447, val_acc:0.807]
Epoch [82/120    avg_loss:0.453, val_acc:0.807]
Epoch [83/120    avg_loss:0.454, val_acc:0.807]
Epoch [84/120    avg_loss:0.455, val_acc:0.808]
Epoch [85/120    avg_loss:0.452, val_acc:0.807]
Epoch [86/120    avg_loss:0.438, val_acc:0.808]
Epoch [87/120    avg_loss:0.444, val_acc:0.808]
Epoch [88/120    avg_loss:0.455, val_acc:0.808]
Epoch [89/120    avg_loss:0.451, val_acc:0.808]
Epoch [90/120    avg_loss:0.440, val_acc:0.808]
Epoch [91/120    avg_loss:0.442, val_acc:0.808]
Epoch [92/120    avg_loss:0.440, val_acc:0.808]
Epoch [93/120    avg_loss:0.444, val_acc:0.808]
Epoch [94/120    avg_loss:0.439, val_acc:0.808]
Epoch [95/120    avg_loss:0.447, val_acc:0.808]
Epoch [96/120    avg_loss:0.460, val_acc:0.808]
Epoch [97/120    avg_loss:0.449, val_acc:0.808]
Epoch [98/120    avg_loss:0.445, val_acc:0.808]
Epoch [99/120    avg_loss:0.452, val_acc:0.808]
Epoch [100/120    avg_loss:0.443, val_acc:0.808]
Epoch [101/120    avg_loss:0.458, val_acc:0.808]
Epoch [102/120    avg_loss:0.458, val_acc:0.808]
Epoch [103/120    avg_loss:0.448, val_acc:0.808]
Epoch [104/120    avg_loss:0.445, val_acc:0.808]
Epoch [105/120    avg_loss:0.442, val_acc:0.808]
Epoch [106/120    avg_loss:0.437, val_acc:0.808]
Epoch [107/120    avg_loss:0.440, val_acc:0.808]
Epoch [108/120    avg_loss:0.453, val_acc:0.808]
Epoch [109/120    avg_loss:0.440, val_acc:0.808]
Epoch [110/120    avg_loss:0.452, val_acc:0.808]
Epoch [111/120    avg_loss:0.435, val_acc:0.808]
Epoch [112/120    avg_loss:0.464, val_acc:0.808]
Epoch [113/120    avg_loss:0.434, val_acc:0.808]
Epoch [114/120    avg_loss:0.451, val_acc:0.808]
Epoch [115/120    avg_loss:0.447, val_acc:0.808]
Epoch [116/120    avg_loss:0.444, val_acc:0.808]
Epoch [117/120    avg_loss:0.439, val_acc:0.808]
Epoch [118/120    avg_loss:0.444, val_acc:0.808]
Epoch [119/120    avg_loss:0.439, val_acc:0.808]
Epoch [120/120    avg_loss:0.438, val_acc:0.808]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5446     0     9   357     0     2     3   469   146]
 [    0     0 15530     0   785     0  1775     0     0     0]
 [    0     4     0  1853     7     0     0     0   141    31]
 [    0   329   146     0  2095     0   223     0   175     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1  1224     0    23     0  3405     0   225     0]
 [    0     8     0     0     8     0     0  1257     0    17]
 [    0   206     0    37     8     0    80     0  3240     0]
 [    0    12     0     5    26   133     2     0     8   733]]

Accuracy:
84.0238112452703

F1 scores:
[       nan 0.87570349 0.88768219 0.94060914 0.66709123 0.95151294
 0.65701881 0.98588235 0.82769191 0.79243243]

Kappa:
0.7911928386583449
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f90c1c3e780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.205, val_acc:0.732]
Epoch [2/120    avg_loss:0.659, val_acc:0.742]
Epoch [3/120    avg_loss:0.497, val_acc:0.786]
Epoch [4/120    avg_loss:0.423, val_acc:0.660]
Epoch [5/120    avg_loss:0.350, val_acc:0.791]
Epoch [6/120    avg_loss:0.374, val_acc:0.842]
Epoch [7/120    avg_loss:0.284, val_acc:0.817]
Epoch [8/120    avg_loss:0.231, val_acc:0.896]
Epoch [9/120    avg_loss:0.223, val_acc:0.863]
Epoch [10/120    avg_loss:0.142, val_acc:0.845]
Epoch [11/120    avg_loss:0.178, val_acc:0.924]
Epoch [12/120    avg_loss:0.206, val_acc:0.910]
Epoch [13/120    avg_loss:0.201, val_acc:0.915]
Epoch [14/120    avg_loss:0.146, val_acc:0.876]
Epoch [15/120    avg_loss:0.154, val_acc:0.912]
Epoch [16/120    avg_loss:0.125, val_acc:0.938]
Epoch [17/120    avg_loss:0.090, val_acc:0.956]
Epoch [18/120    avg_loss:0.110, val_acc:0.941]
Epoch [19/120    avg_loss:0.082, val_acc:0.938]
Epoch [20/120    avg_loss:0.084, val_acc:0.924]
Epoch [21/120    avg_loss:0.086, val_acc:0.943]
Epoch [22/120    avg_loss:0.123, val_acc:0.948]
Epoch [23/120    avg_loss:0.069, val_acc:0.948]
Epoch [24/120    avg_loss:0.054, val_acc:0.954]
Epoch [25/120    avg_loss:0.090, val_acc:0.946]
Epoch [26/120    avg_loss:0.041, val_acc:0.962]
Epoch [27/120    avg_loss:0.223, val_acc:0.882]
Epoch [28/120    avg_loss:0.124, val_acc:0.933]
Epoch [29/120    avg_loss:0.075, val_acc:0.964]
Epoch [30/120    avg_loss:0.073, val_acc:0.948]
Epoch [31/120    avg_loss:0.056, val_acc:0.964]
Epoch [32/120    avg_loss:0.044, val_acc:0.961]
Epoch [33/120    avg_loss:0.112, val_acc:0.947]
Epoch [34/120    avg_loss:0.073, val_acc:0.954]
Epoch [35/120    avg_loss:0.041, val_acc:0.970]
Epoch [36/120    avg_loss:0.067, val_acc:0.963]
Epoch [37/120    avg_loss:0.046, val_acc:0.967]
Epoch [38/120    avg_loss:0.032, val_acc:0.952]
Epoch [39/120    avg_loss:0.019, val_acc:0.970]
Epoch [40/120    avg_loss:0.028, val_acc:0.956]
Epoch [41/120    avg_loss:0.033, val_acc:0.965]
Epoch [42/120    avg_loss:0.026, val_acc:0.976]
Epoch [43/120    avg_loss:0.024, val_acc:0.976]
Epoch [44/120    avg_loss:0.030, val_acc:0.981]
Epoch [45/120    avg_loss:0.021, val_acc:0.974]
Epoch [46/120    avg_loss:0.020, val_acc:0.954]
Epoch [47/120    avg_loss:0.041, val_acc:0.968]
Epoch [48/120    avg_loss:0.042, val_acc:0.943]
Epoch [49/120    avg_loss:0.028, val_acc:0.975]
Epoch [50/120    avg_loss:0.037, val_acc:0.961]
Epoch [51/120    avg_loss:0.019, val_acc:0.973]
Epoch [52/120    avg_loss:0.016, val_acc:0.961]
Epoch [53/120    avg_loss:0.023, val_acc:0.947]
Epoch [54/120    avg_loss:0.040, val_acc:0.972]
Epoch [55/120    avg_loss:0.021, val_acc:0.964]
Epoch [56/120    avg_loss:0.018, val_acc:0.963]
Epoch [57/120    avg_loss:0.016, val_acc:0.968]
Epoch [58/120    avg_loss:0.017, val_acc:0.977]
Epoch [59/120    avg_loss:0.009, val_acc:0.979]
Epoch [60/120    avg_loss:0.007, val_acc:0.981]
Epoch [61/120    avg_loss:0.007, val_acc:0.979]
Epoch [62/120    avg_loss:0.009, val_acc:0.981]
Epoch [63/120    avg_loss:0.007, val_acc:0.981]
Epoch [64/120    avg_loss:0.009, val_acc:0.981]
Epoch [65/120    avg_loss:0.009, val_acc:0.979]
Epoch [66/120    avg_loss:0.010, val_acc:0.979]
Epoch [67/120    avg_loss:0.010, val_acc:0.979]
Epoch [68/120    avg_loss:0.007, val_acc:0.980]
Epoch [69/120    avg_loss:0.007, val_acc:0.979]
Epoch [70/120    avg_loss:0.006, val_acc:0.979]
Epoch [71/120    avg_loss:0.007, val_acc:0.980]
Epoch [72/120    avg_loss:0.010, val_acc:0.982]
Epoch [73/120    avg_loss:0.006, val_acc:0.982]
Epoch [74/120    avg_loss:0.007, val_acc:0.980]
Epoch [75/120    avg_loss:0.009, val_acc:0.982]
Epoch [76/120    avg_loss:0.008, val_acc:0.982]
Epoch [77/120    avg_loss:0.007, val_acc:0.982]
Epoch [78/120    avg_loss:0.007, val_acc:0.981]
Epoch [79/120    avg_loss:0.009, val_acc:0.980]
Epoch [80/120    avg_loss:0.006, val_acc:0.979]
Epoch [81/120    avg_loss:0.007, val_acc:0.979]
Epoch [82/120    avg_loss:0.007, val_acc:0.979]
Epoch [83/120    avg_loss:0.009, val_acc:0.979]
Epoch [84/120    avg_loss:0.010, val_acc:0.979]
Epoch [85/120    avg_loss:0.006, val_acc:0.979]
Epoch [86/120    avg_loss:0.009, val_acc:0.979]
Epoch [87/120    avg_loss:0.006, val_acc:0.980]
Epoch [88/120    avg_loss:0.012, val_acc:0.978]
Epoch [89/120    avg_loss:0.008, val_acc:0.979]
Epoch [90/120    avg_loss:0.008, val_acc:0.979]
Epoch [91/120    avg_loss:0.006, val_acc:0.979]
Epoch [92/120    avg_loss:0.007, val_acc:0.979]
Epoch [93/120    avg_loss:0.006, val_acc:0.979]
Epoch [94/120    avg_loss:0.007, val_acc:0.979]
Epoch [95/120    avg_loss:0.006, val_acc:0.979]
Epoch [96/120    avg_loss:0.010, val_acc:0.979]
Epoch [97/120    avg_loss:0.005, val_acc:0.979]
Epoch [98/120    avg_loss:0.007, val_acc:0.979]
Epoch [99/120    avg_loss:0.006, val_acc:0.979]
Epoch [100/120    avg_loss:0.006, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.979]
Epoch [102/120    avg_loss:0.006, val_acc:0.979]
Epoch [103/120    avg_loss:0.006, val_acc:0.980]
Epoch [104/120    avg_loss:0.005, val_acc:0.980]
Epoch [105/120    avg_loss:0.006, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.009, val_acc:0.980]
Epoch [108/120    avg_loss:0.005, val_acc:0.980]
Epoch [109/120    avg_loss:0.006, val_acc:0.980]
Epoch [110/120    avg_loss:0.007, val_acc:0.980]
Epoch [111/120    avg_loss:0.006, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     5     0     0     4     1    29     3]
 [    0     0 17917     0    63     0   105     0     5     0]
 [    0     3     0  1927     0     0     0     0   105     1]
 [    0    21     6     0  2928     4     1     0     9     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4872     0     5     0]
 [    0    12     0     0     0     0     0  1273     0     5]
 [    0     3     0    20    38     0     0     0  3510     0]
 [    0     0     0     0     0    17     0     0     0   902]]

Accuracy:
98.86968886318175

F1 scores:
[       nan 0.99370189 0.99500194 0.9663992  0.97583736 0.99201824
 0.98823529 0.99297972 0.97041747 0.98417894]

Kappa:
0.9850478487630216
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f805c2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.311, val_acc:0.728]
Epoch [2/120    avg_loss:0.681, val_acc:0.743]
Epoch [3/120    avg_loss:0.584, val_acc:0.735]
Epoch [4/120    avg_loss:0.424, val_acc:0.833]
Epoch [5/120    avg_loss:0.445, val_acc:0.805]
Epoch [6/120    avg_loss:0.339, val_acc:0.900]
Epoch [7/120    avg_loss:0.264, val_acc:0.819]
Epoch [8/120    avg_loss:0.255, val_acc:0.905]
Epoch [9/120    avg_loss:0.237, val_acc:0.932]
Epoch [10/120    avg_loss:0.206, val_acc:0.886]
Epoch [11/120    avg_loss:0.197, val_acc:0.893]
Epoch [12/120    avg_loss:0.176, val_acc:0.883]
Epoch [13/120    avg_loss:0.165, val_acc:0.882]
Epoch [14/120    avg_loss:0.140, val_acc:0.889]
Epoch [15/120    avg_loss:0.179, val_acc:0.929]
Epoch [16/120    avg_loss:0.171, val_acc:0.947]
Epoch [17/120    avg_loss:0.124, val_acc:0.948]
Epoch [18/120    avg_loss:0.105, val_acc:0.929]
Epoch [19/120    avg_loss:0.117, val_acc:0.937]
Epoch [20/120    avg_loss:0.081, val_acc:0.935]
Epoch [21/120    avg_loss:0.103, val_acc:0.945]
Epoch [22/120    avg_loss:0.092, val_acc:0.958]
Epoch [23/120    avg_loss:0.055, val_acc:0.938]
Epoch [24/120    avg_loss:0.081, val_acc:0.949]
Epoch [25/120    avg_loss:0.071, val_acc:0.958]
Epoch [26/120    avg_loss:0.065, val_acc:0.962]
Epoch [27/120    avg_loss:0.053, val_acc:0.971]
Epoch [28/120    avg_loss:0.093, val_acc:0.931]
Epoch [29/120    avg_loss:0.216, val_acc:0.930]
Epoch [30/120    avg_loss:0.065, val_acc:0.957]
Epoch [31/120    avg_loss:0.047, val_acc:0.954]
Epoch [32/120    avg_loss:0.107, val_acc:0.906]
Epoch [33/120    avg_loss:0.070, val_acc:0.938]
Epoch [34/120    avg_loss:0.060, val_acc:0.966]
Epoch [35/120    avg_loss:0.032, val_acc:0.967]
Epoch [36/120    avg_loss:0.088, val_acc:0.917]
Epoch [37/120    avg_loss:0.048, val_acc:0.966]
Epoch [38/120    avg_loss:0.048, val_acc:0.970]
Epoch [39/120    avg_loss:0.033, val_acc:0.971]
Epoch [40/120    avg_loss:0.023, val_acc:0.973]
Epoch [41/120    avg_loss:0.030, val_acc:0.974]
Epoch [42/120    avg_loss:0.024, val_acc:0.912]
Epoch [43/120    avg_loss:0.037, val_acc:0.947]
Epoch [44/120    avg_loss:0.058, val_acc:0.967]
Epoch [45/120    avg_loss:0.026, val_acc:0.960]
Epoch [46/120    avg_loss:0.030, val_acc:0.949]
Epoch [47/120    avg_loss:0.027, val_acc:0.971]
Epoch [48/120    avg_loss:0.017, val_acc:0.975]
Epoch [49/120    avg_loss:0.030, val_acc:0.964]
Epoch [50/120    avg_loss:0.024, val_acc:0.978]
Epoch [51/120    avg_loss:0.018, val_acc:0.968]
Epoch [52/120    avg_loss:0.018, val_acc:0.976]
Epoch [53/120    avg_loss:0.084, val_acc:0.930]
Epoch [54/120    avg_loss:0.037, val_acc:0.970]
Epoch [55/120    avg_loss:0.019, val_acc:0.970]
Epoch [56/120    avg_loss:0.024, val_acc:0.979]
Epoch [57/120    avg_loss:0.019, val_acc:0.958]
Epoch [58/120    avg_loss:0.016, val_acc:0.971]
Epoch [59/120    avg_loss:0.017, val_acc:0.969]
Epoch [60/120    avg_loss:0.032, val_acc:0.959]
Epoch [61/120    avg_loss:0.010, val_acc:0.977]
Epoch [62/120    avg_loss:0.015, val_acc:0.978]
Epoch [63/120    avg_loss:0.019, val_acc:0.964]
Epoch [64/120    avg_loss:0.027, val_acc:0.971]
Epoch [65/120    avg_loss:0.019, val_acc:0.942]
Epoch [66/120    avg_loss:0.017, val_acc:0.977]
Epoch [67/120    avg_loss:0.014, val_acc:0.980]
Epoch [68/120    avg_loss:0.012, val_acc:0.978]
Epoch [69/120    avg_loss:0.024, val_acc:0.975]
Epoch [70/120    avg_loss:0.014, val_acc:0.985]
Epoch [71/120    avg_loss:0.009, val_acc:0.980]
Epoch [72/120    avg_loss:0.029, val_acc:0.967]
Epoch [73/120    avg_loss:0.069, val_acc:0.971]
Epoch [74/120    avg_loss:0.045, val_acc:0.969]
Epoch [75/120    avg_loss:0.033, val_acc:0.971]
Epoch [76/120    avg_loss:0.026, val_acc:0.979]
Epoch [77/120    avg_loss:0.010, val_acc:0.975]
Epoch [78/120    avg_loss:0.016, val_acc:0.975]
Epoch [79/120    avg_loss:0.010, val_acc:0.977]
Epoch [80/120    avg_loss:0.016, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.979]
Epoch [82/120    avg_loss:0.007, val_acc:0.981]
Epoch [83/120    avg_loss:0.014, val_acc:0.971]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.025, val_acc:0.980]
Epoch [88/120    avg_loss:0.007, val_acc:0.980]
Epoch [89/120    avg_loss:0.005, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.980]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.015, val_acc:0.958]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.075, val_acc:0.944]
Epoch [101/120    avg_loss:0.040, val_acc:0.975]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.049, val_acc:0.979]
Epoch [104/120    avg_loss:0.014, val_acc:0.979]
Epoch [105/120    avg_loss:0.013, val_acc:0.979]
Epoch [106/120    avg_loss:0.014, val_acc:0.973]
Epoch [107/120    avg_loss:0.008, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.976]
Epoch [109/120    avg_loss:0.008, val_acc:0.976]
Epoch [110/120    avg_loss:0.007, val_acc:0.978]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     8     0     0     8     9    12     0]
 [    0     0 17890     0     4     0   191     0     5     0]
 [    0     6     0  1905     0     0     0     0   119     6]
 [    0    12     9     2  2939     0     2     0     4     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4867     0    10     0]
 [    0     9     0     0     0     0     0  1279     0     2]
 [    0     4     1    69    52     0     0     0  3445     0]
 [    0     0     0     0     1    16     0     0     0   902]]

Accuracy:
98.635914491601

F1 scores:
[       nan 0.99471146 0.99416505 0.94752549 0.98491957 0.99390708
 0.9786849  0.99224205 0.96148479 0.98417894]

Kappa:
0.9819595043109706
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ff78fb780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.295, val_acc:0.627]
Epoch [2/120    avg_loss:0.665, val_acc:0.732]
Epoch [3/120    avg_loss:0.517, val_acc:0.715]
Epoch [4/120    avg_loss:0.445, val_acc:0.811]
Epoch [5/120    avg_loss:0.362, val_acc:0.821]
Epoch [6/120    avg_loss:0.386, val_acc:0.859]
Epoch [7/120    avg_loss:0.279, val_acc:0.757]
Epoch [8/120    avg_loss:0.352, val_acc:0.721]
Epoch [9/120    avg_loss:0.295, val_acc:0.806]
Epoch [10/120    avg_loss:0.232, val_acc:0.845]
Epoch [11/120    avg_loss:0.290, val_acc:0.830]
Epoch [12/120    avg_loss:0.201, val_acc:0.862]
Epoch [13/120    avg_loss:0.175, val_acc:0.915]
Epoch [14/120    avg_loss:0.149, val_acc:0.914]
Epoch [15/120    avg_loss:0.145, val_acc:0.935]
Epoch [16/120    avg_loss:0.155, val_acc:0.835]
Epoch [17/120    avg_loss:0.162, val_acc:0.925]
Epoch [18/120    avg_loss:0.110, val_acc:0.910]
Epoch [19/120    avg_loss:0.117, val_acc:0.926]
Epoch [20/120    avg_loss:0.121, val_acc:0.897]
Epoch [21/120    avg_loss:0.183, val_acc:0.907]
Epoch [22/120    avg_loss:0.094, val_acc:0.905]
Epoch [23/120    avg_loss:0.107, val_acc:0.937]
Epoch [24/120    avg_loss:0.105, val_acc:0.904]
Epoch [25/120    avg_loss:0.101, val_acc:0.948]
Epoch [26/120    avg_loss:0.086, val_acc:0.919]
Epoch [27/120    avg_loss:0.077, val_acc:0.954]
Epoch [28/120    avg_loss:0.087, val_acc:0.931]
Epoch [29/120    avg_loss:0.093, val_acc:0.945]
Epoch [30/120    avg_loss:0.073, val_acc:0.933]
Epoch [31/120    avg_loss:0.074, val_acc:0.940]
Epoch [32/120    avg_loss:0.069, val_acc:0.947]
Epoch [33/120    avg_loss:0.076, val_acc:0.865]
Epoch [34/120    avg_loss:0.085, val_acc:0.957]
Epoch [35/120    avg_loss:0.176, val_acc:0.931]
Epoch [36/120    avg_loss:0.073, val_acc:0.944]
Epoch [37/120    avg_loss:0.049, val_acc:0.968]
Epoch [38/120    avg_loss:0.040, val_acc:0.971]
Epoch [39/120    avg_loss:0.037, val_acc:0.971]
Epoch [40/120    avg_loss:0.052, val_acc:0.872]
Epoch [41/120    avg_loss:0.059, val_acc:0.965]
Epoch [42/120    avg_loss:0.058, val_acc:0.973]
Epoch [43/120    avg_loss:0.052, val_acc:0.961]
Epoch [44/120    avg_loss:0.067, val_acc:0.942]
Epoch [45/120    avg_loss:0.061, val_acc:0.944]
Epoch [46/120    avg_loss:0.038, val_acc:0.975]
Epoch [47/120    avg_loss:0.038, val_acc:0.960]
Epoch [48/120    avg_loss:0.034, val_acc:0.977]
Epoch [49/120    avg_loss:0.024, val_acc:0.981]
Epoch [50/120    avg_loss:0.046, val_acc:0.970]
Epoch [51/120    avg_loss:0.027, val_acc:0.977]
Epoch [52/120    avg_loss:0.017, val_acc:0.963]
Epoch [53/120    avg_loss:0.027, val_acc:0.953]
Epoch [54/120    avg_loss:0.019, val_acc:0.967]
Epoch [55/120    avg_loss:0.023, val_acc:0.974]
Epoch [56/120    avg_loss:0.025, val_acc:0.934]
Epoch [57/120    avg_loss:0.025, val_acc:0.971]
Epoch [58/120    avg_loss:0.044, val_acc:0.977]
Epoch [59/120    avg_loss:0.053, val_acc:0.928]
Epoch [60/120    avg_loss:0.103, val_acc:0.978]
Epoch [61/120    avg_loss:0.032, val_acc:0.978]
Epoch [62/120    avg_loss:0.039, val_acc:0.977]
Epoch [63/120    avg_loss:0.025, val_acc:0.979]
Epoch [64/120    avg_loss:0.020, val_acc:0.983]
Epoch [65/120    avg_loss:0.016, val_acc:0.985]
Epoch [66/120    avg_loss:0.016, val_acc:0.982]
Epoch [67/120    avg_loss:0.016, val_acc:0.982]
Epoch [68/120    avg_loss:0.017, val_acc:0.983]
Epoch [69/120    avg_loss:0.011, val_acc:0.984]
Epoch [70/120    avg_loss:0.020, val_acc:0.984]
Epoch [71/120    avg_loss:0.012, val_acc:0.985]
Epoch [72/120    avg_loss:0.013, val_acc:0.984]
Epoch [73/120    avg_loss:0.011, val_acc:0.985]
Epoch [74/120    avg_loss:0.015, val_acc:0.986]
Epoch [75/120    avg_loss:0.011, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.010, val_acc:0.985]
Epoch [78/120    avg_loss:0.011, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.011, val_acc:0.985]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.987]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.013, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.013, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.012, val_acc:0.987]
Epoch [93/120    avg_loss:0.011, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.012, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.014, val_acc:0.987]
Epoch [100/120    avg_loss:0.017, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.010, val_acc:0.987]
Epoch [111/120    avg_loss:0.014, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.011, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     7     0     0    20     0    39     1]
 [    0     0 17892     0     5     0   189     0     4     0]
 [    0     0     0  1968     0     0     0     0    62     6]
 [    0    13    10     1  2938     0     1     0     4     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     1     0     0  4868     0     3     0]
 [    0     9     0     0     0     0     0  1276     0     5]
 [    0     5     0    37    46     0     0     0  3482     1]
 [    0     4     0     0     0    21     0     0     0   894]]

Accuracy:
98.7829272407394

F1 scores:
[       nan 0.99236046 0.99405523 0.97185185 0.98574065 0.99201824
 0.97790277 0.99454404 0.97194696 0.97651557]

Kappa:
0.98390349611586
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea6cab0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.263, val_acc:0.729]
Epoch [2/120    avg_loss:0.628, val_acc:0.750]
Epoch [3/120    avg_loss:0.575, val_acc:0.783]
Epoch [4/120    avg_loss:0.518, val_acc:0.803]
Epoch [5/120    avg_loss:0.388, val_acc:0.863]
Epoch [6/120    avg_loss:0.315, val_acc:0.877]
Epoch [7/120    avg_loss:0.288, val_acc:0.806]
Epoch [8/120    avg_loss:0.274, val_acc:0.851]
Epoch [9/120    avg_loss:0.242, val_acc:0.840]
Epoch [10/120    avg_loss:0.283, val_acc:0.865]
Epoch [11/120    avg_loss:0.215, val_acc:0.896]
Epoch [12/120    avg_loss:0.235, val_acc:0.881]
Epoch [13/120    avg_loss:0.215, val_acc:0.897]
Epoch [14/120    avg_loss:0.249, val_acc:0.903]
Epoch [15/120    avg_loss:0.199, val_acc:0.856]
Epoch [16/120    avg_loss:0.219, val_acc:0.908]
Epoch [17/120    avg_loss:0.188, val_acc:0.919]
Epoch [18/120    avg_loss:0.131, val_acc:0.925]
Epoch [19/120    avg_loss:0.203, val_acc:0.928]
Epoch [20/120    avg_loss:0.110, val_acc:0.901]
Epoch [21/120    avg_loss:0.119, val_acc:0.951]
Epoch [22/120    avg_loss:0.090, val_acc:0.957]
Epoch [23/120    avg_loss:0.115, val_acc:0.938]
Epoch [24/120    avg_loss:0.092, val_acc:0.938]
Epoch [25/120    avg_loss:0.115, val_acc:0.929]
Epoch [26/120    avg_loss:0.110, val_acc:0.960]
Epoch [27/120    avg_loss:0.070, val_acc:0.924]
Epoch [28/120    avg_loss:0.070, val_acc:0.947]
Epoch [29/120    avg_loss:0.137, val_acc:0.884]
Epoch [30/120    avg_loss:0.086, val_acc:0.959]
Epoch [31/120    avg_loss:0.081, val_acc:0.954]
Epoch [32/120    avg_loss:0.064, val_acc:0.957]
Epoch [33/120    avg_loss:0.128, val_acc:0.932]
Epoch [34/120    avg_loss:0.087, val_acc:0.960]
Epoch [35/120    avg_loss:0.072, val_acc:0.930]
Epoch [36/120    avg_loss:0.094, val_acc:0.960]
Epoch [37/120    avg_loss:0.043, val_acc:0.965]
Epoch [38/120    avg_loss:0.050, val_acc:0.948]
Epoch [39/120    avg_loss:0.074, val_acc:0.959]
Epoch [40/120    avg_loss:0.054, val_acc:0.950]
Epoch [41/120    avg_loss:0.069, val_acc:0.936]
Epoch [42/120    avg_loss:0.042, val_acc:0.944]
Epoch [43/120    avg_loss:0.055, val_acc:0.962]
Epoch [44/120    avg_loss:0.094, val_acc:0.960]
Epoch [45/120    avg_loss:0.030, val_acc:0.967]
Epoch [46/120    avg_loss:0.051, val_acc:0.972]
Epoch [47/120    avg_loss:0.059, val_acc:0.956]
Epoch [48/120    avg_loss:0.044, val_acc:0.942]
Epoch [49/120    avg_loss:0.063, val_acc:0.965]
Epoch [50/120    avg_loss:0.072, val_acc:0.940]
Epoch [51/120    avg_loss:0.064, val_acc:0.943]
Epoch [52/120    avg_loss:0.082, val_acc:0.970]
Epoch [53/120    avg_loss:0.028, val_acc:0.943]
Epoch [54/120    avg_loss:0.033, val_acc:0.933]
Epoch [55/120    avg_loss:0.027, val_acc:0.975]
Epoch [56/120    avg_loss:0.018, val_acc:0.972]
Epoch [57/120    avg_loss:0.029, val_acc:0.978]
Epoch [58/120    avg_loss:0.031, val_acc:0.979]
Epoch [59/120    avg_loss:0.022, val_acc:0.967]
Epoch [60/120    avg_loss:0.112, val_acc:0.967]
Epoch [61/120    avg_loss:0.040, val_acc:0.959]
Epoch [62/120    avg_loss:0.050, val_acc:0.967]
Epoch [63/120    avg_loss:0.021, val_acc:0.965]
Epoch [64/120    avg_loss:0.022, val_acc:0.977]
Epoch [65/120    avg_loss:0.030, val_acc:0.968]
Epoch [66/120    avg_loss:0.022, val_acc:0.978]
Epoch [67/120    avg_loss:0.037, val_acc:0.976]
Epoch [68/120    avg_loss:0.028, val_acc:0.938]
Epoch [69/120    avg_loss:0.021, val_acc:0.976]
Epoch [70/120    avg_loss:0.031, val_acc:0.978]
Epoch [71/120    avg_loss:0.042, val_acc:0.965]
Epoch [72/120    avg_loss:0.017, val_acc:0.970]
Epoch [73/120    avg_loss:0.016, val_acc:0.973]
Epoch [74/120    avg_loss:0.019, val_acc:0.977]
Epoch [75/120    avg_loss:0.014, val_acc:0.977]
Epoch [76/120    avg_loss:0.011, val_acc:0.975]
Epoch [77/120    avg_loss:0.014, val_acc:0.977]
Epoch [78/120    avg_loss:0.011, val_acc:0.975]
Epoch [79/120    avg_loss:0.014, val_acc:0.977]
Epoch [80/120    avg_loss:0.010, val_acc:0.978]
Epoch [81/120    avg_loss:0.011, val_acc:0.979]
Epoch [82/120    avg_loss:0.008, val_acc:0.979]
Epoch [83/120    avg_loss:0.011, val_acc:0.980]
Epoch [84/120    avg_loss:0.008, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.980]
Epoch [87/120    avg_loss:0.011, val_acc:0.979]
Epoch [88/120    avg_loss:0.012, val_acc:0.979]
Epoch [89/120    avg_loss:0.008, val_acc:0.979]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.012, val_acc:0.979]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.980]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.009, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.009, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.009, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.009, val_acc:0.979]
Epoch [117/120    avg_loss:0.007, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.979]
Epoch [120/120    avg_loss:0.007, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     2     0     0     0     4    56     0]
 [    0     0 18011     0    13     0    65     0     1     0]
 [    0     1     0  1926     0     0     0     0   106     3]
 [    0    26     6     1  2924     0     4     0     5     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0    10     0     0     0     0     2  1274     0     4]
 [    0    15     0    67    44     0     0     0  3444     1]
 [    0     0     0     0     0     7     0     0     0   912]]

Accuracy:
98.91306967440292

F1 scores:
[       nan 0.99113117 0.99764589 0.95535714 0.98236183 0.99732518
 0.99256997 0.99221184 0.95866388 0.98861789]

Kappa:
0.9856080322975094
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f709fe5d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.286, val_acc:0.533]
Epoch [2/120    avg_loss:0.657, val_acc:0.738]
Epoch [3/120    avg_loss:0.521, val_acc:0.817]
Epoch [4/120    avg_loss:0.416, val_acc:0.794]
Epoch [5/120    avg_loss:0.398, val_acc:0.851]
Epoch [6/120    avg_loss:0.346, val_acc:0.837]
Epoch [7/120    avg_loss:0.292, val_acc:0.851]
Epoch [8/120    avg_loss:0.251, val_acc:0.910]
Epoch [9/120    avg_loss:0.314, val_acc:0.799]
Epoch [10/120    avg_loss:0.271, val_acc:0.886]
Epoch [11/120    avg_loss:0.177, val_acc:0.911]
Epoch [12/120    avg_loss:0.297, val_acc:0.915]
Epoch [13/120    avg_loss:0.203, val_acc:0.848]
Epoch [14/120    avg_loss:0.183, val_acc:0.912]
Epoch [15/120    avg_loss:0.199, val_acc:0.916]
Epoch [16/120    avg_loss:0.182, val_acc:0.921]
Epoch [17/120    avg_loss:0.204, val_acc:0.905]
Epoch [18/120    avg_loss:0.134, val_acc:0.942]
Epoch [19/120    avg_loss:0.114, val_acc:0.914]
Epoch [20/120    avg_loss:0.104, val_acc:0.950]
Epoch [21/120    avg_loss:0.096, val_acc:0.959]
Epoch [22/120    avg_loss:0.071, val_acc:0.919]
Epoch [23/120    avg_loss:0.078, val_acc:0.929]
Epoch [24/120    avg_loss:0.128, val_acc:0.940]
Epoch [25/120    avg_loss:0.066, val_acc:0.886]
Epoch [26/120    avg_loss:0.071, val_acc:0.961]
Epoch [27/120    avg_loss:0.048, val_acc:0.963]
Epoch [28/120    avg_loss:0.090, val_acc:0.917]
Epoch [29/120    avg_loss:0.052, val_acc:0.955]
Epoch [30/120    avg_loss:0.052, val_acc:0.972]
Epoch [31/120    avg_loss:0.037, val_acc:0.973]
Epoch [32/120    avg_loss:0.048, val_acc:0.962]
Epoch [33/120    avg_loss:0.037, val_acc:0.961]
Epoch [34/120    avg_loss:0.036, val_acc:0.964]
Epoch [35/120    avg_loss:0.067, val_acc:0.933]
Epoch [36/120    avg_loss:0.107, val_acc:0.919]
Epoch [37/120    avg_loss:0.069, val_acc:0.965]
Epoch [38/120    avg_loss:0.033, val_acc:0.911]
Epoch [39/120    avg_loss:0.030, val_acc:0.970]
Epoch [40/120    avg_loss:0.033, val_acc:0.968]
Epoch [41/120    avg_loss:0.059, val_acc:0.964]
Epoch [42/120    avg_loss:0.116, val_acc:0.971]
Epoch [43/120    avg_loss:0.044, val_acc:0.950]
Epoch [44/120    avg_loss:0.053, val_acc:0.912]
Epoch [45/120    avg_loss:0.034, val_acc:0.965]
Epoch [46/120    avg_loss:0.029, val_acc:0.970]
Epoch [47/120    avg_loss:0.024, val_acc:0.974]
Epoch [48/120    avg_loss:0.023, val_acc:0.975]
Epoch [49/120    avg_loss:0.018, val_acc:0.978]
Epoch [50/120    avg_loss:0.019, val_acc:0.971]
Epoch [51/120    avg_loss:0.013, val_acc:0.974]
Epoch [52/120    avg_loss:0.026, val_acc:0.974]
Epoch [53/120    avg_loss:0.014, val_acc:0.976]
Epoch [54/120    avg_loss:0.016, val_acc:0.976]
Epoch [55/120    avg_loss:0.014, val_acc:0.977]
Epoch [56/120    avg_loss:0.017, val_acc:0.972]
Epoch [57/120    avg_loss:0.017, val_acc:0.978]
Epoch [58/120    avg_loss:0.013, val_acc:0.979]
Epoch [59/120    avg_loss:0.014, val_acc:0.979]
Epoch [60/120    avg_loss:0.015, val_acc:0.980]
Epoch [61/120    avg_loss:0.020, val_acc:0.980]
Epoch [62/120    avg_loss:0.014, val_acc:0.977]
Epoch [63/120    avg_loss:0.013, val_acc:0.975]
Epoch [64/120    avg_loss:0.013, val_acc:0.975]
Epoch [65/120    avg_loss:0.017, val_acc:0.978]
Epoch [66/120    avg_loss:0.010, val_acc:0.975]
Epoch [67/120    avg_loss:0.012, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.975]
Epoch [69/120    avg_loss:0.013, val_acc:0.979]
Epoch [70/120    avg_loss:0.013, val_acc:0.976]
Epoch [71/120    avg_loss:0.010, val_acc:0.978]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.013, val_acc:0.975]
Epoch [74/120    avg_loss:0.012, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.979]
Epoch [77/120    avg_loss:0.009, val_acc:0.979]
Epoch [78/120    avg_loss:0.012, val_acc:0.978]
Epoch [79/120    avg_loss:0.011, val_acc:0.979]
Epoch [80/120    avg_loss:0.012, val_acc:0.977]
Epoch [81/120    avg_loss:0.011, val_acc:0.975]
Epoch [82/120    avg_loss:0.013, val_acc:0.976]
Epoch [83/120    avg_loss:0.012, val_acc:0.975]
Epoch [84/120    avg_loss:0.014, val_acc:0.976]
Epoch [85/120    avg_loss:0.012, val_acc:0.976]
Epoch [86/120    avg_loss:0.010, val_acc:0.977]
Epoch [87/120    avg_loss:0.011, val_acc:0.976]
Epoch [88/120    avg_loss:0.014, val_acc:0.976]
Epoch [89/120    avg_loss:0.011, val_acc:0.976]
Epoch [90/120    avg_loss:0.011, val_acc:0.976]
Epoch [91/120    avg_loss:0.013, val_acc:0.976]
Epoch [92/120    avg_loss:0.010, val_acc:0.976]
Epoch [93/120    avg_loss:0.013, val_acc:0.976]
Epoch [94/120    avg_loss:0.015, val_acc:0.976]
Epoch [95/120    avg_loss:0.014, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.013, val_acc:0.976]
Epoch [99/120    avg_loss:0.010, val_acc:0.976]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.012, val_acc:0.976]
Epoch [103/120    avg_loss:0.010, val_acc:0.976]
Epoch [104/120    avg_loss:0.008, val_acc:0.976]
Epoch [105/120    avg_loss:0.012, val_acc:0.976]
Epoch [106/120    avg_loss:0.012, val_acc:0.976]
Epoch [107/120    avg_loss:0.011, val_acc:0.976]
Epoch [108/120    avg_loss:0.009, val_acc:0.976]
Epoch [109/120    avg_loss:0.012, val_acc:0.976]
Epoch [110/120    avg_loss:0.011, val_acc:0.976]
Epoch [111/120    avg_loss:0.011, val_acc:0.976]
Epoch [112/120    avg_loss:0.013, val_acc:0.976]
Epoch [113/120    avg_loss:0.012, val_acc:0.976]
Epoch [114/120    avg_loss:0.011, val_acc:0.976]
Epoch [115/120    avg_loss:0.011, val_acc:0.976]
Epoch [116/120    avg_loss:0.009, val_acc:0.976]
Epoch [117/120    avg_loss:0.013, val_acc:0.976]
Epoch [118/120    avg_loss:0.007, val_acc:0.976]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.008, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     1     0     0     8     0    25     5]
 [    0     0 17860     0    33     0   184     0    13     0]
 [    0     0     0  1962     0     0     0     0    65     9]
 [    0    35     3     0  2920     0     3     0     8     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4835     0    38     0]
 [    0    23     0     0     0     0     6  1237     0    24]
 [    0    28     0    38    38     0     0     0  3464     3]
 [    0     1     0     0     0     6     0     0     0   912]]

Accuracy:
98.54192273395513

F1 scores:
[       nan 0.99024164 0.99338117 0.97200892 0.9793728  0.99770642
 0.97538834 0.97902651 0.96436526 0.9728    ]

Kappa:
0.9807215537015589
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65542bf780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.365, val_acc:0.685]
Epoch [2/120    avg_loss:0.747, val_acc:0.695]
Epoch [3/120    avg_loss:0.576, val_acc:0.675]
Epoch [4/120    avg_loss:0.445, val_acc:0.849]
Epoch [5/120    avg_loss:0.432, val_acc:0.838]
Epoch [6/120    avg_loss:0.390, val_acc:0.802]
Epoch [7/120    avg_loss:0.338, val_acc:0.832]
Epoch [8/120    avg_loss:0.364, val_acc:0.837]
Epoch [9/120    avg_loss:0.275, val_acc:0.882]
Epoch [10/120    avg_loss:0.270, val_acc:0.837]
Epoch [11/120    avg_loss:0.220, val_acc:0.869]
Epoch [12/120    avg_loss:0.222, val_acc:0.851]
Epoch [13/120    avg_loss:0.182, val_acc:0.931]
Epoch [14/120    avg_loss:0.155, val_acc:0.878]
Epoch [15/120    avg_loss:0.140, val_acc:0.884]
Epoch [16/120    avg_loss:0.183, val_acc:0.920]
Epoch [17/120    avg_loss:0.153, val_acc:0.839]
Epoch [18/120    avg_loss:0.165, val_acc:0.946]
Epoch [19/120    avg_loss:0.132, val_acc:0.956]
Epoch [20/120    avg_loss:0.132, val_acc:0.902]
Epoch [21/120    avg_loss:0.119, val_acc:0.941]
Epoch [22/120    avg_loss:0.128, val_acc:0.910]
Epoch [23/120    avg_loss:0.123, val_acc:0.945]
Epoch [24/120    avg_loss:0.106, val_acc:0.897]
Epoch [25/120    avg_loss:0.056, val_acc:0.920]
Epoch [26/120    avg_loss:0.072, val_acc:0.946]
Epoch [27/120    avg_loss:0.085, val_acc:0.959]
Epoch [28/120    avg_loss:0.089, val_acc:0.944]
Epoch [29/120    avg_loss:0.054, val_acc:0.948]
Epoch [30/120    avg_loss:0.181, val_acc:0.949]
Epoch [31/120    avg_loss:0.075, val_acc:0.953]
Epoch [32/120    avg_loss:0.085, val_acc:0.946]
Epoch [33/120    avg_loss:0.076, val_acc:0.881]
Epoch [34/120    avg_loss:0.089, val_acc:0.950]
Epoch [35/120    avg_loss:0.080, val_acc:0.953]
Epoch [36/120    avg_loss:0.084, val_acc:0.920]
Epoch [37/120    avg_loss:0.088, val_acc:0.954]
Epoch [38/120    avg_loss:0.086, val_acc:0.955]
Epoch [39/120    avg_loss:0.045, val_acc:0.972]
Epoch [40/120    avg_loss:0.066, val_acc:0.970]
Epoch [41/120    avg_loss:0.035, val_acc:0.975]
Epoch [42/120    avg_loss:0.032, val_acc:0.965]
Epoch [43/120    avg_loss:0.037, val_acc:0.953]
Epoch [44/120    avg_loss:0.052, val_acc:0.975]
Epoch [45/120    avg_loss:0.049, val_acc:0.956]
Epoch [46/120    avg_loss:0.042, val_acc:0.964]
Epoch [47/120    avg_loss:0.032, val_acc:0.965]
Epoch [48/120    avg_loss:0.027, val_acc:0.961]
Epoch [49/120    avg_loss:0.049, val_acc:0.916]
Epoch [50/120    avg_loss:0.042, val_acc:0.951]
Epoch [51/120    avg_loss:0.022, val_acc:0.978]
Epoch [52/120    avg_loss:0.019, val_acc:0.966]
Epoch [53/120    avg_loss:0.018, val_acc:0.964]
Epoch [54/120    avg_loss:0.024, val_acc:0.978]
Epoch [55/120    avg_loss:0.026, val_acc:0.985]
Epoch [56/120    avg_loss:0.028, val_acc:0.968]
Epoch [57/120    avg_loss:0.034, val_acc:0.977]
Epoch [58/120    avg_loss:0.033, val_acc:0.961]
Epoch [59/120    avg_loss:0.020, val_acc:0.969]
Epoch [60/120    avg_loss:0.015, val_acc:0.983]
Epoch [61/120    avg_loss:0.066, val_acc:0.911]
Epoch [62/120    avg_loss:0.062, val_acc:0.920]
Epoch [63/120    avg_loss:0.024, val_acc:0.986]
Epoch [64/120    avg_loss:0.016, val_acc:0.973]
Epoch [65/120    avg_loss:0.011, val_acc:0.975]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.020, val_acc:0.983]
Epoch [68/120    avg_loss:0.019, val_acc:0.980]
Epoch [69/120    avg_loss:0.018, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.988]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.013, val_acc:0.986]
Epoch [73/120    avg_loss:0.020, val_acc:0.988]
Epoch [74/120    avg_loss:0.011, val_acc:0.983]
Epoch [75/120    avg_loss:0.013, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.014, val_acc:0.969]
Epoch [78/120    avg_loss:0.009, val_acc:0.988]
Epoch [79/120    avg_loss:0.014, val_acc:0.987]
Epoch [80/120    avg_loss:0.015, val_acc:0.976]
Epoch [81/120    avg_loss:0.042, val_acc:0.974]
Epoch [82/120    avg_loss:0.013, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.974]
Epoch [84/120    avg_loss:0.013, val_acc:0.981]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.989]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.991]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.006, val_acc:0.989]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.991]
Epoch [115/120    avg_loss:0.008, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6335     0     8     0     0    15    12    61     1]
 [    0     0 18049     0    18     0    21     0     2     0]
 [    0     2     0  1907     0     0     0     0   120     7]
 [    0    13     5     0  2944     0     3     0     4     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     2     0     0  4871     0     3     0]
 [    0     6     0     0     0     0     1  1278     1     4]
 [    0     3     0    46    35     0     0     0  3487     0]
 [    0     0     0     0     5    13     0     0     0   901]]

Accuracy:
98.99742125177741

F1 scores:
[       nan 0.99054022 0.99867205 0.95373843 0.98560429 0.99504384
 0.99519869 0.99069767 0.96206373 0.98201635]

Kappa:
0.9867212228354548
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19170f3710>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.283, val_acc:0.703]
Epoch [2/120    avg_loss:0.679, val_acc:0.757]
Epoch [3/120    avg_loss:0.566, val_acc:0.784]
Epoch [4/120    avg_loss:0.447, val_acc:0.810]
Epoch [5/120    avg_loss:0.395, val_acc:0.822]
Epoch [6/120    avg_loss:0.487, val_acc:0.748]
Epoch [7/120    avg_loss:0.340, val_acc:0.879]
Epoch [8/120    avg_loss:0.271, val_acc:0.868]
Epoch [9/120    avg_loss:0.342, val_acc:0.883]
Epoch [10/120    avg_loss:0.286, val_acc:0.867]
Epoch [11/120    avg_loss:0.285, val_acc:0.885]
Epoch [12/120    avg_loss:0.226, val_acc:0.854]
Epoch [13/120    avg_loss:0.175, val_acc:0.816]
Epoch [14/120    avg_loss:0.202, val_acc:0.887]
Epoch [15/120    avg_loss:0.183, val_acc:0.927]
Epoch [16/120    avg_loss:0.136, val_acc:0.876]
Epoch [17/120    avg_loss:0.183, val_acc:0.910]
Epoch [18/120    avg_loss:0.163, val_acc:0.918]
Epoch [19/120    avg_loss:0.162, val_acc:0.924]
Epoch [20/120    avg_loss:0.128, val_acc:0.944]
Epoch [21/120    avg_loss:0.095, val_acc:0.956]
Epoch [22/120    avg_loss:0.102, val_acc:0.947]
Epoch [23/120    avg_loss:0.092, val_acc:0.940]
Epoch [24/120    avg_loss:0.125, val_acc:0.931]
Epoch [25/120    avg_loss:0.128, val_acc:0.896]
Epoch [26/120    avg_loss:0.092, val_acc:0.943]
Epoch [27/120    avg_loss:0.080, val_acc:0.951]
Epoch [28/120    avg_loss:0.065, val_acc:0.951]
Epoch [29/120    avg_loss:0.084, val_acc:0.961]
Epoch [30/120    avg_loss:0.047, val_acc:0.962]
Epoch [31/120    avg_loss:0.065, val_acc:0.960]
Epoch [32/120    avg_loss:0.078, val_acc:0.965]
Epoch [33/120    avg_loss:0.079, val_acc:0.946]
Epoch [34/120    avg_loss:0.134, val_acc:0.955]
Epoch [35/120    avg_loss:0.078, val_acc:0.961]
Epoch [36/120    avg_loss:0.079, val_acc:0.962]
Epoch [37/120    avg_loss:0.068, val_acc:0.965]
Epoch [38/120    avg_loss:0.072, val_acc:0.965]
Epoch [39/120    avg_loss:0.055, val_acc:0.953]
Epoch [40/120    avg_loss:0.048, val_acc:0.923]
Epoch [41/120    avg_loss:0.036, val_acc:0.979]
Epoch [42/120    avg_loss:0.017, val_acc:0.981]
Epoch [43/120    avg_loss:0.065, val_acc:0.952]
Epoch [44/120    avg_loss:0.062, val_acc:0.973]
Epoch [45/120    avg_loss:0.041, val_acc:0.973]
Epoch [46/120    avg_loss:0.030, val_acc:0.975]
Epoch [47/120    avg_loss:0.020, val_acc:0.978]
Epoch [48/120    avg_loss:0.017, val_acc:0.979]
Epoch [49/120    avg_loss:0.029, val_acc:0.931]
Epoch [50/120    avg_loss:0.036, val_acc:0.982]
Epoch [51/120    avg_loss:0.067, val_acc:0.951]
Epoch [52/120    avg_loss:0.043, val_acc:0.974]
Epoch [53/120    avg_loss:0.036, val_acc:0.967]
Epoch [54/120    avg_loss:0.032, val_acc:0.968]
Epoch [55/120    avg_loss:0.028, val_acc:0.979]
Epoch [56/120    avg_loss:0.016, val_acc:0.983]
Epoch [57/120    avg_loss:0.029, val_acc:0.984]
Epoch [58/120    avg_loss:0.016, val_acc:0.980]
Epoch [59/120    avg_loss:0.020, val_acc:0.979]
Epoch [60/120    avg_loss:0.017, val_acc:0.963]
Epoch [61/120    avg_loss:0.067, val_acc:0.972]
Epoch [62/120    avg_loss:0.037, val_acc:0.978]
Epoch [63/120    avg_loss:0.024, val_acc:0.984]
Epoch [64/120    avg_loss:0.022, val_acc:0.980]
Epoch [65/120    avg_loss:0.012, val_acc:0.988]
Epoch [66/120    avg_loss:0.026, val_acc:0.896]
Epoch [67/120    avg_loss:0.028, val_acc:0.975]
Epoch [68/120    avg_loss:0.030, val_acc:0.981]
Epoch [69/120    avg_loss:0.015, val_acc:0.977]
Epoch [70/120    avg_loss:0.016, val_acc:0.988]
Epoch [71/120    avg_loss:0.011, val_acc:0.971]
Epoch [72/120    avg_loss:0.013, val_acc:0.989]
Epoch [73/120    avg_loss:0.011, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.012, val_acc:0.979]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.026, val_acc:0.933]
Epoch [79/120    avg_loss:0.053, val_acc:0.975]
Epoch [80/120    avg_loss:0.016, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.951]
Epoch [83/120    avg_loss:0.009, val_acc:0.991]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.985]
Epoch [89/120    avg_loss:0.022, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.985]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.982]
Epoch [98/120    avg_loss:0.008, val_acc:0.975]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.003, val_acc:0.992]
Epoch [103/120    avg_loss:0.010, val_acc:0.988]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.993]
Epoch [113/120    avg_loss:0.003, val_acc:0.993]
Epoch [114/120    avg_loss:0.018, val_acc:0.975]
Epoch [115/120    avg_loss:0.017, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.004, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     3     0     0     0    23    47     0]
 [    0     0 18065     0    13     0     7     0     5     0]
 [    0     5     0  1948     0     0     0     0    77     6]
 [    0    12    10     0  2938     0     3     0     5     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     1     0     0  4865     0     0     0]
 [    0     6     0     0     0     0     0  1284     0     0]
 [    0     8     0    41    44     0     0     0  3475     3]
 [    0     1     0     0    12    10     0     0     0   896]]

Accuracy:
99.1372038657123

F1 scores:
[       nan 0.99181159 0.99870083 0.96698933 0.98277304 0.99618321
 0.99764175 0.98883327 0.96796657 0.98030635]

Kappa:
0.9885687907013108
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74fe0b9710>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.281, val_acc:0.554]
Epoch [2/120    avg_loss:0.673, val_acc:0.734]
Epoch [3/120    avg_loss:0.519, val_acc:0.734]
Epoch [4/120    avg_loss:0.477, val_acc:0.757]
Epoch [5/120    avg_loss:0.411, val_acc:0.846]
Epoch [6/120    avg_loss:0.358, val_acc:0.737]
Epoch [7/120    avg_loss:0.286, val_acc:0.883]
Epoch [8/120    avg_loss:0.257, val_acc:0.882]
Epoch [9/120    avg_loss:0.249, val_acc:0.752]
Epoch [10/120    avg_loss:0.254, val_acc:0.834]
Epoch [11/120    avg_loss:0.191, val_acc:0.912]
Epoch [12/120    avg_loss:0.210, val_acc:0.909]
Epoch [13/120    avg_loss:0.141, val_acc:0.932]
Epoch [14/120    avg_loss:0.149, val_acc:0.934]
Epoch [15/120    avg_loss:0.186, val_acc:0.940]
Epoch [16/120    avg_loss:0.153, val_acc:0.958]
Epoch [17/120    avg_loss:0.121, val_acc:0.925]
Epoch [18/120    avg_loss:0.146, val_acc:0.918]
Epoch [19/120    avg_loss:0.100, val_acc:0.959]
Epoch [20/120    avg_loss:0.111, val_acc:0.932]
Epoch [21/120    avg_loss:0.172, val_acc:0.933]
Epoch [22/120    avg_loss:0.078, val_acc:0.947]
Epoch [23/120    avg_loss:0.102, val_acc:0.916]
Epoch [24/120    avg_loss:0.075, val_acc:0.958]
Epoch [25/120    avg_loss:0.070, val_acc:0.940]
Epoch [26/120    avg_loss:0.060, val_acc:0.952]
Epoch [27/120    avg_loss:0.079, val_acc:0.944]
Epoch [28/120    avg_loss:0.050, val_acc:0.958]
Epoch [29/120    avg_loss:0.046, val_acc:0.966]
Epoch [30/120    avg_loss:0.128, val_acc:0.888]
Epoch [31/120    avg_loss:0.072, val_acc:0.946]
Epoch [32/120    avg_loss:0.065, val_acc:0.882]
Epoch [33/120    avg_loss:0.087, val_acc:0.968]
Epoch [34/120    avg_loss:0.032, val_acc:0.971]
Epoch [35/120    avg_loss:0.043, val_acc:0.959]
Epoch [36/120    avg_loss:0.031, val_acc:0.971]
Epoch [37/120    avg_loss:0.043, val_acc:0.969]
Epoch [38/120    avg_loss:0.023, val_acc:0.964]
Epoch [39/120    avg_loss:0.070, val_acc:0.935]
Epoch [40/120    avg_loss:0.074, val_acc:0.964]
Epoch [41/120    avg_loss:0.092, val_acc:0.935]
Epoch [42/120    avg_loss:0.059, val_acc:0.966]
Epoch [43/120    avg_loss:0.041, val_acc:0.973]
Epoch [44/120    avg_loss:0.020, val_acc:0.958]
Epoch [45/120    avg_loss:0.033, val_acc:0.945]
Epoch [46/120    avg_loss:0.029, val_acc:0.968]
Epoch [47/120    avg_loss:0.029, val_acc:0.973]
Epoch [48/120    avg_loss:0.022, val_acc:0.978]
Epoch [49/120    avg_loss:0.024, val_acc:0.974]
Epoch [50/120    avg_loss:0.033, val_acc:0.978]
Epoch [51/120    avg_loss:0.088, val_acc:0.964]
Epoch [52/120    avg_loss:0.025, val_acc:0.916]
Epoch [53/120    avg_loss:0.062, val_acc:0.972]
Epoch [54/120    avg_loss:0.016, val_acc:0.973]
Epoch [55/120    avg_loss:0.021, val_acc:0.973]
Epoch [56/120    avg_loss:0.029, val_acc:0.977]
Epoch [57/120    avg_loss:0.023, val_acc:0.970]
Epoch [58/120    avg_loss:0.014, val_acc:0.975]
Epoch [59/120    avg_loss:0.021, val_acc:0.978]
Epoch [60/120    avg_loss:0.017, val_acc:0.980]
Epoch [61/120    avg_loss:0.018, val_acc:0.966]
Epoch [62/120    avg_loss:0.013, val_acc:0.973]
Epoch [63/120    avg_loss:0.021, val_acc:0.973]
Epoch [64/120    avg_loss:0.010, val_acc:0.978]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.016, val_acc:0.979]
Epoch [67/120    avg_loss:0.016, val_acc:0.981]
Epoch [68/120    avg_loss:0.013, val_acc:0.978]
Epoch [69/120    avg_loss:0.014, val_acc:0.983]
Epoch [70/120    avg_loss:0.021, val_acc:0.983]
Epoch [71/120    avg_loss:0.012, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.005, val_acc:0.982]
Epoch [74/120    avg_loss:0.016, val_acc:0.978]
Epoch [75/120    avg_loss:0.006, val_acc:0.978]
Epoch [76/120    avg_loss:0.044, val_acc:0.962]
Epoch [77/120    avg_loss:0.045, val_acc:0.974]
Epoch [78/120    avg_loss:0.028, val_acc:0.969]
Epoch [79/120    avg_loss:0.027, val_acc:0.975]
Epoch [80/120    avg_loss:0.013, val_acc:0.977]
Epoch [81/120    avg_loss:0.011, val_acc:0.974]
Epoch [82/120    avg_loss:0.020, val_acc:0.977]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.012, val_acc:0.980]
Epoch [85/120    avg_loss:0.011, val_acc:0.979]
Epoch [86/120    avg_loss:0.006, val_acc:0.981]
Epoch [87/120    avg_loss:0.005, val_acc:0.982]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.006, val_acc:0.983]
Epoch [91/120    avg_loss:0.007, val_acc:0.982]
Epoch [92/120    avg_loss:0.006, val_acc:0.982]
Epoch [93/120    avg_loss:0.004, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.005, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.983]
Epoch [98/120    avg_loss:0.003, val_acc:0.985]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.983]
Epoch [113/120    avg_loss:0.004, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6361     0     4     0     0     3     4    60     0]
 [    0     0 18040     0    12     0    35     0     3     0]
 [    0     2     0  1913     0     0     0     0   114     7]
 [    0    11     3     0  2949     0     3     0     4     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4867     0     8     0]
 [    0     2     0     0     0     0     1  1287     0     0]
 [    0    10     0    38    37     0     0     0  3484     2]
 [    0     0     0     0     2     8     0     0     0   909]]

Accuracy:
99.08900296435543

F1 scores:
[       nan 0.99251053 0.9984503  0.95865698 0.98760884 0.99694423
 0.99458465 0.99728787 0.9618995  0.98858075]

Kappa:
0.9879347323167271
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab44cee7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.271, val_acc:0.560]
Epoch [2/120    avg_loss:0.680, val_acc:0.670]
Epoch [3/120    avg_loss:0.519, val_acc:0.780]
Epoch [4/120    avg_loss:0.540, val_acc:0.834]
Epoch [5/120    avg_loss:0.437, val_acc:0.866]
Epoch [6/120    avg_loss:0.320, val_acc:0.786]
Epoch [7/120    avg_loss:0.333, val_acc:0.859]
Epoch [8/120    avg_loss:0.346, val_acc:0.876]
Epoch [9/120    avg_loss:0.249, val_acc:0.918]
Epoch [10/120    avg_loss:0.240, val_acc:0.900]
Epoch [11/120    avg_loss:0.259, val_acc:0.921]
Epoch [12/120    avg_loss:0.230, val_acc:0.936]
Epoch [13/120    avg_loss:0.155, val_acc:0.883]
Epoch [14/120    avg_loss:0.144, val_acc:0.933]
Epoch [15/120    avg_loss:0.119, val_acc:0.924]
Epoch [16/120    avg_loss:0.197, val_acc:0.942]
Epoch [17/120    avg_loss:0.174, val_acc:0.946]
Epoch [18/120    avg_loss:0.121, val_acc:0.938]
Epoch [19/120    avg_loss:0.144, val_acc:0.938]
Epoch [20/120    avg_loss:0.104, val_acc:0.877]
Epoch [21/120    avg_loss:0.087, val_acc:0.959]
Epoch [22/120    avg_loss:0.080, val_acc:0.933]
Epoch [23/120    avg_loss:0.104, val_acc:0.910]
Epoch [24/120    avg_loss:0.083, val_acc:0.932]
Epoch [25/120    avg_loss:0.131, val_acc:0.924]
Epoch [26/120    avg_loss:0.166, val_acc:0.931]
Epoch [27/120    avg_loss:0.099, val_acc:0.956]
Epoch [28/120    avg_loss:0.080, val_acc:0.963]
Epoch [29/120    avg_loss:0.087, val_acc:0.958]
Epoch [30/120    avg_loss:0.096, val_acc:0.936]
Epoch [31/120    avg_loss:0.102, val_acc:0.938]
Epoch [32/120    avg_loss:0.081, val_acc:0.932]
Epoch [33/120    avg_loss:0.099, val_acc:0.928]
Epoch [34/120    avg_loss:0.113, val_acc:0.976]
Epoch [35/120    avg_loss:0.054, val_acc:0.978]
Epoch [36/120    avg_loss:0.091, val_acc:0.969]
Epoch [37/120    avg_loss:0.045, val_acc:0.965]
Epoch [38/120    avg_loss:0.048, val_acc:0.929]
Epoch [39/120    avg_loss:0.054, val_acc:0.966]
Epoch [40/120    avg_loss:0.035, val_acc:0.942]
Epoch [41/120    avg_loss:0.031, val_acc:0.970]
Epoch [42/120    avg_loss:0.043, val_acc:0.971]
Epoch [43/120    avg_loss:0.028, val_acc:0.976]
Epoch [44/120    avg_loss:0.063, val_acc:0.967]
Epoch [45/120    avg_loss:0.051, val_acc:0.959]
Epoch [46/120    avg_loss:0.030, val_acc:0.939]
Epoch [47/120    avg_loss:0.019, val_acc:0.978]
Epoch [48/120    avg_loss:0.028, val_acc:0.965]
Epoch [49/120    avg_loss:0.023, val_acc:0.962]
Epoch [50/120    avg_loss:0.057, val_acc:0.974]
Epoch [51/120    avg_loss:0.022, val_acc:0.979]
Epoch [52/120    avg_loss:0.024, val_acc:0.975]
Epoch [53/120    avg_loss:0.013, val_acc:0.984]
Epoch [54/120    avg_loss:0.020, val_acc:0.982]
Epoch [55/120    avg_loss:0.051, val_acc:0.983]
Epoch [56/120    avg_loss:0.043, val_acc:0.964]
Epoch [57/120    avg_loss:0.044, val_acc:0.979]
Epoch [58/120    avg_loss:0.013, val_acc:0.976]
Epoch [59/120    avg_loss:0.028, val_acc:0.971]
Epoch [60/120    avg_loss:0.015, val_acc:0.980]
Epoch [61/120    avg_loss:0.027, val_acc:0.985]
Epoch [62/120    avg_loss:0.012, val_acc:0.979]
Epoch [63/120    avg_loss:0.020, val_acc:0.970]
Epoch [64/120    avg_loss:0.029, val_acc:0.977]
Epoch [65/120    avg_loss:0.030, val_acc:0.981]
Epoch [66/120    avg_loss:0.035, val_acc:0.979]
Epoch [67/120    avg_loss:0.023, val_acc:0.942]
Epoch [68/120    avg_loss:0.026, val_acc:0.956]
Epoch [69/120    avg_loss:0.022, val_acc:0.976]
Epoch [70/120    avg_loss:0.035, val_acc:0.982]
Epoch [71/120    avg_loss:0.014, val_acc:0.980]
Epoch [72/120    avg_loss:0.010, val_acc:0.988]
Epoch [73/120    avg_loss:0.010, val_acc:0.981]
Epoch [74/120    avg_loss:0.005, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.011, val_acc:0.970]
Epoch [77/120    avg_loss:0.055, val_acc:0.957]
Epoch [78/120    avg_loss:0.025, val_acc:0.979]
Epoch [79/120    avg_loss:0.057, val_acc:0.943]
Epoch [80/120    avg_loss:0.046, val_acc:0.970]
Epoch [81/120    avg_loss:0.025, val_acc:0.975]
Epoch [82/120    avg_loss:0.017, val_acc:0.983]
Epoch [83/120    avg_loss:0.020, val_acc:0.975]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.989]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.990]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.991]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     1     1     0     8     0    15     4]
 [    0     0 18031     0    35     0    24     0     0     0]
 [    0     2     0  1891     0     0     0     0   138     5]
 [    0     4     6     0  2955     0     3     0     3     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     3     0  4869     0     3     0]
 [    0    17     0     0     0     0     0  1273     0     0]
 [    0    11     0    20    46     0     0     0  3493     1]
 [    0     0     1     0     0    15     0     0     0   903]]

Accuracy:
99.10828332489818

F1 scores:
[       nan 0.99510451 0.99809028 0.95795339 0.98303393 0.99428571
 0.99550194 0.99336715 0.96718815 0.98527005]

Kappa:
0.9881887196140259
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ab2beb780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.280, val_acc:0.629]
Epoch [2/120    avg_loss:0.632, val_acc:0.654]
Epoch [3/120    avg_loss:0.510, val_acc:0.711]
Epoch [4/120    avg_loss:0.463, val_acc:0.818]
Epoch [5/120    avg_loss:0.433, val_acc:0.732]
Epoch [6/120    avg_loss:0.382, val_acc:0.887]
Epoch [7/120    avg_loss:0.382, val_acc:0.841]
Epoch [8/120    avg_loss:0.308, val_acc:0.849]
Epoch [9/120    avg_loss:0.263, val_acc:0.884]
Epoch [10/120    avg_loss:0.228, val_acc:0.907]
Epoch [11/120    avg_loss:0.227, val_acc:0.850]
Epoch [12/120    avg_loss:0.200, val_acc:0.906]
Epoch [13/120    avg_loss:0.206, val_acc:0.914]
Epoch [14/120    avg_loss:0.201, val_acc:0.911]
Epoch [15/120    avg_loss:0.199, val_acc:0.928]
Epoch [16/120    avg_loss:0.167, val_acc:0.902]
Epoch [17/120    avg_loss:0.181, val_acc:0.916]
Epoch [18/120    avg_loss:0.167, val_acc:0.849]
Epoch [19/120    avg_loss:0.120, val_acc:0.937]
Epoch [20/120    avg_loss:0.127, val_acc:0.916]
Epoch [21/120    avg_loss:0.133, val_acc:0.921]
Epoch [22/120    avg_loss:0.127, val_acc:0.940]
Epoch [23/120    avg_loss:0.127, val_acc:0.873]
Epoch [24/120    avg_loss:0.108, val_acc:0.945]
Epoch [25/120    avg_loss:0.079, val_acc:0.930]
Epoch [26/120    avg_loss:0.088, val_acc:0.926]
Epoch [27/120    avg_loss:0.127, val_acc:0.959]
Epoch [28/120    avg_loss:0.091, val_acc:0.925]
Epoch [29/120    avg_loss:0.116, val_acc:0.937]
Epoch [30/120    avg_loss:0.063, val_acc:0.962]
Epoch [31/120    avg_loss:0.054, val_acc:0.958]
Epoch [32/120    avg_loss:0.075, val_acc:0.945]
Epoch [33/120    avg_loss:0.073, val_acc:0.948]
Epoch [34/120    avg_loss:0.046, val_acc:0.934]
Epoch [35/120    avg_loss:0.055, val_acc:0.962]
Epoch [36/120    avg_loss:0.059, val_acc:0.936]
Epoch [37/120    avg_loss:0.049, val_acc:0.969]
Epoch [38/120    avg_loss:0.030, val_acc:0.954]
Epoch [39/120    avg_loss:0.067, val_acc:0.973]
Epoch [40/120    avg_loss:0.043, val_acc:0.964]
Epoch [41/120    avg_loss:0.069, val_acc:0.954]
Epoch [42/120    avg_loss:0.045, val_acc:0.942]
Epoch [43/120    avg_loss:0.034, val_acc:0.965]
Epoch [44/120    avg_loss:0.045, val_acc:0.978]
Epoch [45/120    avg_loss:0.023, val_acc:0.975]
Epoch [46/120    avg_loss:0.021, val_acc:0.973]
Epoch [47/120    avg_loss:0.065, val_acc:0.954]
Epoch [48/120    avg_loss:0.057, val_acc:0.969]
Epoch [49/120    avg_loss:0.023, val_acc:0.973]
Epoch [50/120    avg_loss:0.047, val_acc:0.969]
Epoch [51/120    avg_loss:0.032, val_acc:0.968]
Epoch [52/120    avg_loss:0.020, val_acc:0.972]
Epoch [53/120    avg_loss:0.022, val_acc:0.979]
Epoch [54/120    avg_loss:0.028, val_acc:0.957]
Epoch [55/120    avg_loss:0.060, val_acc:0.965]
Epoch [56/120    avg_loss:0.033, val_acc:0.974]
Epoch [57/120    avg_loss:0.021, val_acc:0.972]
Epoch [58/120    avg_loss:0.031, val_acc:0.976]
Epoch [59/120    avg_loss:0.033, val_acc:0.979]
Epoch [60/120    avg_loss:0.025, val_acc:0.949]
Epoch [61/120    avg_loss:0.025, val_acc:0.956]
Epoch [62/120    avg_loss:0.012, val_acc:0.976]
Epoch [63/120    avg_loss:0.030, val_acc:0.977]
Epoch [64/120    avg_loss:0.049, val_acc:0.942]
Epoch [65/120    avg_loss:0.057, val_acc:0.950]
Epoch [66/120    avg_loss:0.043, val_acc:0.973]
Epoch [67/120    avg_loss:0.020, val_acc:0.978]
Epoch [68/120    avg_loss:0.027, val_acc:0.971]
Epoch [69/120    avg_loss:0.071, val_acc:0.974]
Epoch [70/120    avg_loss:0.055, val_acc:0.970]
Epoch [71/120    avg_loss:0.027, val_acc:0.970]
Epoch [72/120    avg_loss:0.028, val_acc:0.982]
Epoch [73/120    avg_loss:0.031, val_acc:0.967]
Epoch [74/120    avg_loss:0.011, val_acc:0.981]
Epoch [75/120    avg_loss:0.019, val_acc:0.980]
Epoch [76/120    avg_loss:0.020, val_acc:0.978]
Epoch [77/120    avg_loss:0.014, val_acc:0.981]
Epoch [78/120    avg_loss:0.014, val_acc:0.983]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.007, val_acc:0.983]
Epoch [81/120    avg_loss:0.021, val_acc:0.978]
Epoch [82/120    avg_loss:0.014, val_acc:0.975]
Epoch [83/120    avg_loss:0.015, val_acc:0.977]
Epoch [84/120    avg_loss:0.017, val_acc:0.981]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.969]
Epoch [88/120    avg_loss:0.012, val_acc:0.978]
Epoch [89/120    avg_loss:0.012, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.009, val_acc:0.982]
Epoch [92/120    avg_loss:0.024, val_acc:0.981]
Epoch [93/120    avg_loss:0.018, val_acc:0.979]
Epoch [94/120    avg_loss:0.012, val_acc:0.980]
Epoch [95/120    avg_loss:0.010, val_acc:0.984]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.981]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.082, val_acc:0.947]
Epoch [102/120    avg_loss:0.080, val_acc:0.964]
Epoch [103/120    avg_loss:0.017, val_acc:0.984]
Epoch [104/120    avg_loss:0.040, val_acc:0.935]
Epoch [105/120    avg_loss:0.013, val_acc:0.969]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.039, val_acc:0.975]
Epoch [108/120    avg_loss:0.033, val_acc:0.915]
Epoch [109/120    avg_loss:0.142, val_acc:0.978]
Epoch [110/120    avg_loss:0.050, val_acc:0.979]
Epoch [111/120    avg_loss:0.015, val_acc:0.975]
Epoch [112/120    avg_loss:0.020, val_acc:0.979]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.015, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6364     0     1     0     0    13     2    48     4]
 [    0     0 18020     0    18     0    45     0     7     0]
 [    0     0     0  1913     0     0     0     0   116     7]
 [    0     9     6     0  2951     0     1     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4865     0     4     0]
 [    0    19     0     0     0     0     0  1263     0     8]
 [    0     2     0    65    51     0     5     0  3447     1]
 [    0     0     0     0     0    26     0     0     0   893]]

Accuracy:
98.86245872797822

F1 scores:
[       nan 0.99235927 0.99764706 0.95292653 0.98497997 0.99013657
 0.99214847 0.98864971 0.95816539 0.973297  ]

Kappa:
0.9849358855491411
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f02c8bb27b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.271, val_acc:0.672]
Epoch [2/120    avg_loss:0.647, val_acc:0.787]
Epoch [3/120    avg_loss:0.538, val_acc:0.771]
Epoch [4/120    avg_loss:0.438, val_acc:0.856]
Epoch [5/120    avg_loss:0.344, val_acc:0.865]
Epoch [6/120    avg_loss:0.324, val_acc:0.897]
Epoch [7/120    avg_loss:0.259, val_acc:0.916]
Epoch [8/120    avg_loss:0.282, val_acc:0.861]
Epoch [9/120    avg_loss:0.213, val_acc:0.875]
Epoch [10/120    avg_loss:0.172, val_acc:0.762]
Epoch [11/120    avg_loss:0.177, val_acc:0.927]
Epoch [12/120    avg_loss:0.219, val_acc:0.831]
Epoch [13/120    avg_loss:0.175, val_acc:0.898]
Epoch [14/120    avg_loss:0.151, val_acc:0.900]
Epoch [15/120    avg_loss:0.128, val_acc:0.942]
Epoch [16/120    avg_loss:0.116, val_acc:0.908]
Epoch [17/120    avg_loss:0.128, val_acc:0.914]
Epoch [18/120    avg_loss:0.124, val_acc:0.964]
Epoch [19/120    avg_loss:0.152, val_acc:0.951]
Epoch [20/120    avg_loss:0.091, val_acc:0.953]
Epoch [21/120    avg_loss:0.085, val_acc:0.893]
Epoch [22/120    avg_loss:0.074, val_acc:0.971]
Epoch [23/120    avg_loss:0.041, val_acc:0.969]
Epoch [24/120    avg_loss:0.047, val_acc:0.962]
Epoch [25/120    avg_loss:0.088, val_acc:0.950]
Epoch [26/120    avg_loss:0.043, val_acc:0.958]
Epoch [27/120    avg_loss:0.060, val_acc:0.927]
Epoch [28/120    avg_loss:0.064, val_acc:0.957]
Epoch [29/120    avg_loss:0.056, val_acc:0.971]
Epoch [30/120    avg_loss:0.036, val_acc:0.945]
Epoch [31/120    avg_loss:0.039, val_acc:0.951]
Epoch [32/120    avg_loss:0.057, val_acc:0.972]
Epoch [33/120    avg_loss:0.041, val_acc:0.967]
Epoch [34/120    avg_loss:0.045, val_acc:0.975]
Epoch [35/120    avg_loss:0.027, val_acc:0.960]
Epoch [36/120    avg_loss:0.035, val_acc:0.950]
Epoch [37/120    avg_loss:0.035, val_acc:0.973]
Epoch [38/120    avg_loss:0.029, val_acc:0.963]
Epoch [39/120    avg_loss:0.022, val_acc:0.972]
Epoch [40/120    avg_loss:0.018, val_acc:0.959]
Epoch [41/120    avg_loss:0.030, val_acc:0.975]
Epoch [42/120    avg_loss:0.031, val_acc:0.966]
Epoch [43/120    avg_loss:0.035, val_acc:0.964]
Epoch [44/120    avg_loss:0.115, val_acc:0.951]
Epoch [45/120    avg_loss:0.042, val_acc:0.883]
Epoch [46/120    avg_loss:0.026, val_acc:0.942]
Epoch [47/120    avg_loss:0.024, val_acc:0.956]
Epoch [48/120    avg_loss:0.020, val_acc:0.944]
Epoch [49/120    avg_loss:0.045, val_acc:0.973]
Epoch [50/120    avg_loss:0.072, val_acc:0.909]
Epoch [51/120    avg_loss:0.051, val_acc:0.973]
Epoch [52/120    avg_loss:0.026, val_acc:0.969]
Epoch [53/120    avg_loss:0.045, val_acc:0.960]
Epoch [54/120    avg_loss:0.029, val_acc:0.979]
Epoch [55/120    avg_loss:0.030, val_acc:0.973]
Epoch [56/120    avg_loss:0.017, val_acc:0.979]
Epoch [57/120    avg_loss:0.029, val_acc:0.969]
Epoch [58/120    avg_loss:0.011, val_acc:0.987]
Epoch [59/120    avg_loss:0.021, val_acc:0.982]
Epoch [60/120    avg_loss:0.014, val_acc:0.977]
Epoch [61/120    avg_loss:0.015, val_acc:0.970]
Epoch [62/120    avg_loss:0.005, val_acc:0.982]
Epoch [63/120    avg_loss:0.021, val_acc:0.985]
Epoch [64/120    avg_loss:0.053, val_acc:0.935]
Epoch [65/120    avg_loss:0.027, val_acc:0.982]
Epoch [66/120    avg_loss:0.012, val_acc:0.980]
Epoch [67/120    avg_loss:0.011, val_acc:0.983]
Epoch [68/120    avg_loss:0.014, val_acc:0.982]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.030, val_acc:0.985]
Epoch [71/120    avg_loss:0.016, val_acc:0.978]
Epoch [72/120    avg_loss:0.013, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.985]
Epoch [78/120    avg_loss:0.007, val_acc:0.985]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.004, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.003, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     3     0     5     1     9     0]
 [    0     0 17980     0    31     0    72     0     7     0]
 [    0     0     0  2009     0     0     0     0    15    12]
 [    0    30     2     0  2925     0     3     0    10     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     3     0     0  4871     0     1     0]
 [    0     3     0     0     0     0     2  1283     0     2]
 [    0     1     0    24    40     0     1     0  3494    11]
 [    0     0     0     0    14    15     0     0     0   890]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.99596273 0.9968122  0.9867387  0.97744361 0.99428571
 0.99084622 0.996892   0.98325594 0.96949891]

Kappa:
0.9897282914217466
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f221ac967b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.374, val_acc:0.350]
Epoch [2/120    avg_loss:0.772, val_acc:0.603]
Epoch [3/120    avg_loss:0.514, val_acc:0.755]
Epoch [4/120    avg_loss:0.407, val_acc:0.820]
Epoch [5/120    avg_loss:0.361, val_acc:0.838]
Epoch [6/120    avg_loss:0.315, val_acc:0.906]
Epoch [7/120    avg_loss:0.351, val_acc:0.778]
Epoch [8/120    avg_loss:0.294, val_acc:0.887]
Epoch [9/120    avg_loss:0.282, val_acc:0.790]
Epoch [10/120    avg_loss:0.236, val_acc:0.914]
Epoch [11/120    avg_loss:0.203, val_acc:0.926]
Epoch [12/120    avg_loss:0.278, val_acc:0.897]
Epoch [13/120    avg_loss:0.233, val_acc:0.911]
Epoch [14/120    avg_loss:0.151, val_acc:0.894]
Epoch [15/120    avg_loss:0.134, val_acc:0.928]
Epoch [16/120    avg_loss:0.156, val_acc:0.921]
Epoch [17/120    avg_loss:0.126, val_acc:0.930]
Epoch [18/120    avg_loss:0.100, val_acc:0.940]
Epoch [19/120    avg_loss:0.083, val_acc:0.916]
Epoch [20/120    avg_loss:0.104, val_acc:0.943]
Epoch [21/120    avg_loss:0.270, val_acc:0.916]
Epoch [22/120    avg_loss:0.128, val_acc:0.922]
Epoch [23/120    avg_loss:0.102, val_acc:0.902]
Epoch [24/120    avg_loss:0.097, val_acc:0.934]
Epoch [25/120    avg_loss:0.084, val_acc:0.942]
Epoch [26/120    avg_loss:0.095, val_acc:0.923]
Epoch [27/120    avg_loss:0.066, val_acc:0.946]
Epoch [28/120    avg_loss:0.089, val_acc:0.930]
Epoch [29/120    avg_loss:0.076, val_acc:0.952]
Epoch [30/120    avg_loss:0.053, val_acc:0.947]
Epoch [31/120    avg_loss:0.077, val_acc:0.946]
Epoch [32/120    avg_loss:0.072, val_acc:0.950]
Epoch [33/120    avg_loss:0.065, val_acc:0.940]
Epoch [34/120    avg_loss:0.058, val_acc:0.956]
Epoch [35/120    avg_loss:0.041, val_acc:0.926]
Epoch [36/120    avg_loss:0.061, val_acc:0.961]
Epoch [37/120    avg_loss:0.049, val_acc:0.945]
Epoch [38/120    avg_loss:0.048, val_acc:0.951]
Epoch [39/120    avg_loss:0.068, val_acc:0.956]
Epoch [40/120    avg_loss:0.052, val_acc:0.954]
Epoch [41/120    avg_loss:0.049, val_acc:0.974]
Epoch [42/120    avg_loss:0.041, val_acc:0.960]
Epoch [43/120    avg_loss:0.027, val_acc:0.955]
Epoch [44/120    avg_loss:0.020, val_acc:0.964]
Epoch [45/120    avg_loss:0.044, val_acc:0.908]
Epoch [46/120    avg_loss:0.065, val_acc:0.925]
Epoch [47/120    avg_loss:0.038, val_acc:0.928]
Epoch [48/120    avg_loss:0.027, val_acc:0.974]
Epoch [49/120    avg_loss:0.061, val_acc:0.950]
Epoch [50/120    avg_loss:0.042, val_acc:0.899]
Epoch [51/120    avg_loss:0.035, val_acc:0.972]
Epoch [52/120    avg_loss:0.045, val_acc:0.966]
Epoch [53/120    avg_loss:0.019, val_acc:0.955]
Epoch [54/120    avg_loss:0.032, val_acc:0.950]
Epoch [55/120    avg_loss:0.030, val_acc:0.975]
Epoch [56/120    avg_loss:0.023, val_acc:0.959]
Epoch [57/120    avg_loss:0.015, val_acc:0.980]
Epoch [58/120    avg_loss:0.041, val_acc:0.975]
Epoch [59/120    avg_loss:0.028, val_acc:0.976]
Epoch [60/120    avg_loss:0.010, val_acc:0.982]
Epoch [61/120    avg_loss:0.025, val_acc:0.973]
Epoch [62/120    avg_loss:0.033, val_acc:0.978]
Epoch [63/120    avg_loss:0.015, val_acc:0.979]
Epoch [64/120    avg_loss:0.009, val_acc:0.949]
Epoch [65/120    avg_loss:0.022, val_acc:0.973]
Epoch [66/120    avg_loss:0.057, val_acc:0.962]
Epoch [67/120    avg_loss:0.023, val_acc:0.970]
Epoch [68/120    avg_loss:0.037, val_acc:0.970]
Epoch [69/120    avg_loss:0.030, val_acc:0.978]
Epoch [70/120    avg_loss:0.023, val_acc:0.948]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.010, val_acc:0.983]
Epoch [73/120    avg_loss:0.024, val_acc:0.970]
Epoch [74/120    avg_loss:0.024, val_acc:0.978]
Epoch [75/120    avg_loss:0.019, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.976]
Epoch [78/120    avg_loss:0.017, val_acc:0.984]
Epoch [79/120    avg_loss:0.022, val_acc:0.971]
Epoch [80/120    avg_loss:0.018, val_acc:0.981]
Epoch [81/120    avg_loss:0.010, val_acc:0.983]
Epoch [82/120    avg_loss:0.006, val_acc:0.983]
Epoch [83/120    avg_loss:0.020, val_acc:0.980]
Epoch [84/120    avg_loss:0.005, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.004, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.953]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.975]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.011, val_acc:0.926]
Epoch [96/120    avg_loss:0.012, val_acc:0.977]
Epoch [97/120    avg_loss:0.056, val_acc:0.974]
Epoch [98/120    avg_loss:0.021, val_acc:0.983]
Epoch [99/120    avg_loss:0.011, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.969]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.003, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.934]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6360     0    31     0     0     0     0    41     0]
 [    0     0 18044     0    32     0     8     0     6     0]
 [    0     0     0  1991     0     0     1     0    37     7]
 [    0    23     4     0  2916     0     3     0    16    10]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     3     0     0  4869     0     6     0]
 [    0    10     0     0     0     0     0  1274     0     6]
 [    0     1     0    37    42     0     2     0  3488     1]
 [    0     0     0     0     0    12     0     0     0   907]]

Accuracy:
99.18058467693346

F1 scores:
[       nan 0.99173554 0.99861641 0.97169351 0.97819524 0.99504006
 0.99764368 0.99375975 0.97362177 0.9800108 ]

Kappa:
0.9891488127529541
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa79d1527b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.324, val_acc:0.568]
Epoch [2/120    avg_loss:0.636, val_acc:0.660]
Epoch [3/120    avg_loss:0.552, val_acc:0.796]
Epoch [4/120    avg_loss:0.444, val_acc:0.774]
Epoch [5/120    avg_loss:0.406, val_acc:0.747]
Epoch [6/120    avg_loss:0.300, val_acc:0.884]
Epoch [7/120    avg_loss:0.321, val_acc:0.910]
Epoch [8/120    avg_loss:0.284, val_acc:0.871]
Epoch [9/120    avg_loss:0.249, val_acc:0.891]
Epoch [10/120    avg_loss:0.223, val_acc:0.818]
Epoch [11/120    avg_loss:0.183, val_acc:0.896]
Epoch [12/120    avg_loss:0.291, val_acc:0.888]
Epoch [13/120    avg_loss:0.218, val_acc:0.888]
Epoch [14/120    avg_loss:0.163, val_acc:0.922]
Epoch [15/120    avg_loss:0.179, val_acc:0.942]
Epoch [16/120    avg_loss:0.190, val_acc:0.878]
Epoch [17/120    avg_loss:0.121, val_acc:0.895]
Epoch [18/120    avg_loss:0.175, val_acc:0.943]
Epoch [19/120    avg_loss:0.144, val_acc:0.947]
Epoch [20/120    avg_loss:0.107, val_acc:0.965]
Epoch [21/120    avg_loss:0.118, val_acc:0.943]
Epoch [22/120    avg_loss:0.103, val_acc:0.951]
Epoch [23/120    avg_loss:0.092, val_acc:0.959]
Epoch [24/120    avg_loss:0.083, val_acc:0.967]
Epoch [25/120    avg_loss:0.068, val_acc:0.973]
Epoch [26/120    avg_loss:0.081, val_acc:0.958]
Epoch [27/120    avg_loss:0.084, val_acc:0.951]
Epoch [28/120    avg_loss:0.058, val_acc:0.957]
Epoch [29/120    avg_loss:0.053, val_acc:0.953]
Epoch [30/120    avg_loss:0.040, val_acc:0.961]
Epoch [31/120    avg_loss:0.055, val_acc:0.894]
Epoch [32/120    avg_loss:0.086, val_acc:0.947]
Epoch [33/120    avg_loss:0.239, val_acc:0.941]
Epoch [34/120    avg_loss:0.079, val_acc:0.962]
Epoch [35/120    avg_loss:0.063, val_acc:0.956]
Epoch [36/120    avg_loss:0.056, val_acc:0.965]
Epoch [37/120    avg_loss:0.035, val_acc:0.962]
Epoch [38/120    avg_loss:0.052, val_acc:0.959]
Epoch [39/120    avg_loss:0.046, val_acc:0.970]
Epoch [40/120    avg_loss:0.036, val_acc:0.980]
Epoch [41/120    avg_loss:0.023, val_acc:0.980]
Epoch [42/120    avg_loss:0.025, val_acc:0.980]
Epoch [43/120    avg_loss:0.023, val_acc:0.981]
Epoch [44/120    avg_loss:0.027, val_acc:0.981]
Epoch [45/120    avg_loss:0.023, val_acc:0.980]
Epoch [46/120    avg_loss:0.024, val_acc:0.977]
Epoch [47/120    avg_loss:0.017, val_acc:0.981]
Epoch [48/120    avg_loss:0.026, val_acc:0.978]
Epoch [49/120    avg_loss:0.020, val_acc:0.981]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.018, val_acc:0.983]
Epoch [52/120    avg_loss:0.023, val_acc:0.981]
Epoch [53/120    avg_loss:0.015, val_acc:0.982]
Epoch [54/120    avg_loss:0.014, val_acc:0.985]
Epoch [55/120    avg_loss:0.017, val_acc:0.981]
Epoch [56/120    avg_loss:0.017, val_acc:0.981]
Epoch [57/120    avg_loss:0.015, val_acc:0.983]
Epoch [58/120    avg_loss:0.017, val_acc:0.981]
Epoch [59/120    avg_loss:0.019, val_acc:0.984]
Epoch [60/120    avg_loss:0.015, val_acc:0.985]
Epoch [61/120    avg_loss:0.014, val_acc:0.984]
Epoch [62/120    avg_loss:0.016, val_acc:0.985]
Epoch [63/120    avg_loss:0.017, val_acc:0.987]
Epoch [64/120    avg_loss:0.016, val_acc:0.986]
Epoch [65/120    avg_loss:0.014, val_acc:0.987]
Epoch [66/120    avg_loss:0.015, val_acc:0.986]
Epoch [67/120    avg_loss:0.012, val_acc:0.986]
Epoch [68/120    avg_loss:0.023, val_acc:0.986]
Epoch [69/120    avg_loss:0.013, val_acc:0.986]
Epoch [70/120    avg_loss:0.012, val_acc:0.986]
Epoch [71/120    avg_loss:0.015, val_acc:0.983]
Epoch [72/120    avg_loss:0.014, val_acc:0.988]
Epoch [73/120    avg_loss:0.013, val_acc:0.987]
Epoch [74/120    avg_loss:0.016, val_acc:0.987]
Epoch [75/120    avg_loss:0.014, val_acc:0.985]
Epoch [76/120    avg_loss:0.018, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.987]
Epoch [78/120    avg_loss:0.018, val_acc:0.986]
Epoch [79/120    avg_loss:0.012, val_acc:0.986]
Epoch [80/120    avg_loss:0.013, val_acc:0.986]
Epoch [81/120    avg_loss:0.014, val_acc:0.987]
Epoch [82/120    avg_loss:0.012, val_acc:0.987]
Epoch [83/120    avg_loss:0.013, val_acc:0.985]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.012, val_acc:0.987]
Epoch [86/120    avg_loss:0.015, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.009, val_acc:0.987]
Epoch [89/120    avg_loss:0.011, val_acc:0.987]
Epoch [90/120    avg_loss:0.021, val_acc:0.987]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.987]
Epoch [94/120    avg_loss:0.018, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.987]
Epoch [96/120    avg_loss:0.020, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.012, val_acc:0.987]
Epoch [101/120    avg_loss:0.011, val_acc:0.987]
Epoch [102/120    avg_loss:0.009, val_acc:0.987]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.987]
Epoch [105/120    avg_loss:0.013, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.987]
Epoch [108/120    avg_loss:0.015, val_acc:0.987]
Epoch [109/120    avg_loss:0.011, val_acc:0.987]
Epoch [110/120    avg_loss:0.011, val_acc:0.987]
Epoch [111/120    avg_loss:0.011, val_acc:0.987]
Epoch [112/120    avg_loss:0.013, val_acc:0.987]
Epoch [113/120    avg_loss:0.014, val_acc:0.987]
Epoch [114/120    avg_loss:0.011, val_acc:0.987]
Epoch [115/120    avg_loss:0.012, val_acc:0.987]
Epoch [116/120    avg_loss:0.011, val_acc:0.987]
Epoch [117/120    avg_loss:0.012, val_acc:0.987]
Epoch [118/120    avg_loss:0.011, val_acc:0.987]
Epoch [119/120    avg_loss:0.009, val_acc:0.987]
Epoch [120/120    avg_loss:0.011, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6371     0     0     0     0    23     0    38     0]
 [    0     2 18041     0    32     0    12     0     3     0]
 [    0     0     0  1988     0     0     5     0    35     8]
 [    0    36     1     1  2919     0     1     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     5     0     0  4855     0    17     0]
 [    0     9     0     0     0     0     7  1270     0     4]
 [    0     4     0     9    48     0     0     0  3503     7]
 [    0     0     0     0    12     7     0     0     0   900]]

Accuracy:
99.17817463186562

F1 scores:
[       nan 0.99128676 0.99858855 0.98440208 0.97576467 0.99732518
 0.99274103 0.9921875  0.97563014 0.97932535]

Kappa:
0.9891155219456335
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48d15c07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.306, val_acc:0.710]
Epoch [2/120    avg_loss:0.689, val_acc:0.764]
Epoch [3/120    avg_loss:0.511, val_acc:0.759]
Epoch [4/120    avg_loss:0.437, val_acc:0.774]
Epoch [5/120    avg_loss:0.354, val_acc:0.782]
Epoch [6/120    avg_loss:0.416, val_acc:0.717]
Epoch [7/120    avg_loss:0.333, val_acc:0.787]
Epoch [8/120    avg_loss:0.257, val_acc:0.912]
Epoch [9/120    avg_loss:0.244, val_acc:0.840]
Epoch [10/120    avg_loss:0.202, val_acc:0.912]
Epoch [11/120    avg_loss:0.219, val_acc:0.904]
Epoch [12/120    avg_loss:0.218, val_acc:0.869]
Epoch [13/120    avg_loss:0.193, val_acc:0.865]
Epoch [14/120    avg_loss:0.195, val_acc:0.891]
Epoch [15/120    avg_loss:0.163, val_acc:0.875]
Epoch [16/120    avg_loss:0.103, val_acc:0.925]
Epoch [17/120    avg_loss:0.104, val_acc:0.879]
Epoch [18/120    avg_loss:0.095, val_acc:0.948]
Epoch [19/120    avg_loss:0.110, val_acc:0.950]
Epoch [20/120    avg_loss:0.147, val_acc:0.943]
Epoch [21/120    avg_loss:0.112, val_acc:0.912]
Epoch [22/120    avg_loss:0.116, val_acc:0.942]
Epoch [23/120    avg_loss:0.066, val_acc:0.963]
Epoch [24/120    avg_loss:0.061, val_acc:0.935]
Epoch [25/120    avg_loss:0.066, val_acc:0.964]
Epoch [26/120    avg_loss:0.056, val_acc:0.940]
Epoch [27/120    avg_loss:0.063, val_acc:0.963]
Epoch [28/120    avg_loss:0.123, val_acc:0.953]
Epoch [29/120    avg_loss:0.099, val_acc:0.891]
Epoch [30/120    avg_loss:0.072, val_acc:0.923]
Epoch [31/120    avg_loss:0.098, val_acc:0.937]
Epoch [32/120    avg_loss:0.066, val_acc:0.947]
Epoch [33/120    avg_loss:0.043, val_acc:0.959]
Epoch [34/120    avg_loss:0.052, val_acc:0.964]
Epoch [35/120    avg_loss:0.031, val_acc:0.973]
Epoch [36/120    avg_loss:0.063, val_acc:0.968]
Epoch [37/120    avg_loss:0.059, val_acc:0.954]
Epoch [38/120    avg_loss:0.058, val_acc:0.953]
Epoch [39/120    avg_loss:0.049, val_acc:0.953]
Epoch [40/120    avg_loss:0.076, val_acc:0.963]
Epoch [41/120    avg_loss:0.049, val_acc:0.960]
Epoch [42/120    avg_loss:0.047, val_acc:0.977]
Epoch [43/120    avg_loss:0.031, val_acc:0.980]
Epoch [44/120    avg_loss:0.025, val_acc:0.978]
Epoch [45/120    avg_loss:0.023, val_acc:0.972]
Epoch [46/120    avg_loss:0.045, val_acc:0.968]
Epoch [47/120    avg_loss:0.032, val_acc:0.973]
Epoch [48/120    avg_loss:0.041, val_acc:0.973]
Epoch [49/120    avg_loss:0.040, val_acc:0.914]
Epoch [50/120    avg_loss:0.025, val_acc:0.976]
Epoch [51/120    avg_loss:0.070, val_acc:0.937]
Epoch [52/120    avg_loss:0.030, val_acc:0.974]
Epoch [53/120    avg_loss:0.034, val_acc:0.969]
Epoch [54/120    avg_loss:0.028, val_acc:0.978]
Epoch [55/120    avg_loss:0.037, val_acc:0.974]
Epoch [56/120    avg_loss:0.026, val_acc:0.978]
Epoch [57/120    avg_loss:0.021, val_acc:0.979]
Epoch [58/120    avg_loss:0.013, val_acc:0.979]
Epoch [59/120    avg_loss:0.016, val_acc:0.980]
Epoch [60/120    avg_loss:0.015, val_acc:0.981]
Epoch [61/120    avg_loss:0.014, val_acc:0.980]
Epoch [62/120    avg_loss:0.015, val_acc:0.981]
Epoch [63/120    avg_loss:0.010, val_acc:0.980]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.015, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.983]
Epoch [67/120    avg_loss:0.010, val_acc:0.981]
Epoch [68/120    avg_loss:0.010, val_acc:0.983]
Epoch [69/120    avg_loss:0.012, val_acc:0.981]
Epoch [70/120    avg_loss:0.011, val_acc:0.982]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.009, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.982]
Epoch [74/120    avg_loss:0.016, val_acc:0.982]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.982]
Epoch [77/120    avg_loss:0.010, val_acc:0.983]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.007, val_acc:0.983]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.016, val_acc:0.982]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.018, val_acc:0.983]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.008, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.983]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     1     2     0     9     0    55     0]
 [    0     0 17930     0    31     0   125     0     4     0]
 [    0     0     0  2008     0     0     0     0    20     8]
 [    0    11     4     4  2936     0     3     0     9     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     4     0     0  4856     0     7     0]
 [    0     5     0     0     0     0     5  1280     0     0]
 [    0     6     0    37    52     0     4     0  3467     5]
 [    0     0     0     0     6    20     0     0     0   893]]

Accuracy:
98.90824958426722

F1 scores:
[       nan 0.99305718 0.99514361 0.98190709 0.9788298  0.99239544
 0.98299595 0.99610895 0.9721015  0.97595628]

Kappa:
0.9855573659799791
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4563e63898>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.307, val_acc:0.708]
Epoch [2/120    avg_loss:0.680, val_acc:0.754]
Epoch [3/120    avg_loss:0.584, val_acc:0.801]
Epoch [4/120    avg_loss:0.445, val_acc:0.795]
Epoch [5/120    avg_loss:0.363, val_acc:0.863]
Epoch [6/120    avg_loss:0.319, val_acc:0.904]
Epoch [7/120    avg_loss:0.306, val_acc:0.866]
Epoch [8/120    avg_loss:0.232, val_acc:0.882]
Epoch [9/120    avg_loss:0.235, val_acc:0.902]
Epoch [10/120    avg_loss:0.194, val_acc:0.900]
Epoch [11/120    avg_loss:0.274, val_acc:0.826]
Epoch [12/120    avg_loss:0.209, val_acc:0.869]
Epoch [13/120    avg_loss:0.213, val_acc:0.809]
Epoch [14/120    avg_loss:0.185, val_acc:0.947]
Epoch [15/120    avg_loss:0.186, val_acc:0.901]
Epoch [16/120    avg_loss:0.126, val_acc:0.951]
Epoch [17/120    avg_loss:0.124, val_acc:0.930]
Epoch [18/120    avg_loss:0.123, val_acc:0.942]
Epoch [19/120    avg_loss:0.085, val_acc:0.941]
Epoch [20/120    avg_loss:0.087, val_acc:0.955]
Epoch [21/120    avg_loss:0.070, val_acc:0.942]
Epoch [22/120    avg_loss:0.140, val_acc:0.952]
Epoch [23/120    avg_loss:0.124, val_acc:0.887]
Epoch [24/120    avg_loss:0.086, val_acc:0.935]
Epoch [25/120    avg_loss:0.136, val_acc:0.959]
Epoch [26/120    avg_loss:0.100, val_acc:0.945]
Epoch [27/120    avg_loss:0.060, val_acc:0.945]
Epoch [28/120    avg_loss:0.063, val_acc:0.952]
Epoch [29/120    avg_loss:0.052, val_acc:0.939]
Epoch [30/120    avg_loss:0.066, val_acc:0.957]
Epoch [31/120    avg_loss:0.056, val_acc:0.967]
Epoch [32/120    avg_loss:0.041, val_acc:0.968]
Epoch [33/120    avg_loss:0.044, val_acc:0.969]
Epoch [34/120    avg_loss:0.041, val_acc:0.939]
Epoch [35/120    avg_loss:0.044, val_acc:0.968]
Epoch [36/120    avg_loss:0.034, val_acc:0.945]
Epoch [37/120    avg_loss:0.037, val_acc:0.960]
Epoch [38/120    avg_loss:0.073, val_acc:0.957]
Epoch [39/120    avg_loss:0.032, val_acc:0.984]
Epoch [40/120    avg_loss:0.025, val_acc:0.974]
Epoch [41/120    avg_loss:0.037, val_acc:0.983]
Epoch [42/120    avg_loss:0.017, val_acc:0.978]
Epoch [43/120    avg_loss:0.031, val_acc:0.979]
Epoch [44/120    avg_loss:0.027, val_acc:0.985]
Epoch [45/120    avg_loss:0.019, val_acc:0.984]
Epoch [46/120    avg_loss:0.037, val_acc:0.953]
Epoch [47/120    avg_loss:0.044, val_acc:0.976]
Epoch [48/120    avg_loss:0.029, val_acc:0.964]
Epoch [49/120    avg_loss:0.029, val_acc:0.979]
Epoch [50/120    avg_loss:0.076, val_acc:0.967]
Epoch [51/120    avg_loss:0.030, val_acc:0.972]
Epoch [52/120    avg_loss:0.027, val_acc:0.976]
Epoch [53/120    avg_loss:0.026, val_acc:0.985]
Epoch [54/120    avg_loss:0.021, val_acc:0.983]
Epoch [55/120    avg_loss:0.030, val_acc:0.968]
Epoch [56/120    avg_loss:0.033, val_acc:0.977]
Epoch [57/120    avg_loss:0.035, val_acc:0.978]
Epoch [58/120    avg_loss:0.012, val_acc:0.978]
Epoch [59/120    avg_loss:0.016, val_acc:0.981]
Epoch [60/120    avg_loss:0.012, val_acc:0.988]
Epoch [61/120    avg_loss:0.016, val_acc:0.961]
Epoch [62/120    avg_loss:0.011, val_acc:0.981]
Epoch [63/120    avg_loss:0.011, val_acc:0.986]
Epoch [64/120    avg_loss:0.179, val_acc:0.933]
Epoch [65/120    avg_loss:0.153, val_acc:0.969]
Epoch [66/120    avg_loss:0.061, val_acc:0.956]
Epoch [67/120    avg_loss:0.031, val_acc:0.978]
Epoch [68/120    avg_loss:0.022, val_acc:0.965]
Epoch [69/120    avg_loss:0.016, val_acc:0.983]
Epoch [70/120    avg_loss:0.023, val_acc:0.986]
Epoch [71/120    avg_loss:0.024, val_acc:0.970]
Epoch [72/120    avg_loss:0.019, val_acc:0.984]
Epoch [73/120    avg_loss:0.024, val_acc:0.974]
Epoch [74/120    avg_loss:0.016, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.984]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.010, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.009, val_acc:0.989]
Epoch [87/120    avg_loss:0.011, val_acc:0.988]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     9     0     0    17     1    20     0]
 [    0     2 17863     0    41     0   178     0     6     0]
 [    0     0     0  1975     0     0     0     0    53     8]
 [    0    26     7     5  2915     0     3     0    15     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     7     0     0     0     0     0  1281     0     2]
 [    0     0     0    68    43     0     0     0  3460     0]
 [    0     0     0     0    14    20     0     0     0   885]]

Accuracy:
98.68411539295785

F1 scores:
[       nan 0.99361967 0.99349277 0.9650623  0.97410192 0.99239544
 0.9801085  0.99611198 0.97122807 0.97520661]

Kappa:
0.982603801258019
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89e3d398d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.380, val_acc:0.653]
Epoch [2/120    avg_loss:0.813, val_acc:0.603]
Epoch [3/120    avg_loss:0.536, val_acc:0.764]
Epoch [4/120    avg_loss:0.465, val_acc:0.714]
Epoch [5/120    avg_loss:0.378, val_acc:0.787]
Epoch [6/120    avg_loss:0.339, val_acc:0.892]
Epoch [7/120    avg_loss:0.273, val_acc:0.887]
Epoch [8/120    avg_loss:0.295, val_acc:0.886]
Epoch [9/120    avg_loss:0.209, val_acc:0.909]
Epoch [10/120    avg_loss:0.189, val_acc:0.855]
Epoch [11/120    avg_loss:0.181, val_acc:0.826]
Epoch [12/120    avg_loss:0.215, val_acc:0.895]
Epoch [13/120    avg_loss:0.161, val_acc:0.887]
Epoch [14/120    avg_loss:0.157, val_acc:0.948]
Epoch [15/120    avg_loss:0.105, val_acc:0.939]
Epoch [16/120    avg_loss:0.105, val_acc:0.958]
Epoch [17/120    avg_loss:0.134, val_acc:0.955]
Epoch [18/120    avg_loss:0.108, val_acc:0.957]
Epoch [19/120    avg_loss:0.070, val_acc:0.945]
Epoch [20/120    avg_loss:0.105, val_acc:0.936]
Epoch [21/120    avg_loss:0.086, val_acc:0.963]
Epoch [22/120    avg_loss:0.063, val_acc:0.961]
Epoch [23/120    avg_loss:0.076, val_acc:0.967]
Epoch [24/120    avg_loss:0.046, val_acc:0.972]
Epoch [25/120    avg_loss:0.097, val_acc:0.945]
Epoch [26/120    avg_loss:0.058, val_acc:0.963]
Epoch [27/120    avg_loss:0.054, val_acc:0.953]
Epoch [28/120    avg_loss:0.109, val_acc:0.946]
Epoch [29/120    avg_loss:0.066, val_acc:0.963]
Epoch [30/120    avg_loss:0.061, val_acc:0.968]
Epoch [31/120    avg_loss:0.025, val_acc:0.976]
Epoch [32/120    avg_loss:0.051, val_acc:0.971]
Epoch [33/120    avg_loss:0.044, val_acc:0.986]
Epoch [34/120    avg_loss:0.031, val_acc:0.982]
Epoch [35/120    avg_loss:0.075, val_acc:0.914]
Epoch [36/120    avg_loss:0.040, val_acc:0.963]
Epoch [37/120    avg_loss:0.063, val_acc:0.949]
Epoch [38/120    avg_loss:0.047, val_acc:0.964]
Epoch [39/120    avg_loss:0.027, val_acc:0.976]
Epoch [40/120    avg_loss:0.042, val_acc:0.986]
Epoch [41/120    avg_loss:0.028, val_acc:0.977]
Epoch [42/120    avg_loss:0.042, val_acc:0.950]
Epoch [43/120    avg_loss:0.117, val_acc:0.947]
Epoch [44/120    avg_loss:0.052, val_acc:0.975]
Epoch [45/120    avg_loss:0.041, val_acc:0.960]
Epoch [46/120    avg_loss:0.036, val_acc:0.979]
Epoch [47/120    avg_loss:0.039, val_acc:0.979]
Epoch [48/120    avg_loss:0.025, val_acc:0.981]
Epoch [49/120    avg_loss:0.019, val_acc:0.978]
Epoch [50/120    avg_loss:0.016, val_acc:0.985]
Epoch [51/120    avg_loss:0.017, val_acc:0.988]
Epoch [52/120    avg_loss:0.017, val_acc:0.977]
Epoch [53/120    avg_loss:0.026, val_acc:0.985]
Epoch [54/120    avg_loss:0.021, val_acc:0.985]
Epoch [55/120    avg_loss:0.021, val_acc:0.983]
Epoch [56/120    avg_loss:0.027, val_acc:0.978]
Epoch [57/120    avg_loss:0.022, val_acc:0.989]
Epoch [58/120    avg_loss:0.010, val_acc:0.991]
Epoch [59/120    avg_loss:0.024, val_acc:0.983]
Epoch [60/120    avg_loss:0.015, val_acc:0.990]
Epoch [61/120    avg_loss:0.011, val_acc:0.991]
Epoch [62/120    avg_loss:0.020, val_acc:0.925]
Epoch [63/120    avg_loss:0.031, val_acc:0.975]
Epoch [64/120    avg_loss:0.011, val_acc:0.988]
Epoch [65/120    avg_loss:0.082, val_acc:0.885]
Epoch [66/120    avg_loss:0.078, val_acc:0.959]
Epoch [67/120    avg_loss:0.024, val_acc:0.986]
Epoch [68/120    avg_loss:0.017, val_acc:0.988]
Epoch [69/120    avg_loss:0.010, val_acc:0.949]
Epoch [70/120    avg_loss:0.013, val_acc:0.981]
Epoch [71/120    avg_loss:0.051, val_acc:0.976]
Epoch [72/120    avg_loss:0.017, val_acc:0.988]
Epoch [73/120    avg_loss:0.021, val_acc:0.967]
Epoch [74/120    avg_loss:0.022, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.989]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.010, val_acc:0.990]
Epoch [78/120    avg_loss:0.008, val_acc:0.990]
Epoch [79/120    avg_loss:0.011, val_acc:0.988]
Epoch [80/120    avg_loss:0.007, val_acc:0.989]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.008, val_acc:0.993]
Epoch [83/120    avg_loss:0.006, val_acc:0.993]
Epoch [84/120    avg_loss:0.006, val_acc:0.993]
Epoch [85/120    avg_loss:0.006, val_acc:0.993]
Epoch [86/120    avg_loss:0.009, val_acc:0.993]
Epoch [87/120    avg_loss:0.006, val_acc:0.993]
Epoch [88/120    avg_loss:0.007, val_acc:0.993]
Epoch [89/120    avg_loss:0.006, val_acc:0.993]
Epoch [90/120    avg_loss:0.006, val_acc:0.993]
Epoch [91/120    avg_loss:0.006, val_acc:0.994]
Epoch [92/120    avg_loss:0.008, val_acc:0.993]
Epoch [93/120    avg_loss:0.007, val_acc:0.993]
Epoch [94/120    avg_loss:0.006, val_acc:0.993]
Epoch [95/120    avg_loss:0.010, val_acc:0.992]
Epoch [96/120    avg_loss:0.010, val_acc:0.993]
Epoch [97/120    avg_loss:0.007, val_acc:0.993]
Epoch [98/120    avg_loss:0.005, val_acc:0.993]
Epoch [99/120    avg_loss:0.007, val_acc:0.996]
Epoch [100/120    avg_loss:0.006, val_acc:0.993]
Epoch [101/120    avg_loss:0.007, val_acc:0.993]
Epoch [102/120    avg_loss:0.007, val_acc:0.993]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.007, val_acc:0.993]
Epoch [105/120    avg_loss:0.006, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.993]
Epoch [107/120    avg_loss:0.004, val_acc:0.993]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.007, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6371     0     0     1     0    20     2    35     3]
 [    0     0 18033     0    16     0    41     0     0     0]
 [    0     0     0  1985     0     0     0     0    43     8]
 [    0    11     7     0  2937     0     1     0    15     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4862     0    15     0]
 [    0     4     0     0     0     0     2  1280     0     4]
 [    0     5     0    10    43     0     0     0  3509     4]
 [    0     0     0     0     8    17     0     0     0   894]]

Accuracy:
99.23601571349384

F1 scores:
[       nan 0.99368323 0.99822862 0.98462302 0.98276727 0.99352874
 0.99184007 0.99533437 0.97634947 0.97545008]

Kappa:
0.9898825548396234
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38927ec828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.325, val_acc:0.377]
Epoch [2/120    avg_loss:0.715, val_acc:0.642]
Epoch [3/120    avg_loss:0.515, val_acc:0.731]
Epoch [4/120    avg_loss:0.531, val_acc:0.691]
Epoch [5/120    avg_loss:0.421, val_acc:0.742]
Epoch [6/120    avg_loss:0.373, val_acc:0.769]
Epoch [7/120    avg_loss:0.307, val_acc:0.861]
Epoch [8/120    avg_loss:0.287, val_acc:0.893]
Epoch [9/120    avg_loss:0.285, val_acc:0.865]
Epoch [10/120    avg_loss:0.257, val_acc:0.822]
Epoch [11/120    avg_loss:0.202, val_acc:0.875]
Epoch [12/120    avg_loss:0.192, val_acc:0.915]
Epoch [13/120    avg_loss:0.189, val_acc:0.926]
Epoch [14/120    avg_loss:0.133, val_acc:0.933]
Epoch [15/120    avg_loss:0.214, val_acc:0.945]
Epoch [16/120    avg_loss:0.171, val_acc:0.935]
Epoch [17/120    avg_loss:0.129, val_acc:0.948]
Epoch [18/120    avg_loss:0.184, val_acc:0.895]
Epoch [19/120    avg_loss:0.116, val_acc:0.940]
Epoch [20/120    avg_loss:0.103, val_acc:0.930]
Epoch [21/120    avg_loss:0.086, val_acc:0.957]
Epoch [22/120    avg_loss:0.075, val_acc:0.938]
Epoch [23/120    avg_loss:0.092, val_acc:0.952]
Epoch [24/120    avg_loss:0.051, val_acc:0.964]
Epoch [25/120    avg_loss:0.064, val_acc:0.954]
Epoch [26/120    avg_loss:0.053, val_acc:0.971]
Epoch [27/120    avg_loss:0.073, val_acc:0.969]
Epoch [28/120    avg_loss:0.055, val_acc:0.969]
Epoch [29/120    avg_loss:0.039, val_acc:0.964]
Epoch [30/120    avg_loss:0.037, val_acc:0.972]
Epoch [31/120    avg_loss:0.063, val_acc:0.963]
Epoch [32/120    avg_loss:0.045, val_acc:0.948]
Epoch [33/120    avg_loss:0.038, val_acc:0.965]
Epoch [34/120    avg_loss:0.039, val_acc:0.965]
Epoch [35/120    avg_loss:0.075, val_acc:0.957]
Epoch [36/120    avg_loss:0.085, val_acc:0.960]
Epoch [37/120    avg_loss:0.064, val_acc:0.970]
Epoch [38/120    avg_loss:0.052, val_acc:0.965]
Epoch [39/120    avg_loss:0.026, val_acc:0.956]
Epoch [40/120    avg_loss:0.028, val_acc:0.976]
Epoch [41/120    avg_loss:0.039, val_acc:0.956]
Epoch [42/120    avg_loss:0.044, val_acc:0.965]
Epoch [43/120    avg_loss:0.033, val_acc:0.978]
Epoch [44/120    avg_loss:0.024, val_acc:0.980]
Epoch [45/120    avg_loss:0.038, val_acc:0.972]
Epoch [46/120    avg_loss:0.049, val_acc:0.887]
Epoch [47/120    avg_loss:0.035, val_acc:0.961]
Epoch [48/120    avg_loss:0.056, val_acc:0.955]
Epoch [49/120    avg_loss:0.035, val_acc:0.923]
Epoch [50/120    avg_loss:0.036, val_acc:0.982]
Epoch [51/120    avg_loss:0.020, val_acc:0.979]
Epoch [52/120    avg_loss:0.025, val_acc:0.976]
Epoch [53/120    avg_loss:0.011, val_acc:0.980]
Epoch [54/120    avg_loss:0.016, val_acc:0.983]
Epoch [55/120    avg_loss:0.016, val_acc:0.983]
Epoch [56/120    avg_loss:0.021, val_acc:0.982]
Epoch [57/120    avg_loss:0.015, val_acc:0.978]
Epoch [58/120    avg_loss:0.018, val_acc:0.978]
Epoch [59/120    avg_loss:0.010, val_acc:0.987]
Epoch [60/120    avg_loss:0.012, val_acc:0.985]
Epoch [61/120    avg_loss:0.017, val_acc:0.983]
Epoch [62/120    avg_loss:0.010, val_acc:0.966]
Epoch [63/120    avg_loss:0.010, val_acc:0.982]
Epoch [64/120    avg_loss:0.019, val_acc:0.982]
Epoch [65/120    avg_loss:0.008, val_acc:0.987]
Epoch [66/120    avg_loss:0.132, val_acc:0.902]
Epoch [67/120    avg_loss:0.105, val_acc:0.971]
Epoch [68/120    avg_loss:0.036, val_acc:0.980]
Epoch [69/120    avg_loss:0.044, val_acc:0.949]
Epoch [70/120    avg_loss:0.035, val_acc:0.982]
Epoch [71/120    avg_loss:0.019, val_acc:0.980]
Epoch [72/120    avg_loss:0.016, val_acc:0.974]
Epoch [73/120    avg_loss:0.013, val_acc:0.987]
Epoch [74/120    avg_loss:0.012, val_acc:0.983]
Epoch [75/120    avg_loss:0.011, val_acc:0.980]
Epoch [76/120    avg_loss:0.024, val_acc:0.959]
Epoch [77/120    avg_loss:0.023, val_acc:0.983]
Epoch [78/120    avg_loss:0.012, val_acc:0.984]
Epoch [79/120    avg_loss:0.032, val_acc:0.966]
Epoch [80/120    avg_loss:0.024, val_acc:0.978]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.025, val_acc:0.975]
Epoch [83/120    avg_loss:0.026, val_acc:0.983]
Epoch [84/120    avg_loss:0.040, val_acc:0.900]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.012, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     1     1     0    17     0    54     0]
 [    0     0 17991     0     8     0    86     0     5     0]
 [    0     0     0  2009     0     0     0     0    20     7]
 [    0    18     8     1  2931     0     1     0    10     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4870     0     5     0]
 [    0     0     0     0     0     0     2  1282     0     6]
 [    0     4     0    39    49     0     0     0  3475     4]
 [    0     0     0     0    15    20     0     0     0   884]]

Accuracy:
99.06731255874485

F1 scores:
[       nan 0.99258566 0.99695223 0.98335781 0.98092369 0.99239544
 0.98843109 0.99688958 0.97338936 0.96982995]

Kappa:
0.9876539034334975
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2b73ec5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.337, val_acc:0.555]
Epoch [2/120    avg_loss:0.671, val_acc:0.646]
Epoch [3/120    avg_loss:0.597, val_acc:0.714]
Epoch [4/120    avg_loss:0.485, val_acc:0.800]
Epoch [5/120    avg_loss:0.430, val_acc:0.791]
Epoch [6/120    avg_loss:0.349, val_acc:0.846]
Epoch [7/120    avg_loss:0.288, val_acc:0.748]
Epoch [8/120    avg_loss:0.366, val_acc:0.767]
Epoch [9/120    avg_loss:0.275, val_acc:0.838]
Epoch [10/120    avg_loss:0.237, val_acc:0.839]
Epoch [11/120    avg_loss:0.223, val_acc:0.877]
Epoch [12/120    avg_loss:0.177, val_acc:0.917]
Epoch [13/120    avg_loss:0.185, val_acc:0.925]
Epoch [14/120    avg_loss:0.168, val_acc:0.936]
Epoch [15/120    avg_loss:0.125, val_acc:0.916]
Epoch [16/120    avg_loss:0.199, val_acc:0.924]
Epoch [17/120    avg_loss:0.136, val_acc:0.914]
Epoch [18/120    avg_loss:0.159, val_acc:0.937]
Epoch [19/120    avg_loss:0.108, val_acc:0.912]
Epoch [20/120    avg_loss:0.096, val_acc:0.933]
Epoch [21/120    avg_loss:0.074, val_acc:0.906]
Epoch [22/120    avg_loss:0.120, val_acc:0.942]
Epoch [23/120    avg_loss:0.139, val_acc:0.942]
Epoch [24/120    avg_loss:0.077, val_acc:0.933]
Epoch [25/120    avg_loss:0.051, val_acc:0.965]
Epoch [26/120    avg_loss:0.069, val_acc:0.971]
Epoch [27/120    avg_loss:0.058, val_acc:0.932]
Epoch [28/120    avg_loss:0.040, val_acc:0.971]
Epoch [29/120    avg_loss:0.044, val_acc:0.954]
Epoch [30/120    avg_loss:0.068, val_acc:0.969]
Epoch [31/120    avg_loss:0.079, val_acc:0.954]
Epoch [32/120    avg_loss:0.097, val_acc:0.968]
Epoch [33/120    avg_loss:0.058, val_acc:0.942]
Epoch [34/120    avg_loss:0.059, val_acc:0.922]
Epoch [35/120    avg_loss:0.123, val_acc:0.964]
Epoch [36/120    avg_loss:0.088, val_acc:0.959]
Epoch [37/120    avg_loss:0.066, val_acc:0.937]
Epoch [38/120    avg_loss:0.043, val_acc:0.958]
Epoch [39/120    avg_loss:0.045, val_acc:0.971]
Epoch [40/120    avg_loss:0.030, val_acc:0.976]
Epoch [41/120    avg_loss:0.070, val_acc:0.959]
Epoch [42/120    avg_loss:0.054, val_acc:0.971]
Epoch [43/120    avg_loss:0.035, val_acc:0.959]
Epoch [44/120    avg_loss:0.024, val_acc:0.983]
Epoch [45/120    avg_loss:0.030, val_acc:0.973]
Epoch [46/120    avg_loss:0.029, val_acc:0.976]
Epoch [47/120    avg_loss:0.016, val_acc:0.977]
Epoch [48/120    avg_loss:0.017, val_acc:0.975]
Epoch [49/120    avg_loss:0.038, val_acc:0.978]
Epoch [50/120    avg_loss:0.020, val_acc:0.982]
Epoch [51/120    avg_loss:0.016, val_acc:0.976]
Epoch [52/120    avg_loss:0.012, val_acc:0.977]
Epoch [53/120    avg_loss:0.029, val_acc:0.986]
Epoch [54/120    avg_loss:0.015, val_acc:0.978]
Epoch [55/120    avg_loss:0.026, val_acc:0.970]
Epoch [56/120    avg_loss:0.161, val_acc:0.958]
Epoch [57/120    avg_loss:0.068, val_acc:0.968]
Epoch [58/120    avg_loss:0.050, val_acc:0.966]
Epoch [59/120    avg_loss:0.049, val_acc:0.935]
Epoch [60/120    avg_loss:0.020, val_acc:0.974]
Epoch [61/120    avg_loss:0.017, val_acc:0.977]
Epoch [62/120    avg_loss:0.014, val_acc:0.983]
Epoch [63/120    avg_loss:0.034, val_acc:0.981]
Epoch [64/120    avg_loss:0.018, val_acc:0.980]
Epoch [65/120    avg_loss:0.021, val_acc:0.981]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.008, val_acc:0.983]
Epoch [68/120    avg_loss:0.008, val_acc:0.983]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.007, val_acc:0.984]
Epoch [74/120    avg_loss:0.005, val_acc:0.983]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.983]
Epoch [77/120    avg_loss:0.006, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.983]
Epoch [80/120    avg_loss:0.006, val_acc:0.983]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.006, val_acc:0.984]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.005, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.013, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.011, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6334     0     0     0     0    16    20    62     0]
 [    0     0 17944     0    33     0   110     0     3     0]
 [    0     6     0  1974     0     0     0     0    46    10]
 [    0    19     8     0  2933     0     0     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4867     0     9     0]
 [    0     5     0     0     0     0     2  1281     0     2]
 [    0    10     0     7    56     0     0     0  3496     2]
 [    0     0     0     0    14    18     0     0     0   887]]

Accuracy:
98.86245872797822

F1 scores:
[       nan 0.9892238  0.99572721 0.98233391 0.97636485 0.99315068
 0.9859212  0.98880741 0.97124601 0.97472527]

Kappa:
0.9849499617729627
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7c834ab828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.314, val_acc:0.725]
Epoch [2/120    avg_loss:0.690, val_acc:0.757]
Epoch [3/120    avg_loss:0.508, val_acc:0.805]
Epoch [4/120    avg_loss:0.517, val_acc:0.767]
Epoch [5/120    avg_loss:0.386, val_acc:0.674]
Epoch [6/120    avg_loss:0.414, val_acc:0.838]
Epoch [7/120    avg_loss:0.296, val_acc:0.807]
Epoch [8/120    avg_loss:0.214, val_acc:0.907]
Epoch [9/120    avg_loss:0.222, val_acc:0.880]
Epoch [10/120    avg_loss:0.202, val_acc:0.891]
Epoch [11/120    avg_loss:0.229, val_acc:0.912]
Epoch [12/120    avg_loss:0.153, val_acc:0.938]
Epoch [13/120    avg_loss:0.191, val_acc:0.900]
Epoch [14/120    avg_loss:0.130, val_acc:0.915]
Epoch [15/120    avg_loss:0.177, val_acc:0.941]
Epoch [16/120    avg_loss:0.154, val_acc:0.943]
Epoch [17/120    avg_loss:0.100, val_acc:0.912]
Epoch [18/120    avg_loss:0.178, val_acc:0.938]
Epoch [19/120    avg_loss:0.096, val_acc:0.926]
Epoch [20/120    avg_loss:0.062, val_acc:0.937]
Epoch [21/120    avg_loss:0.100, val_acc:0.965]
Epoch [22/120    avg_loss:0.069, val_acc:0.973]
Epoch [23/120    avg_loss:0.046, val_acc:0.973]
Epoch [24/120    avg_loss:0.078, val_acc:0.955]
Epoch [25/120    avg_loss:0.065, val_acc:0.935]
Epoch [26/120    avg_loss:0.088, val_acc:0.949]
Epoch [27/120    avg_loss:0.132, val_acc:0.956]
Epoch [28/120    avg_loss:0.089, val_acc:0.954]
Epoch [29/120    avg_loss:0.064, val_acc:0.978]
Epoch [30/120    avg_loss:0.038, val_acc:0.958]
Epoch [31/120    avg_loss:0.051, val_acc:0.971]
Epoch [32/120    avg_loss:0.039, val_acc:0.978]
Epoch [33/120    avg_loss:0.053, val_acc:0.982]
Epoch [34/120    avg_loss:0.030, val_acc:0.983]
Epoch [35/120    avg_loss:0.035, val_acc:0.963]
Epoch [36/120    avg_loss:0.052, val_acc:0.980]
Epoch [37/120    avg_loss:0.048, val_acc:0.970]
Epoch [38/120    avg_loss:0.070, val_acc:0.961]
Epoch [39/120    avg_loss:0.075, val_acc:0.971]
Epoch [40/120    avg_loss:0.041, val_acc:0.973]
Epoch [41/120    avg_loss:0.038, val_acc:0.977]
Epoch [42/120    avg_loss:0.026, val_acc:0.983]
Epoch [43/120    avg_loss:0.032, val_acc:0.983]
Epoch [44/120    avg_loss:0.025, val_acc:0.983]
Epoch [45/120    avg_loss:0.017, val_acc:0.975]
Epoch [46/120    avg_loss:0.031, val_acc:0.857]
Epoch [47/120    avg_loss:0.058, val_acc:0.969]
Epoch [48/120    avg_loss:0.033, val_acc:0.979]
Epoch [49/120    avg_loss:0.021, val_acc:0.988]
Epoch [50/120    avg_loss:0.024, val_acc:0.988]
Epoch [51/120    avg_loss:0.023, val_acc:0.968]
Epoch [52/120    avg_loss:0.017, val_acc:0.990]
Epoch [53/120    avg_loss:0.012, val_acc:0.986]
Epoch [54/120    avg_loss:0.031, val_acc:0.980]
Epoch [55/120    avg_loss:0.015, val_acc:0.982]
Epoch [56/120    avg_loss:0.007, val_acc:0.987]
Epoch [57/120    avg_loss:0.027, val_acc:0.982]
Epoch [58/120    avg_loss:0.016, val_acc:0.985]
Epoch [59/120    avg_loss:0.025, val_acc:0.971]
Epoch [60/120    avg_loss:0.016, val_acc:0.986]
Epoch [61/120    avg_loss:0.021, val_acc:0.976]
Epoch [62/120    avg_loss:0.013, val_acc:0.988]
Epoch [63/120    avg_loss:0.015, val_acc:0.979]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.010, val_acc:0.990]
Epoch [67/120    avg_loss:0.006, val_acc:0.992]
Epoch [68/120    avg_loss:0.007, val_acc:0.990]
Epoch [69/120    avg_loss:0.010, val_acc:0.990]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.005, val_acc:0.990]
Epoch [73/120    avg_loss:0.008, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.007, val_acc:0.988]
Epoch [77/120    avg_loss:0.004, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.004, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.003, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.006, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     2     1     0     0     8    19     0]
 [    0     0 18021     0    22     0    46     0     1     0]
 [    0     1     0  1985     0     0     0     0    42     8]
 [    0    26     3     0  2930     0     2     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     3     0     0  4857     0    18     0]
 [    0     6     0     0     0     0     1  1280     0     3]
 [    0    10     0    13    43     0     0     0  3502     3]
 [    0     1     0     0    13    21     0     0     0   884]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99425377 0.99800631 0.98291656 0.97976927 0.99201824
 0.99284546 0.99301784 0.97766611 0.97303247]

Kappa:
0.9895641514882053
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92e4675780>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.294, val_acc:0.486]
Epoch [2/120    avg_loss:0.732, val_acc:0.761]
Epoch [3/120    avg_loss:0.564, val_acc:0.675]
Epoch [4/120    avg_loss:0.470, val_acc:0.821]
Epoch [5/120    avg_loss:0.387, val_acc:0.838]
Epoch [6/120    avg_loss:0.294, val_acc:0.868]
Epoch [7/120    avg_loss:0.322, val_acc:0.818]
Epoch [8/120    avg_loss:0.254, val_acc:0.838]
Epoch [9/120    avg_loss:0.216, val_acc:0.868]
Epoch [10/120    avg_loss:0.194, val_acc:0.899]
Epoch [11/120    avg_loss:0.278, val_acc:0.885]
Epoch [12/120    avg_loss:0.224, val_acc:0.838]
Epoch [13/120    avg_loss:0.257, val_acc:0.890]
Epoch [14/120    avg_loss:0.189, val_acc:0.900]
Epoch [15/120    avg_loss:0.159, val_acc:0.922]
Epoch [16/120    avg_loss:0.135, val_acc:0.922]
Epoch [17/120    avg_loss:0.182, val_acc:0.949]
Epoch [18/120    avg_loss:0.085, val_acc:0.934]
Epoch [19/120    avg_loss:0.096, val_acc:0.938]
Epoch [20/120    avg_loss:0.126, val_acc:0.918]
Epoch [21/120    avg_loss:0.119, val_acc:0.944]
Epoch [22/120    avg_loss:0.081, val_acc:0.958]
Epoch [23/120    avg_loss:0.071, val_acc:0.963]
Epoch [24/120    avg_loss:0.089, val_acc:0.957]
Epoch [25/120    avg_loss:0.054, val_acc:0.946]
Epoch [26/120    avg_loss:0.051, val_acc:0.958]
Epoch [27/120    avg_loss:0.054, val_acc:0.956]
Epoch [28/120    avg_loss:0.068, val_acc:0.960]
Epoch [29/120    avg_loss:0.059, val_acc:0.951]
Epoch [30/120    avg_loss:0.073, val_acc:0.953]
Epoch [31/120    avg_loss:0.061, val_acc:0.953]
Epoch [32/120    avg_loss:0.043, val_acc:0.960]
Epoch [33/120    avg_loss:0.049, val_acc:0.956]
Epoch [34/120    avg_loss:0.028, val_acc:0.967]
Epoch [35/120    avg_loss:0.028, val_acc:0.965]
Epoch [36/120    avg_loss:0.034, val_acc:0.978]
Epoch [37/120    avg_loss:0.029, val_acc:0.973]
Epoch [38/120    avg_loss:0.055, val_acc:0.945]
Epoch [39/120    avg_loss:0.042, val_acc:0.971]
Epoch [40/120    avg_loss:0.095, val_acc:0.958]
Epoch [41/120    avg_loss:0.037, val_acc:0.973]
Epoch [42/120    avg_loss:0.095, val_acc:0.952]
Epoch [43/120    avg_loss:0.059, val_acc:0.958]
Epoch [44/120    avg_loss:0.028, val_acc:0.978]
Epoch [45/120    avg_loss:0.021, val_acc:0.977]
Epoch [46/120    avg_loss:0.027, val_acc:0.914]
Epoch [47/120    avg_loss:0.036, val_acc:0.976]
Epoch [48/120    avg_loss:0.053, val_acc:0.955]
Epoch [49/120    avg_loss:0.018, val_acc:0.982]
Epoch [50/120    avg_loss:0.016, val_acc:0.980]
Epoch [51/120    avg_loss:0.031, val_acc:0.948]
Epoch [52/120    avg_loss:0.023, val_acc:0.978]
Epoch [53/120    avg_loss:0.020, val_acc:0.984]
Epoch [54/120    avg_loss:0.019, val_acc:0.933]
Epoch [55/120    avg_loss:0.074, val_acc:0.975]
Epoch [56/120    avg_loss:0.027, val_acc:0.969]
Epoch [57/120    avg_loss:0.028, val_acc:0.979]
Epoch [58/120    avg_loss:0.020, val_acc:0.968]
Epoch [59/120    avg_loss:0.015, val_acc:0.984]
Epoch [60/120    avg_loss:0.008, val_acc:0.974]
Epoch [61/120    avg_loss:0.027, val_acc:0.979]
Epoch [62/120    avg_loss:0.011, val_acc:0.983]
Epoch [63/120    avg_loss:0.008, val_acc:0.988]
Epoch [64/120    avg_loss:0.032, val_acc:0.978]
Epoch [65/120    avg_loss:0.015, val_acc:0.983]
Epoch [66/120    avg_loss:0.017, val_acc:0.971]
Epoch [67/120    avg_loss:0.029, val_acc:0.976]
Epoch [68/120    avg_loss:0.009, val_acc:0.979]
Epoch [69/120    avg_loss:0.033, val_acc:0.936]
Epoch [70/120    avg_loss:0.058, val_acc:0.981]
Epoch [71/120    avg_loss:0.017, val_acc:0.973]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.011, val_acc:0.983]
Epoch [74/120    avg_loss:0.014, val_acc:0.968]
Epoch [75/120    avg_loss:0.005, val_acc:0.988]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.983]
Epoch [78/120    avg_loss:0.014, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.016, val_acc:0.981]
Epoch [82/120    avg_loss:0.009, val_acc:0.979]
Epoch [83/120    avg_loss:0.009, val_acc:0.983]
Epoch [84/120    avg_loss:0.024, val_acc:0.978]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.010, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.981]
Epoch [95/120    avg_loss:0.003, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.004, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.003, val_acc:0.984]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.003, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.003, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6351     0     0     0     0    14    37    30     0]
 [    0     3 17945     0    16     0   120     0     6     0]
 [    0     0     0  1967     0     0     0     0    61     8]
 [    0    21     6     0  2932     0     3     0     8     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4872     0     6     0]
 [    0     2     0     0     0     0     1  1283     0     4]
 [    0     9     0     5    52     0     0     0  3500     5]
 [    0     2     0     0    10    19     0     0     0   888]]

Accuracy:
98.91547971947075

F1 scores:
[       nan 0.99079563 0.99581033 0.98153693 0.98027416 0.99277292
 0.98543689 0.98314176 0.97465887 0.97261774]

Kappa:
0.9856512692788056
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ff27ce780>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.301, val_acc:0.551]
Epoch [2/120    avg_loss:0.713, val_acc:0.731]
Epoch [3/120    avg_loss:0.618, val_acc:0.757]
Epoch [4/120    avg_loss:0.443, val_acc:0.827]
Epoch [5/120    avg_loss:0.411, val_acc:0.837]
Epoch [6/120    avg_loss:0.348, val_acc:0.789]
Epoch [7/120    avg_loss:0.296, val_acc:0.821]
Epoch [8/120    avg_loss:0.291, val_acc:0.860]
Epoch [9/120    avg_loss:0.234, val_acc:0.912]
Epoch [10/120    avg_loss:0.196, val_acc:0.913]
Epoch [11/120    avg_loss:0.186, val_acc:0.922]
Epoch [12/120    avg_loss:0.179, val_acc:0.930]
Epoch [13/120    avg_loss:0.153, val_acc:0.935]
Epoch [14/120    avg_loss:0.147, val_acc:0.933]
Epoch [15/120    avg_loss:0.111, val_acc:0.929]
Epoch [16/120    avg_loss:0.122, val_acc:0.948]
Epoch [17/120    avg_loss:0.116, val_acc:0.903]
Epoch [18/120    avg_loss:0.088, val_acc:0.923]
Epoch [19/120    avg_loss:0.131, val_acc:0.930]
Epoch [20/120    avg_loss:0.098, val_acc:0.964]
Epoch [21/120    avg_loss:0.072, val_acc:0.934]
Epoch [22/120    avg_loss:0.100, val_acc:0.925]
Epoch [23/120    avg_loss:0.065, val_acc:0.952]
Epoch [24/120    avg_loss:0.070, val_acc:0.951]
Epoch [25/120    avg_loss:0.063, val_acc:0.964]
Epoch [26/120    avg_loss:0.060, val_acc:0.909]
Epoch [27/120    avg_loss:0.065, val_acc:0.950]
Epoch [28/120    avg_loss:0.058, val_acc:0.952]
Epoch [29/120    avg_loss:0.028, val_acc:0.971]
Epoch [30/120    avg_loss:0.049, val_acc:0.958]
Epoch [31/120    avg_loss:0.054, val_acc:0.943]
Epoch [32/120    avg_loss:0.037, val_acc:0.947]
Epoch [33/120    avg_loss:0.031, val_acc:0.968]
Epoch [34/120    avg_loss:0.036, val_acc:0.973]
Epoch [35/120    avg_loss:0.031, val_acc:0.961]
Epoch [36/120    avg_loss:0.040, val_acc:0.971]
Epoch [37/120    avg_loss:0.029, val_acc:0.953]
Epoch [38/120    avg_loss:0.024, val_acc:0.972]
Epoch [39/120    avg_loss:0.031, val_acc:0.966]
Epoch [40/120    avg_loss:0.085, val_acc:0.956]
Epoch [41/120    avg_loss:0.181, val_acc:0.948]
Epoch [42/120    avg_loss:0.085, val_acc:0.964]
Epoch [43/120    avg_loss:0.046, val_acc:0.956]
Epoch [44/120    avg_loss:0.033, val_acc:0.973]
Epoch [45/120    avg_loss:0.023, val_acc:0.970]
Epoch [46/120    avg_loss:0.026, val_acc:0.977]
Epoch [47/120    avg_loss:0.031, val_acc:0.973]
Epoch [48/120    avg_loss:0.026, val_acc:0.972]
Epoch [49/120    avg_loss:0.029, val_acc:0.957]
Epoch [50/120    avg_loss:0.012, val_acc:0.966]
Epoch [51/120    avg_loss:0.018, val_acc:0.965]
Epoch [52/120    avg_loss:0.025, val_acc:0.971]
Epoch [53/120    avg_loss:0.018, val_acc:0.977]
Epoch [54/120    avg_loss:0.017, val_acc:0.980]
Epoch [55/120    avg_loss:0.012, val_acc:0.980]
Epoch [56/120    avg_loss:0.013, val_acc:0.979]
Epoch [57/120    avg_loss:0.007, val_acc:0.984]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.015, val_acc:0.984]
Epoch [60/120    avg_loss:0.006, val_acc:0.980]
Epoch [61/120    avg_loss:0.004, val_acc:0.981]
Epoch [62/120    avg_loss:0.006, val_acc:0.985]
Epoch [63/120    avg_loss:0.005, val_acc:0.985]
Epoch [64/120    avg_loss:0.009, val_acc:0.977]
Epoch [65/120    avg_loss:0.007, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.975]
Epoch [67/120    avg_loss:0.009, val_acc:0.982]
Epoch [68/120    avg_loss:0.010, val_acc:0.977]
Epoch [69/120    avg_loss:0.020, val_acc:0.980]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.982]
Epoch [72/120    avg_loss:0.008, val_acc:0.981]
Epoch [73/120    avg_loss:0.005, val_acc:0.981]
Epoch [74/120    avg_loss:0.006, val_acc:0.979]
Epoch [75/120    avg_loss:0.008, val_acc:0.982]
Epoch [76/120    avg_loss:0.004, val_acc:0.987]
Epoch [77/120    avg_loss:0.018, val_acc:0.940]
Epoch [78/120    avg_loss:0.214, val_acc:0.882]
Epoch [79/120    avg_loss:0.162, val_acc:0.937]
Epoch [80/120    avg_loss:0.054, val_acc:0.964]
Epoch [81/120    avg_loss:0.034, val_acc:0.971]
Epoch [82/120    avg_loss:0.022, val_acc:0.978]
Epoch [83/120    avg_loss:0.074, val_acc:0.895]
Epoch [84/120    avg_loss:0.071, val_acc:0.961]
Epoch [85/120    avg_loss:0.078, val_acc:0.967]
Epoch [86/120    avg_loss:0.032, val_acc:0.927]
Epoch [87/120    avg_loss:0.017, val_acc:0.976]
Epoch [88/120    avg_loss:0.016, val_acc:0.967]
Epoch [89/120    avg_loss:0.029, val_acc:0.979]
Epoch [90/120    avg_loss:0.013, val_acc:0.983]
Epoch [91/120    avg_loss:0.017, val_acc:0.985]
Epoch [92/120    avg_loss:0.014, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     7     0     0    21     1    35     3]
 [    0     1 18009     0    25     0    49     0     6     0]
 [    0     0     0  2020     0     0     0     0    13     3]
 [    0    47     8     4  2887     0     5     0    21     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4873     0     3     2]
 [    0     2     0     0     0     0     2  1283     0     3]
 [    0     0     0    17    51     0     0     0  3501     2]
 [    0     0     0     1    17    17     0     0     0   884]]

Accuracy:
99.11551346010171

F1 scores:
[       nan 0.99089282 0.9975351  0.98898409 0.97009409 0.99314547
 0.99165649 0.996892   0.9793007  0.97303247]

Kappa:
0.9882890867177285
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b442d4748>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.348, val_acc:0.674]
Epoch [2/120    avg_loss:0.732, val_acc:0.561]
Epoch [3/120    avg_loss:0.578, val_acc:0.812]
Epoch [4/120    avg_loss:0.481, val_acc:0.845]
Epoch [5/120    avg_loss:0.460, val_acc:0.859]
Epoch [6/120    avg_loss:0.326, val_acc:0.850]
Epoch [7/120    avg_loss:0.326, val_acc:0.898]
Epoch [8/120    avg_loss:0.281, val_acc:0.827]
Epoch [9/120    avg_loss:0.245, val_acc:0.917]
Epoch [10/120    avg_loss:0.197, val_acc:0.907]
Epoch [11/120    avg_loss:0.148, val_acc:0.927]
Epoch [12/120    avg_loss:0.219, val_acc:0.878]
Epoch [13/120    avg_loss:0.131, val_acc:0.938]
Epoch [14/120    avg_loss:0.180, val_acc:0.919]
Epoch [15/120    avg_loss:0.113, val_acc:0.876]
Epoch [16/120    avg_loss:0.140, val_acc:0.931]
Epoch [17/120    avg_loss:0.156, val_acc:0.924]
Epoch [18/120    avg_loss:0.158, val_acc:0.818]
Epoch [19/120    avg_loss:0.099, val_acc:0.929]
Epoch [20/120    avg_loss:0.170, val_acc:0.917]
Epoch [21/120    avg_loss:0.127, val_acc:0.951]
Epoch [22/120    avg_loss:0.104, val_acc:0.940]
Epoch [23/120    avg_loss:0.125, val_acc:0.944]
Epoch [24/120    avg_loss:0.079, val_acc:0.960]
Epoch [25/120    avg_loss:0.085, val_acc:0.846]
Epoch [26/120    avg_loss:0.074, val_acc:0.957]
Epoch [27/120    avg_loss:0.062, val_acc:0.945]
Epoch [28/120    avg_loss:0.094, val_acc:0.946]
Epoch [29/120    avg_loss:0.049, val_acc:0.955]
Epoch [30/120    avg_loss:0.048, val_acc:0.957]
Epoch [31/120    avg_loss:0.047, val_acc:0.967]
Epoch [32/120    avg_loss:0.052, val_acc:0.967]
Epoch [33/120    avg_loss:0.037, val_acc:0.969]
Epoch [34/120    avg_loss:0.043, val_acc:0.975]
Epoch [35/120    avg_loss:0.027, val_acc:0.980]
Epoch [36/120    avg_loss:0.198, val_acc:0.502]
Epoch [37/120    avg_loss:0.875, val_acc:0.640]
Epoch [38/120    avg_loss:0.732, val_acc:0.591]
Epoch [39/120    avg_loss:0.628, val_acc:0.694]
Epoch [40/120    avg_loss:0.662, val_acc:0.682]
Epoch [41/120    avg_loss:0.634, val_acc:0.734]
Epoch [42/120    avg_loss:0.594, val_acc:0.742]
Epoch [43/120    avg_loss:0.604, val_acc:0.778]
Epoch [44/120    avg_loss:0.443, val_acc:0.789]
Epoch [45/120    avg_loss:0.420, val_acc:0.815]
Epoch [46/120    avg_loss:0.449, val_acc:0.757]
Epoch [47/120    avg_loss:0.410, val_acc:0.827]
Epoch [48/120    avg_loss:0.387, val_acc:0.808]
Epoch [49/120    avg_loss:0.318, val_acc:0.857]
Epoch [50/120    avg_loss:0.314, val_acc:0.869]
Epoch [51/120    avg_loss:0.311, val_acc:0.867]
Epoch [52/120    avg_loss:0.289, val_acc:0.855]
Epoch [53/120    avg_loss:0.285, val_acc:0.852]
Epoch [54/120    avg_loss:0.273, val_acc:0.873]
Epoch [55/120    avg_loss:0.300, val_acc:0.850]
Epoch [56/120    avg_loss:0.247, val_acc:0.874]
Epoch [57/120    avg_loss:0.265, val_acc:0.877]
Epoch [58/120    avg_loss:0.275, val_acc:0.872]
Epoch [59/120    avg_loss:0.273, val_acc:0.873]
Epoch [60/120    avg_loss:0.270, val_acc:0.873]
Epoch [61/120    avg_loss:0.279, val_acc:0.870]
Epoch [62/120    avg_loss:0.238, val_acc:0.872]
Epoch [63/120    avg_loss:0.260, val_acc:0.872]
Epoch [64/120    avg_loss:0.248, val_acc:0.872]
Epoch [65/120    avg_loss:0.267, val_acc:0.876]
Epoch [66/120    avg_loss:0.256, val_acc:0.877]
Epoch [67/120    avg_loss:0.251, val_acc:0.878]
Epoch [68/120    avg_loss:0.239, val_acc:0.874]
Epoch [69/120    avg_loss:0.242, val_acc:0.873]
Epoch [70/120    avg_loss:0.247, val_acc:0.874]
Epoch [71/120    avg_loss:0.247, val_acc:0.872]
Epoch [72/120    avg_loss:0.214, val_acc:0.875]
Epoch [73/120    avg_loss:0.239, val_acc:0.878]
Epoch [74/120    avg_loss:0.245, val_acc:0.878]
Epoch [75/120    avg_loss:0.244, val_acc:0.878]
Epoch [76/120    avg_loss:0.260, val_acc:0.876]
Epoch [77/120    avg_loss:0.231, val_acc:0.877]
Epoch [78/120    avg_loss:0.221, val_acc:0.878]
Epoch [79/120    avg_loss:0.232, val_acc:0.878]
Epoch [80/120    avg_loss:0.263, val_acc:0.878]
Epoch [81/120    avg_loss:0.247, val_acc:0.879]
Epoch [82/120    avg_loss:0.231, val_acc:0.880]
Epoch [83/120    avg_loss:0.261, val_acc:0.880]
Epoch [84/120    avg_loss:0.244, val_acc:0.879]
Epoch [85/120    avg_loss:0.249, val_acc:0.879]
Epoch [86/120    avg_loss:0.236, val_acc:0.878]
Epoch [87/120    avg_loss:0.238, val_acc:0.878]
Epoch [88/120    avg_loss:0.247, val_acc:0.878]
Epoch [89/120    avg_loss:0.244, val_acc:0.878]
Epoch [90/120    avg_loss:0.235, val_acc:0.878]
Epoch [91/120    avg_loss:0.246, val_acc:0.878]
Epoch [92/120    avg_loss:0.239, val_acc:0.878]
Epoch [93/120    avg_loss:0.253, val_acc:0.878]
Epoch [94/120    avg_loss:0.265, val_acc:0.878]
Epoch [95/120    avg_loss:0.241, val_acc:0.878]
Epoch [96/120    avg_loss:0.241, val_acc:0.878]
Epoch [97/120    avg_loss:0.232, val_acc:0.878]
Epoch [98/120    avg_loss:0.248, val_acc:0.878]
Epoch [99/120    avg_loss:0.218, val_acc:0.878]
Epoch [100/120    avg_loss:0.249, val_acc:0.878]
Epoch [101/120    avg_loss:0.273, val_acc:0.878]
Epoch [102/120    avg_loss:0.258, val_acc:0.878]
Epoch [103/120    avg_loss:0.233, val_acc:0.878]
Epoch [104/120    avg_loss:0.239, val_acc:0.878]
Epoch [105/120    avg_loss:0.251, val_acc:0.878]
Epoch [106/120    avg_loss:0.224, val_acc:0.878]
Epoch [107/120    avg_loss:0.243, val_acc:0.878]
Epoch [108/120    avg_loss:0.260, val_acc:0.878]
Epoch [109/120    avg_loss:0.236, val_acc:0.878]
Epoch [110/120    avg_loss:0.263, val_acc:0.878]
Epoch [111/120    avg_loss:0.248, val_acc:0.878]
Epoch [112/120    avg_loss:0.274, val_acc:0.878]
Epoch [113/120    avg_loss:0.244, val_acc:0.878]
Epoch [114/120    avg_loss:0.269, val_acc:0.878]
Epoch [115/120    avg_loss:0.238, val_acc:0.878]
Epoch [116/120    avg_loss:0.226, val_acc:0.878]
Epoch [117/120    avg_loss:0.264, val_acc:0.878]
Epoch [118/120    avg_loss:0.220, val_acc:0.878]
Epoch [119/120    avg_loss:0.244, val_acc:0.878]
Epoch [120/120    avg_loss:0.267, val_acc:0.878]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5737    18    31   102     0   212     1   251    80]
 [    0    11 15837     0   395     0  1847     0     0     0]
 [    0    13     0  1900     3     0     0     0    83    37]
 [    0    58    75     0  2753     0    52     0    23    11]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   294   159     0     0  4376     0    49     0]
 [    0    35     0     0     1     0     6  1233     2    13]
 [    0   141     0   169   117     0    85     0  3059     0]
 [    0    21     0     7    15    40     6     0     0   830]]

Accuracy:
89.24396886221773

F1 scores:
[       nan 0.9217545  0.92306347 0.88331009 0.8659956  0.98490566
 0.76356657 0.9770206  0.86928105 0.87830688]

Kappa:
0.860351650225551
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa4b78fa7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.350, val_acc:0.443]
Epoch [2/120    avg_loss:0.694, val_acc:0.712]
Epoch [3/120    avg_loss:0.535, val_acc:0.819]
Epoch [4/120    avg_loss:0.507, val_acc:0.820]
Epoch [5/120    avg_loss:0.389, val_acc:0.719]
Epoch [6/120    avg_loss:0.388, val_acc:0.792]
Epoch [7/120    avg_loss:0.342, val_acc:0.891]
Epoch [8/120    avg_loss:0.211, val_acc:0.907]
Epoch [9/120    avg_loss:0.249, val_acc:0.898]
Epoch [10/120    avg_loss:0.173, val_acc:0.920]
Epoch [11/120    avg_loss:0.209, val_acc:0.922]
Epoch [12/120    avg_loss:0.144, val_acc:0.911]
Epoch [13/120    avg_loss:0.180, val_acc:0.912]
Epoch [14/120    avg_loss:0.143, val_acc:0.927]
Epoch [15/120    avg_loss:0.143, val_acc:0.943]
Epoch [16/120    avg_loss:0.134, val_acc:0.934]
Epoch [17/120    avg_loss:0.082, val_acc:0.943]
Epoch [18/120    avg_loss:0.099, val_acc:0.948]
Epoch [19/120    avg_loss:0.118, val_acc:0.954]
Epoch [20/120    avg_loss:0.093, val_acc:0.943]
Epoch [21/120    avg_loss:0.120, val_acc:0.908]
Epoch [22/120    avg_loss:0.079, val_acc:0.916]
Epoch [23/120    avg_loss:0.088, val_acc:0.954]
Epoch [24/120    avg_loss:0.077, val_acc:0.936]
Epoch [25/120    avg_loss:0.058, val_acc:0.892]
Epoch [26/120    avg_loss:0.091, val_acc:0.958]
Epoch [27/120    avg_loss:0.054, val_acc:0.957]
Epoch [28/120    avg_loss:0.030, val_acc:0.958]
Epoch [29/120    avg_loss:0.028, val_acc:0.967]
Epoch [30/120    avg_loss:0.043, val_acc:0.956]
Epoch [31/120    avg_loss:0.029, val_acc:0.959]
Epoch [32/120    avg_loss:0.026, val_acc:0.963]
Epoch [33/120    avg_loss:0.025, val_acc:0.968]
Epoch [34/120    avg_loss:0.111, val_acc:0.774]
Epoch [35/120    avg_loss:0.063, val_acc:0.968]
Epoch [36/120    avg_loss:0.043, val_acc:0.968]
Epoch [37/120    avg_loss:0.045, val_acc:0.974]
Epoch [38/120    avg_loss:0.018, val_acc:0.972]
Epoch [39/120    avg_loss:0.022, val_acc:0.964]
Epoch [40/120    avg_loss:0.029, val_acc:0.966]
Epoch [41/120    avg_loss:0.089, val_acc:0.934]
Epoch [42/120    avg_loss:0.062, val_acc:0.955]
Epoch [43/120    avg_loss:0.082, val_acc:0.964]
Epoch [44/120    avg_loss:0.057, val_acc:0.965]
Epoch [45/120    avg_loss:0.026, val_acc:0.977]
Epoch [46/120    avg_loss:0.012, val_acc:0.978]
Epoch [47/120    avg_loss:0.028, val_acc:0.968]
Epoch [48/120    avg_loss:0.022, val_acc:0.965]
Epoch [49/120    avg_loss:0.045, val_acc:0.968]
Epoch [50/120    avg_loss:0.026, val_acc:0.978]
Epoch [51/120    avg_loss:0.017, val_acc:0.975]
Epoch [52/120    avg_loss:0.013, val_acc:0.968]
Epoch [53/120    avg_loss:0.012, val_acc:0.973]
Epoch [54/120    avg_loss:0.014, val_acc:0.973]
Epoch [55/120    avg_loss:0.016, val_acc:0.984]
Epoch [56/120    avg_loss:0.021, val_acc:0.979]
Epoch [57/120    avg_loss:0.014, val_acc:0.964]
Epoch [58/120    avg_loss:0.016, val_acc:0.982]
Epoch [59/120    avg_loss:0.017, val_acc:0.983]
Epoch [60/120    avg_loss:0.018, val_acc:0.984]
Epoch [61/120    avg_loss:0.013, val_acc:0.984]
Epoch [62/120    avg_loss:0.008, val_acc:0.981]
Epoch [63/120    avg_loss:0.013, val_acc:0.945]
Epoch [64/120    avg_loss:0.043, val_acc:0.902]
Epoch [65/120    avg_loss:0.091, val_acc:0.941]
Epoch [66/120    avg_loss:0.023, val_acc:0.974]
Epoch [67/120    avg_loss:0.014, val_acc:0.973]
Epoch [68/120    avg_loss:0.018, val_acc:0.984]
Epoch [69/120    avg_loss:0.035, val_acc:0.971]
Epoch [70/120    avg_loss:0.017, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.983]
Epoch [72/120    avg_loss:0.005, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.973]
Epoch [75/120    avg_loss:0.012, val_acc:0.983]
Epoch [76/120    avg_loss:0.006, val_acc:0.982]
Epoch [77/120    avg_loss:0.009, val_acc:0.972]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.974]
Epoch [80/120    avg_loss:0.037, val_acc:0.967]
Epoch [81/120    avg_loss:0.037, val_acc:0.949]
Epoch [82/120    avg_loss:0.028, val_acc:0.975]
Epoch [83/120    avg_loss:0.144, val_acc:0.919]
Epoch [84/120    avg_loss:0.086, val_acc:0.961]
Epoch [85/120    avg_loss:0.069, val_acc:0.959]
Epoch [86/120    avg_loss:0.035, val_acc:0.968]
Epoch [87/120    avg_loss:0.025, val_acc:0.970]
Epoch [88/120    avg_loss:0.014, val_acc:0.971]
Epoch [89/120    avg_loss:0.014, val_acc:0.973]
Epoch [90/120    avg_loss:0.013, val_acc:0.973]
Epoch [91/120    avg_loss:0.014, val_acc:0.973]
Epoch [92/120    avg_loss:0.019, val_acc:0.972]
Epoch [93/120    avg_loss:0.013, val_acc:0.971]
Epoch [94/120    avg_loss:0.010, val_acc:0.973]
Epoch [95/120    avg_loss:0.011, val_acc:0.974]
Epoch [96/120    avg_loss:0.009, val_acc:0.977]
Epoch [97/120    avg_loss:0.012, val_acc:0.976]
Epoch [98/120    avg_loss:0.014, val_acc:0.973]
Epoch [99/120    avg_loss:0.010, val_acc:0.973]
Epoch [100/120    avg_loss:0.013, val_acc:0.973]
Epoch [101/120    avg_loss:0.011, val_acc:0.973]
Epoch [102/120    avg_loss:0.013, val_acc:0.974]
Epoch [103/120    avg_loss:0.008, val_acc:0.975]
Epoch [104/120    avg_loss:0.012, val_acc:0.974]
Epoch [105/120    avg_loss:0.012, val_acc:0.975]
Epoch [106/120    avg_loss:0.013, val_acc:0.975]
Epoch [107/120    avg_loss:0.012, val_acc:0.975]
Epoch [108/120    avg_loss:0.014, val_acc:0.975]
Epoch [109/120    avg_loss:0.009, val_acc:0.975]
Epoch [110/120    avg_loss:0.012, val_acc:0.975]
Epoch [111/120    avg_loss:0.010, val_acc:0.974]
Epoch [112/120    avg_loss:0.010, val_acc:0.974]
Epoch [113/120    avg_loss:0.011, val_acc:0.974]
Epoch [114/120    avg_loss:0.011, val_acc:0.974]
Epoch [115/120    avg_loss:0.008, val_acc:0.974]
Epoch [116/120    avg_loss:0.014, val_acc:0.975]
Epoch [117/120    avg_loss:0.009, val_acc:0.975]
Epoch [118/120    avg_loss:0.010, val_acc:0.975]
Epoch [119/120    avg_loss:0.009, val_acc:0.975]
Epoch [120/120    avg_loss:0.018, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6284     0     0     0     0    30    36    82     0]
 [    0     0 18049     0    10     0    28     0     3     0]
 [    0     0     0  1989     0     0     0     0    35    12]
 [    0    40     4     0  2901     0     0     0    25     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4873     0     1     0]
 [    0    11     0     0     0     0     8  1261     0    10]
 [    0    23     0     5    50     0     0     0  3488     5]
 [    0     0     0     0    14    38     0     0     0   867]]

Accuracy:
98.85281854770685

F1 scores:
[       nan 0.98264269 0.99875495 0.986118   0.97561796 0.98564955
 0.99276765 0.97487437 0.96821652 0.9553719 ]

Kappa:
0.984806797112146
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4fd2b9c748>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.339, val_acc:0.706]
Epoch [2/120    avg_loss:0.678, val_acc:0.778]
Epoch [3/120    avg_loss:0.520, val_acc:0.679]
Epoch [4/120    avg_loss:0.449, val_acc:0.809]
Epoch [5/120    avg_loss:0.378, val_acc:0.847]
Epoch [6/120    avg_loss:0.395, val_acc:0.883]
Epoch [7/120    avg_loss:0.258, val_acc:0.909]
Epoch [8/120    avg_loss:0.260, val_acc:0.779]
Epoch [9/120    avg_loss:0.216, val_acc:0.921]
Epoch [10/120    avg_loss:0.261, val_acc:0.876]
Epoch [11/120    avg_loss:0.189, val_acc:0.834]
Epoch [12/120    avg_loss:0.157, val_acc:0.888]
Epoch [13/120    avg_loss:0.145, val_acc:0.909]
Epoch [14/120    avg_loss:0.148, val_acc:0.914]
Epoch [15/120    avg_loss:0.159, val_acc:0.855]
Epoch [16/120    avg_loss:0.136, val_acc:0.945]
Epoch [17/120    avg_loss:0.161, val_acc:0.934]
Epoch [18/120    avg_loss:0.124, val_acc:0.946]
Epoch [19/120    avg_loss:0.076, val_acc:0.952]
Epoch [20/120    avg_loss:0.065, val_acc:0.960]
Epoch [21/120    avg_loss:0.059, val_acc:0.961]
Epoch [22/120    avg_loss:0.064, val_acc:0.961]
Epoch [23/120    avg_loss:0.104, val_acc:0.925]
Epoch [24/120    avg_loss:0.123, val_acc:0.924]
Epoch [25/120    avg_loss:0.099, val_acc:0.953]
Epoch [26/120    avg_loss:0.101, val_acc:0.948]
Epoch [27/120    avg_loss:0.061, val_acc:0.966]
Epoch [28/120    avg_loss:0.045, val_acc:0.924]
Epoch [29/120    avg_loss:0.048, val_acc:0.933]
Epoch [30/120    avg_loss:0.038, val_acc:0.958]
Epoch [31/120    avg_loss:0.054, val_acc:0.909]
Epoch [32/120    avg_loss:0.042, val_acc:0.965]
Epoch [33/120    avg_loss:0.036, val_acc:0.971]
Epoch [34/120    avg_loss:0.030, val_acc:0.972]
Epoch [35/120    avg_loss:0.047, val_acc:0.962]
Epoch [36/120    avg_loss:0.026, val_acc:0.970]
Epoch [37/120    avg_loss:0.024, val_acc:0.979]
Epoch [38/120    avg_loss:0.056, val_acc:0.945]
Epoch [39/120    avg_loss:0.089, val_acc:0.893]
Epoch [40/120    avg_loss:0.071, val_acc:0.971]
Epoch [41/120    avg_loss:0.043, val_acc:0.977]
Epoch [42/120    avg_loss:0.024, val_acc:0.955]
Epoch [43/120    avg_loss:0.031, val_acc:0.976]
Epoch [44/120    avg_loss:0.031, val_acc:0.977]
Epoch [45/120    avg_loss:0.026, val_acc:0.965]
Epoch [46/120    avg_loss:0.022, val_acc:0.975]
Epoch [47/120    avg_loss:0.015, val_acc:0.978]
Epoch [48/120    avg_loss:0.016, val_acc:0.980]
Epoch [49/120    avg_loss:0.081, val_acc:0.948]
Epoch [50/120    avg_loss:0.032, val_acc:0.982]
Epoch [51/120    avg_loss:0.064, val_acc:0.966]
Epoch [52/120    avg_loss:0.030, val_acc:0.967]
Epoch [53/120    avg_loss:0.032, val_acc:0.966]
Epoch [54/120    avg_loss:0.032, val_acc:0.977]
Epoch [55/120    avg_loss:0.033, val_acc:0.976]
Epoch [56/120    avg_loss:0.021, val_acc:0.966]
Epoch [57/120    avg_loss:0.024, val_acc:0.976]
Epoch [58/120    avg_loss:0.016, val_acc:0.982]
Epoch [59/120    avg_loss:0.012, val_acc:0.977]
Epoch [60/120    avg_loss:0.010, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.979]
Epoch [62/120    avg_loss:0.013, val_acc:0.915]
Epoch [63/120    avg_loss:0.034, val_acc:0.951]
Epoch [64/120    avg_loss:0.046, val_acc:0.956]
Epoch [65/120    avg_loss:0.015, val_acc:0.983]
Epoch [66/120    avg_loss:0.019, val_acc:0.965]
Epoch [67/120    avg_loss:0.023, val_acc:0.973]
Epoch [68/120    avg_loss:0.026, val_acc:0.983]
Epoch [69/120    avg_loss:0.017, val_acc:0.973]
Epoch [70/120    avg_loss:0.008, val_acc:0.983]
Epoch [71/120    avg_loss:0.008, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.034, val_acc:0.965]
Epoch [74/120    avg_loss:0.019, val_acc:0.982]
Epoch [75/120    avg_loss:0.015, val_acc:0.977]
Epoch [76/120    avg_loss:0.017, val_acc:0.977]
Epoch [77/120    avg_loss:0.022, val_acc:0.984]
Epoch [78/120    avg_loss:0.014, val_acc:0.981]
Epoch [79/120    avg_loss:0.013, val_acc:0.958]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.963]
Epoch [83/120    avg_loss:0.018, val_acc:0.985]
Epoch [84/120    avg_loss:0.016, val_acc:0.983]
Epoch [85/120    avg_loss:0.021, val_acc:0.976]
Epoch [86/120    avg_loss:0.027, val_acc:0.984]
Epoch [87/120    avg_loss:0.015, val_acc:0.944]
Epoch [88/120    avg_loss:0.023, val_acc:0.982]
Epoch [89/120    avg_loss:0.013, val_acc:0.982]
Epoch [90/120    avg_loss:0.014, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.976]
Epoch [92/120    avg_loss:0.010, val_acc:0.984]
Epoch [93/120    avg_loss:0.015, val_acc:0.982]
Epoch [94/120    avg_loss:0.016, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.005, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.984]
Epoch [101/120    avg_loss:0.003, val_acc:0.984]
Epoch [102/120    avg_loss:0.003, val_acc:0.984]
Epoch [103/120    avg_loss:0.003, val_acc:0.983]
Epoch [104/120    avg_loss:0.005, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.004, val_acc:0.983]
Epoch [108/120    avg_loss:0.004, val_acc:0.983]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.003, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6364     0     0     1     0    18    46     3     0]
 [    0     0 18069     0    15     0     2     0     4     0]
 [    0     0     0  2006     0     0     0     0    24     6]
 [    0    23     4     0  2919     0     0     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4870     0     8     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     7     0    10    57     0     0     0  3491     6]
 [    0     0     0     0    15    40     0     0     0   864]]

Accuracy:
99.23601571349384

F1 scores:
[       nan 0.99235927 0.99930869 0.99012833 0.97641746 0.98490566
 0.99692938 0.98170732 0.97965483 0.96267409]

Kappa:
0.9898798971109203
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4d371e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.378, val_acc:0.745]
Epoch [2/120    avg_loss:0.698, val_acc:0.714]
Epoch [3/120    avg_loss:0.559, val_acc:0.753]
Epoch [4/120    avg_loss:0.509, val_acc:0.797]
Epoch [5/120    avg_loss:0.405, val_acc:0.799]
Epoch [6/120    avg_loss:0.286, val_acc:0.895]
Epoch [7/120    avg_loss:0.318, val_acc:0.914]
Epoch [8/120    avg_loss:0.239, val_acc:0.923]
Epoch [9/120    avg_loss:0.236, val_acc:0.912]
Epoch [10/120    avg_loss:0.253, val_acc:0.819]
Epoch [11/120    avg_loss:0.226, val_acc:0.916]
Epoch [12/120    avg_loss:0.159, val_acc:0.920]
Epoch [13/120    avg_loss:0.261, val_acc:0.940]
Epoch [14/120    avg_loss:0.155, val_acc:0.905]
Epoch [15/120    avg_loss:0.137, val_acc:0.941]
Epoch [16/120    avg_loss:0.126, val_acc:0.936]
Epoch [17/120    avg_loss:0.109, val_acc:0.949]
Epoch [18/120    avg_loss:0.082, val_acc:0.956]
Epoch [19/120    avg_loss:0.102, val_acc:0.961]
Epoch [20/120    avg_loss:0.079, val_acc:0.964]
Epoch [21/120    avg_loss:0.051, val_acc:0.974]
Epoch [22/120    avg_loss:0.088, val_acc:0.940]
Epoch [23/120    avg_loss:0.071, val_acc:0.965]
Epoch [24/120    avg_loss:0.036, val_acc:0.969]
Epoch [25/120    avg_loss:0.073, val_acc:0.957]
Epoch [26/120    avg_loss:0.051, val_acc:0.972]
Epoch [27/120    avg_loss:0.039, val_acc:0.969]
Epoch [28/120    avg_loss:0.102, val_acc:0.971]
Epoch [29/120    avg_loss:0.075, val_acc:0.982]
Epoch [30/120    avg_loss:0.034, val_acc:0.976]
Epoch [31/120    avg_loss:0.068, val_acc:0.975]
Epoch [32/120    avg_loss:0.039, val_acc:0.975]
Epoch [33/120    avg_loss:0.069, val_acc:0.970]
Epoch [34/120    avg_loss:0.026, val_acc:0.973]
Epoch [35/120    avg_loss:0.035, val_acc:0.982]
Epoch [36/120    avg_loss:0.031, val_acc:0.977]
Epoch [37/120    avg_loss:0.032, val_acc:0.939]
Epoch [38/120    avg_loss:0.017, val_acc:0.963]
Epoch [39/120    avg_loss:0.017, val_acc:0.982]
Epoch [40/120    avg_loss:0.019, val_acc:0.985]
Epoch [41/120    avg_loss:0.014, val_acc:0.986]
Epoch [42/120    avg_loss:0.015, val_acc:0.971]
Epoch [43/120    avg_loss:0.021, val_acc:0.982]
Epoch [44/120    avg_loss:0.014, val_acc:0.982]
Epoch [45/120    avg_loss:0.072, val_acc:0.965]
Epoch [46/120    avg_loss:0.032, val_acc:0.985]
Epoch [47/120    avg_loss:0.031, val_acc:0.980]
Epoch [48/120    avg_loss:0.136, val_acc:0.967]
Epoch [49/120    avg_loss:0.044, val_acc:0.981]
Epoch [50/120    avg_loss:0.022, val_acc:0.985]
Epoch [51/120    avg_loss:0.017, val_acc:0.987]
Epoch [52/120    avg_loss:0.011, val_acc:0.984]
Epoch [53/120    avg_loss:0.028, val_acc:0.965]
Epoch [54/120    avg_loss:0.030, val_acc:0.980]
Epoch [55/120    avg_loss:0.014, val_acc:0.986]
Epoch [56/120    avg_loss:0.011, val_acc:0.989]
Epoch [57/120    avg_loss:0.012, val_acc:0.986]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.007, val_acc:0.985]
Epoch [60/120    avg_loss:0.020, val_acc:0.980]
Epoch [61/120    avg_loss:0.017, val_acc:0.991]
Epoch [62/120    avg_loss:0.007, val_acc:0.989]
Epoch [63/120    avg_loss:0.007, val_acc:0.989]
Epoch [64/120    avg_loss:0.009, val_acc:0.985]
Epoch [65/120    avg_loss:0.006, val_acc:0.987]
Epoch [66/120    avg_loss:0.005, val_acc:0.988]
Epoch [67/120    avg_loss:0.003, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.987]
Epoch [69/120    avg_loss:0.006, val_acc:0.989]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.982]
Epoch [72/120    avg_loss:0.013, val_acc:0.991]
Epoch [73/120    avg_loss:0.023, val_acc:0.988]
Epoch [74/120    avg_loss:0.007, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.989]
Epoch [76/120    avg_loss:0.004, val_acc:0.990]
Epoch [77/120    avg_loss:0.003, val_acc:0.990]
Epoch [78/120    avg_loss:0.013, val_acc:0.983]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.021, val_acc:0.988]
Epoch [81/120    avg_loss:0.014, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.011, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.003, val_acc:0.989]
Epoch [90/120    avg_loss:0.004, val_acc:0.989]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.003, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.002, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.003, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.003, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.989]
Epoch [112/120    avg_loss:0.003, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.003, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.989]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     1     0    20    18     9     0]
 [    0     1 18059     0    20     0     3     0     7     0]
 [    0     0     0  2001     2     0     0     0    25     8]
 [    0    10     5     0  2941     0     0     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     3  1283     0     4]
 [    0     0     0    14    47     0     0     0  3493    17]
 [    0     0     0     0    14    20     0     0     0   885]]

Accuracy:
99.36374810208952

F1 scores:
[       nan 0.99540033 0.99900426 0.98790422 0.98082375 0.99239544
 0.99734206 0.99035122 0.98104199 0.96563011]

Kappa:
0.991572560189284
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0864db780>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.324, val_acc:0.585]
Epoch [2/120    avg_loss:0.714, val_acc:0.754]
Epoch [3/120    avg_loss:0.567, val_acc:0.676]
Epoch [4/120    avg_loss:0.427, val_acc:0.840]
Epoch [5/120    avg_loss:0.418, val_acc:0.839]
Epoch [6/120    avg_loss:0.425, val_acc:0.846]
Epoch [7/120    avg_loss:0.347, val_acc:0.821]
Epoch [8/120    avg_loss:0.261, val_acc:0.810]
Epoch [9/120    avg_loss:0.276, val_acc:0.837]
Epoch [10/120    avg_loss:0.265, val_acc:0.852]
Epoch [11/120    avg_loss:0.301, val_acc:0.871]
Epoch [12/120    avg_loss:0.179, val_acc:0.930]
Epoch [13/120    avg_loss:0.173, val_acc:0.810]
Epoch [14/120    avg_loss:0.158, val_acc:0.912]
Epoch [15/120    avg_loss:0.142, val_acc:0.937]
Epoch [16/120    avg_loss:0.183, val_acc:0.931]
Epoch [17/120    avg_loss:0.096, val_acc:0.940]
Epoch [18/120    avg_loss:0.102, val_acc:0.871]
Epoch [19/120    avg_loss:0.219, val_acc:0.904]
Epoch [20/120    avg_loss:0.113, val_acc:0.955]
Epoch [21/120    avg_loss:0.123, val_acc:0.924]
Epoch [22/120    avg_loss:0.054, val_acc:0.927]
Epoch [23/120    avg_loss:0.067, val_acc:0.946]
Epoch [24/120    avg_loss:0.051, val_acc:0.952]
Epoch [25/120    avg_loss:0.112, val_acc:0.938]
Epoch [26/120    avg_loss:0.066, val_acc:0.958]
Epoch [27/120    avg_loss:0.108, val_acc:0.927]
Epoch [28/120    avg_loss:0.066, val_acc:0.966]
Epoch [29/120    avg_loss:0.057, val_acc:0.948]
Epoch [30/120    avg_loss:0.054, val_acc:0.952]
Epoch [31/120    avg_loss:0.053, val_acc:0.966]
Epoch [32/120    avg_loss:0.043, val_acc:0.964]
Epoch [33/120    avg_loss:0.040, val_acc:0.970]
Epoch [34/120    avg_loss:0.052, val_acc:0.965]
Epoch [35/120    avg_loss:0.063, val_acc:0.947]
Epoch [36/120    avg_loss:0.078, val_acc:0.936]
Epoch [37/120    avg_loss:0.065, val_acc:0.960]
Epoch [38/120    avg_loss:0.056, val_acc:0.956]
Epoch [39/120    avg_loss:0.043, val_acc:0.965]
Epoch [40/120    avg_loss:0.027, val_acc:0.960]
Epoch [41/120    avg_loss:0.057, val_acc:0.933]
Epoch [42/120    avg_loss:0.037, val_acc:0.969]
Epoch [43/120    avg_loss:0.027, val_acc:0.943]
Epoch [44/120    avg_loss:0.038, val_acc:0.971]
Epoch [45/120    avg_loss:0.029, val_acc:0.966]
Epoch [46/120    avg_loss:0.041, val_acc:0.972]
Epoch [47/120    avg_loss:0.032, val_acc:0.972]
Epoch [48/120    avg_loss:0.020, val_acc:0.947]
Epoch [49/120    avg_loss:0.022, val_acc:0.962]
Epoch [50/120    avg_loss:0.018, val_acc:0.971]
Epoch [51/120    avg_loss:0.016, val_acc:0.975]
Epoch [52/120    avg_loss:0.017, val_acc:0.974]
Epoch [53/120    avg_loss:0.012, val_acc:0.976]
Epoch [54/120    avg_loss:0.012, val_acc:0.943]
Epoch [55/120    avg_loss:0.011, val_acc:0.983]
Epoch [56/120    avg_loss:0.018, val_acc:0.967]
Epoch [57/120    avg_loss:0.020, val_acc:0.966]
Epoch [58/120    avg_loss:0.017, val_acc:0.971]
Epoch [59/120    avg_loss:0.013, val_acc:0.970]
Epoch [60/120    avg_loss:0.028, val_acc:0.971]
Epoch [61/120    avg_loss:0.013, val_acc:0.980]
Epoch [62/120    avg_loss:0.024, val_acc:0.975]
Epoch [63/120    avg_loss:0.029, val_acc:0.951]
Epoch [64/120    avg_loss:0.019, val_acc:0.961]
Epoch [65/120    avg_loss:0.159, val_acc:0.957]
Epoch [66/120    avg_loss:0.043, val_acc:0.960]
Epoch [67/120    avg_loss:0.031, val_acc:0.976]
Epoch [68/120    avg_loss:0.017, val_acc:0.971]
Epoch [69/120    avg_loss:0.016, val_acc:0.977]
Epoch [70/120    avg_loss:0.009, val_acc:0.981]
Epoch [71/120    avg_loss:0.012, val_acc:0.980]
Epoch [72/120    avg_loss:0.013, val_acc:0.978]
Epoch [73/120    avg_loss:0.011, val_acc:0.979]
Epoch [74/120    avg_loss:0.010, val_acc:0.982]
Epoch [75/120    avg_loss:0.010, val_acc:0.982]
Epoch [76/120    avg_loss:0.006, val_acc:0.983]
Epoch [77/120    avg_loss:0.009, val_acc:0.983]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.006, val_acc:0.987]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.985]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.983]
Epoch [85/120    avg_loss:0.011, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.013, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.013, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6349     0     3     3     0    23    12    42     0]
 [    0     0 18003     0    31     0    56     0     0     0]
 [    0     0     0  2024     0     0     0     0     0    12]
 [    0    20     6     0  2927     0     0     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4874     0     1     1]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     4     0    29    55     0     0     0  3471    12]
 [    0     0     0     0    13    46     0     0     0   860]]

Accuracy:
99.05285228833779

F1 scores:
[       nan 0.99164389 0.99742375 0.98876404 0.97550408 0.98268072
 0.99155732 0.99420626 0.97719595 0.9518539 ]

Kappa:
0.9874628171991726
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83b8ed9748>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.348, val_acc:0.518]
Epoch [2/120    avg_loss:0.710, val_acc:0.720]
Epoch [3/120    avg_loss:0.509, val_acc:0.812]
Epoch [4/120    avg_loss:0.435, val_acc:0.756]
Epoch [5/120    avg_loss:0.430, val_acc:0.828]
Epoch [6/120    avg_loss:0.285, val_acc:0.888]
Epoch [7/120    avg_loss:0.240, val_acc:0.823]
Epoch [8/120    avg_loss:0.261, val_acc:0.823]
Epoch [9/120    avg_loss:0.254, val_acc:0.791]
Epoch [10/120    avg_loss:0.187, val_acc:0.883]
Epoch [11/120    avg_loss:0.168, val_acc:0.891]
Epoch [12/120    avg_loss:0.158, val_acc:0.882]
Epoch [13/120    avg_loss:0.176, val_acc:0.897]
Epoch [14/120    avg_loss:0.123, val_acc:0.931]
Epoch [15/120    avg_loss:0.202, val_acc:0.926]
Epoch [16/120    avg_loss:0.155, val_acc:0.932]
Epoch [17/120    avg_loss:0.157, val_acc:0.889]
Epoch [18/120    avg_loss:0.142, val_acc:0.944]
Epoch [19/120    avg_loss:0.085, val_acc:0.926]
Epoch [20/120    avg_loss:0.160, val_acc:0.950]
Epoch [21/120    avg_loss:0.074, val_acc:0.951]
Epoch [22/120    avg_loss:0.059, val_acc:0.964]
Epoch [23/120    avg_loss:0.048, val_acc:0.954]
Epoch [24/120    avg_loss:0.078, val_acc:0.935]
Epoch [25/120    avg_loss:0.058, val_acc:0.954]
Epoch [26/120    avg_loss:0.062, val_acc:0.962]
Epoch [27/120    avg_loss:0.070, val_acc:0.943]
Epoch [28/120    avg_loss:0.059, val_acc:0.972]
Epoch [29/120    avg_loss:0.029, val_acc:0.938]
Epoch [30/120    avg_loss:0.038, val_acc:0.954]
Epoch [31/120    avg_loss:0.038, val_acc:0.976]
Epoch [32/120    avg_loss:0.058, val_acc:0.966]
Epoch [33/120    avg_loss:0.031, val_acc:0.971]
Epoch [34/120    avg_loss:0.020, val_acc:0.979]
Epoch [35/120    avg_loss:0.024, val_acc:0.979]
Epoch [36/120    avg_loss:0.020, val_acc:0.980]
Epoch [37/120    avg_loss:0.014, val_acc:0.974]
Epoch [38/120    avg_loss:0.057, val_acc:0.927]
Epoch [39/120    avg_loss:0.088, val_acc:0.966]
Epoch [40/120    avg_loss:0.043, val_acc:0.977]
Epoch [41/120    avg_loss:0.020, val_acc:0.977]
Epoch [42/120    avg_loss:0.018, val_acc:0.983]
Epoch [43/120    avg_loss:0.033, val_acc:0.971]
Epoch [44/120    avg_loss:0.026, val_acc:0.981]
Epoch [45/120    avg_loss:0.017, val_acc:0.975]
Epoch [46/120    avg_loss:0.021, val_acc:0.982]
Epoch [47/120    avg_loss:0.026, val_acc:0.979]
Epoch [48/120    avg_loss:0.013, val_acc:0.976]
Epoch [49/120    avg_loss:0.054, val_acc:0.953]
Epoch [50/120    avg_loss:0.099, val_acc:0.959]
Epoch [51/120    avg_loss:0.071, val_acc:0.952]
Epoch [52/120    avg_loss:0.080, val_acc:0.952]
Epoch [53/120    avg_loss:0.059, val_acc:0.957]
Epoch [54/120    avg_loss:0.026, val_acc:0.971]
Epoch [55/120    avg_loss:0.016, val_acc:0.965]
Epoch [56/120    avg_loss:0.012, val_acc:0.975]
Epoch [57/120    avg_loss:0.021, val_acc:0.973]
Epoch [58/120    avg_loss:0.011, val_acc:0.978]
Epoch [59/120    avg_loss:0.011, val_acc:0.974]
Epoch [60/120    avg_loss:0.008, val_acc:0.977]
Epoch [61/120    avg_loss:0.012, val_acc:0.977]
Epoch [62/120    avg_loss:0.010, val_acc:0.978]
Epoch [63/120    avg_loss:0.010, val_acc:0.977]
Epoch [64/120    avg_loss:0.008, val_acc:0.976]
Epoch [65/120    avg_loss:0.010, val_acc:0.974]
Epoch [66/120    avg_loss:0.008, val_acc:0.978]
Epoch [67/120    avg_loss:0.017, val_acc:0.979]
Epoch [68/120    avg_loss:0.010, val_acc:0.976]
Epoch [69/120    avg_loss:0.009, val_acc:0.976]
Epoch [70/120    avg_loss:0.007, val_acc:0.976]
Epoch [71/120    avg_loss:0.008, val_acc:0.976]
Epoch [72/120    avg_loss:0.011, val_acc:0.976]
Epoch [73/120    avg_loss:0.012, val_acc:0.976]
Epoch [74/120    avg_loss:0.008, val_acc:0.976]
Epoch [75/120    avg_loss:0.009, val_acc:0.976]
Epoch [76/120    avg_loss:0.006, val_acc:0.976]
Epoch [77/120    avg_loss:0.007, val_acc:0.976]
Epoch [78/120    avg_loss:0.008, val_acc:0.976]
Epoch [79/120    avg_loss:0.007, val_acc:0.976]
Epoch [80/120    avg_loss:0.010, val_acc:0.976]
Epoch [81/120    avg_loss:0.009, val_acc:0.976]
Epoch [82/120    avg_loss:0.010, val_acc:0.976]
Epoch [83/120    avg_loss:0.007, val_acc:0.976]
Epoch [84/120    avg_loss:0.009, val_acc:0.976]
Epoch [85/120    avg_loss:0.009, val_acc:0.976]
Epoch [86/120    avg_loss:0.006, val_acc:0.976]
Epoch [87/120    avg_loss:0.009, val_acc:0.976]
Epoch [88/120    avg_loss:0.007, val_acc:0.976]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.008, val_acc:0.976]
Epoch [91/120    avg_loss:0.007, val_acc:0.976]
Epoch [92/120    avg_loss:0.011, val_acc:0.976]
Epoch [93/120    avg_loss:0.016, val_acc:0.976]
Epoch [94/120    avg_loss:0.009, val_acc:0.976]
Epoch [95/120    avg_loss:0.011, val_acc:0.976]
Epoch [96/120    avg_loss:0.009, val_acc:0.976]
Epoch [97/120    avg_loss:0.012, val_acc:0.976]
Epoch [98/120    avg_loss:0.008, val_acc:0.976]
Epoch [99/120    avg_loss:0.013, val_acc:0.976]
Epoch [100/120    avg_loss:0.006, val_acc:0.976]
Epoch [101/120    avg_loss:0.011, val_acc:0.976]
Epoch [102/120    avg_loss:0.007, val_acc:0.976]
Epoch [103/120    avg_loss:0.006, val_acc:0.976]
Epoch [104/120    avg_loss:0.008, val_acc:0.976]
Epoch [105/120    avg_loss:0.006, val_acc:0.976]
Epoch [106/120    avg_loss:0.006, val_acc:0.976]
Epoch [107/120    avg_loss:0.006, val_acc:0.976]
Epoch [108/120    avg_loss:0.009, val_acc:0.976]
Epoch [109/120    avg_loss:0.007, val_acc:0.976]
Epoch [110/120    avg_loss:0.009, val_acc:0.976]
Epoch [111/120    avg_loss:0.012, val_acc:0.976]
Epoch [112/120    avg_loss:0.007, val_acc:0.976]
Epoch [113/120    avg_loss:0.010, val_acc:0.976]
Epoch [114/120    avg_loss:0.007, val_acc:0.976]
Epoch [115/120    avg_loss:0.007, val_acc:0.976]
Epoch [116/120    avg_loss:0.009, val_acc:0.976]
Epoch [117/120    avg_loss:0.009, val_acc:0.976]
Epoch [118/120    avg_loss:0.011, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.976]
Epoch [120/120    avg_loss:0.009, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0     3     1     0     0     8    65     1]
 [    0     1 16806     0   155     0  1123     0     5     0]
 [    0     1     0  2001     0     0     0     0    21    13]
 [    0    32     1     2  2911     0     1     0    22     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4866     0    11     0]
 [    0     4     0     0     0     0     5  1276     0     5]
 [    0     3     0    26    53     0     0     0  3486     3]
 [    0     0     0     0    21    29     0     0     0   869]]

Accuracy:
96.09813703516257

F1 scores:
[       nan 0.99072269 0.96317735 0.98353404 0.95239653 0.98901099
 0.89506116 0.99145299 0.97089542 0.9586321 ]

Kappa:
0.9489794883675934
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f57973177b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.363, val_acc:0.601]
Epoch [2/120    avg_loss:0.707, val_acc:0.761]
Epoch [3/120    avg_loss:0.537, val_acc:0.775]
Epoch [4/120    avg_loss:0.435, val_acc:0.823]
Epoch [5/120    avg_loss:0.371, val_acc:0.868]
Epoch [6/120    avg_loss:0.339, val_acc:0.903]
Epoch [7/120    avg_loss:0.305, val_acc:0.892]
Epoch [8/120    avg_loss:0.227, val_acc:0.920]
Epoch [9/120    avg_loss:0.255, val_acc:0.900]
Epoch [10/120    avg_loss:0.227, val_acc:0.878]
Epoch [11/120    avg_loss:0.197, val_acc:0.902]
Epoch [12/120    avg_loss:0.139, val_acc:0.917]
Epoch [13/120    avg_loss:0.242, val_acc:0.804]
Epoch [14/120    avg_loss:0.141, val_acc:0.924]
Epoch [15/120    avg_loss:0.127, val_acc:0.935]
Epoch [16/120    avg_loss:0.180, val_acc:0.863]
Epoch [17/120    avg_loss:0.179, val_acc:0.913]
Epoch [18/120    avg_loss:0.101, val_acc:0.948]
Epoch [19/120    avg_loss:0.138, val_acc:0.942]
Epoch [20/120    avg_loss:0.104, val_acc:0.950]
Epoch [21/120    avg_loss:0.113, val_acc:0.953]
Epoch [22/120    avg_loss:0.082, val_acc:0.953]
Epoch [23/120    avg_loss:0.063, val_acc:0.958]
Epoch [24/120    avg_loss:0.041, val_acc:0.976]
Epoch [25/120    avg_loss:0.038, val_acc:0.963]
Epoch [26/120    avg_loss:0.034, val_acc:0.976]
Epoch [27/120    avg_loss:0.057, val_acc:0.972]
Epoch [28/120    avg_loss:0.050, val_acc:0.968]
Epoch [29/120    avg_loss:0.065, val_acc:0.963]
Epoch [30/120    avg_loss:0.083, val_acc:0.972]
Epoch [31/120    avg_loss:0.026, val_acc:0.979]
Epoch [32/120    avg_loss:0.038, val_acc:0.964]
Epoch [33/120    avg_loss:0.020, val_acc:0.974]
Epoch [34/120    avg_loss:0.043, val_acc:0.964]
Epoch [35/120    avg_loss:0.045, val_acc:0.953]
Epoch [36/120    avg_loss:0.129, val_acc:0.968]
Epoch [37/120    avg_loss:0.063, val_acc:0.930]
Epoch [38/120    avg_loss:0.057, val_acc:0.960]
Epoch [39/120    avg_loss:0.045, val_acc:0.981]
Epoch [40/120    avg_loss:0.032, val_acc:0.971]
Epoch [41/120    avg_loss:0.034, val_acc:0.974]
Epoch [42/120    avg_loss:0.020, val_acc:0.980]
Epoch [43/120    avg_loss:0.022, val_acc:0.983]
Epoch [44/120    avg_loss:0.019, val_acc:0.983]
Epoch [45/120    avg_loss:0.032, val_acc:0.982]
Epoch [46/120    avg_loss:0.021, val_acc:0.986]
Epoch [47/120    avg_loss:0.013, val_acc:0.984]
Epoch [48/120    avg_loss:0.028, val_acc:0.978]
Epoch [49/120    avg_loss:0.014, val_acc:0.981]
Epoch [50/120    avg_loss:0.009, val_acc:0.983]
Epoch [51/120    avg_loss:0.019, val_acc:0.979]
Epoch [52/120    avg_loss:0.013, val_acc:0.983]
Epoch [53/120    avg_loss:0.011, val_acc:0.983]
Epoch [54/120    avg_loss:0.031, val_acc:0.962]
Epoch [55/120    avg_loss:0.028, val_acc:0.980]
Epoch [56/120    avg_loss:0.008, val_acc:0.980]
Epoch [57/120    avg_loss:0.098, val_acc:0.963]
Epoch [58/120    avg_loss:0.027, val_acc:0.979]
Epoch [59/120    avg_loss:0.026, val_acc:0.977]
Epoch [60/120    avg_loss:0.017, val_acc:0.983]
Epoch [61/120    avg_loss:0.014, val_acc:0.983]
Epoch [62/120    avg_loss:0.009, val_acc:0.983]
Epoch [63/120    avg_loss:0.008, val_acc:0.983]
Epoch [64/120    avg_loss:0.007, val_acc:0.984]
Epoch [65/120    avg_loss:0.009, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.984]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.007, val_acc:0.985]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.007, val_acc:0.985]
Epoch [71/120    avg_loss:0.006, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.004, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.005, val_acc:0.986]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.004, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.985]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.003, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     0     0     0    16     7     4     0]
 [    0     0 18043     0    26     0    14     0     7     0]
 [    0     0     0  2011     0     0     0     0    14    11]
 [    0    24     7     0  2923     0     3     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4862     0    16     0]
 [    0     1     0     0     0     0     3  1286     0     0]
 [    0     0     0    22    61     0     0     0  3477    11]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
99.27939652471501

F1 scores:
[       nan 0.99595708 0.99850581 0.98844925 0.97498332 0.99126472
 0.99468085 0.99574139 0.97888514 0.96763577]

Kappa:
0.9904560204757265
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7576557f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.312, val_acc:0.698]
Epoch [2/120    avg_loss:0.694, val_acc:0.727]
Epoch [3/120    avg_loss:0.551, val_acc:0.828]
Epoch [4/120    avg_loss:0.411, val_acc:0.852]
Epoch [5/120    avg_loss:0.351, val_acc:0.840]
Epoch [6/120    avg_loss:0.330, val_acc:0.779]
Epoch [7/120    avg_loss:0.375, val_acc:0.817]
Epoch [8/120    avg_loss:0.243, val_acc:0.840]
Epoch [9/120    avg_loss:0.302, val_acc:0.897]
Epoch [10/120    avg_loss:0.255, val_acc:0.912]
Epoch [11/120    avg_loss:0.222, val_acc:0.842]
Epoch [12/120    avg_loss:0.186, val_acc:0.876]
Epoch [13/120    avg_loss:0.143, val_acc:0.935]
Epoch [14/120    avg_loss:0.107, val_acc:0.925]
Epoch [15/120    avg_loss:0.145, val_acc:0.915]
Epoch [16/120    avg_loss:0.130, val_acc:0.940]
Epoch [17/120    avg_loss:0.120, val_acc:0.879]
Epoch [18/120    avg_loss:0.155, val_acc:0.918]
Epoch [19/120    avg_loss:0.081, val_acc:0.963]
Epoch [20/120    avg_loss:0.114, val_acc:0.934]
Epoch [21/120    avg_loss:0.094, val_acc:0.945]
Epoch [22/120    avg_loss:0.086, val_acc:0.966]
Epoch [23/120    avg_loss:0.065, val_acc:0.966]
Epoch [24/120    avg_loss:0.060, val_acc:0.953]
Epoch [25/120    avg_loss:0.080, val_acc:0.926]
Epoch [26/120    avg_loss:0.080, val_acc:0.969]
Epoch [27/120    avg_loss:0.043, val_acc:0.926]
Epoch [28/120    avg_loss:0.068, val_acc:0.963]
Epoch [29/120    avg_loss:0.048, val_acc:0.966]
Epoch [30/120    avg_loss:0.047, val_acc:0.960]
Epoch [31/120    avg_loss:0.059, val_acc:0.966]
Epoch [32/120    avg_loss:0.051, val_acc:0.965]
Epoch [33/120    avg_loss:0.053, val_acc:0.958]
Epoch [34/120    avg_loss:0.054, val_acc:0.952]
Epoch [35/120    avg_loss:0.034, val_acc:0.952]
Epoch [36/120    avg_loss:0.038, val_acc:0.971]
Epoch [37/120    avg_loss:0.025, val_acc:0.965]
Epoch [38/120    avg_loss:0.030, val_acc:0.977]
Epoch [39/120    avg_loss:0.033, val_acc:0.977]
Epoch [40/120    avg_loss:0.027, val_acc:0.956]
Epoch [41/120    avg_loss:0.042, val_acc:0.973]
Epoch [42/120    avg_loss:0.090, val_acc:0.937]
Epoch [43/120    avg_loss:0.184, val_acc:0.925]
Epoch [44/120    avg_loss:0.096, val_acc:0.966]
Epoch [45/120    avg_loss:0.046, val_acc:0.955]
Epoch [46/120    avg_loss:0.044, val_acc:0.971]
Epoch [47/120    avg_loss:0.036, val_acc:0.976]
Epoch [48/120    avg_loss:0.019, val_acc:0.972]
Epoch [49/120    avg_loss:0.017, val_acc:0.977]
Epoch [50/120    avg_loss:0.019, val_acc:0.969]
Epoch [51/120    avg_loss:0.024, val_acc:0.980]
Epoch [52/120    avg_loss:0.033, val_acc:0.980]
Epoch [53/120    avg_loss:0.028, val_acc:0.981]
Epoch [54/120    avg_loss:0.020, val_acc:0.979]
Epoch [55/120    avg_loss:0.019, val_acc:0.982]
Epoch [56/120    avg_loss:0.015, val_acc:0.975]
Epoch [57/120    avg_loss:0.016, val_acc:0.985]
Epoch [58/120    avg_loss:0.016, val_acc:0.984]
Epoch [59/120    avg_loss:0.022, val_acc:0.971]
Epoch [60/120    avg_loss:0.017, val_acc:0.974]
Epoch [61/120    avg_loss:0.014, val_acc:0.987]
Epoch [62/120    avg_loss:0.031, val_acc:0.961]
Epoch [63/120    avg_loss:0.013, val_acc:0.987]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.010, val_acc:0.974]
Epoch [66/120    avg_loss:0.014, val_acc:0.978]
Epoch [67/120    avg_loss:0.014, val_acc:0.977]
Epoch [68/120    avg_loss:0.012, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.985]
Epoch [70/120    avg_loss:0.009, val_acc:0.982]
Epoch [71/120    avg_loss:0.011, val_acc:0.982]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.011, val_acc:0.982]
Epoch [74/120    avg_loss:0.017, val_acc:0.984]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.004, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.985]
Epoch [89/120    avg_loss:0.005, val_acc:0.985]
Epoch [90/120    avg_loss:0.004, val_acc:0.984]
Epoch [91/120    avg_loss:0.004, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.004, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.003, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.984]
Epoch [110/120    avg_loss:0.003, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.004, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     0     0     0    15    16    21     0]
 [    0     0 17981     0    41     0    68     0     0     0]
 [    0     0     0  2020     0     0     0     0     4    12]
 [    0    25     8     0  2926     0     0     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0    18     0]
 [    0     1     0     0     0     0     0  1287     0     2]
 [    0     0     0    21    72     0     0     0  3478     0]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.0817728291519

F1 scores:
[       nan 0.99392429 0.99675712 0.9909247  0.97128631 0.98863636
 0.98971591 0.99267258 0.97902885 0.96792035]

Kappa:
0.9878472468443291
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91328807f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.373, val_acc:0.701]
Epoch [2/120    avg_loss:0.816, val_acc:0.732]
Epoch [3/120    avg_loss:0.611, val_acc:0.804]
Epoch [4/120    avg_loss:0.492, val_acc:0.751]
Epoch [5/120    avg_loss:0.486, val_acc:0.863]
Epoch [6/120    avg_loss:0.348, val_acc:0.857]
Epoch [7/120    avg_loss:0.348, val_acc:0.866]
Epoch [8/120    avg_loss:0.333, val_acc:0.838]
Epoch [9/120    avg_loss:0.278, val_acc:0.736]
Epoch [10/120    avg_loss:0.239, val_acc:0.871]
Epoch [11/120    avg_loss:0.195, val_acc:0.931]
Epoch [12/120    avg_loss:0.167, val_acc:0.921]
Epoch [13/120    avg_loss:0.230, val_acc:0.918]
Epoch [14/120    avg_loss:0.133, val_acc:0.891]
Epoch [15/120    avg_loss:0.120, val_acc:0.941]
Epoch [16/120    avg_loss:0.128, val_acc:0.916]
Epoch [17/120    avg_loss:0.101, val_acc:0.940]
Epoch [18/120    avg_loss:0.120, val_acc:0.947]
Epoch [19/120    avg_loss:0.089, val_acc:0.952]
Epoch [20/120    avg_loss:0.093, val_acc:0.923]
Epoch [21/120    avg_loss:0.120, val_acc:0.961]
Epoch [22/120    avg_loss:0.109, val_acc:0.965]
Epoch [23/120    avg_loss:0.076, val_acc:0.955]
Epoch [24/120    avg_loss:0.082, val_acc:0.966]
Epoch [25/120    avg_loss:0.053, val_acc:0.980]
Epoch [26/120    avg_loss:0.044, val_acc:0.981]
Epoch [27/120    avg_loss:0.045, val_acc:0.980]
Epoch [28/120    avg_loss:0.035, val_acc:0.965]
Epoch [29/120    avg_loss:0.043, val_acc:0.946]
Epoch [30/120    avg_loss:0.040, val_acc:0.983]
Epoch [31/120    avg_loss:0.051, val_acc:0.973]
Epoch [32/120    avg_loss:0.039, val_acc:0.976]
Epoch [33/120    avg_loss:0.067, val_acc:0.961]
Epoch [34/120    avg_loss:0.027, val_acc:0.981]
Epoch [35/120    avg_loss:0.041, val_acc:0.976]
Epoch [36/120    avg_loss:0.039, val_acc:0.980]
Epoch [37/120    avg_loss:0.019, val_acc:0.983]
Epoch [38/120    avg_loss:0.011, val_acc:0.976]
Epoch [39/120    avg_loss:0.015, val_acc:0.978]
Epoch [40/120    avg_loss:0.009, val_acc:0.984]
Epoch [41/120    avg_loss:0.008, val_acc:0.979]
Epoch [42/120    avg_loss:0.025, val_acc:0.983]
Epoch [43/120    avg_loss:0.038, val_acc:0.985]
Epoch [44/120    avg_loss:0.015, val_acc:0.987]
Epoch [45/120    avg_loss:0.014, val_acc:0.985]
Epoch [46/120    avg_loss:0.023, val_acc:0.960]
Epoch [47/120    avg_loss:0.015, val_acc:0.983]
Epoch [48/120    avg_loss:0.010, val_acc:0.987]
Epoch [49/120    avg_loss:0.007, val_acc:0.987]
Epoch [50/120    avg_loss:0.006, val_acc:0.987]
Epoch [51/120    avg_loss:0.007, val_acc:0.988]
Epoch [52/120    avg_loss:0.006, val_acc:0.991]
Epoch [53/120    avg_loss:0.017, val_acc:0.988]
Epoch [54/120    avg_loss:0.011, val_acc:0.986]
Epoch [55/120    avg_loss:0.006, val_acc:0.988]
Epoch [56/120    avg_loss:0.007, val_acc:0.990]
Epoch [57/120    avg_loss:0.017, val_acc:0.984]
Epoch [58/120    avg_loss:0.013, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.982]
Epoch [60/120    avg_loss:0.008, val_acc:0.989]
Epoch [61/120    avg_loss:0.005, val_acc:0.990]
Epoch [62/120    avg_loss:0.047, val_acc:0.866]
Epoch [63/120    avg_loss:0.149, val_acc:0.966]
Epoch [64/120    avg_loss:0.034, val_acc:0.945]
Epoch [65/120    avg_loss:0.091, val_acc:0.981]
Epoch [66/120    avg_loss:0.022, val_acc:0.984]
Epoch [67/120    avg_loss:0.018, val_acc:0.982]
Epoch [68/120    avg_loss:0.014, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.985]
Epoch [70/120    avg_loss:0.017, val_acc:0.986]
Epoch [71/120    avg_loss:0.016, val_acc:0.986]
Epoch [72/120    avg_loss:0.018, val_acc:0.988]
Epoch [73/120    avg_loss:0.012, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.987]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.015, val_acc:0.985]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.011, val_acc:0.985]
Epoch [79/120    avg_loss:0.019, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.010, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.987]
Epoch [83/120    avg_loss:0.014, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.012, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.009, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.012, val_acc:0.987]
Epoch [94/120    avg_loss:0.011, val_acc:0.987]
Epoch [95/120    avg_loss:0.008, val_acc:0.987]
Epoch [96/120    avg_loss:0.013, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.009, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.012, val_acc:0.987]
Epoch [101/120    avg_loss:0.016, val_acc:0.987]
Epoch [102/120    avg_loss:0.011, val_acc:0.987]
Epoch [103/120    avg_loss:0.012, val_acc:0.987]
Epoch [104/120    avg_loss:0.012, val_acc:0.987]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.012, val_acc:0.987]
Epoch [107/120    avg_loss:0.012, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.014, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.010, val_acc:0.987]
Epoch [118/120    avg_loss:0.010, val_acc:0.987]
Epoch [119/120    avg_loss:0.008, val_acc:0.987]
Epoch [120/120    avg_loss:0.011, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6322     0     0     0     0    11    14    85     0]
 [    0     0 18025     0    25     0    39     0     1     0]
 [    0     4     0  1977     1     0     0     0    45     9]
 [    0    40     7     0  2897     0     0     0    26     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4855     0    23     0]
 [    0     0     0     0     0     0     7  1279     0     4]
 [    0     9     0     7    53     0     0     0  3497     5]
 [    0     0     0     0    10    28     0     0     0   881]]

Accuracy:
98.90342949413154

F1 scores:
[       nan 0.98727259 0.99800675 0.98358209 0.97247398 0.9893859
 0.9918284  0.99032133 0.96495585 0.96813187]

Kappa:
0.9854799686223041
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f706479f780>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.447, val_acc:0.407]
Epoch [2/120    avg_loss:0.817, val_acc:0.683]
Epoch [3/120    avg_loss:0.627, val_acc:0.810]
Epoch [4/120    avg_loss:0.454, val_acc:0.826]
Epoch [5/120    avg_loss:0.381, val_acc:0.865]
Epoch [6/120    avg_loss:0.359, val_acc:0.859]
Epoch [7/120    avg_loss:0.316, val_acc:0.890]
Epoch [8/120    avg_loss:0.294, val_acc:0.929]
Epoch [9/120    avg_loss:0.222, val_acc:0.923]
Epoch [10/120    avg_loss:0.235, val_acc:0.792]
Epoch [11/120    avg_loss:0.263, val_acc:0.847]
Epoch [12/120    avg_loss:0.248, val_acc:0.923]
Epoch [13/120    avg_loss:0.173, val_acc:0.910]
Epoch [14/120    avg_loss:0.148, val_acc:0.896]
Epoch [15/120    avg_loss:0.155, val_acc:0.929]
Epoch [16/120    avg_loss:0.141, val_acc:0.958]
Epoch [17/120    avg_loss:0.099, val_acc:0.950]
Epoch [18/120    avg_loss:0.074, val_acc:0.956]
Epoch [19/120    avg_loss:0.099, val_acc:0.952]
Epoch [20/120    avg_loss:0.069, val_acc:0.961]
Epoch [21/120    avg_loss:0.087, val_acc:0.953]
Epoch [22/120    avg_loss:0.103, val_acc:0.944]
Epoch [23/120    avg_loss:0.068, val_acc:0.955]
Epoch [24/120    avg_loss:0.075, val_acc:0.943]
Epoch [25/120    avg_loss:0.051, val_acc:0.962]
Epoch [26/120    avg_loss:0.071, val_acc:0.948]
Epoch [27/120    avg_loss:0.052, val_acc:0.912]
Epoch [28/120    avg_loss:0.096, val_acc:0.920]
Epoch [29/120    avg_loss:0.058, val_acc:0.965]
Epoch [30/120    avg_loss:0.044, val_acc:0.961]
Epoch [31/120    avg_loss:0.038, val_acc:0.969]
Epoch [32/120    avg_loss:0.046, val_acc:0.967]
Epoch [33/120    avg_loss:0.047, val_acc:0.967]
Epoch [34/120    avg_loss:0.030, val_acc:0.975]
Epoch [35/120    avg_loss:0.025, val_acc:0.973]
Epoch [36/120    avg_loss:0.029, val_acc:0.971]
Epoch [37/120    avg_loss:0.034, val_acc:0.979]
Epoch [38/120    avg_loss:0.045, val_acc:0.973]
Epoch [39/120    avg_loss:0.030, val_acc:0.980]
Epoch [40/120    avg_loss:0.022, val_acc:0.967]
Epoch [41/120    avg_loss:0.033, val_acc:0.978]
Epoch [42/120    avg_loss:0.044, val_acc:0.972]
Epoch [43/120    avg_loss:0.026, val_acc:0.972]
Epoch [44/120    avg_loss:0.015, val_acc:0.978]
Epoch [45/120    avg_loss:0.016, val_acc:0.981]
Epoch [46/120    avg_loss:0.052, val_acc:0.955]
Epoch [47/120    avg_loss:0.058, val_acc:0.950]
Epoch [48/120    avg_loss:0.044, val_acc:0.962]
Epoch [49/120    avg_loss:0.035, val_acc:0.972]
Epoch [50/120    avg_loss:0.013, val_acc:0.977]
Epoch [51/120    avg_loss:0.014, val_acc:0.980]
Epoch [52/120    avg_loss:0.012, val_acc:0.974]
Epoch [53/120    avg_loss:0.010, val_acc:0.972]
Epoch [54/120    avg_loss:0.013, val_acc:0.983]
Epoch [55/120    avg_loss:0.016, val_acc:0.974]
Epoch [56/120    avg_loss:0.074, val_acc:0.916]
Epoch [57/120    avg_loss:0.069, val_acc:0.976]
Epoch [58/120    avg_loss:0.015, val_acc:0.980]
Epoch [59/120    avg_loss:0.031, val_acc:0.961]
Epoch [60/120    avg_loss:0.015, val_acc:0.982]
Epoch [61/120    avg_loss:0.015, val_acc:0.985]
Epoch [62/120    avg_loss:0.071, val_acc:0.978]
Epoch [63/120    avg_loss:0.030, val_acc:0.983]
Epoch [64/120    avg_loss:0.012, val_acc:0.976]
Epoch [65/120    avg_loss:0.020, val_acc:0.979]
Epoch [66/120    avg_loss:0.018, val_acc:0.980]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.011, val_acc:0.983]
Epoch [69/120    avg_loss:0.012, val_acc:0.979]
Epoch [70/120    avg_loss:0.010, val_acc:0.979]
Epoch [71/120    avg_loss:0.025, val_acc:0.980]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.008, val_acc:0.983]
Epoch [74/120    avg_loss:0.007, val_acc:0.974]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.006, val_acc:0.985]
Epoch [77/120    avg_loss:0.007, val_acc:0.985]
Epoch [78/120    avg_loss:0.005, val_acc:0.983]
Epoch [79/120    avg_loss:0.004, val_acc:0.984]
Epoch [80/120    avg_loss:0.004, val_acc:0.986]
Epoch [81/120    avg_loss:0.003, val_acc:0.987]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.980]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.983]
Epoch [88/120    avg_loss:0.003, val_acc:0.985]
Epoch [89/120    avg_loss:0.003, val_acc:0.985]
Epoch [90/120    avg_loss:0.004, val_acc:0.983]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.004, val_acc:0.984]
Epoch [95/120    avg_loss:0.003, val_acc:0.985]
Epoch [96/120    avg_loss:0.002, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.988]
Epoch [104/120    avg_loss:0.013, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.003, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.016, val_acc:0.979]
Epoch [112/120    avg_loss:0.017, val_acc:0.965]
Epoch [113/120    avg_loss:0.021, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6326     0     1     6     0    33    31    35     0]
 [    0     0 18033     0    47     0     0     0    10     0]
 [    0     0     0  2021     0     0     0     0     0    15]
 [    0    22     1     0  2914     0     7     0    28     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4851     0    22     5]
 [    0     0     0     0     0     0     1  1278     0    11]
 [    0     0     0     1    61     0     0     0  3507     2]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.07695273901622

F1 scores:
[       nan 0.98998435 0.99839442 0.99581178 0.96907216 0.98862775
 0.99303992 0.98345518 0.97783354 0.95790049]

Kappa:
0.9877797879293532
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4ff7f37f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.519, val_acc:0.701]
Epoch [2/120    avg_loss:0.827, val_acc:0.741]
Epoch [3/120    avg_loss:0.595, val_acc:0.785]
Epoch [4/120    avg_loss:0.515, val_acc:0.756]
Epoch [5/120    avg_loss:0.456, val_acc:0.879]
Epoch [6/120    avg_loss:0.360, val_acc:0.888]
Epoch [7/120    avg_loss:0.378, val_acc:0.859]
Epoch [8/120    avg_loss:0.250, val_acc:0.893]
Epoch [9/120    avg_loss:0.235, val_acc:0.899]
Epoch [10/120    avg_loss:0.227, val_acc:0.886]
Epoch [11/120    avg_loss:0.275, val_acc:0.930]
Epoch [12/120    avg_loss:0.197, val_acc:0.937]
Epoch [13/120    avg_loss:0.205, val_acc:0.937]
Epoch [14/120    avg_loss:0.156, val_acc:0.898]
Epoch [15/120    avg_loss:0.144, val_acc:0.945]
Epoch [16/120    avg_loss:0.188, val_acc:0.950]
Epoch [17/120    avg_loss:0.136, val_acc:0.904]
Epoch [18/120    avg_loss:0.121, val_acc:0.963]
Epoch [19/120    avg_loss:0.083, val_acc:0.920]
Epoch [20/120    avg_loss:0.138, val_acc:0.922]
Epoch [21/120    avg_loss:0.096, val_acc:0.852]
Epoch [22/120    avg_loss:0.128, val_acc:0.953]
Epoch [23/120    avg_loss:0.115, val_acc:0.933]
Epoch [24/120    avg_loss:0.110, val_acc:0.918]
Epoch [25/120    avg_loss:0.091, val_acc:0.953]
Epoch [26/120    avg_loss:0.099, val_acc:0.893]
Epoch [27/120    avg_loss:0.069, val_acc:0.956]
Epoch [28/120    avg_loss:0.058, val_acc:0.972]
Epoch [29/120    avg_loss:0.044, val_acc:0.971]
Epoch [30/120    avg_loss:0.053, val_acc:0.977]
Epoch [31/120    avg_loss:0.046, val_acc:0.973]
Epoch [32/120    avg_loss:0.045, val_acc:0.948]
Epoch [33/120    avg_loss:0.042, val_acc:0.961]
Epoch [34/120    avg_loss:0.032, val_acc:0.966]
Epoch [35/120    avg_loss:0.039, val_acc:0.981]
Epoch [36/120    avg_loss:0.055, val_acc:0.970]
Epoch [37/120    avg_loss:0.021, val_acc:0.975]
Epoch [38/120    avg_loss:0.016, val_acc:0.973]
Epoch [39/120    avg_loss:0.042, val_acc:0.974]
Epoch [40/120    avg_loss:0.039, val_acc:0.974]
Epoch [41/120    avg_loss:0.103, val_acc:0.965]
Epoch [42/120    avg_loss:0.081, val_acc:0.918]
Epoch [43/120    avg_loss:0.036, val_acc:0.971]
Epoch [44/120    avg_loss:0.031, val_acc:0.971]
Epoch [45/120    avg_loss:0.035, val_acc:0.977]
Epoch [46/120    avg_loss:0.019, val_acc:0.971]
Epoch [47/120    avg_loss:0.020, val_acc:0.975]
Epoch [48/120    avg_loss:0.030, val_acc:0.981]
Epoch [49/120    avg_loss:0.036, val_acc:0.974]
Epoch [50/120    avg_loss:0.036, val_acc:0.982]
Epoch [51/120    avg_loss:0.010, val_acc:0.983]
Epoch [52/120    avg_loss:0.020, val_acc:0.972]
Epoch [53/120    avg_loss:0.056, val_acc:0.982]
Epoch [54/120    avg_loss:0.013, val_acc:0.987]
Epoch [55/120    avg_loss:0.061, val_acc:0.982]
Epoch [56/120    avg_loss:0.031, val_acc:0.985]
Epoch [57/120    avg_loss:0.015, val_acc:0.987]
Epoch [58/120    avg_loss:0.010, val_acc:0.987]
Epoch [59/120    avg_loss:0.014, val_acc:0.984]
Epoch [60/120    avg_loss:0.011, val_acc:0.986]
Epoch [61/120    avg_loss:0.009, val_acc:0.982]
Epoch [62/120    avg_loss:0.011, val_acc:0.982]
Epoch [63/120    avg_loss:0.007, val_acc:0.987]
Epoch [64/120    avg_loss:0.009, val_acc:0.979]
Epoch [65/120    avg_loss:0.009, val_acc:0.984]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.010, val_acc:0.985]
Epoch [68/120    avg_loss:0.018, val_acc:0.987]
Epoch [69/120    avg_loss:0.011, val_acc:0.988]
Epoch [70/120    avg_loss:0.010, val_acc:0.988]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.009, val_acc:0.981]
Epoch [73/120    avg_loss:0.008, val_acc:0.987]
Epoch [74/120    avg_loss:0.019, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.983]
Epoch [76/120    avg_loss:0.004, val_acc:0.987]
Epoch [77/120    avg_loss:0.004, val_acc:0.988]
Epoch [78/120    avg_loss:0.005, val_acc:0.986]
Epoch [79/120    avg_loss:0.004, val_acc:0.988]
Epoch [80/120    avg_loss:0.004, val_acc:0.990]
Epoch [81/120    avg_loss:0.028, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.982]
Epoch [83/120    avg_loss:0.005, val_acc:0.989]
Epoch [84/120    avg_loss:0.012, val_acc:0.967]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.004, val_acc:0.987]
Epoch [89/120    avg_loss:0.003, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.991]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.002, val_acc:0.991]
Epoch [97/120    avg_loss:0.003, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.003, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.003, val_acc:0.990]
Epoch [102/120    avg_loss:0.003, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.991]
Epoch [106/120    avg_loss:0.003, val_acc:0.990]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.002, val_acc:0.990]
Epoch [110/120    avg_loss:0.002, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.002, val_acc:0.988]
Epoch [113/120    avg_loss:0.002, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.002, val_acc:0.988]
Epoch [119/120    avg_loss:0.002, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0     0     0     0    25    37    14     0]
 [    0     0 18077     0    13     0     0     0     0     0]
 [    0     0     0  2001     1     0     0     0    25     9]
 [    0    28     6     0  2923     0     0     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4869     0     1     6]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     1     0     0    55     0     0     0  3507     8]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.29144675005423

F1 scores:
[       nan 0.99180776 0.99947475 0.99083932 0.97791904 0.98863636
 0.99631676 0.98430922 0.98331698 0.96206707]

Kappa:
0.990613229238211
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9c449c37b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.479, val_acc:0.577]
Epoch [2/120    avg_loss:0.856, val_acc:0.605]
Epoch [3/120    avg_loss:0.595, val_acc:0.658]
Epoch [4/120    avg_loss:0.451, val_acc:0.864]
Epoch [5/120    avg_loss:0.431, val_acc:0.779]
Epoch [6/120    avg_loss:0.352, val_acc:0.862]
Epoch [7/120    avg_loss:0.398, val_acc:0.741]
Epoch [8/120    avg_loss:0.327, val_acc:0.797]
Epoch [9/120    avg_loss:0.254, val_acc:0.931]
Epoch [10/120    avg_loss:0.171, val_acc:0.932]
Epoch [11/120    avg_loss:0.195, val_acc:0.917]
Epoch [12/120    avg_loss:0.207, val_acc:0.883]
Epoch [13/120    avg_loss:0.219, val_acc:0.897]
Epoch [14/120    avg_loss:0.187, val_acc:0.901]
Epoch [15/120    avg_loss:0.147, val_acc:0.907]
Epoch [16/120    avg_loss:0.127, val_acc:0.933]
Epoch [17/120    avg_loss:0.099, val_acc:0.840]
Epoch [18/120    avg_loss:0.145, val_acc:0.943]
Epoch [19/120    avg_loss:0.099, val_acc:0.857]
Epoch [20/120    avg_loss:0.166, val_acc:0.931]
Epoch [21/120    avg_loss:0.112, val_acc:0.953]
Epoch [22/120    avg_loss:0.063, val_acc:0.963]
Epoch [23/120    avg_loss:0.055, val_acc:0.934]
Epoch [24/120    avg_loss:0.056, val_acc:0.964]
Epoch [25/120    avg_loss:0.072, val_acc:0.950]
Epoch [26/120    avg_loss:0.109, val_acc:0.944]
Epoch [27/120    avg_loss:0.088, val_acc:0.946]
Epoch [28/120    avg_loss:0.067, val_acc:0.954]
Epoch [29/120    avg_loss:0.105, val_acc:0.966]
Epoch [30/120    avg_loss:0.073, val_acc:0.951]
Epoch [31/120    avg_loss:0.045, val_acc:0.958]
Epoch [32/120    avg_loss:0.084, val_acc:0.941]
Epoch [33/120    avg_loss:0.053, val_acc:0.959]
Epoch [34/120    avg_loss:0.074, val_acc:0.970]
Epoch [35/120    avg_loss:0.044, val_acc:0.973]
Epoch [36/120    avg_loss:0.029, val_acc:0.963]
Epoch [37/120    avg_loss:0.030, val_acc:0.977]
Epoch [38/120    avg_loss:0.036, val_acc:0.975]
Epoch [39/120    avg_loss:0.030, val_acc:0.978]
Epoch [40/120    avg_loss:0.017, val_acc:0.978]
Epoch [41/120    avg_loss:0.033, val_acc:0.979]
Epoch [42/120    avg_loss:0.027, val_acc:0.980]
Epoch [43/120    avg_loss:0.021, val_acc:0.980]
Epoch [44/120    avg_loss:0.025, val_acc:0.982]
Epoch [45/120    avg_loss:0.013, val_acc:0.975]
Epoch [46/120    avg_loss:0.036, val_acc:0.981]
Epoch [47/120    avg_loss:0.025, val_acc:0.978]
Epoch [48/120    avg_loss:0.016, val_acc:0.978]
Epoch [49/120    avg_loss:0.014, val_acc:0.978]
Epoch [50/120    avg_loss:0.019, val_acc:0.979]
Epoch [51/120    avg_loss:0.019, val_acc:0.977]
Epoch [52/120    avg_loss:0.010, val_acc:0.971]
Epoch [53/120    avg_loss:0.050, val_acc:0.976]
Epoch [54/120    avg_loss:0.052, val_acc:0.892]
Epoch [55/120    avg_loss:0.024, val_acc:0.987]
Epoch [56/120    avg_loss:0.011, val_acc:0.984]
Epoch [57/120    avg_loss:0.017, val_acc:0.985]
Epoch [58/120    avg_loss:0.008, val_acc:0.988]
Epoch [59/120    avg_loss:0.021, val_acc:0.986]
Epoch [60/120    avg_loss:0.017, val_acc:0.983]
Epoch [61/120    avg_loss:0.011, val_acc:0.978]
Epoch [62/120    avg_loss:0.007, val_acc:0.984]
Epoch [63/120    avg_loss:0.009, val_acc:0.981]
Epoch [64/120    avg_loss:0.009, val_acc:0.984]
Epoch [65/120    avg_loss:0.009, val_acc:0.973]
Epoch [66/120    avg_loss:0.010, val_acc:0.987]
Epoch [67/120    avg_loss:0.036, val_acc:0.957]
Epoch [68/120    avg_loss:0.011, val_acc:0.986]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.006, val_acc:0.987]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.010, val_acc:0.985]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.984]
Epoch [81/120    avg_loss:0.016, val_acc:0.983]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.004, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.985]
Epoch [85/120    avg_loss:0.004, val_acc:0.985]
Epoch [86/120    avg_loss:0.004, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.984]
Epoch [88/120    avg_loss:0.003, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.984]
Epoch [90/120    avg_loss:0.004, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.003, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.003, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     1     0    19     1     9     0]
 [    0     0 18021     0    22     0    43     0     4     0]
 [    0     0     0  1997     1     0     0     0    21    17]
 [    0    34    11     0  2902     0     7     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0     1     0     7    67     0     0     0  3496     0]
 [    0     0     0     0    14    33     0     0     0   872]]

Accuracy:
99.18781481213699

F1 scores:
[       nan 0.9949491  0.99778528 0.98861386 0.97073089 0.98751419
 0.99277501 0.996892   0.98216042 0.96194153]

Kappa:
0.9892438640438094
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa18076f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.357, val_acc:0.687]
Epoch [2/120    avg_loss:0.726, val_acc:0.749]
Epoch [3/120    avg_loss:0.506, val_acc:0.816]
Epoch [4/120    avg_loss:0.460, val_acc:0.844]
Epoch [5/120    avg_loss:0.369, val_acc:0.792]
Epoch [6/120    avg_loss:0.334, val_acc:0.874]
Epoch [7/120    avg_loss:0.248, val_acc:0.897]
Epoch [8/120    avg_loss:0.197, val_acc:0.923]
Epoch [9/120    avg_loss:0.279, val_acc:0.808]
Epoch [10/120    avg_loss:0.251, val_acc:0.915]
Epoch [11/120    avg_loss:0.194, val_acc:0.852]
Epoch [12/120    avg_loss:0.146, val_acc:0.876]
Epoch [13/120    avg_loss:0.189, val_acc:0.896]
Epoch [14/120    avg_loss:0.157, val_acc:0.846]
Epoch [15/120    avg_loss:0.150, val_acc:0.860]
Epoch [16/120    avg_loss:0.144, val_acc:0.911]
Epoch [17/120    avg_loss:0.102, val_acc:0.925]
Epoch [18/120    avg_loss:0.141, val_acc:0.955]
Epoch [19/120    avg_loss:0.091, val_acc:0.952]
Epoch [20/120    avg_loss:0.074, val_acc:0.951]
Epoch [21/120    avg_loss:0.109, val_acc:0.971]
Epoch [22/120    avg_loss:0.087, val_acc:0.956]
Epoch [23/120    avg_loss:0.102, val_acc:0.967]
Epoch [24/120    avg_loss:0.043, val_acc:0.955]
Epoch [25/120    avg_loss:0.120, val_acc:0.905]
Epoch [26/120    avg_loss:0.099, val_acc:0.957]
Epoch [27/120    avg_loss:0.078, val_acc:0.957]
Epoch [28/120    avg_loss:0.048, val_acc:0.979]
Epoch [29/120    avg_loss:0.028, val_acc:0.974]
Epoch [30/120    avg_loss:0.066, val_acc:0.974]
Epoch [31/120    avg_loss:0.033, val_acc:0.982]
Epoch [32/120    avg_loss:0.041, val_acc:0.949]
Epoch [33/120    avg_loss:0.045, val_acc:0.983]
Epoch [34/120    avg_loss:0.067, val_acc:0.970]
Epoch [35/120    avg_loss:0.043, val_acc:0.972]
Epoch [36/120    avg_loss:0.029, val_acc:0.980]
Epoch [37/120    avg_loss:0.061, val_acc:0.955]
Epoch [38/120    avg_loss:0.129, val_acc:0.973]
Epoch [39/120    avg_loss:0.050, val_acc:0.965]
Epoch [40/120    avg_loss:0.061, val_acc:0.951]
Epoch [41/120    avg_loss:0.042, val_acc:0.984]
Epoch [42/120    avg_loss:0.022, val_acc:0.966]
Epoch [43/120    avg_loss:0.028, val_acc:0.985]
Epoch [44/120    avg_loss:0.014, val_acc:0.971]
Epoch [45/120    avg_loss:0.012, val_acc:0.986]
Epoch [46/120    avg_loss:0.027, val_acc:0.980]
Epoch [47/120    avg_loss:0.024, val_acc:0.987]
Epoch [48/120    avg_loss:0.025, val_acc:0.985]
Epoch [49/120    avg_loss:0.009, val_acc:0.985]
Epoch [50/120    avg_loss:0.013, val_acc:0.985]
Epoch [51/120    avg_loss:0.009, val_acc:0.986]
Epoch [52/120    avg_loss:0.012, val_acc:0.984]
Epoch [53/120    avg_loss:0.019, val_acc:0.979]
Epoch [54/120    avg_loss:0.015, val_acc:0.985]
Epoch [55/120    avg_loss:0.047, val_acc:0.967]
Epoch [56/120    avg_loss:0.040, val_acc:0.973]
Epoch [57/120    avg_loss:0.023, val_acc:0.972]
Epoch [58/120    avg_loss:0.026, val_acc:0.985]
Epoch [59/120    avg_loss:0.024, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.935]
Epoch [61/120    avg_loss:0.022, val_acc:0.984]
Epoch [62/120    avg_loss:0.008, val_acc:0.988]
Epoch [63/120    avg_loss:0.007, val_acc:0.988]
Epoch [64/120    avg_loss:0.008, val_acc:0.988]
Epoch [65/120    avg_loss:0.006, val_acc:0.989]
Epoch [66/120    avg_loss:0.007, val_acc:0.988]
Epoch [67/120    avg_loss:0.009, val_acc:0.990]
Epoch [68/120    avg_loss:0.006, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.989]
Epoch [70/120    avg_loss:0.007, val_acc:0.989]
Epoch [71/120    avg_loss:0.006, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.015, val_acc:0.989]
Epoch [74/120    avg_loss:0.006, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.990]
Epoch [76/120    avg_loss:0.003, val_acc:0.990]
Epoch [77/120    avg_loss:0.006, val_acc:0.989]
Epoch [78/120    avg_loss:0.007, val_acc:0.989]
Epoch [79/120    avg_loss:0.007, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.990]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.004, val_acc:0.991]
Epoch [83/120    avg_loss:0.006, val_acc:0.991]
Epoch [84/120    avg_loss:0.004, val_acc:0.991]
Epoch [85/120    avg_loss:0.004, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.991]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.010, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.005, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.010, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6341     0     8     0     0    23    30    30     0]
 [    0     0 17981     0    48     0    45     0    16     0]
 [    0     0     0  2025     2     0     0     0     5     4]
 [    0    27    12     0  2912     0     0     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    10     0     0  4853     0    12     3]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     0     0     5    43     0     0     0  3514     9]
 [    0     0     0     1    20    56     0     0     1   841]]

Accuracy:
98.95645048562409

F1 scores:
[       nan 0.99078125 0.99664662 0.99143207 0.97115224 0.97899475
 0.99050924 0.98773006 0.98019526 0.94600675]

Kappa:
0.9861895426672288
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f572b8947b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.396, val_acc:0.587]
Epoch [2/120    avg_loss:0.774, val_acc:0.669]
Epoch [3/120    avg_loss:0.603, val_acc:0.748]
Epoch [4/120    avg_loss:0.517, val_acc:0.780]
Epoch [5/120    avg_loss:0.416, val_acc:0.776]
Epoch [6/120    avg_loss:0.344, val_acc:0.829]
Epoch [7/120    avg_loss:0.322, val_acc:0.832]
Epoch [8/120    avg_loss:0.318, val_acc:0.867]
Epoch [9/120    avg_loss:0.250, val_acc:0.868]
Epoch [10/120    avg_loss:0.222, val_acc:0.883]
Epoch [11/120    avg_loss:0.231, val_acc:0.906]
Epoch [12/120    avg_loss:0.206, val_acc:0.931]
Epoch [13/120    avg_loss:0.156, val_acc:0.944]
Epoch [14/120    avg_loss:0.130, val_acc:0.943]
Epoch [15/120    avg_loss:0.144, val_acc:0.869]
Epoch [16/120    avg_loss:0.089, val_acc:0.938]
Epoch [17/120    avg_loss:0.092, val_acc:0.912]
Epoch [18/120    avg_loss:0.132, val_acc:0.932]
Epoch [19/120    avg_loss:0.203, val_acc:0.931]
Epoch [20/120    avg_loss:0.087, val_acc:0.924]
Epoch [21/120    avg_loss:0.100, val_acc:0.950]
Epoch [22/120    avg_loss:0.093, val_acc:0.970]
Epoch [23/120    avg_loss:0.073, val_acc:0.959]
Epoch [24/120    avg_loss:0.044, val_acc:0.955]
Epoch [25/120    avg_loss:0.049, val_acc:0.948]
Epoch [26/120    avg_loss:0.034, val_acc:0.946]
Epoch [27/120    avg_loss:0.041, val_acc:0.980]
Epoch [28/120    avg_loss:0.070, val_acc:0.948]
Epoch [29/120    avg_loss:0.053, val_acc:0.969]
Epoch [30/120    avg_loss:0.032, val_acc:0.981]
Epoch [31/120    avg_loss:0.060, val_acc:0.972]
Epoch [32/120    avg_loss:0.042, val_acc:0.971]
Epoch [33/120    avg_loss:0.035, val_acc:0.980]
Epoch [34/120    avg_loss:0.024, val_acc:0.979]
Epoch [35/120    avg_loss:0.017, val_acc:0.981]
Epoch [36/120    avg_loss:0.022, val_acc:0.973]
Epoch [37/120    avg_loss:0.025, val_acc:0.970]
Epoch [38/120    avg_loss:0.036, val_acc:0.986]
Epoch [39/120    avg_loss:0.083, val_acc:0.956]
Epoch [40/120    avg_loss:0.067, val_acc:0.976]
Epoch [41/120    avg_loss:0.041, val_acc:0.983]
Epoch [42/120    avg_loss:0.053, val_acc:0.981]
Epoch [43/120    avg_loss:0.028, val_acc:0.964]
Epoch [44/120    avg_loss:0.037, val_acc:0.976]
Epoch [45/120    avg_loss:0.064, val_acc:0.970]
Epoch [46/120    avg_loss:0.029, val_acc:0.974]
Epoch [47/120    avg_loss:0.016, val_acc:0.984]
Epoch [48/120    avg_loss:0.022, val_acc:0.981]
Epoch [49/120    avg_loss:0.034, val_acc:0.986]
Epoch [50/120    avg_loss:0.013, val_acc:0.983]
Epoch [51/120    avg_loss:0.026, val_acc:0.978]
Epoch [52/120    avg_loss:0.027, val_acc:0.979]
Epoch [53/120    avg_loss:0.018, val_acc:0.985]
Epoch [54/120    avg_loss:0.014, val_acc:0.985]
Epoch [55/120    avg_loss:0.011, val_acc:0.986]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.011, val_acc:0.988]
Epoch [58/120    avg_loss:0.008, val_acc:0.984]
Epoch [59/120    avg_loss:0.012, val_acc:0.981]
Epoch [60/120    avg_loss:0.010, val_acc:0.990]
Epoch [61/120    avg_loss:0.015, val_acc:0.984]
Epoch [62/120    avg_loss:0.011, val_acc:0.990]
Epoch [63/120    avg_loss:0.034, val_acc:0.985]
Epoch [64/120    avg_loss:0.011, val_acc:0.986]
Epoch [65/120    avg_loss:0.017, val_acc:0.989]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.016, val_acc:0.986]
Epoch [68/120    avg_loss:0.010, val_acc:0.991]
Epoch [69/120    avg_loss:0.013, val_acc:0.990]
Epoch [70/120    avg_loss:0.078, val_acc:0.939]
Epoch [71/120    avg_loss:0.080, val_acc:0.978]
Epoch [72/120    avg_loss:0.057, val_acc:0.978]
Epoch [73/120    avg_loss:0.038, val_acc:0.981]
Epoch [74/120    avg_loss:0.022, val_acc:0.965]
Epoch [75/120    avg_loss:0.016, val_acc:0.986]
Epoch [76/120    avg_loss:0.013, val_acc:0.981]
Epoch [77/120    avg_loss:0.017, val_acc:0.978]
Epoch [78/120    avg_loss:0.020, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.989]
Epoch [83/120    avg_loss:0.012, val_acc:0.992]
Epoch [84/120    avg_loss:0.009, val_acc:0.990]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.009, val_acc:0.992]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.006, val_acc:0.991]
Epoch [92/120    avg_loss:0.006, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.991]
Epoch [96/120    avg_loss:0.009, val_acc:0.990]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.006, val_acc:0.992]
Epoch [105/120    avg_loss:0.008, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.003, val_acc:0.992]
Epoch [110/120    avg_loss:0.004, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.992]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     1     0    22     5     2     0]
 [    0     1 18072     0    12     0     4     0     1     0]
 [    0     0     0  2002     3     0     0     0    23     8]
 [    0    40    20     0  2882     0     3     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4861     0    14     3]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     1     0    13    58     0     0     0  3486    13]
 [    0     0     0     0    14    59     0     0     0   846]]

Accuracy:
99.15889427132288

F1 scores:
[       nan 0.9944082  0.99894975 0.98839793 0.97004376 0.97789434
 0.99529075 0.99728997 0.97866367 0.94472362]

Kappa:
0.9888540124135071
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa9efcc8748>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.382, val_acc:0.568]
Epoch [2/120    avg_loss:0.765, val_acc:0.720]
Epoch [3/120    avg_loss:0.577, val_acc:0.795]
Epoch [4/120    avg_loss:0.510, val_acc:0.815]
Epoch [5/120    avg_loss:0.444, val_acc:0.785]
Epoch [6/120    avg_loss:0.332, val_acc:0.864]
Epoch [7/120    avg_loss:0.303, val_acc:0.844]
Epoch [8/120    avg_loss:0.253, val_acc:0.928]
Epoch [9/120    avg_loss:0.243, val_acc:0.903]
Epoch [10/120    avg_loss:0.218, val_acc:0.936]
Epoch [11/120    avg_loss:0.174, val_acc:0.902]
Epoch [12/120    avg_loss:0.162, val_acc:0.912]
Epoch [13/120    avg_loss:0.287, val_acc:0.921]
Epoch [14/120    avg_loss:0.179, val_acc:0.880]
Epoch [15/120    avg_loss:0.159, val_acc:0.929]
Epoch [16/120    avg_loss:0.158, val_acc:0.958]
Epoch [17/120    avg_loss:0.179, val_acc:0.938]
Epoch [18/120    avg_loss:0.122, val_acc:0.949]
Epoch [19/120    avg_loss:0.139, val_acc:0.922]
Epoch [20/120    avg_loss:0.090, val_acc:0.870]
Epoch [21/120    avg_loss:0.076, val_acc:0.971]
Epoch [22/120    avg_loss:0.050, val_acc:0.963]
Epoch [23/120    avg_loss:0.144, val_acc:0.970]
Epoch [24/120    avg_loss:0.074, val_acc:0.953]
Epoch [25/120    avg_loss:0.046, val_acc:0.969]
Epoch [26/120    avg_loss:0.070, val_acc:0.958]
Epoch [27/120    avg_loss:0.039, val_acc:0.970]
Epoch [28/120    avg_loss:0.041, val_acc:0.966]
Epoch [29/120    avg_loss:0.075, val_acc:0.945]
Epoch [30/120    avg_loss:0.051, val_acc:0.984]
Epoch [31/120    avg_loss:0.022, val_acc:0.969]
Epoch [32/120    avg_loss:0.023, val_acc:0.967]
Epoch [33/120    avg_loss:0.048, val_acc:0.983]
Epoch [34/120    avg_loss:0.027, val_acc:0.980]
Epoch [35/120    avg_loss:0.027, val_acc:0.966]
Epoch [36/120    avg_loss:0.020, val_acc:0.982]
Epoch [37/120    avg_loss:0.025, val_acc:0.978]
Epoch [38/120    avg_loss:0.026, val_acc:0.980]
Epoch [39/120    avg_loss:0.038, val_acc:0.989]
Epoch [40/120    avg_loss:0.026, val_acc:0.974]
Epoch [41/120    avg_loss:0.015, val_acc:0.990]
Epoch [42/120    avg_loss:0.041, val_acc:0.972]
Epoch [43/120    avg_loss:0.027, val_acc:0.986]
Epoch [44/120    avg_loss:0.027, val_acc:0.979]
Epoch [45/120    avg_loss:0.017, val_acc:0.989]
Epoch [46/120    avg_loss:0.012, val_acc:0.988]
Epoch [47/120    avg_loss:0.012, val_acc:0.983]
Epoch [48/120    avg_loss:0.008, val_acc:0.989]
Epoch [49/120    avg_loss:0.022, val_acc:0.982]
Epoch [50/120    avg_loss:0.017, val_acc:0.990]
Epoch [51/120    avg_loss:0.017, val_acc:0.982]
Epoch [52/120    avg_loss:0.012, val_acc:0.984]
Epoch [53/120    avg_loss:0.019, val_acc:0.987]
Epoch [54/120    avg_loss:0.056, val_acc:0.944]
Epoch [55/120    avg_loss:0.082, val_acc:0.942]
Epoch [56/120    avg_loss:0.039, val_acc:0.979]
Epoch [57/120    avg_loss:0.015, val_acc:0.991]
Epoch [58/120    avg_loss:0.015, val_acc:0.985]
Epoch [59/120    avg_loss:0.018, val_acc:0.992]
Epoch [60/120    avg_loss:0.007, val_acc:0.990]
Epoch [61/120    avg_loss:0.006, val_acc:0.991]
Epoch [62/120    avg_loss:0.005, val_acc:0.990]
Epoch [63/120    avg_loss:0.015, val_acc:0.975]
Epoch [64/120    avg_loss:0.014, val_acc:0.991]
Epoch [65/120    avg_loss:0.004, val_acc:0.995]
Epoch [66/120    avg_loss:0.056, val_acc:0.958]
Epoch [67/120    avg_loss:0.038, val_acc:0.974]
Epoch [68/120    avg_loss:0.041, val_acc:0.963]
Epoch [69/120    avg_loss:0.585, val_acc:0.832]
Epoch [70/120    avg_loss:0.384, val_acc:0.810]
Epoch [71/120    avg_loss:0.228, val_acc:0.939]
Epoch [72/120    avg_loss:0.203, val_acc:0.861]
Epoch [73/120    avg_loss:0.298, val_acc:0.955]
Epoch [74/120    avg_loss:0.103, val_acc:0.968]
Epoch [75/120    avg_loss:0.072, val_acc:0.948]
Epoch [76/120    avg_loss:0.040, val_acc:0.972]
Epoch [77/120    avg_loss:0.075, val_acc:0.980]
Epoch [78/120    avg_loss:0.036, val_acc:0.983]
Epoch [79/120    avg_loss:0.023, val_acc:0.986]
Epoch [80/120    avg_loss:0.016, val_acc:0.985]
Epoch [81/120    avg_loss:0.018, val_acc:0.986]
Epoch [82/120    avg_loss:0.017, val_acc:0.986]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.016, val_acc:0.987]
Epoch [86/120    avg_loss:0.017, val_acc:0.987]
Epoch [87/120    avg_loss:0.015, val_acc:0.988]
Epoch [88/120    avg_loss:0.014, val_acc:0.987]
Epoch [89/120    avg_loss:0.012, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.988]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.013, val_acc:0.989]
Epoch [94/120    avg_loss:0.010, val_acc:0.989]
Epoch [95/120    avg_loss:0.016, val_acc:0.988]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.014, val_acc:0.988]
Epoch [98/120    avg_loss:0.012, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.988]
Epoch [100/120    avg_loss:0.013, val_acc:0.988]
Epoch [101/120    avg_loss:0.014, val_acc:0.988]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.988]
Epoch [104/120    avg_loss:0.013, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.019, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.012, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.012, val_acc:0.988]
Epoch [116/120    avg_loss:0.014, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.012, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.013, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     3     0    30    20    10     0]
 [    0     0 18030     0    45     0     7     0     8     0]
 [    0     3     0  2005     0     0     0     0    13    15]
 [    0    41    11     0  2891     0     3     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4870     0     0     7]
 [    0     2     0     0     0     0     4  1280     0     4]
 [    0     6    11     1    47     0     0     0  3506     0]
 [    0     2     0     2    14    53     0     0     0   848]]

Accuracy:
99.06249246860916

F1 scores:
[       nan 0.99089848 0.99770357 0.99159248 0.96818486 0.98009763
 0.99468954 0.98841699 0.98289879 0.94590073]

Kappa:
0.9875823641790259
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a3fb5c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.376, val_acc:0.564]
Epoch [2/120    avg_loss:0.741, val_acc:0.663]
Epoch [3/120    avg_loss:0.616, val_acc:0.783]
Epoch [4/120    avg_loss:0.477, val_acc:0.788]
Epoch [5/120    avg_loss:0.435, val_acc:0.821]
Epoch [6/120    avg_loss:0.404, val_acc:0.838]
Epoch [7/120    avg_loss:0.305, val_acc:0.890]
Epoch [8/120    avg_loss:0.247, val_acc:0.878]
Epoch [9/120    avg_loss:0.296, val_acc:0.911]
Epoch [10/120    avg_loss:0.268, val_acc:0.905]
Epoch [11/120    avg_loss:0.249, val_acc:0.888]
Epoch [12/120    avg_loss:0.208, val_acc:0.936]
Epoch [13/120    avg_loss:0.182, val_acc:0.918]
Epoch [14/120    avg_loss:0.153, val_acc:0.899]
Epoch [15/120    avg_loss:0.167, val_acc:0.950]
Epoch [16/120    avg_loss:0.177, val_acc:0.935]
Epoch [17/120    avg_loss:0.099, val_acc:0.962]
Epoch [18/120    avg_loss:0.086, val_acc:0.895]
Epoch [19/120    avg_loss:0.095, val_acc:0.936]
Epoch [20/120    avg_loss:0.064, val_acc:0.956]
Epoch [21/120    avg_loss:0.269, val_acc:0.884]
Epoch [22/120    avg_loss:0.147, val_acc:0.954]
Epoch [23/120    avg_loss:0.118, val_acc:0.952]
Epoch [24/120    avg_loss:0.098, val_acc:0.937]
Epoch [25/120    avg_loss:0.140, val_acc:0.802]
Epoch [26/120    avg_loss:0.104, val_acc:0.943]
Epoch [27/120    avg_loss:0.049, val_acc:0.974]
Epoch [28/120    avg_loss:0.051, val_acc:0.974]
Epoch [29/120    avg_loss:0.028, val_acc:0.974]
Epoch [30/120    avg_loss:0.046, val_acc:0.921]
Epoch [31/120    avg_loss:0.036, val_acc:0.979]
Epoch [32/120    avg_loss:0.045, val_acc:0.973]
Epoch [33/120    avg_loss:0.034, val_acc:0.980]
Epoch [34/120    avg_loss:0.032, val_acc:0.984]
Epoch [35/120    avg_loss:0.019, val_acc:0.972]
Epoch [36/120    avg_loss:0.018, val_acc:0.984]
Epoch [37/120    avg_loss:0.018, val_acc:0.986]
Epoch [38/120    avg_loss:0.056, val_acc:0.974]
Epoch [39/120    avg_loss:0.025, val_acc:0.974]
Epoch [40/120    avg_loss:0.023, val_acc:0.972]
Epoch [41/120    avg_loss:0.030, val_acc:0.901]
Epoch [42/120    avg_loss:0.117, val_acc:0.947]
Epoch [43/120    avg_loss:0.105, val_acc:0.964]
Epoch [44/120    avg_loss:0.035, val_acc:0.976]
Epoch [45/120    avg_loss:0.047, val_acc:0.970]
Epoch [46/120    avg_loss:0.021, val_acc:0.977]
Epoch [47/120    avg_loss:0.022, val_acc:0.979]
Epoch [48/120    avg_loss:0.022, val_acc:0.976]
Epoch [49/120    avg_loss:0.028, val_acc:0.940]
Epoch [50/120    avg_loss:0.021, val_acc:0.969]
Epoch [51/120    avg_loss:0.017, val_acc:0.979]
Epoch [52/120    avg_loss:0.011, val_acc:0.979]
Epoch [53/120    avg_loss:0.010, val_acc:0.980]
Epoch [54/120    avg_loss:0.010, val_acc:0.980]
Epoch [55/120    avg_loss:0.009, val_acc:0.980]
Epoch [56/120    avg_loss:0.009, val_acc:0.982]
Epoch [57/120    avg_loss:0.010, val_acc:0.982]
Epoch [58/120    avg_loss:0.009, val_acc:0.984]
Epoch [59/120    avg_loss:0.008, val_acc:0.985]
Epoch [60/120    avg_loss:0.009, val_acc:0.983]
Epoch [61/120    avg_loss:0.007, val_acc:0.985]
Epoch [62/120    avg_loss:0.009, val_acc:0.984]
Epoch [63/120    avg_loss:0.008, val_acc:0.983]
Epoch [64/120    avg_loss:0.009, val_acc:0.983]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.011, val_acc:0.984]
Epoch [67/120    avg_loss:0.007, val_acc:0.984]
Epoch [68/120    avg_loss:0.008, val_acc:0.982]
Epoch [69/120    avg_loss:0.007, val_acc:0.983]
Epoch [70/120    avg_loss:0.014, val_acc:0.982]
Epoch [71/120    avg_loss:0.012, val_acc:0.981]
Epoch [72/120    avg_loss:0.011, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.982]
Epoch [74/120    avg_loss:0.008, val_acc:0.983]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.982]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.008, val_acc:0.982]
Epoch [80/120    avg_loss:0.009, val_acc:0.982]
Epoch [81/120    avg_loss:0.006, val_acc:0.982]
Epoch [82/120    avg_loss:0.007, val_acc:0.982]
Epoch [83/120    avg_loss:0.013, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.982]
Epoch [85/120    avg_loss:0.008, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.010, val_acc:0.982]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.006, val_acc:0.982]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.982]
Epoch [95/120    avg_loss:0.010, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.982]
Epoch [98/120    avg_loss:0.009, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.982]
Epoch [108/120    avg_loss:0.007, val_acc:0.982]
Epoch [109/120    avg_loss:0.008, val_acc:0.982]
Epoch [110/120    avg_loss:0.012, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.982]
Epoch [112/120    avg_loss:0.007, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     6     0     0    20    34    10     0]
 [    0     0 18050     0    12     0    25     0     3     0]
 [    0     0     0  2012     1     0     0     0    11    12]
 [    0    41    20     0  2873     0     7     0    27     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4867     0     3     2]
 [    0     0     0     0     0     0     4  1284     0     2]
 [    0     3     0    10    62     0     0     0  3476    20]
 [    0     1     0     0    14    43     0     0     0   861]]

Accuracy:
99.02875183765937

F1 scores:
[       nan 0.99104292 0.99834071 0.98869779 0.96831817 0.98379193
 0.99316396 0.98466258 0.97901704 0.94615385]

Kappa:
0.9871344111625093
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9c02d9c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.368, val_acc:0.707]
Epoch [2/120    avg_loss:0.740, val_acc:0.734]
Epoch [3/120    avg_loss:0.531, val_acc:0.851]
Epoch [4/120    avg_loss:0.499, val_acc:0.798]
Epoch [5/120    avg_loss:0.382, val_acc:0.889]
Epoch [6/120    avg_loss:0.322, val_acc:0.761]
Epoch [7/120    avg_loss:0.306, val_acc:0.838]
Epoch [8/120    avg_loss:0.252, val_acc:0.895]
Epoch [9/120    avg_loss:0.289, val_acc:0.907]
Epoch [10/120    avg_loss:0.273, val_acc:0.869]
Epoch [11/120    avg_loss:0.172, val_acc:0.928]
Epoch [12/120    avg_loss:0.206, val_acc:0.760]
Epoch [13/120    avg_loss:0.175, val_acc:0.925]
Epoch [14/120    avg_loss:0.128, val_acc:0.928]
Epoch [15/120    avg_loss:0.107, val_acc:0.909]
Epoch [16/120    avg_loss:0.080, val_acc:0.943]
Epoch [17/120    avg_loss:0.092, val_acc:0.945]
Epoch [18/120    avg_loss:0.143, val_acc:0.931]
Epoch [19/120    avg_loss:0.113, val_acc:0.949]
Epoch [20/120    avg_loss:0.095, val_acc:0.938]
Epoch [21/120    avg_loss:0.070, val_acc:0.950]
Epoch [22/120    avg_loss:0.071, val_acc:0.961]
Epoch [23/120    avg_loss:0.071, val_acc:0.965]
Epoch [24/120    avg_loss:0.082, val_acc:0.952]
Epoch [25/120    avg_loss:0.132, val_acc:0.922]
Epoch [26/120    avg_loss:0.071, val_acc:0.970]
Epoch [27/120    avg_loss:0.055, val_acc:0.965]
Epoch [28/120    avg_loss:0.079, val_acc:0.969]
Epoch [29/120    avg_loss:0.049, val_acc:0.965]
Epoch [30/120    avg_loss:0.046, val_acc:0.967]
Epoch [31/120    avg_loss:0.058, val_acc:0.970]
Epoch [32/120    avg_loss:0.048, val_acc:0.973]
Epoch [33/120    avg_loss:0.044, val_acc:0.971]
Epoch [34/120    avg_loss:0.031, val_acc:0.986]
Epoch [35/120    avg_loss:0.031, val_acc:0.985]
Epoch [36/120    avg_loss:0.033, val_acc:0.980]
Epoch [37/120    avg_loss:0.033, val_acc:0.965]
Epoch [38/120    avg_loss:0.044, val_acc:0.985]
Epoch [39/120    avg_loss:0.020, val_acc:0.969]
Epoch [40/120    avg_loss:0.062, val_acc:0.942]
Epoch [41/120    avg_loss:0.083, val_acc:0.980]
Epoch [42/120    avg_loss:0.026, val_acc:0.981]
Epoch [43/120    avg_loss:0.016, val_acc:0.985]
Epoch [44/120    avg_loss:0.023, val_acc:0.976]
Epoch [45/120    avg_loss:0.033, val_acc:0.986]
Epoch [46/120    avg_loss:0.019, val_acc:0.987]
Epoch [47/120    avg_loss:0.011, val_acc:0.989]
Epoch [48/120    avg_loss:0.012, val_acc:0.984]
Epoch [49/120    avg_loss:0.014, val_acc:0.986]
Epoch [50/120    avg_loss:0.007, val_acc:0.991]
Epoch [51/120    avg_loss:0.014, val_acc:0.987]
Epoch [52/120    avg_loss:0.017, val_acc:0.942]
Epoch [53/120    avg_loss:0.045, val_acc:0.977]
Epoch [54/120    avg_loss:0.061, val_acc:0.960]
Epoch [55/120    avg_loss:0.030, val_acc:0.981]
Epoch [56/120    avg_loss:0.027, val_acc:0.985]
Epoch [57/120    avg_loss:0.015, val_acc:0.989]
Epoch [58/120    avg_loss:0.016, val_acc:0.990]
Epoch [59/120    avg_loss:0.017, val_acc:0.990]
Epoch [60/120    avg_loss:0.015, val_acc:0.986]
Epoch [61/120    avg_loss:0.016, val_acc:0.987]
Epoch [62/120    avg_loss:0.012, val_acc:0.989]
Epoch [63/120    avg_loss:0.008, val_acc:0.989]
Epoch [64/120    avg_loss:0.007, val_acc:0.991]
Epoch [65/120    avg_loss:0.006, val_acc:0.990]
Epoch [66/120    avg_loss:0.007, val_acc:0.990]
Epoch [67/120    avg_loss:0.005, val_acc:0.991]
Epoch [68/120    avg_loss:0.008, val_acc:0.991]
Epoch [69/120    avg_loss:0.006, val_acc:0.991]
Epoch [70/120    avg_loss:0.004, val_acc:0.992]
Epoch [71/120    avg_loss:0.006, val_acc:0.991]
Epoch [72/120    avg_loss:0.004, val_acc:0.991]
Epoch [73/120    avg_loss:0.007, val_acc:0.991]
Epoch [74/120    avg_loss:0.005, val_acc:0.993]
Epoch [75/120    avg_loss:0.008, val_acc:0.992]
Epoch [76/120    avg_loss:0.005, val_acc:0.993]
Epoch [77/120    avg_loss:0.005, val_acc:0.992]
Epoch [78/120    avg_loss:0.006, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.991]
Epoch [80/120    avg_loss:0.007, val_acc:0.991]
Epoch [81/120    avg_loss:0.003, val_acc:0.991]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.991]
Epoch [84/120    avg_loss:0.004, val_acc:0.992]
Epoch [85/120    avg_loss:0.007, val_acc:0.991]
Epoch [86/120    avg_loss:0.004, val_acc:0.991]
Epoch [87/120    avg_loss:0.004, val_acc:0.991]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.009, val_acc:0.991]
Epoch [91/120    avg_loss:0.004, val_acc:0.991]
Epoch [92/120    avg_loss:0.003, val_acc:0.991]
Epoch [93/120    avg_loss:0.003, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.006, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.005, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.005, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.004, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.003, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0    24     1     7     0]
 [    0     3 18003     0    37     0    41     0     6     0]
 [    0     1     0  2019     0     0     0     0    10     6]
 [    0    39     8     0  2904     0     0     0    20     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4855     0     2    21]
 [    0     0     0     0     0     0     4  1282     0     4]
 [    0     5     0     4    29     0     0     0  3520    13]
 [    0     0     0     1    14    35     0     0     0   869]]

Accuracy:
99.19022485720483

F1 scores:
[       nan 0.99378882 0.99736849 0.99458128 0.97515111 0.98676749
 0.99061416 0.99650214 0.98654709 0.94817239]

Kappa:
0.9892789013044565
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1c775cd828>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.457, val_acc:0.551]
Epoch [2/120    avg_loss:0.765, val_acc:0.700]
Epoch [3/120    avg_loss:0.530, val_acc:0.837]
Epoch [4/120    avg_loss:0.465, val_acc:0.815]
Epoch [5/120    avg_loss:0.414, val_acc:0.842]
Epoch [6/120    avg_loss:0.378, val_acc:0.896]
Epoch [7/120    avg_loss:0.348, val_acc:0.872]
Epoch [8/120    avg_loss:0.252, val_acc:0.926]
Epoch [9/120    avg_loss:0.212, val_acc:0.900]
Epoch [10/120    avg_loss:0.298, val_acc:0.902]
Epoch [11/120    avg_loss:0.238, val_acc:0.894]
Epoch [12/120    avg_loss:0.232, val_acc:0.910]
Epoch [13/120    avg_loss:0.222, val_acc:0.945]
Epoch [14/120    avg_loss:0.144, val_acc:0.937]
Epoch [15/120    avg_loss:0.165, val_acc:0.939]
Epoch [16/120    avg_loss:0.150, val_acc:0.946]
Epoch [17/120    avg_loss:0.091, val_acc:0.963]
Epoch [18/120    avg_loss:0.107, val_acc:0.937]
Epoch [19/120    avg_loss:0.115, val_acc:0.930]
Epoch [20/120    avg_loss:0.123, val_acc:0.958]
Epoch [21/120    avg_loss:0.104, val_acc:0.959]
Epoch [22/120    avg_loss:0.067, val_acc:0.973]
Epoch [23/120    avg_loss:0.080, val_acc:0.979]
Epoch [24/120    avg_loss:0.069, val_acc:0.972]
Epoch [25/120    avg_loss:0.092, val_acc:0.894]
Epoch [26/120    avg_loss:0.105, val_acc:0.940]
Epoch [27/120    avg_loss:0.079, val_acc:0.966]
Epoch [28/120    avg_loss:0.053, val_acc:0.969]
Epoch [29/120    avg_loss:0.062, val_acc:0.949]
Epoch [30/120    avg_loss:0.103, val_acc:0.980]
Epoch [31/120    avg_loss:0.076, val_acc:0.952]
Epoch [32/120    avg_loss:0.046, val_acc:0.983]
Epoch [33/120    avg_loss:0.023, val_acc:0.981]
Epoch [34/120    avg_loss:0.046, val_acc:0.975]
Epoch [35/120    avg_loss:0.080, val_acc:0.854]
Epoch [36/120    avg_loss:0.206, val_acc:0.963]
Epoch [37/120    avg_loss:0.041, val_acc:0.967]
Epoch [38/120    avg_loss:0.054, val_acc:0.981]
Epoch [39/120    avg_loss:0.056, val_acc:0.970]
Epoch [40/120    avg_loss:0.050, val_acc:0.971]
Epoch [41/120    avg_loss:0.043, val_acc:0.980]
Epoch [42/120    avg_loss:0.033, val_acc:0.983]
Epoch [43/120    avg_loss:0.019, val_acc:0.987]
Epoch [44/120    avg_loss:0.014, val_acc:0.987]
Epoch [45/120    avg_loss:0.021, val_acc:0.985]
Epoch [46/120    avg_loss:0.035, val_acc:0.982]
Epoch [47/120    avg_loss:0.041, val_acc:0.973]
Epoch [48/120    avg_loss:0.059, val_acc:0.979]
Epoch [49/120    avg_loss:0.016, val_acc:0.989]
Epoch [50/120    avg_loss:0.033, val_acc:0.979]
Epoch [51/120    avg_loss:0.077, val_acc:0.958]
Epoch [52/120    avg_loss:0.025, val_acc:0.985]
Epoch [53/120    avg_loss:0.016, val_acc:0.991]
Epoch [54/120    avg_loss:0.016, val_acc:0.988]
Epoch [55/120    avg_loss:0.013, val_acc:0.990]
Epoch [56/120    avg_loss:0.013, val_acc:0.991]
Epoch [57/120    avg_loss:0.013, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.991]
Epoch [59/120    avg_loss:0.016, val_acc:0.990]
Epoch [60/120    avg_loss:0.009, val_acc:0.993]
Epoch [61/120    avg_loss:0.021, val_acc:0.991]
Epoch [62/120    avg_loss:0.012, val_acc:0.991]
Epoch [63/120    avg_loss:0.011, val_acc:0.993]
Epoch [64/120    avg_loss:0.011, val_acc:0.992]
Epoch [65/120    avg_loss:0.008, val_acc:0.993]
Epoch [66/120    avg_loss:0.009, val_acc:0.992]
Epoch [67/120    avg_loss:0.012, val_acc:0.992]
Epoch [68/120    avg_loss:0.006, val_acc:0.993]
Epoch [69/120    avg_loss:0.059, val_acc:0.967]
Epoch [70/120    avg_loss:0.045, val_acc:0.984]
Epoch [71/120    avg_loss:0.010, val_acc:0.990]
Epoch [72/120    avg_loss:0.021, val_acc:0.986]
Epoch [73/120    avg_loss:0.061, val_acc:0.984]
Epoch [74/120    avg_loss:0.018, val_acc:0.987]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.008, val_acc:0.969]
Epoch [77/120    avg_loss:0.010, val_acc:0.991]
Epoch [78/120    avg_loss:0.006, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.023, val_acc:0.984]
Epoch [81/120    avg_loss:0.028, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.990]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.011, val_acc:0.991]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.992]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.008, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.005, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.006, val_acc:0.991]
Epoch [99/120    avg_loss:0.005, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.007, val_acc:0.991]
Epoch [110/120    avg_loss:0.006, val_acc:0.991]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6378     0     0     0     0    29    17     8     0]
 [    0     0 18037     0    24     0    16     0    13     0]
 [    0     0     0  2029     3     0     0     0     0     4]
 [    0    40    16     0  2886     0     3     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4866     0     3     9]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     0     0    10    61     0     0     0  3498     2]
 [    0     0     0     0    14    39     0     0     0   866]]

Accuracy:
99.17576458679777

F1 scores:
[       nan 0.99268482 0.99809092 0.99582822 0.96845638 0.98527746
 0.99366959 0.99190127 0.98258427 0.96115427]

Kappa:
0.9890831244636626
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0813f88780>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.446, val_acc:0.684]
Epoch [2/120    avg_loss:0.773, val_acc:0.774]
Epoch [3/120    avg_loss:0.642, val_acc:0.762]
Epoch [4/120    avg_loss:0.478, val_acc:0.766]
Epoch [5/120    avg_loss:0.374, val_acc:0.836]
Epoch [6/120    avg_loss:0.362, val_acc:0.842]
Epoch [7/120    avg_loss:0.355, val_acc:0.893]
Epoch [8/120    avg_loss:0.237, val_acc:0.877]
Epoch [9/120    avg_loss:0.324, val_acc:0.889]
Epoch [10/120    avg_loss:0.254, val_acc:0.879]
Epoch [11/120    avg_loss:0.244, val_acc:0.924]
Epoch [12/120    avg_loss:0.188, val_acc:0.912]
Epoch [13/120    avg_loss:0.180, val_acc:0.787]
Epoch [14/120    avg_loss:0.336, val_acc:0.908]
Epoch [15/120    avg_loss:0.215, val_acc:0.847]
Epoch [16/120    avg_loss:0.206, val_acc:0.889]
Epoch [17/120    avg_loss:0.198, val_acc:0.927]
Epoch [18/120    avg_loss:0.145, val_acc:0.917]
Epoch [19/120    avg_loss:0.105, val_acc:0.896]
Epoch [20/120    avg_loss:0.107, val_acc:0.922]
Epoch [21/120    avg_loss:0.084, val_acc:0.945]
Epoch [22/120    avg_loss:0.071, val_acc:0.947]
Epoch [23/120    avg_loss:0.078, val_acc:0.948]
Epoch [24/120    avg_loss:0.060, val_acc:0.944]
Epoch [25/120    avg_loss:0.098, val_acc:0.955]
Epoch [26/120    avg_loss:0.071, val_acc:0.927]
Epoch [27/120    avg_loss:0.061, val_acc:0.966]
Epoch [28/120    avg_loss:0.075, val_acc:0.957]
Epoch [29/120    avg_loss:0.053, val_acc:0.951]
Epoch [30/120    avg_loss:0.043, val_acc:0.951]
Epoch [31/120    avg_loss:0.049, val_acc:0.965]
Epoch [32/120    avg_loss:0.044, val_acc:0.965]
Epoch [33/120    avg_loss:0.048, val_acc:0.973]
Epoch [34/120    avg_loss:0.033, val_acc:0.932]
Epoch [35/120    avg_loss:0.047, val_acc:0.945]
Epoch [36/120    avg_loss:0.042, val_acc:0.971]
Epoch [37/120    avg_loss:0.045, val_acc:0.973]
Epoch [38/120    avg_loss:0.048, val_acc:0.970]
Epoch [39/120    avg_loss:0.028, val_acc:0.972]
Epoch [40/120    avg_loss:0.026, val_acc:0.979]
Epoch [41/120    avg_loss:0.027, val_acc:0.960]
Epoch [42/120    avg_loss:0.018, val_acc:0.972]
Epoch [43/120    avg_loss:0.019, val_acc:0.953]
Epoch [44/120    avg_loss:0.029, val_acc:0.979]
Epoch [45/120    avg_loss:0.025, val_acc:0.942]
Epoch [46/120    avg_loss:0.033, val_acc:0.964]
Epoch [47/120    avg_loss:0.014, val_acc:0.980]
Epoch [48/120    avg_loss:0.025, val_acc:0.946]
Epoch [49/120    avg_loss:0.025, val_acc:0.976]
Epoch [50/120    avg_loss:0.021, val_acc:0.952]
Epoch [51/120    avg_loss:0.018, val_acc:0.972]
Epoch [52/120    avg_loss:0.013, val_acc:0.979]
Epoch [53/120    avg_loss:0.018, val_acc:0.974]
Epoch [54/120    avg_loss:0.014, val_acc:0.988]
Epoch [55/120    avg_loss:0.009, val_acc:0.980]
Epoch [56/120    avg_loss:0.017, val_acc:0.987]
Epoch [57/120    avg_loss:0.007, val_acc:0.988]
Epoch [58/120    avg_loss:0.013, val_acc:0.961]
Epoch [59/120    avg_loss:0.019, val_acc:0.984]
Epoch [60/120    avg_loss:0.016, val_acc:0.975]
Epoch [61/120    avg_loss:0.007, val_acc:0.985]
Epoch [62/120    avg_loss:0.012, val_acc:0.985]
Epoch [63/120    avg_loss:0.014, val_acc:0.984]
Epoch [64/120    avg_loss:0.009, val_acc:0.990]
Epoch [65/120    avg_loss:0.011, val_acc:0.988]
Epoch [66/120    avg_loss:0.007, val_acc:0.985]
Epoch [67/120    avg_loss:0.014, val_acc:0.979]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.026, val_acc:0.981]
Epoch [70/120    avg_loss:0.076, val_acc:0.892]
Epoch [71/120    avg_loss:0.095, val_acc:0.970]
Epoch [72/120    avg_loss:0.034, val_acc:0.972]
Epoch [73/120    avg_loss:0.056, val_acc:0.956]
Epoch [74/120    avg_loss:0.024, val_acc:0.980]
Epoch [75/120    avg_loss:0.009, val_acc:0.980]
Epoch [76/120    avg_loss:0.010, val_acc:0.983]
Epoch [77/120    avg_loss:0.018, val_acc:0.981]
Epoch [78/120    avg_loss:0.011, val_acc:0.984]
Epoch [79/120    avg_loss:0.007, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.013, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.009, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.008, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     2     5     0    14     3    36     0]
 [    0     0 18031     0    57     0     0     0     2     0]
 [    0     7     0  2015     3     0     0     0     6     5]
 [    0    27     9     0  2915     0     0     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    17     0     0  4857     0     1     3]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     0     0     0    68     0     0     0  3489    14]
 [    0     0     0     0    14    31     0     0     0   874]]

Accuracy:
99.16130431639071

F1 scores:
[       nan 0.99267799 0.99811791 0.99017199 0.96619158 0.98826202
 0.99610336 0.99767442 0.97923099 0.9630854 ]

Kappa:
0.9888941747240017
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5be6d46780>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.457, val_acc:0.614]
Epoch [2/120    avg_loss:0.725, val_acc:0.786]
Epoch [3/120    avg_loss:0.540, val_acc:0.812]
Epoch [4/120    avg_loss:0.424, val_acc:0.690]
Epoch [5/120    avg_loss:0.411, val_acc:0.849]
Epoch [6/120    avg_loss:0.323, val_acc:0.863]
Epoch [7/120    avg_loss:0.259, val_acc:0.902]
Epoch [8/120    avg_loss:0.302, val_acc:0.904]
Epoch [9/120    avg_loss:0.292, val_acc:0.896]
Epoch [10/120    avg_loss:0.228, val_acc:0.889]
Epoch [11/120    avg_loss:0.235, val_acc:0.860]
Epoch [12/120    avg_loss:0.204, val_acc:0.915]
Epoch [13/120    avg_loss:0.206, val_acc:0.891]
Epoch [14/120    avg_loss:0.191, val_acc:0.930]
Epoch [15/120    avg_loss:0.161, val_acc:0.941]
Epoch [16/120    avg_loss:0.162, val_acc:0.938]
Epoch [17/120    avg_loss:0.114, val_acc:0.960]
Epoch [18/120    avg_loss:0.094, val_acc:0.928]
Epoch [19/120    avg_loss:0.133, val_acc:0.912]
Epoch [20/120    avg_loss:0.082, val_acc:0.954]
Epoch [21/120    avg_loss:0.069, val_acc:0.959]
Epoch [22/120    avg_loss:0.086, val_acc:0.926]
Epoch [23/120    avg_loss:0.087, val_acc:0.969]
Epoch [24/120    avg_loss:0.044, val_acc:0.966]
Epoch [25/120    avg_loss:0.084, val_acc:0.957]
Epoch [26/120    avg_loss:0.075, val_acc:0.960]
Epoch [27/120    avg_loss:0.053, val_acc:0.970]
Epoch [28/120    avg_loss:0.055, val_acc:0.967]
Epoch [29/120    avg_loss:0.032, val_acc:0.969]
Epoch [30/120    avg_loss:0.049, val_acc:0.972]
Epoch [31/120    avg_loss:0.049, val_acc:0.961]
Epoch [32/120    avg_loss:0.064, val_acc:0.969]
Epoch [33/120    avg_loss:0.040, val_acc:0.968]
Epoch [34/120    avg_loss:0.051, val_acc:0.980]
Epoch [35/120    avg_loss:0.049, val_acc:0.980]
Epoch [36/120    avg_loss:0.037, val_acc:0.969]
Epoch [37/120    avg_loss:0.058, val_acc:0.954]
Epoch [38/120    avg_loss:0.060, val_acc:0.967]
Epoch [39/120    avg_loss:0.049, val_acc:0.980]
Epoch [40/120    avg_loss:0.035, val_acc:0.973]
Epoch [41/120    avg_loss:0.052, val_acc:0.940]
Epoch [42/120    avg_loss:0.081, val_acc:0.974]
Epoch [43/120    avg_loss:0.045, val_acc:0.975]
Epoch [44/120    avg_loss:0.036, val_acc:0.986]
Epoch [45/120    avg_loss:0.026, val_acc:0.983]
Epoch [46/120    avg_loss:0.019, val_acc:0.981]
Epoch [47/120    avg_loss:0.016, val_acc:0.971]
Epoch [48/120    avg_loss:0.012, val_acc:0.982]
Epoch [49/120    avg_loss:0.023, val_acc:0.958]
Epoch [50/120    avg_loss:0.044, val_acc:0.973]
Epoch [51/120    avg_loss:0.020, val_acc:0.976]
Epoch [52/120    avg_loss:0.016, val_acc:0.969]
Epoch [53/120    avg_loss:0.023, val_acc:0.980]
Epoch [54/120    avg_loss:0.022, val_acc:0.964]
Epoch [55/120    avg_loss:0.016, val_acc:0.984]
Epoch [56/120    avg_loss:0.018, val_acc:0.979]
Epoch [57/120    avg_loss:0.014, val_acc:0.981]
Epoch [58/120    avg_loss:0.009, val_acc:0.984]
Epoch [59/120    avg_loss:0.009, val_acc:0.985]
Epoch [60/120    avg_loss:0.012, val_acc:0.984]
Epoch [61/120    avg_loss:0.007, val_acc:0.984]
Epoch [62/120    avg_loss:0.006, val_acc:0.985]
Epoch [63/120    avg_loss:0.008, val_acc:0.986]
Epoch [64/120    avg_loss:0.006, val_acc:0.986]
Epoch [65/120    avg_loss:0.005, val_acc:0.987]
Epoch [66/120    avg_loss:0.005, val_acc:0.986]
Epoch [67/120    avg_loss:0.006, val_acc:0.986]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.006, val_acc:0.987]
Epoch [70/120    avg_loss:0.005, val_acc:0.987]
Epoch [71/120    avg_loss:0.006, val_acc:0.987]
Epoch [72/120    avg_loss:0.006, val_acc:0.987]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.987]
Epoch [75/120    avg_loss:0.006, val_acc:0.986]
Epoch [76/120    avg_loss:0.005, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.988]
Epoch [79/120    avg_loss:0.005, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.987]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.004, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.003, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.003, val_acc:0.987]
Epoch [99/120    avg_loss:0.003, val_acc:0.986]
Epoch [100/120    avg_loss:0.004, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.003, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     0     1     0     9     2     3     0]
 [    0     0 18074     0    12     0     0     0     4     0]
 [    0     0     0  2026     2     0     0     0     0     8]
 [    0    48    12     0  2881     0     2     0    28     1]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4855     0     0    23]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     5     0     0    34     0     0     0  3516    16]
 [    0     0     0     0    14    48     0     0     0   857]]

Accuracy:
99.33000747113971

F1 scores:
[       nan 0.9947295  0.99922601 0.99753816 0.9739689  0.9811747
 0.99630618 0.99767261 0.9873631  0.93763676]

Kappa:
0.9911222179302753
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7713491780>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.493, val_acc:0.513]
Epoch [2/120    avg_loss:0.837, val_acc:0.688]
Epoch [3/120    avg_loss:0.635, val_acc:0.787]
Epoch [4/120    avg_loss:0.532, val_acc:0.742]
Epoch [5/120    avg_loss:0.381, val_acc:0.816]
Epoch [6/120    avg_loss:0.381, val_acc:0.847]
Epoch [7/120    avg_loss:0.283, val_acc:0.928]
Epoch [8/120    avg_loss:0.251, val_acc:0.884]
Epoch [9/120    avg_loss:0.257, val_acc:0.916]
Epoch [10/120    avg_loss:0.221, val_acc:0.933]
Epoch [11/120    avg_loss:0.236, val_acc:0.913]
Epoch [12/120    avg_loss:0.206, val_acc:0.906]
Epoch [13/120    avg_loss:0.198, val_acc:0.938]
Epoch [14/120    avg_loss:0.196, val_acc:0.952]
Epoch [15/120    avg_loss:0.177, val_acc:0.914]
Epoch [16/120    avg_loss:0.140, val_acc:0.901]
Epoch [17/120    avg_loss:0.110, val_acc:0.961]
Epoch [18/120    avg_loss:0.149, val_acc:0.846]
Epoch [19/120    avg_loss:0.277, val_acc:0.767]
Epoch [20/120    avg_loss:0.297, val_acc:0.930]
Epoch [21/120    avg_loss:0.167, val_acc:0.934]
Epoch [22/120    avg_loss:0.101, val_acc:0.932]
Epoch [23/120    avg_loss:0.081, val_acc:0.968]
Epoch [24/120    avg_loss:0.070, val_acc:0.967]
Epoch [25/120    avg_loss:0.111, val_acc:0.884]
Epoch [26/120    avg_loss:0.065, val_acc:0.974]
Epoch [27/120    avg_loss:0.045, val_acc:0.984]
Epoch [28/120    avg_loss:0.130, val_acc:0.936]
Epoch [29/120    avg_loss:0.067, val_acc:0.980]
Epoch [30/120    avg_loss:0.069, val_acc:0.872]
Epoch [31/120    avg_loss:0.198, val_acc:0.961]
Epoch [32/120    avg_loss:0.139, val_acc:0.943]
Epoch [33/120    avg_loss:0.121, val_acc:0.978]
Epoch [34/120    avg_loss:0.063, val_acc:0.959]
Epoch [35/120    avg_loss:0.035, val_acc:0.982]
Epoch [36/120    avg_loss:0.039, val_acc:0.972]
Epoch [37/120    avg_loss:0.029, val_acc:0.976]
Epoch [38/120    avg_loss:0.022, val_acc:0.978]
Epoch [39/120    avg_loss:0.032, val_acc:0.987]
Epoch [40/120    avg_loss:0.031, val_acc:0.984]
Epoch [41/120    avg_loss:0.022, val_acc:0.981]
Epoch [42/120    avg_loss:0.016, val_acc:0.987]
Epoch [43/120    avg_loss:0.022, val_acc:0.988]
Epoch [44/120    avg_loss:0.048, val_acc:0.932]
Epoch [45/120    avg_loss:0.044, val_acc:0.973]
Epoch [46/120    avg_loss:0.020, val_acc:0.987]
Epoch [47/120    avg_loss:0.018, val_acc:0.984]
Epoch [48/120    avg_loss:0.014, val_acc:0.984]
Epoch [49/120    avg_loss:0.018, val_acc:0.987]
Epoch [50/120    avg_loss:0.058, val_acc:0.986]
Epoch [51/120    avg_loss:0.011, val_acc:0.992]
Epoch [52/120    avg_loss:0.012, val_acc:0.995]
Epoch [53/120    avg_loss:0.020, val_acc:0.993]
Epoch [54/120    avg_loss:0.009, val_acc:0.990]
Epoch [55/120    avg_loss:0.008, val_acc:0.991]
Epoch [56/120    avg_loss:0.008, val_acc:0.993]
Epoch [57/120    avg_loss:0.007, val_acc:0.995]
Epoch [58/120    avg_loss:0.036, val_acc:0.991]
Epoch [59/120    avg_loss:0.013, val_acc:0.991]
Epoch [60/120    avg_loss:0.037, val_acc:0.979]
Epoch [61/120    avg_loss:0.123, val_acc:0.972]
Epoch [62/120    avg_loss:0.051, val_acc:0.987]
Epoch [63/120    avg_loss:0.015, val_acc:0.991]
Epoch [64/120    avg_loss:0.011, val_acc:0.992]
Epoch [65/120    avg_loss:0.017, val_acc:0.954]
Epoch [66/120    avg_loss:0.020, val_acc:0.988]
Epoch [67/120    avg_loss:0.018, val_acc:0.978]
Epoch [68/120    avg_loss:0.013, val_acc:0.992]
Epoch [69/120    avg_loss:0.009, val_acc:0.992]
Epoch [70/120    avg_loss:0.006, val_acc:0.994]
Epoch [71/120    avg_loss:0.007, val_acc:0.994]
Epoch [72/120    avg_loss:0.005, val_acc:0.994]
Epoch [73/120    avg_loss:0.006, val_acc:0.994]
Epoch [74/120    avg_loss:0.004, val_acc:0.994]
Epoch [75/120    avg_loss:0.005, val_acc:0.994]
Epoch [76/120    avg_loss:0.007, val_acc:0.995]
Epoch [77/120    avg_loss:0.005, val_acc:0.995]
Epoch [78/120    avg_loss:0.005, val_acc:0.995]
Epoch [79/120    avg_loss:0.005, val_acc:0.995]
Epoch [80/120    avg_loss:0.005, val_acc:0.995]
Epoch [81/120    avg_loss:0.005, val_acc:0.995]
Epoch [82/120    avg_loss:0.006, val_acc:0.995]
Epoch [83/120    avg_loss:0.004, val_acc:0.995]
Epoch [84/120    avg_loss:0.005, val_acc:0.995]
Epoch [85/120    avg_loss:0.008, val_acc:0.995]
Epoch [86/120    avg_loss:0.008, val_acc:0.995]
Epoch [87/120    avg_loss:0.006, val_acc:0.995]
Epoch [88/120    avg_loss:0.006, val_acc:0.995]
Epoch [89/120    avg_loss:0.004, val_acc:0.995]
Epoch [90/120    avg_loss:0.005, val_acc:0.995]
Epoch [91/120    avg_loss:0.007, val_acc:0.995]
Epoch [92/120    avg_loss:0.004, val_acc:0.995]
Epoch [93/120    avg_loss:0.005, val_acc:0.995]
Epoch [94/120    avg_loss:0.005, val_acc:0.994]
Epoch [95/120    avg_loss:0.003, val_acc:0.995]
Epoch [96/120    avg_loss:0.005, val_acc:0.995]
Epoch [97/120    avg_loss:0.005, val_acc:0.995]
Epoch [98/120    avg_loss:0.006, val_acc:0.996]
Epoch [99/120    avg_loss:0.004, val_acc:0.996]
Epoch [100/120    avg_loss:0.004, val_acc:0.996]
Epoch [101/120    avg_loss:0.003, val_acc:0.996]
Epoch [102/120    avg_loss:0.005, val_acc:0.996]
Epoch [103/120    avg_loss:0.004, val_acc:0.996]
Epoch [104/120    avg_loss:0.005, val_acc:0.995]
Epoch [105/120    avg_loss:0.006, val_acc:0.995]
Epoch [106/120    avg_loss:0.003, val_acc:0.995]
Epoch [107/120    avg_loss:0.004, val_acc:0.996]
Epoch [108/120    avg_loss:0.004, val_acc:0.995]
Epoch [109/120    avg_loss:0.003, val_acc:0.995]
Epoch [110/120    avg_loss:0.004, val_acc:0.995]
Epoch [111/120    avg_loss:0.006, val_acc:0.995]
Epoch [112/120    avg_loss:0.004, val_acc:0.996]
Epoch [113/120    avg_loss:0.004, val_acc:0.996]
Epoch [114/120    avg_loss:0.008, val_acc:0.996]
Epoch [115/120    avg_loss:0.006, val_acc:0.996]
Epoch [116/120    avg_loss:0.005, val_acc:0.996]
Epoch [117/120    avg_loss:0.003, val_acc:0.995]
Epoch [118/120    avg_loss:0.003, val_acc:0.995]
Epoch [119/120    avg_loss:0.005, val_acc:0.995]
Epoch [120/120    avg_loss:0.007, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0    28     4     0     0]
 [    0     0 18053     0    34     0     3     0     0     0]
 [    0     0     0  2028     0     0     0     0     0     8]
 [    0    37     9     1  2880     0     7     0    27    11]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4855     0     0    23]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0     1     0     0    47     0     0     0  3503    20]
 [    0     0     0     0    14    37     0     0     0   868]]

Accuracy:
99.24324584869737

F1 scores:
[       nan 0.99456099 0.99872759 0.99778598 0.96855557 0.98602191
 0.99365534 0.99728787 0.9866216  0.93787142]

Kappa:
0.9899762542247482
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd552ba8748>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.489, val_acc:0.684]
Epoch [2/120    avg_loss:0.801, val_acc:0.768]
Epoch [3/120    avg_loss:0.605, val_acc:0.707]
Epoch [4/120    avg_loss:0.464, val_acc:0.734]
Epoch [5/120    avg_loss:0.468, val_acc:0.784]
Epoch [6/120    avg_loss:0.449, val_acc:0.832]
Epoch [7/120    avg_loss:0.371, val_acc:0.870]
Epoch [8/120    avg_loss:0.294, val_acc:0.900]
Epoch [9/120    avg_loss:0.267, val_acc:0.909]
Epoch [10/120    avg_loss:0.287, val_acc:0.860]
Epoch [11/120    avg_loss:0.206, val_acc:0.933]
Epoch [12/120    avg_loss:0.198, val_acc:0.903]
Epoch [13/120    avg_loss:0.218, val_acc:0.934]
Epoch [14/120    avg_loss:0.187, val_acc:0.919]
Epoch [15/120    avg_loss:0.199, val_acc:0.915]
Epoch [16/120    avg_loss:0.182, val_acc:0.940]
Epoch [17/120    avg_loss:0.185, val_acc:0.843]
Epoch [18/120    avg_loss:0.161, val_acc:0.952]
Epoch [19/120    avg_loss:0.128, val_acc:0.947]
Epoch [20/120    avg_loss:0.120, val_acc:0.967]
Epoch [21/120    avg_loss:0.102, val_acc:0.866]
Epoch [22/120    avg_loss:0.092, val_acc:0.925]
Epoch [23/120    avg_loss:0.091, val_acc:0.939]
Epoch [24/120    avg_loss:0.080, val_acc:0.941]
Epoch [25/120    avg_loss:0.062, val_acc:0.966]
Epoch [26/120    avg_loss:0.133, val_acc:0.938]
Epoch [27/120    avg_loss:0.086, val_acc:0.947]
Epoch [28/120    avg_loss:0.053, val_acc:0.943]
Epoch [29/120    avg_loss:0.039, val_acc:0.968]
Epoch [30/120    avg_loss:0.048, val_acc:0.978]
Epoch [31/120    avg_loss:0.165, val_acc:0.940]
Epoch [32/120    avg_loss:0.072, val_acc:0.977]
Epoch [33/120    avg_loss:0.051, val_acc:0.972]
Epoch [34/120    avg_loss:0.045, val_acc:0.977]
Epoch [35/120    avg_loss:0.034, val_acc:0.981]
Epoch [36/120    avg_loss:0.065, val_acc:0.957]
Epoch [37/120    avg_loss:0.050, val_acc:0.980]
Epoch [38/120    avg_loss:0.030, val_acc:0.963]
Epoch [39/120    avg_loss:0.036, val_acc:0.980]
Epoch [40/120    avg_loss:0.019, val_acc:0.976]
Epoch [41/120    avg_loss:0.018, val_acc:0.982]
Epoch [42/120    avg_loss:0.015, val_acc:0.980]
Epoch [43/120    avg_loss:0.017, val_acc:0.981]
Epoch [44/120    avg_loss:0.035, val_acc:0.983]
Epoch [45/120    avg_loss:0.025, val_acc:0.973]
Epoch [46/120    avg_loss:0.033, val_acc:0.959]
Epoch [47/120    avg_loss:0.106, val_acc:0.941]
Epoch [48/120    avg_loss:0.083, val_acc:0.979]
Epoch [49/120    avg_loss:0.027, val_acc:0.983]
Epoch [50/120    avg_loss:0.022, val_acc:0.984]
Epoch [51/120    avg_loss:0.013, val_acc:0.984]
Epoch [52/120    avg_loss:0.012, val_acc:0.979]
Epoch [53/120    avg_loss:0.016, val_acc:0.979]
Epoch [54/120    avg_loss:0.012, val_acc:0.981]
Epoch [55/120    avg_loss:0.007, val_acc:0.985]
Epoch [56/120    avg_loss:0.006, val_acc:0.986]
Epoch [57/120    avg_loss:0.015, val_acc:0.931]
Epoch [58/120    avg_loss:0.021, val_acc:0.970]
Epoch [59/120    avg_loss:0.011, val_acc:0.982]
Epoch [60/120    avg_loss:0.014, val_acc:0.984]
Epoch [61/120    avg_loss:0.016, val_acc:0.984]
Epoch [62/120    avg_loss:0.016, val_acc:0.984]
Epoch [63/120    avg_loss:0.014, val_acc:0.976]
Epoch [64/120    avg_loss:0.006, val_acc:0.988]
Epoch [65/120    avg_loss:0.029, val_acc:0.977]
Epoch [66/120    avg_loss:0.007, val_acc:0.978]
Epoch [67/120    avg_loss:0.008, val_acc:0.975]
Epoch [68/120    avg_loss:0.021, val_acc:0.968]
Epoch [69/120    avg_loss:0.020, val_acc:0.916]
Epoch [70/120    avg_loss:0.025, val_acc:0.984]
Epoch [71/120    avg_loss:0.031, val_acc:0.983]
Epoch [72/120    avg_loss:0.021, val_acc:0.985]
Epoch [73/120    avg_loss:0.114, val_acc:0.966]
Epoch [74/120    avg_loss:0.044, val_acc:0.974]
Epoch [75/120    avg_loss:0.013, val_acc:0.984]
Epoch [76/120    avg_loss:0.021, val_acc:0.986]
Epoch [77/120    avg_loss:0.015, val_acc:0.983]
Epoch [78/120    avg_loss:0.007, val_acc:0.983]
Epoch [79/120    avg_loss:0.006, val_acc:0.984]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.005, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.984]
Epoch [88/120    avg_loss:0.004, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.004, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     1     0     0    21    12     3     0]
 [    0     0 18057     0    28     0     1     0     4     0]
 [    0     0     0  2011     9     0     0     0    10     6]
 [    0    34    22     0  2878     0     7     0    31     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4855     0     0    23]
 [    0     0     0     0     0     0     3  1286     0     1]
 [    0     0     0     0    29     0     0     0  3530    12]
 [    0     0     0     0    14    63     0     0     0   842]]

Accuracy:
99.19504494734052

F1 scores:
[       nan 0.99447943 0.99847936 0.99357708 0.97065767 0.97643098
 0.99436764 0.99381762 0.98755071 0.93399889]

Kappa:
0.98933533255797
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60c467a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.408, val_acc:0.709]
Epoch [2/120    avg_loss:0.786, val_acc:0.701]
Epoch [3/120    avg_loss:0.647, val_acc:0.830]
Epoch [4/120    avg_loss:0.484, val_acc:0.886]
Epoch [5/120    avg_loss:0.485, val_acc:0.840]
Epoch [6/120    avg_loss:0.372, val_acc:0.835]
Epoch [7/120    avg_loss:0.341, val_acc:0.899]
Epoch [8/120    avg_loss:0.264, val_acc:0.794]
Epoch [9/120    avg_loss:0.253, val_acc:0.855]
Epoch [10/120    avg_loss:0.322, val_acc:0.905]
Epoch [11/120    avg_loss:0.214, val_acc:0.942]
Epoch [12/120    avg_loss:0.234, val_acc:0.925]
Epoch [13/120    avg_loss:0.176, val_acc:0.921]
Epoch [14/120    avg_loss:0.178, val_acc:0.925]
Epoch [15/120    avg_loss:0.211, val_acc:0.926]
Epoch [16/120    avg_loss:0.195, val_acc:0.931]
Epoch [17/120    avg_loss:0.131, val_acc:0.942]
Epoch [18/120    avg_loss:0.110, val_acc:0.959]
Epoch [19/120    avg_loss:0.148, val_acc:0.953]
Epoch [20/120    avg_loss:0.107, val_acc:0.928]
Epoch [21/120    avg_loss:0.097, val_acc:0.879]
Epoch [22/120    avg_loss:0.085, val_acc:0.960]
Epoch [23/120    avg_loss:0.107, val_acc:0.939]
Epoch [24/120    avg_loss:0.083, val_acc:0.963]
Epoch [25/120    avg_loss:0.069, val_acc:0.957]
Epoch [26/120    avg_loss:0.117, val_acc:0.898]
Epoch [27/120    avg_loss:0.090, val_acc:0.970]
Epoch [28/120    avg_loss:0.073, val_acc:0.972]
Epoch [29/120    avg_loss:0.050, val_acc:0.973]
Epoch [30/120    avg_loss:0.040, val_acc:0.972]
Epoch [31/120    avg_loss:0.107, val_acc:0.948]
Epoch [32/120    avg_loss:0.070, val_acc:0.972]
Epoch [33/120    avg_loss:0.067, val_acc:0.974]
Epoch [34/120    avg_loss:0.060, val_acc:0.973]
Epoch [35/120    avg_loss:0.045, val_acc:0.971]
Epoch [36/120    avg_loss:0.034, val_acc:0.977]
Epoch [37/120    avg_loss:0.046, val_acc:0.939]
Epoch [38/120    avg_loss:0.045, val_acc:0.981]
Epoch [39/120    avg_loss:0.093, val_acc:0.974]
Epoch [40/120    avg_loss:0.041, val_acc:0.962]
Epoch [41/120    avg_loss:0.031, val_acc:0.969]
Epoch [42/120    avg_loss:0.022, val_acc:0.984]
Epoch [43/120    avg_loss:0.038, val_acc:0.983]
Epoch [44/120    avg_loss:0.016, val_acc:0.976]
Epoch [45/120    avg_loss:0.015, val_acc:0.983]
Epoch [46/120    avg_loss:0.033, val_acc:0.983]
Epoch [47/120    avg_loss:0.023, val_acc:0.984]
Epoch [48/120    avg_loss:0.018, val_acc:0.985]
Epoch [49/120    avg_loss:0.014, val_acc:0.977]
Epoch [50/120    avg_loss:0.035, val_acc:0.972]
Epoch [51/120    avg_loss:0.011, val_acc:0.976]
Epoch [52/120    avg_loss:0.010, val_acc:0.984]
Epoch [53/120    avg_loss:0.008, val_acc:0.986]
Epoch [54/120    avg_loss:0.015, val_acc:0.985]
Epoch [55/120    avg_loss:0.007, val_acc:0.989]
Epoch [56/120    avg_loss:0.023, val_acc:0.963]
Epoch [57/120    avg_loss:0.033, val_acc:0.984]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.016, val_acc:0.983]
Epoch [60/120    avg_loss:0.018, val_acc:0.981]
Epoch [61/120    avg_loss:0.010, val_acc:0.985]
Epoch [62/120    avg_loss:0.014, val_acc:0.986]
Epoch [63/120    avg_loss:0.008, val_acc:0.984]
Epoch [64/120    avg_loss:0.013, val_acc:0.986]
Epoch [65/120    avg_loss:0.009, val_acc:0.984]
Epoch [66/120    avg_loss:0.010, val_acc:0.978]
Epoch [67/120    avg_loss:0.008, val_acc:0.988]
Epoch [68/120    avg_loss:0.005, val_acc:0.986]
Epoch [69/120    avg_loss:0.006, val_acc:0.985]
Epoch [70/120    avg_loss:0.004, val_acc:0.985]
Epoch [71/120    avg_loss:0.005, val_acc:0.985]
Epoch [72/120    avg_loss:0.005, val_acc:0.985]
Epoch [73/120    avg_loss:0.004, val_acc:0.985]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.003, val_acc:0.985]
Epoch [76/120    avg_loss:0.003, val_acc:0.985]
Epoch [77/120    avg_loss:0.003, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.987]
Epoch [79/120    avg_loss:0.004, val_acc:0.986]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.004, val_acc:0.987]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.004, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.004, val_acc:0.987]
Epoch [88/120    avg_loss:0.002, val_acc:0.987]
Epoch [89/120    avg_loss:0.004, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.987]
Epoch [91/120    avg_loss:0.003, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.003, val_acc:0.987]
Epoch [96/120    avg_loss:0.008, val_acc:0.987]
Epoch [97/120    avg_loss:0.003, val_acc:0.987]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.003, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.003, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.004, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     0     3     0    19     0     4     0]
 [    0     0 18074     0    15     0     1     0     0     0]
 [    0     0     0  2036     0     0     0     0     0     0]
 [    0    39    22     1  2881     0     2     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4855     0     0    23]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     5     0     0    45     0     0     0  3491    30]
 [    0     0     0     7    14    47     0     0     1   850]]

Accuracy:
99.25770611910443

F1 scores:
[       nan 0.99456606 0.99894987 0.99803922 0.97166948 0.98231088
 0.99508096 0.99883586 0.98421201 0.93304061]

Kappa:
0.9901635512671564
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f13e80be7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.535, val_acc:0.464]
Epoch [2/120    avg_loss:0.777, val_acc:0.720]
Epoch [3/120    avg_loss:0.624, val_acc:0.817]
Epoch [4/120    avg_loss:0.454, val_acc:0.839]
Epoch [5/120    avg_loss:0.374, val_acc:0.771]
Epoch [6/120    avg_loss:0.408, val_acc:0.873]
Epoch [7/120    avg_loss:0.434, val_acc:0.859]
Epoch [8/120    avg_loss:0.346, val_acc:0.905]
Epoch [9/120    avg_loss:0.282, val_acc:0.842]
Epoch [10/120    avg_loss:0.309, val_acc:0.890]
Epoch [11/120    avg_loss:0.214, val_acc:0.898]
Epoch [12/120    avg_loss:0.224, val_acc:0.896]
Epoch [13/120    avg_loss:0.216, val_acc:0.901]
Epoch [14/120    avg_loss:0.161, val_acc:0.891]
Epoch [15/120    avg_loss:0.159, val_acc:0.943]
Epoch [16/120    avg_loss:0.167, val_acc:0.889]
Epoch [17/120    avg_loss:0.277, val_acc:0.865]
Epoch [18/120    avg_loss:0.174, val_acc:0.920]
Epoch [19/120    avg_loss:0.115, val_acc:0.946]
Epoch [20/120    avg_loss:0.080, val_acc:0.938]
Epoch [21/120    avg_loss:0.102, val_acc:0.954]
Epoch [22/120    avg_loss:0.137, val_acc:0.937]
Epoch [23/120    avg_loss:0.119, val_acc:0.944]
Epoch [24/120    avg_loss:0.136, val_acc:0.934]
Epoch [25/120    avg_loss:0.142, val_acc:0.957]
Epoch [26/120    avg_loss:0.084, val_acc:0.957]
Epoch [27/120    avg_loss:0.080, val_acc:0.940]
Epoch [28/120    avg_loss:0.050, val_acc:0.957]
Epoch [29/120    avg_loss:0.045, val_acc:0.847]
Epoch [30/120    avg_loss:0.054, val_acc:0.977]
Epoch [31/120    avg_loss:0.107, val_acc:0.947]
Epoch [32/120    avg_loss:0.071, val_acc:0.963]
Epoch [33/120    avg_loss:0.035, val_acc:0.970]
Epoch [34/120    avg_loss:0.037, val_acc:0.969]
Epoch [35/120    avg_loss:0.034, val_acc:0.963]
Epoch [36/120    avg_loss:0.088, val_acc:0.951]
Epoch [37/120    avg_loss:0.061, val_acc:0.960]
Epoch [38/120    avg_loss:0.052, val_acc:0.965]
Epoch [39/120    avg_loss:0.022, val_acc:0.979]
Epoch [40/120    avg_loss:0.033, val_acc:0.970]
Epoch [41/120    avg_loss:0.017, val_acc:0.977]
Epoch [42/120    avg_loss:0.034, val_acc:0.956]
Epoch [43/120    avg_loss:0.026, val_acc:0.979]
Epoch [44/120    avg_loss:0.014, val_acc:0.978]
Epoch [45/120    avg_loss:0.033, val_acc:0.982]
Epoch [46/120    avg_loss:0.043, val_acc:0.977]
Epoch [47/120    avg_loss:0.028, val_acc:0.946]
Epoch [48/120    avg_loss:0.058, val_acc:0.978]
Epoch [49/120    avg_loss:0.020, val_acc:0.977]
Epoch [50/120    avg_loss:0.034, val_acc:0.955]
Epoch [51/120    avg_loss:0.012, val_acc:0.977]
Epoch [52/120    avg_loss:0.008, val_acc:0.982]
Epoch [53/120    avg_loss:0.025, val_acc:0.978]
Epoch [54/120    avg_loss:0.019, val_acc:0.983]
Epoch [55/120    avg_loss:0.014, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.982]
Epoch [57/120    avg_loss:0.010, val_acc:0.983]
Epoch [58/120    avg_loss:0.022, val_acc:0.977]
Epoch [59/120    avg_loss:0.009, val_acc:0.985]
Epoch [60/120    avg_loss:0.010, val_acc:0.980]
Epoch [61/120    avg_loss:0.012, val_acc:0.984]
Epoch [62/120    avg_loss:0.013, val_acc:0.981]
Epoch [63/120    avg_loss:0.044, val_acc:0.977]
Epoch [64/120    avg_loss:0.032, val_acc:0.983]
Epoch [65/120    avg_loss:0.013, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.986]
Epoch [67/120    avg_loss:0.018, val_acc:0.986]
Epoch [68/120    avg_loss:0.022, val_acc:0.982]
Epoch [69/120    avg_loss:0.010, val_acc:0.979]
Epoch [70/120    avg_loss:0.010, val_acc:0.982]
Epoch [71/120    avg_loss:0.008, val_acc:0.967]
Epoch [72/120    avg_loss:0.010, val_acc:0.979]
Epoch [73/120    avg_loss:0.188, val_acc:0.903]
Epoch [74/120    avg_loss:0.143, val_acc:0.944]
Epoch [75/120    avg_loss:0.049, val_acc:0.976]
Epoch [76/120    avg_loss:0.014, val_acc:0.980]
Epoch [77/120    avg_loss:0.019, val_acc:0.979]
Epoch [78/120    avg_loss:0.018, val_acc:0.983]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.983]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.011, val_acc:0.982]
Epoch [83/120    avg_loss:0.012, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.983]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.981]
Epoch [92/120    avg_loss:0.005, val_acc:0.981]
Epoch [93/120    avg_loss:0.007, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.005, val_acc:0.982]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.009, val_acc:0.982]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.004, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.005, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.005, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.007, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     9     0    17     5     9     0]
 [    0     0 18052     0    26     0    10     0     2     0]
 [    0     0     0  2021     2     0     0     0     7     6]
 [    0    36    18     0  2871     0     7     0    28    12]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4849     0     0    29]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0     8     0     0    44     0     0     0  3492    27]
 [    0     0     0     0    14    44     0     0     0   861]]

Accuracy:
99.12033355023739

F1 scores:
[       nan 0.99347218 0.99845133 0.99630269 0.96699225 0.9826546
 0.99344397 0.99690163 0.98241665 0.92680301]

Kappa:
0.9883474508076425
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f34810337f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.422, val_acc:0.687]
Epoch [2/120    avg_loss:0.773, val_acc:0.759]
Epoch [3/120    avg_loss:0.584, val_acc:0.766]
Epoch [4/120    avg_loss:0.453, val_acc:0.886]
Epoch [5/120    avg_loss:0.414, val_acc:0.829]
Epoch [6/120    avg_loss:0.325, val_acc:0.843]
Epoch [7/120    avg_loss:0.306, val_acc:0.927]
Epoch [8/120    avg_loss:0.305, val_acc:0.879]
Epoch [9/120    avg_loss:0.279, val_acc:0.924]
Epoch [10/120    avg_loss:0.190, val_acc:0.920]
Epoch [11/120    avg_loss:0.234, val_acc:0.921]
Epoch [12/120    avg_loss:0.192, val_acc:0.917]
Epoch [13/120    avg_loss:0.201, val_acc:0.912]
Epoch [14/120    avg_loss:0.258, val_acc:0.914]
Epoch [15/120    avg_loss:0.175, val_acc:0.934]
Epoch [16/120    avg_loss:0.125, val_acc:0.944]
Epoch [17/120    avg_loss:0.171, val_acc:0.943]
Epoch [18/120    avg_loss:0.216, val_acc:0.954]
Epoch [19/120    avg_loss:0.121, val_acc:0.953]
Epoch [20/120    avg_loss:0.106, val_acc:0.932]
Epoch [21/120    avg_loss:0.100, val_acc:0.928]
Epoch [22/120    avg_loss:0.073, val_acc:0.955]
Epoch [23/120    avg_loss:0.113, val_acc:0.944]
Epoch [24/120    avg_loss:0.063, val_acc:0.961]
Epoch [25/120    avg_loss:0.061, val_acc:0.959]
Epoch [26/120    avg_loss:0.067, val_acc:0.964]
Epoch [27/120    avg_loss:0.055, val_acc:0.965]
Epoch [28/120    avg_loss:0.059, val_acc:0.962]
Epoch [29/120    avg_loss:0.130, val_acc:0.959]
Epoch [30/120    avg_loss:0.073, val_acc:0.934]
Epoch [31/120    avg_loss:0.095, val_acc:0.967]
Epoch [32/120    avg_loss:0.042, val_acc:0.978]
Epoch [33/120    avg_loss:0.045, val_acc:0.983]
Epoch [34/120    avg_loss:0.027, val_acc:0.982]
Epoch [35/120    avg_loss:0.042, val_acc:0.986]
Epoch [36/120    avg_loss:0.037, val_acc:0.978]
Epoch [37/120    avg_loss:0.023, val_acc:0.971]
Epoch [38/120    avg_loss:0.040, val_acc:0.965]
Epoch [39/120    avg_loss:0.043, val_acc:0.977]
Epoch [40/120    avg_loss:0.041, val_acc:0.982]
Epoch [41/120    avg_loss:0.025, val_acc:0.971]
Epoch [42/120    avg_loss:0.022, val_acc:0.984]
Epoch [43/120    avg_loss:0.032, val_acc:0.971]
Epoch [44/120    avg_loss:0.091, val_acc:0.971]
Epoch [45/120    avg_loss:0.047, val_acc:0.980]
Epoch [46/120    avg_loss:0.031, val_acc:0.981]
Epoch [47/120    avg_loss:0.015, val_acc:0.980]
Epoch [48/120    avg_loss:0.042, val_acc:0.978]
Epoch [49/120    avg_loss:0.020, val_acc:0.984]
Epoch [50/120    avg_loss:0.014, val_acc:0.984]
Epoch [51/120    avg_loss:0.016, val_acc:0.984]
Epoch [52/120    avg_loss:0.014, val_acc:0.984]
Epoch [53/120    avg_loss:0.016, val_acc:0.985]
Epoch [54/120    avg_loss:0.011, val_acc:0.984]
Epoch [55/120    avg_loss:0.014, val_acc:0.985]
Epoch [56/120    avg_loss:0.016, val_acc:0.985]
Epoch [57/120    avg_loss:0.010, val_acc:0.985]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.011, val_acc:0.986]
Epoch [60/120    avg_loss:0.007, val_acc:0.987]
Epoch [61/120    avg_loss:0.010, val_acc:0.987]
Epoch [62/120    avg_loss:0.010, val_acc:0.987]
Epoch [63/120    avg_loss:0.008, val_acc:0.987]
Epoch [64/120    avg_loss:0.010, val_acc:0.987]
Epoch [65/120    avg_loss:0.011, val_acc:0.988]
Epoch [66/120    avg_loss:0.007, val_acc:0.989]
Epoch [67/120    avg_loss:0.008, val_acc:0.989]
Epoch [68/120    avg_loss:0.011, val_acc:0.990]
Epoch [69/120    avg_loss:0.013, val_acc:0.990]
Epoch [70/120    avg_loss:0.010, val_acc:0.990]
Epoch [71/120    avg_loss:0.009, val_acc:0.990]
Epoch [72/120    avg_loss:0.008, val_acc:0.990]
Epoch [73/120    avg_loss:0.008, val_acc:0.990]
Epoch [74/120    avg_loss:0.009, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.990]
Epoch [76/120    avg_loss:0.007, val_acc:0.990]
Epoch [77/120    avg_loss:0.014, val_acc:0.990]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.008, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.991]
Epoch [82/120    avg_loss:0.007, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.990]
Epoch [84/120    avg_loss:0.006, val_acc:0.990]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.009, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.991]
Epoch [88/120    avg_loss:0.012, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.008, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.010, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6378     0     0     3     0    26    13    12     0]
 [    0     0 18079     0     9     0     0     0     2     0]
 [    0     0     0  2029     2     0     0     0     0     5]
 [    0    40    20     0  2873     0     7     0    31     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4850     0     0    28]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     4     0     0    35     0     0     0  3527     5]
 [    0     0     0     0    15    41     0     0     0   863]]

Accuracy:
99.26975634444365

F1 scores:
[       nan 0.99237591 0.99914339 0.99827798 0.97241496 0.98453414
 0.99354707 0.99343376 0.98754025 0.946791  ]

Kappa:
0.9903233146127263
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0bdb016748>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.493, val_acc:0.644]
Epoch [2/120    avg_loss:0.829, val_acc:0.736]
Epoch [3/120    avg_loss:0.611, val_acc:0.736]
Epoch [4/120    avg_loss:0.499, val_acc:0.805]
Epoch [5/120    avg_loss:0.385, val_acc:0.867]
Epoch [6/120    avg_loss:0.359, val_acc:0.873]
Epoch [7/120    avg_loss:0.297, val_acc:0.870]
Epoch [8/120    avg_loss:0.281, val_acc:0.856]
Epoch [9/120    avg_loss:0.268, val_acc:0.906]
Epoch [10/120    avg_loss:0.228, val_acc:0.907]
Epoch [11/120    avg_loss:0.227, val_acc:0.915]
Epoch [12/120    avg_loss:0.172, val_acc:0.914]
Epoch [13/120    avg_loss:0.172, val_acc:0.916]
Epoch [14/120    avg_loss:0.167, val_acc:0.921]
Epoch [15/120    avg_loss:0.127, val_acc:0.918]
Epoch [16/120    avg_loss:0.155, val_acc:0.938]
Epoch [17/120    avg_loss:0.200, val_acc:0.946]
Epoch [18/120    avg_loss:0.140, val_acc:0.927]
Epoch [19/120    avg_loss:0.094, val_acc:0.952]
Epoch [20/120    avg_loss:0.092, val_acc:0.940]
Epoch [21/120    avg_loss:0.083, val_acc:0.926]
Epoch [22/120    avg_loss:0.097, val_acc:0.938]
Epoch [23/120    avg_loss:0.061, val_acc:0.928]
Epoch [24/120    avg_loss:0.068, val_acc:0.955]
Epoch [25/120    avg_loss:0.087, val_acc:0.922]
Epoch [26/120    avg_loss:0.106, val_acc:0.960]
Epoch [27/120    avg_loss:0.065, val_acc:0.957]
Epoch [28/120    avg_loss:0.029, val_acc:0.975]
Epoch [29/120    avg_loss:0.042, val_acc:0.962]
Epoch [30/120    avg_loss:0.069, val_acc:0.959]
Epoch [31/120    avg_loss:0.056, val_acc:0.971]
Epoch [32/120    avg_loss:0.058, val_acc:0.959]
Epoch [33/120    avg_loss:0.035, val_acc:0.968]
Epoch [34/120    avg_loss:0.031, val_acc:0.973]
Epoch [35/120    avg_loss:0.021, val_acc:0.976]
Epoch [36/120    avg_loss:0.030, val_acc:0.971]
Epoch [37/120    avg_loss:0.039, val_acc:0.976]
Epoch [38/120    avg_loss:0.285, val_acc:0.910]
Epoch [39/120    avg_loss:0.099, val_acc:0.943]
Epoch [40/120    avg_loss:0.071, val_acc:0.951]
Epoch [41/120    avg_loss:0.044, val_acc:0.955]
Epoch [42/120    avg_loss:0.029, val_acc:0.971]
Epoch [43/120    avg_loss:0.116, val_acc:0.932]
Epoch [44/120    avg_loss:0.093, val_acc:0.938]
Epoch [45/120    avg_loss:0.084, val_acc:0.921]
Epoch [46/120    avg_loss:0.029, val_acc:0.981]
Epoch [47/120    avg_loss:0.026, val_acc:0.980]
Epoch [48/120    avg_loss:0.024, val_acc:0.966]
Epoch [49/120    avg_loss:0.034, val_acc:0.976]
Epoch [50/120    avg_loss:0.018, val_acc:0.976]
Epoch [51/120    avg_loss:0.016, val_acc:0.973]
Epoch [52/120    avg_loss:0.016, val_acc:0.977]
Epoch [53/120    avg_loss:0.014, val_acc:0.973]
Epoch [54/120    avg_loss:0.029, val_acc:0.973]
Epoch [55/120    avg_loss:0.014, val_acc:0.980]
Epoch [56/120    avg_loss:0.025, val_acc:0.986]
Epoch [57/120    avg_loss:0.014, val_acc:0.959]
Epoch [58/120    avg_loss:0.016, val_acc:0.967]
Epoch [59/120    avg_loss:0.016, val_acc:0.981]
Epoch [60/120    avg_loss:0.010, val_acc:0.981]
Epoch [61/120    avg_loss:0.014, val_acc:0.980]
Epoch [62/120    avg_loss:0.022, val_acc:0.978]
Epoch [63/120    avg_loss:0.043, val_acc:0.932]
Epoch [64/120    avg_loss:0.032, val_acc:0.978]
Epoch [65/120    avg_loss:0.018, val_acc:0.970]
Epoch [66/120    avg_loss:0.017, val_acc:0.979]
Epoch [67/120    avg_loss:0.013, val_acc:0.982]
Epoch [68/120    avg_loss:0.016, val_acc:0.984]
Epoch [69/120    avg_loss:0.009, val_acc:0.982]
Epoch [70/120    avg_loss:0.006, val_acc:0.987]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.987]
Epoch [74/120    avg_loss:0.006, val_acc:0.986]
Epoch [75/120    avg_loss:0.005, val_acc:0.986]
Epoch [76/120    avg_loss:0.005, val_acc:0.986]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.004, val_acc:0.987]
Epoch [79/120    avg_loss:0.003, val_acc:0.986]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.004, val_acc:0.988]
Epoch [82/120    avg_loss:0.004, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.985]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.004, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.003, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     3     1     0    15     0     0     0]
 [    0     0 18019     0    26     0    43     0     2     0]
 [    0     0     0  2033     0     0     0     0     0     3]
 [    0    41    22     3  2873     0     5     0    27     1]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4848     0     0    30]
 [    0     0     0     0     0     0     6  1282     0     2]
 [    0     4     0    10    44     0     0     0  3501    12]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.17094449666209

F1 scores:
[       nan 0.99503491 0.99742603 0.99534884 0.96897133 0.98862775
 0.9898928  0.99688958 0.9860583  0.95010846]

Kappa:
0.9890200806242202
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f289626e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.469, val_acc:0.639]
Epoch [2/120    avg_loss:0.861, val_acc:0.768]
Epoch [3/120    avg_loss:0.606, val_acc:0.749]
Epoch [4/120    avg_loss:0.524, val_acc:0.862]
Epoch [5/120    avg_loss:0.449, val_acc:0.827]
Epoch [6/120    avg_loss:0.338, val_acc:0.864]
Epoch [7/120    avg_loss:0.317, val_acc:0.797]
Epoch [8/120    avg_loss:0.271, val_acc:0.839]
Epoch [9/120    avg_loss:0.215, val_acc:0.922]
Epoch [10/120    avg_loss:0.169, val_acc:0.934]
Epoch [11/120    avg_loss:0.184, val_acc:0.884]
Epoch [12/120    avg_loss:0.207, val_acc:0.944]
Epoch [13/120    avg_loss:0.151, val_acc:0.906]
Epoch [14/120    avg_loss:0.144, val_acc:0.833]
Epoch [15/120    avg_loss:0.144, val_acc:0.912]
Epoch [16/120    avg_loss:0.090, val_acc:0.961]
Epoch [17/120    avg_loss:0.090, val_acc:0.955]
Epoch [18/120    avg_loss:0.156, val_acc:0.953]
Epoch [19/120    avg_loss:0.103, val_acc:0.965]
Epoch [20/120    avg_loss:0.164, val_acc:0.898]
Epoch [21/120    avg_loss:0.139, val_acc:0.914]
Epoch [22/120    avg_loss:0.102, val_acc:0.954]
Epoch [23/120    avg_loss:0.151, val_acc:0.925]
Epoch [24/120    avg_loss:0.080, val_acc:0.947]
Epoch [25/120    avg_loss:0.085, val_acc:0.944]
Epoch [26/120    avg_loss:0.047, val_acc:0.952]
Epoch [27/120    avg_loss:0.074, val_acc:0.962]
Epoch [28/120    avg_loss:0.052, val_acc:0.978]
Epoch [29/120    avg_loss:0.041, val_acc:0.968]
Epoch [30/120    avg_loss:0.062, val_acc:0.970]
Epoch [31/120    avg_loss:0.041, val_acc:0.965]
Epoch [32/120    avg_loss:0.039, val_acc:0.963]
Epoch [33/120    avg_loss:0.048, val_acc:0.980]
Epoch [34/120    avg_loss:0.037, val_acc:0.967]
Epoch [35/120    avg_loss:0.025, val_acc:0.984]
Epoch [36/120    avg_loss:0.085, val_acc:0.966]
Epoch [37/120    avg_loss:0.045, val_acc:0.975]
Epoch [38/120    avg_loss:0.039, val_acc:0.971]
Epoch [39/120    avg_loss:0.043, val_acc:0.964]
Epoch [40/120    avg_loss:0.034, val_acc:0.964]
Epoch [41/120    avg_loss:0.042, val_acc:0.981]
Epoch [42/120    avg_loss:0.028, val_acc:0.983]
Epoch [43/120    avg_loss:0.023, val_acc:0.971]
Epoch [44/120    avg_loss:0.023, val_acc:0.981]
Epoch [45/120    avg_loss:0.021, val_acc:0.980]
Epoch [46/120    avg_loss:0.024, val_acc:0.985]
Epoch [47/120    avg_loss:0.031, val_acc:0.975]
Epoch [48/120    avg_loss:0.047, val_acc:0.971]
Epoch [49/120    avg_loss:0.045, val_acc:0.979]
Epoch [50/120    avg_loss:0.014, val_acc:0.988]
Epoch [51/120    avg_loss:0.016, val_acc:0.984]
Epoch [52/120    avg_loss:0.009, val_acc:0.987]
Epoch [53/120    avg_loss:0.017, val_acc:0.985]
Epoch [54/120    avg_loss:0.008, val_acc:0.988]
Epoch [55/120    avg_loss:0.012, val_acc:0.987]
Epoch [56/120    avg_loss:0.742, val_acc:0.586]
Epoch [57/120    avg_loss:0.902, val_acc:0.666]
Epoch [58/120    avg_loss:0.800, val_acc:0.707]
Epoch [59/120    avg_loss:0.666, val_acc:0.701]
Epoch [60/120    avg_loss:0.690, val_acc:0.751]
Epoch [61/120    avg_loss:0.633, val_acc:0.680]
Epoch [62/120    avg_loss:0.631, val_acc:0.690]
Epoch [63/120    avg_loss:0.640, val_acc:0.779]
Epoch [64/120    avg_loss:0.654, val_acc:0.761]
Epoch [65/120    avg_loss:0.546, val_acc:0.805]
Epoch [66/120    avg_loss:0.464, val_acc:0.808]
Epoch [67/120    avg_loss:0.490, val_acc:0.816]
Epoch [68/120    avg_loss:0.373, val_acc:0.850]
Epoch [69/120    avg_loss:0.361, val_acc:0.856]
Epoch [70/120    avg_loss:0.378, val_acc:0.840]
Epoch [71/120    avg_loss:0.361, val_acc:0.825]
Epoch [72/120    avg_loss:0.331, val_acc:0.840]
Epoch [73/120    avg_loss:0.311, val_acc:0.836]
Epoch [74/120    avg_loss:0.312, val_acc:0.849]
Epoch [75/120    avg_loss:0.313, val_acc:0.844]
Epoch [76/120    avg_loss:0.312, val_acc:0.862]
Epoch [77/120    avg_loss:0.297, val_acc:0.841]
Epoch [78/120    avg_loss:0.318, val_acc:0.842]
Epoch [79/120    avg_loss:0.323, val_acc:0.864]
Epoch [80/120    avg_loss:0.330, val_acc:0.846]
Epoch [81/120    avg_loss:0.299, val_acc:0.855]
Epoch [82/120    avg_loss:0.311, val_acc:0.864]
Epoch [83/120    avg_loss:0.284, val_acc:0.862]
Epoch [84/120    avg_loss:0.330, val_acc:0.861]
Epoch [85/120    avg_loss:0.309, val_acc:0.868]
Epoch [86/120    avg_loss:0.298, val_acc:0.864]
Epoch [87/120    avg_loss:0.303, val_acc:0.859]
Epoch [88/120    avg_loss:0.294, val_acc:0.865]
Epoch [89/120    avg_loss:0.307, val_acc:0.864]
Epoch [90/120    avg_loss:0.307, val_acc:0.862]
Epoch [91/120    avg_loss:0.289, val_acc:0.857]
Epoch [92/120    avg_loss:0.285, val_acc:0.859]
Epoch [93/120    avg_loss:0.291, val_acc:0.862]
Epoch [94/120    avg_loss:0.305, val_acc:0.862]
Epoch [95/120    avg_loss:0.316, val_acc:0.862]
Epoch [96/120    avg_loss:0.301, val_acc:0.863]
Epoch [97/120    avg_loss:0.289, val_acc:0.863]
Epoch [98/120    avg_loss:0.296, val_acc:0.863]
Epoch [99/120    avg_loss:0.288, val_acc:0.862]
Epoch [100/120    avg_loss:0.297, val_acc:0.861]
Epoch [101/120    avg_loss:0.278, val_acc:0.863]
Epoch [102/120    avg_loss:0.305, val_acc:0.862]
Epoch [103/120    avg_loss:0.288, val_acc:0.862]
Epoch [104/120    avg_loss:0.302, val_acc:0.862]
Epoch [105/120    avg_loss:0.293, val_acc:0.862]
Epoch [106/120    avg_loss:0.295, val_acc:0.862]
Epoch [107/120    avg_loss:0.311, val_acc:0.862]
Epoch [108/120    avg_loss:0.286, val_acc:0.862]
Epoch [109/120    avg_loss:0.276, val_acc:0.862]
Epoch [110/120    avg_loss:0.306, val_acc:0.862]
Epoch [111/120    avg_loss:0.315, val_acc:0.862]
Epoch [112/120    avg_loss:0.303, val_acc:0.862]
Epoch [113/120    avg_loss:0.320, val_acc:0.862]
Epoch [114/120    avg_loss:0.282, val_acc:0.862]
Epoch [115/120    avg_loss:0.275, val_acc:0.862]
Epoch [116/120    avg_loss:0.293, val_acc:0.862]
Epoch [117/120    avg_loss:0.271, val_acc:0.863]
Epoch [118/120    avg_loss:0.282, val_acc:0.863]
Epoch [119/120    avg_loss:0.278, val_acc:0.863]
Epoch [120/120    avg_loss:0.274, val_acc:0.863]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5190     2    93   340     0    63    10   632   102]
 [    0    95 17242     0   400     0   353     0     0     0]
 [    0    17     0  1896     2     0     0     0    87    34]
 [    0   116    77     0  2684     0    47     0    45     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1   419     3    19     0  4427     0     9     0]
 [    0     3     0     0     1     0    13  1255     0    18]
 [    0    47     0    72   138     0    26     0  3287     1]
 [    0     7     0    26    32   111     3     6     8   726]]

Accuracy:
91.61063311883933

F1 scores:
[       nan 0.8716829  0.96243371 0.91904993 0.81481481 0.95920617
 0.90254842 0.9800859  0.86058385 0.80532446]

Kappa:
0.8895439804524449
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f771116a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.441, val_acc:0.418]
Epoch [2/120    avg_loss:0.843, val_acc:0.760]
Epoch [3/120    avg_loss:0.599, val_acc:0.820]
Epoch [4/120    avg_loss:0.424, val_acc:0.793]
Epoch [5/120    avg_loss:0.405, val_acc:0.854]
Epoch [6/120    avg_loss:0.324, val_acc:0.881]
Epoch [7/120    avg_loss:0.355, val_acc:0.863]
Epoch [8/120    avg_loss:0.261, val_acc:0.930]
Epoch [9/120    avg_loss:0.352, val_acc:0.883]
Epoch [10/120    avg_loss:0.265, val_acc:0.865]
Epoch [11/120    avg_loss:0.252, val_acc:0.904]
Epoch [12/120    avg_loss:0.179, val_acc:0.884]
Epoch [13/120    avg_loss:0.162, val_acc:0.906]
Epoch [14/120    avg_loss:0.130, val_acc:0.933]
Epoch [15/120    avg_loss:0.145, val_acc:0.910]
Epoch [16/120    avg_loss:0.137, val_acc:0.922]
Epoch [17/120    avg_loss:0.157, val_acc:0.898]
Epoch [18/120    avg_loss:0.180, val_acc:0.925]
Epoch [19/120    avg_loss:0.094, val_acc:0.959]
Epoch [20/120    avg_loss:0.112, val_acc:0.937]
Epoch [21/120    avg_loss:0.163, val_acc:0.968]
Epoch [22/120    avg_loss:0.085, val_acc:0.959]
Epoch [23/120    avg_loss:0.084, val_acc:0.964]
Epoch [24/120    avg_loss:0.076, val_acc:0.973]
Epoch [25/120    avg_loss:0.072, val_acc:0.968]
Epoch [26/120    avg_loss:0.076, val_acc:0.977]
Epoch [27/120    avg_loss:0.066, val_acc:0.951]
Epoch [28/120    avg_loss:0.041, val_acc:0.979]
Epoch [29/120    avg_loss:0.059, val_acc:0.964]
Epoch [30/120    avg_loss:0.033, val_acc:0.970]
Epoch [31/120    avg_loss:0.039, val_acc:0.957]
Epoch [32/120    avg_loss:0.039, val_acc:0.969]
Epoch [33/120    avg_loss:0.119, val_acc:0.898]
Epoch [34/120    avg_loss:0.062, val_acc:0.977]
Epoch [35/120    avg_loss:0.051, val_acc:0.982]
Epoch [36/120    avg_loss:0.033, val_acc:0.942]
Epoch [37/120    avg_loss:0.035, val_acc:0.987]
Epoch [38/120    avg_loss:0.035, val_acc:0.966]
Epoch [39/120    avg_loss:0.047, val_acc:0.967]
Epoch [40/120    avg_loss:0.023, val_acc:0.987]
Epoch [41/120    avg_loss:0.015, val_acc:0.987]
Epoch [42/120    avg_loss:0.038, val_acc:0.985]
Epoch [43/120    avg_loss:0.021, val_acc:0.986]
Epoch [44/120    avg_loss:0.020, val_acc:0.983]
Epoch [45/120    avg_loss:0.030, val_acc:0.990]
Epoch [46/120    avg_loss:0.070, val_acc:0.984]
Epoch [47/120    avg_loss:0.034, val_acc:0.977]
Epoch [48/120    avg_loss:0.055, val_acc:0.972]
Epoch [49/120    avg_loss:0.014, val_acc:0.990]
Epoch [50/120    avg_loss:0.014, val_acc:0.989]
Epoch [51/120    avg_loss:0.009, val_acc:0.990]
Epoch [52/120    avg_loss:0.012, val_acc:0.989]
Epoch [53/120    avg_loss:0.017, val_acc:0.984]
Epoch [54/120    avg_loss:0.009, val_acc:0.987]
Epoch [55/120    avg_loss:0.008, val_acc:0.992]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.022, val_acc:0.977]
Epoch [58/120    avg_loss:0.028, val_acc:0.973]
Epoch [59/120    avg_loss:0.047, val_acc:0.974]
Epoch [60/120    avg_loss:0.015, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.990]
Epoch [62/120    avg_loss:0.005, val_acc:0.990]
Epoch [63/120    avg_loss:0.006, val_acc:0.984]
Epoch [64/120    avg_loss:0.008, val_acc:0.990]
Epoch [65/120    avg_loss:0.010, val_acc:0.989]
Epoch [66/120    avg_loss:0.013, val_acc:0.991]
Epoch [67/120    avg_loss:0.005, val_acc:0.991]
Epoch [68/120    avg_loss:0.005, val_acc:0.988]
Epoch [69/120    avg_loss:0.005, val_acc:0.992]
Epoch [70/120    avg_loss:0.004, val_acc:0.992]
Epoch [71/120    avg_loss:0.003, val_acc:0.992]
Epoch [72/120    avg_loss:0.004, val_acc:0.992]
Epoch [73/120    avg_loss:0.005, val_acc:0.993]
Epoch [74/120    avg_loss:0.004, val_acc:0.993]
Epoch [75/120    avg_loss:0.003, val_acc:0.993]
Epoch [76/120    avg_loss:0.003, val_acc:0.993]
Epoch [77/120    avg_loss:0.003, val_acc:0.993]
Epoch [78/120    avg_loss:0.006, val_acc:0.993]
Epoch [79/120    avg_loss:0.004, val_acc:0.993]
Epoch [80/120    avg_loss:0.004, val_acc:0.993]
Epoch [81/120    avg_loss:0.004, val_acc:0.993]
Epoch [82/120    avg_loss:0.003, val_acc:0.993]
Epoch [83/120    avg_loss:0.004, val_acc:0.993]
Epoch [84/120    avg_loss:0.003, val_acc:0.993]
Epoch [85/120    avg_loss:0.003, val_acc:0.993]
Epoch [86/120    avg_loss:0.004, val_acc:0.993]
Epoch [87/120    avg_loss:0.003, val_acc:0.993]
Epoch [88/120    avg_loss:0.003, val_acc:0.993]
Epoch [89/120    avg_loss:0.005, val_acc:0.993]
Epoch [90/120    avg_loss:0.003, val_acc:0.992]
Epoch [91/120    avg_loss:0.003, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.993]
Epoch [93/120    avg_loss:0.005, val_acc:0.993]
Epoch [94/120    avg_loss:0.003, val_acc:0.993]
Epoch [95/120    avg_loss:0.003, val_acc:0.993]
Epoch [96/120    avg_loss:0.003, val_acc:0.993]
Epoch [97/120    avg_loss:0.003, val_acc:0.993]
Epoch [98/120    avg_loss:0.005, val_acc:0.993]
Epoch [99/120    avg_loss:0.003, val_acc:0.993]
Epoch [100/120    avg_loss:0.002, val_acc:0.993]
Epoch [101/120    avg_loss:0.002, val_acc:0.993]
Epoch [102/120    avg_loss:0.002, val_acc:0.993]
Epoch [103/120    avg_loss:0.003, val_acc:0.993]
Epoch [104/120    avg_loss:0.004, val_acc:0.993]
Epoch [105/120    avg_loss:0.003, val_acc:0.993]
Epoch [106/120    avg_loss:0.005, val_acc:0.993]
Epoch [107/120    avg_loss:0.005, val_acc:0.993]
Epoch [108/120    avg_loss:0.004, val_acc:0.993]
Epoch [109/120    avg_loss:0.003, val_acc:0.993]
Epoch [110/120    avg_loss:0.003, val_acc:0.993]
Epoch [111/120    avg_loss:0.003, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.993]
Epoch [113/120    avg_loss:0.004, val_acc:0.993]
Epoch [114/120    avg_loss:0.002, val_acc:0.993]
Epoch [115/120    avg_loss:0.003, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.008, val_acc:0.993]
Epoch [118/120    avg_loss:0.004, val_acc:0.993]
Epoch [119/120    avg_loss:0.003, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     0    11     0    15    12     3     0]
 [    0     0 18072     0    13     0     0     0     5     0]
 [    0     0     0  2026     3     0     0     0     3     4]
 [    0    36    11     0  2896     0     2     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4847     0     0    31]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0     2     0     0    32     0     0     0  3518    19]
 [    0     0     0     3    14    44     0     0     0   858]]

Accuracy:
99.29385679512207

F1 scores:
[       nan 0.9938574  0.9991983  0.99680197 0.97492005 0.98342125
 0.99497075 0.99420626 0.98723165 0.93617021]

Kappa:
0.9906447718180904
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f49b11db7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.529, val_acc:0.675]
Epoch [2/120    avg_loss:0.881, val_acc:0.718]
Epoch [3/120    avg_loss:0.655, val_acc:0.763]
Epoch [4/120    avg_loss:0.451, val_acc:0.878]
Epoch [5/120    avg_loss:0.453, val_acc:0.884]
Epoch [6/120    avg_loss:0.377, val_acc:0.905]
Epoch [7/120    avg_loss:0.314, val_acc:0.847]
Epoch [8/120    avg_loss:0.313, val_acc:0.856]
Epoch [9/120    avg_loss:0.293, val_acc:0.928]
Epoch [10/120    avg_loss:0.278, val_acc:0.904]
Epoch [11/120    avg_loss:0.206, val_acc:0.912]
Epoch [12/120    avg_loss:0.167, val_acc:0.959]
Epoch [13/120    avg_loss:0.182, val_acc:0.949]
Epoch [14/120    avg_loss:0.135, val_acc:0.922]
Epoch [15/120    avg_loss:0.114, val_acc:0.925]
Epoch [16/120    avg_loss:0.117, val_acc:0.934]
Epoch [17/120    avg_loss:0.105, val_acc:0.967]
Epoch [18/120    avg_loss:0.123, val_acc:0.951]
Epoch [19/120    avg_loss:0.103, val_acc:0.949]
Epoch [20/120    avg_loss:0.113, val_acc:0.927]
Epoch [21/120    avg_loss:0.077, val_acc:0.966]
Epoch [22/120    avg_loss:0.176, val_acc:0.871]
Epoch [23/120    avg_loss:0.074, val_acc:0.966]
Epoch [24/120    avg_loss:0.061, val_acc:0.949]
Epoch [25/120    avg_loss:0.087, val_acc:0.891]
Epoch [26/120    avg_loss:0.135, val_acc:0.953]
Epoch [27/120    avg_loss:0.065, val_acc:0.955]
Epoch [28/120    avg_loss:0.069, val_acc:0.982]
Epoch [29/120    avg_loss:0.039, val_acc:0.977]
Epoch [30/120    avg_loss:0.052, val_acc:0.959]
Epoch [31/120    avg_loss:0.048, val_acc:0.977]
Epoch [32/120    avg_loss:0.024, val_acc:0.960]
Epoch [33/120    avg_loss:0.039, val_acc:0.971]
Epoch [34/120    avg_loss:0.033, val_acc:0.978]
Epoch [35/120    avg_loss:0.019, val_acc:0.987]
Epoch [36/120    avg_loss:0.024, val_acc:0.985]
Epoch [37/120    avg_loss:0.016, val_acc:0.987]
Epoch [38/120    avg_loss:0.017, val_acc:0.985]
Epoch [39/120    avg_loss:0.018, val_acc:0.985]
Epoch [40/120    avg_loss:0.023, val_acc:0.984]
Epoch [41/120    avg_loss:0.018, val_acc:0.984]
Epoch [42/120    avg_loss:0.018, val_acc:0.972]
Epoch [43/120    avg_loss:0.016, val_acc:0.987]
Epoch [44/120    avg_loss:0.012, val_acc:0.985]
Epoch [45/120    avg_loss:0.023, val_acc:0.985]
Epoch [46/120    avg_loss:0.025, val_acc:0.963]
Epoch [47/120    avg_loss:1.175, val_acc:0.519]
Epoch [48/120    avg_loss:0.847, val_acc:0.647]
Epoch [49/120    avg_loss:0.730, val_acc:0.695]
Epoch [50/120    avg_loss:0.654, val_acc:0.732]
Epoch [51/120    avg_loss:0.672, val_acc:0.668]
Epoch [52/120    avg_loss:0.628, val_acc:0.749]
Epoch [53/120    avg_loss:0.649, val_acc:0.718]
Epoch [54/120    avg_loss:0.584, val_acc:0.790]
Epoch [55/120    avg_loss:0.610, val_acc:0.716]
Epoch [56/120    avg_loss:0.564, val_acc:0.734]
Epoch [57/120    avg_loss:0.433, val_acc:0.817]
Epoch [58/120    avg_loss:0.418, val_acc:0.819]
Epoch [59/120    avg_loss:0.435, val_acc:0.814]
Epoch [60/120    avg_loss:0.390, val_acc:0.824]
Epoch [61/120    avg_loss:0.406, val_acc:0.833]
Epoch [62/120    avg_loss:0.392, val_acc:0.834]
Epoch [63/120    avg_loss:0.387, val_acc:0.845]
Epoch [64/120    avg_loss:0.390, val_acc:0.852]
Epoch [65/120    avg_loss:0.377, val_acc:0.823]
Epoch [66/120    avg_loss:0.415, val_acc:0.817]
Epoch [67/120    avg_loss:0.381, val_acc:0.859]
Epoch [68/120    avg_loss:0.380, val_acc:0.849]
Epoch [69/120    avg_loss:0.370, val_acc:0.826]
Epoch [70/120    avg_loss:0.359, val_acc:0.831]
Epoch [71/120    avg_loss:0.350, val_acc:0.833]
Epoch [72/120    avg_loss:0.343, val_acc:0.835]
Epoch [73/120    avg_loss:0.348, val_acc:0.848]
Epoch [74/120    avg_loss:0.355, val_acc:0.854]
Epoch [75/120    avg_loss:0.357, val_acc:0.855]
Epoch [76/120    avg_loss:0.352, val_acc:0.856]
Epoch [77/120    avg_loss:0.324, val_acc:0.853]
Epoch [78/120    avg_loss:0.335, val_acc:0.854]
Epoch [79/120    avg_loss:0.367, val_acc:0.854]
Epoch [80/120    avg_loss:0.347, val_acc:0.853]
Epoch [81/120    avg_loss:0.355, val_acc:0.856]
Epoch [82/120    avg_loss:0.356, val_acc:0.854]
Epoch [83/120    avg_loss:0.349, val_acc:0.854]
Epoch [84/120    avg_loss:0.346, val_acc:0.856]
Epoch [85/120    avg_loss:0.326, val_acc:0.856]
Epoch [86/120    avg_loss:0.340, val_acc:0.856]
Epoch [87/120    avg_loss:0.363, val_acc:0.855]
Epoch [88/120    avg_loss:0.367, val_acc:0.855]
Epoch [89/120    avg_loss:0.351, val_acc:0.855]
Epoch [90/120    avg_loss:0.348, val_acc:0.856]
Epoch [91/120    avg_loss:0.337, val_acc:0.857]
Epoch [92/120    avg_loss:0.336, val_acc:0.856]
Epoch [93/120    avg_loss:0.359, val_acc:0.856]
Epoch [94/120    avg_loss:0.338, val_acc:0.855]
Epoch [95/120    avg_loss:0.345, val_acc:0.856]
Epoch [96/120    avg_loss:0.358, val_acc:0.856]
Epoch [97/120    avg_loss:0.334, val_acc:0.857]
Epoch [98/120    avg_loss:0.350, val_acc:0.857]
Epoch [99/120    avg_loss:0.388, val_acc:0.857]
Epoch [100/120    avg_loss:0.351, val_acc:0.857]
Epoch [101/120    avg_loss:0.352, val_acc:0.857]
Epoch [102/120    avg_loss:0.349, val_acc:0.857]
Epoch [103/120    avg_loss:0.329, val_acc:0.857]
Epoch [104/120    avg_loss:0.386, val_acc:0.857]
Epoch [105/120    avg_loss:0.360, val_acc:0.856]
Epoch [106/120    avg_loss:0.353, val_acc:0.856]
Epoch [107/120    avg_loss:0.359, val_acc:0.856]
Epoch [108/120    avg_loss:0.363, val_acc:0.856]
Epoch [109/120    avg_loss:0.329, val_acc:0.856]
Epoch [110/120    avg_loss:0.347, val_acc:0.856]
Epoch [111/120    avg_loss:0.341, val_acc:0.856]
Epoch [112/120    avg_loss:0.346, val_acc:0.856]
Epoch [113/120    avg_loss:0.375, val_acc:0.856]
Epoch [114/120    avg_loss:0.345, val_acc:0.856]
Epoch [115/120    avg_loss:0.338, val_acc:0.856]
Epoch [116/120    avg_loss:0.346, val_acc:0.856]
Epoch [117/120    avg_loss:0.328, val_acc:0.856]
Epoch [118/120    avg_loss:0.335, val_acc:0.856]
Epoch [119/120    avg_loss:0.355, val_acc:0.856]
Epoch [120/120    avg_loss:0.372, val_acc:0.856]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5242     1    57   338     0   108    31   595    60]
 [    0    23 16057     0   591     0  1419     0     0     0]
 [    0    14     0  1876     6     0     1     0   110    29]
 [    0   124    56     0  2630     0    87     0    67     8]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1   214    10    17     0  4611     0    25     0]
 [    0     5     0     0     3     0    11  1261     0    10]
 [    0    61     0    58   109     0    73     0  3270     0]
 [    0    25     0    24    28   158    14     4     6   660]]

Accuracy:
88.95958354421228

F1 scores:
[       nan 0.879014   0.93305828 0.92391037 0.78577831 0.94291908
 0.82324585 0.97525135 0.855573   0.78291815]

Kappa:
0.8566615155401438
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f625dc44828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.478, val_acc:0.643]
Epoch [2/120    avg_loss:0.787, val_acc:0.578]
Epoch [3/120    avg_loss:0.568, val_acc:0.798]
Epoch [4/120    avg_loss:0.444, val_acc:0.748]
Epoch [5/120    avg_loss:0.399, val_acc:0.781]
Epoch [6/120    avg_loss:0.378, val_acc:0.792]
Epoch [7/120    avg_loss:0.319, val_acc:0.797]
Epoch [8/120    avg_loss:0.309, val_acc:0.843]
Epoch [9/120    avg_loss:0.232, val_acc:0.908]
Epoch [10/120    avg_loss:0.256, val_acc:0.895]
Epoch [11/120    avg_loss:0.225, val_acc:0.927]
Epoch [12/120    avg_loss:0.168, val_acc:0.936]
Epoch [13/120    avg_loss:0.188, val_acc:0.873]
Epoch [14/120    avg_loss:0.163, val_acc:0.884]
Epoch [15/120    avg_loss:0.118, val_acc:0.944]
Epoch [16/120    avg_loss:0.109, val_acc:0.935]
Epoch [17/120    avg_loss:0.109, val_acc:0.964]
Epoch [18/120    avg_loss:0.093, val_acc:0.918]
Epoch [19/120    avg_loss:0.147, val_acc:0.963]
Epoch [20/120    avg_loss:0.110, val_acc:0.938]
Epoch [21/120    avg_loss:0.083, val_acc:0.958]
Epoch [22/120    avg_loss:0.077, val_acc:0.948]
Epoch [23/120    avg_loss:0.094, val_acc:0.968]
Epoch [24/120    avg_loss:0.067, val_acc:0.971]
Epoch [25/120    avg_loss:0.128, val_acc:0.930]
Epoch [26/120    avg_loss:0.088, val_acc:0.963]
Epoch [27/120    avg_loss:0.061, val_acc:0.973]
Epoch [28/120    avg_loss:0.089, val_acc:0.875]
Epoch [29/120    avg_loss:0.097, val_acc:0.963]
Epoch [30/120    avg_loss:0.067, val_acc:0.970]
Epoch [31/120    avg_loss:0.040, val_acc:0.978]
Epoch [32/120    avg_loss:0.039, val_acc:0.968]
Epoch [33/120    avg_loss:0.051, val_acc:0.959]
Epoch [34/120    avg_loss:0.085, val_acc:0.928]
Epoch [35/120    avg_loss:0.050, val_acc:0.937]
Epoch [36/120    avg_loss:0.050, val_acc:0.932]
Epoch [37/120    avg_loss:0.032, val_acc:0.970]
Epoch [38/120    avg_loss:0.024, val_acc:0.969]
Epoch [39/120    avg_loss:0.031, val_acc:0.975]
Epoch [40/120    avg_loss:0.025, val_acc:0.978]
Epoch [41/120    avg_loss:0.016, val_acc:0.968]
Epoch [42/120    avg_loss:0.021, val_acc:0.966]
Epoch [43/120    avg_loss:0.039, val_acc:0.969]
Epoch [44/120    avg_loss:0.043, val_acc:0.963]
Epoch [45/120    avg_loss:0.034, val_acc:0.974]
Epoch [46/120    avg_loss:0.023, val_acc:0.979]
Epoch [47/120    avg_loss:0.018, val_acc:0.983]
Epoch [48/120    avg_loss:0.015, val_acc:0.983]
Epoch [49/120    avg_loss:0.014, val_acc:0.983]
Epoch [50/120    avg_loss:0.017, val_acc:0.986]
Epoch [51/120    avg_loss:0.016, val_acc:0.983]
Epoch [52/120    avg_loss:0.020, val_acc:0.984]
Epoch [53/120    avg_loss:0.014, val_acc:0.987]
Epoch [54/120    avg_loss:0.010, val_acc:0.985]
Epoch [55/120    avg_loss:0.010, val_acc:0.985]
Epoch [56/120    avg_loss:0.012, val_acc:0.987]
Epoch [57/120    avg_loss:0.011, val_acc:0.987]
Epoch [58/120    avg_loss:0.011, val_acc:0.986]
Epoch [59/120    avg_loss:0.013, val_acc:0.989]
Epoch [60/120    avg_loss:0.012, val_acc:0.987]
Epoch [61/120    avg_loss:0.009, val_acc:0.988]
Epoch [62/120    avg_loss:0.011, val_acc:0.987]
Epoch [63/120    avg_loss:0.011, val_acc:0.988]
Epoch [64/120    avg_loss:0.013, val_acc:0.988]
Epoch [65/120    avg_loss:0.008, val_acc:0.989]
Epoch [66/120    avg_loss:0.009, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.988]
Epoch [68/120    avg_loss:0.011, val_acc:0.989]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.012, val_acc:0.987]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.990]
Epoch [74/120    avg_loss:0.009, val_acc:0.990]
Epoch [75/120    avg_loss:0.015, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.010, val_acc:0.991]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.986]
Epoch [80/120    avg_loss:0.012, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.009, val_acc:0.990]
Epoch [87/120    avg_loss:0.009, val_acc:0.990]
Epoch [88/120    avg_loss:0.010, val_acc:0.989]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.013, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.989]
Epoch [92/120    avg_loss:0.009, val_acc:0.989]
Epoch [93/120    avg_loss:0.010, val_acc:0.989]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.014, val_acc:0.989]
Epoch [96/120    avg_loss:0.008, val_acc:0.989]
Epoch [97/120    avg_loss:0.014, val_acc:0.989]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.011, val_acc:0.989]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.009, val_acc:0.989]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.989]
Epoch [113/120    avg_loss:0.013, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.011, val_acc:0.989]
Epoch [117/120    avg_loss:0.010, val_acc:0.989]
Epoch [118/120    avg_loss:0.017, val_acc:0.989]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0     0     3    33     0]
 [    0     0 17930     0    38     0   118     0     4     0]
 [    0     0     0  1962     0     0     0     0    70     4]
 [    0    26     3     0  2927     0     6     0     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4875     0     0     0]
 [    0     9     0     0     0     0     0  1280     0     1]
 [    0    10     0    59    50     0     0     0  3452     0]
 [    0     0     0     0     0    14     0     0     0   905]]

Accuracy:
98.88896922372449

F1 scores:
[       nan 0.99370776 0.99539222 0.96721716 0.9777852  0.99466463
 0.98714184 0.99494753 0.96735323 0.98799127]

Kappa:
0.9853014140061528
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93e5cd7b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.433, val_acc:0.355]
Epoch [2/120    avg_loss:0.849, val_acc:0.793]
Epoch [3/120    avg_loss:0.609, val_acc:0.742]
Epoch [4/120    avg_loss:0.462, val_acc:0.705]
Epoch [5/120    avg_loss:0.404, val_acc:0.758]
Epoch [6/120    avg_loss:0.322, val_acc:0.799]
Epoch [7/120    avg_loss:0.337, val_acc:0.894]
Epoch [8/120    avg_loss:0.278, val_acc:0.877]
Epoch [9/120    avg_loss:0.261, val_acc:0.905]
Epoch [10/120    avg_loss:0.272, val_acc:0.886]
Epoch [11/120    avg_loss:0.197, val_acc:0.874]
Epoch [12/120    avg_loss:0.193, val_acc:0.890]
Epoch [13/120    avg_loss:0.199, val_acc:0.886]
Epoch [14/120    avg_loss:0.196, val_acc:0.932]
Epoch [15/120    avg_loss:0.164, val_acc:0.943]
Epoch [16/120    avg_loss:0.120, val_acc:0.949]
Epoch [17/120    avg_loss:0.138, val_acc:0.903]
Epoch [18/120    avg_loss:0.106, val_acc:0.933]
Epoch [19/120    avg_loss:0.102, val_acc:0.957]
Epoch [20/120    avg_loss:0.072, val_acc:0.922]
Epoch [21/120    avg_loss:0.069, val_acc:0.909]
Epoch [22/120    avg_loss:0.117, val_acc:0.968]
Epoch [23/120    avg_loss:0.085, val_acc:0.965]
Epoch [24/120    avg_loss:0.092, val_acc:0.950]
Epoch [25/120    avg_loss:0.087, val_acc:0.952]
Epoch [26/120    avg_loss:0.073, val_acc:0.957]
Epoch [27/120    avg_loss:0.118, val_acc:0.964]
Epoch [28/120    avg_loss:0.064, val_acc:0.892]
Epoch [29/120    avg_loss:0.069, val_acc:0.943]
Epoch [30/120    avg_loss:0.063, val_acc:0.969]
Epoch [31/120    avg_loss:0.057, val_acc:0.958]
Epoch [32/120    avg_loss:0.045, val_acc:0.968]
Epoch [33/120    avg_loss:0.029, val_acc:0.968]
Epoch [34/120    avg_loss:0.030, val_acc:0.958]
Epoch [35/120    avg_loss:0.054, val_acc:0.955]
Epoch [36/120    avg_loss:0.043, val_acc:0.973]
Epoch [37/120    avg_loss:0.052, val_acc:0.964]
Epoch [38/120    avg_loss:0.036, val_acc:0.921]
Epoch [39/120    avg_loss:0.048, val_acc:0.955]
Epoch [40/120    avg_loss:0.043, val_acc:0.948]
Epoch [41/120    avg_loss:0.042, val_acc:0.937]
Epoch [42/120    avg_loss:0.051, val_acc:0.902]
Epoch [43/120    avg_loss:0.042, val_acc:0.970]
Epoch [44/120    avg_loss:0.032, val_acc:0.969]
Epoch [45/120    avg_loss:0.023, val_acc:0.953]
Epoch [46/120    avg_loss:0.075, val_acc:0.963]
Epoch [47/120    avg_loss:0.083, val_acc:0.963]
Epoch [48/120    avg_loss:0.035, val_acc:0.972]
Epoch [49/120    avg_loss:0.033, val_acc:0.965]
Epoch [50/120    avg_loss:0.024, val_acc:0.972]
Epoch [51/120    avg_loss:0.022, val_acc:0.977]
Epoch [52/120    avg_loss:0.017, val_acc:0.977]
Epoch [53/120    avg_loss:0.016, val_acc:0.975]
Epoch [54/120    avg_loss:0.016, val_acc:0.970]
Epoch [55/120    avg_loss:0.016, val_acc:0.976]
Epoch [56/120    avg_loss:0.014, val_acc:0.977]
Epoch [57/120    avg_loss:0.017, val_acc:0.975]
Epoch [58/120    avg_loss:0.022, val_acc:0.976]
Epoch [59/120    avg_loss:0.013, val_acc:0.976]
Epoch [60/120    avg_loss:0.011, val_acc:0.975]
Epoch [61/120    avg_loss:0.011, val_acc:0.976]
Epoch [62/120    avg_loss:0.012, val_acc:0.976]
Epoch [63/120    avg_loss:0.013, val_acc:0.979]
Epoch [64/120    avg_loss:0.014, val_acc:0.975]
Epoch [65/120    avg_loss:0.010, val_acc:0.977]
Epoch [66/120    avg_loss:0.015, val_acc:0.977]
Epoch [67/120    avg_loss:0.010, val_acc:0.979]
Epoch [68/120    avg_loss:0.018, val_acc:0.978]
Epoch [69/120    avg_loss:0.010, val_acc:0.977]
Epoch [70/120    avg_loss:0.013, val_acc:0.980]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.011, val_acc:0.981]
Epoch [73/120    avg_loss:0.009, val_acc:0.983]
Epoch [74/120    avg_loss:0.014, val_acc:0.978]
Epoch [75/120    avg_loss:0.015, val_acc:0.976]
Epoch [76/120    avg_loss:0.016, val_acc:0.976]
Epoch [77/120    avg_loss:0.012, val_acc:0.977]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.010, val_acc:0.978]
Epoch [80/120    avg_loss:0.016, val_acc:0.977]
Epoch [81/120    avg_loss:0.009, val_acc:0.978]
Epoch [82/120    avg_loss:0.012, val_acc:0.979]
Epoch [83/120    avg_loss:0.008, val_acc:0.978]
Epoch [84/120    avg_loss:0.010, val_acc:0.980]
Epoch [85/120    avg_loss:0.016, val_acc:0.978]
Epoch [86/120    avg_loss:0.008, val_acc:0.979]
Epoch [87/120    avg_loss:0.013, val_acc:0.979]
Epoch [88/120    avg_loss:0.006, val_acc:0.979]
Epoch [89/120    avg_loss:0.010, val_acc:0.979]
Epoch [90/120    avg_loss:0.010, val_acc:0.979]
Epoch [91/120    avg_loss:0.008, val_acc:0.980]
Epoch [92/120    avg_loss:0.009, val_acc:0.980]
Epoch [93/120    avg_loss:0.009, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.010, val_acc:0.980]
Epoch [96/120    avg_loss:0.015, val_acc:0.979]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.016, val_acc:0.979]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.009, val_acc:0.979]
Epoch [101/120    avg_loss:0.014, val_acc:0.979]
Epoch [102/120    avg_loss:0.013, val_acc:0.979]
Epoch [103/120    avg_loss:0.010, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.015, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.979]
Epoch [108/120    avg_loss:0.008, val_acc:0.979]
Epoch [109/120    avg_loss:0.011, val_acc:0.979]
Epoch [110/120    avg_loss:0.008, val_acc:0.979]
Epoch [111/120    avg_loss:0.009, val_acc:0.979]
Epoch [112/120    avg_loss:0.014, val_acc:0.979]
Epoch [113/120    avg_loss:0.011, val_acc:0.979]
Epoch [114/120    avg_loss:0.008, val_acc:0.979]
Epoch [115/120    avg_loss:0.015, val_acc:0.979]
Epoch [116/120    avg_loss:0.007, val_acc:0.979]
Epoch [117/120    avg_loss:0.012, val_acc:0.979]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.012, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     4     0     0     3     1    42     2]
 [    0     0 17972     0    34     0    77     0     7     0]
 [    0     0     0  1921     0     0     0     0   114     1]
 [    0    28     7     0  2919     0     4     0    12     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4866     0     6     0]
 [    0     8     0     0     0     0     0  1282     0     0]
 [    0     0     1    41    42     0     0     0  3487     0]
 [    0     2     0     0     5    25     0     0     0   887]]

Accuracy:
98.85763863784253

F1 scores:
[       nan 0.99299611 0.99634106 0.96001999 0.97756196 0.99051233
 0.99023199 0.99650214 0.96339273 0.9795693 ]

Kappa:
0.9848785508533157
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f662f0c97b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.451, val_acc:0.425]
Epoch [2/120    avg_loss:0.798, val_acc:0.744]
Epoch [3/120    avg_loss:0.579, val_acc:0.765]
Epoch [4/120    avg_loss:0.470, val_acc:0.775]
Epoch [5/120    avg_loss:0.355, val_acc:0.835]
Epoch [6/120    avg_loss:0.306, val_acc:0.882]
Epoch [7/120    avg_loss:0.261, val_acc:0.852]
Epoch [8/120    avg_loss:0.266, val_acc:0.851]
Epoch [9/120    avg_loss:0.254, val_acc:0.918]
Epoch [10/120    avg_loss:0.204, val_acc:0.832]
Epoch [11/120    avg_loss:0.239, val_acc:0.864]
Epoch [12/120    avg_loss:0.196, val_acc:0.929]
Epoch [13/120    avg_loss:0.167, val_acc:0.850]
Epoch [14/120    avg_loss:0.172, val_acc:0.940]
Epoch [15/120    avg_loss:0.123, val_acc:0.954]
Epoch [16/120    avg_loss:0.142, val_acc:0.924]
Epoch [17/120    avg_loss:0.108, val_acc:0.944]
Epoch [18/120    avg_loss:0.097, val_acc:0.949]
Epoch [19/120    avg_loss:0.095, val_acc:0.955]
Epoch [20/120    avg_loss:0.100, val_acc:0.954]
Epoch [21/120    avg_loss:0.074, val_acc:0.918]
Epoch [22/120    avg_loss:0.101, val_acc:0.963]
Epoch [23/120    avg_loss:0.064, val_acc:0.951]
Epoch [24/120    avg_loss:0.080, val_acc:0.965]
Epoch [25/120    avg_loss:0.067, val_acc:0.896]
Epoch [26/120    avg_loss:0.092, val_acc:0.960]
Epoch [27/120    avg_loss:0.075, val_acc:0.953]
Epoch [28/120    avg_loss:0.049, val_acc:0.917]
Epoch [29/120    avg_loss:0.071, val_acc:0.945]
Epoch [30/120    avg_loss:0.063, val_acc:0.969]
Epoch [31/120    avg_loss:0.105, val_acc:0.925]
Epoch [32/120    avg_loss:0.062, val_acc:0.979]
Epoch [33/120    avg_loss:0.037, val_acc:0.960]
Epoch [34/120    avg_loss:0.040, val_acc:0.961]
Epoch [35/120    avg_loss:0.050, val_acc:0.966]
Epoch [36/120    avg_loss:0.029, val_acc:0.978]
Epoch [37/120    avg_loss:0.030, val_acc:0.981]
Epoch [38/120    avg_loss:0.046, val_acc:0.946]
Epoch [39/120    avg_loss:0.053, val_acc:0.975]
Epoch [40/120    avg_loss:0.037, val_acc:0.973]
Epoch [41/120    avg_loss:0.040, val_acc:0.984]
Epoch [42/120    avg_loss:0.028, val_acc:0.977]
Epoch [43/120    avg_loss:0.014, val_acc:0.980]
Epoch [44/120    avg_loss:0.026, val_acc:0.966]
Epoch [45/120    avg_loss:0.033, val_acc:0.984]
Epoch [46/120    avg_loss:0.018, val_acc:0.980]
Epoch [47/120    avg_loss:0.033, val_acc:0.984]
Epoch [48/120    avg_loss:0.022, val_acc:0.984]
Epoch [49/120    avg_loss:0.016, val_acc:0.988]
Epoch [50/120    avg_loss:0.024, val_acc:0.976]
Epoch [51/120    avg_loss:0.014, val_acc:0.983]
Epoch [52/120    avg_loss:0.018, val_acc:0.954]
Epoch [53/120    avg_loss:0.023, val_acc:0.973]
Epoch [54/120    avg_loss:0.024, val_acc:0.986]
Epoch [55/120    avg_loss:0.022, val_acc:0.985]
Epoch [56/120    avg_loss:0.014, val_acc:0.975]
Epoch [57/120    avg_loss:0.028, val_acc:0.979]
Epoch [58/120    avg_loss:0.013, val_acc:0.985]
Epoch [59/120    avg_loss:0.013, val_acc:0.979]
Epoch [60/120    avg_loss:0.011, val_acc:0.983]
Epoch [61/120    avg_loss:0.028, val_acc:0.979]
Epoch [62/120    avg_loss:0.017, val_acc:0.979]
Epoch [63/120    avg_loss:0.010, val_acc:0.984]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.005, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.009, val_acc:0.985]
Epoch [68/120    avg_loss:0.011, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.989]
Epoch [70/120    avg_loss:0.008, val_acc:0.988]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.005, val_acc:0.989]
Epoch [75/120    avg_loss:0.006, val_acc:0.988]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.010, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.006, val_acc:0.988]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.009, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.008, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.007, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.008, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.992]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.015, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     4     0     0     0     4    51     1]
 [    0     0 17989     0    38     0    63     0     0     0]
 [    0     5     0  1930     0     0     0     0    98     3]
 [    0    16     5     0  2943     0     0     0     6     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    10     0     0     0     0     0  1280     0     0]
 [    0     4     0    40    41     0     0     0  3484     2]
 [    0     0     0     0    14    12     0     0     0   893]]

Accuracy:
98.99019111657388

F1 scores:
[       nan 0.99260067 0.99706241 0.96259352 0.97969374 0.99542334
 0.99358387 0.99456099 0.96643551 0.98131868]

Kappa:
0.9866327137957269
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f175529e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.476, val_acc:0.491]
Epoch [2/120    avg_loss:0.829, val_acc:0.524]
Epoch [3/120    avg_loss:0.553, val_acc:0.831]
Epoch [4/120    avg_loss:0.482, val_acc:0.695]
Epoch [5/120    avg_loss:0.436, val_acc:0.739]
Epoch [6/120    avg_loss:0.335, val_acc:0.835]
Epoch [7/120    avg_loss:0.273, val_acc:0.835]
Epoch [8/120    avg_loss:0.255, val_acc:0.850]
Epoch [9/120    avg_loss:0.283, val_acc:0.874]
Epoch [10/120    avg_loss:0.207, val_acc:0.872]
Epoch [11/120    avg_loss:0.201, val_acc:0.917]
Epoch [12/120    avg_loss:0.164, val_acc:0.936]
Epoch [13/120    avg_loss:0.163, val_acc:0.935]
Epoch [14/120    avg_loss:0.136, val_acc:0.910]
Epoch [15/120    avg_loss:0.163, val_acc:0.831]
Epoch [16/120    avg_loss:0.109, val_acc:0.949]
Epoch [17/120    avg_loss:0.114, val_acc:0.957]
Epoch [18/120    avg_loss:0.094, val_acc:0.938]
Epoch [19/120    avg_loss:0.111, val_acc:0.953]
Epoch [20/120    avg_loss:0.090, val_acc:0.947]
Epoch [21/120    avg_loss:0.140, val_acc:0.919]
Epoch [22/120    avg_loss:0.095, val_acc:0.955]
Epoch [23/120    avg_loss:0.071, val_acc:0.965]
Epoch [24/120    avg_loss:0.067, val_acc:0.942]
Epoch [25/120    avg_loss:0.071, val_acc:0.959]
Epoch [26/120    avg_loss:0.063, val_acc:0.940]
Epoch [27/120    avg_loss:0.062, val_acc:0.953]
Epoch [28/120    avg_loss:0.052, val_acc:0.962]
Epoch [29/120    avg_loss:0.052, val_acc:0.935]
Epoch [30/120    avg_loss:0.049, val_acc:0.951]
Epoch [31/120    avg_loss:0.040, val_acc:0.965]
Epoch [32/120    avg_loss:0.055, val_acc:0.979]
Epoch [33/120    avg_loss:0.051, val_acc:0.927]
Epoch [34/120    avg_loss:0.060, val_acc:0.944]
Epoch [35/120    avg_loss:0.061, val_acc:0.968]
Epoch [36/120    avg_loss:0.027, val_acc:0.971]
Epoch [37/120    avg_loss:0.032, val_acc:0.980]
Epoch [38/120    avg_loss:0.019, val_acc:0.982]
Epoch [39/120    avg_loss:0.035, val_acc:0.968]
Epoch [40/120    avg_loss:0.017, val_acc:0.982]
Epoch [41/120    avg_loss:0.025, val_acc:0.970]
Epoch [42/120    avg_loss:0.025, val_acc:0.967]
Epoch [43/120    avg_loss:0.021, val_acc:0.975]
Epoch [44/120    avg_loss:0.055, val_acc:0.974]
Epoch [45/120    avg_loss:0.034, val_acc:0.965]
Epoch [46/120    avg_loss:0.023, val_acc:0.975]
Epoch [47/120    avg_loss:0.088, val_acc:0.941]
Epoch [48/120    avg_loss:0.110, val_acc:0.958]
Epoch [49/120    avg_loss:0.052, val_acc:0.970]
Epoch [50/120    avg_loss:0.065, val_acc:0.949]
Epoch [51/120    avg_loss:0.043, val_acc:0.953]
Epoch [52/120    avg_loss:0.044, val_acc:0.948]
Epoch [53/120    avg_loss:0.029, val_acc:0.974]
Epoch [54/120    avg_loss:0.021, val_acc:0.976]
Epoch [55/120    avg_loss:0.013, val_acc:0.979]
Epoch [56/120    avg_loss:0.015, val_acc:0.978]
Epoch [57/120    avg_loss:0.012, val_acc:0.977]
Epoch [58/120    avg_loss:0.017, val_acc:0.977]
Epoch [59/120    avg_loss:0.017, val_acc:0.979]
Epoch [60/120    avg_loss:0.011, val_acc:0.979]
Epoch [61/120    avg_loss:0.014, val_acc:0.983]
Epoch [62/120    avg_loss:0.014, val_acc:0.982]
Epoch [63/120    avg_loss:0.011, val_acc:0.983]
Epoch [64/120    avg_loss:0.016, val_acc:0.984]
Epoch [65/120    avg_loss:0.015, val_acc:0.983]
Epoch [66/120    avg_loss:0.014, val_acc:0.984]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.008, val_acc:0.984]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.982]
Epoch [72/120    avg_loss:0.014, val_acc:0.983]
Epoch [73/120    avg_loss:0.013, val_acc:0.983]
Epoch [74/120    avg_loss:0.008, val_acc:0.984]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.984]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.008, val_acc:0.983]
Epoch [80/120    avg_loss:0.013, val_acc:0.982]
Epoch [81/120    avg_loss:0.013, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.979]
Epoch [85/120    avg_loss:0.015, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.980]
Epoch [89/120    avg_loss:0.008, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.982]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.010, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.010, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     4     0     0     5     3     8     6]
 [    0     0 17824     0    43     0   219     0     4     0]
 [    0     2     0  1918     0     0     0     0   112     4]
 [    0    19     0     3  2943     0     1     0     3     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     8     0  4859     0     4     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     3     0    53    52     0     0     0  3463     0]
 [    0     0     0     0    10    15     0     0     0   894]]

Accuracy:
98.57084327476925

F1 scores:
[       nan 0.99611258 0.99239999 0.95565521 0.97644326 0.99428571
 0.97531112 0.99806277 0.96664341 0.97918949]

Kappa:
0.9811131051626256
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d12e80780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.482, val_acc:0.342]
Epoch [2/120    avg_loss:0.878, val_acc:0.721]
Epoch [3/120    avg_loss:0.586, val_acc:0.692]
Epoch [4/120    avg_loss:0.472, val_acc:0.762]
Epoch [5/120    avg_loss:0.380, val_acc:0.755]
Epoch [6/120    avg_loss:0.350, val_acc:0.823]
Epoch [7/120    avg_loss:0.322, val_acc:0.792]
Epoch [8/120    avg_loss:0.307, val_acc:0.826]
Epoch [9/120    avg_loss:0.386, val_acc:0.812]
Epoch [10/120    avg_loss:0.271, val_acc:0.869]
Epoch [11/120    avg_loss:0.209, val_acc:0.910]
Epoch [12/120    avg_loss:0.175, val_acc:0.925]
Epoch [13/120    avg_loss:0.171, val_acc:0.922]
Epoch [14/120    avg_loss:0.185, val_acc:0.907]
Epoch [15/120    avg_loss:0.173, val_acc:0.928]
Epoch [16/120    avg_loss:0.118, val_acc:0.912]
Epoch [17/120    avg_loss:0.153, val_acc:0.909]
Epoch [18/120    avg_loss:0.110, val_acc:0.892]
Epoch [19/120    avg_loss:0.124, val_acc:0.958]
Epoch [20/120    avg_loss:0.163, val_acc:0.939]
Epoch [21/120    avg_loss:0.122, val_acc:0.936]
Epoch [22/120    avg_loss:0.087, val_acc:0.959]
Epoch [23/120    avg_loss:0.072, val_acc:0.955]
Epoch [24/120    avg_loss:0.077, val_acc:0.939]
Epoch [25/120    avg_loss:0.083, val_acc:0.943]
Epoch [26/120    avg_loss:0.069, val_acc:0.956]
Epoch [27/120    avg_loss:0.085, val_acc:0.885]
Epoch [28/120    avg_loss:0.082, val_acc:0.965]
Epoch [29/120    avg_loss:0.049, val_acc:0.960]
Epoch [30/120    avg_loss:0.050, val_acc:0.947]
Epoch [31/120    avg_loss:0.074, val_acc:0.953]
Epoch [32/120    avg_loss:0.046, val_acc:0.968]
Epoch [33/120    avg_loss:0.067, val_acc:0.956]
Epoch [34/120    avg_loss:0.081, val_acc:0.911]
Epoch [35/120    avg_loss:0.046, val_acc:0.970]
Epoch [36/120    avg_loss:0.031, val_acc:0.958]
Epoch [37/120    avg_loss:0.034, val_acc:0.967]
Epoch [38/120    avg_loss:0.024, val_acc:0.975]
Epoch [39/120    avg_loss:0.067, val_acc:0.919]
Epoch [40/120    avg_loss:0.064, val_acc:0.968]
Epoch [41/120    avg_loss:0.037, val_acc:0.979]
Epoch [42/120    avg_loss:0.028, val_acc:0.975]
Epoch [43/120    avg_loss:0.023, val_acc:0.985]
Epoch [44/120    avg_loss:0.017, val_acc:0.965]
Epoch [45/120    avg_loss:0.066, val_acc:0.927]
Epoch [46/120    avg_loss:0.057, val_acc:0.972]
Epoch [47/120    avg_loss:0.026, val_acc:0.975]
Epoch [48/120    avg_loss:0.020, val_acc:0.961]
Epoch [49/120    avg_loss:0.059, val_acc:0.954]
Epoch [50/120    avg_loss:0.066, val_acc:0.961]
Epoch [51/120    avg_loss:0.035, val_acc:0.965]
Epoch [52/120    avg_loss:0.022, val_acc:0.976]
Epoch [53/120    avg_loss:0.021, val_acc:0.973]
Epoch [54/120    avg_loss:0.027, val_acc:0.981]
Epoch [55/120    avg_loss:0.013, val_acc:0.979]
Epoch [56/120    avg_loss:0.013, val_acc:0.984]
Epoch [57/120    avg_loss:0.015, val_acc:0.982]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.016, val_acc:0.985]
Epoch [60/120    avg_loss:0.008, val_acc:0.985]
Epoch [61/120    avg_loss:0.009, val_acc:0.984]
Epoch [62/120    avg_loss:0.008, val_acc:0.984]
Epoch [63/120    avg_loss:0.012, val_acc:0.983]
Epoch [64/120    avg_loss:0.010, val_acc:0.986]
Epoch [65/120    avg_loss:0.008, val_acc:0.986]
Epoch [66/120    avg_loss:0.013, val_acc:0.984]
Epoch [67/120    avg_loss:0.006, val_acc:0.985]
Epoch [68/120    avg_loss:0.007, val_acc:0.985]
Epoch [69/120    avg_loss:0.007, val_acc:0.984]
Epoch [70/120    avg_loss:0.006, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.987]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.015, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.008, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.014, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     0     0     0     2     0    29     2]
 [    0     0 17997     0    16     0    64     0    13     0]
 [    0     8     0  1928     0     0     0     0    97     3]
 [    0    25     5     0  2934     0     2     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4862     0    11     0]
 [    0     9     0     0     0     0     0  1279     0     2]
 [    0    12     0    65    48     0     0     0  3446     0]
 [    0     0     0     0     0     8     0     0     0   911]]

Accuracy:
98.95886053069192

F1 scores:
[       nan 0.99324796 0.99714658 0.95706131 0.98291457 0.99694423
 0.99143556 0.99571818 0.96082532 0.99183451]

Kappa:
0.9862150627507335
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5b2b82828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.463, val_acc:0.672]
Epoch [2/120    avg_loss:0.728, val_acc:0.771]
Epoch [3/120    avg_loss:0.549, val_acc:0.729]
Epoch [4/120    avg_loss:0.459, val_acc:0.786]
Epoch [5/120    avg_loss:0.473, val_acc:0.766]
Epoch [6/120    avg_loss:0.332, val_acc:0.845]
Epoch [7/120    avg_loss:0.286, val_acc:0.859]
Epoch [8/120    avg_loss:0.272, val_acc:0.879]
Epoch [9/120    avg_loss:0.242, val_acc:0.887]
Epoch [10/120    avg_loss:0.222, val_acc:0.865]
Epoch [11/120    avg_loss:0.218, val_acc:0.912]
Epoch [12/120    avg_loss:0.198, val_acc:0.909]
Epoch [13/120    avg_loss:0.140, val_acc:0.941]
Epoch [14/120    avg_loss:0.108, val_acc:0.870]
Epoch [15/120    avg_loss:0.195, val_acc:0.818]
Epoch [16/120    avg_loss:0.146, val_acc:0.935]
Epoch [17/120    avg_loss:0.132, val_acc:0.917]
Epoch [18/120    avg_loss:0.146, val_acc:0.927]
Epoch [19/120    avg_loss:0.181, val_acc:0.955]
Epoch [20/120    avg_loss:0.129, val_acc:0.900]
Epoch [21/120    avg_loss:0.113, val_acc:0.910]
Epoch [22/120    avg_loss:0.105, val_acc:0.958]
Epoch [23/120    avg_loss:0.145, val_acc:0.871]
Epoch [24/120    avg_loss:0.093, val_acc:0.939]
Epoch [25/120    avg_loss:0.070, val_acc:0.953]
Epoch [26/120    avg_loss:0.064, val_acc:0.953]
Epoch [27/120    avg_loss:0.078, val_acc:0.951]
Epoch [28/120    avg_loss:0.133, val_acc:0.943]
Epoch [29/120    avg_loss:0.097, val_acc:0.840]
Epoch [30/120    avg_loss:0.103, val_acc:0.942]
Epoch [31/120    avg_loss:0.063, val_acc:0.937]
Epoch [32/120    avg_loss:0.096, val_acc:0.956]
Epoch [33/120    avg_loss:0.053, val_acc:0.967]
Epoch [34/120    avg_loss:0.036, val_acc:0.947]
Epoch [35/120    avg_loss:0.037, val_acc:0.958]
Epoch [36/120    avg_loss:0.070, val_acc:0.919]
Epoch [37/120    avg_loss:0.072, val_acc:0.948]
Epoch [38/120    avg_loss:0.047, val_acc:0.961]
Epoch [39/120    avg_loss:0.040, val_acc:0.975]
Epoch [40/120    avg_loss:0.037, val_acc:0.947]
Epoch [41/120    avg_loss:0.034, val_acc:0.923]
Epoch [42/120    avg_loss:0.054, val_acc:0.970]
Epoch [43/120    avg_loss:0.053, val_acc:0.967]
Epoch [44/120    avg_loss:0.020, val_acc:0.940]
Epoch [45/120    avg_loss:0.045, val_acc:0.965]
Epoch [46/120    avg_loss:0.027, val_acc:0.974]
Epoch [47/120    avg_loss:0.028, val_acc:0.976]
Epoch [48/120    avg_loss:0.021, val_acc:0.973]
Epoch [49/120    avg_loss:0.018, val_acc:0.979]
Epoch [50/120    avg_loss:0.030, val_acc:0.961]
Epoch [51/120    avg_loss:0.034, val_acc:0.977]
Epoch [52/120    avg_loss:0.012, val_acc:0.979]
Epoch [53/120    avg_loss:0.032, val_acc:0.959]
Epoch [54/120    avg_loss:0.022, val_acc:0.975]
Epoch [55/120    avg_loss:0.019, val_acc:0.974]
Epoch [56/120    avg_loss:0.011, val_acc:0.976]
Epoch [57/120    avg_loss:0.024, val_acc:0.975]
Epoch [58/120    avg_loss:0.041, val_acc:0.971]
Epoch [59/120    avg_loss:0.050, val_acc:0.938]
Epoch [60/120    avg_loss:0.042, val_acc:0.962]
Epoch [61/120    avg_loss:0.028, val_acc:0.979]
Epoch [62/120    avg_loss:0.020, val_acc:0.978]
Epoch [63/120    avg_loss:0.021, val_acc:0.979]
Epoch [64/120    avg_loss:0.015, val_acc:0.981]
Epoch [65/120    avg_loss:0.019, val_acc:0.966]
Epoch [66/120    avg_loss:0.018, val_acc:0.977]
Epoch [67/120    avg_loss:0.040, val_acc:0.913]
Epoch [68/120    avg_loss:0.076, val_acc:0.958]
Epoch [69/120    avg_loss:0.034, val_acc:0.972]
Epoch [70/120    avg_loss:0.058, val_acc:0.961]
Epoch [71/120    avg_loss:0.028, val_acc:0.956]
Epoch [72/120    avg_loss:0.037, val_acc:0.968]
Epoch [73/120    avg_loss:0.025, val_acc:0.968]
Epoch [74/120    avg_loss:0.029, val_acc:0.961]
Epoch [75/120    avg_loss:0.025, val_acc:0.973]
Epoch [76/120    avg_loss:0.014, val_acc:0.971]
Epoch [77/120    avg_loss:0.014, val_acc:0.972]
Epoch [78/120    avg_loss:0.010, val_acc:0.975]
Epoch [79/120    avg_loss:0.007, val_acc:0.976]
Epoch [80/120    avg_loss:0.010, val_acc:0.976]
Epoch [81/120    avg_loss:0.009, val_acc:0.978]
Epoch [82/120    avg_loss:0.008, val_acc:0.978]
Epoch [83/120    avg_loss:0.006, val_acc:0.978]
Epoch [84/120    avg_loss:0.006, val_acc:0.979]
Epoch [85/120    avg_loss:0.007, val_acc:0.978]
Epoch [86/120    avg_loss:0.011, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.008, val_acc:0.979]
Epoch [89/120    avg_loss:0.008, val_acc:0.979]
Epoch [90/120    avg_loss:0.008, val_acc:0.980]
Epoch [91/120    avg_loss:0.006, val_acc:0.980]
Epoch [92/120    avg_loss:0.006, val_acc:0.980]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.005, val_acc:0.979]
Epoch [95/120    avg_loss:0.005, val_acc:0.980]
Epoch [96/120    avg_loss:0.008, val_acc:0.980]
Epoch [97/120    avg_loss:0.005, val_acc:0.980]
Epoch [98/120    avg_loss:0.009, val_acc:0.980]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.980]
Epoch [101/120    avg_loss:0.008, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.981]
Epoch [104/120    avg_loss:0.009, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.981]
Epoch [107/120    avg_loss:0.005, val_acc:0.981]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.004, val_acc:0.981]
Epoch [113/120    avg_loss:0.005, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6367     0     7     0     0     2     1    53     2]
 [    0     0 17995     0    26     0    64     0     5     0]
 [    0     2     0  1902     0     0     0     0   126     6]
 [    0    26     2     0  2932     0     1     0     9     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4867     0     9     0]
 [    0     9     0     0     0     0     2  1275     0     4]
 [    0     5     0    39    49     0     0     0  3478     0]
 [    0     0     0     0     7    19     0     0     0   893]]

Accuracy:
98.84558841250332

F1 scores:
[       nan 0.99166732 0.99725678 0.95481928 0.97961911 0.99277292
 0.99184838 0.99376461 0.95931596 0.97809419]

Kappa:
0.9847172355859903
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f847f36e780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.427, val_acc:0.607]
Epoch [2/120    avg_loss:0.794, val_acc:0.600]
Epoch [3/120    avg_loss:0.615, val_acc:0.619]
Epoch [4/120    avg_loss:0.440, val_acc:0.719]
Epoch [5/120    avg_loss:0.351, val_acc:0.875]
Epoch [6/120    avg_loss:0.311, val_acc:0.831]
Epoch [7/120    avg_loss:0.260, val_acc:0.873]
Epoch [8/120    avg_loss:0.270, val_acc:0.832]
Epoch [9/120    avg_loss:0.218, val_acc:0.845]
Epoch [10/120    avg_loss:0.305, val_acc:0.891]
Epoch [11/120    avg_loss:0.216, val_acc:0.881]
Epoch [12/120    avg_loss:0.167, val_acc:0.843]
Epoch [13/120    avg_loss:0.159, val_acc:0.851]
Epoch [14/120    avg_loss:0.172, val_acc:0.935]
Epoch [15/120    avg_loss:0.162, val_acc:0.942]
Epoch [16/120    avg_loss:0.137, val_acc:0.943]
Epoch [17/120    avg_loss:0.121, val_acc:0.924]
Epoch [18/120    avg_loss:0.079, val_acc:0.937]
Epoch [19/120    avg_loss:0.104, val_acc:0.946]
Epoch [20/120    avg_loss:0.093, val_acc:0.926]
Epoch [21/120    avg_loss:0.124, val_acc:0.955]
Epoch [22/120    avg_loss:0.067, val_acc:0.942]
Epoch [23/120    avg_loss:0.070, val_acc:0.961]
Epoch [24/120    avg_loss:0.049, val_acc:0.947]
Epoch [25/120    avg_loss:0.081, val_acc:0.954]
Epoch [26/120    avg_loss:0.048, val_acc:0.943]
Epoch [27/120    avg_loss:0.065, val_acc:0.965]
Epoch [28/120    avg_loss:0.064, val_acc:0.935]
Epoch [29/120    avg_loss:0.121, val_acc:0.938]
Epoch [30/120    avg_loss:0.052, val_acc:0.970]
Epoch [31/120    avg_loss:0.030, val_acc:0.958]
Epoch [32/120    avg_loss:0.044, val_acc:0.969]
Epoch [33/120    avg_loss:0.029, val_acc:0.970]
Epoch [34/120    avg_loss:0.056, val_acc:0.961]
Epoch [35/120    avg_loss:0.050, val_acc:0.951]
Epoch [36/120    avg_loss:0.032, val_acc:0.951]
Epoch [37/120    avg_loss:0.028, val_acc:0.967]
Epoch [38/120    avg_loss:0.029, val_acc:0.939]
Epoch [39/120    avg_loss:0.109, val_acc:0.939]
Epoch [40/120    avg_loss:0.068, val_acc:0.918]
Epoch [41/120    avg_loss:0.052, val_acc:0.942]
Epoch [42/120    avg_loss:0.029, val_acc:0.962]
Epoch [43/120    avg_loss:0.054, val_acc:0.965]
Epoch [44/120    avg_loss:0.026, val_acc:0.974]
Epoch [45/120    avg_loss:0.031, val_acc:0.974]
Epoch [46/120    avg_loss:0.025, val_acc:0.971]
Epoch [47/120    avg_loss:0.017, val_acc:0.972]
Epoch [48/120    avg_loss:0.017, val_acc:0.978]
Epoch [49/120    avg_loss:0.016, val_acc:0.961]
Epoch [50/120    avg_loss:0.010, val_acc:0.978]
Epoch [51/120    avg_loss:0.043, val_acc:0.961]
Epoch [52/120    avg_loss:0.084, val_acc:0.954]
Epoch [53/120    avg_loss:0.074, val_acc:0.956]
Epoch [54/120    avg_loss:0.038, val_acc:0.963]
Epoch [55/120    avg_loss:0.032, val_acc:0.975]
Epoch [56/120    avg_loss:0.021, val_acc:0.940]
Epoch [57/120    avg_loss:0.044, val_acc:0.961]
Epoch [58/120    avg_loss:0.035, val_acc:0.974]
Epoch [59/120    avg_loss:0.018, val_acc:0.977]
Epoch [60/120    avg_loss:0.016, val_acc:0.975]
Epoch [61/120    avg_loss:0.023, val_acc:0.982]
Epoch [62/120    avg_loss:0.017, val_acc:0.974]
Epoch [63/120    avg_loss:0.021, val_acc:0.945]
Epoch [64/120    avg_loss:0.020, val_acc:0.976]
Epoch [65/120    avg_loss:0.023, val_acc:0.933]
Epoch [66/120    avg_loss:0.025, val_acc:0.928]
Epoch [67/120    avg_loss:0.017, val_acc:0.984]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.972]
Epoch [70/120    avg_loss:0.018, val_acc:0.982]
Epoch [71/120    avg_loss:0.062, val_acc:0.958]
Epoch [72/120    avg_loss:0.019, val_acc:0.971]
Epoch [73/120    avg_loss:0.055, val_acc:0.955]
Epoch [74/120    avg_loss:0.027, val_acc:0.970]
Epoch [75/120    avg_loss:0.018, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.977]
Epoch [77/120    avg_loss:0.009, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.970]
Epoch [79/120    avg_loss:0.015, val_acc:0.979]
Epoch [80/120    avg_loss:0.022, val_acc:0.966]
Epoch [81/120    avg_loss:0.021, val_acc:0.972]
Epoch [82/120    avg_loss:0.018, val_acc:0.933]
Epoch [83/120    avg_loss:0.012, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.976]
Epoch [85/120    avg_loss:0.006, val_acc:0.979]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.965]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.985]
Epoch [92/120    avg_loss:0.008, val_acc:0.985]
Epoch [93/120    avg_loss:0.013, val_acc:0.980]
Epoch [94/120    avg_loss:0.054, val_acc:0.962]
Epoch [95/120    avg_loss:0.018, val_acc:0.971]
Epoch [96/120    avg_loss:0.036, val_acc:0.958]
Epoch [97/120    avg_loss:0.045, val_acc:0.956]
Epoch [98/120    avg_loss:0.056, val_acc:0.949]
Epoch [99/120    avg_loss:0.033, val_acc:0.973]
Epoch [100/120    avg_loss:0.029, val_acc:0.978]
Epoch [101/120    avg_loss:0.014, val_acc:0.978]
Epoch [102/120    avg_loss:0.016, val_acc:0.967]
Epoch [103/120    avg_loss:0.012, val_acc:0.972]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.982]
Epoch [107/120    avg_loss:0.005, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.003, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6378     0     4     0     0     0     0    50     0]
 [    0     0 18050     0    31     0     5     0     4     0]
 [    0     0     0  1930     0     0     0     0   104     2]
 [    0    20     0     0  2948     0     1     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4862     0     5     0]
 [    0    11     0     0     0     3     0  1270     1     5]
 [    0     1     0    40    30     0     0     0  3499     1]
 [    0     0     0     0     0    13     2     0     0   904]]

Accuracy:
99.16371436145856

F1 scores:
[       nan 0.99330322 0.99858925 0.96259352 0.98578833 0.99390708
 0.99753796 0.9921875  0.96697527 0.98743856]

Kappa:
0.9889220652343444
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51c173c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.455, val_acc:0.487]
Epoch [2/120    avg_loss:0.856, val_acc:0.598]
Epoch [3/120    avg_loss:0.622, val_acc:0.729]
Epoch [4/120    avg_loss:0.530, val_acc:0.768]
Epoch [5/120    avg_loss:0.462, val_acc:0.728]
Epoch [6/120    avg_loss:0.369, val_acc:0.812]
Epoch [7/120    avg_loss:0.301, val_acc:0.804]
Epoch [8/120    avg_loss:0.302, val_acc:0.777]
Epoch [9/120    avg_loss:0.247, val_acc:0.809]
Epoch [10/120    avg_loss:0.189, val_acc:0.905]
Epoch [11/120    avg_loss:0.181, val_acc:0.828]
Epoch [12/120    avg_loss:0.202, val_acc:0.863]
Epoch [13/120    avg_loss:0.159, val_acc:0.892]
Epoch [14/120    avg_loss:0.205, val_acc:0.872]
Epoch [15/120    avg_loss:0.134, val_acc:0.927]
Epoch [16/120    avg_loss:0.123, val_acc:0.933]
Epoch [17/120    avg_loss:0.116, val_acc:0.909]
Epoch [18/120    avg_loss:0.104, val_acc:0.953]
Epoch [19/120    avg_loss:0.122, val_acc:0.900]
Epoch [20/120    avg_loss:0.093, val_acc:0.950]
Epoch [21/120    avg_loss:0.136, val_acc:0.922]
Epoch [22/120    avg_loss:0.118, val_acc:0.954]
Epoch [23/120    avg_loss:0.063, val_acc:0.954]
Epoch [24/120    avg_loss:0.080, val_acc:0.917]
Epoch [25/120    avg_loss:0.103, val_acc:0.932]
Epoch [26/120    avg_loss:0.062, val_acc:0.952]
Epoch [27/120    avg_loss:0.051, val_acc:0.961]
Epoch [28/120    avg_loss:0.045, val_acc:0.969]
Epoch [29/120    avg_loss:0.044, val_acc:0.936]
Epoch [30/120    avg_loss:0.060, val_acc:0.956]
Epoch [31/120    avg_loss:0.068, val_acc:0.946]
Epoch [32/120    avg_loss:0.046, val_acc:0.968]
Epoch [33/120    avg_loss:0.040, val_acc:0.968]
Epoch [34/120    avg_loss:0.032, val_acc:0.956]
Epoch [35/120    avg_loss:0.033, val_acc:0.960]
Epoch [36/120    avg_loss:0.030, val_acc:0.959]
Epoch [37/120    avg_loss:0.019, val_acc:0.966]
Epoch [38/120    avg_loss:0.028, val_acc:0.973]
Epoch [39/120    avg_loss:0.029, val_acc:0.964]
Epoch [40/120    avg_loss:0.026, val_acc:0.958]
Epoch [41/120    avg_loss:0.061, val_acc:0.950]
Epoch [42/120    avg_loss:0.051, val_acc:0.962]
Epoch [43/120    avg_loss:0.044, val_acc:0.973]
Epoch [44/120    avg_loss:0.035, val_acc:0.971]
Epoch [45/120    avg_loss:0.028, val_acc:0.953]
Epoch [46/120    avg_loss:0.034, val_acc:0.963]
Epoch [47/120    avg_loss:0.022, val_acc:0.955]
Epoch [48/120    avg_loss:0.015, val_acc:0.963]
Epoch [49/120    avg_loss:0.018, val_acc:0.978]
Epoch [50/120    avg_loss:0.012, val_acc:0.977]
Epoch [51/120    avg_loss:0.017, val_acc:0.973]
Epoch [52/120    avg_loss:0.022, val_acc:0.974]
Epoch [53/120    avg_loss:0.018, val_acc:0.974]
Epoch [54/120    avg_loss:0.029, val_acc:0.960]
Epoch [55/120    avg_loss:0.064, val_acc:0.892]
Epoch [56/120    avg_loss:0.028, val_acc:0.963]
Epoch [57/120    avg_loss:0.027, val_acc:0.967]
Epoch [58/120    avg_loss:0.016, val_acc:0.971]
Epoch [59/120    avg_loss:0.023, val_acc:0.964]
Epoch [60/120    avg_loss:0.019, val_acc:0.973]
Epoch [61/120    avg_loss:0.012, val_acc:0.977]
Epoch [62/120    avg_loss:0.009, val_acc:0.974]
Epoch [63/120    avg_loss:0.009, val_acc:0.977]
Epoch [64/120    avg_loss:0.007, val_acc:0.978]
Epoch [65/120    avg_loss:0.008, val_acc:0.978]
Epoch [66/120    avg_loss:0.007, val_acc:0.978]
Epoch [67/120    avg_loss:0.006, val_acc:0.977]
Epoch [68/120    avg_loss:0.007, val_acc:0.977]
Epoch [69/120    avg_loss:0.007, val_acc:0.977]
Epoch [70/120    avg_loss:0.008, val_acc:0.978]
Epoch [71/120    avg_loss:0.008, val_acc:0.978]
Epoch [72/120    avg_loss:0.008, val_acc:0.978]
Epoch [73/120    avg_loss:0.005, val_acc:0.978]
Epoch [74/120    avg_loss:0.008, val_acc:0.978]
Epoch [75/120    avg_loss:0.006, val_acc:0.981]
Epoch [76/120    avg_loss:0.004, val_acc:0.980]
Epoch [77/120    avg_loss:0.007, val_acc:0.980]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.007, val_acc:0.978]
Epoch [80/120    avg_loss:0.009, val_acc:0.978]
Epoch [81/120    avg_loss:0.007, val_acc:0.979]
Epoch [82/120    avg_loss:0.005, val_acc:0.978]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.980]
Epoch [85/120    avg_loss:0.012, val_acc:0.978]
Epoch [86/120    avg_loss:0.005, val_acc:0.979]
Epoch [87/120    avg_loss:0.005, val_acc:0.979]
Epoch [88/120    avg_loss:0.010, val_acc:0.979]
Epoch [89/120    avg_loss:0.006, val_acc:0.979]
Epoch [90/120    avg_loss:0.007, val_acc:0.980]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.007, val_acc:0.980]
Epoch [93/120    avg_loss:0.006, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.978]
Epoch [96/120    avg_loss:0.006, val_acc:0.980]
Epoch [97/120    avg_loss:0.007, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.977]
Epoch [99/120    avg_loss:0.006, val_acc:0.978]
Epoch [100/120    avg_loss:0.007, val_acc:0.979]
Epoch [101/120    avg_loss:0.006, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.978]
Epoch [103/120    avg_loss:0.005, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.977]
Epoch [105/120    avg_loss:0.006, val_acc:0.977]
Epoch [106/120    avg_loss:0.004, val_acc:0.978]
Epoch [107/120    avg_loss:0.009, val_acc:0.977]
Epoch [108/120    avg_loss:0.005, val_acc:0.978]
Epoch [109/120    avg_loss:0.010, val_acc:0.977]
Epoch [110/120    avg_loss:0.006, val_acc:0.978]
Epoch [111/120    avg_loss:0.007, val_acc:0.978]
Epoch [112/120    avg_loss:0.005, val_acc:0.978]
Epoch [113/120    avg_loss:0.004, val_acc:0.978]
Epoch [114/120    avg_loss:0.005, val_acc:0.978]
Epoch [115/120    avg_loss:0.004, val_acc:0.978]
Epoch [116/120    avg_loss:0.006, val_acc:0.978]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.007, val_acc:0.978]
Epoch [119/120    avg_loss:0.007, val_acc:0.978]
Epoch [120/120    avg_loss:0.012, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     3     1     0     0     6    12     0]
 [    0     0 17909     0    38     0   134     0     9     0]
 [    0     2     0  1960     0     0     0     0    72     2]
 [    0    13     0     0  2952     0     1     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     6     0  4838     0    12     0]
 [    0     2     0     0     0     0     0  1286     2     0]
 [    0    10     0    39    54     0     0     0  3468     0]
 [    0     0     0     0     7     9     0     0     0   903]]

Accuracy:
98.88655917865664

F1 scores:
[       nan 0.9961924  0.9943644  0.97077761 0.97910448 0.99656357
 0.98223531 0.99612703 0.96979866 0.99013158]

Kappa:
0.9852706910865648
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fda6b1137f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.463, val_acc:0.602]
Epoch [2/120    avg_loss:0.848, val_acc:0.655]
Epoch [3/120    avg_loss:0.615, val_acc:0.752]
Epoch [4/120    avg_loss:0.454, val_acc:0.749]
Epoch [5/120    avg_loss:0.438, val_acc:0.770]
Epoch [6/120    avg_loss:0.314, val_acc:0.708]
Epoch [7/120    avg_loss:0.326, val_acc:0.798]
Epoch [8/120    avg_loss:0.269, val_acc:0.866]
Epoch [9/120    avg_loss:0.223, val_acc:0.823]
Epoch [10/120    avg_loss:0.218, val_acc:0.837]
Epoch [11/120    avg_loss:0.229, val_acc:0.799]
Epoch [12/120    avg_loss:0.182, val_acc:0.879]
Epoch [13/120    avg_loss:0.201, val_acc:0.917]
Epoch [14/120    avg_loss:0.128, val_acc:0.946]
Epoch [15/120    avg_loss:0.144, val_acc:0.953]
Epoch [16/120    avg_loss:0.115, val_acc:0.914]
Epoch [17/120    avg_loss:0.097, val_acc:0.949]
Epoch [18/120    avg_loss:0.103, val_acc:0.933]
Epoch [19/120    avg_loss:0.075, val_acc:0.961]
Epoch [20/120    avg_loss:0.072, val_acc:0.962]
Epoch [21/120    avg_loss:0.107, val_acc:0.951]
Epoch [22/120    avg_loss:0.084, val_acc:0.942]
Epoch [23/120    avg_loss:0.068, val_acc:0.967]
Epoch [24/120    avg_loss:0.061, val_acc:0.946]
Epoch [25/120    avg_loss:0.061, val_acc:0.953]
Epoch [26/120    avg_loss:0.045, val_acc:0.965]
Epoch [27/120    avg_loss:0.041, val_acc:0.964]
Epoch [28/120    avg_loss:0.050, val_acc:0.974]
Epoch [29/120    avg_loss:0.028, val_acc:0.984]
Epoch [30/120    avg_loss:0.038, val_acc:0.979]
Epoch [31/120    avg_loss:0.038, val_acc:0.965]
Epoch [32/120    avg_loss:0.041, val_acc:0.975]
Epoch [33/120    avg_loss:0.044, val_acc:0.964]
Epoch [34/120    avg_loss:0.034, val_acc:0.975]
Epoch [35/120    avg_loss:0.025, val_acc:0.984]
Epoch [36/120    avg_loss:0.034, val_acc:0.963]
Epoch [37/120    avg_loss:0.056, val_acc:0.942]
Epoch [38/120    avg_loss:0.066, val_acc:0.936]
Epoch [39/120    avg_loss:0.047, val_acc:0.973]
Epoch [40/120    avg_loss:0.037, val_acc:0.967]
Epoch [41/120    avg_loss:0.033, val_acc:0.978]
Epoch [42/120    avg_loss:0.058, val_acc:0.970]
Epoch [43/120    avg_loss:0.038, val_acc:0.933]
Epoch [44/120    avg_loss:0.036, val_acc:0.974]
Epoch [45/120    avg_loss:0.022, val_acc:0.979]
Epoch [46/120    avg_loss:0.032, val_acc:0.967]
Epoch [47/120    avg_loss:0.027, val_acc:0.974]
Epoch [48/120    avg_loss:0.029, val_acc:0.974]
Epoch [49/120    avg_loss:0.016, val_acc:0.982]
Epoch [50/120    avg_loss:0.015, val_acc:0.983]
Epoch [51/120    avg_loss:0.015, val_acc:0.985]
Epoch [52/120    avg_loss:0.011, val_acc:0.984]
Epoch [53/120    avg_loss:0.011, val_acc:0.984]
Epoch [54/120    avg_loss:0.013, val_acc:0.985]
Epoch [55/120    avg_loss:0.012, val_acc:0.986]
Epoch [56/120    avg_loss:0.009, val_acc:0.986]
Epoch [57/120    avg_loss:0.012, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.984]
Epoch [59/120    avg_loss:0.013, val_acc:0.985]
Epoch [60/120    avg_loss:0.008, val_acc:0.985]
Epoch [61/120    avg_loss:0.010, val_acc:0.985]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.010, val_acc:0.984]
Epoch [64/120    avg_loss:0.008, val_acc:0.988]
Epoch [65/120    avg_loss:0.008, val_acc:0.987]
Epoch [66/120    avg_loss:0.017, val_acc:0.986]
Epoch [67/120    avg_loss:0.010, val_acc:0.985]
Epoch [68/120    avg_loss:0.013, val_acc:0.985]
Epoch [69/120    avg_loss:0.013, val_acc:0.987]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.989]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.010, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     0     0     0     2     2    36     1]
 [    0     0 17984     0    16     0    82     0     8     0]
 [    0     1     0  1940     0     0     0     0    92     3]
 [    0    17     3     0  2934     0     6     0     8     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0     6     0     0     0     0     0  1279     0     5]
 [    0     5     0    53    38     0     0     0  3475     0]
 [    0     1     0     0     0    15     0     0     0   903]]

Accuracy:
99.02152170245584

F1 scores:
[       nan 0.994476   0.99692342 0.96301812 0.98456376 0.99428571
 0.99065421 0.9949436  0.96662031 0.98419619]

Kappa:
0.9870478490807818
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f143a697780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.398, val_acc:0.443]
Epoch [2/120    avg_loss:0.769, val_acc:0.582]
Epoch [3/120    avg_loss:0.574, val_acc:0.809]
Epoch [4/120    avg_loss:0.468, val_acc:0.711]
Epoch [5/120    avg_loss:0.444, val_acc:0.884]
Epoch [6/120    avg_loss:0.379, val_acc:0.844]
Epoch [7/120    avg_loss:0.296, val_acc:0.895]
Epoch [8/120    avg_loss:0.260, val_acc:0.829]
Epoch [9/120    avg_loss:0.222, val_acc:0.899]
Epoch [10/120    avg_loss:0.252, val_acc:0.792]
Epoch [11/120    avg_loss:0.210, val_acc:0.938]
Epoch [12/120    avg_loss:0.157, val_acc:0.939]
Epoch [13/120    avg_loss:0.144, val_acc:0.883]
Epoch [14/120    avg_loss:0.157, val_acc:0.910]
Epoch [15/120    avg_loss:0.156, val_acc:0.910]
Epoch [16/120    avg_loss:0.106, val_acc:0.935]
Epoch [17/120    avg_loss:0.099, val_acc:0.934]
Epoch [18/120    avg_loss:0.168, val_acc:0.932]
Epoch [19/120    avg_loss:0.119, val_acc:0.933]
Epoch [20/120    avg_loss:0.092, val_acc:0.942]
Epoch [21/120    avg_loss:0.077, val_acc:0.950]
Epoch [22/120    avg_loss:0.071, val_acc:0.946]
Epoch [23/120    avg_loss:0.081, val_acc:0.919]
Epoch [24/120    avg_loss:0.074, val_acc:0.931]
Epoch [25/120    avg_loss:0.099, val_acc:0.932]
Epoch [26/120    avg_loss:0.066, val_acc:0.949]
Epoch [27/120    avg_loss:0.079, val_acc:0.947]
Epoch [28/120    avg_loss:0.057, val_acc:0.956]
Epoch [29/120    avg_loss:0.078, val_acc:0.965]
Epoch [30/120    avg_loss:0.082, val_acc:0.958]
Epoch [31/120    avg_loss:0.060, val_acc:0.945]
Epoch [32/120    avg_loss:0.055, val_acc:0.976]
Epoch [33/120    avg_loss:0.042, val_acc:0.974]
Epoch [34/120    avg_loss:0.053, val_acc:0.968]
Epoch [35/120    avg_loss:0.083, val_acc:0.967]
Epoch [36/120    avg_loss:0.049, val_acc:0.975]
Epoch [37/120    avg_loss:0.032, val_acc:0.974]
Epoch [38/120    avg_loss:0.056, val_acc:0.970]
Epoch [39/120    avg_loss:0.059, val_acc:0.962]
Epoch [40/120    avg_loss:0.057, val_acc:0.976]
Epoch [41/120    avg_loss:0.027, val_acc:0.977]
Epoch [42/120    avg_loss:0.026, val_acc:0.971]
Epoch [43/120    avg_loss:0.024, val_acc:0.975]
Epoch [44/120    avg_loss:0.024, val_acc:0.980]
Epoch [45/120    avg_loss:0.017, val_acc:0.985]
Epoch [46/120    avg_loss:0.035, val_acc:0.979]
Epoch [47/120    avg_loss:0.032, val_acc:0.965]
Epoch [48/120    avg_loss:0.023, val_acc:0.975]
Epoch [49/120    avg_loss:0.029, val_acc:0.983]
Epoch [50/120    avg_loss:0.020, val_acc:0.965]
Epoch [51/120    avg_loss:0.020, val_acc:0.960]
Epoch [52/120    avg_loss:0.032, val_acc:0.985]
Epoch [53/120    avg_loss:0.012, val_acc:0.988]
Epoch [54/120    avg_loss:0.010, val_acc:0.984]
Epoch [55/120    avg_loss:0.010, val_acc:0.985]
Epoch [56/120    avg_loss:0.021, val_acc:0.965]
Epoch [57/120    avg_loss:0.012, val_acc:0.985]
Epoch [58/120    avg_loss:0.011, val_acc:0.982]
Epoch [59/120    avg_loss:0.017, val_acc:0.984]
Epoch [60/120    avg_loss:0.011, val_acc:0.984]
Epoch [61/120    avg_loss:0.020, val_acc:0.973]
Epoch [62/120    avg_loss:0.011, val_acc:0.980]
Epoch [63/120    avg_loss:0.037, val_acc:0.978]
Epoch [64/120    avg_loss:0.015, val_acc:0.981]
Epoch [65/120    avg_loss:0.010, val_acc:0.984]
Epoch [66/120    avg_loss:0.013, val_acc:0.987]
Epoch [67/120    avg_loss:0.013, val_acc:0.985]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.006, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.004, val_acc:0.983]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.006, val_acc:0.985]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.985]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.004, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.005, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.009, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.004, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     4     0     0     0     0    45     9]
 [    0     0 17883     0    41     0   161     0     5     0]
 [    0     0     0  1961     0     0     0     0    73     2]
 [    0    12     4     0  2946     0     1     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0     6     0     0     0     0     2  1277     0     5]
 [    0     0     1    65    44     0     0     0  3461     0]
 [    0     0     0     0     0    18     0     0     0   901]]

Accuracy:
98.75159665485745

F1 scores:
[       nan 0.99407361 0.99380366 0.96458436 0.98150925 0.99315068
 0.98233929 0.99493572 0.96635488 0.9809472 ]

Kappa:
0.9834924867345012
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f531f671828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.489, val_acc:0.654]
Epoch [2/120    avg_loss:0.933, val_acc:0.485]
Epoch [3/120    avg_loss:0.653, val_acc:0.749]
Epoch [4/120    avg_loss:0.505, val_acc:0.625]
Epoch [5/120    avg_loss:0.410, val_acc:0.769]
Epoch [6/120    avg_loss:0.334, val_acc:0.867]
Epoch [7/120    avg_loss:0.336, val_acc:0.801]
Epoch [8/120    avg_loss:0.247, val_acc:0.905]
Epoch [9/120    avg_loss:0.205, val_acc:0.866]
Epoch [10/120    avg_loss:0.276, val_acc:0.858]
Epoch [11/120    avg_loss:0.196, val_acc:0.868]
Epoch [12/120    avg_loss:0.193, val_acc:0.919]
Epoch [13/120    avg_loss:0.189, val_acc:0.941]
Epoch [14/120    avg_loss:0.168, val_acc:0.952]
Epoch [15/120    avg_loss:0.149, val_acc:0.955]
Epoch [16/120    avg_loss:0.152, val_acc:0.905]
Epoch [17/120    avg_loss:0.128, val_acc:0.923]
Epoch [18/120    avg_loss:0.123, val_acc:0.938]
Epoch [19/120    avg_loss:0.124, val_acc:0.943]
Epoch [20/120    avg_loss:0.090, val_acc:0.968]
Epoch [21/120    avg_loss:0.154, val_acc:0.896]
Epoch [22/120    avg_loss:0.131, val_acc:0.953]
Epoch [23/120    avg_loss:0.077, val_acc:0.938]
Epoch [24/120    avg_loss:0.069, val_acc:0.958]
Epoch [25/120    avg_loss:0.072, val_acc:0.965]
Epoch [26/120    avg_loss:0.098, val_acc:0.959]
Epoch [27/120    avg_loss:0.068, val_acc:0.938]
Epoch [28/120    avg_loss:0.069, val_acc:0.965]
Epoch [29/120    avg_loss:0.093, val_acc:0.972]
Epoch [30/120    avg_loss:0.074, val_acc:0.971]
Epoch [31/120    avg_loss:0.046, val_acc:0.970]
Epoch [32/120    avg_loss:0.066, val_acc:0.959]
Epoch [33/120    avg_loss:0.050, val_acc:0.973]
Epoch [34/120    avg_loss:0.032, val_acc:0.978]
Epoch [35/120    avg_loss:0.031, val_acc:0.975]
Epoch [36/120    avg_loss:0.023, val_acc:0.973]
Epoch [37/120    avg_loss:0.038, val_acc:0.972]
Epoch [38/120    avg_loss:0.052, val_acc:0.956]
Epoch [39/120    avg_loss:0.046, val_acc:0.956]
Epoch [40/120    avg_loss:0.032, val_acc:0.976]
Epoch [41/120    avg_loss:0.032, val_acc:0.977]
Epoch [42/120    avg_loss:0.022, val_acc:0.959]
Epoch [43/120    avg_loss:0.026, val_acc:0.983]
Epoch [44/120    avg_loss:0.029, val_acc:0.979]
Epoch [45/120    avg_loss:0.020, val_acc:0.975]
Epoch [46/120    avg_loss:0.036, val_acc:0.981]
Epoch [47/120    avg_loss:0.060, val_acc:0.976]
Epoch [48/120    avg_loss:0.033, val_acc:0.977]
Epoch [49/120    avg_loss:0.038, val_acc:0.974]
Epoch [50/120    avg_loss:0.047, val_acc:0.957]
Epoch [51/120    avg_loss:0.043, val_acc:0.972]
Epoch [52/120    avg_loss:0.045, val_acc:0.975]
Epoch [53/120    avg_loss:0.018, val_acc:0.975]
Epoch [54/120    avg_loss:0.052, val_acc:0.977]
Epoch [55/120    avg_loss:0.046, val_acc:0.976]
Epoch [56/120    avg_loss:0.018, val_acc:0.979]
Epoch [57/120    avg_loss:0.020, val_acc:0.981]
Epoch [58/120    avg_loss:0.015, val_acc:0.983]
Epoch [59/120    avg_loss:0.017, val_acc:0.983]
Epoch [60/120    avg_loss:0.012, val_acc:0.984]
Epoch [61/120    avg_loss:0.012, val_acc:0.984]
Epoch [62/120    avg_loss:0.012, val_acc:0.984]
Epoch [63/120    avg_loss:0.013, val_acc:0.982]
Epoch [64/120    avg_loss:0.011, val_acc:0.984]
Epoch [65/120    avg_loss:0.009, val_acc:0.984]
Epoch [66/120    avg_loss:0.012, val_acc:0.984]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.984]
Epoch [69/120    avg_loss:0.009, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.984]
Epoch [71/120    avg_loss:0.013, val_acc:0.984]
Epoch [72/120    avg_loss:0.009, val_acc:0.985]
Epoch [73/120    avg_loss:0.013, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.013, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.009, val_acc:0.985]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.009, val_acc:0.986]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.986]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.985]
Epoch [93/120    avg_loss:0.009, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.985]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.011, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.013, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6341     0     7     1     0     0     5    76     2]
 [    0     0 17976     0    18     0    82     0    14     0]
 [    0     0     0  1983     1     0     0     0    44     8]
 [    0    31     4     2  2922     0     0     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4865     0    11     0]
 [    0     0     0     0     0     0     2  1277     0    11]
 [    0     0     0    29    55     0     0     0  3483     4]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
98.87932904345311

F1 scores:
[       nan 0.99047173 0.99672858 0.97708795 0.97676751 0.98901099
 0.99012924 0.99300156 0.96589018 0.96263736]

Kappa:
0.9851695980623867
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f440667d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.488, val_acc:0.531]
Epoch [2/120    avg_loss:0.856, val_acc:0.695]
Epoch [3/120    avg_loss:0.557, val_acc:0.715]
Epoch [4/120    avg_loss:0.447, val_acc:0.786]
Epoch [5/120    avg_loss:0.361, val_acc:0.766]
Epoch [6/120    avg_loss:0.369, val_acc:0.850]
Epoch [7/120    avg_loss:0.278, val_acc:0.886]
Epoch [8/120    avg_loss:0.267, val_acc:0.912]
Epoch [9/120    avg_loss:0.262, val_acc:0.903]
Epoch [10/120    avg_loss:0.233, val_acc:0.912]
Epoch [11/120    avg_loss:0.239, val_acc:0.916]
Epoch [12/120    avg_loss:0.154, val_acc:0.938]
Epoch [13/120    avg_loss:0.132, val_acc:0.910]
Epoch [14/120    avg_loss:0.159, val_acc:0.951]
Epoch [15/120    avg_loss:0.116, val_acc:0.941]
Epoch [16/120    avg_loss:0.114, val_acc:0.866]
Epoch [17/120    avg_loss:0.110, val_acc:0.939]
Epoch [18/120    avg_loss:0.075, val_acc:0.939]
Epoch [19/120    avg_loss:0.061, val_acc:0.932]
Epoch [20/120    avg_loss:0.106, val_acc:0.958]
Epoch [21/120    avg_loss:0.055, val_acc:0.951]
Epoch [22/120    avg_loss:0.059, val_acc:0.952]
Epoch [23/120    avg_loss:0.056, val_acc:0.958]
Epoch [24/120    avg_loss:0.106, val_acc:0.956]
Epoch [25/120    avg_loss:0.056, val_acc:0.962]
Epoch [26/120    avg_loss:0.053, val_acc:0.942]
Epoch [27/120    avg_loss:0.053, val_acc:0.938]
Epoch [28/120    avg_loss:0.048, val_acc:0.961]
Epoch [29/120    avg_loss:0.060, val_acc:0.958]
Epoch [30/120    avg_loss:0.050, val_acc:0.967]
Epoch [31/120    avg_loss:0.031, val_acc:0.973]
Epoch [32/120    avg_loss:0.059, val_acc:0.948]
Epoch [33/120    avg_loss:0.027, val_acc:0.963]
Epoch [34/120    avg_loss:0.028, val_acc:0.963]
Epoch [35/120    avg_loss:0.022, val_acc:0.967]
Epoch [36/120    avg_loss:0.020, val_acc:0.962]
Epoch [37/120    avg_loss:0.028, val_acc:0.963]
Epoch [38/120    avg_loss:0.027, val_acc:0.974]
Epoch [39/120    avg_loss:0.056, val_acc:0.959]
Epoch [40/120    avg_loss:0.055, val_acc:0.965]
Epoch [41/120    avg_loss:0.115, val_acc:0.939]
Epoch [42/120    avg_loss:0.062, val_acc:0.920]
Epoch [43/120    avg_loss:0.041, val_acc:0.965]
Epoch [44/120    avg_loss:0.027, val_acc:0.958]
Epoch [45/120    avg_loss:0.033, val_acc:0.960]
Epoch [46/120    avg_loss:0.018, val_acc:0.963]
Epoch [47/120    avg_loss:0.019, val_acc:0.969]
Epoch [48/120    avg_loss:0.030, val_acc:0.970]
Epoch [49/120    avg_loss:0.014, val_acc:0.970]
Epoch [50/120    avg_loss:0.016, val_acc:0.976]
Epoch [51/120    avg_loss:0.028, val_acc:0.965]
Epoch [52/120    avg_loss:0.032, val_acc:0.956]
Epoch [53/120    avg_loss:0.023, val_acc:0.979]
Epoch [54/120    avg_loss:0.036, val_acc:0.968]
Epoch [55/120    avg_loss:0.038, val_acc:0.977]
Epoch [56/120    avg_loss:0.013, val_acc:0.975]
Epoch [57/120    avg_loss:0.030, val_acc:0.975]
Epoch [58/120    avg_loss:0.092, val_acc:0.971]
Epoch [59/120    avg_loss:0.041, val_acc:0.963]
Epoch [60/120    avg_loss:0.027, val_acc:0.961]
Epoch [61/120    avg_loss:0.016, val_acc:0.969]
Epoch [62/120    avg_loss:0.009, val_acc:0.977]
Epoch [63/120    avg_loss:0.022, val_acc:0.972]
Epoch [64/120    avg_loss:0.022, val_acc:0.968]
Epoch [65/120    avg_loss:0.022, val_acc:0.969]
Epoch [66/120    avg_loss:0.009, val_acc:0.974]
Epoch [67/120    avg_loss:0.007, val_acc:0.975]
Epoch [68/120    avg_loss:0.007, val_acc:0.977]
Epoch [69/120    avg_loss:0.007, val_acc:0.978]
Epoch [70/120    avg_loss:0.008, val_acc:0.979]
Epoch [71/120    avg_loss:0.009, val_acc:0.978]
Epoch [72/120    avg_loss:0.007, val_acc:0.978]
Epoch [73/120    avg_loss:0.009, val_acc:0.982]
Epoch [74/120    avg_loss:0.007, val_acc:0.982]
Epoch [75/120    avg_loss:0.013, val_acc:0.981]
Epoch [76/120    avg_loss:0.006, val_acc:0.980]
Epoch [77/120    avg_loss:0.007, val_acc:0.982]
Epoch [78/120    avg_loss:0.006, val_acc:0.983]
Epoch [79/120    avg_loss:0.005, val_acc:0.983]
Epoch [80/120    avg_loss:0.007, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.983]
Epoch [82/120    avg_loss:0.007, val_acc:0.981]
Epoch [83/120    avg_loss:0.018, val_acc:0.980]
Epoch [84/120    avg_loss:0.006, val_acc:0.978]
Epoch [85/120    avg_loss:0.006, val_acc:0.980]
Epoch [86/120    avg_loss:0.007, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.982]
Epoch [89/120    avg_loss:0.004, val_acc:0.982]
Epoch [90/120    avg_loss:0.006, val_acc:0.981]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.004, val_acc:0.982]
Epoch [94/120    avg_loss:0.003, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.005, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.004, val_acc:0.982]
Epoch [100/120    avg_loss:0.003, val_acc:0.982]
Epoch [101/120    avg_loss:0.004, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.982]
Epoch [103/120    avg_loss:0.004, val_acc:0.981]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.005, val_acc:0.980]
Epoch [106/120    avg_loss:0.004, val_acc:0.980]
Epoch [107/120    avg_loss:0.009, val_acc:0.980]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     1     0     0     4    14     0]
 [    0     1 17991     0    35     0    58     0     5     0]
 [    0     1     0  1991     0     0     0     0    38     6]
 [    0    36     3     0  2923     0     0     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4867     0    11     0]
 [    0     5     0     0     0     0     1  1280     0     4]
 [    0     1     0    49    52     0     0     0  3465     4]
 [    0     0     0     0    11    25     0     0     0   883]]

Accuracy:
99.09623309955896

F1 scores:
[       nan 0.99511211 0.99717326 0.97693817 0.97530864 0.99051233
 0.99286006 0.99456099 0.97413551 0.97246696]

Kappa:
0.9880363310052013
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f693e82e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.520, val_acc:0.688]
Epoch [2/120    avg_loss:0.894, val_acc:0.601]
Epoch [3/120    avg_loss:0.649, val_acc:0.791]
Epoch [4/120    avg_loss:0.473, val_acc:0.770]
Epoch [5/120    avg_loss:0.465, val_acc:0.751]
Epoch [6/120    avg_loss:0.482, val_acc:0.738]
Epoch [7/120    avg_loss:0.378, val_acc:0.817]
Epoch [8/120    avg_loss:0.308, val_acc:0.770]
Epoch [9/120    avg_loss:0.293, val_acc:0.871]
Epoch [10/120    avg_loss:0.266, val_acc:0.863]
Epoch [11/120    avg_loss:0.240, val_acc:0.875]
Epoch [12/120    avg_loss:0.194, val_acc:0.899]
Epoch [13/120    avg_loss:0.163, val_acc:0.902]
Epoch [14/120    avg_loss:0.133, val_acc:0.917]
Epoch [15/120    avg_loss:0.127, val_acc:0.939]
Epoch [16/120    avg_loss:0.204, val_acc:0.897]
Epoch [17/120    avg_loss:0.127, val_acc:0.955]
Epoch [18/120    avg_loss:0.103, val_acc:0.940]
Epoch [19/120    avg_loss:0.084, val_acc:0.943]
Epoch [20/120    avg_loss:0.073, val_acc:0.936]
Epoch [21/120    avg_loss:0.076, val_acc:0.965]
Epoch [22/120    avg_loss:0.079, val_acc:0.958]
Epoch [23/120    avg_loss:0.098, val_acc:0.971]
Epoch [24/120    avg_loss:0.068, val_acc:0.961]
Epoch [25/120    avg_loss:0.050, val_acc:0.968]
Epoch [26/120    avg_loss:0.079, val_acc:0.927]
Epoch [27/120    avg_loss:0.054, val_acc:0.922]
Epoch [28/120    avg_loss:0.046, val_acc:0.975]
Epoch [29/120    avg_loss:0.042, val_acc:0.948]
Epoch [30/120    avg_loss:0.050, val_acc:0.977]
Epoch [31/120    avg_loss:0.029, val_acc:0.980]
Epoch [32/120    avg_loss:0.034, val_acc:0.978]
Epoch [33/120    avg_loss:0.026, val_acc:0.980]
Epoch [34/120    avg_loss:0.033, val_acc:0.977]
Epoch [35/120    avg_loss:0.029, val_acc:0.974]
Epoch [36/120    avg_loss:0.027, val_acc:0.970]
Epoch [37/120    avg_loss:0.029, val_acc:0.975]
Epoch [38/120    avg_loss:0.039, val_acc:0.951]
Epoch [39/120    avg_loss:0.045, val_acc:0.972]
Epoch [40/120    avg_loss:0.032, val_acc:0.979]
Epoch [41/120    avg_loss:0.030, val_acc:0.956]
Epoch [42/120    avg_loss:0.020, val_acc:0.982]
Epoch [43/120    avg_loss:0.025, val_acc:0.957]
Epoch [44/120    avg_loss:0.017, val_acc:0.977]
Epoch [45/120    avg_loss:0.023, val_acc:0.978]
Epoch [46/120    avg_loss:0.026, val_acc:0.953]
Epoch [47/120    avg_loss:0.115, val_acc:0.957]
Epoch [48/120    avg_loss:0.032, val_acc:0.978]
Epoch [49/120    avg_loss:0.030, val_acc:0.978]
Epoch [50/120    avg_loss:0.016, val_acc:0.981]
Epoch [51/120    avg_loss:0.021, val_acc:0.980]
Epoch [52/120    avg_loss:0.023, val_acc:0.974]
Epoch [53/120    avg_loss:0.012, val_acc:0.976]
Epoch [54/120    avg_loss:0.018, val_acc:0.979]
Epoch [55/120    avg_loss:0.011, val_acc:0.983]
Epoch [56/120    avg_loss:0.009, val_acc:0.983]
Epoch [57/120    avg_loss:0.011, val_acc:0.988]
Epoch [58/120    avg_loss:0.012, val_acc:0.989]
Epoch [59/120    avg_loss:0.014, val_acc:0.983]
Epoch [60/120    avg_loss:0.009, val_acc:0.986]
Epoch [61/120    avg_loss:0.010, val_acc:0.983]
Epoch [62/120    avg_loss:0.006, val_acc:0.983]
Epoch [63/120    avg_loss:0.012, val_acc:0.971]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.987]
Epoch [66/120    avg_loss:0.009, val_acc:0.986]
Epoch [67/120    avg_loss:0.007, val_acc:0.986]
Epoch [68/120    avg_loss:0.008, val_acc:0.989]
Epoch [69/120    avg_loss:0.023, val_acc:0.982]
Epoch [70/120    avg_loss:0.014, val_acc:0.973]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.014, val_acc:0.986]
Epoch [73/120    avg_loss:0.012, val_acc:0.988]
Epoch [74/120    avg_loss:0.010, val_acc:0.988]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.020, val_acc:0.956]
Epoch [77/120    avg_loss:0.037, val_acc:0.955]
Epoch [78/120    avg_loss:0.021, val_acc:0.983]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.029, val_acc:0.971]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.005, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.009, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.004, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.008, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.003, val_acc:0.992]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     0     0     0    14     0    42     1]
 [    0     5 17965     0    28     0    84     0     8     0]
 [    0     1     0  1970     0     0     0     0    57     8]
 [    0    42     3     0  2912     0     1     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4866     0    11     0]
 [    0    15     0     0     0     0     4  1267     0     4]
 [    0     6     0    41    46     0     0     0  3474     4]
 [    0     0     0     0    11    15     0     0     0   893]]

Accuracy:
98.87691899838528

F1 scores:
[       nan 0.99021435 0.99642253 0.97356066 0.97570782 0.99428571
 0.98832132 0.99100508 0.96822742 0.97595628]

Kappa:
0.985136120667493
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f34ae666828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.515, val_acc:0.464]
Epoch [2/120    avg_loss:0.892, val_acc:0.602]
Epoch [3/120    avg_loss:0.591, val_acc:0.791]
Epoch [4/120    avg_loss:0.523, val_acc:0.709]
Epoch [5/120    avg_loss:0.496, val_acc:0.720]
Epoch [6/120    avg_loss:0.438, val_acc:0.812]
Epoch [7/120    avg_loss:0.379, val_acc:0.867]
Epoch [8/120    avg_loss:0.262, val_acc:0.821]
Epoch [9/120    avg_loss:0.220, val_acc:0.821]
Epoch [10/120    avg_loss:0.218, val_acc:0.911]
Epoch [11/120    avg_loss:0.157, val_acc:0.853]
Epoch [12/120    avg_loss:0.159, val_acc:0.883]
Epoch [13/120    avg_loss:0.187, val_acc:0.889]
Epoch [14/120    avg_loss:0.148, val_acc:0.892]
Epoch [15/120    avg_loss:0.149, val_acc:0.937]
Epoch [16/120    avg_loss:0.128, val_acc:0.946]
Epoch [17/120    avg_loss:0.090, val_acc:0.936]
Epoch [18/120    avg_loss:0.158, val_acc:0.938]
Epoch [19/120    avg_loss:0.094, val_acc:0.956]
Epoch [20/120    avg_loss:0.064, val_acc:0.959]
Epoch [21/120    avg_loss:0.116, val_acc:0.907]
Epoch [22/120    avg_loss:0.121, val_acc:0.892]
Epoch [23/120    avg_loss:0.093, val_acc:0.955]
Epoch [24/120    avg_loss:0.090, val_acc:0.948]
Epoch [25/120    avg_loss:0.080, val_acc:0.932]
Epoch [26/120    avg_loss:0.086, val_acc:0.956]
Epoch [27/120    avg_loss:0.051, val_acc:0.975]
Epoch [28/120    avg_loss:0.066, val_acc:0.974]
Epoch [29/120    avg_loss:0.051, val_acc:0.977]
Epoch [30/120    avg_loss:0.051, val_acc:0.973]
Epoch [31/120    avg_loss:0.051, val_acc:0.903]
Epoch [32/120    avg_loss:0.035, val_acc:0.963]
Epoch [33/120    avg_loss:0.037, val_acc:0.980]
Epoch [34/120    avg_loss:0.028, val_acc:0.978]
Epoch [35/120    avg_loss:0.023, val_acc:0.982]
Epoch [36/120    avg_loss:0.033, val_acc:0.974]
Epoch [37/120    avg_loss:0.024, val_acc:0.968]
Epoch [38/120    avg_loss:0.025, val_acc:0.973]
Epoch [39/120    avg_loss:0.019, val_acc:0.983]
Epoch [40/120    avg_loss:0.021, val_acc:0.982]
Epoch [41/120    avg_loss:0.018, val_acc:0.973]
Epoch [42/120    avg_loss:0.020, val_acc:0.981]
Epoch [43/120    avg_loss:0.015, val_acc:0.988]
Epoch [44/120    avg_loss:0.014, val_acc:0.984]
Epoch [45/120    avg_loss:0.014, val_acc:0.980]
Epoch [46/120    avg_loss:0.012, val_acc:0.985]
Epoch [47/120    avg_loss:0.023, val_acc:0.984]
Epoch [48/120    avg_loss:0.010, val_acc:0.962]
Epoch [49/120    avg_loss:0.018, val_acc:0.984]
Epoch [50/120    avg_loss:0.015, val_acc:0.985]
Epoch [51/120    avg_loss:0.010, val_acc:0.981]
Epoch [52/120    avg_loss:0.016, val_acc:0.973]
Epoch [53/120    avg_loss:0.013, val_acc:0.971]
Epoch [54/120    avg_loss:0.014, val_acc:0.984]
Epoch [55/120    avg_loss:0.010, val_acc:0.981]
Epoch [56/120    avg_loss:0.015, val_acc:0.981]
Epoch [57/120    avg_loss:0.012, val_acc:0.985]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.008, val_acc:0.985]
Epoch [60/120    avg_loss:0.013, val_acc:0.986]
Epoch [61/120    avg_loss:0.009, val_acc:0.986]
Epoch [62/120    avg_loss:0.006, val_acc:0.987]
Epoch [63/120    avg_loss:0.008, val_acc:0.988]
Epoch [64/120    avg_loss:0.005, val_acc:0.988]
Epoch [65/120    avg_loss:0.007, val_acc:0.987]
Epoch [66/120    avg_loss:0.010, val_acc:0.987]
Epoch [67/120    avg_loss:0.009, val_acc:0.987]
Epoch [68/120    avg_loss:0.010, val_acc:0.987]
Epoch [69/120    avg_loss:0.012, val_acc:0.988]
Epoch [70/120    avg_loss:0.010, val_acc:0.987]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.006, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.988]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.005, val_acc:0.988]
Epoch [84/120    avg_loss:0.017, val_acc:0.987]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.020, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     0     0     0     0     4    54     1]
 [    0     0 18012     0    22     0    53     0     3     0]
 [    0     6     0  1980     0     0     0     0    39    11]
 [    0    25     4     0  2920     0     0     0    16     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     2     0     0  4857     0     5     0]
 [    0     5     0     0     0     0     2  1281     0     2]
 [    0     6     0    32    40     0     0     0  3485     8]
 [    0     1     0     0     1    24     0     0     0   893]]

Accuracy:
99.06731255874485

F1 scores:
[       nan 0.99206102 0.99734219 0.97777778 0.9806885  0.99088838
 0.99223698 0.99495146 0.97169943 0.97012493]

Kappa:
0.9876498468286502
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb37d40b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.446, val_acc:0.393]
Epoch [2/120    avg_loss:0.864, val_acc:0.524]
Epoch [3/120    avg_loss:0.554, val_acc:0.804]
Epoch [4/120    avg_loss:0.463, val_acc:0.755]
Epoch [5/120    avg_loss:0.376, val_acc:0.789]
Epoch [6/120    avg_loss:0.284, val_acc:0.854]
Epoch [7/120    avg_loss:0.264, val_acc:0.792]
Epoch [8/120    avg_loss:0.202, val_acc:0.898]
Epoch [9/120    avg_loss:0.179, val_acc:0.863]
Epoch [10/120    avg_loss:0.205, val_acc:0.910]
Epoch [11/120    avg_loss:0.135, val_acc:0.937]
Epoch [12/120    avg_loss:0.124, val_acc:0.953]
Epoch [13/120    avg_loss:0.114, val_acc:0.948]
Epoch [14/120    avg_loss:0.136, val_acc:0.933]
Epoch [15/120    avg_loss:0.088, val_acc:0.922]
Epoch [16/120    avg_loss:0.083, val_acc:0.946]
Epoch [17/120    avg_loss:0.090, val_acc:0.958]
Epoch [18/120    avg_loss:0.073, val_acc:0.946]
Epoch [19/120    avg_loss:0.071, val_acc:0.935]
Epoch [20/120    avg_loss:0.048, val_acc:0.964]
Epoch [21/120    avg_loss:0.054, val_acc:0.949]
Epoch [22/120    avg_loss:0.078, val_acc:0.904]
Epoch [23/120    avg_loss:0.056, val_acc:0.972]
Epoch [24/120    avg_loss:0.066, val_acc:0.960]
Epoch [25/120    avg_loss:0.048, val_acc:0.936]
Epoch [26/120    avg_loss:0.043, val_acc:0.967]
Epoch [27/120    avg_loss:0.037, val_acc:0.892]
Epoch [28/120    avg_loss:0.046, val_acc:0.966]
Epoch [29/120    avg_loss:0.036, val_acc:0.969]
Epoch [30/120    avg_loss:0.035, val_acc:0.938]
Epoch [31/120    avg_loss:0.039, val_acc:0.966]
Epoch [32/120    avg_loss:0.025, val_acc:0.980]
Epoch [33/120    avg_loss:0.021, val_acc:0.986]
Epoch [34/120    avg_loss:0.018, val_acc:0.983]
Epoch [35/120    avg_loss:0.024, val_acc:0.979]
Epoch [36/120    avg_loss:0.021, val_acc:0.983]
Epoch [37/120    avg_loss:0.020, val_acc:0.957]
Epoch [38/120    avg_loss:0.028, val_acc:0.983]
Epoch [39/120    avg_loss:0.014, val_acc:0.986]
Epoch [40/120    avg_loss:0.021, val_acc:0.987]
Epoch [41/120    avg_loss:0.051, val_acc:0.937]
Epoch [42/120    avg_loss:0.051, val_acc:0.972]
Epoch [43/120    avg_loss:0.018, val_acc:0.976]
Epoch [44/120    avg_loss:0.028, val_acc:0.968]
Epoch [45/120    avg_loss:0.017, val_acc:0.984]
Epoch [46/120    avg_loss:0.041, val_acc:0.972]
Epoch [47/120    avg_loss:0.034, val_acc:0.982]
Epoch [48/120    avg_loss:0.018, val_acc:0.976]
Epoch [49/120    avg_loss:0.018, val_acc:0.984]
Epoch [50/120    avg_loss:0.010, val_acc:0.991]
Epoch [51/120    avg_loss:0.010, val_acc:0.972]
Epoch [52/120    avg_loss:0.016, val_acc:0.985]
Epoch [53/120    avg_loss:0.013, val_acc:0.980]
Epoch [54/120    avg_loss:0.017, val_acc:0.986]
Epoch [55/120    avg_loss:0.014, val_acc:0.972]
Epoch [56/120    avg_loss:0.013, val_acc:0.985]
Epoch [57/120    avg_loss:0.009, val_acc:0.982]
Epoch [58/120    avg_loss:0.024, val_acc:0.979]
Epoch [59/120    avg_loss:0.013, val_acc:0.985]
Epoch [60/120    avg_loss:0.015, val_acc:0.963]
Epoch [61/120    avg_loss:0.018, val_acc:0.976]
Epoch [62/120    avg_loss:0.031, val_acc:0.953]
Epoch [63/120    avg_loss:0.015, val_acc:0.979]
Epoch [64/120    avg_loss:0.007, val_acc:0.986]
Epoch [65/120    avg_loss:0.007, val_acc:0.991]
Epoch [66/120    avg_loss:0.007, val_acc:0.991]
Epoch [67/120    avg_loss:0.009, val_acc:0.990]
Epoch [68/120    avg_loss:0.011, val_acc:0.990]
Epoch [69/120    avg_loss:0.005, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.992]
Epoch [71/120    avg_loss:0.005, val_acc:0.990]
Epoch [72/120    avg_loss:0.006, val_acc:0.989]
Epoch [73/120    avg_loss:0.008, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.992]
Epoch [75/120    avg_loss:0.008, val_acc:0.991]
Epoch [76/120    avg_loss:0.004, val_acc:0.992]
Epoch [77/120    avg_loss:0.004, val_acc:0.992]
Epoch [78/120    avg_loss:0.005, val_acc:0.992]
Epoch [79/120    avg_loss:0.003, val_acc:0.992]
Epoch [80/120    avg_loss:0.008, val_acc:0.992]
Epoch [81/120    avg_loss:0.008, val_acc:0.992]
Epoch [82/120    avg_loss:0.005, val_acc:0.992]
Epoch [83/120    avg_loss:0.005, val_acc:0.992]
Epoch [84/120    avg_loss:0.007, val_acc:0.992]
Epoch [85/120    avg_loss:0.005, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.992]
Epoch [87/120    avg_loss:0.004, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.992]
Epoch [89/120    avg_loss:0.005, val_acc:0.991]
Epoch [90/120    avg_loss:0.004, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.004, val_acc:0.992]
Epoch [93/120    avg_loss:0.004, val_acc:0.992]
Epoch [94/120    avg_loss:0.004, val_acc:0.992]
Epoch [95/120    avg_loss:0.004, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.992]
Epoch [98/120    avg_loss:0.005, val_acc:0.992]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.004, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.992]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.003, val_acc:0.992]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.004, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     1     0     0     0     0    10     0]
 [    0     2 18070     0    14     0     1     0     3     0]
 [    0     5     0  1980     0     0     0     0    46     5]
 [    0    29     2     0  2929     0     1     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4859     0    14     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0    14     0    40    51     0     0     0  3463     3]
 [    0     0     0     0     7    17     0     0     0   895]]

Accuracy:
99.32036729086835

F1 scores:
[       nan 0.99519529 0.99925346 0.97609071 0.98074669 0.99352874
 0.99784372 0.99961225 0.97302613 0.98243688]

Kappa:
0.9909950429634043
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faa63acd7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.539, val_acc:0.398]
Epoch [2/120    avg_loss:0.969, val_acc:0.685]
Epoch [3/120    avg_loss:0.764, val_acc:0.590]
Epoch [4/120    avg_loss:0.524, val_acc:0.732]
Epoch [5/120    avg_loss:0.495, val_acc:0.784]
Epoch [6/120    avg_loss:0.405, val_acc:0.794]
Epoch [7/120    avg_loss:0.362, val_acc:0.783]
Epoch [8/120    avg_loss:0.271, val_acc:0.880]
Epoch [9/120    avg_loss:0.254, val_acc:0.818]
Epoch [10/120    avg_loss:0.232, val_acc:0.909]
Epoch [11/120    avg_loss:0.212, val_acc:0.896]
Epoch [12/120    avg_loss:0.259, val_acc:0.900]
Epoch [13/120    avg_loss:0.208, val_acc:0.926]
Epoch [14/120    avg_loss:0.166, val_acc:0.895]
Epoch [15/120    avg_loss:0.185, val_acc:0.945]
Epoch [16/120    avg_loss:0.161, val_acc:0.890]
Epoch [17/120    avg_loss:0.112, val_acc:0.953]
Epoch [18/120    avg_loss:0.115, val_acc:0.946]
Epoch [19/120    avg_loss:0.102, val_acc:0.956]
Epoch [20/120    avg_loss:0.065, val_acc:0.958]
Epoch [21/120    avg_loss:0.067, val_acc:0.952]
Epoch [22/120    avg_loss:0.062, val_acc:0.951]
Epoch [23/120    avg_loss:0.048, val_acc:0.953]
Epoch [24/120    avg_loss:0.058, val_acc:0.970]
Epoch [25/120    avg_loss:0.078, val_acc:0.956]
Epoch [26/120    avg_loss:0.089, val_acc:0.961]
Epoch [27/120    avg_loss:0.051, val_acc:0.967]
Epoch [28/120    avg_loss:0.041, val_acc:0.965]
Epoch [29/120    avg_loss:0.042, val_acc:0.942]
Epoch [30/120    avg_loss:0.037, val_acc:0.969]
Epoch [31/120    avg_loss:0.027, val_acc:0.958]
Epoch [32/120    avg_loss:0.033, val_acc:0.974]
Epoch [33/120    avg_loss:0.023, val_acc:0.972]
Epoch [34/120    avg_loss:0.026, val_acc:0.975]
Epoch [35/120    avg_loss:0.021, val_acc:0.970]
Epoch [36/120    avg_loss:0.021, val_acc:0.962]
Epoch [37/120    avg_loss:0.022, val_acc:0.967]
Epoch [38/120    avg_loss:0.046, val_acc:0.961]
Epoch [39/120    avg_loss:0.068, val_acc:0.951]
Epoch [40/120    avg_loss:0.041, val_acc:0.959]
Epoch [41/120    avg_loss:0.082, val_acc:0.936]
Epoch [42/120    avg_loss:0.104, val_acc:0.951]
Epoch [43/120    avg_loss:0.055, val_acc:0.956]
Epoch [44/120    avg_loss:0.031, val_acc:0.952]
Epoch [45/120    avg_loss:0.064, val_acc:0.968]
Epoch [46/120    avg_loss:0.039, val_acc:0.962]
Epoch [47/120    avg_loss:0.026, val_acc:0.971]
Epoch [48/120    avg_loss:0.016, val_acc:0.974]
Epoch [49/120    avg_loss:0.020, val_acc:0.975]
Epoch [50/120    avg_loss:0.016, val_acc:0.976]
Epoch [51/120    avg_loss:0.016, val_acc:0.975]
Epoch [52/120    avg_loss:0.011, val_acc:0.976]
Epoch [53/120    avg_loss:0.011, val_acc:0.977]
Epoch [54/120    avg_loss:0.017, val_acc:0.977]
Epoch [55/120    avg_loss:0.017, val_acc:0.978]
Epoch [56/120    avg_loss:0.018, val_acc:0.979]
Epoch [57/120    avg_loss:0.011, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.975]
Epoch [59/120    avg_loss:0.009, val_acc:0.978]
Epoch [60/120    avg_loss:0.016, val_acc:0.975]
Epoch [61/120    avg_loss:0.015, val_acc:0.976]
Epoch [62/120    avg_loss:0.014, val_acc:0.979]
Epoch [63/120    avg_loss:0.010, val_acc:0.975]
Epoch [64/120    avg_loss:0.011, val_acc:0.976]
Epoch [65/120    avg_loss:0.010, val_acc:0.978]
Epoch [66/120    avg_loss:0.011, val_acc:0.975]
Epoch [67/120    avg_loss:0.009, val_acc:0.979]
Epoch [68/120    avg_loss:0.010, val_acc:0.975]
Epoch [69/120    avg_loss:0.011, val_acc:0.977]
Epoch [70/120    avg_loss:0.018, val_acc:0.978]
Epoch [71/120    avg_loss:0.007, val_acc:0.978]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.009, val_acc:0.978]
Epoch [74/120    avg_loss:0.008, val_acc:0.977]
Epoch [75/120    avg_loss:0.009, val_acc:0.976]
Epoch [76/120    avg_loss:0.010, val_acc:0.976]
Epoch [77/120    avg_loss:0.012, val_acc:0.977]
Epoch [78/120    avg_loss:0.010, val_acc:0.976]
Epoch [79/120    avg_loss:0.009, val_acc:0.977]
Epoch [80/120    avg_loss:0.015, val_acc:0.978]
Epoch [81/120    avg_loss:0.008, val_acc:0.977]
Epoch [82/120    avg_loss:0.009, val_acc:0.977]
Epoch [83/120    avg_loss:0.009, val_acc:0.977]
Epoch [84/120    avg_loss:0.011, val_acc:0.977]
Epoch [85/120    avg_loss:0.017, val_acc:0.977]
Epoch [86/120    avg_loss:0.009, val_acc:0.977]
Epoch [87/120    avg_loss:0.012, val_acc:0.977]
Epoch [88/120    avg_loss:0.011, val_acc:0.977]
Epoch [89/120    avg_loss:0.012, val_acc:0.977]
Epoch [90/120    avg_loss:0.012, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.977]
Epoch [92/120    avg_loss:0.010, val_acc:0.977]
Epoch [93/120    avg_loss:0.009, val_acc:0.977]
Epoch [94/120    avg_loss:0.009, val_acc:0.977]
Epoch [95/120    avg_loss:0.009, val_acc:0.977]
Epoch [96/120    avg_loss:0.009, val_acc:0.977]
Epoch [97/120    avg_loss:0.017, val_acc:0.977]
Epoch [98/120    avg_loss:0.013, val_acc:0.977]
Epoch [99/120    avg_loss:0.009, val_acc:0.977]
Epoch [100/120    avg_loss:0.012, val_acc:0.977]
Epoch [101/120    avg_loss:0.016, val_acc:0.977]
Epoch [102/120    avg_loss:0.008, val_acc:0.977]
Epoch [103/120    avg_loss:0.013, val_acc:0.977]
Epoch [104/120    avg_loss:0.011, val_acc:0.977]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.010, val_acc:0.977]
Epoch [107/120    avg_loss:0.014, val_acc:0.977]
Epoch [108/120    avg_loss:0.009, val_acc:0.977]
Epoch [109/120    avg_loss:0.010, val_acc:0.977]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.010, val_acc:0.977]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.008, val_acc:0.977]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.007, val_acc:0.977]
Epoch [116/120    avg_loss:0.009, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.011, val_acc:0.977]
Epoch [119/120    avg_loss:0.012, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6353     0     1     1     0    20    19    38     0]
 [    0     2 17977     0    11     0    98     0     2     0]
 [    0     0     0  1986     0     0     0     0    41     9]
 [    0    35     6     3  2920     0     0     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4867     0     9     0]
 [    0     6     0     0     0     0     3  1277     0     4]
 [    0     1     0    40    51     0     0     0  3477     2]
 [    0     0     0     0    14    18     0     0     0   887]]

Accuracy:
98.92993998987781

F1 scores:
[       nan 0.99041235 0.99670113 0.97640118 0.97838834 0.99315068
 0.98662072 0.98762568 0.97313182 0.97419001]

Kappa:
0.9858377147713286
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9abddc37f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.488, val_acc:0.692]
Epoch [2/120    avg_loss:0.844, val_acc:0.635]
Epoch [3/120    avg_loss:0.587, val_acc:0.757]
Epoch [4/120    avg_loss:0.477, val_acc:0.770]
Epoch [5/120    avg_loss:0.356, val_acc:0.783]
Epoch [6/120    avg_loss:0.396, val_acc:0.787]
Epoch [7/120    avg_loss:0.284, val_acc:0.874]
Epoch [8/120    avg_loss:0.236, val_acc:0.768]
Epoch [9/120    avg_loss:0.175, val_acc:0.933]
Epoch [10/120    avg_loss:0.267, val_acc:0.847]
Epoch [11/120    avg_loss:0.211, val_acc:0.885]
Epoch [12/120    avg_loss:0.139, val_acc:0.919]
Epoch [13/120    avg_loss:0.124, val_acc:0.931]
Epoch [14/120    avg_loss:0.159, val_acc:0.907]
Epoch [15/120    avg_loss:0.148, val_acc:0.907]
Epoch [16/120    avg_loss:0.134, val_acc:0.934]
Epoch [17/120    avg_loss:0.098, val_acc:0.887]
Epoch [18/120    avg_loss:0.171, val_acc:0.926]
Epoch [19/120    avg_loss:0.147, val_acc:0.927]
Epoch [20/120    avg_loss:0.102, val_acc:0.943]
Epoch [21/120    avg_loss:0.113, val_acc:0.948]
Epoch [22/120    avg_loss:0.077, val_acc:0.963]
Epoch [23/120    avg_loss:0.081, val_acc:0.920]
Epoch [24/120    avg_loss:0.045, val_acc:0.967]
Epoch [25/120    avg_loss:0.048, val_acc:0.957]
Epoch [26/120    avg_loss:0.054, val_acc:0.953]
Epoch [27/120    avg_loss:0.059, val_acc:0.973]
Epoch [28/120    avg_loss:0.050, val_acc:0.971]
Epoch [29/120    avg_loss:0.055, val_acc:0.961]
Epoch [30/120    avg_loss:0.043, val_acc:0.974]
Epoch [31/120    avg_loss:0.042, val_acc:0.974]
Epoch [32/120    avg_loss:0.033, val_acc:0.981]
Epoch [33/120    avg_loss:0.031, val_acc:0.975]
Epoch [34/120    avg_loss:0.027, val_acc:0.982]
Epoch [35/120    avg_loss:0.027, val_acc:0.931]
Epoch [36/120    avg_loss:0.038, val_acc:0.967]
Epoch [37/120    avg_loss:0.031, val_acc:0.955]
Epoch [38/120    avg_loss:0.032, val_acc:0.966]
Epoch [39/120    avg_loss:0.020, val_acc:0.983]
Epoch [40/120    avg_loss:0.028, val_acc:0.976]
Epoch [41/120    avg_loss:0.024, val_acc:0.982]
Epoch [42/120    avg_loss:0.036, val_acc:0.985]
Epoch [43/120    avg_loss:0.026, val_acc:0.966]
Epoch [44/120    avg_loss:0.015, val_acc:0.985]
Epoch [45/120    avg_loss:0.021, val_acc:0.977]
Epoch [46/120    avg_loss:0.017, val_acc:0.986]
Epoch [47/120    avg_loss:0.009, val_acc:0.983]
Epoch [48/120    avg_loss:0.012, val_acc:0.989]
Epoch [49/120    avg_loss:0.010, val_acc:0.983]
Epoch [50/120    avg_loss:0.011, val_acc:0.986]
Epoch [51/120    avg_loss:0.022, val_acc:0.983]
Epoch [52/120    avg_loss:0.018, val_acc:0.978]
Epoch [53/120    avg_loss:0.110, val_acc:0.944]
Epoch [54/120    avg_loss:0.072, val_acc:0.936]
Epoch [55/120    avg_loss:0.029, val_acc:0.980]
Epoch [56/120    avg_loss:0.017, val_acc:0.983]
Epoch [57/120    avg_loss:0.053, val_acc:0.962]
Epoch [58/120    avg_loss:0.038, val_acc:0.977]
Epoch [59/120    avg_loss:0.031, val_acc:0.981]
Epoch [60/120    avg_loss:0.027, val_acc:0.986]
Epoch [61/120    avg_loss:0.022, val_acc:0.968]
Epoch [62/120    avg_loss:0.020, val_acc:0.987]
Epoch [63/120    avg_loss:0.015, val_acc:0.991]
Epoch [64/120    avg_loss:0.008, val_acc:0.992]
Epoch [65/120    avg_loss:0.008, val_acc:0.992]
Epoch [66/120    avg_loss:0.007, val_acc:0.992]
Epoch [67/120    avg_loss:0.008, val_acc:0.992]
Epoch [68/120    avg_loss:0.022, val_acc:0.992]
Epoch [69/120    avg_loss:0.011, val_acc:0.991]
Epoch [70/120    avg_loss:0.008, val_acc:0.993]
Epoch [71/120    avg_loss:0.011, val_acc:0.992]
Epoch [72/120    avg_loss:0.007, val_acc:0.991]
Epoch [73/120    avg_loss:0.008, val_acc:0.990]
Epoch [74/120    avg_loss:0.006, val_acc:0.992]
Epoch [75/120    avg_loss:0.007, val_acc:0.993]
Epoch [76/120    avg_loss:0.008, val_acc:0.993]
Epoch [77/120    avg_loss:0.007, val_acc:0.993]
Epoch [78/120    avg_loss:0.006, val_acc:0.992]
Epoch [79/120    avg_loss:0.007, val_acc:0.991]
Epoch [80/120    avg_loss:0.005, val_acc:0.992]
Epoch [81/120    avg_loss:0.009, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.991]
Epoch [84/120    avg_loss:0.011, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.993]
Epoch [87/120    avg_loss:0.009, val_acc:0.991]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.005, val_acc:0.992]
Epoch [90/120    avg_loss:0.009, val_acc:0.992]
Epoch [91/120    avg_loss:0.006, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.008, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.008, val_acc:0.991]
Epoch [97/120    avg_loss:0.011, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.991]
Epoch [99/120    avg_loss:0.008, val_acc:0.991]
Epoch [100/120    avg_loss:0.009, val_acc:0.991]
Epoch [101/120    avg_loss:0.009, val_acc:0.992]
Epoch [102/120    avg_loss:0.007, val_acc:0.992]
Epoch [103/120    avg_loss:0.009, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.009, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.992]
Epoch [109/120    avg_loss:0.008, val_acc:0.992]
Epoch [110/120    avg_loss:0.009, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.007, val_acc:0.992]
Epoch [115/120    avg_loss:0.010, val_acc:0.992]
Epoch [116/120    avg_loss:0.008, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6367     0     2     0     0     6    20    37     0]
 [    0     2 18053     0    15     0    14     0     6     0]
 [    0     0     0  2012     0     0     0     0    18     6]
 [    0    37    10     0  2911     0     1     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4865     0    12     0]
 [    0     1     0     0     0     0     4  1279     0     6]
 [    0    21     0    37    52     0     0     0  3458     3]
 [    0     0     1     0    14    24     0     0     0   880]]

Accuracy:
99.12515364037307

F1 scores:
[       nan 0.99020218 0.99867235 0.98434442 0.97619048 0.99088838
 0.99610975 0.98802626 0.97203092 0.97023153]

Kappa:
0.9884115126870742
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d66acc828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.521, val_acc:0.469]
Epoch [2/120    avg_loss:0.920, val_acc:0.514]
Epoch [3/120    avg_loss:0.643, val_acc:0.652]
Epoch [4/120    avg_loss:0.501, val_acc:0.760]
Epoch [5/120    avg_loss:0.492, val_acc:0.795]
Epoch [6/120    avg_loss:0.337, val_acc:0.811]
Epoch [7/120    avg_loss:0.275, val_acc:0.857]
Epoch [8/120    avg_loss:0.272, val_acc:0.810]
Epoch [9/120    avg_loss:0.208, val_acc:0.879]
Epoch [10/120    avg_loss:0.232, val_acc:0.888]
Epoch [11/120    avg_loss:0.164, val_acc:0.918]
Epoch [12/120    avg_loss:0.133, val_acc:0.914]
Epoch [13/120    avg_loss:0.164, val_acc:0.828]
Epoch [14/120    avg_loss:0.125, val_acc:0.946]
Epoch [15/120    avg_loss:0.139, val_acc:0.915]
Epoch [16/120    avg_loss:0.148, val_acc:0.919]
Epoch [17/120    avg_loss:0.108, val_acc:0.932]
Epoch [18/120    avg_loss:0.075, val_acc:0.954]
Epoch [19/120    avg_loss:0.141, val_acc:0.954]
Epoch [20/120    avg_loss:0.073, val_acc:0.954]
Epoch [21/120    avg_loss:0.088, val_acc:0.958]
Epoch [22/120    avg_loss:0.077, val_acc:0.952]
Epoch [23/120    avg_loss:0.072, val_acc:0.960]
Epoch [24/120    avg_loss:0.070, val_acc:0.970]
Epoch [25/120    avg_loss:0.113, val_acc:0.949]
Epoch [26/120    avg_loss:0.056, val_acc:0.977]
Epoch [27/120    avg_loss:0.038, val_acc:0.962]
Epoch [28/120    avg_loss:0.038, val_acc:0.978]
Epoch [29/120    avg_loss:0.078, val_acc:0.899]
Epoch [30/120    avg_loss:0.078, val_acc:0.978]
Epoch [31/120    avg_loss:0.047, val_acc:0.973]
Epoch [32/120    avg_loss:0.031, val_acc:0.980]
Epoch [33/120    avg_loss:0.032, val_acc:0.978]
Epoch [34/120    avg_loss:0.029, val_acc:0.932]
Epoch [35/120    avg_loss:0.028, val_acc:0.979]
Epoch [36/120    avg_loss:0.025, val_acc:0.981]
Epoch [37/120    avg_loss:0.026, val_acc:0.976]
Epoch [38/120    avg_loss:0.029, val_acc:0.978]
Epoch [39/120    avg_loss:0.049, val_acc:0.984]
Epoch [40/120    avg_loss:0.038, val_acc:0.953]
Epoch [41/120    avg_loss:0.026, val_acc:0.982]
Epoch [42/120    avg_loss:0.022, val_acc:0.963]
Epoch [43/120    avg_loss:0.027, val_acc:0.968]
Epoch [44/120    avg_loss:0.028, val_acc:0.986]
Epoch [45/120    avg_loss:0.014, val_acc:0.963]
Epoch [46/120    avg_loss:0.022, val_acc:0.981]
Epoch [47/120    avg_loss:0.015, val_acc:0.981]
Epoch [48/120    avg_loss:0.018, val_acc:0.983]
Epoch [49/120    avg_loss:0.021, val_acc:0.979]
Epoch [50/120    avg_loss:0.011, val_acc:0.982]
Epoch [51/120    avg_loss:0.014, val_acc:0.979]
Epoch [52/120    avg_loss:0.016, val_acc:0.983]
Epoch [53/120    avg_loss:0.014, val_acc:0.988]
Epoch [54/120    avg_loss:0.025, val_acc:0.988]
Epoch [55/120    avg_loss:0.010, val_acc:0.986]
Epoch [56/120    avg_loss:0.010, val_acc:0.987]
Epoch [57/120    avg_loss:0.007, val_acc:0.989]
Epoch [58/120    avg_loss:0.008, val_acc:0.988]
Epoch [59/120    avg_loss:0.005, val_acc:0.988]
Epoch [60/120    avg_loss:0.015, val_acc:0.983]
Epoch [61/120    avg_loss:0.009, val_acc:0.985]
Epoch [62/120    avg_loss:0.010, val_acc:0.988]
Epoch [63/120    avg_loss:0.017, val_acc:0.985]
Epoch [64/120    avg_loss:0.010, val_acc:0.991]
Epoch [65/120    avg_loss:0.005, val_acc:0.988]
Epoch [66/120    avg_loss:0.004, val_acc:0.992]
Epoch [67/120    avg_loss:0.008, val_acc:0.972]
Epoch [68/120    avg_loss:0.012, val_acc:0.988]
Epoch [69/120    avg_loss:0.013, val_acc:0.982]
Epoch [70/120    avg_loss:0.013, val_acc:0.983]
Epoch [71/120    avg_loss:0.012, val_acc:0.980]
Epoch [72/120    avg_loss:0.006, val_acc:0.989]
Epoch [73/120    avg_loss:0.004, val_acc:0.991]
Epoch [74/120    avg_loss:0.004, val_acc:0.991]
Epoch [75/120    avg_loss:0.003, val_acc:0.989]
Epoch [76/120    avg_loss:0.004, val_acc:0.992]
Epoch [77/120    avg_loss:0.007, val_acc:0.967]
Epoch [78/120    avg_loss:0.009, val_acc:0.989]
Epoch [79/120    avg_loss:0.005, val_acc:0.988]
Epoch [80/120    avg_loss:0.006, val_acc:0.989]
Epoch [81/120    avg_loss:0.004, val_acc:0.992]
Epoch [82/120    avg_loss:0.004, val_acc:0.989]
Epoch [83/120    avg_loss:0.003, val_acc:0.990]
Epoch [84/120    avg_loss:0.002, val_acc:0.992]
Epoch [85/120    avg_loss:0.003, val_acc:0.986]
Epoch [86/120    avg_loss:0.016, val_acc:0.991]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.003, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.003, val_acc:0.988]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.003, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.990]
Epoch [96/120    avg_loss:0.003, val_acc:0.983]
Epoch [97/120    avg_loss:0.023, val_acc:0.983]
Epoch [98/120    avg_loss:0.023, val_acc:0.987]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.990]
Epoch [119/120    avg_loss:0.003, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     0     0     0     6     0    39     0]
 [    0     2 18037     0    15     0    35     0     1     0]
 [    0     0     0  1989     0     0     0     0    39     8]
 [    0    38     7     3  2914     0     1     0     9     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     1     0     0  4846     0    18     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    11     0     4    48     0     0     0  3504     4]
 [    0     0     0     0     5    29     0     0     0   885]]

Accuracy:
99.18540476706914

F1 scores:
[       nan 0.99254079 0.99798047 0.98636251 0.97883776 0.98901099
 0.99242269 0.9992242  0.97590865 0.97359736]

Kappa:
0.98920946900891
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f875bc09898>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.533, val_acc:0.575]
Epoch [2/120    avg_loss:0.886, val_acc:0.488]
Epoch [3/120    avg_loss:0.609, val_acc:0.759]
Epoch [4/120    avg_loss:0.521, val_acc:0.626]
Epoch [5/120    avg_loss:0.451, val_acc:0.741]
Epoch [6/120    avg_loss:0.336, val_acc:0.749]
Epoch [7/120    avg_loss:0.296, val_acc:0.896]
Epoch [8/120    avg_loss:0.289, val_acc:0.719]
Epoch [9/120    avg_loss:0.305, val_acc:0.861]
Epoch [10/120    avg_loss:0.210, val_acc:0.869]
Epoch [11/120    avg_loss:0.207, val_acc:0.910]
Epoch [12/120    avg_loss:0.209, val_acc:0.918]
Epoch [13/120    avg_loss:0.142, val_acc:0.948]
Epoch [14/120    avg_loss:0.097, val_acc:0.938]
Epoch [15/120    avg_loss:0.114, val_acc:0.932]
Epoch [16/120    avg_loss:0.108, val_acc:0.938]
Epoch [17/120    avg_loss:0.136, val_acc:0.919]
Epoch [18/120    avg_loss:0.130, val_acc:0.911]
Epoch [19/120    avg_loss:0.094, val_acc:0.942]
Epoch [20/120    avg_loss:0.092, val_acc:0.933]
Epoch [21/120    avg_loss:0.082, val_acc:0.935]
Epoch [22/120    avg_loss:0.094, val_acc:0.961]
Epoch [23/120    avg_loss:0.093, val_acc:0.954]
Epoch [24/120    avg_loss:0.059, val_acc:0.948]
Epoch [25/120    avg_loss:0.053, val_acc:0.964]
Epoch [26/120    avg_loss:0.095, val_acc:0.943]
Epoch [27/120    avg_loss:0.078, val_acc:0.921]
Epoch [28/120    avg_loss:0.108, val_acc:0.965]
Epoch [29/120    avg_loss:0.047, val_acc:0.959]
Epoch [30/120    avg_loss:0.059, val_acc:0.962]
Epoch [31/120    avg_loss:0.062, val_acc:0.920]
Epoch [32/120    avg_loss:0.040, val_acc:0.967]
Epoch [33/120    avg_loss:0.066, val_acc:0.959]
Epoch [34/120    avg_loss:0.035, val_acc:0.968]
Epoch [35/120    avg_loss:0.041, val_acc:0.958]
Epoch [36/120    avg_loss:0.037, val_acc:0.840]
Epoch [37/120    avg_loss:0.054, val_acc:0.975]
Epoch [38/120    avg_loss:0.039, val_acc:0.970]
Epoch [39/120    avg_loss:0.023, val_acc:0.976]
Epoch [40/120    avg_loss:0.029, val_acc:0.981]
Epoch [41/120    avg_loss:0.027, val_acc:0.979]
Epoch [42/120    avg_loss:0.031, val_acc:0.967]
Epoch [43/120    avg_loss:0.017, val_acc:0.980]
Epoch [44/120    avg_loss:0.019, val_acc:0.979]
Epoch [45/120    avg_loss:0.021, val_acc:0.973]
Epoch [46/120    avg_loss:0.093, val_acc:0.973]
Epoch [47/120    avg_loss:0.024, val_acc:0.977]
Epoch [48/120    avg_loss:0.025, val_acc:0.979]
Epoch [49/120    avg_loss:0.026, val_acc:0.971]
Epoch [50/120    avg_loss:0.019, val_acc:0.980]
Epoch [51/120    avg_loss:0.020, val_acc:0.976]
Epoch [52/120    avg_loss:0.012, val_acc:0.976]
Epoch [53/120    avg_loss:0.014, val_acc:0.984]
Epoch [54/120    avg_loss:0.009, val_acc:0.985]
Epoch [55/120    avg_loss:0.010, val_acc:0.986]
Epoch [56/120    avg_loss:0.012, val_acc:0.986]
Epoch [57/120    avg_loss:0.018, val_acc:0.984]
Epoch [58/120    avg_loss:0.013, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.008, val_acc:0.983]
Epoch [61/120    avg_loss:0.020, val_acc:0.985]
Epoch [62/120    avg_loss:0.017, val_acc:0.983]
Epoch [63/120    avg_loss:0.012, val_acc:0.986]
Epoch [64/120    avg_loss:0.014, val_acc:0.986]
Epoch [65/120    avg_loss:0.008, val_acc:0.987]
Epoch [66/120    avg_loss:0.010, val_acc:0.962]
Epoch [67/120    avg_loss:0.054, val_acc:0.945]
Epoch [68/120    avg_loss:0.028, val_acc:0.979]
Epoch [69/120    avg_loss:0.024, val_acc:0.977]
Epoch [70/120    avg_loss:0.030, val_acc:0.950]
Epoch [71/120    avg_loss:0.028, val_acc:0.979]
Epoch [72/120    avg_loss:0.009, val_acc:0.984]
Epoch [73/120    avg_loss:0.030, val_acc:0.986]
Epoch [74/120    avg_loss:0.014, val_acc:0.986]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.004, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.987]
Epoch [78/120    avg_loss:0.014, val_acc:0.976]
Epoch [79/120    avg_loss:0.014, val_acc:0.982]
Epoch [80/120    avg_loss:0.007, val_acc:0.990]
Epoch [81/120    avg_loss:0.018, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.982]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.017, val_acc:0.973]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.003, val_acc:0.991]
Epoch [90/120    avg_loss:0.003, val_acc:0.989]
Epoch [91/120    avg_loss:0.006, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.013, val_acc:0.966]
Epoch [95/120    avg_loss:0.019, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.028, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.983]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.003, val_acc:0.992]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.003, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.003, val_acc:0.992]
Epoch [117/120    avg_loss:0.002, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.003, val_acc:0.992]
Epoch [120/120    avg_loss:0.002, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0     9     0    23     0]
 [    0     1 18040     0     6     0    41     0     2     0]
 [    0     0     0  1984     0     0     0     0    47     5]
 [    0    24     8     0  2926     0     0     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4867     0    11     0]
 [    0     1     0     0     0     0     2  1286     0     1]
 [    0     6     0    20    40     0     0     0  3502     3]
 [    0     0     0     0     1     8     0     0     0   910]]

Accuracy:
99.34205769647893

F1 scores:
[       nan 0.99502488 0.99839504 0.98217822 0.9843566  0.99694423
 0.99356946 0.9984472  0.97698424 0.9896683 ]

Kappa:
0.9912853503686296
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa626493748>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.473, val_acc:0.727]
Epoch [2/120    avg_loss:0.822, val_acc:0.760]
Epoch [3/120    avg_loss:0.538, val_acc:0.859]
Epoch [4/120    avg_loss:0.426, val_acc:0.851]
Epoch [5/120    avg_loss:0.369, val_acc:0.883]
Epoch [6/120    avg_loss:0.353, val_acc:0.924]
Epoch [7/120    avg_loss:0.245, val_acc:0.932]
Epoch [8/120    avg_loss:0.238, val_acc:0.919]
Epoch [9/120    avg_loss:0.235, val_acc:0.931]
Epoch [10/120    avg_loss:0.194, val_acc:0.874]
Epoch [11/120    avg_loss:0.156, val_acc:0.928]
Epoch [12/120    avg_loss:0.160, val_acc:0.953]
Epoch [13/120    avg_loss:0.132, val_acc:0.924]
Epoch [14/120    avg_loss:0.176, val_acc:0.965]
Epoch [15/120    avg_loss:0.138, val_acc:0.965]
Epoch [16/120    avg_loss:0.114, val_acc:0.922]
Epoch [17/120    avg_loss:0.089, val_acc:0.968]
Epoch [18/120    avg_loss:0.085, val_acc:0.957]
Epoch [19/120    avg_loss:0.114, val_acc:0.908]
Epoch [20/120    avg_loss:0.080, val_acc:0.927]
Epoch [21/120    avg_loss:0.093, val_acc:0.964]
Epoch [22/120    avg_loss:0.059, val_acc:0.937]
Epoch [23/120    avg_loss:0.056, val_acc:0.967]
Epoch [24/120    avg_loss:0.053, val_acc:0.977]
Epoch [25/120    avg_loss:0.043, val_acc:0.973]
Epoch [26/120    avg_loss:0.033, val_acc:0.981]
Epoch [27/120    avg_loss:0.088, val_acc:0.956]
Epoch [28/120    avg_loss:0.065, val_acc:0.964]
Epoch [29/120    avg_loss:0.056, val_acc:0.973]
Epoch [30/120    avg_loss:0.033, val_acc:0.979]
Epoch [31/120    avg_loss:0.024, val_acc:0.978]
Epoch [32/120    avg_loss:0.023, val_acc:0.981]
Epoch [33/120    avg_loss:0.031, val_acc:0.979]
Epoch [34/120    avg_loss:0.029, val_acc:0.975]
Epoch [35/120    avg_loss:0.032, val_acc:0.958]
Epoch [36/120    avg_loss:0.049, val_acc:0.982]
Epoch [37/120    avg_loss:0.037, val_acc:0.938]
Epoch [38/120    avg_loss:0.033, val_acc:0.972]
Epoch [39/120    avg_loss:0.025, val_acc:0.981]
Epoch [40/120    avg_loss:0.023, val_acc:0.977]
Epoch [41/120    avg_loss:0.015, val_acc:0.985]
Epoch [42/120    avg_loss:0.021, val_acc:0.977]
Epoch [43/120    avg_loss:0.020, val_acc:0.981]
Epoch [44/120    avg_loss:0.012, val_acc:0.987]
Epoch [45/120    avg_loss:0.014, val_acc:0.988]
Epoch [46/120    avg_loss:0.016, val_acc:0.948]
Epoch [47/120    avg_loss:0.025, val_acc:0.981]
Epoch [48/120    avg_loss:0.015, val_acc:0.986]
Epoch [49/120    avg_loss:0.013, val_acc:0.983]
Epoch [50/120    avg_loss:0.023, val_acc:0.979]
Epoch [51/120    avg_loss:0.081, val_acc:0.924]
Epoch [52/120    avg_loss:0.194, val_acc:0.974]
Epoch [53/120    avg_loss:0.059, val_acc:0.909]
Epoch [54/120    avg_loss:0.042, val_acc:0.968]
Epoch [55/120    avg_loss:0.039, val_acc:0.960]
Epoch [56/120    avg_loss:0.037, val_acc:0.983]
Epoch [57/120    avg_loss:0.023, val_acc:0.984]
Epoch [58/120    avg_loss:0.016, val_acc:0.983]
Epoch [59/120    avg_loss:0.013, val_acc:0.987]
Epoch [60/120    avg_loss:0.011, val_acc:0.989]
Epoch [61/120    avg_loss:0.014, val_acc:0.988]
Epoch [62/120    avg_loss:0.014, val_acc:0.986]
Epoch [63/120    avg_loss:0.011, val_acc:0.988]
Epoch [64/120    avg_loss:0.009, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.988]
Epoch [66/120    avg_loss:0.011, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.989]
Epoch [68/120    avg_loss:0.012, val_acc:0.988]
Epoch [69/120    avg_loss:0.009, val_acc:0.989]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.014, val_acc:0.984]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.008, val_acc:0.987]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.988]
Epoch [80/120    avg_loss:0.007, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.987]
Epoch [83/120    avg_loss:0.010, val_acc:0.987]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.014, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.010, val_acc:0.987]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.987]
Epoch [109/120    avg_loss:0.009, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.013, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.013, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6336     0     0     1     0     3    40    50     2]
 [    0     3 18051     0    11     0    14     0    11     0]
 [    0     0     0  1996     0     0     0     0    35     5]
 [    0    26     3     1  2926     0     2     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4874     0     3     0]
 [    0     0     0     0     0     3     0  1280     0     7]
 [    0     1     0     4    57     0     0     0  3506     3]
 [    0     0     0     0     4    23     0     0     0   892]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99015471 0.99883798 0.98860822 0.98007034 0.99013657
 0.9976461  0.98084291 0.97537905 0.97539639]

Kappa:
0.9895635006649627
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc769d91748>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.484, val_acc:0.333]
Epoch [2/120    avg_loss:0.953, val_acc:0.676]
Epoch [3/120    avg_loss:0.684, val_acc:0.780]
Epoch [4/120    avg_loss:0.518, val_acc:0.751]
Epoch [5/120    avg_loss:0.452, val_acc:0.650]
Epoch [6/120    avg_loss:0.378, val_acc:0.754]
Epoch [7/120    avg_loss:0.342, val_acc:0.728]
Epoch [8/120    avg_loss:0.265, val_acc:0.852]
Epoch [9/120    avg_loss:0.257, val_acc:0.894]
Epoch [10/120    avg_loss:0.210, val_acc:0.917]
Epoch [11/120    avg_loss:0.189, val_acc:0.874]
Epoch [12/120    avg_loss:0.249, val_acc:0.934]
Epoch [13/120    avg_loss:0.182, val_acc:0.906]
Epoch [14/120    avg_loss:0.157, val_acc:0.954]
Epoch [15/120    avg_loss:0.113, val_acc:0.920]
Epoch [16/120    avg_loss:0.106, val_acc:0.948]
Epoch [17/120    avg_loss:0.089, val_acc:0.937]
Epoch [18/120    avg_loss:0.068, val_acc:0.946]
Epoch [19/120    avg_loss:0.101, val_acc:0.967]
Epoch [20/120    avg_loss:0.127, val_acc:0.944]
Epoch [21/120    avg_loss:0.099, val_acc:0.921]
Epoch [22/120    avg_loss:0.080, val_acc:0.973]
Epoch [23/120    avg_loss:0.059, val_acc:0.959]
Epoch [24/120    avg_loss:0.039, val_acc:0.974]
Epoch [25/120    avg_loss:0.062, val_acc:0.930]
Epoch [26/120    avg_loss:0.123, val_acc:0.931]
Epoch [27/120    avg_loss:0.108, val_acc:0.950]
Epoch [28/120    avg_loss:0.049, val_acc:0.965]
Epoch [29/120    avg_loss:0.043, val_acc:0.973]
Epoch [30/120    avg_loss:0.031, val_acc:0.963]
Epoch [31/120    avg_loss:0.037, val_acc:0.983]
Epoch [32/120    avg_loss:0.027, val_acc:0.988]
Epoch [33/120    avg_loss:0.034, val_acc:0.970]
Epoch [34/120    avg_loss:0.026, val_acc:0.980]
Epoch [35/120    avg_loss:0.030, val_acc:0.965]
Epoch [36/120    avg_loss:0.036, val_acc:0.897]
Epoch [37/120    avg_loss:0.030, val_acc:0.987]
Epoch [38/120    avg_loss:0.026, val_acc:0.982]
Epoch [39/120    avg_loss:0.042, val_acc:0.984]
Epoch [40/120    avg_loss:0.032, val_acc:0.986]
Epoch [41/120    avg_loss:0.064, val_acc:0.969]
Epoch [42/120    avg_loss:0.047, val_acc:0.974]
Epoch [43/120    avg_loss:0.030, val_acc:0.973]
Epoch [44/120    avg_loss:0.029, val_acc:0.962]
Epoch [45/120    avg_loss:0.030, val_acc:0.986]
Epoch [46/120    avg_loss:0.017, val_acc:0.988]
Epoch [47/120    avg_loss:0.015, val_acc:0.988]
Epoch [48/120    avg_loss:0.015, val_acc:0.988]
Epoch [49/120    avg_loss:0.010, val_acc:0.988]
Epoch [50/120    avg_loss:0.012, val_acc:0.988]
Epoch [51/120    avg_loss:0.010, val_acc:0.988]
Epoch [52/120    avg_loss:0.014, val_acc:0.986]
Epoch [53/120    avg_loss:0.010, val_acc:0.988]
Epoch [54/120    avg_loss:0.016, val_acc:0.986]
Epoch [55/120    avg_loss:0.010, val_acc:0.986]
Epoch [56/120    avg_loss:0.009, val_acc:0.986]
Epoch [57/120    avg_loss:0.010, val_acc:0.988]
Epoch [58/120    avg_loss:0.008, val_acc:0.986]
Epoch [59/120    avg_loss:0.009, val_acc:0.987]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.987]
Epoch [62/120    avg_loss:0.007, val_acc:0.987]
Epoch [63/120    avg_loss:0.011, val_acc:0.986]
Epoch [64/120    avg_loss:0.013, val_acc:0.986]
Epoch [65/120    avg_loss:0.010, val_acc:0.986]
Epoch [66/120    avg_loss:0.008, val_acc:0.987]
Epoch [67/120    avg_loss:0.007, val_acc:0.987]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.987]
Epoch [72/120    avg_loss:0.009, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.987]
Epoch [74/120    avg_loss:0.007, val_acc:0.987]
Epoch [75/120    avg_loss:0.006, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.011, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.011, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.009, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     2     0     0     0    16     0]
 [    0     4 17978     0    14     0    88     0     6     0]
 [    0     0     0  2009     3     0     0     0    18     6]
 [    0    43    19     0  2894     0     0     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4872     0     6     0]
 [    0     0     0     0     0     0     6  1283     0     1]
 [    0     0     0    23    53     0     0     0  3482    13]
 [    0     0     0     0    14    43     0     0     0   862]]

Accuracy:
99.05044224326996

F1 scores:
[       nan 0.9949585  0.99636988 0.98770895 0.97244624 0.98379193
 0.98984153 0.99727944 0.97877723 0.95724597]

Kappa:
0.9874287606512869
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f21eacdc898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.593, val_acc:0.663]
Epoch [2/120    avg_loss:0.977, val_acc:0.707]
Epoch [3/120    avg_loss:0.755, val_acc:0.670]
Epoch [4/120    avg_loss:0.541, val_acc:0.753]
Epoch [5/120    avg_loss:0.431, val_acc:0.801]
Epoch [6/120    avg_loss:0.348, val_acc:0.878]
Epoch [7/120    avg_loss:0.314, val_acc:0.833]
Epoch [8/120    avg_loss:0.276, val_acc:0.889]
Epoch [9/120    avg_loss:0.188, val_acc:0.885]
Epoch [10/120    avg_loss:0.171, val_acc:0.934]
Epoch [11/120    avg_loss:0.152, val_acc:0.919]
Epoch [12/120    avg_loss:0.112, val_acc:0.915]
Epoch [13/120    avg_loss:0.152, val_acc:0.899]
Epoch [14/120    avg_loss:0.206, val_acc:0.932]
Epoch [15/120    avg_loss:0.106, val_acc:0.928]
Epoch [16/120    avg_loss:0.109, val_acc:0.967]
Epoch [17/120    avg_loss:0.109, val_acc:0.934]
Epoch [18/120    avg_loss:0.131, val_acc:0.949]
Epoch [19/120    avg_loss:0.076, val_acc:0.946]
Epoch [20/120    avg_loss:0.077, val_acc:0.954]
Epoch [21/120    avg_loss:0.082, val_acc:0.966]
Epoch [22/120    avg_loss:0.072, val_acc:0.966]
Epoch [23/120    avg_loss:0.057, val_acc:0.959]
Epoch [24/120    avg_loss:0.065, val_acc:0.949]
Epoch [25/120    avg_loss:0.045, val_acc:0.976]
Epoch [26/120    avg_loss:0.055, val_acc:0.965]
Epoch [27/120    avg_loss:0.046, val_acc:0.974]
Epoch [28/120    avg_loss:0.040, val_acc:0.975]
Epoch [29/120    avg_loss:0.049, val_acc:0.967]
Epoch [30/120    avg_loss:0.039, val_acc:0.967]
Epoch [31/120    avg_loss:0.030, val_acc:0.968]
Epoch [32/120    avg_loss:0.043, val_acc:0.961]
Epoch [33/120    avg_loss:0.032, val_acc:0.966]
Epoch [34/120    avg_loss:0.076, val_acc:0.952]
Epoch [35/120    avg_loss:0.052, val_acc:0.959]
Epoch [36/120    avg_loss:0.028, val_acc:0.980]
Epoch [37/120    avg_loss:0.021, val_acc:0.981]
Epoch [38/120    avg_loss:0.021, val_acc:0.969]
Epoch [39/120    avg_loss:0.023, val_acc:0.975]
Epoch [40/120    avg_loss:0.019, val_acc:0.973]
Epoch [41/120    avg_loss:0.025, val_acc:0.975]
Epoch [42/120    avg_loss:0.018, val_acc:0.961]
Epoch [43/120    avg_loss:0.092, val_acc:0.946]
Epoch [44/120    avg_loss:0.044, val_acc:0.970]
Epoch [45/120    avg_loss:0.031, val_acc:0.972]
Epoch [46/120    avg_loss:0.027, val_acc:0.974]
Epoch [47/120    avg_loss:0.016, val_acc:0.981]
Epoch [48/120    avg_loss:0.012, val_acc:0.975]
Epoch [49/120    avg_loss:0.010, val_acc:0.981]
Epoch [50/120    avg_loss:0.011, val_acc:0.985]
Epoch [51/120    avg_loss:0.016, val_acc:0.982]
Epoch [52/120    avg_loss:0.016, val_acc:0.980]
Epoch [53/120    avg_loss:0.012, val_acc:0.979]
Epoch [54/120    avg_loss:0.008, val_acc:0.983]
Epoch [55/120    avg_loss:0.008, val_acc:0.983]
Epoch [56/120    avg_loss:0.031, val_acc:0.979]
Epoch [57/120    avg_loss:0.029, val_acc:0.978]
Epoch [58/120    avg_loss:0.032, val_acc:0.973]
Epoch [59/120    avg_loss:0.035, val_acc:0.950]
Epoch [60/120    avg_loss:0.038, val_acc:0.958]
Epoch [61/120    avg_loss:0.067, val_acc:0.972]
Epoch [62/120    avg_loss:0.014, val_acc:0.982]
Epoch [63/120    avg_loss:0.008, val_acc:0.985]
Epoch [64/120    avg_loss:0.016, val_acc:0.983]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.982]
Epoch [67/120    avg_loss:0.009, val_acc:0.983]
Epoch [68/120    avg_loss:0.009, val_acc:0.985]
Epoch [69/120    avg_loss:0.010, val_acc:0.985]
Epoch [70/120    avg_loss:0.005, val_acc:0.983]
Epoch [71/120    avg_loss:0.005, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.982]
Epoch [73/120    avg_loss:0.007, val_acc:0.980]
Epoch [74/120    avg_loss:0.006, val_acc:0.982]
Epoch [75/120    avg_loss:0.005, val_acc:0.986]
Epoch [76/120    avg_loss:0.008, val_acc:0.984]
Epoch [77/120    avg_loss:0.030, val_acc:0.978]
Epoch [78/120    avg_loss:0.015, val_acc:0.974]
Epoch [79/120    avg_loss:0.029, val_acc:0.985]
Epoch [80/120    avg_loss:0.015, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.979]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.005, val_acc:0.984]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.016, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.982]
Epoch [92/120    avg_loss:0.003, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.003, val_acc:0.985]
Epoch [96/120    avg_loss:0.003, val_acc:0.985]
Epoch [97/120    avg_loss:0.002, val_acc:0.982]
Epoch [98/120    avg_loss:0.004, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.984]
Epoch [102/120    avg_loss:0.002, val_acc:0.984]
Epoch [103/120    avg_loss:0.003, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.003, val_acc:0.983]
Epoch [106/120    avg_loss:0.003, val_acc:0.984]
Epoch [107/120    avg_loss:0.003, val_acc:0.983]
Epoch [108/120    avg_loss:0.003, val_acc:0.984]
Epoch [109/120    avg_loss:0.002, val_acc:0.984]
Epoch [110/120    avg_loss:0.003, val_acc:0.983]
Epoch [111/120    avg_loss:0.003, val_acc:0.983]
Epoch [112/120    avg_loss:0.003, val_acc:0.984]
Epoch [113/120    avg_loss:0.004, val_acc:0.984]
Epoch [114/120    avg_loss:0.003, val_acc:0.983]
Epoch [115/120    avg_loss:0.002, val_acc:0.983]
Epoch [116/120    avg_loss:0.002, val_acc:0.983]
Epoch [117/120    avg_loss:0.003, val_acc:0.983]
Epoch [118/120    avg_loss:0.003, val_acc:0.983]
Epoch [119/120    avg_loss:0.003, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     1     1     0     0     3     0     0]
 [    0     3 18058     0    12     0    17     0     0     0]
 [    0     3     0  1998     2     0     0     0    22    11]
 [    0    38     7     2  2913     0     0     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     3     0     0  4867     0     0     0]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     0     0     1    49     0     0     0  3512     9]
 [    0     0     0     0    13    24     0     0     0   882]]

Accuracy:
99.41676909358205

F1 scores:
[       nan 0.99620243 0.99870033 0.98886414 0.97718886 0.99088838
 0.9970296  0.99845081 0.9869327  0.96869852]

Kappa:
0.9922724516931516
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f08fe55b748>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.571, val_acc:0.644]
Epoch [2/120    avg_loss:0.933, val_acc:0.696]
Epoch [3/120    avg_loss:0.668, val_acc:0.729]
Epoch [4/120    avg_loss:0.503, val_acc:0.769]
Epoch [5/120    avg_loss:0.421, val_acc:0.847]
Epoch [6/120    avg_loss:0.325, val_acc:0.861]
Epoch [7/120    avg_loss:0.292, val_acc:0.859]
Epoch [8/120    avg_loss:0.227, val_acc:0.905]
Epoch [9/120    avg_loss:0.192, val_acc:0.891]
Epoch [10/120    avg_loss:0.153, val_acc:0.938]
Epoch [11/120    avg_loss:0.152, val_acc:0.926]
Epoch [12/120    avg_loss:0.117, val_acc:0.931]
Epoch [13/120    avg_loss:0.110, val_acc:0.892]
Epoch [14/120    avg_loss:0.175, val_acc:0.940]
Epoch [15/120    avg_loss:0.095, val_acc:0.957]
Epoch [16/120    avg_loss:0.101, val_acc:0.954]
Epoch [17/120    avg_loss:0.093, val_acc:0.954]
Epoch [18/120    avg_loss:0.101, val_acc:0.918]
Epoch [19/120    avg_loss:0.148, val_acc:0.854]
Epoch [20/120    avg_loss:0.152, val_acc:0.951]
Epoch [21/120    avg_loss:0.097, val_acc:0.898]
Epoch [22/120    avg_loss:0.100, val_acc:0.948]
Epoch [23/120    avg_loss:0.071, val_acc:0.936]
Epoch [24/120    avg_loss:0.090, val_acc:0.916]
Epoch [25/120    avg_loss:0.069, val_acc:0.969]
Epoch [26/120    avg_loss:0.051, val_acc:0.956]
Epoch [27/120    avg_loss:0.038, val_acc:0.932]
Epoch [28/120    avg_loss:0.054, val_acc:0.967]
Epoch [29/120    avg_loss:0.038, val_acc:0.963]
Epoch [30/120    avg_loss:0.030, val_acc:0.977]
Epoch [31/120    avg_loss:0.038, val_acc:0.978]
Epoch [32/120    avg_loss:0.025, val_acc:0.982]
Epoch [33/120    avg_loss:0.021, val_acc:0.968]
Epoch [34/120    avg_loss:0.019, val_acc:0.983]
Epoch [35/120    avg_loss:0.021, val_acc:0.980]
Epoch [36/120    avg_loss:0.020, val_acc:0.981]
Epoch [37/120    avg_loss:0.032, val_acc:0.971]
Epoch [38/120    avg_loss:0.025, val_acc:0.985]
Epoch [39/120    avg_loss:0.024, val_acc:0.916]
Epoch [40/120    avg_loss:0.027, val_acc:0.981]
Epoch [41/120    avg_loss:0.026, val_acc:0.978]
Epoch [42/120    avg_loss:0.023, val_acc:0.986]
Epoch [43/120    avg_loss:0.022, val_acc:0.977]
Epoch [44/120    avg_loss:0.020, val_acc:0.984]
Epoch [45/120    avg_loss:0.016, val_acc:0.986]
Epoch [46/120    avg_loss:0.014, val_acc:0.985]
Epoch [47/120    avg_loss:0.013, val_acc:0.980]
Epoch [48/120    avg_loss:0.018, val_acc:0.963]
Epoch [49/120    avg_loss:0.023, val_acc:0.983]
Epoch [50/120    avg_loss:0.012, val_acc:0.984]
Epoch [51/120    avg_loss:0.014, val_acc:0.976]
Epoch [52/120    avg_loss:0.020, val_acc:0.986]
Epoch [53/120    avg_loss:0.018, val_acc:0.925]
Epoch [54/120    avg_loss:0.020, val_acc:0.981]
Epoch [55/120    avg_loss:0.008, val_acc:0.987]
Epoch [56/120    avg_loss:0.012, val_acc:0.985]
Epoch [57/120    avg_loss:0.022, val_acc:0.929]
Epoch [58/120    avg_loss:0.015, val_acc:0.984]
Epoch [59/120    avg_loss:0.014, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.986]
Epoch [61/120    avg_loss:0.007, val_acc:0.987]
Epoch [62/120    avg_loss:0.009, val_acc:0.987]
Epoch [63/120    avg_loss:0.012, val_acc:0.984]
Epoch [64/120    avg_loss:0.007, val_acc:0.990]
Epoch [65/120    avg_loss:0.007, val_acc:0.986]
Epoch [66/120    avg_loss:0.010, val_acc:0.989]
Epoch [67/120    avg_loss:0.013, val_acc:0.990]
Epoch [68/120    avg_loss:0.008, val_acc:0.988]
Epoch [69/120    avg_loss:0.027, val_acc:0.986]
Epoch [70/120    avg_loss:0.055, val_acc:0.978]
Epoch [71/120    avg_loss:0.039, val_acc:0.984]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.036, val_acc:0.985]
Epoch [74/120    avg_loss:0.014, val_acc:0.984]
Epoch [75/120    avg_loss:0.044, val_acc:0.963]
Epoch [76/120    avg_loss:0.020, val_acc:0.973]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.985]
Epoch [79/120    avg_loss:0.021, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.004, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     2     0     0    15    18     3     0]
 [    0     0 18069     0    10     0     3     0     8     0]
 [    0     0     0  2016     2     0     0     0    14     4]
 [    0    21    11     0  2912     0    10     0    16     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     2     0     0  4875     0     0     0]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0     2     0     8    58     0     0     0  3497     6]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.37097823729304

F1 scores:
[       nan 0.99525255 0.99908767 0.99212598 0.97587131 0.9893859
 0.99672869 0.99190751 0.98382332 0.96906077]

Kappa:
0.9916661406929473
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5aa9c7a828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.523, val_acc:0.619]
Epoch [2/120    avg_loss:0.935, val_acc:0.591]
Epoch [3/120    avg_loss:0.675, val_acc:0.778]
Epoch [4/120    avg_loss:0.548, val_acc:0.728]
Epoch [5/120    avg_loss:0.414, val_acc:0.754]
Epoch [6/120    avg_loss:0.413, val_acc:0.790]
Epoch [7/120    avg_loss:0.290, val_acc:0.896]
Epoch [8/120    avg_loss:0.268, val_acc:0.905]
Epoch [9/120    avg_loss:0.208, val_acc:0.922]
Epoch [10/120    avg_loss:0.211, val_acc:0.782]
Epoch [11/120    avg_loss:0.257, val_acc:0.901]
Epoch [12/120    avg_loss:0.189, val_acc:0.868]
Epoch [13/120    avg_loss:0.148, val_acc:0.863]
Epoch [14/120    avg_loss:0.139, val_acc:0.938]
Epoch [15/120    avg_loss:0.135, val_acc:0.938]
Epoch [16/120    avg_loss:0.110, val_acc:0.943]
Epoch [17/120    avg_loss:0.096, val_acc:0.926]
Epoch [18/120    avg_loss:0.083, val_acc:0.935]
Epoch [19/120    avg_loss:0.091, val_acc:0.957]
Epoch [20/120    avg_loss:0.074, val_acc:0.938]
Epoch [21/120    avg_loss:0.103, val_acc:0.952]
Epoch [22/120    avg_loss:0.076, val_acc:0.959]
Epoch [23/120    avg_loss:0.155, val_acc:0.894]
Epoch [24/120    avg_loss:0.129, val_acc:0.961]
Epoch [25/120    avg_loss:0.116, val_acc:0.954]
Epoch [26/120    avg_loss:0.106, val_acc:0.918]
Epoch [27/120    avg_loss:0.062, val_acc:0.958]
Epoch [28/120    avg_loss:0.055, val_acc:0.953]
Epoch [29/120    avg_loss:0.047, val_acc:0.970]
Epoch [30/120    avg_loss:0.032, val_acc:0.964]
Epoch [31/120    avg_loss:0.051, val_acc:0.967]
Epoch [32/120    avg_loss:0.056, val_acc:0.959]
Epoch [33/120    avg_loss:0.043, val_acc:0.965]
Epoch [34/120    avg_loss:0.027, val_acc:0.975]
Epoch [35/120    avg_loss:0.029, val_acc:0.973]
Epoch [36/120    avg_loss:0.018, val_acc:0.978]
Epoch [37/120    avg_loss:0.053, val_acc:0.971]
Epoch [38/120    avg_loss:0.024, val_acc:0.976]
Epoch [39/120    avg_loss:0.027, val_acc:0.902]
Epoch [40/120    avg_loss:0.038, val_acc:0.978]
Epoch [41/120    avg_loss:0.044, val_acc:0.959]
Epoch [42/120    avg_loss:0.051, val_acc:0.961]
Epoch [43/120    avg_loss:0.026, val_acc:0.979]
Epoch [44/120    avg_loss:0.015, val_acc:0.985]
Epoch [45/120    avg_loss:0.031, val_acc:0.981]
Epoch [46/120    avg_loss:0.014, val_acc:0.978]
Epoch [47/120    avg_loss:0.015, val_acc:0.977]
Epoch [48/120    avg_loss:0.027, val_acc:0.975]
Epoch [49/120    avg_loss:0.023, val_acc:0.970]
Epoch [50/120    avg_loss:0.016, val_acc:0.979]
Epoch [51/120    avg_loss:0.017, val_acc:0.959]
Epoch [52/120    avg_loss:0.011, val_acc:0.978]
Epoch [53/120    avg_loss:0.016, val_acc:0.982]
Epoch [54/120    avg_loss:0.013, val_acc:0.976]
Epoch [55/120    avg_loss:0.010, val_acc:0.979]
Epoch [56/120    avg_loss:0.007, val_acc:0.983]
Epoch [57/120    avg_loss:0.009, val_acc:0.978]
Epoch [58/120    avg_loss:0.007, val_acc:0.981]
Epoch [59/120    avg_loss:0.006, val_acc:0.983]
Epoch [60/120    avg_loss:0.006, val_acc:0.983]
Epoch [61/120    avg_loss:0.008, val_acc:0.981]
Epoch [62/120    avg_loss:0.007, val_acc:0.982]
Epoch [63/120    avg_loss:0.006, val_acc:0.983]
Epoch [64/120    avg_loss:0.007, val_acc:0.983]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.007, val_acc:0.983]
Epoch [67/120    avg_loss:0.008, val_acc:0.983]
Epoch [68/120    avg_loss:0.006, val_acc:0.983]
Epoch [69/120    avg_loss:0.005, val_acc:0.985]
Epoch [70/120    avg_loss:0.005, val_acc:0.982]
Epoch [71/120    avg_loss:0.007, val_acc:0.984]
Epoch [72/120    avg_loss:0.005, val_acc:0.984]
Epoch [73/120    avg_loss:0.008, val_acc:0.983]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.007, val_acc:0.981]
Epoch [76/120    avg_loss:0.006, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.005, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.985]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.004, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.981]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.003, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.003, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.981]
Epoch [97/120    avg_loss:0.007, val_acc:0.981]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.004, val_acc:0.982]
Epoch [106/120    avg_loss:0.004, val_acc:0.985]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.003, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     0     2     0     2    22    27     0]
 [    0     0 18046     0    22     0    18     0     4     0]
 [    0     7     0  2020     0     0     0     0     3     6]
 [    0    30     7     0  2920     0     0     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4859     0    13     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     1     0    23    61     0     0     0  3475    11]
 [    0     0     0     0    13    31     0     0     0   875]]

Accuracy:
99.2167353529511

F1 scores:
[       nan 0.99291774 0.99858894 0.98898409 0.97495826 0.98826202
 0.99600287 0.99115725 0.97777153 0.96578366]

Kappa:
0.9896266634145575
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61347757b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.598, val_acc:0.657]
Epoch [2/120    avg_loss:0.977, val_acc:0.738]
Epoch [3/120    avg_loss:0.666, val_acc:0.776]
Epoch [4/120    avg_loss:0.494, val_acc:0.717]
Epoch [5/120    avg_loss:0.391, val_acc:0.850]
Epoch [6/120    avg_loss:0.394, val_acc:0.799]
Epoch [7/120    avg_loss:0.292, val_acc:0.872]
Epoch [8/120    avg_loss:0.295, val_acc:0.903]
Epoch [9/120    avg_loss:0.190, val_acc:0.909]
Epoch [10/120    avg_loss:0.172, val_acc:0.918]
Epoch [11/120    avg_loss:0.211, val_acc:0.927]
Epoch [12/120    avg_loss:0.210, val_acc:0.873]
Epoch [13/120    avg_loss:0.130, val_acc:0.940]
Epoch [14/120    avg_loss:0.117, val_acc:0.964]
Epoch [15/120    avg_loss:0.107, val_acc:0.919]
Epoch [16/120    avg_loss:0.114, val_acc:0.923]
Epoch [17/120    avg_loss:0.097, val_acc:0.944]
Epoch [18/120    avg_loss:0.087, val_acc:0.959]
Epoch [19/120    avg_loss:0.048, val_acc:0.963]
Epoch [20/120    avg_loss:0.084, val_acc:0.897]
Epoch [21/120    avg_loss:0.083, val_acc:0.965]
Epoch [22/120    avg_loss:0.081, val_acc:0.955]
Epoch [23/120    avg_loss:0.054, val_acc:0.967]
Epoch [24/120    avg_loss:0.043, val_acc:0.967]
Epoch [25/120    avg_loss:0.046, val_acc:0.977]
Epoch [26/120    avg_loss:0.067, val_acc:0.890]
Epoch [27/120    avg_loss:0.084, val_acc:0.959]
Epoch [28/120    avg_loss:0.087, val_acc:0.975]
Epoch [29/120    avg_loss:0.057, val_acc:0.976]
Epoch [30/120    avg_loss:0.043, val_acc:0.980]
Epoch [31/120    avg_loss:0.039, val_acc:0.973]
Epoch [32/120    avg_loss:0.033, val_acc:0.954]
Epoch [33/120    avg_loss:0.031, val_acc:0.982]
Epoch [34/120    avg_loss:0.018, val_acc:0.981]
Epoch [35/120    avg_loss:0.031, val_acc:0.985]
Epoch [36/120    avg_loss:0.035, val_acc:0.974]
Epoch [37/120    avg_loss:0.028, val_acc:0.981]
Epoch [38/120    avg_loss:0.021, val_acc:0.980]
Epoch [39/120    avg_loss:0.016, val_acc:0.985]
Epoch [40/120    avg_loss:0.020, val_acc:0.979]
Epoch [41/120    avg_loss:0.024, val_acc:0.979]
Epoch [42/120    avg_loss:0.037, val_acc:0.979]
Epoch [43/120    avg_loss:0.027, val_acc:0.981]
Epoch [44/120    avg_loss:0.019, val_acc:0.979]
Epoch [45/120    avg_loss:0.015, val_acc:0.985]
Epoch [46/120    avg_loss:0.017, val_acc:0.955]
Epoch [47/120    avg_loss:0.016, val_acc:0.985]
Epoch [48/120    avg_loss:0.012, val_acc:0.985]
Epoch [49/120    avg_loss:0.038, val_acc:0.982]
Epoch [50/120    avg_loss:0.027, val_acc:0.976]
Epoch [51/120    avg_loss:0.121, val_acc:0.976]
Epoch [52/120    avg_loss:0.025, val_acc:0.975]
Epoch [53/120    avg_loss:0.021, val_acc:0.981]
Epoch [54/120    avg_loss:0.052, val_acc:0.978]
Epoch [55/120    avg_loss:0.023, val_acc:0.984]
Epoch [56/120    avg_loss:0.020, val_acc:0.976]
Epoch [57/120    avg_loss:0.021, val_acc:0.985]
Epoch [58/120    avg_loss:0.023, val_acc:0.978]
Epoch [59/120    avg_loss:0.016, val_acc:0.986]
Epoch [60/120    avg_loss:0.023, val_acc:0.952]
Epoch [61/120    avg_loss:0.023, val_acc:0.982]
Epoch [62/120    avg_loss:0.013, val_acc:0.988]
Epoch [63/120    avg_loss:0.007, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.987]
Epoch [66/120    avg_loss:0.013, val_acc:0.989]
Epoch [67/120    avg_loss:0.007, val_acc:0.991]
Epoch [68/120    avg_loss:0.005, val_acc:0.987]
Epoch [69/120    avg_loss:0.010, val_acc:0.987]
Epoch [70/120    avg_loss:0.017, val_acc:0.986]
Epoch [71/120    avg_loss:0.013, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.005, val_acc:0.990]
Epoch [74/120    avg_loss:0.005, val_acc:0.988]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.011, val_acc:0.972]
Epoch [77/120    avg_loss:0.012, val_acc:0.991]
Epoch [78/120    avg_loss:0.009, val_acc:0.989]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.992]
Epoch [81/120    avg_loss:0.006, val_acc:0.990]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.005, val_acc:0.990]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.009, val_acc:0.991]
Epoch [88/120    avg_loss:0.003, val_acc:0.992]
Epoch [89/120    avg_loss:0.003, val_acc:0.990]
Epoch [90/120    avg_loss:0.004, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.991]
Epoch [93/120    avg_loss:0.003, val_acc:0.991]
Epoch [94/120    avg_loss:0.003, val_acc:0.991]
Epoch [95/120    avg_loss:0.003, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.002, val_acc:0.991]
Epoch [99/120    avg_loss:0.002, val_acc:0.991]
Epoch [100/120    avg_loss:0.002, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.002, val_acc:0.991]
Epoch [103/120    avg_loss:0.002, val_acc:0.991]
Epoch [104/120    avg_loss:0.003, val_acc:0.991]
Epoch [105/120    avg_loss:0.002, val_acc:0.991]
Epoch [106/120    avg_loss:0.002, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.002, val_acc:0.991]
Epoch [109/120    avg_loss:0.002, val_acc:0.991]
Epoch [110/120    avg_loss:0.002, val_acc:0.991]
Epoch [111/120    avg_loss:0.002, val_acc:0.991]
Epoch [112/120    avg_loss:0.002, val_acc:0.991]
Epoch [113/120    avg_loss:0.002, val_acc:0.991]
Epoch [114/120    avg_loss:0.002, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.002, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.002, val_acc:0.990]
Epoch [119/120    avg_loss:0.002, val_acc:0.990]
Epoch [120/120    avg_loss:0.002, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     0     0     4     6     2     2]
 [    0     0 18079     0     7     0     0     0     4     0]
 [    0     0     0  2013     0     0     0     0    22     1]
 [    0    23    12     0  2916     0     0     0    20     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4865     0     5     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     3     0     6    40     0     0     0  3519     3]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.47943026534597

F1 scores:
[       nan 0.99689345 0.99914339 0.99284834 0.98033283 0.98863636
 0.99805108 0.99651568 0.98530029 0.97114317]

Kappa:
0.9931010767445094
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1bcf360860>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.558, val_acc:0.472]
Epoch [2/120    avg_loss:0.872, val_acc:0.592]
Epoch [3/120    avg_loss:0.578, val_acc:0.781]
Epoch [4/120    avg_loss:0.443, val_acc:0.778]
Epoch [5/120    avg_loss:0.408, val_acc:0.779]
Epoch [6/120    avg_loss:0.296, val_acc:0.832]
Epoch [7/120    avg_loss:0.342, val_acc:0.710]
Epoch [8/120    avg_loss:0.255, val_acc:0.890]
Epoch [9/120    avg_loss:0.217, val_acc:0.890]
Epoch [10/120    avg_loss:0.199, val_acc:0.859]
Epoch [11/120    avg_loss:0.161, val_acc:0.926]
Epoch [12/120    avg_loss:0.169, val_acc:0.914]
Epoch [13/120    avg_loss:0.116, val_acc:0.925]
Epoch [14/120    avg_loss:0.186, val_acc:0.850]
Epoch [15/120    avg_loss:0.147, val_acc:0.888]
Epoch [16/120    avg_loss:0.097, val_acc:0.912]
Epoch [17/120    avg_loss:0.107, val_acc:0.920]
Epoch [18/120    avg_loss:0.082, val_acc:0.946]
Epoch [19/120    avg_loss:0.083, val_acc:0.948]
Epoch [20/120    avg_loss:0.082, val_acc:0.944]
Epoch [21/120    avg_loss:0.070, val_acc:0.928]
Epoch [22/120    avg_loss:0.084, val_acc:0.902]
Epoch [23/120    avg_loss:0.080, val_acc:0.955]
Epoch [24/120    avg_loss:0.048, val_acc:0.962]
Epoch [25/120    avg_loss:0.045, val_acc:0.955]
Epoch [26/120    avg_loss:0.050, val_acc:0.959]
Epoch [27/120    avg_loss:0.038, val_acc:0.956]
Epoch [28/120    avg_loss:0.075, val_acc:0.958]
Epoch [29/120    avg_loss:0.078, val_acc:0.962]
Epoch [30/120    avg_loss:0.080, val_acc:0.960]
Epoch [31/120    avg_loss:0.045, val_acc:0.961]
Epoch [32/120    avg_loss:0.046, val_acc:0.974]
Epoch [33/120    avg_loss:0.030, val_acc:0.967]
Epoch [34/120    avg_loss:0.023, val_acc:0.970]
Epoch [35/120    avg_loss:0.012, val_acc:0.971]
Epoch [36/120    avg_loss:0.045, val_acc:0.961]
Epoch [37/120    avg_loss:0.052, val_acc:0.960]
Epoch [38/120    avg_loss:0.042, val_acc:0.976]
Epoch [39/120    avg_loss:0.026, val_acc:0.976]
Epoch [40/120    avg_loss:0.021, val_acc:0.976]
Epoch [41/120    avg_loss:0.015, val_acc:0.977]
Epoch [42/120    avg_loss:0.030, val_acc:0.975]
Epoch [43/120    avg_loss:0.067, val_acc:0.968]
Epoch [44/120    avg_loss:0.096, val_acc:0.906]
Epoch [45/120    avg_loss:0.082, val_acc:0.958]
Epoch [46/120    avg_loss:0.044, val_acc:0.951]
Epoch [47/120    avg_loss:0.036, val_acc:0.962]
Epoch [48/120    avg_loss:0.033, val_acc:0.948]
Epoch [49/120    avg_loss:0.032, val_acc:0.979]
Epoch [50/120    avg_loss:0.014, val_acc:0.985]
Epoch [51/120    avg_loss:0.015, val_acc:0.973]
Epoch [52/120    avg_loss:0.017, val_acc:0.977]
Epoch [53/120    avg_loss:0.010, val_acc:0.984]
Epoch [54/120    avg_loss:0.010, val_acc:0.985]
Epoch [55/120    avg_loss:0.009, val_acc:0.979]
Epoch [56/120    avg_loss:0.022, val_acc:0.974]
Epoch [57/120    avg_loss:0.015, val_acc:0.987]
Epoch [58/120    avg_loss:0.007, val_acc:0.980]
Epoch [59/120    avg_loss:0.008, val_acc:0.981]
Epoch [60/120    avg_loss:0.007, val_acc:0.982]
Epoch [61/120    avg_loss:0.008, val_acc:0.979]
Epoch [62/120    avg_loss:0.011, val_acc:0.982]
Epoch [63/120    avg_loss:0.011, val_acc:0.983]
Epoch [64/120    avg_loss:0.010, val_acc:0.983]
Epoch [65/120    avg_loss:0.013, val_acc:0.972]
Epoch [66/120    avg_loss:0.013, val_acc:0.984]
Epoch [67/120    avg_loss:0.010, val_acc:0.982]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.006, val_acc:0.985]
Epoch [71/120    avg_loss:0.009, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.985]
Epoch [74/120    avg_loss:0.006, val_acc:0.979]
Epoch [75/120    avg_loss:0.008, val_acc:0.981]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.017, val_acc:0.980]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.985]
Epoch [85/120    avg_loss:0.004, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.987]
Epoch [90/120    avg_loss:0.003, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.989]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.003, val_acc:0.987]
Epoch [96/120    avg_loss:0.003, val_acc:0.987]
Epoch [97/120    avg_loss:0.003, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.003, val_acc:0.988]
Epoch [103/120    avg_loss:0.003, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.003, val_acc:0.988]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.003, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6381     0     0     1     0     5     0    45     0]
 [    0     0 18080     0     9     0     0     0     1     0]
 [    0     0     0  2022     0     0     0     0     9     5]
 [    0    31     8     0  2906     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     3     0    14    51     0     0     0  3492    11]
 [    0     0     0     0    14    18     0     0     0   887]]

Accuracy:
99.37579832742873

F1 scores:
[       nan 0.99338367 0.99950246 0.99312377 0.97631446 0.99315068
 0.99877024 0.9992242  0.97678322 0.97365532]

Kappa:
0.9917294233958839
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fef28b36908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.567, val_acc:0.658]
Epoch [2/120    avg_loss:0.955, val_acc:0.646]
Epoch [3/120    avg_loss:0.602, val_acc:0.759]
Epoch [4/120    avg_loss:0.482, val_acc:0.798]
Epoch [5/120    avg_loss:0.398, val_acc:0.822]
Epoch [6/120    avg_loss:0.303, val_acc:0.887]
Epoch [7/120    avg_loss:0.303, val_acc:0.872]
Epoch [8/120    avg_loss:0.192, val_acc:0.910]
Epoch [9/120    avg_loss:0.189, val_acc:0.919]
Epoch [10/120    avg_loss:0.244, val_acc:0.899]
Epoch [11/120    avg_loss:0.171, val_acc:0.931]
Epoch [12/120    avg_loss:0.140, val_acc:0.929]
Epoch [13/120    avg_loss:0.104, val_acc:0.935]
Epoch [14/120    avg_loss:0.126, val_acc:0.937]
Epoch [15/120    avg_loss:0.162, val_acc:0.872]
Epoch [16/120    avg_loss:0.154, val_acc:0.932]
Epoch [17/120    avg_loss:0.106, val_acc:0.959]
Epoch [18/120    avg_loss:0.108, val_acc:0.921]
Epoch [19/120    avg_loss:0.124, val_acc:0.954]
Epoch [20/120    avg_loss:0.062, val_acc:0.965]
Epoch [21/120    avg_loss:0.052, val_acc:0.949]
Epoch [22/120    avg_loss:0.052, val_acc:0.943]
Epoch [23/120    avg_loss:0.064, val_acc:0.938]
Epoch [24/120    avg_loss:0.047, val_acc:0.959]
Epoch [25/120    avg_loss:0.034, val_acc:0.978]
Epoch [26/120    avg_loss:0.030, val_acc:0.970]
Epoch [27/120    avg_loss:0.057, val_acc:0.976]
Epoch [28/120    avg_loss:0.032, val_acc:0.977]
Epoch [29/120    avg_loss:0.027, val_acc:0.958]
Epoch [30/120    avg_loss:0.069, val_acc:0.953]
Epoch [31/120    avg_loss:0.041, val_acc:0.965]
Epoch [32/120    avg_loss:0.034, val_acc:0.973]
Epoch [33/120    avg_loss:0.031, val_acc:0.979]
Epoch [34/120    avg_loss:0.032, val_acc:0.977]
Epoch [35/120    avg_loss:0.013, val_acc:0.981]
Epoch [36/120    avg_loss:0.025, val_acc:0.970]
Epoch [37/120    avg_loss:0.038, val_acc:0.943]
Epoch [38/120    avg_loss:0.019, val_acc:0.953]
Epoch [39/120    avg_loss:0.045, val_acc:0.970]
Epoch [40/120    avg_loss:0.025, val_acc:0.976]
Epoch [41/120    avg_loss:0.033, val_acc:0.978]
Epoch [42/120    avg_loss:0.023, val_acc:0.979]
Epoch [43/120    avg_loss:0.049, val_acc:0.982]
Epoch [44/120    avg_loss:0.022, val_acc:0.981]
Epoch [45/120    avg_loss:0.022, val_acc:0.983]
Epoch [46/120    avg_loss:0.022, val_acc:0.947]
Epoch [47/120    avg_loss:0.039, val_acc:0.975]
Epoch [48/120    avg_loss:0.115, val_acc:0.968]
Epoch [49/120    avg_loss:0.106, val_acc:0.971]
Epoch [50/120    avg_loss:0.080, val_acc:0.956]
Epoch [51/120    avg_loss:0.037, val_acc:0.977]
Epoch [52/120    avg_loss:0.029, val_acc:0.983]
Epoch [53/120    avg_loss:0.017, val_acc:0.981]
Epoch [54/120    avg_loss:0.016, val_acc:0.982]
Epoch [55/120    avg_loss:0.030, val_acc:0.976]
Epoch [56/120    avg_loss:0.031, val_acc:0.976]
Epoch [57/120    avg_loss:0.019, val_acc:0.981]
Epoch [58/120    avg_loss:0.009, val_acc:0.981]
Epoch [59/120    avg_loss:0.021, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.985]
Epoch [61/120    avg_loss:0.017, val_acc:0.985]
Epoch [62/120    avg_loss:0.018, val_acc:0.987]
Epoch [63/120    avg_loss:0.006, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.976]
Epoch [65/120    avg_loss:0.016, val_acc:0.984]
Epoch [66/120    avg_loss:0.013, val_acc:0.985]
Epoch [67/120    avg_loss:0.008, val_acc:0.990]
Epoch [68/120    avg_loss:0.006, val_acc:0.983]
Epoch [69/120    avg_loss:0.008, val_acc:0.986]
Epoch [70/120    avg_loss:0.005, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.986]
Epoch [73/120    avg_loss:0.028, val_acc:0.982]
Epoch [74/120    avg_loss:0.008, val_acc:0.985]
Epoch [75/120    avg_loss:0.023, val_acc:0.972]
Epoch [76/120    avg_loss:0.042, val_acc:0.979]
Epoch [77/120    avg_loss:0.016, val_acc:0.985]
Epoch [78/120    avg_loss:0.019, val_acc:0.970]
Epoch [79/120    avg_loss:0.010, val_acc:0.990]
Epoch [80/120    avg_loss:0.019, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.982]
Epoch [82/120    avg_loss:0.006, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.991]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.976]
Epoch [90/120    avg_loss:0.011, val_acc:0.982]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.004, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.002, val_acc:0.992]
Epoch [99/120    avg_loss:0.003, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.003, val_acc:0.992]
Epoch [102/120    avg_loss:0.003, val_acc:0.992]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.002, val_acc:0.992]
Epoch [105/120    avg_loss:0.003, val_acc:0.992]
Epoch [106/120    avg_loss:0.003, val_acc:0.992]
Epoch [107/120    avg_loss:0.003, val_acc:0.992]
Epoch [108/120    avg_loss:0.003, val_acc:0.992]
Epoch [109/120    avg_loss:0.002, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.003, val_acc:0.992]
Epoch [112/120    avg_loss:0.005, val_acc:0.992]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.002, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.003, val_acc:0.992]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.002, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     2     0     0     0    16     1]
 [    0     0 18006     0    18     0    65     0     1     0]
 [    0     1     0  2015     0     0     0     0    14     6]
 [    0    27    10     0  2915     0     0     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4867     0    11     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     7     0    31    55     0     0     0  3477     1]
 [    0     0     0     0    15    19     0     0     0   885]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.99580745 0.99739655 0.98726115 0.97540572 0.99277292
 0.99205055 0.9992242  0.97805907 0.97682119]

Kappa:
0.9897246666590155
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f885581c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.608, val_acc:0.534]
Epoch [2/120    avg_loss:0.902, val_acc:0.680]
Epoch [3/120    avg_loss:0.578, val_acc:0.774]
Epoch [4/120    avg_loss:0.442, val_acc:0.800]
Epoch [5/120    avg_loss:0.377, val_acc:0.883]
Epoch [6/120    avg_loss:0.280, val_acc:0.839]
Epoch [7/120    avg_loss:0.241, val_acc:0.883]
Epoch [8/120    avg_loss:0.239, val_acc:0.875]
Epoch [9/120    avg_loss:0.203, val_acc:0.916]
Epoch [10/120    avg_loss:0.160, val_acc:0.944]
Epoch [11/120    avg_loss:0.174, val_acc:0.935]
Epoch [12/120    avg_loss:0.150, val_acc:0.950]
Epoch [13/120    avg_loss:0.156, val_acc:0.944]
Epoch [14/120    avg_loss:0.121, val_acc:0.932]
Epoch [15/120    avg_loss:0.127, val_acc:0.934]
Epoch [16/120    avg_loss:0.095, val_acc:0.954]
Epoch [17/120    avg_loss:0.073, val_acc:0.961]
Epoch [18/120    avg_loss:0.084, val_acc:0.966]
Epoch [19/120    avg_loss:0.070, val_acc:0.955]
Epoch [20/120    avg_loss:0.106, val_acc:0.960]
Epoch [21/120    avg_loss:0.059, val_acc:0.965]
Epoch [22/120    avg_loss:0.062, val_acc:0.953]
Epoch [23/120    avg_loss:0.079, val_acc:0.974]
Epoch [24/120    avg_loss:0.054, val_acc:0.959]
Epoch [25/120    avg_loss:0.061, val_acc:0.943]
Epoch [26/120    avg_loss:0.070, val_acc:0.961]
Epoch [27/120    avg_loss:0.045, val_acc:0.962]
Epoch [28/120    avg_loss:0.039, val_acc:0.981]
Epoch [29/120    avg_loss:0.028, val_acc:0.967]
Epoch [30/120    avg_loss:0.025, val_acc:0.982]
Epoch [31/120    avg_loss:0.021, val_acc:0.982]
Epoch [32/120    avg_loss:0.035, val_acc:0.970]
Epoch [33/120    avg_loss:0.062, val_acc:0.950]
Epoch [34/120    avg_loss:0.027, val_acc:0.981]
Epoch [35/120    avg_loss:0.026, val_acc:0.978]
Epoch [36/120    avg_loss:0.028, val_acc:0.970]
Epoch [37/120    avg_loss:0.029, val_acc:0.984]
Epoch [38/120    avg_loss:0.063, val_acc:0.972]
Epoch [39/120    avg_loss:0.069, val_acc:0.974]
Epoch [40/120    avg_loss:0.079, val_acc:0.921]
Epoch [41/120    avg_loss:0.099, val_acc:0.963]
Epoch [42/120    avg_loss:0.031, val_acc:0.974]
Epoch [43/120    avg_loss:0.020, val_acc:0.984]
Epoch [44/120    avg_loss:0.020, val_acc:0.985]
Epoch [45/120    avg_loss:0.012, val_acc:0.986]
Epoch [46/120    avg_loss:0.018, val_acc:0.985]
Epoch [47/120    avg_loss:0.015, val_acc:0.986]
Epoch [48/120    avg_loss:0.025, val_acc:0.983]
Epoch [49/120    avg_loss:0.016, val_acc:0.985]
Epoch [50/120    avg_loss:0.011, val_acc:0.979]
Epoch [51/120    avg_loss:0.011, val_acc:0.983]
Epoch [52/120    avg_loss:0.017, val_acc:0.986]
Epoch [53/120    avg_loss:0.023, val_acc:0.982]
Epoch [54/120    avg_loss:0.009, val_acc:0.984]
Epoch [55/120    avg_loss:0.015, val_acc:0.981]
Epoch [56/120    avg_loss:0.012, val_acc:0.982]
Epoch [57/120    avg_loss:0.010, val_acc:0.986]
Epoch [58/120    avg_loss:0.008, val_acc:0.987]
Epoch [59/120    avg_loss:0.008, val_acc:0.985]
Epoch [60/120    avg_loss:0.005, val_acc:0.986]
Epoch [61/120    avg_loss:0.007, val_acc:0.992]
Epoch [62/120    avg_loss:0.010, val_acc:0.984]
Epoch [63/120    avg_loss:0.005, val_acc:0.990]
Epoch [64/120    avg_loss:0.019, val_acc:0.984]
Epoch [65/120    avg_loss:0.019, val_acc:0.981]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.015, val_acc:0.986]
Epoch [68/120    avg_loss:0.017, val_acc:0.986]
Epoch [69/120    avg_loss:0.011, val_acc:0.990]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.026, val_acc:0.970]
Epoch [72/120    avg_loss:0.014, val_acc:0.989]
Epoch [73/120    avg_loss:0.008, val_acc:0.992]
Epoch [74/120    avg_loss:0.006, val_acc:0.988]
Epoch [75/120    avg_loss:0.005, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.991]
Epoch [77/120    avg_loss:0.003, val_acc:0.991]
Epoch [78/120    avg_loss:0.005, val_acc:0.991]
Epoch [79/120    avg_loss:0.004, val_acc:0.992]
Epoch [80/120    avg_loss:0.011, val_acc:0.987]
Epoch [81/120    avg_loss:0.003, val_acc:0.992]
Epoch [82/120    avg_loss:0.009, val_acc:0.954]
Epoch [83/120    avg_loss:0.013, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.012, val_acc:0.979]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.003, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.107, val_acc:0.975]
Epoch [93/120    avg_loss:0.048, val_acc:0.977]
Epoch [94/120    avg_loss:0.041, val_acc:0.977]
Epoch [95/120    avg_loss:0.031, val_acc:0.981]
Epoch [96/120    avg_loss:0.024, val_acc:0.980]
Epoch [97/120    avg_loss:0.020, val_acc:0.981]
Epoch [98/120    avg_loss:0.022, val_acc:0.983]
Epoch [99/120    avg_loss:0.019, val_acc:0.981]
Epoch [100/120    avg_loss:0.021, val_acc:0.983]
Epoch [101/120    avg_loss:0.017, val_acc:0.985]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.018, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.986]
Epoch [105/120    avg_loss:0.014, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.013, val_acc:0.985]
Epoch [108/120    avg_loss:0.013, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.985]
Epoch [110/120    avg_loss:0.010, val_acc:0.985]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.014, val_acc:0.986]
Epoch [114/120    avg_loss:0.012, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.986]
Epoch [116/120    avg_loss:0.014, val_acc:0.986]
Epoch [117/120    avg_loss:0.018, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.013, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     0     1     0    13     0    58    10]
 [    0     0 17918     0    39     0   127     0     6     0]
 [    0     0     0  2018     1     0     0     0    13     4]
 [    0    31     0     0  2914     0     7     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     3     0     0  4866     0     9     0]
 [    0     9     0     0     0     0     3  1272     0     6]
 [    0     1     0     9    56     0     0     0  3503     2]
 [    0     0     0     0    14    16     0     0     0   889]]

Accuracy:
98.89619935892802

F1 scores:
[       nan 0.99040786 0.99522328 0.99262174 0.97181924 0.99390708
 0.98362644 0.99297424 0.97576602 0.9715847 ]

Kappa:
0.985401285657658
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbacd76e7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.564, val_acc:0.404]
Epoch [2/120    avg_loss:0.914, val_acc:0.677]
Epoch [3/120    avg_loss:0.689, val_acc:0.772]
Epoch [4/120    avg_loss:0.488, val_acc:0.821]
Epoch [5/120    avg_loss:0.389, val_acc:0.799]
Epoch [6/120    avg_loss:0.385, val_acc:0.743]
Epoch [7/120    avg_loss:0.352, val_acc:0.819]
Epoch [8/120    avg_loss:0.264, val_acc:0.896]
Epoch [9/120    avg_loss:0.314, val_acc:0.895]
Epoch [10/120    avg_loss:0.283, val_acc:0.815]
Epoch [11/120    avg_loss:0.188, val_acc:0.891]
Epoch [12/120    avg_loss:0.187, val_acc:0.872]
Epoch [13/120    avg_loss:0.193, val_acc:0.939]
Epoch [14/120    avg_loss:0.122, val_acc:0.913]
Epoch [15/120    avg_loss:0.096, val_acc:0.911]
Epoch [16/120    avg_loss:0.191, val_acc:0.948]
Epoch [17/120    avg_loss:0.139, val_acc:0.956]
Epoch [18/120    avg_loss:0.111, val_acc:0.920]
Epoch [19/120    avg_loss:0.105, val_acc:0.935]
Epoch [20/120    avg_loss:0.076, val_acc:0.944]
Epoch [21/120    avg_loss:0.097, val_acc:0.969]
Epoch [22/120    avg_loss:0.058, val_acc:0.958]
Epoch [23/120    avg_loss:0.066, val_acc:0.938]
Epoch [24/120    avg_loss:0.086, val_acc:0.972]
Epoch [25/120    avg_loss:0.045, val_acc:0.950]
Epoch [26/120    avg_loss:0.042, val_acc:0.972]
Epoch [27/120    avg_loss:0.058, val_acc:0.981]
Epoch [28/120    avg_loss:0.045, val_acc:0.976]
Epoch [29/120    avg_loss:0.030, val_acc:0.976]
Epoch [30/120    avg_loss:0.028, val_acc:0.981]
Epoch [31/120    avg_loss:0.025, val_acc:0.973]
Epoch [32/120    avg_loss:0.036, val_acc:0.964]
Epoch [33/120    avg_loss:0.022, val_acc:0.970]
Epoch [34/120    avg_loss:0.030, val_acc:0.976]
Epoch [35/120    avg_loss:0.024, val_acc:0.972]
Epoch [36/120    avg_loss:0.014, val_acc:0.983]
Epoch [37/120    avg_loss:0.013, val_acc:0.983]
Epoch [38/120    avg_loss:0.033, val_acc:0.957]
Epoch [39/120    avg_loss:0.052, val_acc:0.977]
Epoch [40/120    avg_loss:0.026, val_acc:0.974]
Epoch [41/120    avg_loss:0.062, val_acc:0.951]
Epoch [42/120    avg_loss:0.059, val_acc:0.963]
Epoch [43/120    avg_loss:0.045, val_acc:0.969]
Epoch [44/120    avg_loss:0.029, val_acc:0.974]
Epoch [45/120    avg_loss:0.016, val_acc:0.984]
Epoch [46/120    avg_loss:0.015, val_acc:0.975]
Epoch [47/120    avg_loss:0.014, val_acc:0.984]
Epoch [48/120    avg_loss:0.013, val_acc:0.979]
Epoch [49/120    avg_loss:0.024, val_acc:0.970]
Epoch [50/120    avg_loss:0.016, val_acc:0.981]
Epoch [51/120    avg_loss:0.015, val_acc:0.967]
Epoch [52/120    avg_loss:0.016, val_acc:0.981]
Epoch [53/120    avg_loss:0.012, val_acc:0.982]
Epoch [54/120    avg_loss:0.018, val_acc:0.980]
Epoch [55/120    avg_loss:0.013, val_acc:0.990]
Epoch [56/120    avg_loss:0.012, val_acc:0.985]
Epoch [57/120    avg_loss:0.017, val_acc:0.985]
Epoch [58/120    avg_loss:0.011, val_acc:0.986]
Epoch [59/120    avg_loss:0.007, val_acc:0.987]
Epoch [60/120    avg_loss:0.006, val_acc:0.984]
Epoch [61/120    avg_loss:0.008, val_acc:0.976]
Epoch [62/120    avg_loss:0.012, val_acc:0.971]
Epoch [63/120    avg_loss:0.011, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.959]
Epoch [65/120    avg_loss:0.009, val_acc:0.990]
Epoch [66/120    avg_loss:0.013, val_acc:0.988]
Epoch [67/120    avg_loss:0.007, val_acc:0.991]
Epoch [68/120    avg_loss:0.006, val_acc:0.989]
Epoch [69/120    avg_loss:0.012, val_acc:0.988]
Epoch [70/120    avg_loss:0.011, val_acc:0.981]
Epoch [71/120    avg_loss:0.007, val_acc:0.979]
Epoch [72/120    avg_loss:0.005, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.989]
Epoch [74/120    avg_loss:0.004, val_acc:0.990]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.014, val_acc:0.954]
Epoch [77/120    avg_loss:0.005, val_acc:0.989]
Epoch [78/120    avg_loss:0.004, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.990]
Epoch [80/120    avg_loss:0.003, val_acc:0.989]
Epoch [81/120    avg_loss:0.004, val_acc:0.989]
Epoch [82/120    avg_loss:0.005, val_acc:0.990]
Epoch [83/120    avg_loss:0.004, val_acc:0.990]
Epoch [84/120    avg_loss:0.004, val_acc:0.992]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.003, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.990]
Epoch [88/120    avg_loss:0.004, val_acc:0.992]
Epoch [89/120    avg_loss:0.003, val_acc:0.992]
Epoch [90/120    avg_loss:0.004, val_acc:0.992]
Epoch [91/120    avg_loss:0.003, val_acc:0.992]
Epoch [92/120    avg_loss:0.003, val_acc:0.993]
Epoch [93/120    avg_loss:0.003, val_acc:0.994]
Epoch [94/120    avg_loss:0.003, val_acc:0.992]
Epoch [95/120    avg_loss:0.003, val_acc:0.992]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.003, val_acc:0.992]
Epoch [98/120    avg_loss:0.003, val_acc:0.992]
Epoch [99/120    avg_loss:0.003, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.993]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.991]
Epoch [104/120    avg_loss:0.003, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.003, val_acc:0.992]
Epoch [108/120    avg_loss:0.003, val_acc:0.992]
Epoch [109/120    avg_loss:0.002, val_acc:0.992]
Epoch [110/120    avg_loss:0.003, val_acc:0.992]
Epoch [111/120    avg_loss:0.003, val_acc:0.992]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.993]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.003, val_acc:0.992]
Epoch [119/120    avg_loss:0.003, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     2     1     0     2    40    16     1]
 [    0     0 18057     0    21     0    12     0     0     0]
 [    0     0     0  2020     1     0     0     0     8     7]
 [    0    35    10     1  2915     0     0     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4874     0     2     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     0     0    13    48     0     0     0  3506     4]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.33241751620756

F1 scores:
[       nan 0.99244372 0.9987555  0.99214145 0.97622237 0.99088838
 0.9979525  0.98395722 0.98566207 0.97240618]

Kappa:
0.9911572277885217
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc56af0d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.522, val_acc:0.564]
Epoch [2/120    avg_loss:0.848, val_acc:0.465]
Epoch [3/120    avg_loss:0.635, val_acc:0.781]
Epoch [4/120    avg_loss:0.493, val_acc:0.807]
Epoch [5/120    avg_loss:0.393, val_acc:0.783]
Epoch [6/120    avg_loss:0.334, val_acc:0.855]
Epoch [7/120    avg_loss:0.308, val_acc:0.848]
Epoch [8/120    avg_loss:0.304, val_acc:0.828]
Epoch [9/120    avg_loss:0.354, val_acc:0.852]
Epoch [10/120    avg_loss:0.198, val_acc:0.911]
Epoch [11/120    avg_loss:0.187, val_acc:0.925]
Epoch [12/120    avg_loss:0.257, val_acc:0.925]
Epoch [13/120    avg_loss:0.156, val_acc:0.926]
Epoch [14/120    avg_loss:0.110, val_acc:0.916]
Epoch [15/120    avg_loss:0.129, val_acc:0.927]
Epoch [16/120    avg_loss:0.107, val_acc:0.947]
Epoch [17/120    avg_loss:0.108, val_acc:0.943]
Epoch [18/120    avg_loss:0.082, val_acc:0.944]
Epoch [19/120    avg_loss:0.093, val_acc:0.954]
Epoch [20/120    avg_loss:0.079, val_acc:0.956]
Epoch [21/120    avg_loss:0.096, val_acc:0.952]
Epoch [22/120    avg_loss:0.067, val_acc:0.956]
Epoch [23/120    avg_loss:0.056, val_acc:0.966]
Epoch [24/120    avg_loss:0.057, val_acc:0.964]
Epoch [25/120    avg_loss:0.053, val_acc:0.924]
Epoch [26/120    avg_loss:0.055, val_acc:0.970]
Epoch [27/120    avg_loss:0.079, val_acc:0.964]
Epoch [28/120    avg_loss:0.051, val_acc:0.960]
Epoch [29/120    avg_loss:0.049, val_acc:0.955]
Epoch [30/120    avg_loss:0.055, val_acc:0.955]
Epoch [31/120    avg_loss:0.042, val_acc:0.959]
Epoch [32/120    avg_loss:0.037, val_acc:0.972]
Epoch [33/120    avg_loss:0.043, val_acc:0.977]
Epoch [34/120    avg_loss:0.032, val_acc:0.964]
Epoch [35/120    avg_loss:0.018, val_acc:0.978]
Epoch [36/120    avg_loss:0.017, val_acc:0.982]
Epoch [37/120    avg_loss:0.016, val_acc:0.976]
Epoch [38/120    avg_loss:0.015, val_acc:0.981]
Epoch [39/120    avg_loss:0.013, val_acc:0.981]
Epoch [40/120    avg_loss:0.019, val_acc:0.977]
Epoch [41/120    avg_loss:0.013, val_acc:0.980]
Epoch [42/120    avg_loss:0.013, val_acc:0.965]
Epoch [43/120    avg_loss:0.025, val_acc:0.967]
Epoch [44/120    avg_loss:0.015, val_acc:0.973]
Epoch [45/120    avg_loss:0.015, val_acc:0.971]
Epoch [46/120    avg_loss:0.026, val_acc:0.982]
Epoch [47/120    avg_loss:0.038, val_acc:0.981]
Epoch [48/120    avg_loss:0.019, val_acc:0.979]
Epoch [49/120    avg_loss:0.013, val_acc:0.982]
Epoch [50/120    avg_loss:0.012, val_acc:0.985]
Epoch [51/120    avg_loss:0.016, val_acc:0.985]
Epoch [52/120    avg_loss:0.008, val_acc:0.986]
Epoch [53/120    avg_loss:0.014, val_acc:0.976]
Epoch [54/120    avg_loss:0.032, val_acc:0.976]
Epoch [55/120    avg_loss:0.061, val_acc:0.971]
Epoch [56/120    avg_loss:0.023, val_acc:0.979]
Epoch [57/120    avg_loss:0.024, val_acc:0.941]
Epoch [58/120    avg_loss:0.034, val_acc:0.976]
Epoch [59/120    avg_loss:0.039, val_acc:0.962]
Epoch [60/120    avg_loss:0.020, val_acc:0.986]
Epoch [61/120    avg_loss:0.030, val_acc:0.966]
Epoch [62/120    avg_loss:0.016, val_acc:0.983]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.014, val_acc:0.984]
Epoch [65/120    avg_loss:0.008, val_acc:0.986]
Epoch [66/120    avg_loss:0.010, val_acc:0.981]
Epoch [67/120    avg_loss:0.011, val_acc:0.985]
Epoch [68/120    avg_loss:0.017, val_acc:0.981]
Epoch [69/120    avg_loss:0.008, val_acc:0.987]
Epoch [70/120    avg_loss:0.011, val_acc:0.986]
Epoch [71/120    avg_loss:0.039, val_acc:0.980]
Epoch [72/120    avg_loss:0.029, val_acc:0.972]
Epoch [73/120    avg_loss:0.014, val_acc:0.985]
Epoch [74/120    avg_loss:0.021, val_acc:0.972]
Epoch [75/120    avg_loss:0.043, val_acc:0.978]
Epoch [76/120    avg_loss:0.018, val_acc:0.979]
Epoch [77/120    avg_loss:0.012, val_acc:0.980]
Epoch [78/120    avg_loss:0.018, val_acc:0.976]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.005, val_acc:0.981]
Epoch [81/120    avg_loss:0.010, val_acc:0.979]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.010, val_acc:0.984]
Epoch [84/120    avg_loss:0.005, val_acc:0.984]
Epoch [85/120    avg_loss:0.004, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.986]
Epoch [88/120    avg_loss:0.003, val_acc:0.985]
Epoch [89/120    avg_loss:0.004, val_acc:0.984]
Epoch [90/120    avg_loss:0.012, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.982]
Epoch [95/120    avg_loss:0.004, val_acc:0.981]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.003, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.984]
Epoch [99/120    avg_loss:0.003, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.985]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.003, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.003, val_acc:0.984]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.003, val_acc:0.984]
Epoch [120/120    avg_loss:0.003, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6349     0     0     0     0    15    21    35    12]
 [    0     0 18060     0    10     0    20     0     0     0]
 [    0     0     0  2013     1     0     0     0    12    10]
 [    0    34     3     0  2917     0     0     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4867     0    11     0]
 [    0     0     0     0     0     0     5  1284     0     1]
 [    0     0     0    16    45     0     0     0  3500    10]
 [    0     0     0     0    14    45     0     0     0   860]]

Accuracy:
99.18540476706914

F1 scores:
[       nan 0.99087007 0.99908721 0.9904059  0.97902333 0.98305085
 0.99478794 0.98959538 0.97943193 0.94922737]

Kappa:
0.9892107693447187
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe0f0e4c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.519, val_acc:0.465]
Epoch [2/120    avg_loss:0.924, val_acc:0.609]
Epoch [3/120    avg_loss:0.615, val_acc:0.686]
Epoch [4/120    avg_loss:0.501, val_acc:0.819]
Epoch [5/120    avg_loss:0.388, val_acc:0.757]
Epoch [6/120    avg_loss:0.419, val_acc:0.833]
Epoch [7/120    avg_loss:0.323, val_acc:0.848]
Epoch [8/120    avg_loss:0.235, val_acc:0.887]
Epoch [9/120    avg_loss:0.242, val_acc:0.913]
Epoch [10/120    avg_loss:0.172, val_acc:0.909]
Epoch [11/120    avg_loss:0.191, val_acc:0.944]
Epoch [12/120    avg_loss:0.149, val_acc:0.954]
Epoch [13/120    avg_loss:0.132, val_acc:0.961]
Epoch [14/120    avg_loss:0.097, val_acc:0.915]
Epoch [15/120    avg_loss:0.124, val_acc:0.931]
Epoch [16/120    avg_loss:0.099, val_acc:0.961]
Epoch [17/120    avg_loss:0.086, val_acc:0.947]
Epoch [18/120    avg_loss:0.082, val_acc:0.943]
Epoch [19/120    avg_loss:0.091, val_acc:0.938]
Epoch [20/120    avg_loss:0.094, val_acc:0.967]
Epoch [21/120    avg_loss:0.106, val_acc:0.952]
Epoch [22/120    avg_loss:0.082, val_acc:0.949]
Epoch [23/120    avg_loss:0.082, val_acc:0.963]
Epoch [24/120    avg_loss:0.083, val_acc:0.932]
Epoch [25/120    avg_loss:0.194, val_acc:0.949]
Epoch [26/120    avg_loss:0.069, val_acc:0.969]
Epoch [27/120    avg_loss:0.046, val_acc:0.945]
Epoch [28/120    avg_loss:0.073, val_acc:0.961]
Epoch [29/120    avg_loss:0.048, val_acc:0.941]
Epoch [30/120    avg_loss:0.048, val_acc:0.973]
Epoch [31/120    avg_loss:0.048, val_acc:0.973]
Epoch [32/120    avg_loss:0.042, val_acc:0.968]
Epoch [33/120    avg_loss:0.034, val_acc:0.979]
Epoch [34/120    avg_loss:0.020, val_acc:0.967]
Epoch [35/120    avg_loss:0.031, val_acc:0.975]
Epoch [36/120    avg_loss:0.024, val_acc:0.980]
Epoch [37/120    avg_loss:0.017, val_acc:0.976]
Epoch [38/120    avg_loss:0.037, val_acc:0.968]
Epoch [39/120    avg_loss:0.046, val_acc:0.977]
Epoch [40/120    avg_loss:0.034, val_acc:0.985]
Epoch [41/120    avg_loss:0.018, val_acc:0.990]
Epoch [42/120    avg_loss:0.012, val_acc:0.982]
Epoch [43/120    avg_loss:0.015, val_acc:0.991]
Epoch [44/120    avg_loss:0.021, val_acc:0.987]
Epoch [45/120    avg_loss:0.014, val_acc:0.984]
Epoch [46/120    avg_loss:0.016, val_acc:0.980]
Epoch [47/120    avg_loss:0.018, val_acc:0.991]
Epoch [48/120    avg_loss:0.011, val_acc:0.986]
Epoch [49/120    avg_loss:0.016, val_acc:0.982]
Epoch [50/120    avg_loss:0.019, val_acc:0.982]
Epoch [51/120    avg_loss:0.026, val_acc:0.984]
Epoch [52/120    avg_loss:0.017, val_acc:0.985]
Epoch [53/120    avg_loss:0.016, val_acc:0.986]
Epoch [54/120    avg_loss:0.016, val_acc:0.985]
Epoch [55/120    avg_loss:0.021, val_acc:0.981]
Epoch [56/120    avg_loss:0.011, val_acc:0.980]
Epoch [57/120    avg_loss:0.014, val_acc:0.985]
Epoch [58/120    avg_loss:0.008, val_acc:0.984]
Epoch [59/120    avg_loss:0.010, val_acc:0.984]
Epoch [60/120    avg_loss:0.009, val_acc:0.984]
Epoch [61/120    avg_loss:0.008, val_acc:0.984]
Epoch [62/120    avg_loss:0.007, val_acc:0.985]
Epoch [63/120    avg_loss:0.006, val_acc:0.985]
Epoch [64/120    avg_loss:0.005, val_acc:0.986]
Epoch [65/120    avg_loss:0.007, val_acc:0.986]
Epoch [66/120    avg_loss:0.005, val_acc:0.985]
Epoch [67/120    avg_loss:0.007, val_acc:0.987]
Epoch [68/120    avg_loss:0.005, val_acc:0.986]
Epoch [69/120    avg_loss:0.009, val_acc:0.987]
Epoch [70/120    avg_loss:0.006, val_acc:0.987]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.005, val_acc:0.985]
Epoch [77/120    avg_loss:0.005, val_acc:0.985]
Epoch [78/120    avg_loss:0.005, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.985]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.985]
Epoch [85/120    avg_loss:0.004, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.005, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.985]
Epoch [109/120    avg_loss:0.006, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.004, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     9     0     0    19     0    14     0]
 [    0     0 17988     0    27     0    73     0     0     2]
 [    0     0     0  2014     1     0     0     0    14     7]
 [    0    39    24     0  2894     0     0     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0     0    18]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     0     0     0    58     0     0     0  3488    25]
 [    0     0     0     0    14    32     0     0     0   873]]

Accuracy:
99.05285228833779

F1 scores:
[       nan 0.99370189 0.99650989 0.99236265 0.97016426 0.98788796
 0.98860862 0.9992242  0.98225852 0.94685466]

Kappa:
0.9874605182492016
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5828e5c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.525, val_acc:0.317]
Epoch [2/120    avg_loss:0.927, val_acc:0.706]
Epoch [3/120    avg_loss:0.613, val_acc:0.787]
Epoch [4/120    avg_loss:0.574, val_acc:0.746]
Epoch [5/120    avg_loss:0.412, val_acc:0.813]
Epoch [6/120    avg_loss:0.294, val_acc:0.870]
Epoch [7/120    avg_loss:0.241, val_acc:0.859]
Epoch [8/120    avg_loss:0.230, val_acc:0.866]
Epoch [9/120    avg_loss:0.206, val_acc:0.924]
Epoch [10/120    avg_loss:0.132, val_acc:0.866]
Epoch [11/120    avg_loss:0.162, val_acc:0.920]
Epoch [12/120    avg_loss:0.148, val_acc:0.934]
Epoch [13/120    avg_loss:0.099, val_acc:0.927]
Epoch [14/120    avg_loss:0.152, val_acc:0.937]
Epoch [15/120    avg_loss:0.128, val_acc:0.932]
Epoch [16/120    avg_loss:0.080, val_acc:0.952]
Epoch [17/120    avg_loss:0.094, val_acc:0.906]
Epoch [18/120    avg_loss:0.088, val_acc:0.896]
Epoch [19/120    avg_loss:0.092, val_acc:0.947]
Epoch [20/120    avg_loss:0.077, val_acc:0.925]
Epoch [21/120    avg_loss:0.088, val_acc:0.943]
Epoch [22/120    avg_loss:0.063, val_acc:0.934]
Epoch [23/120    avg_loss:0.033, val_acc:0.969]
Epoch [24/120    avg_loss:0.067, val_acc:0.950]
Epoch [25/120    avg_loss:0.083, val_acc:0.946]
Epoch [26/120    avg_loss:0.048, val_acc:0.971]
Epoch [27/120    avg_loss:0.073, val_acc:0.949]
Epoch [28/120    avg_loss:0.065, val_acc:0.971]
Epoch [29/120    avg_loss:0.046, val_acc:0.963]
Epoch [30/120    avg_loss:0.033, val_acc:0.958]
Epoch [31/120    avg_loss:0.029, val_acc:0.965]
Epoch [32/120    avg_loss:0.052, val_acc:0.869]
Epoch [33/120    avg_loss:0.048, val_acc:0.965]
Epoch [34/120    avg_loss:0.041, val_acc:0.962]
Epoch [35/120    avg_loss:0.064, val_acc:0.952]
Epoch [36/120    avg_loss:0.035, val_acc:0.974]
Epoch [37/120    avg_loss:0.038, val_acc:0.974]
Epoch [38/120    avg_loss:0.020, val_acc:0.983]
Epoch [39/120    avg_loss:0.016, val_acc:0.979]
Epoch [40/120    avg_loss:0.022, val_acc:0.977]
Epoch [41/120    avg_loss:0.014, val_acc:0.975]
Epoch [42/120    avg_loss:0.015, val_acc:0.981]
Epoch [43/120    avg_loss:0.017, val_acc:0.918]
Epoch [44/120    avg_loss:0.053, val_acc:0.958]
Epoch [45/120    avg_loss:0.046, val_acc:0.975]
Epoch [46/120    avg_loss:0.091, val_acc:0.955]
Epoch [47/120    avg_loss:0.034, val_acc:0.956]
Epoch [48/120    avg_loss:0.025, val_acc:0.968]
Epoch [49/120    avg_loss:0.032, val_acc:0.980]
Epoch [50/120    avg_loss:0.017, val_acc:0.977]
Epoch [51/120    avg_loss:0.018, val_acc:0.970]
Epoch [52/120    avg_loss:0.022, val_acc:0.977]
Epoch [53/120    avg_loss:0.016, val_acc:0.980]
Epoch [54/120    avg_loss:0.016, val_acc:0.982]
Epoch [55/120    avg_loss:0.010, val_acc:0.984]
Epoch [56/120    avg_loss:0.012, val_acc:0.984]
Epoch [57/120    avg_loss:0.010, val_acc:0.985]
Epoch [58/120    avg_loss:0.009, val_acc:0.985]
Epoch [59/120    avg_loss:0.009, val_acc:0.985]
Epoch [60/120    avg_loss:0.007, val_acc:0.984]
Epoch [61/120    avg_loss:0.011, val_acc:0.985]
Epoch [62/120    avg_loss:0.009, val_acc:0.985]
Epoch [63/120    avg_loss:0.011, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.986]
Epoch [65/120    avg_loss:0.007, val_acc:0.985]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.012, val_acc:0.986]
Epoch [68/120    avg_loss:0.008, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.985]
Epoch [70/120    avg_loss:0.007, val_acc:0.985]
Epoch [71/120    avg_loss:0.010, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.985]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.006, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.985]
Epoch [93/120    avg_loss:0.010, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     3     0    19     1     2     0]
 [    0     0 18066     0    22     0     1     0     1     0]
 [    0     4     0  2025     3     0     0     0     1     3]
 [    0    30     8     0  2895     0    10     0    29     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4868     0     4     6]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     0     0     2    51     0     0     0  3507    11]
 [    0     0     3     0    18    35     0     0     0   863]]

Accuracy:
99.34205769647893

F1 scores:
[       nan 0.99541676 0.99903227 0.99680039 0.97082495 0.98600076
 0.99570464 0.99805976 0.98580464 0.95570321]

Kappa:
0.9912826418325117
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3606309898>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.594, val_acc:0.711]
Epoch [2/120    avg_loss:0.851, val_acc:0.778]
Epoch [3/120    avg_loss:0.530, val_acc:0.842]
Epoch [4/120    avg_loss:0.466, val_acc:0.849]
Epoch [5/120    avg_loss:0.361, val_acc:0.783]
Epoch [6/120    avg_loss:0.309, val_acc:0.887]
Epoch [7/120    avg_loss:0.222, val_acc:0.908]
Epoch [8/120    avg_loss:0.231, val_acc:0.941]
Epoch [9/120    avg_loss:0.176, val_acc:0.920]
Epoch [10/120    avg_loss:0.228, val_acc:0.899]
Epoch [11/120    avg_loss:0.187, val_acc:0.908]
Epoch [12/120    avg_loss:0.141, val_acc:0.905]
Epoch [13/120    avg_loss:0.130, val_acc:0.951]
Epoch [14/120    avg_loss:0.100, val_acc:0.941]
Epoch [15/120    avg_loss:0.107, val_acc:0.932]
Epoch [16/120    avg_loss:0.075, val_acc:0.964]
Epoch [17/120    avg_loss:0.130, val_acc:0.962]
Epoch [18/120    avg_loss:0.119, val_acc:0.932]
Epoch [19/120    avg_loss:0.129, val_acc:0.950]
Epoch [20/120    avg_loss:0.073, val_acc:0.972]
Epoch [21/120    avg_loss:0.059, val_acc:0.896]
Epoch [22/120    avg_loss:0.062, val_acc:0.964]
Epoch [23/120    avg_loss:0.050, val_acc:0.948]
Epoch [24/120    avg_loss:0.035, val_acc:0.973]
Epoch [25/120    avg_loss:0.061, val_acc:0.963]
Epoch [26/120    avg_loss:0.034, val_acc:0.980]
Epoch [27/120    avg_loss:0.038, val_acc:0.969]
Epoch [28/120    avg_loss:0.030, val_acc:0.967]
Epoch [29/120    avg_loss:0.029, val_acc:0.973]
Epoch [30/120    avg_loss:0.069, val_acc:0.933]
Epoch [31/120    avg_loss:0.033, val_acc:0.982]
Epoch [32/120    avg_loss:0.021, val_acc:0.967]
Epoch [33/120    avg_loss:0.052, val_acc:0.947]
Epoch [34/120    avg_loss:0.051, val_acc:0.977]
Epoch [35/120    avg_loss:0.021, val_acc:0.981]
Epoch [36/120    avg_loss:0.031, val_acc:0.963]
Epoch [37/120    avg_loss:0.046, val_acc:0.978]
Epoch [38/120    avg_loss:0.039, val_acc:0.952]
Epoch [39/120    avg_loss:0.021, val_acc:0.984]
Epoch [40/120    avg_loss:0.014, val_acc:0.985]
Epoch [41/120    avg_loss:0.012, val_acc:0.980]
Epoch [42/120    avg_loss:0.010, val_acc:0.989]
Epoch [43/120    avg_loss:0.012, val_acc:0.972]
Epoch [44/120    avg_loss:0.008, val_acc:0.989]
Epoch [45/120    avg_loss:0.010, val_acc:0.985]
Epoch [46/120    avg_loss:0.023, val_acc:0.981]
Epoch [47/120    avg_loss:0.016, val_acc:0.984]
Epoch [48/120    avg_loss:0.012, val_acc:0.988]
Epoch [49/120    avg_loss:0.006, val_acc:0.986]
Epoch [50/120    avg_loss:0.009, val_acc:0.985]
Epoch [51/120    avg_loss:0.010, val_acc:0.988]
Epoch [52/120    avg_loss:0.010, val_acc:0.988]
Epoch [53/120    avg_loss:0.012, val_acc:0.988]
Epoch [54/120    avg_loss:0.008, val_acc:0.985]
Epoch [55/120    avg_loss:0.007, val_acc:0.990]
Epoch [56/120    avg_loss:0.004, val_acc:0.989]
Epoch [57/120    avg_loss:0.013, val_acc:0.987]
Epoch [58/120    avg_loss:0.009, val_acc:0.991]
Epoch [59/120    avg_loss:0.018, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.985]
Epoch [61/120    avg_loss:0.014, val_acc:0.989]
Epoch [62/120    avg_loss:0.009, val_acc:0.991]
Epoch [63/120    avg_loss:0.008, val_acc:0.991]
Epoch [64/120    avg_loss:0.006, val_acc:0.987]
Epoch [65/120    avg_loss:0.005, val_acc:0.988]
Epoch [66/120    avg_loss:0.007, val_acc:0.987]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.006, val_acc:0.991]
Epoch [69/120    avg_loss:0.004, val_acc:0.991]
Epoch [70/120    avg_loss:0.007, val_acc:0.983]
Epoch [71/120    avg_loss:0.006, val_acc:0.991]
Epoch [72/120    avg_loss:0.009, val_acc:0.989]
Epoch [73/120    avg_loss:0.010, val_acc:0.988]
Epoch [74/120    avg_loss:0.005, val_acc:0.991]
Epoch [75/120    avg_loss:0.005, val_acc:0.991]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.990]
Epoch [78/120    avg_loss:0.008, val_acc:0.987]
Epoch [79/120    avg_loss:0.008, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.991]
Epoch [81/120    avg_loss:0.005, val_acc:0.991]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.003, val_acc:0.990]
Epoch [84/120    avg_loss:0.004, val_acc:0.991]
Epoch [85/120    avg_loss:0.003, val_acc:0.992]
Epoch [86/120    avg_loss:0.004, val_acc:0.991]
Epoch [87/120    avg_loss:0.014, val_acc:0.981]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.003, val_acc:0.988]
Epoch [91/120    avg_loss:0.002, val_acc:0.992]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.003, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.992]
Epoch [96/120    avg_loss:0.003, val_acc:0.993]
Epoch [97/120    avg_loss:0.003, val_acc:0.991]
Epoch [98/120    avg_loss:0.002, val_acc:0.994]
Epoch [99/120    avg_loss:0.003, val_acc:0.992]
Epoch [100/120    avg_loss:0.002, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.992]
Epoch [102/120    avg_loss:0.002, val_acc:0.994]
Epoch [103/120    avg_loss:0.002, val_acc:0.994]
Epoch [104/120    avg_loss:0.001, val_acc:0.993]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.993]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.983]
Epoch [111/120    avg_loss:0.003, val_acc:0.993]
Epoch [112/120    avg_loss:0.002, val_acc:0.995]
Epoch [113/120    avg_loss:0.002, val_acc:0.993]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.015, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.991]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     0     0     0     0    11     0     0]
 [    0     0 18075     0    15     0     0     0     0     0]
 [    0     0     0  2033     1     0     0     0     0     2]
 [    0    24    14     0  2911     0     0     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4877     0     0     1]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0    13     0     0    62     0     0     0  3494     2]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.4818403104138

F1 scores:
[       nan 0.99627618 0.99919843 0.99926272 0.97439331 0.98901099
 0.99948765 0.99420178 0.98589165 0.97333333]

Kappa:
0.9931337069167913
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94fd3307f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.522, val_acc:0.353]
Epoch [2/120    avg_loss:0.932, val_acc:0.617]
Epoch [3/120    avg_loss:0.647, val_acc:0.675]
Epoch [4/120    avg_loss:0.465, val_acc:0.704]
Epoch [5/120    avg_loss:0.369, val_acc:0.768]
Epoch [6/120    avg_loss:0.322, val_acc:0.796]
Epoch [7/120    avg_loss:0.248, val_acc:0.910]
Epoch [8/120    avg_loss:0.212, val_acc:0.924]
Epoch [9/120    avg_loss:0.206, val_acc:0.889]
Epoch [10/120    avg_loss:0.179, val_acc:0.887]
Epoch [11/120    avg_loss:0.158, val_acc:0.913]
Epoch [12/120    avg_loss:0.150, val_acc:0.933]
Epoch [13/120    avg_loss:0.143, val_acc:0.915]
Epoch [14/120    avg_loss:0.161, val_acc:0.931]
Epoch [15/120    avg_loss:0.130, val_acc:0.937]
Epoch [16/120    avg_loss:0.120, val_acc:0.928]
Epoch [17/120    avg_loss:0.105, val_acc:0.957]
Epoch [18/120    avg_loss:0.059, val_acc:0.935]
Epoch [19/120    avg_loss:0.064, val_acc:0.956]
Epoch [20/120    avg_loss:0.065, val_acc:0.966]
Epoch [21/120    avg_loss:0.042, val_acc:0.962]
Epoch [22/120    avg_loss:0.037, val_acc:0.960]
Epoch [23/120    avg_loss:0.053, val_acc:0.976]
Epoch [24/120    avg_loss:0.118, val_acc:0.853]
Epoch [25/120    avg_loss:0.116, val_acc:0.926]
Epoch [26/120    avg_loss:0.058, val_acc:0.973]
Epoch [27/120    avg_loss:0.035, val_acc:0.969]
Epoch [28/120    avg_loss:0.029, val_acc:0.966]
Epoch [29/120    avg_loss:0.058, val_acc:0.970]
Epoch [30/120    avg_loss:0.041, val_acc:0.969]
Epoch [31/120    avg_loss:0.046, val_acc:0.975]
Epoch [32/120    avg_loss:0.048, val_acc:0.969]
Epoch [33/120    avg_loss:0.029, val_acc:0.973]
Epoch [34/120    avg_loss:0.043, val_acc:0.977]
Epoch [35/120    avg_loss:0.025, val_acc:0.969]
Epoch [36/120    avg_loss:0.015, val_acc:0.980]
Epoch [37/120    avg_loss:0.017, val_acc:0.982]
Epoch [38/120    avg_loss:0.028, val_acc:0.979]
Epoch [39/120    avg_loss:0.024, val_acc:0.979]
Epoch [40/120    avg_loss:0.026, val_acc:0.987]
Epoch [41/120    avg_loss:0.010, val_acc:0.985]
Epoch [42/120    avg_loss:0.015, val_acc:0.980]
Epoch [43/120    avg_loss:0.029, val_acc:0.962]
Epoch [44/120    avg_loss:0.027, val_acc:0.978]
Epoch [45/120    avg_loss:0.022, val_acc:0.976]
Epoch [46/120    avg_loss:0.026, val_acc:0.985]
Epoch [47/120    avg_loss:0.014, val_acc:0.990]
Epoch [48/120    avg_loss:0.014, val_acc:0.984]
Epoch [49/120    avg_loss:0.013, val_acc:0.986]
Epoch [50/120    avg_loss:0.013, val_acc:0.979]
Epoch [51/120    avg_loss:0.011, val_acc:0.989]
Epoch [52/120    avg_loss:0.014, val_acc:0.990]
Epoch [53/120    avg_loss:0.006, val_acc:0.991]
Epoch [54/120    avg_loss:0.006, val_acc:0.988]
Epoch [55/120    avg_loss:0.017, val_acc:0.966]
Epoch [56/120    avg_loss:0.017, val_acc:0.975]
Epoch [57/120    avg_loss:0.015, val_acc:0.985]
Epoch [58/120    avg_loss:0.009, val_acc:0.981]
Epoch [59/120    avg_loss:0.010, val_acc:0.986]
Epoch [60/120    avg_loss:0.011, val_acc:0.982]
Epoch [61/120    avg_loss:0.025, val_acc:0.972]
Epoch [62/120    avg_loss:0.016, val_acc:0.971]
Epoch [63/120    avg_loss:0.009, val_acc:0.983]
Epoch [64/120    avg_loss:0.007, val_acc:0.986]
Epoch [65/120    avg_loss:0.006, val_acc:0.987]
Epoch [66/120    avg_loss:0.025, val_acc:0.979]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.005, val_acc:0.985]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.006, val_acc:0.987]
Epoch [71/120    avg_loss:0.006, val_acc:0.987]
Epoch [72/120    avg_loss:0.006, val_acc:0.986]
Epoch [73/120    avg_loss:0.005, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.987]
Epoch [75/120    avg_loss:0.006, val_acc:0.986]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.004, val_acc:0.989]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.004, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.005, val_acc:0.988]
Epoch [84/120    avg_loss:0.004, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.004, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6345     0     0     0     0    18    25    30    14]
 [    0     0 18056     0    24     0     8     0     2     0]
 [    0     5     0  2021     0     0     0     0     1     9]
 [    0    34    11     0  2900     0     0     0    25     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4868     0     9     1]
 [    0     0     0     0     0     0     5  1285     0     0]
 [    0     4     0     1    51     0     0     0  3503    12]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.19263490227267

F1 scores:
[       nan 0.98985959 0.99875543 0.99605717 0.97299111 0.98863636
 0.99580648 0.98846154 0.98109508 0.95524017]

Kappa:
0.9893061943081031
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9843d2a7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.592, val_acc:0.383]
Epoch [2/120    avg_loss:0.998, val_acc:0.621]
Epoch [3/120    avg_loss:0.736, val_acc:0.632]
Epoch [4/120    avg_loss:0.544, val_acc:0.752]
Epoch [5/120    avg_loss:0.449, val_acc:0.722]
Epoch [6/120    avg_loss:0.432, val_acc:0.838]
Epoch [7/120    avg_loss:0.342, val_acc:0.847]
Epoch [8/120    avg_loss:0.371, val_acc:0.848]
Epoch [9/120    avg_loss:0.254, val_acc:0.857]
Epoch [10/120    avg_loss:0.194, val_acc:0.880]
Epoch [11/120    avg_loss:0.195, val_acc:0.881]
Epoch [12/120    avg_loss:0.204, val_acc:0.894]
Epoch [13/120    avg_loss:0.158, val_acc:0.909]
Epoch [14/120    avg_loss:0.133, val_acc:0.948]
Epoch [15/120    avg_loss:0.111, val_acc:0.914]
Epoch [16/120    avg_loss:0.135, val_acc:0.950]
Epoch [17/120    avg_loss:0.103, val_acc:0.959]
Epoch [18/120    avg_loss:0.097, val_acc:0.952]
Epoch [19/120    avg_loss:0.097, val_acc:0.970]
Epoch [20/120    avg_loss:0.069, val_acc:0.967]
Epoch [21/120    avg_loss:0.077, val_acc:0.976]
Epoch [22/120    avg_loss:0.082, val_acc:0.942]
Epoch [23/120    avg_loss:0.061, val_acc:0.976]
Epoch [24/120    avg_loss:0.056, val_acc:0.975]
Epoch [25/120    avg_loss:0.072, val_acc:0.966]
Epoch [26/120    avg_loss:0.064, val_acc:0.919]
Epoch [27/120    avg_loss:0.040, val_acc:0.957]
Epoch [28/120    avg_loss:0.031, val_acc:0.969]
Epoch [29/120    avg_loss:0.033, val_acc:0.981]
Epoch [30/120    avg_loss:0.028, val_acc:0.968]
Epoch [31/120    avg_loss:0.022, val_acc:0.983]
Epoch [32/120    avg_loss:0.034, val_acc:0.949]
Epoch [33/120    avg_loss:0.036, val_acc:0.977]
Epoch [34/120    avg_loss:0.017, val_acc:0.986]
Epoch [35/120    avg_loss:0.025, val_acc:0.978]
Epoch [36/120    avg_loss:0.026, val_acc:0.975]
Epoch [37/120    avg_loss:0.024, val_acc:0.988]
Epoch [38/120    avg_loss:0.020, val_acc:0.989]
Epoch [39/120    avg_loss:0.014, val_acc:0.988]
Epoch [40/120    avg_loss:0.028, val_acc:0.986]
Epoch [41/120    avg_loss:0.038, val_acc:0.989]
Epoch [42/120    avg_loss:0.015, val_acc:0.981]
Epoch [43/120    avg_loss:0.026, val_acc:0.974]
Epoch [44/120    avg_loss:0.026, val_acc:0.982]
Epoch [45/120    avg_loss:0.028, val_acc:0.974]
Epoch [46/120    avg_loss:0.038, val_acc:0.990]
Epoch [47/120    avg_loss:0.015, val_acc:0.992]
Epoch [48/120    avg_loss:0.010, val_acc:0.992]
Epoch [49/120    avg_loss:0.025, val_acc:0.987]
Epoch [50/120    avg_loss:0.010, val_acc:0.991]
Epoch [51/120    avg_loss:0.009, val_acc:0.989]
Epoch [52/120    avg_loss:0.011, val_acc:0.990]
Epoch [53/120    avg_loss:0.021, val_acc:0.992]
Epoch [54/120    avg_loss:0.010, val_acc:0.992]
Epoch [55/120    avg_loss:0.006, val_acc:0.994]
Epoch [56/120    avg_loss:0.009, val_acc:0.987]
Epoch [57/120    avg_loss:0.053, val_acc:0.991]
Epoch [58/120    avg_loss:0.033, val_acc:0.962]
Epoch [59/120    avg_loss:0.019, val_acc:0.988]
Epoch [60/120    avg_loss:0.009, val_acc:0.989]
Epoch [61/120    avg_loss:0.011, val_acc:0.985]
Epoch [62/120    avg_loss:0.008, val_acc:0.987]
Epoch [63/120    avg_loss:0.007, val_acc:0.991]
Epoch [64/120    avg_loss:0.013, val_acc:0.987]
Epoch [65/120    avg_loss:0.010, val_acc:0.968]
Epoch [66/120    avg_loss:0.008, val_acc:0.992]
Epoch [67/120    avg_loss:0.009, val_acc:0.989]
Epoch [68/120    avg_loss:0.020, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.990]
Epoch [71/120    avg_loss:0.007, val_acc:0.989]
Epoch [72/120    avg_loss:0.009, val_acc:0.989]
Epoch [73/120    avg_loss:0.005, val_acc:0.990]
Epoch [74/120    avg_loss:0.005, val_acc:0.990]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.004, val_acc:0.990]
Epoch [78/120    avg_loss:0.007, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.991]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.005, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.991]
Epoch [84/120    avg_loss:0.005, val_acc:0.991]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.008, val_acc:0.992]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.005, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.006, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.991]
Epoch [111/120    avg_loss:0.008, val_acc:0.991]
Epoch [112/120    avg_loss:0.004, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.006, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     2     0    19    13     0     0]
 [    0     1 18059     0    25     0     3     0     2     0]
 [    0     0     0  2019     1     0     0     0     5    11]
 [    0    22    20     0  2899     0     4     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4861     0    15     1]
 [    0     0     0     0     0     0     6  1284     0     0]
 [    0     1     0    29    61     0     0     0  3454    26]
 [    0     0     0     0    14    42     0     0     0   863]]

Accuracy:
99.15407418118718

F1 scores:
[       nan 0.99548779 0.99856234 0.98873653 0.970539   0.9841629
 0.99498516 0.99265559 0.97653379 0.94835165]

Kappa:
0.9887931626231298
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff5a90937b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.572, val_acc:0.347]
Epoch [2/120    avg_loss:0.962, val_acc:0.695]
Epoch [3/120    avg_loss:0.662, val_acc:0.759]
Epoch [4/120    avg_loss:0.535, val_acc:0.813]
Epoch [5/120    avg_loss:0.419, val_acc:0.828]
Epoch [6/120    avg_loss:0.306, val_acc:0.834]
Epoch [7/120    avg_loss:0.299, val_acc:0.896]
Epoch [8/120    avg_loss:0.244, val_acc:0.872]
Epoch [9/120    avg_loss:0.235, val_acc:0.879]
Epoch [10/120    avg_loss:0.213, val_acc:0.895]
Epoch [11/120    avg_loss:0.193, val_acc:0.909]
Epoch [12/120    avg_loss:0.200, val_acc:0.909]
Epoch [13/120    avg_loss:0.133, val_acc:0.883]
Epoch [14/120    avg_loss:0.177, val_acc:0.929]
Epoch [15/120    avg_loss:0.110, val_acc:0.969]
Epoch [16/120    avg_loss:0.066, val_acc:0.965]
Epoch [17/120    avg_loss:0.085, val_acc:0.963]
Epoch [18/120    avg_loss:0.090, val_acc:0.930]
Epoch [19/120    avg_loss:0.100, val_acc:0.961]
Epoch [20/120    avg_loss:0.094, val_acc:0.940]
Epoch [21/120    avg_loss:0.109, val_acc:0.974]
Epoch [22/120    avg_loss:0.071, val_acc:0.956]
Epoch [23/120    avg_loss:0.048, val_acc:0.954]
Epoch [24/120    avg_loss:0.035, val_acc:0.981]
Epoch [25/120    avg_loss:0.054, val_acc:0.949]
Epoch [26/120    avg_loss:0.042, val_acc:0.984]
Epoch [27/120    avg_loss:0.032, val_acc:0.976]
Epoch [28/120    avg_loss:0.033, val_acc:0.950]
Epoch [29/120    avg_loss:0.030, val_acc:0.984]
Epoch [30/120    avg_loss:0.029, val_acc:0.979]
Epoch [31/120    avg_loss:0.027, val_acc:0.991]
Epoch [32/120    avg_loss:0.064, val_acc:0.965]
Epoch [33/120    avg_loss:0.050, val_acc:0.967]
Epoch [34/120    avg_loss:0.029, val_acc:0.976]
Epoch [35/120    avg_loss:0.039, val_acc:0.983]
Epoch [36/120    avg_loss:0.043, val_acc:0.980]
Epoch [37/120    avg_loss:0.028, val_acc:0.979]
Epoch [38/120    avg_loss:0.024, val_acc:0.985]
Epoch [39/120    avg_loss:0.023, val_acc:0.987]
Epoch [40/120    avg_loss:0.012, val_acc:0.988]
Epoch [41/120    avg_loss:0.014, val_acc:0.993]
Epoch [42/120    avg_loss:0.030, val_acc:0.981]
Epoch [43/120    avg_loss:0.013, val_acc:0.986]
Epoch [44/120    avg_loss:0.012, val_acc:0.989]
Epoch [45/120    avg_loss:0.012, val_acc:0.989]
Epoch [46/120    avg_loss:0.011, val_acc:0.985]
Epoch [47/120    avg_loss:0.024, val_acc:0.971]
Epoch [48/120    avg_loss:0.023, val_acc:0.981]
Epoch [49/120    avg_loss:0.013, val_acc:0.991]
Epoch [50/120    avg_loss:0.008, val_acc:0.993]
Epoch [51/120    avg_loss:0.010, val_acc:0.993]
Epoch [52/120    avg_loss:0.007, val_acc:0.988]
Epoch [53/120    avg_loss:0.019, val_acc:0.993]
Epoch [54/120    avg_loss:0.012, val_acc:0.995]
Epoch [55/120    avg_loss:0.016, val_acc:0.990]
Epoch [56/120    avg_loss:0.040, val_acc:0.986]
Epoch [57/120    avg_loss:0.021, val_acc:0.987]
Epoch [58/120    avg_loss:0.013, val_acc:0.993]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.008, val_acc:0.994]
Epoch [61/120    avg_loss:0.010, val_acc:0.985]
Epoch [62/120    avg_loss:0.007, val_acc:0.995]
Epoch [63/120    avg_loss:0.006, val_acc:0.991]
Epoch [64/120    avg_loss:0.008, val_acc:0.993]
Epoch [65/120    avg_loss:0.007, val_acc:0.994]
Epoch [66/120    avg_loss:0.004, val_acc:0.992]
Epoch [67/120    avg_loss:0.007, val_acc:0.991]
Epoch [68/120    avg_loss:0.006, val_acc:0.993]
Epoch [69/120    avg_loss:0.009, val_acc:0.990]
Epoch [70/120    avg_loss:0.004, val_acc:0.993]
Epoch [71/120    avg_loss:0.004, val_acc:0.993]
Epoch [72/120    avg_loss:0.004, val_acc:0.997]
Epoch [73/120    avg_loss:0.005, val_acc:0.996]
Epoch [74/120    avg_loss:0.005, val_acc:0.991]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.008, val_acc:0.991]
Epoch [78/120    avg_loss:0.004, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.990]
Epoch [80/120    avg_loss:0.011, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.994]
Epoch [82/120    avg_loss:0.011, val_acc:0.990]
Epoch [83/120    avg_loss:0.004, val_acc:0.994]
Epoch [84/120    avg_loss:0.009, val_acc:0.993]
Epoch [85/120    avg_loss:0.004, val_acc:0.991]
Epoch [86/120    avg_loss:0.003, val_acc:0.992]
Epoch [87/120    avg_loss:0.003, val_acc:0.993]
Epoch [88/120    avg_loss:0.005, val_acc:0.993]
Epoch [89/120    avg_loss:0.004, val_acc:0.993]
Epoch [90/120    avg_loss:0.004, val_acc:0.993]
Epoch [91/120    avg_loss:0.010, val_acc:0.991]
Epoch [92/120    avg_loss:0.004, val_acc:0.992]
Epoch [93/120    avg_loss:0.004, val_acc:0.992]
Epoch [94/120    avg_loss:0.003, val_acc:0.992]
Epoch [95/120    avg_loss:0.004, val_acc:0.993]
Epoch [96/120    avg_loss:0.004, val_acc:0.994]
Epoch [97/120    avg_loss:0.002, val_acc:0.994]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.003, val_acc:0.994]
Epoch [100/120    avg_loss:0.003, val_acc:0.994]
Epoch [101/120    avg_loss:0.003, val_acc:0.994]
Epoch [102/120    avg_loss:0.003, val_acc:0.994]
Epoch [103/120    avg_loss:0.004, val_acc:0.994]
Epoch [104/120    avg_loss:0.003, val_acc:0.994]
Epoch [105/120    avg_loss:0.003, val_acc:0.994]
Epoch [106/120    avg_loss:0.003, val_acc:0.994]
Epoch [107/120    avg_loss:0.005, val_acc:0.994]
Epoch [108/120    avg_loss:0.003, val_acc:0.994]
Epoch [109/120    avg_loss:0.003, val_acc:0.994]
Epoch [110/120    avg_loss:0.003, val_acc:0.994]
Epoch [111/120    avg_loss:0.003, val_acc:0.994]
Epoch [112/120    avg_loss:0.004, val_acc:0.994]
Epoch [113/120    avg_loss:0.002, val_acc:0.994]
Epoch [114/120    avg_loss:0.003, val_acc:0.994]
Epoch [115/120    avg_loss:0.003, val_acc:0.994]
Epoch [116/120    avg_loss:0.003, val_acc:0.994]
Epoch [117/120    avg_loss:0.005, val_acc:0.994]
Epoch [118/120    avg_loss:0.004, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.003, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     0     0     0    16    11     2     0]
 [    0     0 18033     0    43     0    10     0     4     0]
 [    0     0     0  2030     1     0     0     0     3     2]
 [    0    35     9     0  2901     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4861     0     6    11]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     0     0     4    56     0     0     0  3500    11]
 [    0     0     0     2    15    26     0     0     0   876]]

Accuracy:
99.2842166148507

F1 scores:
[       nan 0.9950272  0.99817336 0.99705305 0.96893788 0.99013657
 0.99539265 0.99459042 0.98411359 0.96263736]

Kappa:
0.9905206990434172
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d4610e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.640, val_acc:0.643]
Epoch [2/120    avg_loss:1.005, val_acc:0.690]
Epoch [3/120    avg_loss:0.776, val_acc:0.737]
Epoch [4/120    avg_loss:0.574, val_acc:0.791]
Epoch [5/120    avg_loss:0.473, val_acc:0.762]
Epoch [6/120    avg_loss:0.358, val_acc:0.857]
Epoch [7/120    avg_loss:0.348, val_acc:0.867]
Epoch [8/120    avg_loss:0.262, val_acc:0.895]
Epoch [9/120    avg_loss:0.207, val_acc:0.939]
Epoch [10/120    avg_loss:0.220, val_acc:0.941]
Epoch [11/120    avg_loss:0.199, val_acc:0.938]
Epoch [12/120    avg_loss:0.176, val_acc:0.908]
Epoch [13/120    avg_loss:0.170, val_acc:0.937]
Epoch [14/120    avg_loss:0.163, val_acc:0.959]
Epoch [15/120    avg_loss:0.140, val_acc:0.921]
Epoch [16/120    avg_loss:0.095, val_acc:0.944]
Epoch [17/120    avg_loss:0.081, val_acc:0.951]
Epoch [18/120    avg_loss:0.064, val_acc:0.965]
Epoch [19/120    avg_loss:0.086, val_acc:0.965]
Epoch [20/120    avg_loss:0.056, val_acc:0.971]
Epoch [21/120    avg_loss:0.063, val_acc:0.960]
Epoch [22/120    avg_loss:0.110, val_acc:0.963]
Epoch [23/120    avg_loss:0.057, val_acc:0.961]
Epoch [24/120    avg_loss:0.048, val_acc:0.976]
Epoch [25/120    avg_loss:0.056, val_acc:0.949]
Epoch [26/120    avg_loss:0.091, val_acc:0.965]
Epoch [27/120    avg_loss:0.064, val_acc:0.954]
Epoch [28/120    avg_loss:0.036, val_acc:0.948]
Epoch [29/120    avg_loss:0.042, val_acc:0.980]
Epoch [30/120    avg_loss:0.026, val_acc:0.983]
Epoch [31/120    avg_loss:0.031, val_acc:0.976]
Epoch [32/120    avg_loss:0.043, val_acc:0.976]
Epoch [33/120    avg_loss:0.041, val_acc:0.969]
Epoch [34/120    avg_loss:0.027, val_acc:0.982]
Epoch [35/120    avg_loss:0.023, val_acc:0.981]
Epoch [36/120    avg_loss:0.026, val_acc:0.985]
Epoch [37/120    avg_loss:0.012, val_acc:0.986]
Epoch [38/120    avg_loss:0.015, val_acc:0.984]
Epoch [39/120    avg_loss:0.024, val_acc:0.949]
Epoch [40/120    avg_loss:0.023, val_acc:0.986]
Epoch [41/120    avg_loss:0.016, val_acc:0.986]
Epoch [42/120    avg_loss:0.029, val_acc:0.962]
Epoch [43/120    avg_loss:0.013, val_acc:0.990]
Epoch [44/120    avg_loss:0.021, val_acc:0.988]
Epoch [45/120    avg_loss:0.013, val_acc:0.990]
Epoch [46/120    avg_loss:0.041, val_acc:0.977]
Epoch [47/120    avg_loss:0.035, val_acc:0.974]
Epoch [48/120    avg_loss:0.018, val_acc:0.990]
Epoch [49/120    avg_loss:0.010, val_acc:0.993]
Epoch [50/120    avg_loss:0.013, val_acc:0.981]
Epoch [51/120    avg_loss:0.014, val_acc:0.989]
Epoch [52/120    avg_loss:0.009, val_acc:0.984]
Epoch [53/120    avg_loss:0.024, val_acc:0.989]
Epoch [54/120    avg_loss:0.056, val_acc:0.968]
Epoch [55/120    avg_loss:0.051, val_acc:0.987]
Epoch [56/120    avg_loss:0.027, val_acc:0.980]
Epoch [57/120    avg_loss:0.041, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.984]
Epoch [59/120    avg_loss:0.019, val_acc:0.974]
Epoch [60/120    avg_loss:0.010, val_acc:0.991]
Epoch [61/120    avg_loss:0.010, val_acc:0.986]
Epoch [62/120    avg_loss:0.009, val_acc:0.984]
Epoch [63/120    avg_loss:0.008, val_acc:0.989]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.005, val_acc:0.987]
Epoch [66/120    avg_loss:0.009, val_acc:0.990]
Epoch [67/120    avg_loss:0.011, val_acc:0.991]
Epoch [68/120    avg_loss:0.007, val_acc:0.991]
Epoch [69/120    avg_loss:0.009, val_acc:0.991]
Epoch [70/120    avg_loss:0.005, val_acc:0.990]
Epoch [71/120    avg_loss:0.005, val_acc:0.992]
Epoch [72/120    avg_loss:0.004, val_acc:0.991]
Epoch [73/120    avg_loss:0.005, val_acc:0.992]
Epoch [74/120    avg_loss:0.008, val_acc:0.991]
Epoch [75/120    avg_loss:0.006, val_acc:0.992]
Epoch [76/120    avg_loss:0.006, val_acc:0.992]
Epoch [77/120    avg_loss:0.005, val_acc:0.992]
Epoch [78/120    avg_loss:0.006, val_acc:0.992]
Epoch [79/120    avg_loss:0.006, val_acc:0.992]
Epoch [80/120    avg_loss:0.004, val_acc:0.992]
Epoch [81/120    avg_loss:0.006, val_acc:0.992]
Epoch [82/120    avg_loss:0.008, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.005, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.009, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.006, val_acc:0.992]
Epoch [89/120    avg_loss:0.004, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.007, val_acc:0.992]
Epoch [92/120    avg_loss:0.008, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.006, val_acc:0.992]
Epoch [99/120    avg_loss:0.005, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.992]
Epoch [104/120    avg_loss:0.006, val_acc:0.992]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.006, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.992]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.007, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.006, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6383     0     0     2     0    14    12    21     0]
 [    0     0 18047     0    31     0     8     0     4     0]
 [    0     0     0  2019     3     0     0     0    10     4]
 [    0    41    18     0  2888     0     0     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     1     4]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     1     0    12    52     0     0     0  3488    18]
 [    0     0     0     0    15    31     0     0     0   873]]

Accuracy:
99.20468512761188

F1 scores:
[       nan 0.99292214 0.99831282 0.99286944 0.96863995 0.98826202
 0.99693126 0.99420626 0.97977528 0.96039604]

Kappa:
0.9894648270819106
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d0eaa6898>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.578, val_acc:0.537]
Epoch [2/120    avg_loss:0.920, val_acc:0.741]
Epoch [3/120    avg_loss:0.667, val_acc:0.658]
Epoch [4/120    avg_loss:0.470, val_acc:0.810]
Epoch [5/120    avg_loss:0.458, val_acc:0.773]
Epoch [6/120    avg_loss:0.364, val_acc:0.872]
Epoch [7/120    avg_loss:0.275, val_acc:0.873]
Epoch [8/120    avg_loss:0.273, val_acc:0.897]
Epoch [9/120    avg_loss:0.273, val_acc:0.910]
Epoch [10/120    avg_loss:0.208, val_acc:0.919]
Epoch [11/120    avg_loss:0.203, val_acc:0.908]
Epoch [12/120    avg_loss:0.179, val_acc:0.884]
Epoch [13/120    avg_loss:0.158, val_acc:0.902]
Epoch [14/120    avg_loss:0.166, val_acc:0.949]
Epoch [15/120    avg_loss:0.112, val_acc:0.929]
Epoch [16/120    avg_loss:0.104, val_acc:0.955]
Epoch [17/120    avg_loss:0.087, val_acc:0.968]
Epoch [18/120    avg_loss:0.086, val_acc:0.966]
Epoch [19/120    avg_loss:0.099, val_acc:0.972]
Epoch [20/120    avg_loss:0.078, val_acc:0.910]
Epoch [21/120    avg_loss:0.080, val_acc:0.970]
Epoch [22/120    avg_loss:0.057, val_acc:0.959]
Epoch [23/120    avg_loss:0.051, val_acc:0.975]
Epoch [24/120    avg_loss:0.059, val_acc:0.968]
Epoch [25/120    avg_loss:0.045, val_acc:0.977]
Epoch [26/120    avg_loss:0.049, val_acc:0.973]
Epoch [27/120    avg_loss:0.045, val_acc:0.973]
Epoch [28/120    avg_loss:0.057, val_acc:0.977]
Epoch [29/120    avg_loss:0.049, val_acc:0.946]
Epoch [30/120    avg_loss:0.055, val_acc:0.968]
Epoch [31/120    avg_loss:0.067, val_acc:0.976]
Epoch [32/120    avg_loss:0.041, val_acc:0.985]
Epoch [33/120    avg_loss:0.043, val_acc:0.975]
Epoch [34/120    avg_loss:0.019, val_acc:0.986]
Epoch [35/120    avg_loss:0.015, val_acc:0.986]
Epoch [36/120    avg_loss:0.023, val_acc:0.989]
Epoch [37/120    avg_loss:0.019, val_acc:0.990]
Epoch [38/120    avg_loss:0.015, val_acc:0.989]
Epoch [39/120    avg_loss:0.022, val_acc:0.989]
Epoch [40/120    avg_loss:0.010, val_acc:0.990]
Epoch [41/120    avg_loss:0.010, val_acc:0.987]
Epoch [42/120    avg_loss:0.037, val_acc:0.985]
Epoch [43/120    avg_loss:0.019, val_acc:0.986]
Epoch [44/120    avg_loss:0.014, val_acc:0.985]
Epoch [45/120    avg_loss:0.008, val_acc:0.990]
Epoch [46/120    avg_loss:0.007, val_acc:0.992]
Epoch [47/120    avg_loss:0.006, val_acc:0.988]
Epoch [48/120    avg_loss:0.007, val_acc:0.991]
Epoch [49/120    avg_loss:0.023, val_acc:0.978]
Epoch [50/120    avg_loss:0.024, val_acc:0.970]
Epoch [51/120    avg_loss:0.027, val_acc:0.985]
Epoch [52/120    avg_loss:0.048, val_acc:0.975]
Epoch [53/120    avg_loss:0.026, val_acc:0.970]
Epoch [54/120    avg_loss:0.039, val_acc:0.971]
Epoch [55/120    avg_loss:0.031, val_acc:0.980]
Epoch [56/120    avg_loss:0.020, val_acc:0.978]
Epoch [57/120    avg_loss:0.016, val_acc:0.985]
Epoch [58/120    avg_loss:0.050, val_acc:0.976]
Epoch [59/120    avg_loss:0.023, val_acc:0.986]
Epoch [60/120    avg_loss:0.013, val_acc:0.990]
Epoch [61/120    avg_loss:0.010, val_acc:0.989]
Epoch [62/120    avg_loss:0.009, val_acc:0.990]
Epoch [63/120    avg_loss:0.014, val_acc:0.990]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.006, val_acc:0.990]
Epoch [66/120    avg_loss:0.010, val_acc:0.990]
Epoch [67/120    avg_loss:0.009, val_acc:0.990]
Epoch [68/120    avg_loss:0.007, val_acc:0.989]
Epoch [69/120    avg_loss:0.005, val_acc:0.990]
Epoch [70/120    avg_loss:0.008, val_acc:0.990]
Epoch [71/120    avg_loss:0.007, val_acc:0.989]
Epoch [72/120    avg_loss:0.005, val_acc:0.989]
Epoch [73/120    avg_loss:0.005, val_acc:0.989]
Epoch [74/120    avg_loss:0.006, val_acc:0.989]
Epoch [75/120    avg_loss:0.006, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.008, val_acc:0.990]
Epoch [78/120    avg_loss:0.007, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.007, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.006, val_acc:0.990]
Epoch [83/120    avg_loss:0.006, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.005, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.008, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.009, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6345     0     0     0     0    25    26    36     0]
 [    0     1 18056     0    30     0     3     0     0     0]
 [    0     0     0  2030     0     0     0     0     1     5]
 [    0    28    12     0  2898     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4869     0     0     7]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     2     0     0    54     0     0     0  3508     7]
 [    0     0     0     0    16    31     0     0     0   872]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.99078701 0.99872781 0.99803343 0.97085427 0.98826202
 0.99529845 0.98924731 0.98222036 0.96353591]

Kappa:
0.9897210548526206
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f570a059828>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.548, val_acc:0.328]
Epoch [2/120    avg_loss:0.923, val_acc:0.556]
Epoch [3/120    avg_loss:0.650, val_acc:0.759]
Epoch [4/120    avg_loss:0.501, val_acc:0.809]
Epoch [5/120    avg_loss:0.402, val_acc:0.780]
Epoch [6/120    avg_loss:0.409, val_acc:0.787]
Epoch [7/120    avg_loss:0.314, val_acc:0.886]
Epoch [8/120    avg_loss:0.246, val_acc:0.920]
Epoch [9/120    avg_loss:0.222, val_acc:0.814]
Epoch [10/120    avg_loss:0.240, val_acc:0.935]
Epoch [11/120    avg_loss:0.249, val_acc:0.906]
Epoch [12/120    avg_loss:0.170, val_acc:0.937]
Epoch [13/120    avg_loss:0.124, val_acc:0.944]
Epoch [14/120    avg_loss:0.109, val_acc:0.959]
Epoch [15/120    avg_loss:0.112, val_acc:0.955]
Epoch [16/120    avg_loss:0.109, val_acc:0.930]
Epoch [17/120    avg_loss:0.094, val_acc:0.955]
Epoch [18/120    avg_loss:0.103, val_acc:0.961]
Epoch [19/120    avg_loss:0.087, val_acc:0.941]
Epoch [20/120    avg_loss:0.142, val_acc:0.930]
Epoch [21/120    avg_loss:0.090, val_acc:0.949]
Epoch [22/120    avg_loss:0.084, val_acc:0.940]
Epoch [23/120    avg_loss:0.071, val_acc:0.962]
Epoch [24/120    avg_loss:0.080, val_acc:0.969]
Epoch [25/120    avg_loss:0.125, val_acc:0.927]
Epoch [26/120    avg_loss:0.073, val_acc:0.964]
Epoch [27/120    avg_loss:0.043, val_acc:0.966]
Epoch [28/120    avg_loss:0.050, val_acc:0.971]
Epoch [29/120    avg_loss:0.047, val_acc:0.972]
Epoch [30/120    avg_loss:0.047, val_acc:0.931]
Epoch [31/120    avg_loss:0.040, val_acc:0.961]
Epoch [32/120    avg_loss:0.050, val_acc:0.973]
Epoch [33/120    avg_loss:0.033, val_acc:0.973]
Epoch [34/120    avg_loss:0.027, val_acc:0.975]
Epoch [35/120    avg_loss:0.030, val_acc:0.973]
Epoch [36/120    avg_loss:0.043, val_acc:0.955]
Epoch [37/120    avg_loss:0.035, val_acc:0.975]
Epoch [38/120    avg_loss:0.030, val_acc:0.962]
Epoch [39/120    avg_loss:0.018, val_acc:0.973]
Epoch [40/120    avg_loss:0.016, val_acc:0.982]
Epoch [41/120    avg_loss:0.020, val_acc:0.981]
Epoch [42/120    avg_loss:0.040, val_acc:0.954]
Epoch [43/120    avg_loss:0.033, val_acc:0.967]
Epoch [44/120    avg_loss:0.032, val_acc:0.978]
Epoch [45/120    avg_loss:0.019, val_acc:0.982]
Epoch [46/120    avg_loss:0.019, val_acc:0.985]
Epoch [47/120    avg_loss:0.021, val_acc:0.975]
Epoch [48/120    avg_loss:0.021, val_acc:0.970]
Epoch [49/120    avg_loss:0.040, val_acc:0.969]
Epoch [50/120    avg_loss:0.051, val_acc:0.972]
Epoch [51/120    avg_loss:0.055, val_acc:0.976]
Epoch [52/120    avg_loss:0.038, val_acc:0.972]
Epoch [53/120    avg_loss:0.029, val_acc:0.960]
Epoch [54/120    avg_loss:0.034, val_acc:0.974]
Epoch [55/120    avg_loss:0.033, val_acc:0.973]
Epoch [56/120    avg_loss:0.017, val_acc:0.978]
Epoch [57/120    avg_loss:0.032, val_acc:0.979]
Epoch [58/120    avg_loss:0.017, val_acc:0.976]
Epoch [59/120    avg_loss:0.011, val_acc:0.982]
Epoch [60/120    avg_loss:0.010, val_acc:0.983]
Epoch [61/120    avg_loss:0.007, val_acc:0.983]
Epoch [62/120    avg_loss:0.008, val_acc:0.984]
Epoch [63/120    avg_loss:0.008, val_acc:0.984]
Epoch [64/120    avg_loss:0.007, val_acc:0.985]
Epoch [65/120    avg_loss:0.007, val_acc:0.985]
Epoch [66/120    avg_loss:0.006, val_acc:0.985]
Epoch [67/120    avg_loss:0.009, val_acc:0.986]
Epoch [68/120    avg_loss:0.011, val_acc:0.986]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.010, val_acc:0.984]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.007, val_acc:0.986]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.006, val_acc:0.985]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.004, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.986]
Epoch [82/120    avg_loss:0.005, val_acc:0.986]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.006, val_acc:0.985]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.005, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.005, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.003, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     0     0    10     3    37     0]
 [    0     0 18015     0    35     0    38     0     2     0]
 [    0     0     0  2029     3     0     0     0     0     4]
 [    0    41    22     0  2876     0     6     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4869     0     7     2]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     0     0     7    56     0     0     0  3491    17]
 [    0     0     0     0    16    39     0     0     0   864]]

Accuracy:
99.0986431446268

F1 scores:
[       nan 0.99292104 0.99731503 0.99656189 0.96542464 0.98527746
 0.99336938 0.99806277 0.97855641 0.95681063]

Kappa:
0.9880633490394554
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6e0a437f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.569, val_acc:0.610]
Epoch [2/120    avg_loss:0.985, val_acc:0.577]
Epoch [3/120    avg_loss:0.717, val_acc:0.723]
Epoch [4/120    avg_loss:0.578, val_acc:0.746]
Epoch [5/120    avg_loss:0.445, val_acc:0.828]
Epoch [6/120    avg_loss:0.392, val_acc:0.781]
Epoch [7/120    avg_loss:0.363, val_acc:0.861]
Epoch [8/120    avg_loss:0.319, val_acc:0.881]
Epoch [9/120    avg_loss:0.240, val_acc:0.887]
Epoch [10/120    avg_loss:0.242, val_acc:0.891]
Epoch [11/120    avg_loss:0.212, val_acc:0.888]
Epoch [12/120    avg_loss:0.203, val_acc:0.915]
Epoch [13/120    avg_loss:0.188, val_acc:0.907]
Epoch [14/120    avg_loss:0.128, val_acc:0.924]
Epoch [15/120    avg_loss:0.126, val_acc:0.912]
Epoch [16/120    avg_loss:0.120, val_acc:0.918]
Epoch [17/120    avg_loss:0.123, val_acc:0.937]
Epoch [18/120    avg_loss:0.107, val_acc:0.940]
Epoch [19/120    avg_loss:0.132, val_acc:0.927]
Epoch [20/120    avg_loss:0.137, val_acc:0.916]
Epoch [21/120    avg_loss:0.127, val_acc:0.894]
Epoch [22/120    avg_loss:0.081, val_acc:0.950]
Epoch [23/120    avg_loss:0.060, val_acc:0.961]
Epoch [24/120    avg_loss:0.041, val_acc:0.972]
Epoch [25/120    avg_loss:0.103, val_acc:0.967]
Epoch [26/120    avg_loss:0.039, val_acc:0.965]
Epoch [27/120    avg_loss:0.045, val_acc:0.969]
Epoch [28/120    avg_loss:0.062, val_acc:0.946]
Epoch [29/120    avg_loss:0.041, val_acc:0.962]
Epoch [30/120    avg_loss:0.047, val_acc:0.878]
Epoch [31/120    avg_loss:0.035, val_acc:0.978]
Epoch [32/120    avg_loss:0.051, val_acc:0.967]
Epoch [33/120    avg_loss:0.024, val_acc:0.969]
Epoch [34/120    avg_loss:0.028, val_acc:0.939]
Epoch [35/120    avg_loss:0.036, val_acc:0.972]
Epoch [36/120    avg_loss:0.029, val_acc:0.975]
Epoch [37/120    avg_loss:0.023, val_acc:0.970]
Epoch [38/120    avg_loss:0.038, val_acc:0.975]
Epoch [39/120    avg_loss:0.018, val_acc:0.972]
Epoch [40/120    avg_loss:0.018, val_acc:0.979]
Epoch [41/120    avg_loss:0.026, val_acc:0.978]
Epoch [42/120    avg_loss:0.018, val_acc:0.981]
Epoch [43/120    avg_loss:0.014, val_acc:0.983]
Epoch [44/120    avg_loss:0.014, val_acc:0.982]
Epoch [45/120    avg_loss:0.061, val_acc:0.950]
Epoch [46/120    avg_loss:0.023, val_acc:0.963]
Epoch [47/120    avg_loss:0.020, val_acc:0.981]
Epoch [48/120    avg_loss:0.020, val_acc:0.984]
Epoch [49/120    avg_loss:0.014, val_acc:0.962]
Epoch [50/120    avg_loss:0.024, val_acc:0.978]
Epoch [51/120    avg_loss:0.009, val_acc:0.981]
Epoch [52/120    avg_loss:0.009, val_acc:0.984]
Epoch [53/120    avg_loss:0.014, val_acc:0.979]
Epoch [54/120    avg_loss:0.016, val_acc:0.977]
Epoch [55/120    avg_loss:0.017, val_acc:0.961]
Epoch [56/120    avg_loss:0.019, val_acc:0.980]
Epoch [57/120    avg_loss:0.017, val_acc:0.980]
Epoch [58/120    avg_loss:0.021, val_acc:0.961]
Epoch [59/120    avg_loss:0.018, val_acc:0.987]
Epoch [60/120    avg_loss:0.007, val_acc:0.986]
Epoch [61/120    avg_loss:0.020, val_acc:0.983]
Epoch [62/120    avg_loss:0.014, val_acc:0.985]
Epoch [63/120    avg_loss:0.013, val_acc:0.985]
Epoch [64/120    avg_loss:0.010, val_acc:0.987]
Epoch [65/120    avg_loss:0.010, val_acc:0.985]
Epoch [66/120    avg_loss:0.009, val_acc:0.985]
Epoch [67/120    avg_loss:0.006, val_acc:0.985]
Epoch [68/120    avg_loss:0.010, val_acc:0.980]
Epoch [69/120    avg_loss:0.008, val_acc:0.985]
Epoch [70/120    avg_loss:0.008, val_acc:0.983]
Epoch [71/120    avg_loss:0.003, val_acc:0.985]
Epoch [72/120    avg_loss:0.005, val_acc:0.986]
Epoch [73/120    avg_loss:0.003, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.979]
Epoch [75/120    avg_loss:0.005, val_acc:0.985]
Epoch [76/120    avg_loss:0.004, val_acc:0.987]
Epoch [77/120    avg_loss:0.004, val_acc:0.969]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.977]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.988]
Epoch [83/120    avg_loss:0.003, val_acc:0.988]
Epoch [84/120    avg_loss:0.013, val_acc:0.979]
Epoch [85/120    avg_loss:0.008, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.991]
Epoch [88/120    avg_loss:0.003, val_acc:0.987]
Epoch [89/120    avg_loss:0.003, val_acc:0.987]
Epoch [90/120    avg_loss:0.003, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.985]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.990]
Epoch [98/120    avg_loss:0.003, val_acc:0.989]
Epoch [99/120    avg_loss:0.003, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.989]
Epoch [107/120    avg_loss:0.009, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.985]
Epoch [109/120    avg_loss:0.023, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.988]
Epoch [113/120    avg_loss:0.003, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.002, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.003, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6430     0     0     0     0     0     0     2     0]
 [    0     0 17902     0    43     0   145     0     0     0]
 [    0     1     0  2015     0     0     0     0     6    14]
 [    0    28     8     0  2910     0     0     0    25     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4859     0    18     1]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     1     0     1    53     0     0     0  3494    22]
 [    0     0     0     0    16    32     0     0     0   871]]

Accuracy:
98.99260116164173

F1 scores:
[       nan 0.99751784 0.99455556 0.99457058 0.97097097 0.98788796
 0.98330466 0.99961225 0.98201237 0.95295405]

Kappa:
0.9866760733515231
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffac1a6c7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.636, val_acc:0.371]
Epoch [2/120    avg_loss:1.089, val_acc:0.574]
Epoch [3/120    avg_loss:0.800, val_acc:0.751]
Epoch [4/120    avg_loss:0.573, val_acc:0.829]
Epoch [5/120    avg_loss:0.454, val_acc:0.868]
Epoch [6/120    avg_loss:0.356, val_acc:0.879]
Epoch [7/120    avg_loss:0.334, val_acc:0.901]
Epoch [8/120    avg_loss:0.272, val_acc:0.873]
Epoch [9/120    avg_loss:0.208, val_acc:0.933]
Epoch [10/120    avg_loss:0.226, val_acc:0.907]
Epoch [11/120    avg_loss:0.197, val_acc:0.931]
Epoch [12/120    avg_loss:0.181, val_acc:0.940]
Epoch [13/120    avg_loss:0.148, val_acc:0.884]
Epoch [14/120    avg_loss:0.188, val_acc:0.948]
Epoch [15/120    avg_loss:0.250, val_acc:0.924]
Epoch [16/120    avg_loss:0.185, val_acc:0.919]
Epoch [17/120    avg_loss:0.207, val_acc:0.943]
Epoch [18/120    avg_loss:0.120, val_acc:0.943]
Epoch [19/120    avg_loss:0.086, val_acc:0.973]
Epoch [20/120    avg_loss:0.095, val_acc:0.894]
Epoch [21/120    avg_loss:0.089, val_acc:0.950]
Epoch [22/120    avg_loss:0.072, val_acc:0.960]
Epoch [23/120    avg_loss:0.053, val_acc:0.967]
Epoch [24/120    avg_loss:0.073, val_acc:0.939]
Epoch [25/120    avg_loss:0.061, val_acc:0.865]
Epoch [26/120    avg_loss:0.064, val_acc:0.933]
Epoch [27/120    avg_loss:0.048, val_acc:0.962]
Epoch [28/120    avg_loss:0.056, val_acc:0.951]
Epoch [29/120    avg_loss:0.049, val_acc:0.979]
Epoch [30/120    avg_loss:0.034, val_acc:0.981]
Epoch [31/120    avg_loss:0.037, val_acc:0.967]
Epoch [32/120    avg_loss:0.027, val_acc:0.962]
Epoch [33/120    avg_loss:0.043, val_acc:0.980]
Epoch [34/120    avg_loss:0.026, val_acc:0.974]
Epoch [35/120    avg_loss:0.020, val_acc:0.980]
Epoch [36/120    avg_loss:0.056, val_acc:0.975]
Epoch [37/120    avg_loss:0.044, val_acc:0.970]
Epoch [38/120    avg_loss:0.027, val_acc:0.985]
Epoch [39/120    avg_loss:0.017, val_acc:0.989]
Epoch [40/120    avg_loss:0.030, val_acc:0.941]
Epoch [41/120    avg_loss:0.024, val_acc:0.987]
Epoch [42/120    avg_loss:0.019, val_acc:0.977]
Epoch [43/120    avg_loss:0.011, val_acc:0.989]
Epoch [44/120    avg_loss:0.016, val_acc:0.989]
Epoch [45/120    avg_loss:0.011, val_acc:0.989]
Epoch [46/120    avg_loss:0.010, val_acc:0.986]
Epoch [47/120    avg_loss:0.009, val_acc:0.991]
Epoch [48/120    avg_loss:0.007, val_acc:0.988]
Epoch [49/120    avg_loss:0.016, val_acc:0.985]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.008, val_acc:0.988]
Epoch [52/120    avg_loss:0.009, val_acc:0.989]
Epoch [53/120    avg_loss:0.024, val_acc:0.988]
Epoch [54/120    avg_loss:0.009, val_acc:0.988]
Epoch [55/120    avg_loss:0.007, val_acc:0.974]
Epoch [56/120    avg_loss:0.009, val_acc:0.988]
Epoch [57/120    avg_loss:0.006, val_acc:0.988]
Epoch [58/120    avg_loss:0.008, val_acc:0.985]
Epoch [59/120    avg_loss:0.013, val_acc:0.986]
Epoch [60/120    avg_loss:0.009, val_acc:0.987]
Epoch [61/120    avg_loss:0.008, val_acc:0.987]
Epoch [62/120    avg_loss:0.006, val_acc:0.987]
Epoch [63/120    avg_loss:0.007, val_acc:0.989]
Epoch [64/120    avg_loss:0.007, val_acc:0.989]
Epoch [65/120    avg_loss:0.006, val_acc:0.987]
Epoch [66/120    avg_loss:0.004, val_acc:0.988]
Epoch [67/120    avg_loss:0.005, val_acc:0.987]
Epoch [68/120    avg_loss:0.005, val_acc:0.988]
Epoch [69/120    avg_loss:0.006, val_acc:0.987]
Epoch [70/120    avg_loss:0.004, val_acc:0.987]
Epoch [71/120    avg_loss:0.006, val_acc:0.987]
Epoch [72/120    avg_loss:0.004, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.988]
Epoch [74/120    avg_loss:0.004, val_acc:0.988]
Epoch [75/120    avg_loss:0.005, val_acc:0.988]
Epoch [76/120    avg_loss:0.004, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.005, val_acc:0.988]
Epoch [79/120    avg_loss:0.005, val_acc:0.988]
Epoch [80/120    avg_loss:0.003, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.005, val_acc:0.988]
Epoch [84/120    avg_loss:0.005, val_acc:0.988]
Epoch [85/120    avg_loss:0.004, val_acc:0.988]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     1     0     1     0     1     0]
 [    0     0 18069     0    20     0     0     0     1     0]
 [    0     0     0  2027     2     0     0     0     4     3]
 [    0    40    19     0  2886     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4855     0     1    22]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     3     0    10    46     0     0     0  3490    22]
 [    0     0     0     1    14    49     0     0     0   855]]

Accuracy:
99.30590702046129

F1 scores:
[       nan 0.99643521 0.99889436 0.99509082 0.97155361 0.98157202
 0.99753442 0.99961225 0.9837914  0.93852909]

Kappa:
0.9908029262403396
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51a67df7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.636, val_acc:0.342]
Epoch [2/120    avg_loss:0.962, val_acc:0.635]
Epoch [3/120    avg_loss:0.673, val_acc:0.679]
Epoch [4/120    avg_loss:0.545, val_acc:0.777]
Epoch [5/120    avg_loss:0.431, val_acc:0.840]
Epoch [6/120    avg_loss:0.388, val_acc:0.872]
Epoch [7/120    avg_loss:0.350, val_acc:0.759]
Epoch [8/120    avg_loss:0.310, val_acc:0.798]
Epoch [9/120    avg_loss:0.225, val_acc:0.919]
Epoch [10/120    avg_loss:0.276, val_acc:0.915]
Epoch [11/120    avg_loss:0.197, val_acc:0.871]
Epoch [12/120    avg_loss:0.157, val_acc:0.939]
Epoch [13/120    avg_loss:0.175, val_acc:0.873]
Epoch [14/120    avg_loss:0.153, val_acc:0.831]
Epoch [15/120    avg_loss:0.126, val_acc:0.970]
Epoch [16/120    avg_loss:0.095, val_acc:0.967]
Epoch [17/120    avg_loss:0.140, val_acc:0.961]
Epoch [18/120    avg_loss:0.094, val_acc:0.937]
Epoch [19/120    avg_loss:0.078, val_acc:0.967]
Epoch [20/120    avg_loss:0.058, val_acc:0.968]
Epoch [21/120    avg_loss:0.108, val_acc:0.946]
Epoch [22/120    avg_loss:0.070, val_acc:0.977]
Epoch [23/120    avg_loss:0.116, val_acc:0.938]
Epoch [24/120    avg_loss:0.130, val_acc:0.953]
Epoch [25/120    avg_loss:0.063, val_acc:0.978]
Epoch [26/120    avg_loss:0.055, val_acc:0.959]
Epoch [27/120    avg_loss:0.058, val_acc:0.967]
Epoch [28/120    avg_loss:0.076, val_acc:0.967]
Epoch [29/120    avg_loss:0.051, val_acc:0.962]
Epoch [30/120    avg_loss:0.034, val_acc:0.977]
Epoch [31/120    avg_loss:0.033, val_acc:0.982]
Epoch [32/120    avg_loss:0.038, val_acc:0.905]
Epoch [33/120    avg_loss:0.044, val_acc:0.981]
Epoch [34/120    avg_loss:0.035, val_acc:0.980]
Epoch [35/120    avg_loss:0.031, val_acc:0.979]
Epoch [36/120    avg_loss:0.021, val_acc:0.985]
Epoch [37/120    avg_loss:0.022, val_acc:0.984]
Epoch [38/120    avg_loss:0.019, val_acc:0.983]
Epoch [39/120    avg_loss:0.019, val_acc:0.987]
Epoch [40/120    avg_loss:0.016, val_acc:0.988]
Epoch [41/120    avg_loss:0.012, val_acc:0.989]
Epoch [42/120    avg_loss:0.019, val_acc:0.985]
Epoch [43/120    avg_loss:0.047, val_acc:0.973]
Epoch [44/120    avg_loss:0.018, val_acc:0.983]
Epoch [45/120    avg_loss:0.016, val_acc:0.965]
Epoch [46/120    avg_loss:0.018, val_acc:0.958]
Epoch [47/120    avg_loss:0.019, val_acc:0.984]
Epoch [48/120    avg_loss:0.018, val_acc:0.981]
Epoch [49/120    avg_loss:0.027, val_acc:0.975]
Epoch [50/120    avg_loss:0.025, val_acc:0.966]
Epoch [51/120    avg_loss:0.023, val_acc:0.977]
Epoch [52/120    avg_loss:0.022, val_acc:0.973]
Epoch [53/120    avg_loss:0.021, val_acc:0.991]
Epoch [54/120    avg_loss:0.010, val_acc:0.990]
Epoch [55/120    avg_loss:0.007, val_acc:0.990]
Epoch [56/120    avg_loss:0.009, val_acc:0.991]
Epoch [57/120    avg_loss:0.006, val_acc:0.988]
Epoch [58/120    avg_loss:0.004, val_acc:0.990]
Epoch [59/120    avg_loss:0.007, val_acc:0.989]
Epoch [60/120    avg_loss:0.005, val_acc:0.988]
Epoch [61/120    avg_loss:0.005, val_acc:0.990]
Epoch [62/120    avg_loss:0.011, val_acc:0.985]
Epoch [63/120    avg_loss:0.009, val_acc:0.989]
Epoch [64/120    avg_loss:0.008, val_acc:0.990]
Epoch [65/120    avg_loss:0.020, val_acc:0.988]
Epoch [66/120    avg_loss:0.028, val_acc:0.971]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.017, val_acc:0.985]
Epoch [69/120    avg_loss:0.014, val_acc:0.966]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.007, val_acc:0.987]
Epoch [72/120    avg_loss:0.009, val_acc:0.989]
Epoch [73/120    avg_loss:0.008, val_acc:0.990]
Epoch [74/120    avg_loss:0.008, val_acc:0.990]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.991]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.005, val_acc:0.991]
Epoch [79/120    avg_loss:0.004, val_acc:0.991]
Epoch [80/120    avg_loss:0.005, val_acc:0.991]
Epoch [81/120    avg_loss:0.004, val_acc:0.991]
Epoch [82/120    avg_loss:0.004, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.991]
Epoch [84/120    avg_loss:0.004, val_acc:0.991]
Epoch [85/120    avg_loss:0.004, val_acc:0.991]
Epoch [86/120    avg_loss:0.004, val_acc:0.991]
Epoch [87/120    avg_loss:0.004, val_acc:0.991]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.003, val_acc:0.991]
Epoch [90/120    avg_loss:0.004, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.004, val_acc:0.991]
Epoch [93/120    avg_loss:0.005, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.003, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.002, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.003, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     1     0    13     0     0     0]
 [    0     0 18053     0    37     0     0     0     0     0]
 [    0     0     0  2020     7     0     0     0     0     9]
 [    0    36    22     0  2875     0     7     0    31     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4858     0     0    20]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     4     0     0    27     0     0     0  3536     4]
 [    0     0     0     0    14    34     0     0     0   871]]

Accuracy:
99.3516978767503

F1 scores:
[       nan 0.99581071 0.99836859 0.99605523 0.96915557 0.9871407
 0.99569584 0.9992242  0.99075371 0.95504386]

Kappa:
0.9914105637579733
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f17fb05e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.672, val_acc:0.674]
Epoch [2/120    avg_loss:1.060, val_acc:0.639]
Epoch [3/120    avg_loss:0.728, val_acc:0.678]
Epoch [4/120    avg_loss:0.608, val_acc:0.750]
Epoch [5/120    avg_loss:0.496, val_acc:0.824]
Epoch [6/120    avg_loss:0.369, val_acc:0.875]
Epoch [7/120    avg_loss:0.337, val_acc:0.785]
Epoch [8/120    avg_loss:0.284, val_acc:0.766]
Epoch [9/120    avg_loss:0.225, val_acc:0.920]
Epoch [10/120    avg_loss:0.231, val_acc:0.907]
Epoch [11/120    avg_loss:0.187, val_acc:0.938]
Epoch [12/120    avg_loss:0.145, val_acc:0.914]
Epoch [13/120    avg_loss:0.224, val_acc:0.812]
Epoch [14/120    avg_loss:0.234, val_acc:0.824]
Epoch [15/120    avg_loss:0.157, val_acc:0.916]
Epoch [16/120    avg_loss:0.113, val_acc:0.920]
Epoch [17/120    avg_loss:0.119, val_acc:0.955]
Epoch [18/120    avg_loss:0.099, val_acc:0.932]
Epoch [19/120    avg_loss:0.093, val_acc:0.963]
Epoch [20/120    avg_loss:0.083, val_acc:0.950]
Epoch [21/120    avg_loss:0.077, val_acc:0.953]
Epoch [22/120    avg_loss:0.063, val_acc:0.949]
Epoch [23/120    avg_loss:0.058, val_acc:0.966]
Epoch [24/120    avg_loss:0.086, val_acc:0.961]
Epoch [25/120    avg_loss:0.076, val_acc:0.930]
Epoch [26/120    avg_loss:0.085, val_acc:0.966]
Epoch [27/120    avg_loss:0.076, val_acc:0.938]
Epoch [28/120    avg_loss:0.048, val_acc:0.965]
Epoch [29/120    avg_loss:0.051, val_acc:0.969]
Epoch [30/120    avg_loss:0.062, val_acc:0.968]
Epoch [31/120    avg_loss:0.043, val_acc:0.973]
Epoch [32/120    avg_loss:0.072, val_acc:0.966]
Epoch [33/120    avg_loss:0.065, val_acc:0.964]
Epoch [34/120    avg_loss:0.040, val_acc:0.965]
Epoch [35/120    avg_loss:0.030, val_acc:0.967]
Epoch [36/120    avg_loss:0.025, val_acc:0.951]
Epoch [37/120    avg_loss:0.080, val_acc:0.971]
Epoch [38/120    avg_loss:0.030, val_acc:0.963]
Epoch [39/120    avg_loss:0.028, val_acc:0.962]
Epoch [40/120    avg_loss:0.031, val_acc:0.955]
Epoch [41/120    avg_loss:0.021, val_acc:0.974]
Epoch [42/120    avg_loss:0.015, val_acc:0.979]
Epoch [43/120    avg_loss:0.012, val_acc:0.977]
Epoch [44/120    avg_loss:0.012, val_acc:0.980]
Epoch [45/120    avg_loss:0.033, val_acc:0.927]
Epoch [46/120    avg_loss:0.045, val_acc:0.969]
Epoch [47/120    avg_loss:0.024, val_acc:0.940]
Epoch [48/120    avg_loss:0.050, val_acc:0.973]
Epoch [49/120    avg_loss:0.021, val_acc:0.969]
Epoch [50/120    avg_loss:0.034, val_acc:0.959]
Epoch [51/120    avg_loss:0.033, val_acc:0.961]
Epoch [52/120    avg_loss:0.026, val_acc:0.971]
Epoch [53/120    avg_loss:0.027, val_acc:0.978]
Epoch [54/120    avg_loss:0.092, val_acc:0.960]
Epoch [55/120    avg_loss:0.051, val_acc:0.973]
Epoch [56/120    avg_loss:0.031, val_acc:0.974]
Epoch [57/120    avg_loss:0.028, val_acc:0.981]
Epoch [58/120    avg_loss:0.022, val_acc:0.971]
Epoch [59/120    avg_loss:0.033, val_acc:0.968]
Epoch [60/120    avg_loss:0.014, val_acc:0.983]
Epoch [61/120    avg_loss:0.016, val_acc:0.978]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.016, val_acc:0.980]
Epoch [64/120    avg_loss:0.023, val_acc:0.981]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.023, val_acc:0.981]
Epoch [67/120    avg_loss:0.010, val_acc:0.988]
Epoch [68/120    avg_loss:0.015, val_acc:0.976]
Epoch [69/120    avg_loss:0.029, val_acc:0.985]
Epoch [70/120    avg_loss:0.013, val_acc:0.987]
Epoch [71/120    avg_loss:0.013, val_acc:0.985]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.985]
Epoch [74/120    avg_loss:0.059, val_acc:0.943]
Epoch [75/120    avg_loss:0.080, val_acc:0.956]
Epoch [76/120    avg_loss:0.089, val_acc:0.980]
Epoch [77/120    avg_loss:0.029, val_acc:0.947]
Epoch [78/120    avg_loss:0.035, val_acc:0.973]
Epoch [79/120    avg_loss:0.050, val_acc:0.974]
Epoch [80/120    avg_loss:0.026, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.979]
Epoch [82/120    avg_loss:0.008, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.011, val_acc:0.980]
Epoch [89/120    avg_loss:0.008, val_acc:0.980]
Epoch [90/120    avg_loss:0.006, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.005, val_acc:0.980]
Epoch [93/120    avg_loss:0.007, val_acc:0.979]
Epoch [94/120    avg_loss:0.007, val_acc:0.979]
Epoch [95/120    avg_loss:0.006, val_acc:0.979]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.007, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.979]
Epoch [102/120    avg_loss:0.006, val_acc:0.979]
Epoch [103/120    avg_loss:0.007, val_acc:0.979]
Epoch [104/120    avg_loss:0.008, val_acc:0.979]
Epoch [105/120    avg_loss:0.013, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.979]
Epoch [107/120    avg_loss:0.008, val_acc:0.979]
Epoch [108/120    avg_loss:0.009, val_acc:0.979]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.008, val_acc:0.979]
Epoch [111/120    avg_loss:0.008, val_acc:0.979]
Epoch [112/120    avg_loss:0.006, val_acc:0.979]
Epoch [113/120    avg_loss:0.008, val_acc:0.979]
Epoch [114/120    avg_loss:0.009, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.979]
Epoch [116/120    avg_loss:0.009, val_acc:0.979]
Epoch [117/120    avg_loss:0.007, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6366     0     0     0     0    32    17    16     1]
 [    0     2 18040     0    30     0     4     0    14     0]
 [    0     0     0  2019     2     0     0     0    11     4]
 [    0    47    20     0  2869     0     5     0    31     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4862     0     0    16]
 [    0     0     0     0     0     0     4  1282     0     4]
 [    0     3     0     0    40     0     0     0  3520     8]
 [    0     0     0     3    18    32     0     1     0   865]]

Accuracy:
99.11551346010171

F1 scores:
[       nan 0.99081712 0.99806362 0.99507146 0.96745911 0.98712121
 0.99376597 0.98996139 0.98282842 0.95107202]

Kappa:
0.9882840663318732
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fce8c01e4e0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.668, val_acc:0.519]
Epoch [2/120    avg_loss:1.005, val_acc:0.636]
Epoch [3/120    avg_loss:0.713, val_acc:0.779]
Epoch [4/120    avg_loss:0.504, val_acc:0.880]
Epoch [5/120    avg_loss:0.367, val_acc:0.886]
Epoch [6/120    avg_loss:0.353, val_acc:0.872]
Epoch [7/120    avg_loss:0.314, val_acc:0.914]
Epoch [8/120    avg_loss:0.262, val_acc:0.898]
Epoch [9/120    avg_loss:0.234, val_acc:0.923]
Epoch [10/120    avg_loss:0.163, val_acc:0.956]
Epoch [11/120    avg_loss:0.128, val_acc:0.955]
Epoch [12/120    avg_loss:0.157, val_acc:0.946]
Epoch [13/120    avg_loss:0.131, val_acc:0.950]
Epoch [14/120    avg_loss:0.116, val_acc:0.949]
Epoch [15/120    avg_loss:0.116, val_acc:0.890]
Epoch [16/120    avg_loss:0.188, val_acc:0.936]
Epoch [17/120    avg_loss:0.103, val_acc:0.955]
Epoch [18/120    avg_loss:0.075, val_acc:0.964]
Epoch [19/120    avg_loss:0.066, val_acc:0.929]
Epoch [20/120    avg_loss:0.070, val_acc:0.854]
Epoch [21/120    avg_loss:0.058, val_acc:0.952]
Epoch [22/120    avg_loss:0.043, val_acc:0.970]
Epoch [23/120    avg_loss:0.038, val_acc:0.961]
Epoch [24/120    avg_loss:0.063, val_acc:0.970]
Epoch [25/120    avg_loss:0.055, val_acc:0.968]
Epoch [26/120    avg_loss:0.040, val_acc:0.973]
Epoch [27/120    avg_loss:0.020, val_acc:0.976]
Epoch [28/120    avg_loss:0.030, val_acc:0.970]
Epoch [29/120    avg_loss:0.040, val_acc:0.955]
Epoch [30/120    avg_loss:0.043, val_acc:0.968]
Epoch [31/120    avg_loss:0.034, val_acc:0.981]
Epoch [32/120    avg_loss:0.026, val_acc:0.977]
Epoch [33/120    avg_loss:0.058, val_acc:0.966]
Epoch [34/120    avg_loss:0.043, val_acc:0.973]
Epoch [35/120    avg_loss:0.023, val_acc:0.976]
Epoch [36/120    avg_loss:0.017, val_acc:0.985]
Epoch [37/120    avg_loss:0.023, val_acc:0.982]
Epoch [38/120    avg_loss:0.016, val_acc:0.986]
Epoch [39/120    avg_loss:0.014, val_acc:0.969]
Epoch [40/120    avg_loss:0.029, val_acc:0.981]
Epoch [41/120    avg_loss:0.049, val_acc:0.973]
Epoch [42/120    avg_loss:0.025, val_acc:0.978]
Epoch [43/120    avg_loss:0.014, val_acc:0.979]
Epoch [44/120    avg_loss:0.030, val_acc:0.977]
Epoch [45/120    avg_loss:0.016, val_acc:0.984]
Epoch [46/120    avg_loss:0.007, val_acc:0.986]
Epoch [47/120    avg_loss:0.016, val_acc:0.983]
Epoch [48/120    avg_loss:0.023, val_acc:0.973]
Epoch [49/120    avg_loss:0.024, val_acc:0.982]
Epoch [50/120    avg_loss:0.014, val_acc:0.978]
Epoch [51/120    avg_loss:0.012, val_acc:0.988]
Epoch [52/120    avg_loss:0.013, val_acc:0.986]
Epoch [53/120    avg_loss:0.014, val_acc:0.987]
Epoch [54/120    avg_loss:0.027, val_acc:0.964]
Epoch [55/120    avg_loss:0.020, val_acc:0.980]
Epoch [56/120    avg_loss:0.012, val_acc:0.980]
Epoch [57/120    avg_loss:0.008, val_acc:0.986]
Epoch [58/120    avg_loss:0.008, val_acc:0.986]
Epoch [59/120    avg_loss:0.006, val_acc:0.984]
Epoch [60/120    avg_loss:0.009, val_acc:0.967]
Epoch [61/120    avg_loss:0.011, val_acc:0.985]
Epoch [62/120    avg_loss:0.008, val_acc:0.984]
Epoch [63/120    avg_loss:0.008, val_acc:0.981]
Epoch [64/120    avg_loss:0.005, val_acc:0.988]
Epoch [65/120    avg_loss:0.005, val_acc:0.985]
Epoch [66/120    avg_loss:0.007, val_acc:0.984]
Epoch [67/120    avg_loss:0.004, val_acc:0.986]
Epoch [68/120    avg_loss:0.004, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.989]
Epoch [70/120    avg_loss:0.005, val_acc:0.989]
Epoch [71/120    avg_loss:0.007, val_acc:0.987]
Epoch [72/120    avg_loss:0.004, val_acc:0.990]
Epoch [73/120    avg_loss:0.005, val_acc:0.987]
Epoch [74/120    avg_loss:0.005, val_acc:0.985]
Epoch [75/120    avg_loss:0.003, val_acc:0.990]
Epoch [76/120    avg_loss:0.002, val_acc:0.988]
Epoch [77/120    avg_loss:0.002, val_acc:0.988]
Epoch [78/120    avg_loss:0.002, val_acc:0.990]
Epoch [79/120    avg_loss:0.002, val_acc:0.989]
Epoch [80/120    avg_loss:0.003, val_acc:0.990]
Epoch [81/120    avg_loss:0.003, val_acc:0.990]
Epoch [82/120    avg_loss:0.054, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.028, val_acc:0.966]
Epoch [85/120    avg_loss:0.030, val_acc:0.984]
Epoch [86/120    avg_loss:0.051, val_acc:0.970]
Epoch [87/120    avg_loss:0.031, val_acc:0.984]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.012, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.009, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     0     2     0     0     0    37     2]
 [    0     2 18031     0    41     0    13     0     3     0]
 [    0     3     0  2011     3     0     0     0    15     4]
 [    0    42    15     0  2903     0     0     0    12     0]
 [    0     0     0     0     0  1302     0     0     0     3]
 [    0     0     0     0     0     0  4873     0     0     5]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     6     0     0    75     0     0     0  3483     7]
 [    0     0     0     0    15    28     0     0     0   876]]

Accuracy:
99.18781481213699

F1 scores:
[       nan 0.9926996  0.99795218 0.99382258 0.96589586 0.98823529
 0.99774775 0.9984472  0.97823339 0.96475771]

Kappa:
0.989242855556815
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4288550860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.600, val_acc:0.686]
Epoch [2/120    avg_loss:1.019, val_acc:0.622]
Epoch [3/120    avg_loss:0.674, val_acc:0.797]
Epoch [4/120    avg_loss:0.572, val_acc:0.663]
Epoch [5/120    avg_loss:0.551, val_acc:0.815]
Epoch [6/120    avg_loss:0.388, val_acc:0.789]
Epoch [7/120    avg_loss:0.328, val_acc:0.819]
Epoch [8/120    avg_loss:0.293, val_acc:0.906]
Epoch [9/120    avg_loss:0.257, val_acc:0.914]
Epoch [10/120    avg_loss:0.208, val_acc:0.886]
Epoch [11/120    avg_loss:0.260, val_acc:0.854]
Epoch [12/120    avg_loss:0.183, val_acc:0.860]
Epoch [13/120    avg_loss:0.161, val_acc:0.932]
Epoch [14/120    avg_loss:0.137, val_acc:0.866]
Epoch [15/120    avg_loss:0.138, val_acc:0.904]
Epoch [16/120    avg_loss:0.178, val_acc:0.961]
Epoch [17/120    avg_loss:0.091, val_acc:0.965]
Epoch [18/120    avg_loss:0.079, val_acc:0.920]
Epoch [19/120    avg_loss:0.132, val_acc:0.928]
Epoch [20/120    avg_loss:0.132, val_acc:0.943]
Epoch [21/120    avg_loss:0.112, val_acc:0.957]
Epoch [22/120    avg_loss:0.113, val_acc:0.963]
Epoch [23/120    avg_loss:0.067, val_acc:0.961]
Epoch [24/120    avg_loss:0.057, val_acc:0.955]
Epoch [25/120    avg_loss:0.064, val_acc:0.979]
Epoch [26/120    avg_loss:0.052, val_acc:0.971]
Epoch [27/120    avg_loss:0.031, val_acc:0.963]
Epoch [28/120    avg_loss:0.044, val_acc:0.967]
Epoch [29/120    avg_loss:0.033, val_acc:0.978]
Epoch [30/120    avg_loss:0.030, val_acc:0.972]
Epoch [31/120    avg_loss:0.020, val_acc:0.987]
Epoch [32/120    avg_loss:0.029, val_acc:0.967]
Epoch [33/120    avg_loss:0.023, val_acc:0.986]
Epoch [34/120    avg_loss:0.021, val_acc:0.986]
Epoch [35/120    avg_loss:0.024, val_acc:0.985]
Epoch [36/120    avg_loss:0.026, val_acc:0.979]
Epoch [37/120    avg_loss:0.037, val_acc:0.986]
Epoch [38/120    avg_loss:0.026, val_acc:0.989]
Epoch [39/120    avg_loss:0.013, val_acc:0.984]
Epoch [40/120    avg_loss:0.024, val_acc:0.985]
Epoch [41/120    avg_loss:0.048, val_acc:0.952]
Epoch [42/120    avg_loss:0.063, val_acc:0.925]
Epoch [43/120    avg_loss:0.022, val_acc:0.979]
Epoch [44/120    avg_loss:0.096, val_acc:0.946]
Epoch [45/120    avg_loss:0.084, val_acc:0.936]
Epoch [46/120    avg_loss:0.040, val_acc:0.956]
Epoch [47/120    avg_loss:0.029, val_acc:0.985]
Epoch [48/120    avg_loss:0.022, val_acc:0.978]
Epoch [49/120    avg_loss:0.016, val_acc:0.985]
Epoch [50/120    avg_loss:0.012, val_acc:0.988]
Epoch [51/120    avg_loss:0.013, val_acc:0.991]
Epoch [52/120    avg_loss:0.014, val_acc:0.990]
Epoch [53/120    avg_loss:0.022, val_acc:0.985]
Epoch [54/120    avg_loss:0.019, val_acc:0.986]
Epoch [55/120    avg_loss:0.012, val_acc:0.982]
Epoch [56/120    avg_loss:0.010, val_acc:0.984]
Epoch [57/120    avg_loss:0.008, val_acc:0.991]
Epoch [58/120    avg_loss:0.007, val_acc:0.992]
Epoch [59/120    avg_loss:0.017, val_acc:0.986]
Epoch [60/120    avg_loss:0.024, val_acc:0.988]
Epoch [61/120    avg_loss:0.014, val_acc:0.989]
Epoch [62/120    avg_loss:0.043, val_acc:0.952]
Epoch [63/120    avg_loss:0.025, val_acc:0.985]
Epoch [64/120    avg_loss:0.014, val_acc:0.990]
Epoch [65/120    avg_loss:0.011, val_acc:0.991]
Epoch [66/120    avg_loss:0.008, val_acc:0.993]
Epoch [67/120    avg_loss:0.005, val_acc:0.991]
Epoch [68/120    avg_loss:0.009, val_acc:0.991]
Epoch [69/120    avg_loss:0.011, val_acc:0.991]
Epoch [70/120    avg_loss:0.006, val_acc:0.991]
Epoch [71/120    avg_loss:0.007, val_acc:0.991]
Epoch [72/120    avg_loss:0.006, val_acc:0.988]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.990]
Epoch [75/120    avg_loss:0.005, val_acc:0.991]
Epoch [76/120    avg_loss:0.008, val_acc:0.990]
Epoch [77/120    avg_loss:0.004, val_acc:0.993]
Epoch [78/120    avg_loss:0.006, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.005, val_acc:0.990]
Epoch [81/120    avg_loss:0.045, val_acc:0.985]
Epoch [82/120    avg_loss:0.017, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.985]
Epoch [84/120    avg_loss:0.012, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.991]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.993]
Epoch [91/120    avg_loss:0.003, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.991]
Epoch [94/120    avg_loss:0.003, val_acc:0.991]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.008, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.957]
Epoch [98/120    avg_loss:0.024, val_acc:0.986]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.973]
Epoch [101/120    avg_loss:0.022, val_acc:0.990]
Epoch [102/120    avg_loss:0.023, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.993]
Epoch [104/120    avg_loss:0.004, val_acc:0.993]
Epoch [105/120    avg_loss:0.017, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.993]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.993]
Epoch [113/120    avg_loss:0.003, val_acc:0.994]
Epoch [114/120    avg_loss:0.003, val_acc:0.994]
Epoch [115/120    avg_loss:0.002, val_acc:0.994]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.002, val_acc:0.992]
Epoch [118/120    avg_loss:0.003, val_acc:0.993]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     7     0     0     8     2    36     0]
 [    0     0 18057     0    24     0     5     0     4     0]
 [    0     0     0  1991     2     0     0     0    40     3]
 [    0    17    11     0  2900     0     1     0    43     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4872     0     0     2]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     1     0     0    51     0     0     0  3516     3]
 [    0     0     0     0    14    34     0     0     0   871]]

Accuracy:
99.24083580362954

F1 scores:
[       nan 0.99446566 0.99878312 0.98613175 0.97266477 0.9871407
 0.99764513 0.99806126 0.97531207 0.96885428]

Kappa:
0.9899431936687801
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb06cc68780>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.624, val_acc:0.428]
Epoch [2/120    avg_loss:1.039, val_acc:0.624]
Epoch [3/120    avg_loss:0.814, val_acc:0.762]
Epoch [4/120    avg_loss:0.576, val_acc:0.821]
Epoch [5/120    avg_loss:0.413, val_acc:0.750]
Epoch [6/120    avg_loss:0.378, val_acc:0.799]
Epoch [7/120    avg_loss:0.336, val_acc:0.878]
Epoch [8/120    avg_loss:0.317, val_acc:0.883]
Epoch [9/120    avg_loss:0.238, val_acc:0.917]
Epoch [10/120    avg_loss:0.176, val_acc:0.922]
Epoch [11/120    avg_loss:0.210, val_acc:0.924]
Epoch [12/120    avg_loss:0.133, val_acc:0.906]
Epoch [13/120    avg_loss:0.098, val_acc:0.946]
Epoch [14/120    avg_loss:0.098, val_acc:0.926]
Epoch [15/120    avg_loss:0.085, val_acc:0.920]
Epoch [16/120    avg_loss:0.181, val_acc:0.945]
Epoch [17/120    avg_loss:0.159, val_acc:0.952]
Epoch [18/120    avg_loss:0.121, val_acc:0.881]
Epoch [19/120    avg_loss:0.086, val_acc:0.951]
Epoch [20/120    avg_loss:0.057, val_acc:0.972]
Epoch [21/120    avg_loss:0.046, val_acc:0.965]
Epoch [22/120    avg_loss:0.051, val_acc:0.959]
Epoch [23/120    avg_loss:0.062, val_acc:0.965]
Epoch [24/120    avg_loss:0.061, val_acc:0.942]
Epoch [25/120    avg_loss:0.052, val_acc:0.970]
Epoch [26/120    avg_loss:0.051, val_acc:0.958]
Epoch [27/120    avg_loss:0.059, val_acc:0.905]
Epoch [28/120    avg_loss:0.038, val_acc:0.969]
Epoch [29/120    avg_loss:0.037, val_acc:0.952]
Epoch [30/120    avg_loss:0.038, val_acc:0.957]
Epoch [31/120    avg_loss:0.029, val_acc:0.977]
Epoch [32/120    avg_loss:0.098, val_acc:0.918]
Epoch [33/120    avg_loss:0.043, val_acc:0.973]
Epoch [34/120    avg_loss:0.038, val_acc:0.964]
Epoch [35/120    avg_loss:0.036, val_acc:0.969]
Epoch [36/120    avg_loss:0.037, val_acc:0.969]
Epoch [37/120    avg_loss:0.027, val_acc:0.964]
Epoch [38/120    avg_loss:0.042, val_acc:0.971]
Epoch [39/120    avg_loss:0.055, val_acc:0.945]
Epoch [40/120    avg_loss:0.147, val_acc:0.957]
Epoch [41/120    avg_loss:0.067, val_acc:0.966]
Epoch [42/120    avg_loss:0.030, val_acc:0.974]
Epoch [43/120    avg_loss:0.024, val_acc:0.981]
Epoch [44/120    avg_loss:0.023, val_acc:0.968]
Epoch [45/120    avg_loss:0.020, val_acc:0.985]
Epoch [46/120    avg_loss:0.029, val_acc:0.968]
Epoch [47/120    avg_loss:0.015, val_acc:0.978]
Epoch [48/120    avg_loss:0.026, val_acc:0.961]
Epoch [49/120    avg_loss:0.125, val_acc:0.950]
Epoch [50/120    avg_loss:0.058, val_acc:0.951]
Epoch [51/120    avg_loss:0.117, val_acc:0.928]
Epoch [52/120    avg_loss:0.065, val_acc:0.938]
Epoch [53/120    avg_loss:0.072, val_acc:0.974]
Epoch [54/120    avg_loss:0.027, val_acc:0.971]
Epoch [55/120    avg_loss:0.039, val_acc:0.973]
Epoch [56/120    avg_loss:0.024, val_acc:0.971]
Epoch [57/120    avg_loss:0.060, val_acc:0.957]
Epoch [58/120    avg_loss:0.047, val_acc:0.952]
Epoch [59/120    avg_loss:0.018, val_acc:0.969]
Epoch [60/120    avg_loss:0.018, val_acc:0.970]
Epoch [61/120    avg_loss:0.020, val_acc:0.975]
Epoch [62/120    avg_loss:0.015, val_acc:0.977]
Epoch [63/120    avg_loss:0.013, val_acc:0.975]
Epoch [64/120    avg_loss:0.014, val_acc:0.977]
Epoch [65/120    avg_loss:0.013, val_acc:0.978]
Epoch [66/120    avg_loss:0.018, val_acc:0.974]
Epoch [67/120    avg_loss:0.013, val_acc:0.976]
Epoch [68/120    avg_loss:0.010, val_acc:0.977]
Epoch [69/120    avg_loss:0.011, val_acc:0.976]
Epoch [70/120    avg_loss:0.013, val_acc:0.977]
Epoch [71/120    avg_loss:0.012, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.983]
Epoch [74/120    avg_loss:0.015, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.980]
Epoch [76/120    avg_loss:0.010, val_acc:0.979]
Epoch [77/120    avg_loss:0.014, val_acc:0.974]
Epoch [78/120    avg_loss:0.014, val_acc:0.974]
Epoch [79/120    avg_loss:0.011, val_acc:0.974]
Epoch [80/120    avg_loss:0.012, val_acc:0.974]
Epoch [81/120    avg_loss:0.011, val_acc:0.974]
Epoch [82/120    avg_loss:0.009, val_acc:0.976]
Epoch [83/120    avg_loss:0.012, val_acc:0.974]
Epoch [84/120    avg_loss:0.010, val_acc:0.974]
Epoch [85/120    avg_loss:0.012, val_acc:0.974]
Epoch [86/120    avg_loss:0.012, val_acc:0.974]
Epoch [87/120    avg_loss:0.010, val_acc:0.974]
Epoch [88/120    avg_loss:0.014, val_acc:0.974]
Epoch [89/120    avg_loss:0.010, val_acc:0.974]
Epoch [90/120    avg_loss:0.012, val_acc:0.974]
Epoch [91/120    avg_loss:0.010, val_acc:0.974]
Epoch [92/120    avg_loss:0.010, val_acc:0.974]
Epoch [93/120    avg_loss:0.012, val_acc:0.974]
Epoch [94/120    avg_loss:0.009, val_acc:0.974]
Epoch [95/120    avg_loss:0.010, val_acc:0.974]
Epoch [96/120    avg_loss:0.009, val_acc:0.974]
Epoch [97/120    avg_loss:0.009, val_acc:0.974]
Epoch [98/120    avg_loss:0.010, val_acc:0.974]
Epoch [99/120    avg_loss:0.014, val_acc:0.974]
Epoch [100/120    avg_loss:0.009, val_acc:0.974]
Epoch [101/120    avg_loss:0.011, val_acc:0.974]
Epoch [102/120    avg_loss:0.009, val_acc:0.974]
Epoch [103/120    avg_loss:0.011, val_acc:0.974]
Epoch [104/120    avg_loss:0.014, val_acc:0.974]
Epoch [105/120    avg_loss:0.015, val_acc:0.974]
Epoch [106/120    avg_loss:0.010, val_acc:0.974]
Epoch [107/120    avg_loss:0.021, val_acc:0.974]
Epoch [108/120    avg_loss:0.009, val_acc:0.974]
Epoch [109/120    avg_loss:0.011, val_acc:0.974]
Epoch [110/120    avg_loss:0.012, val_acc:0.974]
Epoch [111/120    avg_loss:0.011, val_acc:0.974]
Epoch [112/120    avg_loss:0.012, val_acc:0.974]
Epoch [113/120    avg_loss:0.010, val_acc:0.974]
Epoch [114/120    avg_loss:0.011, val_acc:0.974]
Epoch [115/120    avg_loss:0.009, val_acc:0.974]
Epoch [116/120    avg_loss:0.013, val_acc:0.974]
Epoch [117/120    avg_loss:0.016, val_acc:0.974]
Epoch [118/120    avg_loss:0.009, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.974]
Epoch [120/120    avg_loss:0.012, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     5     0     0     8     0     0]
 [    0     2 18032     0    47     0     3     0     6     0]
 [    0     0     0  2027     3     0     0     0     0     6]
 [    0    51    18     0  2875     0     1     0    27     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4850     0     0    28]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     4     0     1    52     0     0     0  3485    29]
 [    0     0     0     0    17    18     0     0     0   884]]

Accuracy:
99.20227508254405

F1 scores:
[       nan 0.99457701 0.99789707 0.99753937 0.96298777 0.99238385
 0.99640473 0.99574468 0.98321343 0.94646681]

Kappa:
0.9894346511382099
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6bf2eb8860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.647, val_acc:0.454]
Epoch [2/120    avg_loss:1.013, val_acc:0.671]
Epoch [3/120    avg_loss:0.720, val_acc:0.810]
Epoch [4/120    avg_loss:0.598, val_acc:0.723]
Epoch [5/120    avg_loss:0.415, val_acc:0.838]
Epoch [6/120    avg_loss:0.332, val_acc:0.874]
Epoch [7/120    avg_loss:0.255, val_acc:0.883]
Epoch [8/120    avg_loss:0.271, val_acc:0.904]
Epoch [9/120    avg_loss:0.241, val_acc:0.895]
Epoch [10/120    avg_loss:0.188, val_acc:0.904]
Epoch [11/120    avg_loss:0.182, val_acc:0.931]
Epoch [12/120    avg_loss:0.122, val_acc:0.873]
Epoch [13/120    avg_loss:0.125, val_acc:0.942]
Epoch [14/120    avg_loss:0.129, val_acc:0.966]
Epoch [15/120    avg_loss:0.167, val_acc:0.881]
Epoch [16/120    avg_loss:0.212, val_acc:0.929]
Epoch [17/120    avg_loss:0.155, val_acc:0.958]
Epoch [18/120    avg_loss:0.083, val_acc:0.968]
Epoch [19/120    avg_loss:0.074, val_acc:0.964]
Epoch [20/120    avg_loss:0.104, val_acc:0.950]
Epoch [21/120    avg_loss:0.072, val_acc:0.967]
Epoch [22/120    avg_loss:0.095, val_acc:0.962]
Epoch [23/120    avg_loss:0.075, val_acc:0.972]
Epoch [24/120    avg_loss:0.036, val_acc:0.972]
Epoch [25/120    avg_loss:0.038, val_acc:0.965]
Epoch [26/120    avg_loss:0.059, val_acc:0.976]
Epoch [27/120    avg_loss:0.059, val_acc:0.969]
Epoch [28/120    avg_loss:0.052, val_acc:0.950]
Epoch [29/120    avg_loss:0.058, val_acc:0.975]
Epoch [30/120    avg_loss:0.040, val_acc:0.968]
Epoch [31/120    avg_loss:0.042, val_acc:0.970]
Epoch [32/120    avg_loss:0.040, val_acc:0.978]
Epoch [33/120    avg_loss:0.026, val_acc:0.977]
Epoch [34/120    avg_loss:0.019, val_acc:0.978]
Epoch [35/120    avg_loss:0.036, val_acc:0.954]
Epoch [36/120    avg_loss:0.059, val_acc:0.975]
Epoch [37/120    avg_loss:0.064, val_acc:0.944]
Epoch [38/120    avg_loss:0.036, val_acc:0.984]
Epoch [39/120    avg_loss:0.025, val_acc:0.977]
Epoch [40/120    avg_loss:0.021, val_acc:0.971]
Epoch [41/120    avg_loss:0.019, val_acc:0.974]
Epoch [42/120    avg_loss:0.026, val_acc:0.986]
Epoch [43/120    avg_loss:0.020, val_acc:0.976]
Epoch [44/120    avg_loss:0.016, val_acc:0.986]
Epoch [45/120    avg_loss:0.012, val_acc:0.984]
Epoch [46/120    avg_loss:0.009, val_acc:0.986]
Epoch [47/120    avg_loss:0.017, val_acc:0.968]
Epoch [48/120    avg_loss:0.023, val_acc:0.981]
Epoch [49/120    avg_loss:0.010, val_acc:0.988]
Epoch [50/120    avg_loss:0.026, val_acc:0.982]
Epoch [51/120    avg_loss:0.012, val_acc:0.981]
Epoch [52/120    avg_loss:0.015, val_acc:0.986]
Epoch [53/120    avg_loss:0.009, val_acc:0.985]
Epoch [54/120    avg_loss:0.008, val_acc:0.984]
Epoch [55/120    avg_loss:0.014, val_acc:0.984]
Epoch [56/120    avg_loss:0.007, val_acc:0.985]
Epoch [57/120    avg_loss:0.014, val_acc:0.988]
Epoch [58/120    avg_loss:0.008, val_acc:0.985]
Epoch [59/120    avg_loss:0.009, val_acc:0.985]
Epoch [60/120    avg_loss:0.005, val_acc:0.987]
Epoch [61/120    avg_loss:0.004, val_acc:0.985]
Epoch [62/120    avg_loss:0.015, val_acc:0.971]
Epoch [63/120    avg_loss:0.007, val_acc:0.990]
Epoch [64/120    avg_loss:0.012, val_acc:0.980]
Epoch [65/120    avg_loss:0.014, val_acc:0.980]
Epoch [66/120    avg_loss:0.011, val_acc:0.986]
Epoch [67/120    avg_loss:0.008, val_acc:0.989]
Epoch [68/120    avg_loss:0.009, val_acc:0.971]
Epoch [69/120    avg_loss:0.013, val_acc:0.964]
Epoch [70/120    avg_loss:0.009, val_acc:0.989]
Epoch [71/120    avg_loss:0.005, val_acc:0.990]
Epoch [72/120    avg_loss:0.004, val_acc:0.991]
Epoch [73/120    avg_loss:0.005, val_acc:0.985]
Epoch [74/120    avg_loss:0.005, val_acc:0.990]
Epoch [75/120    avg_loss:0.004, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.990]
Epoch [77/120    avg_loss:0.005, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.005, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.003, val_acc:0.990]
Epoch [83/120    avg_loss:0.003, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.003, val_acc:0.989]
Epoch [87/120    avg_loss:0.003, val_acc:0.988]
Epoch [88/120    avg_loss:0.002, val_acc:0.987]
Epoch [89/120    avg_loss:0.003, val_acc:0.987]
Epoch [90/120    avg_loss:0.008, val_acc:0.976]
Epoch [91/120    avg_loss:0.010, val_acc:0.974]
Epoch [92/120    avg_loss:0.011, val_acc:0.991]
Epoch [93/120    avg_loss:0.016, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.975]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.994]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.993]
Epoch [107/120    avg_loss:0.003, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.992]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.989]
Epoch [112/120    avg_loss:0.003, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.002, val_acc:0.991]
Epoch [118/120    avg_loss:0.002, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.990]
Epoch [120/120    avg_loss:0.003, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0    10     0     4     0     0     0]
 [    0     0 18079     0     9     0     0     0     2     0]
 [    0     0     0  2029     1     0     0     0     0     6]
 [    0    30    11     0  2902     0     0     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4862     0     0    16]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     3     0     7    41     0     0     0  3516     4]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.46979008507459

F1 scores:
[       nan 0.99635178 0.99939193 0.99656189 0.97562616 0.98901099
 0.99753796 0.9984472  0.98791795 0.96210873]

Kappa:
0.9929742393733599
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa6d7447b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.694, val_acc:0.505]
Epoch [2/120    avg_loss:1.129, val_acc:0.645]
Epoch [3/120    avg_loss:0.764, val_acc:0.723]
Epoch [4/120    avg_loss:0.587, val_acc:0.775]
Epoch [5/120    avg_loss:0.497, val_acc:0.865]
Epoch [6/120    avg_loss:0.351, val_acc:0.885]
Epoch [7/120    avg_loss:0.322, val_acc:0.862]
Epoch [8/120    avg_loss:0.313, val_acc:0.921]
Epoch [9/120    avg_loss:0.268, val_acc:0.905]
Epoch [10/120    avg_loss:0.248, val_acc:0.815]
Epoch [11/120    avg_loss:0.201, val_acc:0.938]
Epoch [12/120    avg_loss:0.182, val_acc:0.913]
Epoch [13/120    avg_loss:0.184, val_acc:0.907]
Epoch [14/120    avg_loss:0.229, val_acc:0.876]
Epoch [15/120    avg_loss:0.154, val_acc:0.945]
Epoch [16/120    avg_loss:0.183, val_acc:0.874]
Epoch [17/120    avg_loss:0.115, val_acc:0.959]
Epoch [18/120    avg_loss:0.157, val_acc:0.900]
Epoch [19/120    avg_loss:0.111, val_acc:0.950]
Epoch [20/120    avg_loss:0.091, val_acc:0.962]
Epoch [21/120    avg_loss:0.080, val_acc:0.964]
Epoch [22/120    avg_loss:0.090, val_acc:0.931]
Epoch [23/120    avg_loss:0.072, val_acc:0.964]
Epoch [24/120    avg_loss:0.053, val_acc:0.966]
Epoch [25/120    avg_loss:0.041, val_acc:0.971]
Epoch [26/120    avg_loss:0.058, val_acc:0.977]
Epoch [27/120    avg_loss:0.040, val_acc:0.950]
Epoch [28/120    avg_loss:0.058, val_acc:0.981]
Epoch [29/120    avg_loss:0.043, val_acc:0.973]
Epoch [30/120    avg_loss:0.037, val_acc:0.974]
Epoch [31/120    avg_loss:0.034, val_acc:0.974]
Epoch [32/120    avg_loss:0.070, val_acc:0.976]
Epoch [33/120    avg_loss:0.035, val_acc:0.964]
Epoch [34/120    avg_loss:0.032, val_acc:0.970]
Epoch [35/120    avg_loss:0.026, val_acc:0.972]
Epoch [36/120    avg_loss:0.021, val_acc:0.979]
Epoch [37/120    avg_loss:0.023, val_acc:0.980]
Epoch [38/120    avg_loss:0.016, val_acc:0.985]
Epoch [39/120    avg_loss:0.017, val_acc:0.980]
Epoch [40/120    avg_loss:0.037, val_acc:0.956]
Epoch [41/120    avg_loss:0.047, val_acc:0.958]
Epoch [42/120    avg_loss:0.038, val_acc:0.975]
Epoch [43/120    avg_loss:0.041, val_acc:0.977]
Epoch [44/120    avg_loss:0.021, val_acc:0.977]
Epoch [45/120    avg_loss:0.044, val_acc:0.977]
Epoch [46/120    avg_loss:0.024, val_acc:0.977]
Epoch [47/120    avg_loss:0.011, val_acc:0.982]
Epoch [48/120    avg_loss:0.028, val_acc:0.971]
Epoch [49/120    avg_loss:0.015, val_acc:0.986]
Epoch [50/120    avg_loss:0.022, val_acc:0.983]
Epoch [51/120    avg_loss:0.014, val_acc:0.986]
Epoch [52/120    avg_loss:0.008, val_acc:0.983]
Epoch [53/120    avg_loss:0.014, val_acc:0.975]
Epoch [54/120    avg_loss:0.022, val_acc:0.971]
Epoch [55/120    avg_loss:0.028, val_acc:0.984]
Epoch [56/120    avg_loss:0.012, val_acc:0.980]
Epoch [57/120    avg_loss:0.011, val_acc:0.988]
Epoch [58/120    avg_loss:0.009, val_acc:0.978]
Epoch [59/120    avg_loss:0.007, val_acc:0.985]
Epoch [60/120    avg_loss:0.015, val_acc:0.979]
Epoch [61/120    avg_loss:0.013, val_acc:0.945]
Epoch [62/120    avg_loss:0.037, val_acc:0.979]
Epoch [63/120    avg_loss:0.013, val_acc:0.971]
Epoch [64/120    avg_loss:0.030, val_acc:0.985]
Epoch [65/120    avg_loss:0.028, val_acc:0.971]
Epoch [66/120    avg_loss:0.183, val_acc:0.957]
Epoch [67/120    avg_loss:0.052, val_acc:0.975]
Epoch [68/120    avg_loss:0.033, val_acc:0.974]
Epoch [69/120    avg_loss:0.017, val_acc:0.981]
Epoch [70/120    avg_loss:0.013, val_acc:0.973]
Epoch [71/120    avg_loss:0.009, val_acc:0.982]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.008, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.006, val_acc:0.984]
Epoch [78/120    avg_loss:0.006, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.985]
Epoch [82/120    avg_loss:0.005, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.014, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.005, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.005, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     0     0     0    17     9    19     0]
 [    0     0 18065     0    12     0     9     0     4     0]
 [    0     0     0  2009     4     0     0     0    15     8]
 [    0    43    20     0  2874     0     5     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4857     0     0    19]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     2     0     3    46     0     0     0  3497    23]
 [    0     0     0     0    17    54     0     0     0   848]]

Accuracy:
99.12274359530524

F1 scores:
[       nan 0.99300373 0.99870083 0.99258893 0.97012658 0.97972973
 0.99436995 0.99535963 0.9801009  0.93340671]

Kappa:
0.9883763478951146
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6d1a1b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.649, val_acc:0.492]
Epoch [2/120    avg_loss:1.087, val_acc:0.562]
Epoch [3/120    avg_loss:0.716, val_acc:0.762]
Epoch [4/120    avg_loss:0.568, val_acc:0.793]
Epoch [5/120    avg_loss:0.435, val_acc:0.858]
Epoch [6/120    avg_loss:0.448, val_acc:0.773]
Epoch [7/120    avg_loss:0.322, val_acc:0.887]
Epoch [8/120    avg_loss:0.253, val_acc:0.898]
Epoch [9/120    avg_loss:0.245, val_acc:0.891]
Epoch [10/120    avg_loss:0.212, val_acc:0.903]
Epoch [11/120    avg_loss:0.153, val_acc:0.945]
Epoch [12/120    avg_loss:0.129, val_acc:0.931]
Epoch [13/120    avg_loss:0.154, val_acc:0.929]
Epoch [14/120    avg_loss:0.194, val_acc:0.941]
Epoch [15/120    avg_loss:0.154, val_acc:0.892]
Epoch [16/120    avg_loss:0.155, val_acc:0.930]
Epoch [17/120    avg_loss:0.123, val_acc:0.953]
Epoch [18/120    avg_loss:0.116, val_acc:0.954]
Epoch [19/120    avg_loss:0.073, val_acc:0.960]
Epoch [20/120    avg_loss:0.061, val_acc:0.967]
Epoch [21/120    avg_loss:0.080, val_acc:0.977]
Epoch [22/120    avg_loss:0.090, val_acc:0.948]
Epoch [23/120    avg_loss:0.054, val_acc:0.963]
Epoch [24/120    avg_loss:0.063, val_acc:0.960]
Epoch [25/120    avg_loss:0.054, val_acc:0.952]
Epoch [26/120    avg_loss:0.072, val_acc:0.964]
Epoch [27/120    avg_loss:0.042, val_acc:0.972]
Epoch [28/120    avg_loss:0.046, val_acc:0.968]
Epoch [29/120    avg_loss:0.046, val_acc:0.977]
Epoch [30/120    avg_loss:0.052, val_acc:0.958]
Epoch [31/120    avg_loss:0.032, val_acc:0.976]
Epoch [32/120    avg_loss:0.030, val_acc:0.970]
Epoch [33/120    avg_loss:0.038, val_acc:0.949]
Epoch [34/120    avg_loss:0.036, val_acc:0.983]
Epoch [35/120    avg_loss:0.034, val_acc:0.981]
Epoch [36/120    avg_loss:0.024, val_acc:0.978]
Epoch [37/120    avg_loss:0.029, val_acc:0.984]
Epoch [38/120    avg_loss:0.045, val_acc:0.977]
Epoch [39/120    avg_loss:0.027, val_acc:0.984]
Epoch [40/120    avg_loss:0.019, val_acc:0.967]
Epoch [41/120    avg_loss:0.029, val_acc:0.974]
Epoch [42/120    avg_loss:0.025, val_acc:0.980]
Epoch [43/120    avg_loss:0.021, val_acc:0.977]
Epoch [44/120    avg_loss:0.027, val_acc:0.942]
Epoch [45/120    avg_loss:0.043, val_acc:0.971]
Epoch [46/120    avg_loss:0.056, val_acc:0.971]
Epoch [47/120    avg_loss:0.036, val_acc:0.983]
Epoch [48/120    avg_loss:0.033, val_acc:0.971]
Epoch [49/120    avg_loss:0.045, val_acc:0.931]
Epoch [50/120    avg_loss:0.032, val_acc:0.970]
Epoch [51/120    avg_loss:0.023, val_acc:0.985]
Epoch [52/120    avg_loss:0.022, val_acc:0.976]
Epoch [53/120    avg_loss:0.012, val_acc:0.970]
Epoch [54/120    avg_loss:0.011, val_acc:0.984]
Epoch [55/120    avg_loss:0.011, val_acc:0.986]
Epoch [56/120    avg_loss:0.007, val_acc:0.984]
Epoch [57/120    avg_loss:0.015, val_acc:0.974]
Epoch [58/120    avg_loss:0.017, val_acc:0.975]
Epoch [59/120    avg_loss:0.007, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.013, val_acc:0.984]
Epoch [62/120    avg_loss:0.006, val_acc:0.984]
Epoch [63/120    avg_loss:0.007, val_acc:0.988]
Epoch [64/120    avg_loss:0.014, val_acc:0.986]
Epoch [65/120    avg_loss:0.053, val_acc:0.968]
Epoch [66/120    avg_loss:0.070, val_acc:0.974]
Epoch [67/120    avg_loss:0.027, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.983]
Epoch [69/120    avg_loss:0.075, val_acc:0.982]
Epoch [70/120    avg_loss:0.099, val_acc:0.967]
Epoch [71/120    avg_loss:0.022, val_acc:0.969]
Epoch [72/120    avg_loss:0.026, val_acc:0.984]
Epoch [73/120    avg_loss:0.010, val_acc:0.977]
Epoch [74/120    avg_loss:0.012, val_acc:0.983]
Epoch [75/120    avg_loss:0.017, val_acc:0.970]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.984]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.009, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.006, val_acc:0.982]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.005, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     0     0    13     0     0     0]
 [    0     0 18049     0    23     0    12     0     6     0]
 [    0     0     0  2021     3     0     0     0    10     2]
 [    0    38    24     0  2879     0     0     0    31     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4848     0     0    29]
 [    0     0     0     0     0     0     5  1284     0     1]
 [    0     2     0     0    34     0     0     0  3530     5]
 [    0     0     0     0    17    41     0     0     0   861]]

Accuracy:
99.2842166148507

F1 scores:
[       nan 0.9958886  0.99817498 0.99630269 0.97132254 0.98453414
 0.99384994 0.997669   0.98768886 0.94771602]

Kappa:
0.9905165032409126
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff96218f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.645, val_acc:0.488]
Epoch [2/120    avg_loss:0.964, val_acc:0.497]
Epoch [3/120    avg_loss:0.687, val_acc:0.741]
Epoch [4/120    avg_loss:0.573, val_acc:0.774]
Epoch [5/120    avg_loss:0.486, val_acc:0.751]
Epoch [6/120    avg_loss:0.417, val_acc:0.793]
Epoch [7/120    avg_loss:0.333, val_acc:0.902]
Epoch [8/120    avg_loss:0.340, val_acc:0.896]
Epoch [9/120    avg_loss:0.257, val_acc:0.893]
Epoch [10/120    avg_loss:0.208, val_acc:0.878]
Epoch [11/120    avg_loss:0.201, val_acc:0.931]
Epoch [12/120    avg_loss:0.213, val_acc:0.924]
Epoch [13/120    avg_loss:0.183, val_acc:0.928]
Epoch [14/120    avg_loss:0.167, val_acc:0.925]
Epoch [15/120    avg_loss:0.130, val_acc:0.952]
Epoch [16/120    avg_loss:0.146, val_acc:0.924]
Epoch [17/120    avg_loss:0.115, val_acc:0.933]
Epoch [18/120    avg_loss:0.104, val_acc:0.951]
Epoch [19/120    avg_loss:0.145, val_acc:0.956]
Epoch [20/120    avg_loss:0.071, val_acc:0.969]
Epoch [21/120    avg_loss:0.076, val_acc:0.977]
Epoch [22/120    avg_loss:0.096, val_acc:0.973]
Epoch [23/120    avg_loss:0.085, val_acc:0.965]
Epoch [24/120    avg_loss:0.048, val_acc:0.973]
Epoch [25/120    avg_loss:0.041, val_acc:0.977]
Epoch [26/120    avg_loss:0.040, val_acc:0.979]
Epoch [27/120    avg_loss:0.042, val_acc:0.949]
Epoch [28/120    avg_loss:0.085, val_acc:0.959]
Epoch [29/120    avg_loss:0.038, val_acc:0.976]
Epoch [30/120    avg_loss:0.054, val_acc:0.973]
Epoch [31/120    avg_loss:0.025, val_acc:0.977]
Epoch [32/120    avg_loss:0.021, val_acc:0.982]
Epoch [33/120    avg_loss:0.038, val_acc:0.974]
Epoch [34/120    avg_loss:0.033, val_acc:0.953]
Epoch [35/120    avg_loss:0.033, val_acc:0.979]
Epoch [36/120    avg_loss:0.018, val_acc:0.982]
Epoch [37/120    avg_loss:0.018, val_acc:0.984]
Epoch [38/120    avg_loss:0.024, val_acc:0.984]
Epoch [39/120    avg_loss:0.024, val_acc:0.961]
Epoch [40/120    avg_loss:0.029, val_acc:0.983]
Epoch [41/120    avg_loss:0.035, val_acc:0.979]
Epoch [42/120    avg_loss:0.017, val_acc:0.985]
Epoch [43/120    avg_loss:0.013, val_acc:0.984]
Epoch [44/120    avg_loss:0.011, val_acc:0.985]
Epoch [45/120    avg_loss:0.009, val_acc:0.984]
Epoch [46/120    avg_loss:0.011, val_acc:0.985]
Epoch [47/120    avg_loss:0.013, val_acc:0.974]
Epoch [48/120    avg_loss:0.010, val_acc:0.985]
Epoch [49/120    avg_loss:0.010, val_acc:0.985]
Epoch [50/120    avg_loss:0.007, val_acc:0.988]
Epoch [51/120    avg_loss:0.007, val_acc:0.955]
Epoch [52/120    avg_loss:0.008, val_acc:0.984]
Epoch [53/120    avg_loss:0.010, val_acc:0.979]
Epoch [54/120    avg_loss:0.015, val_acc:0.983]
Epoch [55/120    avg_loss:0.007, val_acc:0.981]
Epoch [56/120    avg_loss:0.008, val_acc:0.990]
Epoch [57/120    avg_loss:0.016, val_acc:0.988]
Epoch [58/120    avg_loss:0.009, val_acc:0.987]
Epoch [59/120    avg_loss:0.006, val_acc:0.990]
Epoch [60/120    avg_loss:0.007, val_acc:0.991]
Epoch [61/120    avg_loss:0.014, val_acc:0.978]
Epoch [62/120    avg_loss:0.018, val_acc:0.990]
Epoch [63/120    avg_loss:0.006, val_acc:0.988]
Epoch [64/120    avg_loss:0.006, val_acc:0.990]
Epoch [65/120    avg_loss:0.006, val_acc:0.992]
Epoch [66/120    avg_loss:0.005, val_acc:0.985]
Epoch [67/120    avg_loss:0.008, val_acc:0.989]
Epoch [68/120    avg_loss:0.005, val_acc:0.989]
Epoch [69/120    avg_loss:0.004, val_acc:0.992]
Epoch [70/120    avg_loss:0.003, val_acc:0.993]
Epoch [71/120    avg_loss:0.006, val_acc:0.991]
Epoch [72/120    avg_loss:0.005, val_acc:0.968]
Epoch [73/120    avg_loss:0.010, val_acc:0.990]
Epoch [74/120    avg_loss:0.006, val_acc:0.991]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.991]
Epoch [77/120    avg_loss:0.003, val_acc:0.991]
Epoch [78/120    avg_loss:0.002, val_acc:0.990]
Epoch [79/120    avg_loss:0.003, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.989]
Epoch [81/120    avg_loss:0.027, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.991]
Epoch [84/120    avg_loss:0.003, val_acc:0.991]
Epoch [85/120    avg_loss:0.004, val_acc:0.991]
Epoch [86/120    avg_loss:0.003, val_acc:0.992]
Epoch [87/120    avg_loss:0.004, val_acc:0.992]
Epoch [88/120    avg_loss:0.004, val_acc:0.992]
Epoch [89/120    avg_loss:0.003, val_acc:0.991]
Epoch [90/120    avg_loss:0.003, val_acc:0.992]
Epoch [91/120    avg_loss:0.004, val_acc:0.992]
Epoch [92/120    avg_loss:0.004, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.003, val_acc:0.991]
Epoch [95/120    avg_loss:0.003, val_acc:0.991]
Epoch [96/120    avg_loss:0.003, val_acc:0.991]
Epoch [97/120    avg_loss:0.003, val_acc:0.991]
Epoch [98/120    avg_loss:0.003, val_acc:0.991]
Epoch [99/120    avg_loss:0.003, val_acc:0.991]
Epoch [100/120    avg_loss:0.003, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.002, val_acc:0.991]
Epoch [103/120    avg_loss:0.002, val_acc:0.991]
Epoch [104/120    avg_loss:0.002, val_acc:0.991]
Epoch [105/120    avg_loss:0.003, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.002, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.012, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.002, val_acc:0.991]
Epoch [115/120    avg_loss:0.003, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.002, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.002, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     0     0     0     9    18     0     0]
 [    0     0 18078     0     8     0     2     0     2     0]
 [    0     6     0  2028     1     0     0     0     0     1]
 [    0    56    19     0  2869     0     1     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4865     0     0    12]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     3     0     5    45     0     0     0  3518     0]
 [    0     0     0     0    14    37     0     0     0   868]]

Accuracy:
99.3516978767503

F1 scores:
[       nan 0.99286932 0.99911573 0.99680511 0.97106109 0.98602191
 0.99743721 0.99229584 0.98847991 0.96337403]

Kappa:
0.9914081222712372
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83875ce860>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.548, val_acc:0.621]
Epoch [2/120    avg_loss:0.897, val_acc:0.643]
Epoch [3/120    avg_loss:0.631, val_acc:0.623]
Epoch [4/120    avg_loss:0.511, val_acc:0.710]
Epoch [5/120    avg_loss:0.417, val_acc:0.736]
Epoch [6/120    avg_loss:0.380, val_acc:0.813]
Epoch [7/120    avg_loss:0.345, val_acc:0.783]
Epoch [8/120    avg_loss:0.306, val_acc:0.853]
Epoch [9/120    avg_loss:0.229, val_acc:0.857]
Epoch [10/120    avg_loss:0.239, val_acc:0.886]
Epoch [11/120    avg_loss:0.231, val_acc:0.884]
Epoch [12/120    avg_loss:0.221, val_acc:0.924]
Epoch [13/120    avg_loss:0.137, val_acc:0.911]
Epoch [14/120    avg_loss:0.146, val_acc:0.922]
Epoch [15/120    avg_loss:0.122, val_acc:0.894]
Epoch [16/120    avg_loss:0.108, val_acc:0.927]
Epoch [17/120    avg_loss:0.106, val_acc:0.942]
Epoch [18/120    avg_loss:0.090, val_acc:0.941]
Epoch [19/120    avg_loss:0.123, val_acc:0.952]
Epoch [20/120    avg_loss:0.093, val_acc:0.960]
Epoch [21/120    avg_loss:0.067, val_acc:0.958]
Epoch [22/120    avg_loss:0.074, val_acc:0.941]
Epoch [23/120    avg_loss:0.082, val_acc:0.945]
Epoch [24/120    avg_loss:0.103, val_acc:0.961]
Epoch [25/120    avg_loss:0.070, val_acc:0.951]
Epoch [26/120    avg_loss:0.066, val_acc:0.937]
Epoch [27/120    avg_loss:0.062, val_acc:0.948]
Epoch [28/120    avg_loss:0.054, val_acc:0.958]
Epoch [29/120    avg_loss:0.049, val_acc:0.950]
Epoch [30/120    avg_loss:0.067, val_acc:0.955]
Epoch [31/120    avg_loss:0.074, val_acc:0.966]
Epoch [32/120    avg_loss:0.041, val_acc:0.967]
Epoch [33/120    avg_loss:0.033, val_acc:0.967]
Epoch [34/120    avg_loss:0.057, val_acc:0.930]
Epoch [35/120    avg_loss:0.038, val_acc:0.938]
Epoch [36/120    avg_loss:0.048, val_acc:0.939]
Epoch [37/120    avg_loss:0.043, val_acc:0.967]
Epoch [38/120    avg_loss:0.042, val_acc:0.971]
Epoch [39/120    avg_loss:0.029, val_acc:0.976]
Epoch [40/120    avg_loss:0.035, val_acc:0.943]
Epoch [41/120    avg_loss:0.029, val_acc:0.954]
Epoch [42/120    avg_loss:0.030, val_acc:0.973]
Epoch [43/120    avg_loss:0.017, val_acc:0.981]
Epoch [44/120    avg_loss:0.034, val_acc:0.937]
Epoch [45/120    avg_loss:0.031, val_acc:0.966]
Epoch [46/120    avg_loss:0.036, val_acc:0.975]
Epoch [47/120    avg_loss:0.029, val_acc:0.965]
Epoch [48/120    avg_loss:0.030, val_acc:0.978]
Epoch [49/120    avg_loss:0.017, val_acc:0.977]
Epoch [50/120    avg_loss:0.014, val_acc:0.978]
Epoch [51/120    avg_loss:0.030, val_acc:0.973]
Epoch [52/120    avg_loss:0.012, val_acc:0.968]
Epoch [53/120    avg_loss:0.014, val_acc:0.978]
Epoch [54/120    avg_loss:0.027, val_acc:0.973]
Epoch [55/120    avg_loss:0.015, val_acc:0.968]
Epoch [56/120    avg_loss:0.013, val_acc:0.980]
Epoch [57/120    avg_loss:0.008, val_acc:0.979]
Epoch [58/120    avg_loss:0.017, val_acc:0.978]
Epoch [59/120    avg_loss:0.009, val_acc:0.980]
Epoch [60/120    avg_loss:0.008, val_acc:0.979]
Epoch [61/120    avg_loss:0.011, val_acc:0.981]
Epoch [62/120    avg_loss:0.010, val_acc:0.979]
Epoch [63/120    avg_loss:0.013, val_acc:0.979]
Epoch [64/120    avg_loss:0.009, val_acc:0.981]
Epoch [65/120    avg_loss:0.008, val_acc:0.979]
Epoch [66/120    avg_loss:0.009, val_acc:0.978]
Epoch [67/120    avg_loss:0.006, val_acc:0.980]
Epoch [68/120    avg_loss:0.007, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.979]
Epoch [70/120    avg_loss:0.013, val_acc:0.981]
Epoch [71/120    avg_loss:0.016, val_acc:0.980]
Epoch [72/120    avg_loss:0.009, val_acc:0.979]
Epoch [73/120    avg_loss:0.006, val_acc:0.979]
Epoch [74/120    avg_loss:0.007, val_acc:0.980]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.009, val_acc:0.980]
Epoch [77/120    avg_loss:0.007, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.981]
Epoch [80/120    avg_loss:0.008, val_acc:0.978]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.005, val_acc:0.982]
Epoch [83/120    avg_loss:0.007, val_acc:0.978]
Epoch [84/120    avg_loss:0.007, val_acc:0.979]
Epoch [85/120    avg_loss:0.006, val_acc:0.981]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.006, val_acc:0.982]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.980]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.979]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.010, val_acc:0.982]
Epoch [97/120    avg_loss:0.007, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.004, val_acc:0.982]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.006, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.982]
Epoch [105/120    avg_loss:0.007, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.980]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.007, val_acc:0.982]
Epoch [119/120    avg_loss:0.006, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0     8     2     0     4     0    61     3]
 [    0     0 18015     0    20     0    48     0     7     0]
 [    0     1     0  1943     0     0     0     0    89     3]
 [    0    33     4     0  2924     0     4     0     6     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     0     0     0  4848     0    10     0]
 [    0    12     0     0     0     0     1  1275     0     2]
 [    0    15     0    51    40     0     0     0  3465     0]
 [    0     0     0     0     4     7     0     0     0   908]]

Accuracy:
98.9010194490637

F1 scores:
[       nan 0.98918035 0.99725982 0.9623576  0.9808789  0.99732518
 0.99110702 0.99415205 0.96129838 0.98910675]

Kappa:
0.9854455590206894
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f31595377b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.493, val_acc:0.459]
Epoch [2/120    avg_loss:0.917, val_acc:0.477]
Epoch [3/120    avg_loss:0.668, val_acc:0.731]
Epoch [4/120    avg_loss:0.504, val_acc:0.735]
Epoch [5/120    avg_loss:0.415, val_acc:0.796]
Epoch [6/120    avg_loss:0.381, val_acc:0.795]
Epoch [7/120    avg_loss:0.310, val_acc:0.865]
Epoch [8/120    avg_loss:0.273, val_acc:0.895]
Epoch [9/120    avg_loss:0.251, val_acc:0.858]
Epoch [10/120    avg_loss:0.247, val_acc:0.898]
Epoch [11/120    avg_loss:0.187, val_acc:0.905]
Epoch [12/120    avg_loss:0.181, val_acc:0.943]
Epoch [13/120    avg_loss:0.161, val_acc:0.931]
Epoch [14/120    avg_loss:0.141, val_acc:0.950]
Epoch [15/120    avg_loss:0.150, val_acc:0.952]
Epoch [16/120    avg_loss:0.128, val_acc:0.957]
Epoch [17/120    avg_loss:0.107, val_acc:0.955]
Epoch [18/120    avg_loss:0.103, val_acc:0.935]
Epoch [19/120    avg_loss:0.102, val_acc:0.914]
Epoch [20/120    avg_loss:0.116, val_acc:0.961]
Epoch [21/120    avg_loss:0.102, val_acc:0.961]
Epoch [22/120    avg_loss:0.089, val_acc:0.958]
Epoch [23/120    avg_loss:0.077, val_acc:0.967]
Epoch [24/120    avg_loss:0.058, val_acc:0.964]
Epoch [25/120    avg_loss:0.061, val_acc:0.969]
Epoch [26/120    avg_loss:0.065, val_acc:0.952]
Epoch [27/120    avg_loss:0.072, val_acc:0.918]
Epoch [28/120    avg_loss:0.093, val_acc:0.965]
Epoch [29/120    avg_loss:0.078, val_acc:0.957]
Epoch [30/120    avg_loss:0.097, val_acc:0.956]
Epoch [31/120    avg_loss:0.060, val_acc:0.969]
Epoch [32/120    avg_loss:0.048, val_acc:0.980]
Epoch [33/120    avg_loss:0.054, val_acc:0.979]
Epoch [34/120    avg_loss:0.032, val_acc:0.971]
Epoch [35/120    avg_loss:0.028, val_acc:0.981]
Epoch [36/120    avg_loss:0.028, val_acc:0.980]
Epoch [37/120    avg_loss:0.023, val_acc:0.972]
Epoch [38/120    avg_loss:0.034, val_acc:0.976]
Epoch [39/120    avg_loss:0.030, val_acc:0.985]
Epoch [40/120    avg_loss:0.034, val_acc:0.969]
Epoch [41/120    avg_loss:0.027, val_acc:0.980]
Epoch [42/120    avg_loss:0.021, val_acc:0.981]
Epoch [43/120    avg_loss:0.017, val_acc:0.984]
Epoch [44/120    avg_loss:0.030, val_acc:0.953]
Epoch [45/120    avg_loss:0.037, val_acc:0.981]
Epoch [46/120    avg_loss:0.022, val_acc:0.984]
Epoch [47/120    avg_loss:0.041, val_acc:0.982]
Epoch [48/120    avg_loss:0.027, val_acc:0.974]
Epoch [49/120    avg_loss:0.019, val_acc:0.982]
Epoch [50/120    avg_loss:0.016, val_acc:0.982]
Epoch [51/120    avg_loss:0.017, val_acc:0.985]
Epoch [52/120    avg_loss:0.100, val_acc:0.975]
Epoch [53/120    avg_loss:0.052, val_acc:0.982]
Epoch [54/120    avg_loss:0.027, val_acc:0.978]
Epoch [55/120    avg_loss:0.017, val_acc:0.987]
Epoch [56/120    avg_loss:0.011, val_acc:0.985]
Epoch [57/120    avg_loss:0.031, val_acc:0.981]
Epoch [58/120    avg_loss:0.027, val_acc:0.971]
Epoch [59/120    avg_loss:0.049, val_acc:0.972]
Epoch [60/120    avg_loss:0.037, val_acc:0.980]
Epoch [61/120    avg_loss:0.030, val_acc:0.982]
Epoch [62/120    avg_loss:0.019, val_acc:0.981]
Epoch [63/120    avg_loss:0.014, val_acc:0.986]
Epoch [64/120    avg_loss:0.019, val_acc:0.988]
Epoch [65/120    avg_loss:0.019, val_acc:0.986]
Epoch [66/120    avg_loss:0.014, val_acc:0.984]
Epoch [67/120    avg_loss:0.018, val_acc:0.983]
Epoch [68/120    avg_loss:0.018, val_acc:0.982]
Epoch [69/120    avg_loss:0.021, val_acc:0.987]
Epoch [70/120    avg_loss:0.017, val_acc:0.976]
Epoch [71/120    avg_loss:0.026, val_acc:0.984]
Epoch [72/120    avg_loss:0.015, val_acc:0.984]
Epoch [73/120    avg_loss:0.015, val_acc:0.989]
Epoch [74/120    avg_loss:0.010, val_acc:0.980]
Epoch [75/120    avg_loss:0.008, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.993]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.989]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.989]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.018, val_acc:0.974]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.014, val_acc:0.972]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.028, val_acc:0.982]
Epoch [89/120    avg_loss:0.016, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.009, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.989]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.008, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.990]
Epoch [102/120    avg_loss:0.004, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     2     0     0     0     0    11     0]
 [    0     0 18058     0    14     0    17     0     1     0]
 [    0     3     0  1920     0     0     0     0   111     2]
 [    0    22     5     0  2934     0     3     0     6     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     3     0     0     0     0     0  1281     0     6]
 [    0     4     0    57    53     0     0     0  3457     0]
 [    0     0     0     0     7     7     0     0     0   905]]

Accuracy:
99.19022485720483

F1 scores:
[       nan 0.99650702 0.99897657 0.95641345 0.9812709  0.99732518
 0.99795417 0.99649942 0.96604723 0.98691385]

Kappa:
0.9892718918157214
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb11e5e5748>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.568, val_acc:0.647]
Epoch [2/120    avg_loss:0.945, val_acc:0.740]
Epoch [3/120    avg_loss:0.714, val_acc:0.578]
Epoch [4/120    avg_loss:0.522, val_acc:0.750]
Epoch [5/120    avg_loss:0.473, val_acc:0.708]
Epoch [6/120    avg_loss:0.414, val_acc:0.823]
Epoch [7/120    avg_loss:0.358, val_acc:0.793]
Epoch [8/120    avg_loss:0.313, val_acc:0.851]
Epoch [9/120    avg_loss:0.291, val_acc:0.833]
Epoch [10/120    avg_loss:0.225, val_acc:0.903]
Epoch [11/120    avg_loss:0.176, val_acc:0.859]
Epoch [12/120    avg_loss:0.146, val_acc:0.902]
Epoch [13/120    avg_loss:0.173, val_acc:0.900]
Epoch [14/120    avg_loss:0.205, val_acc:0.889]
Epoch [15/120    avg_loss:0.143, val_acc:0.919]
Epoch [16/120    avg_loss:0.103, val_acc:0.940]
Epoch [17/120    avg_loss:0.122, val_acc:0.926]
Epoch [18/120    avg_loss:0.116, val_acc:0.935]
Epoch [19/120    avg_loss:0.070, val_acc:0.940]
Epoch [20/120    avg_loss:0.073, val_acc:0.943]
Epoch [21/120    avg_loss:0.091, val_acc:0.888]
Epoch [22/120    avg_loss:0.102, val_acc:0.918]
Epoch [23/120    avg_loss:0.092, val_acc:0.913]
Epoch [24/120    avg_loss:0.069, val_acc:0.945]
Epoch [25/120    avg_loss:0.082, val_acc:0.941]
Epoch [26/120    avg_loss:0.060, val_acc:0.944]
Epoch [27/120    avg_loss:0.063, val_acc:0.931]
Epoch [28/120    avg_loss:0.063, val_acc:0.962]
Epoch [29/120    avg_loss:0.063, val_acc:0.943]
Epoch [30/120    avg_loss:0.050, val_acc:0.944]
Epoch [31/120    avg_loss:0.045, val_acc:0.932]
Epoch [32/120    avg_loss:0.036, val_acc:0.961]
Epoch [33/120    avg_loss:0.037, val_acc:0.967]
Epoch [34/120    avg_loss:0.030, val_acc:0.971]
Epoch [35/120    avg_loss:0.031, val_acc:0.961]
Epoch [36/120    avg_loss:0.034, val_acc:0.948]
Epoch [37/120    avg_loss:0.065, val_acc:0.952]
Epoch [38/120    avg_loss:0.044, val_acc:0.952]
Epoch [39/120    avg_loss:0.042, val_acc:0.940]
Epoch [40/120    avg_loss:0.041, val_acc:0.967]
Epoch [41/120    avg_loss:0.024, val_acc:0.950]
Epoch [42/120    avg_loss:0.025, val_acc:0.969]
Epoch [43/120    avg_loss:0.022, val_acc:0.967]
Epoch [44/120    avg_loss:0.023, val_acc:0.958]
Epoch [45/120    avg_loss:0.037, val_acc:0.954]
Epoch [46/120    avg_loss:0.040, val_acc:0.969]
Epoch [47/120    avg_loss:0.028, val_acc:0.971]
Epoch [48/120    avg_loss:0.052, val_acc:0.944]
Epoch [49/120    avg_loss:0.064, val_acc:0.931]
Epoch [50/120    avg_loss:0.042, val_acc:0.963]
Epoch [51/120    avg_loss:0.034, val_acc:0.959]
Epoch [52/120    avg_loss:0.025, val_acc:0.964]
Epoch [53/120    avg_loss:0.024, val_acc:0.952]
Epoch [54/120    avg_loss:0.014, val_acc:0.972]
Epoch [55/120    avg_loss:0.012, val_acc:0.975]
Epoch [56/120    avg_loss:0.011, val_acc:0.967]
Epoch [57/120    avg_loss:0.036, val_acc:0.962]
Epoch [58/120    avg_loss:0.025, val_acc:0.962]
Epoch [59/120    avg_loss:0.012, val_acc:0.974]
Epoch [60/120    avg_loss:0.018, val_acc:0.977]
Epoch [61/120    avg_loss:0.014, val_acc:0.978]
Epoch [62/120    avg_loss:0.008, val_acc:0.976]
Epoch [63/120    avg_loss:0.013, val_acc:0.980]
Epoch [64/120    avg_loss:0.019, val_acc:0.975]
Epoch [65/120    avg_loss:0.015, val_acc:0.979]
Epoch [66/120    avg_loss:0.010, val_acc:0.971]
Epoch [67/120    avg_loss:0.011, val_acc:0.958]
Epoch [68/120    avg_loss:0.011, val_acc:0.973]
Epoch [69/120    avg_loss:0.008, val_acc:0.978]
Epoch [70/120    avg_loss:0.007, val_acc:0.977]
Epoch [71/120    avg_loss:0.019, val_acc:0.970]
Epoch [72/120    avg_loss:0.017, val_acc:0.979]
Epoch [73/120    avg_loss:0.012, val_acc:0.970]
Epoch [74/120    avg_loss:0.018, val_acc:0.971]
Epoch [75/120    avg_loss:0.008, val_acc:0.976]
Epoch [76/120    avg_loss:0.007, val_acc:0.978]
Epoch [77/120    avg_loss:0.004, val_acc:0.979]
Epoch [78/120    avg_loss:0.006, val_acc:0.976]
Epoch [79/120    avg_loss:0.005, val_acc:0.977]
Epoch [80/120    avg_loss:0.004, val_acc:0.978]
Epoch [81/120    avg_loss:0.007, val_acc:0.977]
Epoch [82/120    avg_loss:0.005, val_acc:0.976]
Epoch [83/120    avg_loss:0.004, val_acc:0.977]
Epoch [84/120    avg_loss:0.004, val_acc:0.977]
Epoch [85/120    avg_loss:0.008, val_acc:0.976]
Epoch [86/120    avg_loss:0.005, val_acc:0.978]
Epoch [87/120    avg_loss:0.005, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.977]
Epoch [89/120    avg_loss:0.004, val_acc:0.978]
Epoch [90/120    avg_loss:0.005, val_acc:0.978]
Epoch [91/120    avg_loss:0.004, val_acc:0.978]
Epoch [92/120    avg_loss:0.005, val_acc:0.978]
Epoch [93/120    avg_loss:0.005, val_acc:0.978]
Epoch [94/120    avg_loss:0.004, val_acc:0.978]
Epoch [95/120    avg_loss:0.005, val_acc:0.978]
Epoch [96/120    avg_loss:0.006, val_acc:0.978]
Epoch [97/120    avg_loss:0.005, val_acc:0.978]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.004, val_acc:0.978]
Epoch [101/120    avg_loss:0.004, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.004, val_acc:0.978]
Epoch [104/120    avg_loss:0.004, val_acc:0.978]
Epoch [105/120    avg_loss:0.007, val_acc:0.978]
Epoch [106/120    avg_loss:0.007, val_acc:0.978]
Epoch [107/120    avg_loss:0.004, val_acc:0.978]
Epoch [108/120    avg_loss:0.003, val_acc:0.978]
Epoch [109/120    avg_loss:0.005, val_acc:0.978]
Epoch [110/120    avg_loss:0.004, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.004, val_acc:0.978]
Epoch [113/120    avg_loss:0.005, val_acc:0.978]
Epoch [114/120    avg_loss:0.003, val_acc:0.978]
Epoch [115/120    avg_loss:0.005, val_acc:0.978]
Epoch [116/120    avg_loss:0.004, val_acc:0.978]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.006, val_acc:0.978]
Epoch [119/120    avg_loss:0.004, val_acc:0.978]
Epoch [120/120    avg_loss:0.004, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     0     0     0     1    62     0]
 [    0     0 18031     0    16     0    35     0     8     0]
 [    0     4     0  1860     0     0     0     0   170     2]
 [    0    25     5     0  2930     0     5     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30     0     0     0  4847     0     1     0]
 [    0     2     0     0     0     0     0  1287     0     1]
 [    0     7     0    84    49     0     0     0  3431     0]
 [    0     0     0     0     8    12     0     0     0   899]]

Accuracy:
98.71303593377196

F1 scores:
[       nan 0.99213334 0.99740015 0.93467337 0.98075314 0.99542334
 0.99272913 0.99844841 0.94648276 0.98736958]

Kappa:
0.9829504013124667
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c113587b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.628, val_acc:0.375]
Epoch [2/120    avg_loss:0.992, val_acc:0.667]
Epoch [3/120    avg_loss:0.746, val_acc:0.689]
Epoch [4/120    avg_loss:0.583, val_acc:0.731]
Epoch [5/120    avg_loss:0.477, val_acc:0.696]
Epoch [6/120    avg_loss:0.410, val_acc:0.738]
Epoch [7/120    avg_loss:0.362, val_acc:0.763]
Epoch [8/120    avg_loss:0.363, val_acc:0.816]
Epoch [9/120    avg_loss:0.279, val_acc:0.848]
Epoch [10/120    avg_loss:0.242, val_acc:0.874]
Epoch [11/120    avg_loss:0.228, val_acc:0.898]
Epoch [12/120    avg_loss:0.244, val_acc:0.906]
Epoch [13/120    avg_loss:0.205, val_acc:0.842]
Epoch [14/120    avg_loss:0.167, val_acc:0.917]
Epoch [15/120    avg_loss:0.162, val_acc:0.912]
Epoch [16/120    avg_loss:0.135, val_acc:0.940]
Epoch [17/120    avg_loss:0.139, val_acc:0.922]
Epoch [18/120    avg_loss:0.173, val_acc:0.915]
Epoch [19/120    avg_loss:0.139, val_acc:0.951]
Epoch [20/120    avg_loss:0.090, val_acc:0.945]
Epoch [21/120    avg_loss:0.085, val_acc:0.948]
Epoch [22/120    avg_loss:0.092, val_acc:0.932]
Epoch [23/120    avg_loss:0.081, val_acc:0.922]
Epoch [24/120    avg_loss:0.078, val_acc:0.942]
Epoch [25/120    avg_loss:0.069, val_acc:0.953]
Epoch [26/120    avg_loss:0.071, val_acc:0.957]
Epoch [27/120    avg_loss:0.048, val_acc:0.950]
Epoch [28/120    avg_loss:0.051, val_acc:0.958]
Epoch [29/120    avg_loss:0.050, val_acc:0.956]
Epoch [30/120    avg_loss:0.042, val_acc:0.966]
Epoch [31/120    avg_loss:0.036, val_acc:0.981]
Epoch [32/120    avg_loss:0.044, val_acc:0.964]
Epoch [33/120    avg_loss:0.038, val_acc:0.968]
Epoch [34/120    avg_loss:0.042, val_acc:0.970]
Epoch [35/120    avg_loss:0.032, val_acc:0.968]
Epoch [36/120    avg_loss:0.033, val_acc:0.958]
Epoch [37/120    avg_loss:0.041, val_acc:0.963]
Epoch [38/120    avg_loss:0.047, val_acc:0.973]
Epoch [39/120    avg_loss:0.031, val_acc:0.968]
Epoch [40/120    avg_loss:0.047, val_acc:0.968]
Epoch [41/120    avg_loss:0.028, val_acc:0.965]
Epoch [42/120    avg_loss:0.020, val_acc:0.983]
Epoch [43/120    avg_loss:0.023, val_acc:0.983]
Epoch [44/120    avg_loss:0.023, val_acc:0.977]
Epoch [45/120    avg_loss:0.015, val_acc:0.981]
Epoch [46/120    avg_loss:0.015, val_acc:0.968]
Epoch [47/120    avg_loss:0.011, val_acc:0.978]
Epoch [48/120    avg_loss:0.023, val_acc:0.982]
Epoch [49/120    avg_loss:0.011, val_acc:0.985]
Epoch [50/120    avg_loss:0.013, val_acc:0.984]
Epoch [51/120    avg_loss:0.093, val_acc:0.954]
Epoch [52/120    avg_loss:0.058, val_acc:0.975]
Epoch [53/120    avg_loss:0.023, val_acc:0.981]
Epoch [54/120    avg_loss:0.019, val_acc:0.983]
Epoch [55/120    avg_loss:0.010, val_acc:0.987]
Epoch [56/120    avg_loss:0.036, val_acc:0.969]
Epoch [57/120    avg_loss:0.048, val_acc:0.975]
Epoch [58/120    avg_loss:0.018, val_acc:0.975]
Epoch [59/120    avg_loss:0.013, val_acc:0.983]
Epoch [60/120    avg_loss:0.018, val_acc:0.983]
Epoch [61/120    avg_loss:0.009, val_acc:0.982]
Epoch [62/120    avg_loss:0.011, val_acc:0.987]
Epoch [63/120    avg_loss:0.031, val_acc:0.988]
Epoch [64/120    avg_loss:0.014, val_acc:0.984]
Epoch [65/120    avg_loss:0.012, val_acc:0.991]
Epoch [66/120    avg_loss:0.013, val_acc:0.987]
Epoch [67/120    avg_loss:0.008, val_acc:0.983]
Epoch [68/120    avg_loss:0.006, val_acc:0.984]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.016, val_acc:0.984]
Epoch [71/120    avg_loss:0.023, val_acc:0.978]
Epoch [72/120    avg_loss:0.019, val_acc:0.990]
Epoch [73/120    avg_loss:0.010, val_acc:0.987]
Epoch [74/120    avg_loss:0.014, val_acc:0.993]
Epoch [75/120    avg_loss:0.013, val_acc:0.987]
Epoch [76/120    avg_loss:0.009, val_acc:0.991]
Epoch [77/120    avg_loss:0.007, val_acc:0.991]
Epoch [78/120    avg_loss:0.005, val_acc:0.992]
Epoch [79/120    avg_loss:0.005, val_acc:0.992]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.042, val_acc:0.978]
Epoch [82/120    avg_loss:0.015, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.993]
Epoch [84/120    avg_loss:0.009, val_acc:0.991]
Epoch [85/120    avg_loss:0.005, val_acc:0.992]
Epoch [86/120    avg_loss:0.008, val_acc:0.993]
Epoch [87/120    avg_loss:0.017, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.038, val_acc:0.972]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.979]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.019, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.993]
Epoch [99/120    avg_loss:0.006, val_acc:0.993]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.003, val_acc:0.992]
Epoch [102/120    avg_loss:0.004, val_acc:0.993]
Epoch [103/120    avg_loss:0.003, val_acc:0.993]
Epoch [104/120    avg_loss:0.003, val_acc:0.993]
Epoch [105/120    avg_loss:0.003, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.993]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.993]
Epoch [109/120    avg_loss:0.003, val_acc:0.993]
Epoch [110/120    avg_loss:0.002, val_acc:0.993]
Epoch [111/120    avg_loss:0.003, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.993]
Epoch [113/120    avg_loss:0.004, val_acc:0.993]
Epoch [114/120    avg_loss:0.005, val_acc:0.993]
Epoch [115/120    avg_loss:0.003, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.003, val_acc:0.993]
Epoch [118/120    avg_loss:0.003, val_acc:0.994]
Epoch [119/120    avg_loss:0.003, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     0     0     0     1    18    40     0]
 [    0     0 18066     0     8     0    13     0     3     0]
 [    0     0     0  1910     0     0     0     0   123     3]
 [    0    37     4     0  2921     0     4     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4869     0     0     0]
 [    0     7     0     0     0     0     0  1282     0     1]
 [    0    18     0    46    27     0     1     0  3478     1]
 [    0     0     0     0     2    16     0     0     0   901]]

Accuracy:
99.06490251367701

F1 scores:
[       nan 0.9905961  0.99897702 0.95691383 0.9851602  0.99390708
 0.99713291 0.98996139 0.96330148 0.98739726]

Kappa:
0.9876096934722409
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb055db5780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.486, val_acc:0.727]
Epoch [2/120    avg_loss:0.920, val_acc:0.601]
Epoch [3/120    avg_loss:0.656, val_acc:0.656]
Epoch [4/120    avg_loss:0.554, val_acc:0.773]
Epoch [5/120    avg_loss:0.439, val_acc:0.784]
Epoch [6/120    avg_loss:0.347, val_acc:0.780]
Epoch [7/120    avg_loss:0.300, val_acc:0.875]
Epoch [8/120    avg_loss:0.294, val_acc:0.832]
Epoch [9/120    avg_loss:0.239, val_acc:0.888]
Epoch [10/120    avg_loss:0.232, val_acc:0.891]
Epoch [11/120    avg_loss:0.196, val_acc:0.837]
Epoch [12/120    avg_loss:0.238, val_acc:0.868]
Epoch [13/120    avg_loss:0.156, val_acc:0.883]
Epoch [14/120    avg_loss:0.127, val_acc:0.907]
Epoch [15/120    avg_loss:0.214, val_acc:0.852]
Epoch [16/120    avg_loss:0.272, val_acc:0.908]
Epoch [17/120    avg_loss:0.155, val_acc:0.867]
Epoch [18/120    avg_loss:0.129, val_acc:0.861]
Epoch [19/120    avg_loss:0.134, val_acc:0.942]
Epoch [20/120    avg_loss:0.096, val_acc:0.846]
Epoch [21/120    avg_loss:0.081, val_acc:0.961]
Epoch [22/120    avg_loss:0.072, val_acc:0.957]
Epoch [23/120    avg_loss:0.080, val_acc:0.939]
Epoch [24/120    avg_loss:0.073, val_acc:0.945]
Epoch [25/120    avg_loss:0.059, val_acc:0.960]
Epoch [26/120    avg_loss:0.053, val_acc:0.925]
Epoch [27/120    avg_loss:0.059, val_acc:0.962]
Epoch [28/120    avg_loss:0.054, val_acc:0.965]
Epoch [29/120    avg_loss:0.083, val_acc:0.963]
Epoch [30/120    avg_loss:0.057, val_acc:0.963]
Epoch [31/120    avg_loss:0.073, val_acc:0.905]
Epoch [32/120    avg_loss:0.120, val_acc:0.941]
Epoch [33/120    avg_loss:0.053, val_acc:0.963]
Epoch [34/120    avg_loss:0.039, val_acc:0.953]
Epoch [35/120    avg_loss:0.033, val_acc:0.974]
Epoch [36/120    avg_loss:0.030, val_acc:0.978]
Epoch [37/120    avg_loss:0.038, val_acc:0.937]
Epoch [38/120    avg_loss:0.057, val_acc:0.966]
Epoch [39/120    avg_loss:0.039, val_acc:0.970]
Epoch [40/120    avg_loss:0.028, val_acc:0.978]
Epoch [41/120    avg_loss:0.032, val_acc:0.982]
Epoch [42/120    avg_loss:0.031, val_acc:0.967]
Epoch [43/120    avg_loss:0.022, val_acc:0.971]
Epoch [44/120    avg_loss:0.029, val_acc:0.966]
Epoch [45/120    avg_loss:0.025, val_acc:0.980]
Epoch [46/120    avg_loss:0.017, val_acc:0.982]
Epoch [47/120    avg_loss:0.055, val_acc:0.962]
Epoch [48/120    avg_loss:0.032, val_acc:0.984]
Epoch [49/120    avg_loss:0.041, val_acc:0.946]
Epoch [50/120    avg_loss:0.057, val_acc:0.974]
Epoch [51/120    avg_loss:0.023, val_acc:0.962]
Epoch [52/120    avg_loss:0.032, val_acc:0.955]
Epoch [53/120    avg_loss:0.016, val_acc:0.975]
Epoch [54/120    avg_loss:0.014, val_acc:0.979]
Epoch [55/120    avg_loss:0.016, val_acc:0.975]
Epoch [56/120    avg_loss:0.011, val_acc:0.985]
Epoch [57/120    avg_loss:0.008, val_acc:0.980]
Epoch [58/120    avg_loss:0.010, val_acc:0.979]
Epoch [59/120    avg_loss:0.012, val_acc:0.984]
Epoch [60/120    avg_loss:0.013, val_acc:0.983]
Epoch [61/120    avg_loss:0.014, val_acc:0.979]
Epoch [62/120    avg_loss:0.021, val_acc:0.978]
Epoch [63/120    avg_loss:0.014, val_acc:0.984]
Epoch [64/120    avg_loss:0.010, val_acc:0.977]
Epoch [65/120    avg_loss:0.009, val_acc:0.977]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.009, val_acc:0.979]
Epoch [68/120    avg_loss:0.012, val_acc:0.969]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.009, val_acc:0.984]
Epoch [74/120    avg_loss:0.005, val_acc:0.985]
Epoch [75/120    avg_loss:0.004, val_acc:0.986]
Epoch [76/120    avg_loss:0.004, val_acc:0.987]
Epoch [77/120    avg_loss:0.005, val_acc:0.987]
Epoch [78/120    avg_loss:0.005, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.005, val_acc:0.986]
Epoch [82/120    avg_loss:0.005, val_acc:0.986]
Epoch [83/120    avg_loss:0.004, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.004, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.004, val_acc:0.984]
Epoch [106/120    avg_loss:0.003, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     0     0     0     0     1    66     0]
 [    0     0 18020     0    40     0    22     0     8     0]
 [    0     0     0  1932     0     0     0     0   101     3]
 [    0    29     0     0  2930     0     3     0     6     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4861     0     2     0]
 [    0     3     0     0     0     0     0  1285     1     1]
 [    0     4     0    67    41     0     0     0  3457     2]
 [    0     8     0     0     0    15     0     0     0   896]]

Accuracy:
98.9347600800135

F1 scores:
[       nan 0.99135581 0.99764706 0.95762082 0.97944175 0.99428571
 0.99569848 0.99767081 0.95867998 0.98191781]

Kappa:
0.9858937307512374
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f13b237f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.515, val_acc:0.438]
Epoch [2/120    avg_loss:0.958, val_acc:0.635]
Epoch [3/120    avg_loss:0.712, val_acc:0.627]
Epoch [4/120    avg_loss:0.551, val_acc:0.729]
Epoch [5/120    avg_loss:0.440, val_acc:0.862]
Epoch [6/120    avg_loss:0.372, val_acc:0.769]
Epoch [7/120    avg_loss:0.329, val_acc:0.798]
Epoch [8/120    avg_loss:0.324, val_acc:0.752]
Epoch [9/120    avg_loss:0.248, val_acc:0.912]
Epoch [10/120    avg_loss:0.219, val_acc:0.869]
Epoch [11/120    avg_loss:0.210, val_acc:0.812]
Epoch [12/120    avg_loss:0.183, val_acc:0.935]
Epoch [13/120    avg_loss:0.162, val_acc:0.916]
Epoch [14/120    avg_loss:0.213, val_acc:0.935]
Epoch [15/120    avg_loss:0.138, val_acc:0.930]
Epoch [16/120    avg_loss:0.205, val_acc:0.869]
Epoch [17/120    avg_loss:0.187, val_acc:0.893]
Epoch [18/120    avg_loss:0.169, val_acc:0.911]
Epoch [19/120    avg_loss:0.112, val_acc:0.938]
Epoch [20/120    avg_loss:0.129, val_acc:0.955]
Epoch [21/120    avg_loss:0.120, val_acc:0.966]
Epoch [22/120    avg_loss:0.102, val_acc:0.942]
Epoch [23/120    avg_loss:0.102, val_acc:0.974]
Epoch [24/120    avg_loss:0.062, val_acc:0.932]
Epoch [25/120    avg_loss:0.063, val_acc:0.964]
Epoch [26/120    avg_loss:0.046, val_acc:0.973]
Epoch [27/120    avg_loss:0.039, val_acc:0.963]
Epoch [28/120    avg_loss:0.049, val_acc:0.965]
Epoch [29/120    avg_loss:0.068, val_acc:0.955]
Epoch [30/120    avg_loss:0.076, val_acc:0.920]
Epoch [31/120    avg_loss:0.047, val_acc:0.971]
Epoch [32/120    avg_loss:0.051, val_acc:0.967]
Epoch [33/120    avg_loss:0.040, val_acc:0.973]
Epoch [34/120    avg_loss:0.032, val_acc:0.970]
Epoch [35/120    avg_loss:0.040, val_acc:0.973]
Epoch [36/120    avg_loss:0.043, val_acc:0.952]
Epoch [37/120    avg_loss:0.031, val_acc:0.974]
Epoch [38/120    avg_loss:0.018, val_acc:0.980]
Epoch [39/120    avg_loss:0.016, val_acc:0.979]
Epoch [40/120    avg_loss:0.020, val_acc:0.981]
Epoch [41/120    avg_loss:0.021, val_acc:0.978]
Epoch [42/120    avg_loss:0.016, val_acc:0.978]
Epoch [43/120    avg_loss:0.019, val_acc:0.973]
Epoch [44/120    avg_loss:0.018, val_acc:0.983]
Epoch [45/120    avg_loss:0.016, val_acc:0.983]
Epoch [46/120    avg_loss:0.024, val_acc:0.981]
Epoch [47/120    avg_loss:0.015, val_acc:0.984]
Epoch [48/120    avg_loss:0.013, val_acc:0.983]
Epoch [49/120    avg_loss:0.023, val_acc:0.984]
Epoch [50/120    avg_loss:0.016, val_acc:0.981]
Epoch [51/120    avg_loss:0.015, val_acc:0.983]
Epoch [52/120    avg_loss:0.017, val_acc:0.982]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.019, val_acc:0.978]
Epoch [55/120    avg_loss:0.012, val_acc:0.979]
Epoch [56/120    avg_loss:0.016, val_acc:0.982]
Epoch [57/120    avg_loss:0.012, val_acc:0.982]
Epoch [58/120    avg_loss:0.014, val_acc:0.983]
Epoch [59/120    avg_loss:0.013, val_acc:0.983]
Epoch [60/120    avg_loss:0.012, val_acc:0.982]
Epoch [61/120    avg_loss:0.012, val_acc:0.977]
Epoch [62/120    avg_loss:0.015, val_acc:0.977]
Epoch [63/120    avg_loss:0.011, val_acc:0.978]
Epoch [64/120    avg_loss:0.016, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.981]
Epoch [66/120    avg_loss:0.012, val_acc:0.979]
Epoch [67/120    avg_loss:0.014, val_acc:0.979]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.010, val_acc:0.981]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.018, val_acc:0.982]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.010, val_acc:0.983]
Epoch [74/120    avg_loss:0.013, val_acc:0.983]
Epoch [75/120    avg_loss:0.014, val_acc:0.983]
Epoch [76/120    avg_loss:0.012, val_acc:0.983]
Epoch [77/120    avg_loss:0.009, val_acc:0.983]
Epoch [78/120    avg_loss:0.017, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.013, val_acc:0.983]
Epoch [83/120    avg_loss:0.012, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.013, val_acc:0.983]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.012, val_acc:0.983]
Epoch [88/120    avg_loss:0.015, val_acc:0.983]
Epoch [89/120    avg_loss:0.013, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.014, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.012, val_acc:0.983]
Epoch [97/120    avg_loss:0.018, val_acc:0.983]
Epoch [98/120    avg_loss:0.011, val_acc:0.983]
Epoch [99/120    avg_loss:0.010, val_acc:0.983]
Epoch [100/120    avg_loss:0.015, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.014, val_acc:0.983]
Epoch [104/120    avg_loss:0.014, val_acc:0.983]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.011, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.015, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.983]
Epoch [114/120    avg_loss:0.012, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.015, val_acc:0.983]
Epoch [119/120    avg_loss:0.012, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6320     0    11     0     0     0     1    96     4]
 [    0     0 17957     0    43     0    84     0     6     0]
 [    0     1     0  1936     0     0     0     0    99     0]
 [    0    12     7     0  2944     0     6     0     1     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    53     0     1     0  4802     0    22     0]
 [    0     3     0     0     0     0     2  1283     2     0]
 [    0     2     0    55    48     0     0     0  3466     0]
 [    0     0     0     0     6    10     0     0     0   903]]

Accuracy:
98.60940399585472

F1 scores:
[       nan 0.98981989 0.99465478 0.95889054 0.97904889 0.99618321
 0.98280802 0.996892   0.95442655 0.98796499]

Kappa:
0.9815918989005559
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f7dee27b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.606, val_acc:0.490]
Epoch [2/120    avg_loss:0.995, val_acc:0.736]
Epoch [3/120    avg_loss:0.733, val_acc:0.643]
Epoch [4/120    avg_loss:0.574, val_acc:0.698]
Epoch [5/120    avg_loss:0.446, val_acc:0.803]
Epoch [6/120    avg_loss:0.351, val_acc:0.864]
Epoch [7/120    avg_loss:0.300, val_acc:0.877]
Epoch [8/120    avg_loss:0.262, val_acc:0.858]
Epoch [9/120    avg_loss:0.259, val_acc:0.738]
Epoch [10/120    avg_loss:0.222, val_acc:0.931]
Epoch [11/120    avg_loss:0.212, val_acc:0.912]
Epoch [12/120    avg_loss:0.215, val_acc:0.921]
Epoch [13/120    avg_loss:0.165, val_acc:0.901]
Epoch [14/120    avg_loss:0.161, val_acc:0.925]
Epoch [15/120    avg_loss:0.127, val_acc:0.936]
Epoch [16/120    avg_loss:0.121, val_acc:0.954]
Epoch [17/120    avg_loss:0.090, val_acc:0.940]
Epoch [18/120    avg_loss:0.092, val_acc:0.953]
Epoch [19/120    avg_loss:0.136, val_acc:0.936]
Epoch [20/120    avg_loss:0.108, val_acc:0.939]
Epoch [21/120    avg_loss:0.105, val_acc:0.938]
Epoch [22/120    avg_loss:0.084, val_acc:0.961]
Epoch [23/120    avg_loss:0.066, val_acc:0.936]
Epoch [24/120    avg_loss:0.067, val_acc:0.953]
Epoch [25/120    avg_loss:0.108, val_acc:0.938]
Epoch [26/120    avg_loss:0.110, val_acc:0.935]
Epoch [27/120    avg_loss:0.094, val_acc:0.958]
Epoch [28/120    avg_loss:0.065, val_acc:0.956]
Epoch [29/120    avg_loss:0.058, val_acc:0.964]
Epoch [30/120    avg_loss:0.053, val_acc:0.966]
Epoch [31/120    avg_loss:0.059, val_acc:0.958]
Epoch [32/120    avg_loss:0.055, val_acc:0.959]
Epoch [33/120    avg_loss:0.035, val_acc:0.940]
Epoch [34/120    avg_loss:0.044, val_acc:0.953]
Epoch [35/120    avg_loss:0.056, val_acc:0.950]
Epoch [36/120    avg_loss:0.091, val_acc:0.945]
Epoch [37/120    avg_loss:0.062, val_acc:0.968]
Epoch [38/120    avg_loss:0.051, val_acc:0.958]
Epoch [39/120    avg_loss:0.051, val_acc:0.968]
Epoch [40/120    avg_loss:0.042, val_acc:0.965]
Epoch [41/120    avg_loss:0.036, val_acc:0.966]
Epoch [42/120    avg_loss:0.025, val_acc:0.962]
Epoch [43/120    avg_loss:0.029, val_acc:0.958]
Epoch [44/120    avg_loss:0.032, val_acc:0.968]
Epoch [45/120    avg_loss:0.022, val_acc:0.976]
Epoch [46/120    avg_loss:0.023, val_acc:0.952]
Epoch [47/120    avg_loss:0.027, val_acc:0.956]
Epoch [48/120    avg_loss:0.028, val_acc:0.969]
Epoch [49/120    avg_loss:0.031, val_acc:0.965]
Epoch [50/120    avg_loss:0.030, val_acc:0.966]
Epoch [51/120    avg_loss:0.026, val_acc:0.965]
Epoch [52/120    avg_loss:0.030, val_acc:0.965]
Epoch [53/120    avg_loss:0.018, val_acc:0.970]
Epoch [54/120    avg_loss:0.047, val_acc:0.943]
Epoch [55/120    avg_loss:0.056, val_acc:0.961]
Epoch [56/120    avg_loss:0.030, val_acc:0.965]
Epoch [57/120    avg_loss:0.020, val_acc:0.975]
Epoch [58/120    avg_loss:0.020, val_acc:0.973]
Epoch [59/120    avg_loss:0.013, val_acc:0.978]
Epoch [60/120    avg_loss:0.017, val_acc:0.976]
Epoch [61/120    avg_loss:0.012, val_acc:0.976]
Epoch [62/120    avg_loss:0.007, val_acc:0.977]
Epoch [63/120    avg_loss:0.009, val_acc:0.978]
Epoch [64/120    avg_loss:0.012, val_acc:0.977]
Epoch [65/120    avg_loss:0.013, val_acc:0.977]
Epoch [66/120    avg_loss:0.011, val_acc:0.978]
Epoch [67/120    avg_loss:0.010, val_acc:0.978]
Epoch [68/120    avg_loss:0.011, val_acc:0.977]
Epoch [69/120    avg_loss:0.012, val_acc:0.979]
Epoch [70/120    avg_loss:0.011, val_acc:0.979]
Epoch [71/120    avg_loss:0.010, val_acc:0.979]
Epoch [72/120    avg_loss:0.010, val_acc:0.977]
Epoch [73/120    avg_loss:0.010, val_acc:0.978]
Epoch [74/120    avg_loss:0.008, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.979]
Epoch [76/120    avg_loss:0.006, val_acc:0.978]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.006, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.010, val_acc:0.979]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.005, val_acc:0.979]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.010, val_acc:0.981]
Epoch [85/120    avg_loss:0.006, val_acc:0.980]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.008, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.980]
Epoch [89/120    avg_loss:0.006, val_acc:0.979]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.010, val_acc:0.978]
Epoch [95/120    avg_loss:0.007, val_acc:0.978]
Epoch [96/120    avg_loss:0.009, val_acc:0.976]
Epoch [97/120    avg_loss:0.006, val_acc:0.978]
Epoch [98/120    avg_loss:0.007, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.978]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.009, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.978]
Epoch [104/120    avg_loss:0.008, val_acc:0.978]
Epoch [105/120    avg_loss:0.013, val_acc:0.978]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.007, val_acc:0.978]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.008, val_acc:0.978]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.005, val_acc:0.978]
Epoch [117/120    avg_loss:0.008, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.008, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     3     0     0     9     0    24     4]
 [    0     0 18073     0    10     0     0     0     7     0]
 [    0     1     0  1973     0     0     0     0    61     1]
 [    0    19     0     3  2937     0     5     0     6     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4849     0    16     0]
 [    0    13     0     0     0     0     1  1275     0     1]
 [    0    12     0    56    48     0     0     0  3454     1]
 [    0     2     0     0     6    13     0     0     0   898]]

Accuracy:
99.18781481213699

F1 scores:
[       nan 0.99324062 0.99917072 0.96929501 0.98342541 0.99504384
 0.99548347 0.99415205 0.96764253 0.98357065]

Kappa:
0.9892383505430407
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7dbfb48828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.588, val_acc:0.362]
Epoch [2/120    avg_loss:0.954, val_acc:0.475]
Epoch [3/120    avg_loss:0.734, val_acc:0.734]
Epoch [4/120    avg_loss:0.545, val_acc:0.574]
Epoch [5/120    avg_loss:0.417, val_acc:0.722]
Epoch [6/120    avg_loss:0.417, val_acc:0.737]
Epoch [7/120    avg_loss:0.381, val_acc:0.734]
Epoch [8/120    avg_loss:0.323, val_acc:0.797]
Epoch [9/120    avg_loss:0.281, val_acc:0.832]
Epoch [10/120    avg_loss:0.237, val_acc:0.866]
Epoch [11/120    avg_loss:0.239, val_acc:0.886]
Epoch [12/120    avg_loss:0.185, val_acc:0.842]
Epoch [13/120    avg_loss:0.177, val_acc:0.893]
Epoch [14/120    avg_loss:0.260, val_acc:0.899]
Epoch [15/120    avg_loss:0.167, val_acc:0.922]
Epoch [16/120    avg_loss:0.125, val_acc:0.906]
Epoch [17/120    avg_loss:0.139, val_acc:0.913]
Epoch [18/120    avg_loss:0.123, val_acc:0.910]
Epoch [19/120    avg_loss:0.094, val_acc:0.955]
Epoch [20/120    avg_loss:0.106, val_acc:0.897]
Epoch [21/120    avg_loss:0.089, val_acc:0.938]
Epoch [22/120    avg_loss:0.088, val_acc:0.931]
Epoch [23/120    avg_loss:0.146, val_acc:0.929]
Epoch [24/120    avg_loss:0.111, val_acc:0.958]
Epoch [25/120    avg_loss:0.067, val_acc:0.953]
Epoch [26/120    avg_loss:0.076, val_acc:0.943]
Epoch [27/120    avg_loss:0.068, val_acc:0.920]
Epoch [28/120    avg_loss:0.047, val_acc:0.963]
Epoch [29/120    avg_loss:0.046, val_acc:0.958]
Epoch [30/120    avg_loss:0.054, val_acc:0.970]
Epoch [31/120    avg_loss:0.035, val_acc:0.976]
Epoch [32/120    avg_loss:0.035, val_acc:0.899]
Epoch [33/120    avg_loss:0.039, val_acc:0.968]
Epoch [34/120    avg_loss:0.042, val_acc:0.953]
Epoch [35/120    avg_loss:0.039, val_acc:0.952]
Epoch [36/120    avg_loss:0.054, val_acc:0.965]
Epoch [37/120    avg_loss:0.033, val_acc:0.964]
Epoch [38/120    avg_loss:0.042, val_acc:0.910]
Epoch [39/120    avg_loss:0.062, val_acc:0.968]
Epoch [40/120    avg_loss:0.032, val_acc:0.956]
Epoch [41/120    avg_loss:0.037, val_acc:0.935]
Epoch [42/120    avg_loss:0.038, val_acc:0.973]
Epoch [43/120    avg_loss:0.025, val_acc:0.949]
Epoch [44/120    avg_loss:0.037, val_acc:0.974]
Epoch [45/120    avg_loss:0.029, val_acc:0.977]
Epoch [46/120    avg_loss:0.016, val_acc:0.978]
Epoch [47/120    avg_loss:0.016, val_acc:0.977]
Epoch [48/120    avg_loss:0.016, val_acc:0.979]
Epoch [49/120    avg_loss:0.016, val_acc:0.978]
Epoch [50/120    avg_loss:0.015, val_acc:0.977]
Epoch [51/120    avg_loss:0.014, val_acc:0.978]
Epoch [52/120    avg_loss:0.010, val_acc:0.980]
Epoch [53/120    avg_loss:0.018, val_acc:0.980]
Epoch [54/120    avg_loss:0.019, val_acc:0.979]
Epoch [55/120    avg_loss:0.012, val_acc:0.978]
Epoch [56/120    avg_loss:0.015, val_acc:0.978]
Epoch [57/120    avg_loss:0.013, val_acc:0.977]
Epoch [58/120    avg_loss:0.012, val_acc:0.978]
Epoch [59/120    avg_loss:0.015, val_acc:0.978]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.012, val_acc:0.982]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.009, val_acc:0.982]
Epoch [64/120    avg_loss:0.011, val_acc:0.981]
Epoch [65/120    avg_loss:0.017, val_acc:0.980]
Epoch [66/120    avg_loss:0.007, val_acc:0.983]
Epoch [67/120    avg_loss:0.012, val_acc:0.982]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.011, val_acc:0.980]
Epoch [70/120    avg_loss:0.012, val_acc:0.981]
Epoch [71/120    avg_loss:0.013, val_acc:0.981]
Epoch [72/120    avg_loss:0.010, val_acc:0.983]
Epoch [73/120    avg_loss:0.012, val_acc:0.983]
Epoch [74/120    avg_loss:0.012, val_acc:0.983]
Epoch [75/120    avg_loss:0.011, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.018, val_acc:0.983]
Epoch [78/120    avg_loss:0.009, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.985]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.013, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.982]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.029, val_acc:0.982]
Epoch [100/120    avg_loss:0.011, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     1     0     1     0    34     0]
 [    0     0 18014     0    26     0    47     0     3     0]
 [    0     0     0  1927     0     0     0     0   107     2]
 [    0    32     4     1  2925     0     3     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    31     0     0     0  4847     0     0     0]
 [    0     5     0     0     0     0     0  1285     0     0]
 [    0    11     0    38    41     0     0     0  3481     0]
 [    0     0     0     0     1    19     0     0     0   899]]

Accuracy:
99.00224134191309

F1 scores:
[       nan 0.99347623 0.99692853 0.96301849 0.98055649 0.99277292
 0.99161211 0.99805825 0.96681017 0.98682766]

Kappa:
0.9867834665019022
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0943cc7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.582, val_acc:0.640]
Epoch [2/120    avg_loss:0.999, val_acc:0.489]
Epoch [3/120    avg_loss:0.693, val_acc:0.735]
Epoch [4/120    avg_loss:0.526, val_acc:0.745]
Epoch [5/120    avg_loss:0.478, val_acc:0.697]
Epoch [6/120    avg_loss:0.401, val_acc:0.737]
Epoch [7/120    avg_loss:0.343, val_acc:0.773]
Epoch [8/120    avg_loss:0.302, val_acc:0.804]
Epoch [9/120    avg_loss:0.284, val_acc:0.879]
Epoch [10/120    avg_loss:0.250, val_acc:0.904]
Epoch [11/120    avg_loss:0.242, val_acc:0.874]
Epoch [12/120    avg_loss:0.203, val_acc:0.931]
Epoch [13/120    avg_loss:0.140, val_acc:0.917]
Epoch [14/120    avg_loss:0.131, val_acc:0.943]
Epoch [15/120    avg_loss:0.123, val_acc:0.926]
Epoch [16/120    avg_loss:0.155, val_acc:0.924]
Epoch [17/120    avg_loss:0.122, val_acc:0.947]
Epoch [18/120    avg_loss:0.103, val_acc:0.941]
Epoch [19/120    avg_loss:0.127, val_acc:0.952]
Epoch [20/120    avg_loss:0.098, val_acc:0.952]
Epoch [21/120    avg_loss:0.081, val_acc:0.938]
Epoch [22/120    avg_loss:0.085, val_acc:0.952]
Epoch [23/120    avg_loss:0.098, val_acc:0.958]
Epoch [24/120    avg_loss:0.094, val_acc:0.956]
Epoch [25/120    avg_loss:0.067, val_acc:0.952]
Epoch [26/120    avg_loss:0.051, val_acc:0.971]
Epoch [27/120    avg_loss:0.055, val_acc:0.964]
Epoch [28/120    avg_loss:0.066, val_acc:0.962]
Epoch [29/120    avg_loss:0.053, val_acc:0.931]
Epoch [30/120    avg_loss:0.043, val_acc:0.972]
Epoch [31/120    avg_loss:0.025, val_acc:0.974]
Epoch [32/120    avg_loss:0.023, val_acc:0.964]
Epoch [33/120    avg_loss:0.031, val_acc:0.973]
Epoch [34/120    avg_loss:0.039, val_acc:0.969]
Epoch [35/120    avg_loss:0.052, val_acc:0.934]
Epoch [36/120    avg_loss:0.113, val_acc:0.898]
Epoch [37/120    avg_loss:0.068, val_acc:0.960]
Epoch [38/120    avg_loss:0.054, val_acc:0.917]
Epoch [39/120    avg_loss:0.062, val_acc:0.921]
Epoch [40/120    avg_loss:0.136, val_acc:0.953]
Epoch [41/120    avg_loss:0.060, val_acc:0.907]
Epoch [42/120    avg_loss:0.054, val_acc:0.969]
Epoch [43/120    avg_loss:0.037, val_acc:0.971]
Epoch [44/120    avg_loss:0.075, val_acc:0.973]
Epoch [45/120    avg_loss:0.025, val_acc:0.978]
Epoch [46/120    avg_loss:0.019, val_acc:0.974]
Epoch [47/120    avg_loss:0.023, val_acc:0.975]
Epoch [48/120    avg_loss:0.019, val_acc:0.974]
Epoch [49/120    avg_loss:0.019, val_acc:0.978]
Epoch [50/120    avg_loss:0.015, val_acc:0.978]
Epoch [51/120    avg_loss:0.016, val_acc:0.979]
Epoch [52/120    avg_loss:0.025, val_acc:0.978]
Epoch [53/120    avg_loss:0.016, val_acc:0.978]
Epoch [54/120    avg_loss:0.015, val_acc:0.978]
Epoch [55/120    avg_loss:0.014, val_acc:0.978]
Epoch [56/120    avg_loss:0.014, val_acc:0.977]
Epoch [57/120    avg_loss:0.012, val_acc:0.978]
Epoch [58/120    avg_loss:0.018, val_acc:0.982]
Epoch [59/120    avg_loss:0.012, val_acc:0.983]
Epoch [60/120    avg_loss:0.012, val_acc:0.980]
Epoch [61/120    avg_loss:0.019, val_acc:0.981]
Epoch [62/120    avg_loss:0.018, val_acc:0.979]
Epoch [63/120    avg_loss:0.013, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.980]
Epoch [65/120    avg_loss:0.015, val_acc:0.981]
Epoch [66/120    avg_loss:0.013, val_acc:0.982]
Epoch [67/120    avg_loss:0.014, val_acc:0.979]
Epoch [68/120    avg_loss:0.013, val_acc:0.981]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.012, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.982]
Epoch [72/120    avg_loss:0.015, val_acc:0.981]
Epoch [73/120    avg_loss:0.016, val_acc:0.981]
Epoch [74/120    avg_loss:0.015, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.013, val_acc:0.981]
Epoch [77/120    avg_loss:0.010, val_acc:0.980]
Epoch [78/120    avg_loss:0.011, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.980]
Epoch [80/120    avg_loss:0.011, val_acc:0.979]
Epoch [81/120    avg_loss:0.015, val_acc:0.980]
Epoch [82/120    avg_loss:0.011, val_acc:0.979]
Epoch [83/120    avg_loss:0.016, val_acc:0.978]
Epoch [84/120    avg_loss:0.013, val_acc:0.978]
Epoch [85/120    avg_loss:0.009, val_acc:0.978]
Epoch [86/120    avg_loss:0.012, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.978]
Epoch [88/120    avg_loss:0.009, val_acc:0.978]
Epoch [89/120    avg_loss:0.010, val_acc:0.978]
Epoch [90/120    avg_loss:0.010, val_acc:0.978]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.978]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.978]
Epoch [99/120    avg_loss:0.010, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.011, val_acc:0.978]
Epoch [102/120    avg_loss:0.013, val_acc:0.978]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.012, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.978]
Epoch [106/120    avg_loss:0.009, val_acc:0.978]
Epoch [107/120    avg_loss:0.009, val_acc:0.978]
Epoch [108/120    avg_loss:0.012, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.978]
Epoch [110/120    avg_loss:0.011, val_acc:0.978]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.978]
Epoch [113/120    avg_loss:0.017, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.014, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.015, val_acc:0.978]
Epoch [118/120    avg_loss:0.011, val_acc:0.978]
Epoch [119/120    avg_loss:0.009, val_acc:0.978]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     9     0     0     0    12    23     1]
 [    0     0 18065     0     4     0    18     0     3     0]
 [    0     3     0  1908     0     0     0     0   119     6]
 [    0    30     3     0  2929     0     4     0     5     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4866     0     1     0]
 [    0    10     0     0     0     0     0  1272     0     8]
 [    0     7     7    86    42     0     0     0  3429     0]
 [    0     0     0     0     2    14     0     0     0   903]]

Accuracy:
98.96609066589545

F1 scores:
[       nan 0.99261792 0.99872844 0.94478831 0.98470331 0.99466463
 0.99651853 0.98834499 0.95902671 0.98258977]

Kappa:
0.9863000881757891
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa86f4047b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.611, val_acc:0.336]
Epoch [2/120    avg_loss:1.014, val_acc:0.578]
Epoch [3/120    avg_loss:0.768, val_acc:0.703]
Epoch [4/120    avg_loss:0.579, val_acc:0.705]
Epoch [5/120    avg_loss:0.474, val_acc:0.798]
Epoch [6/120    avg_loss:0.386, val_acc:0.769]
Epoch [7/120    avg_loss:0.340, val_acc:0.827]
Epoch [8/120    avg_loss:0.302, val_acc:0.882]
Epoch [9/120    avg_loss:0.324, val_acc:0.876]
Epoch [10/120    avg_loss:0.269, val_acc:0.909]
Epoch [11/120    avg_loss:0.188, val_acc:0.917]
Epoch [12/120    avg_loss:0.193, val_acc:0.892]
Epoch [13/120    avg_loss:0.178, val_acc:0.927]
Epoch [14/120    avg_loss:0.156, val_acc:0.934]
Epoch [15/120    avg_loss:0.154, val_acc:0.875]
Epoch [16/120    avg_loss:0.142, val_acc:0.929]
Epoch [17/120    avg_loss:0.096, val_acc:0.931]
Epoch [18/120    avg_loss:0.179, val_acc:0.910]
Epoch [19/120    avg_loss:0.116, val_acc:0.956]
Epoch [20/120    avg_loss:0.141, val_acc:0.838]
Epoch [21/120    avg_loss:0.171, val_acc:0.938]
Epoch [22/120    avg_loss:0.093, val_acc:0.966]
Epoch [23/120    avg_loss:0.076, val_acc:0.950]
Epoch [24/120    avg_loss:0.082, val_acc:0.953]
Epoch [25/120    avg_loss:0.055, val_acc:0.965]
Epoch [26/120    avg_loss:0.047, val_acc:0.963]
Epoch [27/120    avg_loss:0.064, val_acc:0.943]
Epoch [28/120    avg_loss:0.069, val_acc:0.945]
Epoch [29/120    avg_loss:0.067, val_acc:0.965]
Epoch [30/120    avg_loss:0.069, val_acc:0.976]
Epoch [31/120    avg_loss:0.052, val_acc:0.973]
Epoch [32/120    avg_loss:0.036, val_acc:0.970]
Epoch [33/120    avg_loss:0.057, val_acc:0.963]
Epoch [34/120    avg_loss:0.056, val_acc:0.961]
Epoch [35/120    avg_loss:0.054, val_acc:0.965]
Epoch [36/120    avg_loss:0.039, val_acc:0.977]
Epoch [37/120    avg_loss:0.036, val_acc:0.975]
Epoch [38/120    avg_loss:0.040, val_acc:0.973]
Epoch [39/120    avg_loss:0.039, val_acc:0.971]
Epoch [40/120    avg_loss:0.034, val_acc:0.975]
Epoch [41/120    avg_loss:0.029, val_acc:0.976]
Epoch [42/120    avg_loss:0.022, val_acc:0.975]
Epoch [43/120    avg_loss:0.019, val_acc:0.976]
Epoch [44/120    avg_loss:0.020, val_acc:0.982]
Epoch [45/120    avg_loss:0.012, val_acc:0.981]
Epoch [46/120    avg_loss:0.014, val_acc:0.989]
Epoch [47/120    avg_loss:0.011, val_acc:0.985]
Epoch [48/120    avg_loss:0.012, val_acc:0.981]
Epoch [49/120    avg_loss:0.035, val_acc:0.979]
Epoch [50/120    avg_loss:0.026, val_acc:0.973]
Epoch [51/120    avg_loss:0.018, val_acc:0.980]
Epoch [52/120    avg_loss:0.009, val_acc:0.983]
Epoch [53/120    avg_loss:0.020, val_acc:0.978]
Epoch [54/120    avg_loss:0.017, val_acc:0.983]
Epoch [55/120    avg_loss:0.010, val_acc:0.975]
Epoch [56/120    avg_loss:0.013, val_acc:0.984]
Epoch [57/120    avg_loss:0.009, val_acc:0.985]
Epoch [58/120    avg_loss:0.016, val_acc:0.983]
Epoch [59/120    avg_loss:0.027, val_acc:0.986]
Epoch [60/120    avg_loss:0.011, val_acc:0.988]
Epoch [61/120    avg_loss:0.012, val_acc:0.988]
Epoch [62/120    avg_loss:0.009, val_acc:0.985]
Epoch [63/120    avg_loss:0.007, val_acc:0.985]
Epoch [64/120    avg_loss:0.007, val_acc:0.985]
Epoch [65/120    avg_loss:0.006, val_acc:0.986]
Epoch [66/120    avg_loss:0.005, val_acc:0.987]
Epoch [67/120    avg_loss:0.007, val_acc:0.987]
Epoch [68/120    avg_loss:0.010, val_acc:0.988]
Epoch [69/120    avg_loss:0.008, val_acc:0.987]
Epoch [70/120    avg_loss:0.006, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.987]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.009, val_acc:0.988]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.005, val_acc:0.987]
Epoch [76/120    avg_loss:0.006, val_acc:0.987]
Epoch [77/120    avg_loss:0.007, val_acc:0.987]
Epoch [78/120    avg_loss:0.007, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.007, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.988]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.013, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.015, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6292     0     3     1     0     5    42    77    12]
 [    0     0 18059     0    23     0     0     0     8     0]
 [    0    11     0  1904     0     0     0     0   117     4]
 [    0    26    11     0  2922     0     9     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    50     0     0     0  4828     0     0     0]
 [    0     4     0     0     0     0     3  1278     3     2]
 [    0     8     0    52    45     0     0     0  3465     1]
 [    0     0     0     0     3    24     0     0     0   892]]

Accuracy:
98.67929530282217

F1 scores:
[       nan 0.98520316 0.99745927 0.95319149 0.97955079 0.99088838
 0.99310912 0.97931034 0.95652174 0.97486339]

Kappa:
0.9824980397010642
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb2dce5b7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.612, val_acc:0.377]
Epoch [2/120    avg_loss:0.950, val_acc:0.620]
Epoch [3/120    avg_loss:0.682, val_acc:0.706]
Epoch [4/120    avg_loss:0.482, val_acc:0.619]
Epoch [5/120    avg_loss:0.438, val_acc:0.810]
Epoch [6/120    avg_loss:0.375, val_acc:0.814]
Epoch [7/120    avg_loss:0.286, val_acc:0.819]
Epoch [8/120    avg_loss:0.247, val_acc:0.919]
Epoch [9/120    avg_loss:0.235, val_acc:0.807]
Epoch [10/120    avg_loss:0.218, val_acc:0.896]
Epoch [11/120    avg_loss:0.185, val_acc:0.877]
Epoch [12/120    avg_loss:0.182, val_acc:0.883]
Epoch [13/120    avg_loss:0.142, val_acc:0.901]
Epoch [14/120    avg_loss:0.125, val_acc:0.944]
Epoch [15/120    avg_loss:0.126, val_acc:0.946]
Epoch [16/120    avg_loss:0.107, val_acc:0.955]
Epoch [17/120    avg_loss:0.096, val_acc:0.950]
Epoch [18/120    avg_loss:0.111, val_acc:0.944]
Epoch [19/120    avg_loss:0.087, val_acc:0.954]
Epoch [20/120    avg_loss:0.097, val_acc:0.954]
Epoch [21/120    avg_loss:0.075, val_acc:0.946]
Epoch [22/120    avg_loss:0.044, val_acc:0.971]
Epoch [23/120    avg_loss:0.064, val_acc:0.963]
Epoch [24/120    avg_loss:0.054, val_acc:0.975]
Epoch [25/120    avg_loss:0.052, val_acc:0.975]
Epoch [26/120    avg_loss:0.045, val_acc:0.949]
Epoch [27/120    avg_loss:0.039, val_acc:0.953]
Epoch [28/120    avg_loss:0.048, val_acc:0.961]
Epoch [29/120    avg_loss:0.037, val_acc:0.978]
Epoch [30/120    avg_loss:0.038, val_acc:0.968]
Epoch [31/120    avg_loss:0.048, val_acc:0.972]
Epoch [32/120    avg_loss:0.018, val_acc:0.980]
Epoch [33/120    avg_loss:0.027, val_acc:0.984]
Epoch [34/120    avg_loss:0.033, val_acc:0.978]
Epoch [35/120    avg_loss:0.036, val_acc:0.954]
Epoch [36/120    avg_loss:0.096, val_acc:0.958]
Epoch [37/120    avg_loss:0.093, val_acc:0.959]
Epoch [38/120    avg_loss:0.084, val_acc:0.961]
Epoch [39/120    avg_loss:0.060, val_acc:0.951]
Epoch [40/120    avg_loss:0.040, val_acc:0.971]
Epoch [41/120    avg_loss:0.087, val_acc:0.974]
Epoch [42/120    avg_loss:0.046, val_acc:0.975]
Epoch [43/120    avg_loss:0.026, val_acc:0.984]
Epoch [44/120    avg_loss:0.020, val_acc:0.985]
Epoch [45/120    avg_loss:0.045, val_acc:0.966]
Epoch [46/120    avg_loss:0.025, val_acc:0.974]
Epoch [47/120    avg_loss:0.030, val_acc:0.976]
Epoch [48/120    avg_loss:0.024, val_acc:0.976]
Epoch [49/120    avg_loss:0.014, val_acc:0.989]
Epoch [50/120    avg_loss:0.021, val_acc:0.977]
Epoch [51/120    avg_loss:0.021, val_acc:0.982]
Epoch [52/120    avg_loss:0.015, val_acc:0.984]
Epoch [53/120    avg_loss:0.011, val_acc:0.971]
Epoch [54/120    avg_loss:0.016, val_acc:0.982]
Epoch [55/120    avg_loss:0.040, val_acc:0.966]
Epoch [56/120    avg_loss:0.025, val_acc:0.974]
Epoch [57/120    avg_loss:0.016, val_acc:0.982]
Epoch [58/120    avg_loss:0.010, val_acc:0.988]
Epoch [59/120    avg_loss:0.009, val_acc:0.987]
Epoch [60/120    avg_loss:0.009, val_acc:0.986]
Epoch [61/120    avg_loss:0.007, val_acc:0.988]
Epoch [62/120    avg_loss:0.006, val_acc:0.988]
Epoch [63/120    avg_loss:0.007, val_acc:0.989]
Epoch [64/120    avg_loss:0.007, val_acc:0.986]
Epoch [65/120    avg_loss:0.009, val_acc:0.987]
Epoch [66/120    avg_loss:0.005, val_acc:0.989]
Epoch [67/120    avg_loss:0.007, val_acc:0.989]
Epoch [68/120    avg_loss:0.006, val_acc:0.988]
Epoch [69/120    avg_loss:0.007, val_acc:0.988]
Epoch [70/120    avg_loss:0.005, val_acc:0.988]
Epoch [71/120    avg_loss:0.006, val_acc:0.989]
Epoch [72/120    avg_loss:0.005, val_acc:0.990]
Epoch [73/120    avg_loss:0.007, val_acc:0.991]
Epoch [74/120    avg_loss:0.005, val_acc:0.989]
Epoch [75/120    avg_loss:0.006, val_acc:0.989]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.004, val_acc:0.992]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.004, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.005, val_acc:0.990]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.005, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.003, val_acc:0.990]
Epoch [89/120    avg_loss:0.005, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.003, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.990]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.003, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     0     0     0    11     0    18     0]
 [    0     2 18035     0    16     0    30     0     7     0]
 [    0     5     0  1989     0     0     0     0    38     4]
 [    0    28    10     0  2922     0     1     0    10     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4865     0    11     0]
 [    0     6     0     0     0     0     2  1277     0     5]
 [    0     0     0    37    52     0     0     0  3482     0]
 [    0     0     0     0     9    12     0     0     0   898]]

Accuracy:
99.23601571349384

F1 scores:
[       nan 0.99456353 0.99820119 0.97883858 0.97873053 0.99542334
 0.99417595 0.99493572 0.97576012 0.98303229]

Kappa:
0.9898810360526837
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efeb31eb7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.635, val_acc:0.479]
Epoch [2/120    avg_loss:1.007, val_acc:0.677]
Epoch [3/120    avg_loss:0.832, val_acc:0.573]
Epoch [4/120    avg_loss:0.617, val_acc:0.726]
Epoch [5/120    avg_loss:0.472, val_acc:0.791]
Epoch [6/120    avg_loss:0.383, val_acc:0.863]
Epoch [7/120    avg_loss:0.365, val_acc:0.850]
Epoch [8/120    avg_loss:0.294, val_acc:0.896]
Epoch [9/120    avg_loss:0.291, val_acc:0.897]
Epoch [10/120    avg_loss:0.220, val_acc:0.905]
Epoch [11/120    avg_loss:0.176, val_acc:0.895]
Epoch [12/120    avg_loss:0.150, val_acc:0.922]
Epoch [13/120    avg_loss:0.164, val_acc:0.909]
Epoch [14/120    avg_loss:0.133, val_acc:0.926]
Epoch [15/120    avg_loss:0.148, val_acc:0.938]
Epoch [16/120    avg_loss:0.098, val_acc:0.963]
Epoch [17/120    avg_loss:0.099, val_acc:0.935]
Epoch [18/120    avg_loss:0.077, val_acc:0.942]
Epoch [19/120    avg_loss:0.086, val_acc:0.958]
Epoch [20/120    avg_loss:0.074, val_acc:0.956]
Epoch [21/120    avg_loss:0.058, val_acc:0.962]
Epoch [22/120    avg_loss:0.067, val_acc:0.915]
Epoch [23/120    avg_loss:0.071, val_acc:0.945]
Epoch [24/120    avg_loss:0.047, val_acc:0.940]
Epoch [25/120    avg_loss:0.070, val_acc:0.959]
Epoch [26/120    avg_loss:0.068, val_acc:0.973]
Epoch [27/120    avg_loss:0.052, val_acc:0.982]
Epoch [28/120    avg_loss:0.038, val_acc:0.952]
Epoch [29/120    avg_loss:0.056, val_acc:0.961]
Epoch [30/120    avg_loss:0.033, val_acc:0.984]
Epoch [31/120    avg_loss:0.030, val_acc:0.975]
Epoch [32/120    avg_loss:0.027, val_acc:0.965]
Epoch [33/120    avg_loss:0.032, val_acc:0.983]
Epoch [34/120    avg_loss:0.039, val_acc:0.973]
Epoch [35/120    avg_loss:0.034, val_acc:0.962]
Epoch [36/120    avg_loss:0.044, val_acc:0.963]
Epoch [37/120    avg_loss:0.053, val_acc:0.975]
Epoch [38/120    avg_loss:0.043, val_acc:0.975]
Epoch [39/120    avg_loss:0.053, val_acc:0.973]
Epoch [40/120    avg_loss:0.050, val_acc:0.983]
Epoch [41/120    avg_loss:0.017, val_acc:0.978]
Epoch [42/120    avg_loss:0.013, val_acc:0.986]
Epoch [43/120    avg_loss:0.010, val_acc:0.986]
Epoch [44/120    avg_loss:0.011, val_acc:0.986]
Epoch [45/120    avg_loss:0.014, val_acc:0.980]
Epoch [46/120    avg_loss:0.010, val_acc:0.983]
Epoch [47/120    avg_loss:0.010, val_acc:0.971]
Epoch [48/120    avg_loss:0.033, val_acc:0.947]
Epoch [49/120    avg_loss:0.041, val_acc:0.983]
Epoch [50/120    avg_loss:0.035, val_acc:0.975]
Epoch [51/120    avg_loss:0.021, val_acc:0.984]
Epoch [52/120    avg_loss:0.012, val_acc:0.980]
Epoch [53/120    avg_loss:0.016, val_acc:0.967]
Epoch [54/120    avg_loss:0.024, val_acc:0.983]
Epoch [55/120    avg_loss:0.010, val_acc:0.986]
Epoch [56/120    avg_loss:0.017, val_acc:0.984]
Epoch [57/120    avg_loss:0.019, val_acc:0.972]
Epoch [58/120    avg_loss:0.020, val_acc:0.981]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.011, val_acc:0.986]
Epoch [61/120    avg_loss:0.006, val_acc:0.985]
Epoch [62/120    avg_loss:0.008, val_acc:0.988]
Epoch [63/120    avg_loss:0.006, val_acc:0.988]
Epoch [64/120    avg_loss:0.009, val_acc:0.987]
Epoch [65/120    avg_loss:0.020, val_acc:0.975]
Epoch [66/120    avg_loss:0.017, val_acc:0.983]
Epoch [67/120    avg_loss:0.025, val_acc:0.979]
Epoch [68/120    avg_loss:0.020, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.988]
Epoch [70/120    avg_loss:0.012, val_acc:0.981]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.984]
Epoch [74/120    avg_loss:0.026, val_acc:0.965]
Epoch [75/120    avg_loss:0.056, val_acc:0.972]
Epoch [76/120    avg_loss:0.015, val_acc:0.986]
Epoch [77/120    avg_loss:0.018, val_acc:0.983]
Epoch [78/120    avg_loss:0.010, val_acc:0.987]
Epoch [79/120    avg_loss:0.014, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.014, val_acc:0.988]
Epoch [83/120    avg_loss:0.010, val_acc:0.988]
Epoch [84/120    avg_loss:0.004, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.003, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.003, val_acc:0.987]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     0     0     0     0    50     0]
 [    0     8 18043     0    18     0    17     0     4     0]
 [    0     0     0  1998     0     0     0     0    29     9]
 [    0    40     5     0  2905     0     5     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4867     0    11     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     2     0    14    45     0     0     0  3506     4]
 [    0     0     0     0    14     9     0     0     0   896]]

Accuracy:
99.27216638951148

F1 scores:
[       nan 0.99222637 0.99856107 0.98715415 0.97581458 0.99656357
 0.99662128 0.99961225 0.97551475 0.97977037]

Kappa:
0.990359911082045
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff6243297f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.706, val_acc:0.229]
Epoch [2/120    avg_loss:1.040, val_acc:0.503]
Epoch [3/120    avg_loss:0.858, val_acc:0.517]
Epoch [4/120    avg_loss:0.699, val_acc:0.672]
Epoch [5/120    avg_loss:0.498, val_acc:0.735]
Epoch [6/120    avg_loss:0.425, val_acc:0.739]
Epoch [7/120    avg_loss:0.383, val_acc:0.727]
Epoch [8/120    avg_loss:0.352, val_acc:0.886]
Epoch [9/120    avg_loss:0.347, val_acc:0.772]
Epoch [10/120    avg_loss:0.273, val_acc:0.905]
Epoch [11/120    avg_loss:0.223, val_acc:0.862]
Epoch [12/120    avg_loss:0.193, val_acc:0.892]
Epoch [13/120    avg_loss:0.219, val_acc:0.898]
Epoch [14/120    avg_loss:0.166, val_acc:0.940]
Epoch [15/120    avg_loss:0.176, val_acc:0.881]
Epoch [16/120    avg_loss:0.139, val_acc:0.943]
Epoch [17/120    avg_loss:0.135, val_acc:0.943]
Epoch [18/120    avg_loss:0.129, val_acc:0.917]
Epoch [19/120    avg_loss:0.088, val_acc:0.922]
Epoch [20/120    avg_loss:0.082, val_acc:0.960]
Epoch [21/120    avg_loss:0.107, val_acc:0.927]
Epoch [22/120    avg_loss:0.131, val_acc:0.955]
Epoch [23/120    avg_loss:0.102, val_acc:0.941]
Epoch [24/120    avg_loss:0.075, val_acc:0.958]
Epoch [25/120    avg_loss:0.058, val_acc:0.955]
Epoch [26/120    avg_loss:0.094, val_acc:0.956]
Epoch [27/120    avg_loss:0.066, val_acc:0.978]
Epoch [28/120    avg_loss:0.070, val_acc:0.956]
Epoch [29/120    avg_loss:0.050, val_acc:0.969]
Epoch [30/120    avg_loss:0.040, val_acc:0.968]
Epoch [31/120    avg_loss:0.043, val_acc:0.922]
Epoch [32/120    avg_loss:0.047, val_acc:0.959]
Epoch [33/120    avg_loss:0.037, val_acc:0.945]
Epoch [34/120    avg_loss:0.032, val_acc:0.969]
Epoch [35/120    avg_loss:0.026, val_acc:0.977]
Epoch [36/120    avg_loss:0.030, val_acc:0.966]
Epoch [37/120    avg_loss:0.024, val_acc:0.980]
Epoch [38/120    avg_loss:0.025, val_acc:0.980]
Epoch [39/120    avg_loss:0.033, val_acc:0.969]
Epoch [40/120    avg_loss:0.056, val_acc:0.968]
Epoch [41/120    avg_loss:0.027, val_acc:0.982]
Epoch [42/120    avg_loss:0.016, val_acc:0.983]
Epoch [43/120    avg_loss:0.020, val_acc:0.985]
Epoch [44/120    avg_loss:0.019, val_acc:0.987]
Epoch [45/120    avg_loss:0.016, val_acc:0.987]
Epoch [46/120    avg_loss:0.009, val_acc:0.984]
Epoch [47/120    avg_loss:0.015, val_acc:0.975]
Epoch [48/120    avg_loss:0.019, val_acc:0.988]
Epoch [49/120    avg_loss:0.035, val_acc:0.966]
Epoch [50/120    avg_loss:0.017, val_acc:0.979]
Epoch [51/120    avg_loss:0.023, val_acc:0.986]
Epoch [52/120    avg_loss:0.031, val_acc:0.984]
Epoch [53/120    avg_loss:0.015, val_acc:0.976]
Epoch [54/120    avg_loss:0.011, val_acc:0.986]
Epoch [55/120    avg_loss:0.009, val_acc:0.986]
Epoch [56/120    avg_loss:0.009, val_acc:0.988]
Epoch [57/120    avg_loss:0.011, val_acc:0.984]
Epoch [58/120    avg_loss:0.018, val_acc:0.987]
Epoch [59/120    avg_loss:0.016, val_acc:0.984]
Epoch [60/120    avg_loss:0.021, val_acc:0.983]
Epoch [61/120    avg_loss:0.022, val_acc:0.978]
Epoch [62/120    avg_loss:0.042, val_acc:0.969]
Epoch [63/120    avg_loss:0.037, val_acc:0.977]
Epoch [64/120    avg_loss:0.034, val_acc:0.983]
Epoch [65/120    avg_loss:0.042, val_acc:0.978]
Epoch [66/120    avg_loss:0.015, val_acc:0.990]
Epoch [67/120    avg_loss:0.014, val_acc:0.983]
Epoch [68/120    avg_loss:0.014, val_acc:0.986]
Epoch [69/120    avg_loss:0.011, val_acc:0.986]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.008, val_acc:0.987]
Epoch [72/120    avg_loss:0.009, val_acc:0.984]
Epoch [73/120    avg_loss:0.015, val_acc:0.974]
Epoch [74/120    avg_loss:0.022, val_acc:0.981]
Epoch [75/120    avg_loss:0.011, val_acc:0.984]
Epoch [76/120    avg_loss:0.022, val_acc:0.976]
Epoch [77/120    avg_loss:0.029, val_acc:0.983]
Epoch [78/120    avg_loss:0.018, val_acc:0.991]
Epoch [79/120    avg_loss:0.010, val_acc:0.987]
Epoch [80/120    avg_loss:0.014, val_acc:0.986]
Epoch [81/120    avg_loss:0.016, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.009, val_acc:0.968]
Epoch [84/120    avg_loss:0.038, val_acc:0.984]
Epoch [85/120    avg_loss:0.021, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.004, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.003, val_acc:0.992]
Epoch [91/120    avg_loss:0.004, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.017, val_acc:0.988]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.031, val_acc:0.978]
Epoch [101/120    avg_loss:0.016, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.017, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.991]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.003, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.993]
Epoch [113/120    avg_loss:0.007, val_acc:0.993]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.991]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     6     1     0     1     2    17     0]
 [    0     0 18053     0    25     0     2     0    10     0]
 [    0     1     0  1999     0     0     0     0    32     4]
 [    0    38     7     0  2911     0     5     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     1     0     0  4872     0     3     0]
 [    0     3     0     0     0     0     4  1281     0     2]
 [    0     6     0    44    53     0     0     0  3467     1]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.22155544308679

F1 scores:
[       nan 0.99417928 0.99872759 0.97846304 0.97423025 0.9893859
 0.99815612 0.99572483 0.97510899 0.97282307]

Kappa:
0.9896877896809381
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9d4e81e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.661, val_acc:0.577]
Epoch [2/120    avg_loss:1.036, val_acc:0.667]
Epoch [3/120    avg_loss:0.761, val_acc:0.618]
Epoch [4/120    avg_loss:0.562, val_acc:0.696]
Epoch [5/120    avg_loss:0.497, val_acc:0.772]
Epoch [6/120    avg_loss:0.400, val_acc:0.804]
Epoch [7/120    avg_loss:0.367, val_acc:0.809]
Epoch [8/120    avg_loss:0.315, val_acc:0.860]
Epoch [9/120    avg_loss:0.243, val_acc:0.909]
Epoch [10/120    avg_loss:0.234, val_acc:0.866]
Epoch [11/120    avg_loss:0.186, val_acc:0.877]
Epoch [12/120    avg_loss:0.181, val_acc:0.909]
Epoch [13/120    avg_loss:0.164, val_acc:0.838]
Epoch [14/120    avg_loss:0.152, val_acc:0.927]
Epoch [15/120    avg_loss:0.106, val_acc:0.912]
Epoch [16/120    avg_loss:0.137, val_acc:0.928]
Epoch [17/120    avg_loss:0.162, val_acc:0.950]
Epoch [18/120    avg_loss:0.122, val_acc:0.940]
Epoch [19/120    avg_loss:0.091, val_acc:0.957]
Epoch [20/120    avg_loss:0.079, val_acc:0.958]
Epoch [21/120    avg_loss:0.054, val_acc:0.963]
Epoch [22/120    avg_loss:0.073, val_acc:0.943]
Epoch [23/120    avg_loss:0.053, val_acc:0.959]
Epoch [24/120    avg_loss:0.046, val_acc:0.968]
Epoch [25/120    avg_loss:0.048, val_acc:0.961]
Epoch [26/120    avg_loss:0.059, val_acc:0.939]
Epoch [27/120    avg_loss:0.101, val_acc:0.931]
Epoch [28/120    avg_loss:0.092, val_acc:0.950]
Epoch [29/120    avg_loss:0.045, val_acc:0.972]
Epoch [30/120    avg_loss:0.055, val_acc:0.941]
Epoch [31/120    avg_loss:0.088, val_acc:0.962]
Epoch [32/120    avg_loss:0.048, val_acc:0.954]
Epoch [33/120    avg_loss:0.062, val_acc:0.958]
Epoch [34/120    avg_loss:0.064, val_acc:0.963]
Epoch [35/120    avg_loss:0.027, val_acc:0.979]
Epoch [36/120    avg_loss:0.032, val_acc:0.953]
Epoch [37/120    avg_loss:0.028, val_acc:0.923]
Epoch [38/120    avg_loss:0.045, val_acc:0.971]
Epoch [39/120    avg_loss:0.024, val_acc:0.973]
Epoch [40/120    avg_loss:0.040, val_acc:0.973]
Epoch [41/120    avg_loss:0.024, val_acc:0.981]
Epoch [42/120    avg_loss:0.023, val_acc:0.975]
Epoch [43/120    avg_loss:0.045, val_acc:0.955]
Epoch [44/120    avg_loss:0.059, val_acc:0.979]
Epoch [45/120    avg_loss:0.023, val_acc:0.982]
Epoch [46/120    avg_loss:0.018, val_acc:0.978]
Epoch [47/120    avg_loss:0.020, val_acc:0.981]
Epoch [48/120    avg_loss:0.065, val_acc:0.963]
Epoch [49/120    avg_loss:0.064, val_acc:0.902]
Epoch [50/120    avg_loss:0.075, val_acc:0.948]
Epoch [51/120    avg_loss:0.027, val_acc:0.974]
Epoch [52/120    avg_loss:0.024, val_acc:0.977]
Epoch [53/120    avg_loss:0.021, val_acc:0.976]
Epoch [54/120    avg_loss:0.026, val_acc:0.972]
Epoch [55/120    avg_loss:0.030, val_acc:0.981]
Epoch [56/120    avg_loss:0.019, val_acc:0.971]
Epoch [57/120    avg_loss:0.018, val_acc:0.973]
Epoch [58/120    avg_loss:0.021, val_acc:0.975]
Epoch [59/120    avg_loss:0.013, val_acc:0.981]
Epoch [60/120    avg_loss:0.010, val_acc:0.983]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.009, val_acc:0.983]
Epoch [64/120    avg_loss:0.010, val_acc:0.982]
Epoch [65/120    avg_loss:0.007, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.983]
Epoch [67/120    avg_loss:0.007, val_acc:0.984]
Epoch [68/120    avg_loss:0.007, val_acc:0.984]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.983]
Epoch [71/120    avg_loss:0.005, val_acc:0.983]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.984]
Epoch [74/120    avg_loss:0.008, val_acc:0.985]
Epoch [75/120    avg_loss:0.006, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.007, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.984]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.009, val_acc:0.983]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     1     0     0     0     0    26     4]
 [    0     0 17954     0    14     0   119     0     3     0]
 [    0     3     0  2002     2     0     0     0    21     8]
 [    0    25     1     0  2919     0     5     0    12    10]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     5     2     0  4859     0     4     0]
 [    0     0     0     0     0     0     1  1284     0     5]
 [    0     8     0    29    45     0     0     0  3488     1]
 [    0     0     0     0     2    23     0     0     0   894]]

Accuracy:
99.06731255874485

F1 scores:
[       nan 0.99479369 0.99597814 0.98305917 0.98018805 0.99126472
 0.9853985  0.997669   0.97908772 0.9712113 ]

Kappa:
0.9876579148289439
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa33debc7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.592, val_acc:0.561]
Epoch [2/120    avg_loss:1.044, val_acc:0.512]
Epoch [3/120    avg_loss:0.799, val_acc:0.706]
Epoch [4/120    avg_loss:0.602, val_acc:0.700]
Epoch [5/120    avg_loss:0.496, val_acc:0.651]
Epoch [6/120    avg_loss:0.443, val_acc:0.802]
Epoch [7/120    avg_loss:0.354, val_acc:0.808]
Epoch [8/120    avg_loss:0.294, val_acc:0.776]
Epoch [9/120    avg_loss:0.248, val_acc:0.841]
Epoch [10/120    avg_loss:0.227, val_acc:0.909]
Epoch [11/120    avg_loss:0.220, val_acc:0.819]
Epoch [12/120    avg_loss:0.199, val_acc:0.865]
Epoch [13/120    avg_loss:0.146, val_acc:0.943]
Epoch [14/120    avg_loss:0.154, val_acc:0.847]
Epoch [15/120    avg_loss:0.114, val_acc:0.931]
Epoch [16/120    avg_loss:0.106, val_acc:0.938]
Epoch [17/120    avg_loss:0.130, val_acc:0.912]
Epoch [18/120    avg_loss:0.102, val_acc:0.945]
Epoch [19/120    avg_loss:0.082, val_acc:0.968]
Epoch [20/120    avg_loss:0.067, val_acc:0.958]
Epoch [21/120    avg_loss:0.084, val_acc:0.973]
Epoch [22/120    avg_loss:0.063, val_acc:0.964]
Epoch [23/120    avg_loss:0.049, val_acc:0.958]
Epoch [24/120    avg_loss:0.042, val_acc:0.973]
Epoch [25/120    avg_loss:0.045, val_acc:0.972]
Epoch [26/120    avg_loss:0.034, val_acc:0.973]
Epoch [27/120    avg_loss:0.041, val_acc:0.951]
Epoch [28/120    avg_loss:0.104, val_acc:0.955]
Epoch [29/120    avg_loss:0.070, val_acc:0.963]
Epoch [30/120    avg_loss:0.036, val_acc:0.971]
Epoch [31/120    avg_loss:0.051, val_acc:0.972]
Epoch [32/120    avg_loss:0.062, val_acc:0.971]
Epoch [33/120    avg_loss:0.026, val_acc:0.972]
Epoch [34/120    avg_loss:0.040, val_acc:0.961]
Epoch [35/120    avg_loss:0.024, val_acc:0.963]
Epoch [36/120    avg_loss:0.059, val_acc:0.964]
Epoch [37/120    avg_loss:0.032, val_acc:0.978]
Epoch [38/120    avg_loss:0.018, val_acc:0.978]
Epoch [39/120    avg_loss:0.013, val_acc:0.976]
Epoch [40/120    avg_loss:0.015, val_acc:0.973]
Epoch [41/120    avg_loss:0.028, val_acc:0.934]
Epoch [42/120    avg_loss:0.038, val_acc:0.932]
Epoch [43/120    avg_loss:0.064, val_acc:0.962]
Epoch [44/120    avg_loss:0.025, val_acc:0.975]
Epoch [45/120    avg_loss:0.033, val_acc:0.948]
Epoch [46/120    avg_loss:0.054, val_acc:0.963]
Epoch [47/120    avg_loss:0.037, val_acc:0.965]
Epoch [48/120    avg_loss:0.032, val_acc:0.968]
Epoch [49/120    avg_loss:0.019, val_acc:0.978]
Epoch [50/120    avg_loss:0.013, val_acc:0.979]
Epoch [51/120    avg_loss:0.010, val_acc:0.981]
Epoch [52/120    avg_loss:0.008, val_acc:0.982]
Epoch [53/120    avg_loss:0.008, val_acc:0.979]
Epoch [54/120    avg_loss:0.014, val_acc:0.986]
Epoch [55/120    avg_loss:0.013, val_acc:0.980]
Epoch [56/120    avg_loss:0.012, val_acc:0.983]
Epoch [57/120    avg_loss:0.010, val_acc:0.985]
Epoch [58/120    avg_loss:0.011, val_acc:0.983]
Epoch [59/120    avg_loss:0.018, val_acc:0.983]
Epoch [60/120    avg_loss:0.010, val_acc:0.983]
Epoch [61/120    avg_loss:0.011, val_acc:0.979]
Epoch [62/120    avg_loss:0.011, val_acc:0.981]
Epoch [63/120    avg_loss:0.017, val_acc:0.977]
Epoch [64/120    avg_loss:0.012, val_acc:0.970]
Epoch [65/120    avg_loss:0.008, val_acc:0.978]
Epoch [66/120    avg_loss:0.007, val_acc:0.987]
Epoch [67/120    avg_loss:0.013, val_acc:0.964]
Epoch [68/120    avg_loss:0.013, val_acc:0.978]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.985]
Epoch [71/120    avg_loss:0.005, val_acc:0.986]
Epoch [72/120    avg_loss:0.004, val_acc:0.988]
Epoch [73/120    avg_loss:0.004, val_acc:0.988]
Epoch [74/120    avg_loss:0.004, val_acc:0.985]
Epoch [75/120    avg_loss:0.006, val_acc:0.963]
Epoch [76/120    avg_loss:0.016, val_acc:0.937]
Epoch [77/120    avg_loss:0.008, val_acc:0.978]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.006, val_acc:0.975]
Epoch [81/120    avg_loss:0.012, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.985]
Epoch [83/120    avg_loss:0.008, val_acc:0.980]
Epoch [84/120    avg_loss:0.006, val_acc:0.983]
Epoch [85/120    avg_loss:0.007, val_acc:0.979]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.004, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.981]
Epoch [89/120    avg_loss:0.006, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.984]
Epoch [92/120    avg_loss:0.003, val_acc:0.985]
Epoch [93/120    avg_loss:0.003, val_acc:0.985]
Epoch [94/120    avg_loss:0.003, val_acc:0.985]
Epoch [95/120    avg_loss:0.003, val_acc:0.985]
Epoch [96/120    avg_loss:0.003, val_acc:0.986]
Epoch [97/120    avg_loss:0.003, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.985]
Epoch [100/120    avg_loss:0.003, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.003, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.002, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.003, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.002, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.003, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6363     0     5     0     0     4     2    57     1]
 [    0     0 18077     0     9     0     0     0     4     0]
 [    0     2     0  1989     0     0     0     0    39     6]
 [    0    37     3     3  2916     0     4     0     9     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4869     0     4     0]
 [    0     5     0     0     0     1     0  1281     0     3]
 [    0     7     0    44    53     0     0     0  3466     1]
 [    0     0     0     0     3    16     0     0     0   900]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99065857 0.99941949 0.97571744 0.97967411 0.99352874
 0.9982573  0.99572483 0.96951049 0.98360656]

Kappa:
0.9895585246898563
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0b7d375ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.614, val_acc:0.382]
Epoch [2/120    avg_loss:1.006, val_acc:0.661]
Epoch [3/120    avg_loss:0.807, val_acc:0.725]
Epoch [4/120    avg_loss:0.668, val_acc:0.623]
Epoch [5/120    avg_loss:0.524, val_acc:0.719]
Epoch [6/120    avg_loss:0.404, val_acc:0.776]
Epoch [7/120    avg_loss:0.319, val_acc:0.850]
Epoch [8/120    avg_loss:0.323, val_acc:0.788]
Epoch [9/120    avg_loss:0.297, val_acc:0.917]
Epoch [10/120    avg_loss:0.234, val_acc:0.929]
Epoch [11/120    avg_loss:0.174, val_acc:0.940]
Epoch [12/120    avg_loss:0.175, val_acc:0.920]
Epoch [13/120    avg_loss:0.144, val_acc:0.931]
Epoch [14/120    avg_loss:0.155, val_acc:0.916]
Epoch [15/120    avg_loss:0.112, val_acc:0.961]
Epoch [16/120    avg_loss:0.116, val_acc:0.966]
Epoch [17/120    avg_loss:0.124, val_acc:0.970]
Epoch [18/120    avg_loss:0.096, val_acc:0.963]
Epoch [19/120    avg_loss:0.115, val_acc:0.942]
Epoch [20/120    avg_loss:0.112, val_acc:0.962]
Epoch [21/120    avg_loss:0.079, val_acc:0.965]
Epoch [22/120    avg_loss:0.074, val_acc:0.967]
Epoch [23/120    avg_loss:0.073, val_acc:0.963]
Epoch [24/120    avg_loss:0.050, val_acc:0.955]
Epoch [25/120    avg_loss:0.052, val_acc:0.978]
Epoch [26/120    avg_loss:0.045, val_acc:0.951]
Epoch [27/120    avg_loss:0.065, val_acc:0.977]
Epoch [28/120    avg_loss:0.045, val_acc:0.906]
Epoch [29/120    avg_loss:0.052, val_acc:0.977]
Epoch [30/120    avg_loss:0.101, val_acc:0.954]
Epoch [31/120    avg_loss:0.079, val_acc:0.973]
Epoch [32/120    avg_loss:0.047, val_acc:0.978]
Epoch [33/120    avg_loss:0.038, val_acc:0.988]
Epoch [34/120    avg_loss:0.034, val_acc:0.976]
Epoch [35/120    avg_loss:0.031, val_acc:0.980]
Epoch [36/120    avg_loss:0.015, val_acc:0.989]
Epoch [37/120    avg_loss:0.039, val_acc:0.986]
Epoch [38/120    avg_loss:0.123, val_acc:0.954]
Epoch [39/120    avg_loss:0.094, val_acc:0.969]
Epoch [40/120    avg_loss:0.089, val_acc:0.980]
Epoch [41/120    avg_loss:0.035, val_acc:0.969]
Epoch [42/120    avg_loss:0.048, val_acc:0.927]
Epoch [43/120    avg_loss:0.036, val_acc:0.981]
Epoch [44/120    avg_loss:0.060, val_acc:0.981]
Epoch [45/120    avg_loss:0.041, val_acc:0.974]
Epoch [46/120    avg_loss:0.024, val_acc:0.988]
Epoch [47/120    avg_loss:0.031, val_acc:0.986]
Epoch [48/120    avg_loss:0.015, val_acc:0.985]
Epoch [49/120    avg_loss:0.025, val_acc:0.985]
Epoch [50/120    avg_loss:0.017, val_acc:0.986]
Epoch [51/120    avg_loss:0.012, val_acc:0.986]
Epoch [52/120    avg_loss:0.013, val_acc:0.987]
Epoch [53/120    avg_loss:0.011, val_acc:0.987]
Epoch [54/120    avg_loss:0.017, val_acc:0.986]
Epoch [55/120    avg_loss:0.012, val_acc:0.990]
Epoch [56/120    avg_loss:0.013, val_acc:0.990]
Epoch [57/120    avg_loss:0.013, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.989]
Epoch [59/120    avg_loss:0.011, val_acc:0.989]
Epoch [60/120    avg_loss:0.012, val_acc:0.989]
Epoch [61/120    avg_loss:0.012, val_acc:0.987]
Epoch [62/120    avg_loss:0.016, val_acc:0.988]
Epoch [63/120    avg_loss:0.011, val_acc:0.990]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.011, val_acc:0.989]
Epoch [66/120    avg_loss:0.013, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.989]
Epoch [68/120    avg_loss:0.007, val_acc:0.989]
Epoch [69/120    avg_loss:0.011, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.990]
Epoch [71/120    avg_loss:0.010, val_acc:0.987]
Epoch [72/120    avg_loss:0.016, val_acc:0.989]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.011, val_acc:0.987]
Epoch [76/120    avg_loss:0.010, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.009, val_acc:0.990]
Epoch [79/120    avg_loss:0.008, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.991]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.989]
Epoch [88/120    avg_loss:0.009, val_acc:0.991]
Epoch [89/120    avg_loss:0.010, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.991]
Epoch [91/120    avg_loss:0.009, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.009, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.989]
Epoch [101/120    avg_loss:0.012, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.007, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0     2     0     0    24    16    34     0]
 [    0     0 17984     0    32     0    47     0    27     0]
 [    0     6     0  2000     0     0     0     0    27     3]
 [    0    29     0     0  2929     0     4     0     9     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4859     0    19     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     3     0    33    51     0     0     0  3480     4]
 [    0     0     0     0     5    32     0     0     0   882]]

Accuracy:
99.01188152218447

F1 scores:
[       nan 0.9911118  0.9970616  0.98255957 0.97812657 0.98788796
 0.99021806 0.99306091 0.97111762 0.97512438]

Kappa:
0.98692318678814
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60cdfde860>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.622, val_acc:0.298]
Epoch [2/120    avg_loss:1.022, val_acc:0.620]
Epoch [3/120    avg_loss:0.804, val_acc:0.573]
Epoch [4/120    avg_loss:0.595, val_acc:0.653]
Epoch [5/120    avg_loss:0.472, val_acc:0.669]
Epoch [6/120    avg_loss:0.379, val_acc:0.789]
Epoch [7/120    avg_loss:0.349, val_acc:0.843]
Epoch [8/120    avg_loss:0.275, val_acc:0.850]
Epoch [9/120    avg_loss:0.275, val_acc:0.801]
Epoch [10/120    avg_loss:0.200, val_acc:0.927]
Epoch [11/120    avg_loss:0.171, val_acc:0.907]
Epoch [12/120    avg_loss:0.145, val_acc:0.927]
Epoch [13/120    avg_loss:0.150, val_acc:0.864]
Epoch [14/120    avg_loss:0.140, val_acc:0.928]
Epoch [15/120    avg_loss:0.132, val_acc:0.930]
Epoch [16/120    avg_loss:0.084, val_acc:0.968]
Epoch [17/120    avg_loss:0.087, val_acc:0.920]
Epoch [18/120    avg_loss:0.064, val_acc:0.973]
Epoch [19/120    avg_loss:0.059, val_acc:0.951]
Epoch [20/120    avg_loss:0.069, val_acc:0.955]
Epoch [21/120    avg_loss:0.071, val_acc:0.961]
Epoch [22/120    avg_loss:0.064, val_acc:0.973]
Epoch [23/120    avg_loss:0.084, val_acc:0.935]
Epoch [24/120    avg_loss:0.088, val_acc:0.960]
Epoch [25/120    avg_loss:0.061, val_acc:0.953]
Epoch [26/120    avg_loss:0.039, val_acc:0.976]
Epoch [27/120    avg_loss:0.056, val_acc:0.977]
Epoch [28/120    avg_loss:0.029, val_acc:0.980]
Epoch [29/120    avg_loss:0.036, val_acc:0.957]
Epoch [30/120    avg_loss:0.035, val_acc:0.969]
Epoch [31/120    avg_loss:0.033, val_acc:0.974]
Epoch [32/120    avg_loss:0.026, val_acc:0.973]
Epoch [33/120    avg_loss:0.032, val_acc:0.948]
Epoch [34/120    avg_loss:0.023, val_acc:0.983]
Epoch [35/120    avg_loss:0.011, val_acc:0.981]
Epoch [36/120    avg_loss:0.018, val_acc:0.986]
Epoch [37/120    avg_loss:0.018, val_acc:0.980]
Epoch [38/120    avg_loss:0.015, val_acc:0.983]
Epoch [39/120    avg_loss:0.015, val_acc:0.980]
Epoch [40/120    avg_loss:0.027, val_acc:0.966]
Epoch [41/120    avg_loss:0.066, val_acc:0.963]
Epoch [42/120    avg_loss:0.025, val_acc:0.974]
Epoch [43/120    avg_loss:0.020, val_acc:0.984]
Epoch [44/120    avg_loss:0.016, val_acc:0.982]
Epoch [45/120    avg_loss:0.010, val_acc:0.978]
Epoch [46/120    avg_loss:0.006, val_acc:0.981]
Epoch [47/120    avg_loss:0.011, val_acc:0.985]
Epoch [48/120    avg_loss:0.009, val_acc:0.984]
Epoch [49/120    avg_loss:0.011, val_acc:0.981]
Epoch [50/120    avg_loss:0.007, val_acc:0.981]
Epoch [51/120    avg_loss:0.008, val_acc:0.983]
Epoch [52/120    avg_loss:0.005, val_acc:0.984]
Epoch [53/120    avg_loss:0.009, val_acc:0.983]
Epoch [54/120    avg_loss:0.010, val_acc:0.983]
Epoch [55/120    avg_loss:0.005, val_acc:0.983]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.006, val_acc:0.984]
Epoch [58/120    avg_loss:0.008, val_acc:0.983]
Epoch [59/120    avg_loss:0.005, val_acc:0.985]
Epoch [60/120    avg_loss:0.007, val_acc:0.985]
Epoch [61/120    avg_loss:0.006, val_acc:0.983]
Epoch [62/120    avg_loss:0.005, val_acc:0.983]
Epoch [63/120    avg_loss:0.004, val_acc:0.983]
Epoch [64/120    avg_loss:0.005, val_acc:0.983]
Epoch [65/120    avg_loss:0.006, val_acc:0.983]
Epoch [66/120    avg_loss:0.006, val_acc:0.984]
Epoch [67/120    avg_loss:0.008, val_acc:0.984]
Epoch [68/120    avg_loss:0.006, val_acc:0.984]
Epoch [69/120    avg_loss:0.007, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.983]
Epoch [71/120    avg_loss:0.006, val_acc:0.983]
Epoch [72/120    avg_loss:0.005, val_acc:0.983]
Epoch [73/120    avg_loss:0.006, val_acc:0.983]
Epoch [74/120    avg_loss:0.007, val_acc:0.983]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.005, val_acc:0.983]
Epoch [78/120    avg_loss:0.007, val_acc:0.983]
Epoch [79/120    avg_loss:0.006, val_acc:0.983]
Epoch [80/120    avg_loss:0.005, val_acc:0.983]
Epoch [81/120    avg_loss:0.006, val_acc:0.983]
Epoch [82/120    avg_loss:0.004, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.004, val_acc:0.983]
Epoch [86/120    avg_loss:0.004, val_acc:0.983]
Epoch [87/120    avg_loss:0.005, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.006, val_acc:0.983]
Epoch [94/120    avg_loss:0.004, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.983]
Epoch [103/120    avg_loss:0.004, val_acc:0.983]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.007, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.005, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     1     0     0    22     0    47     0]
 [    0     2 18051     0    18     0    12     0     7     0]
 [    0     0     0  1999     0     0     0     0    33     4]
 [    0    25     6     0  2926     0     1     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     4     0     0  4861     0     0     0]
 [    0     1     0     0     0     0     2  1286     0     1]
 [    0     7     0    19    49     0     0     0  3494     2]
 [    0     0     0     0    10    23     0     0     0   886]]

Accuracy:
99.22155544308679

F1 scores:
[       nan 0.99181542 0.99839602 0.98497167 0.97941423 0.99126472
 0.99447627 0.9984472  0.97529658 0.97738555]

Kappa:
0.9896878353444775
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd51ff24898>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.581, val_acc:0.398]
Epoch [2/120    avg_loss:1.042, val_acc:0.583]
Epoch [3/120    avg_loss:0.822, val_acc:0.680]
Epoch [4/120    avg_loss:0.604, val_acc:0.737]
Epoch [5/120    avg_loss:0.445, val_acc:0.747]
Epoch [6/120    avg_loss:0.360, val_acc:0.738]
Epoch [7/120    avg_loss:0.334, val_acc:0.818]
Epoch [8/120    avg_loss:0.322, val_acc:0.726]
Epoch [9/120    avg_loss:0.311, val_acc:0.857]
Epoch [10/120    avg_loss:0.280, val_acc:0.861]
Epoch [11/120    avg_loss:0.201, val_acc:0.893]
Epoch [12/120    avg_loss:0.238, val_acc:0.895]
Epoch [13/120    avg_loss:0.152, val_acc:0.922]
Epoch [14/120    avg_loss:0.159, val_acc:0.892]
Epoch [15/120    avg_loss:0.126, val_acc:0.887]
Epoch [16/120    avg_loss:0.138, val_acc:0.933]
Epoch [17/120    avg_loss:0.116, val_acc:0.944]
Epoch [18/120    avg_loss:0.135, val_acc:0.943]
Epoch [19/120    avg_loss:0.081, val_acc:0.956]
Epoch [20/120    avg_loss:0.086, val_acc:0.955]
Epoch [21/120    avg_loss:0.070, val_acc:0.935]
Epoch [22/120    avg_loss:0.067, val_acc:0.957]
Epoch [23/120    avg_loss:0.128, val_acc:0.927]
Epoch [24/120    avg_loss:0.078, val_acc:0.966]
Epoch [25/120    avg_loss:0.099, val_acc:0.926]
Epoch [26/120    avg_loss:0.086, val_acc:0.958]
Epoch [27/120    avg_loss:0.080, val_acc:0.954]
Epoch [28/120    avg_loss:0.056, val_acc:0.963]
Epoch [29/120    avg_loss:0.037, val_acc:0.974]
Epoch [30/120    avg_loss:0.039, val_acc:0.973]
Epoch [31/120    avg_loss:0.028, val_acc:0.976]
Epoch [32/120    avg_loss:0.016, val_acc:0.978]
Epoch [33/120    avg_loss:0.022, val_acc:0.973]
Epoch [34/120    avg_loss:0.029, val_acc:0.975]
Epoch [35/120    avg_loss:0.032, val_acc:0.976]
Epoch [36/120    avg_loss:0.024, val_acc:0.969]
Epoch [37/120    avg_loss:0.019, val_acc:0.983]
Epoch [38/120    avg_loss:0.024, val_acc:0.983]
Epoch [39/120    avg_loss:0.050, val_acc:0.968]
Epoch [40/120    avg_loss:0.030, val_acc:0.972]
Epoch [41/120    avg_loss:0.022, val_acc:0.976]
Epoch [42/120    avg_loss:0.067, val_acc:0.953]
Epoch [43/120    avg_loss:0.032, val_acc:0.975]
Epoch [44/120    avg_loss:0.018, val_acc:0.978]
Epoch [45/120    avg_loss:0.015, val_acc:0.983]
Epoch [46/120    avg_loss:0.018, val_acc:0.987]
Epoch [47/120    avg_loss:0.033, val_acc:0.983]
Epoch [48/120    avg_loss:0.017, val_acc:0.981]
Epoch [49/120    avg_loss:0.021, val_acc:0.974]
Epoch [50/120    avg_loss:0.023, val_acc:0.984]
Epoch [51/120    avg_loss:0.016, val_acc:0.987]
Epoch [52/120    avg_loss:0.011, val_acc:0.978]
Epoch [53/120    avg_loss:0.024, val_acc:0.965]
Epoch [54/120    avg_loss:0.012, val_acc:0.980]
Epoch [55/120    avg_loss:0.010, val_acc:0.977]
Epoch [56/120    avg_loss:0.018, val_acc:0.957]
Epoch [57/120    avg_loss:0.027, val_acc:0.978]
Epoch [58/120    avg_loss:0.017, val_acc:0.986]
Epoch [59/120    avg_loss:0.009, val_acc:0.978]
Epoch [60/120    avg_loss:0.012, val_acc:0.973]
Epoch [61/120    avg_loss:0.012, val_acc:0.987]
Epoch [62/120    avg_loss:0.009, val_acc:0.977]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.005, val_acc:0.980]
Epoch [65/120    avg_loss:0.007, val_acc:0.985]
Epoch [66/120    avg_loss:0.006, val_acc:0.984]
Epoch [67/120    avg_loss:0.009, val_acc:0.988]
Epoch [68/120    avg_loss:0.011, val_acc:0.983]
Epoch [69/120    avg_loss:0.005, val_acc:0.982]
Epoch [70/120    avg_loss:0.003, val_acc:0.983]
Epoch [71/120    avg_loss:0.004, val_acc:0.982]
Epoch [72/120    avg_loss:0.004, val_acc:0.985]
Epoch [73/120    avg_loss:0.004, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.015, val_acc:0.977]
Epoch [77/120    avg_loss:0.014, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.984]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.979]
Epoch [83/120    avg_loss:0.007, val_acc:0.984]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.019, val_acc:0.964]
Epoch [86/120    avg_loss:0.014, val_acc:0.973]
Epoch [87/120    avg_loss:0.033, val_acc:0.978]
Epoch [88/120    avg_loss:0.067, val_acc:0.968]
Epoch [89/120    avg_loss:0.025, val_acc:0.975]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.019, val_acc:0.974]
Epoch [93/120    avg_loss:0.014, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.980]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.017, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.007, val_acc:0.983]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     1     0     0    10     0    27     0]
 [    0     0 18056     0    15     0     8     0    11     0]
 [    0     0     0  1983     0     0     0     0    48     5]
 [    0    36    11     4  2911     0     2     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4871     0     6     0]
 [    0     8     0     0     0     0     2  1272     0     8]
 [    0     5     0    52    52     0     1     0  3458     3]
 [    0     0     0     0     6    19     0     0     0   894]]

Accuracy:
99.15889427132288

F1 scores:
[       nan 0.99324272 0.99875543 0.9727741  0.97750168 0.99277292
 0.99693    0.99297424 0.97012204 0.97758338]

Kappa:
0.9888571955070083
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3cd3aa4898>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.658, val_acc:0.626]
Epoch [2/120    avg_loss:1.053, val_acc:0.595]
Epoch [3/120    avg_loss:0.802, val_acc:0.678]
Epoch [4/120    avg_loss:0.593, val_acc:0.655]
Epoch [5/120    avg_loss:0.484, val_acc:0.811]
Epoch [6/120    avg_loss:0.413, val_acc:0.769]
Epoch [7/120    avg_loss:0.344, val_acc:0.872]
Epoch [8/120    avg_loss:0.317, val_acc:0.855]
Epoch [9/120    avg_loss:0.237, val_acc:0.895]
Epoch [10/120    avg_loss:0.225, val_acc:0.897]
Epoch [11/120    avg_loss:0.242, val_acc:0.901]
Epoch [12/120    avg_loss:0.180, val_acc:0.925]
Epoch [13/120    avg_loss:0.177, val_acc:0.933]
Epoch [14/120    avg_loss:0.274, val_acc:0.912]
Epoch [15/120    avg_loss:0.224, val_acc:0.904]
Epoch [16/120    avg_loss:0.142, val_acc:0.955]
Epoch [17/120    avg_loss:0.157, val_acc:0.856]
Epoch [18/120    avg_loss:0.111, val_acc:0.927]
Epoch [19/120    avg_loss:0.118, val_acc:0.945]
Epoch [20/120    avg_loss:0.096, val_acc:0.957]
Epoch [21/120    avg_loss:0.099, val_acc:0.957]
Epoch [22/120    avg_loss:0.076, val_acc:0.963]
Epoch [23/120    avg_loss:0.077, val_acc:0.864]
Epoch [24/120    avg_loss:0.104, val_acc:0.963]
Epoch [25/120    avg_loss:0.071, val_acc:0.946]
Epoch [26/120    avg_loss:0.070, val_acc:0.965]
Epoch [27/120    avg_loss:0.057, val_acc:0.970]
Epoch [28/120    avg_loss:0.043, val_acc:0.953]
Epoch [29/120    avg_loss:0.030, val_acc:0.970]
Epoch [30/120    avg_loss:0.045, val_acc:0.976]
Epoch [31/120    avg_loss:0.058, val_acc:0.972]
Epoch [32/120    avg_loss:0.054, val_acc:0.973]
Epoch [33/120    avg_loss:0.037, val_acc:0.956]
Epoch [34/120    avg_loss:0.030, val_acc:0.969]
Epoch [35/120    avg_loss:0.033, val_acc:0.971]
Epoch [36/120    avg_loss:0.052, val_acc:0.928]
Epoch [37/120    avg_loss:0.116, val_acc:0.935]
Epoch [38/120    avg_loss:0.070, val_acc:0.946]
Epoch [39/120    avg_loss:0.042, val_acc:0.973]
Epoch [40/120    avg_loss:0.125, val_acc:0.943]
Epoch [41/120    avg_loss:0.081, val_acc:0.965]
Epoch [42/120    avg_loss:0.057, val_acc:0.965]
Epoch [43/120    avg_loss:0.067, val_acc:0.940]
Epoch [44/120    avg_loss:0.043, val_acc:0.978]
Epoch [45/120    avg_loss:0.030, val_acc:0.977]
Epoch [46/120    avg_loss:0.033, val_acc:0.982]
Epoch [47/120    avg_loss:0.026, val_acc:0.980]
Epoch [48/120    avg_loss:0.024, val_acc:0.979]
Epoch [49/120    avg_loss:0.017, val_acc:0.983]
Epoch [50/120    avg_loss:0.021, val_acc:0.983]
Epoch [51/120    avg_loss:0.018, val_acc:0.983]
Epoch [52/120    avg_loss:0.020, val_acc:0.981]
Epoch [53/120    avg_loss:0.019, val_acc:0.982]
Epoch [54/120    avg_loss:0.020, val_acc:0.981]
Epoch [55/120    avg_loss:0.016, val_acc:0.982]
Epoch [56/120    avg_loss:0.016, val_acc:0.980]
Epoch [57/120    avg_loss:0.023, val_acc:0.982]
Epoch [58/120    avg_loss:0.017, val_acc:0.983]
Epoch [59/120    avg_loss:0.016, val_acc:0.981]
Epoch [60/120    avg_loss:0.020, val_acc:0.983]
Epoch [61/120    avg_loss:0.019, val_acc:0.979]
Epoch [62/120    avg_loss:0.022, val_acc:0.982]
Epoch [63/120    avg_loss:0.016, val_acc:0.984]
Epoch [64/120    avg_loss:0.016, val_acc:0.986]
Epoch [65/120    avg_loss:0.022, val_acc:0.982]
Epoch [66/120    avg_loss:0.019, val_acc:0.984]
Epoch [67/120    avg_loss:0.020, val_acc:0.980]
Epoch [68/120    avg_loss:0.020, val_acc:0.983]
Epoch [69/120    avg_loss:0.014, val_acc:0.982]
Epoch [70/120    avg_loss:0.015, val_acc:0.981]
Epoch [71/120    avg_loss:0.017, val_acc:0.981]
Epoch [72/120    avg_loss:0.019, val_acc:0.980]
Epoch [73/120    avg_loss:0.016, val_acc:0.980]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.982]
Epoch [76/120    avg_loss:0.015, val_acc:0.986]
Epoch [77/120    avg_loss:0.017, val_acc:0.986]
Epoch [78/120    avg_loss:0.013, val_acc:0.984]
Epoch [79/120    avg_loss:0.012, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.985]
Epoch [82/120    avg_loss:0.013, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.015, val_acc:0.986]
Epoch [85/120    avg_loss:0.015, val_acc:0.986]
Epoch [86/120    avg_loss:0.013, val_acc:0.986]
Epoch [87/120    avg_loss:0.015, val_acc:0.985]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.012, val_acc:0.988]
Epoch [90/120    avg_loss:0.024, val_acc:0.986]
Epoch [91/120    avg_loss:0.013, val_acc:0.987]
Epoch [92/120    avg_loss:0.012, val_acc:0.987]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.013, val_acc:0.987]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.013, val_acc:0.986]
Epoch [98/120    avg_loss:0.015, val_acc:0.987]
Epoch [99/120    avg_loss:0.019, val_acc:0.986]
Epoch [100/120    avg_loss:0.012, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.013, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.986]
Epoch [113/120    avg_loss:0.012, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.012, val_acc:0.987]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6323     0     1     0     0     5    61    34     8]
 [    0     0 18075     0    12     0     2     0     1     0]
 [    0     0     0  1997     0     0     0     0    33     6]
 [    0    20     6     0  2924     0     6     0    13     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     4     0     0  4852     0    10     0]
 [    0     0     0     0     0     0     2  1281     0     7]
 [    0     8     0    39    45     0     0     0  3475     4]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.08418287421975

F1 scores:
[       nan 0.98928264 0.99908797 0.97964189 0.98005698 0.99088838
 0.99579271 0.97340426 0.97379851 0.96389497]

Kappa:
0.9878683291416956
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d228157f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.693, val_acc:0.330]
Epoch [2/120    avg_loss:1.061, val_acc:0.408]
Epoch [3/120    avg_loss:0.820, val_acc:0.748]
Epoch [4/120    avg_loss:0.635, val_acc:0.731]
Epoch [5/120    avg_loss:0.494, val_acc:0.787]
Epoch [6/120    avg_loss:0.400, val_acc:0.726]
Epoch [7/120    avg_loss:0.349, val_acc:0.791]
Epoch [8/120    avg_loss:0.301, val_acc:0.806]
Epoch [9/120    avg_loss:0.279, val_acc:0.853]
Epoch [10/120    avg_loss:0.231, val_acc:0.863]
Epoch [11/120    avg_loss:0.177, val_acc:0.882]
Epoch [12/120    avg_loss:0.143, val_acc:0.924]
Epoch [13/120    avg_loss:0.140, val_acc:0.919]
Epoch [14/120    avg_loss:0.115, val_acc:0.932]
Epoch [15/120    avg_loss:0.105, val_acc:0.908]
Epoch [16/120    avg_loss:0.073, val_acc:0.941]
Epoch [17/120    avg_loss:0.099, val_acc:0.909]
Epoch [18/120    avg_loss:0.117, val_acc:0.936]
Epoch [19/120    avg_loss:0.095, val_acc:0.931]
Epoch [20/120    avg_loss:0.075, val_acc:0.922]
Epoch [21/120    avg_loss:0.087, val_acc:0.935]
Epoch [22/120    avg_loss:0.127, val_acc:0.931]
Epoch [23/120    avg_loss:0.110, val_acc:0.947]
Epoch [24/120    avg_loss:0.056, val_acc:0.945]
Epoch [25/120    avg_loss:0.066, val_acc:0.962]
Epoch [26/120    avg_loss:0.114, val_acc:0.897]
Epoch [27/120    avg_loss:0.101, val_acc:0.946]
Epoch [28/120    avg_loss:0.086, val_acc:0.935]
Epoch [29/120    avg_loss:0.079, val_acc:0.955]
Epoch [30/120    avg_loss:0.065, val_acc:0.921]
Epoch [31/120    avg_loss:0.070, val_acc:0.953]
Epoch [32/120    avg_loss:0.071, val_acc:0.939]
Epoch [33/120    avg_loss:0.041, val_acc:0.953]
Epoch [34/120    avg_loss:0.040, val_acc:0.957]
Epoch [35/120    avg_loss:0.028, val_acc:0.965]
Epoch [36/120    avg_loss:0.020, val_acc:0.967]
Epoch [37/120    avg_loss:0.024, val_acc:0.969]
Epoch [38/120    avg_loss:0.027, val_acc:0.963]
Epoch [39/120    avg_loss:0.031, val_acc:0.968]
Epoch [40/120    avg_loss:0.021, val_acc:0.972]
Epoch [41/120    avg_loss:0.015, val_acc:0.957]
Epoch [42/120    avg_loss:0.029, val_acc:0.957]
Epoch [43/120    avg_loss:0.020, val_acc:0.966]
Epoch [44/120    avg_loss:0.018, val_acc:0.971]
Epoch [45/120    avg_loss:0.030, val_acc:0.914]
Epoch [46/120    avg_loss:0.033, val_acc:0.973]
Epoch [47/120    avg_loss:0.016, val_acc:0.975]
Epoch [48/120    avg_loss:0.022, val_acc:0.968]
Epoch [49/120    avg_loss:0.017, val_acc:0.978]
Epoch [50/120    avg_loss:0.016, val_acc:0.973]
Epoch [51/120    avg_loss:0.023, val_acc:0.956]
Epoch [52/120    avg_loss:0.037, val_acc:0.932]
Epoch [53/120    avg_loss:0.024, val_acc:0.973]
Epoch [54/120    avg_loss:0.013, val_acc:0.968]
Epoch [55/120    avg_loss:0.022, val_acc:0.971]
Epoch [56/120    avg_loss:0.013, val_acc:0.966]
Epoch [57/120    avg_loss:0.016, val_acc:0.968]
Epoch [58/120    avg_loss:0.009, val_acc:0.965]
Epoch [59/120    avg_loss:0.021, val_acc:0.976]
Epoch [60/120    avg_loss:0.012, val_acc:0.976]
Epoch [61/120    avg_loss:0.011, val_acc:0.974]
Epoch [62/120    avg_loss:0.010, val_acc:0.974]
Epoch [63/120    avg_loss:0.007, val_acc:0.977]
Epoch [64/120    avg_loss:0.008, val_acc:0.978]
Epoch [65/120    avg_loss:0.008, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.978]
Epoch [67/120    avg_loss:0.005, val_acc:0.978]
Epoch [68/120    avg_loss:0.005, val_acc:0.978]
Epoch [69/120    avg_loss:0.010, val_acc:0.979]
Epoch [70/120    avg_loss:0.008, val_acc:0.978]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.005, val_acc:0.980]
Epoch [73/120    avg_loss:0.004, val_acc:0.980]
Epoch [74/120    avg_loss:0.006, val_acc:0.981]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.007, val_acc:0.979]
Epoch [77/120    avg_loss:0.004, val_acc:0.979]
Epoch [78/120    avg_loss:0.006, val_acc:0.979]
Epoch [79/120    avg_loss:0.007, val_acc:0.979]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.004, val_acc:0.979]
Epoch [82/120    avg_loss:0.005, val_acc:0.981]
Epoch [83/120    avg_loss:0.005, val_acc:0.982]
Epoch [84/120    avg_loss:0.005, val_acc:0.981]
Epoch [85/120    avg_loss:0.005, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.982]
Epoch [88/120    avg_loss:0.005, val_acc:0.980]
Epoch [89/120    avg_loss:0.005, val_acc:0.980]
Epoch [90/120    avg_loss:0.005, val_acc:0.979]
Epoch [91/120    avg_loss:0.004, val_acc:0.980]
Epoch [92/120    avg_loss:0.004, val_acc:0.981]
Epoch [93/120    avg_loss:0.004, val_acc:0.980]
Epoch [94/120    avg_loss:0.005, val_acc:0.979]
Epoch [95/120    avg_loss:0.005, val_acc:0.981]
Epoch [96/120    avg_loss:0.007, val_acc:0.980]
Epoch [97/120    avg_loss:0.005, val_acc:0.979]
Epoch [98/120    avg_loss:0.005, val_acc:0.980]
Epoch [99/120    avg_loss:0.004, val_acc:0.980]
Epoch [100/120    avg_loss:0.004, val_acc:0.980]
Epoch [101/120    avg_loss:0.005, val_acc:0.980]
Epoch [102/120    avg_loss:0.005, val_acc:0.980]
Epoch [103/120    avg_loss:0.005, val_acc:0.980]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.006, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.003, val_acc:0.980]
Epoch [108/120    avg_loss:0.006, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.004, val_acc:0.980]
Epoch [111/120    avg_loss:0.004, val_acc:0.980]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.004, val_acc:0.980]
Epoch [116/120    avg_loss:0.005, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.005, val_acc:0.980]
Epoch [119/120    avg_loss:0.005, val_acc:0.980]
Epoch [120/120    avg_loss:0.005, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6361     0     1     0     0    10     3    57     0]
 [    0     0 17925     0    35     0   127     0     3     0]
 [    0     0     0  1967     0     0     0     0    60     9]
 [    0    34     5     0  2926     0     2     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4865     0    11     0]
 [    0     3     0     0     0     0     3  1281     0     3]
 [    0    25     0    41    54     0     0     0  3447     4]
 [    0     0     0     0     9    16     0     0     0   894]]

Accuracy:
98.74195647458608

F1 scores:
[       nan 0.98965383 0.9952804  0.97207808 0.97598399 0.99390708
 0.98431968 0.995338   0.9636567  0.97758338]

Kappa:
0.983357978700789
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e071be828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.636, val_acc:0.514]
Epoch [2/120    avg_loss:1.042, val_acc:0.676]
Epoch [3/120    avg_loss:0.812, val_acc:0.545]
Epoch [4/120    avg_loss:0.601, val_acc:0.645]
Epoch [5/120    avg_loss:0.510, val_acc:0.783]
Epoch [6/120    avg_loss:0.400, val_acc:0.815]
Epoch [7/120    avg_loss:0.329, val_acc:0.818]
Epoch [8/120    avg_loss:0.268, val_acc:0.786]
Epoch [9/120    avg_loss:0.232, val_acc:0.923]
Epoch [10/120    avg_loss:0.177, val_acc:0.903]
Epoch [11/120    avg_loss:0.172, val_acc:0.927]
Epoch [12/120    avg_loss:0.164, val_acc:0.787]
Epoch [13/120    avg_loss:0.159, val_acc:0.860]
Epoch [14/120    avg_loss:0.128, val_acc:0.941]
Epoch [15/120    avg_loss:0.122, val_acc:0.935]
Epoch [16/120    avg_loss:0.130, val_acc:0.873]
Epoch [17/120    avg_loss:0.097, val_acc:0.958]
Epoch [18/120    avg_loss:0.109, val_acc:0.946]
Epoch [19/120    avg_loss:0.074, val_acc:0.957]
Epoch [20/120    avg_loss:0.057, val_acc:0.969]
Epoch [21/120    avg_loss:0.045, val_acc:0.973]
Epoch [22/120    avg_loss:0.051, val_acc:0.968]
Epoch [23/120    avg_loss:0.050, val_acc:0.963]
Epoch [24/120    avg_loss:0.064, val_acc:0.923]
Epoch [25/120    avg_loss:0.052, val_acc:0.969]
Epoch [26/120    avg_loss:0.050, val_acc:0.974]
Epoch [27/120    avg_loss:0.044, val_acc:0.971]
Epoch [28/120    avg_loss:0.049, val_acc:0.978]
Epoch [29/120    avg_loss:0.045, val_acc:0.963]
Epoch [30/120    avg_loss:0.028, val_acc:0.973]
Epoch [31/120    avg_loss:0.028, val_acc:0.972]
Epoch [32/120    avg_loss:0.030, val_acc:0.965]
Epoch [33/120    avg_loss:0.023, val_acc:0.967]
Epoch [34/120    avg_loss:0.018, val_acc:0.981]
Epoch [35/120    avg_loss:0.014, val_acc:0.979]
Epoch [36/120    avg_loss:0.013, val_acc:0.972]
Epoch [37/120    avg_loss:0.017, val_acc:0.970]
Epoch [38/120    avg_loss:0.016, val_acc:0.977]
Epoch [39/120    avg_loss:0.025, val_acc:0.977]
Epoch [40/120    avg_loss:0.033, val_acc:0.984]
Epoch [41/120    avg_loss:0.012, val_acc:0.981]
Epoch [42/120    avg_loss:0.019, val_acc:0.978]
Epoch [43/120    avg_loss:0.022, val_acc:0.969]
Epoch [44/120    avg_loss:0.013, val_acc:0.984]
Epoch [45/120    avg_loss:0.008, val_acc:0.980]
Epoch [46/120    avg_loss:0.015, val_acc:0.974]
Epoch [47/120    avg_loss:0.065, val_acc:0.967]
Epoch [48/120    avg_loss:0.025, val_acc:0.948]
Epoch [49/120    avg_loss:0.034, val_acc:0.960]
Epoch [50/120    avg_loss:0.030, val_acc:0.976]
Epoch [51/120    avg_loss:0.025, val_acc:0.969]
Epoch [52/120    avg_loss:0.020, val_acc:0.981]
Epoch [53/120    avg_loss:0.031, val_acc:0.966]
Epoch [54/120    avg_loss:0.020, val_acc:0.983]
Epoch [55/120    avg_loss:0.021, val_acc:0.980]
Epoch [56/120    avg_loss:0.018, val_acc:0.980]
Epoch [57/120    avg_loss:0.012, val_acc:0.985]
Epoch [58/120    avg_loss:0.008, val_acc:0.976]
Epoch [59/120    avg_loss:0.011, val_acc:0.980]
Epoch [60/120    avg_loss:0.008, val_acc:0.986]
Epoch [61/120    avg_loss:0.006, val_acc:0.985]
Epoch [62/120    avg_loss:0.007, val_acc:0.983]
Epoch [63/120    avg_loss:0.008, val_acc:0.985]
Epoch [64/120    avg_loss:0.005, val_acc:0.982]
Epoch [65/120    avg_loss:0.006, val_acc:0.986]
Epoch [66/120    avg_loss:0.010, val_acc:0.982]
Epoch [67/120    avg_loss:0.011, val_acc:0.986]
Epoch [68/120    avg_loss:0.015, val_acc:0.986]
Epoch [69/120    avg_loss:0.009, val_acc:0.986]
Epoch [70/120    avg_loss:0.007, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.986]
Epoch [72/120    avg_loss:0.006, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.989]
Epoch [74/120    avg_loss:0.019, val_acc:0.955]
Epoch [75/120    avg_loss:0.018, val_acc:0.989]
Epoch [76/120    avg_loss:0.010, val_acc:0.987]
Epoch [77/120    avg_loss:0.007, val_acc:0.986]
Epoch [78/120    avg_loss:0.033, val_acc:0.972]
Epoch [79/120    avg_loss:0.015, val_acc:0.969]
Epoch [80/120    avg_loss:0.079, val_acc:0.953]
Epoch [81/120    avg_loss:0.036, val_acc:0.970]
Epoch [82/120    avg_loss:0.015, val_acc:0.983]
Epoch [83/120    avg_loss:0.010, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.989]
Epoch [87/120    avg_loss:0.008, val_acc:0.991]
Epoch [88/120    avg_loss:0.007, val_acc:0.991]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.021, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.991]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.012, val_acc:0.989]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.003, val_acc:0.990]
Epoch [103/120    avg_loss:0.003, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.002, val_acc:0.990]
Epoch [106/120    avg_loss:0.003, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.003, val_acc:0.990]
Epoch [110/120    avg_loss:0.002, val_acc:0.991]
Epoch [111/120    avg_loss:0.002, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.002, val_acc:0.991]
Epoch [115/120    avg_loss:0.002, val_acc:0.991]
Epoch [116/120    avg_loss:0.002, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.002, val_acc:0.991]
Epoch [119/120    avg_loss:0.002, val_acc:0.991]
Epoch [120/120    avg_loss:0.003, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     0     0     0     5    26     0]
 [    0     0 18070     0     8     0    11     0     1     0]
 [    0     0     0  2017     1     0     0     0    14     4]
 [    0    34     5     0  2914     0     1     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     3     0    29    45     0     0     0  3483    11]
 [    0     0     0     0    13    35     0     0     0   871]]

Accuracy:
99.35892801195382

F1 scores:
[       nan 0.99471639 0.99930872 0.98824106 0.97900218 0.98676749
 0.99856646 0.99806576 0.97905833 0.96509695]

Kappa:
0.9915067913619705
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd25d40860>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.655, val_acc:0.368]
Epoch [2/120    avg_loss:1.067, val_acc:0.460]
Epoch [3/120    avg_loss:0.729, val_acc:0.724]
Epoch [4/120    avg_loss:0.592, val_acc:0.798]
Epoch [5/120    avg_loss:0.478, val_acc:0.821]
Epoch [6/120    avg_loss:0.349, val_acc:0.864]
Epoch [7/120    avg_loss:0.316, val_acc:0.855]
Epoch [8/120    avg_loss:0.282, val_acc:0.921]
Epoch [9/120    avg_loss:0.213, val_acc:0.917]
Epoch [10/120    avg_loss:0.183, val_acc:0.946]
Epoch [11/120    avg_loss:0.151, val_acc:0.939]
Epoch [12/120    avg_loss:0.201, val_acc:0.884]
Epoch [13/120    avg_loss:0.162, val_acc:0.957]
Epoch [14/120    avg_loss:0.118, val_acc:0.952]
Epoch [15/120    avg_loss:0.156, val_acc:0.954]
Epoch [16/120    avg_loss:0.121, val_acc:0.949]
Epoch [17/120    avg_loss:0.103, val_acc:0.940]
Epoch [18/120    avg_loss:0.080, val_acc:0.955]
Epoch [19/120    avg_loss:0.077, val_acc:0.969]
Epoch [20/120    avg_loss:0.066, val_acc:0.969]
Epoch [21/120    avg_loss:0.084, val_acc:0.963]
Epoch [22/120    avg_loss:0.058, val_acc:0.959]
Epoch [23/120    avg_loss:0.063, val_acc:0.983]
Epoch [24/120    avg_loss:0.046, val_acc:0.985]
Epoch [25/120    avg_loss:0.043, val_acc:0.983]
Epoch [26/120    avg_loss:0.036, val_acc:0.983]
Epoch [27/120    avg_loss:0.037, val_acc:0.986]
Epoch [28/120    avg_loss:0.051, val_acc:0.977]
Epoch [29/120    avg_loss:0.042, val_acc:0.969]
Epoch [30/120    avg_loss:0.077, val_acc:0.976]
Epoch [31/120    avg_loss:0.047, val_acc:0.967]
Epoch [32/120    avg_loss:0.033, val_acc:0.981]
Epoch [33/120    avg_loss:0.070, val_acc:0.973]
Epoch [34/120    avg_loss:0.042, val_acc:0.980]
Epoch [35/120    avg_loss:0.026, val_acc:0.986]
Epoch [36/120    avg_loss:0.033, val_acc:0.985]
Epoch [37/120    avg_loss:0.020, val_acc:0.984]
Epoch [38/120    avg_loss:0.014, val_acc:0.987]
Epoch [39/120    avg_loss:0.020, val_acc:0.969]
Epoch [40/120    avg_loss:0.047, val_acc:0.962]
Epoch [41/120    avg_loss:0.056, val_acc:0.968]
Epoch [42/120    avg_loss:0.058, val_acc:0.980]
Epoch [43/120    avg_loss:0.017, val_acc:0.989]
Epoch [44/120    avg_loss:0.013, val_acc:0.989]
Epoch [45/120    avg_loss:0.015, val_acc:0.989]
Epoch [46/120    avg_loss:0.015, val_acc:0.981]
Epoch [47/120    avg_loss:0.021, val_acc:0.990]
Epoch [48/120    avg_loss:0.020, val_acc:0.989]
Epoch [49/120    avg_loss:0.013, val_acc:0.971]
Epoch [50/120    avg_loss:0.011, val_acc:0.981]
Epoch [51/120    avg_loss:0.013, val_acc:0.986]
Epoch [52/120    avg_loss:0.011, val_acc:0.993]
Epoch [53/120    avg_loss:0.011, val_acc:0.991]
Epoch [54/120    avg_loss:0.017, val_acc:0.979]
Epoch [55/120    avg_loss:0.007, val_acc:0.987]
Epoch [56/120    avg_loss:0.010, val_acc:0.986]
Epoch [57/120    avg_loss:0.010, val_acc:0.993]
Epoch [58/120    avg_loss:0.005, val_acc:0.989]
Epoch [59/120    avg_loss:0.004, val_acc:0.991]
Epoch [60/120    avg_loss:0.006, val_acc:0.992]
Epoch [61/120    avg_loss:0.010, val_acc:0.991]
Epoch [62/120    avg_loss:0.017, val_acc:0.987]
Epoch [63/120    avg_loss:0.014, val_acc:0.990]
Epoch [64/120    avg_loss:0.006, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.991]
Epoch [66/120    avg_loss:0.012, val_acc:0.981]
Epoch [67/120    avg_loss:0.010, val_acc:0.992]
Epoch [68/120    avg_loss:0.007, val_acc:0.991]
Epoch [69/120    avg_loss:0.007, val_acc:0.992]
Epoch [70/120    avg_loss:0.007, val_acc:0.992]
Epoch [71/120    avg_loss:0.006, val_acc:0.993]
Epoch [72/120    avg_loss:0.004, val_acc:0.993]
Epoch [73/120    avg_loss:0.004, val_acc:0.995]
Epoch [74/120    avg_loss:0.005, val_acc:0.995]
Epoch [75/120    avg_loss:0.006, val_acc:0.993]
Epoch [76/120    avg_loss:0.004, val_acc:0.993]
Epoch [77/120    avg_loss:0.005, val_acc:0.993]
Epoch [78/120    avg_loss:0.005, val_acc:0.993]
Epoch [79/120    avg_loss:0.004, val_acc:0.993]
Epoch [80/120    avg_loss:0.004, val_acc:0.993]
Epoch [81/120    avg_loss:0.003, val_acc:0.993]
Epoch [82/120    avg_loss:0.004, val_acc:0.993]
Epoch [83/120    avg_loss:0.003, val_acc:0.993]
Epoch [84/120    avg_loss:0.005, val_acc:0.993]
Epoch [85/120    avg_loss:0.003, val_acc:0.993]
Epoch [86/120    avg_loss:0.004, val_acc:0.993]
Epoch [87/120    avg_loss:0.004, val_acc:0.994]
Epoch [88/120    avg_loss:0.006, val_acc:0.993]
Epoch [89/120    avg_loss:0.003, val_acc:0.993]
Epoch [90/120    avg_loss:0.005, val_acc:0.993]
Epoch [91/120    avg_loss:0.005, val_acc:0.993]
Epoch [92/120    avg_loss:0.003, val_acc:0.993]
Epoch [93/120    avg_loss:0.004, val_acc:0.994]
Epoch [94/120    avg_loss:0.004, val_acc:0.994]
Epoch [95/120    avg_loss:0.004, val_acc:0.994]
Epoch [96/120    avg_loss:0.003, val_acc:0.994]
Epoch [97/120    avg_loss:0.003, val_acc:0.994]
Epoch [98/120    avg_loss:0.004, val_acc:0.994]
Epoch [99/120    avg_loss:0.004, val_acc:0.994]
Epoch [100/120    avg_loss:0.005, val_acc:0.994]
Epoch [101/120    avg_loss:0.004, val_acc:0.994]
Epoch [102/120    avg_loss:0.004, val_acc:0.994]
Epoch [103/120    avg_loss:0.004, val_acc:0.994]
Epoch [104/120    avg_loss:0.004, val_acc:0.994]
Epoch [105/120    avg_loss:0.004, val_acc:0.994]
Epoch [106/120    avg_loss:0.003, val_acc:0.994]
Epoch [107/120    avg_loss:0.003, val_acc:0.994]
Epoch [108/120    avg_loss:0.004, val_acc:0.994]
Epoch [109/120    avg_loss:0.004, val_acc:0.994]
Epoch [110/120    avg_loss:0.003, val_acc:0.994]
Epoch [111/120    avg_loss:0.003, val_acc:0.994]
Epoch [112/120    avg_loss:0.005, val_acc:0.994]
Epoch [113/120    avg_loss:0.004, val_acc:0.994]
Epoch [114/120    avg_loss:0.003, val_acc:0.994]
Epoch [115/120    avg_loss:0.003, val_acc:0.994]
Epoch [116/120    avg_loss:0.005, val_acc:0.994]
Epoch [117/120    avg_loss:0.003, val_acc:0.994]
Epoch [118/120    avg_loss:0.004, val_acc:0.994]
Epoch [119/120    avg_loss:0.003, val_acc:0.994]
Epoch [120/120    avg_loss:0.003, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     1     0    12     0    26     0]
 [    0     0 18068     0    15     0     4     0     3     0]
 [    0     2     0  2012     3     0     0     0    13     6]
 [    0    32    11     0  2904     0     1     0    24     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4859     0    19     0]
 [    0     0     0     1     0     0     1  1288     0     0]
 [    0     3     0    42    54     0     0     0  3469     3]
 [    0     0     0     0    15    32     0     0     0   872]]

Accuracy:
99.22155544308679

F1 scores:
[       nan 0.99409112 0.99908762 0.98362259 0.97384306 0.98788796
 0.99620707 0.9992242  0.97375439 0.96888889]

Kappa:
0.9896865289457142
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f85088fc828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.677, val_acc:0.345]
Epoch [2/120    avg_loss:1.098, val_acc:0.486]
Epoch [3/120    avg_loss:0.839, val_acc:0.614]
Epoch [4/120    avg_loss:0.592, val_acc:0.698]
Epoch [5/120    avg_loss:0.454, val_acc:0.797]
Epoch [6/120    avg_loss:0.380, val_acc:0.846]
Epoch [7/120    avg_loss:0.313, val_acc:0.900]
Epoch [8/120    avg_loss:0.266, val_acc:0.903]
Epoch [9/120    avg_loss:0.232, val_acc:0.840]
Epoch [10/120    avg_loss:0.192, val_acc:0.923]
Epoch [11/120    avg_loss:0.182, val_acc:0.946]
Epoch [12/120    avg_loss:0.225, val_acc:0.912]
Epoch [13/120    avg_loss:0.145, val_acc:0.945]
Epoch [14/120    avg_loss:0.156, val_acc:0.952]
Epoch [15/120    avg_loss:0.137, val_acc:0.944]
Epoch [16/120    avg_loss:0.098, val_acc:0.948]
Epoch [17/120    avg_loss:0.104, val_acc:0.953]
Epoch [18/120    avg_loss:0.069, val_acc:0.974]
Epoch [19/120    avg_loss:0.071, val_acc:0.957]
Epoch [20/120    avg_loss:0.123, val_acc:0.943]
Epoch [21/120    avg_loss:0.076, val_acc:0.960]
Epoch [22/120    avg_loss:0.067, val_acc:0.969]
Epoch [23/120    avg_loss:0.083, val_acc:0.966]
Epoch [24/120    avg_loss:0.052, val_acc:0.967]
Epoch [25/120    avg_loss:0.085, val_acc:0.967]
Epoch [26/120    avg_loss:0.046, val_acc:0.967]
Epoch [27/120    avg_loss:0.042, val_acc:0.955]
Epoch [28/120    avg_loss:0.036, val_acc:0.969]
Epoch [29/120    avg_loss:0.024, val_acc:0.960]
Epoch [30/120    avg_loss:0.049, val_acc:0.969]
Epoch [31/120    avg_loss:0.103, val_acc:0.851]
Epoch [32/120    avg_loss:0.142, val_acc:0.935]
Epoch [33/120    avg_loss:0.085, val_acc:0.948]
Epoch [34/120    avg_loss:0.055, val_acc:0.963]
Epoch [35/120    avg_loss:0.055, val_acc:0.959]
Epoch [36/120    avg_loss:0.048, val_acc:0.968]
Epoch [37/120    avg_loss:0.043, val_acc:0.968]
Epoch [38/120    avg_loss:0.039, val_acc:0.969]
Epoch [39/120    avg_loss:0.032, val_acc:0.972]
Epoch [40/120    avg_loss:0.031, val_acc:0.969]
Epoch [41/120    avg_loss:0.032, val_acc:0.971]
Epoch [42/120    avg_loss:0.028, val_acc:0.972]
Epoch [43/120    avg_loss:0.026, val_acc:0.973]
Epoch [44/120    avg_loss:0.032, val_acc:0.969]
Epoch [45/120    avg_loss:0.028, val_acc:0.971]
Epoch [46/120    avg_loss:0.024, val_acc:0.974]
Epoch [47/120    avg_loss:0.025, val_acc:0.974]
Epoch [48/120    avg_loss:0.025, val_acc:0.974]
Epoch [49/120    avg_loss:0.029, val_acc:0.975]
Epoch [50/120    avg_loss:0.022, val_acc:0.974]
Epoch [51/120    avg_loss:0.023, val_acc:0.974]
Epoch [52/120    avg_loss:0.025, val_acc:0.973]
Epoch [53/120    avg_loss:0.025, val_acc:0.974]
Epoch [54/120    avg_loss:0.032, val_acc:0.972]
Epoch [55/120    avg_loss:0.022, val_acc:0.974]
Epoch [56/120    avg_loss:0.027, val_acc:0.974]
Epoch [57/120    avg_loss:0.023, val_acc:0.974]
Epoch [58/120    avg_loss:0.027, val_acc:0.975]
Epoch [59/120    avg_loss:0.022, val_acc:0.975]
Epoch [60/120    avg_loss:0.022, val_acc:0.974]
Epoch [61/120    avg_loss:0.029, val_acc:0.974]
Epoch [62/120    avg_loss:0.031, val_acc:0.975]
Epoch [63/120    avg_loss:0.028, val_acc:0.974]
Epoch [64/120    avg_loss:0.023, val_acc:0.974]
Epoch [65/120    avg_loss:0.024, val_acc:0.974]
Epoch [66/120    avg_loss:0.023, val_acc:0.975]
Epoch [67/120    avg_loss:0.024, val_acc:0.976]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.025, val_acc:0.975]
Epoch [70/120    avg_loss:0.023, val_acc:0.974]
Epoch [71/120    avg_loss:0.027, val_acc:0.975]
Epoch [72/120    avg_loss:0.026, val_acc:0.975]
Epoch [73/120    avg_loss:0.022, val_acc:0.975]
Epoch [74/120    avg_loss:0.025, val_acc:0.976]
Epoch [75/120    avg_loss:0.023, val_acc:0.976]
Epoch [76/120    avg_loss:0.026, val_acc:0.975]
Epoch [77/120    avg_loss:0.022, val_acc:0.975]
Epoch [78/120    avg_loss:0.027, val_acc:0.976]
Epoch [79/120    avg_loss:0.025, val_acc:0.975]
Epoch [80/120    avg_loss:0.027, val_acc:0.975]
Epoch [81/120    avg_loss:0.031, val_acc:0.975]
Epoch [82/120    avg_loss:0.020, val_acc:0.975]
Epoch [83/120    avg_loss:0.025, val_acc:0.975]
Epoch [84/120    avg_loss:0.020, val_acc:0.976]
Epoch [85/120    avg_loss:0.028, val_acc:0.976]
Epoch [86/120    avg_loss:0.026, val_acc:0.975]
Epoch [87/120    avg_loss:0.025, val_acc:0.975]
Epoch [88/120    avg_loss:0.021, val_acc:0.975]
Epoch [89/120    avg_loss:0.025, val_acc:0.975]
Epoch [90/120    avg_loss:0.022, val_acc:0.976]
Epoch [91/120    avg_loss:0.025, val_acc:0.976]
Epoch [92/120    avg_loss:0.026, val_acc:0.976]
Epoch [93/120    avg_loss:0.028, val_acc:0.977]
Epoch [94/120    avg_loss:0.025, val_acc:0.976]
Epoch [95/120    avg_loss:0.019, val_acc:0.977]
Epoch [96/120    avg_loss:0.026, val_acc:0.976]
Epoch [97/120    avg_loss:0.021, val_acc:0.976]
Epoch [98/120    avg_loss:0.023, val_acc:0.977]
Epoch [99/120    avg_loss:0.020, val_acc:0.976]
Epoch [100/120    avg_loss:0.020, val_acc:0.976]
Epoch [101/120    avg_loss:0.020, val_acc:0.976]
Epoch [102/120    avg_loss:0.023, val_acc:0.977]
Epoch [103/120    avg_loss:0.024, val_acc:0.977]
Epoch [104/120    avg_loss:0.024, val_acc:0.977]
Epoch [105/120    avg_loss:0.020, val_acc:0.977]
Epoch [106/120    avg_loss:0.025, val_acc:0.977]
Epoch [107/120    avg_loss:0.019, val_acc:0.977]
Epoch [108/120    avg_loss:0.022, val_acc:0.975]
Epoch [109/120    avg_loss:0.024, val_acc:0.977]
Epoch [110/120    avg_loss:0.023, val_acc:0.977]
Epoch [111/120    avg_loss:0.024, val_acc:0.977]
Epoch [112/120    avg_loss:0.022, val_acc:0.977]
Epoch [113/120    avg_loss:0.021, val_acc:0.977]
Epoch [114/120    avg_loss:0.028, val_acc:0.977]
Epoch [115/120    avg_loss:0.022, val_acc:0.977]
Epoch [116/120    avg_loss:0.019, val_acc:0.977]
Epoch [117/120    avg_loss:0.031, val_acc:0.977]
Epoch [118/120    avg_loss:0.020, val_acc:0.977]
Epoch [119/120    avg_loss:0.022, val_acc:0.978]
Epoch [120/120    avg_loss:0.020, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6363     0     1     4     0     0     1    63     0]
 [    0     4 17903     0    60     0   118     0     5     0]
 [    0     0     0  2001     2     0     0     0    24     9]
 [    0    27     3     0  2916     0     3     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4859     0     8     5]
 [    0     0     0     0     0     0     5  1282     0     3]
 [    0     0     0    10    58     0     0     0  3489    14]
 [    0     0     0     0    14    31     2     0     0   872]]

Accuracy:
98.78774733087509

F1 scores:
[       nan 0.99220334 0.99472164 0.98717316 0.96780617 0.98826202
 0.98509883 0.99650214 0.97146039 0.9571899 ]

Kappa:
0.9839698740857725
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f238720a828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.663, val_acc:0.313]
Epoch [2/120    avg_loss:1.095, val_acc:0.729]
Epoch [3/120    avg_loss:0.809, val_acc:0.760]
Epoch [4/120    avg_loss:0.635, val_acc:0.811]
Epoch [5/120    avg_loss:0.474, val_acc:0.791]
Epoch [6/120    avg_loss:0.375, val_acc:0.689]
Epoch [7/120    avg_loss:0.381, val_acc:0.760]
Epoch [8/120    avg_loss:0.314, val_acc:0.872]
Epoch [9/120    avg_loss:0.325, val_acc:0.810]
Epoch [10/120    avg_loss:0.250, val_acc:0.874]
Epoch [11/120    avg_loss:0.259, val_acc:0.929]
Epoch [12/120    avg_loss:0.208, val_acc:0.918]
Epoch [13/120    avg_loss:0.185, val_acc:0.914]
Epoch [14/120    avg_loss:0.186, val_acc:0.912]
Epoch [15/120    avg_loss:0.154, val_acc:0.849]
Epoch [16/120    avg_loss:0.143, val_acc:0.929]
Epoch [17/120    avg_loss:0.096, val_acc:0.946]
Epoch [18/120    avg_loss:0.085, val_acc:0.951]
Epoch [19/120    avg_loss:0.075, val_acc:0.939]
Epoch [20/120    avg_loss:0.086, val_acc:0.941]
Epoch [21/120    avg_loss:0.085, val_acc:0.933]
Epoch [22/120    avg_loss:0.098, val_acc:0.954]
Epoch [23/120    avg_loss:0.083, val_acc:0.957]
Epoch [24/120    avg_loss:0.073, val_acc:0.963]
Epoch [25/120    avg_loss:0.054, val_acc:0.966]
Epoch [26/120    avg_loss:0.052, val_acc:0.974]
Epoch [27/120    avg_loss:0.051, val_acc:0.961]
Epoch [28/120    avg_loss:0.043, val_acc:0.970]
Epoch [29/120    avg_loss:0.038, val_acc:0.969]
Epoch [30/120    avg_loss:0.098, val_acc:0.949]
Epoch [31/120    avg_loss:0.069, val_acc:0.960]
Epoch [32/120    avg_loss:0.031, val_acc:0.962]
Epoch [33/120    avg_loss:0.064, val_acc:0.946]
Epoch [34/120    avg_loss:0.066, val_acc:0.950]
Epoch [35/120    avg_loss:0.049, val_acc:0.972]
Epoch [36/120    avg_loss:0.023, val_acc:0.974]
Epoch [37/120    avg_loss:0.023, val_acc:0.984]
Epoch [38/120    avg_loss:0.015, val_acc:0.983]
Epoch [39/120    avg_loss:0.015, val_acc:0.982]
Epoch [40/120    avg_loss:0.022, val_acc:0.986]
Epoch [41/120    avg_loss:0.026, val_acc:0.984]
Epoch [42/120    avg_loss:0.020, val_acc:0.978]
Epoch [43/120    avg_loss:0.036, val_acc:0.980]
Epoch [44/120    avg_loss:0.018, val_acc:0.985]
Epoch [45/120    avg_loss:0.016, val_acc:0.987]
Epoch [46/120    avg_loss:0.014, val_acc:0.986]
Epoch [47/120    avg_loss:0.014, val_acc:0.957]
Epoch [48/120    avg_loss:0.020, val_acc:0.986]
Epoch [49/120    avg_loss:0.018, val_acc:0.983]
Epoch [50/120    avg_loss:0.013, val_acc:0.989]
Epoch [51/120    avg_loss:0.016, val_acc:0.978]
Epoch [52/120    avg_loss:0.014, val_acc:0.980]
Epoch [53/120    avg_loss:0.010, val_acc:0.985]
Epoch [54/120    avg_loss:0.011, val_acc:0.987]
Epoch [55/120    avg_loss:0.009, val_acc:0.985]
Epoch [56/120    avg_loss:0.010, val_acc:0.992]
Epoch [57/120    avg_loss:0.007, val_acc:0.989]
Epoch [58/120    avg_loss:0.009, val_acc:0.986]
Epoch [59/120    avg_loss:0.009, val_acc:0.989]
Epoch [60/120    avg_loss:0.014, val_acc:0.986]
Epoch [61/120    avg_loss:0.015, val_acc:0.985]
Epoch [62/120    avg_loss:0.008, val_acc:0.989]
Epoch [63/120    avg_loss:0.005, val_acc:0.989]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.991]
Epoch [66/120    avg_loss:0.019, val_acc:0.983]
Epoch [67/120    avg_loss:0.034, val_acc:0.977]
Epoch [68/120    avg_loss:0.047, val_acc:0.969]
Epoch [69/120    avg_loss:0.050, val_acc:0.957]
Epoch [70/120    avg_loss:0.030, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.985]
Epoch [72/120    avg_loss:0.010, val_acc:0.987]
Epoch [73/120    avg_loss:0.013, val_acc:0.987]
Epoch [74/120    avg_loss:0.010, val_acc:0.986]
Epoch [75/120    avg_loss:0.007, val_acc:0.986]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.005, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.009, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     0     0     0     0     0     2     5]
 [    0     0 18054     0    19     0    16     0     1     0]
 [    0     5     0  2016     2     0     0     0     7     6]
 [    0    27    14     0  2911     0     0     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4870     0     7     0]
 [    0     1     0     0     0     0     2  1286     0     1]
 [    0     1     0     0    53     0     0     0  3506    11]
 [    0     0     0     0    14    27     0     0     0   878]]

Accuracy:
99.41676909358205

F1 scores:
[       nan 0.99681949 0.99861718 0.99481865 0.97504606 0.98976109
 0.9973377  0.9984472  0.98566207 0.96483516]

Kappa:
0.9922731490690087
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f067063b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.666, val_acc:0.396]
Epoch [2/120    avg_loss:1.057, val_acc:0.730]
Epoch [3/120    avg_loss:0.782, val_acc:0.776]
Epoch [4/120    avg_loss:0.580, val_acc:0.747]
Epoch [5/120    avg_loss:0.446, val_acc:0.722]
Epoch [6/120    avg_loss:0.423, val_acc:0.793]
Epoch [7/120    avg_loss:0.367, val_acc:0.818]
Epoch [8/120    avg_loss:0.299, val_acc:0.830]
Epoch [9/120    avg_loss:0.304, val_acc:0.882]
Epoch [10/120    avg_loss:0.233, val_acc:0.907]
Epoch [11/120    avg_loss:0.179, val_acc:0.914]
Epoch [12/120    avg_loss:0.215, val_acc:0.896]
Epoch [13/120    avg_loss:0.227, val_acc:0.951]
Epoch [14/120    avg_loss:0.132, val_acc:0.952]
Epoch [15/120    avg_loss:0.126, val_acc:0.945]
Epoch [16/120    avg_loss:0.129, val_acc:0.905]
Epoch [17/120    avg_loss:0.096, val_acc:0.945]
Epoch [18/120    avg_loss:0.099, val_acc:0.963]
Epoch [19/120    avg_loss:0.100, val_acc:0.944]
Epoch [20/120    avg_loss:0.088, val_acc:0.938]
Epoch [21/120    avg_loss:0.098, val_acc:0.974]
Epoch [22/120    avg_loss:0.053, val_acc:0.969]
Epoch [23/120    avg_loss:0.042, val_acc:0.969]
Epoch [24/120    avg_loss:0.046, val_acc:0.972]
Epoch [25/120    avg_loss:0.043, val_acc:0.975]
Epoch [26/120    avg_loss:0.042, val_acc:0.922]
Epoch [27/120    avg_loss:0.066, val_acc:0.957]
Epoch [28/120    avg_loss:0.060, val_acc:0.966]
Epoch [29/120    avg_loss:0.102, val_acc:0.941]
Epoch [30/120    avg_loss:0.033, val_acc:0.972]
Epoch [31/120    avg_loss:0.032, val_acc:0.956]
Epoch [32/120    avg_loss:0.025, val_acc:0.969]
Epoch [33/120    avg_loss:0.024, val_acc:0.974]
Epoch [34/120    avg_loss:0.018, val_acc:0.977]
Epoch [35/120    avg_loss:0.031, val_acc:0.980]
Epoch [36/120    avg_loss:0.024, val_acc:0.978]
Epoch [37/120    avg_loss:0.026, val_acc:0.982]
Epoch [38/120    avg_loss:0.044, val_acc:0.976]
Epoch [39/120    avg_loss:0.025, val_acc:0.979]
Epoch [40/120    avg_loss:0.032, val_acc:0.968]
Epoch [41/120    avg_loss:0.027, val_acc:0.984]
Epoch [42/120    avg_loss:0.030, val_acc:0.969]
Epoch [43/120    avg_loss:0.020, val_acc:0.973]
Epoch [44/120    avg_loss:0.020, val_acc:0.983]
Epoch [45/120    avg_loss:0.022, val_acc:0.985]
Epoch [46/120    avg_loss:0.024, val_acc:0.985]
Epoch [47/120    avg_loss:0.042, val_acc:0.945]
Epoch [48/120    avg_loss:0.057, val_acc:0.969]
Epoch [49/120    avg_loss:0.064, val_acc:0.976]
Epoch [50/120    avg_loss:0.041, val_acc:0.955]
Epoch [51/120    avg_loss:0.036, val_acc:0.977]
Epoch [52/120    avg_loss:0.060, val_acc:0.979]
Epoch [53/120    avg_loss:0.020, val_acc:0.977]
Epoch [54/120    avg_loss:0.029, val_acc:0.982]
Epoch [55/120    avg_loss:0.014, val_acc:0.969]
Epoch [56/120    avg_loss:0.019, val_acc:0.975]
Epoch [57/120    avg_loss:0.036, val_acc:0.983]
Epoch [58/120    avg_loss:0.018, val_acc:0.975]
Epoch [59/120    avg_loss:0.013, val_acc:0.980]
Epoch [60/120    avg_loss:0.014, val_acc:0.983]
Epoch [61/120    avg_loss:0.013, val_acc:0.986]
Epoch [62/120    avg_loss:0.008, val_acc:0.987]
Epoch [63/120    avg_loss:0.007, val_acc:0.987]
Epoch [64/120    avg_loss:0.007, val_acc:0.988]
Epoch [65/120    avg_loss:0.007, val_acc:0.987]
Epoch [66/120    avg_loss:0.007, val_acc:0.988]
Epoch [67/120    avg_loss:0.007, val_acc:0.988]
Epoch [68/120    avg_loss:0.007, val_acc:0.987]
Epoch [69/120    avg_loss:0.006, val_acc:0.990]
Epoch [70/120    avg_loss:0.014, val_acc:0.990]
Epoch [71/120    avg_loss:0.009, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.991]
Epoch [74/120    avg_loss:0.007, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.990]
Epoch [76/120    avg_loss:0.009, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.006, val_acc:0.989]
Epoch [79/120    avg_loss:0.009, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.009, val_acc:0.990]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.009, val_acc:0.989]
Epoch [91/120    avg_loss:0.006, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     3     0     0     5    31     0]
 [    0     3 18042     0    28     0     8     0     9     0]
 [    0     2     0  1984     0     0     0     0    42     8]
 [    0    19    11     1  2920     0     0     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4850     0    13     0]
 [    0     2     0     0     0     0     0  1284     0     4]
 [    0     0     0    43    43     0     0     0  3472    13]
 [    0     0     0     0     6    22     0     0     0   891]]

Accuracy:
99.15166413611935

F1 scores:
[       nan 0.99494203 0.99795343 0.97637795 0.97789685 0.99164134
 0.99630238 0.99573478 0.96996787 0.97111717]

Kappa:
0.9887627291063884
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f550aabb828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.620, val_acc:0.614]
Epoch [2/120    avg_loss:1.038, val_acc:0.683]
Epoch [3/120    avg_loss:0.825, val_acc:0.686]
Epoch [4/120    avg_loss:0.656, val_acc:0.694]
Epoch [5/120    avg_loss:0.532, val_acc:0.729]
Epoch [6/120    avg_loss:0.436, val_acc:0.744]
Epoch [7/120    avg_loss:0.350, val_acc:0.761]
Epoch [8/120    avg_loss:0.294, val_acc:0.810]
Epoch [9/120    avg_loss:0.248, val_acc:0.848]
Epoch [10/120    avg_loss:0.231, val_acc:0.897]
Epoch [11/120    avg_loss:0.192, val_acc:0.934]
Epoch [12/120    avg_loss:0.174, val_acc:0.927]
Epoch [13/120    avg_loss:0.158, val_acc:0.958]
Epoch [14/120    avg_loss:0.096, val_acc:0.960]
Epoch [15/120    avg_loss:0.086, val_acc:0.958]
Epoch [16/120    avg_loss:0.121, val_acc:0.941]
Epoch [17/120    avg_loss:0.097, val_acc:0.954]
Epoch [18/120    avg_loss:0.061, val_acc:0.959]
Epoch [19/120    avg_loss:0.097, val_acc:0.961]
Epoch [20/120    avg_loss:0.070, val_acc:0.967]
Epoch [21/120    avg_loss:0.087, val_acc:0.964]
Epoch [22/120    avg_loss:0.053, val_acc:0.970]
Epoch [23/120    avg_loss:0.060, val_acc:0.970]
Epoch [24/120    avg_loss:0.050, val_acc:0.977]
Epoch [25/120    avg_loss:0.056, val_acc:0.978]
Epoch [26/120    avg_loss:0.059, val_acc:0.976]
Epoch [27/120    avg_loss:0.038, val_acc:0.976]
Epoch [28/120    avg_loss:0.036, val_acc:0.977]
Epoch [29/120    avg_loss:0.077, val_acc:0.956]
Epoch [30/120    avg_loss:0.056, val_acc:0.955]
Epoch [31/120    avg_loss:0.083, val_acc:0.964]
Epoch [32/120    avg_loss:0.034, val_acc:0.974]
Epoch [33/120    avg_loss:0.047, val_acc:0.970]
Epoch [34/120    avg_loss:0.035, val_acc:0.976]
Epoch [35/120    avg_loss:0.043, val_acc:0.984]
Epoch [36/120    avg_loss:0.057, val_acc:0.927]
Epoch [37/120    avg_loss:0.038, val_acc:0.964]
Epoch [38/120    avg_loss:0.025, val_acc:0.988]
Epoch [39/120    avg_loss:0.016, val_acc:0.987]
Epoch [40/120    avg_loss:0.024, val_acc:0.989]
Epoch [41/120    avg_loss:0.027, val_acc:0.977]
Epoch [42/120    avg_loss:0.024, val_acc:0.986]
Epoch [43/120    avg_loss:0.057, val_acc:0.977]
Epoch [44/120    avg_loss:0.017, val_acc:0.987]
Epoch [45/120    avg_loss:0.021, val_acc:0.984]
Epoch [46/120    avg_loss:0.017, val_acc:0.982]
Epoch [47/120    avg_loss:0.011, val_acc:0.985]
Epoch [48/120    avg_loss:0.009, val_acc:0.990]
Epoch [49/120    avg_loss:0.011, val_acc:0.981]
Epoch [50/120    avg_loss:0.012, val_acc:0.984]
Epoch [51/120    avg_loss:0.013, val_acc:0.989]
Epoch [52/120    avg_loss:0.011, val_acc:0.987]
Epoch [53/120    avg_loss:0.009, val_acc:0.987]
Epoch [54/120    avg_loss:0.015, val_acc:0.988]
Epoch [55/120    avg_loss:0.014, val_acc:0.985]
Epoch [56/120    avg_loss:0.015, val_acc:0.981]
Epoch [57/120    avg_loss:0.044, val_acc:0.984]
Epoch [58/120    avg_loss:0.029, val_acc:0.984]
Epoch [59/120    avg_loss:0.027, val_acc:0.984]
Epoch [60/120    avg_loss:0.020, val_acc:0.985]
Epoch [61/120    avg_loss:0.010, val_acc:0.990]
Epoch [62/120    avg_loss:0.008, val_acc:0.986]
Epoch [63/120    avg_loss:0.008, val_acc:0.988]
Epoch [64/120    avg_loss:0.009, val_acc:0.978]
Epoch [65/120    avg_loss:0.014, val_acc:0.987]
Epoch [66/120    avg_loss:0.015, val_acc:0.983]
Epoch [67/120    avg_loss:0.011, val_acc:0.987]
Epoch [68/120    avg_loss:0.010, val_acc:0.990]
Epoch [69/120    avg_loss:0.007, val_acc:0.984]
Epoch [70/120    avg_loss:0.010, val_acc:0.983]
Epoch [71/120    avg_loss:0.026, val_acc:0.984]
Epoch [72/120    avg_loss:0.013, val_acc:0.984]
Epoch [73/120    avg_loss:0.007, val_acc:0.987]
Epoch [74/120    avg_loss:0.005, val_acc:0.991]
Epoch [75/120    avg_loss:0.005, val_acc:0.991]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.011, val_acc:0.991]
Epoch [78/120    avg_loss:0.005, val_acc:0.990]
Epoch [79/120    avg_loss:0.005, val_acc:0.991]
Epoch [80/120    avg_loss:0.005, val_acc:0.993]
Epoch [81/120    avg_loss:0.006, val_acc:0.994]
Epoch [82/120    avg_loss:0.007, val_acc:0.993]
Epoch [83/120    avg_loss:0.005, val_acc:0.983]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.021, val_acc:0.990]
Epoch [86/120    avg_loss:0.018, val_acc:0.984]
Epoch [87/120    avg_loss:0.012, val_acc:0.987]
Epoch [88/120    avg_loss:0.009, val_acc:0.990]
Epoch [89/120    avg_loss:0.004, val_acc:0.993]
Epoch [90/120    avg_loss:0.005, val_acc:0.991]
Epoch [91/120    avg_loss:0.004, val_acc:0.992]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.990]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.003, val_acc:0.991]
Epoch [100/120    avg_loss:0.003, val_acc:0.992]
Epoch [101/120    avg_loss:0.002, val_acc:0.992]
Epoch [102/120    avg_loss:0.003, val_acc:0.993]
Epoch [103/120    avg_loss:0.003, val_acc:0.993]
Epoch [104/120    avg_loss:0.003, val_acc:0.993]
Epoch [105/120    avg_loss:0.003, val_acc:0.993]
Epoch [106/120    avg_loss:0.003, val_acc:0.993]
Epoch [107/120    avg_loss:0.003, val_acc:0.993]
Epoch [108/120    avg_loss:0.003, val_acc:0.993]
Epoch [109/120    avg_loss:0.003, val_acc:0.993]
Epoch [110/120    avg_loss:0.003, val_acc:0.993]
Epoch [111/120    avg_loss:0.003, val_acc:0.993]
Epoch [112/120    avg_loss:0.002, val_acc:0.993]
Epoch [113/120    avg_loss:0.002, val_acc:0.993]
Epoch [114/120    avg_loss:0.005, val_acc:0.993]
Epoch [115/120    avg_loss:0.003, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.003, val_acc:0.993]
Epoch [118/120    avg_loss:0.003, val_acc:0.993]
Epoch [119/120    avg_loss:0.003, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     0     1     0     0     3    22     0]
 [    0     0 18062     0    16     0     6     0     6     0]
 [    0     0     0  2013     0     0     0     0    20     3]
 [    0    33     4     0  2918     0     0     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     8     0     0  4865     0     5     0]
 [    0     1     0     0     0     0     2  1281     0     6]
 [    0     6     0    20    57     0     0     0  3483     5]
 [    0     0     0     0    15    39     0     0     0   865]]

Accuracy:
99.28903670498639

F1 scores:
[       nan 0.99487498 0.99911495 0.9874908  0.97608296 0.98527746
 0.99784637 0.995338   0.97782145 0.9621802 ]

Kappa:
0.9905815698479903
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf4f70b7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.695, val_acc:0.326]
Epoch [2/120    avg_loss:1.080, val_acc:0.464]
Epoch [3/120    avg_loss:0.797, val_acc:0.764]
Epoch [4/120    avg_loss:0.535, val_acc:0.713]
Epoch [5/120    avg_loss:0.412, val_acc:0.811]
Epoch [6/120    avg_loss:0.323, val_acc:0.882]
Epoch [7/120    avg_loss:0.292, val_acc:0.913]
Epoch [8/120    avg_loss:0.247, val_acc:0.926]
Epoch [9/120    avg_loss:0.187, val_acc:0.933]
Epoch [10/120    avg_loss:0.186, val_acc:0.928]
Epoch [11/120    avg_loss:0.162, val_acc:0.957]
Epoch [12/120    avg_loss:0.140, val_acc:0.932]
Epoch [13/120    avg_loss:0.134, val_acc:0.953]
Epoch [14/120    avg_loss:0.091, val_acc:0.954]
Epoch [15/120    avg_loss:0.078, val_acc:0.967]
Epoch [16/120    avg_loss:0.079, val_acc:0.950]
Epoch [17/120    avg_loss:0.111, val_acc:0.964]
Epoch [18/120    avg_loss:0.073, val_acc:0.973]
Epoch [19/120    avg_loss:0.059, val_acc:0.966]
Epoch [20/120    avg_loss:0.093, val_acc:0.946]
Epoch [21/120    avg_loss:0.085, val_acc:0.966]
Epoch [22/120    avg_loss:0.064, val_acc:0.976]
Epoch [23/120    avg_loss:0.054, val_acc:0.964]
Epoch [24/120    avg_loss:0.037, val_acc:0.975]
Epoch [25/120    avg_loss:0.058, val_acc:0.961]
Epoch [26/120    avg_loss:0.069, val_acc:0.958]
Epoch [27/120    avg_loss:0.059, val_acc:0.974]
Epoch [28/120    avg_loss:0.040, val_acc:0.929]
Epoch [29/120    avg_loss:0.035, val_acc:0.968]
Epoch [30/120    avg_loss:0.023, val_acc:0.978]
Epoch [31/120    avg_loss:0.027, val_acc:0.983]
Epoch [32/120    avg_loss:0.021, val_acc:0.978]
Epoch [33/120    avg_loss:0.021, val_acc:0.985]
Epoch [34/120    avg_loss:0.024, val_acc:0.980]
Epoch [35/120    avg_loss:0.017, val_acc:0.985]
Epoch [36/120    avg_loss:0.062, val_acc:0.888]
Epoch [37/120    avg_loss:0.068, val_acc:0.975]
Epoch [38/120    avg_loss:0.037, val_acc:0.974]
Epoch [39/120    avg_loss:0.020, val_acc:0.974]
Epoch [40/120    avg_loss:0.041, val_acc:0.962]
Epoch [41/120    avg_loss:0.026, val_acc:0.986]
Epoch [42/120    avg_loss:0.023, val_acc:0.968]
Epoch [43/120    avg_loss:0.015, val_acc:0.985]
Epoch [44/120    avg_loss:0.023, val_acc:0.986]
Epoch [45/120    avg_loss:0.015, val_acc:0.986]
Epoch [46/120    avg_loss:0.013, val_acc:0.986]
Epoch [47/120    avg_loss:0.009, val_acc:0.986]
Epoch [48/120    avg_loss:0.011, val_acc:0.986]
Epoch [49/120    avg_loss:0.014, val_acc:0.986]
Epoch [50/120    avg_loss:0.019, val_acc:0.978]
Epoch [51/120    avg_loss:0.010, val_acc:0.987]
Epoch [52/120    avg_loss:0.012, val_acc:0.980]
Epoch [53/120    avg_loss:0.017, val_acc:0.978]
Epoch [54/120    avg_loss:0.012, val_acc:0.986]
Epoch [55/120    avg_loss:0.010, val_acc:0.960]
Epoch [56/120    avg_loss:0.011, val_acc:0.985]
Epoch [57/120    avg_loss:0.014, val_acc:0.985]
Epoch [58/120    avg_loss:0.010, val_acc:0.986]
Epoch [59/120    avg_loss:0.008, val_acc:0.988]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.021, val_acc:0.984]
Epoch [62/120    avg_loss:0.026, val_acc:0.980]
Epoch [63/120    avg_loss:0.040, val_acc:0.974]
Epoch [64/120    avg_loss:0.027, val_acc:0.974]
Epoch [65/120    avg_loss:0.029, val_acc:0.983]
Epoch [66/120    avg_loss:0.013, val_acc:0.976]
Epoch [67/120    avg_loss:0.020, val_acc:0.969]
Epoch [68/120    avg_loss:0.021, val_acc:0.980]
Epoch [69/120    avg_loss:0.021, val_acc:0.976]
Epoch [70/120    avg_loss:0.016, val_acc:0.983]
Epoch [71/120    avg_loss:0.010, val_acc:0.987]
Epoch [72/120    avg_loss:0.030, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.983]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.006, val_acc:0.984]
Epoch [79/120    avg_loss:0.006, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.983]
Epoch [82/120    avg_loss:0.004, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.007, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     2     0     0     1    17    12]
 [    0     7 18061     0    15     0     5     0     2     0]
 [    0     6     0  2016     1     0     0     0     8     5]
 [    0    31     2     0  2920     0     3     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     3     0     0     0     0     2  1283     0     2]
 [    0     2     0    23    54     0     0     0  3486     6]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.3516978767503

F1 scores:
[       nan 0.99371167 0.99914253 0.98944785 0.97691536 0.98863636
 0.99897604 0.996892   0.98197183 0.96206707]

Kappa:
0.9914119793427979
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88c9ebd7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.647, val_acc:0.587]
Epoch [2/120    avg_loss:1.062, val_acc:0.733]
Epoch [3/120    avg_loss:0.746, val_acc:0.655]
Epoch [4/120    avg_loss:0.575, val_acc:0.685]
Epoch [5/120    avg_loss:0.443, val_acc:0.827]
Epoch [6/120    avg_loss:0.352, val_acc:0.842]
Epoch [7/120    avg_loss:0.359, val_acc:0.815]
Epoch [8/120    avg_loss:0.347, val_acc:0.856]
Epoch [9/120    avg_loss:0.253, val_acc:0.905]
Epoch [10/120    avg_loss:0.218, val_acc:0.926]
Epoch [11/120    avg_loss:0.186, val_acc:0.913]
Epoch [12/120    avg_loss:0.158, val_acc:0.902]
Epoch [13/120    avg_loss:0.149, val_acc:0.911]
Epoch [14/120    avg_loss:0.116, val_acc:0.948]
Epoch [15/120    avg_loss:0.126, val_acc:0.923]
Epoch [16/120    avg_loss:0.154, val_acc:0.935]
Epoch [17/120    avg_loss:0.091, val_acc:0.969]
Epoch [18/120    avg_loss:0.142, val_acc:0.957]
Epoch [19/120    avg_loss:0.125, val_acc:0.962]
Epoch [20/120    avg_loss:0.073, val_acc:0.963]
Epoch [21/120    avg_loss:0.048, val_acc:0.875]
Epoch [22/120    avg_loss:0.097, val_acc:0.962]
Epoch [23/120    avg_loss:0.068, val_acc:0.967]
Epoch [24/120    avg_loss:0.051, val_acc:0.935]
Epoch [25/120    avg_loss:0.076, val_acc:0.948]
Epoch [26/120    avg_loss:0.047, val_acc:0.974]
Epoch [27/120    avg_loss:0.039, val_acc:0.979]
Epoch [28/120    avg_loss:0.058, val_acc:0.959]
Epoch [29/120    avg_loss:0.048, val_acc:0.972]
Epoch [30/120    avg_loss:0.044, val_acc:0.977]
Epoch [31/120    avg_loss:0.028, val_acc:0.980]
Epoch [32/120    avg_loss:0.026, val_acc:0.985]
Epoch [33/120    avg_loss:0.028, val_acc:0.974]
Epoch [34/120    avg_loss:0.052, val_acc:0.988]
Epoch [35/120    avg_loss:0.034, val_acc:0.963]
Epoch [36/120    avg_loss:0.030, val_acc:0.985]
Epoch [37/120    avg_loss:0.040, val_acc:0.979]
Epoch [38/120    avg_loss:0.022, val_acc:0.986]
Epoch [39/120    avg_loss:0.026, val_acc:0.985]
Epoch [40/120    avg_loss:0.020, val_acc:0.983]
Epoch [41/120    avg_loss:0.015, val_acc:0.985]
Epoch [42/120    avg_loss:0.013, val_acc:0.984]
Epoch [43/120    avg_loss:0.012, val_acc:0.987]
Epoch [44/120    avg_loss:0.013, val_acc:0.981]
Epoch [45/120    avg_loss:0.014, val_acc:0.990]
Epoch [46/120    avg_loss:0.016, val_acc:0.986]
Epoch [47/120    avg_loss:0.025, val_acc:0.963]
Epoch [48/120    avg_loss:0.059, val_acc:0.959]
Epoch [49/120    avg_loss:0.058, val_acc:0.964]
Epoch [50/120    avg_loss:0.151, val_acc:0.953]
Epoch [51/120    avg_loss:0.057, val_acc:0.980]
Epoch [52/120    avg_loss:0.043, val_acc:0.990]
Epoch [53/120    avg_loss:0.035, val_acc:0.987]
Epoch [54/120    avg_loss:0.025, val_acc:0.984]
Epoch [55/120    avg_loss:0.022, val_acc:0.985]
Epoch [56/120    avg_loss:0.057, val_acc:0.979]
Epoch [57/120    avg_loss:0.025, val_acc:0.985]
Epoch [58/120    avg_loss:0.012, val_acc:0.991]
Epoch [59/120    avg_loss:0.025, val_acc:0.978]
Epoch [60/120    avg_loss:0.022, val_acc:0.987]
Epoch [61/120    avg_loss:0.009, val_acc:0.990]
Epoch [62/120    avg_loss:0.019, val_acc:0.980]
Epoch [63/120    avg_loss:0.014, val_acc:0.989]
Epoch [64/120    avg_loss:0.012, val_acc:0.984]
Epoch [65/120    avg_loss:0.013, val_acc:0.992]
Epoch [66/120    avg_loss:0.007, val_acc:0.990]
Epoch [67/120    avg_loss:0.012, val_acc:0.993]
Epoch [68/120    avg_loss:0.008, val_acc:0.990]
Epoch [69/120    avg_loss:0.010, val_acc:0.990]
Epoch [70/120    avg_loss:0.009, val_acc:0.989]
Epoch [71/120    avg_loss:0.008, val_acc:0.992]
Epoch [72/120    avg_loss:0.006, val_acc:0.987]
Epoch [73/120    avg_loss:0.005, val_acc:0.991]
Epoch [74/120    avg_loss:0.013, val_acc:0.973]
Epoch [75/120    avg_loss:0.013, val_acc:0.991]
Epoch [76/120    avg_loss:0.008, val_acc:0.991]
Epoch [77/120    avg_loss:0.006, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.992]
Epoch [79/120    avg_loss:0.004, val_acc:0.992]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.008, val_acc:0.993]
Epoch [82/120    avg_loss:0.003, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.005, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.004, val_acc:0.992]
Epoch [89/120    avg_loss:0.004, val_acc:0.992]
Epoch [90/120    avg_loss:0.004, val_acc:0.992]
Epoch [91/120    avg_loss:0.006, val_acc:0.992]
Epoch [92/120    avg_loss:0.006, val_acc:0.992]
Epoch [93/120    avg_loss:0.003, val_acc:0.992]
Epoch [94/120    avg_loss:0.003, val_acc:0.992]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.991]
Epoch [98/120    avg_loss:0.009, val_acc:0.992]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.003, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.003, val_acc:0.991]
Epoch [116/120    avg_loss:0.006, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     3     0     0    16     0     0]
 [    0     0 18056     0    16     0    16     0     2     0]
 [    0    10     0  2009     0     0     0     0    14     3]
 [    0    26     4     0  2928     0     0     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4876     0     1     0]
 [    0     0     0     0     0     0     2  1275     0    13]
 [    0     9     0    19    52     0     0     0  3489     2]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.35892801195382

F1 scores:
[       nan 0.99503491 0.99894882 0.98843788 0.97844612 0.98901099
 0.99795334 0.98798915 0.98406431 0.96635411]

Kappa:
0.9915077342956479
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e86f2a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.691, val_acc:0.368]
Epoch [2/120    avg_loss:1.043, val_acc:0.582]
Epoch [3/120    avg_loss:0.745, val_acc:0.693]
Epoch [4/120    avg_loss:0.558, val_acc:0.803]
Epoch [5/120    avg_loss:0.471, val_acc:0.808]
Epoch [6/120    avg_loss:0.385, val_acc:0.781]
Epoch [7/120    avg_loss:0.337, val_acc:0.890]
Epoch [8/120    avg_loss:0.296, val_acc:0.914]
Epoch [9/120    avg_loss:0.232, val_acc:0.956]
Epoch [10/120    avg_loss:0.171, val_acc:0.957]
Epoch [11/120    avg_loss:0.157, val_acc:0.937]
Epoch [12/120    avg_loss:0.153, val_acc:0.940]
Epoch [13/120    avg_loss:0.161, val_acc:0.918]
Epoch [14/120    avg_loss:0.108, val_acc:0.963]
Epoch [15/120    avg_loss:0.090, val_acc:0.961]
Epoch [16/120    avg_loss:0.120, val_acc:0.967]
Epoch [17/120    avg_loss:0.097, val_acc:0.969]
Epoch [18/120    avg_loss:0.063, val_acc:0.973]
Epoch [19/120    avg_loss:0.071, val_acc:0.976]
Epoch [20/120    avg_loss:0.053, val_acc:0.985]
Epoch [21/120    avg_loss:0.055, val_acc:0.974]
Epoch [22/120    avg_loss:0.049, val_acc:0.980]
Epoch [23/120    avg_loss:0.041, val_acc:0.986]
Epoch [24/120    avg_loss:0.038, val_acc:0.978]
Epoch [25/120    avg_loss:0.038, val_acc:0.980]
Epoch [26/120    avg_loss:0.048, val_acc:0.971]
Epoch [27/120    avg_loss:0.047, val_acc:0.986]
Epoch [28/120    avg_loss:0.066, val_acc:0.980]
Epoch [29/120    avg_loss:0.049, val_acc:0.984]
Epoch [30/120    avg_loss:0.038, val_acc:0.961]
Epoch [31/120    avg_loss:0.031, val_acc:0.986]
Epoch [32/120    avg_loss:0.041, val_acc:0.990]
Epoch [33/120    avg_loss:0.033, val_acc:0.979]
Epoch [34/120    avg_loss:0.020, val_acc:0.987]
Epoch [35/120    avg_loss:0.017, val_acc:0.987]
Epoch [36/120    avg_loss:0.021, val_acc:0.988]
Epoch [37/120    avg_loss:0.027, val_acc:0.979]
Epoch [38/120    avg_loss:0.023, val_acc:0.989]
Epoch [39/120    avg_loss:0.017, val_acc:0.991]
Epoch [40/120    avg_loss:0.010, val_acc:0.991]
Epoch [41/120    avg_loss:0.011, val_acc:0.993]
Epoch [42/120    avg_loss:0.008, val_acc:0.989]
Epoch [43/120    avg_loss:0.020, val_acc:0.981]
Epoch [44/120    avg_loss:0.024, val_acc:0.985]
Epoch [45/120    avg_loss:0.011, val_acc:0.992]
Epoch [46/120    avg_loss:0.024, val_acc:0.989]
Epoch [47/120    avg_loss:0.024, val_acc:0.991]
Epoch [48/120    avg_loss:0.019, val_acc:0.969]
Epoch [49/120    avg_loss:0.013, val_acc:0.991]
Epoch [50/120    avg_loss:0.017, val_acc:0.983]
Epoch [51/120    avg_loss:0.025, val_acc:0.986]
Epoch [52/120    avg_loss:0.029, val_acc:0.988]
Epoch [53/120    avg_loss:0.015, val_acc:0.976]
Epoch [54/120    avg_loss:0.011, val_acc:0.992]
Epoch [55/120    avg_loss:0.007, val_acc:0.993]
Epoch [56/120    avg_loss:0.007, val_acc:0.994]
Epoch [57/120    avg_loss:0.006, val_acc:0.993]
Epoch [58/120    avg_loss:0.007, val_acc:0.994]
Epoch [59/120    avg_loss:0.010, val_acc:0.990]
Epoch [60/120    avg_loss:0.008, val_acc:0.993]
Epoch [61/120    avg_loss:0.005, val_acc:0.993]
Epoch [62/120    avg_loss:0.005, val_acc:0.994]
Epoch [63/120    avg_loss:0.007, val_acc:0.995]
Epoch [64/120    avg_loss:0.007, val_acc:0.993]
Epoch [65/120    avg_loss:0.007, val_acc:0.993]
Epoch [66/120    avg_loss:0.007, val_acc:0.993]
Epoch [67/120    avg_loss:0.009, val_acc:0.994]
Epoch [68/120    avg_loss:0.006, val_acc:0.993]
Epoch [69/120    avg_loss:0.006, val_acc:0.994]
Epoch [70/120    avg_loss:0.008, val_acc:0.993]
Epoch [71/120    avg_loss:0.005, val_acc:0.994]
Epoch [72/120    avg_loss:0.006, val_acc:0.994]
Epoch [73/120    avg_loss:0.005, val_acc:0.993]
Epoch [74/120    avg_loss:0.005, val_acc:0.993]
Epoch [75/120    avg_loss:0.005, val_acc:0.993]
Epoch [76/120    avg_loss:0.005, val_acc:0.993]
Epoch [77/120    avg_loss:0.008, val_acc:0.993]
Epoch [78/120    avg_loss:0.004, val_acc:0.993]
Epoch [79/120    avg_loss:0.006, val_acc:0.993]
Epoch [80/120    avg_loss:0.005, val_acc:0.993]
Epoch [81/120    avg_loss:0.008, val_acc:0.993]
Epoch [82/120    avg_loss:0.006, val_acc:0.993]
Epoch [83/120    avg_loss:0.007, val_acc:0.993]
Epoch [84/120    avg_loss:0.006, val_acc:0.993]
Epoch [85/120    avg_loss:0.007, val_acc:0.993]
Epoch [86/120    avg_loss:0.006, val_acc:0.993]
Epoch [87/120    avg_loss:0.006, val_acc:0.993]
Epoch [88/120    avg_loss:0.005, val_acc:0.993]
Epoch [89/120    avg_loss:0.008, val_acc:0.993]
Epoch [90/120    avg_loss:0.004, val_acc:0.993]
Epoch [91/120    avg_loss:0.005, val_acc:0.993]
Epoch [92/120    avg_loss:0.005, val_acc:0.993]
Epoch [93/120    avg_loss:0.006, val_acc:0.993]
Epoch [94/120    avg_loss:0.006, val_acc:0.993]
Epoch [95/120    avg_loss:0.006, val_acc:0.993]
Epoch [96/120    avg_loss:0.006, val_acc:0.993]
Epoch [97/120    avg_loss:0.006, val_acc:0.993]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.005, val_acc:0.993]
Epoch [100/120    avg_loss:0.005, val_acc:0.993]
Epoch [101/120    avg_loss:0.005, val_acc:0.993]
Epoch [102/120    avg_loss:0.005, val_acc:0.993]
Epoch [103/120    avg_loss:0.005, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.993]
Epoch [105/120    avg_loss:0.006, val_acc:0.993]
Epoch [106/120    avg_loss:0.006, val_acc:0.993]
Epoch [107/120    avg_loss:0.007, val_acc:0.993]
Epoch [108/120    avg_loss:0.004, val_acc:0.993]
Epoch [109/120    avg_loss:0.005, val_acc:0.993]
Epoch [110/120    avg_loss:0.006, val_acc:0.993]
Epoch [111/120    avg_loss:0.005, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.993]
Epoch [113/120    avg_loss:0.005, val_acc:0.993]
Epoch [114/120    avg_loss:0.006, val_acc:0.993]
Epoch [115/120    avg_loss:0.006, val_acc:0.993]
Epoch [116/120    avg_loss:0.007, val_acc:0.993]
Epoch [117/120    avg_loss:0.008, val_acc:0.993]
Epoch [118/120    avg_loss:0.007, val_acc:0.993]
Epoch [119/120    avg_loss:0.007, val_acc:0.993]
Epoch [120/120    avg_loss:0.005, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     0     4     0    22    10    21     0]
 [    0     0 18053     0    22     0    10     0     5     0]
 [    0     2     0  2017     2     0     0     0    12     3]
 [    0    35     7     0  2914     0     1     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4869     0     7     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     0     0     2    53     0     0     0  3504    12]
 [    0     0     0     0    14    33     0     0     0   872]]

Accuracy:
99.28662665991854

F1 scores:
[       nan 0.99268141 0.99878285 0.99433079 0.97441899 0.98751419
 0.99570552 0.99536321 0.98220042 0.96460177]

Kappa:
0.9905508196623439
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a95bf7780>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.669, val_acc:0.553]
Epoch [2/120    avg_loss:1.118, val_acc:0.610]
Epoch [3/120    avg_loss:0.853, val_acc:0.487]
Epoch [4/120    avg_loss:0.716, val_acc:0.636]
Epoch [5/120    avg_loss:0.465, val_acc:0.762]
Epoch [6/120    avg_loss:0.478, val_acc:0.772]
Epoch [7/120    avg_loss:0.372, val_acc:0.835]
Epoch [8/120    avg_loss:0.298, val_acc:0.833]
Epoch [9/120    avg_loss:0.254, val_acc:0.897]
Epoch [10/120    avg_loss:0.229, val_acc:0.908]
Epoch [11/120    avg_loss:0.186, val_acc:0.920]
Epoch [12/120    avg_loss:0.178, val_acc:0.927]
Epoch [13/120    avg_loss:0.148, val_acc:0.948]
Epoch [14/120    avg_loss:0.141, val_acc:0.931]
Epoch [15/120    avg_loss:0.137, val_acc:0.948]
Epoch [16/120    avg_loss:0.111, val_acc:0.954]
Epoch [17/120    avg_loss:0.081, val_acc:0.955]
Epoch [18/120    avg_loss:0.103, val_acc:0.881]
Epoch [19/120    avg_loss:0.139, val_acc:0.958]
Epoch [20/120    avg_loss:0.073, val_acc:0.978]
Epoch [21/120    avg_loss:0.062, val_acc:0.978]
Epoch [22/120    avg_loss:0.068, val_acc:0.921]
Epoch [23/120    avg_loss:0.061, val_acc:0.952]
Epoch [24/120    avg_loss:0.072, val_acc:0.973]
Epoch [25/120    avg_loss:0.047, val_acc:0.980]
Epoch [26/120    avg_loss:0.034, val_acc:0.983]
Epoch [27/120    avg_loss:0.099, val_acc:0.973]
Epoch [28/120    avg_loss:0.036, val_acc:0.975]
Epoch [29/120    avg_loss:0.040, val_acc:0.962]
Epoch [30/120    avg_loss:0.044, val_acc:0.978]
Epoch [31/120    avg_loss:0.031, val_acc:0.967]
Epoch [32/120    avg_loss:0.033, val_acc:0.987]
Epoch [33/120    avg_loss:0.023, val_acc:0.973]
Epoch [34/120    avg_loss:0.035, val_acc:0.952]
Epoch [35/120    avg_loss:0.052, val_acc:0.968]
Epoch [36/120    avg_loss:0.030, val_acc:0.977]
Epoch [37/120    avg_loss:0.024, val_acc:0.978]
Epoch [38/120    avg_loss:0.020, val_acc:0.988]
Epoch [39/120    avg_loss:0.025, val_acc:0.987]
Epoch [40/120    avg_loss:0.021, val_acc:0.988]
Epoch [41/120    avg_loss:0.015, val_acc:0.969]
Epoch [42/120    avg_loss:0.024, val_acc:0.973]
Epoch [43/120    avg_loss:0.025, val_acc:0.987]
Epoch [44/120    avg_loss:0.021, val_acc:0.977]
Epoch [45/120    avg_loss:0.014, val_acc:0.981]
Epoch [46/120    avg_loss:0.010, val_acc:0.987]
Epoch [47/120    avg_loss:0.011, val_acc:0.982]
Epoch [48/120    avg_loss:0.010, val_acc:0.980]
Epoch [49/120    avg_loss:0.011, val_acc:0.988]
Epoch [50/120    avg_loss:0.008, val_acc:0.988]
Epoch [51/120    avg_loss:0.063, val_acc:0.980]
Epoch [52/120    avg_loss:0.020, val_acc:0.979]
Epoch [53/120    avg_loss:0.025, val_acc:0.984]
Epoch [54/120    avg_loss:0.014, val_acc:0.985]
Epoch [55/120    avg_loss:0.015, val_acc:0.983]
Epoch [56/120    avg_loss:0.010, val_acc:0.983]
Epoch [57/120    avg_loss:0.008, val_acc:0.984]
Epoch [58/120    avg_loss:0.010, val_acc:0.984]
Epoch [59/120    avg_loss:0.009, val_acc:0.988]
Epoch [60/120    avg_loss:0.010, val_acc:0.988]
Epoch [61/120    avg_loss:0.012, val_acc:0.981]
Epoch [62/120    avg_loss:0.010, val_acc:0.983]
Epoch [63/120    avg_loss:0.008, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.988]
Epoch [65/120    avg_loss:0.007, val_acc:0.987]
Epoch [66/120    avg_loss:0.012, val_acc:0.983]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.985]
Epoch [69/120    avg_loss:0.008, val_acc:0.985]
Epoch [70/120    avg_loss:0.008, val_acc:0.985]
Epoch [71/120    avg_loss:0.012, val_acc:0.985]
Epoch [72/120    avg_loss:0.007, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.987]
Epoch [75/120    avg_loss:0.006, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.013, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     1     0     0    22    14     0]
 [    0     0 17974     0    22     0    91     0     3     0]
 [    0     0     0  2013     2     0     0     0    10    11]
 [    0    35    16     0  2905     0     2     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     0     2]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     0     0     3    51     0     0     0  3492    25]
 [    0     0     0     0    14    51     0     0     0   854]]

Accuracy:
99.06008242354132

F1 scores:
[       nan 0.99440211 0.99634146 0.99358342 0.97368862 0.98083427
 0.99035239 0.99115725 0.98310811 0.94260486]

Kappa:
0.9875590852004325
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff3b9f01898>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.697, val_acc:0.319]
Epoch [2/120    avg_loss:1.116, val_acc:0.738]
Epoch [3/120    avg_loss:0.827, val_acc:0.664]
Epoch [4/120    avg_loss:0.585, val_acc:0.686]
Epoch [5/120    avg_loss:0.461, val_acc:0.806]
Epoch [6/120    avg_loss:0.409, val_acc:0.830]
Epoch [7/120    avg_loss:0.342, val_acc:0.864]
Epoch [8/120    avg_loss:0.287, val_acc:0.834]
Epoch [9/120    avg_loss:0.295, val_acc:0.883]
Epoch [10/120    avg_loss:0.226, val_acc:0.898]
Epoch [11/120    avg_loss:0.223, val_acc:0.936]
Epoch [12/120    avg_loss:0.179, val_acc:0.911]
Epoch [13/120    avg_loss:0.194, val_acc:0.931]
Epoch [14/120    avg_loss:0.139, val_acc:0.935]
Epoch [15/120    avg_loss:0.124, val_acc:0.936]
Epoch [16/120    avg_loss:0.116, val_acc:0.894]
Epoch [17/120    avg_loss:0.087, val_acc:0.948]
Epoch [18/120    avg_loss:0.083, val_acc:0.939]
Epoch [19/120    avg_loss:0.072, val_acc:0.957]
Epoch [20/120    avg_loss:0.051, val_acc:0.968]
Epoch [21/120    avg_loss:0.072, val_acc:0.945]
Epoch [22/120    avg_loss:0.055, val_acc:0.964]
Epoch [23/120    avg_loss:0.056, val_acc:0.973]
Epoch [24/120    avg_loss:0.092, val_acc:0.863]
Epoch [25/120    avg_loss:0.162, val_acc:0.898]
Epoch [26/120    avg_loss:0.125, val_acc:0.957]
Epoch [27/120    avg_loss:0.079, val_acc:0.964]
Epoch [28/120    avg_loss:0.055, val_acc:0.973]
Epoch [29/120    avg_loss:0.046, val_acc:0.957]
Epoch [30/120    avg_loss:0.055, val_acc:0.936]
Epoch [31/120    avg_loss:0.050, val_acc:0.950]
Epoch [32/120    avg_loss:0.041, val_acc:0.964]
Epoch [33/120    avg_loss:0.026, val_acc:0.975]
Epoch [34/120    avg_loss:0.026, val_acc:0.972]
Epoch [35/120    avg_loss:0.028, val_acc:0.973]
Epoch [36/120    avg_loss:0.029, val_acc:0.972]
Epoch [37/120    avg_loss:0.038, val_acc:0.968]
Epoch [38/120    avg_loss:0.023, val_acc:0.975]
Epoch [39/120    avg_loss:0.027, val_acc:0.970]
Epoch [40/120    avg_loss:0.036, val_acc:0.978]
Epoch [41/120    avg_loss:0.023, val_acc:0.975]
Epoch [42/120    avg_loss:0.016, val_acc:0.982]
Epoch [43/120    avg_loss:0.038, val_acc:0.965]
Epoch [44/120    avg_loss:0.044, val_acc:0.970]
Epoch [45/120    avg_loss:0.022, val_acc:0.976]
Epoch [46/120    avg_loss:0.024, val_acc:0.982]
Epoch [47/120    avg_loss:0.021, val_acc:0.978]
Epoch [48/120    avg_loss:0.017, val_acc:0.984]
Epoch [49/120    avg_loss:0.014, val_acc:0.985]
Epoch [50/120    avg_loss:0.010, val_acc:0.979]
Epoch [51/120    avg_loss:0.011, val_acc:0.980]
Epoch [52/120    avg_loss:0.017, val_acc:0.986]
Epoch [53/120    avg_loss:0.009, val_acc:0.989]
Epoch [54/120    avg_loss:0.024, val_acc:0.973]
Epoch [55/120    avg_loss:0.013, val_acc:0.985]
Epoch [56/120    avg_loss:0.008, val_acc:0.980]
Epoch [57/120    avg_loss:0.015, val_acc:0.975]
Epoch [58/120    avg_loss:0.009, val_acc:0.979]
Epoch [59/120    avg_loss:0.007, val_acc:0.989]
Epoch [60/120    avg_loss:0.007, val_acc:0.984]
Epoch [61/120    avg_loss:0.020, val_acc:0.984]
Epoch [62/120    avg_loss:0.014, val_acc:0.980]
Epoch [63/120    avg_loss:0.009, val_acc:0.981]
Epoch [64/120    avg_loss:0.006, val_acc:0.986]
Epoch [65/120    avg_loss:0.014, val_acc:0.971]
Epoch [66/120    avg_loss:0.011, val_acc:0.988]
Epoch [67/120    avg_loss:0.031, val_acc:0.971]
Epoch [68/120    avg_loss:0.056, val_acc:0.972]
Epoch [69/120    avg_loss:0.051, val_acc:0.979]
Epoch [70/120    avg_loss:0.036, val_acc:0.970]
Epoch [71/120    avg_loss:0.021, val_acc:0.976]
Epoch [72/120    avg_loss:0.012, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.984]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.987]
Epoch [76/120    avg_loss:0.011, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.008, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.989]
Epoch [82/120    avg_loss:0.005, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.007, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.006, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.989]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6383     0    10     1     0    10    23     5     0]
 [    0     0 18053     0    16     0    13     0     8     0]
 [    0     0     0  2028     2     0     0     0     3     3]
 [    0    20    13     0  2904     0     0     0    28     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    11     0     0  4861     0     6     0]
 [    0     0     0     0     0     0     7  1283     0     0]
 [    0     3     0     1    61     0     0     0  3482    24]
 [    0     0     0     0    15    42     0     0     0   862]]

Accuracy:
99.1998650374762

F1 scores:
[       nan 0.99439165 0.9986171  0.99265786 0.97270139 0.9841629
 0.99518886 0.98844376 0.9804308  0.94986226]

Kappa:
0.9894019224792403
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f901d8007f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.754, val_acc:0.390]
Epoch [2/120    avg_loss:1.141, val_acc:0.619]
Epoch [3/120    avg_loss:0.876, val_acc:0.698]
Epoch [4/120    avg_loss:0.663, val_acc:0.644]
Epoch [5/120    avg_loss:0.527, val_acc:0.690]
Epoch [6/120    avg_loss:0.412, val_acc:0.805]
Epoch [7/120    avg_loss:0.339, val_acc:0.829]
Epoch [8/120    avg_loss:0.307, val_acc:0.890]
Epoch [9/120    avg_loss:0.249, val_acc:0.913]
Epoch [10/120    avg_loss:0.219, val_acc:0.883]
Epoch [11/120    avg_loss:0.166, val_acc:0.929]
Epoch [12/120    avg_loss:0.133, val_acc:0.936]
Epoch [13/120    avg_loss:0.147, val_acc:0.852]
Epoch [14/120    avg_loss:0.135, val_acc:0.846]
Epoch [15/120    avg_loss:0.112, val_acc:0.931]
Epoch [16/120    avg_loss:0.111, val_acc:0.965]
Epoch [17/120    avg_loss:0.068, val_acc:0.927]
Epoch [18/120    avg_loss:0.072, val_acc:0.971]
Epoch [19/120    avg_loss:0.068, val_acc:0.940]
Epoch [20/120    avg_loss:0.073, val_acc:0.944]
Epoch [21/120    avg_loss:0.078, val_acc:0.952]
Epoch [22/120    avg_loss:0.071, val_acc:0.955]
Epoch [23/120    avg_loss:0.047, val_acc:0.967]
Epoch [24/120    avg_loss:0.042, val_acc:0.961]
Epoch [25/120    avg_loss:0.038, val_acc:0.955]
Epoch [26/120    avg_loss:0.044, val_acc:0.973]
Epoch [27/120    avg_loss:0.025, val_acc:0.969]
Epoch [28/120    avg_loss:0.039, val_acc:0.958]
Epoch [29/120    avg_loss:0.054, val_acc:0.938]
Epoch [30/120    avg_loss:0.041, val_acc:0.970]
Epoch [31/120    avg_loss:0.034, val_acc:0.955]
Epoch [32/120    avg_loss:0.054, val_acc:0.955]
Epoch [33/120    avg_loss:0.033, val_acc:0.972]
Epoch [34/120    avg_loss:0.014, val_acc:0.979]
Epoch [35/120    avg_loss:0.021, val_acc:0.977]
Epoch [36/120    avg_loss:0.014, val_acc:0.981]
Epoch [37/120    avg_loss:0.013, val_acc:0.980]
Epoch [38/120    avg_loss:0.010, val_acc:0.979]
Epoch [39/120    avg_loss:0.016, val_acc:0.978]
Epoch [40/120    avg_loss:0.017, val_acc:0.984]
Epoch [41/120    avg_loss:0.010, val_acc:0.982]
Epoch [42/120    avg_loss:0.015, val_acc:0.983]
Epoch [43/120    avg_loss:0.014, val_acc:0.980]
Epoch [44/120    avg_loss:0.008, val_acc:0.987]
Epoch [45/120    avg_loss:0.009, val_acc:0.982]
Epoch [46/120    avg_loss:0.005, val_acc:0.984]
Epoch [47/120    avg_loss:0.009, val_acc:0.981]
Epoch [48/120    avg_loss:0.015, val_acc:0.983]
Epoch [49/120    avg_loss:0.008, val_acc:0.986]
Epoch [50/120    avg_loss:0.008, val_acc:0.986]
Epoch [51/120    avg_loss:0.006, val_acc:0.983]
Epoch [52/120    avg_loss:0.011, val_acc:0.986]
Epoch [53/120    avg_loss:0.006, val_acc:0.984]
Epoch [54/120    avg_loss:0.008, val_acc:0.983]
Epoch [55/120    avg_loss:0.005, val_acc:0.987]
Epoch [56/120    avg_loss:0.005, val_acc:0.988]
Epoch [57/120    avg_loss:0.006, val_acc:0.981]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.008, val_acc:0.984]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.034, val_acc:0.903]
Epoch [62/120    avg_loss:0.060, val_acc:0.966]
Epoch [63/120    avg_loss:0.029, val_acc:0.980]
Epoch [64/120    avg_loss:0.010, val_acc:0.984]
Epoch [65/120    avg_loss:0.016, val_acc:0.981]
Epoch [66/120    avg_loss:0.008, val_acc:0.989]
Epoch [67/120    avg_loss:0.007, val_acc:0.987]
Epoch [68/120    avg_loss:0.010, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.984]
Epoch [70/120    avg_loss:0.010, val_acc:0.987]
Epoch [71/120    avg_loss:0.011, val_acc:0.982]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.979]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.011, val_acc:0.987]
Epoch [77/120    avg_loss:0.006, val_acc:0.987]
Epoch [78/120    avg_loss:0.004, val_acc:0.990]
Epoch [79/120    avg_loss:0.004, val_acc:0.989]
Epoch [80/120    avg_loss:0.003, val_acc:0.985]
Epoch [81/120    avg_loss:0.004, val_acc:0.977]
Epoch [82/120    avg_loss:0.003, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.989]
Epoch [84/120    avg_loss:0.003, val_acc:0.991]
Epoch [85/120    avg_loss:0.003, val_acc:0.990]
Epoch [86/120    avg_loss:0.003, val_acc:0.989]
Epoch [87/120    avg_loss:0.005, val_acc:0.977]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.003, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.990]
Epoch [96/120    avg_loss:0.003, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.990]
Epoch [98/120    avg_loss:0.002, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.002, val_acc:0.990]
Epoch [101/120    avg_loss:0.002, val_acc:0.990]
Epoch [102/120    avg_loss:0.003, val_acc:0.990]
Epoch [103/120    avg_loss:0.003, val_acc:0.990]
Epoch [104/120    avg_loss:0.002, val_acc:0.990]
Epoch [105/120    avg_loss:0.002, val_acc:0.990]
Epoch [106/120    avg_loss:0.002, val_acc:0.990]
Epoch [107/120    avg_loss:0.002, val_acc:0.990]
Epoch [108/120    avg_loss:0.002, val_acc:0.990]
Epoch [109/120    avg_loss:0.002, val_acc:0.990]
Epoch [110/120    avg_loss:0.002, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.002, val_acc:0.990]
Epoch [113/120    avg_loss:0.003, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.002, val_acc:0.990]
Epoch [116/120    avg_loss:0.002, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.002, val_acc:0.990]
Epoch [119/120    avg_loss:0.002, val_acc:0.990]
Epoch [120/120    avg_loss:0.003, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     1     0    22     7     0     2]
 [    0     0 18079     0    10     0     1     0     0     0]
 [    0     0     0  2031     3     0     0     0     0     2]
 [    0    31    18     0  2899     0     0     0    24     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     5     0     0  4868     0     3     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     2     0     0    33     0     0     0  3529     7]
 [    0     0     0     0    18    28     0     0     0   873]]

Accuracy:
99.46738004000674

F1 scores:
[       nan 0.99494753 0.99914339 0.9975442  0.97675202 0.9893859
 0.99641797 0.99651838 0.99031851 0.96838602]

Kappa:
0.9929415082544543
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d6fc39828>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.664, val_acc:0.622]
Epoch [2/120    avg_loss:1.085, val_acc:0.576]
Epoch [3/120    avg_loss:0.803, val_acc:0.617]
Epoch [4/120    avg_loss:0.616, val_acc:0.761]
Epoch [5/120    avg_loss:0.481, val_acc:0.842]
Epoch [6/120    avg_loss:0.392, val_acc:0.795]
Epoch [7/120    avg_loss:0.341, val_acc:0.776]
Epoch [8/120    avg_loss:0.245, val_acc:0.840]
Epoch [9/120    avg_loss:0.216, val_acc:0.930]
Epoch [10/120    avg_loss:0.261, val_acc:0.908]
Epoch [11/120    avg_loss:0.190, val_acc:0.929]
Epoch [12/120    avg_loss:0.172, val_acc:0.929]
Epoch [13/120    avg_loss:0.124, val_acc:0.941]
Epoch [14/120    avg_loss:0.145, val_acc:0.930]
Epoch [15/120    avg_loss:0.091, val_acc:0.956]
Epoch [16/120    avg_loss:0.079, val_acc:0.954]
Epoch [17/120    avg_loss:0.121, val_acc:0.956]
Epoch [18/120    avg_loss:0.089, val_acc:0.954]
Epoch [19/120    avg_loss:0.076, val_acc:0.976]
Epoch [20/120    avg_loss:0.095, val_acc:0.960]
Epoch [21/120    avg_loss:0.064, val_acc:0.935]
Epoch [22/120    avg_loss:0.084, val_acc:0.912]
Epoch [23/120    avg_loss:0.057, val_acc:0.980]
Epoch [24/120    avg_loss:0.048, val_acc:0.966]
Epoch [25/120    avg_loss:0.043, val_acc:0.963]
Epoch [26/120    avg_loss:0.030, val_acc:0.974]
Epoch [27/120    avg_loss:0.039, val_acc:0.980]
Epoch [28/120    avg_loss:0.037, val_acc:0.974]
Epoch [29/120    avg_loss:0.044, val_acc:0.975]
Epoch [30/120    avg_loss:0.027, val_acc:0.988]
Epoch [31/120    avg_loss:0.030, val_acc:0.973]
Epoch [32/120    avg_loss:0.024, val_acc:0.983]
Epoch [33/120    avg_loss:0.030, val_acc:0.981]
Epoch [34/120    avg_loss:0.026, val_acc:0.988]
Epoch [35/120    avg_loss:0.024, val_acc:0.975]
Epoch [36/120    avg_loss:0.036, val_acc:0.989]
Epoch [37/120    avg_loss:0.028, val_acc:0.967]
Epoch [38/120    avg_loss:0.017, val_acc:0.992]
Epoch [39/120    avg_loss:0.018, val_acc:0.984]
Epoch [40/120    avg_loss:0.021, val_acc:0.982]
Epoch [41/120    avg_loss:0.083, val_acc:0.955]
Epoch [42/120    avg_loss:0.080, val_acc:0.925]
Epoch [43/120    avg_loss:0.095, val_acc:0.963]
Epoch [44/120    avg_loss:0.059, val_acc:0.971]
Epoch [45/120    avg_loss:0.036, val_acc:0.977]
Epoch [46/120    avg_loss:0.021, val_acc:0.963]
Epoch [47/120    avg_loss:0.033, val_acc:0.978]
Epoch [48/120    avg_loss:0.024, val_acc:0.987]
Epoch [49/120    avg_loss:0.021, val_acc:0.981]
Epoch [50/120    avg_loss:0.026, val_acc:0.984]
Epoch [51/120    avg_loss:0.017, val_acc:0.991]
Epoch [52/120    avg_loss:0.010, val_acc:0.994]
Epoch [53/120    avg_loss:0.011, val_acc:0.993]
Epoch [54/120    avg_loss:0.008, val_acc:0.993]
Epoch [55/120    avg_loss:0.007, val_acc:0.994]
Epoch [56/120    avg_loss:0.008, val_acc:0.993]
Epoch [57/120    avg_loss:0.009, val_acc:0.994]
Epoch [58/120    avg_loss:0.007, val_acc:0.994]
Epoch [59/120    avg_loss:0.010, val_acc:0.994]
Epoch [60/120    avg_loss:0.009, val_acc:0.994]
Epoch [61/120    avg_loss:0.006, val_acc:0.992]
Epoch [62/120    avg_loss:0.008, val_acc:0.992]
Epoch [63/120    avg_loss:0.007, val_acc:0.993]
Epoch [64/120    avg_loss:0.008, val_acc:0.993]
Epoch [65/120    avg_loss:0.007, val_acc:0.994]
Epoch [66/120    avg_loss:0.007, val_acc:0.994]
Epoch [67/120    avg_loss:0.007, val_acc:0.994]
Epoch [68/120    avg_loss:0.005, val_acc:0.995]
Epoch [69/120    avg_loss:0.006, val_acc:0.994]
Epoch [70/120    avg_loss:0.015, val_acc:0.994]
Epoch [71/120    avg_loss:0.008, val_acc:0.995]
Epoch [72/120    avg_loss:0.007, val_acc:0.995]
Epoch [73/120    avg_loss:0.008, val_acc:0.994]
Epoch [74/120    avg_loss:0.008, val_acc:0.994]
Epoch [75/120    avg_loss:0.008, val_acc:0.994]
Epoch [76/120    avg_loss:0.008, val_acc:0.991]
Epoch [77/120    avg_loss:0.006, val_acc:0.993]
Epoch [78/120    avg_loss:0.006, val_acc:0.993]
Epoch [79/120    avg_loss:0.005, val_acc:0.994]
Epoch [80/120    avg_loss:0.007, val_acc:0.995]
Epoch [81/120    avg_loss:0.005, val_acc:0.995]
Epoch [82/120    avg_loss:0.007, val_acc:0.992]
Epoch [83/120    avg_loss:0.005, val_acc:0.995]
Epoch [84/120    avg_loss:0.006, val_acc:0.995]
Epoch [85/120    avg_loss:0.007, val_acc:0.995]
Epoch [86/120    avg_loss:0.007, val_acc:0.995]
Epoch [87/120    avg_loss:0.005, val_acc:0.995]
Epoch [88/120    avg_loss:0.007, val_acc:0.994]
Epoch [89/120    avg_loss:0.005, val_acc:0.994]
Epoch [90/120    avg_loss:0.006, val_acc:0.995]
Epoch [91/120    avg_loss:0.007, val_acc:0.995]
Epoch [92/120    avg_loss:0.007, val_acc:0.995]
Epoch [93/120    avg_loss:0.006, val_acc:0.995]
Epoch [94/120    avg_loss:0.006, val_acc:0.995]
Epoch [95/120    avg_loss:0.006, val_acc:0.995]
Epoch [96/120    avg_loss:0.006, val_acc:0.995]
Epoch [97/120    avg_loss:0.005, val_acc:0.995]
Epoch [98/120    avg_loss:0.005, val_acc:0.995]
Epoch [99/120    avg_loss:0.008, val_acc:0.995]
Epoch [100/120    avg_loss:0.005, val_acc:0.995]
Epoch [101/120    avg_loss:0.007, val_acc:0.995]
Epoch [102/120    avg_loss:0.005, val_acc:0.995]
Epoch [103/120    avg_loss:0.004, val_acc:0.995]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.006, val_acc:0.995]
Epoch [106/120    avg_loss:0.005, val_acc:0.995]
Epoch [107/120    avg_loss:0.005, val_acc:0.995]
Epoch [108/120    avg_loss:0.007, val_acc:0.995]
Epoch [109/120    avg_loss:0.005, val_acc:0.995]
Epoch [110/120    avg_loss:0.005, val_acc:0.993]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.005, val_acc:0.995]
Epoch [113/120    avg_loss:0.006, val_acc:0.995]
Epoch [114/120    avg_loss:0.004, val_acc:0.995]
Epoch [115/120    avg_loss:0.005, val_acc:0.995]
Epoch [116/120    avg_loss:0.009, val_acc:0.995]
Epoch [117/120    avg_loss:0.005, val_acc:0.995]
Epoch [118/120    avg_loss:0.005, val_acc:0.995]
Epoch [119/120    avg_loss:0.005, val_acc:0.995]
Epoch [120/120    avg_loss:0.004, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     0     0     0     6    22    51     3]
 [    0     0 18062     0    14     0    14     0     0     0]
 [    0     0     0  2014     4     0     0     0    11     7]
 [    0    47    22     0  2869     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     3     0     0  4875     0     0     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     2     0     0    53     0     0     0  3516     0]
 [    0     0     0     0    15    28     0     0     0   876]]

Accuracy:
99.18540476706914

F1 scores:
[       nan 0.98979035 0.99861779 0.99383173 0.96811203 0.9893859
 0.99672869 0.99076923 0.97993311 0.97063712]

Kappa:
0.9892071988795529
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f225603b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.702, val_acc:0.359]
Epoch [2/120    avg_loss:1.148, val_acc:0.431]
Epoch [3/120    avg_loss:0.866, val_acc:0.631]
Epoch [4/120    avg_loss:0.700, val_acc:0.698]
Epoch [5/120    avg_loss:0.549, val_acc:0.796]
Epoch [6/120    avg_loss:0.405, val_acc:0.822]
Epoch [7/120    avg_loss:0.337, val_acc:0.828]
Epoch [8/120    avg_loss:0.301, val_acc:0.853]
Epoch [9/120    avg_loss:0.288, val_acc:0.910]
Epoch [10/120    avg_loss:0.192, val_acc:0.942]
Epoch [11/120    avg_loss:0.181, val_acc:0.918]
Epoch [12/120    avg_loss:0.169, val_acc:0.929]
Epoch [13/120    avg_loss:0.148, val_acc:0.925]
Epoch [14/120    avg_loss:0.120, val_acc:0.951]
Epoch [15/120    avg_loss:0.099, val_acc:0.956]
Epoch [16/120    avg_loss:0.091, val_acc:0.959]
Epoch [17/120    avg_loss:0.126, val_acc:0.888]
Epoch [18/120    avg_loss:0.193, val_acc:0.954]
Epoch [19/120    avg_loss:0.119, val_acc:0.830]
Epoch [20/120    avg_loss:0.142, val_acc:0.940]
Epoch [21/120    avg_loss:0.109, val_acc:0.956]
Epoch [22/120    avg_loss:0.078, val_acc:0.946]
Epoch [23/120    avg_loss:0.055, val_acc:0.970]
Epoch [24/120    avg_loss:0.065, val_acc:0.963]
Epoch [25/120    avg_loss:0.091, val_acc:0.952]
Epoch [26/120    avg_loss:0.047, val_acc:0.980]
Epoch [27/120    avg_loss:0.055, val_acc:0.965]
Epoch [28/120    avg_loss:0.043, val_acc:0.976]
Epoch [29/120    avg_loss:0.048, val_acc:0.970]
Epoch [30/120    avg_loss:0.071, val_acc:0.973]
Epoch [31/120    avg_loss:0.044, val_acc:0.978]
Epoch [32/120    avg_loss:0.032, val_acc:0.984]
Epoch [33/120    avg_loss:0.028, val_acc:0.984]
Epoch [34/120    avg_loss:0.034, val_acc:0.982]
Epoch [35/120    avg_loss:0.114, val_acc:0.971]
Epoch [36/120    avg_loss:0.080, val_acc:0.972]
Epoch [37/120    avg_loss:0.046, val_acc:0.971]
Epoch [38/120    avg_loss:0.039, val_acc:0.974]
Epoch [39/120    avg_loss:0.026, val_acc:0.983]
Epoch [40/120    avg_loss:0.029, val_acc:0.919]
Epoch [41/120    avg_loss:0.032, val_acc:0.985]
Epoch [42/120    avg_loss:0.024, val_acc:0.984]
Epoch [43/120    avg_loss:0.022, val_acc:0.990]
Epoch [44/120    avg_loss:0.031, val_acc:0.975]
Epoch [45/120    avg_loss:0.035, val_acc:0.990]
Epoch [46/120    avg_loss:0.025, val_acc:0.981]
Epoch [47/120    avg_loss:0.015, val_acc:0.985]
Epoch [48/120    avg_loss:0.012, val_acc:0.983]
Epoch [49/120    avg_loss:0.018, val_acc:0.986]
Epoch [50/120    avg_loss:0.032, val_acc:0.988]
Epoch [51/120    avg_loss:0.014, val_acc:0.991]
Epoch [52/120    avg_loss:0.009, val_acc:0.990]
Epoch [53/120    avg_loss:0.011, val_acc:0.982]
Epoch [54/120    avg_loss:0.014, val_acc:0.990]
Epoch [55/120    avg_loss:0.024, val_acc:0.990]
Epoch [56/120    avg_loss:0.013, val_acc:0.977]
Epoch [57/120    avg_loss:0.023, val_acc:0.989]
Epoch [58/120    avg_loss:0.023, val_acc:0.976]
Epoch [59/120    avg_loss:0.021, val_acc:0.981]
Epoch [60/120    avg_loss:0.008, val_acc:0.990]
Epoch [61/120    avg_loss:0.020, val_acc:0.990]
Epoch [62/120    avg_loss:0.040, val_acc:0.978]
Epoch [63/120    avg_loss:0.022, val_acc:0.994]
Epoch [64/120    avg_loss:0.038, val_acc:0.977]
Epoch [65/120    avg_loss:0.023, val_acc:0.991]
Epoch [66/120    avg_loss:0.015, val_acc:0.985]
Epoch [67/120    avg_loss:0.009, val_acc:0.992]
Epoch [68/120    avg_loss:0.013, val_acc:0.989]
Epoch [69/120    avg_loss:0.009, val_acc:0.990]
Epoch [70/120    avg_loss:0.007, val_acc:0.993]
Epoch [71/120    avg_loss:0.008, val_acc:0.991]
Epoch [72/120    avg_loss:0.013, val_acc:0.989]
Epoch [73/120    avg_loss:0.008, val_acc:0.990]
Epoch [74/120    avg_loss:0.006, val_acc:0.996]
Epoch [75/120    avg_loss:0.012, val_acc:0.982]
Epoch [76/120    avg_loss:0.027, val_acc:0.984]
Epoch [77/120    avg_loss:0.012, val_acc:0.991]
Epoch [78/120    avg_loss:0.008, val_acc:0.992]
Epoch [79/120    avg_loss:0.009, val_acc:0.991]
Epoch [80/120    avg_loss:0.018, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.988]
Epoch [82/120    avg_loss:0.016, val_acc:0.993]
Epoch [83/120    avg_loss:0.017, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.993]
Epoch [87/120    avg_loss:0.009, val_acc:0.993]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.004, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.993]
Epoch [92/120    avg_loss:0.004, val_acc:0.993]
Epoch [93/120    avg_loss:0.005, val_acc:0.994]
Epoch [94/120    avg_loss:0.011, val_acc:0.994]
Epoch [95/120    avg_loss:0.005, val_acc:0.994]
Epoch [96/120    avg_loss:0.004, val_acc:0.994]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.005, val_acc:0.994]
Epoch [99/120    avg_loss:0.004, val_acc:0.993]
Epoch [100/120    avg_loss:0.005, val_acc:0.993]
Epoch [101/120    avg_loss:0.003, val_acc:0.994]
Epoch [102/120    avg_loss:0.004, val_acc:0.994]
Epoch [103/120    avg_loss:0.004, val_acc:0.994]
Epoch [104/120    avg_loss:0.004, val_acc:0.994]
Epoch [105/120    avg_loss:0.004, val_acc:0.993]
Epoch [106/120    avg_loss:0.003, val_acc:0.993]
Epoch [107/120    avg_loss:0.004, val_acc:0.993]
Epoch [108/120    avg_loss:0.003, val_acc:0.993]
Epoch [109/120    avg_loss:0.005, val_acc:0.994]
Epoch [110/120    avg_loss:0.003, val_acc:0.994]
Epoch [111/120    avg_loss:0.005, val_acc:0.994]
Epoch [112/120    avg_loss:0.004, val_acc:0.994]
Epoch [113/120    avg_loss:0.003, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.003, val_acc:0.994]
Epoch [116/120    avg_loss:0.005, val_acc:0.994]
Epoch [117/120    avg_loss:0.003, val_acc:0.994]
Epoch [118/120    avg_loss:0.004, val_acc:0.994]
Epoch [119/120    avg_loss:0.003, val_acc:0.994]
Epoch [120/120    avg_loss:0.004, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     0     3     0     0     0    13     0]
 [    0     0 18049     0    34     0     2     0     5     0]
 [    0     6     0  2018     4     0     0     0     2     6]
 [    0    30    16     0  2898     0     0     0    27     1]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     7     0     0  4854     0     2    15]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     3     0     0    47     0     0     0  3498    23]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.31313715566482

F1 scores:
[       nan 0.99573213 0.99842345 0.99384388 0.97052914 0.99050513
 0.99732895 1.         0.98286035 0.95449621]

Kappa:
0.9909014474791104
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2374ce3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.749, val_acc:0.291]
Epoch [2/120    avg_loss:1.202, val_acc:0.444]
Epoch [3/120    avg_loss:0.887, val_acc:0.656]
Epoch [4/120    avg_loss:0.650, val_acc:0.759]
Epoch [5/120    avg_loss:0.539, val_acc:0.679]
Epoch [6/120    avg_loss:0.418, val_acc:0.680]
Epoch [7/120    avg_loss:0.380, val_acc:0.833]
Epoch [8/120    avg_loss:0.274, val_acc:0.883]
Epoch [9/120    avg_loss:0.276, val_acc:0.897]
Epoch [10/120    avg_loss:0.253, val_acc:0.862]
Epoch [11/120    avg_loss:0.184, val_acc:0.907]
Epoch [12/120    avg_loss:0.166, val_acc:0.932]
Epoch [13/120    avg_loss:0.151, val_acc:0.943]
Epoch [14/120    avg_loss:0.181, val_acc:0.934]
Epoch [15/120    avg_loss:0.134, val_acc:0.933]
Epoch [16/120    avg_loss:0.102, val_acc:0.957]
Epoch [17/120    avg_loss:0.102, val_acc:0.954]
Epoch [18/120    avg_loss:0.085, val_acc:0.972]
Epoch [19/120    avg_loss:0.153, val_acc:0.950]
Epoch [20/120    avg_loss:0.224, val_acc:0.956]
Epoch [21/120    avg_loss:0.127, val_acc:0.944]
Epoch [22/120    avg_loss:0.106, val_acc:0.959]
Epoch [23/120    avg_loss:0.093, val_acc:0.974]
Epoch [24/120    avg_loss:0.079, val_acc:0.974]
Epoch [25/120    avg_loss:0.062, val_acc:0.970]
Epoch [26/120    avg_loss:0.049, val_acc:0.979]
Epoch [27/120    avg_loss:0.031, val_acc:0.977]
Epoch [28/120    avg_loss:0.032, val_acc:0.980]
Epoch [29/120    avg_loss:0.042, val_acc:0.982]
Epoch [30/120    avg_loss:0.031, val_acc:0.975]
Epoch [31/120    avg_loss:0.024, val_acc:0.975]
Epoch [32/120    avg_loss:0.018, val_acc:0.984]
Epoch [33/120    avg_loss:0.026, val_acc:0.977]
Epoch [34/120    avg_loss:0.023, val_acc:0.980]
Epoch [35/120    avg_loss:0.026, val_acc:0.970]
Epoch [36/120    avg_loss:0.029, val_acc:0.977]
Epoch [37/120    avg_loss:0.023, val_acc:0.971]
Epoch [38/120    avg_loss:0.021, val_acc:0.986]
Epoch [39/120    avg_loss:0.017, val_acc:0.986]
Epoch [40/120    avg_loss:0.012, val_acc:0.986]
Epoch [41/120    avg_loss:0.014, val_acc:0.982]
Epoch [42/120    avg_loss:0.022, val_acc:0.965]
Epoch [43/120    avg_loss:0.116, val_acc:0.970]
Epoch [44/120    avg_loss:0.031, val_acc:0.975]
Epoch [45/120    avg_loss:0.023, val_acc:0.972]
Epoch [46/120    avg_loss:0.016, val_acc:0.983]
Epoch [47/120    avg_loss:0.015, val_acc:0.982]
Epoch [48/120    avg_loss:0.021, val_acc:0.977]
Epoch [49/120    avg_loss:0.032, val_acc:0.969]
Epoch [50/120    avg_loss:0.017, val_acc:0.982]
Epoch [51/120    avg_loss:0.016, val_acc:0.981]
Epoch [52/120    avg_loss:0.010, val_acc:0.985]
Epoch [53/120    avg_loss:0.012, val_acc:0.985]
Epoch [54/120    avg_loss:0.009, val_acc:0.986]
Epoch [55/120    avg_loss:0.007, val_acc:0.984]
Epoch [56/120    avg_loss:0.007, val_acc:0.985]
Epoch [57/120    avg_loss:0.007, val_acc:0.984]
Epoch [58/120    avg_loss:0.008, val_acc:0.985]
Epoch [59/120    avg_loss:0.007, val_acc:0.985]
Epoch [60/120    avg_loss:0.006, val_acc:0.987]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.007, val_acc:0.985]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.005, val_acc:0.988]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.006, val_acc:0.988]
Epoch [68/120    avg_loss:0.006, val_acc:0.987]
Epoch [69/120    avg_loss:0.006, val_acc:0.987]
Epoch [70/120    avg_loss:0.008, val_acc:0.987]
Epoch [71/120    avg_loss:0.007, val_acc:0.987]
Epoch [72/120    avg_loss:0.006, val_acc:0.987]
Epoch [73/120    avg_loss:0.007, val_acc:0.988]
Epoch [74/120    avg_loss:0.006, val_acc:0.988]
Epoch [75/120    avg_loss:0.010, val_acc:0.987]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.987]
Epoch [79/120    avg_loss:0.006, val_acc:0.988]
Epoch [80/120    avg_loss:0.010, val_acc:0.987]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.007, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.987]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     2     0    18     4     0     1]
 [    0     0 18031     0    58     0     0     0     1     0]
 [    0     5     0  1984     2     0     0     0    41     4]
 [    0    34    21     0  2890     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     0     0     0  4861     0     0     5]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     4     0     0    55     0     0     0  3501    11]
 [    0     0     0     0    14    37     0     0     0   868]]

Accuracy:
99.1372038657123

F1 scores:
[       nan 0.99472132 0.99745533 0.98706468 0.96445853 0.98602191
 0.99620863 0.99767622 0.98053494 0.96017699]

Kappa:
0.988569897183373
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1d91647b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.737, val_acc:0.402]
Epoch [2/120    avg_loss:1.151, val_acc:0.598]
Epoch [3/120    avg_loss:0.892, val_acc:0.532]
Epoch [4/120    avg_loss:0.650, val_acc:0.719]
Epoch [5/120    avg_loss:0.573, val_acc:0.713]
Epoch [6/120    avg_loss:0.436, val_acc:0.768]
Epoch [7/120    avg_loss:0.418, val_acc:0.795]
Epoch [8/120    avg_loss:0.314, val_acc:0.891]
Epoch [9/120    avg_loss:0.282, val_acc:0.853]
Epoch [10/120    avg_loss:0.224, val_acc:0.868]
Epoch [11/120    avg_loss:0.166, val_acc:0.896]
Epoch [12/120    avg_loss:0.178, val_acc:0.931]
Epoch [13/120    avg_loss:0.194, val_acc:0.941]
Epoch [14/120    avg_loss:0.138, val_acc:0.950]
Epoch [15/120    avg_loss:0.133, val_acc:0.932]
Epoch [16/120    avg_loss:0.145, val_acc:0.923]
Epoch [17/120    avg_loss:0.156, val_acc:0.940]
Epoch [18/120    avg_loss:0.086, val_acc:0.944]
Epoch [19/120    avg_loss:0.095, val_acc:0.959]
Epoch [20/120    avg_loss:0.073, val_acc:0.912]
Epoch [21/120    avg_loss:0.071, val_acc:0.958]
Epoch [22/120    avg_loss:0.055, val_acc:0.963]
Epoch [23/120    avg_loss:0.047, val_acc:0.969]
Epoch [24/120    avg_loss:0.050, val_acc:0.963]
Epoch [25/120    avg_loss:0.043, val_acc:0.935]
Epoch [26/120    avg_loss:0.033, val_acc:0.976]
Epoch [27/120    avg_loss:0.027, val_acc:0.977]
Epoch [28/120    avg_loss:0.053, val_acc:0.969]
Epoch [29/120    avg_loss:0.078, val_acc:0.952]
Epoch [30/120    avg_loss:0.030, val_acc:0.977]
Epoch [31/120    avg_loss:0.023, val_acc:0.978]
Epoch [32/120    avg_loss:0.053, val_acc:0.926]
Epoch [33/120    avg_loss:0.053, val_acc:0.966]
Epoch [34/120    avg_loss:0.030, val_acc:0.972]
Epoch [35/120    avg_loss:0.029, val_acc:0.973]
Epoch [36/120    avg_loss:0.041, val_acc:0.950]
Epoch [37/120    avg_loss:0.033, val_acc:0.977]
Epoch [38/120    avg_loss:0.018, val_acc:0.977]
Epoch [39/120    avg_loss:0.036, val_acc:0.980]
Epoch [40/120    avg_loss:0.033, val_acc:0.974]
Epoch [41/120    avg_loss:0.020, val_acc:0.977]
Epoch [42/120    avg_loss:0.020, val_acc:0.976]
Epoch [43/120    avg_loss:0.019, val_acc:0.973]
Epoch [44/120    avg_loss:0.012, val_acc:0.979]
Epoch [45/120    avg_loss:0.013, val_acc:0.975]
Epoch [46/120    avg_loss:0.015, val_acc:0.977]
Epoch [47/120    avg_loss:0.017, val_acc:0.979]
Epoch [48/120    avg_loss:0.013, val_acc:0.965]
Epoch [49/120    avg_loss:0.029, val_acc:0.976]
Epoch [50/120    avg_loss:0.028, val_acc:0.979]
Epoch [51/120    avg_loss:0.008, val_acc:0.981]
Epoch [52/120    avg_loss:0.023, val_acc:0.954]
Epoch [53/120    avg_loss:0.035, val_acc:0.969]
Epoch [54/120    avg_loss:0.028, val_acc:0.974]
Epoch [55/120    avg_loss:0.021, val_acc:0.977]
Epoch [56/120    avg_loss:0.023, val_acc:0.979]
Epoch [57/120    avg_loss:0.017, val_acc:0.983]
Epoch [58/120    avg_loss:0.010, val_acc:0.983]
Epoch [59/120    avg_loss:0.008, val_acc:0.984]
Epoch [60/120    avg_loss:0.005, val_acc:0.983]
Epoch [61/120    avg_loss:0.008, val_acc:0.979]
Epoch [62/120    avg_loss:0.010, val_acc:0.978]
Epoch [63/120    avg_loss:0.009, val_acc:0.982]
Epoch [64/120    avg_loss:0.004, val_acc:0.984]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.005, val_acc:0.983]
Epoch [67/120    avg_loss:0.005, val_acc:0.981]
Epoch [68/120    avg_loss:0.008, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.973]
Epoch [70/120    avg_loss:0.006, val_acc:0.980]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.005, val_acc:0.986]
Epoch [73/120    avg_loss:0.013, val_acc:0.977]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.005, val_acc:0.986]
Epoch [76/120    avg_loss:0.004, val_acc:0.984]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.004, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.004, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.004, val_acc:0.987]
Epoch [85/120    avg_loss:0.003, val_acc:0.986]
Epoch [86/120    avg_loss:0.004, val_acc:0.984]
Epoch [87/120    avg_loss:0.002, val_acc:0.986]
Epoch [88/120    avg_loss:0.004, val_acc:0.986]
Epoch [89/120    avg_loss:0.003, val_acc:0.982]
Epoch [90/120    avg_loss:0.003, val_acc:0.984]
Epoch [91/120    avg_loss:0.004, val_acc:0.985]
Epoch [92/120    avg_loss:0.003, val_acc:0.983]
Epoch [93/120    avg_loss:0.003, val_acc:0.984]
Epoch [94/120    avg_loss:0.003, val_acc:0.984]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.986]
Epoch [100/120    avg_loss:0.004, val_acc:0.986]
Epoch [101/120    avg_loss:0.003, val_acc:0.986]
Epoch [102/120    avg_loss:0.003, val_acc:0.986]
Epoch [103/120    avg_loss:0.003, val_acc:0.986]
Epoch [104/120    avg_loss:0.002, val_acc:0.986]
Epoch [105/120    avg_loss:0.002, val_acc:0.986]
Epoch [106/120    avg_loss:0.002, val_acc:0.986]
Epoch [107/120    avg_loss:0.003, val_acc:0.986]
Epoch [108/120    avg_loss:0.002, val_acc:0.986]
Epoch [109/120    avg_loss:0.002, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.002, val_acc:0.986]
Epoch [112/120    avg_loss:0.002, val_acc:0.986]
Epoch [113/120    avg_loss:0.002, val_acc:0.986]
Epoch [114/120    avg_loss:0.003, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.003, val_acc:0.986]
Epoch [117/120    avg_loss:0.003, val_acc:0.986]
Epoch [118/120    avg_loss:0.002, val_acc:0.986]
Epoch [119/120    avg_loss:0.002, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     0     0    18     0    13     0]
 [    0     0 18057     0    29     0     1     0     3     0]
 [    0     1     0  2015     0     0     0     0    19     1]
 [    0    46    13     0  2884     0     3     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4851     0    20     7]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     3     0    12    44     0     0     0  3506     6]
 [    0     0     0     0    14    45     0     0     0   860]]

Accuracy:
99.20950521774758

F1 scores:
[       nan 0.99371264 0.99872788 0.99187792 0.97055359 0.98305085
 0.99477084 0.9984472  0.97960324 0.95821727]

Kappa:
0.9895271136734999
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff12d8817b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.667, val_acc:0.630]
Epoch [2/120    avg_loss:1.069, val_acc:0.703]
Epoch [3/120    avg_loss:0.893, val_acc:0.726]
Epoch [4/120    avg_loss:0.697, val_acc:0.793]
Epoch [5/120    avg_loss:0.527, val_acc:0.770]
Epoch [6/120    avg_loss:0.405, val_acc:0.757]
Epoch [7/120    avg_loss:0.352, val_acc:0.793]
Epoch [8/120    avg_loss:0.313, val_acc:0.808]
Epoch [9/120    avg_loss:0.290, val_acc:0.832]
Epoch [10/120    avg_loss:0.279, val_acc:0.867]
Epoch [11/120    avg_loss:0.245, val_acc:0.906]
Epoch [12/120    avg_loss:0.230, val_acc:0.940]
Epoch [13/120    avg_loss:0.153, val_acc:0.923]
Epoch [14/120    avg_loss:0.182, val_acc:0.898]
Epoch [15/120    avg_loss:0.109, val_acc:0.929]
Epoch [16/120    avg_loss:0.107, val_acc:0.955]
Epoch [17/120    avg_loss:0.087, val_acc:0.952]
Epoch [18/120    avg_loss:0.096, val_acc:0.971]
Epoch [19/120    avg_loss:0.068, val_acc:0.961]
Epoch [20/120    avg_loss:0.079, val_acc:0.952]
Epoch [21/120    avg_loss:0.073, val_acc:0.973]
Epoch [22/120    avg_loss:0.069, val_acc:0.949]
Epoch [23/120    avg_loss:0.112, val_acc:0.930]
Epoch [24/120    avg_loss:0.082, val_acc:0.968]
Epoch [25/120    avg_loss:0.056, val_acc:0.968]
Epoch [26/120    avg_loss:0.086, val_acc:0.931]
Epoch [27/120    avg_loss:0.077, val_acc:0.930]
Epoch [28/120    avg_loss:0.051, val_acc:0.973]
Epoch [29/120    avg_loss:0.047, val_acc:0.970]
Epoch [30/120    avg_loss:0.029, val_acc:0.978]
Epoch [31/120    avg_loss:0.032, val_acc:0.973]
Epoch [32/120    avg_loss:0.074, val_acc:0.974]
Epoch [33/120    avg_loss:0.027, val_acc:0.977]
Epoch [34/120    avg_loss:0.031, val_acc:0.963]
Epoch [35/120    avg_loss:0.022, val_acc:0.986]
Epoch [36/120    avg_loss:0.028, val_acc:0.976]
Epoch [37/120    avg_loss:0.041, val_acc:0.974]
Epoch [38/120    avg_loss:0.038, val_acc:0.969]
Epoch [39/120    avg_loss:0.026, val_acc:0.982]
Epoch [40/120    avg_loss:0.022, val_acc:0.980]
Epoch [41/120    avg_loss:0.021, val_acc:0.986]
Epoch [42/120    avg_loss:0.021, val_acc:0.983]
Epoch [43/120    avg_loss:0.017, val_acc:0.986]
Epoch [44/120    avg_loss:0.012, val_acc:0.986]
Epoch [45/120    avg_loss:0.012, val_acc:0.984]
Epoch [46/120    avg_loss:0.012, val_acc:0.986]
Epoch [47/120    avg_loss:0.010, val_acc:0.986]
Epoch [48/120    avg_loss:0.012, val_acc:0.986]
Epoch [49/120    avg_loss:0.012, val_acc:0.985]
Epoch [50/120    avg_loss:0.009, val_acc:0.982]
Epoch [51/120    avg_loss:0.009, val_acc:0.988]
Epoch [52/120    avg_loss:0.011, val_acc:0.988]
Epoch [53/120    avg_loss:0.029, val_acc:0.979]
Epoch [54/120    avg_loss:0.025, val_acc:0.981]
Epoch [55/120    avg_loss:0.045, val_acc:0.953]
Epoch [56/120    avg_loss:0.046, val_acc:0.978]
Epoch [57/120    avg_loss:0.033, val_acc:0.971]
Epoch [58/120    avg_loss:0.014, val_acc:0.986]
Epoch [59/120    avg_loss:0.015, val_acc:0.986]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.013, val_acc:0.986]
Epoch [62/120    avg_loss:0.010, val_acc:0.984]
Epoch [63/120    avg_loss:0.012, val_acc:0.986]
Epoch [64/120    avg_loss:0.009, val_acc:0.983]
Epoch [65/120    avg_loss:0.010, val_acc:0.986]
Epoch [66/120    avg_loss:0.007, val_acc:0.985]
Epoch [67/120    avg_loss:0.005, val_acc:0.987]
Epoch [68/120    avg_loss:0.005, val_acc:0.988]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.006, val_acc:0.989]
Epoch [71/120    avg_loss:0.006, val_acc:0.989]
Epoch [72/120    avg_loss:0.005, val_acc:0.989]
Epoch [73/120    avg_loss:0.006, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.987]
Epoch [75/120    avg_loss:0.004, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.986]
Epoch [77/120    avg_loss:0.005, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.989]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.004, val_acc:0.987]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.989]
Epoch [88/120    avg_loss:0.004, val_acc:0.989]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.004, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.989]
Epoch [103/120    avg_loss:0.003, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     0     0     0     2    24    34     0]
 [    0     8 18061     0    13     0     4     0     4     0]
 [    0     0     0  2033     0     0     0     0     0     3]
 [    0    35     8     0  2902     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4863     0    13     2]
 [    0     0     0     0     0     0     6  1283     0     1]
 [    0     2     0     6    56     0     0     0  3496    11]
 [    0     0     0     1    16    27     0     0     0   875]]

Accuracy:
99.26975634444365

F1 scores:
[       nan 0.99182816 0.99897674 0.99754661 0.97398892 0.98976109
 0.99723162 0.98806315 0.97858642 0.96631695]

Kappa:
0.9903267727924809
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa304d087f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.751, val_acc:0.395]
Epoch [2/120    avg_loss:1.096, val_acc:0.682]
Epoch [3/120    avg_loss:0.807, val_acc:0.748]
Epoch [4/120    avg_loss:0.626, val_acc:0.781]
Epoch [5/120    avg_loss:0.475, val_acc:0.793]
Epoch [6/120    avg_loss:0.346, val_acc:0.799]
Epoch [7/120    avg_loss:0.316, val_acc:0.913]
Epoch [8/120    avg_loss:0.339, val_acc:0.890]
Epoch [9/120    avg_loss:0.226, val_acc:0.917]
Epoch [10/120    avg_loss:0.262, val_acc:0.866]
Epoch [11/120    avg_loss:0.201, val_acc:0.923]
Epoch [12/120    avg_loss:0.169, val_acc:0.915]
Epoch [13/120    avg_loss:0.173, val_acc:0.937]
Epoch [14/120    avg_loss:0.142, val_acc:0.928]
Epoch [15/120    avg_loss:0.104, val_acc:0.930]
Epoch [16/120    avg_loss:0.093, val_acc:0.947]
Epoch [17/120    avg_loss:0.086, val_acc:0.940]
Epoch [18/120    avg_loss:0.071, val_acc:0.959]
Epoch [19/120    avg_loss:0.082, val_acc:0.969]
Epoch [20/120    avg_loss:0.070, val_acc:0.941]
Epoch [21/120    avg_loss:0.075, val_acc:0.968]
Epoch [22/120    avg_loss:0.068, val_acc:0.963]
Epoch [23/120    avg_loss:0.078, val_acc:0.971]
Epoch [24/120    avg_loss:0.066, val_acc:0.972]
Epoch [25/120    avg_loss:0.043, val_acc:0.976]
Epoch [26/120    avg_loss:0.040, val_acc:0.969]
Epoch [27/120    avg_loss:0.035, val_acc:0.962]
Epoch [28/120    avg_loss:0.067, val_acc:0.971]
Epoch [29/120    avg_loss:0.048, val_acc:0.965]
Epoch [30/120    avg_loss:0.051, val_acc:0.973]
Epoch [31/120    avg_loss:0.036, val_acc:0.965]
Epoch [32/120    avg_loss:0.034, val_acc:0.945]
Epoch [33/120    avg_loss:0.030, val_acc:0.977]
Epoch [34/120    avg_loss:0.031, val_acc:0.976]
Epoch [35/120    avg_loss:0.030, val_acc:0.963]
Epoch [36/120    avg_loss:0.030, val_acc:0.951]
Epoch [37/120    avg_loss:0.025, val_acc:0.974]
Epoch [38/120    avg_loss:0.018, val_acc:0.976]
Epoch [39/120    avg_loss:0.014, val_acc:0.980]
Epoch [40/120    avg_loss:0.020, val_acc:0.974]
Epoch [41/120    avg_loss:0.021, val_acc:0.977]
Epoch [42/120    avg_loss:0.019, val_acc:0.984]
Epoch [43/120    avg_loss:0.015, val_acc:0.986]
Epoch [44/120    avg_loss:0.029, val_acc:0.974]
Epoch [45/120    avg_loss:0.015, val_acc:0.986]
Epoch [46/120    avg_loss:0.019, val_acc:0.984]
Epoch [47/120    avg_loss:0.015, val_acc:0.983]
Epoch [48/120    avg_loss:0.008, val_acc:0.985]
Epoch [49/120    avg_loss:0.012, val_acc:0.986]
Epoch [50/120    avg_loss:0.018, val_acc:0.985]
Epoch [51/120    avg_loss:0.014, val_acc:0.986]
Epoch [52/120    avg_loss:0.016, val_acc:0.974]
Epoch [53/120    avg_loss:0.013, val_acc:0.987]
Epoch [54/120    avg_loss:0.008, val_acc:0.987]
Epoch [55/120    avg_loss:0.011, val_acc:0.986]
Epoch [56/120    avg_loss:0.023, val_acc:0.979]
Epoch [57/120    avg_loss:0.040, val_acc:0.982]
Epoch [58/120    avg_loss:0.030, val_acc:0.978]
Epoch [59/120    avg_loss:0.030, val_acc:0.974]
Epoch [60/120    avg_loss:0.086, val_acc:0.958]
Epoch [61/120    avg_loss:0.039, val_acc:0.976]
Epoch [62/120    avg_loss:0.017, val_acc:0.980]
Epoch [63/120    avg_loss:0.021, val_acc:0.970]
Epoch [64/120    avg_loss:0.046, val_acc:0.969]
Epoch [65/120    avg_loss:0.208, val_acc:0.936]
Epoch [66/120    avg_loss:0.126, val_acc:0.924]
Epoch [67/120    avg_loss:0.071, val_acc:0.967]
Epoch [68/120    avg_loss:0.056, val_acc:0.972]
Epoch [69/120    avg_loss:0.036, val_acc:0.976]
Epoch [70/120    avg_loss:0.035, val_acc:0.976]
Epoch [71/120    avg_loss:0.032, val_acc:0.980]
Epoch [72/120    avg_loss:0.026, val_acc:0.980]
Epoch [73/120    avg_loss:0.019, val_acc:0.984]
Epoch [74/120    avg_loss:0.027, val_acc:0.984]
Epoch [75/120    avg_loss:0.024, val_acc:0.981]
Epoch [76/120    avg_loss:0.022, val_acc:0.985]
Epoch [77/120    avg_loss:0.021, val_acc:0.987]
Epoch [78/120    avg_loss:0.019, val_acc:0.986]
Epoch [79/120    avg_loss:0.022, val_acc:0.986]
Epoch [80/120    avg_loss:0.017, val_acc:0.983]
Epoch [81/120    avg_loss:0.021, val_acc:0.986]
Epoch [82/120    avg_loss:0.015, val_acc:0.984]
Epoch [83/120    avg_loss:0.019, val_acc:0.985]
Epoch [84/120    avg_loss:0.023, val_acc:0.986]
Epoch [85/120    avg_loss:0.021, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.986]
Epoch [87/120    avg_loss:0.016, val_acc:0.986]
Epoch [88/120    avg_loss:0.014, val_acc:0.986]
Epoch [89/120    avg_loss:0.021, val_acc:0.987]
Epoch [90/120    avg_loss:0.016, val_acc:0.988]
Epoch [91/120    avg_loss:0.020, val_acc:0.988]
Epoch [92/120    avg_loss:0.019, val_acc:0.988]
Epoch [93/120    avg_loss:0.017, val_acc:0.986]
Epoch [94/120    avg_loss:0.016, val_acc:0.988]
Epoch [95/120    avg_loss:0.018, val_acc:0.986]
Epoch [96/120    avg_loss:0.014, val_acc:0.987]
Epoch [97/120    avg_loss:0.010, val_acc:0.987]
Epoch [98/120    avg_loss:0.013, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.013, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.015, val_acc:0.989]
Epoch [107/120    avg_loss:0.011, val_acc:0.987]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.013, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.010, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     0     0     0    51     8    23     0]
 [    0     8 18050     0    26     0     0     0     6     0]
 [    0     0     0  2011     1     0     0     0    19     5]
 [    0    25    24     1  2893     0     0     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4869     0     0     8]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0     2     0     0    51     0     0     0  3508    10]
 [    0     0     0     0    16    54     0     0     0   849]]

Accuracy:
99.09623309955896

F1 scores:
[       nan 0.9908715  0.99820268 0.99357708 0.97096828 0.97972973
 0.99367347 0.9941883  0.980436   0.9454343 ]

Kappa:
0.9880271241740565
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3930cea828>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.818, val_acc:0.337]
Epoch [2/120    avg_loss:1.158, val_acc:0.672]
Epoch [3/120    avg_loss:0.923, val_acc:0.670]
Epoch [4/120    avg_loss:0.745, val_acc:0.710]
Epoch [5/120    avg_loss:0.619, val_acc:0.778]
Epoch [6/120    avg_loss:0.539, val_acc:0.815]
Epoch [7/120    avg_loss:0.428, val_acc:0.840]
Epoch [8/120    avg_loss:0.379, val_acc:0.880]
Epoch [9/120    avg_loss:0.296, val_acc:0.899]
Epoch [10/120    avg_loss:0.227, val_acc:0.883]
Epoch [11/120    avg_loss:0.179, val_acc:0.927]
Epoch [12/120    avg_loss:0.182, val_acc:0.931]
Epoch [13/120    avg_loss:0.138, val_acc:0.940]
Epoch [14/120    avg_loss:0.122, val_acc:0.964]
Epoch [15/120    avg_loss:0.141, val_acc:0.909]
Epoch [16/120    avg_loss:0.143, val_acc:0.926]
Epoch [17/120    avg_loss:0.099, val_acc:0.963]
Epoch [18/120    avg_loss:0.107, val_acc:0.965]
Epoch [19/120    avg_loss:0.144, val_acc:0.927]
Epoch [20/120    avg_loss:0.073, val_acc:0.969]
Epoch [21/120    avg_loss:0.074, val_acc:0.976]
Epoch [22/120    avg_loss:0.055, val_acc:0.968]
Epoch [23/120    avg_loss:0.081, val_acc:0.938]
Epoch [24/120    avg_loss:0.066, val_acc:0.971]
Epoch [25/120    avg_loss:0.055, val_acc:0.976]
Epoch [26/120    avg_loss:0.028, val_acc:0.977]
Epoch [27/120    avg_loss:0.032, val_acc:0.983]
Epoch [28/120    avg_loss:0.040, val_acc:0.980]
Epoch [29/120    avg_loss:0.078, val_acc:0.957]
Epoch [30/120    avg_loss:0.039, val_acc:0.975]
Epoch [31/120    avg_loss:0.027, val_acc:0.961]
Epoch [32/120    avg_loss:0.023, val_acc:0.978]
Epoch [33/120    avg_loss:0.044, val_acc:0.971]
Epoch [34/120    avg_loss:0.023, val_acc:0.981]
Epoch [35/120    avg_loss:0.020, val_acc:0.980]
Epoch [36/120    avg_loss:0.019, val_acc:0.981]
Epoch [37/120    avg_loss:0.040, val_acc:0.964]
Epoch [38/120    avg_loss:0.032, val_acc:0.985]
Epoch [39/120    avg_loss:0.016, val_acc:0.982]
Epoch [40/120    avg_loss:0.019, val_acc:0.984]
Epoch [41/120    avg_loss:0.040, val_acc:0.968]
Epoch [42/120    avg_loss:0.082, val_acc:0.951]
Epoch [43/120    avg_loss:0.054, val_acc:0.964]
Epoch [44/120    avg_loss:0.027, val_acc:0.972]
Epoch [45/120    avg_loss:0.037, val_acc:0.983]
Epoch [46/120    avg_loss:0.034, val_acc:0.975]
Epoch [47/120    avg_loss:0.026, val_acc:0.977]
Epoch [48/120    avg_loss:0.017, val_acc:0.978]
Epoch [49/120    avg_loss:0.010, val_acc:0.978]
Epoch [50/120    avg_loss:0.019, val_acc:0.977]
Epoch [51/120    avg_loss:0.016, val_acc:0.978]
Epoch [52/120    avg_loss:0.015, val_acc:0.983]
Epoch [53/120    avg_loss:0.007, val_acc:0.982]
Epoch [54/120    avg_loss:0.010, val_acc:0.983]
Epoch [55/120    avg_loss:0.008, val_acc:0.983]
Epoch [56/120    avg_loss:0.006, val_acc:0.983]
Epoch [57/120    avg_loss:0.011, val_acc:0.983]
Epoch [58/120    avg_loss:0.007, val_acc:0.984]
Epoch [59/120    avg_loss:0.008, val_acc:0.983]
Epoch [60/120    avg_loss:0.006, val_acc:0.983]
Epoch [61/120    avg_loss:0.005, val_acc:0.985]
Epoch [62/120    avg_loss:0.009, val_acc:0.984]
Epoch [63/120    avg_loss:0.009, val_acc:0.985]
Epoch [64/120    avg_loss:0.007, val_acc:0.985]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.006, val_acc:0.984]
Epoch [67/120    avg_loss:0.005, val_acc:0.985]
Epoch [68/120    avg_loss:0.007, val_acc:0.983]
Epoch [69/120    avg_loss:0.011, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.007, val_acc:0.984]
Epoch [74/120    avg_loss:0.006, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.006, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.006, val_acc:0.984]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.005, val_acc:0.984]
Epoch [89/120    avg_loss:0.006, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     2     0     0    17     0     1     0]
 [    0     0 18037     0    47     0     5     0     1     0]
 [    0     0     0  2018     3     0     0     0    12     3]
 [    0    31    14     0  2904     0     0     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4871     0     1     0]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     4     0     1    59     0     0     0  3504     3]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
99.34205769647893

F1 scores:
[       nan 0.99572948 0.99814615 0.99335466 0.96816136 0.99126472
 0.99672601 0.99883586 0.9852383  0.97620365]

Kappa:
0.9912850999305601
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4414e667f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.664, val_acc:0.371]
Epoch [2/120    avg_loss:1.134, val_acc:0.427]
Epoch [3/120    avg_loss:0.897, val_acc:0.700]
Epoch [4/120    avg_loss:0.666, val_acc:0.717]
Epoch [5/120    avg_loss:0.530, val_acc:0.734]
Epoch [6/120    avg_loss:0.388, val_acc:0.781]
Epoch [7/120    avg_loss:0.329, val_acc:0.852]
Epoch [8/120    avg_loss:0.275, val_acc:0.834]
Epoch [9/120    avg_loss:0.250, val_acc:0.909]
Epoch [10/120    avg_loss:0.231, val_acc:0.913]
Epoch [11/120    avg_loss:0.191, val_acc:0.914]
Epoch [12/120    avg_loss:0.189, val_acc:0.931]
Epoch [13/120    avg_loss:0.145, val_acc:0.910]
Epoch [14/120    avg_loss:0.138, val_acc:0.941]
Epoch [15/120    avg_loss:0.095, val_acc:0.965]
Epoch [16/120    avg_loss:0.084, val_acc:0.951]
Epoch [17/120    avg_loss:0.120, val_acc:0.944]
Epoch [18/120    avg_loss:0.192, val_acc:0.935]
Epoch [19/120    avg_loss:0.102, val_acc:0.927]
Epoch [20/120    avg_loss:0.076, val_acc:0.947]
Epoch [21/120    avg_loss:0.058, val_acc:0.962]
Epoch [22/120    avg_loss:0.122, val_acc:0.964]
Epoch [23/120    avg_loss:0.081, val_acc:0.950]
Epoch [24/120    avg_loss:0.078, val_acc:0.968]
Epoch [25/120    avg_loss:0.042, val_acc:0.974]
Epoch [26/120    avg_loss:0.039, val_acc:0.972]
Epoch [27/120    avg_loss:0.040, val_acc:0.974]
Epoch [28/120    avg_loss:0.023, val_acc:0.973]
Epoch [29/120    avg_loss:0.021, val_acc:0.976]
Epoch [30/120    avg_loss:0.052, val_acc:0.969]
Epoch [31/120    avg_loss:0.077, val_acc:0.966]
Epoch [32/120    avg_loss:0.040, val_acc:0.969]
Epoch [33/120    avg_loss:0.038, val_acc:0.963]
Epoch [34/120    avg_loss:0.026, val_acc:0.978]
Epoch [35/120    avg_loss:0.014, val_acc:0.978]
Epoch [36/120    avg_loss:0.019, val_acc:0.975]
Epoch [37/120    avg_loss:0.022, val_acc:0.977]
Epoch [38/120    avg_loss:0.015, val_acc:0.981]
Epoch [39/120    avg_loss:0.016, val_acc:0.977]
Epoch [40/120    avg_loss:0.014, val_acc:0.981]
Epoch [41/120    avg_loss:0.012, val_acc:0.981]
Epoch [42/120    avg_loss:0.019, val_acc:0.978]
Epoch [43/120    avg_loss:0.019, val_acc:0.976]
Epoch [44/120    avg_loss:0.040, val_acc:0.967]
Epoch [45/120    avg_loss:0.019, val_acc:0.981]
Epoch [46/120    avg_loss:0.036, val_acc:0.972]
Epoch [47/120    avg_loss:0.018, val_acc:0.970]
Epoch [48/120    avg_loss:0.014, val_acc:0.978]
Epoch [49/120    avg_loss:0.014, val_acc:0.980]
Epoch [50/120    avg_loss:0.008, val_acc:0.981]
Epoch [51/120    avg_loss:0.012, val_acc:0.979]
Epoch [52/120    avg_loss:0.013, val_acc:0.978]
Epoch [53/120    avg_loss:0.017, val_acc:0.978]
Epoch [54/120    avg_loss:0.007, val_acc:0.984]
Epoch [55/120    avg_loss:0.009, val_acc:0.982]
Epoch [56/120    avg_loss:0.006, val_acc:0.984]
Epoch [57/120    avg_loss:0.007, val_acc:0.983]
Epoch [58/120    avg_loss:0.007, val_acc:0.986]
Epoch [59/120    avg_loss:0.005, val_acc:0.986]
Epoch [60/120    avg_loss:0.005, val_acc:0.984]
Epoch [61/120    avg_loss:0.015, val_acc:0.981]
Epoch [62/120    avg_loss:0.007, val_acc:0.984]
Epoch [63/120    avg_loss:0.006, val_acc:0.987]
Epoch [64/120    avg_loss:0.008, val_acc:0.986]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.007, val_acc:0.983]
Epoch [67/120    avg_loss:0.008, val_acc:0.977]
Epoch [68/120    avg_loss:0.009, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.981]
Epoch [71/120    avg_loss:0.006, val_acc:0.984]
Epoch [72/120    avg_loss:0.007, val_acc:0.980]
Epoch [73/120    avg_loss:0.006, val_acc:0.983]
Epoch [74/120    avg_loss:0.005, val_acc:0.981]
Epoch [75/120    avg_loss:0.005, val_acc:0.983]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.004, val_acc:0.985]
Epoch [78/120    avg_loss:0.004, val_acc:0.984]
Epoch [79/120    avg_loss:0.004, val_acc:0.983]
Epoch [80/120    avg_loss:0.002, val_acc:0.983]
Epoch [81/120    avg_loss:0.004, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.984]
Epoch [83/120    avg_loss:0.004, val_acc:0.984]
Epoch [84/120    avg_loss:0.002, val_acc:0.984]
Epoch [85/120    avg_loss:0.004, val_acc:0.984]
Epoch [86/120    avg_loss:0.003, val_acc:0.984]
Epoch [87/120    avg_loss:0.003, val_acc:0.984]
Epoch [88/120    avg_loss:0.003, val_acc:0.984]
Epoch [89/120    avg_loss:0.003, val_acc:0.984]
Epoch [90/120    avg_loss:0.002, val_acc:0.985]
Epoch [91/120    avg_loss:0.003, val_acc:0.985]
Epoch [92/120    avg_loss:0.003, val_acc:0.984]
Epoch [93/120    avg_loss:0.003, val_acc:0.984]
Epoch [94/120    avg_loss:0.003, val_acc:0.984]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.002, val_acc:0.985]
Epoch [97/120    avg_loss:0.003, val_acc:0.985]
Epoch [98/120    avg_loss:0.003, val_acc:0.985]
Epoch [99/120    avg_loss:0.003, val_acc:0.985]
Epoch [100/120    avg_loss:0.003, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.985]
Epoch [102/120    avg_loss:0.003, val_acc:0.985]
Epoch [103/120    avg_loss:0.002, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.003, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.985]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.003, val_acc:0.985]
Epoch [109/120    avg_loss:0.003, val_acc:0.985]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.002, val_acc:0.985]
Epoch [112/120    avg_loss:0.002, val_acc:0.985]
Epoch [113/120    avg_loss:0.002, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.004, val_acc:0.984]
Epoch [116/120    avg_loss:0.003, val_acc:0.984]
Epoch [117/120    avg_loss:0.003, val_acc:0.984]
Epoch [118/120    avg_loss:0.002, val_acc:0.984]
Epoch [119/120    avg_loss:0.003, val_acc:0.984]
Epoch [120/120    avg_loss:0.003, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     0     0     9     0     6     4]
 [    0     0 18065     0    25     0     0     0     0     0]
 [    0     0     0  2026     4     0     0     0     0     6]
 [    0    39    17     0  2890     0     0     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4851     0    25     1]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     1     0    20    52     0     0     0  3492     6]
 [    0     0     0     0    14    47     0     0     0   858]]

Accuracy:
99.26975634444365

F1 scores:
[       nan 0.99542103 0.99881127 0.99265066 0.97028706 0.98231088
 0.99630314 1.         0.98089888 0.95652174]

Kappa:
0.9903244115242564
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b4f9ab7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.733, val_acc:0.409]
Epoch [2/120    avg_loss:1.193, val_acc:0.667]
Epoch [3/120    avg_loss:0.982, val_acc:0.606]
Epoch [4/120    avg_loss:0.707, val_acc:0.668]
Epoch [5/120    avg_loss:0.620, val_acc:0.763]
Epoch [6/120    avg_loss:0.444, val_acc:0.779]
Epoch [7/120    avg_loss:0.360, val_acc:0.806]
Epoch [8/120    avg_loss:0.350, val_acc:0.872]
Epoch [9/120    avg_loss:0.252, val_acc:0.942]
Epoch [10/120    avg_loss:0.243, val_acc:0.928]
Epoch [11/120    avg_loss:0.240, val_acc:0.942]
Epoch [12/120    avg_loss:0.214, val_acc:0.887]
Epoch [13/120    avg_loss:0.203, val_acc:0.962]
Epoch [14/120    avg_loss:0.151, val_acc:0.935]
Epoch [15/120    avg_loss:0.131, val_acc:0.957]
Epoch [16/120    avg_loss:0.166, val_acc:0.960]
Epoch [17/120    avg_loss:0.129, val_acc:0.944]
Epoch [18/120    avg_loss:0.125, val_acc:0.961]
Epoch [19/120    avg_loss:0.106, val_acc:0.970]
Epoch [20/120    avg_loss:0.081, val_acc:0.974]
Epoch [21/120    avg_loss:0.078, val_acc:0.946]
Epoch [22/120    avg_loss:0.073, val_acc:0.948]
Epoch [23/120    avg_loss:0.102, val_acc:0.943]
Epoch [24/120    avg_loss:0.193, val_acc:0.925]
Epoch [25/120    avg_loss:0.099, val_acc:0.964]
Epoch [26/120    avg_loss:0.052, val_acc:0.954]
Epoch [27/120    avg_loss:0.107, val_acc:0.946]
Epoch [28/120    avg_loss:0.063, val_acc:0.957]
Epoch [29/120    avg_loss:0.058, val_acc:0.964]
Epoch [30/120    avg_loss:0.074, val_acc:0.971]
Epoch [31/120    avg_loss:0.047, val_acc:0.981]
Epoch [32/120    avg_loss:0.044, val_acc:0.968]
Epoch [33/120    avg_loss:0.034, val_acc:0.981]
Epoch [34/120    avg_loss:0.057, val_acc:0.971]
Epoch [35/120    avg_loss:0.046, val_acc:0.951]
Epoch [36/120    avg_loss:0.073, val_acc:0.971]
Epoch [37/120    avg_loss:0.054, val_acc:0.987]
Epoch [38/120    avg_loss:0.027, val_acc:0.984]
Epoch [39/120    avg_loss:0.043, val_acc:0.981]
Epoch [40/120    avg_loss:0.023, val_acc:0.985]
Epoch [41/120    avg_loss:0.029, val_acc:0.980]
Epoch [42/120    avg_loss:0.045, val_acc:0.973]
Epoch [43/120    avg_loss:0.025, val_acc:0.984]
Epoch [44/120    avg_loss:0.021, val_acc:0.982]
Epoch [45/120    avg_loss:0.023, val_acc:0.985]
Epoch [46/120    avg_loss:0.036, val_acc:0.981]
Epoch [47/120    avg_loss:0.019, val_acc:0.984]
Epoch [48/120    avg_loss:0.019, val_acc:0.974]
Epoch [49/120    avg_loss:0.016, val_acc:0.990]
Epoch [50/120    avg_loss:0.016, val_acc:0.988]
Epoch [51/120    avg_loss:0.019, val_acc:0.985]
Epoch [52/120    avg_loss:0.012, val_acc:0.982]
Epoch [53/120    avg_loss:0.009, val_acc:0.989]
Epoch [54/120    avg_loss:0.007, val_acc:0.988]
Epoch [55/120    avg_loss:0.009, val_acc:0.989]
Epoch [56/120    avg_loss:0.013, val_acc:0.986]
Epoch [57/120    avg_loss:0.010, val_acc:0.990]
Epoch [58/120    avg_loss:0.011, val_acc:0.990]
Epoch [59/120    avg_loss:0.008, val_acc:0.990]
Epoch [60/120    avg_loss:0.006, val_acc:0.989]
Epoch [61/120    avg_loss:0.009, val_acc:0.988]
Epoch [62/120    avg_loss:0.014, val_acc:0.990]
Epoch [63/120    avg_loss:0.018, val_acc:0.978]
Epoch [64/120    avg_loss:0.017, val_acc:0.982]
Epoch [65/120    avg_loss:0.012, val_acc:0.988]
Epoch [66/120    avg_loss:0.028, val_acc:0.972]
Epoch [67/120    avg_loss:0.015, val_acc:0.990]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.014, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.993]
Epoch [71/120    avg_loss:0.006, val_acc:0.989]
Epoch [72/120    avg_loss:0.009, val_acc:0.990]
Epoch [73/120    avg_loss:0.033, val_acc:0.984]
Epoch [74/120    avg_loss:0.021, val_acc:0.990]
Epoch [75/120    avg_loss:0.011, val_acc:0.986]
Epoch [76/120    avg_loss:0.008, val_acc:0.990]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.008, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.992]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.004, val_acc:0.990]
Epoch [82/120    avg_loss:0.005, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.004, val_acc:0.990]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.003, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.004, val_acc:0.993]
Epoch [89/120    avg_loss:0.003, val_acc:0.993]
Epoch [90/120    avg_loss:0.006, val_acc:0.993]
Epoch [91/120    avg_loss:0.005, val_acc:0.994]
Epoch [92/120    avg_loss:0.006, val_acc:0.992]
Epoch [93/120    avg_loss:0.004, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.003, val_acc:0.992]
Epoch [96/120    avg_loss:0.003, val_acc:0.993]
Epoch [97/120    avg_loss:0.004, val_acc:0.992]
Epoch [98/120    avg_loss:0.003, val_acc:0.992]
Epoch [99/120    avg_loss:0.003, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.994]
Epoch [101/120    avg_loss:0.004, val_acc:0.994]
Epoch [102/120    avg_loss:0.004, val_acc:0.993]
Epoch [103/120    avg_loss:0.003, val_acc:0.993]
Epoch [104/120    avg_loss:0.003, val_acc:0.993]
Epoch [105/120    avg_loss:0.003, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.993]
Epoch [107/120    avg_loss:0.003, val_acc:0.993]
Epoch [108/120    avg_loss:0.002, val_acc:0.993]
Epoch [109/120    avg_loss:0.003, val_acc:0.993]
Epoch [110/120    avg_loss:0.003, val_acc:0.993]
Epoch [111/120    avg_loss:0.005, val_acc:0.993]
Epoch [112/120    avg_loss:0.003, val_acc:0.993]
Epoch [113/120    avg_loss:0.004, val_acc:0.993]
Epoch [114/120    avg_loss:0.003, val_acc:0.993]
Epoch [115/120    avg_loss:0.003, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.004, val_acc:0.993]
Epoch [118/120    avg_loss:0.005, val_acc:0.993]
Epoch [119/120    avg_loss:0.003, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     1     0     4    43     0     0]
 [    0     0 18042     0    47     0     1     0     0     0]
 [    0     5     0  2019     2     0     0     0     4     6]
 [    0    41    18     0  2879     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0     0    18]
 [    0     0     0     0     0     0     6  1282     0     2]
 [    0     2     0     0    51     0     0     0  3508    10]
 [    0     0     0     0    19    51     0     0     0   849]]

Accuracy:
99.12033355023739

F1 scores:
[       nan 0.99253731 0.99817427 0.99580764 0.96432758 0.98083427
 0.99630996 0.98049713 0.98677918 0.94124169]

Kappa:
0.9883486487612714
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f832416c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.707, val_acc:0.378]
Epoch [2/120    avg_loss:1.192, val_acc:0.594]
Epoch [3/120    avg_loss:0.961, val_acc:0.584]
Epoch [4/120    avg_loss:0.769, val_acc:0.735]
Epoch [5/120    avg_loss:0.659, val_acc:0.744]
Epoch [6/120    avg_loss:0.536, val_acc:0.770]
Epoch [7/120    avg_loss:0.415, val_acc:0.803]
Epoch [8/120    avg_loss:0.357, val_acc:0.816]
Epoch [9/120    avg_loss:0.339, val_acc:0.802]
Epoch [10/120    avg_loss:0.272, val_acc:0.823]
Epoch [11/120    avg_loss:0.276, val_acc:0.849]
Epoch [12/120    avg_loss:0.213, val_acc:0.822]
Epoch [13/120    avg_loss:0.204, val_acc:0.890]
Epoch [14/120    avg_loss:0.131, val_acc:0.941]
Epoch [15/120    avg_loss:0.128, val_acc:0.951]
Epoch [16/120    avg_loss:0.163, val_acc:0.949]
Epoch [17/120    avg_loss:0.139, val_acc:0.955]
Epoch [18/120    avg_loss:0.131, val_acc:0.951]
Epoch [19/120    avg_loss:0.112, val_acc:0.917]
Epoch [20/120    avg_loss:0.163, val_acc:0.945]
Epoch [21/120    avg_loss:0.104, val_acc:0.950]
Epoch [22/120    avg_loss:0.079, val_acc:0.952]
Epoch [23/120    avg_loss:0.076, val_acc:0.965]
Epoch [24/120    avg_loss:0.048, val_acc:0.973]
Epoch [25/120    avg_loss:0.052, val_acc:0.928]
Epoch [26/120    avg_loss:0.065, val_acc:0.962]
Epoch [27/120    avg_loss:0.048, val_acc:0.973]
Epoch [28/120    avg_loss:0.052, val_acc:0.980]
Epoch [29/120    avg_loss:0.054, val_acc:0.949]
Epoch [30/120    avg_loss:0.038, val_acc:0.981]
Epoch [31/120    avg_loss:0.031, val_acc:0.986]
Epoch [32/120    avg_loss:0.048, val_acc:0.963]
Epoch [33/120    avg_loss:0.071, val_acc:0.968]
Epoch [34/120    avg_loss:0.058, val_acc:0.967]
Epoch [35/120    avg_loss:0.075, val_acc:0.971]
Epoch [36/120    avg_loss:0.050, val_acc:0.978]
Epoch [37/120    avg_loss:0.040, val_acc:0.980]
Epoch [38/120    avg_loss:0.023, val_acc:0.985]
Epoch [39/120    avg_loss:0.036, val_acc:0.985]
Epoch [40/120    avg_loss:0.026, val_acc:0.985]
Epoch [41/120    avg_loss:0.023, val_acc:0.989]
Epoch [42/120    avg_loss:0.020, val_acc:0.988]
Epoch [43/120    avg_loss:0.018, val_acc:0.985]
Epoch [44/120    avg_loss:0.049, val_acc:0.976]
Epoch [45/120    avg_loss:0.043, val_acc:0.985]
Epoch [46/120    avg_loss:0.032, val_acc:0.970]
Epoch [47/120    avg_loss:0.023, val_acc:0.990]
Epoch [48/120    avg_loss:0.014, val_acc:0.987]
Epoch [49/120    avg_loss:0.013, val_acc:0.990]
Epoch [50/120    avg_loss:0.011, val_acc:0.989]
Epoch [51/120    avg_loss:0.014, val_acc:0.988]
Epoch [52/120    avg_loss:0.012, val_acc:0.990]
Epoch [53/120    avg_loss:0.010, val_acc:0.992]
Epoch [54/120    avg_loss:0.008, val_acc:0.992]
Epoch [55/120    avg_loss:0.010, val_acc:0.992]
Epoch [56/120    avg_loss:0.008, val_acc:0.989]
Epoch [57/120    avg_loss:0.006, val_acc:0.990]
Epoch [58/120    avg_loss:0.012, val_acc:0.988]
Epoch [59/120    avg_loss:0.010, val_acc:0.990]
Epoch [60/120    avg_loss:0.007, val_acc:0.992]
Epoch [61/120    avg_loss:0.006, val_acc:0.993]
Epoch [62/120    avg_loss:0.011, val_acc:0.991]
Epoch [63/120    avg_loss:0.010, val_acc:0.992]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.023, val_acc:0.980]
Epoch [66/120    avg_loss:0.047, val_acc:0.986]
Epoch [67/120    avg_loss:0.027, val_acc:0.976]
Epoch [68/120    avg_loss:0.048, val_acc:0.976]
Epoch [69/120    avg_loss:0.017, val_acc:0.991]
Epoch [70/120    avg_loss:0.023, val_acc:0.984]
Epoch [71/120    avg_loss:0.014, val_acc:0.987]
Epoch [72/120    avg_loss:0.013, val_acc:0.990]
Epoch [73/120    avg_loss:0.009, val_acc:0.988]
Epoch [74/120    avg_loss:0.012, val_acc:0.992]
Epoch [75/120    avg_loss:0.007, val_acc:0.994]
Epoch [76/120    avg_loss:0.006, val_acc:0.994]
Epoch [77/120    avg_loss:0.006, val_acc:0.994]
Epoch [78/120    avg_loss:0.006, val_acc:0.995]
Epoch [79/120    avg_loss:0.005, val_acc:0.995]
Epoch [80/120    avg_loss:0.005, val_acc:0.995]
Epoch [81/120    avg_loss:0.006, val_acc:0.995]
Epoch [82/120    avg_loss:0.004, val_acc:0.995]
Epoch [83/120    avg_loss:0.005, val_acc:0.995]
Epoch [84/120    avg_loss:0.006, val_acc:0.994]
Epoch [85/120    avg_loss:0.004, val_acc:0.995]
Epoch [86/120    avg_loss:0.010, val_acc:0.994]
Epoch [87/120    avg_loss:0.006, val_acc:0.994]
Epoch [88/120    avg_loss:0.006, val_acc:0.993]
Epoch [89/120    avg_loss:0.005, val_acc:0.994]
Epoch [90/120    avg_loss:0.006, val_acc:0.994]
Epoch [91/120    avg_loss:0.008, val_acc:0.994]
Epoch [92/120    avg_loss:0.004, val_acc:0.994]
Epoch [93/120    avg_loss:0.005, val_acc:0.995]
Epoch [94/120    avg_loss:0.005, val_acc:0.995]
Epoch [95/120    avg_loss:0.005, val_acc:0.995]
Epoch [96/120    avg_loss:0.005, val_acc:0.995]
Epoch [97/120    avg_loss:0.005, val_acc:0.995]
Epoch [98/120    avg_loss:0.004, val_acc:0.995]
Epoch [99/120    avg_loss:0.006, val_acc:0.995]
Epoch [100/120    avg_loss:0.006, val_acc:0.996]
Epoch [101/120    avg_loss:0.005, val_acc:0.996]
Epoch [102/120    avg_loss:0.004, val_acc:0.996]
Epoch [103/120    avg_loss:0.005, val_acc:0.996]
Epoch [104/120    avg_loss:0.007, val_acc:0.994]
Epoch [105/120    avg_loss:0.004, val_acc:0.995]
Epoch [106/120    avg_loss:0.005, val_acc:0.996]
Epoch [107/120    avg_loss:0.004, val_acc:0.996]
Epoch [108/120    avg_loss:0.005, val_acc:0.996]
Epoch [109/120    avg_loss:0.006, val_acc:0.995]
Epoch [110/120    avg_loss:0.003, val_acc:0.995]
Epoch [111/120    avg_loss:0.005, val_acc:0.996]
Epoch [112/120    avg_loss:0.005, val_acc:0.996]
Epoch [113/120    avg_loss:0.004, val_acc:0.995]
Epoch [114/120    avg_loss:0.004, val_acc:0.995]
Epoch [115/120    avg_loss:0.004, val_acc:0.995]
Epoch [116/120    avg_loss:0.003, val_acc:0.995]
Epoch [117/120    avg_loss:0.005, val_acc:0.995]
Epoch [118/120    avg_loss:0.004, val_acc:0.995]
Epoch [119/120    avg_loss:0.006, val_acc:0.995]
Epoch [120/120    avg_loss:0.003, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     0     2     0     8    33     0     0]
 [    0     0 18073     0    17     0     0     0     0     0]
 [    0     0     0  2024     5     0     0     0     0     7]
 [    0    24    19     0  2897     0     2     0    28     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4855     0     0    23]
 [    0     0     0     0     0     0     3  1281     0     6]
 [    0     6     0     9    41     0     0     0  3515     0]
 [    0     0     0     1    14    34     0     0     0   870]]

Accuracy:
99.31554720073265

F1 scores:
[       nan 0.99431951 0.99900503 0.99459459 0.97410894 0.9871407
 0.99630618 0.98387097 0.9881923  0.95238095]

Kappa:
0.9909314466152326
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f249887f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.815, val_acc:0.375]
Epoch [2/120    avg_loss:1.170, val_acc:0.691]
Epoch [3/120    avg_loss:0.975, val_acc:0.471]
Epoch [4/120    avg_loss:0.724, val_acc:0.715]
Epoch [5/120    avg_loss:0.543, val_acc:0.799]
Epoch [6/120    avg_loss:0.473, val_acc:0.826]
Epoch [7/120    avg_loss:0.381, val_acc:0.832]
Epoch [8/120    avg_loss:0.309, val_acc:0.832]
Epoch [9/120    avg_loss:0.282, val_acc:0.908]
Epoch [10/120    avg_loss:0.220, val_acc:0.915]
Epoch [11/120    avg_loss:0.198, val_acc:0.924]
Epoch [12/120    avg_loss:0.189, val_acc:0.923]
Epoch [13/120    avg_loss:0.148, val_acc:0.938]
Epoch [14/120    avg_loss:0.153, val_acc:0.905]
Epoch [15/120    avg_loss:0.159, val_acc:0.944]
Epoch [16/120    avg_loss:0.152, val_acc:0.938]
Epoch [17/120    avg_loss:0.164, val_acc:0.886]
Epoch [18/120    avg_loss:0.143, val_acc:0.958]
Epoch [19/120    avg_loss:0.101, val_acc:0.950]
Epoch [20/120    avg_loss:0.082, val_acc:0.966]
Epoch [21/120    avg_loss:0.079, val_acc:0.958]
Epoch [22/120    avg_loss:0.060, val_acc:0.961]
Epoch [23/120    avg_loss:0.086, val_acc:0.969]
Epoch [24/120    avg_loss:0.071, val_acc:0.966]
Epoch [25/120    avg_loss:0.038, val_acc:0.981]
Epoch [26/120    avg_loss:0.045, val_acc:0.977]
Epoch [27/120    avg_loss:0.072, val_acc:0.956]
Epoch [28/120    avg_loss:0.071, val_acc:0.975]
Epoch [29/120    avg_loss:0.069, val_acc:0.981]
Epoch [30/120    avg_loss:0.051, val_acc:0.921]
Epoch [31/120    avg_loss:0.057, val_acc:0.970]
Epoch [32/120    avg_loss:0.053, val_acc:0.968]
Epoch [33/120    avg_loss:0.038, val_acc:0.982]
Epoch [34/120    avg_loss:0.027, val_acc:0.991]
Epoch [35/120    avg_loss:0.025, val_acc:0.984]
Epoch [36/120    avg_loss:0.030, val_acc:0.968]
Epoch [37/120    avg_loss:0.033, val_acc:0.977]
Epoch [38/120    avg_loss:0.031, val_acc:0.918]
Epoch [39/120    avg_loss:0.049, val_acc:0.958]
Epoch [40/120    avg_loss:0.034, val_acc:0.982]
Epoch [41/120    avg_loss:0.029, val_acc:0.984]
Epoch [42/120    avg_loss:0.123, val_acc:0.962]
Epoch [43/120    avg_loss:0.037, val_acc:0.974]
Epoch [44/120    avg_loss:0.027, val_acc:0.991]
Epoch [45/120    avg_loss:0.019, val_acc:0.981]
Epoch [46/120    avg_loss:0.015, val_acc:0.987]
Epoch [47/120    avg_loss:0.024, val_acc:0.972]
Epoch [48/120    avg_loss:0.018, val_acc:0.985]
Epoch [49/120    avg_loss:0.034, val_acc:0.981]
Epoch [50/120    avg_loss:0.018, val_acc:0.989]
Epoch [51/120    avg_loss:0.015, val_acc:0.979]
Epoch [52/120    avg_loss:0.020, val_acc:0.987]
Epoch [53/120    avg_loss:0.023, val_acc:0.982]
Epoch [54/120    avg_loss:0.021, val_acc:0.990]
Epoch [55/120    avg_loss:0.012, val_acc:0.993]
Epoch [56/120    avg_loss:0.012, val_acc:0.988]
Epoch [57/120    avg_loss:0.026, val_acc:0.983]
Epoch [58/120    avg_loss:0.022, val_acc:0.977]
Epoch [59/120    avg_loss:0.017, val_acc:0.987]
Epoch [60/120    avg_loss:0.020, val_acc:0.987]
Epoch [61/120    avg_loss:0.011, val_acc:0.991]
Epoch [62/120    avg_loss:0.015, val_acc:0.987]
Epoch [63/120    avg_loss:0.011, val_acc:0.990]
Epoch [64/120    avg_loss:0.007, val_acc:0.992]
Epoch [65/120    avg_loss:0.006, val_acc:0.992]
Epoch [66/120    avg_loss:0.008, val_acc:0.995]
Epoch [67/120    avg_loss:0.011, val_acc:0.992]
Epoch [68/120    avg_loss:0.005, val_acc:0.993]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.032, val_acc:0.969]
Epoch [71/120    avg_loss:0.015, val_acc:0.978]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.987]
Epoch [74/120    avg_loss:0.006, val_acc:0.991]
Epoch [75/120    avg_loss:0.005, val_acc:0.993]
Epoch [76/120    avg_loss:0.004, val_acc:0.993]
Epoch [77/120    avg_loss:0.006, val_acc:0.992]
Epoch [78/120    avg_loss:0.018, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.987]
Epoch [80/120    avg_loss:0.010, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.992]
Epoch [82/120    avg_loss:0.006, val_acc:0.993]
Epoch [83/120    avg_loss:0.005, val_acc:0.993]
Epoch [84/120    avg_loss:0.006, val_acc:0.993]
Epoch [85/120    avg_loss:0.005, val_acc:0.994]
Epoch [86/120    avg_loss:0.004, val_acc:0.994]
Epoch [87/120    avg_loss:0.004, val_acc:0.994]
Epoch [88/120    avg_loss:0.005, val_acc:0.993]
Epoch [89/120    avg_loss:0.004, val_acc:0.993]
Epoch [90/120    avg_loss:0.003, val_acc:0.993]
Epoch [91/120    avg_loss:0.004, val_acc:0.993]
Epoch [92/120    avg_loss:0.006, val_acc:0.993]
Epoch [93/120    avg_loss:0.004, val_acc:0.993]
Epoch [94/120    avg_loss:0.005, val_acc:0.993]
Epoch [95/120    avg_loss:0.004, val_acc:0.993]
Epoch [96/120    avg_loss:0.004, val_acc:0.993]
Epoch [97/120    avg_loss:0.005, val_acc:0.993]
Epoch [98/120    avg_loss:0.003, val_acc:0.993]
Epoch [99/120    avg_loss:0.005, val_acc:0.993]
Epoch [100/120    avg_loss:0.004, val_acc:0.993]
Epoch [101/120    avg_loss:0.004, val_acc:0.993]
Epoch [102/120    avg_loss:0.005, val_acc:0.993]
Epoch [103/120    avg_loss:0.005, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.993]
Epoch [105/120    avg_loss:0.005, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.993]
Epoch [107/120    avg_loss:0.005, val_acc:0.993]
Epoch [108/120    avg_loss:0.007, val_acc:0.993]
Epoch [109/120    avg_loss:0.005, val_acc:0.993]
Epoch [110/120    avg_loss:0.004, val_acc:0.993]
Epoch [111/120    avg_loss:0.005, val_acc:0.993]
Epoch [112/120    avg_loss:0.004, val_acc:0.993]
Epoch [113/120    avg_loss:0.004, val_acc:0.993]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.005, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.003, val_acc:0.993]
Epoch [118/120    avg_loss:0.005, val_acc:0.993]
Epoch [119/120    avg_loss:0.005, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     1     1     0    11     0     0     2]
 [    0     1 18065     0    23     0     1     0     0     0]
 [    0     0     0  2031     3     0     0     0     0     2]
 [    0    38    19     0  2879     0     7     0    28     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4856     0     0    20]
 [    0     0     0     0     0     0     4  1284     0     2]
 [    0     2     0     0    39     0     0     0  3524     6]
 [    0     0     0     1    16    41     0     0     0   861]]

Accuracy:
99.34687778661461

F1 scores:
[       nan 0.99565555 0.99878366 0.99778924 0.97050396 0.98453414
 0.99538793 0.997669   0.98947073 0.94980695]

Kappa:
0.9913458441739441
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa369dd4c88>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.780, val_acc:0.488]
Epoch [2/120    avg_loss:1.229, val_acc:0.582]
Epoch [3/120    avg_loss:0.958, val_acc:0.689]
Epoch [4/120    avg_loss:0.788, val_acc:0.685]
Epoch [5/120    avg_loss:0.590, val_acc:0.751]
Epoch [6/120    avg_loss:0.506, val_acc:0.819]
Epoch [7/120    avg_loss:0.409, val_acc:0.844]
Epoch [8/120    avg_loss:0.326, val_acc:0.903]
Epoch [9/120    avg_loss:0.329, val_acc:0.930]
Epoch [10/120    avg_loss:0.296, val_acc:0.925]
Epoch [11/120    avg_loss:0.212, val_acc:0.930]
Epoch [12/120    avg_loss:0.215, val_acc:0.937]
Epoch [13/120    avg_loss:0.200, val_acc:0.929]
Epoch [14/120    avg_loss:0.211, val_acc:0.951]
Epoch [15/120    avg_loss:0.168, val_acc:0.931]
Epoch [16/120    avg_loss:0.160, val_acc:0.928]
Epoch [17/120    avg_loss:0.140, val_acc:0.934]
Epoch [18/120    avg_loss:0.138, val_acc:0.954]
Epoch [19/120    avg_loss:0.117, val_acc:0.955]
Epoch [20/120    avg_loss:0.101, val_acc:0.959]
Epoch [21/120    avg_loss:0.108, val_acc:0.944]
Epoch [22/120    avg_loss:0.077, val_acc:0.975]
Epoch [23/120    avg_loss:0.088, val_acc:0.968]
Epoch [24/120    avg_loss:0.073, val_acc:0.964]
Epoch [25/120    avg_loss:0.062, val_acc:0.963]
Epoch [26/120    avg_loss:0.117, val_acc:0.924]
Epoch [27/120    avg_loss:0.109, val_acc:0.960]
Epoch [28/120    avg_loss:0.046, val_acc:0.973]
Epoch [29/120    avg_loss:0.045, val_acc:0.960]
Epoch [30/120    avg_loss:0.038, val_acc:0.977]
Epoch [31/120    avg_loss:0.034, val_acc:0.974]
Epoch [32/120    avg_loss:0.062, val_acc:0.976]
Epoch [33/120    avg_loss:0.035, val_acc:0.969]
Epoch [34/120    avg_loss:0.039, val_acc:0.974]
Epoch [35/120    avg_loss:0.056, val_acc:0.977]
Epoch [36/120    avg_loss:0.031, val_acc:0.975]
Epoch [37/120    avg_loss:0.018, val_acc:0.983]
Epoch [38/120    avg_loss:0.022, val_acc:0.984]
Epoch [39/120    avg_loss:0.020, val_acc:0.982]
Epoch [40/120    avg_loss:0.030, val_acc:0.957]
Epoch [41/120    avg_loss:0.031, val_acc:0.986]
Epoch [42/120    avg_loss:0.026, val_acc:0.982]
Epoch [43/120    avg_loss:0.022, val_acc:0.987]
Epoch [44/120    avg_loss:0.025, val_acc:0.972]
Epoch [45/120    avg_loss:0.063, val_acc:0.985]
Epoch [46/120    avg_loss:0.027, val_acc:0.975]
Epoch [47/120    avg_loss:0.038, val_acc:0.980]
Epoch [48/120    avg_loss:0.020, val_acc:0.987]
Epoch [49/120    avg_loss:0.024, val_acc:0.981]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.012, val_acc:0.990]
Epoch [52/120    avg_loss:0.020, val_acc:0.987]
Epoch [53/120    avg_loss:0.017, val_acc:0.985]
Epoch [54/120    avg_loss:0.013, val_acc:0.988]
Epoch [55/120    avg_loss:0.009, val_acc:0.984]
Epoch [56/120    avg_loss:0.008, val_acc:0.984]
Epoch [57/120    avg_loss:0.007, val_acc:0.987]
Epoch [58/120    avg_loss:0.008, val_acc:0.987]
Epoch [59/120    avg_loss:0.015, val_acc:0.981]
Epoch [60/120    avg_loss:0.013, val_acc:0.984]
Epoch [61/120    avg_loss:0.035, val_acc:0.973]
Epoch [62/120    avg_loss:0.018, val_acc:0.985]
Epoch [63/120    avg_loss:0.013, val_acc:0.967]
Epoch [64/120    avg_loss:0.020, val_acc:0.980]
Epoch [65/120    avg_loss:0.010, val_acc:0.986]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.007, val_acc:0.989]
Epoch [68/120    avg_loss:0.006, val_acc:0.989]
Epoch [69/120    avg_loss:0.009, val_acc:0.989]
Epoch [70/120    avg_loss:0.010, val_acc:0.989]
Epoch [71/120    avg_loss:0.008, val_acc:0.990]
Epoch [72/120    avg_loss:0.011, val_acc:0.990]
Epoch [73/120    avg_loss:0.005, val_acc:0.990]
Epoch [74/120    avg_loss:0.011, val_acc:0.990]
Epoch [75/120    avg_loss:0.008, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.006, val_acc:0.990]
Epoch [78/120    avg_loss:0.005, val_acc:0.990]
Epoch [79/120    avg_loss:0.007, val_acc:0.990]
Epoch [80/120    avg_loss:0.005, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.011, val_acc:0.991]
Epoch [84/120    avg_loss:0.005, val_acc:0.990]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.989]
Epoch [88/120    avg_loss:0.006, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     0     0    25    38     0     0]
 [    0     0 18054     0    28     0     3     0     5     0]
 [    0     0     0  2006     2     0     0     0    24     4]
 [    0    34    19     0  2883     0     7     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4861     0     0    17]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     2     0     0    42     0     0     0  3526     1]
 [    0     0     0     5    14    39     0     0     0   861]]

Accuracy:
99.17576458679777

F1 scores:
[       nan 0.99228792 0.99847911 0.99135162 0.97054368 0.98527746
 0.99427286 0.98393267 0.98560447 0.95560488]

Kappa:
0.9890811191069352
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb876f3e748>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.767, val_acc:0.352]
Epoch [2/120    avg_loss:1.187, val_acc:0.526]
Epoch [3/120    avg_loss:0.894, val_acc:0.653]
Epoch [4/120    avg_loss:0.680, val_acc:0.812]
Epoch [5/120    avg_loss:0.572, val_acc:0.812]
Epoch [6/120    avg_loss:0.416, val_acc:0.754]
Epoch [7/120    avg_loss:0.382, val_acc:0.799]
Epoch [8/120    avg_loss:0.321, val_acc:0.814]
Epoch [9/120    avg_loss:0.275, val_acc:0.833]
Epoch [10/120    avg_loss:0.246, val_acc:0.840]
Epoch [11/120    avg_loss:0.215, val_acc:0.877]
Epoch [12/120    avg_loss:0.221, val_acc:0.812]
Epoch [13/120    avg_loss:0.211, val_acc:0.843]
Epoch [14/120    avg_loss:0.161, val_acc:0.920]
Epoch [15/120    avg_loss:0.161, val_acc:0.962]
Epoch [16/120    avg_loss:0.118, val_acc:0.911]
Epoch [17/120    avg_loss:0.126, val_acc:0.961]
Epoch [18/120    avg_loss:0.124, val_acc:0.963]
Epoch [19/120    avg_loss:0.075, val_acc:0.984]
Epoch [20/120    avg_loss:0.067, val_acc:0.964]
Epoch [21/120    avg_loss:0.056, val_acc:0.980]
Epoch [22/120    avg_loss:0.052, val_acc:0.976]
Epoch [23/120    avg_loss:0.055, val_acc:0.972]
Epoch [24/120    avg_loss:0.057, val_acc:0.983]
Epoch [25/120    avg_loss:0.064, val_acc:0.964]
Epoch [26/120    avg_loss:0.063, val_acc:0.977]
Epoch [27/120    avg_loss:0.047, val_acc:0.985]
Epoch [28/120    avg_loss:0.047, val_acc:0.976]
Epoch [29/120    avg_loss:0.062, val_acc:0.988]
Epoch [30/120    avg_loss:0.041, val_acc:0.989]
Epoch [31/120    avg_loss:0.066, val_acc:0.963]
Epoch [32/120    avg_loss:0.045, val_acc:0.985]
Epoch [33/120    avg_loss:0.046, val_acc:0.951]
Epoch [34/120    avg_loss:0.044, val_acc:0.968]
Epoch [35/120    avg_loss:0.046, val_acc:0.942]
Epoch [36/120    avg_loss:0.047, val_acc:0.989]
Epoch [37/120    avg_loss:0.029, val_acc:0.990]
Epoch [38/120    avg_loss:0.025, val_acc:0.954]
Epoch [39/120    avg_loss:0.019, val_acc:0.988]
Epoch [40/120    avg_loss:0.243, val_acc:0.887]
Epoch [41/120    avg_loss:0.162, val_acc:0.970]
Epoch [42/120    avg_loss:0.124, val_acc:0.964]
Epoch [43/120    avg_loss:0.075, val_acc:0.978]
Epoch [44/120    avg_loss:0.044, val_acc:0.979]
Epoch [45/120    avg_loss:0.045, val_acc:0.987]
Epoch [46/120    avg_loss:0.028, val_acc:0.986]
Epoch [47/120    avg_loss:0.027, val_acc:0.991]
Epoch [48/120    avg_loss:0.017, val_acc:0.984]
Epoch [49/120    avg_loss:0.029, val_acc:0.990]
Epoch [50/120    avg_loss:0.028, val_acc:0.984]
Epoch [51/120    avg_loss:0.029, val_acc:0.987]
Epoch [52/120    avg_loss:0.028, val_acc:0.990]
Epoch [53/120    avg_loss:0.018, val_acc:0.993]
Epoch [54/120    avg_loss:0.015, val_acc:0.995]
Epoch [55/120    avg_loss:0.019, val_acc:0.934]
Epoch [56/120    avg_loss:0.016, val_acc:0.991]
Epoch [57/120    avg_loss:0.013, val_acc:0.991]
Epoch [58/120    avg_loss:0.010, val_acc:0.993]
Epoch [59/120    avg_loss:0.010, val_acc:0.991]
Epoch [60/120    avg_loss:0.008, val_acc:0.994]
Epoch [61/120    avg_loss:0.013, val_acc:0.990]
Epoch [62/120    avg_loss:0.012, val_acc:0.972]
Epoch [63/120    avg_loss:0.057, val_acc:0.979]
Epoch [64/120    avg_loss:0.035, val_acc:0.990]
Epoch [65/120    avg_loss:0.019, val_acc:0.990]
Epoch [66/120    avg_loss:0.028, val_acc:0.993]
Epoch [67/120    avg_loss:0.011, val_acc:0.994]
Epoch [68/120    avg_loss:0.010, val_acc:0.993]
Epoch [69/120    avg_loss:0.008, val_acc:0.993]
Epoch [70/120    avg_loss:0.007, val_acc:0.993]
Epoch [71/120    avg_loss:0.006, val_acc:0.994]
Epoch [72/120    avg_loss:0.009, val_acc:0.994]
Epoch [73/120    avg_loss:0.007, val_acc:0.994]
Epoch [74/120    avg_loss:0.006, val_acc:0.994]
Epoch [75/120    avg_loss:0.006, val_acc:0.993]
Epoch [76/120    avg_loss:0.009, val_acc:0.993]
Epoch [77/120    avg_loss:0.007, val_acc:0.993]
Epoch [78/120    avg_loss:0.008, val_acc:0.993]
Epoch [79/120    avg_loss:0.008, val_acc:0.993]
Epoch [80/120    avg_loss:0.007, val_acc:0.994]
Epoch [81/120    avg_loss:0.006, val_acc:0.994]
Epoch [82/120    avg_loss:0.009, val_acc:0.994]
Epoch [83/120    avg_loss:0.007, val_acc:0.994]
Epoch [84/120    avg_loss:0.006, val_acc:0.994]
Epoch [85/120    avg_loss:0.009, val_acc:0.994]
Epoch [86/120    avg_loss:0.006, val_acc:0.994]
Epoch [87/120    avg_loss:0.007, val_acc:0.994]
Epoch [88/120    avg_loss:0.005, val_acc:0.994]
Epoch [89/120    avg_loss:0.007, val_acc:0.994]
Epoch [90/120    avg_loss:0.006, val_acc:0.994]
Epoch [91/120    avg_loss:0.006, val_acc:0.994]
Epoch [92/120    avg_loss:0.006, val_acc:0.994]
Epoch [93/120    avg_loss:0.008, val_acc:0.994]
Epoch [94/120    avg_loss:0.006, val_acc:0.994]
Epoch [95/120    avg_loss:0.005, val_acc:0.994]
Epoch [96/120    avg_loss:0.007, val_acc:0.994]
Epoch [97/120    avg_loss:0.007, val_acc:0.994]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.008, val_acc:0.994]
Epoch [100/120    avg_loss:0.007, val_acc:0.994]
Epoch [101/120    avg_loss:0.006, val_acc:0.994]
Epoch [102/120    avg_loss:0.008, val_acc:0.994]
Epoch [103/120    avg_loss:0.008, val_acc:0.994]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.008, val_acc:0.994]
Epoch [106/120    avg_loss:0.007, val_acc:0.994]
Epoch [107/120    avg_loss:0.005, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.008, val_acc:0.994]
Epoch [110/120    avg_loss:0.006, val_acc:0.994]
Epoch [111/120    avg_loss:0.007, val_acc:0.994]
Epoch [112/120    avg_loss:0.007, val_acc:0.994]
Epoch [113/120    avg_loss:0.007, val_acc:0.994]
Epoch [114/120    avg_loss:0.007, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.994]
Epoch [116/120    avg_loss:0.006, val_acc:0.994]
Epoch [117/120    avg_loss:0.010, val_acc:0.994]
Epoch [118/120    avg_loss:0.006, val_acc:0.994]
Epoch [119/120    avg_loss:0.006, val_acc:0.994]
Epoch [120/120    avg_loss:0.005, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     1     0     0     5     0     0]
 [    0     0 18031     0    39     0    18     0     2     0]
 [    0     0     0  2026     4     0     0     0     3     3]
 [    0    57    17     0  2873     0     0     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4844     0     0    34]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     6     0     0    42     0     0     0  3508    15]
 [    0     0     0     1    14    30     0     0     0   874]]

Accuracy:
99.23842575856169

F1 scores:
[       nan 0.99465986 0.99789695 0.99729264 0.96652649 0.98863636
 0.99466119 0.99806576 0.98691799 0.94742547]

Kappa:
0.9899129694907014
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7102aaf7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.823, val_acc:0.349]
Epoch [2/120    avg_loss:1.220, val_acc:0.681]
Epoch [3/120    avg_loss:0.918, val_acc:0.667]
Epoch [4/120    avg_loss:0.709, val_acc:0.775]
Epoch [5/120    avg_loss:0.564, val_acc:0.752]
Epoch [6/120    avg_loss:0.477, val_acc:0.789]
Epoch [7/120    avg_loss:0.376, val_acc:0.817]
Epoch [8/120    avg_loss:0.354, val_acc:0.885]
Epoch [9/120    avg_loss:0.298, val_acc:0.866]
Epoch [10/120    avg_loss:0.220, val_acc:0.845]
Epoch [11/120    avg_loss:0.158, val_acc:0.933]
Epoch [12/120    avg_loss:0.211, val_acc:0.931]
Epoch [13/120    avg_loss:0.175, val_acc:0.952]
Epoch [14/120    avg_loss:0.133, val_acc:0.944]
Epoch [15/120    avg_loss:0.157, val_acc:0.897]
Epoch [16/120    avg_loss:0.118, val_acc:0.944]
Epoch [17/120    avg_loss:0.123, val_acc:0.961]
Epoch [18/120    avg_loss:0.106, val_acc:0.922]
Epoch [19/120    avg_loss:0.134, val_acc:0.935]
Epoch [20/120    avg_loss:0.130, val_acc:0.965]
Epoch [21/120    avg_loss:0.075, val_acc:0.956]
Epoch [22/120    avg_loss:0.084, val_acc:0.966]
Epoch [23/120    avg_loss:0.090, val_acc:0.951]
Epoch [24/120    avg_loss:0.081, val_acc:0.972]
Epoch [25/120    avg_loss:0.051, val_acc:0.982]
Epoch [26/120    avg_loss:0.051, val_acc:0.966]
Epoch [27/120    avg_loss:0.065, val_acc:0.972]
Epoch [28/120    avg_loss:0.065, val_acc:0.977]
Epoch [29/120    avg_loss:0.044, val_acc:0.977]
Epoch [30/120    avg_loss:0.026, val_acc:0.982]
Epoch [31/120    avg_loss:0.030, val_acc:0.972]
Epoch [32/120    avg_loss:0.030, val_acc:0.979]
Epoch [33/120    avg_loss:0.023, val_acc:0.987]
Epoch [34/120    avg_loss:0.019, val_acc:0.988]
Epoch [35/120    avg_loss:0.026, val_acc:0.974]
Epoch [36/120    avg_loss:0.027, val_acc:0.984]
Epoch [37/120    avg_loss:0.039, val_acc:0.986]
Epoch [38/120    avg_loss:0.042, val_acc:0.968]
Epoch [39/120    avg_loss:0.042, val_acc:0.964]
Epoch [40/120    avg_loss:0.038, val_acc:0.984]
Epoch [41/120    avg_loss:0.019, val_acc:0.984]
Epoch [42/120    avg_loss:0.023, val_acc:0.938]
Epoch [43/120    avg_loss:0.028, val_acc:0.975]
Epoch [44/120    avg_loss:0.020, val_acc:0.986]
Epoch [45/120    avg_loss:0.031, val_acc:0.981]
Epoch [46/120    avg_loss:0.030, val_acc:0.986]
Epoch [47/120    avg_loss:0.019, val_acc:0.988]
Epoch [48/120    avg_loss:0.028, val_acc:0.985]
Epoch [49/120    avg_loss:0.025, val_acc:0.984]
Epoch [50/120    avg_loss:0.016, val_acc:0.991]
Epoch [51/120    avg_loss:0.017, val_acc:0.977]
Epoch [52/120    avg_loss:0.012, val_acc:0.986]
Epoch [53/120    avg_loss:0.020, val_acc:0.987]
Epoch [54/120    avg_loss:0.016, val_acc:0.987]
Epoch [55/120    avg_loss:0.011, val_acc:0.985]
Epoch [56/120    avg_loss:0.010, val_acc:0.990]
Epoch [57/120    avg_loss:0.008, val_acc:0.989]
Epoch [58/120    avg_loss:0.007, val_acc:0.990]
Epoch [59/120    avg_loss:0.007, val_acc:0.990]
Epoch [60/120    avg_loss:0.009, val_acc:0.992]
Epoch [61/120    avg_loss:0.009, val_acc:0.973]
Epoch [62/120    avg_loss:0.010, val_acc:0.991]
Epoch [63/120    avg_loss:0.007, val_acc:0.990]
Epoch [64/120    avg_loss:0.022, val_acc:0.991]
Epoch [65/120    avg_loss:0.009, val_acc:0.990]
Epoch [66/120    avg_loss:0.012, val_acc:0.990]
Epoch [67/120    avg_loss:0.016, val_acc:0.995]
Epoch [68/120    avg_loss:0.009, val_acc:0.995]
Epoch [69/120    avg_loss:0.013, val_acc:0.994]
Epoch [70/120    avg_loss:0.009, val_acc:0.990]
Epoch [71/120    avg_loss:0.004, val_acc:0.991]
Epoch [72/120    avg_loss:0.005, val_acc:0.990]
Epoch [73/120    avg_loss:0.005, val_acc:0.989]
Epoch [74/120    avg_loss:0.005, val_acc:0.990]
Epoch [75/120    avg_loss:0.004, val_acc:0.992]
Epoch [76/120    avg_loss:0.011, val_acc:0.990]
Epoch [77/120    avg_loss:0.004, val_acc:0.994]
Epoch [78/120    avg_loss:0.004, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.993]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.004, val_acc:0.993]
Epoch [82/120    avg_loss:0.004, val_acc:0.993]
Epoch [83/120    avg_loss:0.003, val_acc:0.994]
Epoch [84/120    avg_loss:0.004, val_acc:0.993]
Epoch [85/120    avg_loss:0.004, val_acc:0.994]
Epoch [86/120    avg_loss:0.003, val_acc:0.994]
Epoch [87/120    avg_loss:0.004, val_acc:0.994]
Epoch [88/120    avg_loss:0.003, val_acc:0.994]
Epoch [89/120    avg_loss:0.004, val_acc:0.994]
Epoch [90/120    avg_loss:0.003, val_acc:0.993]
Epoch [91/120    avg_loss:0.003, val_acc:0.993]
Epoch [92/120    avg_loss:0.002, val_acc:0.993]
Epoch [93/120    avg_loss:0.003, val_acc:0.994]
Epoch [94/120    avg_loss:0.003, val_acc:0.994]
Epoch [95/120    avg_loss:0.003, val_acc:0.994]
Epoch [96/120    avg_loss:0.006, val_acc:0.993]
Epoch [97/120    avg_loss:0.004, val_acc:0.993]
Epoch [98/120    avg_loss:0.003, val_acc:0.993]
Epoch [99/120    avg_loss:0.003, val_acc:0.993]
Epoch [100/120    avg_loss:0.003, val_acc:0.993]
Epoch [101/120    avg_loss:0.003, val_acc:0.993]
Epoch [102/120    avg_loss:0.003, val_acc:0.993]
Epoch [103/120    avg_loss:0.003, val_acc:0.993]
Epoch [104/120    avg_loss:0.003, val_acc:0.993]
Epoch [105/120    avg_loss:0.003, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.993]
Epoch [107/120    avg_loss:0.003, val_acc:0.993]
Epoch [108/120    avg_loss:0.003, val_acc:0.993]
Epoch [109/120    avg_loss:0.003, val_acc:0.993]
Epoch [110/120    avg_loss:0.004, val_acc:0.993]
Epoch [111/120    avg_loss:0.003, val_acc:0.993]
Epoch [112/120    avg_loss:0.003, val_acc:0.993]
Epoch [113/120    avg_loss:0.003, val_acc:0.993]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.002, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.004, val_acc:0.993]
Epoch [118/120    avg_loss:0.004, val_acc:0.993]
Epoch [119/120    avg_loss:0.002, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6430     0     0     0     0     0     0     0     2]
 [    0     0 18044     0    44     0     2     0     0     0]
 [    0     0     0  2030     3     0     0     0     1     2]
 [    0    49    18     0  2876     0     1     0    27     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4868     0     0    10]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     3     0     7    30     0     0     0  3518    13]
 [    0     0     0     0    14    62     0     0     0   843]]

Accuracy:
99.3010869303256

F1 scores:
[       nan 0.99581849 0.9982297  0.99680825 0.96851322 0.97679641
 0.9985641  0.99961225 0.9886188  0.94189944]

Kappa:
0.9907409805092784
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c37879828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.790, val_acc:0.320]
Epoch [2/120    avg_loss:1.236, val_acc:0.396]
Epoch [3/120    avg_loss:0.949, val_acc:0.683]
Epoch [4/120    avg_loss:0.724, val_acc:0.777]
Epoch [5/120    avg_loss:0.533, val_acc:0.813]
Epoch [6/120    avg_loss:0.459, val_acc:0.855]
Epoch [7/120    avg_loss:0.339, val_acc:0.901]
Epoch [8/120    avg_loss:0.337, val_acc:0.864]
Epoch [9/120    avg_loss:0.272, val_acc:0.815]
Epoch [10/120    avg_loss:0.259, val_acc:0.909]
Epoch [11/120    avg_loss:0.218, val_acc:0.927]
Epoch [12/120    avg_loss:0.166, val_acc:0.911]
Epoch [13/120    avg_loss:0.141, val_acc:0.957]
Epoch [14/120    avg_loss:0.113, val_acc:0.922]
Epoch [15/120    avg_loss:0.110, val_acc:0.944]
Epoch [16/120    avg_loss:0.131, val_acc:0.919]
Epoch [17/120    avg_loss:0.161, val_acc:0.927]
Epoch [18/120    avg_loss:0.132, val_acc:0.941]
Epoch [19/120    avg_loss:0.099, val_acc:0.916]
Epoch [20/120    avg_loss:0.120, val_acc:0.924]
Epoch [21/120    avg_loss:0.094, val_acc:0.946]
Epoch [22/120    avg_loss:0.074, val_acc:0.964]
Epoch [23/120    avg_loss:0.086, val_acc:0.932]
Epoch [24/120    avg_loss:0.071, val_acc:0.967]
Epoch [25/120    avg_loss:0.047, val_acc:0.977]
Epoch [26/120    avg_loss:0.054, val_acc:0.965]
Epoch [27/120    avg_loss:0.046, val_acc:0.954]
Epoch [28/120    avg_loss:0.040, val_acc:0.971]
Epoch [29/120    avg_loss:0.047, val_acc:0.969]
Epoch [30/120    avg_loss:0.031, val_acc:0.976]
Epoch [31/120    avg_loss:0.032, val_acc:0.966]
Epoch [32/120    avg_loss:0.044, val_acc:0.981]
Epoch [33/120    avg_loss:0.047, val_acc:0.964]
Epoch [34/120    avg_loss:0.030, val_acc:0.979]
Epoch [35/120    avg_loss:0.026, val_acc:0.976]
Epoch [36/120    avg_loss:0.034, val_acc:0.972]
Epoch [37/120    avg_loss:0.033, val_acc:0.978]
Epoch [38/120    avg_loss:0.034, val_acc:0.970]
Epoch [39/120    avg_loss:0.026, val_acc:0.975]
Epoch [40/120    avg_loss:0.026, val_acc:0.970]
Epoch [41/120    avg_loss:0.020, val_acc:0.977]
Epoch [42/120    avg_loss:0.016, val_acc:0.966]
Epoch [43/120    avg_loss:0.021, val_acc:0.977]
Epoch [44/120    avg_loss:0.013, val_acc:0.984]
Epoch [45/120    avg_loss:0.021, val_acc:0.983]
Epoch [46/120    avg_loss:0.022, val_acc:0.975]
Epoch [47/120    avg_loss:0.039, val_acc:0.979]
Epoch [48/120    avg_loss:0.018, val_acc:0.983]
Epoch [49/120    avg_loss:0.025, val_acc:0.969]
Epoch [50/120    avg_loss:0.016, val_acc:0.978]
Epoch [51/120    avg_loss:0.022, val_acc:0.978]
Epoch [52/120    avg_loss:0.019, val_acc:0.981]
Epoch [53/120    avg_loss:0.022, val_acc:0.979]
Epoch [54/120    avg_loss:0.018, val_acc:0.978]
Epoch [55/120    avg_loss:0.008, val_acc:0.978]
Epoch [56/120    avg_loss:0.013, val_acc:0.986]
Epoch [57/120    avg_loss:0.012, val_acc:0.985]
Epoch [58/120    avg_loss:0.008, val_acc:0.984]
Epoch [59/120    avg_loss:0.010, val_acc:0.984]
Epoch [60/120    avg_loss:0.008, val_acc:0.984]
Epoch [61/120    avg_loss:0.010, val_acc:0.984]
Epoch [62/120    avg_loss:0.017, val_acc:0.971]
Epoch [63/120    avg_loss:0.020, val_acc:0.959]
Epoch [64/120    avg_loss:0.020, val_acc:0.983]
Epoch [65/120    avg_loss:0.014, val_acc:0.989]
Epoch [66/120    avg_loss:0.012, val_acc:0.985]
Epoch [67/120    avg_loss:0.008, val_acc:0.981]
Epoch [68/120    avg_loss:0.007, val_acc:0.984]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.078, val_acc:0.953]
Epoch [71/120    avg_loss:0.090, val_acc:0.955]
Epoch [72/120    avg_loss:0.055, val_acc:0.973]
Epoch [73/120    avg_loss:0.018, val_acc:0.980]
Epoch [74/120    avg_loss:0.017, val_acc:0.980]
Epoch [75/120    avg_loss:0.025, val_acc:0.973]
Epoch [76/120    avg_loss:0.018, val_acc:0.984]
Epoch [77/120    avg_loss:0.015, val_acc:0.972]
Epoch [78/120    avg_loss:0.012, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.984]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.005, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.985]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.019, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     9     0     0     0    27     1]
 [    0     0 18039     0    37     0    12     0     2     0]
 [    0     1     0  2031     3     0     0     0     0     1]
 [    0    38    18     0  2879     0    10     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4862     0     0    16]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     1     0     1    50     0     0     0  3519     0]
 [    0     0     0     0    15    83     0     0     0   821]]

Accuracy:
99.1492540910515

F1 scores:
[       nan 0.9940157  0.99809113 0.99852507 0.96529757 0.96917935
 0.99600533 0.99961225 0.98488665 0.93401593]

Kappa:
0.9887309317396371
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0698ca3780>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.730, val_acc:0.291]
Epoch [2/120    avg_loss:1.167, val_acc:0.603]
Epoch [3/120    avg_loss:0.910, val_acc:0.694]
Epoch [4/120    avg_loss:0.759, val_acc:0.694]
Epoch [5/120    avg_loss:0.591, val_acc:0.697]
Epoch [6/120    avg_loss:0.495, val_acc:0.774]
Epoch [7/120    avg_loss:0.391, val_acc:0.799]
Epoch [8/120    avg_loss:0.333, val_acc:0.837]
Epoch [9/120    avg_loss:0.308, val_acc:0.925]
Epoch [10/120    avg_loss:0.270, val_acc:0.856]
Epoch [11/120    avg_loss:0.249, val_acc:0.935]
Epoch [12/120    avg_loss:0.185, val_acc:0.942]
Epoch [13/120    avg_loss:0.191, val_acc:0.920]
Epoch [14/120    avg_loss:0.208, val_acc:0.931]
Epoch [15/120    avg_loss:0.162, val_acc:0.946]
Epoch [16/120    avg_loss:0.150, val_acc:0.953]
Epoch [17/120    avg_loss:0.116, val_acc:0.965]
Epoch [18/120    avg_loss:0.110, val_acc:0.951]
Epoch [19/120    avg_loss:0.081, val_acc:0.953]
Epoch [20/120    avg_loss:0.068, val_acc:0.944]
Epoch [21/120    avg_loss:0.070, val_acc:0.952]
Epoch [22/120    avg_loss:0.136, val_acc:0.959]
Epoch [23/120    avg_loss:0.066, val_acc:0.953]
Epoch [24/120    avg_loss:0.063, val_acc:0.947]
Epoch [25/120    avg_loss:0.033, val_acc:0.974]
Epoch [26/120    avg_loss:0.051, val_acc:0.974]
Epoch [27/120    avg_loss:0.069, val_acc:0.954]
Epoch [28/120    avg_loss:0.058, val_acc:0.966]
Epoch [29/120    avg_loss:0.053, val_acc:0.970]
Epoch [30/120    avg_loss:0.073, val_acc:0.932]
Epoch [31/120    avg_loss:0.037, val_acc:0.982]
Epoch [32/120    avg_loss:0.027, val_acc:0.979]
Epoch [33/120    avg_loss:0.040, val_acc:0.957]
Epoch [34/120    avg_loss:0.073, val_acc:0.975]
Epoch [35/120    avg_loss:0.041, val_acc:0.961]
Epoch [36/120    avg_loss:0.027, val_acc:0.984]
Epoch [37/120    avg_loss:0.023, val_acc:0.984]
Epoch [38/120    avg_loss:0.019, val_acc:0.984]
Epoch [39/120    avg_loss:0.016, val_acc:0.989]
Epoch [40/120    avg_loss:0.014, val_acc:0.984]
Epoch [41/120    avg_loss:0.014, val_acc:0.988]
Epoch [42/120    avg_loss:0.013, val_acc:0.985]
Epoch [43/120    avg_loss:0.012, val_acc:0.988]
Epoch [44/120    avg_loss:0.013, val_acc:0.989]
Epoch [45/120    avg_loss:0.009, val_acc:0.988]
Epoch [46/120    avg_loss:0.012, val_acc:0.989]
Epoch [47/120    avg_loss:0.014, val_acc:0.990]
Epoch [48/120    avg_loss:0.013, val_acc:0.989]
Epoch [49/120    avg_loss:0.020, val_acc:0.990]
Epoch [50/120    avg_loss:0.009, val_acc:0.987]
Epoch [51/120    avg_loss:0.010, val_acc:0.986]
Epoch [52/120    avg_loss:0.010, val_acc:0.990]
Epoch [53/120    avg_loss:0.006, val_acc:0.990]
Epoch [54/120    avg_loss:0.010, val_acc:0.987]
Epoch [55/120    avg_loss:0.013, val_acc:0.954]
Epoch [56/120    avg_loss:0.014, val_acc:0.988]
Epoch [57/120    avg_loss:0.023, val_acc:0.967]
Epoch [58/120    avg_loss:0.031, val_acc:0.979]
Epoch [59/120    avg_loss:0.012, val_acc:0.987]
Epoch [60/120    avg_loss:0.008, val_acc:0.989]
Epoch [61/120    avg_loss:0.010, val_acc:0.986]
Epoch [62/120    avg_loss:0.007, val_acc:0.984]
Epoch [63/120    avg_loss:0.007, val_acc:0.989]
Epoch [64/120    avg_loss:0.006, val_acc:0.986]
Epoch [65/120    avg_loss:0.009, val_acc:0.990]
Epoch [66/120    avg_loss:0.010, val_acc:0.979]
Epoch [67/120    avg_loss:0.007, val_acc:0.985]
Epoch [68/120    avg_loss:0.006, val_acc:0.987]
Epoch [69/120    avg_loss:0.005, val_acc:0.988]
Epoch [70/120    avg_loss:0.005, val_acc:0.989]
Epoch [71/120    avg_loss:0.004, val_acc:0.990]
Epoch [72/120    avg_loss:0.005, val_acc:0.990]
Epoch [73/120    avg_loss:0.004, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.990]
Epoch [75/120    avg_loss:0.004, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.990]
Epoch [77/120    avg_loss:0.006, val_acc:0.990]
Epoch [78/120    avg_loss:0.005, val_acc:0.989]
Epoch [79/120    avg_loss:0.004, val_acc:0.989]
Epoch [80/120    avg_loss:0.004, val_acc:0.989]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.005, val_acc:0.990]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.004, val_acc:0.990]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.003, val_acc:0.990]
Epoch [87/120    avg_loss:0.004, val_acc:0.990]
Epoch [88/120    avg_loss:0.006, val_acc:0.990]
Epoch [89/120    avg_loss:0.005, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     8     0     0     6    64     0     4]
 [    0     0 18050     0    39     0     1     0     0     0]
 [    0     4     0  2028     2     0     0     0     0     2]
 [    0    41    18     0  2879     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4856     0    10     8]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     2     0     3    52     0     0     0  3514     0]
 [    0     0     0     0    15    30     0     0     0   874]]

Accuracy:
99.15648422625503

F1 scores:
[       nan 0.98994466 0.99828549 0.99533742 0.96626951 0.98863636
 0.99600041 0.97463082 0.98680146 0.9673492 ]

Kappa:
0.9888272619439976
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdbfcdfd780>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.743, val_acc:0.391]
Epoch [2/120    avg_loss:1.210, val_acc:0.721]
Epoch [3/120    avg_loss:0.928, val_acc:0.575]
Epoch [4/120    avg_loss:0.701, val_acc:0.747]
Epoch [5/120    avg_loss:0.480, val_acc:0.845]
Epoch [6/120    avg_loss:0.407, val_acc:0.903]
Epoch [7/120    avg_loss:0.322, val_acc:0.931]
Epoch [8/120    avg_loss:0.321, val_acc:0.898]
Epoch [9/120    avg_loss:0.274, val_acc:0.901]
Epoch [10/120    avg_loss:0.183, val_acc:0.865]
Epoch [11/120    avg_loss:0.169, val_acc:0.899]
Epoch [12/120    avg_loss:0.198, val_acc:0.872]
Epoch [13/120    avg_loss:0.164, val_acc:0.931]
Epoch [14/120    avg_loss:0.112, val_acc:0.929]
Epoch [15/120    avg_loss:0.127, val_acc:0.944]
Epoch [16/120    avg_loss:0.116, val_acc:0.955]
Epoch [17/120    avg_loss:0.108, val_acc:0.965]
Epoch [18/120    avg_loss:0.141, val_acc:0.957]
Epoch [19/120    avg_loss:0.109, val_acc:0.961]
Epoch [20/120    avg_loss:0.095, val_acc:0.971]
Epoch [21/120    avg_loss:0.101, val_acc:0.952]
Epoch [22/120    avg_loss:0.116, val_acc:0.929]
Epoch [23/120    avg_loss:0.090, val_acc:0.953]
Epoch [24/120    avg_loss:0.092, val_acc:0.970]
Epoch [25/120    avg_loss:0.056, val_acc:0.962]
Epoch [26/120    avg_loss:0.047, val_acc:0.964]
Epoch [27/120    avg_loss:0.039, val_acc:0.970]
Epoch [28/120    avg_loss:0.042, val_acc:0.978]
Epoch [29/120    avg_loss:0.026, val_acc:0.976]
Epoch [30/120    avg_loss:0.033, val_acc:0.980]
Epoch [31/120    avg_loss:0.026, val_acc:0.977]
Epoch [32/120    avg_loss:0.018, val_acc:0.972]
Epoch [33/120    avg_loss:0.027, val_acc:0.977]
Epoch [34/120    avg_loss:0.054, val_acc:0.965]
Epoch [35/120    avg_loss:0.043, val_acc:0.948]
Epoch [36/120    avg_loss:0.046, val_acc:0.967]
Epoch [37/120    avg_loss:0.033, val_acc:0.981]
Epoch [38/120    avg_loss:0.030, val_acc:0.981]
Epoch [39/120    avg_loss:0.021, val_acc:0.962]
Epoch [40/120    avg_loss:0.024, val_acc:0.979]
Epoch [41/120    avg_loss:0.020, val_acc:0.983]
Epoch [42/120    avg_loss:0.013, val_acc:0.980]
Epoch [43/120    avg_loss:0.016, val_acc:0.979]
Epoch [44/120    avg_loss:0.082, val_acc:0.958]
Epoch [45/120    avg_loss:0.075, val_acc:0.978]
Epoch [46/120    avg_loss:0.045, val_acc:0.965]
Epoch [47/120    avg_loss:0.040, val_acc:0.989]
Epoch [48/120    avg_loss:0.018, val_acc:0.982]
Epoch [49/120    avg_loss:0.021, val_acc:0.982]
Epoch [50/120    avg_loss:0.018, val_acc:0.984]
Epoch [51/120    avg_loss:0.019, val_acc:0.963]
Epoch [52/120    avg_loss:0.014, val_acc:0.978]
Epoch [53/120    avg_loss:0.010, val_acc:0.984]
Epoch [54/120    avg_loss:0.011, val_acc:0.989]
Epoch [55/120    avg_loss:0.012, val_acc:0.971]
Epoch [56/120    avg_loss:0.026, val_acc:0.977]
Epoch [57/120    avg_loss:0.018, val_acc:0.988]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.028, val_acc:0.979]
Epoch [60/120    avg_loss:0.035, val_acc:0.976]
Epoch [61/120    avg_loss:0.021, val_acc:0.976]
Epoch [62/120    avg_loss:0.028, val_acc:0.989]
Epoch [63/120    avg_loss:0.017, val_acc:0.983]
Epoch [64/120    avg_loss:0.016, val_acc:0.956]
Epoch [65/120    avg_loss:0.016, val_acc:0.984]
Epoch [66/120    avg_loss:0.007, val_acc:0.989]
Epoch [67/120    avg_loss:0.005, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.989]
Epoch [70/120    avg_loss:0.007, val_acc:0.989]
Epoch [71/120    avg_loss:0.005, val_acc:0.990]
Epoch [72/120    avg_loss:0.012, val_acc:0.956]
Epoch [73/120    avg_loss:0.026, val_acc:0.977]
Epoch [74/120    avg_loss:0.016, val_acc:0.984]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.015, val_acc:0.948]
Epoch [77/120    avg_loss:0.015, val_acc:0.990]
Epoch [78/120    avg_loss:0.007, val_acc:0.990]
Epoch [79/120    avg_loss:0.008, val_acc:0.990]
Epoch [80/120    avg_loss:0.008, val_acc:0.990]
Epoch [81/120    avg_loss:0.006, val_acc:0.990]
Epoch [82/120    avg_loss:0.003, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.003, val_acc:0.990]
Epoch [89/120    avg_loss:0.003, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.003, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.003, val_acc:0.990]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.003, val_acc:0.989]
Epoch [99/120    avg_loss:0.003, val_acc:0.990]
Epoch [100/120    avg_loss:0.003, val_acc:0.990]
Epoch [101/120    avg_loss:0.004, val_acc:0.990]
Epoch [102/120    avg_loss:0.002, val_acc:0.989]
Epoch [103/120    avg_loss:0.003, val_acc:0.989]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.002, val_acc:0.988]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.003, val_acc:0.989]
Epoch [110/120    avg_loss:0.002, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.002, val_acc:0.990]
Epoch [113/120    avg_loss:0.002, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.002, val_acc:0.990]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.002, val_acc:0.990]
Epoch [118/120    avg_loss:0.002, val_acc:0.990]
Epoch [119/120    avg_loss:0.002, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     0     2     0     0     9     0     0]
 [    0    13 18066     0    11     0     0     0     0     0]
 [    0     0     0  2032     1     0     0     0     1     2]
 [    0    26    14     0  2899     0     2     0    28     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0     0    18]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     1     0     0    48     0     0     0  3497    25]
 [    0     0     0     0    14    35     0     1     0   869]]

Accuracy:
99.37820837249657

F1 scores:
[       nan 0.99604437 0.99894941 0.99901672 0.97494535 0.98676749
 0.99753695 0.99458623 0.98548683 0.94662309]

Kappa:
0.9917619077530031
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0060ee828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.717, val_acc:0.580]
Epoch [2/120    avg_loss:1.166, val_acc:0.657]
Epoch [3/120    avg_loss:0.935, val_acc:0.680]
Epoch [4/120    avg_loss:0.706, val_acc:0.717]
Epoch [5/120    avg_loss:0.562, val_acc:0.802]
Epoch [6/120    avg_loss:0.549, val_acc:0.798]
Epoch [7/120    avg_loss:0.398, val_acc:0.907]
Epoch [8/120    avg_loss:0.319, val_acc:0.859]
Epoch [9/120    avg_loss:0.279, val_acc:0.935]
Epoch [10/120    avg_loss:0.209, val_acc:0.910]
Epoch [11/120    avg_loss:0.212, val_acc:0.934]
Epoch [12/120    avg_loss:0.247, val_acc:0.905]
Epoch [13/120    avg_loss:0.192, val_acc:0.966]
Epoch [14/120    avg_loss:0.172, val_acc:0.940]
Epoch [15/120    avg_loss:0.137, val_acc:0.945]
Epoch [16/120    avg_loss:0.142, val_acc:0.970]
Epoch [17/120    avg_loss:0.084, val_acc:0.978]
Epoch [18/120    avg_loss:0.062, val_acc:0.973]
Epoch [19/120    avg_loss:0.084, val_acc:0.977]
Epoch [20/120    avg_loss:0.076, val_acc:0.960]
Epoch [21/120    avg_loss:0.059, val_acc:0.982]
Epoch [22/120    avg_loss:0.051, val_acc:0.975]
Epoch [23/120    avg_loss:0.051, val_acc:0.976]
Epoch [24/120    avg_loss:0.062, val_acc:0.973]
Epoch [25/120    avg_loss:0.059, val_acc:0.978]
Epoch [26/120    avg_loss:0.048, val_acc:0.975]
Epoch [27/120    avg_loss:0.041, val_acc:0.984]
Epoch [28/120    avg_loss:0.061, val_acc:0.969]
Epoch [29/120    avg_loss:0.044, val_acc:0.968]
Epoch [30/120    avg_loss:0.038, val_acc:0.977]
Epoch [31/120    avg_loss:0.051, val_acc:0.978]
Epoch [32/120    avg_loss:0.078, val_acc:0.966]
Epoch [33/120    avg_loss:0.043, val_acc:0.989]
Epoch [34/120    avg_loss:0.034, val_acc:0.987]
Epoch [35/120    avg_loss:0.016, val_acc:0.989]
Epoch [36/120    avg_loss:0.019, val_acc:0.977]
Epoch [37/120    avg_loss:0.024, val_acc:0.972]
Epoch [38/120    avg_loss:0.020, val_acc:0.987]
Epoch [39/120    avg_loss:0.020, val_acc:0.988]
Epoch [40/120    avg_loss:0.016, val_acc:0.984]
Epoch [41/120    avg_loss:0.017, val_acc:0.979]
Epoch [42/120    avg_loss:0.024, val_acc:0.987]
Epoch [43/120    avg_loss:0.017, val_acc:0.990]
Epoch [44/120    avg_loss:0.014, val_acc:0.990]
Epoch [45/120    avg_loss:0.020, val_acc:0.987]
Epoch [46/120    avg_loss:0.015, val_acc:0.991]
Epoch [47/120    avg_loss:0.011, val_acc:0.989]
Epoch [48/120    avg_loss:0.013, val_acc:0.990]
Epoch [49/120    avg_loss:0.010, val_acc:0.989]
Epoch [50/120    avg_loss:0.009, val_acc:0.991]
Epoch [51/120    avg_loss:0.009, val_acc:0.990]
Epoch [52/120    avg_loss:0.010, val_acc:0.990]
Epoch [53/120    avg_loss:0.009, val_acc:0.993]
Epoch [54/120    avg_loss:0.009, val_acc:0.990]
Epoch [55/120    avg_loss:0.010, val_acc:0.993]
Epoch [56/120    avg_loss:0.011, val_acc:0.990]
Epoch [57/120    avg_loss:0.009, val_acc:0.990]
Epoch [58/120    avg_loss:0.007, val_acc:0.988]
Epoch [59/120    avg_loss:0.006, val_acc:0.991]
Epoch [60/120    avg_loss:0.007, val_acc:0.991]
Epoch [61/120    avg_loss:0.006, val_acc:0.989]
Epoch [62/120    avg_loss:0.010, val_acc:0.988]
Epoch [63/120    avg_loss:0.008, val_acc:0.980]
Epoch [64/120    avg_loss:0.012, val_acc:0.988]
Epoch [65/120    avg_loss:0.011, val_acc:0.986]
Epoch [66/120    avg_loss:0.005, val_acc:0.991]
Epoch [67/120    avg_loss:0.005, val_acc:0.989]
Epoch [68/120    avg_loss:0.007, val_acc:0.990]
Epoch [69/120    avg_loss:0.006, val_acc:0.990]
Epoch [70/120    avg_loss:0.005, val_acc:0.990]
Epoch [71/120    avg_loss:0.006, val_acc:0.990]
Epoch [72/120    avg_loss:0.004, val_acc:0.991]
Epoch [73/120    avg_loss:0.006, val_acc:0.991]
Epoch [74/120    avg_loss:0.004, val_acc:0.990]
Epoch [75/120    avg_loss:0.004, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.990]
Epoch [77/120    avg_loss:0.004, val_acc:0.990]
Epoch [78/120    avg_loss:0.004, val_acc:0.991]
Epoch [79/120    avg_loss:0.004, val_acc:0.991]
Epoch [80/120    avg_loss:0.003, val_acc:0.991]
Epoch [81/120    avg_loss:0.005, val_acc:0.991]
Epoch [82/120    avg_loss:0.003, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.004, val_acc:0.991]
Epoch [85/120    avg_loss:0.003, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.004, val_acc:0.991]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.003, val_acc:0.991]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.004, val_acc:0.991]
Epoch [92/120    avg_loss:0.003, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.003, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.003, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.003, val_acc:0.991]
Epoch [100/120    avg_loss:0.003, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.003, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.003, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.003, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     2     0     3    13     0     0]
 [    0     0 18074     0    13     0     2     0     1     0]
 [    0     7     0  2006     0     0     0     0    23     0]
 [    0    48    19     0  2878     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     3     0     0  4872     0     0     3]
 [    0     0     0     0     0     0     5  1285     0     0]
 [    0     7     0     0    37     0     0     0  3524     3]
 [    0     0     0     0    14    57     0     0     0   848]]

Accuracy:
99.30831706552912

F1 scores:
[       nan 0.99380229 0.99903269 0.99184178 0.9729547  0.97862767
 0.99836066 0.99304482 0.98628603 0.95657078]

Kappa:
0.9908329640638418
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1714afb828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.649, val_acc:0.387]
Epoch [2/120    avg_loss:1.037, val_acc:0.492]
Epoch [3/120    avg_loss:0.778, val_acc:0.738]
Epoch [4/120    avg_loss:0.590, val_acc:0.666]
Epoch [5/120    avg_loss:0.471, val_acc:0.701]
Epoch [6/120    avg_loss:0.424, val_acc:0.768]
Epoch [7/120    avg_loss:0.352, val_acc:0.780]
Epoch [8/120    avg_loss:0.370, val_acc:0.788]
Epoch [9/120    avg_loss:0.305, val_acc:0.827]
Epoch [10/120    avg_loss:0.244, val_acc:0.829]
Epoch [11/120    avg_loss:0.205, val_acc:0.833]
Epoch [12/120    avg_loss:0.199, val_acc:0.861]
Epoch [13/120    avg_loss:0.185, val_acc:0.891]
Epoch [14/120    avg_loss:0.189, val_acc:0.851]
Epoch [15/120    avg_loss:0.140, val_acc:0.911]
Epoch [16/120    avg_loss:0.138, val_acc:0.923]
Epoch [17/120    avg_loss:0.158, val_acc:0.943]
Epoch [18/120    avg_loss:0.104, val_acc:0.927]
Epoch [19/120    avg_loss:0.110, val_acc:0.948]
Epoch [20/120    avg_loss:0.104, val_acc:0.943]
Epoch [21/120    avg_loss:0.077, val_acc:0.960]
Epoch [22/120    avg_loss:0.067, val_acc:0.954]
Epoch [23/120    avg_loss:0.086, val_acc:0.919]
Epoch [24/120    avg_loss:0.098, val_acc:0.932]
Epoch [25/120    avg_loss:0.101, val_acc:0.953]
Epoch [26/120    avg_loss:0.071, val_acc:0.965]
Epoch [27/120    avg_loss:0.062, val_acc:0.968]
Epoch [28/120    avg_loss:0.052, val_acc:0.971]
Epoch [29/120    avg_loss:0.048, val_acc:0.975]
Epoch [30/120    avg_loss:0.048, val_acc:0.962]
Epoch [31/120    avg_loss:0.033, val_acc:0.977]
Epoch [32/120    avg_loss:0.036, val_acc:0.927]
Epoch [33/120    avg_loss:0.050, val_acc:0.960]
Epoch [34/120    avg_loss:0.031, val_acc:0.971]
Epoch [35/120    avg_loss:0.038, val_acc:0.966]
Epoch [36/120    avg_loss:0.030, val_acc:0.954]
Epoch [37/120    avg_loss:0.036, val_acc:0.936]
Epoch [38/120    avg_loss:0.066, val_acc:0.916]
Epoch [39/120    avg_loss:0.059, val_acc:0.970]
Epoch [40/120    avg_loss:0.059, val_acc:0.965]
Epoch [41/120    avg_loss:0.049, val_acc:0.974]
Epoch [42/120    avg_loss:0.021, val_acc:0.971]
Epoch [43/120    avg_loss:0.022, val_acc:0.976]
Epoch [44/120    avg_loss:0.015, val_acc:0.979]
Epoch [45/120    avg_loss:0.024, val_acc:0.981]
Epoch [46/120    avg_loss:0.012, val_acc:0.976]
Epoch [47/120    avg_loss:0.011, val_acc:0.981]
Epoch [48/120    avg_loss:0.014, val_acc:0.980]
Epoch [49/120    avg_loss:0.025, val_acc:0.953]
Epoch [50/120    avg_loss:0.026, val_acc:0.972]
Epoch [51/120    avg_loss:0.008, val_acc:0.976]
Epoch [52/120    avg_loss:0.007, val_acc:0.977]
Epoch [53/120    avg_loss:0.013, val_acc:0.972]
Epoch [54/120    avg_loss:0.013, val_acc:0.980]
Epoch [55/120    avg_loss:0.013, val_acc:0.972]
Epoch [56/120    avg_loss:0.008, val_acc:0.985]
Epoch [57/120    avg_loss:0.020, val_acc:0.960]
Epoch [58/120    avg_loss:0.015, val_acc:0.982]
Epoch [59/120    avg_loss:0.016, val_acc:0.960]
Epoch [60/120    avg_loss:0.012, val_acc:0.967]
Epoch [61/120    avg_loss:0.007, val_acc:0.986]
Epoch [62/120    avg_loss:0.011, val_acc:0.979]
Epoch [63/120    avg_loss:0.014, val_acc:0.969]
Epoch [64/120    avg_loss:0.014, val_acc:0.964]
Epoch [65/120    avg_loss:0.012, val_acc:0.980]
Epoch [66/120    avg_loss:0.011, val_acc:0.980]
Epoch [67/120    avg_loss:0.007, val_acc:0.981]
Epoch [68/120    avg_loss:0.007, val_acc:0.983]
Epoch [69/120    avg_loss:0.004, val_acc:0.981]
Epoch [70/120    avg_loss:0.005, val_acc:0.983]
Epoch [71/120    avg_loss:0.008, val_acc:0.980]
Epoch [72/120    avg_loss:0.004, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.978]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.005, val_acc:0.984]
Epoch [76/120    avg_loss:0.005, val_acc:0.985]
Epoch [77/120    avg_loss:0.004, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.982]
Epoch [80/120    avg_loss:0.004, val_acc:0.982]
Epoch [81/120    avg_loss:0.004, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.983]
Epoch [83/120    avg_loss:0.003, val_acc:0.983]
Epoch [84/120    avg_loss:0.004, val_acc:0.983]
Epoch [85/120    avg_loss:0.004, val_acc:0.983]
Epoch [86/120    avg_loss:0.004, val_acc:0.982]
Epoch [87/120    avg_loss:0.004, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.983]
Epoch [90/120    avg_loss:0.004, val_acc:0.983]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.003, val_acc:0.983]
Epoch [93/120    avg_loss:0.005, val_acc:0.983]
Epoch [94/120    avg_loss:0.003, val_acc:0.983]
Epoch [95/120    avg_loss:0.003, val_acc:0.984]
Epoch [96/120    avg_loss:0.003, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.983]
Epoch [99/120    avg_loss:0.005, val_acc:0.983]
Epoch [100/120    avg_loss:0.004, val_acc:0.983]
Epoch [101/120    avg_loss:0.003, val_acc:0.983]
Epoch [102/120    avg_loss:0.004, val_acc:0.983]
Epoch [103/120    avg_loss:0.003, val_acc:0.983]
Epoch [104/120    avg_loss:0.004, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.004, val_acc:0.983]
Epoch [108/120    avg_loss:0.004, val_acc:0.983]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.003, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.003, val_acc:0.983]
Epoch [120/120    avg_loss:0.003, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     1     0     0     1     0    29     0]
 [    0     0 18062     0    18     0     4     0     6     0]
 [    0     3     0  1932     0     0     0     0   100     1]
 [    0    24     1     0  2934     0     7     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4869     0     0     0]
 [    0     5     0     0     0     9     0  1275     0     1]
 [    0    39     0    53    10     0     0     0  3469     0]
 [    0     0     0     0     2    10     0     0     0   907]]

Accuracy:
99.1829947220013

F1 scores:
[       nan 0.99209547 0.99894917 0.96071606 0.98854447 0.99277292
 0.99784814 0.99415205 0.9661607  0.99234136]

Kappa:
0.9891745511384292
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc756ec87f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.689, val_acc:0.655]
Epoch [2/120    avg_loss:1.067, val_acc:0.709]
Epoch [3/120    avg_loss:0.828, val_acc:0.729]
Epoch [4/120    avg_loss:0.624, val_acc:0.640]
Epoch [5/120    avg_loss:0.490, val_acc:0.729]
Epoch [6/120    avg_loss:0.411, val_acc:0.710]
Epoch [7/120    avg_loss:0.383, val_acc:0.756]
Epoch [8/120    avg_loss:0.339, val_acc:0.812]
Epoch [9/120    avg_loss:0.339, val_acc:0.834]
Epoch [10/120    avg_loss:0.280, val_acc:0.849]
Epoch [11/120    avg_loss:0.242, val_acc:0.894]
Epoch [12/120    avg_loss:0.194, val_acc:0.904]
Epoch [13/120    avg_loss:0.201, val_acc:0.887]
Epoch [14/120    avg_loss:0.180, val_acc:0.893]
Epoch [15/120    avg_loss:0.167, val_acc:0.902]
Epoch [16/120    avg_loss:0.166, val_acc:0.922]
Epoch [17/120    avg_loss:0.117, val_acc:0.895]
Epoch [18/120    avg_loss:0.147, val_acc:0.898]
Epoch [19/120    avg_loss:0.131, val_acc:0.943]
Epoch [20/120    avg_loss:0.106, val_acc:0.796]
Epoch [21/120    avg_loss:0.114, val_acc:0.934]
Epoch [22/120    avg_loss:0.109, val_acc:0.948]
Epoch [23/120    avg_loss:0.096, val_acc:0.899]
Epoch [24/120    avg_loss:0.084, val_acc:0.934]
Epoch [25/120    avg_loss:0.061, val_acc:0.930]
Epoch [26/120    avg_loss:0.047, val_acc:0.966]
Epoch [27/120    avg_loss:0.046, val_acc:0.957]
Epoch [28/120    avg_loss:0.048, val_acc:0.958]
Epoch [29/120    avg_loss:0.040, val_acc:0.949]
Epoch [30/120    avg_loss:0.048, val_acc:0.941]
Epoch [31/120    avg_loss:0.075, val_acc:0.901]
Epoch [32/120    avg_loss:0.048, val_acc:0.951]
Epoch [33/120    avg_loss:0.053, val_acc:0.963]
Epoch [34/120    avg_loss:0.034, val_acc:0.967]
Epoch [35/120    avg_loss:0.062, val_acc:0.941]
Epoch [36/120    avg_loss:0.085, val_acc:0.951]
Epoch [37/120    avg_loss:0.067, val_acc:0.945]
Epoch [38/120    avg_loss:0.051, val_acc:0.960]
Epoch [39/120    avg_loss:0.043, val_acc:0.965]
Epoch [40/120    avg_loss:0.042, val_acc:0.954]
Epoch [41/120    avg_loss:0.055, val_acc:0.943]
Epoch [42/120    avg_loss:0.039, val_acc:0.953]
Epoch [43/120    avg_loss:0.043, val_acc:0.955]
Epoch [44/120    avg_loss:0.030, val_acc:0.968]
Epoch [45/120    avg_loss:0.038, val_acc:0.956]
Epoch [46/120    avg_loss:0.016, val_acc:0.965]
Epoch [47/120    avg_loss:0.016, val_acc:0.970]
Epoch [48/120    avg_loss:0.017, val_acc:0.971]
Epoch [49/120    avg_loss:0.033, val_acc:0.965]
Epoch [50/120    avg_loss:0.022, val_acc:0.976]
Epoch [51/120    avg_loss:0.014, val_acc:0.972]
Epoch [52/120    avg_loss:0.026, val_acc:0.959]
Epoch [53/120    avg_loss:0.026, val_acc:0.945]
Epoch [54/120    avg_loss:0.019, val_acc:0.980]
Epoch [55/120    avg_loss:0.022, val_acc:0.970]
Epoch [56/120    avg_loss:0.034, val_acc:0.969]
Epoch [57/120    avg_loss:0.020, val_acc:0.968]
Epoch [58/120    avg_loss:0.032, val_acc:0.965]
Epoch [59/120    avg_loss:0.020, val_acc:0.971]
Epoch [60/120    avg_loss:0.016, val_acc:0.954]
Epoch [61/120    avg_loss:0.021, val_acc:0.962]
Epoch [62/120    avg_loss:0.027, val_acc:0.977]
Epoch [63/120    avg_loss:0.021, val_acc:0.975]
Epoch [64/120    avg_loss:0.010, val_acc:0.974]
Epoch [65/120    avg_loss:0.008, val_acc:0.979]
Epoch [66/120    avg_loss:0.006, val_acc:0.985]
Epoch [67/120    avg_loss:0.006, val_acc:0.986]
Epoch [68/120    avg_loss:0.007, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.978]
Epoch [70/120    avg_loss:0.007, val_acc:0.974]
Epoch [71/120    avg_loss:0.007, val_acc:0.975]
Epoch [72/120    avg_loss:0.008, val_acc:0.981]
Epoch [73/120    avg_loss:0.008, val_acc:0.979]
Epoch [74/120    avg_loss:0.009, val_acc:0.981]
Epoch [75/120    avg_loss:0.006, val_acc:0.979]
Epoch [76/120    avg_loss:0.005, val_acc:0.981]
Epoch [77/120    avg_loss:0.005, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.981]
Epoch [79/120    avg_loss:0.013, val_acc:0.976]
Epoch [80/120    avg_loss:0.029, val_acc:0.971]
Epoch [81/120    avg_loss:0.026, val_acc:0.973]
Epoch [82/120    avg_loss:0.012, val_acc:0.978]
Epoch [83/120    avg_loss:0.008, val_acc:0.980]
Epoch [84/120    avg_loss:0.012, val_acc:0.981]
Epoch [85/120    avg_loss:0.016, val_acc:0.950]
Epoch [86/120    avg_loss:0.017, val_acc:0.974]
Epoch [87/120    avg_loss:0.013, val_acc:0.977]
Epoch [88/120    avg_loss:0.005, val_acc:0.979]
Epoch [89/120    avg_loss:0.004, val_acc:0.981]
Epoch [90/120    avg_loss:0.004, val_acc:0.984]
Epoch [91/120    avg_loss:0.005, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.003, val_acc:0.986]
Epoch [95/120    avg_loss:0.003, val_acc:0.983]
Epoch [96/120    avg_loss:0.003, val_acc:0.983]
Epoch [97/120    avg_loss:0.003, val_acc:0.983]
Epoch [98/120    avg_loss:0.003, val_acc:0.983]
Epoch [99/120    avg_loss:0.003, val_acc:0.982]
Epoch [100/120    avg_loss:0.004, val_acc:0.981]
Epoch [101/120    avg_loss:0.005, val_acc:0.983]
Epoch [102/120    avg_loss:0.004, val_acc:0.981]
Epoch [103/120    avg_loss:0.002, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.986]
Epoch [116/120    avg_loss:0.003, val_acc:0.986]
Epoch [117/120    avg_loss:0.002, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.002, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6336     0     5     0     0     0     0    81    10]
 [    0     2 18041     0    34     0     7     0     6     0]
 [    0     0     0  1953     0     0     0     0    82     1]
 [    0    26     5     3  2925     0     8     0     3     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0    19     0  4855     0     0     0]
 [    0     6     0     0     0     0     0  1284     0     0]
 [    0    37     0    90    33     0     3     0  3408     0]
 [    0     0     0     0     2     5     0     0     0   912]]

Accuracy:
98.85763863784253

F1 scores:
[       nan 0.98699276 0.99839513 0.95571324 0.97744361 0.99808795
 0.9957953  0.997669   0.95315341 0.98915401]

Kappa:
0.9848714047923287
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60b2f617b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.717, val_acc:0.313]
Epoch [2/120    avg_loss:1.043, val_acc:0.394]
Epoch [3/120    avg_loss:0.833, val_acc:0.719]
Epoch [4/120    avg_loss:0.681, val_acc:0.630]
Epoch [5/120    avg_loss:0.529, val_acc:0.739]
Epoch [6/120    avg_loss:0.482, val_acc:0.651]
Epoch [7/120    avg_loss:0.373, val_acc:0.777]
Epoch [8/120    avg_loss:0.337, val_acc:0.800]
Epoch [9/120    avg_loss:0.297, val_acc:0.867]
Epoch [10/120    avg_loss:0.292, val_acc:0.845]
Epoch [11/120    avg_loss:0.242, val_acc:0.859]
Epoch [12/120    avg_loss:0.199, val_acc:0.850]
Epoch [13/120    avg_loss:0.193, val_acc:0.939]
Epoch [14/120    avg_loss:0.172, val_acc:0.908]
Epoch [15/120    avg_loss:0.131, val_acc:0.932]
Epoch [16/120    avg_loss:0.110, val_acc:0.919]
Epoch [17/120    avg_loss:0.148, val_acc:0.923]
Epoch [18/120    avg_loss:0.102, val_acc:0.938]
Epoch [19/120    avg_loss:0.104, val_acc:0.896]
Epoch [20/120    avg_loss:0.118, val_acc:0.914]
Epoch [21/120    avg_loss:0.139, val_acc:0.914]
Epoch [22/120    avg_loss:0.113, val_acc:0.916]
Epoch [23/120    avg_loss:0.067, val_acc:0.959]
Epoch [24/120    avg_loss:0.078, val_acc:0.963]
Epoch [25/120    avg_loss:0.092, val_acc:0.934]
Epoch [26/120    avg_loss:0.083, val_acc:0.966]
Epoch [27/120    avg_loss:0.088, val_acc:0.946]
Epoch [28/120    avg_loss:0.045, val_acc:0.938]
Epoch [29/120    avg_loss:0.056, val_acc:0.955]
Epoch [30/120    avg_loss:0.069, val_acc:0.948]
Epoch [31/120    avg_loss:0.038, val_acc:0.941]
Epoch [32/120    avg_loss:0.064, val_acc:0.964]
Epoch [33/120    avg_loss:0.046, val_acc:0.950]
Epoch [34/120    avg_loss:0.050, val_acc:0.961]
Epoch [35/120    avg_loss:0.033, val_acc:0.951]
Epoch [36/120    avg_loss:0.038, val_acc:0.968]
Epoch [37/120    avg_loss:0.033, val_acc:0.942]
Epoch [38/120    avg_loss:0.048, val_acc:0.957]
Epoch [39/120    avg_loss:0.040, val_acc:0.951]
Epoch [40/120    avg_loss:0.040, val_acc:0.965]
Epoch [41/120    avg_loss:0.033, val_acc:0.974]
Epoch [42/120    avg_loss:0.047, val_acc:0.951]
Epoch [43/120    avg_loss:0.044, val_acc:0.946]
Epoch [44/120    avg_loss:0.046, val_acc:0.965]
Epoch [45/120    avg_loss:0.021, val_acc:0.965]
Epoch [46/120    avg_loss:0.039, val_acc:0.953]
Epoch [47/120    avg_loss:0.060, val_acc:0.958]
Epoch [48/120    avg_loss:0.041, val_acc:0.973]
Epoch [49/120    avg_loss:0.031, val_acc:0.952]
Epoch [50/120    avg_loss:0.027, val_acc:0.957]
Epoch [51/120    avg_loss:0.017, val_acc:0.971]
Epoch [52/120    avg_loss:0.020, val_acc:0.961]
Epoch [53/120    avg_loss:0.018, val_acc:0.965]
Epoch [54/120    avg_loss:0.012, val_acc:0.974]
Epoch [55/120    avg_loss:0.011, val_acc:0.975]
Epoch [56/120    avg_loss:0.020, val_acc:0.970]
Epoch [57/120    avg_loss:0.018, val_acc:0.968]
Epoch [58/120    avg_loss:0.016, val_acc:0.972]
Epoch [59/120    avg_loss:0.009, val_acc:0.979]
Epoch [60/120    avg_loss:0.009, val_acc:0.977]
Epoch [61/120    avg_loss:0.028, val_acc:0.968]
Epoch [62/120    avg_loss:0.015, val_acc:0.980]
Epoch [63/120    avg_loss:0.011, val_acc:0.974]
Epoch [64/120    avg_loss:0.010, val_acc:0.975]
Epoch [65/120    avg_loss:0.013, val_acc:0.971]
Epoch [66/120    avg_loss:0.014, val_acc:0.974]
Epoch [67/120    avg_loss:0.012, val_acc:0.982]
Epoch [68/120    avg_loss:0.018, val_acc:0.970]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.008, val_acc:0.981]
Epoch [71/120    avg_loss:0.010, val_acc:0.981]
Epoch [72/120    avg_loss:0.013, val_acc:0.977]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.015, val_acc:0.979]
Epoch [75/120    avg_loss:0.006, val_acc:0.982]
Epoch [76/120    avg_loss:0.007, val_acc:0.971]
Epoch [77/120    avg_loss:0.014, val_acc:0.971]
Epoch [78/120    avg_loss:0.006, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.974]
Epoch [80/120    avg_loss:0.005, val_acc:0.979]
Epoch [81/120    avg_loss:0.006, val_acc:0.977]
Epoch [82/120    avg_loss:0.009, val_acc:0.979]
Epoch [83/120    avg_loss:0.008, val_acc:0.980]
Epoch [84/120    avg_loss:0.010, val_acc:0.976]
Epoch [85/120    avg_loss:0.019, val_acc:0.965]
Epoch [86/120    avg_loss:0.021, val_acc:0.971]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.010, val_acc:0.980]
Epoch [90/120    avg_loss:0.007, val_acc:0.975]
Epoch [91/120    avg_loss:0.003, val_acc:0.987]
Epoch [92/120    avg_loss:0.016, val_acc:0.971]
Epoch [93/120    avg_loss:0.017, val_acc:0.960]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.007, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.006, val_acc:0.975]
Epoch [98/120    avg_loss:0.006, val_acc:0.981]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.974]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.018, val_acc:0.961]
Epoch [103/120    avg_loss:0.060, val_acc:0.957]
Epoch [104/120    avg_loss:0.017, val_acc:0.947]
Epoch [105/120    avg_loss:0.011, val_acc:0.974]
Epoch [106/120    avg_loss:0.004, val_acc:0.979]
Epoch [107/120    avg_loss:0.007, val_acc:0.979]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.006, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.976]
Epoch [112/120    avg_loss:0.005, val_acc:0.978]
Epoch [113/120    avg_loss:0.005, val_acc:0.978]
Epoch [114/120    avg_loss:0.005, val_acc:0.979]
Epoch [115/120    avg_loss:0.007, val_acc:0.976]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.007, val_acc:0.979]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.005, val_acc:0.979]
Epoch [120/120    avg_loss:0.005, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     2     0     0     1    18     4]
 [    0     0 18045     0    33     0     4     0     8     0]
 [    0     4     1  1933     0     0     0     0    96     2]
 [    0    33    13     4  2907     0     5     0     6     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    34     0     3     0  4841     0     0     0]
 [    0    13     0     0     0     0     0  1274     0     3]
 [    0    14     0    24    36     0     0     0  3495     2]
 [    0     2     0     0     3    10     0     0     0   904]]

Accuracy:
99.07936278408407

F1 scores:
[       nan 0.99294847 0.99742973 0.96722542 0.9761585  0.99618321
 0.99527138 0.99337232 0.97164304 0.98367791]

Kappa:
0.9877987117983215
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5fde89f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.657, val_acc:0.326]
Epoch [2/120    avg_loss:1.074, val_acc:0.730]
Epoch [3/120    avg_loss:0.820, val_acc:0.725]
Epoch [4/120    avg_loss:0.644, val_acc:0.695]
Epoch [5/120    avg_loss:0.516, val_acc:0.722]
Epoch [6/120    avg_loss:0.442, val_acc:0.745]
Epoch [7/120    avg_loss:0.406, val_acc:0.789]
Epoch [8/120    avg_loss:0.345, val_acc:0.808]
Epoch [9/120    avg_loss:0.294, val_acc:0.836]
Epoch [10/120    avg_loss:0.255, val_acc:0.862]
Epoch [11/120    avg_loss:0.230, val_acc:0.848]
Epoch [12/120    avg_loss:0.217, val_acc:0.840]
Epoch [13/120    avg_loss:0.171, val_acc:0.883]
Epoch [14/120    avg_loss:0.130, val_acc:0.934]
Epoch [15/120    avg_loss:0.132, val_acc:0.907]
Epoch [16/120    avg_loss:0.120, val_acc:0.931]
Epoch [17/120    avg_loss:0.136, val_acc:0.917]
Epoch [18/120    avg_loss:0.114, val_acc:0.921]
Epoch [19/120    avg_loss:0.094, val_acc:0.931]
Epoch [20/120    avg_loss:0.091, val_acc:0.914]
Epoch [21/120    avg_loss:0.089, val_acc:0.910]
Epoch [22/120    avg_loss:0.071, val_acc:0.961]
Epoch [23/120    avg_loss:0.080, val_acc:0.951]
Epoch [24/120    avg_loss:0.073, val_acc:0.940]
Epoch [25/120    avg_loss:0.067, val_acc:0.939]
Epoch [26/120    avg_loss:0.064, val_acc:0.947]
Epoch [27/120    avg_loss:0.068, val_acc:0.950]
Epoch [28/120    avg_loss:0.057, val_acc:0.942]
Epoch [29/120    avg_loss:0.042, val_acc:0.972]
Epoch [30/120    avg_loss:0.042, val_acc:0.963]
Epoch [31/120    avg_loss:0.038, val_acc:0.961]
Epoch [32/120    avg_loss:0.045, val_acc:0.960]
Epoch [33/120    avg_loss:0.037, val_acc:0.925]
Epoch [34/120    avg_loss:0.026, val_acc:0.969]
Epoch [35/120    avg_loss:0.024, val_acc:0.975]
Epoch [36/120    avg_loss:0.029, val_acc:0.972]
Epoch [37/120    avg_loss:0.017, val_acc:0.956]
Epoch [38/120    avg_loss:0.017, val_acc:0.969]
Epoch [39/120    avg_loss:0.019, val_acc:0.961]
Epoch [40/120    avg_loss:0.015, val_acc:0.974]
Epoch [41/120    avg_loss:0.018, val_acc:0.976]
Epoch [42/120    avg_loss:0.017, val_acc:0.970]
Epoch [43/120    avg_loss:0.017, val_acc:0.972]
Epoch [44/120    avg_loss:0.016, val_acc:0.975]
Epoch [45/120    avg_loss:0.032, val_acc:0.953]
Epoch [46/120    avg_loss:0.081, val_acc:0.928]
Epoch [47/120    avg_loss:0.068, val_acc:0.933]
Epoch [48/120    avg_loss:0.055, val_acc:0.963]
Epoch [49/120    avg_loss:0.034, val_acc:0.962]
Epoch [50/120    avg_loss:0.018, val_acc:0.969]
Epoch [51/120    avg_loss:0.019, val_acc:0.969]
Epoch [52/120    avg_loss:0.018, val_acc:0.966]
Epoch [53/120    avg_loss:0.014, val_acc:0.974]
Epoch [54/120    avg_loss:0.017, val_acc:0.974]
Epoch [55/120    avg_loss:0.008, val_acc:0.977]
Epoch [56/120    avg_loss:0.009, val_acc:0.979]
Epoch [57/120    avg_loss:0.010, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.979]
Epoch [59/120    avg_loss:0.008, val_acc:0.979]
Epoch [60/120    avg_loss:0.007, val_acc:0.978]
Epoch [61/120    avg_loss:0.008, val_acc:0.979]
Epoch [62/120    avg_loss:0.009, val_acc:0.977]
Epoch [63/120    avg_loss:0.008, val_acc:0.979]
Epoch [64/120    avg_loss:0.008, val_acc:0.978]
Epoch [65/120    avg_loss:0.008, val_acc:0.981]
Epoch [66/120    avg_loss:0.007, val_acc:0.979]
Epoch [67/120    avg_loss:0.006, val_acc:0.981]
Epoch [68/120    avg_loss:0.008, val_acc:0.979]
Epoch [69/120    avg_loss:0.006, val_acc:0.978]
Epoch [70/120    avg_loss:0.008, val_acc:0.979]
Epoch [71/120    avg_loss:0.009, val_acc:0.978]
Epoch [72/120    avg_loss:0.008, val_acc:0.978]
Epoch [73/120    avg_loss:0.006, val_acc:0.979]
Epoch [74/120    avg_loss:0.010, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.979]
Epoch [76/120    avg_loss:0.008, val_acc:0.979]
Epoch [77/120    avg_loss:0.008, val_acc:0.979]
Epoch [78/120    avg_loss:0.008, val_acc:0.979]
Epoch [79/120    avg_loss:0.009, val_acc:0.970]
Epoch [80/120    avg_loss:0.013, val_acc:0.977]
Epoch [81/120    avg_loss:0.007, val_acc:0.979]
Epoch [82/120    avg_loss:0.006, val_acc:0.979]
Epoch [83/120    avg_loss:0.006, val_acc:0.978]
Epoch [84/120    avg_loss:0.007, val_acc:0.980]
Epoch [85/120    avg_loss:0.006, val_acc:0.980]
Epoch [86/120    avg_loss:0.008, val_acc:0.979]
Epoch [87/120    avg_loss:0.007, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.978]
Epoch [89/120    avg_loss:0.008, val_acc:0.978]
Epoch [90/120    avg_loss:0.008, val_acc:0.978]
Epoch [91/120    avg_loss:0.006, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.979]
Epoch [93/120    avg_loss:0.007, val_acc:0.978]
Epoch [94/120    avg_loss:0.007, val_acc:0.979]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.007, val_acc:0.979]
Epoch [98/120    avg_loss:0.005, val_acc:0.979]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.006, val_acc:0.979]
Epoch [101/120    avg_loss:0.008, val_acc:0.979]
Epoch [102/120    avg_loss:0.007, val_acc:0.979]
Epoch [103/120    avg_loss:0.006, val_acc:0.979]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.007, val_acc:0.979]
Epoch [106/120    avg_loss:0.007, val_acc:0.979]
Epoch [107/120    avg_loss:0.006, val_acc:0.979]
Epoch [108/120    avg_loss:0.006, val_acc:0.979]
Epoch [109/120    avg_loss:0.004, val_acc:0.979]
Epoch [110/120    avg_loss:0.007, val_acc:0.979]
Epoch [111/120    avg_loss:0.006, val_acc:0.979]
Epoch [112/120    avg_loss:0.008, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.007, val_acc:0.979]
Epoch [115/120    avg_loss:0.010, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.005, val_acc:0.979]
Epoch [118/120    avg_loss:0.006, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.006, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     2     0     0     0     1    50     4]
 [    0     1 18007     0    17     0    56     0     9     0]
 [    0     0     0  1921     0     0     0     0   113     2]
 [    0    32     8     0  2919     0     5     0     4     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     4     0  4871     0     0     0]
 [    0    11     0     0     0     0     3  1274     0     2]
 [    0     6     0    52    50     0     0     0  3463     0]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.92993998987781

F1 scores:
[       nan 0.99167769 0.9973967  0.95786587 0.97920161 0.99808795
 0.9927647  0.99337232 0.96061026 0.99078591]

Kappa:
0.9858309136430262
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65db9b07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.630, val_acc:0.326]
Epoch [2/120    avg_loss:1.074, val_acc:0.515]
Epoch [3/120    avg_loss:0.807, val_acc:0.689]
Epoch [4/120    avg_loss:0.651, val_acc:0.635]
Epoch [5/120    avg_loss:0.529, val_acc:0.637]
Epoch [6/120    avg_loss:0.461, val_acc:0.721]
Epoch [7/120    avg_loss:0.409, val_acc:0.752]
Epoch [8/120    avg_loss:0.337, val_acc:0.726]
Epoch [9/120    avg_loss:0.304, val_acc:0.819]
Epoch [10/120    avg_loss:0.296, val_acc:0.758]
Epoch [11/120    avg_loss:0.257, val_acc:0.860]
Epoch [12/120    avg_loss:0.257, val_acc:0.873]
Epoch [13/120    avg_loss:0.191, val_acc:0.917]
Epoch [14/120    avg_loss:0.192, val_acc:0.858]
Epoch [15/120    avg_loss:0.199, val_acc:0.894]
Epoch [16/120    avg_loss:0.152, val_acc:0.881]
Epoch [17/120    avg_loss:0.160, val_acc:0.883]
Epoch [18/120    avg_loss:0.156, val_acc:0.859]
Epoch [19/120    avg_loss:0.186, val_acc:0.893]
Epoch [20/120    avg_loss:0.139, val_acc:0.927]
Epoch [21/120    avg_loss:0.145, val_acc:0.910]
Epoch [22/120    avg_loss:0.103, val_acc:0.938]
Epoch [23/120    avg_loss:0.088, val_acc:0.959]
Epoch [24/120    avg_loss:0.088, val_acc:0.921]
Epoch [25/120    avg_loss:0.085, val_acc:0.947]
Epoch [26/120    avg_loss:0.118, val_acc:0.888]
Epoch [27/120    avg_loss:0.094, val_acc:0.965]
Epoch [28/120    avg_loss:0.071, val_acc:0.938]
Epoch [29/120    avg_loss:0.076, val_acc:0.959]
Epoch [30/120    avg_loss:0.045, val_acc:0.962]
Epoch [31/120    avg_loss:0.041, val_acc:0.909]
Epoch [32/120    avg_loss:0.057, val_acc:0.935]
Epoch [33/120    avg_loss:0.064, val_acc:0.949]
Epoch [34/120    avg_loss:0.043, val_acc:0.939]
Epoch [35/120    avg_loss:0.036, val_acc:0.960]
Epoch [36/120    avg_loss:0.038, val_acc:0.970]
Epoch [37/120    avg_loss:0.028, val_acc:0.963]
Epoch [38/120    avg_loss:0.026, val_acc:0.959]
Epoch [39/120    avg_loss:0.034, val_acc:0.967]
Epoch [40/120    avg_loss:0.032, val_acc:0.976]
Epoch [41/120    avg_loss:0.021, val_acc:0.965]
Epoch [42/120    avg_loss:0.015, val_acc:0.972]
Epoch [43/120    avg_loss:0.018, val_acc:0.973]
Epoch [44/120    avg_loss:0.018, val_acc:0.970]
Epoch [45/120    avg_loss:0.025, val_acc:0.961]
Epoch [46/120    avg_loss:0.048, val_acc:0.947]
Epoch [47/120    avg_loss:0.028, val_acc:0.961]
Epoch [48/120    avg_loss:0.043, val_acc:0.959]
Epoch [49/120    avg_loss:0.043, val_acc:0.944]
Epoch [50/120    avg_loss:0.046, val_acc:0.946]
Epoch [51/120    avg_loss:0.032, val_acc:0.972]
Epoch [52/120    avg_loss:0.027, val_acc:0.966]
Epoch [53/120    avg_loss:0.020, val_acc:0.965]
Epoch [54/120    avg_loss:0.017, val_acc:0.971]
Epoch [55/120    avg_loss:0.015, val_acc:0.972]
Epoch [56/120    avg_loss:0.012, val_acc:0.975]
Epoch [57/120    avg_loss:0.009, val_acc:0.975]
Epoch [58/120    avg_loss:0.011, val_acc:0.975]
Epoch [59/120    avg_loss:0.011, val_acc:0.974]
Epoch [60/120    avg_loss:0.012, val_acc:0.973]
Epoch [61/120    avg_loss:0.012, val_acc:0.972]
Epoch [62/120    avg_loss:0.010, val_acc:0.975]
Epoch [63/120    avg_loss:0.010, val_acc:0.973]
Epoch [64/120    avg_loss:0.011, val_acc:0.974]
Epoch [65/120    avg_loss:0.010, val_acc:0.975]
Epoch [66/120    avg_loss:0.008, val_acc:0.974]
Epoch [67/120    avg_loss:0.008, val_acc:0.974]
Epoch [68/120    avg_loss:0.008, val_acc:0.974]
Epoch [69/120    avg_loss:0.008, val_acc:0.974]
Epoch [70/120    avg_loss:0.010, val_acc:0.974]
Epoch [71/120    avg_loss:0.009, val_acc:0.975]
Epoch [72/120    avg_loss:0.011, val_acc:0.975]
Epoch [73/120    avg_loss:0.009, val_acc:0.975]
Epoch [74/120    avg_loss:0.008, val_acc:0.975]
Epoch [75/120    avg_loss:0.010, val_acc:0.975]
Epoch [76/120    avg_loss:0.009, val_acc:0.975]
Epoch [77/120    avg_loss:0.010, val_acc:0.975]
Epoch [78/120    avg_loss:0.011, val_acc:0.975]
Epoch [79/120    avg_loss:0.010, val_acc:0.975]
Epoch [80/120    avg_loss:0.008, val_acc:0.975]
Epoch [81/120    avg_loss:0.010, val_acc:0.975]
Epoch [82/120    avg_loss:0.009, val_acc:0.975]
Epoch [83/120    avg_loss:0.008, val_acc:0.975]
Epoch [84/120    avg_loss:0.010, val_acc:0.975]
Epoch [85/120    avg_loss:0.011, val_acc:0.975]
Epoch [86/120    avg_loss:0.010, val_acc:0.975]
Epoch [87/120    avg_loss:0.008, val_acc:0.975]
Epoch [88/120    avg_loss:0.008, val_acc:0.975]
Epoch [89/120    avg_loss:0.009, val_acc:0.975]
Epoch [90/120    avg_loss:0.010, val_acc:0.975]
Epoch [91/120    avg_loss:0.008, val_acc:0.975]
Epoch [92/120    avg_loss:0.009, val_acc:0.975]
Epoch [93/120    avg_loss:0.011, val_acc:0.975]
Epoch [94/120    avg_loss:0.008, val_acc:0.975]
Epoch [95/120    avg_loss:0.007, val_acc:0.975]
Epoch [96/120    avg_loss:0.009, val_acc:0.975]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.010, val_acc:0.975]
Epoch [99/120    avg_loss:0.010, val_acc:0.975]
Epoch [100/120    avg_loss:0.008, val_acc:0.975]
Epoch [101/120    avg_loss:0.007, val_acc:0.975]
Epoch [102/120    avg_loss:0.010, val_acc:0.975]
Epoch [103/120    avg_loss:0.010, val_acc:0.975]
Epoch [104/120    avg_loss:0.011, val_acc:0.975]
Epoch [105/120    avg_loss:0.008, val_acc:0.975]
Epoch [106/120    avg_loss:0.009, val_acc:0.975]
Epoch [107/120    avg_loss:0.009, val_acc:0.975]
Epoch [108/120    avg_loss:0.009, val_acc:0.975]
Epoch [109/120    avg_loss:0.010, val_acc:0.975]
Epoch [110/120    avg_loss:0.009, val_acc:0.975]
Epoch [111/120    avg_loss:0.011, val_acc:0.975]
Epoch [112/120    avg_loss:0.010, val_acc:0.975]
Epoch [113/120    avg_loss:0.014, val_acc:0.975]
Epoch [114/120    avg_loss:0.010, val_acc:0.975]
Epoch [115/120    avg_loss:0.012, val_acc:0.975]
Epoch [116/120    avg_loss:0.010, val_acc:0.975]
Epoch [117/120    avg_loss:0.008, val_acc:0.975]
Epoch [118/120    avg_loss:0.016, val_acc:0.975]
Epoch [119/120    avg_loss:0.010, val_acc:0.975]
Epoch [120/120    avg_loss:0.008, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6311     0     7     0     0     0     0   107     7]
 [    0     0 17980     0    35     0    63     0    12     0]
 [    0     1     0  1926     0     0     0     0   108     1]
 [    0    23     6     0  2930     0     3     0     4     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    58     0     0     0  4796     0    24     0]
 [    0     1     0     0     0     0     2  1287     0     0]
 [    0    39     0    50    42     0     2     0  3438     0]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.53951268888729

F1 scores:
[       nan 0.98555477 0.99518459 0.95844737 0.98009701 0.99808795
 0.98440066 0.99883586 0.9465859  0.98971305]

Kappa:
0.980659786209981
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe72ce64828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.716, val_acc:0.289]
Epoch [2/120    avg_loss:1.091, val_acc:0.573]
Epoch [3/120    avg_loss:0.785, val_acc:0.554]
Epoch [4/120    avg_loss:0.647, val_acc:0.657]
Epoch [5/120    avg_loss:0.495, val_acc:0.739]
Epoch [6/120    avg_loss:0.425, val_acc:0.737]
Epoch [7/120    avg_loss:0.373, val_acc:0.720]
Epoch [8/120    avg_loss:0.318, val_acc:0.743]
Epoch [9/120    avg_loss:0.324, val_acc:0.797]
Epoch [10/120    avg_loss:0.266, val_acc:0.874]
Epoch [11/120    avg_loss:0.255, val_acc:0.832]
Epoch [12/120    avg_loss:0.197, val_acc:0.902]
Epoch [13/120    avg_loss:0.204, val_acc:0.900]
Epoch [14/120    avg_loss:0.201, val_acc:0.894]
Epoch [15/120    avg_loss:0.186, val_acc:0.943]
Epoch [16/120    avg_loss:0.132, val_acc:0.933]
Epoch [17/120    avg_loss:0.145, val_acc:0.914]
Epoch [18/120    avg_loss:0.122, val_acc:0.897]
Epoch [19/120    avg_loss:0.123, val_acc:0.945]
Epoch [20/120    avg_loss:0.089, val_acc:0.944]
Epoch [21/120    avg_loss:0.081, val_acc:0.952]
Epoch [22/120    avg_loss:0.087, val_acc:0.946]
Epoch [23/120    avg_loss:0.088, val_acc:0.954]
Epoch [24/120    avg_loss:0.109, val_acc:0.924]
Epoch [25/120    avg_loss:0.092, val_acc:0.956]
Epoch [26/120    avg_loss:0.051, val_acc:0.937]
Epoch [27/120    avg_loss:0.085, val_acc:0.938]
Epoch [28/120    avg_loss:0.084, val_acc:0.950]
Epoch [29/120    avg_loss:0.099, val_acc:0.955]
Epoch [30/120    avg_loss:0.069, val_acc:0.957]
Epoch [31/120    avg_loss:0.052, val_acc:0.953]
Epoch [32/120    avg_loss:0.043, val_acc:0.942]
Epoch [33/120    avg_loss:0.039, val_acc:0.949]
Epoch [34/120    avg_loss:0.035, val_acc:0.905]
Epoch [35/120    avg_loss:0.077, val_acc:0.961]
Epoch [36/120    avg_loss:0.051, val_acc:0.879]
Epoch [37/120    avg_loss:0.044, val_acc:0.961]
Epoch [38/120    avg_loss:0.021, val_acc:0.969]
Epoch [39/120    avg_loss:0.024, val_acc:0.958]
Epoch [40/120    avg_loss:0.021, val_acc:0.971]
Epoch [41/120    avg_loss:0.033, val_acc:0.971]
Epoch [42/120    avg_loss:0.022, val_acc:0.967]
Epoch [43/120    avg_loss:0.027, val_acc:0.967]
Epoch [44/120    avg_loss:0.017, val_acc:0.973]
Epoch [45/120    avg_loss:0.015, val_acc:0.976]
Epoch [46/120    avg_loss:0.049, val_acc:0.931]
Epoch [47/120    avg_loss:0.037, val_acc:0.959]
Epoch [48/120    avg_loss:0.049, val_acc:0.957]
Epoch [49/120    avg_loss:0.043, val_acc:0.966]
Epoch [50/120    avg_loss:0.039, val_acc:0.962]
Epoch [51/120    avg_loss:0.027, val_acc:0.971]
Epoch [52/120    avg_loss:0.026, val_acc:0.968]
Epoch [53/120    avg_loss:0.024, val_acc:0.965]
Epoch [54/120    avg_loss:0.031, val_acc:0.975]
Epoch [55/120    avg_loss:0.018, val_acc:0.961]
Epoch [56/120    avg_loss:0.018, val_acc:0.975]
Epoch [57/120    avg_loss:0.023, val_acc:0.970]
Epoch [58/120    avg_loss:0.014, val_acc:0.977]
Epoch [59/120    avg_loss:0.020, val_acc:0.956]
Epoch [60/120    avg_loss:0.028, val_acc:0.972]
Epoch [61/120    avg_loss:0.022, val_acc:0.977]
Epoch [62/120    avg_loss:0.015, val_acc:0.974]
Epoch [63/120    avg_loss:0.010, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.977]
Epoch [65/120    avg_loss:0.023, val_acc:0.972]
Epoch [66/120    avg_loss:0.012, val_acc:0.976]
Epoch [67/120    avg_loss:0.013, val_acc:0.977]
Epoch [68/120    avg_loss:0.008, val_acc:0.975]
Epoch [69/120    avg_loss:0.013, val_acc:0.978]
Epoch [70/120    avg_loss:0.011, val_acc:0.977]
Epoch [71/120    avg_loss:0.009, val_acc:0.979]
Epoch [72/120    avg_loss:0.009, val_acc:0.978]
Epoch [73/120    avg_loss:0.006, val_acc:0.979]
Epoch [74/120    avg_loss:0.010, val_acc:0.979]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.005, val_acc:0.979]
Epoch [77/120    avg_loss:0.010, val_acc:0.979]
Epoch [78/120    avg_loss:0.005, val_acc:0.979]
Epoch [79/120    avg_loss:0.011, val_acc:0.975]
Epoch [80/120    avg_loss:0.011, val_acc:0.967]
Epoch [81/120    avg_loss:0.015, val_acc:0.979]
Epoch [82/120    avg_loss:0.038, val_acc:0.965]
Epoch [83/120    avg_loss:0.034, val_acc:0.972]
Epoch [84/120    avg_loss:0.015, val_acc:0.979]
Epoch [85/120    avg_loss:0.015, val_acc:0.976]
Epoch [86/120    avg_loss:0.015, val_acc:0.960]
Epoch [87/120    avg_loss:0.015, val_acc:0.971]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.004, val_acc:0.985]
Epoch [91/120    avg_loss:0.005, val_acc:0.978]
Epoch [92/120    avg_loss:0.008, val_acc:0.977]
Epoch [93/120    avg_loss:0.017, val_acc:0.975]
Epoch [94/120    avg_loss:0.016, val_acc:0.962]
Epoch [95/120    avg_loss:0.017, val_acc:0.973]
Epoch [96/120    avg_loss:0.008, val_acc:0.980]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.014, val_acc:0.979]
Epoch [100/120    avg_loss:0.009, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.016, val_acc:0.965]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.977]
Epoch [110/120    avg_loss:0.014, val_acc:0.969]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.003, val_acc:0.984]
Epoch [114/120    avg_loss:0.002, val_acc:0.984]
Epoch [115/120    avg_loss:0.003, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.002, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.979]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     0     0     0     6    43    14]
 [    0     0 18057     0    23     0     5     0     5     0]
 [    0     0     0  1934     0     0     0     0   100     2]
 [    0    30    14     0  2909     0     9     0     5     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    44     0     0     0  4830     0     4     0]
 [    0     3     0     0     0     0     0  1284     1     2]
 [    0     9     0    27    29     0     0     0  3506     0]
 [    0     0     0     0     6     9     0     0     0   904]]

Accuracy:
99.04803219820211

F1 scores:
[       nan 0.99182434 0.99748654 0.96772579 0.9796262  0.99656357
 0.99362271 0.99534884 0.96917761 0.97941495]

Kappa:
0.9873825988033854
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa5a38947f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.677, val_acc:0.288]
Epoch [2/120    avg_loss:1.096, val_acc:0.404]
Epoch [3/120    avg_loss:0.871, val_acc:0.721]
Epoch [4/120    avg_loss:0.697, val_acc:0.657]
Epoch [5/120    avg_loss:0.529, val_acc:0.638]
Epoch [6/120    avg_loss:0.442, val_acc:0.786]
Epoch [7/120    avg_loss:0.397, val_acc:0.697]
Epoch [8/120    avg_loss:0.328, val_acc:0.753]
Epoch [9/120    avg_loss:0.293, val_acc:0.803]
Epoch [10/120    avg_loss:0.281, val_acc:0.798]
Epoch [11/120    avg_loss:0.277, val_acc:0.846]
Epoch [12/120    avg_loss:0.240, val_acc:0.821]
Epoch [13/120    avg_loss:0.221, val_acc:0.887]
Epoch [14/120    avg_loss:0.174, val_acc:0.910]
Epoch [15/120    avg_loss:0.153, val_acc:0.885]
Epoch [16/120    avg_loss:0.139, val_acc:0.951]
Epoch [17/120    avg_loss:0.128, val_acc:0.945]
Epoch [18/120    avg_loss:0.145, val_acc:0.960]
Epoch [19/120    avg_loss:0.097, val_acc:0.939]
Epoch [20/120    avg_loss:0.099, val_acc:0.954]
Epoch [21/120    avg_loss:0.098, val_acc:0.963]
Epoch [22/120    avg_loss:0.095, val_acc:0.904]
Epoch [23/120    avg_loss:0.108, val_acc:0.947]
Epoch [24/120    avg_loss:0.071, val_acc:0.959]
Epoch [25/120    avg_loss:0.054, val_acc:0.973]
Epoch [26/120    avg_loss:0.065, val_acc:0.953]
Epoch [27/120    avg_loss:0.091, val_acc:0.968]
Epoch [28/120    avg_loss:0.059, val_acc:0.966]
Epoch [29/120    avg_loss:0.058, val_acc:0.958]
Epoch [30/120    avg_loss:0.053, val_acc:0.956]
Epoch [31/120    avg_loss:0.076, val_acc:0.966]
Epoch [32/120    avg_loss:0.164, val_acc:0.944]
Epoch [33/120    avg_loss:0.076, val_acc:0.935]
Epoch [34/120    avg_loss:0.110, val_acc:0.961]
Epoch [35/120    avg_loss:0.056, val_acc:0.975]
Epoch [36/120    avg_loss:0.052, val_acc:0.975]
Epoch [37/120    avg_loss:0.029, val_acc:0.979]
Epoch [38/120    avg_loss:0.029, val_acc:0.961]
Epoch [39/120    avg_loss:0.029, val_acc:0.971]
Epoch [40/120    avg_loss:0.030, val_acc:0.979]
Epoch [41/120    avg_loss:0.024, val_acc:0.979]
Epoch [42/120    avg_loss:0.019, val_acc:0.975]
Epoch [43/120    avg_loss:0.016, val_acc:0.976]
Epoch [44/120    avg_loss:0.016, val_acc:0.983]
Epoch [45/120    avg_loss:0.027, val_acc:0.939]
Epoch [46/120    avg_loss:0.023, val_acc:0.973]
Epoch [47/120    avg_loss:0.019, val_acc:0.979]
Epoch [48/120    avg_loss:0.016, val_acc:0.977]
Epoch [49/120    avg_loss:0.017, val_acc:0.971]
Epoch [50/120    avg_loss:0.019, val_acc:0.979]
Epoch [51/120    avg_loss:0.021, val_acc:0.976]
Epoch [52/120    avg_loss:0.021, val_acc:0.976]
Epoch [53/120    avg_loss:0.026, val_acc:0.979]
Epoch [54/120    avg_loss:0.017, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.972]
Epoch [56/120    avg_loss:0.021, val_acc:0.969]
Epoch [57/120    avg_loss:0.032, val_acc:0.978]
Epoch [58/120    avg_loss:0.016, val_acc:0.982]
Epoch [59/120    avg_loss:0.013, val_acc:0.980]
Epoch [60/120    avg_loss:0.009, val_acc:0.979]
Epoch [61/120    avg_loss:0.010, val_acc:0.979]
Epoch [62/120    avg_loss:0.013, val_acc:0.979]
Epoch [63/120    avg_loss:0.012, val_acc:0.980]
Epoch [64/120    avg_loss:0.011, val_acc:0.982]
Epoch [65/120    avg_loss:0.012, val_acc:0.982]
Epoch [66/120    avg_loss:0.008, val_acc:0.981]
Epoch [67/120    avg_loss:0.009, val_acc:0.981]
Epoch [68/120    avg_loss:0.010, val_acc:0.983]
Epoch [69/120    avg_loss:0.012, val_acc:0.984]
Epoch [70/120    avg_loss:0.006, val_acc:0.984]
Epoch [71/120    avg_loss:0.010, val_acc:0.983]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.983]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.983]
Epoch [78/120    avg_loss:0.009, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.987]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.985]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.005, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     0     0     0     0    28     3]
 [    0     0 18059     0    15     0     5     0    11     0]
 [    0     2     0  1926     0     0     0     0   104     4]
 [    0    28     6     0  2929     0     2     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     0     0     0  4844     0    17     0]
 [    0    15     0     0     0     0     0  1275     0     0]
 [    0    13     0    66    51     0     0     0  3439     2]
 [    0     3     0     0     4    11     0     0     0   901]]

Accuracy:
99.00224134191309

F1 scores:
[       nan 0.9928649  0.99850713 0.95630586 0.9810752  0.99580313
 0.9957858  0.99415205 0.95860627 0.98416166]

Kappa:
0.9867789566155053
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbe32c00780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.680, val_acc:0.658]
Epoch [2/120    avg_loss:1.102, val_acc:0.604]
Epoch [3/120    avg_loss:0.872, val_acc:0.542]
Epoch [4/120    avg_loss:0.704, val_acc:0.646]
Epoch [5/120    avg_loss:0.560, val_acc:0.635]
Epoch [6/120    avg_loss:0.462, val_acc:0.785]
Epoch [7/120    avg_loss:0.389, val_acc:0.744]
Epoch [8/120    avg_loss:0.317, val_acc:0.828]
Epoch [9/120    avg_loss:0.296, val_acc:0.887]
Epoch [10/120    avg_loss:0.230, val_acc:0.898]
Epoch [11/120    avg_loss:0.230, val_acc:0.882]
Epoch [12/120    avg_loss:0.207, val_acc:0.900]
Epoch [13/120    avg_loss:0.212, val_acc:0.927]
Epoch [14/120    avg_loss:0.185, val_acc:0.771]
Epoch [15/120    avg_loss:0.142, val_acc:0.931]
Epoch [16/120    avg_loss:0.124, val_acc:0.906]
Epoch [17/120    avg_loss:0.104, val_acc:0.941]
Epoch [18/120    avg_loss:0.121, val_acc:0.841]
Epoch [19/120    avg_loss:0.150, val_acc:0.929]
Epoch [20/120    avg_loss:0.108, val_acc:0.905]
Epoch [21/120    avg_loss:0.091, val_acc:0.957]
Epoch [22/120    avg_loss:0.074, val_acc:0.938]
Epoch [23/120    avg_loss:0.060, val_acc:0.953]
Epoch [24/120    avg_loss:0.045, val_acc:0.961]
Epoch [25/120    avg_loss:0.054, val_acc:0.962]
Epoch [26/120    avg_loss:0.048, val_acc:0.960]
Epoch [27/120    avg_loss:0.039, val_acc:0.958]
Epoch [28/120    avg_loss:0.078, val_acc:0.950]
Epoch [29/120    avg_loss:0.096, val_acc:0.920]
Epoch [30/120    avg_loss:0.092, val_acc:0.949]
Epoch [31/120    avg_loss:0.047, val_acc:0.937]
Epoch [32/120    avg_loss:0.101, val_acc:0.942]
Epoch [33/120    avg_loss:0.098, val_acc:0.956]
Epoch [34/120    avg_loss:0.054, val_acc:0.952]
Epoch [35/120    avg_loss:0.046, val_acc:0.927]
Epoch [36/120    avg_loss:0.056, val_acc:0.941]
Epoch [37/120    avg_loss:0.096, val_acc:0.947]
Epoch [38/120    avg_loss:0.065, val_acc:0.959]
Epoch [39/120    avg_loss:0.042, val_acc:0.973]
Epoch [40/120    avg_loss:0.028, val_acc:0.977]
Epoch [41/120    avg_loss:0.024, val_acc:0.977]
Epoch [42/120    avg_loss:0.030, val_acc:0.978]
Epoch [43/120    avg_loss:0.024, val_acc:0.980]
Epoch [44/120    avg_loss:0.028, val_acc:0.978]
Epoch [45/120    avg_loss:0.027, val_acc:0.973]
Epoch [46/120    avg_loss:0.022, val_acc:0.975]
Epoch [47/120    avg_loss:0.020, val_acc:0.978]
Epoch [48/120    avg_loss:0.021, val_acc:0.979]
Epoch [49/120    avg_loss:0.026, val_acc:0.978]
Epoch [50/120    avg_loss:0.024, val_acc:0.978]
Epoch [51/120    avg_loss:0.026, val_acc:0.979]
Epoch [52/120    avg_loss:0.017, val_acc:0.979]
Epoch [53/120    avg_loss:0.025, val_acc:0.975]
Epoch [54/120    avg_loss:0.016, val_acc:0.978]
Epoch [55/120    avg_loss:0.020, val_acc:0.980]
Epoch [56/120    avg_loss:0.015, val_acc:0.981]
Epoch [57/120    avg_loss:0.016, val_acc:0.982]
Epoch [58/120    avg_loss:0.018, val_acc:0.982]
Epoch [59/120    avg_loss:0.021, val_acc:0.979]
Epoch [60/120    avg_loss:0.017, val_acc:0.978]
Epoch [61/120    avg_loss:0.019, val_acc:0.981]
Epoch [62/120    avg_loss:0.016, val_acc:0.979]
Epoch [63/120    avg_loss:0.016, val_acc:0.979]
Epoch [64/120    avg_loss:0.017, val_acc:0.978]
Epoch [65/120    avg_loss:0.020, val_acc:0.979]
Epoch [66/120    avg_loss:0.017, val_acc:0.980]
Epoch [67/120    avg_loss:0.014, val_acc:0.982]
Epoch [68/120    avg_loss:0.020, val_acc:0.983]
Epoch [69/120    avg_loss:0.017, val_acc:0.979]
Epoch [70/120    avg_loss:0.012, val_acc:0.980]
Epoch [71/120    avg_loss:0.015, val_acc:0.978]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.015, val_acc:0.982]
Epoch [74/120    avg_loss:0.013, val_acc:0.979]
Epoch [75/120    avg_loss:0.016, val_acc:0.982]
Epoch [76/120    avg_loss:0.015, val_acc:0.981]
Epoch [77/120    avg_loss:0.018, val_acc:0.979]
Epoch [78/120    avg_loss:0.017, val_acc:0.981]
Epoch [79/120    avg_loss:0.013, val_acc:0.979]
Epoch [80/120    avg_loss:0.017, val_acc:0.980]
Epoch [81/120    avg_loss:0.016, val_acc:0.983]
Epoch [82/120    avg_loss:0.014, val_acc:0.983]
Epoch [83/120    avg_loss:0.019, val_acc:0.979]
Epoch [84/120    avg_loss:0.015, val_acc:0.975]
Epoch [85/120    avg_loss:0.017, val_acc:0.982]
Epoch [86/120    avg_loss:0.014, val_acc:0.983]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.012, val_acc:0.987]
Epoch [90/120    avg_loss:0.013, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.012, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.013, val_acc:0.983]
Epoch [95/120    avg_loss:0.019, val_acc:0.979]
Epoch [96/120    avg_loss:0.013, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.014, val_acc:0.984]
Epoch [99/120    avg_loss:0.011, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.013, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.985]
Epoch [108/120    avg_loss:0.012, val_acc:0.984]
Epoch [109/120    avg_loss:0.016, val_acc:0.984]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.011, val_acc:0.984]
Epoch [113/120    avg_loss:0.013, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.984]
Epoch [115/120    avg_loss:0.011, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.013, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     1     0     0    16     0    38     3]
 [    0     0 18012     0    53     0    20     0     5     0]
 [    0     5     0  1959     0     0     0     0    72     0]
 [    0    18     8     0  2935     0     1     0     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21     0     0     0  4856     0     1     0]
 [    0    23     0     0     0     0     1  1251     4    11]
 [    0     6     6    59    34     0     0     0  3466     0]
 [    0     0     0     0     3    10     0     0     0   906]]

Accuracy:
98.96609066589545

F1 scores:
[       nan 0.99144501 0.99687301 0.96621455 0.97882274 0.99618321
 0.99386001 0.98465171 0.96761586 0.98371336]

Kappa:
0.9863060898304689
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa072d06780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.679, val_acc:0.296]
Epoch [2/120    avg_loss:1.094, val_acc:0.485]
Epoch [3/120    avg_loss:0.873, val_acc:0.581]
Epoch [4/120    avg_loss:0.701, val_acc:0.691]
Epoch [5/120    avg_loss:0.592, val_acc:0.746]
Epoch [6/120    avg_loss:0.456, val_acc:0.792]
Epoch [7/120    avg_loss:0.389, val_acc:0.764]
Epoch [8/120    avg_loss:0.352, val_acc:0.827]
Epoch [9/120    avg_loss:0.316, val_acc:0.867]
Epoch [10/120    avg_loss:0.281, val_acc:0.823]
Epoch [11/120    avg_loss:0.229, val_acc:0.889]
Epoch [12/120    avg_loss:0.210, val_acc:0.890]
Epoch [13/120    avg_loss:0.228, val_acc:0.906]
Epoch [14/120    avg_loss:0.169, val_acc:0.894]
Epoch [15/120    avg_loss:0.170, val_acc:0.946]
Epoch [16/120    avg_loss:0.130, val_acc:0.908]
Epoch [17/120    avg_loss:0.113, val_acc:0.899]
Epoch [18/120    avg_loss:0.115, val_acc:0.858]
Epoch [19/120    avg_loss:0.109, val_acc:0.949]
Epoch [20/120    avg_loss:0.191, val_acc:0.942]
Epoch [21/120    avg_loss:0.137, val_acc:0.948]
Epoch [22/120    avg_loss:0.109, val_acc:0.948]
Epoch [23/120    avg_loss:0.075, val_acc:0.952]
Epoch [24/120    avg_loss:0.095, val_acc:0.967]
Epoch [25/120    avg_loss:0.056, val_acc:0.921]
Epoch [26/120    avg_loss:0.085, val_acc:0.951]
Epoch [27/120    avg_loss:0.052, val_acc:0.968]
Epoch [28/120    avg_loss:0.065, val_acc:0.961]
Epoch [29/120    avg_loss:0.044, val_acc:0.967]
Epoch [30/120    avg_loss:0.042, val_acc:0.980]
Epoch [31/120    avg_loss:0.046, val_acc:0.981]
Epoch [32/120    avg_loss:0.039, val_acc:0.967]
Epoch [33/120    avg_loss:0.045, val_acc:0.972]
Epoch [34/120    avg_loss:0.067, val_acc:0.947]
Epoch [35/120    avg_loss:0.039, val_acc:0.980]
Epoch [36/120    avg_loss:0.027, val_acc:0.970]
Epoch [37/120    avg_loss:0.035, val_acc:0.975]
Epoch [38/120    avg_loss:0.036, val_acc:0.974]
Epoch [39/120    avg_loss:0.026, val_acc:0.977]
Epoch [40/120    avg_loss:0.021, val_acc:0.974]
Epoch [41/120    avg_loss:0.029, val_acc:0.970]
Epoch [42/120    avg_loss:0.029, val_acc:0.976]
Epoch [43/120    avg_loss:0.025, val_acc:0.970]
Epoch [44/120    avg_loss:0.032, val_acc:0.967]
Epoch [45/120    avg_loss:0.025, val_acc:0.980]
Epoch [46/120    avg_loss:0.017, val_acc:0.983]
Epoch [47/120    avg_loss:0.012, val_acc:0.985]
Epoch [48/120    avg_loss:0.017, val_acc:0.981]
Epoch [49/120    avg_loss:0.018, val_acc:0.984]
Epoch [50/120    avg_loss:0.017, val_acc:0.986]
Epoch [51/120    avg_loss:0.013, val_acc:0.983]
Epoch [52/120    avg_loss:0.013, val_acc:0.986]
Epoch [53/120    avg_loss:0.018, val_acc:0.985]
Epoch [54/120    avg_loss:0.018, val_acc:0.981]
Epoch [55/120    avg_loss:0.011, val_acc:0.984]
Epoch [56/120    avg_loss:0.013, val_acc:0.985]
Epoch [57/120    avg_loss:0.009, val_acc:0.985]
Epoch [58/120    avg_loss:0.010, val_acc:0.985]
Epoch [59/120    avg_loss:0.013, val_acc:0.984]
Epoch [60/120    avg_loss:0.011, val_acc:0.984]
Epoch [61/120    avg_loss:0.011, val_acc:0.984]
Epoch [62/120    avg_loss:0.010, val_acc:0.985]
Epoch [63/120    avg_loss:0.012, val_acc:0.983]
Epoch [64/120    avg_loss:0.011, val_acc:0.984]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.012, val_acc:0.984]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.985]
Epoch [71/120    avg_loss:0.010, val_acc:0.985]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.011, val_acc:0.985]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.011, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.009, val_acc:0.985]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.012, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.012, val_acc:0.985]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.012, val_acc:0.985]
Epoch [98/120    avg_loss:0.011, val_acc:0.985]
Epoch [99/120    avg_loss:0.013, val_acc:0.985]
Epoch [100/120    avg_loss:0.012, val_acc:0.985]
Epoch [101/120    avg_loss:0.013, val_acc:0.985]
Epoch [102/120    avg_loss:0.011, val_acc:0.985]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.010, val_acc:0.985]
Epoch [111/120    avg_loss:0.010, val_acc:0.985]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.012, val_acc:0.985]
Epoch [118/120    avg_loss:0.015, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6330     0     3     0     0     0    21    75     3]
 [    0     0 18044     0    18     0    14     0    14     0]
 [    0     3     1  1945     0     0     0     0    85     2]
 [    0    14    10     0  2940     0     3     0     3     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    43     0     6     0  4790     0    39     0]
 [    0     5     0     0     0     0     2  1281     0     2]
 [    0    26     0    43    51     0     0     0  3451     0]
 [    0     0     0     0     0    13     0     0     0   906]]

Accuracy:
98.79256742101077

F1 scores:
[       nan 0.9882904  0.99723665 0.96597964 0.98212794 0.99504384
 0.98895427 0.98842593 0.95357834 0.98800436]

Kappa:
0.9840024071875673
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15faeec828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.712, val_acc:0.557]
Epoch [2/120    avg_loss:1.046, val_acc:0.607]
Epoch [3/120    avg_loss:0.824, val_acc:0.702]
Epoch [4/120    avg_loss:0.680, val_acc:0.753]
Epoch [5/120    avg_loss:0.562, val_acc:0.715]
Epoch [6/120    avg_loss:0.459, val_acc:0.707]
Epoch [7/120    avg_loss:0.405, val_acc:0.739]
Epoch [8/120    avg_loss:0.352, val_acc:0.743]
Epoch [9/120    avg_loss:0.319, val_acc:0.823]
Epoch [10/120    avg_loss:0.271, val_acc:0.861]
Epoch [11/120    avg_loss:0.295, val_acc:0.824]
Epoch [12/120    avg_loss:0.207, val_acc:0.862]
Epoch [13/120    avg_loss:0.211, val_acc:0.842]
Epoch [14/120    avg_loss:0.193, val_acc:0.872]
Epoch [15/120    avg_loss:0.172, val_acc:0.930]
Epoch [16/120    avg_loss:0.149, val_acc:0.962]
Epoch [17/120    avg_loss:0.127, val_acc:0.938]
Epoch [18/120    avg_loss:0.120, val_acc:0.965]
Epoch [19/120    avg_loss:0.115, val_acc:0.929]
Epoch [20/120    avg_loss:0.105, val_acc:0.956]
Epoch [21/120    avg_loss:0.087, val_acc:0.957]
Epoch [22/120    avg_loss:0.070, val_acc:0.938]
Epoch [23/120    avg_loss:0.125, val_acc:0.944]
Epoch [24/120    avg_loss:0.114, val_acc:0.956]
Epoch [25/120    avg_loss:0.075, val_acc:0.959]
Epoch [26/120    avg_loss:0.057, val_acc:0.958]
Epoch [27/120    avg_loss:0.063, val_acc:0.972]
Epoch [28/120    avg_loss:0.044, val_acc:0.974]
Epoch [29/120    avg_loss:0.057, val_acc:0.948]
Epoch [30/120    avg_loss:0.059, val_acc:0.969]
Epoch [31/120    avg_loss:0.069, val_acc:0.964]
Epoch [32/120    avg_loss:0.052, val_acc:0.957]
Epoch [33/120    avg_loss:0.049, val_acc:0.971]
Epoch [34/120    avg_loss:0.043, val_acc:0.961]
Epoch [35/120    avg_loss:0.035, val_acc:0.973]
Epoch [36/120    avg_loss:0.028, val_acc:0.977]
Epoch [37/120    avg_loss:0.036, val_acc:0.953]
Epoch [38/120    avg_loss:0.059, val_acc:0.930]
Epoch [39/120    avg_loss:0.040, val_acc:0.952]
Epoch [40/120    avg_loss:0.032, val_acc:0.981]
Epoch [41/120    avg_loss:0.024, val_acc:0.965]
Epoch [42/120    avg_loss:0.033, val_acc:0.959]
Epoch [43/120    avg_loss:0.028, val_acc:0.975]
Epoch [44/120    avg_loss:0.027, val_acc:0.968]
Epoch [45/120    avg_loss:0.035, val_acc:0.973]
Epoch [46/120    avg_loss:0.020, val_acc:0.976]
Epoch [47/120    avg_loss:0.019, val_acc:0.978]
Epoch [48/120    avg_loss:0.015, val_acc:0.982]
Epoch [49/120    avg_loss:0.013, val_acc:0.977]
Epoch [50/120    avg_loss:0.011, val_acc:0.981]
Epoch [51/120    avg_loss:0.011, val_acc:0.976]
Epoch [52/120    avg_loss:0.009, val_acc:0.985]
Epoch [53/120    avg_loss:0.012, val_acc:0.979]
Epoch [54/120    avg_loss:0.016, val_acc:0.970]
Epoch [55/120    avg_loss:0.022, val_acc:0.976]
Epoch [56/120    avg_loss:0.014, val_acc:0.981]
Epoch [57/120    avg_loss:0.014, val_acc:0.966]
Epoch [58/120    avg_loss:0.021, val_acc:0.960]
Epoch [59/120    avg_loss:0.024, val_acc:0.966]
Epoch [60/120    avg_loss:0.019, val_acc:0.981]
Epoch [61/120    avg_loss:0.008, val_acc:0.982]
Epoch [62/120    avg_loss:0.015, val_acc:0.973]
Epoch [63/120    avg_loss:0.014, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.978]
Epoch [65/120    avg_loss:0.022, val_acc:0.981]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.013, val_acc:0.985]
Epoch [68/120    avg_loss:0.007, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.986]
Epoch [71/120    avg_loss:0.011, val_acc:0.986]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.984]
Epoch [79/120    avg_loss:0.006, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.006, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0     0     0    38     0]
 [    0     2 18058     0     7     0    16     0     7     0]
 [    0     3     0  1954     0     0     0     0    79     0]
 [    0    27     6     0  2924     0     8     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0     0     0  4849     0     0     0]
 [    0    13     0     0     0     0     0  1276     0     1]
 [    0    17     0    49    42     0     0     0  3463     0]
 [    0     2     0     0     0    12     0     0     0   905]]

Accuracy:
99.12033355023739

F1 scores:
[       nan 0.99208689 0.9981483  0.96756623 0.98368377 0.99542334
 0.99456466 0.99454404 0.96664341 0.99178082]

Kappa:
0.9883420875939355
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4747644828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.732, val_acc:0.536]
Epoch [2/120    avg_loss:1.191, val_acc:0.537]
Epoch [3/120    avg_loss:0.945, val_acc:0.688]
Epoch [4/120    avg_loss:0.749, val_acc:0.633]
Epoch [5/120    avg_loss:0.588, val_acc:0.749]
Epoch [6/120    avg_loss:0.473, val_acc:0.789]
Epoch [7/120    avg_loss:0.399, val_acc:0.698]
Epoch [8/120    avg_loss:0.353, val_acc:0.774]
Epoch [9/120    avg_loss:0.279, val_acc:0.824]
Epoch [10/120    avg_loss:0.277, val_acc:0.827]
Epoch [11/120    avg_loss:0.227, val_acc:0.899]
Epoch [12/120    avg_loss:0.207, val_acc:0.835]
Epoch [13/120    avg_loss:0.190, val_acc:0.907]
Epoch [14/120    avg_loss:0.149, val_acc:0.932]
Epoch [15/120    avg_loss:0.129, val_acc:0.921]
Epoch [16/120    avg_loss:0.117, val_acc:0.931]
Epoch [17/120    avg_loss:0.123, val_acc:0.949]
Epoch [18/120    avg_loss:0.150, val_acc:0.940]
Epoch [19/120    avg_loss:0.102, val_acc:0.918]
Epoch [20/120    avg_loss:0.095, val_acc:0.954]
Epoch [21/120    avg_loss:0.073, val_acc:0.963]
Epoch [22/120    avg_loss:0.058, val_acc:0.965]
Epoch [23/120    avg_loss:0.063, val_acc:0.947]
Epoch [24/120    avg_loss:0.097, val_acc:0.960]
Epoch [25/120    avg_loss:0.083, val_acc:0.961]
Epoch [26/120    avg_loss:0.082, val_acc:0.898]
Epoch [27/120    avg_loss:0.065, val_acc:0.973]
Epoch [28/120    avg_loss:0.047, val_acc:0.976]
Epoch [29/120    avg_loss:0.031, val_acc:0.979]
Epoch [30/120    avg_loss:0.039, val_acc:0.978]
Epoch [31/120    avg_loss:0.047, val_acc:0.967]
Epoch [32/120    avg_loss:0.075, val_acc:0.976]
Epoch [33/120    avg_loss:0.039, val_acc:0.976]
Epoch [34/120    avg_loss:0.031, val_acc:0.979]
Epoch [35/120    avg_loss:0.027, val_acc:0.981]
Epoch [36/120    avg_loss:0.019, val_acc:0.982]
Epoch [37/120    avg_loss:0.015, val_acc:0.982]
Epoch [38/120    avg_loss:0.020, val_acc:0.986]
Epoch [39/120    avg_loss:0.016, val_acc:0.975]
Epoch [40/120    avg_loss:0.034, val_acc:0.971]
Epoch [41/120    avg_loss:0.018, val_acc:0.968]
Epoch [42/120    avg_loss:0.039, val_acc:0.964]
Epoch [43/120    avg_loss:0.038, val_acc:0.974]
Epoch [44/120    avg_loss:0.016, val_acc:0.984]
Epoch [45/120    avg_loss:0.013, val_acc:0.984]
Epoch [46/120    avg_loss:0.012, val_acc:0.985]
Epoch [47/120    avg_loss:0.015, val_acc:0.989]
Epoch [48/120    avg_loss:0.010, val_acc:0.985]
Epoch [49/120    avg_loss:0.007, val_acc:0.986]
Epoch [50/120    avg_loss:0.007, val_acc:0.985]
Epoch [51/120    avg_loss:0.015, val_acc:0.986]
Epoch [52/120    avg_loss:0.010, val_acc:0.976]
Epoch [53/120    avg_loss:0.012, val_acc:0.986]
Epoch [54/120    avg_loss:0.009, val_acc:0.985]
Epoch [55/120    avg_loss:0.014, val_acc:0.977]
Epoch [56/120    avg_loss:0.027, val_acc:0.985]
Epoch [57/120    avg_loss:0.012, val_acc:0.984]
Epoch [58/120    avg_loss:0.010, val_acc:0.981]
Epoch [59/120    avg_loss:0.014, val_acc:0.986]
Epoch [60/120    avg_loss:0.009, val_acc:0.985]
Epoch [61/120    avg_loss:0.011, val_acc:0.989]
Epoch [62/120    avg_loss:0.008, val_acc:0.988]
Epoch [63/120    avg_loss:0.007, val_acc:0.988]
Epoch [64/120    avg_loss:0.009, val_acc:0.989]
Epoch [65/120    avg_loss:0.007, val_acc:0.989]
Epoch [66/120    avg_loss:0.008, val_acc:0.989]
Epoch [67/120    avg_loss:0.008, val_acc:0.988]
Epoch [68/120    avg_loss:0.007, val_acc:0.989]
Epoch [69/120    avg_loss:0.010, val_acc:0.988]
Epoch [70/120    avg_loss:0.011, val_acc:0.987]
Epoch [71/120    avg_loss:0.005, val_acc:0.986]
Epoch [72/120    avg_loss:0.005, val_acc:0.987]
Epoch [73/120    avg_loss:0.007, val_acc:0.987]
Epoch [74/120    avg_loss:0.005, val_acc:0.989]
Epoch [75/120    avg_loss:0.004, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.986]
Epoch [77/120    avg_loss:0.005, val_acc:0.986]
Epoch [78/120    avg_loss:0.004, val_acc:0.986]
Epoch [79/120    avg_loss:0.004, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.005, val_acc:0.987]
Epoch [83/120    avg_loss:0.003, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.004, val_acc:0.986]
Epoch [86/120    avg_loss:0.014, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     1     0     0     5    23     9     0]
 [    0     2 18073     0    12     0     1     0     2     0]
 [    0     0     1  1999     0     0     0     0    32     4]
 [    0    28     1     2  2921     0     7     0    12     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     2     0     0  4863     0     3     0]
 [    0     7     0     0     0     0     0  1280     0     3]
 [    0     2     0    27    52     0     0     0  3489     1]
 [    0     0     0     0    12    19     0     0     0   888]]

Accuracy:
99.32277733593618

F1 scores:
[       nan 0.99401477 0.99919834 0.98303418 0.9787234  0.99277292
 0.99712938 0.98727343 0.98033155 0.97797357]

Kappa:
0.9910270398632294
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba78f377b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.713, val_acc:0.568]
Epoch [2/120    avg_loss:1.129, val_acc:0.695]
Epoch [3/120    avg_loss:0.900, val_acc:0.508]
Epoch [4/120    avg_loss:0.740, val_acc:0.758]
Epoch [5/120    avg_loss:0.588, val_acc:0.696]
Epoch [6/120    avg_loss:0.479, val_acc:0.774]
Epoch [7/120    avg_loss:0.379, val_acc:0.812]
Epoch [8/120    avg_loss:0.304, val_acc:0.780]
Epoch [9/120    avg_loss:0.288, val_acc:0.785]
Epoch [10/120    avg_loss:0.276, val_acc:0.816]
Epoch [11/120    avg_loss:0.255, val_acc:0.869]
Epoch [12/120    avg_loss:0.203, val_acc:0.920]
Epoch [13/120    avg_loss:0.168, val_acc:0.949]
Epoch [14/120    avg_loss:0.224, val_acc:0.935]
Epoch [15/120    avg_loss:0.129, val_acc:0.947]
Epoch [16/120    avg_loss:0.117, val_acc:0.950]
Epoch [17/120    avg_loss:0.094, val_acc:0.957]
Epoch [18/120    avg_loss:0.104, val_acc:0.960]
Epoch [19/120    avg_loss:0.112, val_acc:0.954]
Epoch [20/120    avg_loss:0.076, val_acc:0.915]
Epoch [21/120    avg_loss:0.139, val_acc:0.961]
Epoch [22/120    avg_loss:0.112, val_acc:0.959]
Epoch [23/120    avg_loss:0.078, val_acc:0.966]
Epoch [24/120    avg_loss:0.074, val_acc:0.956]
Epoch [25/120    avg_loss:0.061, val_acc:0.970]
Epoch [26/120    avg_loss:0.066, val_acc:0.964]
Epoch [27/120    avg_loss:0.040, val_acc:0.975]
Epoch [28/120    avg_loss:0.049, val_acc:0.970]
Epoch [29/120    avg_loss:0.086, val_acc:0.972]
Epoch [30/120    avg_loss:0.043, val_acc:0.979]
Epoch [31/120    avg_loss:0.027, val_acc:0.978]
Epoch [32/120    avg_loss:0.069, val_acc:0.969]
Epoch [33/120    avg_loss:0.046, val_acc:0.955]
Epoch [34/120    avg_loss:0.043, val_acc:0.967]
Epoch [35/120    avg_loss:0.039, val_acc:0.974]
Epoch [36/120    avg_loss:0.028, val_acc:0.976]
Epoch [37/120    avg_loss:0.038, val_acc:0.961]
Epoch [38/120    avg_loss:0.039, val_acc:0.973]
Epoch [39/120    avg_loss:0.027, val_acc:0.980]
Epoch [40/120    avg_loss:0.022, val_acc:0.981]
Epoch [41/120    avg_loss:0.016, val_acc:0.981]
Epoch [42/120    avg_loss:0.031, val_acc:0.976]
Epoch [43/120    avg_loss:0.017, val_acc:0.983]
Epoch [44/120    avg_loss:0.021, val_acc:0.971]
Epoch [45/120    avg_loss:0.030, val_acc:0.978]
Epoch [46/120    avg_loss:0.019, val_acc:0.983]
Epoch [47/120    avg_loss:0.032, val_acc:0.986]
Epoch [48/120    avg_loss:0.043, val_acc:0.961]
Epoch [49/120    avg_loss:0.046, val_acc:0.981]
Epoch [50/120    avg_loss:0.023, val_acc:0.974]
Epoch [51/120    avg_loss:0.019, val_acc:0.986]
Epoch [52/120    avg_loss:0.014, val_acc:0.980]
Epoch [53/120    avg_loss:0.012, val_acc:0.985]
Epoch [54/120    avg_loss:0.019, val_acc:0.985]
Epoch [55/120    avg_loss:0.013, val_acc:0.986]
Epoch [56/120    avg_loss:0.014, val_acc:0.984]
Epoch [57/120    avg_loss:0.024, val_acc:0.973]
Epoch [58/120    avg_loss:0.015, val_acc:0.985]
Epoch [59/120    avg_loss:0.010, val_acc:0.985]
Epoch [60/120    avg_loss:0.010, val_acc:0.989]
Epoch [61/120    avg_loss:0.007, val_acc:0.991]
Epoch [62/120    avg_loss:0.013, val_acc:0.981]
Epoch [63/120    avg_loss:0.008, val_acc:0.990]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.005, val_acc:0.988]
Epoch [66/120    avg_loss:0.004, val_acc:0.987]
Epoch [67/120    avg_loss:0.005, val_acc:0.986]
Epoch [68/120    avg_loss:0.007, val_acc:0.986]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.005, val_acc:0.987]
Epoch [71/120    avg_loss:0.007, val_acc:0.991]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.013, val_acc:0.990]
Epoch [74/120    avg_loss:0.014, val_acc:0.986]
Epoch [75/120    avg_loss:0.010, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.990]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.027, val_acc:0.974]
Epoch [80/120    avg_loss:0.057, val_acc:0.971]
Epoch [81/120    avg_loss:0.028, val_acc:0.989]
Epoch [82/120    avg_loss:0.014, val_acc:0.982]
Epoch [83/120    avg_loss:0.041, val_acc:0.965]
Epoch [84/120    avg_loss:0.044, val_acc:0.973]
Epoch [85/120    avg_loss:0.023, val_acc:0.981]
Epoch [86/120    avg_loss:0.017, val_acc:0.985]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     0     0     0     9     9    35     0]
 [    0     0 18055     0    15     0    14     0     6     0]
 [    0     2     0  1987     0     0     0     0    42     5]
 [    0    33     7     0  2921     0     1     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4870     0     7     0]
 [    0     4     0     0     0     0     0  1279     0     7]
 [    0     6     0    45    54     0     0     0  3453    13]
 [    0     0     0     0     7    28     0     0     0   884]]

Accuracy:
99.1323837755766

F1 scores:
[       nan 0.9923771  0.99881061 0.97689282 0.9787234  0.9893859
 0.99672534 0.99224205 0.96939921 0.96717724]

Kappa:
0.9885074102594457
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f207e2607b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.787, val_acc:0.318]
Epoch [2/120    avg_loss:1.171, val_acc:0.450]
Epoch [3/120    avg_loss:0.919, val_acc:0.675]
Epoch [4/120    avg_loss:0.743, val_acc:0.693]
Epoch [5/120    avg_loss:0.612, val_acc:0.669]
Epoch [6/120    avg_loss:0.498, val_acc:0.710]
Epoch [7/120    avg_loss:0.430, val_acc:0.799]
Epoch [8/120    avg_loss:0.337, val_acc:0.841]
Epoch [9/120    avg_loss:0.315, val_acc:0.747]
Epoch [10/120    avg_loss:0.311, val_acc:0.818]
Epoch [11/120    avg_loss:0.262, val_acc:0.798]
Epoch [12/120    avg_loss:0.261, val_acc:0.910]
Epoch [13/120    avg_loss:0.282, val_acc:0.857]
Epoch [14/120    avg_loss:0.221, val_acc:0.829]
Epoch [15/120    avg_loss:0.183, val_acc:0.886]
Epoch [16/120    avg_loss:0.163, val_acc:0.954]
Epoch [17/120    avg_loss:0.130, val_acc:0.961]
Epoch [18/120    avg_loss:0.110, val_acc:0.963]
Epoch [19/120    avg_loss:0.095, val_acc:0.967]
Epoch [20/120    avg_loss:0.080, val_acc:0.957]
Epoch [21/120    avg_loss:0.082, val_acc:0.956]
Epoch [22/120    avg_loss:0.076, val_acc:0.940]
Epoch [23/120    avg_loss:0.070, val_acc:0.970]
Epoch [24/120    avg_loss:0.049, val_acc:0.979]
Epoch [25/120    avg_loss:0.055, val_acc:0.966]
Epoch [26/120    avg_loss:0.080, val_acc:0.947]
Epoch [27/120    avg_loss:0.082, val_acc:0.959]
Epoch [28/120    avg_loss:0.051, val_acc:0.966]
Epoch [29/120    avg_loss:0.044, val_acc:0.970]
Epoch [30/120    avg_loss:0.029, val_acc:0.984]
Epoch [31/120    avg_loss:0.024, val_acc:0.981]
Epoch [32/120    avg_loss:0.031, val_acc:0.981]
Epoch [33/120    avg_loss:0.030, val_acc:0.976]
Epoch [34/120    avg_loss:0.033, val_acc:0.974]
Epoch [35/120    avg_loss:0.057, val_acc:0.974]
Epoch [36/120    avg_loss:0.035, val_acc:0.976]
Epoch [37/120    avg_loss:0.056, val_acc:0.975]
Epoch [38/120    avg_loss:0.035, val_acc:0.977]
Epoch [39/120    avg_loss:0.030, val_acc:0.981]
Epoch [40/120    avg_loss:0.028, val_acc:0.954]
Epoch [41/120    avg_loss:0.029, val_acc:0.971]
Epoch [42/120    avg_loss:0.023, val_acc:0.976]
Epoch [43/120    avg_loss:0.020, val_acc:0.986]
Epoch [44/120    avg_loss:0.015, val_acc:0.984]
Epoch [45/120    avg_loss:0.023, val_acc:0.981]
Epoch [46/120    avg_loss:0.011, val_acc:0.983]
Epoch [47/120    avg_loss:0.020, val_acc:0.983]
Epoch [48/120    avg_loss:0.022, val_acc:0.981]
Epoch [49/120    avg_loss:0.016, val_acc:0.976]
Epoch [50/120    avg_loss:0.012, val_acc:0.985]
Epoch [51/120    avg_loss:0.015, val_acc:0.983]
Epoch [52/120    avg_loss:0.013, val_acc:0.986]
Epoch [53/120    avg_loss:0.009, val_acc:0.986]
Epoch [54/120    avg_loss:0.012, val_acc:0.983]
Epoch [55/120    avg_loss:0.023, val_acc:0.976]
Epoch [56/120    avg_loss:0.054, val_acc:0.941]
Epoch [57/120    avg_loss:0.024, val_acc:0.981]
Epoch [58/120    avg_loss:0.027, val_acc:0.980]
Epoch [59/120    avg_loss:0.017, val_acc:0.984]
Epoch [60/120    avg_loss:0.014, val_acc:0.979]
Epoch [61/120    avg_loss:0.018, val_acc:0.988]
Epoch [62/120    avg_loss:0.010, val_acc:0.990]
Epoch [63/120    avg_loss:0.008, val_acc:0.982]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.010, val_acc:0.986]
Epoch [66/120    avg_loss:0.008, val_acc:0.984]
Epoch [67/120    avg_loss:0.007, val_acc:0.985]
Epoch [68/120    avg_loss:0.007, val_acc:0.986]
Epoch [69/120    avg_loss:0.014, val_acc:0.986]
Epoch [70/120    avg_loss:0.012, val_acc:0.974]
Epoch [71/120    avg_loss:0.016, val_acc:0.986]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.990]
Epoch [75/120    avg_loss:0.005, val_acc:0.989]
Epoch [76/120    avg_loss:0.009, val_acc:0.986]
Epoch [77/120    avg_loss:0.009, val_acc:0.989]
Epoch [78/120    avg_loss:0.011, val_acc:0.985]
Epoch [79/120    avg_loss:0.018, val_acc:0.975]
Epoch [80/120    avg_loss:0.022, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.024, val_acc:0.954]
Epoch [85/120    avg_loss:0.010, val_acc:0.981]
Epoch [86/120    avg_loss:0.012, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.990]
Epoch [88/120    avg_loss:0.009, val_acc:0.992]
Epoch [89/120    avg_loss:0.014, val_acc:0.982]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.981]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.007, val_acc:0.981]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.002, val_acc:0.988]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.003, val_acc:0.990]
Epoch [107/120    avg_loss:0.003, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.988]
Epoch [109/120    avg_loss:0.003, val_acc:0.988]
Epoch [110/120    avg_loss:0.002, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.989]
Epoch [112/120    avg_loss:0.003, val_acc:0.989]
Epoch [113/120    avg_loss:0.002, val_acc:0.989]
Epoch [114/120    avg_loss:0.003, val_acc:0.989]
Epoch [115/120    avg_loss:0.003, val_acc:0.988]
Epoch [116/120    avg_loss:0.002, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.989]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.002, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     0     1     0     0     0    42     0]
 [    0     0 18079     0    10     0     1     0     0     0]
 [    0     0     0  2000     0     0     0     0    31     5]
 [    0    27     4     0  2932     0     1     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0    12     0    39    58     0     0     0  3461     1]
 [    0     0     0     0     0    27     0     0     0   892]]

Accuracy:
99.33241751620756

F1 scores:
[       nan 0.99362364 0.99958533 0.98159509 0.98175121 0.98976109
 0.9990774  0.99805825 0.97246418 0.98021978]

Kappa:
0.9911550709911351
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efddc79e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.688, val_acc:0.385]
Epoch [2/120    avg_loss:1.132, val_acc:0.619]
Epoch [3/120    avg_loss:0.900, val_acc:0.769]
Epoch [4/120    avg_loss:0.756, val_acc:0.671]
Epoch [5/120    avg_loss:0.622, val_acc:0.751]
Epoch [6/120    avg_loss:0.465, val_acc:0.753]
Epoch [7/120    avg_loss:0.432, val_acc:0.763]
Epoch [8/120    avg_loss:0.380, val_acc:0.739]
Epoch [9/120    avg_loss:0.308, val_acc:0.845]
Epoch [10/120    avg_loss:0.306, val_acc:0.801]
Epoch [11/120    avg_loss:0.261, val_acc:0.770]
Epoch [12/120    avg_loss:0.236, val_acc:0.845]
Epoch [13/120    avg_loss:0.254, val_acc:0.867]
Epoch [14/120    avg_loss:0.221, val_acc:0.872]
Epoch [15/120    avg_loss:0.163, val_acc:0.909]
Epoch [16/120    avg_loss:0.164, val_acc:0.849]
Epoch [17/120    avg_loss:0.136, val_acc:0.923]
Epoch [18/120    avg_loss:0.127, val_acc:0.931]
Epoch [19/120    avg_loss:0.115, val_acc:0.952]
Epoch [20/120    avg_loss:0.098, val_acc:0.954]
Epoch [21/120    avg_loss:0.148, val_acc:0.940]
Epoch [22/120    avg_loss:0.092, val_acc:0.962]
Epoch [23/120    avg_loss:0.077, val_acc:0.955]
Epoch [24/120    avg_loss:0.075, val_acc:0.942]
Epoch [25/120    avg_loss:0.071, val_acc:0.946]
Epoch [26/120    avg_loss:0.075, val_acc:0.970]
Epoch [27/120    avg_loss:0.080, val_acc:0.930]
Epoch [28/120    avg_loss:0.049, val_acc:0.977]
Epoch [29/120    avg_loss:0.045, val_acc:0.956]
Epoch [30/120    avg_loss:0.070, val_acc:0.914]
Epoch [31/120    avg_loss:0.059, val_acc:0.970]
Epoch [32/120    avg_loss:0.042, val_acc:0.967]
Epoch [33/120    avg_loss:0.037, val_acc:0.967]
Epoch [34/120    avg_loss:0.034, val_acc:0.974]
Epoch [35/120    avg_loss:0.047, val_acc:0.948]
Epoch [36/120    avg_loss:0.049, val_acc:0.981]
Epoch [37/120    avg_loss:0.034, val_acc:0.973]
Epoch [38/120    avg_loss:0.028, val_acc:0.976]
Epoch [39/120    avg_loss:0.036, val_acc:0.975]
Epoch [40/120    avg_loss:0.041, val_acc:0.980]
Epoch [41/120    avg_loss:0.039, val_acc:0.973]
Epoch [42/120    avg_loss:0.038, val_acc:0.966]
Epoch [43/120    avg_loss:0.023, val_acc:0.979]
Epoch [44/120    avg_loss:0.025, val_acc:0.983]
Epoch [45/120    avg_loss:0.025, val_acc:0.973]
Epoch [46/120    avg_loss:0.033, val_acc:0.978]
Epoch [47/120    avg_loss:0.024, val_acc:0.976]
Epoch [48/120    avg_loss:0.015, val_acc:0.978]
Epoch [49/120    avg_loss:0.012, val_acc:0.981]
Epoch [50/120    avg_loss:0.019, val_acc:0.979]
Epoch [51/120    avg_loss:0.013, val_acc:0.983]
Epoch [52/120    avg_loss:0.016, val_acc:0.984]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.020, val_acc:0.970]
Epoch [55/120    avg_loss:0.016, val_acc:0.982]
Epoch [56/120    avg_loss:0.012, val_acc:0.977]
Epoch [57/120    avg_loss:0.036, val_acc:0.971]
Epoch [58/120    avg_loss:0.021, val_acc:0.985]
Epoch [59/120    avg_loss:0.020, val_acc:0.981]
Epoch [60/120    avg_loss:0.016, val_acc:0.981]
Epoch [61/120    avg_loss:0.013, val_acc:0.968]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.013, val_acc:0.976]
Epoch [64/120    avg_loss:0.021, val_acc:0.978]
Epoch [65/120    avg_loss:0.029, val_acc:0.965]
Epoch [66/120    avg_loss:0.028, val_acc:0.972]
Epoch [67/120    avg_loss:0.037, val_acc:0.967]
Epoch [68/120    avg_loss:0.034, val_acc:0.974]
Epoch [69/120    avg_loss:0.019, val_acc:0.958]
Epoch [70/120    avg_loss:0.019, val_acc:0.969]
Epoch [71/120    avg_loss:0.008, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.984]
Epoch [73/120    avg_loss:0.008, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.982]
Epoch [75/120    avg_loss:0.006, val_acc:0.984]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.007, val_acc:0.982]
Epoch [79/120    avg_loss:0.007, val_acc:0.983]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.983]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.005, val_acc:0.983]
Epoch [85/120    avg_loss:0.006, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.005, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.982]
Epoch [92/120    avg_loss:0.004, val_acc:0.982]
Epoch [93/120    avg_loss:0.006, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.982]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.005, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.982]
Epoch [104/120    avg_loss:0.004, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.982]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.982]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6310     0     0     2     0    21     0    98     1]
 [    0     0 18047     0    26     0     3     0    14     0]
 [    0     0     0  2009     0     0     0     0    18     9]
 [    0    26     1     3  2932     0     3     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0     1     0     0     0     0     2  1285     0     2]
 [    0     6     0    30    54     0     0     0  3477     4]
 [    0     0     0     0    16    49     0     0     0   854]]

Accuracy:
99.04080206299858

F1 scores:
[       nan 0.98786693 0.99872717 0.98528691 0.97700766 0.98157202
 0.99683124 0.99805825 0.96784969 0.95472331]

Kappa:
0.9872980029348155
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f73635c3860>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.703, val_acc:0.312]
Epoch [2/120    avg_loss:1.157, val_acc:0.441]
Epoch [3/120    avg_loss:0.912, val_acc:0.610]
Epoch [4/120    avg_loss:0.714, val_acc:0.698]
Epoch [5/120    avg_loss:0.570, val_acc:0.730]
Epoch [6/120    avg_loss:0.503, val_acc:0.772]
Epoch [7/120    avg_loss:0.468, val_acc:0.740]
Epoch [8/120    avg_loss:0.339, val_acc:0.826]
Epoch [9/120    avg_loss:0.295, val_acc:0.770]
Epoch [10/120    avg_loss:0.276, val_acc:0.883]
Epoch [11/120    avg_loss:0.278, val_acc:0.780]
Epoch [12/120    avg_loss:0.233, val_acc:0.907]
Epoch [13/120    avg_loss:0.175, val_acc:0.912]
Epoch [14/120    avg_loss:0.162, val_acc:0.816]
Epoch [15/120    avg_loss:0.153, val_acc:0.926]
Epoch [16/120    avg_loss:0.148, val_acc:0.926]
Epoch [17/120    avg_loss:0.126, val_acc:0.922]
Epoch [18/120    avg_loss:0.073, val_acc:0.965]
Epoch [19/120    avg_loss:0.069, val_acc:0.970]
Epoch [20/120    avg_loss:0.067, val_acc:0.967]
Epoch [21/120    avg_loss:0.083, val_acc:0.944]
Epoch [22/120    avg_loss:0.123, val_acc:0.910]
Epoch [23/120    avg_loss:0.123, val_acc:0.954]
Epoch [24/120    avg_loss:0.095, val_acc:0.891]
Epoch [25/120    avg_loss:0.064, val_acc:0.965]
Epoch [26/120    avg_loss:0.058, val_acc:0.968]
Epoch [27/120    avg_loss:0.043, val_acc:0.973]
Epoch [28/120    avg_loss:0.037, val_acc:0.973]
Epoch [29/120    avg_loss:0.044, val_acc:0.963]
Epoch [30/120    avg_loss:0.040, val_acc:0.965]
Epoch [31/120    avg_loss:0.054, val_acc:0.970]
Epoch [32/120    avg_loss:0.041, val_acc:0.968]
Epoch [33/120    avg_loss:0.037, val_acc:0.982]
Epoch [34/120    avg_loss:0.027, val_acc:0.980]
Epoch [35/120    avg_loss:0.033, val_acc:0.976]
Epoch [36/120    avg_loss:0.025, val_acc:0.974]
Epoch [37/120    avg_loss:0.038, val_acc:0.977]
Epoch [38/120    avg_loss:0.043, val_acc:0.971]
Epoch [39/120    avg_loss:0.030, val_acc:0.975]
Epoch [40/120    avg_loss:0.020, val_acc:0.983]
Epoch [41/120    avg_loss:0.023, val_acc:0.982]
Epoch [42/120    avg_loss:0.020, val_acc:0.973]
Epoch [43/120    avg_loss:0.023, val_acc:0.924]
Epoch [44/120    avg_loss:0.019, val_acc:0.982]
Epoch [45/120    avg_loss:0.034, val_acc:0.981]
Epoch [46/120    avg_loss:0.018, val_acc:0.980]
Epoch [47/120    avg_loss:0.013, val_acc:0.980]
Epoch [48/120    avg_loss:0.012, val_acc:0.961]
Epoch [49/120    avg_loss:0.017, val_acc:0.981]
Epoch [50/120    avg_loss:0.010, val_acc:0.980]
Epoch [51/120    avg_loss:0.010, val_acc:0.984]
Epoch [52/120    avg_loss:0.021, val_acc:0.979]
Epoch [53/120    avg_loss:0.014, val_acc:0.982]
Epoch [54/120    avg_loss:0.012, val_acc:0.971]
Epoch [55/120    avg_loss:0.011, val_acc:0.984]
Epoch [56/120    avg_loss:0.008, val_acc:0.985]
Epoch [57/120    avg_loss:0.013, val_acc:0.986]
Epoch [58/120    avg_loss:0.017, val_acc:0.976]
Epoch [59/120    avg_loss:0.018, val_acc:0.982]
Epoch [60/120    avg_loss:0.056, val_acc:0.964]
Epoch [61/120    avg_loss:0.026, val_acc:0.964]
Epoch [62/120    avg_loss:0.018, val_acc:0.979]
Epoch [63/120    avg_loss:0.011, val_acc:0.981]
Epoch [64/120    avg_loss:0.017, val_acc:0.984]
Epoch [65/120    avg_loss:0.013, val_acc:0.981]
Epoch [66/120    avg_loss:0.015, val_acc:0.970]
Epoch [67/120    avg_loss:0.021, val_acc:0.979]
Epoch [68/120    avg_loss:0.010, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.981]
Epoch [70/120    avg_loss:0.025, val_acc:0.982]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.004, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.005, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.010, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     0     0     7    11    22     0]
 [    0     0 17996     0    43     0    43     0     8     0]
 [    0     0     0  1989     3     0     0     0    35     9]
 [    0    25     1     0  2931     0     5     0     9     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4874     0     4     0]
 [    0     8     0     0     0     7     0  1272     0     3]
 [    0     2     0    11    50     0     0     0  3507     1]
 [    0     0     0     0    14    22     0     0     0   883]]

Accuracy:
99.17094449666209

F1 scores:
[       nan 0.99416751 0.99736747 0.98562934 0.97488774 0.98901099
 0.99398389 0.98872911 0.98015651 0.97246696]

Kappa:
0.9890252864297778
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4648f72780>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.711, val_acc:0.519]
Epoch [2/120    avg_loss:1.135, val_acc:0.579]
Epoch [3/120    avg_loss:0.899, val_acc:0.514]
Epoch [4/120    avg_loss:0.765, val_acc:0.746]
Epoch [5/120    avg_loss:0.617, val_acc:0.716]
Epoch [6/120    avg_loss:0.522, val_acc:0.667]
Epoch [7/120    avg_loss:0.412, val_acc:0.788]
Epoch [8/120    avg_loss:0.351, val_acc:0.752]
Epoch [9/120    avg_loss:0.298, val_acc:0.788]
Epoch [10/120    avg_loss:0.277, val_acc:0.837]
Epoch [11/120    avg_loss:0.277, val_acc:0.810]
Epoch [12/120    avg_loss:0.242, val_acc:0.845]
Epoch [13/120    avg_loss:0.234, val_acc:0.877]
Epoch [14/120    avg_loss:0.190, val_acc:0.916]
Epoch [15/120    avg_loss:0.162, val_acc:0.938]
Epoch [16/120    avg_loss:0.126, val_acc:0.914]
Epoch [17/120    avg_loss:0.103, val_acc:0.930]
Epoch [18/120    avg_loss:0.095, val_acc:0.938]
Epoch [19/120    avg_loss:0.108, val_acc:0.945]
Epoch [20/120    avg_loss:0.106, val_acc:0.820]
Epoch [21/120    avg_loss:0.113, val_acc:0.959]
Epoch [22/120    avg_loss:0.081, val_acc:0.936]
Epoch [23/120    avg_loss:0.100, val_acc:0.959]
Epoch [24/120    avg_loss:0.092, val_acc:0.970]
Epoch [25/120    avg_loss:0.089, val_acc:0.920]
Epoch [26/120    avg_loss:0.063, val_acc:0.967]
Epoch [27/120    avg_loss:0.077, val_acc:0.968]
Epoch [28/120    avg_loss:0.036, val_acc:0.982]
Epoch [29/120    avg_loss:0.038, val_acc:0.973]
Epoch [30/120    avg_loss:0.038, val_acc:0.977]
Epoch [31/120    avg_loss:0.043, val_acc:0.974]
Epoch [32/120    avg_loss:0.036, val_acc:0.979]
Epoch [33/120    avg_loss:0.031, val_acc:0.976]
Epoch [34/120    avg_loss:0.054, val_acc:0.970]
Epoch [35/120    avg_loss:0.052, val_acc:0.983]
Epoch [36/120    avg_loss:0.048, val_acc:0.981]
Epoch [37/120    avg_loss:0.047, val_acc:0.986]
Epoch [38/120    avg_loss:0.029, val_acc:0.984]
Epoch [39/120    avg_loss:0.036, val_acc:0.909]
Epoch [40/120    avg_loss:0.036, val_acc:0.969]
Epoch [41/120    avg_loss:0.024, val_acc:0.984]
Epoch [42/120    avg_loss:0.042, val_acc:0.984]
Epoch [43/120    avg_loss:0.027, val_acc:0.985]
Epoch [44/120    avg_loss:0.034, val_acc:0.976]
Epoch [45/120    avg_loss:0.014, val_acc:0.981]
Epoch [46/120    avg_loss:0.019, val_acc:0.981]
Epoch [47/120    avg_loss:0.021, val_acc:0.986]
Epoch [48/120    avg_loss:0.061, val_acc:0.978]
Epoch [49/120    avg_loss:0.064, val_acc:0.970]
Epoch [50/120    avg_loss:0.040, val_acc:0.982]
Epoch [51/120    avg_loss:0.028, val_acc:0.953]
Epoch [52/120    avg_loss:0.019, val_acc:0.984]
Epoch [53/120    avg_loss:0.029, val_acc:0.983]
Epoch [54/120    avg_loss:0.019, val_acc:0.985]
Epoch [55/120    avg_loss:0.011, val_acc:0.989]
Epoch [56/120    avg_loss:0.013, val_acc:0.984]
Epoch [57/120    avg_loss:0.016, val_acc:0.985]
Epoch [58/120    avg_loss:0.018, val_acc:0.970]
Epoch [59/120    avg_loss:0.023, val_acc:0.981]
Epoch [60/120    avg_loss:0.052, val_acc:0.961]
Epoch [61/120    avg_loss:0.067, val_acc:0.969]
Epoch [62/120    avg_loss:0.033, val_acc:0.974]
Epoch [63/120    avg_loss:0.012, val_acc:0.986]
Epoch [64/120    avg_loss:0.024, val_acc:0.989]
Epoch [65/120    avg_loss:0.016, val_acc:0.986]
Epoch [66/120    avg_loss:0.008, val_acc:0.989]
Epoch [67/120    avg_loss:0.022, val_acc:0.968]
Epoch [68/120    avg_loss:0.018, val_acc:0.990]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.012, val_acc:0.974]
Epoch [71/120    avg_loss:0.009, val_acc:0.991]
Epoch [72/120    avg_loss:0.008, val_acc:0.990]
Epoch [73/120    avg_loss:0.011, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.010, val_acc:0.989]
Epoch [76/120    avg_loss:0.006, val_acc:0.992]
Epoch [77/120    avg_loss:0.008, val_acc:0.992]
Epoch [78/120    avg_loss:0.016, val_acc:0.992]
Epoch [79/120    avg_loss:0.009, val_acc:0.990]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.012, val_acc:0.988]
Epoch [82/120    avg_loss:0.086, val_acc:0.961]
Epoch [83/120    avg_loss:0.037, val_acc:0.973]
Epoch [84/120    avg_loss:0.018, val_acc:0.981]
Epoch [85/120    avg_loss:0.016, val_acc:0.969]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.990]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.010, val_acc:0.993]
Epoch [91/120    avg_loss:0.007, val_acc:0.994]
Epoch [92/120    avg_loss:0.006, val_acc:0.993]
Epoch [93/120    avg_loss:0.008, val_acc:0.994]
Epoch [94/120    avg_loss:0.006, val_acc:0.994]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.993]
Epoch [97/120    avg_loss:0.006, val_acc:0.994]
Epoch [98/120    avg_loss:0.008, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.006, val_acc:0.994]
Epoch [101/120    avg_loss:0.005, val_acc:0.994]
Epoch [102/120    avg_loss:0.004, val_acc:0.994]
Epoch [103/120    avg_loss:0.006, val_acc:0.993]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.004, val_acc:0.993]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.993]
Epoch [111/120    avg_loss:0.005, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.993]
Epoch [113/120    avg_loss:0.007, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.993]
Epoch [115/120    avg_loss:0.007, val_acc:0.994]
Epoch [116/120    avg_loss:0.005, val_acc:0.994]
Epoch [117/120    avg_loss:0.003, val_acc:0.994]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.007, val_acc:0.993]
Epoch [120/120    avg_loss:0.004, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     0     0     0     0    33     2]
 [    0     0 18044     0    26     0    10     0    10     0]
 [    0     3     0  2013     0     0     0     0    19     1]
 [    0    28     6     0  2923     0     1     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4856     0    13     0]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0     2     0    42    52     0     0     0  3471     4]
 [    0     0     0     0     0    33     0     0     0   886]]

Accuracy:
99.24565589376522

F1 scores:
[       nan 0.99471311 0.99831254 0.98411146 0.97873765 0.98751419
 0.99640915 0.99805825 0.973496   0.97630854]

Kappa:
0.9900084566512366
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4da3800828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.707, val_acc:0.351]
Epoch [2/120    avg_loss:1.116, val_acc:0.530]
Epoch [3/120    avg_loss:0.893, val_acc:0.616]
Epoch [4/120    avg_loss:0.723, val_acc:0.623]
Epoch [5/120    avg_loss:0.562, val_acc:0.663]
Epoch [6/120    avg_loss:0.508, val_acc:0.660]
Epoch [7/120    avg_loss:0.413, val_acc:0.802]
Epoch [8/120    avg_loss:0.347, val_acc:0.806]
Epoch [9/120    avg_loss:0.297, val_acc:0.852]
Epoch [10/120    avg_loss:0.236, val_acc:0.891]
Epoch [11/120    avg_loss:0.184, val_acc:0.946]
Epoch [12/120    avg_loss:0.197, val_acc:0.922]
Epoch [13/120    avg_loss:0.239, val_acc:0.937]
Epoch [14/120    avg_loss:0.182, val_acc:0.949]
Epoch [15/120    avg_loss:0.134, val_acc:0.959]
Epoch [16/120    avg_loss:0.147, val_acc:0.954]
Epoch [17/120    avg_loss:0.100, val_acc:0.970]
Epoch [18/120    avg_loss:0.120, val_acc:0.943]
Epoch [19/120    avg_loss:0.127, val_acc:0.938]
Epoch [20/120    avg_loss:0.135, val_acc:0.953]
Epoch [21/120    avg_loss:0.111, val_acc:0.952]
Epoch [22/120    avg_loss:0.111, val_acc:0.946]
Epoch [23/120    avg_loss:0.100, val_acc:0.964]
Epoch [24/120    avg_loss:0.050, val_acc:0.976]
Epoch [25/120    avg_loss:0.093, val_acc:0.930]
Epoch [26/120    avg_loss:0.074, val_acc:0.976]
Epoch [27/120    avg_loss:0.074, val_acc:0.980]
Epoch [28/120    avg_loss:0.039, val_acc:0.972]
Epoch [29/120    avg_loss:0.038, val_acc:0.968]
Epoch [30/120    avg_loss:0.039, val_acc:0.977]
Epoch [31/120    avg_loss:0.030, val_acc:0.981]
Epoch [32/120    avg_loss:0.039, val_acc:0.974]
Epoch [33/120    avg_loss:0.022, val_acc:0.986]
Epoch [34/120    avg_loss:0.021, val_acc:0.983]
Epoch [35/120    avg_loss:0.026, val_acc:0.986]
Epoch [36/120    avg_loss:0.022, val_acc:0.984]
Epoch [37/120    avg_loss:0.022, val_acc:0.985]
Epoch [38/120    avg_loss:0.020, val_acc:0.980]
Epoch [39/120    avg_loss:0.036, val_acc:0.978]
Epoch [40/120    avg_loss:0.069, val_acc:0.965]
Epoch [41/120    avg_loss:0.059, val_acc:0.972]
Epoch [42/120    avg_loss:0.078, val_acc:0.954]
Epoch [43/120    avg_loss:0.056, val_acc:0.968]
Epoch [44/120    avg_loss:0.050, val_acc:0.986]
Epoch [45/120    avg_loss:0.026, val_acc:0.983]
Epoch [46/120    avg_loss:0.017, val_acc:0.987]
Epoch [47/120    avg_loss:0.017, val_acc:0.989]
Epoch [48/120    avg_loss:0.013, val_acc:0.989]
Epoch [49/120    avg_loss:0.018, val_acc:0.986]
Epoch [50/120    avg_loss:0.015, val_acc:0.986]
Epoch [51/120    avg_loss:0.020, val_acc:0.986]
Epoch [52/120    avg_loss:0.031, val_acc:0.970]
Epoch [53/120    avg_loss:0.029, val_acc:0.981]
Epoch [54/120    avg_loss:0.012, val_acc:0.986]
Epoch [55/120    avg_loss:0.014, val_acc:0.990]
Epoch [56/120    avg_loss:0.014, val_acc:0.990]
Epoch [57/120    avg_loss:0.010, val_acc:0.987]
Epoch [58/120    avg_loss:0.013, val_acc:0.986]
Epoch [59/120    avg_loss:0.011, val_acc:0.981]
Epoch [60/120    avg_loss:0.012, val_acc:0.988]
Epoch [61/120    avg_loss:0.011, val_acc:0.989]
Epoch [62/120    avg_loss:0.007, val_acc:0.981]
Epoch [63/120    avg_loss:0.014, val_acc:0.926]
Epoch [64/120    avg_loss:0.035, val_acc:0.983]
Epoch [65/120    avg_loss:0.045, val_acc:0.981]
Epoch [66/120    avg_loss:0.039, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.987]
Epoch [68/120    avg_loss:0.012, val_acc:0.955]
Epoch [69/120    avg_loss:0.012, val_acc:0.986]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.006, val_acc:0.992]
Epoch [74/120    avg_loss:0.005, val_acc:0.992]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.008, val_acc:0.992]
Epoch [77/120    avg_loss:0.008, val_acc:0.992]
Epoch [78/120    avg_loss:0.007, val_acc:0.992]
Epoch [79/120    avg_loss:0.007, val_acc:0.992]
Epoch [80/120    avg_loss:0.007, val_acc:0.992]
Epoch [81/120    avg_loss:0.007, val_acc:0.991]
Epoch [82/120    avg_loss:0.006, val_acc:0.992]
Epoch [83/120    avg_loss:0.005, val_acc:0.992]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.992]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.008, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.993]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.992]
Epoch [97/120    avg_loss:0.004, val_acc:0.993]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.006, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.992]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.993]
Epoch [110/120    avg_loss:0.007, val_acc:0.993]
Epoch [111/120    avg_loss:0.005, val_acc:0.993]
Epoch [112/120    avg_loss:0.009, val_acc:0.993]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.993]
Epoch [115/120    avg_loss:0.004, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.992]
Epoch [119/120    avg_loss:0.007, val_acc:0.992]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6366     0     0     0     0     0     4    62     0]
 [    0     0 18064     0    10     0     0     0    16     0]
 [    0     3     0  1982     0     0     0     0    48     3]
 [    0    30    13     0  2910     0     3     0    15     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     0     0  4864     0     0     0]
 [    0     0     0     0     0     0     0  1279     0    11]
 [    0     2     0    17    52     0     0     0  3499     1]
 [    0     0     0     0     3    17     0     0     0   899]]

Accuracy:
99.2167353529511

F1 scores:
[       nan 0.99212967 0.99853514 0.98240397 0.97864469 0.99352874
 0.99825552 0.99417023 0.97046179 0.98037077]

Kappa:
0.9896216337208918
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd312e5e780>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.693, val_acc:0.578]
Epoch [2/120    avg_loss:1.106, val_acc:0.639]
Epoch [3/120    avg_loss:0.815, val_acc:0.632]
Epoch [4/120    avg_loss:0.622, val_acc:0.601]
Epoch [5/120    avg_loss:0.498, val_acc:0.698]
Epoch [6/120    avg_loss:0.419, val_acc:0.720]
Epoch [7/120    avg_loss:0.368, val_acc:0.753]
Epoch [8/120    avg_loss:0.353, val_acc:0.778]
Epoch [9/120    avg_loss:0.306, val_acc:0.810]
Epoch [10/120    avg_loss:0.290, val_acc:0.829]
Epoch [11/120    avg_loss:0.273, val_acc:0.736]
Epoch [12/120    avg_loss:0.199, val_acc:0.925]
Epoch [13/120    avg_loss:0.196, val_acc:0.835]
Epoch [14/120    avg_loss:0.195, val_acc:0.864]
Epoch [15/120    avg_loss:0.200, val_acc:0.926]
Epoch [16/120    avg_loss:0.147, val_acc:0.935]
Epoch [17/120    avg_loss:0.138, val_acc:0.952]
Epoch [18/120    avg_loss:0.094, val_acc:0.914]
Epoch [19/120    avg_loss:0.088, val_acc:0.935]
Epoch [20/120    avg_loss:0.118, val_acc:0.934]
Epoch [21/120    avg_loss:0.093, val_acc:0.970]
Epoch [22/120    avg_loss:0.100, val_acc:0.968]
Epoch [23/120    avg_loss:0.080, val_acc:0.943]
Epoch [24/120    avg_loss:0.062, val_acc:0.943]
Epoch [25/120    avg_loss:0.056, val_acc:0.977]
Epoch [26/120    avg_loss:0.063, val_acc:0.970]
Epoch [27/120    avg_loss:0.053, val_acc:0.941]
Epoch [28/120    avg_loss:0.100, val_acc:0.971]
Epoch [29/120    avg_loss:0.040, val_acc:0.969]
Epoch [30/120    avg_loss:0.050, val_acc:0.957]
Epoch [31/120    avg_loss:0.027, val_acc:0.974]
Epoch [32/120    avg_loss:0.052, val_acc:0.956]
Epoch [33/120    avg_loss:0.049, val_acc:0.962]
Epoch [34/120    avg_loss:0.070, val_acc:0.954]
Epoch [35/120    avg_loss:0.058, val_acc:0.972]
Epoch [36/120    avg_loss:0.041, val_acc:0.976]
Epoch [37/120    avg_loss:0.035, val_acc:0.965]
Epoch [38/120    avg_loss:0.045, val_acc:0.960]
Epoch [39/120    avg_loss:0.041, val_acc:0.978]
Epoch [40/120    avg_loss:0.025, val_acc:0.981]
Epoch [41/120    avg_loss:0.019, val_acc:0.981]
Epoch [42/120    avg_loss:0.018, val_acc:0.982]
Epoch [43/120    avg_loss:0.013, val_acc:0.981]
Epoch [44/120    avg_loss:0.021, val_acc:0.981]
Epoch [45/120    avg_loss:0.016, val_acc:0.981]
Epoch [46/120    avg_loss:0.015, val_acc:0.983]
Epoch [47/120    avg_loss:0.017, val_acc:0.981]
Epoch [48/120    avg_loss:0.013, val_acc:0.982]
Epoch [49/120    avg_loss:0.017, val_acc:0.985]
Epoch [50/120    avg_loss:0.015, val_acc:0.986]
Epoch [51/120    avg_loss:0.014, val_acc:0.983]
Epoch [52/120    avg_loss:0.013, val_acc:0.984]
Epoch [53/120    avg_loss:0.016, val_acc:0.985]
Epoch [54/120    avg_loss:0.015, val_acc:0.984]
Epoch [55/120    avg_loss:0.017, val_acc:0.987]
Epoch [56/120    avg_loss:0.013, val_acc:0.985]
Epoch [57/120    avg_loss:0.013, val_acc:0.986]
Epoch [58/120    avg_loss:0.013, val_acc:0.986]
Epoch [59/120    avg_loss:0.015, val_acc:0.984]
Epoch [60/120    avg_loss:0.012, val_acc:0.985]
Epoch [61/120    avg_loss:0.011, val_acc:0.986]
Epoch [62/120    avg_loss:0.016, val_acc:0.985]
Epoch [63/120    avg_loss:0.011, val_acc:0.984]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.985]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.986]
Epoch [68/120    avg_loss:0.011, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.014, val_acc:0.986]
Epoch [71/120    avg_loss:0.014, val_acc:0.986]
Epoch [72/120    avg_loss:0.011, val_acc:0.986]
Epoch [73/120    avg_loss:0.011, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.988]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.987]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.986]
Epoch [80/120    avg_loss:0.014, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.985]
Epoch [85/120    avg_loss:0.009, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.009, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.010, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.013, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.010, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     0     0     0     0    63     1]
 [    0     0 18052     0    12     0    15     0    11     0]
 [    0     4     0  1994     2     0     0     0    30     6]
 [    0    44     4     0  2913     0     0     0     9     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4866     0     8     0]
 [    0     0     0     0     0     0     1  1285     0     4]
 [    0    35     0    51    42     0     0     0  3443     0]
 [    0     0     0     0     9    14     0     0     0   896]]

Accuracy:
99.10587327983033

F1 scores:
[       nan 0.98858961 0.99872752 0.97721147 0.97915966 0.99466463
 0.99713115 0.99805825 0.96510161 0.98030635]

Kappa:
0.9881558103958271
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e7f8bc898>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.722, val_acc:0.264]
Epoch [2/120    avg_loss:1.168, val_acc:0.649]
Epoch [3/120    avg_loss:0.939, val_acc:0.627]
Epoch [4/120    avg_loss:0.782, val_acc:0.647]
Epoch [5/120    avg_loss:0.613, val_acc:0.682]
Epoch [6/120    avg_loss:0.490, val_acc:0.682]
Epoch [7/120    avg_loss:0.384, val_acc:0.761]
Epoch [8/120    avg_loss:0.365, val_acc:0.708]
Epoch [9/120    avg_loss:0.371, val_acc:0.776]
Epoch [10/120    avg_loss:0.316, val_acc:0.886]
Epoch [11/120    avg_loss:0.241, val_acc:0.844]
Epoch [12/120    avg_loss:0.194, val_acc:0.937]
Epoch [13/120    avg_loss:0.167, val_acc:0.911]
Epoch [14/120    avg_loss:0.167, val_acc:0.942]
Epoch [15/120    avg_loss:0.143, val_acc:0.948]
Epoch [16/120    avg_loss:0.119, val_acc:0.961]
Epoch [17/120    avg_loss:0.087, val_acc:0.948]
Epoch [18/120    avg_loss:0.111, val_acc:0.959]
Epoch [19/120    avg_loss:0.113, val_acc:0.957]
Epoch [20/120    avg_loss:0.099, val_acc:0.946]
Epoch [21/120    avg_loss:0.079, val_acc:0.970]
Epoch [22/120    avg_loss:0.060, val_acc:0.959]
Epoch [23/120    avg_loss:0.062, val_acc:0.966]
Epoch [24/120    avg_loss:0.060, val_acc:0.977]
Epoch [25/120    avg_loss:0.074, val_acc:0.969]
Epoch [26/120    avg_loss:0.069, val_acc:0.943]
Epoch [27/120    avg_loss:0.056, val_acc:0.973]
Epoch [28/120    avg_loss:0.028, val_acc:0.981]
Epoch [29/120    avg_loss:0.039, val_acc:0.978]
Epoch [30/120    avg_loss:0.024, val_acc:0.982]
Epoch [31/120    avg_loss:0.062, val_acc:0.976]
Epoch [32/120    avg_loss:0.044, val_acc:0.981]
Epoch [33/120    avg_loss:0.027, val_acc:0.977]
Epoch [34/120    avg_loss:0.044, val_acc:0.981]
Epoch [35/120    avg_loss:0.023, val_acc:0.986]
Epoch [36/120    avg_loss:0.034, val_acc:0.982]
Epoch [37/120    avg_loss:0.018, val_acc:0.984]
Epoch [38/120    avg_loss:0.027, val_acc:0.952]
Epoch [39/120    avg_loss:0.017, val_acc:0.988]
Epoch [40/120    avg_loss:0.024, val_acc:0.969]
Epoch [41/120    avg_loss:0.019, val_acc:0.983]
Epoch [42/120    avg_loss:0.017, val_acc:0.983]
Epoch [43/120    avg_loss:0.018, val_acc:0.986]
Epoch [44/120    avg_loss:0.020, val_acc:0.981]
Epoch [45/120    avg_loss:0.016, val_acc:0.989]
Epoch [46/120    avg_loss:0.012, val_acc:0.991]
Epoch [47/120    avg_loss:0.040, val_acc:0.980]
Epoch [48/120    avg_loss:0.017, val_acc:0.989]
Epoch [49/120    avg_loss:0.013, val_acc:0.989]
Epoch [50/120    avg_loss:0.014, val_acc:0.984]
Epoch [51/120    avg_loss:0.011, val_acc:0.987]
Epoch [52/120    avg_loss:0.013, val_acc:0.989]
Epoch [53/120    avg_loss:0.012, val_acc:0.990]
Epoch [54/120    avg_loss:0.023, val_acc:0.956]
Epoch [55/120    avg_loss:0.020, val_acc:0.976]
Epoch [56/120    avg_loss:0.027, val_acc:0.981]
Epoch [57/120    avg_loss:0.044, val_acc:0.979]
Epoch [58/120    avg_loss:0.025, val_acc:0.991]
Epoch [59/120    avg_loss:0.021, val_acc:0.983]
Epoch [60/120    avg_loss:0.018, val_acc:0.984]
Epoch [61/120    avg_loss:0.014, val_acc:0.989]
Epoch [62/120    avg_loss:0.008, val_acc:0.990]
Epoch [63/120    avg_loss:0.006, val_acc:0.994]
Epoch [64/120    avg_loss:0.018, val_acc:0.976]
Epoch [65/120    avg_loss:0.016, val_acc:0.986]
Epoch [66/120    avg_loss:0.011, val_acc:0.992]
Epoch [67/120    avg_loss:0.008, val_acc:0.990]
Epoch [68/120    avg_loss:0.008, val_acc:0.990]
Epoch [69/120    avg_loss:0.015, val_acc:0.991]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.987]
Epoch [74/120    avg_loss:0.016, val_acc:0.985]
Epoch [75/120    avg_loss:0.011, val_acc:0.983]
Epoch [76/120    avg_loss:0.011, val_acc:0.991]
Epoch [77/120    avg_loss:0.005, val_acc:0.991]
Epoch [78/120    avg_loss:0.005, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.007, val_acc:0.991]
Epoch [82/120    avg_loss:0.004, val_acc:0.991]
Epoch [83/120    avg_loss:0.010, val_acc:0.992]
Epoch [84/120    avg_loss:0.004, val_acc:0.992]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.992]
Epoch [87/120    avg_loss:0.004, val_acc:0.992]
Epoch [88/120    avg_loss:0.004, val_acc:0.992]
Epoch [89/120    avg_loss:0.003, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.003, val_acc:0.992]
Epoch [93/120    avg_loss:0.004, val_acc:0.992]
Epoch [94/120    avg_loss:0.004, val_acc:0.992]
Epoch [95/120    avg_loss:0.004, val_acc:0.992]
Epoch [96/120    avg_loss:0.003, val_acc:0.992]
Epoch [97/120    avg_loss:0.004, val_acc:0.992]
Epoch [98/120    avg_loss:0.004, val_acc:0.992]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.992]
Epoch [101/120    avg_loss:0.004, val_acc:0.992]
Epoch [102/120    avg_loss:0.003, val_acc:0.992]
Epoch [103/120    avg_loss:0.003, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.003, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.003, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.003, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.004, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     0     0     0     6    21     3]
 [    0     1 18059     0    18     0     0     0    12     0]
 [    0     5     0  1998     0     0     0     0    29     4]
 [    0    38     4     1  2918     0     2     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0     1     0  4841     0     7     0]
 [    0     2     0     0     0     0     0  1281     0     7]
 [    0     4     0    17    52     0     0     0  3497     1]
 [    0     0     0     0    13    14     0     0     0   892]]

Accuracy:
99.27698647964718

F1 scores:
[       nan 0.99379075 0.99823116 0.98617966 0.9768999  0.99466463
 0.99598807 0.99417928 0.97886634 0.97646415]

Kappa:
0.9904192098847948
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa87aff4828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.769, val_acc:0.313]
Epoch [2/120    avg_loss:1.146, val_acc:0.712]
Epoch [3/120    avg_loss:0.920, val_acc:0.763]
Epoch [4/120    avg_loss:0.725, val_acc:0.653]
Epoch [5/120    avg_loss:0.568, val_acc:0.739]
Epoch [6/120    avg_loss:0.473, val_acc:0.758]
Epoch [7/120    avg_loss:0.373, val_acc:0.825]
Epoch [8/120    avg_loss:0.368, val_acc:0.816]
Epoch [9/120    avg_loss:0.275, val_acc:0.792]
Epoch [10/120    avg_loss:0.266, val_acc:0.831]
Epoch [11/120    avg_loss:0.218, val_acc:0.871]
Epoch [12/120    avg_loss:0.199, val_acc:0.927]
Epoch [13/120    avg_loss:0.158, val_acc:0.932]
Epoch [14/120    avg_loss:0.151, val_acc:0.947]
Epoch [15/120    avg_loss:0.118, val_acc:0.944]
Epoch [16/120    avg_loss:0.133, val_acc:0.965]
Epoch [17/120    avg_loss:0.094, val_acc:0.969]
Epoch [18/120    avg_loss:0.124, val_acc:0.946]
Epoch [19/120    avg_loss:0.137, val_acc:0.943]
Epoch [20/120    avg_loss:0.124, val_acc:0.952]
Epoch [21/120    avg_loss:0.121, val_acc:0.909]
Epoch [22/120    avg_loss:0.110, val_acc:0.946]
Epoch [23/120    avg_loss:0.080, val_acc:0.978]
Epoch [24/120    avg_loss:0.068, val_acc:0.969]
Epoch [25/120    avg_loss:0.069, val_acc:0.932]
Epoch [26/120    avg_loss:0.041, val_acc:0.970]
Epoch [27/120    avg_loss:0.059, val_acc:0.979]
Epoch [28/120    avg_loss:0.046, val_acc:0.976]
Epoch [29/120    avg_loss:0.047, val_acc:0.985]
Epoch [30/120    avg_loss:0.055, val_acc:0.976]
Epoch [31/120    avg_loss:0.038, val_acc:0.986]
Epoch [32/120    avg_loss:0.027, val_acc:0.986]
Epoch [33/120    avg_loss:0.022, val_acc:0.988]
Epoch [34/120    avg_loss:0.019, val_acc:0.992]
Epoch [35/120    avg_loss:0.019, val_acc:0.981]
Epoch [36/120    avg_loss:0.024, val_acc:0.968]
Epoch [37/120    avg_loss:0.044, val_acc:0.980]
Epoch [38/120    avg_loss:0.042, val_acc:0.989]
Epoch [39/120    avg_loss:0.033, val_acc:0.976]
Epoch [40/120    avg_loss:0.044, val_acc:0.974]
Epoch [41/120    avg_loss:0.024, val_acc:0.981]
Epoch [42/120    avg_loss:0.026, val_acc:0.976]
Epoch [43/120    avg_loss:0.013, val_acc:0.986]
Epoch [44/120    avg_loss:0.015, val_acc:0.963]
Epoch [45/120    avg_loss:0.014, val_acc:0.992]
Epoch [46/120    avg_loss:0.018, val_acc:0.986]
Epoch [47/120    avg_loss:0.011, val_acc:0.992]
Epoch [48/120    avg_loss:0.011, val_acc:0.989]
Epoch [49/120    avg_loss:0.013, val_acc:0.992]
Epoch [50/120    avg_loss:0.007, val_acc:0.984]
Epoch [51/120    avg_loss:0.010, val_acc:0.988]
Epoch [52/120    avg_loss:0.007, val_acc:0.987]
Epoch [53/120    avg_loss:0.007, val_acc:0.991]
Epoch [54/120    avg_loss:0.007, val_acc:0.994]
Epoch [55/120    avg_loss:0.011, val_acc:0.982]
Epoch [56/120    avg_loss:0.007, val_acc:0.992]
Epoch [57/120    avg_loss:0.006, val_acc:0.992]
Epoch [58/120    avg_loss:0.005, val_acc:0.992]
Epoch [59/120    avg_loss:0.007, val_acc:0.990]
Epoch [60/120    avg_loss:0.008, val_acc:0.992]
Epoch [61/120    avg_loss:0.029, val_acc:0.982]
Epoch [62/120    avg_loss:0.014, val_acc:0.991]
Epoch [63/120    avg_loss:0.016, val_acc:0.982]
Epoch [64/120    avg_loss:0.011, val_acc:0.994]
Epoch [65/120    avg_loss:0.007, val_acc:0.989]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.049, val_acc:0.981]
Epoch [68/120    avg_loss:0.020, val_acc:0.981]
Epoch [69/120    avg_loss:0.024, val_acc:0.989]
Epoch [70/120    avg_loss:0.011, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.985]
Epoch [72/120    avg_loss:0.021, val_acc:0.981]
Epoch [73/120    avg_loss:0.018, val_acc:0.990]
Epoch [74/120    avg_loss:0.014, val_acc:0.992]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.006, val_acc:0.992]
Epoch [77/120    avg_loss:0.009, val_acc:0.989]
Epoch [78/120    avg_loss:0.009, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.992]
Epoch [80/120    avg_loss:0.005, val_acc:0.992]
Epoch [81/120    avg_loss:0.006, val_acc:0.992]
Epoch [82/120    avg_loss:0.004, val_acc:0.992]
Epoch [83/120    avg_loss:0.005, val_acc:0.992]
Epoch [84/120    avg_loss:0.003, val_acc:0.992]
Epoch [85/120    avg_loss:0.005, val_acc:0.992]
Epoch [86/120    avg_loss:0.004, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.005, val_acc:0.992]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.004, val_acc:0.990]
Epoch [96/120    avg_loss:0.003, val_acc:0.990]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.003, val_acc:0.991]
Epoch [99/120    avg_loss:0.005, val_acc:0.991]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.991]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     1     0     0     6     0     8     0]
 [    0     0 18049     0    30     0     1     0    10     0]
 [    0     0     0  2007     0     0     0     0    24     5]
 [    0    23    10     3  2916     0     1     0    16     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     0     0     0  4855     0     6     0]
 [    0     4     0     0     0     9     3  1272     0     2]
 [    0     1     0    24    51     0     0     0  3492     3]
 [    0     0     0     0     1    24     0     0     0   894]]

Accuracy:
99.31072711059697

F1 scores:
[       nan 0.99666071 0.99811978 0.98599853 0.97688442 0.98751419
 0.99651067 0.99297424 0.97993546 0.97918949]

Kappa:
0.9908681072934334
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ef52917f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.712, val_acc:0.280]
Epoch [2/120    avg_loss:1.165, val_acc:0.481]
Epoch [3/120    avg_loss:0.889, val_acc:0.683]
Epoch [4/120    avg_loss:0.651, val_acc:0.652]
Epoch [5/120    avg_loss:0.496, val_acc:0.741]
Epoch [6/120    avg_loss:0.407, val_acc:0.776]
Epoch [7/120    avg_loss:0.393, val_acc:0.848]
Epoch [8/120    avg_loss:0.316, val_acc:0.876]
Epoch [9/120    avg_loss:0.296, val_acc:0.894]
Epoch [10/120    avg_loss:0.224, val_acc:0.912]
Epoch [11/120    avg_loss:0.180, val_acc:0.942]
Epoch [12/120    avg_loss:0.154, val_acc:0.941]
Epoch [13/120    avg_loss:0.145, val_acc:0.921]
Epoch [14/120    avg_loss:0.144, val_acc:0.938]
Epoch [15/120    avg_loss:0.118, val_acc:0.921]
Epoch [16/120    avg_loss:0.197, val_acc:0.949]
Epoch [17/120    avg_loss:0.112, val_acc:0.965]
Epoch [18/120    avg_loss:0.104, val_acc:0.960]
Epoch [19/120    avg_loss:0.120, val_acc:0.968]
Epoch [20/120    avg_loss:0.068, val_acc:0.958]
Epoch [21/120    avg_loss:0.066, val_acc:0.964]
Epoch [22/120    avg_loss:0.053, val_acc:0.976]
Epoch [23/120    avg_loss:0.049, val_acc:0.961]
Epoch [24/120    avg_loss:0.067, val_acc:0.972]
Epoch [25/120    avg_loss:0.069, val_acc:0.947]
Epoch [26/120    avg_loss:0.073, val_acc:0.962]
Epoch [27/120    avg_loss:0.053, val_acc:0.970]
Epoch [28/120    avg_loss:0.043, val_acc:0.975]
Epoch [29/120    avg_loss:0.033, val_acc:0.976]
Epoch [30/120    avg_loss:0.024, val_acc:0.972]
Epoch [31/120    avg_loss:0.039, val_acc:0.974]
Epoch [32/120    avg_loss:0.070, val_acc:0.918]
Epoch [33/120    avg_loss:0.062, val_acc:0.976]
Epoch [34/120    avg_loss:0.041, val_acc:0.939]
Epoch [35/120    avg_loss:0.037, val_acc:0.977]
Epoch [36/120    avg_loss:0.021, val_acc:0.983]
Epoch [37/120    avg_loss:0.024, val_acc:0.975]
Epoch [38/120    avg_loss:0.033, val_acc:0.948]
Epoch [39/120    avg_loss:0.028, val_acc:0.982]
Epoch [40/120    avg_loss:0.021, val_acc:0.982]
Epoch [41/120    avg_loss:0.026, val_acc:0.975]
Epoch [42/120    avg_loss:0.055, val_acc:0.965]
Epoch [43/120    avg_loss:0.057, val_acc:0.980]
Epoch [44/120    avg_loss:0.019, val_acc:0.981]
Epoch [45/120    avg_loss:0.016, val_acc:0.981]
Epoch [46/120    avg_loss:0.031, val_acc:0.965]
Epoch [47/120    avg_loss:0.017, val_acc:0.974]
Epoch [48/120    avg_loss:0.017, val_acc:0.980]
Epoch [49/120    avg_loss:0.013, val_acc:0.980]
Epoch [50/120    avg_loss:0.013, val_acc:0.980]
Epoch [51/120    avg_loss:0.010, val_acc:0.981]
Epoch [52/120    avg_loss:0.011, val_acc:0.982]
Epoch [53/120    avg_loss:0.007, val_acc:0.981]
Epoch [54/120    avg_loss:0.009, val_acc:0.982]
Epoch [55/120    avg_loss:0.012, val_acc:0.982]
Epoch [56/120    avg_loss:0.006, val_acc:0.982]
Epoch [57/120    avg_loss:0.009, val_acc:0.983]
Epoch [58/120    avg_loss:0.009, val_acc:0.982]
Epoch [59/120    avg_loss:0.009, val_acc:0.983]
Epoch [60/120    avg_loss:0.008, val_acc:0.983]
Epoch [61/120    avg_loss:0.007, val_acc:0.982]
Epoch [62/120    avg_loss:0.015, val_acc:0.981]
Epoch [63/120    avg_loss:0.011, val_acc:0.981]
Epoch [64/120    avg_loss:0.007, val_acc:0.982]
Epoch [65/120    avg_loss:0.007, val_acc:0.981]
Epoch [66/120    avg_loss:0.007, val_acc:0.981]
Epoch [67/120    avg_loss:0.007, val_acc:0.981]
Epoch [68/120    avg_loss:0.007, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.980]
Epoch [70/120    avg_loss:0.012, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.010, val_acc:0.982]
Epoch [73/120    avg_loss:0.008, val_acc:0.983]
Epoch [74/120    avg_loss:0.007, val_acc:0.983]
Epoch [75/120    avg_loss:0.006, val_acc:0.982]
Epoch [76/120    avg_loss:0.008, val_acc:0.981]
Epoch [77/120    avg_loss:0.008, val_acc:0.981]
Epoch [78/120    avg_loss:0.006, val_acc:0.981]
Epoch [79/120    avg_loss:0.006, val_acc:0.981]
Epoch [80/120    avg_loss:0.007, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.983]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.010, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.007, val_acc:0.981]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.005, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.983]
Epoch [103/120    avg_loss:0.008, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.006, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     1     0     0     5    30     3]
 [    0     5 18039     0    29     0     4     0    13     0]
 [    0     0     0  2010     4     0     0     0    16     6]
 [    0    36    14     0  2888     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     3     0     0  4850     0     3     0]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0     1     0    33    53     0     0     0  3465    19]
 [    0     0     0     0    14    22     0     0     1   882]]

Accuracy:
99.09382305449112

F1 scores:
[       nan 0.99370483 0.99759436 0.98481137 0.96896494 0.99164134
 0.99579099 0.99612403 0.97249509 0.9628821 ]

Kappa:
0.9879952716043783
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa011e65828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.799, val_acc:0.288]
Epoch [2/120    avg_loss:1.204, val_acc:0.423]
Epoch [3/120    avg_loss:0.931, val_acc:0.548]
Epoch [4/120    avg_loss:0.710, val_acc:0.673]
Epoch [5/120    avg_loss:0.548, val_acc:0.734]
Epoch [6/120    avg_loss:0.448, val_acc:0.778]
Epoch [7/120    avg_loss:0.376, val_acc:0.871]
Epoch [8/120    avg_loss:0.322, val_acc:0.877]
Epoch [9/120    avg_loss:0.245, val_acc:0.849]
Epoch [10/120    avg_loss:0.246, val_acc:0.920]
Epoch [11/120    avg_loss:0.201, val_acc:0.926]
Epoch [12/120    avg_loss:0.188, val_acc:0.952]
Epoch [13/120    avg_loss:0.186, val_acc:0.930]
Epoch [14/120    avg_loss:0.181, val_acc:0.923]
Epoch [15/120    avg_loss:0.129, val_acc:0.954]
Epoch [16/120    avg_loss:0.089, val_acc:0.890]
Epoch [17/120    avg_loss:0.139, val_acc:0.847]
Epoch [18/120    avg_loss:0.146, val_acc:0.876]
Epoch [19/120    avg_loss:0.126, val_acc:0.967]
Epoch [20/120    avg_loss:0.093, val_acc:0.943]
Epoch [21/120    avg_loss:0.092, val_acc:0.965]
Epoch [22/120    avg_loss:0.058, val_acc:0.976]
Epoch [23/120    avg_loss:0.062, val_acc:0.957]
Epoch [24/120    avg_loss:0.051, val_acc:0.961]
Epoch [25/120    avg_loss:0.038, val_acc:0.959]
Epoch [26/120    avg_loss:0.051, val_acc:0.964]
Epoch [27/120    avg_loss:0.059, val_acc:0.973]
Epoch [28/120    avg_loss:0.045, val_acc:0.971]
Epoch [29/120    avg_loss:0.129, val_acc:0.948]
Epoch [30/120    avg_loss:0.072, val_acc:0.960]
Epoch [31/120    avg_loss:0.045, val_acc:0.964]
Epoch [32/120    avg_loss:0.053, val_acc:0.975]
Epoch [33/120    avg_loss:0.034, val_acc:0.965]
Epoch [34/120    avg_loss:0.030, val_acc:0.981]
Epoch [35/120    avg_loss:0.059, val_acc:0.970]
Epoch [36/120    avg_loss:0.032, val_acc:0.974]
Epoch [37/120    avg_loss:0.028, val_acc:0.958]
Epoch [38/120    avg_loss:0.018, val_acc:0.984]
Epoch [39/120    avg_loss:0.017, val_acc:0.979]
Epoch [40/120    avg_loss:0.021, val_acc:0.964]
Epoch [41/120    avg_loss:0.029, val_acc:0.976]
Epoch [42/120    avg_loss:0.018, val_acc:0.981]
Epoch [43/120    avg_loss:0.012, val_acc:0.985]
Epoch [44/120    avg_loss:0.017, val_acc:0.967]
Epoch [45/120    avg_loss:0.014, val_acc:0.980]
Epoch [46/120    avg_loss:0.011, val_acc:0.982]
Epoch [47/120    avg_loss:0.014, val_acc:0.983]
Epoch [48/120    avg_loss:0.019, val_acc:0.980]
Epoch [49/120    avg_loss:0.016, val_acc:0.983]
Epoch [50/120    avg_loss:0.013, val_acc:0.986]
Epoch [51/120    avg_loss:0.010, val_acc:0.984]
Epoch [52/120    avg_loss:0.011, val_acc:0.981]
Epoch [53/120    avg_loss:0.020, val_acc:0.981]
Epoch [54/120    avg_loss:0.010, val_acc:0.981]
Epoch [55/120    avg_loss:0.008, val_acc:0.982]
Epoch [56/120    avg_loss:0.006, val_acc:0.984]
Epoch [57/120    avg_loss:0.026, val_acc:0.932]
Epoch [58/120    avg_loss:0.026, val_acc:0.980]
Epoch [59/120    avg_loss:0.028, val_acc:0.979]
Epoch [60/120    avg_loss:0.019, val_acc:0.981]
Epoch [61/120    avg_loss:0.020, val_acc:0.974]
Epoch [62/120    avg_loss:0.016, val_acc:0.981]
Epoch [63/120    avg_loss:0.007, val_acc:0.982]
Epoch [64/120    avg_loss:0.008, val_acc:0.985]
Epoch [65/120    avg_loss:0.007, val_acc:0.985]
Epoch [66/120    avg_loss:0.007, val_acc:0.984]
Epoch [67/120    avg_loss:0.007, val_acc:0.984]
Epoch [68/120    avg_loss:0.005, val_acc:0.984]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.005, val_acc:0.986]
Epoch [71/120    avg_loss:0.005, val_acc:0.986]
Epoch [72/120    avg_loss:0.005, val_acc:0.986]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.005, val_acc:0.987]
Epoch [75/120    avg_loss:0.005, val_acc:0.986]
Epoch [76/120    avg_loss:0.005, val_acc:0.987]
Epoch [77/120    avg_loss:0.004, val_acc:0.985]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.986]
Epoch [80/120    avg_loss:0.004, val_acc:0.987]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.004, val_acc:0.987]
Epoch [83/120    avg_loss:0.004, val_acc:0.987]
Epoch [84/120    avg_loss:0.004, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.003, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.003, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.003, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     3     0     0     0    22    45     0]
 [    0     0 18053     0    24     0    10     0     3     0]
 [    0     0     0  2007     3     0     0     0    18     8]
 [    0    39     8     0  2896     0     7     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4859     0    13     0]
 [    0     1     0     0     0     0     0  1286     0     3]
 [    0     3     0    14    58     0     0     0  3488     8]
 [    0     0     0     0    14    32     0     0     0   873]]

Accuracy:
99.12274359530524

F1 scores:
[       nan 0.99119732 0.99875522 0.98721102 0.97067203 0.98788796
 0.99630921 0.9899923  0.97430168 0.96410823]

Kappa:
0.9883807183367477
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67c2dae7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.796, val_acc:0.579]
Epoch [2/120    avg_loss:1.253, val_acc:0.422]
Epoch [3/120    avg_loss:0.956, val_acc:0.719]
Epoch [4/120    avg_loss:0.797, val_acc:0.592]
Epoch [5/120    avg_loss:0.670, val_acc:0.669]
Epoch [6/120    avg_loss:0.487, val_acc:0.716]
Epoch [7/120    avg_loss:0.393, val_acc:0.778]
Epoch [8/120    avg_loss:0.347, val_acc:0.809]
Epoch [9/120    avg_loss:0.326, val_acc:0.822]
Epoch [10/120    avg_loss:0.248, val_acc:0.872]
Epoch [11/120    avg_loss:0.193, val_acc:0.883]
Epoch [12/120    avg_loss:0.192, val_acc:0.913]
Epoch [13/120    avg_loss:0.164, val_acc:0.948]
Epoch [14/120    avg_loss:0.142, val_acc:0.924]
Epoch [15/120    avg_loss:0.114, val_acc:0.928]
Epoch [16/120    avg_loss:0.138, val_acc:0.916]
Epoch [17/120    avg_loss:0.102, val_acc:0.949]
Epoch [18/120    avg_loss:0.096, val_acc:0.930]
Epoch [19/120    avg_loss:0.102, val_acc:0.958]
Epoch [20/120    avg_loss:0.073, val_acc:0.954]
Epoch [21/120    avg_loss:0.102, val_acc:0.953]
Epoch [22/120    avg_loss:0.064, val_acc:0.952]
Epoch [23/120    avg_loss:0.050, val_acc:0.954]
Epoch [24/120    avg_loss:0.035, val_acc:0.971]
Epoch [25/120    avg_loss:0.038, val_acc:0.972]
Epoch [26/120    avg_loss:0.027, val_acc:0.976]
Epoch [27/120    avg_loss:0.028, val_acc:0.967]
Epoch [28/120    avg_loss:0.042, val_acc:0.955]
Epoch [29/120    avg_loss:0.033, val_acc:0.938]
Epoch [30/120    avg_loss:0.042, val_acc:0.940]
Epoch [31/120    avg_loss:0.036, val_acc:0.967]
Epoch [32/120    avg_loss:0.043, val_acc:0.963]
Epoch [33/120    avg_loss:0.026, val_acc:0.970]
Epoch [34/120    avg_loss:0.027, val_acc:0.981]
Epoch [35/120    avg_loss:0.016, val_acc:0.977]
Epoch [36/120    avg_loss:0.018, val_acc:0.976]
Epoch [37/120    avg_loss:0.012, val_acc:0.975]
Epoch [38/120    avg_loss:0.026, val_acc:0.933]
Epoch [39/120    avg_loss:0.025, val_acc:0.976]
Epoch [40/120    avg_loss:0.018, val_acc:0.973]
Epoch [41/120    avg_loss:0.018, val_acc:0.972]
Epoch [42/120    avg_loss:0.025, val_acc:0.965]
Epoch [43/120    avg_loss:0.015, val_acc:0.968]
Epoch [44/120    avg_loss:0.011, val_acc:0.976]
Epoch [45/120    avg_loss:0.017, val_acc:0.982]
Epoch [46/120    avg_loss:0.019, val_acc:0.976]
Epoch [47/120    avg_loss:0.015, val_acc:0.935]
Epoch [48/120    avg_loss:0.023, val_acc:0.976]
Epoch [49/120    avg_loss:0.030, val_acc:0.969]
Epoch [50/120    avg_loss:0.018, val_acc:0.978]
Epoch [51/120    avg_loss:0.010, val_acc:0.984]
Epoch [52/120    avg_loss:0.016, val_acc:0.964]
Epoch [53/120    avg_loss:0.032, val_acc:0.964]
Epoch [54/120    avg_loss:0.028, val_acc:0.970]
Epoch [55/120    avg_loss:0.017, val_acc:0.976]
Epoch [56/120    avg_loss:0.011, val_acc:0.983]
Epoch [57/120    avg_loss:0.007, val_acc:0.987]
Epoch [58/120    avg_loss:0.008, val_acc:0.981]
Epoch [59/120    avg_loss:0.017, val_acc:0.957]
Epoch [60/120    avg_loss:0.016, val_acc:0.979]
Epoch [61/120    avg_loss:0.008, val_acc:0.966]
Epoch [62/120    avg_loss:0.018, val_acc:0.981]
Epoch [63/120    avg_loss:0.008, val_acc:0.982]
Epoch [64/120    avg_loss:0.008, val_acc:0.980]
Epoch [65/120    avg_loss:0.008, val_acc:0.987]
Epoch [66/120    avg_loss:0.004, val_acc:0.986]
Epoch [67/120    avg_loss:0.006, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.986]
Epoch [69/120    avg_loss:0.006, val_acc:0.981]
Epoch [70/120    avg_loss:0.005, val_acc:0.987]
Epoch [71/120    avg_loss:0.010, val_acc:0.984]
Epoch [72/120    avg_loss:0.012, val_acc:0.981]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.004, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.986]
Epoch [76/120    avg_loss:0.007, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.976]
Epoch [78/120    avg_loss:0.005, val_acc:0.983]
Epoch [79/120    avg_loss:0.004, val_acc:0.984]
Epoch [80/120    avg_loss:0.009, val_acc:0.976]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.004, val_acc:0.988]
Epoch [83/120    avg_loss:0.007, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.979]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.982]
Epoch [90/120    avg_loss:0.032, val_acc:0.974]
Epoch [91/120    avg_loss:0.036, val_acc:0.977]
Epoch [92/120    avg_loss:0.031, val_acc:0.980]
Epoch [93/120    avg_loss:0.022, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.978]
Epoch [98/120    avg_loss:0.019, val_acc:0.976]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.019, val_acc:0.962]
Epoch [101/120    avg_loss:0.008, val_acc:0.975]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.003, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     0     0     0    13     8    43    10]
 [    0     0 18038     0    36     0    12     0     4     0]
 [    0     4     0  2003     1     0     0     0    21     7]
 [    0    35     0     1  2914     0     0     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4872     0     6     0]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     2     0    36    58     0     0     0  3454    21]
 [    0     0     0     0    16    43     0     0     0   860]]

Accuracy:
99.02875183765937

F1 scores:
[       nan 0.99103733 0.99856067 0.9828263  0.97181924 0.98379193
 0.99662473 0.99535604 0.97008847 0.94557449]

Kappa:
0.9871394418571671
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f238c3860>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.844, val_acc:0.327]
Epoch [2/120    avg_loss:1.208, val_acc:0.485]
Epoch [3/120    avg_loss:0.891, val_acc:0.520]
Epoch [4/120    avg_loss:0.741, val_acc:0.579]
Epoch [5/120    avg_loss:0.608, val_acc:0.666]
Epoch [6/120    avg_loss:0.481, val_acc:0.741]
Epoch [7/120    avg_loss:0.380, val_acc:0.792]
Epoch [8/120    avg_loss:0.417, val_acc:0.811]
Epoch [9/120    avg_loss:0.325, val_acc:0.905]
Epoch [10/120    avg_loss:0.237, val_acc:0.895]
Epoch [11/120    avg_loss:0.227, val_acc:0.911]
Epoch [12/120    avg_loss:0.206, val_acc:0.946]
Epoch [13/120    avg_loss:0.173, val_acc:0.902]
Epoch [14/120    avg_loss:0.162, val_acc:0.951]
Epoch [15/120    avg_loss:0.136, val_acc:0.883]
Epoch [16/120    avg_loss:0.126, val_acc:0.964]
Epoch [17/120    avg_loss:0.128, val_acc:0.966]
Epoch [18/120    avg_loss:0.100, val_acc:0.947]
Epoch [19/120    avg_loss:0.079, val_acc:0.966]
Epoch [20/120    avg_loss:0.094, val_acc:0.947]
Epoch [21/120    avg_loss:0.112, val_acc:0.960]
Epoch [22/120    avg_loss:0.080, val_acc:0.962]
Epoch [23/120    avg_loss:0.051, val_acc:0.978]
Epoch [24/120    avg_loss:0.060, val_acc:0.955]
Epoch [25/120    avg_loss:0.075, val_acc:0.961]
Epoch [26/120    avg_loss:0.057, val_acc:0.974]
Epoch [27/120    avg_loss:0.046, val_acc:0.976]
Epoch [28/120    avg_loss:0.066, val_acc:0.976]
Epoch [29/120    avg_loss:0.101, val_acc:0.970]
Epoch [30/120    avg_loss:0.091, val_acc:0.965]
Epoch [31/120    avg_loss:0.040, val_acc:0.970]
Epoch [32/120    avg_loss:0.039, val_acc:0.969]
Epoch [33/120    avg_loss:0.029, val_acc:0.965]
Epoch [34/120    avg_loss:0.036, val_acc:0.984]
Epoch [35/120    avg_loss:0.028, val_acc:0.964]
Epoch [36/120    avg_loss:0.024, val_acc:0.983]
Epoch [37/120    avg_loss:0.021, val_acc:0.978]
Epoch [38/120    avg_loss:0.019, val_acc:0.984]
Epoch [39/120    avg_loss:0.023, val_acc:0.987]
Epoch [40/120    avg_loss:0.065, val_acc:0.987]
Epoch [41/120    avg_loss:0.028, val_acc:0.981]
Epoch [42/120    avg_loss:0.031, val_acc:0.976]
Epoch [43/120    avg_loss:0.044, val_acc:0.979]
Epoch [44/120    avg_loss:0.022, val_acc:0.986]
Epoch [45/120    avg_loss:0.036, val_acc:0.985]
Epoch [46/120    avg_loss:0.018, val_acc:0.989]
Epoch [47/120    avg_loss:0.015, val_acc:0.992]
Epoch [48/120    avg_loss:0.018, val_acc:0.986]
Epoch [49/120    avg_loss:0.016, val_acc:0.986]
Epoch [50/120    avg_loss:0.019, val_acc:0.984]
Epoch [51/120    avg_loss:0.013, val_acc:0.985]
Epoch [52/120    avg_loss:0.010, val_acc:0.987]
Epoch [53/120    avg_loss:0.016, val_acc:0.986]
Epoch [54/120    avg_loss:0.011, val_acc:0.989]
Epoch [55/120    avg_loss:0.010, val_acc:0.988]
Epoch [56/120    avg_loss:0.012, val_acc:0.988]
Epoch [57/120    avg_loss:0.015, val_acc:0.983]
Epoch [58/120    avg_loss:0.015, val_acc:0.987]
Epoch [59/120    avg_loss:0.008, val_acc:0.990]
Epoch [60/120    avg_loss:0.011, val_acc:0.992]
Epoch [61/120    avg_loss:0.007, val_acc:0.992]
Epoch [62/120    avg_loss:0.009, val_acc:0.991]
Epoch [63/120    avg_loss:0.009, val_acc:0.989]
Epoch [64/120    avg_loss:0.006, val_acc:0.990]
Epoch [65/120    avg_loss:0.010, val_acc:0.988]
Epoch [66/120    avg_loss:0.011, val_acc:0.979]
Epoch [67/120    avg_loss:0.069, val_acc:0.983]
Epoch [68/120    avg_loss:0.024, val_acc:0.983]
Epoch [69/120    avg_loss:0.013, val_acc:0.981]
Epoch [70/120    avg_loss:0.009, val_acc:0.989]
Epoch [71/120    avg_loss:0.013, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.006, val_acc:0.990]
Epoch [74/120    avg_loss:0.008, val_acc:0.990]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.991]
Epoch [77/120    avg_loss:0.005, val_acc:0.992]
Epoch [78/120    avg_loss:0.006, val_acc:0.992]
Epoch [79/120    avg_loss:0.005, val_acc:0.991]
Epoch [80/120    avg_loss:0.007, val_acc:0.991]
Epoch [81/120    avg_loss:0.005, val_acc:0.991]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.005, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.991]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.004, val_acc:0.991]
Epoch [90/120    avg_loss:0.004, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.004, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.006, val_acc:0.991]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.004, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6363     0     0     0     0     1    25    43     0]
 [    0     0 18071     0    12     0     0     0     7     0]
 [    0     7     0  1996     0     0     0     0    28     5]
 [    0    28    16     0  2913     0     0     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4870     0     1     0]
 [    0     2     0     0     0     0     0  1288     0     0]
 [    0     2     0    26    53     0     0     0  3478    12]
 [    0     0     0     0    14    18     0     0     0   887]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.99158485 0.99883927 0.98373583 0.97686117 0.99315068
 0.99907683 0.98962735 0.97382052 0.97312123]

Kappa:
0.9897177138886115
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f208757f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.661, val_acc:0.326]
Epoch [2/120    avg_loss:1.113, val_acc:0.456]
Epoch [3/120    avg_loss:0.881, val_acc:0.601]
Epoch [4/120    avg_loss:0.741, val_acc:0.711]
Epoch [5/120    avg_loss:0.510, val_acc:0.756]
Epoch [6/120    avg_loss:0.412, val_acc:0.857]
Epoch [7/120    avg_loss:0.358, val_acc:0.818]
Epoch [8/120    avg_loss:0.284, val_acc:0.839]
Epoch [9/120    avg_loss:0.274, val_acc:0.919]
Epoch [10/120    avg_loss:0.245, val_acc:0.917]
Epoch [11/120    avg_loss:0.213, val_acc:0.948]
Epoch [12/120    avg_loss:0.227, val_acc:0.926]
Epoch [13/120    avg_loss:0.173, val_acc:0.866]
Epoch [14/120    avg_loss:0.182, val_acc:0.902]
Epoch [15/120    avg_loss:0.176, val_acc:0.958]
Epoch [16/120    avg_loss:0.182, val_acc:0.867]
Epoch [17/120    avg_loss:0.175, val_acc:0.961]
Epoch [18/120    avg_loss:0.119, val_acc:0.944]
Epoch [19/120    avg_loss:0.138, val_acc:0.921]
Epoch [20/120    avg_loss:0.107, val_acc:0.939]
Epoch [21/120    avg_loss:0.069, val_acc:0.975]
Epoch [22/120    avg_loss:0.078, val_acc:0.970]
Epoch [23/120    avg_loss:0.084, val_acc:0.953]
Epoch [24/120    avg_loss:0.060, val_acc:0.977]
Epoch [25/120    avg_loss:0.055, val_acc:0.967]
Epoch [26/120    avg_loss:0.051, val_acc:0.976]
Epoch [27/120    avg_loss:0.042, val_acc:0.981]
Epoch [28/120    avg_loss:0.053, val_acc:0.981]
Epoch [29/120    avg_loss:0.040, val_acc:0.983]
Epoch [30/120    avg_loss:0.039, val_acc:0.962]
Epoch [31/120    avg_loss:0.058, val_acc:0.943]
Epoch [32/120    avg_loss:0.034, val_acc:0.986]
Epoch [33/120    avg_loss:0.025, val_acc:0.984]
Epoch [34/120    avg_loss:0.019, val_acc:0.986]
Epoch [35/120    avg_loss:0.018, val_acc:0.992]
Epoch [36/120    avg_loss:0.023, val_acc:0.988]
Epoch [37/120    avg_loss:0.017, val_acc:0.992]
Epoch [38/120    avg_loss:0.033, val_acc:0.976]
Epoch [39/120    avg_loss:0.029, val_acc:0.976]
Epoch [40/120    avg_loss:0.020, val_acc:0.981]
Epoch [41/120    avg_loss:0.042, val_acc:0.932]
Epoch [42/120    avg_loss:0.050, val_acc:0.975]
Epoch [43/120    avg_loss:0.032, val_acc:0.988]
Epoch [44/120    avg_loss:0.018, val_acc:0.994]
Epoch [45/120    avg_loss:0.015, val_acc:0.991]
Epoch [46/120    avg_loss:0.021, val_acc:0.993]
Epoch [47/120    avg_loss:0.030, val_acc:0.982]
Epoch [48/120    avg_loss:0.034, val_acc:0.970]
Epoch [49/120    avg_loss:0.024, val_acc:0.990]
Epoch [50/120    avg_loss:0.034, val_acc:0.982]
Epoch [51/120    avg_loss:0.034, val_acc:0.978]
Epoch [52/120    avg_loss:0.051, val_acc:0.972]
Epoch [53/120    avg_loss:0.032, val_acc:0.982]
Epoch [54/120    avg_loss:0.019, val_acc:0.986]
Epoch [55/120    avg_loss:0.009, val_acc:0.987]
Epoch [56/120    avg_loss:0.016, val_acc:0.991]
Epoch [57/120    avg_loss:0.018, val_acc:0.992]
Epoch [58/120    avg_loss:0.010, val_acc:0.993]
Epoch [59/120    avg_loss:0.009, val_acc:0.991]
Epoch [60/120    avg_loss:0.008, val_acc:0.991]
Epoch [61/120    avg_loss:0.010, val_acc:0.991]
Epoch [62/120    avg_loss:0.008, val_acc:0.992]
Epoch [63/120    avg_loss:0.007, val_acc:0.992]
Epoch [64/120    avg_loss:0.006, val_acc:0.991]
Epoch [65/120    avg_loss:0.008, val_acc:0.992]
Epoch [66/120    avg_loss:0.009, val_acc:0.992]
Epoch [67/120    avg_loss:0.007, val_acc:0.992]
Epoch [68/120    avg_loss:0.006, val_acc:0.992]
Epoch [69/120    avg_loss:0.006, val_acc:0.992]
Epoch [70/120    avg_loss:0.007, val_acc:0.992]
Epoch [71/120    avg_loss:0.007, val_acc:0.992]
Epoch [72/120    avg_loss:0.006, val_acc:0.992]
Epoch [73/120    avg_loss:0.006, val_acc:0.992]
Epoch [74/120    avg_loss:0.006, val_acc:0.992]
Epoch [75/120    avg_loss:0.007, val_acc:0.992]
Epoch [76/120    avg_loss:0.006, val_acc:0.992]
Epoch [77/120    avg_loss:0.005, val_acc:0.992]
Epoch [78/120    avg_loss:0.007, val_acc:0.992]
Epoch [79/120    avg_loss:0.005, val_acc:0.992]
Epoch [80/120    avg_loss:0.006, val_acc:0.992]
Epoch [81/120    avg_loss:0.006, val_acc:0.992]
Epoch [82/120    avg_loss:0.006, val_acc:0.992]
Epoch [83/120    avg_loss:0.008, val_acc:0.992]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.010, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.992]
Epoch [89/120    avg_loss:0.006, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.008, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.007, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.005, val_acc:0.992]
Epoch [99/120    avg_loss:0.006, val_acc:0.992]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.006, val_acc:0.992]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.006, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.006, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6341     0     0     0     0     9     0    82     0]
 [    0     2 18062     0    17     0     2     0     7     0]
 [    0     0     0  2004     2     0     0     0    28     2]
 [    0    34    16     4  2900     0     0     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     4     0     0  4838     0    16     0]
 [    0     0     0     0     0     0     2  1284     0     4]
 [    0     5     0    45    52     0     0     0  3463     6]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
99.00224134191309

F1 scores:
[       nan 0.98969877 0.99823146 0.97923284 0.97364445 0.99126472
 0.99455237 0.997669   0.96395268 0.97297297]

Kappa:
0.9867798514912376
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f578ac967f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.742, val_acc:0.500]
Epoch [2/120    avg_loss:1.138, val_acc:0.621]
Epoch [3/120    avg_loss:0.904, val_acc:0.704]
Epoch [4/120    avg_loss:0.783, val_acc:0.736]
Epoch [5/120    avg_loss:0.650, val_acc:0.666]
Epoch [6/120    avg_loss:0.553, val_acc:0.622]
Epoch [7/120    avg_loss:0.443, val_acc:0.785]
Epoch [8/120    avg_loss:0.338, val_acc:0.788]
Epoch [9/120    avg_loss:0.305, val_acc:0.809]
Epoch [10/120    avg_loss:0.261, val_acc:0.860]
Epoch [11/120    avg_loss:0.221, val_acc:0.899]
Epoch [12/120    avg_loss:0.198, val_acc:0.936]
Epoch [13/120    avg_loss:0.198, val_acc:0.893]
Epoch [14/120    avg_loss:0.191, val_acc:0.957]
Epoch [15/120    avg_loss:0.134, val_acc:0.947]
Epoch [16/120    avg_loss:0.129, val_acc:0.938]
Epoch [17/120    avg_loss:0.083, val_acc:0.932]
Epoch [18/120    avg_loss:0.090, val_acc:0.965]
Epoch [19/120    avg_loss:0.089, val_acc:0.945]
Epoch [20/120    avg_loss:0.085, val_acc:0.955]
Epoch [21/120    avg_loss:0.091, val_acc:0.883]
Epoch [22/120    avg_loss:0.088, val_acc:0.961]
Epoch [23/120    avg_loss:0.053, val_acc:0.965]
Epoch [24/120    avg_loss:0.054, val_acc:0.962]
Epoch [25/120    avg_loss:0.050, val_acc:0.970]
Epoch [26/120    avg_loss:0.038, val_acc:0.932]
Epoch [27/120    avg_loss:0.049, val_acc:0.965]
Epoch [28/120    avg_loss:0.046, val_acc:0.964]
Epoch [29/120    avg_loss:0.080, val_acc:0.966]
Epoch [30/120    avg_loss:0.048, val_acc:0.975]
Epoch [31/120    avg_loss:0.037, val_acc:0.978]
Epoch [32/120    avg_loss:0.036, val_acc:0.977]
Epoch [33/120    avg_loss:0.031, val_acc:0.967]
Epoch [34/120    avg_loss:0.036, val_acc:0.978]
Epoch [35/120    avg_loss:0.030, val_acc:0.972]
Epoch [36/120    avg_loss:0.036, val_acc:0.978]
Epoch [37/120    avg_loss:0.019, val_acc:0.975]
Epoch [38/120    avg_loss:0.013, val_acc:0.985]
Epoch [39/120    avg_loss:0.020, val_acc:0.975]
Epoch [40/120    avg_loss:0.018, val_acc:0.976]
Epoch [41/120    avg_loss:0.015, val_acc:0.980]
Epoch [42/120    avg_loss:0.014, val_acc:0.981]
Epoch [43/120    avg_loss:0.015, val_acc:0.980]
Epoch [44/120    avg_loss:0.020, val_acc:0.976]
Epoch [45/120    avg_loss:0.023, val_acc:0.968]
Epoch [46/120    avg_loss:0.034, val_acc:0.974]
Epoch [47/120    avg_loss:0.019, val_acc:0.981]
Epoch [48/120    avg_loss:0.021, val_acc:0.975]
Epoch [49/120    avg_loss:0.023, val_acc:0.976]
Epoch [50/120    avg_loss:0.012, val_acc:0.981]
Epoch [51/120    avg_loss:0.011, val_acc:0.978]
Epoch [52/120    avg_loss:0.009, val_acc:0.979]
Epoch [53/120    avg_loss:0.014, val_acc:0.980]
Epoch [54/120    avg_loss:0.008, val_acc:0.980]
Epoch [55/120    avg_loss:0.015, val_acc:0.978]
Epoch [56/120    avg_loss:0.009, val_acc:0.979]
Epoch [57/120    avg_loss:0.007, val_acc:0.981]
Epoch [58/120    avg_loss:0.007, val_acc:0.982]
Epoch [59/120    avg_loss:0.008, val_acc:0.984]
Epoch [60/120    avg_loss:0.007, val_acc:0.982]
Epoch [61/120    avg_loss:0.006, val_acc:0.981]
Epoch [62/120    avg_loss:0.005, val_acc:0.981]
Epoch [63/120    avg_loss:0.008, val_acc:0.981]
Epoch [64/120    avg_loss:0.007, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.981]
Epoch [66/120    avg_loss:0.008, val_acc:0.981]
Epoch [67/120    avg_loss:0.007, val_acc:0.981]
Epoch [68/120    avg_loss:0.008, val_acc:0.981]
Epoch [69/120    avg_loss:0.008, val_acc:0.981]
Epoch [70/120    avg_loss:0.008, val_acc:0.981]
Epoch [71/120    avg_loss:0.006, val_acc:0.981]
Epoch [72/120    avg_loss:0.009, val_acc:0.981]
Epoch [73/120    avg_loss:0.012, val_acc:0.981]
Epoch [74/120    avg_loss:0.008, val_acc:0.981]
Epoch [75/120    avg_loss:0.006, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.981]
Epoch [78/120    avg_loss:0.007, val_acc:0.981]
Epoch [79/120    avg_loss:0.007, val_acc:0.981]
Epoch [80/120    avg_loss:0.007, val_acc:0.981]
Epoch [81/120    avg_loss:0.007, val_acc:0.981]
Epoch [82/120    avg_loss:0.007, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.981]
Epoch [85/120    avg_loss:0.006, val_acc:0.981]
Epoch [86/120    avg_loss:0.009, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.008, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.981]
Epoch [91/120    avg_loss:0.007, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.981]
Epoch [93/120    avg_loss:0.007, val_acc:0.981]
Epoch [94/120    avg_loss:0.007, val_acc:0.981]
Epoch [95/120    avg_loss:0.007, val_acc:0.981]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.007, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.981]
Epoch [102/120    avg_loss:0.006, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.981]
Epoch [104/120    avg_loss:0.007, val_acc:0.981]
Epoch [105/120    avg_loss:0.009, val_acc:0.981]
Epoch [106/120    avg_loss:0.006, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.010, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6366     0     7     0     0    12     0    45     2]
 [    0     0 17908     0    64     0   112     0     6     0]
 [    0     5     0  2022     1     0     0     0     6     2]
 [    0    36    10     0  2894     0     6     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0     0     0     0     0     0     3  1283     0     4]
 [    0     3     0    40    53     0     0     0  3467     8]
 [    0     0     0     0    14    26     0     0     0   879]]

Accuracy:
98.80461764634998

F1 scores:
[       nan 0.99143436 0.99466785 0.98514007 0.96498833 0.99013657
 0.98603804 0.99727944 0.97305641 0.969129  ]

Kappa:
0.9841903360953829
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53aa3db860>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.783, val_acc:0.292]
Epoch [2/120    avg_loss:1.262, val_acc:0.526]
Epoch [3/120    avg_loss:0.985, val_acc:0.607]
Epoch [4/120    avg_loss:0.761, val_acc:0.609]
Epoch [5/120    avg_loss:0.575, val_acc:0.688]
Epoch [6/120    avg_loss:0.464, val_acc:0.798]
Epoch [7/120    avg_loss:0.376, val_acc:0.804]
Epoch [8/120    avg_loss:0.338, val_acc:0.791]
Epoch [9/120    avg_loss:0.289, val_acc:0.882]
Epoch [10/120    avg_loss:0.248, val_acc:0.813]
Epoch [11/120    avg_loss:0.210, val_acc:0.927]
Epoch [12/120    avg_loss:0.162, val_acc:0.943]
Epoch [13/120    avg_loss:0.146, val_acc:0.903]
Epoch [14/120    avg_loss:0.125, val_acc:0.947]
Epoch [15/120    avg_loss:0.143, val_acc:0.899]
Epoch [16/120    avg_loss:0.185, val_acc:0.891]
Epoch [17/120    avg_loss:0.136, val_acc:0.955]
Epoch [18/120    avg_loss:0.090, val_acc:0.946]
Epoch [19/120    avg_loss:0.075, val_acc:0.966]
Epoch [20/120    avg_loss:0.061, val_acc:0.944]
Epoch [21/120    avg_loss:0.063, val_acc:0.956]
Epoch [22/120    avg_loss:0.075, val_acc:0.970]
Epoch [23/120    avg_loss:0.047, val_acc:0.979]
Epoch [24/120    avg_loss:0.065, val_acc:0.948]
Epoch [25/120    avg_loss:0.052, val_acc:0.974]
Epoch [26/120    avg_loss:0.089, val_acc:0.941]
Epoch [27/120    avg_loss:0.057, val_acc:0.981]
Epoch [28/120    avg_loss:0.039, val_acc:0.978]
Epoch [29/120    avg_loss:0.025, val_acc:0.972]
Epoch [30/120    avg_loss:0.028, val_acc:0.971]
Epoch [31/120    avg_loss:0.045, val_acc:0.953]
Epoch [32/120    avg_loss:0.057, val_acc:0.961]
Epoch [33/120    avg_loss:0.051, val_acc:0.936]
Epoch [34/120    avg_loss:0.034, val_acc:0.985]
Epoch [35/120    avg_loss:0.026, val_acc:0.979]
Epoch [36/120    avg_loss:0.020, val_acc:0.970]
Epoch [37/120    avg_loss:0.016, val_acc:0.983]
Epoch [38/120    avg_loss:0.016, val_acc:0.989]
Epoch [39/120    avg_loss:0.026, val_acc:0.968]
Epoch [40/120    avg_loss:0.048, val_acc:0.952]
Epoch [41/120    avg_loss:0.061, val_acc:0.978]
Epoch [42/120    avg_loss:0.030, val_acc:0.967]
Epoch [43/120    avg_loss:0.027, val_acc:0.982]
Epoch [44/120    avg_loss:0.018, val_acc:0.983]
Epoch [45/120    avg_loss:0.019, val_acc:0.982]
Epoch [46/120    avg_loss:0.020, val_acc:0.979]
Epoch [47/120    avg_loss:0.015, val_acc:0.984]
Epoch [48/120    avg_loss:0.028, val_acc:0.944]
Epoch [49/120    avg_loss:0.050, val_acc:0.976]
Epoch [50/120    avg_loss:0.032, val_acc:0.967]
Epoch [51/120    avg_loss:0.016, val_acc:0.988]
Epoch [52/120    avg_loss:0.011, val_acc:0.987]
Epoch [53/120    avg_loss:0.013, val_acc:0.990]
Epoch [54/120    avg_loss:0.011, val_acc:0.989]
Epoch [55/120    avg_loss:0.013, val_acc:0.988]
Epoch [56/120    avg_loss:0.011, val_acc:0.991]
Epoch [57/120    avg_loss:0.008, val_acc:0.991]
Epoch [58/120    avg_loss:0.009, val_acc:0.992]
Epoch [59/120    avg_loss:0.010, val_acc:0.991]
Epoch [60/120    avg_loss:0.009, val_acc:0.991]
Epoch [61/120    avg_loss:0.010, val_acc:0.990]
Epoch [62/120    avg_loss:0.010, val_acc:0.990]
Epoch [63/120    avg_loss:0.007, val_acc:0.991]
Epoch [64/120    avg_loss:0.008, val_acc:0.992]
Epoch [65/120    avg_loss:0.009, val_acc:0.992]
Epoch [66/120    avg_loss:0.010, val_acc:0.992]
Epoch [67/120    avg_loss:0.009, val_acc:0.992]
Epoch [68/120    avg_loss:0.008, val_acc:0.992]
Epoch [69/120    avg_loss:0.006, val_acc:0.992]
Epoch [70/120    avg_loss:0.010, val_acc:0.992]
Epoch [71/120    avg_loss:0.007, val_acc:0.992]
Epoch [72/120    avg_loss:0.009, val_acc:0.992]
Epoch [73/120    avg_loss:0.007, val_acc:0.992]
Epoch [74/120    avg_loss:0.006, val_acc:0.992]
Epoch [75/120    avg_loss:0.009, val_acc:0.992]
Epoch [76/120    avg_loss:0.006, val_acc:0.992]
Epoch [77/120    avg_loss:0.006, val_acc:0.992]
Epoch [78/120    avg_loss:0.009, val_acc:0.991]
Epoch [79/120    avg_loss:0.007, val_acc:0.992]
Epoch [80/120    avg_loss:0.006, val_acc:0.992]
Epoch [81/120    avg_loss:0.009, val_acc:0.992]
Epoch [82/120    avg_loss:0.006, val_acc:0.992]
Epoch [83/120    avg_loss:0.011, val_acc:0.992]
Epoch [84/120    avg_loss:0.010, val_acc:0.992]
Epoch [85/120    avg_loss:0.010, val_acc:0.992]
Epoch [86/120    avg_loss:0.009, val_acc:0.992]
Epoch [87/120    avg_loss:0.008, val_acc:0.993]
Epoch [88/120    avg_loss:0.007, val_acc:0.993]
Epoch [89/120    avg_loss:0.007, val_acc:0.993]
Epoch [90/120    avg_loss:0.007, val_acc:0.993]
Epoch [91/120    avg_loss:0.010, val_acc:0.992]
Epoch [92/120    avg_loss:0.006, val_acc:0.992]
Epoch [93/120    avg_loss:0.008, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.992]
Epoch [95/120    avg_loss:0.007, val_acc:0.992]
Epoch [96/120    avg_loss:0.007, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.993]
Epoch [99/120    avg_loss:0.007, val_acc:0.993]
Epoch [100/120    avg_loss:0.006, val_acc:0.994]
Epoch [101/120    avg_loss:0.006, val_acc:0.994]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.007, val_acc:0.993]
Epoch [104/120    avg_loss:0.006, val_acc:0.993]
Epoch [105/120    avg_loss:0.004, val_acc:0.993]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.993]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.994]
Epoch [113/120    avg_loss:0.007, val_acc:0.993]
Epoch [114/120    avg_loss:0.006, val_acc:0.993]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     1     0     0    11    27     6     0]
 [    0     0 18008     0    39     0    34     0     9     0]
 [    0     7     0  2019     0     0     0     0     3     7]
 [    0    42     5     0  2902     0     1     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4864     0    12     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    10     1    14    59     0     0     0  3486     1]
 [    0     0     0     0    14    35     0     0     0   870]]

Accuracy:
99.12515364037307

F1 scores:
[       nan 0.99192421 0.9975626  0.99165029 0.96959572 0.98676749
 0.99387004 0.98925556 0.98072865 0.96774194]

Kappa:
0.9884173782635266
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a2f1817f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.716, val_acc:0.348]
Epoch [2/120    avg_loss:1.206, val_acc:0.457]
Epoch [3/120    avg_loss:0.907, val_acc:0.683]
Epoch [4/120    avg_loss:0.676, val_acc:0.651]
Epoch [5/120    avg_loss:0.566, val_acc:0.775]
Epoch [6/120    avg_loss:0.447, val_acc:0.723]
Epoch [7/120    avg_loss:0.416, val_acc:0.781]
Epoch [8/120    avg_loss:0.310, val_acc:0.875]
Epoch [9/120    avg_loss:0.288, val_acc:0.857]
Epoch [10/120    avg_loss:0.253, val_acc:0.843]
Epoch [11/120    avg_loss:0.259, val_acc:0.903]
Epoch [12/120    avg_loss:0.215, val_acc:0.902]
Epoch [13/120    avg_loss:0.169, val_acc:0.937]
Epoch [14/120    avg_loss:0.143, val_acc:0.961]
Epoch [15/120    avg_loss:0.127, val_acc:0.925]
Epoch [16/120    avg_loss:0.122, val_acc:0.934]
Epoch [17/120    avg_loss:0.143, val_acc:0.932]
Epoch [18/120    avg_loss:0.117, val_acc:0.951]
Epoch [19/120    avg_loss:0.096, val_acc:0.953]
Epoch [20/120    avg_loss:0.106, val_acc:0.959]
Epoch [21/120    avg_loss:0.094, val_acc:0.954]
Epoch [22/120    avg_loss:0.092, val_acc:0.964]
Epoch [23/120    avg_loss:0.063, val_acc:0.957]
Epoch [24/120    avg_loss:0.068, val_acc:0.973]
Epoch [25/120    avg_loss:0.057, val_acc:0.967]
Epoch [26/120    avg_loss:0.058, val_acc:0.949]
Epoch [27/120    avg_loss:0.112, val_acc:0.940]
Epoch [28/120    avg_loss:0.077, val_acc:0.968]
Epoch [29/120    avg_loss:0.040, val_acc:0.978]
Epoch [30/120    avg_loss:0.057, val_acc:0.974]
Epoch [31/120    avg_loss:0.045, val_acc:0.965]
Epoch [32/120    avg_loss:0.044, val_acc:0.984]
Epoch [33/120    avg_loss:0.027, val_acc:0.983]
Epoch [34/120    avg_loss:0.037, val_acc:0.973]
Epoch [35/120    avg_loss:0.030, val_acc:0.981]
Epoch [36/120    avg_loss:0.022, val_acc:0.981]
Epoch [37/120    avg_loss:0.028, val_acc:0.966]
Epoch [38/120    avg_loss:0.025, val_acc:0.986]
Epoch [39/120    avg_loss:0.015, val_acc:0.985]
Epoch [40/120    avg_loss:0.027, val_acc:0.975]
Epoch [41/120    avg_loss:0.023, val_acc:0.978]
Epoch [42/120    avg_loss:0.026, val_acc:0.981]
Epoch [43/120    avg_loss:0.021, val_acc:0.977]
Epoch [44/120    avg_loss:0.027, val_acc:0.987]
Epoch [45/120    avg_loss:0.024, val_acc:0.981]
Epoch [46/120    avg_loss:0.025, val_acc:0.980]
Epoch [47/120    avg_loss:0.022, val_acc:0.981]
Epoch [48/120    avg_loss:0.020, val_acc:0.986]
Epoch [49/120    avg_loss:0.017, val_acc:0.987]
Epoch [50/120    avg_loss:0.023, val_acc:0.978]
Epoch [51/120    avg_loss:0.032, val_acc:0.980]
Epoch [52/120    avg_loss:0.016, val_acc:0.975]
Epoch [53/120    avg_loss:0.018, val_acc:0.979]
Epoch [54/120    avg_loss:0.015, val_acc:0.986]
Epoch [55/120    avg_loss:0.014, val_acc:0.982]
Epoch [56/120    avg_loss:0.011, val_acc:0.987]
Epoch [57/120    avg_loss:0.008, val_acc:0.986]
Epoch [58/120    avg_loss:0.008, val_acc:0.986]
Epoch [59/120    avg_loss:0.012, val_acc:0.983]
Epoch [60/120    avg_loss:0.015, val_acc:0.978]
Epoch [61/120    avg_loss:0.016, val_acc:0.986]
Epoch [62/120    avg_loss:0.021, val_acc:0.986]
Epoch [63/120    avg_loss:0.011, val_acc:0.991]
Epoch [64/120    avg_loss:0.006, val_acc:0.990]
Epoch [65/120    avg_loss:0.010, val_acc:0.991]
Epoch [66/120    avg_loss:0.005, val_acc:0.988]
Epoch [67/120    avg_loss:0.007, val_acc:0.990]
Epoch [68/120    avg_loss:0.015, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.989]
Epoch [70/120    avg_loss:0.006, val_acc:0.991]
Epoch [71/120    avg_loss:0.004, val_acc:0.991]
Epoch [72/120    avg_loss:0.004, val_acc:0.990]
Epoch [73/120    avg_loss:0.006, val_acc:0.992]
Epoch [74/120    avg_loss:0.004, val_acc:0.992]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.016, val_acc:0.982]
Epoch [77/120    avg_loss:0.005, val_acc:0.992]
Epoch [78/120    avg_loss:0.005, val_acc:0.992]
Epoch [79/120    avg_loss:0.005, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.991]
Epoch [81/120    avg_loss:0.003, val_acc:0.992]
Epoch [82/120    avg_loss:0.004, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.992]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.003, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.987]
Epoch [88/120    avg_loss:0.005, val_acc:0.992]
Epoch [89/120    avg_loss:0.003, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.004, val_acc:0.992]
Epoch [92/120    avg_loss:0.003, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.003, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.002, val_acc:0.989]
Epoch [98/120    avg_loss:0.003, val_acc:0.990]
Epoch [99/120    avg_loss:0.003, val_acc:0.991]
Epoch [100/120    avg_loss:0.003, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.990]
Epoch [102/120    avg_loss:0.002, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.990]
Epoch [104/120    avg_loss:0.003, val_acc:0.992]
Epoch [105/120    avg_loss:0.003, val_acc:0.992]
Epoch [106/120    avg_loss:0.003, val_acc:0.992]
Epoch [107/120    avg_loss:0.002, val_acc:0.991]
Epoch [108/120    avg_loss:0.002, val_acc:0.992]
Epoch [109/120    avg_loss:0.003, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.002, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.003, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6376     0     3     0     0     7     2    44     0]
 [    0     0 18060     0    22     0     4     0     4     0]
 [    0     0     0  2025     0     0     0     0    10     1]
 [    0    31    13     0  2911     0     0     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     9     0     0  4864     0     0     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     0     0     0    58     0     0     0  3511     2]
 [    0     0     0     1    14    22     0     0     0   882]]

Accuracy:
99.34687778661461

F1 scores:
[       nan 0.99322377 0.99867286 0.99410898 0.97406726 0.99164134
 0.99723219 0.99844961 0.98113735 0.97782705]

Kappa:
0.9913472650465109
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f90b058f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.660, val_acc:0.588]
Epoch [2/120    avg_loss:1.096, val_acc:0.456]
Epoch [3/120    avg_loss:0.908, val_acc:0.769]
Epoch [4/120    avg_loss:0.778, val_acc:0.621]
Epoch [5/120    avg_loss:0.615, val_acc:0.665]
Epoch [6/120    avg_loss:0.499, val_acc:0.747]
Epoch [7/120    avg_loss:0.439, val_acc:0.693]
Epoch [8/120    avg_loss:0.373, val_acc:0.796]
Epoch [9/120    avg_loss:0.294, val_acc:0.842]
Epoch [10/120    avg_loss:0.308, val_acc:0.784]
Epoch [11/120    avg_loss:0.271, val_acc:0.809]
Epoch [12/120    avg_loss:0.229, val_acc:0.837]
Epoch [13/120    avg_loss:0.218, val_acc:0.856]
Epoch [14/120    avg_loss:0.157, val_acc:0.902]
Epoch [15/120    avg_loss:0.138, val_acc:0.923]
Epoch [16/120    avg_loss:0.143, val_acc:0.963]
Epoch [17/120    avg_loss:0.129, val_acc:0.910]
Epoch [18/120    avg_loss:0.136, val_acc:0.913]
Epoch [19/120    avg_loss:0.224, val_acc:0.942]
Epoch [20/120    avg_loss:0.117, val_acc:0.958]
Epoch [21/120    avg_loss:0.101, val_acc:0.974]
Epoch [22/120    avg_loss:0.098, val_acc:0.935]
Epoch [23/120    avg_loss:0.114, val_acc:0.925]
Epoch [24/120    avg_loss:0.111, val_acc:0.970]
Epoch [25/120    avg_loss:0.074, val_acc:0.963]
Epoch [26/120    avg_loss:0.086, val_acc:0.968]
Epoch [27/120    avg_loss:0.053, val_acc:0.980]
Epoch [28/120    avg_loss:0.046, val_acc:0.970]
Epoch [29/120    avg_loss:0.045, val_acc:0.963]
Epoch [30/120    avg_loss:0.061, val_acc:0.963]
Epoch [31/120    avg_loss:0.050, val_acc:0.971]
Epoch [32/120    avg_loss:0.033, val_acc:0.986]
Epoch [33/120    avg_loss:0.027, val_acc:0.983]
Epoch [34/120    avg_loss:0.022, val_acc:0.993]
Epoch [35/120    avg_loss:0.028, val_acc:0.959]
Epoch [36/120    avg_loss:0.037, val_acc:0.979]
Epoch [37/120    avg_loss:0.024, val_acc:0.991]
Epoch [38/120    avg_loss:0.040, val_acc:0.903]
Epoch [39/120    avg_loss:0.030, val_acc:0.977]
Epoch [40/120    avg_loss:0.028, val_acc:0.983]
Epoch [41/120    avg_loss:0.025, val_acc:0.970]
Epoch [42/120    avg_loss:0.025, val_acc:0.994]
Epoch [43/120    avg_loss:0.025, val_acc:0.991]
Epoch [44/120    avg_loss:0.033, val_acc:0.987]
Epoch [45/120    avg_loss:0.013, val_acc:0.990]
Epoch [46/120    avg_loss:0.016, val_acc:0.990]
Epoch [47/120    avg_loss:0.018, val_acc:0.991]
Epoch [48/120    avg_loss:0.012, val_acc:0.993]
Epoch [49/120    avg_loss:0.015, val_acc:0.984]
Epoch [50/120    avg_loss:0.012, val_acc:0.992]
Epoch [51/120    avg_loss:0.016, val_acc:0.985]
Epoch [52/120    avg_loss:0.021, val_acc:0.986]
Epoch [53/120    avg_loss:0.014, val_acc:0.989]
Epoch [54/120    avg_loss:0.015, val_acc:0.986]
Epoch [55/120    avg_loss:0.010, val_acc:0.990]
Epoch [56/120    avg_loss:0.009, val_acc:0.992]
Epoch [57/120    avg_loss:0.007, val_acc:0.991]
Epoch [58/120    avg_loss:0.007, val_acc:0.991]
Epoch [59/120    avg_loss:0.007, val_acc:0.990]
Epoch [60/120    avg_loss:0.007, val_acc:0.991]
Epoch [61/120    avg_loss:0.006, val_acc:0.990]
Epoch [62/120    avg_loss:0.006, val_acc:0.992]
Epoch [63/120    avg_loss:0.006, val_acc:0.992]
Epoch [64/120    avg_loss:0.006, val_acc:0.992]
Epoch [65/120    avg_loss:0.006, val_acc:0.992]
Epoch [66/120    avg_loss:0.007, val_acc:0.992]
Epoch [67/120    avg_loss:0.007, val_acc:0.992]
Epoch [68/120    avg_loss:0.006, val_acc:0.991]
Epoch [69/120    avg_loss:0.007, val_acc:0.992]
Epoch [70/120    avg_loss:0.007, val_acc:0.991]
Epoch [71/120    avg_loss:0.007, val_acc:0.992]
Epoch [72/120    avg_loss:0.005, val_acc:0.992]
Epoch [73/120    avg_loss:0.005, val_acc:0.992]
Epoch [74/120    avg_loss:0.007, val_acc:0.992]
Epoch [75/120    avg_loss:0.006, val_acc:0.992]
Epoch [76/120    avg_loss:0.005, val_acc:0.992]
Epoch [77/120    avg_loss:0.005, val_acc:0.992]
Epoch [78/120    avg_loss:0.005, val_acc:0.992]
Epoch [79/120    avg_loss:0.006, val_acc:0.992]
Epoch [80/120    avg_loss:0.006, val_acc:0.992]
Epoch [81/120    avg_loss:0.005, val_acc:0.992]
Epoch [82/120    avg_loss:0.006, val_acc:0.992]
Epoch [83/120    avg_loss:0.005, val_acc:0.992]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.006, val_acc:0.992]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.006, val_acc:0.992]
Epoch [91/120    avg_loss:0.006, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.007, val_acc:0.992]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.006, val_acc:0.992]
Epoch [99/120    avg_loss:0.005, val_acc:0.992]
Epoch [100/120    avg_loss:0.007, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.006, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     0     0    15     0    16     0]
 [    0     1 18043     0    31     0    13     0     2     0]
 [    0     0     0  2018     0     0     0     0    14     4]
 [    0    45    12     0  2888     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4865     0    13     0]
 [    0     7     0     0     0     1     2  1270     0    10]
 [    0     2     0    23    53     0     0     0  3486     7]
 [    0     0     0     0    22    31     0     0     0   866]]

Accuracy:
99.15407418118718

F1 scores:
[       nan 0.99332713 0.99836769 0.98994359 0.96815287 0.98788796
 0.99560012 0.9921875  0.97797728 0.95902547]

Kappa:
0.988794250589164
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f959ebf0898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.791, val_acc:0.312]
Epoch [2/120    avg_loss:1.194, val_acc:0.598]
Epoch [3/120    avg_loss:0.937, val_acc:0.691]
Epoch [4/120    avg_loss:0.807, val_acc:0.666]
Epoch [5/120    avg_loss:0.607, val_acc:0.659]
Epoch [6/120    avg_loss:0.515, val_acc:0.758]
Epoch [7/120    avg_loss:0.409, val_acc:0.720]
Epoch [8/120    avg_loss:0.355, val_acc:0.777]
Epoch [9/120    avg_loss:0.317, val_acc:0.848]
Epoch [10/120    avg_loss:0.284, val_acc:0.805]
Epoch [11/120    avg_loss:0.260, val_acc:0.867]
Epoch [12/120    avg_loss:0.199, val_acc:0.926]
Epoch [13/120    avg_loss:0.162, val_acc:0.928]
Epoch [14/120    avg_loss:0.137, val_acc:0.943]
Epoch [15/120    avg_loss:0.138, val_acc:0.948]
Epoch [16/120    avg_loss:0.124, val_acc:0.951]
Epoch [17/120    avg_loss:0.100, val_acc:0.969]
Epoch [18/120    avg_loss:0.073, val_acc:0.965]
Epoch [19/120    avg_loss:0.064, val_acc:0.967]
Epoch [20/120    avg_loss:0.065, val_acc:0.970]
Epoch [21/120    avg_loss:0.063, val_acc:0.915]
Epoch [22/120    avg_loss:0.094, val_acc:0.944]
Epoch [23/120    avg_loss:0.079, val_acc:0.965]
Epoch [24/120    avg_loss:0.052, val_acc:0.974]
Epoch [25/120    avg_loss:0.105, val_acc:0.962]
Epoch [26/120    avg_loss:0.093, val_acc:0.954]
Epoch [27/120    avg_loss:0.063, val_acc:0.975]
Epoch [28/120    avg_loss:0.049, val_acc:0.967]
Epoch [29/120    avg_loss:0.050, val_acc:0.982]
Epoch [30/120    avg_loss:0.031, val_acc:0.973]
Epoch [31/120    avg_loss:0.028, val_acc:0.979]
Epoch [32/120    avg_loss:0.027, val_acc:0.962]
Epoch [33/120    avg_loss:0.019, val_acc:0.979]
Epoch [34/120    avg_loss:0.026, val_acc:0.963]
Epoch [35/120    avg_loss:0.046, val_acc:0.980]
Epoch [36/120    avg_loss:0.065, val_acc:0.951]
Epoch [37/120    avg_loss:0.043, val_acc:0.959]
Epoch [38/120    avg_loss:0.027, val_acc:0.970]
Epoch [39/120    avg_loss:0.036, val_acc:0.976]
Epoch [40/120    avg_loss:0.036, val_acc:0.965]
Epoch [41/120    avg_loss:0.028, val_acc:0.974]
Epoch [42/120    avg_loss:0.026, val_acc:0.980]
Epoch [43/120    avg_loss:0.021, val_acc:0.982]
Epoch [44/120    avg_loss:0.014, val_acc:0.986]
Epoch [45/120    avg_loss:0.014, val_acc:0.986]
Epoch [46/120    avg_loss:0.013, val_acc:0.986]
Epoch [47/120    avg_loss:0.013, val_acc:0.984]
Epoch [48/120    avg_loss:0.011, val_acc:0.986]
Epoch [49/120    avg_loss:0.012, val_acc:0.987]
Epoch [50/120    avg_loss:0.009, val_acc:0.988]
Epoch [51/120    avg_loss:0.012, val_acc:0.986]
Epoch [52/120    avg_loss:0.009, val_acc:0.988]
Epoch [53/120    avg_loss:0.010, val_acc:0.986]
Epoch [54/120    avg_loss:0.009, val_acc:0.989]
Epoch [55/120    avg_loss:0.010, val_acc:0.987]
Epoch [56/120    avg_loss:0.010, val_acc:0.988]
Epoch [57/120    avg_loss:0.008, val_acc:0.988]
Epoch [58/120    avg_loss:0.010, val_acc:0.987]
Epoch [59/120    avg_loss:0.009, val_acc:0.988]
Epoch [60/120    avg_loss:0.009, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.988]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.010, val_acc:0.988]
Epoch [64/120    avg_loss:0.008, val_acc:0.988]
Epoch [65/120    avg_loss:0.010, val_acc:0.988]
Epoch [66/120    avg_loss:0.007, val_acc:0.988]
Epoch [67/120    avg_loss:0.009, val_acc:0.988]
Epoch [68/120    avg_loss:0.010, val_acc:0.988]
Epoch [69/120    avg_loss:0.010, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.988]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.010, val_acc:0.988]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.011, val_acc:0.988]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6329     0     0     0     0    15    30    57     1]
 [    0     0 18070     0    18     0     0     0     2     0]
 [    0     8     0  2008     2     0     0     0    10     8]
 [    0    29     7     0  2913     0     6     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     9     0     0  4869     0     0     0]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     3     0     1    55     0     0     0  3498    14]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.19745499240835

F1 scores:
[       nan 0.988829   0.99925346 0.99062654 0.97522598 0.99088838
 0.99662266 0.98734177 0.97777778 0.96653867]

Kappa:
0.9893691968456053
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbee66f9748>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.814, val_acc:0.312]
Epoch [2/120    avg_loss:1.198, val_acc:0.360]
Epoch [3/120    avg_loss:0.917, val_acc:0.655]
Epoch [4/120    avg_loss:0.699, val_acc:0.703]
Epoch [5/120    avg_loss:0.579, val_acc:0.804]
Epoch [6/120    avg_loss:0.456, val_acc:0.839]
Epoch [7/120    avg_loss:0.360, val_acc:0.860]
Epoch [8/120    avg_loss:0.315, val_acc:0.891]
Epoch [9/120    avg_loss:0.331, val_acc:0.887]
Epoch [10/120    avg_loss:0.237, val_acc:0.922]
Epoch [11/120    avg_loss:0.219, val_acc:0.919]
Epoch [12/120    avg_loss:0.183, val_acc:0.938]
Epoch [13/120    avg_loss:0.154, val_acc:0.942]
Epoch [14/120    avg_loss:0.158, val_acc:0.916]
Epoch [15/120    avg_loss:0.133, val_acc:0.936]
Epoch [16/120    avg_loss:0.150, val_acc:0.954]
Epoch [17/120    avg_loss:0.103, val_acc:0.952]
Epoch [18/120    avg_loss:0.081, val_acc:0.969]
Epoch [19/120    avg_loss:0.085, val_acc:0.946]
Epoch [20/120    avg_loss:0.096, val_acc:0.949]
Epoch [21/120    avg_loss:0.082, val_acc:0.970]
Epoch [22/120    avg_loss:0.081, val_acc:0.958]
Epoch [23/120    avg_loss:0.078, val_acc:0.977]
Epoch [24/120    avg_loss:0.075, val_acc:0.979]
Epoch [25/120    avg_loss:0.050, val_acc:0.979]
Epoch [26/120    avg_loss:0.046, val_acc:0.974]
Epoch [27/120    avg_loss:0.064, val_acc:0.972]
Epoch [28/120    avg_loss:0.044, val_acc:0.973]
Epoch [29/120    avg_loss:0.054, val_acc:0.969]
Epoch [30/120    avg_loss:0.038, val_acc:0.971]
Epoch [31/120    avg_loss:0.036, val_acc:0.982]
Epoch [32/120    avg_loss:0.050, val_acc:0.963]
Epoch [33/120    avg_loss:0.033, val_acc:0.983]
Epoch [34/120    avg_loss:0.044, val_acc:0.978]
Epoch [35/120    avg_loss:0.037, val_acc:0.973]
Epoch [36/120    avg_loss:0.034, val_acc:0.977]
Epoch [37/120    avg_loss:0.029, val_acc:0.980]
Epoch [38/120    avg_loss:0.034, val_acc:0.979]
Epoch [39/120    avg_loss:0.028, val_acc:0.984]
Epoch [40/120    avg_loss:0.022, val_acc:0.986]
Epoch [41/120    avg_loss:0.025, val_acc:0.985]
Epoch [42/120    avg_loss:0.018, val_acc:0.989]
Epoch [43/120    avg_loss:0.020, val_acc:0.990]
Epoch [44/120    avg_loss:0.021, val_acc:0.983]
Epoch [45/120    avg_loss:0.022, val_acc:0.972]
Epoch [46/120    avg_loss:0.017, val_acc:0.988]
Epoch [47/120    avg_loss:0.046, val_acc:0.974]
Epoch [48/120    avg_loss:0.031, val_acc:0.985]
Epoch [49/120    avg_loss:0.050, val_acc:0.982]
Epoch [50/120    avg_loss:0.026, val_acc:0.984]
Epoch [51/120    avg_loss:0.015, val_acc:0.987]
Epoch [52/120    avg_loss:0.012, val_acc:0.992]
Epoch [53/120    avg_loss:0.011, val_acc:0.989]
Epoch [54/120    avg_loss:0.021, val_acc:0.984]
Epoch [55/120    avg_loss:0.013, val_acc:0.988]
Epoch [56/120    avg_loss:0.022, val_acc:0.984]
Epoch [57/120    avg_loss:0.019, val_acc:0.986]
Epoch [58/120    avg_loss:0.042, val_acc:0.984]
Epoch [59/120    avg_loss:0.039, val_acc:0.981]
Epoch [60/120    avg_loss:0.013, val_acc:0.987]
Epoch [61/120    avg_loss:0.015, val_acc:0.970]
Epoch [62/120    avg_loss:0.011, val_acc:0.989]
Epoch [63/120    avg_loss:0.011, val_acc:0.989]
Epoch [64/120    avg_loss:0.018, val_acc:0.986]
Epoch [65/120    avg_loss:0.022, val_acc:0.988]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.008, val_acc:0.990]
Epoch [68/120    avg_loss:0.007, val_acc:0.990]
Epoch [69/120    avg_loss:0.010, val_acc:0.990]
Epoch [70/120    avg_loss:0.007, val_acc:0.990]
Epoch [71/120    avg_loss:0.007, val_acc:0.992]
Epoch [72/120    avg_loss:0.007, val_acc:0.992]
Epoch [73/120    avg_loss:0.007, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.990]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.009, val_acc:0.990]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.990]
Epoch [83/120    avg_loss:0.008, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.990]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.006, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.007, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.008, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6376     0     0     0     0    13    32    11     0]
 [    0     0 18074     0    16     0     0     0     0     0]
 [    0     7     0  2025     0     0     0     0     2     2]
 [    0    43    21     0  2876     0     4     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4864     0     4    10]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0    10     0     6    51     0     0     0  3504     0]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.26252620924012

F1 scores:
[       nan 0.99098539 0.99897748 0.99582001 0.97014674 0.98863636
 0.9966192  0.98697318 0.98426966 0.96899225]

Kappa:
0.9902276079672326
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbb72c26860>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.719, val_acc:0.350]
Epoch [2/120    avg_loss:1.230, val_acc:0.448]
Epoch [3/120    avg_loss:0.999, val_acc:0.543]
Epoch [4/120    avg_loss:0.777, val_acc:0.635]
Epoch [5/120    avg_loss:0.607, val_acc:0.677]
Epoch [6/120    avg_loss:0.548, val_acc:0.705]
Epoch [7/120    avg_loss:0.482, val_acc:0.795]
Epoch [8/120    avg_loss:0.343, val_acc:0.839]
Epoch [9/120    avg_loss:0.318, val_acc:0.816]
Epoch [10/120    avg_loss:0.295, val_acc:0.872]
Epoch [11/120    avg_loss:0.246, val_acc:0.878]
Epoch [12/120    avg_loss:0.248, val_acc:0.924]
Epoch [13/120    avg_loss:0.208, val_acc:0.932]
Epoch [14/120    avg_loss:0.157, val_acc:0.942]
Epoch [15/120    avg_loss:0.140, val_acc:0.954]
Epoch [16/120    avg_loss:0.131, val_acc:0.952]
Epoch [17/120    avg_loss:0.108, val_acc:0.969]
Epoch [18/120    avg_loss:0.091, val_acc:0.964]
Epoch [19/120    avg_loss:0.155, val_acc:0.955]
Epoch [20/120    avg_loss:0.148, val_acc:0.939]
Epoch [21/120    avg_loss:0.109, val_acc:0.947]
Epoch [22/120    avg_loss:0.091, val_acc:0.972]
Epoch [23/120    avg_loss:0.094, val_acc:0.966]
Epoch [24/120    avg_loss:0.070, val_acc:0.972]
Epoch [25/120    avg_loss:0.055, val_acc:0.957]
Epoch [26/120    avg_loss:0.062, val_acc:0.920]
Epoch [27/120    avg_loss:0.055, val_acc:0.968]
Epoch [28/120    avg_loss:0.036, val_acc:0.980]
Epoch [29/120    avg_loss:0.024, val_acc:0.979]
Epoch [30/120    avg_loss:0.025, val_acc:0.983]
Epoch [31/120    avg_loss:0.021, val_acc:0.984]
Epoch [32/120    avg_loss:0.021, val_acc:0.979]
Epoch [33/120    avg_loss:0.020, val_acc:0.982]
Epoch [34/120    avg_loss:0.028, val_acc:0.984]
Epoch [35/120    avg_loss:0.036, val_acc:0.976]
Epoch [36/120    avg_loss:0.032, val_acc:0.981]
Epoch [37/120    avg_loss:0.027, val_acc:0.964]
Epoch [38/120    avg_loss:0.058, val_acc:0.962]
Epoch [39/120    avg_loss:0.032, val_acc:0.981]
Epoch [40/120    avg_loss:0.018, val_acc:0.973]
Epoch [41/120    avg_loss:0.037, val_acc:0.967]
Epoch [42/120    avg_loss:0.023, val_acc:0.982]
Epoch [43/120    avg_loss:0.016, val_acc:0.984]
Epoch [44/120    avg_loss:0.013, val_acc:0.980]
Epoch [45/120    avg_loss:0.010, val_acc:0.982]
Epoch [46/120    avg_loss:0.010, val_acc:0.984]
Epoch [47/120    avg_loss:0.009, val_acc:0.985]
Epoch [48/120    avg_loss:0.008, val_acc:0.984]
Epoch [49/120    avg_loss:0.012, val_acc:0.984]
Epoch [50/120    avg_loss:0.012, val_acc:0.985]
Epoch [51/120    avg_loss:0.010, val_acc:0.984]
Epoch [52/120    avg_loss:0.007, val_acc:0.984]
Epoch [53/120    avg_loss:0.010, val_acc:0.985]
Epoch [54/120    avg_loss:0.010, val_acc:0.984]
Epoch [55/120    avg_loss:0.007, val_acc:0.985]
Epoch [56/120    avg_loss:0.009, val_acc:0.984]
Epoch [57/120    avg_loss:0.008, val_acc:0.984]
Epoch [58/120    avg_loss:0.008, val_acc:0.984]
Epoch [59/120    avg_loss:0.008, val_acc:0.984]
Epoch [60/120    avg_loss:0.009, val_acc:0.984]
Epoch [61/120    avg_loss:0.009, val_acc:0.986]
Epoch [62/120    avg_loss:0.009, val_acc:0.986]
Epoch [63/120    avg_loss:0.008, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.984]
Epoch [65/120    avg_loss:0.008, val_acc:0.985]
Epoch [66/120    avg_loss:0.007, val_acc:0.986]
Epoch [67/120    avg_loss:0.008, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.984]
Epoch [69/120    avg_loss:0.007, val_acc:0.984]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.008, val_acc:0.984]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.985]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     0     0     0    18     6    49     0]
 [    0     0 18033     0    51     0     6     0     0     0]
 [    0     4     0  2021     0     0     0     0     6     5]
 [    0    49    24     0  2873     0     1     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4837     0    11    23]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     0     0    13    55     0     0     0  3476    27]
 [    0     0     0     0    17    49     0     0     0   853]]

Accuracy:
98.9178897645386

F1 scores:
[       nan 0.99018997 0.99756597 0.99312039 0.96280161 0.98157202
 0.99301991 0.99651568 0.97394228 0.93326039]

Kappa:
0.985667515287108
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c473907f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.782, val_acc:0.307]
Epoch [2/120    avg_loss:1.271, val_acc:0.465]
Epoch [3/120    avg_loss:1.039, val_acc:0.685]
Epoch [4/120    avg_loss:0.863, val_acc:0.700]
Epoch [5/120    avg_loss:0.703, val_acc:0.780]
Epoch [6/120    avg_loss:0.537, val_acc:0.736]
Epoch [7/120    avg_loss:0.477, val_acc:0.767]
Epoch [8/120    avg_loss:0.398, val_acc:0.873]
Epoch [9/120    avg_loss:0.297, val_acc:0.854]
Epoch [10/120    avg_loss:0.261, val_acc:0.885]
Epoch [11/120    avg_loss:0.209, val_acc:0.902]
Epoch [12/120    avg_loss:0.196, val_acc:0.933]
Epoch [13/120    avg_loss:0.165, val_acc:0.956]
Epoch [14/120    avg_loss:0.122, val_acc:0.952]
Epoch [15/120    avg_loss:0.112, val_acc:0.961]
Epoch [16/120    avg_loss:0.120, val_acc:0.933]
Epoch [17/120    avg_loss:0.108, val_acc:0.951]
Epoch [18/120    avg_loss:0.089, val_acc:0.955]
Epoch [19/120    avg_loss:0.108, val_acc:0.949]
Epoch [20/120    avg_loss:0.113, val_acc:0.926]
Epoch [21/120    avg_loss:0.087, val_acc:0.960]
Epoch [22/120    avg_loss:0.100, val_acc:0.931]
Epoch [23/120    avg_loss:0.070, val_acc:0.965]
Epoch [24/120    avg_loss:0.060, val_acc:0.962]
Epoch [25/120    avg_loss:0.056, val_acc:0.966]
Epoch [26/120    avg_loss:0.041, val_acc:0.961]
Epoch [27/120    avg_loss:0.055, val_acc:0.957]
Epoch [28/120    avg_loss:0.056, val_acc:0.961]
Epoch [29/120    avg_loss:0.054, val_acc:0.946]
Epoch [30/120    avg_loss:0.061, val_acc:0.947]
Epoch [31/120    avg_loss:0.064, val_acc:0.963]
Epoch [32/120    avg_loss:0.118, val_acc:0.948]
Epoch [33/120    avg_loss:0.077, val_acc:0.961]
Epoch [34/120    avg_loss:0.056, val_acc:0.973]
Epoch [35/120    avg_loss:0.047, val_acc:0.973]
Epoch [36/120    avg_loss:0.028, val_acc:0.969]
Epoch [37/120    avg_loss:0.044, val_acc:0.947]
Epoch [38/120    avg_loss:0.039, val_acc:0.968]
Epoch [39/120    avg_loss:0.032, val_acc:0.978]
Epoch [40/120    avg_loss:0.018, val_acc:0.978]
Epoch [41/120    avg_loss:0.017, val_acc:0.979]
Epoch [42/120    avg_loss:0.025, val_acc:0.965]
Epoch [43/120    avg_loss:0.031, val_acc:0.974]
Epoch [44/120    avg_loss:0.073, val_acc:0.951]
Epoch [45/120    avg_loss:0.038, val_acc:0.977]
Epoch [46/120    avg_loss:0.034, val_acc:0.971]
Epoch [47/120    avg_loss:0.024, val_acc:0.977]
Epoch [48/120    avg_loss:0.026, val_acc:0.966]
Epoch [49/120    avg_loss:0.032, val_acc:0.977]
Epoch [50/120    avg_loss:0.018, val_acc:0.980]
Epoch [51/120    avg_loss:0.030, val_acc:0.887]
Epoch [52/120    avg_loss:0.030, val_acc:0.979]
Epoch [53/120    avg_loss:0.015, val_acc:0.977]
Epoch [54/120    avg_loss:0.023, val_acc:0.964]
Epoch [55/120    avg_loss:0.016, val_acc:0.976]
Epoch [56/120    avg_loss:0.010, val_acc:0.962]
Epoch [57/120    avg_loss:0.019, val_acc:0.967]
Epoch [58/120    avg_loss:0.017, val_acc:0.977]
Epoch [59/120    avg_loss:0.008, val_acc:0.984]
Epoch [60/120    avg_loss:0.007, val_acc:0.984]
Epoch [61/120    avg_loss:0.007, val_acc:0.980]
Epoch [62/120    avg_loss:0.010, val_acc:0.981]
Epoch [63/120    avg_loss:0.011, val_acc:0.982]
Epoch [64/120    avg_loss:0.008, val_acc:0.981]
Epoch [65/120    avg_loss:0.010, val_acc:0.977]
Epoch [66/120    avg_loss:0.010, val_acc:0.976]
Epoch [67/120    avg_loss:0.008, val_acc:0.970]
Epoch [68/120    avg_loss:0.007, val_acc:0.983]
Epoch [69/120    avg_loss:0.005, val_acc:0.984]
Epoch [70/120    avg_loss:0.005, val_acc:0.985]
Epoch [71/120    avg_loss:0.012, val_acc:0.974]
Epoch [72/120    avg_loss:0.008, val_acc:0.984]
Epoch [73/120    avg_loss:0.015, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.004, val_acc:0.987]
Epoch [80/120    avg_loss:0.004, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.984]
Epoch [82/120    avg_loss:0.005, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.005, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.977]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.005, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.003, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.003, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.985]
Epoch [102/120    avg_loss:0.003, val_acc:0.985]
Epoch [103/120    avg_loss:0.004, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.985]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.003, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.003, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     1     0     0     7     0    11     0]
 [    0     0 18032     0    45     0    13     0     0     0]
 [    0     2     0  2028     2     0     0     0     1     3]
 [    0    42    18     0  2880     0     5     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     5     0     0  4852     0    15     4]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0     1    55     0     0     0  3500    13]
 [    0     0     0     0    14    33     0     0     0   872]]

Accuracy:
99.22637553322248

F1 scores:
[       nan 0.99495772 0.99784185 0.9963154  0.96514745 0.98751419
 0.99477191 1.         0.98245614 0.96300387]

Kappa:
0.9897528870996077
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff62900b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.818, val_acc:0.251]
Epoch [2/120    avg_loss:1.223, val_acc:0.343]
Epoch [3/120    avg_loss:0.966, val_acc:0.658]
Epoch [4/120    avg_loss:0.783, val_acc:0.639]
Epoch [5/120    avg_loss:0.622, val_acc:0.780]
Epoch [6/120    avg_loss:0.487, val_acc:0.774]
Epoch [7/120    avg_loss:0.401, val_acc:0.813]
Epoch [8/120    avg_loss:0.377, val_acc:0.848]
Epoch [9/120    avg_loss:0.329, val_acc:0.824]
Epoch [10/120    avg_loss:0.245, val_acc:0.914]
Epoch [11/120    avg_loss:0.227, val_acc:0.891]
Epoch [12/120    avg_loss:0.217, val_acc:0.910]
Epoch [13/120    avg_loss:0.167, val_acc:0.941]
Epoch [14/120    avg_loss:0.169, val_acc:0.891]
Epoch [15/120    avg_loss:0.178, val_acc:0.858]
Epoch [16/120    avg_loss:0.159, val_acc:0.953]
Epoch [17/120    avg_loss:0.150, val_acc:0.905]
Epoch [18/120    avg_loss:0.131, val_acc:0.950]
Epoch [19/120    avg_loss:0.091, val_acc:0.973]
Epoch [20/120    avg_loss:0.103, val_acc:0.960]
Epoch [21/120    avg_loss:0.096, val_acc:0.965]
Epoch [22/120    avg_loss:0.081, val_acc:0.971]
Epoch [23/120    avg_loss:0.075, val_acc:0.972]
Epoch [24/120    avg_loss:0.053, val_acc:0.969]
Epoch [25/120    avg_loss:0.049, val_acc:0.977]
Epoch [26/120    avg_loss:0.058, val_acc:0.972]
Epoch [27/120    avg_loss:0.045, val_acc:0.959]
Epoch [28/120    avg_loss:0.049, val_acc:0.908]
Epoch [29/120    avg_loss:0.035, val_acc:0.979]
Epoch [30/120    avg_loss:0.027, val_acc:0.985]
Epoch [31/120    avg_loss:0.027, val_acc:0.986]
Epoch [32/120    avg_loss:0.027, val_acc:0.980]
Epoch [33/120    avg_loss:0.026, val_acc:0.978]
Epoch [34/120    avg_loss:0.023, val_acc:0.987]
Epoch [35/120    avg_loss:0.016, val_acc:0.988]
Epoch [36/120    avg_loss:0.018, val_acc:0.988]
Epoch [37/120    avg_loss:0.020, val_acc:0.979]
Epoch [38/120    avg_loss:0.021, val_acc:0.989]
Epoch [39/120    avg_loss:0.036, val_acc:0.979]
Epoch [40/120    avg_loss:0.040, val_acc:0.976]
Epoch [41/120    avg_loss:0.047, val_acc:0.985]
Epoch [42/120    avg_loss:0.060, val_acc:0.969]
Epoch [43/120    avg_loss:0.034, val_acc:0.981]
Epoch [44/120    avg_loss:0.027, val_acc:0.966]
Epoch [45/120    avg_loss:0.035, val_acc:0.972]
Epoch [46/120    avg_loss:0.025, val_acc:0.986]
Epoch [47/120    avg_loss:0.020, val_acc:0.981]
Epoch [48/120    avg_loss:0.017, val_acc:0.990]
Epoch [49/120    avg_loss:0.018, val_acc:0.992]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.013, val_acc:0.989]
Epoch [52/120    avg_loss:0.008, val_acc:0.990]
Epoch [53/120    avg_loss:0.007, val_acc:0.992]
Epoch [54/120    avg_loss:0.010, val_acc:0.992]
Epoch [55/120    avg_loss:0.010, val_acc:0.986]
Epoch [56/120    avg_loss:0.007, val_acc:0.986]
Epoch [57/120    avg_loss:0.010, val_acc:0.992]
Epoch [58/120    avg_loss:0.010, val_acc:0.992]
Epoch [59/120    avg_loss:0.006, val_acc:0.992]
Epoch [60/120    avg_loss:0.010, val_acc:0.992]
Epoch [61/120    avg_loss:0.009, val_acc:0.990]
Epoch [62/120    avg_loss:0.006, val_acc:0.993]
Epoch [63/120    avg_loss:0.005, val_acc:0.994]
Epoch [64/120    avg_loss:0.011, val_acc:0.992]
Epoch [65/120    avg_loss:0.006, val_acc:0.992]
Epoch [66/120    avg_loss:0.009, val_acc:0.977]
Epoch [67/120    avg_loss:0.007, val_acc:0.990]
Epoch [68/120    avg_loss:0.005, val_acc:0.992]
Epoch [69/120    avg_loss:0.004, val_acc:0.992]
Epoch [70/120    avg_loss:0.005, val_acc:0.995]
Epoch [71/120    avg_loss:0.005, val_acc:0.993]
Epoch [72/120    avg_loss:0.005, val_acc:0.993]
Epoch [73/120    avg_loss:0.004, val_acc:0.994]
Epoch [74/120    avg_loss:0.004, val_acc:0.992]
Epoch [75/120    avg_loss:0.003, val_acc:0.994]
Epoch [76/120    avg_loss:0.004, val_acc:0.995]
Epoch [77/120    avg_loss:0.007, val_acc:0.987]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.005, val_acc:0.994]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.005, val_acc:0.992]
Epoch [82/120    avg_loss:0.006, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.990]
Epoch [84/120    avg_loss:0.003, val_acc:0.993]
Epoch [85/120    avg_loss:0.005, val_acc:0.967]
Epoch [86/120    avg_loss:0.010, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.994]
Epoch [88/120    avg_loss:0.003, val_acc:0.995]
Epoch [89/120    avg_loss:0.008, val_acc:0.990]
Epoch [90/120    avg_loss:0.009, val_acc:0.992]
Epoch [91/120    avg_loss:0.010, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.994]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.992]
Epoch [97/120    avg_loss:0.004, val_acc:0.992]
Epoch [98/120    avg_loss:0.003, val_acc:0.994]
Epoch [99/120    avg_loss:0.003, val_acc:0.993]
Epoch [100/120    avg_loss:0.006, val_acc:0.993]
Epoch [101/120    avg_loss:0.011, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.992]
Epoch [103/120    avg_loss:0.003, val_acc:0.994]
Epoch [104/120    avg_loss:0.006, val_acc:0.993]
Epoch [105/120    avg_loss:0.004, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.994]
Epoch [107/120    avg_loss:0.003, val_acc:0.995]
Epoch [108/120    avg_loss:0.004, val_acc:0.994]
Epoch [109/120    avg_loss:0.004, val_acc:0.994]
Epoch [110/120    avg_loss:0.003, val_acc:0.994]
Epoch [111/120    avg_loss:0.002, val_acc:0.994]
Epoch [112/120    avg_loss:0.002, val_acc:0.994]
Epoch [113/120    avg_loss:0.003, val_acc:0.993]
Epoch [114/120    avg_loss:0.002, val_acc:0.993]
Epoch [115/120    avg_loss:0.003, val_acc:0.994]
Epoch [116/120    avg_loss:0.003, val_acc:0.994]
Epoch [117/120    avg_loss:0.002, val_acc:0.994]
Epoch [118/120    avg_loss:0.003, val_acc:0.994]
Epoch [119/120    avg_loss:0.003, val_acc:0.994]
Epoch [120/120    avg_loss:0.002, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     0     0     1     1    13     4]
 [    0     0 18062     0    27     0     1     0     0     0]
 [    0     0     0  2033     1     0     0     0     0     2]
 [    0    41    15     0  2887     0     3     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     9     0     0  4856     0     0    13]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     2     0    25    42     0     0     0  3497     5]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.3348275612754

F1 scores:
[       nan 0.99518932 0.99881107 0.99098221 0.97156318 0.9893859
 0.99702289 0.99844841 0.98410018 0.96320703]

Kappa:
0.9911874231207848
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ada13e7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.867, val_acc:0.278]
Epoch [2/120    avg_loss:1.246, val_acc:0.529]
Epoch [3/120    avg_loss:0.971, val_acc:0.624]
Epoch [4/120    avg_loss:0.812, val_acc:0.724]
Epoch [5/120    avg_loss:0.656, val_acc:0.703]
Epoch [6/120    avg_loss:0.510, val_acc:0.768]
Epoch [7/120    avg_loss:0.422, val_acc:0.809]
Epoch [8/120    avg_loss:0.358, val_acc:0.825]
Epoch [9/120    avg_loss:0.332, val_acc:0.805]
Epoch [10/120    avg_loss:0.286, val_acc:0.884]
Epoch [11/120    avg_loss:0.238, val_acc:0.904]
Epoch [12/120    avg_loss:0.174, val_acc:0.937]
Epoch [13/120    avg_loss:0.147, val_acc:0.938]
Epoch [14/120    avg_loss:0.124, val_acc:0.907]
Epoch [15/120    avg_loss:0.149, val_acc:0.855]
Epoch [16/120    avg_loss:0.159, val_acc:0.927]
Epoch [17/120    avg_loss:0.090, val_acc:0.940]
Epoch [18/120    avg_loss:0.103, val_acc:0.951]
Epoch [19/120    avg_loss:0.079, val_acc:0.965]
Epoch [20/120    avg_loss:0.066, val_acc:0.974]
Epoch [21/120    avg_loss:0.074, val_acc:0.960]
Epoch [22/120    avg_loss:0.069, val_acc:0.928]
Epoch [23/120    avg_loss:0.064, val_acc:0.940]
Epoch [24/120    avg_loss:0.054, val_acc:0.971]
Epoch [25/120    avg_loss:0.035, val_acc:0.982]
Epoch [26/120    avg_loss:0.068, val_acc:0.976]
Epoch [27/120    avg_loss:0.031, val_acc:0.982]
Epoch [28/120    avg_loss:0.029, val_acc:0.982]
Epoch [29/120    avg_loss:0.029, val_acc:0.982]
Epoch [30/120    avg_loss:0.025, val_acc:0.981]
Epoch [31/120    avg_loss:0.034, val_acc:0.979]
Epoch [32/120    avg_loss:0.036, val_acc:0.981]
Epoch [33/120    avg_loss:0.023, val_acc:0.987]
Epoch [34/120    avg_loss:0.026, val_acc:0.986]
Epoch [35/120    avg_loss:0.034, val_acc:0.983]
Epoch [36/120    avg_loss:0.047, val_acc:0.960]
Epoch [37/120    avg_loss:0.076, val_acc:0.948]
Epoch [38/120    avg_loss:0.081, val_acc:0.975]
Epoch [39/120    avg_loss:0.065, val_acc:0.966]
Epoch [40/120    avg_loss:0.029, val_acc:0.979]
Epoch [41/120    avg_loss:0.045, val_acc:0.926]
Epoch [42/120    avg_loss:0.023, val_acc:0.977]
Epoch [43/120    avg_loss:0.015, val_acc:0.984]
Epoch [44/120    avg_loss:0.020, val_acc:0.976]
Epoch [45/120    avg_loss:0.018, val_acc:0.987]
Epoch [46/120    avg_loss:0.014, val_acc:0.989]
Epoch [47/120    avg_loss:0.009, val_acc:0.985]
Epoch [48/120    avg_loss:0.009, val_acc:0.986]
Epoch [49/120    avg_loss:0.016, val_acc:0.989]
Epoch [50/120    avg_loss:0.015, val_acc:0.985]
Epoch [51/120    avg_loss:0.011, val_acc:0.988]
Epoch [52/120    avg_loss:0.017, val_acc:0.989]
Epoch [53/120    avg_loss:0.022, val_acc:0.989]
Epoch [54/120    avg_loss:0.018, val_acc:0.987]
Epoch [55/120    avg_loss:0.012, val_acc:0.990]
Epoch [56/120    avg_loss:0.009, val_acc:0.989]
Epoch [57/120    avg_loss:0.009, val_acc:0.989]
Epoch [58/120    avg_loss:0.007, val_acc:0.991]
Epoch [59/120    avg_loss:0.008, val_acc:0.986]
Epoch [60/120    avg_loss:0.016, val_acc:0.965]
Epoch [61/120    avg_loss:0.027, val_acc:0.988]
Epoch [62/120    avg_loss:0.010, val_acc:0.986]
Epoch [63/120    avg_loss:0.007, val_acc:0.991]
Epoch [64/120    avg_loss:0.009, val_acc:0.990]
Epoch [65/120    avg_loss:0.010, val_acc:0.990]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.005, val_acc:0.990]
Epoch [69/120    avg_loss:0.007, val_acc:0.991]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.982]
Epoch [73/120    avg_loss:0.008, val_acc:0.992]
Epoch [74/120    avg_loss:0.008, val_acc:0.991]
Epoch [75/120    avg_loss:0.005, val_acc:0.989]
Epoch [76/120    avg_loss:0.003, val_acc:0.991]
Epoch [77/120    avg_loss:0.004, val_acc:0.989]
Epoch [78/120    avg_loss:0.005, val_acc:0.992]
Epoch [79/120    avg_loss:0.012, val_acc:0.960]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.960]
Epoch [82/120    avg_loss:0.008, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.003, val_acc:0.992]
Epoch [87/120    avg_loss:0.003, val_acc:0.991]
Epoch [88/120    avg_loss:0.005, val_acc:0.992]
Epoch [89/120    avg_loss:0.006, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.993]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.992]
Epoch [96/120    avg_loss:0.019, val_acc:0.943]
Epoch [97/120    avg_loss:0.105, val_acc:0.941]
Epoch [98/120    avg_loss:0.063, val_acc:0.976]
Epoch [99/120    avg_loss:0.030, val_acc:0.985]
Epoch [100/120    avg_loss:0.021, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.004, val_acc:0.992]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     0     0     1     8     0     0]
 [    0     7 18054     0    27     0     2     0     0     0]
 [    0     0     0  2007     3     0     0     0    24     2]
 [    0    41     9     0  2895     0     1     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     5     0     0  4864     0     1     8]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     2     0     0    47     0     0     0  3499    23]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.32759742607188

F1 scores:
[       nan 0.99542813 0.99875529 0.99160079 0.97180262 0.99088838
 0.99774359 0.99535604 0.98272715 0.96126568]

Kappa:
0.9910923462430208
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f562635d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.767, val_acc:0.373]
Epoch [2/120    avg_loss:1.223, val_acc:0.615]
Epoch [3/120    avg_loss:0.934, val_acc:0.704]
Epoch [4/120    avg_loss:0.724, val_acc:0.695]
Epoch [5/120    avg_loss:0.568, val_acc:0.739]
Epoch [6/120    avg_loss:0.437, val_acc:0.845]
Epoch [7/120    avg_loss:0.360, val_acc:0.895]
Epoch [8/120    avg_loss:0.272, val_acc:0.926]
Epoch [9/120    avg_loss:0.262, val_acc:0.854]
Epoch [10/120    avg_loss:0.213, val_acc:0.908]
Epoch [11/120    avg_loss:0.161, val_acc:0.947]
Epoch [12/120    avg_loss:0.130, val_acc:0.949]
Epoch [13/120    avg_loss:0.131, val_acc:0.951]
Epoch [14/120    avg_loss:0.101, val_acc:0.938]
Epoch [15/120    avg_loss:0.107, val_acc:0.939]
Epoch [16/120    avg_loss:0.083, val_acc:0.963]
Epoch [17/120    avg_loss:0.081, val_acc:0.966]
Epoch [18/120    avg_loss:0.106, val_acc:0.969]
Epoch [19/120    avg_loss:0.059, val_acc:0.964]
Epoch [20/120    avg_loss:0.048, val_acc:0.968]
Epoch [21/120    avg_loss:0.044, val_acc:0.969]
Epoch [22/120    avg_loss:0.044, val_acc:0.964]
Epoch [23/120    avg_loss:0.046, val_acc:0.967]
Epoch [24/120    avg_loss:0.070, val_acc:0.965]
Epoch [25/120    avg_loss:0.050, val_acc:0.973]
Epoch [26/120    avg_loss:0.028, val_acc:0.970]
Epoch [27/120    avg_loss:0.025, val_acc:0.979]
Epoch [28/120    avg_loss:0.031, val_acc:0.964]
Epoch [29/120    avg_loss:0.046, val_acc:0.964]
Epoch [30/120    avg_loss:0.026, val_acc:0.969]
Epoch [31/120    avg_loss:0.024, val_acc:0.976]
Epoch [32/120    avg_loss:0.052, val_acc:0.951]
Epoch [33/120    avg_loss:0.030, val_acc:0.969]
Epoch [34/120    avg_loss:0.021, val_acc:0.977]
Epoch [35/120    avg_loss:0.024, val_acc:0.975]
Epoch [36/120    avg_loss:0.042, val_acc:0.962]
Epoch [37/120    avg_loss:0.053, val_acc:0.948]
Epoch [38/120    avg_loss:0.036, val_acc:0.974]
Epoch [39/120    avg_loss:0.019, val_acc:0.977]
Epoch [40/120    avg_loss:0.016, val_acc:0.980]
Epoch [41/120    avg_loss:0.012, val_acc:0.970]
Epoch [42/120    avg_loss:0.009, val_acc:0.973]
Epoch [43/120    avg_loss:0.018, val_acc:0.983]
Epoch [44/120    avg_loss:0.017, val_acc:0.984]
Epoch [45/120    avg_loss:0.009, val_acc:0.987]
Epoch [46/120    avg_loss:0.009, val_acc:0.972]
Epoch [47/120    avg_loss:0.009, val_acc:0.985]
Epoch [48/120    avg_loss:0.009, val_acc:0.980]
Epoch [49/120    avg_loss:0.008, val_acc:0.985]
Epoch [50/120    avg_loss:0.007, val_acc:0.985]
Epoch [51/120    avg_loss:0.008, val_acc:0.982]
Epoch [52/120    avg_loss:0.007, val_acc:0.962]
Epoch [53/120    avg_loss:0.014, val_acc:0.971]
Epoch [54/120    avg_loss:0.016, val_acc:0.984]
Epoch [55/120    avg_loss:0.009, val_acc:0.978]
Epoch [56/120    avg_loss:0.012, val_acc:0.970]
Epoch [57/120    avg_loss:0.015, val_acc:0.973]
Epoch [58/120    avg_loss:0.009, val_acc:0.982]
Epoch [59/120    avg_loss:0.012, val_acc:0.984]
Epoch [60/120    avg_loss:0.008, val_acc:0.988]
Epoch [61/120    avg_loss:0.006, val_acc:0.986]
Epoch [62/120    avg_loss:0.006, val_acc:0.987]
Epoch [63/120    avg_loss:0.007, val_acc:0.985]
Epoch [64/120    avg_loss:0.007, val_acc:0.984]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.005, val_acc:0.988]
Epoch [68/120    avg_loss:0.004, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.989]
Epoch [70/120    avg_loss:0.006, val_acc:0.987]
Epoch [71/120    avg_loss:0.005, val_acc:0.988]
Epoch [72/120    avg_loss:0.004, val_acc:0.989]
Epoch [73/120    avg_loss:0.004, val_acc:0.990]
Epoch [74/120    avg_loss:0.006, val_acc:0.990]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.989]
Epoch [77/120    avg_loss:0.005, val_acc:0.989]
Epoch [78/120    avg_loss:0.006, val_acc:0.989]
Epoch [79/120    avg_loss:0.005, val_acc:0.990]
Epoch [80/120    avg_loss:0.008, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.004, val_acc:0.990]
Epoch [83/120    avg_loss:0.004, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.990]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.004, val_acc:0.990]
Epoch [90/120    avg_loss:0.003, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     1     0     8     9    17     0]
 [    0     8 18033     0    33     0     9     0     7     0]
 [    0     0     0  2031     3     0     0     0     0     2]
 [    0    28    18     0  2899     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4864     0    14     0]
 [    0     0     0     0     0     0     5  1284     0     1]
 [    0     0     0     6    59     0     0     0  3497     9]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
99.27457643457933

F1 scores:
[       nan 0.99448115 0.99792479 0.99729929 0.96940311 0.99126472
 0.99631299 0.9941928  0.98051311 0.97297297]

Kappa:
0.9903919092737796
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83644a57b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.805, val_acc:0.320]
Epoch [2/120    avg_loss:1.244, val_acc:0.354]
Epoch [3/120    avg_loss:0.971, val_acc:0.616]
Epoch [4/120    avg_loss:0.782, val_acc:0.683]
Epoch [5/120    avg_loss:0.581, val_acc:0.743]
Epoch [6/120    avg_loss:0.458, val_acc:0.816]
Epoch [7/120    avg_loss:0.378, val_acc:0.768]
Epoch [8/120    avg_loss:0.340, val_acc:0.812]
Epoch [9/120    avg_loss:0.307, val_acc:0.873]
Epoch [10/120    avg_loss:0.230, val_acc:0.899]
Epoch [11/120    avg_loss:0.196, val_acc:0.924]
Epoch [12/120    avg_loss:0.186, val_acc:0.931]
Epoch [13/120    avg_loss:0.172, val_acc:0.939]
Epoch [14/120    avg_loss:0.153, val_acc:0.943]
Epoch [15/120    avg_loss:0.143, val_acc:0.946]
Epoch [16/120    avg_loss:0.098, val_acc:0.938]
Epoch [17/120    avg_loss:0.100, val_acc:0.967]
Epoch [18/120    avg_loss:0.081, val_acc:0.955]
Epoch [19/120    avg_loss:0.087, val_acc:0.965]
Epoch [20/120    avg_loss:0.106, val_acc:0.911]
Epoch [21/120    avg_loss:0.100, val_acc:0.955]
Epoch [22/120    avg_loss:0.070, val_acc:0.959]
Epoch [23/120    avg_loss:0.062, val_acc:0.964]
Epoch [24/120    avg_loss:0.050, val_acc:0.970]
Epoch [25/120    avg_loss:0.044, val_acc:0.961]
Epoch [26/120    avg_loss:0.037, val_acc:0.961]
Epoch [27/120    avg_loss:0.040, val_acc:0.971]
Epoch [28/120    avg_loss:0.050, val_acc:0.964]
Epoch [29/120    avg_loss:0.046, val_acc:0.970]
Epoch [30/120    avg_loss:0.048, val_acc:0.971]
Epoch [31/120    avg_loss:0.051, val_acc:0.938]
Epoch [32/120    avg_loss:0.073, val_acc:0.952]
Epoch [33/120    avg_loss:0.055, val_acc:0.970]
Epoch [34/120    avg_loss:0.045, val_acc:0.970]
Epoch [35/120    avg_loss:0.042, val_acc:0.964]
Epoch [36/120    avg_loss:0.040, val_acc:0.968]
Epoch [37/120    avg_loss:0.029, val_acc:0.971]
Epoch [38/120    avg_loss:0.037, val_acc:0.974]
Epoch [39/120    avg_loss:0.068, val_acc:0.961]
Epoch [40/120    avg_loss:0.038, val_acc:0.963]
Epoch [41/120    avg_loss:0.035, val_acc:0.982]
Epoch [42/120    avg_loss:0.022, val_acc:0.981]
Epoch [43/120    avg_loss:0.016, val_acc:0.971]
Epoch [44/120    avg_loss:0.018, val_acc:0.982]
Epoch [45/120    avg_loss:0.024, val_acc:0.979]
Epoch [46/120    avg_loss:0.016, val_acc:0.982]
Epoch [47/120    avg_loss:0.018, val_acc:0.948]
Epoch [48/120    avg_loss:0.036, val_acc:0.977]
Epoch [49/120    avg_loss:0.014, val_acc:0.983]
Epoch [50/120    avg_loss:0.009, val_acc:0.984]
Epoch [51/120    avg_loss:0.016, val_acc:0.983]
Epoch [52/120    avg_loss:0.012, val_acc:0.980]
Epoch [53/120    avg_loss:0.010, val_acc:0.984]
Epoch [54/120    avg_loss:0.012, val_acc:0.982]
Epoch [55/120    avg_loss:0.009, val_acc:0.975]
Epoch [56/120    avg_loss:0.010, val_acc:0.973]
Epoch [57/120    avg_loss:0.008, val_acc:0.983]
Epoch [58/120    avg_loss:0.009, val_acc:0.982]
Epoch [59/120    avg_loss:0.007, val_acc:0.983]
Epoch [60/120    avg_loss:0.010, val_acc:0.979]
Epoch [61/120    avg_loss:0.017, val_acc:0.975]
Epoch [62/120    avg_loss:0.016, val_acc:0.982]
Epoch [63/120    avg_loss:0.019, val_acc:0.963]
Epoch [64/120    avg_loss:0.022, val_acc:0.977]
Epoch [65/120    avg_loss:0.010, val_acc:0.980]
Epoch [66/120    avg_loss:0.009, val_acc:0.982]
Epoch [67/120    avg_loss:0.008, val_acc:0.980]
Epoch [68/120    avg_loss:0.008, val_acc:0.981]
Epoch [69/120    avg_loss:0.007, val_acc:0.982]
Epoch [70/120    avg_loss:0.008, val_acc:0.982]
Epoch [71/120    avg_loss:0.006, val_acc:0.983]
Epoch [72/120    avg_loss:0.006, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.982]
Epoch [74/120    avg_loss:0.008, val_acc:0.984]
Epoch [75/120    avg_loss:0.006, val_acc:0.984]
Epoch [76/120    avg_loss:0.005, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.005, val_acc:0.982]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.982]
Epoch [85/120    avg_loss:0.006, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.983]
Epoch [89/120    avg_loss:0.007, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.984]
Epoch [91/120    avg_loss:0.005, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.004, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.003, val_acc:0.984]
Epoch [119/120    avg_loss:0.004, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     0     0     1    13     9     0]
 [    0     0 18062     0    25     0     3     0     0     0]
 [    0     0     0  2027     3     0     0     0     4     2]
 [    0    45    16     0  2885     0     0     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     1     0     0  4864     0     0     1]
 [    0    17     0     0     0     0     4  1267     0     2]
 [    0     2     0    13    72     0     0     0  3479     5]
 [    0     0     0     0    14    37     0     0     0   868]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99325843 0.99845218 0.9943586  0.9663373  0.98602191
 0.99774359 0.98599222 0.98152067 0.96605454]

Kappa:
0.989556329664331
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f565575f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.747, val_acc:0.598]
Epoch [2/120    avg_loss:1.263, val_acc:0.558]
Epoch [3/120    avg_loss:0.999, val_acc:0.713]
Epoch [4/120    avg_loss:0.784, val_acc:0.721]
Epoch [5/120    avg_loss:0.584, val_acc:0.748]
Epoch [6/120    avg_loss:0.480, val_acc:0.768]
Epoch [7/120    avg_loss:0.404, val_acc:0.782]
Epoch [8/120    avg_loss:0.349, val_acc:0.826]
Epoch [9/120    avg_loss:0.302, val_acc:0.859]
Epoch [10/120    avg_loss:0.264, val_acc:0.874]
Epoch [11/120    avg_loss:0.221, val_acc:0.865]
Epoch [12/120    avg_loss:0.206, val_acc:0.851]
Epoch [13/120    avg_loss:0.181, val_acc:0.909]
Epoch [14/120    avg_loss:0.159, val_acc:0.942]
Epoch [15/120    avg_loss:0.207, val_acc:0.890]
Epoch [16/120    avg_loss:0.145, val_acc:0.909]
Epoch [17/120    avg_loss:0.133, val_acc:0.942]
Epoch [18/120    avg_loss:0.109, val_acc:0.941]
Epoch [19/120    avg_loss:0.088, val_acc:0.958]
Epoch [20/120    avg_loss:0.074, val_acc:0.960]
Epoch [21/120    avg_loss:0.072, val_acc:0.959]
Epoch [22/120    avg_loss:0.064, val_acc:0.953]
Epoch [23/120    avg_loss:0.180, val_acc:0.891]
Epoch [24/120    avg_loss:0.131, val_acc:0.949]
Epoch [25/120    avg_loss:0.098, val_acc:0.947]
Epoch [26/120    avg_loss:0.066, val_acc:0.953]
Epoch [27/120    avg_loss:0.100, val_acc:0.907]
Epoch [28/120    avg_loss:0.089, val_acc:0.949]
Epoch [29/120    avg_loss:0.059, val_acc:0.970]
Epoch [30/120    avg_loss:0.054, val_acc:0.980]
Epoch [31/120    avg_loss:0.035, val_acc:0.973]
Epoch [32/120    avg_loss:0.026, val_acc:0.983]
Epoch [33/120    avg_loss:0.028, val_acc:0.980]
Epoch [34/120    avg_loss:0.027, val_acc:0.984]
Epoch [35/120    avg_loss:0.060, val_acc:0.927]
Epoch [36/120    avg_loss:0.060, val_acc:0.969]
Epoch [37/120    avg_loss:0.051, val_acc:0.972]
Epoch [38/120    avg_loss:0.030, val_acc:0.976]
Epoch [39/120    avg_loss:0.041, val_acc:0.981]
Epoch [40/120    avg_loss:0.018, val_acc:0.984]
Epoch [41/120    avg_loss:0.016, val_acc:0.983]
Epoch [42/120    avg_loss:0.025, val_acc:0.960]
Epoch [43/120    avg_loss:0.031, val_acc:0.983]
Epoch [44/120    avg_loss:0.023, val_acc:0.987]
Epoch [45/120    avg_loss:0.014, val_acc:0.984]
Epoch [46/120    avg_loss:0.011, val_acc:0.990]
Epoch [47/120    avg_loss:0.018, val_acc:0.988]
Epoch [48/120    avg_loss:0.011, val_acc:0.986]
Epoch [49/120    avg_loss:0.015, val_acc:0.986]
Epoch [50/120    avg_loss:0.011, val_acc:0.988]
Epoch [51/120    avg_loss:0.011, val_acc:0.988]
Epoch [52/120    avg_loss:0.014, val_acc:0.972]
Epoch [53/120    avg_loss:0.015, val_acc:0.984]
Epoch [54/120    avg_loss:0.021, val_acc:0.986]
Epoch [55/120    avg_loss:0.017, val_acc:0.986]
Epoch [56/120    avg_loss:0.010, val_acc:0.988]
Epoch [57/120    avg_loss:0.009, val_acc:0.991]
Epoch [58/120    avg_loss:0.010, val_acc:0.986]
Epoch [59/120    avg_loss:0.006, val_acc:0.988]
Epoch [60/120    avg_loss:0.008, val_acc:0.988]
Epoch [61/120    avg_loss:0.005, val_acc:0.986]
Epoch [62/120    avg_loss:0.005, val_acc:0.988]
Epoch [63/120    avg_loss:0.004, val_acc:0.990]
Epoch [64/120    avg_loss:0.004, val_acc:0.989]
Epoch [65/120    avg_loss:0.008, val_acc:0.988]
Epoch [66/120    avg_loss:0.006, val_acc:0.984]
Epoch [67/120    avg_loss:0.005, val_acc:0.986]
Epoch [68/120    avg_loss:0.005, val_acc:0.990]
Epoch [69/120    avg_loss:0.005, val_acc:0.990]
Epoch [70/120    avg_loss:0.005, val_acc:0.989]
Epoch [71/120    avg_loss:0.007, val_acc:0.990]
Epoch [72/120    avg_loss:0.005, val_acc:0.989]
Epoch [73/120    avg_loss:0.005, val_acc:0.990]
Epoch [74/120    avg_loss:0.004, val_acc:0.990]
Epoch [75/120    avg_loss:0.003, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.990]
Epoch [77/120    avg_loss:0.005, val_acc:0.990]
Epoch [78/120    avg_loss:0.004, val_acc:0.991]
Epoch [79/120    avg_loss:0.004, val_acc:0.991]
Epoch [80/120    avg_loss:0.004, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.004, val_acc:0.992]
Epoch [83/120    avg_loss:0.004, val_acc:0.992]
Epoch [84/120    avg_loss:0.004, val_acc:0.990]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.005, val_acc:0.991]
Epoch [88/120    avg_loss:0.004, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.004, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.004, val_acc:0.991]
Epoch [95/120    avg_loss:0.003, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.003, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.991]
Epoch [104/120    avg_loss:0.003, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.003, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.003, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     0     2     0     0     0     9     0]
 [    0     1 18073     0    13     0     1     0     2     0]
 [    0     0     0  2006     3     0     0     0    18     9]
 [    0    20    21     0  2906     0     1     0    24     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7    10     0     0  4859     0     0     2]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     6     0     0    63     0     0     0  3499     3]
 [    0     0     0     0    19    36     0     0     0   864]]

Accuracy:
99.33964765141108

F1 scores:
[       nan 0.99704969 0.9987566  0.99012833 0.97223152 0.98639456
 0.99763885 0.9984472  0.98245121 0.96053363]

Kappa:
0.9912484022502817
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5152480780>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.789, val_acc:0.366]
Epoch [2/120    avg_loss:1.245, val_acc:0.626]
Epoch [3/120    avg_loss:0.980, val_acc:0.594]
Epoch [4/120    avg_loss:0.751, val_acc:0.719]
Epoch [5/120    avg_loss:0.651, val_acc:0.742]
Epoch [6/120    avg_loss:0.473, val_acc:0.786]
Epoch [7/120    avg_loss:0.376, val_acc:0.844]
Epoch [8/120    avg_loss:0.293, val_acc:0.879]
Epoch [9/120    avg_loss:0.257, val_acc:0.917]
Epoch [10/120    avg_loss:0.251, val_acc:0.912]
Epoch [11/120    avg_loss:0.216, val_acc:0.942]
Epoch [12/120    avg_loss:0.201, val_acc:0.938]
Epoch [13/120    avg_loss:0.133, val_acc:0.954]
Epoch [14/120    avg_loss:0.163, val_acc:0.956]
Epoch [15/120    avg_loss:0.122, val_acc:0.964]
Epoch [16/120    avg_loss:0.115, val_acc:0.939]
Epoch [17/120    avg_loss:0.111, val_acc:0.942]
Epoch [18/120    avg_loss:0.081, val_acc:0.971]
Epoch [19/120    avg_loss:0.091, val_acc:0.961]
Epoch [20/120    avg_loss:0.070, val_acc:0.945]
Epoch [21/120    avg_loss:0.066, val_acc:0.970]
Epoch [22/120    avg_loss:0.060, val_acc:0.976]
Epoch [23/120    avg_loss:0.057, val_acc:0.974]
Epoch [24/120    avg_loss:0.049, val_acc:0.972]
Epoch [25/120    avg_loss:0.033, val_acc:0.984]
Epoch [26/120    avg_loss:0.041, val_acc:0.979]
Epoch [27/120    avg_loss:0.035, val_acc:0.980]
Epoch [28/120    avg_loss:0.046, val_acc:0.970]
Epoch [29/120    avg_loss:0.034, val_acc:0.958]
Epoch [30/120    avg_loss:0.036, val_acc:0.979]
Epoch [31/120    avg_loss:0.042, val_acc:0.967]
Epoch [32/120    avg_loss:0.036, val_acc:0.982]
Epoch [33/120    avg_loss:0.022, val_acc:0.980]
Epoch [34/120    avg_loss:0.034, val_acc:0.949]
Epoch [35/120    avg_loss:0.049, val_acc:0.953]
Epoch [36/120    avg_loss:0.032, val_acc:0.983]
Epoch [37/120    avg_loss:0.045, val_acc:0.975]
Epoch [38/120    avg_loss:0.076, val_acc:0.954]
Epoch [39/120    avg_loss:0.084, val_acc:0.980]
Epoch [40/120    avg_loss:0.040, val_acc:0.984]
Epoch [41/120    avg_loss:0.031, val_acc:0.984]
Epoch [42/120    avg_loss:0.027, val_acc:0.985]
Epoch [43/120    avg_loss:0.022, val_acc:0.984]
Epoch [44/120    avg_loss:0.026, val_acc:0.984]
Epoch [45/120    avg_loss:0.024, val_acc:0.984]
Epoch [46/120    avg_loss:0.023, val_acc:0.984]
Epoch [47/120    avg_loss:0.019, val_acc:0.984]
Epoch [48/120    avg_loss:0.020, val_acc:0.984]
Epoch [49/120    avg_loss:0.018, val_acc:0.986]
Epoch [50/120    avg_loss:0.017, val_acc:0.984]
Epoch [51/120    avg_loss:0.013, val_acc:0.985]
Epoch [52/120    avg_loss:0.016, val_acc:0.984]
Epoch [53/120    avg_loss:0.016, val_acc:0.984]
Epoch [54/120    avg_loss:0.021, val_acc:0.985]
Epoch [55/120    avg_loss:0.017, val_acc:0.984]
Epoch [56/120    avg_loss:0.013, val_acc:0.987]
Epoch [57/120    avg_loss:0.015, val_acc:0.988]
Epoch [58/120    avg_loss:0.015, val_acc:0.988]
Epoch [59/120    avg_loss:0.014, val_acc:0.984]
Epoch [60/120    avg_loss:0.017, val_acc:0.987]
Epoch [61/120    avg_loss:0.017, val_acc:0.985]
Epoch [62/120    avg_loss:0.013, val_acc:0.988]
Epoch [63/120    avg_loss:0.025, val_acc:0.985]
Epoch [64/120    avg_loss:0.018, val_acc:0.987]
Epoch [65/120    avg_loss:0.012, val_acc:0.986]
Epoch [66/120    avg_loss:0.013, val_acc:0.987]
Epoch [67/120    avg_loss:0.014, val_acc:0.986]
Epoch [68/120    avg_loss:0.013, val_acc:0.986]
Epoch [69/120    avg_loss:0.012, val_acc:0.988]
Epoch [70/120    avg_loss:0.012, val_acc:0.988]
Epoch [71/120    avg_loss:0.015, val_acc:0.988]
Epoch [72/120    avg_loss:0.013, val_acc:0.988]
Epoch [73/120    avg_loss:0.014, val_acc:0.987]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.987]
Epoch [76/120    avg_loss:0.012, val_acc:0.987]
Epoch [77/120    avg_loss:0.014, val_acc:0.985]
Epoch [78/120    avg_loss:0.013, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.987]
Epoch [80/120    avg_loss:0.010, val_acc:0.989]
Epoch [81/120    avg_loss:0.014, val_acc:0.988]
Epoch [82/120    avg_loss:0.017, val_acc:0.987]
Epoch [83/120    avg_loss:0.016, val_acc:0.989]
Epoch [84/120    avg_loss:0.016, val_acc:0.987]
Epoch [85/120    avg_loss:0.011, val_acc:0.988]
Epoch [86/120    avg_loss:0.012, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.987]
Epoch [88/120    avg_loss:0.012, val_acc:0.989]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.011, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.989]
Epoch [92/120    avg_loss:0.011, val_acc:0.985]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.014, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.012, val_acc:0.989]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.989]
Epoch [102/120    avg_loss:0.010, val_acc:0.992]
Epoch [103/120    avg_loss:0.010, val_acc:0.991]
Epoch [104/120    avg_loss:0.014, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.013, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.990]
Epoch [108/120    avg_loss:0.011, val_acc:0.987]
Epoch [109/120    avg_loss:0.013, val_acc:0.989]
Epoch [110/120    avg_loss:0.012, val_acc:0.991]
Epoch [111/120    avg_loss:0.008, val_acc:0.992]
Epoch [112/120    avg_loss:0.010, val_acc:0.991]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.010, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.991]
Epoch [117/120    avg_loss:0.009, val_acc:0.989]
Epoch [118/120    avg_loss:0.014, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.991]
Epoch [120/120    avg_loss:0.011, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6312     0    41     0     0     5    40    34     0]
 [    0    12 17966     0    29     0    74     0     9     0]
 [    0     5     0  2000     1     0     0     2    26     2]
 [    0    33    16     0  2895     0     0     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4854     0     1    22]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     0     0     0    55     0     0     0  3487    29]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
98.76846697033234

F1 scores:
[       nan 0.98671252 0.99609126 0.98111356 0.9704995  0.98901099
 0.9891991  0.98281787 0.9745668  0.94805195]

Kappa:
0.9837050214931224
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7de1f7908>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.743, val_acc:0.594]
Epoch [2/120    avg_loss:1.218, val_acc:0.615]
Epoch [3/120    avg_loss:0.976, val_acc:0.734]
Epoch [4/120    avg_loss:0.757, val_acc:0.668]
Epoch [5/120    avg_loss:0.607, val_acc:0.734]
Epoch [6/120    avg_loss:0.448, val_acc:0.744]
Epoch [7/120    avg_loss:0.416, val_acc:0.718]
Epoch [8/120    avg_loss:0.393, val_acc:0.831]
Epoch [9/120    avg_loss:0.296, val_acc:0.819]
Epoch [10/120    avg_loss:0.275, val_acc:0.836]
Epoch [11/120    avg_loss:0.225, val_acc:0.853]
Epoch [12/120    avg_loss:0.184, val_acc:0.920]
Epoch [13/120    avg_loss:0.196, val_acc:0.808]
Epoch [14/120    avg_loss:0.152, val_acc:0.959]
Epoch [15/120    avg_loss:0.136, val_acc:0.949]
Epoch [16/120    avg_loss:0.133, val_acc:0.959]
Epoch [17/120    avg_loss:0.098, val_acc:0.951]
Epoch [18/120    avg_loss:0.088, val_acc:0.964]
Epoch [19/120    avg_loss:0.099, val_acc:0.969]
Epoch [20/120    avg_loss:0.089, val_acc:0.953]
Epoch [21/120    avg_loss:0.078, val_acc:0.950]
Epoch [22/120    avg_loss:0.062, val_acc:0.964]
Epoch [23/120    avg_loss:0.080, val_acc:0.889]
Epoch [24/120    avg_loss:0.081, val_acc:0.973]
Epoch [25/120    avg_loss:0.062, val_acc:0.966]
Epoch [26/120    avg_loss:0.053, val_acc:0.967]
Epoch [27/120    avg_loss:0.054, val_acc:0.966]
Epoch [28/120    avg_loss:0.060, val_acc:0.980]
Epoch [29/120    avg_loss:0.046, val_acc:0.981]
Epoch [30/120    avg_loss:0.039, val_acc:0.971]
Epoch [31/120    avg_loss:0.044, val_acc:0.971]
Epoch [32/120    avg_loss:0.044, val_acc:0.979]
Epoch [33/120    avg_loss:0.033, val_acc:0.976]
Epoch [34/120    avg_loss:0.023, val_acc:0.979]
Epoch [35/120    avg_loss:0.025, val_acc:0.977]
Epoch [36/120    avg_loss:0.025, val_acc:0.980]
Epoch [37/120    avg_loss:0.017, val_acc:0.990]
Epoch [38/120    avg_loss:0.027, val_acc:0.981]
Epoch [39/120    avg_loss:0.023, val_acc:0.970]
Epoch [40/120    avg_loss:0.019, val_acc:0.987]
Epoch [41/120    avg_loss:0.022, val_acc:0.984]
Epoch [42/120    avg_loss:0.021, val_acc:0.987]
Epoch [43/120    avg_loss:0.027, val_acc:0.976]
Epoch [44/120    avg_loss:0.017, val_acc:0.981]
Epoch [45/120    avg_loss:0.020, val_acc:0.981]
Epoch [46/120    avg_loss:0.024, val_acc:0.983]
Epoch [47/120    avg_loss:0.018, val_acc:0.967]
Epoch [48/120    avg_loss:0.021, val_acc:0.976]
Epoch [49/120    avg_loss:0.013, val_acc:0.986]
Epoch [50/120    avg_loss:0.019, val_acc:0.967]
Epoch [51/120    avg_loss:0.015, val_acc:0.989]
Epoch [52/120    avg_loss:0.010, val_acc:0.988]
Epoch [53/120    avg_loss:0.008, val_acc:0.987]
Epoch [54/120    avg_loss:0.009, val_acc:0.986]
Epoch [55/120    avg_loss:0.009, val_acc:0.988]
Epoch [56/120    avg_loss:0.009, val_acc:0.990]
Epoch [57/120    avg_loss:0.008, val_acc:0.990]
Epoch [58/120    avg_loss:0.007, val_acc:0.990]
Epoch [59/120    avg_loss:0.010, val_acc:0.989]
Epoch [60/120    avg_loss:0.007, val_acc:0.990]
Epoch [61/120    avg_loss:0.008, val_acc:0.990]
Epoch [62/120    avg_loss:0.009, val_acc:0.990]
Epoch [63/120    avg_loss:0.007, val_acc:0.989]
Epoch [64/120    avg_loss:0.009, val_acc:0.990]
Epoch [65/120    avg_loss:0.007, val_acc:0.990]
Epoch [66/120    avg_loss:0.008, val_acc:0.990]
Epoch [67/120    avg_loss:0.008, val_acc:0.990]
Epoch [68/120    avg_loss:0.006, val_acc:0.990]
Epoch [69/120    avg_loss:0.007, val_acc:0.990]
Epoch [70/120    avg_loss:0.008, val_acc:0.990]
Epoch [71/120    avg_loss:0.008, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.007, val_acc:0.991]
Epoch [74/120    avg_loss:0.007, val_acc:0.991]
Epoch [75/120    avg_loss:0.007, val_acc:0.991]
Epoch [76/120    avg_loss:0.007, val_acc:0.990]
Epoch [77/120    avg_loss:0.008, val_acc:0.990]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.006, val_acc:0.991]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.008, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.008, val_acc:0.991]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.991]
Epoch [92/120    avg_loss:0.007, val_acc:0.991]
Epoch [93/120    avg_loss:0.007, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.009, val_acc:0.990]
Epoch [97/120    avg_loss:0.007, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.991]
Epoch [107/120    avg_loss:0.008, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.991]
Epoch [115/120    avg_loss:0.007, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.006, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     2     0     1     6    33     6]
 [    0     0 17989     0    90     0     9     0     2     0]
 [    0     0     0  2032     2     0     0     0     1     1]
 [    0    40    16     0  2886     0     5     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     7     0     0  4868     0     3     0]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0     3     0     4    50     0     0     0  3508     6]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.12756368544092

F1 scores:
[       nan 0.99292324 0.99675855 0.99632263 0.95944149 0.98901099
 0.99723446 0.99495929 0.98222036 0.96635411]

Kappa:
0.9884511603625341
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5cb6917f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.913, val_acc:0.299]
Epoch [2/120    avg_loss:1.330, val_acc:0.435]
Epoch [3/120    avg_loss:1.054, val_acc:0.454]
Epoch [4/120    avg_loss:0.901, val_acc:0.645]
Epoch [5/120    avg_loss:0.748, val_acc:0.613]
Epoch [6/120    avg_loss:0.622, val_acc:0.727]
Epoch [7/120    avg_loss:0.540, val_acc:0.720]
Epoch [8/120    avg_loss:0.516, val_acc:0.804]
Epoch [9/120    avg_loss:0.452, val_acc:0.804]
Epoch [10/120    avg_loss:0.345, val_acc:0.844]
Epoch [11/120    avg_loss:0.285, val_acc:0.849]
Epoch [12/120    avg_loss:0.265, val_acc:0.884]
Epoch [13/120    avg_loss:0.248, val_acc:0.897]
Epoch [14/120    avg_loss:0.190, val_acc:0.887]
Epoch [15/120    avg_loss:0.163, val_acc:0.916]
Epoch [16/120    avg_loss:0.137, val_acc:0.964]
Epoch [17/120    avg_loss:0.124, val_acc:0.958]
Epoch [18/120    avg_loss:0.127, val_acc:0.841]
Epoch [19/120    avg_loss:0.142, val_acc:0.948]
Epoch [20/120    avg_loss:0.143, val_acc:0.947]
Epoch [21/120    avg_loss:0.106, val_acc:0.961]
Epoch [22/120    avg_loss:0.099, val_acc:0.957]
Epoch [23/120    avg_loss:0.098, val_acc:0.944]
Epoch [24/120    avg_loss:0.087, val_acc:0.964]
Epoch [25/120    avg_loss:0.062, val_acc:0.971]
Epoch [26/120    avg_loss:0.037, val_acc:0.969]
Epoch [27/120    avg_loss:0.074, val_acc:0.950]
Epoch [28/120    avg_loss:0.066, val_acc:0.974]
Epoch [29/120    avg_loss:0.042, val_acc:0.983]
Epoch [30/120    avg_loss:0.037, val_acc:0.977]
Epoch [31/120    avg_loss:0.037, val_acc:0.976]
Epoch [32/120    avg_loss:0.033, val_acc:0.978]
Epoch [33/120    avg_loss:0.024, val_acc:0.984]
Epoch [34/120    avg_loss:0.031, val_acc:0.982]
Epoch [35/120    avg_loss:0.019, val_acc:0.984]
Epoch [36/120    avg_loss:0.035, val_acc:0.982]
Epoch [37/120    avg_loss:0.023, val_acc:0.977]
Epoch [38/120    avg_loss:0.030, val_acc:0.990]
Epoch [39/120    avg_loss:0.023, val_acc:0.984]
Epoch [40/120    avg_loss:0.022, val_acc:0.983]
Epoch [41/120    avg_loss:0.019, val_acc:0.986]
Epoch [42/120    avg_loss:0.017, val_acc:0.990]
Epoch [43/120    avg_loss:0.021, val_acc:0.982]
Epoch [44/120    avg_loss:0.029, val_acc:0.985]
Epoch [45/120    avg_loss:0.020, val_acc:0.989]
Epoch [46/120    avg_loss:0.023, val_acc:0.972]
Epoch [47/120    avg_loss:0.027, val_acc:0.968]
Epoch [48/120    avg_loss:0.057, val_acc:0.983]
Epoch [49/120    avg_loss:0.037, val_acc:0.981]
Epoch [50/120    avg_loss:0.040, val_acc:0.987]
Epoch [51/120    avg_loss:0.025, val_acc:0.984]
Epoch [52/120    avg_loss:0.021, val_acc:0.987]
Epoch [53/120    avg_loss:0.033, val_acc:0.984]
Epoch [54/120    avg_loss:0.013, val_acc:0.985]
Epoch [55/120    avg_loss:0.017, val_acc:0.984]
Epoch [56/120    avg_loss:0.020, val_acc:0.987]
Epoch [57/120    avg_loss:0.011, val_acc:0.991]
Epoch [58/120    avg_loss:0.011, val_acc:0.991]
Epoch [59/120    avg_loss:0.009, val_acc:0.991]
Epoch [60/120    avg_loss:0.011, val_acc:0.992]
Epoch [61/120    avg_loss:0.010, val_acc:0.990]
Epoch [62/120    avg_loss:0.008, val_acc:0.990]
Epoch [63/120    avg_loss:0.009, val_acc:0.990]
Epoch [64/120    avg_loss:0.008, val_acc:0.990]
Epoch [65/120    avg_loss:0.009, val_acc:0.990]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.989]
Epoch [68/120    avg_loss:0.009, val_acc:0.989]
Epoch [69/120    avg_loss:0.008, val_acc:0.990]
Epoch [70/120    avg_loss:0.011, val_acc:0.990]
Epoch [71/120    avg_loss:0.010, val_acc:0.990]
Epoch [72/120    avg_loss:0.008, val_acc:0.990]
Epoch [73/120    avg_loss:0.008, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.009, val_acc:0.990]
Epoch [76/120    avg_loss:0.008, val_acc:0.990]
Epoch [77/120    avg_loss:0.006, val_acc:0.990]
Epoch [78/120    avg_loss:0.007, val_acc:0.990]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.007, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.012, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.990]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.009, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.007, val_acc:0.990]
Epoch [92/120    avg_loss:0.009, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.008, val_acc:0.990]
Epoch [97/120    avg_loss:0.007, val_acc:0.990]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.008, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.009, val_acc:0.990]
Epoch [106/120    avg_loss:0.009, val_acc:0.990]
Epoch [107/120    avg_loss:0.008, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.009, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.990]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.007, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     0     0     9     7     7     0]
 [    0     3 18040     0    37     0     8     0     2     0]
 [    0     0     0  2023     2     0     0     0     6     5]
 [    0    46    19     0  2871     0    11     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4851     0     0    27]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     1     0     1    59     0     0     0  3503     7]
 [    0     0     0     1    20    44     0     0     0   854]]

Accuracy:
99.15648422625503

F1 scores:
[       nan 0.99433713 0.99809123 0.99630633 0.9632612  0.98342125
 0.99405738 0.99613003 0.98481867 0.94260486]

Kappa:
0.9888263531171311
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb72eff4828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.772, val_acc:0.272]
Epoch [2/120    avg_loss:1.236, val_acc:0.568]
Epoch [3/120    avg_loss:1.055, val_acc:0.458]
Epoch [4/120    avg_loss:0.907, val_acc:0.789]
Epoch [5/120    avg_loss:0.722, val_acc:0.728]
Epoch [6/120    avg_loss:0.538, val_acc:0.724]
Epoch [7/120    avg_loss:0.461, val_acc:0.792]
Epoch [8/120    avg_loss:0.351, val_acc:0.779]
Epoch [9/120    avg_loss:0.312, val_acc:0.866]
Epoch [10/120    avg_loss:0.263, val_acc:0.866]
Epoch [11/120    avg_loss:0.232, val_acc:0.890]
Epoch [12/120    avg_loss:0.202, val_acc:0.883]
Epoch [13/120    avg_loss:0.179, val_acc:0.915]
Epoch [14/120    avg_loss:0.147, val_acc:0.902]
Epoch [15/120    avg_loss:0.152, val_acc:0.911]
Epoch [16/120    avg_loss:0.103, val_acc:0.930]
Epoch [17/120    avg_loss:0.087, val_acc:0.958]
Epoch [18/120    avg_loss:0.136, val_acc:0.924]
Epoch [19/120    avg_loss:0.117, val_acc:0.953]
Epoch [20/120    avg_loss:0.083, val_acc:0.964]
Epoch [21/120    avg_loss:0.080, val_acc:0.945]
Epoch [22/120    avg_loss:0.095, val_acc:0.927]
Epoch [23/120    avg_loss:0.087, val_acc:0.967]
Epoch [24/120    avg_loss:0.082, val_acc:0.971]
Epoch [25/120    avg_loss:0.067, val_acc:0.970]
Epoch [26/120    avg_loss:0.052, val_acc:0.956]
Epoch [27/120    avg_loss:0.055, val_acc:0.952]
Epoch [28/120    avg_loss:0.055, val_acc:0.972]
Epoch [29/120    avg_loss:0.033, val_acc:0.976]
Epoch [30/120    avg_loss:0.049, val_acc:0.928]
Epoch [31/120    avg_loss:0.064, val_acc:0.950]
Epoch [32/120    avg_loss:0.050, val_acc:0.970]
Epoch [33/120    avg_loss:0.036, val_acc:0.973]
Epoch [34/120    avg_loss:0.032, val_acc:0.976]
Epoch [35/120    avg_loss:0.030, val_acc:0.980]
Epoch [36/120    avg_loss:0.027, val_acc:0.984]
Epoch [37/120    avg_loss:0.020, val_acc:0.980]
Epoch [38/120    avg_loss:0.019, val_acc:0.978]
Epoch [39/120    avg_loss:0.040, val_acc:0.973]
Epoch [40/120    avg_loss:0.028, val_acc:0.971]
Epoch [41/120    avg_loss:0.029, val_acc:0.955]
Epoch [42/120    avg_loss:0.036, val_acc:0.983]
Epoch [43/120    avg_loss:0.031, val_acc:0.970]
Epoch [44/120    avg_loss:0.023, val_acc:0.982]
Epoch [45/120    avg_loss:0.013, val_acc:0.981]
Epoch [46/120    avg_loss:0.013, val_acc:0.979]
Epoch [47/120    avg_loss:0.015, val_acc:0.982]
Epoch [48/120    avg_loss:0.012, val_acc:0.957]
Epoch [49/120    avg_loss:0.020, val_acc:0.971]
Epoch [50/120    avg_loss:0.015, val_acc:0.980]
Epoch [51/120    avg_loss:0.010, val_acc:0.983]
Epoch [52/120    avg_loss:0.013, val_acc:0.983]
Epoch [53/120    avg_loss:0.010, val_acc:0.984]
Epoch [54/120    avg_loss:0.010, val_acc:0.984]
Epoch [55/120    avg_loss:0.010, val_acc:0.985]
Epoch [56/120    avg_loss:0.010, val_acc:0.985]
Epoch [57/120    avg_loss:0.011, val_acc:0.984]
Epoch [58/120    avg_loss:0.009, val_acc:0.985]
Epoch [59/120    avg_loss:0.008, val_acc:0.986]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.009, val_acc:0.984]
Epoch [62/120    avg_loss:0.011, val_acc:0.982]
Epoch [63/120    avg_loss:0.010, val_acc:0.984]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.008, val_acc:0.987]
Epoch [66/120    avg_loss:0.009, val_acc:0.985]
Epoch [67/120    avg_loss:0.009, val_acc:0.986]
Epoch [68/120    avg_loss:0.010, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.985]
Epoch [70/120    avg_loss:0.011, val_acc:0.986]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.008, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.987]
Epoch [74/120    avg_loss:0.007, val_acc:0.986]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.015, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.008, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     3     0     0     0     0     0]
 [    0     8 17995     0    77     0    10     0     0     0]
 [    0     2     0  2027     3     0     0     0     2     2]
 [    0    41    18     0  2874     0     9     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4861     0     0    17]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     1     0     0    50     0     0     0  3506    14]
 [    0     0     0     2    17    45     0     0     0   855]]

Accuracy:
99.1492540910515

F1 scores:
[       nan 0.99574073 0.99687007 0.99729397 0.95863909 0.98305085
 0.99610656 0.9992242  0.98635532 0.94631987]

Kappa:
0.9887358427455823
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8856f94860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.898, val_acc:0.281]
Epoch [2/120    avg_loss:1.324, val_acc:0.364]
Epoch [3/120    avg_loss:1.061, val_acc:0.655]
Epoch [4/120    avg_loss:0.901, val_acc:0.661]
Epoch [5/120    avg_loss:0.726, val_acc:0.755]
Epoch [6/120    avg_loss:0.590, val_acc:0.793]
Epoch [7/120    avg_loss:0.520, val_acc:0.794]
Epoch [8/120    avg_loss:0.413, val_acc:0.836]
Epoch [9/120    avg_loss:0.355, val_acc:0.793]
Epoch [10/120    avg_loss:0.427, val_acc:0.822]
Epoch [11/120    avg_loss:0.325, val_acc:0.910]
Epoch [12/120    avg_loss:0.235, val_acc:0.942]
Epoch [13/120    avg_loss:0.207, val_acc:0.912]
Epoch [14/120    avg_loss:0.182, val_acc:0.922]
Epoch [15/120    avg_loss:0.178, val_acc:0.939]
Epoch [16/120    avg_loss:0.166, val_acc:0.931]
Epoch [17/120    avg_loss:0.142, val_acc:0.950]
Epoch [18/120    avg_loss:0.106, val_acc:0.961]
Epoch [19/120    avg_loss:0.121, val_acc:0.957]
Epoch [20/120    avg_loss:0.119, val_acc:0.960]
Epoch [21/120    avg_loss:0.134, val_acc:0.941]
Epoch [22/120    avg_loss:0.118, val_acc:0.954]
Epoch [23/120    avg_loss:0.074, val_acc:0.970]
Epoch [24/120    avg_loss:0.086, val_acc:0.962]
Epoch [25/120    avg_loss:0.088, val_acc:0.970]
Epoch [26/120    avg_loss:0.098, val_acc:0.978]
Epoch [27/120    avg_loss:0.055, val_acc:0.978]
Epoch [28/120    avg_loss:0.057, val_acc:0.971]
Epoch [29/120    avg_loss:0.086, val_acc:0.972]
Epoch [30/120    avg_loss:0.069, val_acc:0.944]
Epoch [31/120    avg_loss:0.048, val_acc:0.977]
Epoch [32/120    avg_loss:0.033, val_acc:0.973]
Epoch [33/120    avg_loss:0.033, val_acc:0.978]
Epoch [34/120    avg_loss:0.034, val_acc:0.977]
Epoch [35/120    avg_loss:0.030, val_acc:0.969]
Epoch [36/120    avg_loss:0.027, val_acc:0.983]
Epoch [37/120    avg_loss:0.058, val_acc:0.972]
Epoch [38/120    avg_loss:0.039, val_acc:0.966]
Epoch [39/120    avg_loss:0.032, val_acc:0.981]
Epoch [40/120    avg_loss:0.032, val_acc:0.962]
Epoch [41/120    avg_loss:0.028, val_acc:0.978]
Epoch [42/120    avg_loss:0.026, val_acc:0.976]
Epoch [43/120    avg_loss:0.032, val_acc:0.980]
Epoch [44/120    avg_loss:0.021, val_acc:0.975]
Epoch [45/120    avg_loss:0.019, val_acc:0.981]
Epoch [46/120    avg_loss:0.020, val_acc:0.984]
Epoch [47/120    avg_loss:0.017, val_acc:0.970]
Epoch [48/120    avg_loss:0.016, val_acc:0.978]
Epoch [49/120    avg_loss:0.013, val_acc:0.979]
Epoch [50/120    avg_loss:0.015, val_acc:0.985]
Epoch [51/120    avg_loss:0.018, val_acc:0.981]
Epoch [52/120    avg_loss:0.022, val_acc:0.985]
Epoch [53/120    avg_loss:0.013, val_acc:0.983]
Epoch [54/120    avg_loss:0.015, val_acc:0.978]
Epoch [55/120    avg_loss:0.017, val_acc:0.972]
Epoch [56/120    avg_loss:0.013, val_acc:0.977]
Epoch [57/120    avg_loss:0.013, val_acc:0.986]
Epoch [58/120    avg_loss:0.015, val_acc:0.980]
Epoch [59/120    avg_loss:0.008, val_acc:0.984]
Epoch [60/120    avg_loss:0.014, val_acc:0.979]
Epoch [61/120    avg_loss:0.013, val_acc:0.977]
Epoch [62/120    avg_loss:0.032, val_acc:0.981]
Epoch [63/120    avg_loss:0.018, val_acc:0.981]
Epoch [64/120    avg_loss:0.012, val_acc:0.963]
Epoch [65/120    avg_loss:0.015, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.980]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.008, val_acc:0.984]
Epoch [69/120    avg_loss:0.007, val_acc:0.984]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.985]
Epoch [72/120    avg_loss:0.012, val_acc:0.981]
Epoch [73/120    avg_loss:0.013, val_acc:0.982]
Epoch [74/120    avg_loss:0.008, val_acc:0.979]
Epoch [75/120    avg_loss:0.008, val_acc:0.983]
Epoch [76/120    avg_loss:0.016, val_acc:0.987]
Epoch [77/120    avg_loss:0.029, val_acc:0.981]
Epoch [78/120    avg_loss:0.019, val_acc:0.978]
Epoch [79/120    avg_loss:0.009, val_acc:0.977]
Epoch [80/120    avg_loss:0.010, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.977]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.025, val_acc:0.977]
Epoch [86/120    avg_loss:0.030, val_acc:0.980]
Epoch [87/120    avg_loss:0.025, val_acc:0.974]
Epoch [88/120    avg_loss:0.020, val_acc:0.976]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.983]
Epoch [93/120    avg_loss:0.012, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.978]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.003, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     0     0     4    10     0     0]
 [    0     0 17990     0    98     0     1     0     1     0]
 [    0     1     0  2019     2     0     0     0    10     4]
 [    0    47    18     0  2877     0     4     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4858     0     1    19]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     8     0     0    51     0     0     0  3512     0]
 [    0     0     0     4    14    51     0     0     0   850]]

Accuracy:
99.0986431446268

F1 scores:
[       nan 0.99457617 0.99673112 0.99482631 0.95676754 0.98083427
 0.99702411 0.996139   0.98637832 0.94866071]

Kappa:
0.9880667923226034
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f494e6f3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.838, val_acc:0.375]
Epoch [2/120    avg_loss:1.305, val_acc:0.464]
Epoch [3/120    avg_loss:1.084, val_acc:0.730]
Epoch [4/120    avg_loss:0.882, val_acc:0.745]
Epoch [5/120    avg_loss:0.743, val_acc:0.701]
Epoch [6/120    avg_loss:0.592, val_acc:0.819]
Epoch [7/120    avg_loss:0.472, val_acc:0.775]
Epoch [8/120    avg_loss:0.396, val_acc:0.857]
Epoch [9/120    avg_loss:0.290, val_acc:0.885]
Epoch [10/120    avg_loss:0.256, val_acc:0.911]
Epoch [11/120    avg_loss:0.217, val_acc:0.930]
Epoch [12/120    avg_loss:0.241, val_acc:0.916]
Epoch [13/120    avg_loss:0.176, val_acc:0.918]
Epoch [14/120    avg_loss:0.154, val_acc:0.955]
Epoch [15/120    avg_loss:0.176, val_acc:0.918]
Epoch [16/120    avg_loss:0.140, val_acc:0.938]
Epoch [17/120    avg_loss:0.139, val_acc:0.938]
Epoch [18/120    avg_loss:0.118, val_acc:0.957]
Epoch [19/120    avg_loss:0.088, val_acc:0.941]
Epoch [20/120    avg_loss:0.078, val_acc:0.973]
Epoch [21/120    avg_loss:0.082, val_acc:0.971]
Epoch [22/120    avg_loss:0.078, val_acc:0.967]
Epoch [23/120    avg_loss:0.047, val_acc:0.974]
Epoch [24/120    avg_loss:0.050, val_acc:0.974]
Epoch [25/120    avg_loss:0.063, val_acc:0.966]
Epoch [26/120    avg_loss:0.062, val_acc:0.975]
Epoch [27/120    avg_loss:0.063, val_acc:0.974]
Epoch [28/120    avg_loss:0.070, val_acc:0.963]
Epoch [29/120    avg_loss:0.053, val_acc:0.970]
Epoch [30/120    avg_loss:0.052, val_acc:0.984]
Epoch [31/120    avg_loss:0.033, val_acc:0.984]
Epoch [32/120    avg_loss:0.043, val_acc:0.968]
Epoch [33/120    avg_loss:0.038, val_acc:0.977]
Epoch [34/120    avg_loss:0.050, val_acc:0.973]
Epoch [35/120    avg_loss:0.041, val_acc:0.976]
Epoch [36/120    avg_loss:0.030, val_acc:0.973]
Epoch [37/120    avg_loss:0.097, val_acc:0.967]
Epoch [38/120    avg_loss:0.076, val_acc:0.976]
Epoch [39/120    avg_loss:0.086, val_acc:0.934]
Epoch [40/120    avg_loss:0.075, val_acc:0.974]
Epoch [41/120    avg_loss:0.065, val_acc:0.970]
Epoch [42/120    avg_loss:0.042, val_acc:0.976]
Epoch [43/120    avg_loss:0.055, val_acc:0.966]
Epoch [44/120    avg_loss:0.036, val_acc:0.968]
Epoch [45/120    avg_loss:0.030, val_acc:0.981]
Epoch [46/120    avg_loss:0.021, val_acc:0.985]
Epoch [47/120    avg_loss:0.019, val_acc:0.987]
Epoch [48/120    avg_loss:0.023, val_acc:0.987]
Epoch [49/120    avg_loss:0.021, val_acc:0.990]
Epoch [50/120    avg_loss:0.019, val_acc:0.990]
Epoch [51/120    avg_loss:0.023, val_acc:0.989]
Epoch [52/120    avg_loss:0.017, val_acc:0.990]
Epoch [53/120    avg_loss:0.016, val_acc:0.990]
Epoch [54/120    avg_loss:0.020, val_acc:0.989]
Epoch [55/120    avg_loss:0.017, val_acc:0.990]
Epoch [56/120    avg_loss:0.016, val_acc:0.988]
Epoch [57/120    avg_loss:0.015, val_acc:0.989]
Epoch [58/120    avg_loss:0.016, val_acc:0.989]
Epoch [59/120    avg_loss:0.018, val_acc:0.990]
Epoch [60/120    avg_loss:0.015, val_acc:0.990]
Epoch [61/120    avg_loss:0.013, val_acc:0.989]
Epoch [62/120    avg_loss:0.012, val_acc:0.989]
Epoch [63/120    avg_loss:0.014, val_acc:0.990]
Epoch [64/120    avg_loss:0.014, val_acc:0.989]
Epoch [65/120    avg_loss:0.016, val_acc:0.988]
Epoch [66/120    avg_loss:0.013, val_acc:0.989]
Epoch [67/120    avg_loss:0.014, val_acc:0.989]
Epoch [68/120    avg_loss:0.014, val_acc:0.988]
Epoch [69/120    avg_loss:0.015, val_acc:0.989]
Epoch [70/120    avg_loss:0.012, val_acc:0.990]
Epoch [71/120    avg_loss:0.011, val_acc:0.988]
Epoch [72/120    avg_loss:0.014, val_acc:0.990]
Epoch [73/120    avg_loss:0.013, val_acc:0.990]
Epoch [74/120    avg_loss:0.012, val_acc:0.989]
Epoch [75/120    avg_loss:0.014, val_acc:0.990]
Epoch [76/120    avg_loss:0.011, val_acc:0.990]
Epoch [77/120    avg_loss:0.013, val_acc:0.990]
Epoch [78/120    avg_loss:0.014, val_acc:0.989]
Epoch [79/120    avg_loss:0.012, val_acc:0.990]
Epoch [80/120    avg_loss:0.011, val_acc:0.990]
Epoch [81/120    avg_loss:0.012, val_acc:0.990]
Epoch [82/120    avg_loss:0.011, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.013, val_acc:0.991]
Epoch [85/120    avg_loss:0.017, val_acc:0.990]
Epoch [86/120    avg_loss:0.013, val_acc:0.990]
Epoch [87/120    avg_loss:0.016, val_acc:0.990]
Epoch [88/120    avg_loss:0.013, val_acc:0.990]
Epoch [89/120    avg_loss:0.012, val_acc:0.990]
Epoch [90/120    avg_loss:0.011, val_acc:0.990]
Epoch [91/120    avg_loss:0.013, val_acc:0.990]
Epoch [92/120    avg_loss:0.011, val_acc:0.990]
Epoch [93/120    avg_loss:0.014, val_acc:0.990]
Epoch [94/120    avg_loss:0.013, val_acc:0.992]
Epoch [95/120    avg_loss:0.015, val_acc:0.990]
Epoch [96/120    avg_loss:0.009, val_acc:0.991]
Epoch [97/120    avg_loss:0.010, val_acc:0.992]
Epoch [98/120    avg_loss:0.012, val_acc:0.993]
Epoch [99/120    avg_loss:0.010, val_acc:0.992]
Epoch [100/120    avg_loss:0.011, val_acc:0.991]
Epoch [101/120    avg_loss:0.009, val_acc:0.990]
Epoch [102/120    avg_loss:0.010, val_acc:0.992]
Epoch [103/120    avg_loss:0.010, val_acc:0.992]
Epoch [104/120    avg_loss:0.010, val_acc:0.991]
Epoch [105/120    avg_loss:0.011, val_acc:0.990]
Epoch [106/120    avg_loss:0.009, val_acc:0.990]
Epoch [107/120    avg_loss:0.010, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.009, val_acc:0.991]
Epoch [110/120    avg_loss:0.009, val_acc:0.990]
Epoch [111/120    avg_loss:0.011, val_acc:0.991]
Epoch [112/120    avg_loss:0.009, val_acc:0.991]
Epoch [113/120    avg_loss:0.012, val_acc:0.991]
Epoch [114/120    avg_loss:0.009, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.991]
Epoch [116/120    avg_loss:0.009, val_acc:0.991]
Epoch [117/120    avg_loss:0.009, val_acc:0.991]
Epoch [118/120    avg_loss:0.012, val_acc:0.991]
Epoch [119/120    avg_loss:0.009, val_acc:0.991]
Epoch [120/120    avg_loss:0.009, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     0     3     0     8     2     0     2]
 [    0     0 18055     0    31     0     4     0     0     0]
 [    0     4     0  2003     2     0     0     0    27     0]
 [    0    47    20     0  2867     0     9     0    28     1]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0    10     0     0  4853     0     0    15]
 [    0     0     0     0     0     0     3  1285     0     2]
 [    0    28     0     0    48     0     0     0  3495     0]
 [    0     0     0     9    14    28     0     0     0   868]]

Accuracy:
99.16612440652641

F1 scores:
[       nan 0.99272896 0.99847919 0.98718581 0.96580765 0.98900265
 0.99497693 0.99728366 0.98160371 0.96017699]

Kappa:
0.9889505178438881
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b2e51b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.829, val_acc:0.310]
Epoch [2/120    avg_loss:1.294, val_acc:0.648]
Epoch [3/120    avg_loss:1.091, val_acc:0.623]
Epoch [4/120    avg_loss:0.908, val_acc:0.605]
Epoch [5/120    avg_loss:0.692, val_acc:0.659]
Epoch [6/120    avg_loss:0.629, val_acc:0.738]
Epoch [7/120    avg_loss:0.480, val_acc:0.812]
Epoch [8/120    avg_loss:0.375, val_acc:0.799]
Epoch [9/120    avg_loss:0.326, val_acc:0.840]
Epoch [10/120    avg_loss:0.290, val_acc:0.909]
Epoch [11/120    avg_loss:0.239, val_acc:0.914]
Epoch [12/120    avg_loss:0.278, val_acc:0.854]
Epoch [13/120    avg_loss:0.316, val_acc:0.918]
Epoch [14/120    avg_loss:0.246, val_acc:0.896]
Epoch [15/120    avg_loss:0.180, val_acc:0.921]
Epoch [16/120    avg_loss:0.167, val_acc:0.877]
Epoch [17/120    avg_loss:0.132, val_acc:0.938]
Epoch [18/120    avg_loss:0.115, val_acc:0.932]
Epoch [19/120    avg_loss:0.113, val_acc:0.956]
Epoch [20/120    avg_loss:0.076, val_acc:0.960]
Epoch [21/120    avg_loss:0.075, val_acc:0.946]
Epoch [22/120    avg_loss:0.087, val_acc:0.954]
Epoch [23/120    avg_loss:0.076, val_acc:0.961]
Epoch [24/120    avg_loss:0.049, val_acc:0.963]
Epoch [25/120    avg_loss:0.063, val_acc:0.945]
Epoch [26/120    avg_loss:0.063, val_acc:0.954]
Epoch [27/120    avg_loss:0.054, val_acc:0.958]
Epoch [28/120    avg_loss:0.044, val_acc:0.963]
Epoch [29/120    avg_loss:0.036, val_acc:0.964]
Epoch [30/120    avg_loss:0.071, val_acc:0.940]
Epoch [31/120    avg_loss:0.065, val_acc:0.958]
Epoch [32/120    avg_loss:0.046, val_acc:0.964]
Epoch [33/120    avg_loss:0.031, val_acc:0.967]
Epoch [34/120    avg_loss:0.034, val_acc:0.886]
Epoch [35/120    avg_loss:0.039, val_acc:0.960]
Epoch [36/120    avg_loss:0.042, val_acc:0.972]
Epoch [37/120    avg_loss:0.031, val_acc:0.967]
Epoch [38/120    avg_loss:0.036, val_acc:0.967]
Epoch [39/120    avg_loss:0.024, val_acc:0.977]
Epoch [40/120    avg_loss:0.024, val_acc:0.973]
Epoch [41/120    avg_loss:0.035, val_acc:0.961]
Epoch [42/120    avg_loss:0.021, val_acc:0.968]
Epoch [43/120    avg_loss:0.017, val_acc:0.976]
Epoch [44/120    avg_loss:0.014, val_acc:0.972]
Epoch [45/120    avg_loss:0.013, val_acc:0.973]
Epoch [46/120    avg_loss:0.012, val_acc:0.979]
Epoch [47/120    avg_loss:0.019, val_acc:0.976]
Epoch [48/120    avg_loss:0.014, val_acc:0.975]
Epoch [49/120    avg_loss:0.011, val_acc:0.974]
Epoch [50/120    avg_loss:0.020, val_acc:0.971]
Epoch [51/120    avg_loss:0.013, val_acc:0.971]
Epoch [52/120    avg_loss:0.011, val_acc:0.979]
Epoch [53/120    avg_loss:0.007, val_acc:0.977]
Epoch [54/120    avg_loss:0.007, val_acc:0.976]
Epoch [55/120    avg_loss:0.007, val_acc:0.979]
Epoch [56/120    avg_loss:0.013, val_acc:0.963]
Epoch [57/120    avg_loss:0.019, val_acc:0.976]
Epoch [58/120    avg_loss:0.012, val_acc:0.976]
Epoch [59/120    avg_loss:0.024, val_acc:0.959]
Epoch [60/120    avg_loss:0.011, val_acc:0.974]
Epoch [61/120    avg_loss:0.010, val_acc:0.976]
Epoch [62/120    avg_loss:0.014, val_acc:0.974]
Epoch [63/120    avg_loss:0.010, val_acc:0.963]
Epoch [64/120    avg_loss:0.025, val_acc:0.969]
Epoch [65/120    avg_loss:0.027, val_acc:0.973]
Epoch [66/120    avg_loss:0.018, val_acc:0.976]
Epoch [67/120    avg_loss:0.009, val_acc:0.978]
Epoch [68/120    avg_loss:0.008, val_acc:0.979]
Epoch [69/120    avg_loss:0.008, val_acc:0.979]
Epoch [70/120    avg_loss:0.010, val_acc:0.979]
Epoch [71/120    avg_loss:0.006, val_acc:0.979]
Epoch [72/120    avg_loss:0.006, val_acc:0.977]
Epoch [73/120    avg_loss:0.008, val_acc:0.979]
Epoch [74/120    avg_loss:0.005, val_acc:0.979]
Epoch [75/120    avg_loss:0.007, val_acc:0.979]
Epoch [76/120    avg_loss:0.006, val_acc:0.978]
Epoch [77/120    avg_loss:0.006, val_acc:0.978]
Epoch [78/120    avg_loss:0.006, val_acc:0.978]
Epoch [79/120    avg_loss:0.005, val_acc:0.978]
Epoch [80/120    avg_loss:0.007, val_acc:0.978]
Epoch [81/120    avg_loss:0.006, val_acc:0.977]
Epoch [82/120    avg_loss:0.007, val_acc:0.976]
Epoch [83/120    avg_loss:0.008, val_acc:0.976]
Epoch [84/120    avg_loss:0.005, val_acc:0.978]
Epoch [85/120    avg_loss:0.006, val_acc:0.978]
Epoch [86/120    avg_loss:0.006, val_acc:0.978]
Epoch [87/120    avg_loss:0.006, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.977]
Epoch [89/120    avg_loss:0.006, val_acc:0.977]
Epoch [90/120    avg_loss:0.006, val_acc:0.977]
Epoch [91/120    avg_loss:0.005, val_acc:0.977]
Epoch [92/120    avg_loss:0.005, val_acc:0.977]
Epoch [93/120    avg_loss:0.005, val_acc:0.976]
Epoch [94/120    avg_loss:0.007, val_acc:0.976]
Epoch [95/120    avg_loss:0.006, val_acc:0.976]
Epoch [96/120    avg_loss:0.005, val_acc:0.976]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.005, val_acc:0.977]
Epoch [99/120    avg_loss:0.006, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.977]
Epoch [101/120    avg_loss:0.007, val_acc:0.977]
Epoch [102/120    avg_loss:0.006, val_acc:0.977]
Epoch [103/120    avg_loss:0.005, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.005, val_acc:0.977]
Epoch [106/120    avg_loss:0.006, val_acc:0.977]
Epoch [107/120    avg_loss:0.005, val_acc:0.977]
Epoch [108/120    avg_loss:0.007, val_acc:0.977]
Epoch [109/120    avg_loss:0.006, val_acc:0.977]
Epoch [110/120    avg_loss:0.005, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.977]
Epoch [112/120    avg_loss:0.006, val_acc:0.977]
Epoch [113/120    avg_loss:0.006, val_acc:0.977]
Epoch [114/120    avg_loss:0.005, val_acc:0.977]
Epoch [115/120    avg_loss:0.006, val_acc:0.977]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.005, val_acc:0.977]
Epoch [118/120    avg_loss:0.006, val_acc:0.977]
Epoch [119/120    avg_loss:0.006, val_acc:0.977]
Epoch [120/120    avg_loss:0.006, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     0     0     0    24     0     9     0]
 [    0     5 18054     0    24     0     7     0     0     0]
 [    0     0     0  2032     2     0     0     0     1     1]
 [    0    29    19     0  2882     0     7     0    35     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0     0    18]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     4     0     0    52     0     0     0  3497    18]
 [    0     0     0     2    15    51     0     0     0   851]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99448287 0.99847911 0.9985258  0.96922818 0.98083427
 0.99386503 0.9984472  0.98327007 0.94189264]

Kappa:
0.9895592020362879
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f39edd20860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.820, val_acc:0.322]
Epoch [2/120    avg_loss:1.263, val_acc:0.411]
Epoch [3/120    avg_loss:1.043, val_acc:0.661]
Epoch [4/120    avg_loss:0.850, val_acc:0.661]
Epoch [5/120    avg_loss:0.698, val_acc:0.700]
Epoch [6/120    avg_loss:0.538, val_acc:0.736]
Epoch [7/120    avg_loss:0.405, val_acc:0.776]
Epoch [8/120    avg_loss:0.359, val_acc:0.819]
Epoch [9/120    avg_loss:0.323, val_acc:0.879]
Epoch [10/120    avg_loss:0.271, val_acc:0.903]
Epoch [11/120    avg_loss:0.215, val_acc:0.871]
Epoch [12/120    avg_loss:0.272, val_acc:0.926]
Epoch [13/120    avg_loss:0.170, val_acc:0.898]
Epoch [14/120    avg_loss:0.138, val_acc:0.943]
Epoch [15/120    avg_loss:0.174, val_acc:0.935]
Epoch [16/120    avg_loss:0.146, val_acc:0.943]
Epoch [17/120    avg_loss:0.143, val_acc:0.917]
Epoch [18/120    avg_loss:0.144, val_acc:0.919]
Epoch [19/120    avg_loss:0.113, val_acc:0.897]
Epoch [20/120    avg_loss:0.111, val_acc:0.933]
Epoch [21/120    avg_loss:0.127, val_acc:0.959]
Epoch [22/120    avg_loss:0.085, val_acc:0.969]
Epoch [23/120    avg_loss:0.063, val_acc:0.967]
Epoch [24/120    avg_loss:0.058, val_acc:0.972]
Epoch [25/120    avg_loss:0.055, val_acc:0.938]
Epoch [26/120    avg_loss:0.056, val_acc:0.966]
Epoch [27/120    avg_loss:0.036, val_acc:0.971]
Epoch [28/120    avg_loss:0.047, val_acc:0.959]
Epoch [29/120    avg_loss:0.028, val_acc:0.965]
Epoch [30/120    avg_loss:0.030, val_acc:0.953]
Epoch [31/120    avg_loss:0.037, val_acc:0.975]
Epoch [32/120    avg_loss:0.038, val_acc:0.974]
Epoch [33/120    avg_loss:0.076, val_acc:0.964]
Epoch [34/120    avg_loss:0.039, val_acc:0.981]
Epoch [35/120    avg_loss:0.042, val_acc:0.968]
Epoch [36/120    avg_loss:0.476, val_acc:0.660]
Epoch [37/120    avg_loss:0.553, val_acc:0.461]
Epoch [38/120    avg_loss:0.957, val_acc:0.584]
Epoch [39/120    avg_loss:0.817, val_acc:0.687]
Epoch [40/120    avg_loss:0.690, val_acc:0.720]
Epoch [41/120    avg_loss:0.623, val_acc:0.728]
Epoch [42/120    avg_loss:0.567, val_acc:0.706]
Epoch [43/120    avg_loss:0.493, val_acc:0.793]
Epoch [44/120    avg_loss:0.455, val_acc:0.848]
Epoch [45/120    avg_loss:0.438, val_acc:0.863]
Epoch [46/120    avg_loss:0.396, val_acc:0.845]
Epoch [47/120    avg_loss:0.398, val_acc:0.845]
Epoch [48/120    avg_loss:0.295, val_acc:0.880]
Epoch [49/120    avg_loss:0.284, val_acc:0.880]
Epoch [50/120    avg_loss:0.272, val_acc:0.893]
Epoch [51/120    avg_loss:0.250, val_acc:0.892]
Epoch [52/120    avg_loss:0.245, val_acc:0.892]
Epoch [53/120    avg_loss:0.242, val_acc:0.894]
Epoch [54/120    avg_loss:0.248, val_acc:0.891]
Epoch [55/120    avg_loss:0.256, val_acc:0.885]
Epoch [56/120    avg_loss:0.215, val_acc:0.887]
Epoch [57/120    avg_loss:0.217, val_acc:0.894]
Epoch [58/120    avg_loss:0.211, val_acc:0.891]
Epoch [59/120    avg_loss:0.213, val_acc:0.893]
Epoch [60/120    avg_loss:0.212, val_acc:0.902]
Epoch [61/120    avg_loss:0.205, val_acc:0.901]
Epoch [62/120    avg_loss:0.209, val_acc:0.902]
Epoch [63/120    avg_loss:0.212, val_acc:0.900]
Epoch [64/120    avg_loss:0.210, val_acc:0.900]
Epoch [65/120    avg_loss:0.206, val_acc:0.899]
Epoch [66/120    avg_loss:0.206, val_acc:0.901]
Epoch [67/120    avg_loss:0.218, val_acc:0.902]
Epoch [68/120    avg_loss:0.209, val_acc:0.899]
Epoch [69/120    avg_loss:0.218, val_acc:0.901]
Epoch [70/120    avg_loss:0.203, val_acc:0.901]
Epoch [71/120    avg_loss:0.212, val_acc:0.900]
Epoch [72/120    avg_loss:0.208, val_acc:0.899]
Epoch [73/120    avg_loss:0.192, val_acc:0.901]
Epoch [74/120    avg_loss:0.201, val_acc:0.899]
Epoch [75/120    avg_loss:0.204, val_acc:0.900]
Epoch [76/120    avg_loss:0.218, val_acc:0.900]
Epoch [77/120    avg_loss:0.209, val_acc:0.899]
Epoch [78/120    avg_loss:0.201, val_acc:0.901]
Epoch [79/120    avg_loss:0.203, val_acc:0.901]
Epoch [80/120    avg_loss:0.205, val_acc:0.901]
Epoch [81/120    avg_loss:0.210, val_acc:0.901]
Epoch [82/120    avg_loss:0.205, val_acc:0.901]
Epoch [83/120    avg_loss:0.210, val_acc:0.901]
Epoch [84/120    avg_loss:0.203, val_acc:0.901]
Epoch [85/120    avg_loss:0.201, val_acc:0.901]
Epoch [86/120    avg_loss:0.203, val_acc:0.900]
Epoch [87/120    avg_loss:0.204, val_acc:0.901]
Epoch [88/120    avg_loss:0.195, val_acc:0.901]
Epoch [89/120    avg_loss:0.206, val_acc:0.901]
Epoch [90/120    avg_loss:0.217, val_acc:0.901]
Epoch [91/120    avg_loss:0.202, val_acc:0.901]
Epoch [92/120    avg_loss:0.192, val_acc:0.901]
Epoch [93/120    avg_loss:0.198, val_acc:0.901]
Epoch [94/120    avg_loss:0.199, val_acc:0.901]
Epoch [95/120    avg_loss:0.208, val_acc:0.901]
Epoch [96/120    avg_loss:0.205, val_acc:0.901]
Epoch [97/120    avg_loss:0.218, val_acc:0.901]
Epoch [98/120    avg_loss:0.203, val_acc:0.901]
Epoch [99/120    avg_loss:0.190, val_acc:0.901]
Epoch [100/120    avg_loss:0.199, val_acc:0.901]
Epoch [101/120    avg_loss:0.206, val_acc:0.901]
Epoch [102/120    avg_loss:0.215, val_acc:0.901]
Epoch [103/120    avg_loss:0.186, val_acc:0.901]
Epoch [104/120    avg_loss:0.209, val_acc:0.901]
Epoch [105/120    avg_loss:0.206, val_acc:0.901]
Epoch [106/120    avg_loss:0.204, val_acc:0.901]
Epoch [107/120    avg_loss:0.211, val_acc:0.901]
Epoch [108/120    avg_loss:0.198, val_acc:0.901]
Epoch [109/120    avg_loss:0.198, val_acc:0.901]
Epoch [110/120    avg_loss:0.200, val_acc:0.901]
Epoch [111/120    avg_loss:0.193, val_acc:0.901]
Epoch [112/120    avg_loss:0.199, val_acc:0.901]
Epoch [113/120    avg_loss:0.204, val_acc:0.901]
Epoch [114/120    avg_loss:0.211, val_acc:0.901]
Epoch [115/120    avg_loss:0.196, val_acc:0.901]
Epoch [116/120    avg_loss:0.212, val_acc:0.901]
Epoch [117/120    avg_loss:0.194, val_acc:0.901]
Epoch [118/120    avg_loss:0.211, val_acc:0.901]
Epoch [119/120    avg_loss:0.206, val_acc:0.901]
Epoch [120/120    avg_loss:0.199, val_acc:0.901]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5374     0    35   303     0     1    40   638    41]
 [    0    14 17471     0   199     0   405     0     1     0]
 [    0    23     0  1958     0     0     0     0    29    26]
 [    0   185    77     0  2646     0    20     0    41     3]
 [    0     4     0     0     0  1301     0     0     0     0]
 [    0     1    86     0    10     0  4776     0     3     2]
 [    0     3     0     0     0     0     6  1250     9    22]
 [    0    44     0     0    89     0     0     0  3429     9]
 [    0     9     0    11    19    97     3     0    15   765]]

Accuracy:
93.9194562938327

F1 scores:
[       nan 0.88907271 0.97810995 0.96930693 0.84834883 0.96263411
 0.94677371 0.96899225 0.88650465 0.85618355]

Kappa:
0.9199592223313641
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d92aa4828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.828, val_acc:0.334]
Epoch [2/120    avg_loss:1.321, val_acc:0.499]
Epoch [3/120    avg_loss:1.032, val_acc:0.497]
Epoch [4/120    avg_loss:0.865, val_acc:0.662]
Epoch [5/120    avg_loss:0.676, val_acc:0.666]
Epoch [6/120    avg_loss:0.543, val_acc:0.767]
Epoch [7/120    avg_loss:0.434, val_acc:0.741]
Epoch [8/120    avg_loss:0.351, val_acc:0.915]
Epoch [9/120    avg_loss:0.298, val_acc:0.872]
Epoch [10/120    avg_loss:0.236, val_acc:0.927]
Epoch [11/120    avg_loss:0.211, val_acc:0.859]
Epoch [12/120    avg_loss:0.225, val_acc:0.931]
Epoch [13/120    avg_loss:0.186, val_acc:0.953]
Epoch [14/120    avg_loss:0.179, val_acc:0.915]
Epoch [15/120    avg_loss:0.120, val_acc:0.947]
Epoch [16/120    avg_loss:0.106, val_acc:0.922]
Epoch [17/120    avg_loss:0.111, val_acc:0.965]
Epoch [18/120    avg_loss:0.099, val_acc:0.957]
Epoch [19/120    avg_loss:0.093, val_acc:0.964]
Epoch [20/120    avg_loss:0.082, val_acc:0.963]
Epoch [21/120    avg_loss:0.057, val_acc:0.965]
Epoch [22/120    avg_loss:0.065, val_acc:0.970]
Epoch [23/120    avg_loss:0.068, val_acc:0.962]
Epoch [24/120    avg_loss:0.055, val_acc:0.971]
Epoch [25/120    avg_loss:0.047, val_acc:0.981]
Epoch [26/120    avg_loss:0.046, val_acc:0.977]
Epoch [27/120    avg_loss:0.071, val_acc:0.957]
Epoch [28/120    avg_loss:0.094, val_acc:0.967]
Epoch [29/120    avg_loss:0.040, val_acc:0.969]
Epoch [30/120    avg_loss:0.042, val_acc:0.901]
Epoch [31/120    avg_loss:0.054, val_acc:0.960]
Epoch [32/120    avg_loss:0.049, val_acc:0.967]
Epoch [33/120    avg_loss:0.057, val_acc:0.984]
Epoch [34/120    avg_loss:0.037, val_acc:0.971]
Epoch [35/120    avg_loss:0.028, val_acc:0.981]
Epoch [36/120    avg_loss:0.045, val_acc:0.972]
Epoch [37/120    avg_loss:0.042, val_acc:0.977]
Epoch [38/120    avg_loss:0.024, val_acc:0.985]
Epoch [39/120    avg_loss:0.026, val_acc:0.979]
Epoch [40/120    avg_loss:0.017, val_acc:0.972]
Epoch [41/120    avg_loss:0.020, val_acc:0.988]
Epoch [42/120    avg_loss:0.043, val_acc:0.977]
Epoch [43/120    avg_loss:0.022, val_acc:0.990]
Epoch [44/120    avg_loss:0.019, val_acc:0.989]
Epoch [45/120    avg_loss:0.022, val_acc:0.985]
Epoch [46/120    avg_loss:0.017, val_acc:0.987]
Epoch [47/120    avg_loss:0.017, val_acc:0.982]
Epoch [48/120    avg_loss:0.012, val_acc:0.989]
Epoch [49/120    avg_loss:0.014, val_acc:0.988]
Epoch [50/120    avg_loss:0.016, val_acc:0.986]
Epoch [51/120    avg_loss:0.022, val_acc:0.984]
Epoch [52/120    avg_loss:0.026, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.984]
Epoch [54/120    avg_loss:0.013, val_acc:0.977]
Epoch [55/120    avg_loss:0.015, val_acc:0.987]
Epoch [56/120    avg_loss:0.011, val_acc:0.990]
Epoch [57/120    avg_loss:0.019, val_acc:0.975]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.011, val_acc:0.989]
Epoch [60/120    avg_loss:0.009, val_acc:0.991]
Epoch [61/120    avg_loss:0.009, val_acc:0.990]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.016, val_acc:0.987]
Epoch [64/120    avg_loss:0.017, val_acc:0.981]
Epoch [65/120    avg_loss:0.010, val_acc:0.992]
Epoch [66/120    avg_loss:0.009, val_acc:0.989]
Epoch [67/120    avg_loss:0.009, val_acc:0.991]
Epoch [68/120    avg_loss:0.011, val_acc:0.989]
Epoch [69/120    avg_loss:0.009, val_acc:0.987]
Epoch [70/120    avg_loss:0.007, val_acc:0.991]
Epoch [71/120    avg_loss:0.005, val_acc:0.990]
Epoch [72/120    avg_loss:0.018, val_acc:0.988]
Epoch [73/120    avg_loss:0.022, val_acc:0.974]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.990]
Epoch [76/120    avg_loss:0.007, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.008, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.003, val_acc:0.990]
Epoch [83/120    avg_loss:0.004, val_acc:0.990]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.005, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.005, val_acc:0.991]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.005, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.003, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.004, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     0     0     0    16     0     0     0]
 [    0     0 18055     0    31     0     1     0     3     0]
 [    0     0     0  2017     2     0     0     0    12     5]
 [    0    40    18     0  2879     0     7     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4858     0     0    20]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0     7     0     0    54     0     0     0  3483    27]
 [    0     0     0     0    14    54     0     0     0   851]]

Accuracy:
99.17094449666209

F1 scores:
[       nan 0.99511439 0.99853441 0.99531211 0.96740591 0.97972973
 0.99528785 0.99805825 0.9815415  0.93260274]

Kappa:
0.9890162247894203
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca6c278828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.815, val_acc:0.272]
Epoch [2/120    avg_loss:1.289, val_acc:0.346]
Epoch [3/120    avg_loss:1.071, val_acc:0.439]
Epoch [4/120    avg_loss:0.962, val_acc:0.618]
Epoch [5/120    avg_loss:0.781, val_acc:0.591]
Epoch [6/120    avg_loss:0.594, val_acc:0.657]
Epoch [7/120    avg_loss:0.461, val_acc:0.703]
Epoch [8/120    avg_loss:0.423, val_acc:0.800]
Epoch [9/120    avg_loss:0.349, val_acc:0.843]
Epoch [10/120    avg_loss:0.284, val_acc:0.834]
Epoch [11/120    avg_loss:0.261, val_acc:0.885]
Epoch [12/120    avg_loss:0.195, val_acc:0.905]
Epoch [13/120    avg_loss:0.188, val_acc:0.853]
Epoch [14/120    avg_loss:0.172, val_acc:0.931]
Epoch [15/120    avg_loss:0.153, val_acc:0.906]
Epoch [16/120    avg_loss:0.172, val_acc:0.932]
Epoch [17/120    avg_loss:0.128, val_acc:0.961]
Epoch [18/120    avg_loss:0.087, val_acc:0.959]
Epoch [19/120    avg_loss:0.081, val_acc:0.970]
Epoch [20/120    avg_loss:0.093, val_acc:0.950]
Epoch [21/120    avg_loss:0.089, val_acc:0.954]
Epoch [22/120    avg_loss:0.064, val_acc:0.975]
Epoch [23/120    avg_loss:0.060, val_acc:0.964]
Epoch [24/120    avg_loss:0.054, val_acc:0.976]
Epoch [25/120    avg_loss:0.054, val_acc:0.981]
Epoch [26/120    avg_loss:0.054, val_acc:0.982]
Epoch [27/120    avg_loss:0.041, val_acc:0.975]
Epoch [28/120    avg_loss:0.032, val_acc:0.986]
Epoch [29/120    avg_loss:0.023, val_acc:0.983]
Epoch [30/120    avg_loss:0.023, val_acc:0.986]
Epoch [31/120    avg_loss:0.040, val_acc:0.959]
Epoch [32/120    avg_loss:0.039, val_acc:0.977]
Epoch [33/120    avg_loss:0.031, val_acc:0.985]
Epoch [34/120    avg_loss:0.060, val_acc:0.954]
Epoch [35/120    avg_loss:0.058, val_acc:0.970]
Epoch [36/120    avg_loss:0.032, val_acc:0.964]
Epoch [37/120    avg_loss:0.019, val_acc:0.987]
Epoch [38/120    avg_loss:0.015, val_acc:0.988]
Epoch [39/120    avg_loss:0.016, val_acc:0.985]
Epoch [40/120    avg_loss:0.015, val_acc:0.985]
Epoch [41/120    avg_loss:0.012, val_acc:0.989]
Epoch [42/120    avg_loss:0.017, val_acc:0.982]
Epoch [43/120    avg_loss:0.016, val_acc:0.987]
Epoch [44/120    avg_loss:0.020, val_acc:0.987]
Epoch [45/120    avg_loss:0.015, val_acc:0.986]
Epoch [46/120    avg_loss:0.016, val_acc:0.986]
Epoch [47/120    avg_loss:0.011, val_acc:0.985]
Epoch [48/120    avg_loss:0.014, val_acc:0.981]
Epoch [49/120    avg_loss:0.028, val_acc:0.951]
Epoch [50/120    avg_loss:0.012, val_acc:0.986]
Epoch [51/120    avg_loss:0.012, val_acc:0.982]
Epoch [52/120    avg_loss:0.014, val_acc:0.990]
Epoch [53/120    avg_loss:0.019, val_acc:0.973]
Epoch [54/120    avg_loss:0.049, val_acc:0.970]
Epoch [55/120    avg_loss:0.039, val_acc:0.956]
Epoch [56/120    avg_loss:0.033, val_acc:0.985]
Epoch [57/120    avg_loss:0.052, val_acc:0.983]
Epoch [58/120    avg_loss:0.021, val_acc:0.988]
Epoch [59/120    avg_loss:0.018, val_acc:0.986]
Epoch [60/120    avg_loss:0.037, val_acc:0.969]
Epoch [61/120    avg_loss:0.019, val_acc:0.985]
Epoch [62/120    avg_loss:0.012, val_acc:0.989]
Epoch [63/120    avg_loss:0.020, val_acc:0.987]
Epoch [64/120    avg_loss:0.012, val_acc:0.989]
Epoch [65/120    avg_loss:0.011, val_acc:0.990]
Epoch [66/120    avg_loss:0.010, val_acc:0.991]
Epoch [67/120    avg_loss:0.006, val_acc:0.990]
Epoch [68/120    avg_loss:0.006, val_acc:0.991]
Epoch [69/120    avg_loss:0.006, val_acc:0.991]
Epoch [70/120    avg_loss:0.008, val_acc:0.991]
Epoch [71/120    avg_loss:0.006, val_acc:0.992]
Epoch [72/120    avg_loss:0.006, val_acc:0.992]
Epoch [73/120    avg_loss:0.006, val_acc:0.991]
Epoch [74/120    avg_loss:0.006, val_acc:0.992]
Epoch [75/120    avg_loss:0.005, val_acc:0.992]
Epoch [76/120    avg_loss:0.006, val_acc:0.991]
Epoch [77/120    avg_loss:0.006, val_acc:0.992]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.992]
Epoch [80/120    avg_loss:0.004, val_acc:0.992]
Epoch [81/120    avg_loss:0.005, val_acc:0.992]
Epoch [82/120    avg_loss:0.005, val_acc:0.992]
Epoch [83/120    avg_loss:0.005, val_acc:0.992]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.991]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.007, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.007, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.007, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     8     1     0    13     0     0     5]
 [    0     5 18055     0    29     0     1     0     0     0]
 [    0     0     0  2028     1     0     0     0     0     7]
 [    0    29    19     0  2884     0     7     0    33     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     2     0     0  4825     0     5    31]
 [    0     0     0     0     0     0     4  1283     0     3]
 [    0     2     0     8    48     0     0     0  3493    20]
 [    0     0     0     1    14    38     0     0     0   866]]

Accuracy:
99.15889427132288

F1 scores:
[       nan 0.99510604 0.99809282 0.99338722 0.96957472 0.98564955
 0.99198191 0.99727944 0.98366657 0.93571043]

Kappa:
0.9888561263679729
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8cce5667b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.800, val_acc:0.533]
Epoch [2/120    avg_loss:1.291, val_acc:0.555]
Epoch [3/120    avg_loss:1.079, val_acc:0.684]
Epoch [4/120    avg_loss:0.895, val_acc:0.656]
Epoch [5/120    avg_loss:0.799, val_acc:0.733]
Epoch [6/120    avg_loss:0.648, val_acc:0.760]
Epoch [7/120    avg_loss:0.508, val_acc:0.810]
Epoch [8/120    avg_loss:0.413, val_acc:0.819]
Epoch [9/120    avg_loss:0.353, val_acc:0.859]
Epoch [10/120    avg_loss:0.304, val_acc:0.874]
Epoch [11/120    avg_loss:0.287, val_acc:0.866]
Epoch [12/120    avg_loss:0.239, val_acc:0.908]
Epoch [13/120    avg_loss:0.233, val_acc:0.920]
Epoch [14/120    avg_loss:0.152, val_acc:0.941]
Epoch [15/120    avg_loss:0.160, val_acc:0.899]
Epoch [16/120    avg_loss:0.154, val_acc:0.940]
Epoch [17/120    avg_loss:0.119, val_acc:0.868]
Epoch [18/120    avg_loss:0.181, val_acc:0.955]
Epoch [19/120    avg_loss:0.145, val_acc:0.909]
Epoch [20/120    avg_loss:0.112, val_acc:0.943]
Epoch [21/120    avg_loss:0.087, val_acc:0.954]
Epoch [22/120    avg_loss:0.078, val_acc:0.971]
Epoch [23/120    avg_loss:0.066, val_acc:0.975]
Epoch [24/120    avg_loss:0.062, val_acc:0.975]
Epoch [25/120    avg_loss:0.071, val_acc:0.973]
Epoch [26/120    avg_loss:0.093, val_acc:0.977]
Epoch [27/120    avg_loss:0.062, val_acc:0.973]
Epoch [28/120    avg_loss:0.082, val_acc:0.892]
Epoch [29/120    avg_loss:0.065, val_acc:0.977]
Epoch [30/120    avg_loss:0.060, val_acc:0.982]
Epoch [31/120    avg_loss:0.039, val_acc:0.980]
Epoch [32/120    avg_loss:0.045, val_acc:0.982]
Epoch [33/120    avg_loss:0.042, val_acc:0.963]
Epoch [34/120    avg_loss:0.036, val_acc:0.977]
Epoch [35/120    avg_loss:0.031, val_acc:0.983]
Epoch [36/120    avg_loss:0.026, val_acc:0.982]
Epoch [37/120    avg_loss:0.021, val_acc:0.986]
Epoch [38/120    avg_loss:0.040, val_acc:0.973]
Epoch [39/120    avg_loss:0.037, val_acc:0.973]
Epoch [40/120    avg_loss:0.034, val_acc:0.983]
Epoch [41/120    avg_loss:0.025, val_acc:0.986]
Epoch [42/120    avg_loss:0.026, val_acc:0.986]
Epoch [43/120    avg_loss:0.021, val_acc:0.982]
Epoch [44/120    avg_loss:0.013, val_acc:0.992]
Epoch [45/120    avg_loss:0.019, val_acc:0.989]
Epoch [46/120    avg_loss:0.016, val_acc:0.988]
Epoch [47/120    avg_loss:0.021, val_acc:0.985]
Epoch [48/120    avg_loss:0.034, val_acc:0.990]
Epoch [49/120    avg_loss:0.014, val_acc:0.989]
Epoch [50/120    avg_loss:0.025, val_acc:0.988]
Epoch [51/120    avg_loss:0.023, val_acc:0.984]
Epoch [52/120    avg_loss:0.017, val_acc:0.985]
Epoch [53/120    avg_loss:0.015, val_acc:0.986]
Epoch [54/120    avg_loss:0.015, val_acc:0.985]
Epoch [55/120    avg_loss:0.011, val_acc:0.988]
Epoch [56/120    avg_loss:0.007, val_acc:0.991]
Epoch [57/120    avg_loss:0.009, val_acc:0.995]
Epoch [58/120    avg_loss:0.007, val_acc:0.994]
Epoch [59/120    avg_loss:0.018, val_acc:0.988]
Epoch [60/120    avg_loss:0.009, val_acc:0.991]
Epoch [61/120    avg_loss:0.009, val_acc:0.991]
Epoch [62/120    avg_loss:0.011, val_acc:0.990]
Epoch [63/120    avg_loss:0.009, val_acc:0.985]
Epoch [64/120    avg_loss:0.009, val_acc:0.984]
Epoch [65/120    avg_loss:0.008, val_acc:0.986]
Epoch [66/120    avg_loss:0.012, val_acc:0.991]
Epoch [67/120    avg_loss:0.006, val_acc:0.990]
Epoch [68/120    avg_loss:0.011, val_acc:0.980]
Epoch [69/120    avg_loss:0.010, val_acc:0.985]
Epoch [70/120    avg_loss:0.009, val_acc:0.989]
Epoch [71/120    avg_loss:0.009, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.006, val_acc:0.990]
Epoch [74/120    avg_loss:0.006, val_acc:0.990]
Epoch [75/120    avg_loss:0.006, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.007, val_acc:0.991]
Epoch [78/120    avg_loss:0.005, val_acc:0.991]
Epoch [79/120    avg_loss:0.006, val_acc:0.991]
Epoch [80/120    avg_loss:0.005, val_acc:0.991]
Epoch [81/120    avg_loss:0.004, val_acc:0.990]
Epoch [82/120    avg_loss:0.005, val_acc:0.990]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.004, val_acc:0.992]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.004, val_acc:0.992]
Epoch [87/120    avg_loss:0.004, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.992]
Epoch [89/120    avg_loss:0.004, val_acc:0.992]
Epoch [90/120    avg_loss:0.004, val_acc:0.992]
Epoch [91/120    avg_loss:0.004, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.004, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.004, val_acc:0.992]
Epoch [99/120    avg_loss:0.005, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.006, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.004, val_acc:0.992]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.003, val_acc:0.992]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.004, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.004, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.007, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     0     0     1     0]
 [    0     0 18062     0    26     0     2     0     0     0]
 [    0     0     0  2031     2     0     0     0     2     1]
 [    0    32    19     0  2882     0     7     0    29     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4845     0     0    33]
 [    0     0     0     0     0     0     5  1282     0     3]
 [    0     5     0     5    59     0     0     0  3490    12]
 [    0     0     0     5    15    34     0     0     0   865]]

Accuracy:
99.27698647964718

F1 scores:
[       nan 0.99705426 0.99870062 0.99632082 0.9677636  0.9871407
 0.99517305 0.99688958 0.9840688  0.9422658 ]

Kappa:
0.9904205252140538
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd51315e780>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.838, val_acc:0.186]
Epoch [2/120    avg_loss:1.310, val_acc:0.391]
Epoch [3/120    avg_loss:1.028, val_acc:0.700]
Epoch [4/120    avg_loss:0.831, val_acc:0.619]
Epoch [5/120    avg_loss:0.652, val_acc:0.745]
Epoch [6/120    avg_loss:0.533, val_acc:0.794]
Epoch [7/120    avg_loss:0.443, val_acc:0.803]
Epoch [8/120    avg_loss:0.423, val_acc:0.833]
Epoch [9/120    avg_loss:0.394, val_acc:0.795]
Epoch [10/120    avg_loss:0.303, val_acc:0.872]
Epoch [11/120    avg_loss:0.275, val_acc:0.838]
Epoch [12/120    avg_loss:0.232, val_acc:0.908]
Epoch [13/120    avg_loss:0.179, val_acc:0.951]
Epoch [14/120    avg_loss:0.136, val_acc:0.905]
Epoch [15/120    avg_loss:0.166, val_acc:0.961]
Epoch [16/120    avg_loss:0.128, val_acc:0.952]
Epoch [17/120    avg_loss:0.130, val_acc:0.965]
Epoch [18/120    avg_loss:0.096, val_acc:0.974]
Epoch [19/120    avg_loss:0.108, val_acc:0.907]
Epoch [20/120    avg_loss:0.103, val_acc:0.964]
Epoch [21/120    avg_loss:0.132, val_acc:0.953]
Epoch [22/120    avg_loss:0.129, val_acc:0.964]
Epoch [23/120    avg_loss:0.076, val_acc:0.976]
Epoch [24/120    avg_loss:0.066, val_acc:0.971]
Epoch [25/120    avg_loss:0.078, val_acc:0.967]
Epoch [26/120    avg_loss:0.081, val_acc:0.975]
Epoch [27/120    avg_loss:0.063, val_acc:0.965]
Epoch [28/120    avg_loss:0.065, val_acc:0.969]
Epoch [29/120    avg_loss:0.053, val_acc:0.986]
Epoch [30/120    avg_loss:0.079, val_acc:0.969]
Epoch [31/120    avg_loss:0.073, val_acc:0.968]
Epoch [32/120    avg_loss:0.048, val_acc:0.984]
Epoch [33/120    avg_loss:0.036, val_acc:0.984]
Epoch [34/120    avg_loss:0.032, val_acc:0.978]
Epoch [35/120    avg_loss:0.023, val_acc:0.980]
Epoch [36/120    avg_loss:0.024, val_acc:0.984]
Epoch [37/120    avg_loss:0.026, val_acc:0.988]
Epoch [38/120    avg_loss:0.035, val_acc:0.986]
Epoch [39/120    avg_loss:0.032, val_acc:0.977]
Epoch [40/120    avg_loss:0.025, val_acc:0.980]
Epoch [41/120    avg_loss:0.041, val_acc:0.976]
Epoch [42/120    avg_loss:0.019, val_acc:0.989]
Epoch [43/120    avg_loss:0.071, val_acc:0.982]
Epoch [44/120    avg_loss:0.037, val_acc:0.986]
Epoch [45/120    avg_loss:0.035, val_acc:0.985]
Epoch [46/120    avg_loss:0.044, val_acc:0.985]
Epoch [47/120    avg_loss:0.026, val_acc:0.979]
Epoch [48/120    avg_loss:0.027, val_acc:0.986]
Epoch [49/120    avg_loss:0.019, val_acc:0.985]
Epoch [50/120    avg_loss:0.014, val_acc:0.988]
Epoch [51/120    avg_loss:0.015, val_acc:0.989]
Epoch [52/120    avg_loss:0.020, val_acc:0.990]
Epoch [53/120    avg_loss:0.012, val_acc:0.989]
Epoch [54/120    avg_loss:0.009, val_acc:0.990]
Epoch [55/120    avg_loss:0.013, val_acc:0.989]
Epoch [56/120    avg_loss:0.008, val_acc:0.989]
Epoch [57/120    avg_loss:0.013, val_acc:0.987]
Epoch [58/120    avg_loss:0.010, val_acc:0.990]
Epoch [59/120    avg_loss:0.007, val_acc:0.990]
Epoch [60/120    avg_loss:0.007, val_acc:0.990]
Epoch [61/120    avg_loss:0.008, val_acc:0.988]
Epoch [62/120    avg_loss:0.011, val_acc:0.991]
Epoch [63/120    avg_loss:0.008, val_acc:0.990]
Epoch [64/120    avg_loss:0.009, val_acc:0.992]
Epoch [65/120    avg_loss:0.006, val_acc:0.990]
Epoch [66/120    avg_loss:0.007, val_acc:0.991]
Epoch [67/120    avg_loss:0.007, val_acc:0.991]
Epoch [68/120    avg_loss:0.013, val_acc:0.991]
Epoch [69/120    avg_loss:0.040, val_acc:0.970]
Epoch [70/120    avg_loss:0.112, val_acc:0.972]
Epoch [71/120    avg_loss:0.040, val_acc:0.981]
Epoch [72/120    avg_loss:0.026, val_acc:0.985]
Epoch [73/120    avg_loss:0.029, val_acc:0.988]
Epoch [74/120    avg_loss:0.033, val_acc:0.977]
Epoch [75/120    avg_loss:0.041, val_acc:0.986]
Epoch [76/120    avg_loss:0.055, val_acc:0.977]
Epoch [77/120    avg_loss:0.022, val_acc:0.988]
Epoch [78/120    avg_loss:0.015, val_acc:0.990]
Epoch [79/120    avg_loss:0.015, val_acc:0.989]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.013, val_acc:0.990]
Epoch [82/120    avg_loss:0.011, val_acc:0.990]
Epoch [83/120    avg_loss:0.011, val_acc:0.990]
Epoch [84/120    avg_loss:0.010, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.010, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.990]
Epoch [88/120    avg_loss:0.008, val_acc:0.990]
Epoch [89/120    avg_loss:0.012, val_acc:0.990]
Epoch [90/120    avg_loss:0.010, val_acc:0.990]
Epoch [91/120    avg_loss:0.011, val_acc:0.990]
Epoch [92/120    avg_loss:0.011, val_acc:0.990]
Epoch [93/120    avg_loss:0.009, val_acc:0.990]
Epoch [94/120    avg_loss:0.013, val_acc:0.990]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.010, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.010, val_acc:0.989]
Epoch [99/120    avg_loss:0.009, val_acc:0.989]
Epoch [100/120    avg_loss:0.010, val_acc:0.989]
Epoch [101/120    avg_loss:0.009, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.011, val_acc:0.989]
Epoch [104/120    avg_loss:0.016, val_acc:0.989]
Epoch [105/120    avg_loss:0.009, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.989]
Epoch [108/120    avg_loss:0.009, val_acc:0.989]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.011, val_acc:0.989]
Epoch [111/120    avg_loss:0.010, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.010, val_acc:0.989]
Epoch [115/120    avg_loss:0.010, val_acc:0.989]
Epoch [116/120    avg_loss:0.016, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.011, val_acc:0.989]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.009, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0     0     0    19     0     1     2]
 [    0     0 18055     0    15     0    19     0     1     0]
 [    0     0     0  2019     0     0     0     0    16     1]
 [    0    52    20     0  2872     0     1     0    25     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4850     0     0    28]
 [    0     0     0     0     0     0    14  1274     0     2]
 [    0     1     0     0    48     0     0     0  3494    28]
 [    0     0     0     0    16    34     0     0     0   869]]

Accuracy:
99.16853445159424

F1 scores:
[       nan 0.99418379 0.99847919 0.99580764 0.96977883 0.9871407
 0.99171864 0.99375975 0.98311761 0.93895192]

Kappa:
0.9889837049323842
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a95ffb860>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.738, val_acc:0.554]
Epoch [2/120    avg_loss:1.193, val_acc:0.703]
Epoch [3/120    avg_loss:0.945, val_acc:0.706]
Epoch [4/120    avg_loss:0.762, val_acc:0.713]
Epoch [5/120    avg_loss:0.613, val_acc:0.751]
Epoch [6/120    avg_loss:0.515, val_acc:0.775]
Epoch [7/120    avg_loss:0.375, val_acc:0.777]
Epoch [8/120    avg_loss:0.327, val_acc:0.842]
Epoch [9/120    avg_loss:0.321, val_acc:0.752]
Epoch [10/120    avg_loss:0.309, val_acc:0.883]
Epoch [11/120    avg_loss:0.221, val_acc:0.873]
Epoch [12/120    avg_loss:0.231, val_acc:0.919]
Epoch [13/120    avg_loss:0.182, val_acc:0.931]
Epoch [14/120    avg_loss:0.183, val_acc:0.898]
Epoch [15/120    avg_loss:0.142, val_acc:0.954]
Epoch [16/120    avg_loss:0.160, val_acc:0.938]
Epoch [17/120    avg_loss:0.118, val_acc:0.931]
Epoch [18/120    avg_loss:0.117, val_acc:0.946]
Epoch [19/120    avg_loss:0.105, val_acc:0.953]
Epoch [20/120    avg_loss:0.082, val_acc:0.961]
Epoch [21/120    avg_loss:0.070, val_acc:0.956]
Epoch [22/120    avg_loss:0.084, val_acc:0.959]
Epoch [23/120    avg_loss:0.130, val_acc:0.971]
Epoch [24/120    avg_loss:0.076, val_acc:0.946]
Epoch [25/120    avg_loss:0.064, val_acc:0.982]
Epoch [26/120    avg_loss:0.051, val_acc:0.948]
Epoch [27/120    avg_loss:0.047, val_acc:0.965]
Epoch [28/120    avg_loss:0.068, val_acc:0.957]
Epoch [29/120    avg_loss:0.060, val_acc:0.978]
Epoch [30/120    avg_loss:0.050, val_acc:0.973]
Epoch [31/120    avg_loss:0.042, val_acc:0.971]
Epoch [32/120    avg_loss:0.039, val_acc:0.972]
Epoch [33/120    avg_loss:0.048, val_acc:0.932]
Epoch [34/120    avg_loss:0.036, val_acc:0.974]
Epoch [35/120    avg_loss:0.026, val_acc:0.974]
Epoch [36/120    avg_loss:0.037, val_acc:0.974]
Epoch [37/120    avg_loss:0.047, val_acc:0.978]
Epoch [38/120    avg_loss:0.029, val_acc:0.976]
Epoch [39/120    avg_loss:0.019, val_acc:0.986]
Epoch [40/120    avg_loss:0.016, val_acc:0.983]
Epoch [41/120    avg_loss:0.014, val_acc:0.988]
Epoch [42/120    avg_loss:0.017, val_acc:0.985]
Epoch [43/120    avg_loss:0.013, val_acc:0.983]
Epoch [44/120    avg_loss:0.016, val_acc:0.983]
Epoch [45/120    avg_loss:0.014, val_acc:0.988]
Epoch [46/120    avg_loss:0.016, val_acc:0.985]
Epoch [47/120    avg_loss:0.018, val_acc:0.987]
Epoch [48/120    avg_loss:0.015, val_acc:0.988]
Epoch [49/120    avg_loss:0.013, val_acc:0.988]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.014, val_acc:0.987]
Epoch [52/120    avg_loss:0.017, val_acc:0.987]
Epoch [53/120    avg_loss:0.015, val_acc:0.987]
Epoch [54/120    avg_loss:0.016, val_acc:0.988]
Epoch [55/120    avg_loss:0.013, val_acc:0.986]
Epoch [56/120    avg_loss:0.013, val_acc:0.988]
Epoch [57/120    avg_loss:0.011, val_acc:0.988]
Epoch [58/120    avg_loss:0.012, val_acc:0.987]
Epoch [59/120    avg_loss:0.011, val_acc:0.986]
Epoch [60/120    avg_loss:0.012, val_acc:0.988]
Epoch [61/120    avg_loss:0.013, val_acc:0.988]
Epoch [62/120    avg_loss:0.014, val_acc:0.987]
Epoch [63/120    avg_loss:0.012, val_acc:0.987]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.012, val_acc:0.988]
Epoch [66/120    avg_loss:0.016, val_acc:0.986]
Epoch [67/120    avg_loss:0.011, val_acc:0.989]
Epoch [68/120    avg_loss:0.014, val_acc:0.986]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.013, val_acc:0.988]
Epoch [72/120    avg_loss:0.013, val_acc:0.989]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.013, val_acc:0.989]
Epoch [75/120    avg_loss:0.011, val_acc:0.990]
Epoch [76/120    avg_loss:0.012, val_acc:0.987]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.013, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.989]
Epoch [81/120    avg_loss:0.011, val_acc:0.989]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.021, val_acc:0.987]
Epoch [84/120    avg_loss:0.014, val_acc:0.989]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.014, val_acc:0.985]
Epoch [87/120    avg_loss:0.012, val_acc:0.985]
Epoch [88/120    avg_loss:0.012, val_acc:0.988]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.012, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.011, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     0     0     2     2    30     0]
 [    0     1 18046     0     9     0    26     0     8     0]
 [    0     3     0  1937     0     0     0     0    92     4]
 [    0    31     7     0  2924     0     2     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    32     0     0     0  4846     0     0     0]
 [    0     1     0     0     0     0     0  1288     0     1]
 [    0     7     0    57    36     0     0     0  3471     0]
 [    0     0     0     0     0    16     0     0     0   903]]

Accuracy:
99.09623309955896

F1 scores:
[       nan 0.99401849 0.9977056  0.96129032 0.98434607 0.99390708
 0.99364363 0.99844961 0.96725651 0.98688525]

Kappa:
0.9880245263703114
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5a3698828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.816, val_acc:0.297]
Epoch [2/120    avg_loss:1.221, val_acc:0.488]
Epoch [3/120    avg_loss:0.951, val_acc:0.560]
Epoch [4/120    avg_loss:0.766, val_acc:0.584]
Epoch [5/120    avg_loss:0.598, val_acc:0.648]
Epoch [6/120    avg_loss:0.492, val_acc:0.627]
Epoch [7/120    avg_loss:0.437, val_acc:0.777]
Epoch [8/120    avg_loss:0.406, val_acc:0.802]
Epoch [9/120    avg_loss:0.378, val_acc:0.709]
Epoch [10/120    avg_loss:0.332, val_acc:0.812]
Epoch [11/120    avg_loss:0.299, val_acc:0.856]
Epoch [12/120    avg_loss:0.262, val_acc:0.821]
Epoch [13/120    avg_loss:0.269, val_acc:0.899]
Epoch [14/120    avg_loss:0.208, val_acc:0.908]
Epoch [15/120    avg_loss:0.193, val_acc:0.913]
Epoch [16/120    avg_loss:0.168, val_acc:0.937]
Epoch [17/120    avg_loss:0.193, val_acc:0.939]
Epoch [18/120    avg_loss:0.142, val_acc:0.942]
Epoch [19/120    avg_loss:0.111, val_acc:0.961]
Epoch [20/120    avg_loss:0.100, val_acc:0.927]
Epoch [21/120    avg_loss:0.112, val_acc:0.962]
Epoch [22/120    avg_loss:0.082, val_acc:0.926]
Epoch [23/120    avg_loss:0.111, val_acc:0.938]
Epoch [24/120    avg_loss:0.112, val_acc:0.918]
Epoch [25/120    avg_loss:0.119, val_acc:0.961]
Epoch [26/120    avg_loss:0.094, val_acc:0.948]
Epoch [27/120    avg_loss:0.085, val_acc:0.957]
Epoch [28/120    avg_loss:0.095, val_acc:0.942]
Epoch [29/120    avg_loss:0.097, val_acc:0.899]
Epoch [30/120    avg_loss:0.136, val_acc:0.963]
Epoch [31/120    avg_loss:0.069, val_acc:0.902]
Epoch [32/120    avg_loss:0.065, val_acc:0.978]
Epoch [33/120    avg_loss:0.058, val_acc:0.973]
Epoch [34/120    avg_loss:0.065, val_acc:0.970]
Epoch [35/120    avg_loss:0.040, val_acc:0.965]
Epoch [36/120    avg_loss:0.033, val_acc:0.978]
Epoch [37/120    avg_loss:0.040, val_acc:0.969]
Epoch [38/120    avg_loss:0.042, val_acc:0.982]
Epoch [39/120    avg_loss:0.034, val_acc:0.978]
Epoch [40/120    avg_loss:0.024, val_acc:0.981]
Epoch [41/120    avg_loss:0.022, val_acc:0.983]
Epoch [42/120    avg_loss:0.027, val_acc:0.971]
Epoch [43/120    avg_loss:0.050, val_acc:0.953]
Epoch [44/120    avg_loss:0.053, val_acc:0.973]
Epoch [45/120    avg_loss:0.038, val_acc:0.979]
Epoch [46/120    avg_loss:0.029, val_acc:0.982]
Epoch [47/120    avg_loss:0.021, val_acc:0.983]
Epoch [48/120    avg_loss:0.030, val_acc:0.983]
Epoch [49/120    avg_loss:0.018, val_acc:0.986]
Epoch [50/120    avg_loss:0.018, val_acc:0.981]
Epoch [51/120    avg_loss:0.023, val_acc:0.976]
Epoch [52/120    avg_loss:0.029, val_acc:0.979]
Epoch [53/120    avg_loss:0.038, val_acc:0.945]
Epoch [54/120    avg_loss:0.062, val_acc:0.986]
Epoch [55/120    avg_loss:0.046, val_acc:0.977]
Epoch [56/120    avg_loss:0.022, val_acc:0.985]
Epoch [57/120    avg_loss:0.024, val_acc:0.973]
Epoch [58/120    avg_loss:0.024, val_acc:0.988]
Epoch [59/120    avg_loss:0.023, val_acc:0.979]
Epoch [60/120    avg_loss:0.030, val_acc:0.976]
Epoch [61/120    avg_loss:0.017, val_acc:0.984]
Epoch [62/120    avg_loss:0.010, val_acc:0.988]
Epoch [63/120    avg_loss:0.016, val_acc:0.979]
Epoch [64/120    avg_loss:0.016, val_acc:0.975]
Epoch [65/120    avg_loss:0.028, val_acc:0.983]
Epoch [66/120    avg_loss:0.027, val_acc:0.979]
Epoch [67/120    avg_loss:0.020, val_acc:0.981]
Epoch [68/120    avg_loss:0.023, val_acc:0.972]
Epoch [69/120    avg_loss:0.030, val_acc:0.981]
Epoch [70/120    avg_loss:0.027, val_acc:0.968]
Epoch [71/120    avg_loss:0.020, val_acc:0.986]
Epoch [72/120    avg_loss:0.014, val_acc:0.983]
Epoch [73/120    avg_loss:0.016, val_acc:0.971]
Epoch [74/120    avg_loss:0.013, val_acc:0.985]
Epoch [75/120    avg_loss:0.011, val_acc:0.983]
Epoch [76/120    avg_loss:0.006, val_acc:0.985]
Epoch [77/120    avg_loss:0.014, val_acc:0.983]
Epoch [78/120    avg_loss:0.006, val_acc:0.983]
Epoch [79/120    avg_loss:0.008, val_acc:0.982]
Epoch [80/120    avg_loss:0.008, val_acc:0.982]
Epoch [81/120    avg_loss:0.006, val_acc:0.982]
Epoch [82/120    avg_loss:0.004, val_acc:0.983]
Epoch [83/120    avg_loss:0.006, val_acc:0.983]
Epoch [84/120    avg_loss:0.005, val_acc:0.983]
Epoch [85/120    avg_loss:0.005, val_acc:0.984]
Epoch [86/120    avg_loss:0.004, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.006, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.985]
Epoch [103/120    avg_loss:0.004, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     1     0     0     0     0    29     7]
 [    0     1 18048     0    29     0     4     0     8     0]
 [    0     5     0  1930     0     0     0     0   101     0]
 [    0    33     5     0  2923     0     6     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     0     0  4854     0     0     0]
 [    0     5     0     0     0     0     1  1281     2     1]
 [    0    45     0    39    17     0     0     0  3469     1]
 [    0     1     0     0     5    28     0     0     0   885]]

Accuracy:
99.02875183765937

F1 scores:
[       nan 0.990168   0.99803688 0.96355467 0.98318197 0.9893859
 0.99640768 0.99649942 0.96562283 0.9762824 ]

Kappa:
0.9871296878539342
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6e96ff0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.828, val_acc:0.317]
Epoch [2/120    avg_loss:1.171, val_acc:0.343]
Epoch [3/120    avg_loss:0.900, val_acc:0.681]
Epoch [4/120    avg_loss:0.723, val_acc:0.694]
Epoch [5/120    avg_loss:0.604, val_acc:0.690]
Epoch [6/120    avg_loss:0.481, val_acc:0.724]
Epoch [7/120    avg_loss:0.410, val_acc:0.686]
Epoch [8/120    avg_loss:0.379, val_acc:0.694]
Epoch [9/120    avg_loss:0.317, val_acc:0.802]
Epoch [10/120    avg_loss:0.281, val_acc:0.814]
Epoch [11/120    avg_loss:0.285, val_acc:0.811]
Epoch [12/120    avg_loss:0.259, val_acc:0.762]
Epoch [13/120    avg_loss:0.269, val_acc:0.861]
Epoch [14/120    avg_loss:0.240, val_acc:0.791]
Epoch [15/120    avg_loss:0.229, val_acc:0.904]
Epoch [16/120    avg_loss:0.166, val_acc:0.877]
Epoch [17/120    avg_loss:0.188, val_acc:0.913]
Epoch [18/120    avg_loss:0.160, val_acc:0.917]
Epoch [19/120    avg_loss:0.129, val_acc:0.921]
Epoch [20/120    avg_loss:0.140, val_acc:0.902]
Epoch [21/120    avg_loss:0.097, val_acc:0.957]
Epoch [22/120    avg_loss:0.076, val_acc:0.950]
Epoch [23/120    avg_loss:0.082, val_acc:0.958]
Epoch [24/120    avg_loss:0.061, val_acc:0.979]
Epoch [25/120    avg_loss:0.063, val_acc:0.968]
Epoch [26/120    avg_loss:0.059, val_acc:0.956]
Epoch [27/120    avg_loss:0.056, val_acc:0.968]
Epoch [28/120    avg_loss:0.057, val_acc:0.964]
Epoch [29/120    avg_loss:0.058, val_acc:0.956]
Epoch [30/120    avg_loss:0.082, val_acc:0.940]
Epoch [31/120    avg_loss:0.078, val_acc:0.979]
Epoch [32/120    avg_loss:0.047, val_acc:0.969]
Epoch [33/120    avg_loss:0.059, val_acc:0.934]
Epoch [34/120    avg_loss:0.078, val_acc:0.964]
Epoch [35/120    avg_loss:0.083, val_acc:0.953]
Epoch [36/120    avg_loss:0.099, val_acc:0.968]
Epoch [37/120    avg_loss:0.077, val_acc:0.943]
Epoch [38/120    avg_loss:0.061, val_acc:0.970]
Epoch [39/120    avg_loss:0.031, val_acc:0.975]
Epoch [40/120    avg_loss:0.023, val_acc:0.981]
Epoch [41/120    avg_loss:0.030, val_acc:0.977]
Epoch [42/120    avg_loss:0.032, val_acc:0.978]
Epoch [43/120    avg_loss:0.028, val_acc:0.980]
Epoch [44/120    avg_loss:0.029, val_acc:0.978]
Epoch [45/120    avg_loss:0.020, val_acc:0.978]
Epoch [46/120    avg_loss:0.026, val_acc:0.970]
Epoch [47/120    avg_loss:0.033, val_acc:0.977]
Epoch [48/120    avg_loss:0.025, val_acc:0.981]
Epoch [49/120    avg_loss:0.025, val_acc:0.983]
Epoch [50/120    avg_loss:0.046, val_acc:0.982]
Epoch [51/120    avg_loss:0.027, val_acc:0.972]
Epoch [52/120    avg_loss:0.022, val_acc:0.981]
Epoch [53/120    avg_loss:0.029, val_acc:0.968]
Epoch [54/120    avg_loss:0.021, val_acc:0.983]
Epoch [55/120    avg_loss:0.014, val_acc:0.975]
Epoch [56/120    avg_loss:0.015, val_acc:0.976]
Epoch [57/120    avg_loss:0.013, val_acc:0.978]
Epoch [58/120    avg_loss:0.012, val_acc:0.983]
Epoch [59/120    avg_loss:0.014, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.985]
Epoch [61/120    avg_loss:0.020, val_acc:0.983]
Epoch [62/120    avg_loss:0.008, val_acc:0.986]
Epoch [63/120    avg_loss:0.010, val_acc:0.978]
Epoch [64/120    avg_loss:0.023, val_acc:0.965]
Epoch [65/120    avg_loss:0.037, val_acc:0.967]
Epoch [66/120    avg_loss:0.031, val_acc:0.968]
Epoch [67/120    avg_loss:0.030, val_acc:0.944]
Epoch [68/120    avg_loss:0.052, val_acc:0.963]
Epoch [69/120    avg_loss:0.052, val_acc:0.976]
Epoch [70/120    avg_loss:0.044, val_acc:0.973]
Epoch [71/120    avg_loss:0.043, val_acc:0.953]
Epoch [72/120    avg_loss:0.030, val_acc:0.973]
Epoch [73/120    avg_loss:0.025, val_acc:0.979]
Epoch [74/120    avg_loss:0.023, val_acc:0.966]
Epoch [75/120    avg_loss:0.024, val_acc:0.978]
Epoch [76/120    avg_loss:0.011, val_acc:0.983]
Epoch [77/120    avg_loss:0.009, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.008, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     5     0     0     0     0    10     0]
 [    0     0 18058     0    20     0     4     0     8     0]
 [    0     4     0  1932     0     0     0     0    97     3]
 [    0    37     0     0  2926     0     2     0     6     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0    11     0     0     0     0     1  1267     1    10]
 [    0    13     0    60    36     0     0     0  3461     1]
 [    0     0     0     0     0     6     0     0     0   913]]

Accuracy:
99.16371436145856

F1 scores:
[       nan 0.99380517 0.99881081 0.95809571 0.98286866 0.99770642
 0.99815422 0.99100508 0.96757059 0.98863021]

Kappa:
0.9889195476060604
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f18efe5f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.742, val_acc:0.325]
Epoch [2/120    avg_loss:1.122, val_acc:0.517]
Epoch [3/120    avg_loss:0.887, val_acc:0.623]
Epoch [4/120    avg_loss:0.698, val_acc:0.619]
Epoch [5/120    avg_loss:0.553, val_acc:0.651]
Epoch [6/120    avg_loss:0.485, val_acc:0.769]
Epoch [7/120    avg_loss:0.403, val_acc:0.791]
Epoch [8/120    avg_loss:0.355, val_acc:0.844]
Epoch [9/120    avg_loss:0.334, val_acc:0.820]
Epoch [10/120    avg_loss:0.276, val_acc:0.832]
Epoch [11/120    avg_loss:0.268, val_acc:0.823]
Epoch [12/120    avg_loss:0.220, val_acc:0.919]
Epoch [13/120    avg_loss:0.206, val_acc:0.922]
Epoch [14/120    avg_loss:0.171, val_acc:0.918]
Epoch [15/120    avg_loss:0.176, val_acc:0.876]
Epoch [16/120    avg_loss:0.140, val_acc:0.923]
Epoch [17/120    avg_loss:0.100, val_acc:0.925]
Epoch [18/120    avg_loss:0.116, val_acc:0.958]
Epoch [19/120    avg_loss:0.098, val_acc:0.948]
Epoch [20/120    avg_loss:0.094, val_acc:0.956]
Epoch [21/120    avg_loss:0.108, val_acc:0.964]
Epoch [22/120    avg_loss:0.087, val_acc:0.969]
Epoch [23/120    avg_loss:0.105, val_acc:0.954]
Epoch [24/120    avg_loss:0.103, val_acc:0.964]
Epoch [25/120    avg_loss:0.073, val_acc:0.952]
Epoch [26/120    avg_loss:0.090, val_acc:0.936]
Epoch [27/120    avg_loss:0.076, val_acc:0.963]
Epoch [28/120    avg_loss:0.053, val_acc:0.966]
Epoch [29/120    avg_loss:0.039, val_acc:0.983]
Epoch [30/120    avg_loss:0.036, val_acc:0.981]
Epoch [31/120    avg_loss:0.049, val_acc:0.972]
Epoch [32/120    avg_loss:0.037, val_acc:0.973]
Epoch [33/120    avg_loss:0.036, val_acc:0.968]
Epoch [34/120    avg_loss:0.049, val_acc:0.984]
Epoch [35/120    avg_loss:0.039, val_acc:0.980]
Epoch [36/120    avg_loss:0.027, val_acc:0.976]
Epoch [37/120    avg_loss:0.030, val_acc:0.982]
Epoch [38/120    avg_loss:0.032, val_acc:0.981]
Epoch [39/120    avg_loss:0.031, val_acc:0.974]
Epoch [40/120    avg_loss:0.026, val_acc:0.960]
Epoch [41/120    avg_loss:0.073, val_acc:0.947]
Epoch [42/120    avg_loss:0.058, val_acc:0.965]
Epoch [43/120    avg_loss:0.047, val_acc:0.976]
Epoch [44/120    avg_loss:0.028, val_acc:0.976]
Epoch [45/120    avg_loss:0.025, val_acc:0.982]
Epoch [46/120    avg_loss:0.026, val_acc:0.973]
Epoch [47/120    avg_loss:0.028, val_acc:0.974]
Epoch [48/120    avg_loss:0.018, val_acc:0.982]
Epoch [49/120    avg_loss:0.020, val_acc:0.985]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.015, val_acc:0.984]
Epoch [52/120    avg_loss:0.011, val_acc:0.984]
Epoch [53/120    avg_loss:0.015, val_acc:0.984]
Epoch [54/120    avg_loss:0.011, val_acc:0.986]
Epoch [55/120    avg_loss:0.020, val_acc:0.987]
Epoch [56/120    avg_loss:0.013, val_acc:0.986]
Epoch [57/120    avg_loss:0.013, val_acc:0.987]
Epoch [58/120    avg_loss:0.016, val_acc:0.987]
Epoch [59/120    avg_loss:0.014, val_acc:0.985]
Epoch [60/120    avg_loss:0.009, val_acc:0.987]
Epoch [61/120    avg_loss:0.013, val_acc:0.988]
Epoch [62/120    avg_loss:0.013, val_acc:0.988]
Epoch [63/120    avg_loss:0.013, val_acc:0.987]
Epoch [64/120    avg_loss:0.012, val_acc:0.983]
Epoch [65/120    avg_loss:0.014, val_acc:0.983]
Epoch [66/120    avg_loss:0.011, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.988]
Epoch [68/120    avg_loss:0.012, val_acc:0.987]
Epoch [69/120    avg_loss:0.010, val_acc:0.988]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.010, val_acc:0.988]
Epoch [72/120    avg_loss:0.011, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.987]
Epoch [74/120    avg_loss:0.011, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.987]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.011, val_acc:0.988]
Epoch [80/120    avg_loss:0.016, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.989]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.016, val_acc:0.990]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.989]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.012, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.017, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.012, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6329     0     7     0     0     9     1    86     0]
 [    0     2 18012     0    24     0    48     0     4     0]
 [    0     0     0  1961     0     0     0     0    70     5]
 [    0    27     6     6  2931     0     0     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    34     0     0     0  4843     0     1     0]
 [    0    12     0     0     0     0     0  1274     0     4]
 [    0    10     0    54    51     0     0     0  3456     0]
 [    0     1     0     0     0     9     0     0     0   909]]

Accuracy:
98.86004868291037

F1 scores:
[       nan 0.98790291 0.9967351  0.96505906 0.98059552 0.99656357
 0.99059112 0.99337232 0.96133519 0.98965705]

Kappa:
0.9849026310543662
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81700c1780>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.743, val_acc:0.542]
Epoch [2/120    avg_loss:1.158, val_acc:0.732]
Epoch [3/120    avg_loss:0.901, val_acc:0.747]
Epoch [4/120    avg_loss:0.755, val_acc:0.777]
Epoch [5/120    avg_loss:0.622, val_acc:0.692]
Epoch [6/120    avg_loss:0.516, val_acc:0.715]
Epoch [7/120    avg_loss:0.454, val_acc:0.740]
Epoch [8/120    avg_loss:0.394, val_acc:0.763]
Epoch [9/120    avg_loss:0.346, val_acc:0.748]
Epoch [10/120    avg_loss:0.293, val_acc:0.805]
Epoch [11/120    avg_loss:0.272, val_acc:0.818]
Epoch [12/120    avg_loss:0.260, val_acc:0.812]
Epoch [13/120    avg_loss:0.249, val_acc:0.868]
Epoch [14/120    avg_loss:0.216, val_acc:0.898]
Epoch [15/120    avg_loss:0.177, val_acc:0.918]
Epoch [16/120    avg_loss:0.160, val_acc:0.911]
Epoch [17/120    avg_loss:0.188, val_acc:0.909]
Epoch [18/120    avg_loss:0.143, val_acc:0.917]
Epoch [19/120    avg_loss:0.109, val_acc:0.956]
Epoch [20/120    avg_loss:0.096, val_acc:0.927]
Epoch [21/120    avg_loss:0.089, val_acc:0.939]
Epoch [22/120    avg_loss:0.106, val_acc:0.960]
Epoch [23/120    avg_loss:0.076, val_acc:0.969]
Epoch [24/120    avg_loss:0.079, val_acc:0.963]
Epoch [25/120    avg_loss:0.066, val_acc:0.976]
Epoch [26/120    avg_loss:0.078, val_acc:0.927]
Epoch [27/120    avg_loss:0.062, val_acc:0.943]
Epoch [28/120    avg_loss:0.065, val_acc:0.958]
Epoch [29/120    avg_loss:0.092, val_acc:0.938]
Epoch [30/120    avg_loss:0.076, val_acc:0.961]
Epoch [31/120    avg_loss:0.062, val_acc:0.954]
Epoch [32/120    avg_loss:0.078, val_acc:0.953]
Epoch [33/120    avg_loss:0.038, val_acc:0.968]
Epoch [34/120    avg_loss:0.069, val_acc:0.968]
Epoch [35/120    avg_loss:0.071, val_acc:0.973]
Epoch [36/120    avg_loss:0.082, val_acc:0.950]
Epoch [37/120    avg_loss:0.039, val_acc:0.959]
Epoch [38/120    avg_loss:0.039, val_acc:0.979]
Epoch [39/120    avg_loss:0.029, val_acc:0.982]
Epoch [40/120    avg_loss:0.024, val_acc:0.969]
Epoch [41/120    avg_loss:0.022, val_acc:0.978]
Epoch [42/120    avg_loss:0.023, val_acc:0.973]
Epoch [43/120    avg_loss:0.022, val_acc:0.977]
Epoch [44/120    avg_loss:0.026, val_acc:0.969]
Epoch [45/120    avg_loss:0.031, val_acc:0.953]
Epoch [46/120    avg_loss:0.031, val_acc:0.958]
Epoch [47/120    avg_loss:0.028, val_acc:0.975]
Epoch [48/120    avg_loss:0.032, val_acc:0.980]
Epoch [49/120    avg_loss:0.023, val_acc:0.980]
Epoch [50/120    avg_loss:0.024, val_acc:0.986]
Epoch [51/120    avg_loss:0.014, val_acc:0.983]
Epoch [52/120    avg_loss:0.032, val_acc:0.972]
Epoch [53/120    avg_loss:0.030, val_acc:0.983]
Epoch [54/120    avg_loss:0.013, val_acc:0.987]
Epoch [55/120    avg_loss:0.013, val_acc:0.981]
Epoch [56/120    avg_loss:0.013, val_acc:0.988]
Epoch [57/120    avg_loss:0.019, val_acc:0.976]
Epoch [58/120    avg_loss:0.017, val_acc:0.992]
Epoch [59/120    avg_loss:0.015, val_acc:0.983]
Epoch [60/120    avg_loss:0.011, val_acc:0.968]
Epoch [61/120    avg_loss:0.012, val_acc:0.989]
Epoch [62/120    avg_loss:0.011, val_acc:0.983]
Epoch [63/120    avg_loss:0.012, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.982]
Epoch [65/120    avg_loss:0.010, val_acc:0.987]
Epoch [66/120    avg_loss:0.010, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.977]
Epoch [68/120    avg_loss:0.016, val_acc:0.980]
Epoch [69/120    avg_loss:0.011, val_acc:0.984]
Epoch [70/120    avg_loss:0.007, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.981]
Epoch [72/120    avg_loss:0.013, val_acc:0.985]
Epoch [73/120    avg_loss:0.006, val_acc:0.985]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.005, val_acc:0.987]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.987]
Epoch [79/120    avg_loss:0.004, val_acc:0.987]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.004, val_acc:0.988]
Epoch [82/120    avg_loss:0.004, val_acc:0.988]
Epoch [83/120    avg_loss:0.004, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.005, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.005, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0    11     0     0     5     0    38     1]
 [    0     0 18051     0    19     0    14     0     6     0]
 [    0     3     0  1956     0     0     0     0    77     0]
 [    0    29     4     0  2930     0     0     0     7     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    27     0     0     0  4849     0     2     0]
 [    0     9     0     0     0     0     0  1277     0     4]
 [    0     4     0    47    52     0     0     0  3467     1]
 [    0     2     0     0     0     6     0     0     0   911]]

Accuracy:
99.10828332489818

F1 scores:
[       nan 0.99206596 0.9980648  0.96592593 0.98108153 0.99770642
 0.9950749  0.99493572 0.96735491 0.99129489]

Kappa:
0.988185303438217
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4d60ce7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.725, val_acc:0.289]
Epoch [2/120    avg_loss:1.168, val_acc:0.490]
Epoch [3/120    avg_loss:0.914, val_acc:0.548]
Epoch [4/120    avg_loss:0.775, val_acc:0.705]
Epoch [5/120    avg_loss:0.644, val_acc:0.693]
Epoch [6/120    avg_loss:0.533, val_acc:0.691]
Epoch [7/120    avg_loss:0.429, val_acc:0.743]
Epoch [8/120    avg_loss:0.384, val_acc:0.761]
Epoch [9/120    avg_loss:0.341, val_acc:0.804]
Epoch [10/120    avg_loss:0.317, val_acc:0.802]
Epoch [11/120    avg_loss:0.276, val_acc:0.863]
Epoch [12/120    avg_loss:0.238, val_acc:0.769]
Epoch [13/120    avg_loss:0.248, val_acc:0.873]
Epoch [14/120    avg_loss:0.225, val_acc:0.853]
Epoch [15/120    avg_loss:0.223, val_acc:0.873]
Epoch [16/120    avg_loss:0.174, val_acc:0.908]
Epoch [17/120    avg_loss:0.169, val_acc:0.924]
Epoch [18/120    avg_loss:0.175, val_acc:0.915]
Epoch [19/120    avg_loss:0.134, val_acc:0.929]
Epoch [20/120    avg_loss:0.107, val_acc:0.949]
Epoch [21/120    avg_loss:0.103, val_acc:0.900]
Epoch [22/120    avg_loss:0.112, val_acc:0.867]
Epoch [23/120    avg_loss:0.157, val_acc:0.883]
Epoch [24/120    avg_loss:0.107, val_acc:0.909]
Epoch [25/120    avg_loss:0.109, val_acc:0.950]
Epoch [26/120    avg_loss:0.079, val_acc:0.953]
Epoch [27/120    avg_loss:0.117, val_acc:0.935]
Epoch [28/120    avg_loss:0.078, val_acc:0.953]
Epoch [29/120    avg_loss:0.099, val_acc:0.965]
Epoch [30/120    avg_loss:0.096, val_acc:0.937]
Epoch [31/120    avg_loss:0.086, val_acc:0.951]
Epoch [32/120    avg_loss:0.071, val_acc:0.967]
Epoch [33/120    avg_loss:0.054, val_acc:0.958]
Epoch [34/120    avg_loss:0.041, val_acc:0.964]
Epoch [35/120    avg_loss:0.064, val_acc:0.961]
Epoch [36/120    avg_loss:0.071, val_acc:0.968]
Epoch [37/120    avg_loss:0.050, val_acc:0.968]
Epoch [38/120    avg_loss:0.035, val_acc:0.978]
Epoch [39/120    avg_loss:0.031, val_acc:0.973]
Epoch [40/120    avg_loss:0.031, val_acc:0.959]
Epoch [41/120    avg_loss:0.023, val_acc:0.978]
Epoch [42/120    avg_loss:0.029, val_acc:0.953]
Epoch [43/120    avg_loss:0.046, val_acc:0.973]
Epoch [44/120    avg_loss:0.039, val_acc:0.973]
Epoch [45/120    avg_loss:0.029, val_acc:0.976]
Epoch [46/120    avg_loss:0.025, val_acc:0.977]
Epoch [47/120    avg_loss:0.039, val_acc:0.984]
Epoch [48/120    avg_loss:0.021, val_acc:0.981]
Epoch [49/120    avg_loss:0.020, val_acc:0.980]
Epoch [50/120    avg_loss:0.044, val_acc:0.980]
Epoch [51/120    avg_loss:0.029, val_acc:0.963]
Epoch [52/120    avg_loss:0.026, val_acc:0.973]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.019, val_acc:0.978]
Epoch [55/120    avg_loss:0.025, val_acc:0.978]
Epoch [56/120    avg_loss:0.025, val_acc:0.976]
Epoch [57/120    avg_loss:0.018, val_acc:0.986]
Epoch [58/120    avg_loss:0.013, val_acc:0.983]
Epoch [59/120    avg_loss:0.015, val_acc:0.962]
Epoch [60/120    avg_loss:0.024, val_acc:0.975]
Epoch [61/120    avg_loss:0.018, val_acc:0.981]
Epoch [62/120    avg_loss:0.012, val_acc:0.981]
Epoch [63/120    avg_loss:0.016, val_acc:0.973]
Epoch [64/120    avg_loss:0.013, val_acc:0.982]
Epoch [65/120    avg_loss:0.014, val_acc:0.979]
Epoch [66/120    avg_loss:0.018, val_acc:0.979]
Epoch [67/120    avg_loss:0.016, val_acc:0.987]
Epoch [68/120    avg_loss:0.009, val_acc:0.983]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.987]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.007, val_acc:0.981]
Epoch [74/120    avg_loss:0.010, val_acc:0.972]
Epoch [75/120    avg_loss:0.012, val_acc:0.978]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.008, val_acc:0.983]
Epoch [78/120    avg_loss:0.016, val_acc:0.974]
Epoch [79/120    avg_loss:0.022, val_acc:0.972]
Epoch [80/120    avg_loss:0.014, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.006, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.003, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.003, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     0     1     0     0     0    37     5]
 [    0     0 18066     0     8     0     6     0    10     0]
 [    0     0     0  1944     0     0     0     0    91     1]
 [    0    32    10     0  2920     0     3     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    43     0     0     0  4834     0     1     0]
 [    0     1     0     0     0     0     1  1279     0     9]
 [    0    39     0    59    38     0     0     0  3435     0]
 [    0     0     0     0     3    15     0     0     0   901]]

Accuracy:
98.98778107150603

F1 scores:
[       nan 0.99108043 0.99787346 0.96261451 0.98283406 0.99428571
 0.99444559 0.99571818 0.96057047 0.98201635]

Kappa:
0.9865822160137645
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7537896748>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.693, val_acc:0.557]
Epoch [2/120    avg_loss:1.199, val_acc:0.399]
Epoch [3/120    avg_loss:0.924, val_acc:0.677]
Epoch [4/120    avg_loss:0.764, val_acc:0.707]
Epoch [5/120    avg_loss:0.612, val_acc:0.708]
Epoch [6/120    avg_loss:0.494, val_acc:0.730]
Epoch [7/120    avg_loss:0.394, val_acc:0.702]
Epoch [8/120    avg_loss:0.345, val_acc:0.776]
Epoch [9/120    avg_loss:0.376, val_acc:0.767]
Epoch [10/120    avg_loss:0.358, val_acc:0.841]
Epoch [11/120    avg_loss:0.316, val_acc:0.851]
Epoch [12/120    avg_loss:0.309, val_acc:0.848]
Epoch [13/120    avg_loss:0.241, val_acc:0.891]
Epoch [14/120    avg_loss:0.189, val_acc:0.910]
Epoch [15/120    avg_loss:0.178, val_acc:0.920]
Epoch [16/120    avg_loss:0.200, val_acc:0.895]
Epoch [17/120    avg_loss:0.194, val_acc:0.887]
Epoch [18/120    avg_loss:0.155, val_acc:0.931]
Epoch [19/120    avg_loss:0.127, val_acc:0.935]
Epoch [20/120    avg_loss:0.136, val_acc:0.940]
Epoch [21/120    avg_loss:0.119, val_acc:0.922]
Epoch [22/120    avg_loss:0.125, val_acc:0.923]
Epoch [23/120    avg_loss:0.158, val_acc:0.906]
Epoch [24/120    avg_loss:0.132, val_acc:0.937]
Epoch [25/120    avg_loss:0.087, val_acc:0.953]
Epoch [26/120    avg_loss:0.061, val_acc:0.944]
Epoch [27/120    avg_loss:0.093, val_acc:0.890]
Epoch [28/120    avg_loss:0.095, val_acc:0.950]
Epoch [29/120    avg_loss:0.136, val_acc:0.927]
Epoch [30/120    avg_loss:0.103, val_acc:0.953]
Epoch [31/120    avg_loss:0.057, val_acc:0.932]
Epoch [32/120    avg_loss:0.041, val_acc:0.949]
Epoch [33/120    avg_loss:0.044, val_acc:0.957]
Epoch [34/120    avg_loss:0.038, val_acc:0.969]
Epoch [35/120    avg_loss:0.052, val_acc:0.955]
Epoch [36/120    avg_loss:0.053, val_acc:0.963]
Epoch [37/120    avg_loss:0.040, val_acc:0.962]
Epoch [38/120    avg_loss:0.030, val_acc:0.955]
Epoch [39/120    avg_loss:0.060, val_acc:0.929]
Epoch [40/120    avg_loss:0.078, val_acc:0.949]
Epoch [41/120    avg_loss:0.048, val_acc:0.951]
Epoch [42/120    avg_loss:0.037, val_acc:0.971]
Epoch [43/120    avg_loss:0.032, val_acc:0.968]
Epoch [44/120    avg_loss:0.042, val_acc:0.943]
Epoch [45/120    avg_loss:0.074, val_acc:0.950]
Epoch [46/120    avg_loss:0.062, val_acc:0.938]
Epoch [47/120    avg_loss:0.040, val_acc:0.953]
Epoch [48/120    avg_loss:0.040, val_acc:0.969]
Epoch [49/120    avg_loss:0.039, val_acc:0.958]
Epoch [50/120    avg_loss:0.029, val_acc:0.970]
Epoch [51/120    avg_loss:0.049, val_acc:0.956]
Epoch [52/120    avg_loss:0.036, val_acc:0.962]
Epoch [53/120    avg_loss:0.029, val_acc:0.969]
Epoch [54/120    avg_loss:0.030, val_acc:0.962]
Epoch [55/120    avg_loss:0.017, val_acc:0.967]
Epoch [56/120    avg_loss:0.020, val_acc:0.970]
Epoch [57/120    avg_loss:0.019, val_acc:0.972]
Epoch [58/120    avg_loss:0.017, val_acc:0.971]
Epoch [59/120    avg_loss:0.015, val_acc:0.973]
Epoch [60/120    avg_loss:0.012, val_acc:0.972]
Epoch [61/120    avg_loss:0.019, val_acc:0.971]
Epoch [62/120    avg_loss:0.013, val_acc:0.970]
Epoch [63/120    avg_loss:0.013, val_acc:0.971]
Epoch [64/120    avg_loss:0.010, val_acc:0.971]
Epoch [65/120    avg_loss:0.014, val_acc:0.970]
Epoch [66/120    avg_loss:0.008, val_acc:0.973]
Epoch [67/120    avg_loss:0.013, val_acc:0.973]
Epoch [68/120    avg_loss:0.012, val_acc:0.973]
Epoch [69/120    avg_loss:0.014, val_acc:0.973]
Epoch [70/120    avg_loss:0.013, val_acc:0.972]
Epoch [71/120    avg_loss:0.011, val_acc:0.974]
Epoch [72/120    avg_loss:0.012, val_acc:0.975]
Epoch [73/120    avg_loss:0.010, val_acc:0.973]
Epoch [74/120    avg_loss:0.009, val_acc:0.973]
Epoch [75/120    avg_loss:0.011, val_acc:0.973]
Epoch [76/120    avg_loss:0.008, val_acc:0.973]
Epoch [77/120    avg_loss:0.011, val_acc:0.973]
Epoch [78/120    avg_loss:0.014, val_acc:0.974]
Epoch [79/120    avg_loss:0.011, val_acc:0.973]
Epoch [80/120    avg_loss:0.010, val_acc:0.973]
Epoch [81/120    avg_loss:0.012, val_acc:0.973]
Epoch [82/120    avg_loss:0.014, val_acc:0.973]
Epoch [83/120    avg_loss:0.015, val_acc:0.976]
Epoch [84/120    avg_loss:0.008, val_acc:0.974]
Epoch [85/120    avg_loss:0.020, val_acc:0.974]
Epoch [86/120    avg_loss:0.012, val_acc:0.975]
Epoch [87/120    avg_loss:0.011, val_acc:0.974]
Epoch [88/120    avg_loss:0.010, val_acc:0.977]
Epoch [89/120    avg_loss:0.011, val_acc:0.974]
Epoch [90/120    avg_loss:0.013, val_acc:0.973]
Epoch [91/120    avg_loss:0.013, val_acc:0.976]
Epoch [92/120    avg_loss:0.010, val_acc:0.975]
Epoch [93/120    avg_loss:0.012, val_acc:0.973]
Epoch [94/120    avg_loss:0.008, val_acc:0.969]
Epoch [95/120    avg_loss:0.012, val_acc:0.971]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.009, val_acc:0.973]
Epoch [98/120    avg_loss:0.009, val_acc:0.976]
Epoch [99/120    avg_loss:0.008, val_acc:0.975]
Epoch [100/120    avg_loss:0.008, val_acc:0.975]
Epoch [101/120    avg_loss:0.009, val_acc:0.977]
Epoch [102/120    avg_loss:0.012, val_acc:0.975]
Epoch [103/120    avg_loss:0.009, val_acc:0.973]
Epoch [104/120    avg_loss:0.017, val_acc:0.975]
Epoch [105/120    avg_loss:0.008, val_acc:0.977]
Epoch [106/120    avg_loss:0.007, val_acc:0.976]
Epoch [107/120    avg_loss:0.009, val_acc:0.977]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.010, val_acc:0.974]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.009, val_acc:0.976]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.008, val_acc:0.979]
Epoch [114/120    avg_loss:0.016, val_acc:0.969]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.977]
Epoch [118/120    avg_loss:0.006, val_acc:0.976]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.010, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0    12     0     0     0     1    58     2]
 [    0     0 18012     0    25     0    38     0    15     0]
 [    0     6     0  1929     0     0     0     0   100     1]
 [    0    12     5     0  2930     0    18     0     3     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    74     0     0     0  4804     0     0     0]
 [    0     9     0     0     0     0     0  1279     0     2]
 [    0    10     0    35    33     0     3     0  3490     0]
 [    0     0     0     0     6     9     0     0     0   904]]

Accuracy:
98.84076832236764

F1 scores:
[       nan 0.99142501 0.99566071 0.96161515 0.98223265 0.99656357
 0.98634637 0.99533074 0.96448805 0.98689956]

Kappa:
0.984639608102323
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd5560387b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.711, val_acc:0.589]
Epoch [2/120    avg_loss:1.161, val_acc:0.725]
Epoch [3/120    avg_loss:0.910, val_acc:0.601]
Epoch [4/120    avg_loss:0.777, val_acc:0.682]
Epoch [5/120    avg_loss:0.691, val_acc:0.723]
Epoch [6/120    avg_loss:0.585, val_acc:0.793]
Epoch [7/120    avg_loss:0.484, val_acc:0.767]
Epoch [8/120    avg_loss:0.449, val_acc:0.787]
Epoch [9/120    avg_loss:0.413, val_acc:0.788]
Epoch [10/120    avg_loss:0.336, val_acc:0.796]
Epoch [11/120    avg_loss:0.299, val_acc:0.794]
Epoch [12/120    avg_loss:0.280, val_acc:0.802]
Epoch [13/120    avg_loss:0.286, val_acc:0.861]
Epoch [14/120    avg_loss:0.300, val_acc:0.859]
Epoch [15/120    avg_loss:0.275, val_acc:0.896]
Epoch [16/120    avg_loss:0.205, val_acc:0.871]
Epoch [17/120    avg_loss:0.197, val_acc:0.943]
Epoch [18/120    avg_loss:0.158, val_acc:0.939]
Epoch [19/120    avg_loss:0.143, val_acc:0.952]
Epoch [20/120    avg_loss:0.127, val_acc:0.932]
Epoch [21/120    avg_loss:0.135, val_acc:0.926]
Epoch [22/120    avg_loss:0.128, val_acc:0.915]
Epoch [23/120    avg_loss:0.146, val_acc:0.918]
Epoch [24/120    avg_loss:0.138, val_acc:0.913]
Epoch [25/120    avg_loss:0.142, val_acc:0.958]
Epoch [26/120    avg_loss:0.106, val_acc:0.956]
Epoch [27/120    avg_loss:0.096, val_acc:0.949]
Epoch [28/120    avg_loss:0.065, val_acc:0.963]
Epoch [29/120    avg_loss:0.076, val_acc:0.958]
Epoch [30/120    avg_loss:0.073, val_acc:0.959]
Epoch [31/120    avg_loss:0.055, val_acc:0.963]
Epoch [32/120    avg_loss:0.063, val_acc:0.968]
Epoch [33/120    avg_loss:0.057, val_acc:0.945]
Epoch [34/120    avg_loss:0.052, val_acc:0.971]
Epoch [35/120    avg_loss:0.050, val_acc:0.978]
Epoch [36/120    avg_loss:0.049, val_acc:0.975]
Epoch [37/120    avg_loss:0.051, val_acc:0.953]
Epoch [38/120    avg_loss:0.044, val_acc:0.979]
Epoch [39/120    avg_loss:0.032, val_acc:0.973]
Epoch [40/120    avg_loss:0.207, val_acc:0.902]
Epoch [41/120    avg_loss:0.124, val_acc:0.951]
Epoch [42/120    avg_loss:0.061, val_acc:0.963]
Epoch [43/120    avg_loss:0.046, val_acc:0.959]
Epoch [44/120    avg_loss:0.041, val_acc:0.963]
Epoch [45/120    avg_loss:0.044, val_acc:0.972]
Epoch [46/120    avg_loss:0.028, val_acc:0.972]
Epoch [47/120    avg_loss:0.022, val_acc:0.973]
Epoch [48/120    avg_loss:0.023, val_acc:0.979]
Epoch [49/120    avg_loss:0.022, val_acc:0.972]
Epoch [50/120    avg_loss:0.025, val_acc:0.970]
Epoch [51/120    avg_loss:0.056, val_acc:0.944]
Epoch [52/120    avg_loss:0.075, val_acc:0.972]
Epoch [53/120    avg_loss:0.037, val_acc:0.968]
Epoch [54/120    avg_loss:0.044, val_acc:0.948]
Epoch [55/120    avg_loss:0.031, val_acc:0.976]
Epoch [56/120    avg_loss:0.019, val_acc:0.982]
Epoch [57/120    avg_loss:0.017, val_acc:0.968]
Epoch [58/120    avg_loss:0.019, val_acc:0.978]
Epoch [59/120    avg_loss:0.023, val_acc:0.971]
Epoch [60/120    avg_loss:0.019, val_acc:0.977]
Epoch [61/120    avg_loss:0.014, val_acc:0.982]
Epoch [62/120    avg_loss:0.011, val_acc:0.974]
Epoch [63/120    avg_loss:0.014, val_acc:0.974]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.015, val_acc:0.967]
Epoch [66/120    avg_loss:0.014, val_acc:0.986]
Epoch [67/120    avg_loss:0.009, val_acc:0.983]
Epoch [68/120    avg_loss:0.010, val_acc:0.978]
Epoch [69/120    avg_loss:0.019, val_acc:0.983]
Epoch [70/120    avg_loss:0.030, val_acc:0.973]
Epoch [71/120    avg_loss:0.016, val_acc:0.983]
Epoch [72/120    avg_loss:0.016, val_acc:0.980]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.013, val_acc:0.984]
Epoch [75/120    avg_loss:0.006, val_acc:0.983]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.010, val_acc:0.979]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.009, val_acc:0.981]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.988]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.005, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.005, val_acc:0.987]
Epoch [89/120    avg_loss:0.005, val_acc:0.987]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.004, val_acc:0.988]
Epoch [109/120    avg_loss:0.003, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     2     0     0     0     0    54     6]
 [    0     5 18044     0    30     0     0     0    11     0]
 [    0     2     0  1963     0     0     0     0    71     0]
 [    0    29    10     0  2912     0    12     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    53     0     1     0  4804     0    20     0]
 [    0    10     0     0     0     0     0  1280     0     0]
 [    0    10     3    53    28     0    16     0  3460     1]
 [    0     0     0     0    10     1     0     0     0   908]]

Accuracy:
98.92270985467428

F1 scores:
[       nan 0.99082283 0.99690608 0.96842625 0.97833025 0.999617
 0.98949537 0.99610895 0.96204643 0.98856832]

Kappa:
0.9857224140068755
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d64a808d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.722, val_acc:0.586]
Epoch [2/120    avg_loss:1.172, val_acc:0.677]
Epoch [3/120    avg_loss:0.897, val_acc:0.703]
Epoch [4/120    avg_loss:0.731, val_acc:0.695]
Epoch [5/120    avg_loss:0.596, val_acc:0.691]
Epoch [6/120    avg_loss:0.491, val_acc:0.764]
Epoch [7/120    avg_loss:0.424, val_acc:0.762]
Epoch [8/120    avg_loss:0.359, val_acc:0.811]
Epoch [9/120    avg_loss:0.354, val_acc:0.837]
Epoch [10/120    avg_loss:0.373, val_acc:0.852]
Epoch [11/120    avg_loss:0.276, val_acc:0.875]
Epoch [12/120    avg_loss:0.247, val_acc:0.907]
Epoch [13/120    avg_loss:0.205, val_acc:0.890]
Epoch [14/120    avg_loss:0.230, val_acc:0.869]
Epoch [15/120    avg_loss:0.293, val_acc:0.906]
Epoch [16/120    avg_loss:0.196, val_acc:0.908]
Epoch [17/120    avg_loss:0.152, val_acc:0.907]
Epoch [18/120    avg_loss:0.160, val_acc:0.930]
Epoch [19/120    avg_loss:0.130, val_acc:0.954]
Epoch [20/120    avg_loss:0.123, val_acc:0.956]
Epoch [21/120    avg_loss:0.102, val_acc:0.963]
Epoch [22/120    avg_loss:0.097, val_acc:0.950]
Epoch [23/120    avg_loss:0.076, val_acc:0.948]
Epoch [24/120    avg_loss:0.086, val_acc:0.962]
Epoch [25/120    avg_loss:0.074, val_acc:0.968]
Epoch [26/120    avg_loss:0.076, val_acc:0.972]
Epoch [27/120    avg_loss:0.068, val_acc:0.961]
Epoch [28/120    avg_loss:0.062, val_acc:0.974]
Epoch [29/120    avg_loss:0.056, val_acc:0.959]
Epoch [30/120    avg_loss:0.045, val_acc:0.976]
Epoch [31/120    avg_loss:0.039, val_acc:0.982]
Epoch [32/120    avg_loss:0.039, val_acc:0.969]
Epoch [33/120    avg_loss:0.042, val_acc:0.968]
Epoch [34/120    avg_loss:0.033, val_acc:0.974]
Epoch [35/120    avg_loss:0.034, val_acc:0.969]
Epoch [36/120    avg_loss:0.024, val_acc:0.980]
Epoch [37/120    avg_loss:0.022, val_acc:0.983]
Epoch [38/120    avg_loss:0.025, val_acc:0.983]
Epoch [39/120    avg_loss:0.038, val_acc:0.957]
Epoch [40/120    avg_loss:0.022, val_acc:0.978]
Epoch [41/120    avg_loss:0.017, val_acc:0.978]
Epoch [42/120    avg_loss:0.019, val_acc:0.977]
Epoch [43/120    avg_loss:0.021, val_acc:0.974]
Epoch [44/120    avg_loss:0.027, val_acc:0.971]
Epoch [45/120    avg_loss:0.049, val_acc:0.983]
Epoch [46/120    avg_loss:0.044, val_acc:0.965]
Epoch [47/120    avg_loss:0.021, val_acc:0.974]
Epoch [48/120    avg_loss:0.043, val_acc:0.948]
Epoch [49/120    avg_loss:0.146, val_acc:0.927]
Epoch [50/120    avg_loss:0.084, val_acc:0.973]
Epoch [51/120    avg_loss:0.046, val_acc:0.973]
Epoch [52/120    avg_loss:0.027, val_acc:0.970]
Epoch [53/120    avg_loss:0.021, val_acc:0.977]
Epoch [54/120    avg_loss:0.022, val_acc:0.971]
Epoch [55/120    avg_loss:0.040, val_acc:0.967]
Epoch [56/120    avg_loss:0.043, val_acc:0.968]
Epoch [57/120    avg_loss:0.039, val_acc:0.975]
Epoch [58/120    avg_loss:0.022, val_acc:0.971]
Epoch [59/120    avg_loss:0.015, val_acc:0.973]
Epoch [60/120    avg_loss:0.014, val_acc:0.974]
Epoch [61/120    avg_loss:0.014, val_acc:0.973]
Epoch [62/120    avg_loss:0.015, val_acc:0.974]
Epoch [63/120    avg_loss:0.013, val_acc:0.974]
Epoch [64/120    avg_loss:0.016, val_acc:0.976]
Epoch [65/120    avg_loss:0.011, val_acc:0.975]
Epoch [66/120    avg_loss:0.011, val_acc:0.974]
Epoch [67/120    avg_loss:0.013, val_acc:0.973]
Epoch [68/120    avg_loss:0.017, val_acc:0.976]
Epoch [69/120    avg_loss:0.013, val_acc:0.978]
Epoch [70/120    avg_loss:0.011, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.980]
Epoch [72/120    avg_loss:0.012, val_acc:0.980]
Epoch [73/120    avg_loss:0.012, val_acc:0.979]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.013, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.010, val_acc:0.978]
Epoch [78/120    avg_loss:0.009, val_acc:0.978]
Epoch [79/120    avg_loss:0.010, val_acc:0.978]
Epoch [80/120    avg_loss:0.012, val_acc:0.978]
Epoch [81/120    avg_loss:0.009, val_acc:0.978]
Epoch [82/120    avg_loss:0.009, val_acc:0.978]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.011, val_acc:0.978]
Epoch [85/120    avg_loss:0.014, val_acc:0.978]
Epoch [86/120    avg_loss:0.013, val_acc:0.978]
Epoch [87/120    avg_loss:0.010, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.978]
Epoch [89/120    avg_loss:0.010, val_acc:0.978]
Epoch [90/120    avg_loss:0.009, val_acc:0.978]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.014, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.978]
Epoch [94/120    avg_loss:0.015, val_acc:0.978]
Epoch [95/120    avg_loss:0.015, val_acc:0.978]
Epoch [96/120    avg_loss:0.014, val_acc:0.978]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.009, val_acc:0.978]
Epoch [99/120    avg_loss:0.010, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.978]
Epoch [103/120    avg_loss:0.010, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.011, val_acc:0.978]
Epoch [106/120    avg_loss:0.009, val_acc:0.978]
Epoch [107/120    avg_loss:0.012, val_acc:0.978]
Epoch [108/120    avg_loss:0.014, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.978]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.978]
Epoch [113/120    avg_loss:0.013, val_acc:0.978]
Epoch [114/120    avg_loss:0.011, val_acc:0.978]
Epoch [115/120    avg_loss:0.011, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.011, val_acc:0.978]
Epoch [119/120    avg_loss:0.011, val_acc:0.978]
Epoch [120/120    avg_loss:0.011, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     0     0     0     2     0    65     7]
 [    0     0 18065     0    11     0     6     0     8     0]
 [    0     7     0  1932     0     0     0     0    96     1]
 [    0    13     5     0  2931     0     4     0     4    15]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    41     0     0     0  4837     0     0     0]
 [    0    25     0     0     0     0     0  1263     1     1]
 [    0    11     0    41    18     0     2     0  3499     0]
 [    0     0     0     0     8    17     0     0     0   894]]

Accuracy:
99.01429156725231

F1 scores:
[       nan 0.98988012 0.99803873 0.96383138 0.98686869 0.99352874
 0.9943468  0.98942421 0.96604086 0.97332608]

Kappa:
0.9869353196744614
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e6f5427b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 32629==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.796, val_acc:0.247]
Epoch [2/120    avg_loss:1.184, val_acc:0.427]
Epoch [3/120    avg_loss:0.922, val_acc:0.713]
Epoch [4/120    avg_loss:0.791, val_acc:0.698]
Epoch [5/120    avg_loss:0.679, val_acc:0.719]
Epoch [6/120    avg_loss:0.592, val_acc:0.665]
Epoch [7/120    avg_loss:0.456, val_acc:0.722]
Epoch [8/120    avg_loss:0.376, val_acc:0.781]
Epoch [9/120    avg_loss:0.333, val_acc:0.794]
Epoch [10/120    avg_loss:0.308, val_acc:0.794]
Epoch [11/120    avg_loss:0.279, val_acc:0.805]
Epoch [12/120    avg_loss:0.228, val_acc:0.911]
Epoch [13/120    avg_loss:0.206, val_acc:0.938]
Epoch [14/120    avg_loss:0.172, val_acc:0.901]
Epoch [15/120    avg_loss:0.141, val_acc:0.906]
Epoch [16/120    avg_loss:0.117, val_acc:0.955]
Epoch [17/120    avg_loss:0.140, val_acc:0.927]
Epoch [18/120    avg_loss:0.132, val_acc:0.954]
Epoch [19/120    avg_loss:0.102, val_acc:0.939]
Epoch [20/120    avg_loss:0.094, val_acc:0.895]
Epoch [21/120    avg_loss:0.084, val_acc:0.968]
Epoch [22/120    avg_loss:0.082, val_acc:0.968]
Epoch [23/120    avg_loss:0.066, val_acc:0.951]
Epoch [24/120    avg_loss:0.075, val_acc:0.963]
Epoch [25/120    avg_loss:0.075, val_acc:0.967]
Epoch [26/120    avg_loss:0.117, val_acc:0.936]
Epoch [27/120    avg_loss:0.100, val_acc:0.946]
Epoch [28/120    avg_loss:0.094, val_acc:0.955]
Epoch [29/120    avg_loss:0.090, val_acc:0.928]
Epoch [30/120    avg_loss:0.086, val_acc:0.957]
Epoch [31/120    avg_loss:0.057, val_acc:0.976]
Epoch [32/120    avg_loss:0.047, val_acc:0.973]
Epoch [33/120    avg_loss:0.047, val_acc:0.945]
Epoch [34/120    avg_loss:0.054, val_acc:0.963]
Epoch [35/120    avg_loss:0.051, val_acc:0.968]
Epoch [36/120    avg_loss:0.041, val_acc:0.983]
Epoch [37/120    avg_loss:0.030, val_acc:0.978]
Epoch [38/120    avg_loss:0.025, val_acc:0.983]
Epoch [39/120    avg_loss:0.021, val_acc:0.960]
Epoch [40/120    avg_loss:0.040, val_acc:0.963]
Epoch [41/120    avg_loss:0.060, val_acc:0.959]
Epoch [42/120    avg_loss:0.054, val_acc:0.960]
Epoch [43/120    avg_loss:0.068, val_acc:0.961]
Epoch [44/120    avg_loss:0.052, val_acc:0.969]
Epoch [45/120    avg_loss:0.036, val_acc:0.977]
Epoch [46/120    avg_loss:0.028, val_acc:0.975]
Epoch [47/120    avg_loss:0.029, val_acc:0.974]
Epoch [48/120    avg_loss:0.029, val_acc:0.974]
Epoch [49/120    avg_loss:0.049, val_acc:0.978]
Epoch [50/120    avg_loss:0.037, val_acc:0.980]
Epoch [51/120    avg_loss:0.019, val_acc:0.977]
Epoch [52/120    avg_loss:0.014, val_acc:0.983]
Epoch [53/120    avg_loss:0.014, val_acc:0.985]
Epoch [54/120    avg_loss:0.013, val_acc:0.986]
Epoch [55/120    avg_loss:0.014, val_acc:0.987]
Epoch [56/120    avg_loss:0.011, val_acc:0.987]
Epoch [57/120    avg_loss:0.013, val_acc:0.985]
Epoch [58/120    avg_loss:0.013, val_acc:0.986]
Epoch [59/120    avg_loss:0.012, val_acc:0.986]
Epoch [60/120    avg_loss:0.014, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.987]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.010, val_acc:0.988]
Epoch [64/120    avg_loss:0.017, val_acc:0.978]
Epoch [65/120    avg_loss:0.012, val_acc:0.983]
Epoch [66/120    avg_loss:0.011, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.986]
Epoch [68/120    avg_loss:0.011, val_acc:0.987]
Epoch [69/120    avg_loss:0.010, val_acc:0.987]
Epoch [70/120    avg_loss:0.014, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.011, val_acc:0.988]
Epoch [73/120    avg_loss:0.011, val_acc:0.988]
Epoch [74/120    avg_loss:0.011, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.010, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.012, val_acc:0.988]
Epoch [82/120    avg_loss:0.011, val_acc:0.989]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.989]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.989]
Epoch [90/120    avg_loss:0.008, val_acc:0.989]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.009, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.989]
Epoch [97/120    avg_loss:0.012, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.989]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     0     0     8     0    52     3]
 [    0     0 18030     0    15     0    40     0     5     0]
 [    0     3     0  1902     0     0     0     0   127     4]
 [    0    33     6     0  2925     0     4     0     4     0]
 [    0     0     0     0     0  1304     1     0     0     0]
 [    0     0    12     0     0     0  4866     0     0     0]
 [    0     2     0     0     0     0     3  1284     1     0]
 [    0    11     0    73    28     0    10     0  3448     1]
 [    0     0     0     0     6     8     2     0     0   903]]

Accuracy:
98.88655917865664

F1 scores:
[       nan 0.99128405 0.99784161 0.94839192 0.98385469 0.99656095
 0.99184672 0.997669   0.95671476 0.98688525]

Kappa:
0.9852515168355755
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd9bf18f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.844, val_acc:0.257]
Epoch [2/120    avg_loss:1.204, val_acc:0.282]
Epoch [3/120    avg_loss:0.990, val_acc:0.673]
Epoch [4/120    avg_loss:0.870, val_acc:0.764]
Epoch [5/120    avg_loss:0.722, val_acc:0.715]
Epoch [6/120    avg_loss:0.615, val_acc:0.762]
Epoch [7/120    avg_loss:0.483, val_acc:0.753]
Epoch [8/120    avg_loss:0.417, val_acc:0.697]
Epoch [9/120    avg_loss:0.336, val_acc:0.751]
Epoch [10/120    avg_loss:0.305, val_acc:0.735]
Epoch [11/120    avg_loss:0.262, val_acc:0.856]
Epoch [12/120    avg_loss:0.229, val_acc:0.859]
Epoch [13/120    avg_loss:0.238, val_acc:0.831]
Epoch [14/120    avg_loss:0.254, val_acc:0.872]
Epoch [15/120    avg_loss:0.197, val_acc:0.859]
Epoch [16/120    avg_loss:0.147, val_acc:0.941]
Epoch [17/120    avg_loss:0.108, val_acc:0.943]
Epoch [18/120    avg_loss:0.121, val_acc:0.927]
Epoch [19/120    avg_loss:0.151, val_acc:0.907]
Epoch [20/120    avg_loss:0.109, val_acc:0.958]
Epoch [21/120    avg_loss:0.097, val_acc:0.958]
Epoch [22/120    avg_loss:0.065, val_acc:0.973]
Epoch [23/120    avg_loss:0.068, val_acc:0.969]
Epoch [24/120    avg_loss:0.049, val_acc:0.968]
Epoch [25/120    avg_loss:0.055, val_acc:0.971]
Epoch [26/120    avg_loss:0.067, val_acc:0.972]
Epoch [27/120    avg_loss:0.091, val_acc:0.889]
Epoch [28/120    avg_loss:0.107, val_acc:0.936]
Epoch [29/120    avg_loss:0.086, val_acc:0.906]
Epoch [30/120    avg_loss:0.057, val_acc:0.963]
Epoch [31/120    avg_loss:0.046, val_acc:0.963]
Epoch [32/120    avg_loss:0.062, val_acc:0.932]
Epoch [33/120    avg_loss:0.057, val_acc:0.983]
Epoch [34/120    avg_loss:0.034, val_acc:0.983]
Epoch [35/120    avg_loss:0.032, val_acc:0.968]
Epoch [36/120    avg_loss:0.024, val_acc:0.988]
Epoch [37/120    avg_loss:0.019, val_acc:0.983]
Epoch [38/120    avg_loss:0.017, val_acc:0.983]
Epoch [39/120    avg_loss:0.024, val_acc:0.973]
Epoch [40/120    avg_loss:0.050, val_acc:0.934]
Epoch [41/120    avg_loss:0.127, val_acc:0.951]
Epoch [42/120    avg_loss:0.067, val_acc:0.925]
Epoch [43/120    avg_loss:0.044, val_acc:0.956]
Epoch [44/120    avg_loss:0.025, val_acc:0.984]
Epoch [45/120    avg_loss:0.024, val_acc:0.983]
Epoch [46/120    avg_loss:0.023, val_acc:0.951]
Epoch [47/120    avg_loss:0.030, val_acc:0.978]
Epoch [48/120    avg_loss:0.024, val_acc:0.964]
Epoch [49/120    avg_loss:0.028, val_acc:0.983]
Epoch [50/120    avg_loss:0.018, val_acc:0.983]
Epoch [51/120    avg_loss:0.013, val_acc:0.986]
Epoch [52/120    avg_loss:0.012, val_acc:0.987]
Epoch [53/120    avg_loss:0.011, val_acc:0.988]
Epoch [54/120    avg_loss:0.010, val_acc:0.988]
Epoch [55/120    avg_loss:0.013, val_acc:0.988]
Epoch [56/120    avg_loss:0.011, val_acc:0.988]
Epoch [57/120    avg_loss:0.010, val_acc:0.988]
Epoch [58/120    avg_loss:0.011, val_acc:0.988]
Epoch [59/120    avg_loss:0.010, val_acc:0.988]
Epoch [60/120    avg_loss:0.012, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.988]
Epoch [62/120    avg_loss:0.011, val_acc:0.989]
Epoch [63/120    avg_loss:0.010, val_acc:0.988]
Epoch [64/120    avg_loss:0.013, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.988]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.988]
Epoch [68/120    avg_loss:0.010, val_acc:0.988]
Epoch [69/120    avg_loss:0.011, val_acc:0.988]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.012, val_acc:0.988]
Epoch [72/120    avg_loss:0.010, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.008, val_acc:0.989]
Epoch [75/120    avg_loss:0.010, val_acc:0.989]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.010, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.989]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.008, val_acc:0.990]
Epoch [97/120    avg_loss:0.012, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.013, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     1     1     0     0     0    40     5]
 [    0     4 18042     0    25     0     4     0    15     0]
 [    0     2     0  1997     0     0     0     0    30     7]
 [    0    24     3     1  2919     0     3     0    19     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     1     0     0  4840     0     8     0]
 [    0     9     0     0     0     0     1  1271     0     9]
 [    0     1     0    33    50     0     0     0  3487     0]
 [    0     0     0     0     3     9     0     0     0   907]]

Accuracy:
99.18058467693346

F1 scores:
[       nan 0.99323326 0.99778786 0.98156795 0.97788945 0.99656357
 0.99527041 0.99258102 0.97266388 0.98054054]

Kappa:
0.989144853515182
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d8ce75860>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.756, val_acc:0.372]
Epoch [2/120    avg_loss:1.180, val_acc:0.422]
Epoch [3/120    avg_loss:0.945, val_acc:0.598]
Epoch [4/120    avg_loss:0.791, val_acc:0.578]
Epoch [5/120    avg_loss:0.663, val_acc:0.672]
Epoch [6/120    avg_loss:0.529, val_acc:0.697]
Epoch [7/120    avg_loss:0.443, val_acc:0.777]
Epoch [8/120    avg_loss:0.382, val_acc:0.751]
Epoch [9/120    avg_loss:0.321, val_acc:0.752]
Epoch [10/120    avg_loss:0.263, val_acc:0.806]
Epoch [11/120    avg_loss:0.264, val_acc:0.797]
Epoch [12/120    avg_loss:0.235, val_acc:0.880]
Epoch [13/120    avg_loss:0.186, val_acc:0.872]
Epoch [14/120    avg_loss:0.184, val_acc:0.938]
Epoch [15/120    avg_loss:0.138, val_acc:0.890]
Epoch [16/120    avg_loss:0.108, val_acc:0.928]
Epoch [17/120    avg_loss:0.139, val_acc:0.920]
Epoch [18/120    avg_loss:0.121, val_acc:0.928]
Epoch [19/120    avg_loss:0.134, val_acc:0.934]
Epoch [20/120    avg_loss:0.122, val_acc:0.944]
Epoch [21/120    avg_loss:0.101, val_acc:0.957]
Epoch [22/120    avg_loss:0.079, val_acc:0.947]
Epoch [23/120    avg_loss:0.061, val_acc:0.964]
Epoch [24/120    avg_loss:0.053, val_acc:0.969]
Epoch [25/120    avg_loss:0.066, val_acc:0.964]
Epoch [26/120    avg_loss:0.043, val_acc:0.971]
Epoch [27/120    avg_loss:0.053, val_acc:0.969]
Epoch [28/120    avg_loss:0.038, val_acc:0.970]
Epoch [29/120    avg_loss:0.045, val_acc:0.973]
Epoch [30/120    avg_loss:0.041, val_acc:0.979]
Epoch [31/120    avg_loss:0.026, val_acc:0.978]
Epoch [32/120    avg_loss:0.029, val_acc:0.984]
Epoch [33/120    avg_loss:0.026, val_acc:0.982]
Epoch [34/120    avg_loss:0.032, val_acc:0.977]
Epoch [35/120    avg_loss:0.033, val_acc:0.974]
Epoch [36/120    avg_loss:0.031, val_acc:0.968]
Epoch [37/120    avg_loss:0.030, val_acc:0.960]
Epoch [38/120    avg_loss:0.029, val_acc:0.971]
Epoch [39/120    avg_loss:0.081, val_acc:0.958]
Epoch [40/120    avg_loss:0.054, val_acc:0.974]
Epoch [41/120    avg_loss:0.053, val_acc:0.951]
Epoch [42/120    avg_loss:0.038, val_acc:0.973]
Epoch [43/120    avg_loss:0.025, val_acc:0.983]
Epoch [44/120    avg_loss:0.021, val_acc:0.981]
Epoch [45/120    avg_loss:0.020, val_acc:0.981]
Epoch [46/120    avg_loss:0.015, val_acc:0.982]
Epoch [47/120    avg_loss:0.015, val_acc:0.983]
Epoch [48/120    avg_loss:0.014, val_acc:0.984]
Epoch [49/120    avg_loss:0.013, val_acc:0.982]
Epoch [50/120    avg_loss:0.012, val_acc:0.983]
Epoch [51/120    avg_loss:0.012, val_acc:0.983]
Epoch [52/120    avg_loss:0.012, val_acc:0.983]
Epoch [53/120    avg_loss:0.012, val_acc:0.983]
Epoch [54/120    avg_loss:0.012, val_acc:0.984]
Epoch [55/120    avg_loss:0.011, val_acc:0.985]
Epoch [56/120    avg_loss:0.009, val_acc:0.983]
Epoch [57/120    avg_loss:0.011, val_acc:0.984]
Epoch [58/120    avg_loss:0.009, val_acc:0.983]
Epoch [59/120    avg_loss:0.009, val_acc:0.983]
Epoch [60/120    avg_loss:0.014, val_acc:0.984]
Epoch [61/120    avg_loss:0.009, val_acc:0.984]
Epoch [62/120    avg_loss:0.012, val_acc:0.984]
Epoch [63/120    avg_loss:0.011, val_acc:0.983]
Epoch [64/120    avg_loss:0.014, val_acc:0.983]
Epoch [65/120    avg_loss:0.012, val_acc:0.982]
Epoch [66/120    avg_loss:0.010, val_acc:0.984]
Epoch [67/120    avg_loss:0.012, val_acc:0.985]
Epoch [68/120    avg_loss:0.009, val_acc:0.984]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.011, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.015, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.985]
Epoch [78/120    avg_loss:0.011, val_acc:0.985]
Epoch [79/120    avg_loss:0.009, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.014, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.017, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0     0     5    31     0]
 [    0     0 18045     0    16     0    22     0     7     0]
 [    0    11     0  1969     2     0     0     0    51     3]
 [    0    30    13     0  2917     0     1     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0     0     0  4839     0    10     0]
 [    0     1     0     0     0     0     1  1286     0     2]
 [    0     4     0    18    52     0     0     0  3497     0]
 [    0     0     0     0    14    19     0     0     0   886]]

Accuracy:
99.1492540910515

F1 scores:
[       nan 0.99363057 0.99759516 0.97887149 0.97672861 0.99277292
 0.99353249 0.99651298 0.97436612 0.97900552]

Kappa:
0.9887266498002175
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa7a3f45780>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.740, val_acc:0.263]
Epoch [2/120    avg_loss:1.161, val_acc:0.611]
Epoch [3/120    avg_loss:0.917, val_acc:0.614]
Epoch [4/120    avg_loss:0.768, val_acc:0.639]
Epoch [5/120    avg_loss:0.595, val_acc:0.631]
Epoch [6/120    avg_loss:0.494, val_acc:0.722]
Epoch [7/120    avg_loss:0.450, val_acc:0.704]
Epoch [8/120    avg_loss:0.376, val_acc:0.758]
Epoch [9/120    avg_loss:0.305, val_acc:0.807]
Epoch [10/120    avg_loss:0.308, val_acc:0.783]
Epoch [11/120    avg_loss:0.252, val_acc:0.838]
Epoch [12/120    avg_loss:0.236, val_acc:0.849]
Epoch [13/120    avg_loss:0.184, val_acc:0.945]
Epoch [14/120    avg_loss:0.176, val_acc:0.856]
Epoch [15/120    avg_loss:0.194, val_acc:0.871]
Epoch [16/120    avg_loss:0.180, val_acc:0.809]
Epoch [17/120    avg_loss:0.164, val_acc:0.925]
Epoch [18/120    avg_loss:0.126, val_acc:0.948]
Epoch [19/120    avg_loss:0.104, val_acc:0.944]
Epoch [20/120    avg_loss:0.125, val_acc:0.956]
Epoch [21/120    avg_loss:0.118, val_acc:0.951]
Epoch [22/120    avg_loss:0.079, val_acc:0.948]
Epoch [23/120    avg_loss:0.074, val_acc:0.958]
Epoch [24/120    avg_loss:0.049, val_acc:0.956]
Epoch [25/120    avg_loss:0.042, val_acc:0.972]
Epoch [26/120    avg_loss:0.047, val_acc:0.977]
Epoch [27/120    avg_loss:0.069, val_acc:0.949]
Epoch [28/120    avg_loss:0.047, val_acc:0.973]
Epoch [29/120    avg_loss:0.082, val_acc:0.953]
Epoch [30/120    avg_loss:0.066, val_acc:0.958]
Epoch [31/120    avg_loss:0.044, val_acc:0.969]
Epoch [32/120    avg_loss:0.043, val_acc:0.953]
Epoch [33/120    avg_loss:0.039, val_acc:0.964]
Epoch [34/120    avg_loss:0.039, val_acc:0.978]
Epoch [35/120    avg_loss:0.037, val_acc:0.970]
Epoch [36/120    avg_loss:0.030, val_acc:0.976]
Epoch [37/120    avg_loss:0.030, val_acc:0.979]
Epoch [38/120    avg_loss:0.017, val_acc:0.986]
Epoch [39/120    avg_loss:0.028, val_acc:0.955]
Epoch [40/120    avg_loss:0.026, val_acc:0.983]
Epoch [41/120    avg_loss:0.034, val_acc:0.977]
Epoch [42/120    avg_loss:0.027, val_acc:0.981]
Epoch [43/120    avg_loss:0.023, val_acc:0.983]
Epoch [44/120    avg_loss:0.040, val_acc:0.982]
Epoch [45/120    avg_loss:0.029, val_acc:0.980]
Epoch [46/120    avg_loss:0.018, val_acc:0.984]
Epoch [47/120    avg_loss:0.014, val_acc:0.988]
Epoch [48/120    avg_loss:0.020, val_acc:0.978]
Epoch [49/120    avg_loss:0.019, val_acc:0.987]
Epoch [50/120    avg_loss:0.012, val_acc:0.986]
Epoch [51/120    avg_loss:0.014, val_acc:0.986]
Epoch [52/120    avg_loss:0.009, val_acc:0.985]
Epoch [53/120    avg_loss:0.015, val_acc:0.979]
Epoch [54/120    avg_loss:0.013, val_acc:0.983]
Epoch [55/120    avg_loss:0.010, val_acc:0.989]
Epoch [56/120    avg_loss:0.015, val_acc:0.970]
Epoch [57/120    avg_loss:0.017, val_acc:0.983]
Epoch [58/120    avg_loss:0.010, val_acc:0.988]
Epoch [59/120    avg_loss:0.019, val_acc:0.978]
Epoch [60/120    avg_loss:0.018, val_acc:0.983]
Epoch [61/120    avg_loss:0.013, val_acc:0.989]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.016, val_acc:0.988]
Epoch [64/120    avg_loss:0.015, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.988]
Epoch [66/120    avg_loss:0.012, val_acc:0.987]
Epoch [67/120    avg_loss:0.010, val_acc:0.987]
Epoch [68/120    avg_loss:0.008, val_acc:0.984]
Epoch [69/120    avg_loss:0.008, val_acc:0.989]
Epoch [70/120    avg_loss:0.010, val_acc:0.989]
Epoch [71/120    avg_loss:0.007, val_acc:0.992]
Epoch [72/120    avg_loss:0.006, val_acc:0.993]
Epoch [73/120    avg_loss:0.004, val_acc:0.987]
Epoch [74/120    avg_loss:0.011, val_acc:0.975]
Epoch [75/120    avg_loss:0.016, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.968]
Epoch [77/120    avg_loss:0.014, val_acc:0.976]
Epoch [78/120    avg_loss:0.019, val_acc:0.989]
Epoch [79/120    avg_loss:0.009, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.993]
Epoch [81/120    avg_loss:0.005, val_acc:0.992]
Epoch [82/120    avg_loss:0.004, val_acc:0.992]
Epoch [83/120    avg_loss:0.005, val_acc:0.992]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.003, val_acc:0.991]
Epoch [86/120    avg_loss:0.004, val_acc:0.993]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.005, val_acc:0.993]
Epoch [90/120    avg_loss:0.003, val_acc:0.991]
Epoch [91/120    avg_loss:0.003, val_acc:0.993]
Epoch [92/120    avg_loss:0.003, val_acc:0.993]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.004, val_acc:0.993]
Epoch [96/120    avg_loss:0.004, val_acc:0.993]
Epoch [97/120    avg_loss:0.002, val_acc:0.993]
Epoch [98/120    avg_loss:0.003, val_acc:0.993]
Epoch [99/120    avg_loss:0.003, val_acc:0.993]
Epoch [100/120    avg_loss:0.003, val_acc:0.993]
Epoch [101/120    avg_loss:0.003, val_acc:0.993]
Epoch [102/120    avg_loss:0.002, val_acc:0.993]
Epoch [103/120    avg_loss:0.003, val_acc:0.993]
Epoch [104/120    avg_loss:0.003, val_acc:0.993]
Epoch [105/120    avg_loss:0.003, val_acc:0.993]
Epoch [106/120    avg_loss:0.003, val_acc:0.993]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.002, val_acc:0.993]
Epoch [109/120    avg_loss:0.004, val_acc:0.993]
Epoch [110/120    avg_loss:0.002, val_acc:0.993]
Epoch [111/120    avg_loss:0.003, val_acc:0.993]
Epoch [112/120    avg_loss:0.003, val_acc:0.993]
Epoch [113/120    avg_loss:0.003, val_acc:0.993]
Epoch [114/120    avg_loss:0.003, val_acc:0.993]
Epoch [115/120    avg_loss:0.002, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.002, val_acc:0.993]
Epoch [118/120    avg_loss:0.002, val_acc:0.993]
Epoch [119/120    avg_loss:0.002, val_acc:0.993]
Epoch [120/120    avg_loss:0.004, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     3     0     0     1    10    22     0]
 [    0     3 18062     0    10     0     0     0    15     0]
 [    0     5     0  1975     0     0     0     0    52     4]
 [    0    30     1     2  2917     0     9     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     1     0     0  4872     0     0     0]
 [    0     2     0     0     0     0     0  1285     0     3]
 [    0     7     0     3    45     0     0     0  3507     9]
 [    0     0     0     0    12    30     0     0     0   877]]

Accuracy:
99.2842166148507

F1 scores:
[       nan 0.9935534  0.99905968 0.98258706 0.97951645 0.98863636
 0.99836066 0.99419729 0.97688022 0.96799117]

Kappa:
0.9905170472341429
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c45dbfcf8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.770, val_acc:0.257]
Epoch [2/120    avg_loss:1.234, val_acc:0.507]
Epoch [3/120    avg_loss:0.986, val_acc:0.632]
Epoch [4/120    avg_loss:0.869, val_acc:0.671]
Epoch [5/120    avg_loss:0.726, val_acc:0.682]
Epoch [6/120    avg_loss:0.594, val_acc:0.703]
Epoch [7/120    avg_loss:0.495, val_acc:0.706]
Epoch [8/120    avg_loss:0.395, val_acc:0.759]
Epoch [9/120    avg_loss:0.327, val_acc:0.797]
Epoch [10/120    avg_loss:0.309, val_acc:0.803]
Epoch [11/120    avg_loss:0.278, val_acc:0.814]
Epoch [12/120    avg_loss:0.253, val_acc:0.813]
Epoch [13/120    avg_loss:0.233, val_acc:0.868]
Epoch [14/120    avg_loss:0.193, val_acc:0.866]
Epoch [15/120    avg_loss:0.183, val_acc:0.934]
Epoch [16/120    avg_loss:0.163, val_acc:0.939]
Epoch [17/120    avg_loss:0.153, val_acc:0.902]
Epoch [18/120    avg_loss:0.174, val_acc:0.938]
Epoch [19/120    avg_loss:0.177, val_acc:0.932]
Epoch [20/120    avg_loss:0.140, val_acc:0.935]
Epoch [21/120    avg_loss:0.096, val_acc:0.914]
Epoch [22/120    avg_loss:0.089, val_acc:0.961]
Epoch [23/120    avg_loss:0.077, val_acc:0.957]
Epoch [24/120    avg_loss:0.103, val_acc:0.949]
Epoch [25/120    avg_loss:0.087, val_acc:0.954]
Epoch [26/120    avg_loss:0.055, val_acc:0.970]
Epoch [27/120    avg_loss:0.047, val_acc:0.973]
Epoch [28/120    avg_loss:0.053, val_acc:0.969]
Epoch [29/120    avg_loss:0.041, val_acc:0.970]
Epoch [30/120    avg_loss:0.043, val_acc:0.977]
Epoch [31/120    avg_loss:0.032, val_acc:0.974]
Epoch [32/120    avg_loss:0.063, val_acc:0.973]
Epoch [33/120    avg_loss:0.045, val_acc:0.971]
Epoch [34/120    avg_loss:0.060, val_acc:0.970]
Epoch [35/120    avg_loss:0.041, val_acc:0.958]
Epoch [36/120    avg_loss:0.041, val_acc:0.965]
Epoch [37/120    avg_loss:0.038, val_acc:0.974]
Epoch [38/120    avg_loss:0.025, val_acc:0.981]
Epoch [39/120    avg_loss:0.033, val_acc:0.967]
Epoch [40/120    avg_loss:0.030, val_acc:0.982]
Epoch [41/120    avg_loss:0.024, val_acc:0.984]
Epoch [42/120    avg_loss:0.023, val_acc:0.981]
Epoch [43/120    avg_loss:0.013, val_acc:0.983]
Epoch [44/120    avg_loss:0.011, val_acc:0.985]
Epoch [45/120    avg_loss:0.029, val_acc:0.974]
Epoch [46/120    avg_loss:0.071, val_acc:0.964]
Epoch [47/120    avg_loss:0.037, val_acc:0.978]
Epoch [48/120    avg_loss:0.024, val_acc:0.985]
Epoch [49/120    avg_loss:0.014, val_acc:0.985]
Epoch [50/120    avg_loss:0.016, val_acc:0.974]
Epoch [51/120    avg_loss:0.015, val_acc:0.984]
Epoch [52/120    avg_loss:0.014, val_acc:0.982]
Epoch [53/120    avg_loss:0.051, val_acc:0.970]
Epoch [54/120    avg_loss:0.044, val_acc:0.978]
Epoch [55/120    avg_loss:0.025, val_acc:0.972]
Epoch [56/120    avg_loss:0.028, val_acc:0.964]
Epoch [57/120    avg_loss:0.074, val_acc:0.965]
Epoch [58/120    avg_loss:0.027, val_acc:0.978]
Epoch [59/120    avg_loss:0.020, val_acc:0.985]
Epoch [60/120    avg_loss:0.023, val_acc:0.979]
Epoch [61/120    avg_loss:0.022, val_acc:0.979]
Epoch [62/120    avg_loss:0.014, val_acc:0.967]
Epoch [63/120    avg_loss:0.015, val_acc:0.983]
Epoch [64/120    avg_loss:0.008, val_acc:0.983]
Epoch [65/120    avg_loss:0.007, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.986]
Epoch [67/120    avg_loss:0.006, val_acc:0.987]
Epoch [68/120    avg_loss:0.012, val_acc:0.974]
Epoch [69/120    avg_loss:0.019, val_acc:0.986]
Epoch [70/120    avg_loss:0.011, val_acc:0.986]
Epoch [71/120    avg_loss:0.010, val_acc:0.982]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.987]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.014, val_acc:0.986]
Epoch [78/120    avg_loss:0.013, val_acc:0.984]
Epoch [79/120    avg_loss:0.033, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.981]
Epoch [81/120    avg_loss:0.017, val_acc:0.983]
Epoch [82/120    avg_loss:0.009, val_acc:0.983]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.980]
Epoch [88/120    avg_loss:0.010, val_acc:0.991]
Epoch [89/120    avg_loss:0.004, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.013, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.991]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.003, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.003, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.009, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.993]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.002, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6363     0     0     0     0     6     0    63     0]
 [    0     0 18079     0     8     0     0     0     3     0]
 [    0     0     0  1979     0     0     0     0    53     4]
 [    0    27    11     0  2919     0     1     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     0     0  4864     0     0     0]
 [    0     0     0     0     0     0     0  1284     0     6]
 [    0     0     0    37    47     0     0     0  3484     3]
 [    0     0     0     0    13    14     0     0     0   892]]

Accuracy:
99.21914539801895

F1 scores:
[       nan 0.99251287 0.99900536 0.97680158 0.97969458 0.99466463
 0.99784593 0.997669   0.96952832 0.97753425]

Kappa:
0.9896523120309205
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0beb4ce860>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.770, val_acc:0.298]
Epoch [2/120    avg_loss:1.168, val_acc:0.377]
Epoch [3/120    avg_loss:0.948, val_acc:0.654]
Epoch [4/120    avg_loss:0.748, val_acc:0.672]
Epoch [5/120    avg_loss:0.619, val_acc:0.673]
Epoch [6/120    avg_loss:0.496, val_acc:0.718]
Epoch [7/120    avg_loss:0.395, val_acc:0.707]
Epoch [8/120    avg_loss:0.340, val_acc:0.787]
Epoch [9/120    avg_loss:0.280, val_acc:0.784]
Epoch [10/120    avg_loss:0.271, val_acc:0.798]
Epoch [11/120    avg_loss:0.318, val_acc:0.826]
Epoch [12/120    avg_loss:0.252, val_acc:0.825]
Epoch [13/120    avg_loss:0.249, val_acc:0.819]
Epoch [14/120    avg_loss:0.205, val_acc:0.884]
Epoch [15/120    avg_loss:0.151, val_acc:0.922]
Epoch [16/120    avg_loss:0.130, val_acc:0.939]
Epoch [17/120    avg_loss:0.152, val_acc:0.905]
Epoch [18/120    avg_loss:0.101, val_acc:0.936]
Epoch [19/120    avg_loss:0.085, val_acc:0.900]
Epoch [20/120    avg_loss:0.164, val_acc:0.952]
Epoch [21/120    avg_loss:0.083, val_acc:0.956]
Epoch [22/120    avg_loss:0.103, val_acc:0.953]
Epoch [23/120    avg_loss:0.091, val_acc:0.897]
Epoch [24/120    avg_loss:0.070, val_acc:0.942]
Epoch [25/120    avg_loss:0.073, val_acc:0.962]
Epoch [26/120    avg_loss:0.053, val_acc:0.953]
Epoch [27/120    avg_loss:0.063, val_acc:0.958]
Epoch [28/120    avg_loss:0.074, val_acc:0.963]
Epoch [29/120    avg_loss:0.069, val_acc:0.953]
Epoch [30/120    avg_loss:0.059, val_acc:0.943]
Epoch [31/120    avg_loss:0.036, val_acc:0.972]
Epoch [32/120    avg_loss:0.053, val_acc:0.863]
Epoch [33/120    avg_loss:0.061, val_acc:0.917]
Epoch [34/120    avg_loss:0.055, val_acc:0.969]
Epoch [35/120    avg_loss:0.028, val_acc:0.964]
Epoch [36/120    avg_loss:0.037, val_acc:0.973]
Epoch [37/120    avg_loss:0.036, val_acc:0.970]
Epoch [38/120    avg_loss:0.025, val_acc:0.964]
Epoch [39/120    avg_loss:0.024, val_acc:0.973]
Epoch [40/120    avg_loss:0.017, val_acc:0.974]
Epoch [41/120    avg_loss:0.019, val_acc:0.975]
Epoch [42/120    avg_loss:0.015, val_acc:0.977]
Epoch [43/120    avg_loss:0.025, val_acc:0.968]
Epoch [44/120    avg_loss:0.020, val_acc:0.972]
Epoch [45/120    avg_loss:0.018, val_acc:0.981]
Epoch [46/120    avg_loss:0.025, val_acc:0.970]
Epoch [47/120    avg_loss:0.027, val_acc:0.959]
Epoch [48/120    avg_loss:0.020, val_acc:0.978]
Epoch [49/120    avg_loss:0.011, val_acc:0.983]
Epoch [50/120    avg_loss:0.010, val_acc:0.977]
Epoch [51/120    avg_loss:0.012, val_acc:0.984]
Epoch [52/120    avg_loss:0.009, val_acc:0.978]
Epoch [53/120    avg_loss:0.010, val_acc:0.980]
Epoch [54/120    avg_loss:0.015, val_acc:0.980]
Epoch [55/120    avg_loss:0.015, val_acc:0.980]
Epoch [56/120    avg_loss:0.015, val_acc:0.943]
Epoch [57/120    avg_loss:0.019, val_acc:0.972]
Epoch [58/120    avg_loss:0.013, val_acc:0.971]
Epoch [59/120    avg_loss:0.015, val_acc:0.980]
Epoch [60/120    avg_loss:0.021, val_acc:0.973]
Epoch [61/120    avg_loss:0.042, val_acc:0.973]
Epoch [62/120    avg_loss:0.026, val_acc:0.983]
Epoch [63/120    avg_loss:0.013, val_acc:0.980]
Epoch [64/120    avg_loss:0.011, val_acc:0.979]
Epoch [65/120    avg_loss:0.009, val_acc:0.981]
Epoch [66/120    avg_loss:0.009, val_acc:0.980]
Epoch [67/120    avg_loss:0.012, val_acc:0.979]
Epoch [68/120    avg_loss:0.008, val_acc:0.980]
Epoch [69/120    avg_loss:0.006, val_acc:0.982]
Epoch [70/120    avg_loss:0.006, val_acc:0.982]
Epoch [71/120    avg_loss:0.007, val_acc:0.982]
Epoch [72/120    avg_loss:0.006, val_acc:0.981]
Epoch [73/120    avg_loss:0.007, val_acc:0.981]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.008, val_acc:0.983]
Epoch [76/120    avg_loss:0.006, val_acc:0.981]
Epoch [77/120    avg_loss:0.007, val_acc:0.981]
Epoch [78/120    avg_loss:0.007, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.983]
Epoch [80/120    avg_loss:0.008, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.982]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.985]
Epoch [85/120    avg_loss:0.006, val_acc:0.985]
Epoch [86/120    avg_loss:0.005, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.983]
Epoch [89/120    avg_loss:0.007, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.983]
Epoch [91/120    avg_loss:0.005, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.005, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     0     0     0     0    18     0]
 [    0     2 18042     0    29     0     2     0    15     0]
 [    0    12     0  1984     0     0     0     0    36     4]
 [    0    42    16     3  2889     0     1     0    19     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     1     0     0  4864     0     0     0]
 [    0     8     0     0     0     0     0  1282     0     0]
 [    0    11     0    30    49     0     0     0  3477     4]
 [    0     0     0     0    13    16     0     0     0   890]]

Accuracy:
99.16612440652641

F1 scores:
[       nan 0.99280241 0.99787063 0.97878638 0.97076613 0.99390708
 0.99825552 0.99688958 0.97449552 0.97855965]

Kappa:
0.9889512564598829
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92f18ae860>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.737, val_acc:0.537]
Epoch [2/120    avg_loss:1.197, val_acc:0.737]
Epoch [3/120    avg_loss:0.940, val_acc:0.613]
Epoch [4/120    avg_loss:0.754, val_acc:0.546]
Epoch [5/120    avg_loss:0.639, val_acc:0.654]
Epoch [6/120    avg_loss:0.483, val_acc:0.702]
Epoch [7/120    avg_loss:0.426, val_acc:0.767]
Epoch [8/120    avg_loss:0.378, val_acc:0.818]
Epoch [9/120    avg_loss:0.353, val_acc:0.769]
Epoch [10/120    avg_loss:0.321, val_acc:0.822]
Epoch [11/120    avg_loss:0.282, val_acc:0.835]
Epoch [12/120    avg_loss:0.238, val_acc:0.826]
Epoch [13/120    avg_loss:0.279, val_acc:0.833]
Epoch [14/120    avg_loss:0.192, val_acc:0.892]
Epoch [15/120    avg_loss:0.207, val_acc:0.783]
Epoch [16/120    avg_loss:0.200, val_acc:0.833]
Epoch [17/120    avg_loss:0.166, val_acc:0.871]
Epoch [18/120    avg_loss:0.195, val_acc:0.889]
Epoch [19/120    avg_loss:0.163, val_acc:0.905]
Epoch [20/120    avg_loss:0.119, val_acc:0.889]
Epoch [21/120    avg_loss:0.154, val_acc:0.894]
Epoch [22/120    avg_loss:0.094, val_acc:0.889]
Epoch [23/120    avg_loss:0.087, val_acc:0.922]
Epoch [24/120    avg_loss:0.113, val_acc:0.910]
Epoch [25/120    avg_loss:0.104, val_acc:0.928]
Epoch [26/120    avg_loss:0.086, val_acc:0.941]
Epoch [27/120    avg_loss:0.078, val_acc:0.935]
Epoch [28/120    avg_loss:0.075, val_acc:0.935]
Epoch [29/120    avg_loss:0.051, val_acc:0.963]
Epoch [30/120    avg_loss:0.046, val_acc:0.949]
Epoch [31/120    avg_loss:0.060, val_acc:0.951]
Epoch [32/120    avg_loss:0.058, val_acc:0.947]
Epoch [33/120    avg_loss:0.038, val_acc:0.961]
Epoch [34/120    avg_loss:0.034, val_acc:0.963]
Epoch [35/120    avg_loss:0.028, val_acc:0.973]
Epoch [36/120    avg_loss:0.041, val_acc:0.970]
Epoch [37/120    avg_loss:0.036, val_acc:0.967]
Epoch [38/120    avg_loss:0.035, val_acc:0.964]
Epoch [39/120    avg_loss:0.021, val_acc:0.964]
Epoch [40/120    avg_loss:0.025, val_acc:0.960]
Epoch [41/120    avg_loss:0.029, val_acc:0.974]
Epoch [42/120    avg_loss:0.025, val_acc:0.974]
Epoch [43/120    avg_loss:0.026, val_acc:0.976]
Epoch [44/120    avg_loss:0.025, val_acc:0.964]
Epoch [45/120    avg_loss:0.036, val_acc:0.972]
Epoch [46/120    avg_loss:0.034, val_acc:0.948]
Epoch [47/120    avg_loss:0.030, val_acc:0.980]
Epoch [48/120    avg_loss:0.013, val_acc:0.983]
Epoch [49/120    avg_loss:0.011, val_acc:0.980]
Epoch [50/120    avg_loss:0.015, val_acc:0.978]
Epoch [51/120    avg_loss:0.022, val_acc:0.968]
Epoch [52/120    avg_loss:0.014, val_acc:0.978]
Epoch [53/120    avg_loss:0.012, val_acc:0.967]
Epoch [54/120    avg_loss:0.015, val_acc:0.980]
Epoch [55/120    avg_loss:0.014, val_acc:0.983]
Epoch [56/120    avg_loss:0.014, val_acc:0.973]
Epoch [57/120    avg_loss:0.014, val_acc:0.978]
Epoch [58/120    avg_loss:0.010, val_acc:0.980]
Epoch [59/120    avg_loss:0.024, val_acc:0.969]
Epoch [60/120    avg_loss:0.012, val_acc:0.981]
Epoch [61/120    avg_loss:0.008, val_acc:0.981]
Epoch [62/120    avg_loss:0.013, val_acc:0.979]
Epoch [63/120    avg_loss:0.007, val_acc:0.983]
Epoch [64/120    avg_loss:0.009, val_acc:0.983]
Epoch [65/120    avg_loss:0.012, val_acc:0.981]
Epoch [66/120    avg_loss:0.015, val_acc:0.971]
Epoch [67/120    avg_loss:0.015, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.981]
Epoch [69/120    avg_loss:0.009, val_acc:0.988]
Epoch [70/120    avg_loss:0.062, val_acc:0.958]
Epoch [71/120    avg_loss:0.112, val_acc:0.927]
Epoch [72/120    avg_loss:0.067, val_acc:0.956]
Epoch [73/120    avg_loss:0.047, val_acc:0.964]
Epoch [74/120    avg_loss:0.024, val_acc:0.960]
Epoch [75/120    avg_loss:0.018, val_acc:0.969]
Epoch [76/120    avg_loss:0.032, val_acc:0.978]
Epoch [77/120    avg_loss:0.023, val_acc:0.979]
Epoch [78/120    avg_loss:0.016, val_acc:0.973]
Epoch [79/120    avg_loss:0.011, val_acc:0.978]
Epoch [80/120    avg_loss:0.014, val_acc:0.975]
Epoch [81/120    avg_loss:0.010, val_acc:0.975]
Epoch [82/120    avg_loss:0.031, val_acc:0.949]
Epoch [83/120    avg_loss:0.017, val_acc:0.979]
Epoch [84/120    avg_loss:0.007, val_acc:0.979]
Epoch [85/120    avg_loss:0.008, val_acc:0.979]
Epoch [86/120    avg_loss:0.008, val_acc:0.981]
Epoch [87/120    avg_loss:0.011, val_acc:0.979]
Epoch [88/120    avg_loss:0.008, val_acc:0.980]
Epoch [89/120    avg_loss:0.006, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.007, val_acc:0.980]
Epoch [93/120    avg_loss:0.007, val_acc:0.982]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.005, val_acc:0.982]
Epoch [97/120    avg_loss:0.006, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.983]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6305     0     0     0     0     5     0   113     9]
 [    0     5 18045     0    34     0     0     0     6     0]
 [    0     1     0  1996     0     0     0     0    37     2]
 [    0    43     9     0  2906     0     0     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     0     0  4856     0     0     0]
 [    0     2     0     0     0     0     2  1286     0     0]
 [    0     7     0    21    53     0     0     0  3484     6]
 [    0     0     0     0    14    16     0     0     0   889]]

Accuracy:
98.9853710264382

F1 scores:
[       nan 0.98554123 0.99789858 0.98494942 0.97206891 0.99390708
 0.99702289 0.9984472  0.96442907 0.97424658]

Kappa:
0.9865597398611823
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f13e39fb828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.806, val_acc:0.588]
Epoch [2/120    avg_loss:1.233, val_acc:0.698]
Epoch [3/120    avg_loss:0.988, val_acc:0.640]
Epoch [4/120    avg_loss:0.804, val_acc:0.657]
Epoch [5/120    avg_loss:0.652, val_acc:0.715]
Epoch [6/120    avg_loss:0.526, val_acc:0.762]
Epoch [7/120    avg_loss:0.419, val_acc:0.784]
Epoch [8/120    avg_loss:0.375, val_acc:0.776]
Epoch [9/120    avg_loss:0.329, val_acc:0.803]
Epoch [10/120    avg_loss:0.353, val_acc:0.807]
Epoch [11/120    avg_loss:0.247, val_acc:0.818]
Epoch [12/120    avg_loss:0.234, val_acc:0.853]
Epoch [13/120    avg_loss:0.218, val_acc:0.930]
Epoch [14/120    avg_loss:0.200, val_acc:0.873]
Epoch [15/120    avg_loss:0.176, val_acc:0.932]
Epoch [16/120    avg_loss:0.150, val_acc:0.940]
Epoch [17/120    avg_loss:0.164, val_acc:0.873]
Epoch [18/120    avg_loss:0.115, val_acc:0.946]
Epoch [19/120    avg_loss:0.086, val_acc:0.960]
Epoch [20/120    avg_loss:0.082, val_acc:0.951]
Epoch [21/120    avg_loss:0.067, val_acc:0.970]
Epoch [22/120    avg_loss:0.107, val_acc:0.945]
Epoch [23/120    avg_loss:0.091, val_acc:0.937]
Epoch [24/120    avg_loss:0.062, val_acc:0.973]
Epoch [25/120    avg_loss:0.063, val_acc:0.973]
Epoch [26/120    avg_loss:0.108, val_acc:0.945]
Epoch [27/120    avg_loss:0.101, val_acc:0.949]
Epoch [28/120    avg_loss:0.091, val_acc:0.963]
Epoch [29/120    avg_loss:0.050, val_acc:0.943]
Epoch [30/120    avg_loss:0.052, val_acc:0.983]
Epoch [31/120    avg_loss:0.042, val_acc:0.968]
Epoch [32/120    avg_loss:0.032, val_acc:0.983]
Epoch [33/120    avg_loss:0.025, val_acc:0.978]
Epoch [34/120    avg_loss:0.021, val_acc:0.983]
Epoch [35/120    avg_loss:0.024, val_acc:0.987]
Epoch [36/120    avg_loss:0.033, val_acc:0.973]
Epoch [37/120    avg_loss:0.043, val_acc:0.986]
Epoch [38/120    avg_loss:0.028, val_acc:0.987]
Epoch [39/120    avg_loss:0.032, val_acc:0.988]
Epoch [40/120    avg_loss:0.020, val_acc:0.987]
Epoch [41/120    avg_loss:0.024, val_acc:0.986]
Epoch [42/120    avg_loss:0.020, val_acc:0.987]
Epoch [43/120    avg_loss:0.012, val_acc:0.988]
Epoch [44/120    avg_loss:0.019, val_acc:0.986]
Epoch [45/120    avg_loss:0.022, val_acc:0.985]
Epoch [46/120    avg_loss:0.017, val_acc:0.978]
Epoch [47/120    avg_loss:0.015, val_acc:0.986]
Epoch [48/120    avg_loss:0.012, val_acc:0.989]
Epoch [49/120    avg_loss:0.016, val_acc:0.987]
Epoch [50/120    avg_loss:0.015, val_acc:0.984]
Epoch [51/120    avg_loss:0.010, val_acc:0.992]
Epoch [52/120    avg_loss:0.014, val_acc:0.973]
Epoch [53/120    avg_loss:0.009, val_acc:0.992]
Epoch [54/120    avg_loss:0.012, val_acc:0.983]
Epoch [55/120    avg_loss:0.016, val_acc:0.976]
Epoch [56/120    avg_loss:0.012, val_acc:0.988]
Epoch [57/120    avg_loss:0.017, val_acc:0.977]
Epoch [58/120    avg_loss:0.042, val_acc:0.928]
Epoch [59/120    avg_loss:0.021, val_acc:0.985]
Epoch [60/120    avg_loss:0.017, val_acc:0.976]
Epoch [61/120    avg_loss:0.048, val_acc:0.980]
Epoch [62/120    avg_loss:0.048, val_acc:0.952]
Epoch [63/120    avg_loss:0.083, val_acc:0.978]
Epoch [64/120    avg_loss:0.056, val_acc:0.968]
Epoch [65/120    avg_loss:0.024, val_acc:0.974]
Epoch [66/120    avg_loss:0.018, val_acc:0.983]
Epoch [67/120    avg_loss:0.020, val_acc:0.988]
Epoch [68/120    avg_loss:0.012, val_acc:0.988]
Epoch [69/120    avg_loss:0.010, val_acc:0.988]
Epoch [70/120    avg_loss:0.012, val_acc:0.988]
Epoch [71/120    avg_loss:0.010, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.007, val_acc:0.988]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.012, val_acc:0.989]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.989]
Epoch [79/120    avg_loss:0.007, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.989]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.007, val_acc:0.988]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     0     0     0     0     2    66     6]
 [    0     0 18041     0    35     0    11     0     3     0]
 [    0     3     0  1984     0     0     0     0    48     1]
 [    0    25     8     3  2920     0     1     0    10     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     0     0  4851     0     3     0]
 [    0     3     0     0     0     0     0  1286     0     1]
 [    0     5     0    35    55     0     0     0  3472     4]
 [    0     0     0     0     7    13     0     0     0   899]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.99142367 0.99776014 0.97782159 0.97512106 0.99504384
 0.9959963  0.99767261 0.96807472 0.97983651]

Kappa:
0.9879643560965431
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e13bbe828>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.696, val_acc:0.333]
Epoch [2/120    avg_loss:1.188, val_acc:0.396]
Epoch [3/120    avg_loss:0.938, val_acc:0.732]
Epoch [4/120    avg_loss:0.804, val_acc:0.684]
Epoch [5/120    avg_loss:0.659, val_acc:0.634]
Epoch [6/120    avg_loss:0.593, val_acc:0.582]
Epoch [7/120    avg_loss:0.484, val_acc:0.707]
Epoch [8/120    avg_loss:0.426, val_acc:0.747]
Epoch [9/120    avg_loss:0.363, val_acc:0.798]
Epoch [10/120    avg_loss:0.341, val_acc:0.796]
Epoch [11/120    avg_loss:0.287, val_acc:0.817]
Epoch [12/120    avg_loss:0.263, val_acc:0.840]
Epoch [13/120    avg_loss:0.243, val_acc:0.850]
Epoch [14/120    avg_loss:0.209, val_acc:0.893]
Epoch [15/120    avg_loss:0.206, val_acc:0.878]
Epoch [16/120    avg_loss:0.169, val_acc:0.921]
Epoch [17/120    avg_loss:0.156, val_acc:0.929]
Epoch [18/120    avg_loss:0.159, val_acc:0.835]
Epoch [19/120    avg_loss:0.138, val_acc:0.959]
Epoch [20/120    avg_loss:0.119, val_acc:0.933]
Epoch [21/120    avg_loss:0.098, val_acc:0.967]
Epoch [22/120    avg_loss:0.090, val_acc:0.964]
Epoch [23/120    avg_loss:0.072, val_acc:0.957]
Epoch [24/120    avg_loss:0.098, val_acc:0.848]
Epoch [25/120    avg_loss:0.083, val_acc:0.966]
Epoch [26/120    avg_loss:0.058, val_acc:0.971]
Epoch [27/120    avg_loss:0.050, val_acc:0.966]
Epoch [28/120    avg_loss:0.045, val_acc:0.972]
Epoch [29/120    avg_loss:0.036, val_acc:0.955]
Epoch [30/120    avg_loss:0.039, val_acc:0.935]
Epoch [31/120    avg_loss:0.064, val_acc:0.971]
Epoch [32/120    avg_loss:0.040, val_acc:0.976]
Epoch [33/120    avg_loss:0.035, val_acc:0.975]
Epoch [34/120    avg_loss:0.041, val_acc:0.938]
Epoch [35/120    avg_loss:0.040, val_acc:0.983]
Epoch [36/120    avg_loss:0.035, val_acc:0.983]
Epoch [37/120    avg_loss:0.028, val_acc:0.987]
Epoch [38/120    avg_loss:0.029, val_acc:0.982]
Epoch [39/120    avg_loss:0.047, val_acc:0.970]
Epoch [40/120    avg_loss:0.028, val_acc:0.980]
Epoch [41/120    avg_loss:0.024, val_acc:0.980]
Epoch [42/120    avg_loss:0.022, val_acc:0.984]
Epoch [43/120    avg_loss:0.028, val_acc:0.979]
Epoch [44/120    avg_loss:0.028, val_acc:0.981]
Epoch [45/120    avg_loss:0.033, val_acc:0.903]
Epoch [46/120    avg_loss:0.046, val_acc:0.971]
Epoch [47/120    avg_loss:0.030, val_acc:0.984]
Epoch [48/120    avg_loss:0.014, val_acc:0.985]
Epoch [49/120    avg_loss:0.023, val_acc:0.979]
Epoch [50/120    avg_loss:0.017, val_acc:0.985]
Epoch [51/120    avg_loss:0.020, val_acc:0.984]
Epoch [52/120    avg_loss:0.010, val_acc:0.984]
Epoch [53/120    avg_loss:0.012, val_acc:0.987]
Epoch [54/120    avg_loss:0.014, val_acc:0.986]
Epoch [55/120    avg_loss:0.009, val_acc:0.985]
Epoch [56/120    avg_loss:0.012, val_acc:0.986]
Epoch [57/120    avg_loss:0.012, val_acc:0.986]
Epoch [58/120    avg_loss:0.009, val_acc:0.986]
Epoch [59/120    avg_loss:0.009, val_acc:0.987]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.011, val_acc:0.986]
Epoch [62/120    avg_loss:0.015, val_acc:0.986]
Epoch [63/120    avg_loss:0.013, val_acc:0.986]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.014, val_acc:0.986]
Epoch [66/120    avg_loss:0.012, val_acc:0.987]
Epoch [67/120    avg_loss:0.008, val_acc:0.987]
Epoch [68/120    avg_loss:0.010, val_acc:0.987]
Epoch [69/120    avg_loss:0.010, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.987]
Epoch [71/120    avg_loss:0.008, val_acc:0.989]
Epoch [72/120    avg_loss:0.008, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.987]
Epoch [74/120    avg_loss:0.012, val_acc:0.987]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.011, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.011, val_acc:0.989]
Epoch [87/120    avg_loss:0.010, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.009, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.013, val_acc:0.990]
Epoch [97/120    avg_loss:0.012, val_acc:0.989]
Epoch [98/120    avg_loss:0.009, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.008, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.009, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.990]
Epoch [107/120    avg_loss:0.008, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.013, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.989]
Epoch [115/120    avg_loss:0.011, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.011, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.989]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6381     0     0     2     0     6     0    42     1]
 [    0     0 18027     0    24     0    28     0    11     0]
 [    0     3     0  2005     0     0     0     0    27     1]
 [    0    34     0     0  2918     0     5     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     3     0     0  4865     0     9     0]
 [    0     5     0     0     0     0     7  1272     0     6]
 [    0     5     0    11    46     0     0     0  3496    13]
 [    0     0     0     0     4    25     0     0     0   890]]

Accuracy:
99.19504494734052

F1 scores:
[       nan 0.99237947 0.99822803 0.98890259 0.97820986 0.99051233
 0.99397283 0.99297424 0.97503835 0.9726776 ]

Kappa:
0.9893408320533301
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0901d9860>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.750, val_acc:0.531]
Epoch [2/120    avg_loss:1.185, val_acc:0.474]
Epoch [3/120    avg_loss:0.957, val_acc:0.693]
Epoch [4/120    avg_loss:0.815, val_acc:0.720]
Epoch [5/120    avg_loss:0.650, val_acc:0.679]
Epoch [6/120    avg_loss:0.542, val_acc:0.768]
Epoch [7/120    avg_loss:0.453, val_acc:0.723]
Epoch [8/120    avg_loss:0.429, val_acc:0.752]
Epoch [9/120    avg_loss:0.364, val_acc:0.825]
Epoch [10/120    avg_loss:0.293, val_acc:0.848]
Epoch [11/120    avg_loss:0.287, val_acc:0.881]
Epoch [12/120    avg_loss:0.215, val_acc:0.898]
Epoch [13/120    avg_loss:0.222, val_acc:0.913]
Epoch [14/120    avg_loss:0.187, val_acc:0.926]
Epoch [15/120    avg_loss:0.191, val_acc:0.941]
Epoch [16/120    avg_loss:0.190, val_acc:0.934]
Epoch [17/120    avg_loss:0.140, val_acc:0.936]
Epoch [18/120    avg_loss:0.120, val_acc:0.946]
Epoch [19/120    avg_loss:0.115, val_acc:0.938]
Epoch [20/120    avg_loss:0.114, val_acc:0.961]
Epoch [21/120    avg_loss:0.083, val_acc:0.943]
Epoch [22/120    avg_loss:0.124, val_acc:0.950]
Epoch [23/120    avg_loss:0.092, val_acc:0.962]
Epoch [24/120    avg_loss:0.089, val_acc:0.969]
Epoch [25/120    avg_loss:0.081, val_acc:0.965]
Epoch [26/120    avg_loss:0.068, val_acc:0.963]
Epoch [27/120    avg_loss:0.072, val_acc:0.923]
Epoch [28/120    avg_loss:0.068, val_acc:0.958]
Epoch [29/120    avg_loss:0.066, val_acc:0.973]
Epoch [30/120    avg_loss:0.059, val_acc:0.968]
Epoch [31/120    avg_loss:0.043, val_acc:0.977]
Epoch [32/120    avg_loss:0.037, val_acc:0.969]
Epoch [33/120    avg_loss:0.038, val_acc:0.977]
Epoch [34/120    avg_loss:0.034, val_acc:0.969]
Epoch [35/120    avg_loss:0.043, val_acc:0.944]
Epoch [36/120    avg_loss:0.065, val_acc:0.973]
Epoch [37/120    avg_loss:0.034, val_acc:0.970]
Epoch [38/120    avg_loss:0.025, val_acc:0.969]
Epoch [39/120    avg_loss:0.031, val_acc:0.972]
Epoch [40/120    avg_loss:0.029, val_acc:0.977]
Epoch [41/120    avg_loss:0.024, val_acc:0.975]
Epoch [42/120    avg_loss:0.072, val_acc:0.976]
Epoch [43/120    avg_loss:0.064, val_acc:0.943]
Epoch [44/120    avg_loss:0.044, val_acc:0.975]
Epoch [45/120    avg_loss:0.020, val_acc:0.979]
Epoch [46/120    avg_loss:0.029, val_acc:0.973]
Epoch [47/120    avg_loss:0.026, val_acc:0.975]
Epoch [48/120    avg_loss:0.035, val_acc:0.973]
Epoch [49/120    avg_loss:0.030, val_acc:0.983]
Epoch [50/120    avg_loss:0.016, val_acc:0.980]
Epoch [51/120    avg_loss:0.016, val_acc:0.978]
Epoch [52/120    avg_loss:0.018, val_acc:0.980]
Epoch [53/120    avg_loss:0.024, val_acc:0.973]
Epoch [54/120    avg_loss:0.023, val_acc:0.978]
Epoch [55/120    avg_loss:0.014, val_acc:0.983]
Epoch [56/120    avg_loss:0.026, val_acc:0.974]
Epoch [57/120    avg_loss:0.016, val_acc:0.973]
Epoch [58/120    avg_loss:0.020, val_acc:0.978]
Epoch [59/120    avg_loss:0.022, val_acc:0.978]
Epoch [60/120    avg_loss:0.036, val_acc:0.971]
Epoch [61/120    avg_loss:0.018, val_acc:0.981]
Epoch [62/120    avg_loss:0.019, val_acc:0.980]
Epoch [63/120    avg_loss:0.012, val_acc:0.979]
Epoch [64/120    avg_loss:0.011, val_acc:0.979]
Epoch [65/120    avg_loss:0.010, val_acc:0.979]
Epoch [66/120    avg_loss:0.008, val_acc:0.979]
Epoch [67/120    avg_loss:0.012, val_acc:0.980]
Epoch [68/120    avg_loss:0.008, val_acc:0.981]
Epoch [69/120    avg_loss:0.009, val_acc:0.980]
Epoch [70/120    avg_loss:0.009, val_acc:0.980]
Epoch [71/120    avg_loss:0.008, val_acc:0.980]
Epoch [72/120    avg_loss:0.006, val_acc:0.980]
Epoch [73/120    avg_loss:0.007, val_acc:0.980]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.007, val_acc:0.981]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.006, val_acc:0.981]
Epoch [78/120    avg_loss:0.008, val_acc:0.981]
Epoch [79/120    avg_loss:0.008, val_acc:0.981]
Epoch [80/120    avg_loss:0.007, val_acc:0.981]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.008, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.981]
Epoch [84/120    avg_loss:0.008, val_acc:0.981]
Epoch [85/120    avg_loss:0.007, val_acc:0.981]
Epoch [86/120    avg_loss:0.010, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.009, val_acc:0.981]
Epoch [90/120    avg_loss:0.007, val_acc:0.981]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.007, val_acc:0.981]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.006, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.007, val_acc:0.981]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.981]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.013, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     0     0     1     0    47     0]
 [    0     1 18039     0    27     0     6     0    17     0]
 [    0     0     1  1997     1     0     0     0    33     4]
 [    0    26    12     0  2909     0    10     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     1     0     0  4861     0     4     0]
 [    0     1     0     0     0     0     1  1284     1     3]
 [    0    10     0    37    57     0     0     0  3466     1]
 [    0     0     0     0    12    18     0     0     0   889]]

Accuracy:
99.13479382064445

F1 scores:
[       nan 0.99330948 0.99789788 0.98108573 0.9732352  0.99315068
 0.99641283 0.997669   0.96896841 0.97907489]

Kappa:
0.9885389566788343
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe08135e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 36629==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.771, val_acc:0.540]
Epoch [2/120    avg_loss:1.189, val_acc:0.663]
Epoch [3/120    avg_loss:0.967, val_acc:0.568]
Epoch [4/120    avg_loss:0.784, val_acc:0.598]
Epoch [5/120    avg_loss:0.671, val_acc:0.610]
Epoch [6/120    avg_loss:0.514, val_acc:0.762]
Epoch [7/120    avg_loss:0.436, val_acc:0.768]
Epoch [8/120    avg_loss:0.365, val_acc:0.808]
Epoch [9/120    avg_loss:0.328, val_acc:0.832]
Epoch [10/120    avg_loss:0.299, val_acc:0.885]
Epoch [11/120    avg_loss:0.285, val_acc:0.859]
Epoch [12/120    avg_loss:0.218, val_acc:0.912]
Epoch [13/120    avg_loss:0.206, val_acc:0.894]
Epoch [14/120    avg_loss:0.181, val_acc:0.911]
Epoch [15/120    avg_loss:0.180, val_acc:0.917]
Epoch [16/120    avg_loss:0.153, val_acc:0.953]
Epoch [17/120    avg_loss:0.098, val_acc:0.951]
Epoch [18/120    avg_loss:0.108, val_acc:0.939]
Epoch [19/120    avg_loss:0.119, val_acc:0.949]
Epoch [20/120    avg_loss:0.105, val_acc:0.953]
Epoch [21/120    avg_loss:0.096, val_acc:0.962]
Epoch [22/120    avg_loss:0.135, val_acc:0.952]
Epoch [23/120    avg_loss:0.121, val_acc:0.968]
Epoch [24/120    avg_loss:0.118, val_acc:0.960]
Epoch [25/120    avg_loss:0.073, val_acc:0.973]
Epoch [26/120    avg_loss:0.061, val_acc:0.959]
Epoch [27/120    avg_loss:0.062, val_acc:0.956]
Epoch [28/120    avg_loss:0.060, val_acc:0.961]
Epoch [29/120    avg_loss:0.058, val_acc:0.969]
Epoch [30/120    avg_loss:0.038, val_acc:0.977]
Epoch [31/120    avg_loss:0.031, val_acc:0.980]
Epoch [32/120    avg_loss:0.033, val_acc:0.977]
Epoch [33/120    avg_loss:0.041, val_acc:0.963]
Epoch [34/120    avg_loss:0.045, val_acc:0.964]
Epoch [35/120    avg_loss:0.042, val_acc:0.958]
Epoch [36/120    avg_loss:0.052, val_acc:0.966]
Epoch [37/120    avg_loss:0.035, val_acc:0.973]
Epoch [38/120    avg_loss:0.022, val_acc:0.983]
Epoch [39/120    avg_loss:0.029, val_acc:0.981]
Epoch [40/120    avg_loss:0.019, val_acc:0.980]
Epoch [41/120    avg_loss:0.023, val_acc:0.976]
Epoch [42/120    avg_loss:0.036, val_acc:0.974]
Epoch [43/120    avg_loss:0.035, val_acc:0.978]
Epoch [44/120    avg_loss:0.034, val_acc:0.962]
Epoch [45/120    avg_loss:0.040, val_acc:0.976]
Epoch [46/120    avg_loss:0.026, val_acc:0.984]
Epoch [47/120    avg_loss:0.022, val_acc:0.983]
Epoch [48/120    avg_loss:0.016, val_acc:0.982]
Epoch [49/120    avg_loss:0.015, val_acc:0.983]
Epoch [50/120    avg_loss:0.013, val_acc:0.974]
Epoch [51/120    avg_loss:0.044, val_acc:0.963]
Epoch [52/120    avg_loss:0.050, val_acc:0.952]
Epoch [53/120    avg_loss:0.041, val_acc:0.977]
Epoch [54/120    avg_loss:0.038, val_acc:0.972]
Epoch [55/120    avg_loss:0.022, val_acc:0.981]
Epoch [56/120    avg_loss:0.016, val_acc:0.982]
Epoch [57/120    avg_loss:0.022, val_acc:0.982]
Epoch [58/120    avg_loss:0.016, val_acc:0.985]
Epoch [59/120    avg_loss:0.024, val_acc:0.980]
Epoch [60/120    avg_loss:0.016, val_acc:0.983]
Epoch [61/120    avg_loss:0.015, val_acc:0.982]
Epoch [62/120    avg_loss:0.012, val_acc:0.984]
Epoch [63/120    avg_loss:0.010, val_acc:0.989]
Epoch [64/120    avg_loss:0.042, val_acc:0.973]
Epoch [65/120    avg_loss:0.017, val_acc:0.983]
Epoch [66/120    avg_loss:0.061, val_acc:0.950]
Epoch [67/120    avg_loss:0.065, val_acc:0.968]
Epoch [68/120    avg_loss:0.022, val_acc:0.978]
Epoch [69/120    avg_loss:0.021, val_acc:0.985]
Epoch [70/120    avg_loss:0.013, val_acc:0.982]
Epoch [71/120    avg_loss:0.026, val_acc:0.985]
Epoch [72/120    avg_loss:0.016, val_acc:0.986]
Epoch [73/120    avg_loss:0.016, val_acc:0.974]
Epoch [74/120    avg_loss:0.015, val_acc:0.980]
Epoch [75/120    avg_loss:0.013, val_acc:0.984]
Epoch [76/120    avg_loss:0.020, val_acc:0.979]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.990]
Epoch [80/120    avg_loss:0.009, val_acc:0.990]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.006, val_acc:0.989]
Epoch [83/120    avg_loss:0.008, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.008, val_acc:0.991]
Epoch [86/120    avg_loss:0.007, val_acc:0.991]
Epoch [87/120    avg_loss:0.008, val_acc:0.991]
Epoch [88/120    avg_loss:0.009, val_acc:0.991]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.010, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.991]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.991]
Epoch [98/120    avg_loss:0.008, val_acc:0.991]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.008, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.007, val_acc:0.991]
Epoch [110/120    avg_loss:0.006, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.008, val_acc:0.991]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6366     0     2     0     0    19     0    38     7]
 [    0     5 18050     0    14     0     4     0    17     0]
 [    0     2     0  1981     1     0     0     0    50     2]
 [    0    19     4     0  2939     0     2     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     0     0  4854     0     0     0]
 [    0     8     0     0     0     0     2  1277     0     3]
 [    0    11     0     5    53     0     0     0  3502     0]
 [    0     0     2     0     2    21     0     0     0   894]]

Accuracy:
99.2167353529511

F1 scores:
[       nan 0.99135716 0.99806469 0.98459245 0.9827788  0.99201824
 0.99477405 0.99493572 0.97467298 0.97972603]

Kappa:
0.9896223348942261
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f775260b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.785, val_acc:0.402]
Epoch [2/120    avg_loss:1.298, val_acc:0.463]
Epoch [3/120    avg_loss:1.063, val_acc:0.657]
Epoch [4/120    avg_loss:0.858, val_acc:0.654]
Epoch [5/120    avg_loss:0.700, val_acc:0.678]
Epoch [6/120    avg_loss:0.577, val_acc:0.718]
Epoch [7/120    avg_loss:0.507, val_acc:0.744]
Epoch [8/120    avg_loss:0.392, val_acc:0.807]
Epoch [9/120    avg_loss:0.342, val_acc:0.831]
Epoch [10/120    avg_loss:0.259, val_acc:0.900]
Epoch [11/120    avg_loss:0.261, val_acc:0.837]
Epoch [12/120    avg_loss:0.193, val_acc:0.898]
Epoch [13/120    avg_loss:0.171, val_acc:0.938]
Epoch [14/120    avg_loss:0.178, val_acc:0.949]
Epoch [15/120    avg_loss:0.108, val_acc:0.882]
Epoch [16/120    avg_loss:0.104, val_acc:0.959]
Epoch [17/120    avg_loss:0.093, val_acc:0.942]
Epoch [18/120    avg_loss:0.088, val_acc:0.945]
Epoch [19/120    avg_loss:0.093, val_acc:0.928]
Epoch [20/120    avg_loss:0.077, val_acc:0.947]
Epoch [21/120    avg_loss:0.069, val_acc:0.948]
Epoch [22/120    avg_loss:0.064, val_acc:0.964]
Epoch [23/120    avg_loss:0.048, val_acc:0.958]
Epoch [24/120    avg_loss:0.042, val_acc:0.955]
Epoch [25/120    avg_loss:0.037, val_acc:0.967]
Epoch [26/120    avg_loss:0.043, val_acc:0.971]
Epoch [27/120    avg_loss:0.063, val_acc:0.972]
Epoch [28/120    avg_loss:0.049, val_acc:0.969]
Epoch [29/120    avg_loss:0.026, val_acc:0.983]
Epoch [30/120    avg_loss:0.026, val_acc:0.974]
Epoch [31/120    avg_loss:0.041, val_acc:0.976]
Epoch [32/120    avg_loss:0.038, val_acc:0.959]
Epoch [33/120    avg_loss:0.041, val_acc:0.972]
Epoch [34/120    avg_loss:0.048, val_acc:0.973]
Epoch [35/120    avg_loss:0.059, val_acc:0.966]
Epoch [36/120    avg_loss:0.053, val_acc:0.965]
Epoch [37/120    avg_loss:0.025, val_acc:0.972]
Epoch [38/120    avg_loss:0.021, val_acc:0.979]
Epoch [39/120    avg_loss:0.022, val_acc:0.972]
Epoch [40/120    avg_loss:0.019, val_acc:0.983]
Epoch [41/120    avg_loss:0.017, val_acc:0.951]
Epoch [42/120    avg_loss:0.021, val_acc:0.978]
Epoch [43/120    avg_loss:0.013, val_acc:0.983]
Epoch [44/120    avg_loss:0.015, val_acc:0.981]
Epoch [45/120    avg_loss:0.013, val_acc:0.986]
Epoch [46/120    avg_loss:0.010, val_acc:0.984]
Epoch [47/120    avg_loss:0.016, val_acc:0.980]
Epoch [48/120    avg_loss:0.016, val_acc:0.979]
Epoch [49/120    avg_loss:0.015, val_acc:0.983]
Epoch [50/120    avg_loss:0.013, val_acc:0.985]
Epoch [51/120    avg_loss:0.008, val_acc:0.986]
Epoch [52/120    avg_loss:0.008, val_acc:0.988]
Epoch [53/120    avg_loss:0.008, val_acc:0.985]
Epoch [54/120    avg_loss:0.014, val_acc:0.975]
Epoch [55/120    avg_loss:0.008, val_acc:0.984]
Epoch [56/120    avg_loss:0.008, val_acc:0.981]
Epoch [57/120    avg_loss:0.019, val_acc:0.961]
Epoch [58/120    avg_loss:0.013, val_acc:0.986]
Epoch [59/120    avg_loss:0.021, val_acc:0.977]
Epoch [60/120    avg_loss:0.031, val_acc:0.969]
Epoch [61/120    avg_loss:0.016, val_acc:0.974]
Epoch [62/120    avg_loss:0.027, val_acc:0.982]
Epoch [63/120    avg_loss:0.032, val_acc:0.978]
Epoch [64/120    avg_loss:0.010, val_acc:0.983]
Epoch [65/120    avg_loss:0.008, val_acc:0.986]
Epoch [66/120    avg_loss:0.006, val_acc:0.984]
Epoch [67/120    avg_loss:0.007, val_acc:0.984]
Epoch [68/120    avg_loss:0.007, val_acc:0.984]
Epoch [69/120    avg_loss:0.008, val_acc:0.984]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.984]
Epoch [72/120    avg_loss:0.006, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.984]
Epoch [74/120    avg_loss:0.005, val_acc:0.983]
Epoch [75/120    avg_loss:0.009, val_acc:0.983]
Epoch [76/120    avg_loss:0.006, val_acc:0.983]
Epoch [77/120    avg_loss:0.006, val_acc:0.983]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.006, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.983]
Epoch [81/120    avg_loss:0.007, val_acc:0.983]
Epoch [82/120    avg_loss:0.006, val_acc:0.983]
Epoch [83/120    avg_loss:0.005, val_acc:0.983]
Epoch [84/120    avg_loss:0.006, val_acc:0.983]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.983]
Epoch [87/120    avg_loss:0.004, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.006, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.007, val_acc:0.983]
Epoch [97/120    avg_loss:0.005, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.983]
Epoch [104/120    avg_loss:0.005, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.983]
Epoch [112/120    avg_loss:0.006, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     3     0     0     0    17    23     0]
 [    0     2 18049     0    32     0     0     0     7     0]
 [    0     9     0  2015     0     0     0     0     5     7]
 [    0    38    13     0  2889     0     7     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     4     0     0  4862     0     1     0]
 [    0     0     0     0     0     0     3  1286     0     1]
 [    0     5     0    25    51     0     0     0  3487     3]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.20468512761188

F1 scores:
[       nan 0.99246602 0.99820258 0.98701935 0.96978852 0.99088838
 0.99733333 0.99190127 0.97963197 0.97294313]

Kappa:
0.9894637131900951
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7fd18727f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.733, val_acc:0.283]
Epoch [2/120    avg_loss:1.254, val_acc:0.384]
Epoch [3/120    avg_loss:1.029, val_acc:0.553]
Epoch [4/120    avg_loss:0.856, val_acc:0.576]
Epoch [5/120    avg_loss:0.721, val_acc:0.674]
Epoch [6/120    avg_loss:0.609, val_acc:0.742]
Epoch [7/120    avg_loss:0.458, val_acc:0.744]
Epoch [8/120    avg_loss:0.384, val_acc:0.808]
Epoch [9/120    avg_loss:0.316, val_acc:0.876]
Epoch [10/120    avg_loss:0.303, val_acc:0.879]
Epoch [11/120    avg_loss:0.260, val_acc:0.858]
Epoch [12/120    avg_loss:0.192, val_acc:0.904]
Epoch [13/120    avg_loss:0.207, val_acc:0.928]
Epoch [14/120    avg_loss:0.160, val_acc:0.937]
Epoch [15/120    avg_loss:0.174, val_acc:0.903]
Epoch [16/120    avg_loss:0.153, val_acc:0.905]
Epoch [17/120    avg_loss:0.125, val_acc:0.959]
Epoch [18/120    avg_loss:0.094, val_acc:0.953]
Epoch [19/120    avg_loss:0.087, val_acc:0.966]
Epoch [20/120    avg_loss:0.089, val_acc:0.942]
Epoch [21/120    avg_loss:0.055, val_acc:0.972]
Epoch [22/120    avg_loss:0.064, val_acc:0.959]
Epoch [23/120    avg_loss:0.080, val_acc:0.955]
Epoch [24/120    avg_loss:0.078, val_acc:0.968]
Epoch [25/120    avg_loss:0.055, val_acc:0.942]
Epoch [26/120    avg_loss:0.054, val_acc:0.970]
Epoch [27/120    avg_loss:0.084, val_acc:0.966]
Epoch [28/120    avg_loss:0.066, val_acc:0.959]
Epoch [29/120    avg_loss:0.059, val_acc:0.966]
Epoch [30/120    avg_loss:0.043, val_acc:0.972]
Epoch [31/120    avg_loss:0.032, val_acc:0.968]
Epoch [32/120    avg_loss:0.030, val_acc:0.973]
Epoch [33/120    avg_loss:0.059, val_acc:0.969]
Epoch [34/120    avg_loss:0.035, val_acc:0.975]
Epoch [35/120    avg_loss:0.023, val_acc:0.979]
Epoch [36/120    avg_loss:0.024, val_acc:0.979]
Epoch [37/120    avg_loss:0.019, val_acc:0.983]
Epoch [38/120    avg_loss:0.018, val_acc:0.976]
Epoch [39/120    avg_loss:0.014, val_acc:0.974]
Epoch [40/120    avg_loss:0.020, val_acc:0.980]
Epoch [41/120    avg_loss:0.017, val_acc:0.981]
Epoch [42/120    avg_loss:0.016, val_acc:0.978]
Epoch [43/120    avg_loss:0.016, val_acc:0.978]
Epoch [44/120    avg_loss:0.013, val_acc:0.980]
Epoch [45/120    avg_loss:0.010, val_acc:0.983]
Epoch [46/120    avg_loss:0.022, val_acc:0.978]
Epoch [47/120    avg_loss:0.017, val_acc:0.970]
Epoch [48/120    avg_loss:0.033, val_acc:0.968]
Epoch [49/120    avg_loss:0.018, val_acc:0.982]
Epoch [50/120    avg_loss:0.013, val_acc:0.981]
Epoch [51/120    avg_loss:0.011, val_acc:0.982]
Epoch [52/120    avg_loss:0.012, val_acc:0.973]
Epoch [53/120    avg_loss:0.024, val_acc:0.978]
Epoch [54/120    avg_loss:0.029, val_acc:0.975]
Epoch [55/120    avg_loss:0.022, val_acc:0.978]
Epoch [56/120    avg_loss:0.016, val_acc:0.983]
Epoch [57/120    avg_loss:0.013, val_acc:0.982]
Epoch [58/120    avg_loss:0.010, val_acc:0.978]
Epoch [59/120    avg_loss:0.010, val_acc:0.975]
Epoch [60/120    avg_loss:0.006, val_acc:0.984]
Epoch [61/120    avg_loss:0.007, val_acc:0.984]
Epoch [62/120    avg_loss:0.007, val_acc:0.984]
Epoch [63/120    avg_loss:0.015, val_acc:0.982]
Epoch [64/120    avg_loss:0.012, val_acc:0.979]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.005, val_acc:0.985]
Epoch [67/120    avg_loss:0.021, val_acc:0.948]
Epoch [68/120    avg_loss:0.024, val_acc:0.979]
Epoch [69/120    avg_loss:0.033, val_acc:0.978]
Epoch [70/120    avg_loss:0.046, val_acc:0.970]
Epoch [71/120    avg_loss:0.043, val_acc:0.980]
Epoch [72/120    avg_loss:0.020, val_acc:0.974]
Epoch [73/120    avg_loss:0.016, val_acc:0.974]
Epoch [74/120    avg_loss:0.010, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.016, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.981]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.960]
Epoch [81/120    avg_loss:0.020, val_acc:0.972]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.982]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.005, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.984]
Epoch [87/120    avg_loss:0.004, val_acc:0.984]
Epoch [88/120    avg_loss:0.004, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.980]
Epoch [91/120    avg_loss:0.005, val_acc:0.984]
Epoch [92/120    avg_loss:0.014, val_acc:0.984]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.003, val_acc:0.985]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.003, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.985]
Epoch [102/120    avg_loss:0.002, val_acc:0.986]
Epoch [103/120    avg_loss:0.003, val_acc:0.987]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.003, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.002, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.002, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.003, val_acc:0.988]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6388     0     0     0     0    26    15     3     0]
 [    0     0 18073     0    12     0     2     0     3     0]
 [    0     0     0  2015     0     0     0     0    16     5]
 [    0    32    11     1  2908     0     0     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     3     0     0  4866     0     3     0]
 [    0     0     0     0     0     0     2  1281     0     7]
 [    0     0     0    27    50     0     0     0  3490     4]
 [    0     0     0     0    14    38     0     0     0   867]]

Accuracy:
99.27698647964718

F1 scores:
[       nan 0.99408652 0.99906025 0.98726115 0.97649429 0.98564955
 0.99570289 0.99071926 0.98226851 0.96226415]

Kappa:
0.9904199118437023
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff46dc9828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.782, val_acc:0.228]
Epoch [2/120    avg_loss:1.252, val_acc:0.435]
Epoch [3/120    avg_loss:0.981, val_acc:0.516]
Epoch [4/120    avg_loss:0.805, val_acc:0.737]
Epoch [5/120    avg_loss:0.716, val_acc:0.618]
Epoch [6/120    avg_loss:0.575, val_acc:0.736]
Epoch [7/120    avg_loss:0.429, val_acc:0.784]
Epoch [8/120    avg_loss:0.371, val_acc:0.808]
Epoch [9/120    avg_loss:0.309, val_acc:0.810]
Epoch [10/120    avg_loss:0.299, val_acc:0.831]
Epoch [11/120    avg_loss:0.283, val_acc:0.835]
Epoch [12/120    avg_loss:0.224, val_acc:0.900]
Epoch [13/120    avg_loss:0.218, val_acc:0.923]
Epoch [14/120    avg_loss:0.168, val_acc:0.908]
Epoch [15/120    avg_loss:0.182, val_acc:0.869]
Epoch [16/120    avg_loss:0.148, val_acc:0.925]
Epoch [17/120    avg_loss:0.135, val_acc:0.923]
Epoch [18/120    avg_loss:0.127, val_acc:0.868]
Epoch [19/120    avg_loss:0.107, val_acc:0.940]
Epoch [20/120    avg_loss:0.105, val_acc:0.962]
Epoch [21/120    avg_loss:0.104, val_acc:0.946]
Epoch [22/120    avg_loss:0.083, val_acc:0.971]
Epoch [23/120    avg_loss:0.059, val_acc:0.945]
Epoch [24/120    avg_loss:0.071, val_acc:0.964]
Epoch [25/120    avg_loss:0.055, val_acc:0.975]
Epoch [26/120    avg_loss:0.057, val_acc:0.971]
Epoch [27/120    avg_loss:0.058, val_acc:0.967]
Epoch [28/120    avg_loss:0.048, val_acc:0.971]
Epoch [29/120    avg_loss:0.095, val_acc:0.957]
Epoch [30/120    avg_loss:0.060, val_acc:0.975]
Epoch [31/120    avg_loss:0.052, val_acc:0.970]
Epoch [32/120    avg_loss:0.066, val_acc:0.973]
Epoch [33/120    avg_loss:0.059, val_acc:0.962]
Epoch [34/120    avg_loss:0.034, val_acc:0.970]
Epoch [35/120    avg_loss:0.032, val_acc:0.977]
Epoch [36/120    avg_loss:0.023, val_acc:0.983]
Epoch [37/120    avg_loss:0.027, val_acc:0.979]
Epoch [38/120    avg_loss:0.024, val_acc:0.983]
Epoch [39/120    avg_loss:0.022, val_acc:0.983]
Epoch [40/120    avg_loss:0.026, val_acc:0.986]
Epoch [41/120    avg_loss:0.022, val_acc:0.986]
Epoch [42/120    avg_loss:0.038, val_acc:0.979]
Epoch [43/120    avg_loss:0.029, val_acc:0.983]
Epoch [44/120    avg_loss:0.027, val_acc:0.988]
Epoch [45/120    avg_loss:0.025, val_acc:0.985]
Epoch [46/120    avg_loss:0.034, val_acc:0.980]
Epoch [47/120    avg_loss:0.035, val_acc:0.983]
Epoch [48/120    avg_loss:0.039, val_acc:0.947]
Epoch [49/120    avg_loss:0.026, val_acc:0.985]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.012, val_acc:0.988]
Epoch [52/120    avg_loss:0.015, val_acc:0.986]
Epoch [53/120    avg_loss:0.013, val_acc:0.988]
Epoch [54/120    avg_loss:0.012, val_acc:0.990]
Epoch [55/120    avg_loss:0.013, val_acc:0.988]
Epoch [56/120    avg_loss:0.011, val_acc:0.988]
Epoch [57/120    avg_loss:0.009, val_acc:0.988]
Epoch [58/120    avg_loss:0.032, val_acc:0.972]
Epoch [59/120    avg_loss:0.021, val_acc:0.986]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.023, val_acc:0.988]
Epoch [62/120    avg_loss:0.010, val_acc:0.989]
Epoch [63/120    avg_loss:0.010, val_acc:0.985]
Epoch [64/120    avg_loss:0.016, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.988]
Epoch [66/120    avg_loss:0.007, val_acc:0.991]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.010, val_acc:0.970]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.020, val_acc:0.980]
Epoch [71/120    avg_loss:0.020, val_acc:0.972]
Epoch [72/120    avg_loss:0.012, val_acc:0.990]
Epoch [73/120    avg_loss:0.009, val_acc:0.986]
Epoch [74/120    avg_loss:0.014, val_acc:0.981]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.993]
Epoch [79/120    avg_loss:0.006, val_acc:0.988]
Epoch [80/120    avg_loss:0.007, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.989]
Epoch [82/120    avg_loss:0.005, val_acc:0.991]
Epoch [83/120    avg_loss:0.006, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.004, val_acc:0.991]
Epoch [91/120    avg_loss:0.004, val_acc:0.991]
Epoch [92/120    avg_loss:0.003, val_acc:0.991]
Epoch [93/120    avg_loss:0.003, val_acc:0.992]
Epoch [94/120    avg_loss:0.004, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.990]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.003, val_acc:0.991]
Epoch [99/120    avg_loss:0.003, val_acc:0.991]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.992]
Epoch [102/120    avg_loss:0.004, val_acc:0.992]
Epoch [103/120    avg_loss:0.003, val_acc:0.991]
Epoch [104/120    avg_loss:0.003, val_acc:0.992]
Epoch [105/120    avg_loss:0.003, val_acc:0.992]
Epoch [106/120    avg_loss:0.003, val_acc:0.992]
Epoch [107/120    avg_loss:0.003, val_acc:0.992]
Epoch [108/120    avg_loss:0.003, val_acc:0.992]
Epoch [109/120    avg_loss:0.003, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.002, val_acc:0.992]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.004, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.004, val_acc:0.992]
Epoch [116/120    avg_loss:0.003, val_acc:0.992]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.003, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     0     0     0     0     1     9     1]
 [    0     0 18045     0    35     0     2     0     8     0]
 [    0     0     0  2016     2     0     0     0    15     3]
 [    0    40     9     0  2906     0     1     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     6     0     0  4867     0     3     0]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0     7     0     4    54     0     0     0  3499     7]
 [    0     0     0     0    12    28     0     0     0   879]]

Accuracy:
99.34928783168246

F1 scores:
[       nan 0.99550388 0.99845073 0.99261448 0.97174386 0.9893859
 0.99835897 0.99767081 0.98272715 0.97019868]

Kappa:
0.9913802624911044
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3b27927b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.773, val_acc:0.258]
Epoch [2/120    avg_loss:1.259, val_acc:0.364]
Epoch [3/120    avg_loss:1.039, val_acc:0.481]
Epoch [4/120    avg_loss:0.855, val_acc:0.781]
Epoch [5/120    avg_loss:0.732, val_acc:0.748]
Epoch [6/120    avg_loss:0.621, val_acc:0.828]
Epoch [7/120    avg_loss:0.499, val_acc:0.747]
Epoch [8/120    avg_loss:0.400, val_acc:0.820]
Epoch [9/120    avg_loss:0.346, val_acc:0.839]
Epoch [10/120    avg_loss:0.271, val_acc:0.817]
Epoch [11/120    avg_loss:0.243, val_acc:0.866]
Epoch [12/120    avg_loss:0.229, val_acc:0.908]
Epoch [13/120    avg_loss:0.220, val_acc:0.894]
Epoch [14/120    avg_loss:0.195, val_acc:0.938]
Epoch [15/120    avg_loss:0.144, val_acc:0.971]
Epoch [16/120    avg_loss:0.128, val_acc:0.922]
Epoch [17/120    avg_loss:0.115, val_acc:0.957]
Epoch [18/120    avg_loss:0.106, val_acc:0.965]
Epoch [19/120    avg_loss:0.079, val_acc:0.966]
Epoch [20/120    avg_loss:0.083, val_acc:0.955]
Epoch [21/120    avg_loss:0.107, val_acc:0.958]
Epoch [22/120    avg_loss:0.079, val_acc:0.975]
Epoch [23/120    avg_loss:0.070, val_acc:0.958]
Epoch [24/120    avg_loss:0.050, val_acc:0.975]
Epoch [25/120    avg_loss:0.050, val_acc:0.978]
Epoch [26/120    avg_loss:0.080, val_acc:0.967]
Epoch [27/120    avg_loss:0.097, val_acc:0.977]
Epoch [28/120    avg_loss:0.076, val_acc:0.960]
Epoch [29/120    avg_loss:0.056, val_acc:0.967]
Epoch [30/120    avg_loss:0.050, val_acc:0.976]
Epoch [31/120    avg_loss:0.032, val_acc:0.980]
Epoch [32/120    avg_loss:0.032, val_acc:0.984]
Epoch [33/120    avg_loss:0.028, val_acc:0.980]
Epoch [34/120    avg_loss:0.026, val_acc:0.984]
Epoch [35/120    avg_loss:0.031, val_acc:0.984]
Epoch [36/120    avg_loss:0.033, val_acc:0.983]
Epoch [37/120    avg_loss:0.017, val_acc:0.985]
Epoch [38/120    avg_loss:0.016, val_acc:0.984]
Epoch [39/120    avg_loss:0.037, val_acc:0.952]
Epoch [40/120    avg_loss:0.060, val_acc:0.978]
Epoch [41/120    avg_loss:0.055, val_acc:0.978]
Epoch [42/120    avg_loss:0.045, val_acc:0.977]
Epoch [43/120    avg_loss:0.024, val_acc:0.984]
Epoch [44/120    avg_loss:0.023, val_acc:0.978]
Epoch [45/120    avg_loss:0.025, val_acc:0.986]
Epoch [46/120    avg_loss:0.025, val_acc:0.985]
Epoch [47/120    avg_loss:0.023, val_acc:0.978]
Epoch [48/120    avg_loss:0.018, val_acc:0.980]
Epoch [49/120    avg_loss:0.024, val_acc:0.985]
Epoch [50/120    avg_loss:0.018, val_acc:0.984]
Epoch [51/120    avg_loss:0.020, val_acc:0.986]
Epoch [52/120    avg_loss:0.020, val_acc:0.981]
Epoch [53/120    avg_loss:0.015, val_acc:0.979]
Epoch [54/120    avg_loss:0.021, val_acc:0.984]
Epoch [55/120    avg_loss:0.049, val_acc:0.961]
Epoch [56/120    avg_loss:0.034, val_acc:0.972]
Epoch [57/120    avg_loss:0.037, val_acc:0.981]
Epoch [58/120    avg_loss:0.017, val_acc:0.988]
Epoch [59/120    avg_loss:0.013, val_acc:0.972]
Epoch [60/120    avg_loss:0.025, val_acc:0.978]
Epoch [61/120    avg_loss:0.016, val_acc:0.986]
Epoch [62/120    avg_loss:0.011, val_acc:0.986]
Epoch [63/120    avg_loss:0.020, val_acc:0.988]
Epoch [64/120    avg_loss:0.018, val_acc:0.991]
Epoch [65/120    avg_loss:0.009, val_acc:0.988]
Epoch [66/120    avg_loss:0.009, val_acc:0.990]
Epoch [67/120    avg_loss:0.011, val_acc:0.965]
Epoch [68/120    avg_loss:0.013, val_acc:0.991]
Epoch [69/120    avg_loss:0.010, val_acc:0.990]
Epoch [70/120    avg_loss:0.010, val_acc:0.991]
Epoch [71/120    avg_loss:0.007, val_acc:0.989]
Epoch [72/120    avg_loss:0.008, val_acc:0.992]
Epoch [73/120    avg_loss:0.008, val_acc:0.992]
Epoch [74/120    avg_loss:0.006, val_acc:0.991]
Epoch [75/120    avg_loss:0.007, val_acc:0.979]
Epoch [76/120    avg_loss:0.014, val_acc:0.985]
Epoch [77/120    avg_loss:0.006, val_acc:0.990]
Epoch [78/120    avg_loss:0.007, val_acc:0.983]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.990]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.993]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.016, val_acc:0.991]
Epoch [89/120    avg_loss:0.025, val_acc:0.973]
Epoch [90/120    avg_loss:0.018, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.981]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.994]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.972]
Epoch [104/120    avg_loss:0.006, val_acc:0.992]
Epoch [105/120    avg_loss:0.003, val_acc:0.991]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.993]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.003, val_acc:0.993]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.002, val_acc:0.994]
Epoch [114/120    avg_loss:0.002, val_acc:0.994]
Epoch [115/120    avg_loss:0.003, val_acc:0.994]
Epoch [116/120    avg_loss:0.003, val_acc:0.994]
Epoch [117/120    avg_loss:0.002, val_acc:0.994]
Epoch [118/120    avg_loss:0.003, val_acc:0.993]
Epoch [119/120    avg_loss:0.003, val_acc:0.994]
Epoch [120/120    avg_loss:0.002, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     0     1     0     0    30     0     2]
 [    0     0 18079     0     4     0     2     0     5     0]
 [    0     0     0  2015     2     0     0     0    18     1]
 [    0    33     5     0  2911     0     2     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4866     0     8     0]
 [    0     0     0     0     0     0     4  1284     0     2]
 [    0     0     0    24    50     0     0     0  3485    12]
 [    0     0     0     0    14    41     0     0     0   864]]

Accuracy:
99.31313715566482

F1 scores:
[       nan 0.9948694  0.99955769 0.98798725 0.97783003 0.98453414
 0.99794914 0.98617512 0.98058526 0.96      ]

Kappa:
0.9908997446831355
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdaf1934828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.845, val_acc:0.495]
Epoch [2/120    avg_loss:1.265, val_acc:0.485]
Epoch [3/120    avg_loss:1.024, val_acc:0.524]
Epoch [4/120    avg_loss:0.871, val_acc:0.643]
Epoch [5/120    avg_loss:0.730, val_acc:0.512]
Epoch [6/120    avg_loss:0.656, val_acc:0.635]
Epoch [7/120    avg_loss:0.522, val_acc:0.695]
Epoch [8/120    avg_loss:0.421, val_acc:0.752]
Epoch [9/120    avg_loss:0.350, val_acc:0.752]
Epoch [10/120    avg_loss:0.318, val_acc:0.768]
Epoch [11/120    avg_loss:0.320, val_acc:0.853]
Epoch [12/120    avg_loss:0.255, val_acc:0.778]
Epoch [13/120    avg_loss:0.250, val_acc:0.830]
Epoch [14/120    avg_loss:0.201, val_acc:0.828]
Epoch [15/120    avg_loss:0.186, val_acc:0.877]
Epoch [16/120    avg_loss:0.176, val_acc:0.843]
Epoch [17/120    avg_loss:0.197, val_acc:0.917]
Epoch [18/120    avg_loss:0.181, val_acc:0.882]
Epoch [19/120    avg_loss:0.128, val_acc:0.908]
Epoch [20/120    avg_loss:0.104, val_acc:0.934]
Epoch [21/120    avg_loss:0.095, val_acc:0.919]
Epoch [22/120    avg_loss:0.082, val_acc:0.968]
Epoch [23/120    avg_loss:0.066, val_acc:0.968]
Epoch [24/120    avg_loss:0.067, val_acc:0.922]
Epoch [25/120    avg_loss:0.064, val_acc:0.932]
Epoch [26/120    avg_loss:0.058, val_acc:0.940]
Epoch [27/120    avg_loss:0.058, val_acc:0.955]
Epoch [28/120    avg_loss:0.075, val_acc:0.965]
Epoch [29/120    avg_loss:0.079, val_acc:0.948]
Epoch [30/120    avg_loss:0.077, val_acc:0.958]
Epoch [31/120    avg_loss:0.051, val_acc:0.887]
Epoch [32/120    avg_loss:0.048, val_acc:0.973]
Epoch [33/120    avg_loss:0.064, val_acc:0.921]
Epoch [34/120    avg_loss:0.070, val_acc:0.964]
Epoch [35/120    avg_loss:0.057, val_acc:0.939]
Epoch [36/120    avg_loss:0.049, val_acc:0.973]
Epoch [37/120    avg_loss:0.049, val_acc:0.955]
Epoch [38/120    avg_loss:0.043, val_acc:0.979]
Epoch [39/120    avg_loss:0.035, val_acc:0.968]
Epoch [40/120    avg_loss:0.027, val_acc:0.979]
Epoch [41/120    avg_loss:0.027, val_acc:0.968]
Epoch [42/120    avg_loss:0.024, val_acc:0.980]
Epoch [43/120    avg_loss:0.016, val_acc:0.983]
Epoch [44/120    avg_loss:0.031, val_acc:0.973]
Epoch [45/120    avg_loss:0.036, val_acc:0.932]
Epoch [46/120    avg_loss:0.042, val_acc:0.983]
Epoch [47/120    avg_loss:0.030, val_acc:0.985]
Epoch [48/120    avg_loss:0.014, val_acc:0.988]
Epoch [49/120    avg_loss:0.020, val_acc:0.979]
Epoch [50/120    avg_loss:0.034, val_acc:0.966]
Epoch [51/120    avg_loss:0.021, val_acc:0.988]
Epoch [52/120    avg_loss:0.019, val_acc:0.982]
Epoch [53/120    avg_loss:0.017, val_acc:0.988]
Epoch [54/120    avg_loss:0.014, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.973]
Epoch [56/120    avg_loss:0.014, val_acc:0.983]
Epoch [57/120    avg_loss:0.013, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.979]
Epoch [59/120    avg_loss:0.012, val_acc:0.989]
Epoch [60/120    avg_loss:0.009, val_acc:0.988]
Epoch [61/120    avg_loss:0.011, val_acc:0.989]
Epoch [62/120    avg_loss:0.015, val_acc:0.983]
Epoch [63/120    avg_loss:0.010, val_acc:0.983]
Epoch [64/120    avg_loss:0.008, val_acc:0.991]
Epoch [65/120    avg_loss:0.007, val_acc:0.990]
Epoch [66/120    avg_loss:0.007, val_acc:0.992]
Epoch [67/120    avg_loss:0.007, val_acc:0.986]
Epoch [68/120    avg_loss:0.007, val_acc:0.985]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.981]
Epoch [71/120    avg_loss:0.008, val_acc:0.990]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.012, val_acc:0.984]
Epoch [74/120    avg_loss:0.007, val_acc:0.972]
Epoch [75/120    avg_loss:0.009, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.991]
Epoch [77/120    avg_loss:0.005, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.989]
Epoch [80/120    avg_loss:0.004, val_acc:0.991]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.003, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.004, val_acc:0.990]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.003, val_acc:0.990]
Epoch [88/120    avg_loss:0.004, val_acc:0.990]
Epoch [89/120    avg_loss:0.003, val_acc:0.990]
Epoch [90/120    avg_loss:0.004, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.989]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.003, val_acc:0.989]
Epoch [94/120    avg_loss:0.003, val_acc:0.989]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.989]
Epoch [112/120    avg_loss:0.003, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.003, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     0     0     0     5     2    78     0]
 [    0     2 18052     0    22     0     2     0    12     0]
 [    0     0     0  2007     4     0     0     0    19     6]
 [    0    31    11     0  2910     0     2     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     4     0     0  4850     0    19     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0    30     0     1    43     0     0     0  3494     3]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.1323837755766

F1 scores:
[       nan 0.98847532 0.99850655 0.99160079 0.97569153 0.99088838
 0.99599548 0.99806126 0.96907502 0.97348066]

Kappa:
0.9885067291515495
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fe59777f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.802, val_acc:0.403]
Epoch [2/120    avg_loss:1.255, val_acc:0.516]
Epoch [3/120    avg_loss:1.002, val_acc:0.684]
Epoch [4/120    avg_loss:0.833, val_acc:0.668]
Epoch [5/120    avg_loss:0.683, val_acc:0.724]
Epoch [6/120    avg_loss:0.522, val_acc:0.747]
Epoch [7/120    avg_loss:0.440, val_acc:0.780]
Epoch [8/120    avg_loss:0.367, val_acc:0.805]
Epoch [9/120    avg_loss:0.321, val_acc:0.816]
Epoch [10/120    avg_loss:0.263, val_acc:0.797]
Epoch [11/120    avg_loss:0.240, val_acc:0.872]
Epoch [12/120    avg_loss:0.202, val_acc:0.879]
Epoch [13/120    avg_loss:0.174, val_acc:0.920]
Epoch [14/120    avg_loss:0.156, val_acc:0.933]
Epoch [15/120    avg_loss:0.146, val_acc:0.953]
Epoch [16/120    avg_loss:0.136, val_acc:0.928]
Epoch [17/120    avg_loss:0.120, val_acc:0.933]
Epoch [18/120    avg_loss:0.112, val_acc:0.903]
Epoch [19/120    avg_loss:0.083, val_acc:0.970]
Epoch [20/120    avg_loss:0.075, val_acc:0.967]
Epoch [21/120    avg_loss:0.072, val_acc:0.968]
Epoch [22/120    avg_loss:0.058, val_acc:0.974]
Epoch [23/120    avg_loss:0.053, val_acc:0.984]
Epoch [24/120    avg_loss:0.051, val_acc:0.978]
Epoch [25/120    avg_loss:0.050, val_acc:0.980]
Epoch [26/120    avg_loss:0.052, val_acc:0.960]
Epoch [27/120    avg_loss:0.050, val_acc:0.949]
Epoch [28/120    avg_loss:0.060, val_acc:0.984]
Epoch [29/120    avg_loss:0.035, val_acc:0.984]
Epoch [30/120    avg_loss:0.034, val_acc:0.981]
Epoch [31/120    avg_loss:0.038, val_acc:0.973]
Epoch [32/120    avg_loss:0.065, val_acc:0.968]
Epoch [33/120    avg_loss:0.075, val_acc:0.963]
Epoch [34/120    avg_loss:0.055, val_acc:0.973]
Epoch [35/120    avg_loss:0.041, val_acc:0.977]
Epoch [36/120    avg_loss:0.026, val_acc:0.978]
Epoch [37/120    avg_loss:0.020, val_acc:0.986]
Epoch [38/120    avg_loss:0.026, val_acc:0.987]
Epoch [39/120    avg_loss:0.016, val_acc:0.987]
Epoch [40/120    avg_loss:0.018, val_acc:0.986]
Epoch [41/120    avg_loss:0.020, val_acc:0.986]
Epoch [42/120    avg_loss:0.016, val_acc:0.987]
Epoch [43/120    avg_loss:0.017, val_acc:0.989]
Epoch [44/120    avg_loss:0.015, val_acc:0.990]
Epoch [45/120    avg_loss:0.016, val_acc:0.987]
Epoch [46/120    avg_loss:0.016, val_acc:0.988]
Epoch [47/120    avg_loss:0.019, val_acc:0.989]
Epoch [48/120    avg_loss:0.015, val_acc:0.988]
Epoch [49/120    avg_loss:0.019, val_acc:0.989]
Epoch [50/120    avg_loss:0.016, val_acc:0.989]
Epoch [51/120    avg_loss:0.016, val_acc:0.988]
Epoch [52/120    avg_loss:0.016, val_acc:0.986]
Epoch [53/120    avg_loss:0.018, val_acc:0.990]
Epoch [54/120    avg_loss:0.016, val_acc:0.990]
Epoch [55/120    avg_loss:0.015, val_acc:0.991]
Epoch [56/120    avg_loss:0.016, val_acc:0.990]
Epoch [57/120    avg_loss:0.015, val_acc:0.990]
Epoch [58/120    avg_loss:0.017, val_acc:0.987]
Epoch [59/120    avg_loss:0.015, val_acc:0.990]
Epoch [60/120    avg_loss:0.013, val_acc:0.991]
Epoch [61/120    avg_loss:0.011, val_acc:0.990]
Epoch [62/120    avg_loss:0.012, val_acc:0.991]
Epoch [63/120    avg_loss:0.013, val_acc:0.990]
Epoch [64/120    avg_loss:0.013, val_acc:0.991]
Epoch [65/120    avg_loss:0.012, val_acc:0.991]
Epoch [66/120    avg_loss:0.011, val_acc:0.991]
Epoch [67/120    avg_loss:0.011, val_acc:0.991]
Epoch [68/120    avg_loss:0.012, val_acc:0.991]
Epoch [69/120    avg_loss:0.018, val_acc:0.988]
Epoch [70/120    avg_loss:0.013, val_acc:0.990]
Epoch [71/120    avg_loss:0.012, val_acc:0.990]
Epoch [72/120    avg_loss:0.016, val_acc:0.991]
Epoch [73/120    avg_loss:0.012, val_acc:0.991]
Epoch [74/120    avg_loss:0.013, val_acc:0.991]
Epoch [75/120    avg_loss:0.011, val_acc:0.991]
Epoch [76/120    avg_loss:0.014, val_acc:0.990]
Epoch [77/120    avg_loss:0.013, val_acc:0.989]
Epoch [78/120    avg_loss:0.012, val_acc:0.992]
Epoch [79/120    avg_loss:0.014, val_acc:0.991]
Epoch [80/120    avg_loss:0.014, val_acc:0.991]
Epoch [81/120    avg_loss:0.013, val_acc:0.991]
Epoch [82/120    avg_loss:0.010, val_acc:0.991]
Epoch [83/120    avg_loss:0.010, val_acc:0.989]
Epoch [84/120    avg_loss:0.013, val_acc:0.990]
Epoch [85/120    avg_loss:0.011, val_acc:0.990]
Epoch [86/120    avg_loss:0.015, val_acc:0.990]
Epoch [87/120    avg_loss:0.015, val_acc:0.989]
Epoch [88/120    avg_loss:0.011, val_acc:0.990]
Epoch [89/120    avg_loss:0.013, val_acc:0.991]
Epoch [90/120    avg_loss:0.013, val_acc:0.985]
Epoch [91/120    avg_loss:0.016, val_acc:0.991]
Epoch [92/120    avg_loss:0.013, val_acc:0.991]
Epoch [93/120    avg_loss:0.011, val_acc:0.991]
Epoch [94/120    avg_loss:0.010, val_acc:0.991]
Epoch [95/120    avg_loss:0.011, val_acc:0.991]
Epoch [96/120    avg_loss:0.011, val_acc:0.991]
Epoch [97/120    avg_loss:0.013, val_acc:0.991]
Epoch [98/120    avg_loss:0.010, val_acc:0.991]
Epoch [99/120    avg_loss:0.011, val_acc:0.991]
Epoch [100/120    avg_loss:0.010, val_acc:0.991]
Epoch [101/120    avg_loss:0.008, val_acc:0.991]
Epoch [102/120    avg_loss:0.009, val_acc:0.991]
Epoch [103/120    avg_loss:0.011, val_acc:0.991]
Epoch [104/120    avg_loss:0.010, val_acc:0.991]
Epoch [105/120    avg_loss:0.013, val_acc:0.991]
Epoch [106/120    avg_loss:0.009, val_acc:0.991]
Epoch [107/120    avg_loss:0.010, val_acc:0.991]
Epoch [108/120    avg_loss:0.011, val_acc:0.991]
Epoch [109/120    avg_loss:0.011, val_acc:0.991]
Epoch [110/120    avg_loss:0.009, val_acc:0.991]
Epoch [111/120    avg_loss:0.014, val_acc:0.991]
Epoch [112/120    avg_loss:0.013, val_acc:0.991]
Epoch [113/120    avg_loss:0.013, val_acc:0.991]
Epoch [114/120    avg_loss:0.012, val_acc:0.991]
Epoch [115/120    avg_loss:0.010, val_acc:0.991]
Epoch [116/120    avg_loss:0.010, val_acc:0.991]
Epoch [117/120    avg_loss:0.010, val_acc:0.991]
Epoch [118/120    avg_loss:0.011, val_acc:0.991]
Epoch [119/120    avg_loss:0.014, val_acc:0.991]
Epoch [120/120    avg_loss:0.010, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0     0     3     0     5     3    64     1]
 [    0     1 18025     0    41     0    15     0     8     0]
 [    0     0     0  2030     0     0     0     0     1     5]
 [    0    28    14     1  2903     0     6     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     2  1280     0     8]
 [    0     1     0    37    54     0     0     0  3466    13]
 [    0     0     0     0    14    27     0     0     0   878]]

Accuracy:
99.10346323476249

F1 scores:
[       nan 0.99173038 0.99781339 0.98927875 0.96976783 0.98976109
 0.99713818 0.99494753 0.97223001 0.9627193 ]

Kappa:
0.9881287377284109
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8feea53828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.739, val_acc:0.256]
Epoch [2/120    avg_loss:1.248, val_acc:0.676]
Epoch [3/120    avg_loss:0.982, val_acc:0.729]
Epoch [4/120    avg_loss:0.874, val_acc:0.703]
Epoch [5/120    avg_loss:0.726, val_acc:0.623]
Epoch [6/120    avg_loss:0.579, val_acc:0.711]
Epoch [7/120    avg_loss:0.492, val_acc:0.723]
Epoch [8/120    avg_loss:0.394, val_acc:0.827]
Epoch [9/120    avg_loss:0.324, val_acc:0.813]
Epoch [10/120    avg_loss:0.270, val_acc:0.860]
Epoch [11/120    avg_loss:0.252, val_acc:0.893]
Epoch [12/120    avg_loss:0.233, val_acc:0.933]
Epoch [13/120    avg_loss:0.175, val_acc:0.920]
Epoch [14/120    avg_loss:0.128, val_acc:0.920]
Epoch [15/120    avg_loss:0.246, val_acc:0.912]
Epoch [16/120    avg_loss:0.154, val_acc:0.932]
Epoch [17/120    avg_loss:0.134, val_acc:0.950]
Epoch [18/120    avg_loss:0.134, val_acc:0.942]
Epoch [19/120    avg_loss:0.120, val_acc:0.963]
Epoch [20/120    avg_loss:0.085, val_acc:0.970]
Epoch [21/120    avg_loss:0.073, val_acc:0.938]
Epoch [22/120    avg_loss:0.061, val_acc:0.963]
Epoch [23/120    avg_loss:0.055, val_acc:0.968]
Epoch [24/120    avg_loss:0.057, val_acc:0.946]
Epoch [25/120    avg_loss:0.069, val_acc:0.973]
Epoch [26/120    avg_loss:0.053, val_acc:0.975]
Epoch [27/120    avg_loss:0.051, val_acc:0.973]
Epoch [28/120    avg_loss:0.043, val_acc:0.966]
Epoch [29/120    avg_loss:0.029, val_acc:0.981]
Epoch [30/120    avg_loss:0.022, val_acc:0.979]
Epoch [31/120    avg_loss:0.024, val_acc:0.974]
Epoch [32/120    avg_loss:0.027, val_acc:0.978]
Epoch [33/120    avg_loss:0.033, val_acc:0.964]
Epoch [34/120    avg_loss:0.026, val_acc:0.979]
Epoch [35/120    avg_loss:0.028, val_acc:0.981]
Epoch [36/120    avg_loss:0.022, val_acc:0.981]
Epoch [37/120    avg_loss:0.017, val_acc:0.981]
Epoch [38/120    avg_loss:0.017, val_acc:0.980]
Epoch [39/120    avg_loss:0.022, val_acc:0.978]
Epoch [40/120    avg_loss:0.023, val_acc:0.969]
Epoch [41/120    avg_loss:0.075, val_acc:0.921]
Epoch [42/120    avg_loss:0.073, val_acc:0.968]
Epoch [43/120    avg_loss:0.047, val_acc:0.970]
Epoch [44/120    avg_loss:0.046, val_acc:0.968]
Epoch [45/120    avg_loss:0.032, val_acc:0.977]
Epoch [46/120    avg_loss:0.024, val_acc:0.983]
Epoch [47/120    avg_loss:0.019, val_acc:0.984]
Epoch [48/120    avg_loss:0.025, val_acc:0.979]
Epoch [49/120    avg_loss:0.017, val_acc:0.987]
Epoch [50/120    avg_loss:0.017, val_acc:0.986]
Epoch [51/120    avg_loss:0.018, val_acc:0.967]
Epoch [52/120    avg_loss:0.033, val_acc:0.978]
Epoch [53/120    avg_loss:0.029, val_acc:0.935]
Epoch [54/120    avg_loss:0.022, val_acc:0.974]
Epoch [55/120    avg_loss:0.012, val_acc:0.983]
Epoch [56/120    avg_loss:0.010, val_acc:0.984]
Epoch [57/120    avg_loss:0.007, val_acc:0.987]
Epoch [58/120    avg_loss:0.013, val_acc:0.988]
Epoch [59/120    avg_loss:0.014, val_acc:0.984]
Epoch [60/120    avg_loss:0.009, val_acc:0.981]
Epoch [61/120    avg_loss:0.013, val_acc:0.973]
Epoch [62/120    avg_loss:0.015, val_acc:0.968]
Epoch [63/120    avg_loss:0.014, val_acc:0.982]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.006, val_acc:0.986]
Epoch [66/120    avg_loss:0.007, val_acc:0.988]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.015, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.975]
Epoch [70/120    avg_loss:0.021, val_acc:0.988]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.011, val_acc:0.967]
Epoch [73/120    avg_loss:0.033, val_acc:0.973]
Epoch [74/120    avg_loss:0.015, val_acc:0.971]
Epoch [75/120    avg_loss:0.012, val_acc:0.978]
Epoch [76/120    avg_loss:0.010, val_acc:0.976]
Epoch [77/120    avg_loss:0.010, val_acc:0.976]
Epoch [78/120    avg_loss:0.011, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.014, val_acc:0.985]
Epoch [81/120    avg_loss:0.010, val_acc:0.989]
Epoch [82/120    avg_loss:0.014, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.983]
Epoch [85/120    avg_loss:0.012, val_acc:0.974]
Epoch [86/120    avg_loss:0.016, val_acc:0.985]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.982]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.991]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.003, val_acc:0.991]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.004, val_acc:0.990]
Epoch [96/120    avg_loss:0.003, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.981]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.002, val_acc:0.991]
Epoch [106/120    avg_loss:0.002, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.003, val_acc:0.985]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.003, val_acc:0.989]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.002, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.003, val_acc:0.991]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     0     0     0     0     0    20     6]
 [    0     0 18046     0    42     0     0     0     2     0]
 [    0     8     0  2017     0     0     0     0     4     7]
 [    0    33     6     0  2914     0     0     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4856     0    21     0]
 [    0     2     0     0     0     0     2  1271     0    15]
 [    0     3     0    20    52     0     0     0  3492     4]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
99.2673462993758

F1 scores:
[       nan 0.99441167 0.99861657 0.99018164 0.97230564 0.99126472
 0.99753492 0.99258102 0.97966054 0.96235679]

Kappa:
0.9902959975049368
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a115a3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.869, val_acc:0.494]
Epoch [2/120    avg_loss:1.329, val_acc:0.324]
Epoch [3/120    avg_loss:1.051, val_acc:0.617]
Epoch [4/120    avg_loss:0.869, val_acc:0.721]
Epoch [5/120    avg_loss:0.672, val_acc:0.652]
Epoch [6/120    avg_loss:0.579, val_acc:0.778]
Epoch [7/120    avg_loss:0.481, val_acc:0.794]
Epoch [8/120    avg_loss:0.408, val_acc:0.811]
Epoch [9/120    avg_loss:0.403, val_acc:0.717]
Epoch [10/120    avg_loss:0.492, val_acc:0.828]
Epoch [11/120    avg_loss:0.348, val_acc:0.819]
Epoch [12/120    avg_loss:0.284, val_acc:0.744]
Epoch [13/120    avg_loss:0.238, val_acc:0.925]
Epoch [14/120    avg_loss:0.201, val_acc:0.940]
Epoch [15/120    avg_loss:0.172, val_acc:0.932]
Epoch [16/120    avg_loss:0.150, val_acc:0.945]
Epoch [17/120    avg_loss:0.151, val_acc:0.945]
Epoch [18/120    avg_loss:0.156, val_acc:0.872]
Epoch [19/120    avg_loss:0.122, val_acc:0.953]
Epoch [20/120    avg_loss:0.082, val_acc:0.954]
Epoch [21/120    avg_loss:0.090, val_acc:0.915]
Epoch [22/120    avg_loss:0.071, val_acc:0.974]
Epoch [23/120    avg_loss:0.080, val_acc:0.957]
Epoch [24/120    avg_loss:0.100, val_acc:0.956]
Epoch [25/120    avg_loss:0.080, val_acc:0.935]
Epoch [26/120    avg_loss:0.060, val_acc:0.964]
Epoch [27/120    avg_loss:0.050, val_acc:0.950]
Epoch [28/120    avg_loss:0.060, val_acc:0.975]
Epoch [29/120    avg_loss:0.060, val_acc:0.941]
Epoch [30/120    avg_loss:0.043, val_acc:0.961]
Epoch [31/120    avg_loss:0.037, val_acc:0.969]
Epoch [32/120    avg_loss:0.030, val_acc:0.968]
Epoch [33/120    avg_loss:0.033, val_acc:0.971]
Epoch [34/120    avg_loss:0.053, val_acc:0.961]
Epoch [35/120    avg_loss:0.052, val_acc:0.974]
Epoch [36/120    avg_loss:0.027, val_acc:0.972]
Epoch [37/120    avg_loss:0.024, val_acc:0.978]
Epoch [38/120    avg_loss:0.026, val_acc:0.977]
Epoch [39/120    avg_loss:0.037, val_acc:0.953]
Epoch [40/120    avg_loss:0.040, val_acc:0.968]
Epoch [41/120    avg_loss:0.047, val_acc:0.972]
Epoch [42/120    avg_loss:0.039, val_acc:0.971]
Epoch [43/120    avg_loss:0.025, val_acc:0.972]
Epoch [44/120    avg_loss:0.025, val_acc:0.974]
Epoch [45/120    avg_loss:0.026, val_acc:0.916]
Epoch [46/120    avg_loss:0.019, val_acc:0.978]
Epoch [47/120    avg_loss:0.019, val_acc:0.978]
Epoch [48/120    avg_loss:0.025, val_acc:0.972]
Epoch [49/120    avg_loss:0.018, val_acc:0.979]
Epoch [50/120    avg_loss:0.019, val_acc:0.971]
Epoch [51/120    avg_loss:0.035, val_acc:0.979]
Epoch [52/120    avg_loss:0.018, val_acc:0.974]
Epoch [53/120    avg_loss:0.025, val_acc:0.978]
Epoch [54/120    avg_loss:0.029, val_acc:0.977]
Epoch [55/120    avg_loss:0.011, val_acc:0.972]
Epoch [56/120    avg_loss:0.011, val_acc:0.982]
Epoch [57/120    avg_loss:0.011, val_acc:0.981]
Epoch [58/120    avg_loss:0.011, val_acc:0.987]
Epoch [59/120    avg_loss:0.008, val_acc:0.976]
Epoch [60/120    avg_loss:0.007, val_acc:0.981]
Epoch [61/120    avg_loss:0.008, val_acc:0.989]
Epoch [62/120    avg_loss:0.008, val_acc:0.985]
Epoch [63/120    avg_loss:0.014, val_acc:0.981]
Epoch [64/120    avg_loss:0.006, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.985]
Epoch [66/120    avg_loss:0.008, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.978]
Epoch [68/120    avg_loss:0.011, val_acc:0.988]
Epoch [69/120    avg_loss:0.008, val_acc:0.978]
Epoch [70/120    avg_loss:0.011, val_acc:0.972]
Epoch [71/120    avg_loss:0.016, val_acc:0.981]
Epoch [72/120    avg_loss:0.018, val_acc:0.984]
Epoch [73/120    avg_loss:0.012, val_acc:0.984]
Epoch [74/120    avg_loss:0.016, val_acc:0.970]
Epoch [75/120    avg_loss:0.025, val_acc:0.981]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.006, val_acc:0.985]
Epoch [78/120    avg_loss:0.004, val_acc:0.987]
Epoch [79/120    avg_loss:0.004, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.991]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.989]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     0     0     0    13    19    20     7]
 [    0     0 18051     0    26     0    10     0     3     0]
 [    0    12     0  1998     0     0     0     0    22     4]
 [    0    26     4     0  2916     0     6     0    19     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4857     0    15     2]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0    21     0     8    53     0     0     0  3480     9]
 [    0     0     0     0    14    22     0     0     0   883]]

Accuracy:
99.16371436145856

F1 scores:
[       nan 0.99082711 0.99881035 0.98764212 0.97508778 0.99164134
 0.9946754  0.98996914 0.97615708 0.96502732]

Kappa:
0.9889232570365839
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f128c34e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.870, val_acc:0.217]
Epoch [2/120    avg_loss:1.299, val_acc:0.364]
Epoch [3/120    avg_loss:1.029, val_acc:0.569]
Epoch [4/120    avg_loss:0.886, val_acc:0.728]
Epoch [5/120    avg_loss:0.709, val_acc:0.683]
Epoch [6/120    avg_loss:0.569, val_acc:0.757]
Epoch [7/120    avg_loss:0.475, val_acc:0.782]
Epoch [8/120    avg_loss:0.393, val_acc:0.803]
Epoch [9/120    avg_loss:0.329, val_acc:0.848]
Epoch [10/120    avg_loss:0.287, val_acc:0.838]
Epoch [11/120    avg_loss:0.267, val_acc:0.906]
Epoch [12/120    avg_loss:0.204, val_acc:0.934]
Epoch [13/120    avg_loss:0.189, val_acc:0.930]
Epoch [14/120    avg_loss:0.173, val_acc:0.926]
Epoch [15/120    avg_loss:0.141, val_acc:0.958]
Epoch [16/120    avg_loss:0.137, val_acc:0.857]
Epoch [17/120    avg_loss:0.106, val_acc:0.953]
Epoch [18/120    avg_loss:0.111, val_acc:0.966]
Epoch [19/120    avg_loss:0.118, val_acc:0.915]
Epoch [20/120    avg_loss:0.111, val_acc:0.960]
Epoch [21/120    avg_loss:0.085, val_acc:0.968]
Epoch [22/120    avg_loss:0.086, val_acc:0.956]
Epoch [23/120    avg_loss:0.071, val_acc:0.978]
Epoch [24/120    avg_loss:0.053, val_acc:0.980]
Epoch [25/120    avg_loss:0.045, val_acc:0.970]
Epoch [26/120    avg_loss:0.043, val_acc:0.970]
Epoch [27/120    avg_loss:0.053, val_acc:0.968]
Epoch [28/120    avg_loss:0.068, val_acc:0.975]
Epoch [29/120    avg_loss:0.056, val_acc:0.973]
Epoch [30/120    avg_loss:0.044, val_acc:0.977]
Epoch [31/120    avg_loss:0.038, val_acc:0.973]
Epoch [32/120    avg_loss:0.037, val_acc:0.972]
Epoch [33/120    avg_loss:0.056, val_acc:0.983]
Epoch [34/120    avg_loss:0.041, val_acc:0.963]
Epoch [35/120    avg_loss:0.048, val_acc:0.983]
Epoch [36/120    avg_loss:0.030, val_acc:0.983]
Epoch [37/120    avg_loss:0.036, val_acc:0.987]
Epoch [38/120    avg_loss:0.029, val_acc:0.990]
Epoch [39/120    avg_loss:0.026, val_acc:0.981]
Epoch [40/120    avg_loss:0.048, val_acc:0.968]
Epoch [41/120    avg_loss:0.064, val_acc:0.938]
Epoch [42/120    avg_loss:0.047, val_acc:0.983]
Epoch [43/120    avg_loss:0.037, val_acc:0.963]
Epoch [44/120    avg_loss:0.045, val_acc:0.968]
Epoch [45/120    avg_loss:0.066, val_acc:0.978]
Epoch [46/120    avg_loss:0.044, val_acc:0.980]
Epoch [47/120    avg_loss:0.024, val_acc:0.975]
Epoch [48/120    avg_loss:0.032, val_acc:0.984]
Epoch [49/120    avg_loss:0.018, val_acc:0.984]
Epoch [50/120    avg_loss:0.022, val_acc:0.987]
Epoch [51/120    avg_loss:0.027, val_acc:0.986]
Epoch [52/120    avg_loss:0.013, val_acc:0.987]
Epoch [53/120    avg_loss:0.011, val_acc:0.985]
Epoch [54/120    avg_loss:0.009, val_acc:0.985]
Epoch [55/120    avg_loss:0.013, val_acc:0.986]
Epoch [56/120    avg_loss:0.009, val_acc:0.988]
Epoch [57/120    avg_loss:0.009, val_acc:0.988]
Epoch [58/120    avg_loss:0.011, val_acc:0.987]
Epoch [59/120    avg_loss:0.009, val_acc:0.988]
Epoch [60/120    avg_loss:0.011, val_acc:0.988]
Epoch [61/120    avg_loss:0.014, val_acc:0.988]
Epoch [62/120    avg_loss:0.010, val_acc:0.988]
Epoch [63/120    avg_loss:0.009, val_acc:0.988]
Epoch [64/120    avg_loss:0.011, val_acc:0.988]
Epoch [65/120    avg_loss:0.008, val_acc:0.988]
Epoch [66/120    avg_loss:0.008, val_acc:0.988]
Epoch [67/120    avg_loss:0.009, val_acc:0.988]
Epoch [68/120    avg_loss:0.008, val_acc:0.988]
Epoch [69/120    avg_loss:0.008, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.009, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.007, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.010, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.013, val_acc:0.988]
Epoch [88/120    avg_loss:0.012, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     4     0     7    10    26     1]
 [    0     6 18040     0    25     0     2     0    17     0]
 [    0     8     0  2008     0     0     0     0    18     2]
 [    0    31    17     0  2896     0     1     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     3     0     0  4862     0     2     0]
 [    0     1     0     0     0     0     2  1285     0     2]
 [    0     3     0    12    56     0     0     0  3495     5]
 [    0     0     0     0    14    27     0     0     0   878]]

Accuracy:
99.18058467693346

F1 scores:
[       nan 0.99246016 0.9978428  0.98940626 0.97067203 0.98976109
 0.99712879 0.99419729 0.97680268 0.97177643]

Kappa:
0.9891448083529081
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb772dd1828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41429==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:1.822, val_acc:0.269]
Epoch [2/120    avg_loss:1.279, val_acc:0.521]
Epoch [3/120    avg_loss:1.007, val_acc:0.688]
Epoch [4/120    avg_loss:0.875, val_acc:0.752]
Epoch [5/120    avg_loss:0.762, val_acc:0.694]
Epoch [6/120    avg_loss:0.665, val_acc:0.767]
Epoch [7/120    avg_loss:0.531, val_acc:0.674]
Epoch [8/120    avg_loss:0.425, val_acc:0.758]
Epoch [9/120    avg_loss:0.362, val_acc:0.787]
Epoch [10/120    avg_loss:0.324, val_acc:0.842]
Epoch [11/120    avg_loss:0.277, val_acc:0.837]
Epoch [12/120    avg_loss:0.246, val_acc:0.848]
Epoch [13/120    avg_loss:0.197, val_acc:0.841]
Epoch [14/120    avg_loss:0.235, val_acc:0.844]
Epoch [15/120    avg_loss:0.256, val_acc:0.830]
Epoch [16/120    avg_loss:0.210, val_acc:0.867]
Epoch [17/120    avg_loss:0.168, val_acc:0.858]
Epoch [18/120    avg_loss:0.139, val_acc:0.896]
Epoch [19/120    avg_loss:0.129, val_acc:0.911]
Epoch [20/120    avg_loss:0.140, val_acc:0.957]
Epoch [21/120    avg_loss:0.112, val_acc:0.940]
Epoch [22/120    avg_loss:0.108, val_acc:0.949]
Epoch [23/120    avg_loss:0.084, val_acc:0.910]
Epoch [24/120    avg_loss:0.075, val_acc:0.961]
Epoch [25/120    avg_loss:0.066, val_acc:0.971]
Epoch [26/120    avg_loss:0.056, val_acc:0.977]
Epoch [27/120    avg_loss:0.056, val_acc:0.977]
Epoch [28/120    avg_loss:0.060, val_acc:0.973]
Epoch [29/120    avg_loss:0.046, val_acc:0.969]
Epoch [30/120    avg_loss:0.044, val_acc:0.972]
Epoch [31/120    avg_loss:0.028, val_acc:0.980]
Epoch [32/120    avg_loss:0.034, val_acc:0.983]
Epoch [33/120    avg_loss:0.050, val_acc:0.967]
Epoch [34/120    avg_loss:0.039, val_acc:0.975]
Epoch [35/120    avg_loss:0.035, val_acc:0.973]
Epoch [36/120    avg_loss:0.044, val_acc:0.965]
Epoch [37/120    avg_loss:0.067, val_acc:0.953]
Epoch [38/120    avg_loss:0.037, val_acc:0.976]
Epoch [39/120    avg_loss:0.039, val_acc:0.978]
Epoch [40/120    avg_loss:0.038, val_acc:0.978]
Epoch [41/120    avg_loss:0.026, val_acc:0.988]
Epoch [42/120    avg_loss:0.021, val_acc:0.986]
Epoch [43/120    avg_loss:0.022, val_acc:0.980]
Epoch [44/120    avg_loss:0.022, val_acc:0.983]
Epoch [45/120    avg_loss:0.021, val_acc:0.982]
Epoch [46/120    avg_loss:0.017, val_acc:0.978]
Epoch [47/120    avg_loss:0.015, val_acc:0.986]
Epoch [48/120    avg_loss:0.012, val_acc:0.984]
Epoch [49/120    avg_loss:0.011, val_acc:0.990]
Epoch [50/120    avg_loss:0.018, val_acc:0.983]
Epoch [51/120    avg_loss:0.016, val_acc:0.984]
Epoch [52/120    avg_loss:0.017, val_acc:0.955]
Epoch [53/120    avg_loss:0.020, val_acc:0.968]
Epoch [54/120    avg_loss:0.014, val_acc:0.987]
Epoch [55/120    avg_loss:0.010, val_acc:0.990]
Epoch [56/120    avg_loss:0.012, val_acc:0.985]
Epoch [57/120    avg_loss:0.011, val_acc:0.990]
Epoch [58/120    avg_loss:0.035, val_acc:0.983]
Epoch [59/120    avg_loss:0.022, val_acc:0.984]
Epoch [60/120    avg_loss:0.027, val_acc:0.988]
Epoch [61/120    avg_loss:0.015, val_acc:0.985]
Epoch [62/120    avg_loss:0.016, val_acc:0.989]
Epoch [63/120    avg_loss:0.009, val_acc:0.991]
Epoch [64/120    avg_loss:0.008, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.991]
Epoch [66/120    avg_loss:0.010, val_acc:0.993]
Epoch [67/120    avg_loss:0.009, val_acc:0.994]
Epoch [68/120    avg_loss:0.007, val_acc:0.993]
Epoch [69/120    avg_loss:0.007, val_acc:0.990]
Epoch [70/120    avg_loss:0.016, val_acc:0.987]
Epoch [71/120    avg_loss:0.009, val_acc:0.993]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.006, val_acc:0.995]
Epoch [74/120    avg_loss:0.005, val_acc:0.992]
Epoch [75/120    avg_loss:0.004, val_acc:0.993]
Epoch [76/120    avg_loss:0.011, val_acc:0.993]
Epoch [77/120    avg_loss:0.024, val_acc:0.980]
Epoch [78/120    avg_loss:0.048, val_acc:0.983]
Epoch [79/120    avg_loss:0.028, val_acc:0.986]
Epoch [80/120    avg_loss:0.018, val_acc:0.989]
Epoch [81/120    avg_loss:0.026, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.022, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.991]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.005, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.991]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.004, val_acc:0.993]
Epoch [100/120    avg_loss:0.004, val_acc:0.993]
Epoch [101/120    avg_loss:0.006, val_acc:0.993]
Epoch [102/120    avg_loss:0.006, val_acc:0.993]
Epoch [103/120    avg_loss:0.004, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.993]
Epoch [105/120    avg_loss:0.005, val_acc:0.993]
Epoch [106/120    avg_loss:0.005, val_acc:0.993]
Epoch [107/120    avg_loss:0.005, val_acc:0.993]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.004, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     3     3     0     1     3    42     0]
 [    0     1 18061     0    17     0     9     0     2     0]
 [    0     0     0  2023     1     0     0     0     5     7]
 [    0    34    17     1  2897     0     3     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     5     0     0  4867     0     1     0]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0     2     0    11    58     0     0     0  3481    19]
 [    0     0     0     0    10    12     0     0     0   897]]

Accuracy:
99.28903670498639

F1 scores:
[       nan 0.99307339 0.99859011 0.99190978 0.97247398 0.99542334
 0.99743826 0.99767442 0.9775344  0.97288503]

Kappa:
0.9905807775140394
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3d3fe4d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.862, val_acc:0.196]
Epoch [2/120    avg_loss:1.347, val_acc:0.384]
Epoch [3/120    avg_loss:1.070, val_acc:0.618]
Epoch [4/120    avg_loss:0.884, val_acc:0.720]
Epoch [5/120    avg_loss:0.702, val_acc:0.653]
Epoch [6/120    avg_loss:0.562, val_acc:0.727]
Epoch [7/120    avg_loss:0.465, val_acc:0.809]
Epoch [8/120    avg_loss:0.419, val_acc:0.822]
Epoch [9/120    avg_loss:0.357, val_acc:0.817]
Epoch [10/120    avg_loss:0.279, val_acc:0.884]
Epoch [11/120    avg_loss:0.241, val_acc:0.893]
Epoch [12/120    avg_loss:0.203, val_acc:0.909]
Epoch [13/120    avg_loss:0.175, val_acc:0.953]
Epoch [14/120    avg_loss:0.184, val_acc:0.959]
Epoch [15/120    avg_loss:0.143, val_acc:0.965]
Epoch [16/120    avg_loss:0.127, val_acc:0.950]
Epoch [17/120    avg_loss:0.125, val_acc:0.948]
Epoch [18/120    avg_loss:0.139, val_acc:0.928]
Epoch [19/120    avg_loss:0.118, val_acc:0.968]
Epoch [20/120    avg_loss:0.099, val_acc:0.959]
Epoch [21/120    avg_loss:0.078, val_acc:0.975]
Epoch [22/120    avg_loss:0.059, val_acc:0.980]
Epoch [23/120    avg_loss:0.054, val_acc:0.968]
Epoch [24/120    avg_loss:0.053, val_acc:0.957]
Epoch [25/120    avg_loss:0.069, val_acc:0.972]
Epoch [26/120    avg_loss:0.054, val_acc:0.978]
Epoch [27/120    avg_loss:0.041, val_acc:0.978]
Epoch [28/120    avg_loss:0.043, val_acc:0.981]
Epoch [29/120    avg_loss:0.056, val_acc:0.972]
Epoch [30/120    avg_loss:0.036, val_acc:0.974]
Epoch [31/120    avg_loss:0.046, val_acc:0.985]
Epoch [32/120    avg_loss:0.036, val_acc:0.988]
Epoch [33/120    avg_loss:0.041, val_acc:0.982]
Epoch [34/120    avg_loss:0.035, val_acc:0.984]
Epoch [35/120    avg_loss:0.045, val_acc:0.984]
Epoch [36/120    avg_loss:0.051, val_acc:0.981]
Epoch [37/120    avg_loss:0.028, val_acc:0.981]
Epoch [38/120    avg_loss:0.023, val_acc:0.978]
Epoch [39/120    avg_loss:0.030, val_acc:0.950]
Epoch [40/120    avg_loss:0.027, val_acc:0.984]
Epoch [41/120    avg_loss:0.021, val_acc:0.982]
Epoch [42/120    avg_loss:0.016, val_acc:0.988]
Epoch [43/120    avg_loss:0.011, val_acc:0.987]
Epoch [44/120    avg_loss:0.012, val_acc:0.989]
Epoch [45/120    avg_loss:0.011, val_acc:0.986]
Epoch [46/120    avg_loss:0.011, val_acc:0.988]
Epoch [47/120    avg_loss:0.012, val_acc:0.985]
Epoch [48/120    avg_loss:0.037, val_acc:0.983]
Epoch [49/120    avg_loss:0.014, val_acc:0.986]
Epoch [50/120    avg_loss:0.009, val_acc:0.987]
Epoch [51/120    avg_loss:0.010, val_acc:0.989]
Epoch [52/120    avg_loss:0.013, val_acc:0.992]
Epoch [53/120    avg_loss:0.009, val_acc:0.989]
Epoch [54/120    avg_loss:0.009, val_acc:0.988]
Epoch [55/120    avg_loss:0.009, val_acc:0.993]
Epoch [56/120    avg_loss:0.007, val_acc:0.993]
Epoch [57/120    avg_loss:0.008, val_acc:0.993]
Epoch [58/120    avg_loss:0.007, val_acc:0.987]
Epoch [59/120    avg_loss:0.011, val_acc:0.991]
Epoch [60/120    avg_loss:0.044, val_acc:0.959]
Epoch [61/120    avg_loss:0.052, val_acc:0.976]
Epoch [62/120    avg_loss:0.032, val_acc:0.983]
Epoch [63/120    avg_loss:0.019, val_acc:0.984]
Epoch [64/120    avg_loss:0.022, val_acc:0.986]
Epoch [65/120    avg_loss:0.014, val_acc:0.982]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.036, val_acc:0.984]
Epoch [68/120    avg_loss:0.022, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.987]
Epoch [70/120    avg_loss:0.011, val_acc:0.991]
Epoch [71/120    avg_loss:0.007, val_acc:0.991]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.007, val_acc:0.991]
Epoch [74/120    avg_loss:0.006, val_acc:0.990]
Epoch [75/120    avg_loss:0.009, val_acc:0.991]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.005, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.007, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.007, val_acc:0.992]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.007, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.991]
Epoch [92/120    avg_loss:0.009, val_acc:0.991]
Epoch [93/120    avg_loss:0.005, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.007, val_acc:0.991]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.991]
Epoch [98/120    avg_loss:0.007, val_acc:0.991]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.006, val_acc:0.991]
Epoch [102/120    avg_loss:0.008, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.991]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.006, val_acc:0.991]
Epoch [111/120    avg_loss:0.007, val_acc:0.991]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.991]
Epoch [115/120    avg_loss:0.008, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     0     0     0     9    12     8     0]
 [    0     2 18048     0    37     0     2     0     1     0]
 [    0     5     0  2005     1     0     0     0    22     3]
 [    0    37    11     0  2890     0     3     0    31     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     9     0     0  4849     0     6     6]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     4     0     0    52     0     0     0  3488    27]
 [    0     0     0     0    14    39     0     0     0   866]]

Accuracy:
99.15407418118718

F1 scores:
[       nan 0.99402313 0.99831291 0.99012346 0.96882333 0.98527746
 0.9953813  0.99459459 0.97881296 0.95112576]

Kappa:
0.9887939222082178
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab93b39780>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.834, val_acc:0.226]
Epoch [2/120    avg_loss:1.284, val_acc:0.536]
Epoch [3/120    avg_loss:1.034, val_acc:0.676]
Epoch [4/120    avg_loss:0.897, val_acc:0.711]
Epoch [5/120    avg_loss:0.756, val_acc:0.634]
Epoch [6/120    avg_loss:0.584, val_acc:0.734]
Epoch [7/120    avg_loss:0.484, val_acc:0.732]
Epoch [8/120    avg_loss:0.420, val_acc:0.853]
Epoch [9/120    avg_loss:0.324, val_acc:0.812]
Epoch [10/120    avg_loss:0.288, val_acc:0.843]
Epoch [11/120    avg_loss:0.299, val_acc:0.894]
Epoch [12/120    avg_loss:0.268, val_acc:0.904]
Epoch [13/120    avg_loss:0.201, val_acc:0.950]
Epoch [14/120    avg_loss:0.157, val_acc:0.955]
Epoch [15/120    avg_loss:0.135, val_acc:0.927]
Epoch [16/120    avg_loss:0.140, val_acc:0.952]
Epoch [17/120    avg_loss:0.126, val_acc:0.970]
Epoch [18/120    avg_loss:0.093, val_acc:0.966]
Epoch [19/120    avg_loss:0.139, val_acc:0.972]
Epoch [20/120    avg_loss:0.111, val_acc:0.972]
Epoch [21/120    avg_loss:0.109, val_acc:0.951]
Epoch [22/120    avg_loss:0.087, val_acc:0.953]
Epoch [23/120    avg_loss:0.086, val_acc:0.918]
Epoch [24/120    avg_loss:0.094, val_acc:0.949]
Epoch [25/120    avg_loss:0.070, val_acc:0.972]
Epoch [26/120    avg_loss:0.053, val_acc:0.978]
Epoch [27/120    avg_loss:0.063, val_acc:0.970]
Epoch [28/120    avg_loss:0.066, val_acc:0.970]
Epoch [29/120    avg_loss:0.054, val_acc:0.984]
Epoch [30/120    avg_loss:0.044, val_acc:0.975]
Epoch [31/120    avg_loss:0.039, val_acc:0.981]
Epoch [32/120    avg_loss:0.034, val_acc:0.983]
Epoch [33/120    avg_loss:0.026, val_acc:0.979]
Epoch [34/120    avg_loss:0.028, val_acc:0.989]
Epoch [35/120    avg_loss:0.066, val_acc:0.972]
Epoch [36/120    avg_loss:0.056, val_acc:0.980]
Epoch [37/120    avg_loss:0.042, val_acc:0.976]
Epoch [38/120    avg_loss:0.032, val_acc:0.984]
Epoch [39/120    avg_loss:0.033, val_acc:0.977]
Epoch [40/120    avg_loss:0.025, val_acc:0.984]
Epoch [41/120    avg_loss:0.039, val_acc:0.982]
Epoch [42/120    avg_loss:0.025, val_acc:0.975]
Epoch [43/120    avg_loss:0.031, val_acc:0.973]
Epoch [44/120    avg_loss:0.050, val_acc:0.982]
Epoch [45/120    avg_loss:0.035, val_acc:0.968]
Epoch [46/120    avg_loss:0.034, val_acc:0.989]
Epoch [47/120    avg_loss:0.023, val_acc:0.975]
Epoch [48/120    avg_loss:0.066, val_acc:0.966]
Epoch [49/120    avg_loss:0.055, val_acc:0.950]
Epoch [50/120    avg_loss:0.060, val_acc:0.953]
Epoch [51/120    avg_loss:0.029, val_acc:0.982]
Epoch [52/120    avg_loss:0.028, val_acc:0.976]
Epoch [53/120    avg_loss:0.016, val_acc:0.992]
Epoch [54/120    avg_loss:0.013, val_acc:0.983]
Epoch [55/120    avg_loss:0.024, val_acc:0.985]
Epoch [56/120    avg_loss:0.017, val_acc:0.954]
Epoch [57/120    avg_loss:0.030, val_acc:0.989]
Epoch [58/120    avg_loss:0.026, val_acc:0.974]
Epoch [59/120    avg_loss:0.017, val_acc:0.975]
Epoch [60/120    avg_loss:0.019, val_acc:0.991]
Epoch [61/120    avg_loss:0.044, val_acc:0.971]
Epoch [62/120    avg_loss:0.026, val_acc:0.969]
Epoch [63/120    avg_loss:0.020, val_acc:0.987]
Epoch [64/120    avg_loss:0.019, val_acc:0.990]
Epoch [65/120    avg_loss:0.013, val_acc:0.986]
Epoch [66/120    avg_loss:0.011, val_acc:0.991]
Epoch [67/120    avg_loss:0.010, val_acc:0.991]
Epoch [68/120    avg_loss:0.010, val_acc:0.991]
Epoch [69/120    avg_loss:0.008, val_acc:0.991]
Epoch [70/120    avg_loss:0.010, val_acc:0.991]
Epoch [71/120    avg_loss:0.007, val_acc:0.991]
Epoch [72/120    avg_loss:0.010, val_acc:0.991]
Epoch [73/120    avg_loss:0.008, val_acc:0.991]
Epoch [74/120    avg_loss:0.008, val_acc:0.991]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.008, val_acc:0.991]
Epoch [77/120    avg_loss:0.009, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.009, val_acc:0.991]
Epoch [80/120    avg_loss:0.010, val_acc:0.991]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.007, val_acc:0.991]
Epoch [83/120    avg_loss:0.007, val_acc:0.991]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.008, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.009, val_acc:0.991]
Epoch [92/120    avg_loss:0.007, val_acc:0.991]
Epoch [93/120    avg_loss:0.007, val_acc:0.991]
Epoch [94/120    avg_loss:0.008, val_acc:0.991]
Epoch [95/120    avg_loss:0.007, val_acc:0.991]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.991]
Epoch [98/120    avg_loss:0.007, val_acc:0.991]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.991]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.007, val_acc:0.991]
Epoch [110/120    avg_loss:0.007, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.007, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.009, val_acc:0.991]
Epoch [115/120    avg_loss:0.007, val_acc:0.991]
Epoch [116/120    avg_loss:0.010, val_acc:0.991]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.008, val_acc:0.991]
Epoch [120/120    avg_loss:0.007, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     0     4     0     0    10    30    15]
 [    0     0 18070     0    20     0     0     0     0     0]
 [    0     2     0  2027     2     0     0     0     1     4]
 [    0    51    16     0  2878     0     1     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     5     0     0  4870     0     0     3]
 [    0     0     0     0     0     0     6  1284     0     0]
 [    0     6     0     6    65     0     0     0  3468    26]
 [    0     0     0     0    14    31     0     0     0   874]]

Accuracy:
99.17094449666209

F1 scores:
[       nan 0.99082711 0.99900487 0.99509082 0.9665827  0.98826202
 0.99846233 0.99380805 0.97745209 0.94948398]

Kappa:
0.9890158357037625
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c243d97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.867, val_acc:0.521]
Epoch [2/120    avg_loss:1.331, val_acc:0.605]
Epoch [3/120    avg_loss:1.091, val_acc:0.654]
Epoch [4/120    avg_loss:0.881, val_acc:0.657]
Epoch [5/120    avg_loss:0.740, val_acc:0.722]
Epoch [6/120    avg_loss:0.650, val_acc:0.694]
Epoch [7/120    avg_loss:0.538, val_acc:0.776]
Epoch [8/120    avg_loss:0.410, val_acc:0.816]
Epoch [9/120    avg_loss:0.330, val_acc:0.838]
Epoch [10/120    avg_loss:0.285, val_acc:0.872]
Epoch [11/120    avg_loss:0.265, val_acc:0.918]
Epoch [12/120    avg_loss:0.220, val_acc:0.897]
Epoch [13/120    avg_loss:0.204, val_acc:0.886]
Epoch [14/120    avg_loss:0.179, val_acc:0.935]
Epoch [15/120    avg_loss:0.155, val_acc:0.898]
Epoch [16/120    avg_loss:0.134, val_acc:0.947]
Epoch [17/120    avg_loss:0.105, val_acc:0.916]
Epoch [18/120    avg_loss:0.093, val_acc:0.947]
Epoch [19/120    avg_loss:0.086, val_acc:0.963]
Epoch [20/120    avg_loss:0.106, val_acc:0.943]
Epoch [21/120    avg_loss:0.076, val_acc:0.969]
Epoch [22/120    avg_loss:0.071, val_acc:0.972]
Epoch [23/120    avg_loss:0.080, val_acc:0.964]
Epoch [24/120    avg_loss:0.054, val_acc:0.966]
Epoch [25/120    avg_loss:0.090, val_acc:0.972]
Epoch [26/120    avg_loss:0.065, val_acc:0.964]
Epoch [27/120    avg_loss:0.049, val_acc:0.970]
Epoch [28/120    avg_loss:0.035, val_acc:0.969]
Epoch [29/120    avg_loss:0.039, val_acc:0.970]
Epoch [30/120    avg_loss:0.027, val_acc:0.974]
Epoch [31/120    avg_loss:0.038, val_acc:0.969]
Epoch [32/120    avg_loss:0.029, val_acc:0.973]
Epoch [33/120    avg_loss:0.033, val_acc:0.966]
Epoch [34/120    avg_loss:0.043, val_acc:0.976]
Epoch [35/120    avg_loss:0.025, val_acc:0.972]
Epoch [36/120    avg_loss:0.029, val_acc:0.968]
Epoch [37/120    avg_loss:0.043, val_acc:0.957]
Epoch [38/120    avg_loss:0.031, val_acc:0.975]
Epoch [39/120    avg_loss:0.032, val_acc:0.970]
Epoch [40/120    avg_loss:0.035, val_acc:0.966]
Epoch [41/120    avg_loss:0.021, val_acc:0.975]
Epoch [42/120    avg_loss:0.026, val_acc:0.968]
Epoch [43/120    avg_loss:0.028, val_acc:0.972]
Epoch [44/120    avg_loss:0.031, val_acc:0.977]
Epoch [45/120    avg_loss:0.025, val_acc:0.977]
Epoch [46/120    avg_loss:0.019, val_acc:0.978]
Epoch [47/120    avg_loss:0.016, val_acc:0.978]
Epoch [48/120    avg_loss:0.028, val_acc:0.972]
Epoch [49/120    avg_loss:0.036, val_acc:0.971]
Epoch [50/120    avg_loss:0.023, val_acc:0.978]
Epoch [51/120    avg_loss:0.015, val_acc:0.978]
Epoch [52/120    avg_loss:0.015, val_acc:0.973]
Epoch [53/120    avg_loss:0.016, val_acc:0.978]
Epoch [54/120    avg_loss:0.013, val_acc:0.971]
Epoch [55/120    avg_loss:0.030, val_acc:0.979]
Epoch [56/120    avg_loss:0.012, val_acc:0.982]
Epoch [57/120    avg_loss:0.014, val_acc:0.981]
Epoch [58/120    avg_loss:0.010, val_acc:0.984]
Epoch [59/120    avg_loss:0.011, val_acc:0.980]
Epoch [60/120    avg_loss:0.012, val_acc:0.979]
Epoch [61/120    avg_loss:0.010, val_acc:0.976]
Epoch [62/120    avg_loss:0.009, val_acc:0.984]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.006, val_acc:0.980]
Epoch [65/120    avg_loss:0.005, val_acc:0.984]
Epoch [66/120    avg_loss:0.006, val_acc:0.978]
Epoch [67/120    avg_loss:0.006, val_acc:0.978]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.007, val_acc:0.981]
Epoch [70/120    avg_loss:0.006, val_acc:0.982]
Epoch [71/120    avg_loss:0.015, val_acc:0.968]
Epoch [72/120    avg_loss:0.045, val_acc:0.966]
Epoch [73/120    avg_loss:0.059, val_acc:0.978]
Epoch [74/120    avg_loss:0.030, val_acc:0.972]
Epoch [75/120    avg_loss:0.027, val_acc:0.947]
Epoch [76/120    avg_loss:0.034, val_acc:0.975]
Epoch [77/120    avg_loss:0.018, val_acc:0.974]
Epoch [78/120    avg_loss:0.012, val_acc:0.978]
Epoch [79/120    avg_loss:0.010, val_acc:0.978]
Epoch [80/120    avg_loss:0.009, val_acc:0.980]
Epoch [81/120    avg_loss:0.007, val_acc:0.980]
Epoch [82/120    avg_loss:0.007, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.981]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.982]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.982]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6338     0     5     1     0    18     2    67     1]
 [    0     0 18053     0    27     0     8     0     2     0]
 [    0    13     0  2005     0     0     0     0     9     9]
 [    0    51    24     0  2872     0     0     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     0     0  4861     0     0     7]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     2     0     4    62     0     0     0  3481    22]
 [    0     0     0     0    14    37     0     0     0   868]]

Accuracy:
98.9805509363025

F1 scores:
[       nan 0.98753506 0.99803743 0.99012346 0.96570276 0.98602191
 0.99529075 0.99806126 0.97302586 0.95071194]

Kappa:
0.9864929975241997
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f37dcb5f780>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.899, val_acc:0.239]
Epoch [2/120    avg_loss:1.327, val_acc:0.384]
Epoch [3/120    avg_loss:1.058, val_acc:0.462]
Epoch [4/120    avg_loss:0.931, val_acc:0.732]
Epoch [5/120    avg_loss:0.818, val_acc:0.689]
Epoch [6/120    avg_loss:0.671, val_acc:0.700]
Epoch [7/120    avg_loss:0.573, val_acc:0.731]
Epoch [8/120    avg_loss:0.471, val_acc:0.809]
Epoch [9/120    avg_loss:0.464, val_acc:0.833]
Epoch [10/120    avg_loss:0.329, val_acc:0.830]
Epoch [11/120    avg_loss:0.285, val_acc:0.852]
Epoch [12/120    avg_loss:0.225, val_acc:0.951]
Epoch [13/120    avg_loss:0.248, val_acc:0.884]
Epoch [14/120    avg_loss:0.197, val_acc:0.936]
Epoch [15/120    avg_loss:0.165, val_acc:0.928]
Epoch [16/120    avg_loss:0.160, val_acc:0.932]
Epoch [17/120    avg_loss:0.125, val_acc:0.944]
Epoch [18/120    avg_loss:0.109, val_acc:0.946]
Epoch [19/120    avg_loss:0.105, val_acc:0.961]
Epoch [20/120    avg_loss:0.079, val_acc:0.956]
Epoch [21/120    avg_loss:0.076, val_acc:0.891]
Epoch [22/120    avg_loss:0.091, val_acc:0.972]
Epoch [23/120    avg_loss:0.090, val_acc:0.940]
Epoch [24/120    avg_loss:0.083, val_acc:0.962]
Epoch [25/120    avg_loss:0.068, val_acc:0.972]
Epoch [26/120    avg_loss:0.055, val_acc:0.969]
Epoch [27/120    avg_loss:0.083, val_acc:0.963]
Epoch [28/120    avg_loss:0.063, val_acc:0.967]
Epoch [29/120    avg_loss:0.055, val_acc:0.973]
Epoch [30/120    avg_loss:0.048, val_acc:0.978]
Epoch [31/120    avg_loss:0.036, val_acc:0.960]
Epoch [32/120    avg_loss:0.042, val_acc:0.978]
Epoch [33/120    avg_loss:0.044, val_acc:0.978]
Epoch [34/120    avg_loss:0.034, val_acc:0.979]
Epoch [35/120    avg_loss:0.033, val_acc:0.987]
Epoch [36/120    avg_loss:0.032, val_acc:0.976]
Epoch [37/120    avg_loss:0.025, val_acc:0.984]
Epoch [38/120    avg_loss:0.023, val_acc:0.987]
Epoch [39/120    avg_loss:0.022, val_acc:0.961]
Epoch [40/120    avg_loss:0.026, val_acc:0.983]
Epoch [41/120    avg_loss:0.027, val_acc:0.982]
Epoch [42/120    avg_loss:0.057, val_acc:0.938]
Epoch [43/120    avg_loss:0.036, val_acc:0.978]
Epoch [44/120    avg_loss:0.022, val_acc:0.971]
Epoch [45/120    avg_loss:0.026, val_acc:0.980]
Epoch [46/120    avg_loss:0.019, val_acc:0.978]
Epoch [47/120    avg_loss:0.016, val_acc:0.984]
Epoch [48/120    avg_loss:0.011, val_acc:0.991]
Epoch [49/120    avg_loss:0.014, val_acc:0.984]
Epoch [50/120    avg_loss:0.016, val_acc:0.985]
Epoch [51/120    avg_loss:0.023, val_acc:0.983]
Epoch [52/120    avg_loss:0.015, val_acc:0.986]
Epoch [53/120    avg_loss:0.010, val_acc:0.986]
Epoch [54/120    avg_loss:0.013, val_acc:0.987]
Epoch [55/120    avg_loss:0.011, val_acc:0.981]
Epoch [56/120    avg_loss:0.013, val_acc:0.985]
Epoch [57/120    avg_loss:0.014, val_acc:0.990]
Epoch [58/120    avg_loss:0.012, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.984]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.023, val_acc:0.987]
Epoch [62/120    avg_loss:0.019, val_acc:0.988]
Epoch [63/120    avg_loss:0.009, val_acc:0.989]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.009, val_acc:0.989]
Epoch [66/120    avg_loss:0.008, val_acc:0.989]
Epoch [67/120    avg_loss:0.012, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.990]
Epoch [69/120    avg_loss:0.008, val_acc:0.990]
Epoch [70/120    avg_loss:0.008, val_acc:0.990]
Epoch [71/120    avg_loss:0.007, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.006, val_acc:0.990]
Epoch [74/120    avg_loss:0.008, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.009, val_acc:0.990]
Epoch [78/120    avg_loss:0.008, val_acc:0.990]
Epoch [79/120    avg_loss:0.007, val_acc:0.990]
Epoch [80/120    avg_loss:0.009, val_acc:0.990]
Epoch [81/120    avg_loss:0.009, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.990]
Epoch [83/120    avg_loss:0.007, val_acc:0.990]
Epoch [84/120    avg_loss:0.007, val_acc:0.990]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.990]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.010, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.007, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.007, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.008, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.990]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.007, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.007, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     2     0    20     5    11    25]
 [    0     7 18019     0    43     0    21     0     0     0]
 [    0     5     0  2021     0     0     0     0     6     4]
 [    0    22    14     0  2896     0    10     0    28     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     5     0     0  4858     0    15     0]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0     1     0    11    63     0     0     0  3473    23]
 [    0     0     0     0    15    43     0     0     0   861]]

Accuracy:
99.01670161232015

F1 scores:
[       nan 0.99236522 0.99764693 0.9923889  0.96678351 0.98379193
 0.99254265 0.99534523 0.97775901 0.93637847]

Kappa:
0.9869806201615667
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd5e1e43828>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.960, val_acc:0.212]
Epoch [2/120    avg_loss:1.418, val_acc:0.357]
Epoch [3/120    avg_loss:1.119, val_acc:0.367]
Epoch [4/120    avg_loss:0.989, val_acc:0.659]
Epoch [5/120    avg_loss:0.808, val_acc:0.688]
Epoch [6/120    avg_loss:0.630, val_acc:0.719]
Epoch [7/120    avg_loss:0.501, val_acc:0.715]
Epoch [8/120    avg_loss:0.413, val_acc:0.772]
Epoch [9/120    avg_loss:0.358, val_acc:0.836]
Epoch [10/120    avg_loss:0.273, val_acc:0.853]
Epoch [11/120    avg_loss:0.276, val_acc:0.804]
Epoch [12/120    avg_loss:0.242, val_acc:0.897]
Epoch [13/120    avg_loss:0.245, val_acc:0.930]
Epoch [14/120    avg_loss:0.176, val_acc:0.934]
Epoch [15/120    avg_loss:0.159, val_acc:0.942]
Epoch [16/120    avg_loss:0.142, val_acc:0.929]
Epoch [17/120    avg_loss:0.111, val_acc:0.951]
Epoch [18/120    avg_loss:0.105, val_acc:0.957]
Epoch [19/120    avg_loss:0.100, val_acc:0.965]
Epoch [20/120    avg_loss:0.089, val_acc:0.963]
Epoch [21/120    avg_loss:0.079, val_acc:0.949]
Epoch [22/120    avg_loss:0.080, val_acc:0.961]
Epoch [23/120    avg_loss:0.064, val_acc:0.963]
Epoch [24/120    avg_loss:0.076, val_acc:0.964]
Epoch [25/120    avg_loss:0.059, val_acc:0.974]
Epoch [26/120    avg_loss:0.051, val_acc:0.959]
Epoch [27/120    avg_loss:0.041, val_acc:0.970]
Epoch [28/120    avg_loss:0.037, val_acc:0.975]
Epoch [29/120    avg_loss:0.037, val_acc:0.970]
Epoch [30/120    avg_loss:0.029, val_acc:0.983]
Epoch [31/120    avg_loss:0.027, val_acc:0.983]
Epoch [32/120    avg_loss:0.032, val_acc:0.975]
Epoch [33/120    avg_loss:0.023, val_acc:0.979]
Epoch [34/120    avg_loss:0.027, val_acc:0.968]
Epoch [35/120    avg_loss:0.093, val_acc:0.960]
Epoch [36/120    avg_loss:0.042, val_acc:0.973]
Epoch [37/120    avg_loss:0.048, val_acc:0.943]
Epoch [38/120    avg_loss:0.053, val_acc:0.968]
Epoch [39/120    avg_loss:0.082, val_acc:0.963]
Epoch [40/120    avg_loss:0.040, val_acc:0.978]
Epoch [41/120    avg_loss:0.025, val_acc:0.983]
Epoch [42/120    avg_loss:0.049, val_acc:0.985]
Epoch [43/120    avg_loss:0.036, val_acc:0.991]
Epoch [44/120    avg_loss:0.024, val_acc:0.984]
Epoch [45/120    avg_loss:0.023, val_acc:0.984]
Epoch [46/120    avg_loss:0.024, val_acc:0.959]
Epoch [47/120    avg_loss:0.016, val_acc:0.991]
Epoch [48/120    avg_loss:0.016, val_acc:0.990]
Epoch [49/120    avg_loss:0.011, val_acc:0.987]
Epoch [50/120    avg_loss:0.015, val_acc:0.967]
Epoch [51/120    avg_loss:0.024, val_acc:0.986]
Epoch [52/120    avg_loss:0.015, val_acc:0.951]
Epoch [53/120    avg_loss:0.020, val_acc:0.986]
Epoch [54/120    avg_loss:0.017, val_acc:0.991]
Epoch [55/120    avg_loss:0.020, val_acc:0.987]
Epoch [56/120    avg_loss:0.013, val_acc:0.988]
Epoch [57/120    avg_loss:0.011, val_acc:0.989]
Epoch [58/120    avg_loss:0.010, val_acc:0.990]
Epoch [59/120    avg_loss:0.011, val_acc:0.990]
Epoch [60/120    avg_loss:0.014, val_acc:0.968]
Epoch [61/120    avg_loss:0.014, val_acc:0.990]
Epoch [62/120    avg_loss:0.009, val_acc:0.990]
Epoch [63/120    avg_loss:0.009, val_acc:0.991]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.046, val_acc:0.987]
Epoch [66/120    avg_loss:0.050, val_acc:0.975]
Epoch [67/120    avg_loss:0.032, val_acc:0.986]
Epoch [68/120    avg_loss:0.023, val_acc:0.978]
Epoch [69/120    avg_loss:0.016, val_acc:0.990]
Epoch [70/120    avg_loss:0.015, val_acc:0.987]
Epoch [71/120    avg_loss:0.017, val_acc:0.989]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.067, val_acc:0.930]
Epoch [74/120    avg_loss:0.056, val_acc:0.978]
Epoch [75/120    avg_loss:0.044, val_acc:0.984]
Epoch [76/120    avg_loss:0.022, val_acc:0.986]
Epoch [77/120    avg_loss:0.016, val_acc:0.988]
Epoch [78/120    avg_loss:0.013, val_acc:0.989]
Epoch [79/120    avg_loss:0.012, val_acc:0.990]
Epoch [80/120    avg_loss:0.010, val_acc:0.989]
Epoch [81/120    avg_loss:0.014, val_acc:0.990]
Epoch [82/120    avg_loss:0.010, val_acc:0.991]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.010, val_acc:0.990]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.989]
Epoch [88/120    avg_loss:0.008, val_acc:0.989]
Epoch [89/120    avg_loss:0.009, val_acc:0.989]
Epoch [90/120    avg_loss:0.013, val_acc:0.989]
Epoch [91/120    avg_loss:0.011, val_acc:0.989]
Epoch [92/120    avg_loss:0.008, val_acc:0.989]
Epoch [93/120    avg_loss:0.010, val_acc:0.989]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.008, val_acc:0.989]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.008, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.989]
Epoch [101/120    avg_loss:0.009, val_acc:0.989]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.007, val_acc:0.989]
Epoch [106/120    avg_loss:0.009, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.009, val_acc:0.989]
Epoch [109/120    avg_loss:0.009, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.010, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.009, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.989]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     0     1     0     0     9    40    35]
 [    0     0 18007     0    46     0    33     0     4     0]
 [    0     0     0  2003     2     0     0     0    24     7]
 [    0    53    18     0  2875     0     0     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4827     0    35     7]
 [    0     0     0     0     0     0     6  1282     0     2]
 [    0     2     0     0    58     0     0     0  3511     0]
 [    0     0     0     2    13    27     0     0     0   877]]

Accuracy:
98.89378931386017

F1 scores:
[       nan 0.98909148 0.99695493 0.99133878 0.96363332 0.98976109
 0.99076355 0.99341341 0.97379004 0.94964808]

Kappa:
0.9853529535274138
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0da10287f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.850, val_acc:0.263]
Epoch [2/120    avg_loss:1.272, val_acc:0.389]
Epoch [3/120    avg_loss:1.016, val_acc:0.602]
Epoch [4/120    avg_loss:0.848, val_acc:0.688]
Epoch [5/120    avg_loss:0.630, val_acc:0.749]
Epoch [6/120    avg_loss:0.533, val_acc:0.768]
Epoch [7/120    avg_loss:0.445, val_acc:0.805]
Epoch [8/120    avg_loss:0.424, val_acc:0.751]
Epoch [9/120    avg_loss:0.327, val_acc:0.868]
Epoch [10/120    avg_loss:0.269, val_acc:0.870]
Epoch [11/120    avg_loss:0.236, val_acc:0.901]
Epoch [12/120    avg_loss:0.205, val_acc:0.932]
Epoch [13/120    avg_loss:0.181, val_acc:0.905]
Epoch [14/120    avg_loss:0.155, val_acc:0.947]
Epoch [15/120    avg_loss:0.121, val_acc:0.916]
Epoch [16/120    avg_loss:0.135, val_acc:0.884]
Epoch [17/120    avg_loss:0.126, val_acc:0.953]
Epoch [18/120    avg_loss:0.107, val_acc:0.959]
Epoch [19/120    avg_loss:0.123, val_acc:0.916]
Epoch [20/120    avg_loss:0.084, val_acc:0.933]
Epoch [21/120    avg_loss:0.075, val_acc:0.978]
Epoch [22/120    avg_loss:0.067, val_acc:0.937]
Epoch [23/120    avg_loss:0.078, val_acc:0.966]
Epoch [24/120    avg_loss:0.054, val_acc:0.978]
Epoch [25/120    avg_loss:0.055, val_acc:0.946]
Epoch [26/120    avg_loss:0.074, val_acc:0.944]
Epoch [27/120    avg_loss:0.050, val_acc:0.974]
Epoch [28/120    avg_loss:0.045, val_acc:0.936]
Epoch [29/120    avg_loss:0.051, val_acc:0.969]
Epoch [30/120    avg_loss:0.037, val_acc:0.984]
Epoch [31/120    avg_loss:0.066, val_acc:0.958]
Epoch [32/120    avg_loss:0.166, val_acc:0.947]
Epoch [33/120    avg_loss:0.083, val_acc:0.969]
Epoch [34/120    avg_loss:0.066, val_acc:0.966]
Epoch [35/120    avg_loss:0.089, val_acc:0.944]
Epoch [36/120    avg_loss:0.064, val_acc:0.974]
Epoch [37/120    avg_loss:0.042, val_acc:0.972]
Epoch [38/120    avg_loss:0.037, val_acc:0.977]
Epoch [39/120    avg_loss:0.035, val_acc:0.973]
Epoch [40/120    avg_loss:0.036, val_acc:0.969]
Epoch [41/120    avg_loss:0.034, val_acc:0.967]
Epoch [42/120    avg_loss:0.021, val_acc:0.984]
Epoch [43/120    avg_loss:0.027, val_acc:0.977]
Epoch [44/120    avg_loss:0.023, val_acc:0.978]
Epoch [45/120    avg_loss:0.016, val_acc:0.978]
Epoch [46/120    avg_loss:0.015, val_acc:0.978]
Epoch [47/120    avg_loss:0.015, val_acc:0.980]
Epoch [48/120    avg_loss:0.017, val_acc:0.982]
Epoch [49/120    avg_loss:0.016, val_acc:0.982]
Epoch [50/120    avg_loss:0.015, val_acc:0.982]
Epoch [51/120    avg_loss:0.013, val_acc:0.982]
Epoch [52/120    avg_loss:0.014, val_acc:0.986]
Epoch [53/120    avg_loss:0.014, val_acc:0.985]
Epoch [54/120    avg_loss:0.011, val_acc:0.984]
Epoch [55/120    avg_loss:0.010, val_acc:0.984]
Epoch [56/120    avg_loss:0.011, val_acc:0.984]
Epoch [57/120    avg_loss:0.011, val_acc:0.981]
Epoch [58/120    avg_loss:0.010, val_acc:0.984]
Epoch [59/120    avg_loss:0.013, val_acc:0.983]
Epoch [60/120    avg_loss:0.013, val_acc:0.985]
Epoch [61/120    avg_loss:0.011, val_acc:0.984]
Epoch [62/120    avg_loss:0.011, val_acc:0.985]
Epoch [63/120    avg_loss:0.011, val_acc:0.984]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.012, val_acc:0.987]
Epoch [66/120    avg_loss:0.010, val_acc:0.989]
Epoch [67/120    avg_loss:0.011, val_acc:0.987]
Epoch [68/120    avg_loss:0.015, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.011, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.988]
Epoch [77/120    avg_loss:0.010, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.988]
Epoch [80/120    avg_loss:0.011, val_acc:0.988]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.010, val_acc:0.988]
Epoch [84/120    avg_loss:0.012, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.012, val_acc:0.987]
Epoch [87/120    avg_loss:0.013, val_acc:0.987]
Epoch [88/120    avg_loss:0.011, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.013, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.010, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.013, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.011, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.010, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     2     0     0    17     9    33     1]
 [    0     0 18049     0    38     0     0     0     2     1]
 [    0     0     0  2005     0     0     0     0    27     4]
 [    0    50    21     0  2868     0     6     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4867     0     5     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     2     0     0    48     0     0     0  3518     3]
 [    0     0     0     0    14    34     0     0     0   871]]

Accuracy:
99.15166413611935

F1 scores:
[       nan 0.99113117 0.9982854  0.99036799 0.96565657 0.9871407
 0.99631525 0.99574797 0.97953501 0.96831573]

Kappa:
0.9887612220831155
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75322be828>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.847, val_acc:0.352]
Epoch [2/120    avg_loss:1.311, val_acc:0.328]
Epoch [3/120    avg_loss:1.044, val_acc:0.484]
Epoch [4/120    avg_loss:0.859, val_acc:0.576]
Epoch [5/120    avg_loss:0.724, val_acc:0.670]
Epoch [6/120    avg_loss:0.610, val_acc:0.752]
Epoch [7/120    avg_loss:0.557, val_acc:0.835]
Epoch [8/120    avg_loss:0.419, val_acc:0.878]
Epoch [9/120    avg_loss:0.348, val_acc:0.831]
Epoch [10/120    avg_loss:0.289, val_acc:0.905]
Epoch [11/120    avg_loss:0.235, val_acc:0.901]
Epoch [12/120    avg_loss:0.220, val_acc:0.889]
Epoch [13/120    avg_loss:0.245, val_acc:0.879]
Epoch [14/120    avg_loss:0.162, val_acc:0.934]
Epoch [15/120    avg_loss:0.198, val_acc:0.928]
Epoch [16/120    avg_loss:0.220, val_acc:0.927]
Epoch [17/120    avg_loss:0.154, val_acc:0.936]
Epoch [18/120    avg_loss:0.120, val_acc:0.938]
Epoch [19/120    avg_loss:0.127, val_acc:0.916]
Epoch [20/120    avg_loss:0.110, val_acc:0.941]
Epoch [21/120    avg_loss:0.089, val_acc:0.951]
Epoch [22/120    avg_loss:0.094, val_acc:0.864]
Epoch [23/120    avg_loss:0.074, val_acc:0.940]
Epoch [24/120    avg_loss:0.083, val_acc:0.955]
Epoch [25/120    avg_loss:0.063, val_acc:0.959]
Epoch [26/120    avg_loss:0.088, val_acc:0.953]
Epoch [27/120    avg_loss:0.052, val_acc:0.966]
Epoch [28/120    avg_loss:0.039, val_acc:0.968]
Epoch [29/120    avg_loss:0.051, val_acc:0.966]
Epoch [30/120    avg_loss:0.054, val_acc:0.973]
Epoch [31/120    avg_loss:0.041, val_acc:0.926]
Epoch [32/120    avg_loss:0.036, val_acc:0.963]
Epoch [33/120    avg_loss:0.028, val_acc:0.974]
Epoch [34/120    avg_loss:0.027, val_acc:0.979]
Epoch [35/120    avg_loss:0.023, val_acc:0.980]
Epoch [36/120    avg_loss:0.023, val_acc:0.973]
Epoch [37/120    avg_loss:0.026, val_acc:0.980]
Epoch [38/120    avg_loss:0.030, val_acc:0.971]
Epoch [39/120    avg_loss:0.024, val_acc:0.972]
Epoch [40/120    avg_loss:0.019, val_acc:0.977]
Epoch [41/120    avg_loss:0.019, val_acc:0.981]
Epoch [42/120    avg_loss:0.035, val_acc:0.922]
Epoch [43/120    avg_loss:0.121, val_acc:0.949]
Epoch [44/120    avg_loss:0.064, val_acc:0.959]
Epoch [45/120    avg_loss:0.055, val_acc:0.973]
Epoch [46/120    avg_loss:0.055, val_acc:0.971]
Epoch [47/120    avg_loss:0.045, val_acc:0.958]
Epoch [48/120    avg_loss:0.037, val_acc:0.971]
Epoch [49/120    avg_loss:0.030, val_acc:0.976]
Epoch [50/120    avg_loss:0.020, val_acc:0.950]
Epoch [51/120    avg_loss:0.024, val_acc:0.980]
Epoch [52/120    avg_loss:0.022, val_acc:0.972]
Epoch [53/120    avg_loss:0.023, val_acc:0.984]
Epoch [54/120    avg_loss:0.019, val_acc:0.984]
Epoch [55/120    avg_loss:0.013, val_acc:0.983]
Epoch [56/120    avg_loss:0.015, val_acc:0.978]
Epoch [57/120    avg_loss:0.018, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.972]
Epoch [59/120    avg_loss:0.015, val_acc:0.984]
Epoch [60/120    avg_loss:0.014, val_acc:0.986]
Epoch [61/120    avg_loss:0.011, val_acc:0.981]
Epoch [62/120    avg_loss:0.009, val_acc:0.984]
Epoch [63/120    avg_loss:0.009, val_acc:0.987]
Epoch [64/120    avg_loss:0.008, val_acc:0.985]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.011, val_acc:0.984]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.006, val_acc:0.988]
Epoch [71/120    avg_loss:0.011, val_acc:0.980]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.018, val_acc:0.984]
Epoch [75/120    avg_loss:0.016, val_acc:0.987]
Epoch [76/120    avg_loss:0.039, val_acc:0.956]
Epoch [77/120    avg_loss:0.042, val_acc:0.970]
Epoch [78/120    avg_loss:0.017, val_acc:0.983]
Epoch [79/120    avg_loss:0.019, val_acc:0.985]
Epoch [80/120    avg_loss:0.023, val_acc:0.975]
Epoch [81/120    avg_loss:0.026, val_acc:0.981]
Epoch [82/120    avg_loss:0.084, val_acc:0.949]
Epoch [83/120    avg_loss:0.037, val_acc:0.982]
Epoch [84/120    avg_loss:0.014, val_acc:0.982]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.014, val_acc:0.984]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.012, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.011, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.014, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0     0     0     0     5    13    58     0]
 [    0     0 18016     0    46     0    27     0     1     0]
 [    0     0     0  2029     2     0     0     0     0     5]
 [    0    53    17     0  2870     0     5     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4857     0    19     2]
 [    0     0     0     0     0     0     6  1281     0     3]
 [    0     7     0    36    62     0     0     0  3462     4]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
98.9516303954884

F1 scores:
[       nan 0.98941469 0.99748083 0.98951475 0.96211867 0.99126472
 0.99345469 0.99148607 0.97001961 0.97190083]

Kappa:
0.9861181122759419
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f369dc507b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.827, val_acc:0.241]
Epoch [2/120    avg_loss:1.290, val_acc:0.589]
Epoch [3/120    avg_loss:1.063, val_acc:0.555]
Epoch [4/120    avg_loss:0.886, val_acc:0.736]
Epoch [5/120    avg_loss:0.734, val_acc:0.724]
Epoch [6/120    avg_loss:0.594, val_acc:0.770]
Epoch [7/120    avg_loss:0.482, val_acc:0.872]
Epoch [8/120    avg_loss:0.405, val_acc:0.870]
Epoch [9/120    avg_loss:0.350, val_acc:0.884]
Epoch [10/120    avg_loss:0.281, val_acc:0.855]
Epoch [11/120    avg_loss:0.245, val_acc:0.919]
Epoch [12/120    avg_loss:0.194, val_acc:0.944]
Epoch [13/120    avg_loss:0.145, val_acc:0.921]
Epoch [14/120    avg_loss:0.154, val_acc:0.942]
Epoch [15/120    avg_loss:0.127, val_acc:0.967]
Epoch [16/120    avg_loss:0.107, val_acc:0.967]
Epoch [17/120    avg_loss:0.104, val_acc:0.966]
Epoch [18/120    avg_loss:0.091, val_acc:0.956]
Epoch [19/120    avg_loss:0.110, val_acc:0.923]
Epoch [20/120    avg_loss:0.084, val_acc:0.942]
Epoch [21/120    avg_loss:0.064, val_acc:0.975]
Epoch [22/120    avg_loss:0.069, val_acc:0.977]
Epoch [23/120    avg_loss:0.053, val_acc:0.969]
Epoch [24/120    avg_loss:0.039, val_acc:0.953]
Epoch [25/120    avg_loss:0.104, val_acc:0.959]
Epoch [26/120    avg_loss:0.101, val_acc:0.940]
Epoch [27/120    avg_loss:0.133, val_acc:0.967]
Epoch [28/120    avg_loss:0.061, val_acc:0.975]
Epoch [29/120    avg_loss:0.051, val_acc:0.982]
Epoch [30/120    avg_loss:0.039, val_acc:0.980]
Epoch [31/120    avg_loss:0.037, val_acc:0.987]
Epoch [32/120    avg_loss:0.027, val_acc:0.984]
Epoch [33/120    avg_loss:0.030, val_acc:0.991]
Epoch [34/120    avg_loss:0.024, val_acc:0.990]
Epoch [35/120    avg_loss:0.029, val_acc:0.981]
Epoch [36/120    avg_loss:0.026, val_acc:0.986]
Epoch [37/120    avg_loss:0.023, val_acc:0.979]
Epoch [38/120    avg_loss:0.038, val_acc:0.977]
Epoch [39/120    avg_loss:0.045, val_acc:0.962]
Epoch [40/120    avg_loss:0.073, val_acc:0.970]
Epoch [41/120    avg_loss:0.063, val_acc:0.976]
Epoch [42/120    avg_loss:0.044, val_acc:0.961]
Epoch [43/120    avg_loss:0.062, val_acc:0.958]
Epoch [44/120    avg_loss:0.059, val_acc:0.975]
Epoch [45/120    avg_loss:0.025, val_acc:0.984]
Epoch [46/120    avg_loss:0.024, val_acc:0.978]
Epoch [47/120    avg_loss:0.017, val_acc:0.985]
Epoch [48/120    avg_loss:0.016, val_acc:0.986]
Epoch [49/120    avg_loss:0.019, val_acc:0.988]
Epoch [50/120    avg_loss:0.015, val_acc:0.988]
Epoch [51/120    avg_loss:0.013, val_acc:0.989]
Epoch [52/120    avg_loss:0.022, val_acc:0.987]
Epoch [53/120    avg_loss:0.017, val_acc:0.989]
Epoch [54/120    avg_loss:0.015, val_acc:0.990]
Epoch [55/120    avg_loss:0.014, val_acc:0.989]
Epoch [56/120    avg_loss:0.016, val_acc:0.991]
Epoch [57/120    avg_loss:0.013, val_acc:0.989]
Epoch [58/120    avg_loss:0.015, val_acc:0.988]
Epoch [59/120    avg_loss:0.012, val_acc:0.989]
Epoch [60/120    avg_loss:0.014, val_acc:0.988]
Epoch [61/120    avg_loss:0.013, val_acc:0.987]
Epoch [62/120    avg_loss:0.011, val_acc:0.990]
Epoch [63/120    avg_loss:0.011, val_acc:0.990]
Epoch [64/120    avg_loss:0.014, val_acc:0.989]
Epoch [65/120    avg_loss:0.010, val_acc:0.990]
Epoch [66/120    avg_loss:0.011, val_acc:0.989]
Epoch [67/120    avg_loss:0.011, val_acc:0.989]
Epoch [68/120    avg_loss:0.014, val_acc:0.990]
Epoch [69/120    avg_loss:0.012, val_acc:0.989]
Epoch [70/120    avg_loss:0.011, val_acc:0.989]
Epoch [71/120    avg_loss:0.012, val_acc:0.989]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.010, val_acc:0.989]
Epoch [74/120    avg_loss:0.011, val_acc:0.989]
Epoch [75/120    avg_loss:0.010, val_acc:0.989]
Epoch [76/120    avg_loss:0.012, val_acc:0.990]
Epoch [77/120    avg_loss:0.011, val_acc:0.990]
Epoch [78/120    avg_loss:0.013, val_acc:0.989]
Epoch [79/120    avg_loss:0.011, val_acc:0.989]
Epoch [80/120    avg_loss:0.010, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.990]
Epoch [82/120    avg_loss:0.012, val_acc:0.990]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.012, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.990]
Epoch [88/120    avg_loss:0.012, val_acc:0.990]
Epoch [89/120    avg_loss:0.010, val_acc:0.990]
Epoch [90/120    avg_loss:0.009, val_acc:0.990]
Epoch [91/120    avg_loss:0.013, val_acc:0.990]
Epoch [92/120    avg_loss:0.013, val_acc:0.990]
Epoch [93/120    avg_loss:0.011, val_acc:0.990]
Epoch [94/120    avg_loss:0.010, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.012, val_acc:0.990]
Epoch [98/120    avg_loss:0.014, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.990]
Epoch [102/120    avg_loss:0.014, val_acc:0.990]
Epoch [103/120    avg_loss:0.016, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.990]
Epoch [107/120    avg_loss:0.010, val_acc:0.990]
Epoch [108/120    avg_loss:0.012, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.990]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.012, val_acc:0.990]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.013, val_acc:0.990]
Epoch [114/120    avg_loss:0.011, val_acc:0.990]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.011, val_acc:0.990]
Epoch [117/120    avg_loss:0.013, val_acc:0.990]
Epoch [118/120    avg_loss:0.010, val_acc:0.990]
Epoch [119/120    avg_loss:0.010, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6324     0     0     3     0    16    45    44     0]
 [    0     4 17963     0    89     0    20     0    14     0]
 [    0     4     0  2001     3     0     0     0    24     4]
 [    0    38    13     0  2886     0     8     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4864     0     1    13]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     5     0     0    50     0     0     0  3503    13]
 [    0     0     0     3    14    29     0     0     0   873]]

Accuracy:
98.82389800689273

F1 scores:
[       nan 0.98758491 0.99611823 0.99059406 0.95928203 0.98901099
 0.99366701 0.98130485 0.97522272 0.9582876 ]

Kappa:
0.9844385168219588
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f69c2c657f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.880, val_acc:0.271]
Epoch [2/120    avg_loss:1.340, val_acc:0.659]
Epoch [3/120    avg_loss:1.059, val_acc:0.716]
Epoch [4/120    avg_loss:0.814, val_acc:0.758]
Epoch [5/120    avg_loss:0.673, val_acc:0.815]
Epoch [6/120    avg_loss:0.537, val_acc:0.796]
Epoch [7/120    avg_loss:0.448, val_acc:0.836]
Epoch [8/120    avg_loss:0.388, val_acc:0.849]
Epoch [9/120    avg_loss:0.362, val_acc:0.810]
Epoch [10/120    avg_loss:0.285, val_acc:0.878]
Epoch [11/120    avg_loss:0.269, val_acc:0.837]
Epoch [12/120    avg_loss:0.259, val_acc:0.860]
Epoch [13/120    avg_loss:0.241, val_acc:0.895]
Epoch [14/120    avg_loss:0.209, val_acc:0.939]
Epoch [15/120    avg_loss:0.147, val_acc:0.954]
Epoch [16/120    avg_loss:0.140, val_acc:0.949]
Epoch [17/120    avg_loss:0.124, val_acc:0.855]
Epoch [18/120    avg_loss:0.131, val_acc:0.956]
Epoch [19/120    avg_loss:0.124, val_acc:0.926]
Epoch [20/120    avg_loss:0.145, val_acc:0.957]
Epoch [21/120    avg_loss:0.094, val_acc:0.953]
Epoch [22/120    avg_loss:0.072, val_acc:0.971]
Epoch [23/120    avg_loss:0.052, val_acc:0.964]
Epoch [24/120    avg_loss:0.061, val_acc:0.963]
Epoch [25/120    avg_loss:0.073, val_acc:0.963]
Epoch [26/120    avg_loss:0.048, val_acc:0.944]
Epoch [27/120    avg_loss:0.063, val_acc:0.965]
Epoch [28/120    avg_loss:0.046, val_acc:0.975]
Epoch [29/120    avg_loss:0.039, val_acc:0.976]
Epoch [30/120    avg_loss:0.039, val_acc:0.967]
Epoch [31/120    avg_loss:0.029, val_acc:0.982]
Epoch [32/120    avg_loss:0.031, val_acc:0.974]
Epoch [33/120    avg_loss:0.037, val_acc:0.974]
Epoch [34/120    avg_loss:0.068, val_acc:0.969]
Epoch [35/120    avg_loss:0.054, val_acc:0.971]
Epoch [36/120    avg_loss:0.040, val_acc:0.971]
Epoch [37/120    avg_loss:0.047, val_acc:0.971]
Epoch [38/120    avg_loss:0.036, val_acc:0.979]
Epoch [39/120    avg_loss:0.022, val_acc:0.965]
Epoch [40/120    avg_loss:0.024, val_acc:0.984]
Epoch [41/120    avg_loss:0.025, val_acc:0.981]
Epoch [42/120    avg_loss:0.024, val_acc:0.977]
Epoch [43/120    avg_loss:0.024, val_acc:0.976]
Epoch [44/120    avg_loss:0.030, val_acc:0.963]
Epoch [45/120    avg_loss:0.026, val_acc:0.974]
Epoch [46/120    avg_loss:0.024, val_acc:0.981]
Epoch [47/120    avg_loss:0.014, val_acc:0.979]
Epoch [48/120    avg_loss:0.016, val_acc:0.987]
Epoch [49/120    avg_loss:0.016, val_acc:0.984]
Epoch [50/120    avg_loss:0.028, val_acc:0.984]
Epoch [51/120    avg_loss:0.011, val_acc:0.977]
Epoch [52/120    avg_loss:0.013, val_acc:0.988]
Epoch [53/120    avg_loss:0.015, val_acc:0.978]
Epoch [54/120    avg_loss:0.008, val_acc:0.988]
Epoch [55/120    avg_loss:0.008, val_acc:0.987]
Epoch [56/120    avg_loss:0.012, val_acc:0.977]
Epoch [57/120    avg_loss:0.009, val_acc:0.990]
Epoch [58/120    avg_loss:0.008, val_acc:0.978]
Epoch [59/120    avg_loss:0.036, val_acc:0.986]
Epoch [60/120    avg_loss:0.021, val_acc:0.964]
Epoch [61/120    avg_loss:0.013, val_acc:0.983]
Epoch [62/120    avg_loss:0.010, val_acc:0.987]
Epoch [63/120    avg_loss:0.030, val_acc:0.981]
Epoch [64/120    avg_loss:0.009, val_acc:0.988]
Epoch [65/120    avg_loss:0.010, val_acc:0.987]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.009, val_acc:0.989]
Epoch [68/120    avg_loss:0.007, val_acc:0.990]
Epoch [69/120    avg_loss:0.007, val_acc:0.991]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.016, val_acc:0.977]
Epoch [74/120    avg_loss:0.012, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.992]
Epoch [77/120    avg_loss:0.006, val_acc:0.991]
Epoch [78/120    avg_loss:0.009, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.988]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.007, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.004, val_acc:0.993]
Epoch [88/120    avg_loss:0.005, val_acc:0.989]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.008, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.976]
Epoch [93/120    avg_loss:0.009, val_acc:0.988]
Epoch [94/120    avg_loss:0.015, val_acc:0.977]
Epoch [95/120    avg_loss:0.079, val_acc:0.958]
Epoch [96/120    avg_loss:0.065, val_acc:0.980]
Epoch [97/120    avg_loss:0.043, val_acc:0.963]
Epoch [98/120    avg_loss:0.035, val_acc:0.972]
Epoch [99/120    avg_loss:0.010, val_acc:0.989]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6378     0     0     0     0    13     0    23    18]
 [    0     0 18059     0    17     0    11     0     3     0]
 [    0    10     0  1999     0     0     0     0    24     3]
 [    0    38     8     0  2899     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     5     0     0  4871     0     1     0]
 [    0     0     0     0     0     0     2  1275     0    13]
 [    0    15     0    21    51     0     0     0  3476     8]
 [    0     0     0     0    15    21     0     0     0   883]]

Accuracy:
99.16130431639071

F1 scores:
[       nan 0.99091121 0.99889374 0.98448658 0.97379913 0.99201824
 0.99662404 0.99415205 0.9757193  0.95770065]

Kappa:
0.9888892317120954
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb631e83898>
supervision:full
center_pixel:True
Network :
Number of parameter: 47029==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.872, val_acc:0.249]
Epoch [2/120    avg_loss:1.297, val_acc:0.360]
Epoch [3/120    avg_loss:1.032, val_acc:0.638]
Epoch [4/120    avg_loss:0.896, val_acc:0.674]
Epoch [5/120    avg_loss:0.751, val_acc:0.728]
Epoch [6/120    avg_loss:0.582, val_acc:0.738]
Epoch [7/120    avg_loss:0.531, val_acc:0.791]
Epoch [8/120    avg_loss:0.436, val_acc:0.814]
Epoch [9/120    avg_loss:0.395, val_acc:0.838]
Epoch [10/120    avg_loss:0.306, val_acc:0.860]
Epoch [11/120    avg_loss:0.261, val_acc:0.897]
Epoch [12/120    avg_loss:0.216, val_acc:0.936]
Epoch [13/120    avg_loss:0.172, val_acc:0.934]
Epoch [14/120    avg_loss:0.188, val_acc:0.927]
Epoch [15/120    avg_loss:0.173, val_acc:0.943]
Epoch [16/120    avg_loss:0.140, val_acc:0.953]
Epoch [17/120    avg_loss:0.116, val_acc:0.952]
Epoch [18/120    avg_loss:0.138, val_acc:0.940]
Epoch [19/120    avg_loss:0.144, val_acc:0.928]
Epoch [20/120    avg_loss:0.122, val_acc:0.934]
Epoch [21/120    avg_loss:0.097, val_acc:0.961]
Epoch [22/120    avg_loss:0.141, val_acc:0.932]
Epoch [23/120    avg_loss:0.115, val_acc:0.949]
Epoch [24/120    avg_loss:0.097, val_acc:0.940]
Epoch [25/120    avg_loss:0.089, val_acc:0.953]
Epoch [26/120    avg_loss:0.062, val_acc:0.974]
Epoch [27/120    avg_loss:0.048, val_acc:0.967]
Epoch [28/120    avg_loss:0.053, val_acc:0.962]
Epoch [29/120    avg_loss:0.042, val_acc:0.965]
Epoch [30/120    avg_loss:0.057, val_acc:0.953]
Epoch [31/120    avg_loss:0.068, val_acc:0.965]
Epoch [32/120    avg_loss:0.054, val_acc:0.972]
Epoch [33/120    avg_loss:0.048, val_acc:0.972]
Epoch [34/120    avg_loss:0.047, val_acc:0.959]
Epoch [35/120    avg_loss:0.038, val_acc:0.970]
Epoch [36/120    avg_loss:0.035, val_acc:0.976]
Epoch [37/120    avg_loss:0.044, val_acc:0.972]
Epoch [38/120    avg_loss:0.035, val_acc:0.975]
Epoch [39/120    avg_loss:0.031, val_acc:0.966]
Epoch [40/120    avg_loss:0.038, val_acc:0.974]
Epoch [41/120    avg_loss:0.022, val_acc:0.978]
Epoch [42/120    avg_loss:0.029, val_acc:0.977]
Epoch [43/120    avg_loss:0.023, val_acc:0.957]
Epoch [44/120    avg_loss:0.033, val_acc:0.974]
Epoch [45/120    avg_loss:0.019, val_acc:0.980]
Epoch [46/120    avg_loss:0.015, val_acc:0.986]
Epoch [47/120    avg_loss:0.016, val_acc:0.983]
Epoch [48/120    avg_loss:0.016, val_acc:0.985]
Epoch [49/120    avg_loss:0.017, val_acc:0.980]
Epoch [50/120    avg_loss:0.015, val_acc:0.980]
Epoch [51/120    avg_loss:0.017, val_acc:0.980]
Epoch [52/120    avg_loss:0.010, val_acc:0.985]
Epoch [53/120    avg_loss:0.010, val_acc:0.982]
Epoch [54/120    avg_loss:0.010, val_acc:0.980]
Epoch [55/120    avg_loss:0.012, val_acc:0.983]
Epoch [56/120    avg_loss:0.012, val_acc:0.984]
Epoch [57/120    avg_loss:0.009, val_acc:0.981]
Epoch [58/120    avg_loss:0.012, val_acc:0.978]
Epoch [59/120    avg_loss:0.013, val_acc:0.984]
Epoch [60/120    avg_loss:0.013, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.986]
Epoch [62/120    avg_loss:0.006, val_acc:0.987]
Epoch [63/120    avg_loss:0.007, val_acc:0.987]
Epoch [64/120    avg_loss:0.007, val_acc:0.987]
Epoch [65/120    avg_loss:0.006, val_acc:0.987]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.007, val_acc:0.988]
Epoch [68/120    avg_loss:0.006, val_acc:0.988]
Epoch [69/120    avg_loss:0.008, val_acc:0.987]
Epoch [70/120    avg_loss:0.006, val_acc:0.987]
Epoch [71/120    avg_loss:0.006, val_acc:0.988]
Epoch [72/120    avg_loss:0.006, val_acc:0.989]
Epoch [73/120    avg_loss:0.006, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.989]
Epoch [77/120    avg_loss:0.008, val_acc:0.989]
Epoch [78/120    avg_loss:0.006, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.989]
Epoch [81/120    avg_loss:0.005, val_acc:0.989]
Epoch [82/120    avg_loss:0.010, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.989]
Epoch [88/120    avg_loss:0.008, val_acc:0.989]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     0     0     0     0     5     2     0]
 [    0     0 18011     0    73     0     6     0     0     0]
 [    0     4     0  2027     2     0     0     0     3     0]
 [    0    33    10     0  2893     0     8     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4877     0     1     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     0     0     0    60     0     0     0  3510     1]
 [    0     0     0     4    14    32     0     0     0   869]]

Accuracy:
99.30349697539344

F1 scores:
[       nan 0.99658756 0.99753538 0.99680354 0.96208846 0.98788796
 0.99826016 0.99690163 0.98664793 0.97094972]

Kappa:
0.9907771635591165
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc99599a828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.856, val_acc:0.317]
Epoch [2/120    avg_loss:1.387, val_acc:0.405]
Epoch [3/120    avg_loss:1.129, val_acc:0.575]
Epoch [4/120    avg_loss:0.925, val_acc:0.684]
Epoch [5/120    avg_loss:0.769, val_acc:0.672]
Epoch [6/120    avg_loss:0.605, val_acc:0.746]
Epoch [7/120    avg_loss:0.510, val_acc:0.793]
Epoch [8/120    avg_loss:0.426, val_acc:0.784]
Epoch [9/120    avg_loss:0.367, val_acc:0.847]
Epoch [10/120    avg_loss:0.322, val_acc:0.828]
Epoch [11/120    avg_loss:0.304, val_acc:0.843]
Epoch [12/120    avg_loss:0.321, val_acc:0.899]
Epoch [13/120    avg_loss:0.282, val_acc:0.921]
Epoch [14/120    avg_loss:0.210, val_acc:0.911]
Epoch [15/120    avg_loss:0.167, val_acc:0.907]
Epoch [16/120    avg_loss:0.187, val_acc:0.938]
Epoch [17/120    avg_loss:0.136, val_acc:0.960]
Epoch [18/120    avg_loss:0.146, val_acc:0.929]
Epoch [19/120    avg_loss:0.197, val_acc:0.938]
Epoch [20/120    avg_loss:0.142, val_acc:0.937]
Epoch [21/120    avg_loss:0.133, val_acc:0.934]
Epoch [22/120    avg_loss:0.116, val_acc:0.942]
Epoch [23/120    avg_loss:0.085, val_acc:0.942]
Epoch [24/120    avg_loss:0.076, val_acc:0.966]
Epoch [25/120    avg_loss:0.077, val_acc:0.957]
Epoch [26/120    avg_loss:0.073, val_acc:0.965]
Epoch [27/120    avg_loss:0.078, val_acc:0.963]
Epoch [28/120    avg_loss:0.074, val_acc:0.963]
Epoch [29/120    avg_loss:0.065, val_acc:0.969]
Epoch [30/120    avg_loss:0.068, val_acc:0.970]
Epoch [31/120    avg_loss:0.055, val_acc:0.967]
Epoch [32/120    avg_loss:0.038, val_acc:0.978]
Epoch [33/120    avg_loss:0.033, val_acc:0.978]
Epoch [34/120    avg_loss:0.026, val_acc:0.979]
Epoch [35/120    avg_loss:0.027, val_acc:0.976]
Epoch [36/120    avg_loss:0.031, val_acc:0.970]
Epoch [37/120    avg_loss:0.030, val_acc:0.979]
Epoch [38/120    avg_loss:0.024, val_acc:0.983]
Epoch [39/120    avg_loss:0.029, val_acc:0.983]
Epoch [40/120    avg_loss:0.021, val_acc:0.983]
Epoch [41/120    avg_loss:0.018, val_acc:0.983]
Epoch [42/120    avg_loss:0.019, val_acc:0.986]
Epoch [43/120    avg_loss:0.014, val_acc:0.986]
Epoch [44/120    avg_loss:0.016, val_acc:0.988]
Epoch [45/120    avg_loss:0.014, val_acc:0.979]
Epoch [46/120    avg_loss:0.023, val_acc:0.979]
Epoch [47/120    avg_loss:0.036, val_acc:0.975]
Epoch [48/120    avg_loss:0.023, val_acc:0.982]
Epoch [49/120    avg_loss:0.022, val_acc:0.982]
Epoch [50/120    avg_loss:0.017, val_acc:0.981]
Epoch [51/120    avg_loss:0.090, val_acc:0.943]
Epoch [52/120    avg_loss:0.075, val_acc:0.975]
Epoch [53/120    avg_loss:0.031, val_acc:0.979]
Epoch [54/120    avg_loss:0.026, val_acc:0.963]
Epoch [55/120    avg_loss:0.031, val_acc:0.976]
Epoch [56/120    avg_loss:0.019, val_acc:0.979]
Epoch [57/120    avg_loss:0.017, val_acc:0.975]
Epoch [58/120    avg_loss:0.013, val_acc:0.979]
Epoch [59/120    avg_loss:0.011, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.010, val_acc:0.984]
Epoch [63/120    avg_loss:0.012, val_acc:0.983]
Epoch [64/120    avg_loss:0.010, val_acc:0.983]
Epoch [65/120    avg_loss:0.011, val_acc:0.984]
Epoch [66/120    avg_loss:0.011, val_acc:0.985]
Epoch [67/120    avg_loss:0.009, val_acc:0.985]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.010, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.010, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.016, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.986]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.011, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.011, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     0     6     0    12    38     2     0]
 [    0     0 18033     0    28     0    21     0     8     0]
 [    0     0     0  2008     8     0     0     0    12     8]
 [    0    55    21     0  2863     0     7     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     0     3]
 [    0     0     0     0     0     0     8  1281     0     1]
 [    0     2     0     0    50     0     0     0  3518     1]
 [    0     0     0     0    15    67     0     0     0   837]]

Accuracy:
99.03839201793073

F1 scores:
[       nan 0.99105963 0.99784197 0.99307616 0.9636486  0.97497198
 0.99479645 0.98198544 0.9858484  0.94629734]

Kappa:
0.987262598682861
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2393c5b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.860, val_acc:0.187]
Epoch [2/120    avg_loss:1.376, val_acc:0.354]
Epoch [3/120    avg_loss:1.167, val_acc:0.454]
Epoch [4/120    avg_loss:0.975, val_acc:0.643]
Epoch [5/120    avg_loss:0.806, val_acc:0.595]
Epoch [6/120    avg_loss:0.640, val_acc:0.648]
Epoch [7/120    avg_loss:0.527, val_acc:0.694]
Epoch [8/120    avg_loss:0.477, val_acc:0.831]
Epoch [9/120    avg_loss:0.390, val_acc:0.852]
Epoch [10/120    avg_loss:0.313, val_acc:0.920]
Epoch [11/120    avg_loss:0.268, val_acc:0.886]
Epoch [12/120    avg_loss:0.270, val_acc:0.922]
Epoch [13/120    avg_loss:0.239, val_acc:0.922]
Epoch [14/120    avg_loss:0.213, val_acc:0.941]
Epoch [15/120    avg_loss:0.164, val_acc:0.954]
Epoch [16/120    avg_loss:0.127, val_acc:0.948]
Epoch [17/120    avg_loss:0.116, val_acc:0.963]
Epoch [18/120    avg_loss:0.097, val_acc:0.953]
Epoch [19/120    avg_loss:0.107, val_acc:0.968]
Epoch [20/120    avg_loss:0.083, val_acc:0.966]
Epoch [21/120    avg_loss:0.084, val_acc:0.972]
Epoch [22/120    avg_loss:0.073, val_acc:0.967]
Epoch [23/120    avg_loss:0.087, val_acc:0.957]
Epoch [24/120    avg_loss:0.074, val_acc:0.965]
Epoch [25/120    avg_loss:0.097, val_acc:0.974]
Epoch [26/120    avg_loss:0.067, val_acc:0.972]
Epoch [27/120    avg_loss:0.058, val_acc:0.961]
Epoch [28/120    avg_loss:0.043, val_acc:0.978]
Epoch [29/120    avg_loss:0.042, val_acc:0.955]
Epoch [30/120    avg_loss:0.040, val_acc:0.987]
Epoch [31/120    avg_loss:0.029, val_acc:0.985]
Epoch [32/120    avg_loss:0.029, val_acc:0.969]
Epoch [33/120    avg_loss:0.037, val_acc:0.977]
Epoch [34/120    avg_loss:0.034, val_acc:0.975]
Epoch [35/120    avg_loss:0.044, val_acc:0.983]
Epoch [36/120    avg_loss:0.050, val_acc:0.978]
Epoch [37/120    avg_loss:0.028, val_acc:0.985]
Epoch [38/120    avg_loss:0.020, val_acc:0.984]
Epoch [39/120    avg_loss:0.022, val_acc:0.984]
Epoch [40/120    avg_loss:0.017, val_acc:0.988]
Epoch [41/120    avg_loss:0.028, val_acc:0.977]
Epoch [42/120    avg_loss:0.036, val_acc:0.972]
Epoch [43/120    avg_loss:0.022, val_acc:0.987]
Epoch [44/120    avg_loss:0.014, val_acc:0.988]
Epoch [45/120    avg_loss:0.027, val_acc:0.978]
Epoch [46/120    avg_loss:0.019, val_acc:0.982]
Epoch [47/120    avg_loss:0.019, val_acc:0.978]
Epoch [48/120    avg_loss:0.021, val_acc:0.980]
Epoch [49/120    avg_loss:0.026, val_acc:0.980]
Epoch [50/120    avg_loss:0.017, val_acc:0.987]
Epoch [51/120    avg_loss:0.014, val_acc:0.984]
Epoch [52/120    avg_loss:0.018, val_acc:0.978]
Epoch [53/120    avg_loss:0.021, val_acc:0.983]
Epoch [54/120    avg_loss:0.022, val_acc:0.985]
Epoch [55/120    avg_loss:0.019, val_acc:0.989]
Epoch [56/120    avg_loss:0.013, val_acc:0.975]
Epoch [57/120    avg_loss:0.010, val_acc:0.991]
Epoch [58/120    avg_loss:0.008, val_acc:0.985]
Epoch [59/120    avg_loss:0.009, val_acc:0.984]
Epoch [60/120    avg_loss:0.014, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.988]
Epoch [62/120    avg_loss:0.010, val_acc:0.990]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.007, val_acc:0.988]
Epoch [65/120    avg_loss:0.011, val_acc:0.990]
Epoch [66/120    avg_loss:0.008, val_acc:0.989]
Epoch [67/120    avg_loss:0.006, val_acc:0.990]
Epoch [68/120    avg_loss:0.005, val_acc:0.992]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.021, val_acc:0.979]
Epoch [71/120    avg_loss:0.016, val_acc:0.978]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.006, val_acc:0.984]
Epoch [74/120    avg_loss:0.006, val_acc:0.989]
Epoch [75/120    avg_loss:0.006, val_acc:0.990]
Epoch [76/120    avg_loss:0.018, val_acc:0.974]
Epoch [77/120    avg_loss:0.022, val_acc:0.972]
Epoch [78/120    avg_loss:0.028, val_acc:0.990]
Epoch [79/120    avg_loss:0.029, val_acc:0.968]
Epoch [80/120    avg_loss:0.019, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.007, val_acc:0.991]
Epoch [90/120    avg_loss:0.005, val_acc:0.991]
Epoch [91/120    avg_loss:0.006, val_acc:0.991]
Epoch [92/120    avg_loss:0.008, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.991]
Epoch [98/120    avg_loss:0.008, val_acc:0.991]
Epoch [99/120    avg_loss:0.006, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.006, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     9     0     0     0    16     0     0]
 [    0     0 18069     0    18     0     3     0     0     0]
 [    0     1     0  2023     4     0     0     0     4     4]
 [    0    47    19     0  2870     0     8     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4861     0     0    17]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     9     0     0    52     0     0     0  3491    19]
 [    0     0     0     2    15    47     0     2     0   853]]

Accuracy:
99.21432530788326

F1 scores:
[       nan 0.99364144 0.99889436 0.99410319 0.96779632 0.98231088
 0.99692371 0.99229584 0.98421201 0.9415011 ]

Kappa:
0.9895893830464276
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f44b1c32860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.856, val_acc:0.220]
Epoch [2/120    avg_loss:1.430, val_acc:0.337]
Epoch [3/120    avg_loss:1.169, val_acc:0.489]
Epoch [4/120    avg_loss:0.988, val_acc:0.549]
Epoch [5/120    avg_loss:0.851, val_acc:0.573]
Epoch [6/120    avg_loss:0.701, val_acc:0.694]
Epoch [7/120    avg_loss:0.584, val_acc:0.759]
Epoch [8/120    avg_loss:0.494, val_acc:0.773]
Epoch [9/120    avg_loss:0.428, val_acc:0.787]
Epoch [10/120    avg_loss:0.363, val_acc:0.866]
Epoch [11/120    avg_loss:0.291, val_acc:0.901]
Epoch [12/120    avg_loss:0.229, val_acc:0.930]
Epoch [13/120    avg_loss:0.224, val_acc:0.910]
Epoch [14/120    avg_loss:0.192, val_acc:0.946]
Epoch [15/120    avg_loss:0.212, val_acc:0.934]
Epoch [16/120    avg_loss:0.158, val_acc:0.948]
Epoch [17/120    avg_loss:0.161, val_acc:0.932]
Epoch [18/120    avg_loss:0.144, val_acc:0.951]
Epoch [19/120    avg_loss:0.166, val_acc:0.931]
Epoch [20/120    avg_loss:0.139, val_acc:0.957]
Epoch [21/120    avg_loss:0.118, val_acc:0.954]
Epoch [22/120    avg_loss:0.097, val_acc:0.965]
Epoch [23/120    avg_loss:0.068, val_acc:0.974]
Epoch [24/120    avg_loss:0.067, val_acc:0.957]
Epoch [25/120    avg_loss:0.061, val_acc:0.967]
Epoch [26/120    avg_loss:0.070, val_acc:0.971]
Epoch [27/120    avg_loss:0.235, val_acc:0.938]
Epoch [28/120    avg_loss:0.116, val_acc:0.976]
Epoch [29/120    avg_loss:0.078, val_acc:0.938]
Epoch [30/120    avg_loss:0.106, val_acc:0.963]
Epoch [31/120    avg_loss:0.065, val_acc:0.973]
Epoch [32/120    avg_loss:0.062, val_acc:0.978]
Epoch [33/120    avg_loss:0.051, val_acc:0.974]
Epoch [34/120    avg_loss:0.047, val_acc:0.981]
Epoch [35/120    avg_loss:0.051, val_acc:0.965]
Epoch [36/120    avg_loss:0.045, val_acc:0.972]
Epoch [37/120    avg_loss:0.034, val_acc:0.983]
Epoch [38/120    avg_loss:0.048, val_acc:0.959]
Epoch [39/120    avg_loss:0.074, val_acc:0.963]
Epoch [40/120    avg_loss:0.038, val_acc:0.979]
Epoch [41/120    avg_loss:0.025, val_acc:0.984]
Epoch [42/120    avg_loss:0.023, val_acc:0.984]
Epoch [43/120    avg_loss:0.021, val_acc:0.987]
Epoch [44/120    avg_loss:0.021, val_acc:0.984]
Epoch [45/120    avg_loss:0.019, val_acc:0.984]
Epoch [46/120    avg_loss:0.022, val_acc:0.988]
Epoch [47/120    avg_loss:0.031, val_acc:0.968]
Epoch [48/120    avg_loss:0.078, val_acc:0.971]
Epoch [49/120    avg_loss:0.034, val_acc:0.973]
Epoch [50/120    avg_loss:0.035, val_acc:0.983]
Epoch [51/120    avg_loss:0.030, val_acc:0.982]
Epoch [52/120    avg_loss:0.023, val_acc:0.980]
Epoch [53/120    avg_loss:0.022, val_acc:0.985]
Epoch [54/120    avg_loss:0.023, val_acc:0.977]
Epoch [55/120    avg_loss:0.016, val_acc:0.981]
Epoch [56/120    avg_loss:0.013, val_acc:0.984]
Epoch [57/120    avg_loss:0.017, val_acc:0.975]
Epoch [58/120    avg_loss:0.018, val_acc:0.979]
Epoch [59/120    avg_loss:0.013, val_acc:0.983]
Epoch [60/120    avg_loss:0.011, val_acc:0.984]
Epoch [61/120    avg_loss:0.008, val_acc:0.985]
Epoch [62/120    avg_loss:0.010, val_acc:0.985]
Epoch [63/120    avg_loss:0.009, val_acc:0.985]
Epoch [64/120    avg_loss:0.010, val_acc:0.986]
Epoch [65/120    avg_loss:0.009, val_acc:0.985]
Epoch [66/120    avg_loss:0.010, val_acc:0.985]
Epoch [67/120    avg_loss:0.010, val_acc:0.987]
Epoch [68/120    avg_loss:0.008, val_acc:0.986]
Epoch [69/120    avg_loss:0.011, val_acc:0.986]
Epoch [70/120    avg_loss:0.014, val_acc:0.987]
Epoch [71/120    avg_loss:0.011, val_acc:0.987]
Epoch [72/120    avg_loss:0.008, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.009, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.011, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.011, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6364     0     0     1     0    28     1     9    29]
 [    0     0 18046     0    40     0     2     0     2     0]
 [    0     5     0  2020     0     0     0     0     9     2]
 [    0    36    20     0  2885     0     0     0    31     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4835     0     0    30]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     1     0     0    68     0     0     0  3482    20]
 [    0     0     0     4    14    40     0     0     0   861]]

Accuracy:
99.01670161232015

F1 scores:
[       nan 0.99143169 0.99787111 0.99507389 0.96488294 0.98490566
 0.99230375 0.99844841 0.98029279 0.92481203]

Kappa:
0.9869747257571032
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcce29197f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.888, val_acc:0.245]
Epoch [2/120    avg_loss:1.417, val_acc:0.296]
Epoch [3/120    avg_loss:1.153, val_acc:0.391]
Epoch [4/120    avg_loss:0.981, val_acc:0.588]
Epoch [5/120    avg_loss:0.819, val_acc:0.686]
Epoch [6/120    avg_loss:0.703, val_acc:0.721]
Epoch [7/120    avg_loss:0.554, val_acc:0.776]
Epoch [8/120    avg_loss:0.479, val_acc:0.830]
Epoch [9/120    avg_loss:0.416, val_acc:0.813]
Epoch [10/120    avg_loss:0.334, val_acc:0.872]
Epoch [11/120    avg_loss:0.296, val_acc:0.925]
Epoch [12/120    avg_loss:0.241, val_acc:0.944]
Epoch [13/120    avg_loss:0.205, val_acc:0.944]
Epoch [14/120    avg_loss:0.182, val_acc:0.940]
Epoch [15/120    avg_loss:0.146, val_acc:0.958]
Epoch [16/120    avg_loss:0.142, val_acc:0.922]
Epoch [17/120    avg_loss:0.108, val_acc:0.956]
Epoch [18/120    avg_loss:0.086, val_acc:0.961]
Epoch [19/120    avg_loss:0.094, val_acc:0.954]
Epoch [20/120    avg_loss:0.081, val_acc:0.970]
Epoch [21/120    avg_loss:0.096, val_acc:0.963]
Epoch [22/120    avg_loss:0.085, val_acc:0.972]
Epoch [23/120    avg_loss:0.071, val_acc:0.970]
Epoch [24/120    avg_loss:0.061, val_acc:0.972]
Epoch [25/120    avg_loss:0.053, val_acc:0.979]
Epoch [26/120    avg_loss:0.070, val_acc:0.962]
Epoch [27/120    avg_loss:0.056, val_acc:0.972]
Epoch [28/120    avg_loss:0.039, val_acc:0.979]
Epoch [29/120    avg_loss:0.045, val_acc:0.981]
Epoch [30/120    avg_loss:0.039, val_acc:0.966]
Epoch [31/120    avg_loss:0.024, val_acc:0.984]
Epoch [32/120    avg_loss:0.034, val_acc:0.974]
Epoch [33/120    avg_loss:0.033, val_acc:0.985]
Epoch [34/120    avg_loss:0.023, val_acc:0.988]
Epoch [35/120    avg_loss:0.027, val_acc:0.985]
Epoch [36/120    avg_loss:0.023, val_acc:0.982]
Epoch [37/120    avg_loss:0.019, val_acc:0.986]
Epoch [38/120    avg_loss:0.021, val_acc:0.980]
Epoch [39/120    avg_loss:0.021, val_acc:0.980]
Epoch [40/120    avg_loss:0.021, val_acc:0.990]
Epoch [41/120    avg_loss:0.019, val_acc:0.982]
Epoch [42/120    avg_loss:0.019, val_acc:0.990]
Epoch [43/120    avg_loss:0.018, val_acc:0.990]
Epoch [44/120    avg_loss:0.018, val_acc:0.990]
Epoch [45/120    avg_loss:0.018, val_acc:0.971]
Epoch [46/120    avg_loss:0.036, val_acc:0.986]
Epoch [47/120    avg_loss:0.121, val_acc:0.912]
Epoch [48/120    avg_loss:0.105, val_acc:0.973]
Epoch [49/120    avg_loss:0.071, val_acc:0.981]
Epoch [50/120    avg_loss:0.041, val_acc:0.971]
Epoch [51/120    avg_loss:0.033, val_acc:0.978]
Epoch [52/120    avg_loss:0.031, val_acc:0.984]
Epoch [53/120    avg_loss:0.021, val_acc:0.989]
Epoch [54/120    avg_loss:0.021, val_acc:0.988]
Epoch [55/120    avg_loss:0.016, val_acc:0.986]
Epoch [56/120    avg_loss:0.016, val_acc:0.989]
Epoch [57/120    avg_loss:0.017, val_acc:0.987]
Epoch [58/120    avg_loss:0.010, val_acc:0.991]
Epoch [59/120    avg_loss:0.010, val_acc:0.990]
Epoch [60/120    avg_loss:0.011, val_acc:0.991]
Epoch [61/120    avg_loss:0.012, val_acc:0.991]
Epoch [62/120    avg_loss:0.009, val_acc:0.992]
Epoch [63/120    avg_loss:0.009, val_acc:0.992]
Epoch [64/120    avg_loss:0.011, val_acc:0.992]
Epoch [65/120    avg_loss:0.009, val_acc:0.992]
Epoch [66/120    avg_loss:0.008, val_acc:0.992]
Epoch [67/120    avg_loss:0.009, val_acc:0.993]
Epoch [68/120    avg_loss:0.010, val_acc:0.992]
Epoch [69/120    avg_loss:0.008, val_acc:0.993]
Epoch [70/120    avg_loss:0.009, val_acc:0.993]
Epoch [71/120    avg_loss:0.008, val_acc:0.993]
Epoch [72/120    avg_loss:0.009, val_acc:0.993]
Epoch [73/120    avg_loss:0.007, val_acc:0.992]
Epoch [74/120    avg_loss:0.008, val_acc:0.993]
Epoch [75/120    avg_loss:0.007, val_acc:0.993]
Epoch [76/120    avg_loss:0.008, val_acc:0.993]
Epoch [77/120    avg_loss:0.007, val_acc:0.992]
Epoch [78/120    avg_loss:0.010, val_acc:0.993]
Epoch [79/120    avg_loss:0.008, val_acc:0.993]
Epoch [80/120    avg_loss:0.013, val_acc:0.991]
Epoch [81/120    avg_loss:0.011, val_acc:0.994]
Epoch [82/120    avg_loss:0.009, val_acc:0.993]
Epoch [83/120    avg_loss:0.007, val_acc:0.993]
Epoch [84/120    avg_loss:0.008, val_acc:0.992]
Epoch [85/120    avg_loss:0.008, val_acc:0.992]
Epoch [86/120    avg_loss:0.010, val_acc:0.992]
Epoch [87/120    avg_loss:0.009, val_acc:0.992]
Epoch [88/120    avg_loss:0.006, val_acc:0.992]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.007, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.992]
Epoch [92/120    avg_loss:0.007, val_acc:0.992]
Epoch [93/120    avg_loss:0.009, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.008, val_acc:0.991]
Epoch [96/120    avg_loss:0.009, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.991]
Epoch [98/120    avg_loss:0.008, val_acc:0.991]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.991]
Epoch [101/120    avg_loss:0.008, val_acc:0.992]
Epoch [102/120    avg_loss:0.008, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.007, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.007, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.992]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.008, val_acc:0.992]
Epoch [112/120    avg_loss:0.007, val_acc:0.992]
Epoch [113/120    avg_loss:0.007, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     0     0    23     6     1     0]
 [    0     2 18056     0    30     0     2     0     0     0]
 [    0     0     0  2028     3     0     0     0     2     3]
 [    0    40    20     0  2874     0     5     0    33     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4852     0     0    26]
 [    0     0     0     0     0     0     1  1285     0     4]
 [    0     5     0     0    43     0     0     0  3497    26]
 [    0     0     0     5    14    43     0     0     0   857]]

Accuracy:
99.18781481213699

F1 scores:
[       nan 0.9940222  0.99850688 0.99680511 0.96832884 0.98379193
 0.99416043 0.99573809 0.98451577 0.93405995]

Kappa:
0.9892399641808537
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b442d2828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.889, val_acc:0.233]
Epoch [2/120    avg_loss:1.369, val_acc:0.365]
Epoch [3/120    avg_loss:1.110, val_acc:0.537]
Epoch [4/120    avg_loss:0.946, val_acc:0.501]
Epoch [5/120    avg_loss:0.793, val_acc:0.600]
Epoch [6/120    avg_loss:0.630, val_acc:0.750]
Epoch [7/120    avg_loss:0.497, val_acc:0.772]
Epoch [8/120    avg_loss:0.453, val_acc:0.798]
Epoch [9/120    avg_loss:0.392, val_acc:0.816]
Epoch [10/120    avg_loss:0.325, val_acc:0.851]
Epoch [11/120    avg_loss:0.282, val_acc:0.870]
Epoch [12/120    avg_loss:0.240, val_acc:0.825]
Epoch [13/120    avg_loss:0.197, val_acc:0.939]
Epoch [14/120    avg_loss:0.181, val_acc:0.880]
Epoch [15/120    avg_loss:0.141, val_acc:0.956]
Epoch [16/120    avg_loss:0.132, val_acc:0.947]
Epoch [17/120    avg_loss:0.122, val_acc:0.946]
Epoch [18/120    avg_loss:0.117, val_acc:0.960]
Epoch [19/120    avg_loss:0.103, val_acc:0.950]
Epoch [20/120    avg_loss:0.101, val_acc:0.946]
Epoch [21/120    avg_loss:0.089, val_acc:0.960]
Epoch [22/120    avg_loss:0.076, val_acc:0.978]
Epoch [23/120    avg_loss:0.069, val_acc:0.985]
Epoch [24/120    avg_loss:0.051, val_acc:0.981]
Epoch [25/120    avg_loss:0.058, val_acc:0.965]
Epoch [26/120    avg_loss:0.049, val_acc:0.981]
Epoch [27/120    avg_loss:0.044, val_acc:0.979]
Epoch [28/120    avg_loss:0.031, val_acc:0.988]
Epoch [29/120    avg_loss:0.042, val_acc:0.975]
Epoch [30/120    avg_loss:0.096, val_acc:0.955]
Epoch [31/120    avg_loss:0.059, val_acc:0.979]
Epoch [32/120    avg_loss:0.057, val_acc:0.979]
Epoch [33/120    avg_loss:0.057, val_acc:0.979]
Epoch [34/120    avg_loss:0.049, val_acc:0.986]
Epoch [35/120    avg_loss:0.037, val_acc:0.990]
Epoch [36/120    avg_loss:0.036, val_acc:0.987]
Epoch [37/120    avg_loss:0.025, val_acc:0.985]
Epoch [38/120    avg_loss:0.024, val_acc:0.992]
Epoch [39/120    avg_loss:0.029, val_acc:0.983]
Epoch [40/120    avg_loss:0.027, val_acc:0.989]
Epoch [41/120    avg_loss:0.017, val_acc:0.990]
Epoch [42/120    avg_loss:0.040, val_acc:0.971]
Epoch [43/120    avg_loss:0.048, val_acc:0.988]
Epoch [44/120    avg_loss:0.029, val_acc:0.986]
Epoch [45/120    avg_loss:0.018, val_acc:0.993]
Epoch [46/120    avg_loss:0.014, val_acc:0.990]
Epoch [47/120    avg_loss:0.014, val_acc:0.988]
Epoch [48/120    avg_loss:0.014, val_acc:0.988]
Epoch [49/120    avg_loss:0.012, val_acc:0.994]
Epoch [50/120    avg_loss:0.014, val_acc:0.991]
Epoch [51/120    avg_loss:0.012, val_acc:0.993]
Epoch [52/120    avg_loss:0.010, val_acc:0.995]
Epoch [53/120    avg_loss:0.010, val_acc:0.993]
Epoch [54/120    avg_loss:0.011, val_acc:0.992]
Epoch [55/120    avg_loss:0.013, val_acc:0.994]
Epoch [56/120    avg_loss:0.013, val_acc:0.993]
Epoch [57/120    avg_loss:0.011, val_acc:0.993]
Epoch [58/120    avg_loss:0.008, val_acc:0.993]
Epoch [59/120    avg_loss:0.009, val_acc:0.994]
Epoch [60/120    avg_loss:0.005, val_acc:0.996]
Epoch [61/120    avg_loss:0.009, val_acc:0.990]
Epoch [62/120    avg_loss:0.011, val_acc:0.995]
Epoch [63/120    avg_loss:0.007, val_acc:0.993]
Epoch [64/120    avg_loss:0.009, val_acc:0.992]
Epoch [65/120    avg_loss:0.006, val_acc:0.995]
Epoch [66/120    avg_loss:0.006, val_acc:0.993]
Epoch [67/120    avg_loss:0.012, val_acc:0.964]
Epoch [68/120    avg_loss:0.030, val_acc:0.989]
Epoch [69/120    avg_loss:0.021, val_acc:0.992]
Epoch [70/120    avg_loss:0.017, val_acc:0.986]
Epoch [71/120    avg_loss:0.010, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.992]
Epoch [73/120    avg_loss:0.007, val_acc:0.996]
Epoch [74/120    avg_loss:0.006, val_acc:0.995]
Epoch [75/120    avg_loss:0.005, val_acc:0.997]
Epoch [76/120    avg_loss:0.005, val_acc:0.997]
Epoch [77/120    avg_loss:0.006, val_acc:0.996]
Epoch [78/120    avg_loss:0.006, val_acc:0.995]
Epoch [79/120    avg_loss:0.005, val_acc:0.996]
Epoch [80/120    avg_loss:0.004, val_acc:0.997]
Epoch [81/120    avg_loss:0.006, val_acc:0.997]
Epoch [82/120    avg_loss:0.005, val_acc:0.996]
Epoch [83/120    avg_loss:0.005, val_acc:0.997]
Epoch [84/120    avg_loss:0.006, val_acc:0.995]
Epoch [85/120    avg_loss:0.014, val_acc:0.994]
Epoch [86/120    avg_loss:0.010, val_acc:0.996]
Epoch [87/120    avg_loss:0.011, val_acc:0.994]
Epoch [88/120    avg_loss:0.009, val_acc:0.996]
Epoch [89/120    avg_loss:0.005, val_acc:0.996]
Epoch [90/120    avg_loss:0.010, val_acc:0.994]
Epoch [91/120    avg_loss:0.010, val_acc:0.997]
Epoch [92/120    avg_loss:0.007, val_acc:0.994]
Epoch [93/120    avg_loss:0.008, val_acc:0.998]
Epoch [94/120    avg_loss:0.005, val_acc:0.998]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.996]
Epoch [98/120    avg_loss:0.010, val_acc:0.996]
Epoch [99/120    avg_loss:0.007, val_acc:0.996]
Epoch [100/120    avg_loss:0.004, val_acc:0.996]
Epoch [101/120    avg_loss:0.003, val_acc:0.997]
Epoch [102/120    avg_loss:0.004, val_acc:0.996]
Epoch [103/120    avg_loss:0.003, val_acc:0.998]
Epoch [104/120    avg_loss:0.004, val_acc:0.996]
Epoch [105/120    avg_loss:0.003, val_acc:0.995]
Epoch [106/120    avg_loss:0.003, val_acc:0.998]
Epoch [107/120    avg_loss:0.003, val_acc:0.997]
Epoch [108/120    avg_loss:0.002, val_acc:0.996]
Epoch [109/120    avg_loss:0.002, val_acc:0.997]
Epoch [110/120    avg_loss:0.002, val_acc:0.996]
Epoch [111/120    avg_loss:0.002, val_acc:0.997]
Epoch [112/120    avg_loss:0.003, val_acc:0.998]
Epoch [113/120    avg_loss:0.003, val_acc:0.998]
Epoch [114/120    avg_loss:0.002, val_acc:0.998]
Epoch [115/120    avg_loss:0.002, val_acc:0.997]
Epoch [116/120    avg_loss:0.003, val_acc:0.997]
Epoch [117/120    avg_loss:0.002, val_acc:0.997]
Epoch [118/120    avg_loss:0.004, val_acc:0.997]
Epoch [119/120    avg_loss:0.004, val_acc:0.996]
Epoch [120/120    avg_loss:0.004, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     1     0     0     0     0     0]
 [    0     5 17992     0    90     0     3     0     0     0]
 [    0     0     0  2025     1     0     0     0     2     8]
 [    0    50    18     0  2877     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4855     0     0    21]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     3     0     0    29     0     0     0  3539     0]
 [    0     0     0     1    14    32     0     0     0   872]]

Accuracy:
99.26011616417227

F1 scores:
[       nan 0.99543379 0.9967867  0.99655512 0.96156417 0.98788796
 0.9973295  1.         0.99145539 0.95824176]

Kappa:
0.990204091740258
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2cc5f6b7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.854, val_acc:0.233]
Epoch [2/120    avg_loss:1.347, val_acc:0.384]
Epoch [3/120    avg_loss:1.091, val_acc:0.585]
Epoch [4/120    avg_loss:0.973, val_acc:0.629]
Epoch [5/120    avg_loss:0.791, val_acc:0.641]
Epoch [6/120    avg_loss:0.683, val_acc:0.741]
Epoch [7/120    avg_loss:0.573, val_acc:0.753]
Epoch [8/120    avg_loss:0.466, val_acc:0.779]
Epoch [9/120    avg_loss:0.378, val_acc:0.897]
Epoch [10/120    avg_loss:0.366, val_acc:0.856]
Epoch [11/120    avg_loss:0.250, val_acc:0.914]
Epoch [12/120    avg_loss:0.206, val_acc:0.922]
Epoch [13/120    avg_loss:0.161, val_acc:0.868]
Epoch [14/120    avg_loss:0.191, val_acc:0.940]
Epoch [15/120    avg_loss:0.167, val_acc:0.956]
Epoch [16/120    avg_loss:0.173, val_acc:0.916]
Epoch [17/120    avg_loss:0.157, val_acc:0.934]
Epoch [18/120    avg_loss:0.143, val_acc:0.955]
Epoch [19/120    avg_loss:0.135, val_acc:0.956]
Epoch [20/120    avg_loss:0.098, val_acc:0.968]
Epoch [21/120    avg_loss:0.093, val_acc:0.952]
Epoch [22/120    avg_loss:0.077, val_acc:0.970]
Epoch [23/120    avg_loss:0.063, val_acc:0.971]
Epoch [24/120    avg_loss:0.050, val_acc:0.978]
Epoch [25/120    avg_loss:0.054, val_acc:0.976]
Epoch [26/120    avg_loss:0.046, val_acc:0.980]
Epoch [27/120    avg_loss:0.053, val_acc:0.941]
Epoch [28/120    avg_loss:0.055, val_acc:0.961]
Epoch [29/120    avg_loss:0.067, val_acc:0.918]
Epoch [30/120    avg_loss:0.074, val_acc:0.981]
Epoch [31/120    avg_loss:0.075, val_acc:0.976]
Epoch [32/120    avg_loss:0.071, val_acc:0.962]
Epoch [33/120    avg_loss:0.046, val_acc:0.975]
Epoch [34/120    avg_loss:0.031, val_acc:0.978]
Epoch [35/120    avg_loss:0.037, val_acc:0.978]
Epoch [36/120    avg_loss:0.041, val_acc:0.984]
Epoch [37/120    avg_loss:0.037, val_acc:0.948]
Epoch [38/120    avg_loss:0.029, val_acc:0.978]
Epoch [39/120    avg_loss:0.026, val_acc:0.984]
Epoch [40/120    avg_loss:0.017, val_acc:0.982]
Epoch [41/120    avg_loss:0.016, val_acc:0.984]
Epoch [42/120    avg_loss:0.016, val_acc:0.990]
Epoch [43/120    avg_loss:0.016, val_acc:0.981]
Epoch [44/120    avg_loss:0.020, val_acc:0.984]
Epoch [45/120    avg_loss:0.024, val_acc:0.983]
Epoch [46/120    avg_loss:0.013, val_acc:0.991]
Epoch [47/120    avg_loss:0.011, val_acc:0.989]
Epoch [48/120    avg_loss:0.016, val_acc:0.991]
Epoch [49/120    avg_loss:0.017, val_acc:0.963]
Epoch [50/120    avg_loss:0.022, val_acc:0.986]
Epoch [51/120    avg_loss:0.020, val_acc:0.981]
Epoch [52/120    avg_loss:0.011, val_acc:0.989]
Epoch [53/120    avg_loss:0.010, val_acc:0.989]
Epoch [54/120    avg_loss:0.014, val_acc:0.989]
Epoch [55/120    avg_loss:0.020, val_acc:0.984]
Epoch [56/120    avg_loss:0.028, val_acc:0.984]
Epoch [57/120    avg_loss:0.014, val_acc:0.990]
Epoch [58/120    avg_loss:0.010, val_acc:0.987]
Epoch [59/120    avg_loss:0.011, val_acc:0.973]
Epoch [60/120    avg_loss:0.009, val_acc:0.991]
Epoch [61/120    avg_loss:0.010, val_acc:0.989]
Epoch [62/120    avg_loss:0.010, val_acc:0.989]
Epoch [63/120    avg_loss:0.007, val_acc:0.989]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.006, val_acc:0.989]
Epoch [66/120    avg_loss:0.007, val_acc:0.988]
Epoch [67/120    avg_loss:0.008, val_acc:0.988]
Epoch [68/120    avg_loss:0.009, val_acc:0.989]
Epoch [69/120    avg_loss:0.006, val_acc:0.990]
Epoch [70/120    avg_loss:0.007, val_acc:0.990]
Epoch [71/120    avg_loss:0.006, val_acc:0.990]
Epoch [72/120    avg_loss:0.008, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.007, val_acc:0.991]
Epoch [76/120    avg_loss:0.006, val_acc:0.991]
Epoch [77/120    avg_loss:0.007, val_acc:0.991]
Epoch [78/120    avg_loss:0.009, val_acc:0.989]
Epoch [79/120    avg_loss:0.007, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.006, val_acc:0.990]
Epoch [82/120    avg_loss:0.008, val_acc:0.991]
Epoch [83/120    avg_loss:0.010, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.006, val_acc:0.990]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.006, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.007, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.009, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6388     0     0     0     0     9     0    32     3]
 [    0     0 18047     0    39     0     4     0     0     0]
 [    0     0     0  2027     2     0     0     0     2     5]
 [    0    48    19     0  2871     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4853     0     0    24]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     3     0    17    42     0     0     0  3493    16]
 [    0     0     0     0    14    45     0     0     0   860]]

Accuracy:
99.12756368544092

F1 scores:
[       nan 0.99261907 0.9982576  0.99362745 0.96666667 0.98305085
 0.99538509 0.99883586 0.98049123 0.93989071]

Kappa:
0.9884432873468355
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb41d565860>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.938, val_acc:0.209]
Epoch [2/120    avg_loss:1.435, val_acc:0.415]
Epoch [3/120    avg_loss:1.132, val_acc:0.518]
Epoch [4/120    avg_loss:0.941, val_acc:0.490]
Epoch [5/120    avg_loss:0.778, val_acc:0.612]
Epoch [6/120    avg_loss:0.641, val_acc:0.736]
Epoch [7/120    avg_loss:0.543, val_acc:0.785]
Epoch [8/120    avg_loss:0.432, val_acc:0.722]
Epoch [9/120    avg_loss:0.368, val_acc:0.781]
Epoch [10/120    avg_loss:0.365, val_acc:0.784]
Epoch [11/120    avg_loss:0.344, val_acc:0.801]
Epoch [12/120    avg_loss:0.265, val_acc:0.861]
Epoch [13/120    avg_loss:0.265, val_acc:0.841]
Epoch [14/120    avg_loss:0.240, val_acc:0.844]
Epoch [15/120    avg_loss:0.197, val_acc:0.859]
Epoch [16/120    avg_loss:0.183, val_acc:0.849]
Epoch [17/120    avg_loss:0.155, val_acc:0.953]
Epoch [18/120    avg_loss:0.112, val_acc:0.946]
Epoch [19/120    avg_loss:0.097, val_acc:0.966]
Epoch [20/120    avg_loss:0.089, val_acc:0.966]
Epoch [21/120    avg_loss:0.080, val_acc:0.924]
Epoch [22/120    avg_loss:0.072, val_acc:0.954]
Epoch [23/120    avg_loss:0.062, val_acc:0.972]
Epoch [24/120    avg_loss:0.088, val_acc:0.955]
Epoch [25/120    avg_loss:0.119, val_acc:0.958]
Epoch [26/120    avg_loss:0.065, val_acc:0.967]
Epoch [27/120    avg_loss:0.065, val_acc:0.976]
Epoch [28/120    avg_loss:0.048, val_acc:0.968]
Epoch [29/120    avg_loss:0.054, val_acc:0.979]
Epoch [30/120    avg_loss:0.047, val_acc:0.967]
Epoch [31/120    avg_loss:0.042, val_acc:0.980]
Epoch [32/120    avg_loss:0.072, val_acc:0.970]
Epoch [33/120    avg_loss:0.069, val_acc:0.969]
Epoch [34/120    avg_loss:0.040, val_acc:0.951]
Epoch [35/120    avg_loss:0.091, val_acc:0.947]
Epoch [36/120    avg_loss:0.082, val_acc:0.972]
Epoch [37/120    avg_loss:0.045, val_acc:0.967]
Epoch [38/120    avg_loss:0.044, val_acc:0.969]
Epoch [39/120    avg_loss:0.031, val_acc:0.976]
Epoch [40/120    avg_loss:0.030, val_acc:0.988]
Epoch [41/120    avg_loss:0.026, val_acc:0.984]
Epoch [42/120    avg_loss:0.021, val_acc:0.988]
Epoch [43/120    avg_loss:0.019, val_acc:0.984]
Epoch [44/120    avg_loss:0.022, val_acc:0.982]
Epoch [45/120    avg_loss:0.019, val_acc:0.984]
Epoch [46/120    avg_loss:0.024, val_acc:0.980]
Epoch [47/120    avg_loss:0.030, val_acc:0.973]
Epoch [48/120    avg_loss:0.032, val_acc:0.984]
Epoch [49/120    avg_loss:0.026, val_acc:0.979]
Epoch [50/120    avg_loss:0.021, val_acc:0.973]
Epoch [51/120    avg_loss:0.020, val_acc:0.978]
Epoch [52/120    avg_loss:0.017, val_acc:0.987]
Epoch [53/120    avg_loss:0.013, val_acc:0.983]
Epoch [54/120    avg_loss:0.012, val_acc:0.987]
Epoch [55/120    avg_loss:0.011, val_acc:0.983]
Epoch [56/120    avg_loss:0.011, val_acc:0.986]
Epoch [57/120    avg_loss:0.008, val_acc:0.987]
Epoch [58/120    avg_loss:0.009, val_acc:0.988]
Epoch [59/120    avg_loss:0.009, val_acc:0.986]
Epoch [60/120    avg_loss:0.008, val_acc:0.988]
Epoch [61/120    avg_loss:0.008, val_acc:0.988]
Epoch [62/120    avg_loss:0.007, val_acc:0.988]
Epoch [63/120    avg_loss:0.008, val_acc:0.988]
Epoch [64/120    avg_loss:0.009, val_acc:0.988]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.008, val_acc:0.989]
Epoch [67/120    avg_loss:0.009, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.989]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.007, val_acc:0.988]
Epoch [73/120    avg_loss:0.007, val_acc:0.988]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.006, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.009, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.989]
Epoch [94/120    avg_loss:0.010, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.006, val_acc:0.991]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.008, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     3     2     0     0    15     7     0]
 [    0     0 18035     0    43     0    10     0     2     0]
 [    0     1     0  2012     2     0     0     0    16     5]
 [    0    48    18     0  2875     0     0     0    31     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     0     3]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     4     0     0    52     0     0     0  3493    22]
 [    0     0     0     0    17    51     0     0     0   851]]

Accuracy:
99.1492540910515

F1 scores:
[       nan 0.99379364 0.99798025 0.99333498 0.96427972 0.98083427
 0.99856616 0.99383192 0.98117978 0.94555556]

Kappa:
0.9887312504452395
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75557e0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.836, val_acc:0.239]
Epoch [2/120    avg_loss:1.392, val_acc:0.380]
Epoch [3/120    avg_loss:1.125, val_acc:0.562]
Epoch [4/120    avg_loss:0.966, val_acc:0.653]
Epoch [5/120    avg_loss:0.787, val_acc:0.678]
Epoch [6/120    avg_loss:0.670, val_acc:0.741]
Epoch [7/120    avg_loss:0.539, val_acc:0.766]
Epoch [8/120    avg_loss:0.443, val_acc:0.784]
Epoch [9/120    avg_loss:0.378, val_acc:0.797]
Epoch [10/120    avg_loss:0.347, val_acc:0.845]
Epoch [11/120    avg_loss:0.293, val_acc:0.913]
Epoch [12/120    avg_loss:0.276, val_acc:0.836]
Epoch [13/120    avg_loss:0.221, val_acc:0.947]
Epoch [14/120    avg_loss:0.179, val_acc:0.877]
Epoch [15/120    avg_loss:0.175, val_acc:0.914]
Epoch [16/120    avg_loss:0.155, val_acc:0.930]
Epoch [17/120    avg_loss:0.119, val_acc:0.943]
Epoch [18/120    avg_loss:0.114, val_acc:0.967]
Epoch [19/120    avg_loss:0.124, val_acc:0.959]
Epoch [20/120    avg_loss:0.139, val_acc:0.954]
Epoch [21/120    avg_loss:0.100, val_acc:0.972]
Epoch [22/120    avg_loss:0.109, val_acc:0.944]
Epoch [23/120    avg_loss:0.084, val_acc:0.971]
Epoch [24/120    avg_loss:0.061, val_acc:0.950]
Epoch [25/120    avg_loss:0.056, val_acc:0.973]
Epoch [26/120    avg_loss:0.073, val_acc:0.969]
Epoch [27/120    avg_loss:0.054, val_acc:0.965]
Epoch [28/120    avg_loss:0.052, val_acc:0.977]
Epoch [29/120    avg_loss:0.057, val_acc:0.973]
Epoch [30/120    avg_loss:0.057, val_acc:0.969]
Epoch [31/120    avg_loss:0.063, val_acc:0.973]
Epoch [32/120    avg_loss:0.044, val_acc:0.981]
Epoch [33/120    avg_loss:0.041, val_acc:0.990]
Epoch [34/120    avg_loss:0.035, val_acc:0.987]
Epoch [35/120    avg_loss:0.031, val_acc:0.974]
Epoch [36/120    avg_loss:0.026, val_acc:0.984]
Epoch [37/120    avg_loss:0.024, val_acc:0.989]
Epoch [38/120    avg_loss:0.020, val_acc:0.990]
Epoch [39/120    avg_loss:0.030, val_acc:0.978]
Epoch [40/120    avg_loss:0.025, val_acc:0.988]
Epoch [41/120    avg_loss:0.017, val_acc:0.992]
Epoch [42/120    avg_loss:0.023, val_acc:0.988]
Epoch [43/120    avg_loss:0.024, val_acc:0.983]
Epoch [44/120    avg_loss:0.037, val_acc:0.987]
Epoch [45/120    avg_loss:0.018, val_acc:0.984]
Epoch [46/120    avg_loss:0.019, val_acc:0.989]
Epoch [47/120    avg_loss:0.024, val_acc:0.991]
Epoch [48/120    avg_loss:0.020, val_acc:0.959]
Epoch [49/120    avg_loss:0.027, val_acc:0.987]
Epoch [50/120    avg_loss:0.026, val_acc:0.984]
Epoch [51/120    avg_loss:0.025, val_acc:0.987]
Epoch [52/120    avg_loss:0.026, val_acc:0.985]
Epoch [53/120    avg_loss:0.029, val_acc:0.990]
Epoch [54/120    avg_loss:0.021, val_acc:0.985]
Epoch [55/120    avg_loss:0.014, val_acc:0.988]
Epoch [56/120    avg_loss:0.010, val_acc:0.987]
Epoch [57/120    avg_loss:0.010, val_acc:0.987]
Epoch [58/120    avg_loss:0.010, val_acc:0.987]
Epoch [59/120    avg_loss:0.009, val_acc:0.988]
Epoch [60/120    avg_loss:0.008, val_acc:0.989]
Epoch [61/120    avg_loss:0.009, val_acc:0.989]
Epoch [62/120    avg_loss:0.009, val_acc:0.989]
Epoch [63/120    avg_loss:0.009, val_acc:0.989]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.010, val_acc:0.990]
Epoch [66/120    avg_loss:0.007, val_acc:0.990]
Epoch [67/120    avg_loss:0.009, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.989]
Epoch [69/120    avg_loss:0.009, val_acc:0.989]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.010, val_acc:0.989]
Epoch [72/120    avg_loss:0.009, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.009, val_acc:0.989]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.009, val_acc:0.989]
Epoch [79/120    avg_loss:0.009, val_acc:0.989]
Epoch [80/120    avg_loss:0.009, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.007, val_acc:0.990]
Epoch [84/120    avg_loss:0.007, val_acc:0.990]
Epoch [85/120    avg_loss:0.011, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.990]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.009, val_acc:0.990]
Epoch [90/120    avg_loss:0.008, val_acc:0.990]
Epoch [91/120    avg_loss:0.009, val_acc:0.990]
Epoch [92/120    avg_loss:0.008, val_acc:0.990]
Epoch [93/120    avg_loss:0.010, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.010, val_acc:0.990]
Epoch [96/120    avg_loss:0.008, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.008, val_acc:0.990]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.008, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.008, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6367     0     0     0     0     6     0    59     0]
 [    0     2 17992     0    66     0    16     0    14     0]
 [    0     1     0  2027     2     0     0     0     2     4]
 [    0    50    20     0  2867     0     7     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4872     0     0     6]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0     8     0     0    46     0     0     0  3501    16]
 [    0     0     0     3    16    44     0     0     0   856]]

Accuracy:
98.99019111657388

F1 scores:
[       nan 0.99020218 0.99673148 0.9970487  0.96062992 0.98342125
 0.99631902 0.99883586 0.9758885  0.94952856]

Kappa:
0.9866309295474595
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f17ddf2e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.939, val_acc:0.289]
Epoch [2/120    avg_loss:1.391, val_acc:0.373]
Epoch [3/120    avg_loss:1.140, val_acc:0.428]
Epoch [4/120    avg_loss:0.965, val_acc:0.548]
Epoch [5/120    avg_loss:0.822, val_acc:0.698]
Epoch [6/120    avg_loss:0.703, val_acc:0.735]
Epoch [7/120    avg_loss:0.554, val_acc:0.744]
Epoch [8/120    avg_loss:0.496, val_acc:0.757]
Epoch [9/120    avg_loss:0.442, val_acc:0.802]
Epoch [10/120    avg_loss:0.394, val_acc:0.831]
Epoch [11/120    avg_loss:0.315, val_acc:0.828]
Epoch [12/120    avg_loss:0.304, val_acc:0.825]
Epoch [13/120    avg_loss:0.262, val_acc:0.838]
Epoch [14/120    avg_loss:0.248, val_acc:0.863]
Epoch [15/120    avg_loss:0.275, val_acc:0.841]
Epoch [16/120    avg_loss:0.257, val_acc:0.879]
Epoch [17/120    avg_loss:0.233, val_acc:0.895]
Epoch [18/120    avg_loss:0.153, val_acc:0.950]
Epoch [19/120    avg_loss:0.157, val_acc:0.947]
Epoch [20/120    avg_loss:0.168, val_acc:0.924]
Epoch [21/120    avg_loss:0.135, val_acc:0.947]
Epoch [22/120    avg_loss:0.109, val_acc:0.934]
Epoch [23/120    avg_loss:0.092, val_acc:0.963]
Epoch [24/120    avg_loss:0.084, val_acc:0.948]
Epoch [25/120    avg_loss:0.088, val_acc:0.966]
Epoch [26/120    avg_loss:0.066, val_acc:0.972]
Epoch [27/120    avg_loss:0.058, val_acc:0.981]
Epoch [28/120    avg_loss:0.058, val_acc:0.966]
Epoch [29/120    avg_loss:0.068, val_acc:0.972]
Epoch [30/120    avg_loss:0.079, val_acc:0.975]
Epoch [31/120    avg_loss:0.049, val_acc:0.980]
Epoch [32/120    avg_loss:0.040, val_acc:0.975]
Epoch [33/120    avg_loss:0.030, val_acc:0.976]
Epoch [34/120    avg_loss:0.029, val_acc:0.983]
Epoch [35/120    avg_loss:0.032, val_acc:0.955]
Epoch [36/120    avg_loss:0.044, val_acc:0.967]
Epoch [37/120    avg_loss:0.034, val_acc:0.976]
Epoch [38/120    avg_loss:0.043, val_acc:0.940]
Epoch [39/120    avg_loss:0.043, val_acc:0.978]
Epoch [40/120    avg_loss:0.040, val_acc:0.962]
Epoch [41/120    avg_loss:0.033, val_acc:0.967]
Epoch [42/120    avg_loss:0.048, val_acc:0.980]
Epoch [43/120    avg_loss:0.030, val_acc:0.984]
Epoch [44/120    avg_loss:0.036, val_acc:0.974]
Epoch [45/120    avg_loss:0.034, val_acc:0.984]
Epoch [46/120    avg_loss:0.029, val_acc:0.978]
Epoch [47/120    avg_loss:0.021, val_acc:0.980]
Epoch [48/120    avg_loss:0.018, val_acc:0.985]
Epoch [49/120    avg_loss:0.017, val_acc:0.972]
Epoch [50/120    avg_loss:0.013, val_acc:0.986]
Epoch [51/120    avg_loss:0.014, val_acc:0.986]
Epoch [52/120    avg_loss:0.014, val_acc:0.986]
Epoch [53/120    avg_loss:0.024, val_acc:0.971]
Epoch [54/120    avg_loss:0.024, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.988]
Epoch [56/120    avg_loss:0.018, val_acc:0.989]
Epoch [57/120    avg_loss:0.016, val_acc:0.984]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.008, val_acc:0.990]
Epoch [60/120    avg_loss:0.015, val_acc:0.988]
Epoch [61/120    avg_loss:0.015, val_acc:0.989]
Epoch [62/120    avg_loss:0.011, val_acc:0.986]
Epoch [63/120    avg_loss:0.010, val_acc:0.984]
Epoch [64/120    avg_loss:0.038, val_acc:0.963]
Epoch [65/120    avg_loss:0.054, val_acc:0.984]
Epoch [66/120    avg_loss:0.025, val_acc:0.982]
Epoch [67/120    avg_loss:0.015, val_acc:0.987]
Epoch [68/120    avg_loss:0.015, val_acc:0.991]
Epoch [69/120    avg_loss:0.015, val_acc:0.990]
Epoch [70/120    avg_loss:0.030, val_acc:0.982]
Epoch [71/120    avg_loss:0.016, val_acc:0.985]
Epoch [72/120    avg_loss:0.010, val_acc:0.991]
Epoch [73/120    avg_loss:0.007, val_acc:0.989]
Epoch [74/120    avg_loss:0.012, val_acc:0.989]
Epoch [75/120    avg_loss:0.007, val_acc:0.989]
Epoch [76/120    avg_loss:0.007, val_acc:0.978]
Epoch [77/120    avg_loss:0.009, val_acc:0.989]
Epoch [78/120    avg_loss:0.008, val_acc:0.991]
Epoch [79/120    avg_loss:0.009, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.007, val_acc:0.991]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.008, val_acc:0.976]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.991]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.004, val_acc:0.991]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.991]
Epoch [92/120    avg_loss:0.004, val_acc:0.991]
Epoch [93/120    avg_loss:0.005, val_acc:0.991]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.010, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.991]
Epoch [104/120    avg_loss:0.003, val_acc:0.991]
Epoch [105/120    avg_loss:0.003, val_acc:0.991]
Epoch [106/120    avg_loss:0.003, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.992]
Epoch [111/120    avg_loss:0.003, val_acc:0.992]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.004, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.003, val_acc:0.990]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     3     0     0    23    21     1     0]
 [    0     2 18073     0    13     0     0     0     2     0]
 [    0     0     0  2014     2     0     0     0    12     8]
 [    0    37    21     0  2878     0     7     0    27     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    12     0     0  4863     0     2     1]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     5     0     0    35     0     0     0  3523     8]
 [    0     0     0     0    15    63     0     0     0   841]]

Accuracy:
99.21914539801895

F1 scores:
[       nan 0.99284603 0.99894981 0.99089791 0.97311919 0.97643098
 0.99519083 0.99115044 0.98711124 0.94547499]

Kappa:
0.9896526685961011
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fef9693e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 53429==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:1.860, val_acc:0.253]
Epoch [2/120    avg_loss:1.344, val_acc:0.375]
Epoch [3/120    avg_loss:1.083, val_acc:0.522]
Epoch [4/120    avg_loss:0.903, val_acc:0.615]
Epoch [5/120    avg_loss:0.776, val_acc:0.689]
Epoch [6/120    avg_loss:0.600, val_acc:0.753]
Epoch [7/120    avg_loss:0.491, val_acc:0.791]
Epoch [8/120    avg_loss:0.455, val_acc:0.814]
Epoch [9/120    avg_loss:0.373, val_acc:0.829]
Epoch [10/120    avg_loss:0.306, val_acc:0.858]
Epoch [11/120    avg_loss:0.276, val_acc:0.907]
Epoch [12/120    avg_loss:0.232, val_acc:0.954]
Epoch [13/120    avg_loss:0.182, val_acc:0.936]
Epoch [14/120    avg_loss:0.159, val_acc:0.930]
Epoch [15/120    avg_loss:0.145, val_acc:0.944]
Epoch [16/120    avg_loss:0.106, val_acc:0.941]
Epoch [17/120    avg_loss:0.120, val_acc:0.958]
Epoch [18/120    avg_loss:0.109, val_acc:0.954]
Epoch [19/120    avg_loss:0.081, val_acc:0.944]
Epoch [20/120    avg_loss:0.084, val_acc:0.971]
Epoch [21/120    avg_loss:0.103, val_acc:0.971]
Epoch [22/120    avg_loss:0.070, val_acc:0.976]
Epoch [23/120    avg_loss:0.099, val_acc:0.973]
Epoch [24/120    avg_loss:0.067, val_acc:0.981]
Epoch [25/120    avg_loss:0.058, val_acc:0.982]
Epoch [26/120    avg_loss:0.046, val_acc:0.982]
Epoch [27/120    avg_loss:0.046, val_acc:0.974]
Epoch [28/120    avg_loss:0.047, val_acc:0.984]
Epoch [29/120    avg_loss:0.054, val_acc:0.983]
Epoch [30/120    avg_loss:0.040, val_acc:0.984]
Epoch [31/120    avg_loss:0.026, val_acc:0.986]
Epoch [32/120    avg_loss:0.029, val_acc:0.979]
Epoch [33/120    avg_loss:0.030, val_acc:0.976]
Epoch [34/120    avg_loss:0.037, val_acc:0.966]
Epoch [35/120    avg_loss:0.030, val_acc:0.985]
Epoch [36/120    avg_loss:0.028, val_acc:0.983]
Epoch [37/120    avg_loss:0.030, val_acc:0.972]
Epoch [38/120    avg_loss:0.034, val_acc:0.977]
Epoch [39/120    avg_loss:0.039, val_acc:0.981]
Epoch [40/120    avg_loss:0.038, val_acc:0.985]
Epoch [41/120    avg_loss:0.044, val_acc:0.986]
Epoch [42/120    avg_loss:0.026, val_acc:0.972]
Epoch [43/120    avg_loss:0.018, val_acc:0.980]
Epoch [44/120    avg_loss:0.014, val_acc:0.984]
Epoch [45/120    avg_loss:0.027, val_acc:0.957]
Epoch [46/120    avg_loss:0.048, val_acc:0.953]
Epoch [47/120    avg_loss:0.025, val_acc:0.968]
Epoch [48/120    avg_loss:0.021, val_acc:0.976]
Epoch [49/120    avg_loss:0.014, val_acc:0.989]
Epoch [50/120    avg_loss:0.011, val_acc:0.983]
Epoch [51/120    avg_loss:0.017, val_acc:0.985]
Epoch [52/120    avg_loss:0.014, val_acc:0.987]
Epoch [53/120    avg_loss:0.009, val_acc:0.986]
Epoch [54/120    avg_loss:0.015, val_acc:0.986]
Epoch [55/120    avg_loss:0.011, val_acc:0.986]
Epoch [56/120    avg_loss:0.012, val_acc:0.991]
Epoch [57/120    avg_loss:0.023, val_acc:0.975]
Epoch [58/120    avg_loss:0.011, val_acc:0.988]
Epoch [59/120    avg_loss:0.015, val_acc:0.984]
Epoch [60/120    avg_loss:0.017, val_acc:0.977]
Epoch [61/120    avg_loss:0.009, val_acc:0.986]
Epoch [62/120    avg_loss:0.008, val_acc:0.989]
Epoch [63/120    avg_loss:0.008, val_acc:0.989]
Epoch [64/120    avg_loss:0.006, val_acc:0.989]
Epoch [65/120    avg_loss:0.016, val_acc:0.979]
Epoch [66/120    avg_loss:0.018, val_acc:0.975]
Epoch [67/120    avg_loss:0.016, val_acc:0.981]
Epoch [68/120    avg_loss:0.011, val_acc:0.991]
Epoch [69/120    avg_loss:0.012, val_acc:0.990]
Epoch [70/120    avg_loss:0.009, val_acc:0.989]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.986]
Epoch [75/120    avg_loss:0.012, val_acc:0.988]
Epoch [76/120    avg_loss:0.010, val_acc:0.989]
Epoch [77/120    avg_loss:0.010, val_acc:0.962]
Epoch [78/120    avg_loss:0.010, val_acc:0.991]
Epoch [79/120    avg_loss:0.009, val_acc:0.991]
Epoch [80/120    avg_loss:0.005, val_acc:0.989]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.991]
Epoch [84/120    avg_loss:0.005, val_acc:0.991]
Epoch [85/120    avg_loss:0.005, val_acc:0.987]
Epoch [86/120    avg_loss:0.003, val_acc:0.986]
Epoch [87/120    avg_loss:0.003, val_acc:0.991]
Epoch [88/120    avg_loss:0.006, val_acc:0.989]
Epoch [89/120    avg_loss:0.004, val_acc:0.990]
Epoch [90/120    avg_loss:0.003, val_acc:0.990]
Epoch [91/120    avg_loss:0.012, val_acc:0.929]
Epoch [92/120    avg_loss:0.009, val_acc:0.991]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.003, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.003, val_acc:0.990]
Epoch [103/120    avg_loss:0.003, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.012, val_acc:0.991]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.003, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     4     0     0     0     0    19     0]
 [    0     5 18035     0    15     0    35     0     0     0]
 [    0     0     0  2029     0     0     0     0     6     1]
 [    0    44    23     1  2874     0     7     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4853     0     0    25]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     5     0     8    44     0     0     0  3510     4]
 [    0     0     0     3    15    30     0     0     0   871]]

Accuracy:
99.23601571349384

F1 scores:
[       nan 0.99402869 0.9978422  0.99436413 0.97094595 0.98863636
 0.99314438 1.         0.98471034 0.95714286]

Kappa:
0.9898796549875509
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff575af7748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.219, val_acc:0.455]
Epoch [2/120    avg_loss:0.692, val_acc:0.529]
Epoch [3/120    avg_loss:0.710, val_acc:0.756]
Epoch [4/120    avg_loss:0.580, val_acc:0.631]
Epoch [5/120    avg_loss:0.373, val_acc:0.825]
Epoch [6/120    avg_loss:0.341, val_acc:0.793]
Epoch [7/120    avg_loss:0.294, val_acc:0.802]
Epoch [8/120    avg_loss:0.317, val_acc:0.817]
Epoch [9/120    avg_loss:0.254, val_acc:0.825]
Epoch [10/120    avg_loss:0.225, val_acc:0.896]
Epoch [11/120    avg_loss:0.215, val_acc:0.896]
Epoch [12/120    avg_loss:0.186, val_acc:0.913]
Epoch [13/120    avg_loss:0.185, val_acc:0.899]
Epoch [14/120    avg_loss:0.175, val_acc:0.900]
Epoch [15/120    avg_loss:0.149, val_acc:0.904]
Epoch [16/120    avg_loss:0.122, val_acc:0.904]
Epoch [17/120    avg_loss:0.113, val_acc:0.913]
Epoch [18/120    avg_loss:0.162, val_acc:0.917]
Epoch [19/120    avg_loss:0.163, val_acc:0.883]
Epoch [20/120    avg_loss:0.218, val_acc:0.884]
Epoch [21/120    avg_loss:0.182, val_acc:0.935]
Epoch [22/120    avg_loss:0.098, val_acc:0.928]
Epoch [23/120    avg_loss:0.102, val_acc:0.920]
Epoch [24/120    avg_loss:0.105, val_acc:0.890]
Epoch [25/120    avg_loss:0.094, val_acc:0.956]
Epoch [26/120    avg_loss:0.086, val_acc:0.938]
Epoch [27/120    avg_loss:0.097, val_acc:0.913]
Epoch [28/120    avg_loss:0.085, val_acc:0.945]
Epoch [29/120    avg_loss:0.076, val_acc:0.921]
Epoch [30/120    avg_loss:0.096, val_acc:0.915]
Epoch [31/120    avg_loss:0.080, val_acc:0.939]
Epoch [32/120    avg_loss:0.078, val_acc:0.935]
Epoch [33/120    avg_loss:0.094, val_acc:0.944]
Epoch [34/120    avg_loss:0.047, val_acc:0.867]
Epoch [35/120    avg_loss:0.039, val_acc:0.967]
Epoch [36/120    avg_loss:0.049, val_acc:0.967]
Epoch [37/120    avg_loss:0.039, val_acc:0.965]
Epoch [38/120    avg_loss:0.038, val_acc:0.964]
Epoch [39/120    avg_loss:0.037, val_acc:0.962]
Epoch [40/120    avg_loss:0.027, val_acc:0.962]
Epoch [41/120    avg_loss:0.032, val_acc:0.958]
Epoch [42/120    avg_loss:0.051, val_acc:0.931]
Epoch [43/120    avg_loss:0.028, val_acc:0.949]
Epoch [44/120    avg_loss:0.031, val_acc:0.964]
Epoch [45/120    avg_loss:0.024, val_acc:0.967]
Epoch [46/120    avg_loss:0.045, val_acc:0.936]
Epoch [47/120    avg_loss:0.044, val_acc:0.958]
Epoch [48/120    avg_loss:0.035, val_acc:0.958]
Epoch [49/120    avg_loss:0.044, val_acc:0.958]
Epoch [50/120    avg_loss:0.024, val_acc:0.967]
Epoch [51/120    avg_loss:0.018, val_acc:0.968]
Epoch [52/120    avg_loss:0.015, val_acc:0.969]
Epoch [53/120    avg_loss:0.021, val_acc:0.971]
Epoch [54/120    avg_loss:0.020, val_acc:0.967]
Epoch [55/120    avg_loss:0.022, val_acc:0.968]
Epoch [56/120    avg_loss:0.023, val_acc:0.971]
Epoch [57/120    avg_loss:0.022, val_acc:0.974]
Epoch [58/120    avg_loss:0.019, val_acc:0.970]
Epoch [59/120    avg_loss:0.019, val_acc:0.970]
Epoch [60/120    avg_loss:0.015, val_acc:0.974]
Epoch [61/120    avg_loss:0.018, val_acc:0.968]
Epoch [62/120    avg_loss:0.019, val_acc:0.968]
Epoch [63/120    avg_loss:0.026, val_acc:0.972]
Epoch [64/120    avg_loss:0.023, val_acc:0.972]
Epoch [65/120    avg_loss:0.017, val_acc:0.969]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.024, val_acc:0.967]
Epoch [68/120    avg_loss:0.012, val_acc:0.968]
Epoch [69/120    avg_loss:0.020, val_acc:0.975]
Epoch [70/120    avg_loss:0.016, val_acc:0.974]
Epoch [71/120    avg_loss:0.015, val_acc:0.967]
Epoch [72/120    avg_loss:0.018, val_acc:0.969]
Epoch [73/120    avg_loss:0.013, val_acc:0.970]
Epoch [74/120    avg_loss:0.019, val_acc:0.970]
Epoch [75/120    avg_loss:0.017, val_acc:0.970]
Epoch [76/120    avg_loss:0.029, val_acc:0.968]
Epoch [77/120    avg_loss:0.019, val_acc:0.973]
Epoch [78/120    avg_loss:0.011, val_acc:0.972]
Epoch [79/120    avg_loss:0.015, val_acc:0.973]
Epoch [80/120    avg_loss:0.013, val_acc:0.974]
Epoch [81/120    avg_loss:0.012, val_acc:0.975]
Epoch [82/120    avg_loss:0.019, val_acc:0.975]
Epoch [83/120    avg_loss:0.010, val_acc:0.974]
Epoch [84/120    avg_loss:0.017, val_acc:0.973]
Epoch [85/120    avg_loss:0.015, val_acc:0.974]
Epoch [86/120    avg_loss:0.017, val_acc:0.974]
Epoch [87/120    avg_loss:0.008, val_acc:0.975]
Epoch [88/120    avg_loss:0.018, val_acc:0.975]
Epoch [89/120    avg_loss:0.015, val_acc:0.975]
Epoch [90/120    avg_loss:0.017, val_acc:0.971]
Epoch [91/120    avg_loss:0.012, val_acc:0.973]
Epoch [92/120    avg_loss:0.012, val_acc:0.975]
Epoch [93/120    avg_loss:0.008, val_acc:0.975]
Epoch [94/120    avg_loss:0.009, val_acc:0.975]
Epoch [95/120    avg_loss:0.014, val_acc:0.974]
Epoch [96/120    avg_loss:0.022, val_acc:0.966]
Epoch [97/120    avg_loss:0.015, val_acc:0.972]
Epoch [98/120    avg_loss:0.013, val_acc:0.974]
Epoch [99/120    avg_loss:0.012, val_acc:0.972]
Epoch [100/120    avg_loss:0.016, val_acc:0.971]
Epoch [101/120    avg_loss:0.014, val_acc:0.973]
Epoch [102/120    avg_loss:0.012, val_acc:0.975]
Epoch [103/120    avg_loss:0.009, val_acc:0.975]
Epoch [104/120    avg_loss:0.022, val_acc:0.971]
Epoch [105/120    avg_loss:0.012, val_acc:0.970]
Epoch [106/120    avg_loss:0.011, val_acc:0.971]
Epoch [107/120    avg_loss:0.023, val_acc:0.971]
Epoch [108/120    avg_loss:0.010, val_acc:0.971]
Epoch [109/120    avg_loss:0.014, val_acc:0.971]
Epoch [110/120    avg_loss:0.011, val_acc:0.971]
Epoch [111/120    avg_loss:0.009, val_acc:0.975]
Epoch [112/120    avg_loss:0.009, val_acc:0.974]
Epoch [113/120    avg_loss:0.015, val_acc:0.975]
Epoch [114/120    avg_loss:0.016, val_acc:0.973]
Epoch [115/120    avg_loss:0.013, val_acc:0.973]
Epoch [116/120    avg_loss:0.011, val_acc:0.973]
Epoch [117/120    avg_loss:0.009, val_acc:0.973]
Epoch [118/120    avg_loss:0.015, val_acc:0.973]
Epoch [119/120    avg_loss:0.013, val_acc:0.973]
Epoch [120/120    avg_loss:0.009, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     5     0     0     6     1    61     1]
 [    0     0 17877     0    27     0   186     0     0     0]
 [    0     6     0  1942     0     0     0     0    86     2]
 [    0    15     0     0  2951     0     3     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4872     0     1     0]
 [    0    12     0     0     0     0     0  1277     0     1]
 [    0    15     0    30    27     0     0     0  3499     0]
 [    0     0     0     0     9    13     0     0     0   897]]

Accuracy:
98.75882679006098

F1 scores:
[       nan 0.99049696 0.99393973 0.96785447 0.98596726 0.99504384
 0.97978884 0.99454829 0.96911785 0.98571429]

Kappa:
0.983588267138384
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe295d16710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.241, val_acc:0.529]
Epoch [2/120    avg_loss:0.719, val_acc:0.675]
Epoch [3/120    avg_loss:0.652, val_acc:0.791]
Epoch [4/120    avg_loss:0.526, val_acc:0.701]
Epoch [5/120    avg_loss:0.420, val_acc:0.854]
Epoch [6/120    avg_loss:0.345, val_acc:0.637]
Epoch [7/120    avg_loss:0.360, val_acc:0.786]
Epoch [8/120    avg_loss:0.306, val_acc:0.869]
Epoch [9/120    avg_loss:0.295, val_acc:0.830]
Epoch [10/120    avg_loss:0.281, val_acc:0.737]
Epoch [11/120    avg_loss:0.254, val_acc:0.833]
Epoch [12/120    avg_loss:0.276, val_acc:0.889]
Epoch [13/120    avg_loss:0.250, val_acc:0.902]
Epoch [14/120    avg_loss:0.190, val_acc:0.933]
Epoch [15/120    avg_loss:0.215, val_acc:0.912]
Epoch [16/120    avg_loss:0.216, val_acc:0.927]
Epoch [17/120    avg_loss:0.117, val_acc:0.945]
Epoch [18/120    avg_loss:0.128, val_acc:0.928]
Epoch [19/120    avg_loss:0.206, val_acc:0.755]
Epoch [20/120    avg_loss:0.186, val_acc:0.891]
Epoch [21/120    avg_loss:0.120, val_acc:0.909]
Epoch [22/120    avg_loss:0.163, val_acc:0.857]
Epoch [23/120    avg_loss:0.113, val_acc:0.954]
Epoch [24/120    avg_loss:0.080, val_acc:0.936]
Epoch [25/120    avg_loss:0.108, val_acc:0.935]
Epoch [26/120    avg_loss:0.093, val_acc:0.954]
Epoch [27/120    avg_loss:0.074, val_acc:0.959]
Epoch [28/120    avg_loss:0.099, val_acc:0.923]
Epoch [29/120    avg_loss:0.067, val_acc:0.966]
Epoch [30/120    avg_loss:0.078, val_acc:0.879]
Epoch [31/120    avg_loss:0.057, val_acc:0.949]
Epoch [32/120    avg_loss:0.089, val_acc:0.908]
Epoch [33/120    avg_loss:0.081, val_acc:0.953]
Epoch [34/120    avg_loss:0.069, val_acc:0.967]
Epoch [35/120    avg_loss:0.118, val_acc:0.954]
Epoch [36/120    avg_loss:0.134, val_acc:0.933]
Epoch [37/120    avg_loss:0.090, val_acc:0.962]
Epoch [38/120    avg_loss:0.050, val_acc:0.963]
Epoch [39/120    avg_loss:0.051, val_acc:0.956]
Epoch [40/120    avg_loss:0.042, val_acc:0.946]
Epoch [41/120    avg_loss:0.032, val_acc:0.965]
Epoch [42/120    avg_loss:0.061, val_acc:0.925]
Epoch [43/120    avg_loss:0.048, val_acc:0.968]
Epoch [44/120    avg_loss:0.056, val_acc:0.962]
Epoch [45/120    avg_loss:0.065, val_acc:0.961]
Epoch [46/120    avg_loss:0.060, val_acc:0.968]
Epoch [47/120    avg_loss:0.069, val_acc:0.945]
Epoch [48/120    avg_loss:0.053, val_acc:0.958]
Epoch [49/120    avg_loss:0.102, val_acc:0.947]
Epoch [50/120    avg_loss:0.039, val_acc:0.938]
Epoch [51/120    avg_loss:0.050, val_acc:0.963]
Epoch [52/120    avg_loss:0.027, val_acc:0.975]
Epoch [53/120    avg_loss:0.031, val_acc:0.967]
Epoch [54/120    avg_loss:0.099, val_acc:0.961]
Epoch [55/120    avg_loss:0.031, val_acc:0.961]
Epoch [56/120    avg_loss:0.049, val_acc:0.961]
Epoch [57/120    avg_loss:0.026, val_acc:0.979]
Epoch [58/120    avg_loss:0.021, val_acc:0.970]
Epoch [59/120    avg_loss:0.040, val_acc:0.953]
Epoch [60/120    avg_loss:0.039, val_acc:0.935]
Epoch [61/120    avg_loss:0.035, val_acc:0.960]
Epoch [62/120    avg_loss:0.028, val_acc:0.962]
Epoch [63/120    avg_loss:0.034, val_acc:0.977]
Epoch [64/120    avg_loss:0.018, val_acc:0.977]
Epoch [65/120    avg_loss:0.056, val_acc:0.970]
Epoch [66/120    avg_loss:0.033, val_acc:0.956]
Epoch [67/120    avg_loss:0.041, val_acc:0.970]
Epoch [68/120    avg_loss:0.023, val_acc:0.977]
Epoch [69/120    avg_loss:0.036, val_acc:0.971]
Epoch [70/120    avg_loss:0.030, val_acc:0.968]
Epoch [71/120    avg_loss:0.015, val_acc:0.974]
Epoch [72/120    avg_loss:0.021, val_acc:0.980]
Epoch [73/120    avg_loss:0.015, val_acc:0.978]
Epoch [74/120    avg_loss:0.029, val_acc:0.980]
Epoch [75/120    avg_loss:0.009, val_acc:0.983]
Epoch [76/120    avg_loss:0.018, val_acc:0.983]
Epoch [77/120    avg_loss:0.009, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.019, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.015, val_acc:0.985]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.983]
Epoch [87/120    avg_loss:0.014, val_acc:0.984]
Epoch [88/120    avg_loss:0.023, val_acc:0.986]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.021, val_acc:0.987]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.012, val_acc:0.985]
Epoch [94/120    avg_loss:0.012, val_acc:0.984]
Epoch [95/120    avg_loss:0.013, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.984]
Epoch [97/120    avg_loss:0.013, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.013, val_acc:0.987]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.015, val_acc:0.987]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.013, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     4     0     0     1     4    19     3]
 [    0     0 18021     0    32     0    36     0     1     0]
 [    0     0     0  1966     0     0     0     0    69     1]
 [    0     7     0     0  2959     0     1     0     4     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4874     0     0     0]
 [    0     8     0     0     0     0     0  1278     0     4]
 [    0    14     1    34    28     0     0     0  3493     1]
 [    0     1     0     0     0    10     0     0     0   908]]

Accuracy:
99.30590702046129

F1 scores:
[       nan 0.99525772 0.99795105 0.97326733 0.98781506 0.99618321
 0.99570991 0.99377916 0.97610731 0.98856832]

Kappa:
0.9908089918231497
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde1e93f748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.300, val_acc:0.498]
Epoch [2/120    avg_loss:0.740, val_acc:0.776]
Epoch [3/120    avg_loss:0.581, val_acc:0.771]
Epoch [4/120    avg_loss:0.513, val_acc:0.765]
Epoch [5/120    avg_loss:0.448, val_acc:0.836]
Epoch [6/120    avg_loss:0.355, val_acc:0.800]
Epoch [7/120    avg_loss:0.319, val_acc:0.834]
Epoch [8/120    avg_loss:0.303, val_acc:0.861]
Epoch [9/120    avg_loss:0.276, val_acc:0.851]
Epoch [10/120    avg_loss:0.199, val_acc:0.904]
Epoch [11/120    avg_loss:0.232, val_acc:0.842]
Epoch [12/120    avg_loss:0.178, val_acc:0.914]
Epoch [13/120    avg_loss:0.224, val_acc:0.897]
Epoch [14/120    avg_loss:0.178, val_acc:0.834]
Epoch [15/120    avg_loss:0.205, val_acc:0.898]
Epoch [16/120    avg_loss:0.157, val_acc:0.944]
Epoch [17/120    avg_loss:0.116, val_acc:0.920]
Epoch [18/120    avg_loss:0.123, val_acc:0.851]
Epoch [19/120    avg_loss:0.134, val_acc:0.904]
Epoch [20/120    avg_loss:0.126, val_acc:0.924]
Epoch [21/120    avg_loss:0.089, val_acc:0.921]
Epoch [22/120    avg_loss:0.084, val_acc:0.935]
Epoch [23/120    avg_loss:0.164, val_acc:0.868]
Epoch [24/120    avg_loss:0.144, val_acc:0.937]
Epoch [25/120    avg_loss:0.085, val_acc:0.954]
Epoch [26/120    avg_loss:0.096, val_acc:0.952]
Epoch [27/120    avg_loss:0.108, val_acc:0.931]
Epoch [28/120    avg_loss:0.089, val_acc:0.955]
Epoch [29/120    avg_loss:0.061, val_acc:0.961]
Epoch [30/120    avg_loss:0.056, val_acc:0.949]
Epoch [31/120    avg_loss:0.053, val_acc:0.959]
Epoch [32/120    avg_loss:0.042, val_acc:0.967]
Epoch [33/120    avg_loss:0.048, val_acc:0.901]
Epoch [34/120    avg_loss:0.108, val_acc:0.906]
Epoch [35/120    avg_loss:0.148, val_acc:0.931]
Epoch [36/120    avg_loss:0.039, val_acc:0.958]
Epoch [37/120    avg_loss:0.081, val_acc:0.966]
Epoch [38/120    avg_loss:0.063, val_acc:0.940]
Epoch [39/120    avg_loss:0.041, val_acc:0.959]
Epoch [40/120    avg_loss:0.028, val_acc:0.971]
Epoch [41/120    avg_loss:0.041, val_acc:0.958]
Epoch [42/120    avg_loss:0.031, val_acc:0.962]
Epoch [43/120    avg_loss:0.035, val_acc:0.970]
Epoch [44/120    avg_loss:0.020, val_acc:0.975]
Epoch [45/120    avg_loss:0.044, val_acc:0.971]
Epoch [46/120    avg_loss:0.027, val_acc:0.962]
Epoch [47/120    avg_loss:0.025, val_acc:0.971]
Epoch [48/120    avg_loss:0.016, val_acc:0.967]
Epoch [49/120    avg_loss:0.042, val_acc:0.967]
Epoch [50/120    avg_loss:0.060, val_acc:0.961]
Epoch [51/120    avg_loss:0.054, val_acc:0.957]
Epoch [52/120    avg_loss:0.050, val_acc:0.963]
Epoch [53/120    avg_loss:0.074, val_acc:0.958]
Epoch [54/120    avg_loss:0.027, val_acc:0.971]
Epoch [55/120    avg_loss:0.024, val_acc:0.975]
Epoch [56/120    avg_loss:0.033, val_acc:0.979]
Epoch [57/120    avg_loss:0.022, val_acc:0.967]
Epoch [58/120    avg_loss:0.023, val_acc:0.977]
Epoch [59/120    avg_loss:0.017, val_acc:0.974]
Epoch [60/120    avg_loss:0.029, val_acc:0.967]
Epoch [61/120    avg_loss:0.016, val_acc:0.979]
Epoch [62/120    avg_loss:0.011, val_acc:0.980]
Epoch [63/120    avg_loss:0.008, val_acc:0.979]
Epoch [64/120    avg_loss:0.035, val_acc:0.949]
Epoch [65/120    avg_loss:0.036, val_acc:0.975]
Epoch [66/120    avg_loss:0.047, val_acc:0.945]
Epoch [67/120    avg_loss:0.069, val_acc:0.961]
Epoch [68/120    avg_loss:0.055, val_acc:0.967]
Epoch [69/120    avg_loss:0.043, val_acc:0.939]
Epoch [70/120    avg_loss:0.074, val_acc:0.953]
Epoch [71/120    avg_loss:0.023, val_acc:0.966]
Epoch [72/120    avg_loss:0.024, val_acc:0.969]
Epoch [73/120    avg_loss:0.032, val_acc:0.967]
Epoch [74/120    avg_loss:0.015, val_acc:0.971]
Epoch [75/120    avg_loss:0.023, val_acc:0.973]
Epoch [76/120    avg_loss:0.016, val_acc:0.973]
Epoch [77/120    avg_loss:0.012, val_acc:0.980]
Epoch [78/120    avg_loss:0.016, val_acc:0.979]
Epoch [79/120    avg_loss:0.012, val_acc:0.980]
Epoch [80/120    avg_loss:0.006, val_acc:0.980]
Epoch [81/120    avg_loss:0.015, val_acc:0.980]
Epoch [82/120    avg_loss:0.012, val_acc:0.978]
Epoch [83/120    avg_loss:0.014, val_acc:0.974]
Epoch [84/120    avg_loss:0.010, val_acc:0.979]
Epoch [85/120    avg_loss:0.008, val_acc:0.979]
Epoch [86/120    avg_loss:0.008, val_acc:0.979]
Epoch [87/120    avg_loss:0.010, val_acc:0.977]
Epoch [88/120    avg_loss:0.015, val_acc:0.977]
Epoch [89/120    avg_loss:0.016, val_acc:0.978]
Epoch [90/120    avg_loss:0.008, val_acc:0.979]
Epoch [91/120    avg_loss:0.016, val_acc:0.980]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.014, val_acc:0.979]
Epoch [94/120    avg_loss:0.012, val_acc:0.979]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.978]
Epoch [97/120    avg_loss:0.007, val_acc:0.979]
Epoch [98/120    avg_loss:0.010, val_acc:0.979]
Epoch [99/120    avg_loss:0.016, val_acc:0.979]
Epoch [100/120    avg_loss:0.018, val_acc:0.978]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.020, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.978]
Epoch [104/120    avg_loss:0.020, val_acc:0.978]
Epoch [105/120    avg_loss:0.011, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.013, val_acc:0.978]
Epoch [108/120    avg_loss:0.020, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.978]
Epoch [110/120    avg_loss:0.011, val_acc:0.978]
Epoch [111/120    avg_loss:0.010, val_acc:0.978]
Epoch [112/120    avg_loss:0.012, val_acc:0.978]
Epoch [113/120    avg_loss:0.010, val_acc:0.978]
Epoch [114/120    avg_loss:0.008, val_acc:0.978]
Epoch [115/120    avg_loss:0.007, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.008, val_acc:0.978]
Epoch [118/120    avg_loss:0.011, val_acc:0.978]
Epoch [119/120    avg_loss:0.019, val_acc:0.978]
Epoch [120/120    avg_loss:0.013, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     3     0     0     9     0    39     2]
 [    0     0 17721     0    62     0   303     0     4     0]
 [    0     0     0  1969     0     0     0     0    64     3]
 [    0     6     0     0  2961     0     1     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     1     1     0  4859     0     6     0]
 [    0     4     0     0     0     0     0  1286     0     0]
 [    0    44     0    42    25     0     3     0  3457     0]
 [    0     0     0     0    12    20     0     0     0   887]]

Accuracy:
98.38767984961319

F1 scores:
[       nan 0.99168286 0.98939199 0.97210565 0.98160119 0.99239544
 0.96667661 0.9984472  0.9676697  0.9795693 ]

Kappa:
0.9787142949747069
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe494afe748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.103, val_acc:0.663]
Epoch [2/120    avg_loss:0.662, val_acc:0.759]
Epoch [3/120    avg_loss:0.521, val_acc:0.776]
Epoch [4/120    avg_loss:0.571, val_acc:0.665]
Epoch [5/120    avg_loss:0.439, val_acc:0.735]
Epoch [6/120    avg_loss:0.413, val_acc:0.891]
Epoch [7/120    avg_loss:0.347, val_acc:0.884]
Epoch [8/120    avg_loss:0.333, val_acc:0.806]
Epoch [9/120    avg_loss:0.225, val_acc:0.892]
Epoch [10/120    avg_loss:0.260, val_acc:0.889]
Epoch [11/120    avg_loss:0.204, val_acc:0.839]
Epoch [12/120    avg_loss:0.215, val_acc:0.893]
Epoch [13/120    avg_loss:0.220, val_acc:0.904]
Epoch [14/120    avg_loss:0.179, val_acc:0.926]
Epoch [15/120    avg_loss:0.135, val_acc:0.908]
Epoch [16/120    avg_loss:0.130, val_acc:0.922]
Epoch [17/120    avg_loss:0.127, val_acc:0.880]
Epoch [18/120    avg_loss:0.118, val_acc:0.913]
Epoch [19/120    avg_loss:0.117, val_acc:0.932]
Epoch [20/120    avg_loss:0.107, val_acc:0.871]
Epoch [21/120    avg_loss:0.113, val_acc:0.943]
Epoch [22/120    avg_loss:0.091, val_acc:0.933]
Epoch [23/120    avg_loss:0.076, val_acc:0.929]
Epoch [24/120    avg_loss:0.100, val_acc:0.934]
Epoch [25/120    avg_loss:0.078, val_acc:0.925]
Epoch [26/120    avg_loss:0.135, val_acc:0.935]
Epoch [27/120    avg_loss:0.100, val_acc:0.928]
Epoch [28/120    avg_loss:0.044, val_acc:0.957]
Epoch [29/120    avg_loss:0.052, val_acc:0.931]
Epoch [30/120    avg_loss:0.049, val_acc:0.963]
Epoch [31/120    avg_loss:0.117, val_acc:0.851]
Epoch [32/120    avg_loss:0.126, val_acc:0.939]
Epoch [33/120    avg_loss:0.073, val_acc:0.957]
Epoch [34/120    avg_loss:0.061, val_acc:0.925]
Epoch [35/120    avg_loss:0.046, val_acc:0.952]
Epoch [36/120    avg_loss:0.107, val_acc:0.950]
Epoch [37/120    avg_loss:0.049, val_acc:0.961]
Epoch [38/120    avg_loss:0.112, val_acc:0.958]
Epoch [39/120    avg_loss:0.069, val_acc:0.944]
Epoch [40/120    avg_loss:0.053, val_acc:0.929]
Epoch [41/120    avg_loss:0.054, val_acc:0.962]
Epoch [42/120    avg_loss:0.037, val_acc:0.967]
Epoch [43/120    avg_loss:0.037, val_acc:0.964]
Epoch [44/120    avg_loss:0.129, val_acc:0.904]
Epoch [45/120    avg_loss:0.060, val_acc:0.914]
Epoch [46/120    avg_loss:0.046, val_acc:0.970]
Epoch [47/120    avg_loss:0.099, val_acc:0.958]
Epoch [48/120    avg_loss:0.053, val_acc:0.965]
Epoch [49/120    avg_loss:0.041, val_acc:0.967]
Epoch [50/120    avg_loss:0.062, val_acc:0.958]
Epoch [51/120    avg_loss:0.028, val_acc:0.972]
Epoch [52/120    avg_loss:0.041, val_acc:0.972]
Epoch [53/120    avg_loss:0.066, val_acc:0.962]
Epoch [54/120    avg_loss:0.024, val_acc:0.977]
Epoch [55/120    avg_loss:0.040, val_acc:0.972]
Epoch [56/120    avg_loss:0.021, val_acc:0.972]
Epoch [57/120    avg_loss:0.037, val_acc:0.976]
Epoch [58/120    avg_loss:0.020, val_acc:0.972]
Epoch [59/120    avg_loss:0.021, val_acc:0.967]
Epoch [60/120    avg_loss:0.021, val_acc:0.978]
Epoch [61/120    avg_loss:0.035, val_acc:0.976]
Epoch [62/120    avg_loss:0.095, val_acc:0.959]
Epoch [63/120    avg_loss:0.026, val_acc:0.974]
Epoch [64/120    avg_loss:0.045, val_acc:0.937]
Epoch [65/120    avg_loss:0.021, val_acc:0.977]
Epoch [66/120    avg_loss:0.039, val_acc:0.941]
Epoch [67/120    avg_loss:0.028, val_acc:0.976]
Epoch [68/120    avg_loss:0.049, val_acc:0.872]
Epoch [69/120    avg_loss:0.029, val_acc:0.974]
Epoch [70/120    avg_loss:0.014, val_acc:0.976]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.019, val_acc:0.977]
Epoch [73/120    avg_loss:0.017, val_acc:0.967]
Epoch [74/120    avg_loss:0.019, val_acc:0.976]
Epoch [75/120    avg_loss:0.013, val_acc:0.975]
Epoch [76/120    avg_loss:0.018, val_acc:0.975]
Epoch [77/120    avg_loss:0.008, val_acc:0.980]
Epoch [78/120    avg_loss:0.011, val_acc:0.983]
Epoch [79/120    avg_loss:0.020, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.008, val_acc:0.978]
Epoch [82/120    avg_loss:0.020, val_acc:0.974]
Epoch [83/120    avg_loss:0.013, val_acc:0.973]
Epoch [84/120    avg_loss:0.009, val_acc:0.980]
Epoch [85/120    avg_loss:0.015, val_acc:0.979]
Epoch [86/120    avg_loss:0.022, val_acc:0.962]
Epoch [87/120    avg_loss:0.019, val_acc:0.969]
Epoch [88/120    avg_loss:0.033, val_acc:0.972]
Epoch [89/120    avg_loss:0.012, val_acc:0.977]
Epoch [90/120    avg_loss:0.018, val_acc:0.977]
Epoch [91/120    avg_loss:0.024, val_acc:0.978]
Epoch [92/120    avg_loss:0.012, val_acc:0.975]
Epoch [93/120    avg_loss:0.007, val_acc:0.978]
Epoch [94/120    avg_loss:0.006, val_acc:0.979]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.006, val_acc:0.980]
Epoch [98/120    avg_loss:0.009, val_acc:0.980]
Epoch [99/120    avg_loss:0.007, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.983]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.981]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.005, val_acc:0.980]
Epoch [107/120    avg_loss:0.004, val_acc:0.980]
Epoch [108/120    avg_loss:0.005, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.005, val_acc:0.980]
Epoch [111/120    avg_loss:0.005, val_acc:0.980]
Epoch [112/120    avg_loss:0.004, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.014, val_acc:0.982]
Epoch [117/120    avg_loss:0.013, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6404     0     3     0     0     2     6    16     1]
 [    0     0 17942     0     5     0   141     0     2     0]
 [    0     0     0  1936     0     0     0     0    98     2]
 [    0    12     0     0  2951     0     3     0     2     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4872     0     1     0]
 [    0    11     0     0     0     0     0  1279     0     0]
 [    0    23     0    21    25     0     1     0  3501     0]
 [    0     0     0     0     0     8     0     0     0   911]]

Accuracy:
99.05526233340564

F1 scores:
[       nan 0.99425555 0.99575436 0.96896897 0.99143289 0.99694423
 0.98454077 0.99339806 0.97371715 0.99183451]

Kappa:
0.9874983319970607
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40e9cdd748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.242, val_acc:0.599]
Epoch [2/120    avg_loss:0.718, val_acc:0.701]
Epoch [3/120    avg_loss:0.575, val_acc:0.760]
Epoch [4/120    avg_loss:0.540, val_acc:0.631]
Epoch [5/120    avg_loss:0.426, val_acc:0.594]
Epoch [6/120    avg_loss:0.471, val_acc:0.787]
Epoch [7/120    avg_loss:0.411, val_acc:0.730]
Epoch [8/120    avg_loss:0.326, val_acc:0.882]
Epoch [9/120    avg_loss:0.340, val_acc:0.796]
Epoch [10/120    avg_loss:0.295, val_acc:0.849]
Epoch [11/120    avg_loss:0.248, val_acc:0.900]
Epoch [12/120    avg_loss:0.244, val_acc:0.882]
Epoch [13/120    avg_loss:0.250, val_acc:0.905]
Epoch [14/120    avg_loss:0.200, val_acc:0.914]
Epoch [15/120    avg_loss:0.150, val_acc:0.887]
Epoch [16/120    avg_loss:0.171, val_acc:0.905]
Epoch [17/120    avg_loss:0.204, val_acc:0.860]
Epoch [18/120    avg_loss:0.207, val_acc:0.919]
Epoch [19/120    avg_loss:0.130, val_acc:0.929]
Epoch [20/120    avg_loss:0.128, val_acc:0.849]
Epoch [21/120    avg_loss:0.097, val_acc:0.925]
Epoch [22/120    avg_loss:0.113, val_acc:0.898]
Epoch [23/120    avg_loss:0.105, val_acc:0.944]
Epoch [24/120    avg_loss:0.094, val_acc:0.933]
Epoch [25/120    avg_loss:0.171, val_acc:0.870]
Epoch [26/120    avg_loss:0.107, val_acc:0.941]
Epoch [27/120    avg_loss:0.089, val_acc:0.928]
Epoch [28/120    avg_loss:0.088, val_acc:0.931]
Epoch [29/120    avg_loss:0.141, val_acc:0.947]
Epoch [30/120    avg_loss:0.093, val_acc:0.954]
Epoch [31/120    avg_loss:0.066, val_acc:0.932]
Epoch [32/120    avg_loss:0.069, val_acc:0.943]
Epoch [33/120    avg_loss:0.039, val_acc:0.960]
Epoch [34/120    avg_loss:0.048, val_acc:0.962]
Epoch [35/120    avg_loss:0.054, val_acc:0.961]
Epoch [36/120    avg_loss:0.061, val_acc:0.954]
Epoch [37/120    avg_loss:0.041, val_acc:0.970]
Epoch [38/120    avg_loss:0.045, val_acc:0.933]
Epoch [39/120    avg_loss:0.061, val_acc:0.928]
Epoch [40/120    avg_loss:0.060, val_acc:0.970]
Epoch [41/120    avg_loss:0.035, val_acc:0.974]
Epoch [42/120    avg_loss:0.043, val_acc:0.965]
Epoch [43/120    avg_loss:0.035, val_acc:0.953]
Epoch [44/120    avg_loss:0.032, val_acc:0.962]
Epoch [45/120    avg_loss:0.063, val_acc:0.924]
Epoch [46/120    avg_loss:0.060, val_acc:0.958]
Epoch [47/120    avg_loss:0.039, val_acc:0.956]
Epoch [48/120    avg_loss:0.057, val_acc:0.951]
Epoch [49/120    avg_loss:0.056, val_acc:0.961]
Epoch [50/120    avg_loss:0.034, val_acc:0.967]
Epoch [51/120    avg_loss:0.037, val_acc:0.924]
Epoch [52/120    avg_loss:0.035, val_acc:0.914]
Epoch [53/120    avg_loss:0.058, val_acc:0.959]
Epoch [54/120    avg_loss:0.050, val_acc:0.913]
Epoch [55/120    avg_loss:0.037, val_acc:0.964]
Epoch [56/120    avg_loss:0.022, val_acc:0.970]
Epoch [57/120    avg_loss:0.042, val_acc:0.969]
Epoch [58/120    avg_loss:0.024, val_acc:0.976]
Epoch [59/120    avg_loss:0.017, val_acc:0.975]
Epoch [60/120    avg_loss:0.018, val_acc:0.975]
Epoch [61/120    avg_loss:0.018, val_acc:0.971]
Epoch [62/120    avg_loss:0.022, val_acc:0.973]
Epoch [63/120    avg_loss:0.017, val_acc:0.975]
Epoch [64/120    avg_loss:0.017, val_acc:0.975]
Epoch [65/120    avg_loss:0.016, val_acc:0.974]
Epoch [66/120    avg_loss:0.021, val_acc:0.975]
Epoch [67/120    avg_loss:0.016, val_acc:0.979]
Epoch [68/120    avg_loss:0.026, val_acc:0.972]
Epoch [69/120    avg_loss:0.030, val_acc:0.976]
Epoch [70/120    avg_loss:0.015, val_acc:0.978]
Epoch [71/120    avg_loss:0.017, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.976]
Epoch [73/120    avg_loss:0.012, val_acc:0.977]
Epoch [74/120    avg_loss:0.013, val_acc:0.976]
Epoch [75/120    avg_loss:0.010, val_acc:0.979]
Epoch [76/120    avg_loss:0.025, val_acc:0.979]
Epoch [77/120    avg_loss:0.015, val_acc:0.977]
Epoch [78/120    avg_loss:0.019, val_acc:0.973]
Epoch [79/120    avg_loss:0.013, val_acc:0.975]
Epoch [80/120    avg_loss:0.013, val_acc:0.979]
Epoch [81/120    avg_loss:0.011, val_acc:0.978]
Epoch [82/120    avg_loss:0.020, val_acc:0.976]
Epoch [83/120    avg_loss:0.019, val_acc:0.977]
Epoch [84/120    avg_loss:0.016, val_acc:0.975]
Epoch [85/120    avg_loss:0.008, val_acc:0.977]
Epoch [86/120    avg_loss:0.010, val_acc:0.976]
Epoch [87/120    avg_loss:0.019, val_acc:0.975]
Epoch [88/120    avg_loss:0.017, val_acc:0.975]
Epoch [89/120    avg_loss:0.013, val_acc:0.976]
Epoch [90/120    avg_loss:0.020, val_acc:0.975]
Epoch [91/120    avg_loss:0.013, val_acc:0.975]
Epoch [92/120    avg_loss:0.015, val_acc:0.976]
Epoch [93/120    avg_loss:0.011, val_acc:0.977]
Epoch [94/120    avg_loss:0.020, val_acc:0.977]
Epoch [95/120    avg_loss:0.016, val_acc:0.979]
Epoch [96/120    avg_loss:0.013, val_acc:0.979]
Epoch [97/120    avg_loss:0.017, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.979]
Epoch [101/120    avg_loss:0.015, val_acc:0.979]
Epoch [102/120    avg_loss:0.011, val_acc:0.979]
Epoch [103/120    avg_loss:0.012, val_acc:0.979]
Epoch [104/120    avg_loss:0.021, val_acc:0.979]
Epoch [105/120    avg_loss:0.012, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.979]
Epoch [107/120    avg_loss:0.016, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.979]
Epoch [109/120    avg_loss:0.016, val_acc:0.978]
Epoch [110/120    avg_loss:0.011, val_acc:0.979]
Epoch [111/120    avg_loss:0.014, val_acc:0.979]
Epoch [112/120    avg_loss:0.010, val_acc:0.979]
Epoch [113/120    avg_loss:0.020, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.978]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.015, val_acc:0.978]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.014, val_acc:0.978]
Epoch [120/120    avg_loss:0.013, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6422     0     0     2     0     0     2     3     3]
 [    0     0 17801     0    67     0   221     0     1     0]
 [    0     0     0  2016     0     0     0     0    20     0]
 [    0    16     0     0  2950     0     1     0     2     3]
 [    0     0     0     0     0  1209     0     0     8    88]
 [    0     0    10     0     0     0  4868     0     0     0]
 [    0     2     0     0     0     0     0  1288     0     0]
 [    0    56     0    48    17     0     0     0  3449     1]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.61181404092257

F1 scores:
[       nan 0.99350248 0.99167154 0.98341463 0.98202397 0.95990472
 0.97672552 0.99844961 0.97788489 0.94813278]

Kappa:
0.9816579914311654
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f21f406d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.301, val_acc:0.641]
Epoch [2/120    avg_loss:0.736, val_acc:0.573]
Epoch [3/120    avg_loss:0.541, val_acc:0.704]
Epoch [4/120    avg_loss:0.456, val_acc:0.757]
Epoch [5/120    avg_loss:0.441, val_acc:0.777]
Epoch [6/120    avg_loss:0.348, val_acc:0.881]
Epoch [7/120    avg_loss:0.283, val_acc:0.829]
Epoch [8/120    avg_loss:0.313, val_acc:0.895]
Epoch [9/120    avg_loss:0.200, val_acc:0.912]
Epoch [10/120    avg_loss:0.273, val_acc:0.873]
Epoch [11/120    avg_loss:0.175, val_acc:0.907]
Epoch [12/120    avg_loss:0.185, val_acc:0.915]
Epoch [13/120    avg_loss:0.210, val_acc:0.877]
Epoch [14/120    avg_loss:0.222, val_acc:0.906]
Epoch [15/120    avg_loss:0.142, val_acc:0.936]
Epoch [16/120    avg_loss:0.119, val_acc:0.929]
Epoch [17/120    avg_loss:0.088, val_acc:0.931]
Epoch [18/120    avg_loss:0.123, val_acc:0.929]
Epoch [19/120    avg_loss:0.122, val_acc:0.886]
Epoch [20/120    avg_loss:0.117, val_acc:0.918]
Epoch [21/120    avg_loss:0.105, val_acc:0.944]
Epoch [22/120    avg_loss:0.092, val_acc:0.944]
Epoch [23/120    avg_loss:0.089, val_acc:0.949]
Epoch [24/120    avg_loss:0.086, val_acc:0.952]
Epoch [25/120    avg_loss:0.068, val_acc:0.938]
Epoch [26/120    avg_loss:0.061, val_acc:0.953]
Epoch [27/120    avg_loss:0.049, val_acc:0.966]
Epoch [28/120    avg_loss:0.073, val_acc:0.953]
Epoch [29/120    avg_loss:0.058, val_acc:0.956]
Epoch [30/120    avg_loss:0.054, val_acc:0.958]
Epoch [31/120    avg_loss:0.042, val_acc:0.962]
Epoch [32/120    avg_loss:0.044, val_acc:0.962]
Epoch [33/120    avg_loss:0.045, val_acc:0.964]
Epoch [34/120    avg_loss:0.028, val_acc:0.967]
Epoch [35/120    avg_loss:0.065, val_acc:0.948]
Epoch [36/120    avg_loss:0.090, val_acc:0.959]
Epoch [37/120    avg_loss:0.061, val_acc:0.962]
Epoch [38/120    avg_loss:0.053, val_acc:0.972]
Epoch [39/120    avg_loss:0.035, val_acc:0.949]
Epoch [40/120    avg_loss:0.038, val_acc:0.940]
Epoch [41/120    avg_loss:0.039, val_acc:0.972]
Epoch [42/120    avg_loss:0.102, val_acc:0.922]
Epoch [43/120    avg_loss:0.082, val_acc:0.974]
Epoch [44/120    avg_loss:0.048, val_acc:0.976]
Epoch [45/120    avg_loss:0.051, val_acc:0.940]
Epoch [46/120    avg_loss:0.057, val_acc:0.967]
Epoch [47/120    avg_loss:0.034, val_acc:0.969]
Epoch [48/120    avg_loss:0.026, val_acc:0.935]
Epoch [49/120    avg_loss:0.040, val_acc:0.972]
Epoch [50/120    avg_loss:0.022, val_acc:0.967]
Epoch [51/120    avg_loss:0.031, val_acc:0.980]
Epoch [52/120    avg_loss:0.014, val_acc:0.978]
Epoch [53/120    avg_loss:0.017, val_acc:0.969]
Epoch [54/120    avg_loss:0.017, val_acc:0.975]
Epoch [55/120    avg_loss:0.022, val_acc:0.959]
Epoch [56/120    avg_loss:0.017, val_acc:0.976]
Epoch [57/120    avg_loss:0.037, val_acc:0.971]
Epoch [58/120    avg_loss:0.014, val_acc:0.982]
Epoch [59/120    avg_loss:0.022, val_acc:0.962]
Epoch [60/120    avg_loss:0.051, val_acc:0.982]
Epoch [61/120    avg_loss:0.023, val_acc:0.979]
Epoch [62/120    avg_loss:0.030, val_acc:0.981]
Epoch [63/120    avg_loss:0.013, val_acc:0.958]
Epoch [64/120    avg_loss:0.020, val_acc:0.963]
Epoch [65/120    avg_loss:0.019, val_acc:0.975]
Epoch [66/120    avg_loss:0.012, val_acc:0.978]
Epoch [67/120    avg_loss:0.022, val_acc:0.980]
Epoch [68/120    avg_loss:0.024, val_acc:0.982]
Epoch [69/120    avg_loss:0.014, val_acc:0.980]
Epoch [70/120    avg_loss:0.022, val_acc:0.975]
Epoch [71/120    avg_loss:0.015, val_acc:0.976]
Epoch [72/120    avg_loss:0.015, val_acc:0.978]
Epoch [73/120    avg_loss:0.017, val_acc:0.980]
Epoch [74/120    avg_loss:0.010, val_acc:0.962]
Epoch [75/120    avg_loss:0.016, val_acc:0.966]
Epoch [76/120    avg_loss:0.064, val_acc:0.940]
Epoch [77/120    avg_loss:0.130, val_acc:0.947]
Epoch [78/120    avg_loss:0.028, val_acc:0.980]
Epoch [79/120    avg_loss:0.016, val_acc:0.971]
Epoch [80/120    avg_loss:0.015, val_acc:0.980]
Epoch [81/120    avg_loss:0.030, val_acc:0.970]
Epoch [82/120    avg_loss:0.017, val_acc:0.982]
Epoch [83/120    avg_loss:0.015, val_acc:0.978]
Epoch [84/120    avg_loss:0.016, val_acc:0.980]
Epoch [85/120    avg_loss:0.013, val_acc:0.979]
Epoch [86/120    avg_loss:0.011, val_acc:0.979]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.010, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.017, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.010, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.004, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.980]
Epoch [105/120    avg_loss:0.006, val_acc:0.982]
Epoch [106/120    avg_loss:0.004, val_acc:0.982]
Epoch [107/120    avg_loss:0.006, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.007, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.010, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.005, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     9     0     0     0     6    28     7]
 [    0     0 18037     0    20     0    29     0     4     0]
 [    0     0     0  1947     0     0     0     0    84     5]
 [    0     1     1     0  2960     0     6     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    51     0     0     0  4811     0    16     0]
 [    0     9     0     0     0     0     0  1280     0     1]
 [    0    18     1    71    53     0     5     0  3423     0]
 [    0     0     0     0    11    16     0     0     0   892]]

Accuracy:
98.9010194490637

F1 scores:
[       nan 0.99392618 0.9970702  0.95840512 0.98404255 0.99390708
 0.98900195 0.99378882 0.96057247 0.97646415]

Kappa:
0.9854392503298604
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f8d8177b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.142, val_acc:0.660]
Epoch [2/120    avg_loss:0.729, val_acc:0.744]
Epoch [3/120    avg_loss:0.647, val_acc:0.665]
Epoch [4/120    avg_loss:0.517, val_acc:0.821]
Epoch [5/120    avg_loss:0.409, val_acc:0.795]
Epoch [6/120    avg_loss:0.454, val_acc:0.815]
Epoch [7/120    avg_loss:0.364, val_acc:0.854]
Epoch [8/120    avg_loss:0.350, val_acc:0.733]
Epoch [9/120    avg_loss:0.339, val_acc:0.853]
Epoch [10/120    avg_loss:0.334, val_acc:0.768]
Epoch [11/120    avg_loss:0.297, val_acc:0.901]
Epoch [12/120    avg_loss:0.239, val_acc:0.879]
Epoch [13/120    avg_loss:0.253, val_acc:0.867]
Epoch [14/120    avg_loss:0.218, val_acc:0.895]
Epoch [15/120    avg_loss:0.232, val_acc:0.853]
Epoch [16/120    avg_loss:0.192, val_acc:0.805]
Epoch [17/120    avg_loss:0.169, val_acc:0.908]
Epoch [18/120    avg_loss:0.168, val_acc:0.904]
Epoch [19/120    avg_loss:0.137, val_acc:0.935]
Epoch [20/120    avg_loss:0.105, val_acc:0.926]
Epoch [21/120    avg_loss:0.095, val_acc:0.920]
Epoch [22/120    avg_loss:0.093, val_acc:0.884]
Epoch [23/120    avg_loss:0.129, val_acc:0.935]
Epoch [24/120    avg_loss:0.135, val_acc:0.918]
Epoch [25/120    avg_loss:0.085, val_acc:0.964]
Epoch [26/120    avg_loss:0.100, val_acc:0.943]
Epoch [27/120    avg_loss:0.080, val_acc:0.927]
Epoch [28/120    avg_loss:0.095, val_acc:0.875]
Epoch [29/120    avg_loss:0.080, val_acc:0.923]
Epoch [30/120    avg_loss:0.061, val_acc:0.950]
Epoch [31/120    avg_loss:0.073, val_acc:0.954]
Epoch [32/120    avg_loss:0.094, val_acc:0.966]
Epoch [33/120    avg_loss:0.079, val_acc:0.949]
Epoch [34/120    avg_loss:0.083, val_acc:0.916]
Epoch [35/120    avg_loss:0.089, val_acc:0.930]
Epoch [36/120    avg_loss:0.124, val_acc:0.950]
Epoch [37/120    avg_loss:0.083, val_acc:0.930]
Epoch [38/120    avg_loss:0.071, val_acc:0.953]
Epoch [39/120    avg_loss:0.041, val_acc:0.963]
Epoch [40/120    avg_loss:0.042, val_acc:0.958]
Epoch [41/120    avg_loss:0.065, val_acc:0.953]
Epoch [42/120    avg_loss:0.057, val_acc:0.965]
Epoch [43/120    avg_loss:0.051, val_acc:0.959]
Epoch [44/120    avg_loss:0.064, val_acc:0.945]
Epoch [45/120    avg_loss:0.076, val_acc:0.917]
Epoch [46/120    avg_loss:0.037, val_acc:0.966]
Epoch [47/120    avg_loss:0.051, val_acc:0.968]
Epoch [48/120    avg_loss:0.029, val_acc:0.974]
Epoch [49/120    avg_loss:0.019, val_acc:0.973]
Epoch [50/120    avg_loss:0.029, val_acc:0.970]
Epoch [51/120    avg_loss:0.024, val_acc:0.974]
Epoch [52/120    avg_loss:0.021, val_acc:0.970]
Epoch [53/120    avg_loss:0.029, val_acc:0.970]
Epoch [54/120    avg_loss:0.025, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.977]
Epoch [56/120    avg_loss:0.028, val_acc:0.970]
Epoch [57/120    avg_loss:0.015, val_acc:0.978]
Epoch [58/120    avg_loss:0.019, val_acc:0.978]
Epoch [59/120    avg_loss:0.029, val_acc:0.978]
Epoch [60/120    avg_loss:0.020, val_acc:0.976]
Epoch [61/120    avg_loss:0.022, val_acc:0.980]
Epoch [62/120    avg_loss:0.021, val_acc:0.980]
Epoch [63/120    avg_loss:0.025, val_acc:0.975]
Epoch [64/120    avg_loss:0.018, val_acc:0.980]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.014, val_acc:0.981]
Epoch [67/120    avg_loss:0.020, val_acc:0.979]
Epoch [68/120    avg_loss:0.024, val_acc:0.976]
Epoch [69/120    avg_loss:0.022, val_acc:0.980]
Epoch [70/120    avg_loss:0.017, val_acc:0.979]
Epoch [71/120    avg_loss:0.020, val_acc:0.980]
Epoch [72/120    avg_loss:0.013, val_acc:0.977]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.014, val_acc:0.978]
Epoch [75/120    avg_loss:0.016, val_acc:0.980]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.020, val_acc:0.978]
Epoch [78/120    avg_loss:0.012, val_acc:0.979]
Epoch [79/120    avg_loss:0.014, val_acc:0.978]
Epoch [80/120    avg_loss:0.012, val_acc:0.977]
Epoch [81/120    avg_loss:0.017, val_acc:0.977]
Epoch [82/120    avg_loss:0.023, val_acc:0.978]
Epoch [83/120    avg_loss:0.020, val_acc:0.978]
Epoch [84/120    avg_loss:0.019, val_acc:0.978]
Epoch [85/120    avg_loss:0.016, val_acc:0.978]
Epoch [86/120    avg_loss:0.014, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.978]
Epoch [88/120    avg_loss:0.022, val_acc:0.978]
Epoch [89/120    avg_loss:0.014, val_acc:0.978]
Epoch [90/120    avg_loss:0.026, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.979]
Epoch [92/120    avg_loss:0.012, val_acc:0.979]
Epoch [93/120    avg_loss:0.024, val_acc:0.979]
Epoch [94/120    avg_loss:0.019, val_acc:0.979]
Epoch [95/120    avg_loss:0.016, val_acc:0.979]
Epoch [96/120    avg_loss:0.020, val_acc:0.979]
Epoch [97/120    avg_loss:0.014, val_acc:0.979]
Epoch [98/120    avg_loss:0.026, val_acc:0.979]
Epoch [99/120    avg_loss:0.027, val_acc:0.979]
Epoch [100/120    avg_loss:0.015, val_acc:0.979]
Epoch [101/120    avg_loss:0.018, val_acc:0.979]
Epoch [102/120    avg_loss:0.014, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.979]
Epoch [104/120    avg_loss:0.019, val_acc:0.979]
Epoch [105/120    avg_loss:0.012, val_acc:0.979]
Epoch [106/120    avg_loss:0.023, val_acc:0.979]
Epoch [107/120    avg_loss:0.019, val_acc:0.979]
Epoch [108/120    avg_loss:0.022, val_acc:0.979]
Epoch [109/120    avg_loss:0.016, val_acc:0.979]
Epoch [110/120    avg_loss:0.017, val_acc:0.979]
Epoch [111/120    avg_loss:0.015, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.027, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.019, val_acc:0.979]
Epoch [116/120    avg_loss:0.013, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.034, val_acc:0.979]
Epoch [119/120    avg_loss:0.018, val_acc:0.979]
Epoch [120/120    avg_loss:0.013, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6376     0     2     0     0     1     2    49     2]
 [    0     0 17262     0    22     0   806     0     0     0]
 [    0     0     0  1911     0     0     0     0   123     2]
 [    0    22     2     0  2938     0     4     0     2     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4872     0     4     0]
 [    0     7     0     0     0     0     0  1283     0     0]
 [    0    26     1    26     2     0     0     0  3516     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
97.29833947894826

F1 scores:
[       nan 0.9913706  0.97644031 0.96150943 0.99022582 0.99618321
 0.9226399  0.99650485 0.96792842 0.99019608]

Kappa:
0.9644951445038057
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1805942780>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.215, val_acc:0.499]
Epoch [2/120    avg_loss:0.755, val_acc:0.696]
Epoch [3/120    avg_loss:0.587, val_acc:0.786]
Epoch [4/120    avg_loss:0.502, val_acc:0.788]
Epoch [5/120    avg_loss:0.509, val_acc:0.801]
Epoch [6/120    avg_loss:0.379, val_acc:0.814]
Epoch [7/120    avg_loss:0.311, val_acc:0.880]
Epoch [8/120    avg_loss:0.286, val_acc:0.815]
Epoch [9/120    avg_loss:0.348, val_acc:0.887]
Epoch [10/120    avg_loss:0.272, val_acc:0.889]
Epoch [11/120    avg_loss:0.229, val_acc:0.879]
Epoch [12/120    avg_loss:0.197, val_acc:0.917]
Epoch [13/120    avg_loss:0.198, val_acc:0.850]
Epoch [14/120    avg_loss:0.196, val_acc:0.841]
Epoch [15/120    avg_loss:0.171, val_acc:0.918]
Epoch [16/120    avg_loss:0.212, val_acc:0.845]
Epoch [17/120    avg_loss:0.207, val_acc:0.883]
Epoch [18/120    avg_loss:0.172, val_acc:0.892]
Epoch [19/120    avg_loss:0.227, val_acc:0.929]
Epoch [20/120    avg_loss:0.112, val_acc:0.890]
Epoch [21/120    avg_loss:0.112, val_acc:0.944]
Epoch [22/120    avg_loss:0.111, val_acc:0.930]
Epoch [23/120    avg_loss:0.127, val_acc:0.947]
Epoch [24/120    avg_loss:0.142, val_acc:0.912]
Epoch [25/120    avg_loss:0.086, val_acc:0.943]
Epoch [26/120    avg_loss:0.139, val_acc:0.943]
Epoch [27/120    avg_loss:0.108, val_acc:0.950]
Epoch [28/120    avg_loss:0.109, val_acc:0.948]
Epoch [29/120    avg_loss:0.072, val_acc:0.928]
Epoch [30/120    avg_loss:0.064, val_acc:0.911]
Epoch [31/120    avg_loss:0.059, val_acc:0.952]
Epoch [32/120    avg_loss:0.038, val_acc:0.962]
Epoch [33/120    avg_loss:0.044, val_acc:0.955]
Epoch [34/120    avg_loss:0.141, val_acc:0.910]
Epoch [35/120    avg_loss:0.143, val_acc:0.921]
Epoch [36/120    avg_loss:0.073, val_acc:0.952]
Epoch [37/120    avg_loss:0.078, val_acc:0.943]
Epoch [38/120    avg_loss:0.082, val_acc:0.974]
Epoch [39/120    avg_loss:0.057, val_acc:0.952]
Epoch [40/120    avg_loss:0.100, val_acc:0.960]
Epoch [41/120    avg_loss:0.074, val_acc:0.961]
Epoch [42/120    avg_loss:0.036, val_acc:0.950]
Epoch [43/120    avg_loss:0.042, val_acc:0.954]
Epoch [44/120    avg_loss:0.042, val_acc:0.963]
Epoch [45/120    avg_loss:0.051, val_acc:0.963]
Epoch [46/120    avg_loss:0.030, val_acc:0.973]
Epoch [47/120    avg_loss:0.034, val_acc:0.954]
Epoch [48/120    avg_loss:0.040, val_acc:0.970]
Epoch [49/120    avg_loss:0.030, val_acc:0.938]
Epoch [50/120    avg_loss:0.025, val_acc:0.967]
Epoch [51/120    avg_loss:0.031, val_acc:0.972]
Epoch [52/120    avg_loss:0.032, val_acc:0.978]
Epoch [53/120    avg_loss:0.017, val_acc:0.980]
Epoch [54/120    avg_loss:0.026, val_acc:0.981]
Epoch [55/120    avg_loss:0.033, val_acc:0.980]
Epoch [56/120    avg_loss:0.017, val_acc:0.981]
Epoch [57/120    avg_loss:0.013, val_acc:0.981]
Epoch [58/120    avg_loss:0.018, val_acc:0.978]
Epoch [59/120    avg_loss:0.019, val_acc:0.980]
Epoch [60/120    avg_loss:0.018, val_acc:0.977]
Epoch [61/120    avg_loss:0.017, val_acc:0.980]
Epoch [62/120    avg_loss:0.015, val_acc:0.984]
Epoch [63/120    avg_loss:0.024, val_acc:0.981]
Epoch [64/120    avg_loss:0.017, val_acc:0.982]
Epoch [65/120    avg_loss:0.012, val_acc:0.981]
Epoch [66/120    avg_loss:0.018, val_acc:0.979]
Epoch [67/120    avg_loss:0.012, val_acc:0.981]
Epoch [68/120    avg_loss:0.012, val_acc:0.980]
Epoch [69/120    avg_loss:0.016, val_acc:0.981]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.013, val_acc:0.984]
Epoch [72/120    avg_loss:0.018, val_acc:0.982]
Epoch [73/120    avg_loss:0.020, val_acc:0.980]
Epoch [74/120    avg_loss:0.014, val_acc:0.981]
Epoch [75/120    avg_loss:0.017, val_acc:0.978]
Epoch [76/120    avg_loss:0.017, val_acc:0.980]
Epoch [77/120    avg_loss:0.023, val_acc:0.979]
Epoch [78/120    avg_loss:0.019, val_acc:0.981]
Epoch [79/120    avg_loss:0.013, val_acc:0.981]
Epoch [80/120    avg_loss:0.014, val_acc:0.982]
Epoch [81/120    avg_loss:0.019, val_acc:0.982]
Epoch [82/120    avg_loss:0.014, val_acc:0.979]
Epoch [83/120    avg_loss:0.018, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.985]
Epoch [87/120    avg_loss:0.014, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.014, val_acc:0.985]
Epoch [90/120    avg_loss:0.015, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.015, val_acc:0.984]
Epoch [93/120    avg_loss:0.017, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.016, val_acc:0.981]
Epoch [96/120    avg_loss:0.019, val_acc:0.983]
Epoch [97/120    avg_loss:0.017, val_acc:0.983]
Epoch [98/120    avg_loss:0.013, val_acc:0.984]
Epoch [99/120    avg_loss:0.014, val_acc:0.985]
Epoch [100/120    avg_loss:0.013, val_acc:0.985]
Epoch [101/120    avg_loss:0.015, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.015, val_acc:0.985]
Epoch [104/120    avg_loss:0.018, val_acc:0.985]
Epoch [105/120    avg_loss:0.028, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.021, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.016, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0    14     0     0     2     4    38     5]
 [    0     0 17804     0    10     0   276     0     0     0]
 [    0     0     0  1990     0     0     0     0    45     1]
 [    0     9     0     0  2954     0     2     0     4     3]
 [    0     0     0     0     0  1302     0     0     0     3]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    11     0     0     0     0     4  1271     0     4]
 [    0    30     2    69    24     0     3     0  3443     0]
 [    0     0     0     0     0     7     0     0     0   912]]

Accuracy:
98.62627431132962

F1 scores:
[       nan 0.99120691 0.99197682 0.9686055  0.99127517 0.99617445
 0.97142288 0.99103314 0.96972257 0.98754737]

Kappa:
0.9818510899712324
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68143436d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.226, val_acc:0.484]
Epoch [2/120    avg_loss:0.700, val_acc:0.605]
Epoch [3/120    avg_loss:0.662, val_acc:0.797]
Epoch [4/120    avg_loss:0.541, val_acc:0.733]
Epoch [5/120    avg_loss:0.395, val_acc:0.804]
Epoch [6/120    avg_loss:0.363, val_acc:0.845]
Epoch [7/120    avg_loss:0.291, val_acc:0.798]
Epoch [8/120    avg_loss:0.390, val_acc:0.868]
Epoch [9/120    avg_loss:0.278, val_acc:0.892]
Epoch [10/120    avg_loss:0.271, val_acc:0.902]
Epoch [11/120    avg_loss:0.259, val_acc:0.882]
Epoch [12/120    avg_loss:0.218, val_acc:0.917]
Epoch [13/120    avg_loss:0.204, val_acc:0.889]
Epoch [14/120    avg_loss:0.147, val_acc:0.907]
Epoch [15/120    avg_loss:0.169, val_acc:0.928]
Epoch [16/120    avg_loss:0.168, val_acc:0.897]
Epoch [17/120    avg_loss:0.162, val_acc:0.913]
Epoch [18/120    avg_loss:0.200, val_acc:0.897]
Epoch [19/120    avg_loss:0.149, val_acc:0.909]
Epoch [20/120    avg_loss:0.122, val_acc:0.928]
Epoch [21/120    avg_loss:0.127, val_acc:0.929]
Epoch [22/120    avg_loss:0.100, val_acc:0.937]
Epoch [23/120    avg_loss:0.117, val_acc:0.931]
Epoch [24/120    avg_loss:0.084, val_acc:0.868]
Epoch [25/120    avg_loss:0.078, val_acc:0.934]
Epoch [26/120    avg_loss:0.078, val_acc:0.943]
Epoch [27/120    avg_loss:0.146, val_acc:0.914]
Epoch [28/120    avg_loss:0.092, val_acc:0.919]
Epoch [29/120    avg_loss:0.186, val_acc:0.882]
Epoch [30/120    avg_loss:0.103, val_acc:0.904]
Epoch [31/120    avg_loss:0.057, val_acc:0.954]
Epoch [32/120    avg_loss:0.078, val_acc:0.935]
Epoch [33/120    avg_loss:0.081, val_acc:0.886]
Epoch [34/120    avg_loss:0.061, val_acc:0.918]
Epoch [35/120    avg_loss:0.051, val_acc:0.940]
Epoch [36/120    avg_loss:0.058, val_acc:0.953]
Epoch [37/120    avg_loss:0.029, val_acc:0.957]
Epoch [38/120    avg_loss:0.046, val_acc:0.958]
Epoch [39/120    avg_loss:0.057, val_acc:0.954]
Epoch [40/120    avg_loss:0.038, val_acc:0.968]
Epoch [41/120    avg_loss:0.043, val_acc:0.960]
Epoch [42/120    avg_loss:0.033, val_acc:0.959]
Epoch [43/120    avg_loss:0.051, val_acc:0.941]
Epoch [44/120    avg_loss:0.053, val_acc:0.960]
Epoch [45/120    avg_loss:0.046, val_acc:0.971]
Epoch [46/120    avg_loss:0.044, val_acc:0.965]
Epoch [47/120    avg_loss:0.035, val_acc:0.922]
Epoch [48/120    avg_loss:0.030, val_acc:0.967]
Epoch [49/120    avg_loss:0.049, val_acc:0.946]
Epoch [50/120    avg_loss:0.027, val_acc:0.966]
Epoch [51/120    avg_loss:0.022, val_acc:0.958]
Epoch [52/120    avg_loss:0.027, val_acc:0.956]
Epoch [53/120    avg_loss:0.048, val_acc:0.945]
Epoch [54/120    avg_loss:0.056, val_acc:0.962]
Epoch [55/120    avg_loss:0.022, val_acc:0.943]
Epoch [56/120    avg_loss:0.041, val_acc:0.932]
Epoch [57/120    avg_loss:0.073, val_acc:0.946]
Epoch [58/120    avg_loss:0.048, val_acc:0.944]
Epoch [59/120    avg_loss:0.046, val_acc:0.961]
Epoch [60/120    avg_loss:0.020, val_acc:0.967]
Epoch [61/120    avg_loss:0.026, val_acc:0.967]
Epoch [62/120    avg_loss:0.024, val_acc:0.968]
Epoch [63/120    avg_loss:0.020, val_acc:0.968]
Epoch [64/120    avg_loss:0.022, val_acc:0.969]
Epoch [65/120    avg_loss:0.015, val_acc:0.971]
Epoch [66/120    avg_loss:0.037, val_acc:0.967]
Epoch [67/120    avg_loss:0.026, val_acc:0.973]
Epoch [68/120    avg_loss:0.023, val_acc:0.972]
Epoch [69/120    avg_loss:0.023, val_acc:0.970]
Epoch [70/120    avg_loss:0.014, val_acc:0.972]
Epoch [71/120    avg_loss:0.009, val_acc:0.970]
Epoch [72/120    avg_loss:0.021, val_acc:0.972]
Epoch [73/120    avg_loss:0.023, val_acc:0.969]
Epoch [74/120    avg_loss:0.014, val_acc:0.972]
Epoch [75/120    avg_loss:0.026, val_acc:0.974]
Epoch [76/120    avg_loss:0.012, val_acc:0.973]
Epoch [77/120    avg_loss:0.014, val_acc:0.974]
Epoch [78/120    avg_loss:0.016, val_acc:0.974]
Epoch [79/120    avg_loss:0.012, val_acc:0.973]
Epoch [80/120    avg_loss:0.012, val_acc:0.972]
Epoch [81/120    avg_loss:0.013, val_acc:0.972]
Epoch [82/120    avg_loss:0.021, val_acc:0.973]
Epoch [83/120    avg_loss:0.008, val_acc:0.973]
Epoch [84/120    avg_loss:0.013, val_acc:0.972]
Epoch [85/120    avg_loss:0.010, val_acc:0.971]
Epoch [86/120    avg_loss:0.024, val_acc:0.973]
Epoch [87/120    avg_loss:0.013, val_acc:0.975]
Epoch [88/120    avg_loss:0.022, val_acc:0.975]
Epoch [89/120    avg_loss:0.012, val_acc:0.973]
Epoch [90/120    avg_loss:0.008, val_acc:0.973]
Epoch [91/120    avg_loss:0.018, val_acc:0.973]
Epoch [92/120    avg_loss:0.012, val_acc:0.972]
Epoch [93/120    avg_loss:0.018, val_acc:0.972]
Epoch [94/120    avg_loss:0.014, val_acc:0.973]
Epoch [95/120    avg_loss:0.015, val_acc:0.975]
Epoch [96/120    avg_loss:0.017, val_acc:0.973]
Epoch [97/120    avg_loss:0.023, val_acc:0.968]
Epoch [98/120    avg_loss:0.011, val_acc:0.972]
Epoch [99/120    avg_loss:0.012, val_acc:0.973]
Epoch [100/120    avg_loss:0.012, val_acc:0.972]
Epoch [101/120    avg_loss:0.022, val_acc:0.972]
Epoch [102/120    avg_loss:0.022, val_acc:0.973]
Epoch [103/120    avg_loss:0.012, val_acc:0.973]
Epoch [104/120    avg_loss:0.012, val_acc:0.973]
Epoch [105/120    avg_loss:0.009, val_acc:0.975]
Epoch [106/120    avg_loss:0.023, val_acc:0.973]
Epoch [107/120    avg_loss:0.013, val_acc:0.970]
Epoch [108/120    avg_loss:0.023, val_acc:0.970]
Epoch [109/120    avg_loss:0.018, val_acc:0.973]
Epoch [110/120    avg_loss:0.018, val_acc:0.973]
Epoch [111/120    avg_loss:0.009, val_acc:0.974]
Epoch [112/120    avg_loss:0.010, val_acc:0.973]
Epoch [113/120    avg_loss:0.023, val_acc:0.978]
Epoch [114/120    avg_loss:0.018, val_acc:0.974]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.010, val_acc:0.975]
Epoch [117/120    avg_loss:0.013, val_acc:0.973]
Epoch [118/120    avg_loss:0.009, val_acc:0.973]
Epoch [119/120    avg_loss:0.015, val_acc:0.974]
Epoch [120/120    avg_loss:0.012, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     9     0     0     4     0     8     1]
 [    0     0 17910     0    36     0   144     0     0     0]
 [    0     9     0  1892     0     0     0     0   134     1]
 [    0    23     0     0  2940     0     2     0     3     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     0     0  4854     0     1     0]
 [    0     9     0     0     0     0     0  1281     0     0]
 [    0    69     0    39    13     0     0     0  3450     0]
 [    0     0     0     0     0    43     0     0     0   876]]

Accuracy:
98.61422408599041

F1 scores:
[       nan 0.98980852 0.99436471 0.95171026 0.98641168 0.98379193
 0.98239223 0.99649942 0.96274592 0.97279289]

Kappa:
0.9816616911088355
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7252e9f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.201, val_acc:0.525]
Epoch [2/120    avg_loss:0.791, val_acc:0.708]
Epoch [3/120    avg_loss:0.688, val_acc:0.766]
Epoch [4/120    avg_loss:0.502, val_acc:0.766]
Epoch [5/120    avg_loss:0.508, val_acc:0.831]
Epoch [6/120    avg_loss:0.361, val_acc:0.801]
Epoch [7/120    avg_loss:0.347, val_acc:0.811]
Epoch [8/120    avg_loss:0.336, val_acc:0.841]
Epoch [9/120    avg_loss:0.334, val_acc:0.873]
Epoch [10/120    avg_loss:0.247, val_acc:0.891]
Epoch [11/120    avg_loss:0.284, val_acc:0.917]
Epoch [12/120    avg_loss:0.248, val_acc:0.874]
Epoch [13/120    avg_loss:0.263, val_acc:0.887]
Epoch [14/120    avg_loss:0.183, val_acc:0.792]
Epoch [15/120    avg_loss:0.242, val_acc:0.926]
Epoch [16/120    avg_loss:0.165, val_acc:0.914]
Epoch [17/120    avg_loss:0.158, val_acc:0.922]
Epoch [18/120    avg_loss:0.140, val_acc:0.930]
Epoch [19/120    avg_loss:0.156, val_acc:0.933]
Epoch [20/120    avg_loss:0.133, val_acc:0.951]
Epoch [21/120    avg_loss:0.108, val_acc:0.803]
Epoch [22/120    avg_loss:0.160, val_acc:0.914]
Epoch [23/120    avg_loss:0.143, val_acc:0.945]
Epoch [24/120    avg_loss:0.101, val_acc:0.956]
Epoch [25/120    avg_loss:0.086, val_acc:0.896]
Epoch [26/120    avg_loss:0.114, val_acc:0.947]
Epoch [27/120    avg_loss:0.110, val_acc:0.931]
Epoch [28/120    avg_loss:0.079, val_acc:0.958]
Epoch [29/120    avg_loss:0.073, val_acc:0.953]
Epoch [30/120    avg_loss:0.065, val_acc:0.960]
Epoch [31/120    avg_loss:0.090, val_acc:0.965]
Epoch [32/120    avg_loss:0.121, val_acc:0.928]
Epoch [33/120    avg_loss:0.169, val_acc:0.870]
Epoch [34/120    avg_loss:0.110, val_acc:0.958]
Epoch [35/120    avg_loss:0.059, val_acc:0.964]
Epoch [36/120    avg_loss:0.052, val_acc:0.954]
Epoch [37/120    avg_loss:0.062, val_acc:0.965]
Epoch [38/120    avg_loss:0.097, val_acc:0.943]
Epoch [39/120    avg_loss:0.086, val_acc:0.967]
Epoch [40/120    avg_loss:0.073, val_acc:0.944]
Epoch [41/120    avg_loss:0.104, val_acc:0.895]
Epoch [42/120    avg_loss:0.103, val_acc:0.952]
Epoch [43/120    avg_loss:0.082, val_acc:0.958]
Epoch [44/120    avg_loss:0.051, val_acc:0.951]
Epoch [45/120    avg_loss:0.052, val_acc:0.938]
Epoch [46/120    avg_loss:0.039, val_acc:0.961]
Epoch [47/120    avg_loss:0.053, val_acc:0.955]
Epoch [48/120    avg_loss:0.020, val_acc:0.970]
Epoch [49/120    avg_loss:0.026, val_acc:0.972]
Epoch [50/120    avg_loss:0.050, val_acc:0.968]
Epoch [51/120    avg_loss:0.043, val_acc:0.958]
Epoch [52/120    avg_loss:0.108, val_acc:0.906]
Epoch [53/120    avg_loss:0.071, val_acc:0.956]
Epoch [54/120    avg_loss:0.064, val_acc:0.945]
Epoch [55/120    avg_loss:0.057, val_acc:0.952]
Epoch [56/120    avg_loss:0.032, val_acc:0.960]
Epoch [57/120    avg_loss:0.030, val_acc:0.925]
Epoch [58/120    avg_loss:0.022, val_acc:0.977]
Epoch [59/120    avg_loss:0.018, val_acc:0.973]
Epoch [60/120    avg_loss:0.019, val_acc:0.966]
Epoch [61/120    avg_loss:0.024, val_acc:0.970]
Epoch [62/120    avg_loss:0.026, val_acc:0.960]
Epoch [63/120    avg_loss:0.026, val_acc:0.953]
Epoch [64/120    avg_loss:0.030, val_acc:0.970]
Epoch [65/120    avg_loss:0.023, val_acc:0.965]
Epoch [66/120    avg_loss:0.027, val_acc:0.970]
Epoch [67/120    avg_loss:0.024, val_acc:0.973]
Epoch [68/120    avg_loss:0.022, val_acc:0.963]
Epoch [69/120    avg_loss:0.035, val_acc:0.949]
Epoch [70/120    avg_loss:0.018, val_acc:0.970]
Epoch [71/120    avg_loss:0.039, val_acc:0.973]
Epoch [72/120    avg_loss:0.043, val_acc:0.975]
Epoch [73/120    avg_loss:0.019, val_acc:0.978]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.014, val_acc:0.979]
Epoch [76/120    avg_loss:0.021, val_acc:0.976]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.016, val_acc:0.979]
Epoch [79/120    avg_loss:0.011, val_acc:0.982]
Epoch [80/120    avg_loss:0.015, val_acc:0.982]
Epoch [81/120    avg_loss:0.018, val_acc:0.983]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.025, val_acc:0.978]
Epoch [85/120    avg_loss:0.016, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.014, val_acc:0.982]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.984]
Epoch [93/120    avg_loss:0.013, val_acc:0.984]
Epoch [94/120    avg_loss:0.013, val_acc:0.984]
Epoch [95/120    avg_loss:0.010, val_acc:0.984]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.015, val_acc:0.980]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.011, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.982]
Epoch [103/120    avg_loss:0.011, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.020, val_acc:0.981]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.017, val_acc:0.979]
Epoch [110/120    avg_loss:0.017, val_acc:0.975]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.015, val_acc:0.976]
Epoch [114/120    avg_loss:0.015, val_acc:0.979]
Epoch [115/120    avg_loss:0.024, val_acc:0.979]
Epoch [116/120    avg_loss:0.020, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.016, val_acc:0.979]
Epoch [119/120    avg_loss:0.010, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0    12     0     0     0     0    20     0]
 [    0     0 17824     0    80     0   185     0     1     0]
 [    0     0     0  1963     0     0     0     0    72     1]
 [    0    15     0     0  2947     0     1     0     6     3]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     1     0     0     0  4868     0     9     0]
 [    0     5     0     0     0     0     0  1285     0     0]
 [    0    39     1    28    11     0     0     0  3492     0]
 [    0     0     0     0     2     7     0     0     0   910]]

Accuracy:
98.79256742101077

F1 scores:
[       nan 0.99294081 0.99253814 0.97202278 0.98037259 0.99655832
 0.98026581 0.99805825 0.97392274 0.99182561]

Kappa:
0.9840438660826835
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ea36c7780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.227, val_acc:0.633]
Epoch [2/120    avg_loss:0.823, val_acc:0.657]
Epoch [3/120    avg_loss:0.580, val_acc:0.762]
Epoch [4/120    avg_loss:0.477, val_acc:0.851]
Epoch [5/120    avg_loss:0.369, val_acc:0.824]
Epoch [6/120    avg_loss:0.370, val_acc:0.815]
Epoch [7/120    avg_loss:0.395, val_acc:0.888]
Epoch [8/120    avg_loss:0.379, val_acc:0.822]
Epoch [9/120    avg_loss:0.262, val_acc:0.815]
Epoch [10/120    avg_loss:0.191, val_acc:0.907]
Epoch [11/120    avg_loss:0.195, val_acc:0.915]
Epoch [12/120    avg_loss:0.173, val_acc:0.912]
Epoch [13/120    avg_loss:0.184, val_acc:0.877]
Epoch [14/120    avg_loss:0.259, val_acc:0.874]
Epoch [15/120    avg_loss:0.157, val_acc:0.924]
Epoch [16/120    avg_loss:0.127, val_acc:0.903]
Epoch [17/120    avg_loss:0.178, val_acc:0.853]
Epoch [18/120    avg_loss:0.211, val_acc:0.916]
Epoch [19/120    avg_loss:0.185, val_acc:0.888]
Epoch [20/120    avg_loss:0.141, val_acc:0.882]
Epoch [21/120    avg_loss:0.073, val_acc:0.950]
Epoch [22/120    avg_loss:0.075, val_acc:0.936]
Epoch [23/120    avg_loss:0.076, val_acc:0.952]
Epoch [24/120    avg_loss:0.097, val_acc:0.931]
Epoch [25/120    avg_loss:0.210, val_acc:0.927]
Epoch [26/120    avg_loss:0.110, val_acc:0.915]
Epoch [27/120    avg_loss:0.077, val_acc:0.958]
Epoch [28/120    avg_loss:0.068, val_acc:0.921]
Epoch [29/120    avg_loss:0.065, val_acc:0.963]
Epoch [30/120    avg_loss:0.106, val_acc:0.953]
Epoch [31/120    avg_loss:0.057, val_acc:0.953]
Epoch [32/120    avg_loss:0.064, val_acc:0.968]
Epoch [33/120    avg_loss:0.104, val_acc:0.963]
Epoch [34/120    avg_loss:0.061, val_acc:0.969]
Epoch [35/120    avg_loss:0.046, val_acc:0.962]
Epoch [36/120    avg_loss:0.040, val_acc:0.952]
Epoch [37/120    avg_loss:0.068, val_acc:0.952]
Epoch [38/120    avg_loss:0.056, val_acc:0.951]
Epoch [39/120    avg_loss:0.048, val_acc:0.962]
Epoch [40/120    avg_loss:0.044, val_acc:0.969]
Epoch [41/120    avg_loss:0.044, val_acc:0.961]
Epoch [42/120    avg_loss:0.029, val_acc:0.978]
Epoch [43/120    avg_loss:0.039, val_acc:0.976]
Epoch [44/120    avg_loss:0.033, val_acc:0.938]
Epoch [45/120    avg_loss:0.063, val_acc:0.945]
Epoch [46/120    avg_loss:0.055, val_acc:0.971]
Epoch [47/120    avg_loss:0.057, val_acc:0.946]
Epoch [48/120    avg_loss:0.053, val_acc:0.953]
Epoch [49/120    avg_loss:0.046, val_acc:0.975]
Epoch [50/120    avg_loss:0.040, val_acc:0.976]
Epoch [51/120    avg_loss:0.036, val_acc:0.971]
Epoch [52/120    avg_loss:0.028, val_acc:0.980]
Epoch [53/120    avg_loss:0.040, val_acc:0.962]
Epoch [54/120    avg_loss:0.033, val_acc:0.972]
Epoch [55/120    avg_loss:0.013, val_acc:0.972]
Epoch [56/120    avg_loss:0.018, val_acc:0.975]
Epoch [57/120    avg_loss:0.030, val_acc:0.976]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.015, val_acc:0.958]
Epoch [60/120    avg_loss:0.030, val_acc:0.974]
Epoch [61/120    avg_loss:0.023, val_acc:0.978]
Epoch [62/120    avg_loss:0.032, val_acc:0.969]
Epoch [63/120    avg_loss:0.044, val_acc:0.978]
Epoch [64/120    avg_loss:0.018, val_acc:0.975]
Epoch [65/120    avg_loss:0.019, val_acc:0.977]
Epoch [66/120    avg_loss:0.017, val_acc:0.983]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.011, val_acc:0.983]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.983]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.007, val_acc:0.984]
Epoch [74/120    avg_loss:0.016, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.011, val_acc:0.983]
Epoch [78/120    avg_loss:0.020, val_acc:0.986]
Epoch [79/120    avg_loss:0.021, val_acc:0.989]
Epoch [80/120    avg_loss:0.018, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.985]
Epoch [86/120    avg_loss:0.010, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.986]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.014, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.019, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.025, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.013, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.020, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     0     0     0     4     2     0]
 [    0     0 18056     0    14     0    17     0     3     0]
 [    0     0     0  1998     0     0     0     0    36     2]
 [    0    31     0     0  2931     0     3     0     6     1]
 [    0     0    54     0     0  1251     0     0     0     0]
 [    0     0     2     0     0     0  4871     0     5     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    46     1    21     8     0     0     0  3494     1]
 [    0     0     0     0    13    13     0     0     0   893]]

Accuracy:
99.3179572458005

F1 scores:
[       nan 0.9935833  0.9974864  0.98545006 0.98720108 0.97391981
 0.99723616 0.99845201 0.98187439 0.98348018]

Kappa:
0.9909574698286082
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f039fcfb748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.366, val_acc:0.583]
Epoch [2/120    avg_loss:0.713, val_acc:0.694]
Epoch [3/120    avg_loss:0.636, val_acc:0.657]
Epoch [4/120    avg_loss:0.489, val_acc:0.836]
Epoch [5/120    avg_loss:0.474, val_acc:0.711]
Epoch [6/120    avg_loss:0.360, val_acc:0.905]
Epoch [7/120    avg_loss:0.349, val_acc:0.824]
Epoch [8/120    avg_loss:0.363, val_acc:0.772]
Epoch [9/120    avg_loss:0.317, val_acc:0.846]
Epoch [10/120    avg_loss:0.231, val_acc:0.904]
Epoch [11/120    avg_loss:0.212, val_acc:0.857]
Epoch [12/120    avg_loss:0.197, val_acc:0.885]
Epoch [13/120    avg_loss:0.179, val_acc:0.884]
Epoch [14/120    avg_loss:0.194, val_acc:0.904]
Epoch [15/120    avg_loss:0.151, val_acc:0.908]
Epoch [16/120    avg_loss:0.115, val_acc:0.908]
Epoch [17/120    avg_loss:0.123, val_acc:0.936]
Epoch [18/120    avg_loss:0.134, val_acc:0.883]
Epoch [19/120    avg_loss:0.120, val_acc:0.915]
Epoch [20/120    avg_loss:0.121, val_acc:0.945]
Epoch [21/120    avg_loss:0.079, val_acc:0.944]
Epoch [22/120    avg_loss:0.124, val_acc:0.893]
Epoch [23/120    avg_loss:0.090, val_acc:0.946]
Epoch [24/120    avg_loss:0.148, val_acc:0.950]
Epoch [25/120    avg_loss:0.102, val_acc:0.957]
Epoch [26/120    avg_loss:0.067, val_acc:0.954]
Epoch [27/120    avg_loss:0.061, val_acc:0.940]
Epoch [28/120    avg_loss:0.057, val_acc:0.967]
Epoch [29/120    avg_loss:0.072, val_acc:0.960]
Epoch [30/120    avg_loss:0.065, val_acc:0.961]
Epoch [31/120    avg_loss:0.071, val_acc:0.934]
Epoch [32/120    avg_loss:0.031, val_acc:0.904]
Epoch [33/120    avg_loss:0.088, val_acc:0.941]
Epoch [34/120    avg_loss:0.047, val_acc:0.966]
Epoch [35/120    avg_loss:0.112, val_acc:0.930]
Epoch [36/120    avg_loss:0.078, val_acc:0.969]
Epoch [37/120    avg_loss:0.046, val_acc:0.953]
Epoch [38/120    avg_loss:0.074, val_acc:0.944]
Epoch [39/120    avg_loss:0.064, val_acc:0.966]
Epoch [40/120    avg_loss:0.037, val_acc:0.973]
Epoch [41/120    avg_loss:0.083, val_acc:0.952]
Epoch [42/120    avg_loss:0.046, val_acc:0.966]
Epoch [43/120    avg_loss:0.122, val_acc:0.923]
Epoch [44/120    avg_loss:0.043, val_acc:0.963]
Epoch [45/120    avg_loss:0.028, val_acc:0.962]
Epoch [46/120    avg_loss:0.031, val_acc:0.961]
Epoch [47/120    avg_loss:0.076, val_acc:0.956]
Epoch [48/120    avg_loss:0.053, val_acc:0.973]
Epoch [49/120    avg_loss:0.043, val_acc:0.913]
Epoch [50/120    avg_loss:0.021, val_acc:0.973]
Epoch [51/120    avg_loss:0.046, val_acc:0.966]
Epoch [52/120    avg_loss:0.026, val_acc:0.970]
Epoch [53/120    avg_loss:0.026, val_acc:0.957]
Epoch [54/120    avg_loss:0.032, val_acc:0.965]
Epoch [55/120    avg_loss:0.019, val_acc:0.971]
Epoch [56/120    avg_loss:0.026, val_acc:0.976]
Epoch [57/120    avg_loss:0.034, val_acc:0.975]
Epoch [58/120    avg_loss:0.019, val_acc:0.971]
Epoch [59/120    avg_loss:0.020, val_acc:0.976]
Epoch [60/120    avg_loss:0.015, val_acc:0.972]
Epoch [61/120    avg_loss:0.014, val_acc:0.976]
Epoch [62/120    avg_loss:0.019, val_acc:0.974]
Epoch [63/120    avg_loss:0.020, val_acc:0.969]
Epoch [64/120    avg_loss:0.030, val_acc:0.973]
Epoch [65/120    avg_loss:0.040, val_acc:0.969]
Epoch [66/120    avg_loss:0.025, val_acc:0.970]
Epoch [67/120    avg_loss:0.019, val_acc:0.979]
Epoch [68/120    avg_loss:0.014, val_acc:0.977]
Epoch [69/120    avg_loss:0.013, val_acc:0.977]
Epoch [70/120    avg_loss:0.010, val_acc:0.980]
Epoch [71/120    avg_loss:0.197, val_acc:0.851]
Epoch [72/120    avg_loss:0.260, val_acc:0.956]
Epoch [73/120    avg_loss:0.076, val_acc:0.939]
Epoch [74/120    avg_loss:0.078, val_acc:0.961]
Epoch [75/120    avg_loss:0.044, val_acc:0.967]
Epoch [76/120    avg_loss:0.051, val_acc:0.970]
Epoch [77/120    avg_loss:0.024, val_acc:0.968]
Epoch [78/120    avg_loss:0.032, val_acc:0.968]
Epoch [79/120    avg_loss:0.025, val_acc:0.966]
Epoch [80/120    avg_loss:0.018, val_acc:0.977]
Epoch [81/120    avg_loss:0.032, val_acc:0.965]
Epoch [82/120    avg_loss:0.021, val_acc:0.976]
Epoch [83/120    avg_loss:0.011, val_acc:0.976]
Epoch [84/120    avg_loss:0.012, val_acc:0.974]
Epoch [85/120    avg_loss:0.019, val_acc:0.975]
Epoch [86/120    avg_loss:0.009, val_acc:0.977]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.009, val_acc:0.977]
Epoch [89/120    avg_loss:0.009, val_acc:0.977]
Epoch [90/120    avg_loss:0.014, val_acc:0.977]
Epoch [91/120    avg_loss:0.014, val_acc:0.978]
Epoch [92/120    avg_loss:0.005, val_acc:0.978]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.014, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.977]
Epoch [96/120    avg_loss:0.006, val_acc:0.979]
Epoch [97/120    avg_loss:0.021, val_acc:0.978]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.011, val_acc:0.978]
Epoch [101/120    avg_loss:0.008, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.010, val_acc:0.980]
Epoch [104/120    avg_loss:0.006, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.980]
Epoch [106/120    avg_loss:0.012, val_acc:0.979]
Epoch [107/120    avg_loss:0.016, val_acc:0.980]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.980]
Epoch [110/120    avg_loss:0.015, val_acc:0.980]
Epoch [111/120    avg_loss:0.006, val_acc:0.980]
Epoch [112/120    avg_loss:0.007, val_acc:0.981]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.979]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.005, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     0     0     0     3    35     2]
 [    0     0 17954     0    20     0   116     0     0     0]
 [    0     4     0  1955     0     0     0     0    72     5]
 [    0    28     0     0  2930     0     2     0    10     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4868     0     7     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    31     0    11    10     0     0     0  3518     1]
 [    0     0     0     0    10    16     0     0     0   893]]

Accuracy:
99.06490251367701

F1 scores:
[       nan 0.99200745 0.99614392 0.97701149 0.98619993 0.99390708
 0.98702352 0.99883856 0.97546097 0.98024149]

Kappa:
0.9876247857665787
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd8bb2a0710>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.273, val_acc:0.528]
Epoch [2/120    avg_loss:0.830, val_acc:0.716]
Epoch [3/120    avg_loss:0.651, val_acc:0.717]
Epoch [4/120    avg_loss:0.476, val_acc:0.757]
Epoch [5/120    avg_loss:0.487, val_acc:0.834]
Epoch [6/120    avg_loss:0.369, val_acc:0.858]
Epoch [7/120    avg_loss:0.322, val_acc:0.755]
Epoch [8/120    avg_loss:0.260, val_acc:0.833]
Epoch [9/120    avg_loss:0.233, val_acc:0.909]
Epoch [10/120    avg_loss:0.190, val_acc:0.885]
Epoch [11/120    avg_loss:0.260, val_acc:0.902]
Epoch [12/120    avg_loss:0.154, val_acc:0.868]
Epoch [13/120    avg_loss:0.139, val_acc:0.920]
Epoch [14/120    avg_loss:0.126, val_acc:0.909]
Epoch [15/120    avg_loss:0.162, val_acc:0.926]
Epoch [16/120    avg_loss:0.172, val_acc:0.899]
Epoch [17/120    avg_loss:0.138, val_acc:0.903]
Epoch [18/120    avg_loss:0.123, val_acc:0.933]
Epoch [19/120    avg_loss:0.121, val_acc:0.876]
Epoch [20/120    avg_loss:0.131, val_acc:0.944]
Epoch [21/120    avg_loss:0.142, val_acc:0.949]
Epoch [22/120    avg_loss:0.102, val_acc:0.954]
Epoch [23/120    avg_loss:0.116, val_acc:0.938]
Epoch [24/120    avg_loss:0.112, val_acc:0.964]
Epoch [25/120    avg_loss:0.070, val_acc:0.976]
Epoch [26/120    avg_loss:0.047, val_acc:0.958]
Epoch [27/120    avg_loss:0.063, val_acc:0.964]
Epoch [28/120    avg_loss:0.043, val_acc:0.972]
Epoch [29/120    avg_loss:0.086, val_acc:0.949]
Epoch [30/120    avg_loss:0.044, val_acc:0.966]
Epoch [31/120    avg_loss:0.066, val_acc:0.974]
Epoch [32/120    avg_loss:0.057, val_acc:0.962]
Epoch [33/120    avg_loss:0.054, val_acc:0.973]
Epoch [34/120    avg_loss:0.033, val_acc:0.971]
Epoch [35/120    avg_loss:0.051, val_acc:0.974]
Epoch [36/120    avg_loss:0.034, val_acc:0.971]
Epoch [37/120    avg_loss:0.057, val_acc:0.973]
Epoch [38/120    avg_loss:0.054, val_acc:0.963]
Epoch [39/120    avg_loss:0.041, val_acc:0.978]
Epoch [40/120    avg_loss:0.024, val_acc:0.978]
Epoch [41/120    avg_loss:0.019, val_acc:0.983]
Epoch [42/120    avg_loss:0.029, val_acc:0.983]
Epoch [43/120    avg_loss:0.019, val_acc:0.984]
Epoch [44/120    avg_loss:0.036, val_acc:0.983]
Epoch [45/120    avg_loss:0.025, val_acc:0.983]
Epoch [46/120    avg_loss:0.018, val_acc:0.983]
Epoch [47/120    avg_loss:0.019, val_acc:0.983]
Epoch [48/120    avg_loss:0.021, val_acc:0.983]
Epoch [49/120    avg_loss:0.026, val_acc:0.983]
Epoch [50/120    avg_loss:0.018, val_acc:0.983]
Epoch [51/120    avg_loss:0.025, val_acc:0.983]
Epoch [52/120    avg_loss:0.024, val_acc:0.984]
Epoch [53/120    avg_loss:0.017, val_acc:0.986]
Epoch [54/120    avg_loss:0.023, val_acc:0.984]
Epoch [55/120    avg_loss:0.019, val_acc:0.984]
Epoch [56/120    avg_loss:0.020, val_acc:0.986]
Epoch [57/120    avg_loss:0.022, val_acc:0.985]
Epoch [58/120    avg_loss:0.023, val_acc:0.981]
Epoch [59/120    avg_loss:0.018, val_acc:0.984]
Epoch [60/120    avg_loss:0.020, val_acc:0.985]
Epoch [61/120    avg_loss:0.017, val_acc:0.982]
Epoch [62/120    avg_loss:0.017, val_acc:0.983]
Epoch [63/120    avg_loss:0.015, val_acc:0.984]
Epoch [64/120    avg_loss:0.009, val_acc:0.982]
Epoch [65/120    avg_loss:0.015, val_acc:0.985]
Epoch [66/120    avg_loss:0.020, val_acc:0.983]
Epoch [67/120    avg_loss:0.016, val_acc:0.984]
Epoch [68/120    avg_loss:0.022, val_acc:0.983]
Epoch [69/120    avg_loss:0.015, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.022, val_acc:0.985]
Epoch [73/120    avg_loss:0.015, val_acc:0.985]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.015, val_acc:0.985]
Epoch [77/120    avg_loss:0.019, val_acc:0.985]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.015, val_acc:0.984]
Epoch [82/120    avg_loss:0.015, val_acc:0.984]
Epoch [83/120    avg_loss:0.016, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.984]
Epoch [85/120    avg_loss:0.024, val_acc:0.984]
Epoch [86/120    avg_loss:0.019, val_acc:0.984]
Epoch [87/120    avg_loss:0.014, val_acc:0.984]
Epoch [88/120    avg_loss:0.015, val_acc:0.984]
Epoch [89/120    avg_loss:0.015, val_acc:0.984]
Epoch [90/120    avg_loss:0.015, val_acc:0.984]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.028, val_acc:0.983]
Epoch [93/120    avg_loss:0.018, val_acc:0.983]
Epoch [94/120    avg_loss:0.023, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.012, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.983]
Epoch [99/120    avg_loss:0.015, val_acc:0.983]
Epoch [100/120    avg_loss:0.014, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.013, val_acc:0.983]
Epoch [103/120    avg_loss:0.018, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.983]
Epoch [107/120    avg_loss:0.021, val_acc:0.983]
Epoch [108/120    avg_loss:0.017, val_acc:0.983]
Epoch [109/120    avg_loss:0.017, val_acc:0.983]
Epoch [110/120    avg_loss:0.019, val_acc:0.983]
Epoch [111/120    avg_loss:0.014, val_acc:0.983]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.014, val_acc:0.983]
Epoch [114/120    avg_loss:0.022, val_acc:0.983]
Epoch [115/120    avg_loss:0.023, val_acc:0.983]
Epoch [116/120    avg_loss:0.019, val_acc:0.983]
Epoch [117/120    avg_loss:0.021, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.014, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     4     0     0     0     0     8     6]
 [    0     0 18001     0    31     0    58     0     0     0]
 [    0     8     0  1974     0     0     0     0    42    12]
 [    0    29     0     0  2934     0     4     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4868     0    10     0]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0    18     0     7    40     0     0     0  3504     2]
 [    0     0     0     0    11    25     0     0     0   883]]

Accuracy:
99.22637553322248

F1 scores:
[       nan 0.99434152 0.99753401 0.98184531 0.97995992 0.99051233
 0.99255786 0.99961225 0.98151261 0.96926454]

Kappa:
0.9897574990740955
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efdb57266d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.384, val_acc:0.521]
Epoch [2/120    avg_loss:0.739, val_acc:0.733]
Epoch [3/120    avg_loss:0.608, val_acc:0.721]
Epoch [4/120    avg_loss:0.530, val_acc:0.853]
Epoch [5/120    avg_loss:0.390, val_acc:0.754]
Epoch [6/120    avg_loss:0.399, val_acc:0.821]
Epoch [7/120    avg_loss:0.279, val_acc:0.853]
Epoch [8/120    avg_loss:0.257, val_acc:0.899]
Epoch [9/120    avg_loss:0.301, val_acc:0.833]
Epoch [10/120    avg_loss:0.242, val_acc:0.912]
Epoch [11/120    avg_loss:0.220, val_acc:0.882]
Epoch [12/120    avg_loss:0.178, val_acc:0.922]
Epoch [13/120    avg_loss:0.190, val_acc:0.913]
Epoch [14/120    avg_loss:0.187, val_acc:0.870]
Epoch [15/120    avg_loss:0.171, val_acc:0.921]
Epoch [16/120    avg_loss:0.164, val_acc:0.914]
Epoch [17/120    avg_loss:0.179, val_acc:0.926]
Epoch [18/120    avg_loss:0.111, val_acc:0.953]
Epoch [19/120    avg_loss:0.098, val_acc:0.863]
Epoch [20/120    avg_loss:0.139, val_acc:0.917]
Epoch [21/120    avg_loss:0.119, val_acc:0.934]
Epoch [22/120    avg_loss:0.084, val_acc:0.940]
Epoch [23/120    avg_loss:0.133, val_acc:0.927]
Epoch [24/120    avg_loss:0.089, val_acc:0.944]
Epoch [25/120    avg_loss:0.097, val_acc:0.944]
Epoch [26/120    avg_loss:0.115, val_acc:0.926]
Epoch [27/120    avg_loss:0.091, val_acc:0.926]
Epoch [28/120    avg_loss:0.083, val_acc:0.924]
Epoch [29/120    avg_loss:0.082, val_acc:0.934]
Epoch [30/120    avg_loss:0.079, val_acc:0.948]
Epoch [31/120    avg_loss:0.052, val_acc:0.948]
Epoch [32/120    avg_loss:0.050, val_acc:0.970]
Epoch [33/120    avg_loss:0.042, val_acc:0.974]
Epoch [34/120    avg_loss:0.034, val_acc:0.972]
Epoch [35/120    avg_loss:0.045, val_acc:0.967]
Epoch [36/120    avg_loss:0.039, val_acc:0.969]
Epoch [37/120    avg_loss:0.022, val_acc:0.972]
Epoch [38/120    avg_loss:0.035, val_acc:0.971]
Epoch [39/120    avg_loss:0.030, val_acc:0.970]
Epoch [40/120    avg_loss:0.026, val_acc:0.973]
Epoch [41/120    avg_loss:0.026, val_acc:0.971]
Epoch [42/120    avg_loss:0.032, val_acc:0.972]
Epoch [43/120    avg_loss:0.019, val_acc:0.972]
Epoch [44/120    avg_loss:0.029, val_acc:0.974]
Epoch [45/120    avg_loss:0.029, val_acc:0.969]
Epoch [46/120    avg_loss:0.025, val_acc:0.973]
Epoch [47/120    avg_loss:0.021, val_acc:0.973]
Epoch [48/120    avg_loss:0.024, val_acc:0.973]
Epoch [49/120    avg_loss:0.023, val_acc:0.974]
Epoch [50/120    avg_loss:0.034, val_acc:0.976]
Epoch [51/120    avg_loss:0.028, val_acc:0.974]
Epoch [52/120    avg_loss:0.032, val_acc:0.970]
Epoch [53/120    avg_loss:0.030, val_acc:0.974]
Epoch [54/120    avg_loss:0.027, val_acc:0.975]
Epoch [55/120    avg_loss:0.025, val_acc:0.969]
Epoch [56/120    avg_loss:0.041, val_acc:0.974]
Epoch [57/120    avg_loss:0.025, val_acc:0.972]
Epoch [58/120    avg_loss:0.027, val_acc:0.973]
Epoch [59/120    avg_loss:0.028, val_acc:0.978]
Epoch [60/120    avg_loss:0.024, val_acc:0.976]
Epoch [61/120    avg_loss:0.019, val_acc:0.971]
Epoch [62/120    avg_loss:0.031, val_acc:0.974]
Epoch [63/120    avg_loss:0.026, val_acc:0.977]
Epoch [64/120    avg_loss:0.033, val_acc:0.976]
Epoch [65/120    avg_loss:0.018, val_acc:0.978]
Epoch [66/120    avg_loss:0.025, val_acc:0.973]
Epoch [67/120    avg_loss:0.024, val_acc:0.972]
Epoch [68/120    avg_loss:0.018, val_acc:0.976]
Epoch [69/120    avg_loss:0.020, val_acc:0.975]
Epoch [70/120    avg_loss:0.026, val_acc:0.974]
Epoch [71/120    avg_loss:0.012, val_acc:0.977]
Epoch [72/120    avg_loss:0.017, val_acc:0.973]
Epoch [73/120    avg_loss:0.018, val_acc:0.975]
Epoch [74/120    avg_loss:0.028, val_acc:0.980]
Epoch [75/120    avg_loss:0.019, val_acc:0.979]
Epoch [76/120    avg_loss:0.020, val_acc:0.975]
Epoch [77/120    avg_loss:0.025, val_acc:0.976]
Epoch [78/120    avg_loss:0.016, val_acc:0.975]
Epoch [79/120    avg_loss:0.020, val_acc:0.977]
Epoch [80/120    avg_loss:0.019, val_acc:0.978]
Epoch [81/120    avg_loss:0.018, val_acc:0.971]
Epoch [82/120    avg_loss:0.034, val_acc:0.978]
Epoch [83/120    avg_loss:0.018, val_acc:0.979]
Epoch [84/120    avg_loss:0.021, val_acc:0.978]
Epoch [85/120    avg_loss:0.017, val_acc:0.981]
Epoch [86/120    avg_loss:0.032, val_acc:0.978]
Epoch [87/120    avg_loss:0.017, val_acc:0.978]
Epoch [88/120    avg_loss:0.018, val_acc:0.975]
Epoch [89/120    avg_loss:0.023, val_acc:0.979]
Epoch [90/120    avg_loss:0.015, val_acc:0.980]
Epoch [91/120    avg_loss:0.020, val_acc:0.978]
Epoch [92/120    avg_loss:0.020, val_acc:0.976]
Epoch [93/120    avg_loss:0.018, val_acc:0.978]
Epoch [94/120    avg_loss:0.027, val_acc:0.982]
Epoch [95/120    avg_loss:0.020, val_acc:0.980]
Epoch [96/120    avg_loss:0.023, val_acc:0.980]
Epoch [97/120    avg_loss:0.019, val_acc:0.978]
Epoch [98/120    avg_loss:0.016, val_acc:0.977]
Epoch [99/120    avg_loss:0.019, val_acc:0.978]
Epoch [100/120    avg_loss:0.014, val_acc:0.980]
Epoch [101/120    avg_loss:0.020, val_acc:0.977]
Epoch [102/120    avg_loss:0.015, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.019, val_acc:0.980]
Epoch [105/120    avg_loss:0.018, val_acc:0.981]
Epoch [106/120    avg_loss:0.018, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.020, val_acc:0.983]
Epoch [109/120    avg_loss:0.015, val_acc:0.976]
Epoch [110/120    avg_loss:0.014, val_acc:0.978]
Epoch [111/120    avg_loss:0.013, val_acc:0.983]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.025, val_acc:0.973]
Epoch [114/120    avg_loss:0.018, val_acc:0.976]
Epoch [115/120    avg_loss:0.016, val_acc:0.983]
Epoch [116/120    avg_loss:0.022, val_acc:0.974]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.017, val_acc:0.980]
Epoch [119/120    avg_loss:0.013, val_acc:0.979]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0     0     0    15     1     4     2]
 [    0     2 16955     0    48     0  1082     0     3     0]
 [    0     0     0  1982     0     0     0     0    51     3]
 [    0    16     0     0  2941     0     0     0    12     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4871     0     7     0]
 [    0     2     0     0     0     0     3  1283     0     2]
 [    0    15     0    10    35     0     0     0  3510     1]
 [    0     0     0     0    11    20     0     0     0   888]]

Accuracy:
96.75125924854795

F1 scores:
[       nan 0.9955735  0.96761307 0.98411122 0.97919094 0.99239544
 0.89796295 0.996892   0.98072087 0.97689769]

Kappa:
0.9574422915661109
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca655a3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.393, val_acc:0.580]
Epoch [2/120    avg_loss:0.742, val_acc:0.608]
Epoch [3/120    avg_loss:0.617, val_acc:0.764]
Epoch [4/120    avg_loss:0.448, val_acc:0.822]
Epoch [5/120    avg_loss:0.390, val_acc:0.761]
Epoch [6/120    avg_loss:0.417, val_acc:0.694]
Epoch [7/120    avg_loss:0.290, val_acc:0.798]
Epoch [8/120    avg_loss:0.317, val_acc:0.892]
Epoch [9/120    avg_loss:0.221, val_acc:0.898]
Epoch [10/120    avg_loss:0.184, val_acc:0.906]
Epoch [11/120    avg_loss:0.208, val_acc:0.865]
Epoch [12/120    avg_loss:0.169, val_acc:0.897]
Epoch [13/120    avg_loss:0.191, val_acc:0.921]
Epoch [14/120    avg_loss:0.185, val_acc:0.918]
Epoch [15/120    avg_loss:0.135, val_acc:0.920]
Epoch [16/120    avg_loss:0.118, val_acc:0.934]
Epoch [17/120    avg_loss:0.217, val_acc:0.788]
Epoch [18/120    avg_loss:0.139, val_acc:0.921]
Epoch [19/120    avg_loss:0.088, val_acc:0.906]
Epoch [20/120    avg_loss:0.095, val_acc:0.875]
Epoch [21/120    avg_loss:0.085, val_acc:0.910]
Epoch [22/120    avg_loss:0.084, val_acc:0.960]
Epoch [23/120    avg_loss:0.101, val_acc:0.943]
Epoch [24/120    avg_loss:0.075, val_acc:0.937]
Epoch [25/120    avg_loss:0.156, val_acc:0.882]
Epoch [26/120    avg_loss:0.090, val_acc:0.941]
Epoch [27/120    avg_loss:0.073, val_acc:0.964]
Epoch [28/120    avg_loss:0.069, val_acc:0.884]
Epoch [29/120    avg_loss:0.099, val_acc:0.905]
Epoch [30/120    avg_loss:0.076, val_acc:0.904]
Epoch [31/120    avg_loss:0.059, val_acc:0.911]
Epoch [32/120    avg_loss:0.115, val_acc:0.958]
Epoch [33/120    avg_loss:0.058, val_acc:0.972]
Epoch [34/120    avg_loss:0.033, val_acc:0.962]
Epoch [35/120    avg_loss:0.045, val_acc:0.973]
Epoch [36/120    avg_loss:0.050, val_acc:0.950]
Epoch [37/120    avg_loss:0.028, val_acc:0.977]
Epoch [38/120    avg_loss:0.056, val_acc:0.967]
Epoch [39/120    avg_loss:0.040, val_acc:0.972]
Epoch [40/120    avg_loss:0.051, val_acc:0.958]
Epoch [41/120    avg_loss:0.037, val_acc:0.970]
Epoch [42/120    avg_loss:0.067, val_acc:0.969]
Epoch [43/120    avg_loss:0.042, val_acc:0.962]
Epoch [44/120    avg_loss:0.042, val_acc:0.969]
Epoch [45/120    avg_loss:0.035, val_acc:0.934]
Epoch [46/120    avg_loss:0.057, val_acc:0.969]
Epoch [47/120    avg_loss:0.040, val_acc:0.953]
Epoch [48/120    avg_loss:0.048, val_acc:0.945]
Epoch [49/120    avg_loss:0.096, val_acc:0.967]
Epoch [50/120    avg_loss:0.039, val_acc:0.972]
Epoch [51/120    avg_loss:0.026, val_acc:0.976]
Epoch [52/120    avg_loss:0.024, val_acc:0.974]
Epoch [53/120    avg_loss:0.022, val_acc:0.976]
Epoch [54/120    avg_loss:0.018, val_acc:0.978]
Epoch [55/120    avg_loss:0.017, val_acc:0.976]
Epoch [56/120    avg_loss:0.020, val_acc:0.977]
Epoch [57/120    avg_loss:0.023, val_acc:0.978]
Epoch [58/120    avg_loss:0.013, val_acc:0.979]
Epoch [59/120    avg_loss:0.020, val_acc:0.978]
Epoch [60/120    avg_loss:0.023, val_acc:0.977]
Epoch [61/120    avg_loss:0.017, val_acc:0.977]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.019, val_acc:0.978]
Epoch [64/120    avg_loss:0.015, val_acc:0.981]
Epoch [65/120    avg_loss:0.017, val_acc:0.980]
Epoch [66/120    avg_loss:0.014, val_acc:0.977]
Epoch [67/120    avg_loss:0.019, val_acc:0.980]
Epoch [68/120    avg_loss:0.031, val_acc:0.981]
Epoch [69/120    avg_loss:0.024, val_acc:0.978]
Epoch [70/120    avg_loss:0.020, val_acc:0.981]
Epoch [71/120    avg_loss:0.019, val_acc:0.976]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.015, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.011, val_acc:0.980]
Epoch [76/120    avg_loss:0.011, val_acc:0.982]
Epoch [77/120    avg_loss:0.011, val_acc:0.979]
Epoch [78/120    avg_loss:0.015, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.016, val_acc:0.982]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.020, val_acc:0.983]
Epoch [83/120    avg_loss:0.020, val_acc:0.974]
Epoch [84/120    avg_loss:0.020, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.978]
Epoch [86/120    avg_loss:0.019, val_acc:0.982]
Epoch [87/120    avg_loss:0.018, val_acc:0.980]
Epoch [88/120    avg_loss:0.010, val_acc:0.979]
Epoch [89/120    avg_loss:0.016, val_acc:0.979]
Epoch [90/120    avg_loss:0.013, val_acc:0.978]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.017, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.016, val_acc:0.984]
Epoch [98/120    avg_loss:0.025, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.984]
Epoch [101/120    avg_loss:0.017, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.014, val_acc:0.984]
Epoch [105/120    avg_loss:0.013, val_acc:0.984]
Epoch [106/120    avg_loss:0.017, val_acc:0.984]
Epoch [107/120    avg_loss:0.018, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.015, val_acc:0.983]
Epoch [110/120    avg_loss:0.015, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.020, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.016, val_acc:0.983]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.018, val_acc:0.983]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.015, val_acc:0.983]
Epoch [120/120    avg_loss:0.015, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     3     0     0     0     1     7     4]
 [    0     0 17827     0    29     0   229     0     3     2]
 [    0     0     0  2016     0     0     0     0    17     3]
 [    0    25     0     3  2922     0     0     0    10    12]
 [    0     0     0    20     0  1278     0     0     1     6]
 [    0     0     0     0     0     0  4861     0    17     0]
 [    0     0     0     0     0     0     0  1284     0     6]
 [    0     3     0    24    30     0     0     0  3514     0]
 [    0     0     0     2     0     4     0     0     0   913]]

Accuracy:
98.88896922372449

F1 scores:
[       nan 0.99666071 0.99267756 0.98245614 0.9816899  0.98801701
 0.97532103 0.99728155 0.98431373 0.97908847]

Kappa:
0.9853184741096345
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd284162828>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.227, val_acc:0.598]
Epoch [2/120    avg_loss:0.696, val_acc:0.623]
Epoch [3/120    avg_loss:0.527, val_acc:0.728]
Epoch [4/120    avg_loss:0.543, val_acc:0.795]
Epoch [5/120    avg_loss:0.362, val_acc:0.844]
Epoch [6/120    avg_loss:0.307, val_acc:0.849]
Epoch [7/120    avg_loss:0.344, val_acc:0.870]
Epoch [8/120    avg_loss:0.328, val_acc:0.905]
Epoch [9/120    avg_loss:0.211, val_acc:0.935]
Epoch [10/120    avg_loss:0.232, val_acc:0.916]
Epoch [11/120    avg_loss:0.214, val_acc:0.831]
Epoch [12/120    avg_loss:0.197, val_acc:0.936]
Epoch [13/120    avg_loss:0.207, val_acc:0.936]
Epoch [14/120    avg_loss:0.188, val_acc:0.868]
Epoch [15/120    avg_loss:0.168, val_acc:0.877]
Epoch [16/120    avg_loss:0.144, val_acc:0.925]
Epoch [17/120    avg_loss:0.168, val_acc:0.930]
Epoch [18/120    avg_loss:0.217, val_acc:0.935]
Epoch [19/120    avg_loss:0.119, val_acc:0.954]
Epoch [20/120    avg_loss:0.082, val_acc:0.962]
Epoch [21/120    avg_loss:0.117, val_acc:0.929]
Epoch [22/120    avg_loss:0.103, val_acc:0.940]
Epoch [23/120    avg_loss:0.126, val_acc:0.955]
Epoch [24/120    avg_loss:0.143, val_acc:0.921]
Epoch [25/120    avg_loss:0.089, val_acc:0.919]
Epoch [26/120    avg_loss:0.076, val_acc:0.940]
Epoch [27/120    avg_loss:0.103, val_acc:0.975]
Epoch [28/120    avg_loss:0.079, val_acc:0.963]
Epoch [29/120    avg_loss:0.082, val_acc:0.895]
Epoch [30/120    avg_loss:0.083, val_acc:0.968]
Epoch [31/120    avg_loss:0.100, val_acc:0.959]
Epoch [32/120    avg_loss:0.089, val_acc:0.933]
Epoch [33/120    avg_loss:0.092, val_acc:0.951]
Epoch [34/120    avg_loss:0.081, val_acc:0.967]
Epoch [35/120    avg_loss:0.066, val_acc:0.968]
Epoch [36/120    avg_loss:0.056, val_acc:0.964]
Epoch [37/120    avg_loss:0.052, val_acc:0.965]
Epoch [38/120    avg_loss:0.091, val_acc:0.964]
Epoch [39/120    avg_loss:0.074, val_acc:0.974]
Epoch [40/120    avg_loss:0.061, val_acc:0.960]
Epoch [41/120    avg_loss:0.046, val_acc:0.974]
Epoch [42/120    avg_loss:0.037, val_acc:0.977]
Epoch [43/120    avg_loss:0.027, val_acc:0.977]
Epoch [44/120    avg_loss:0.030, val_acc:0.977]
Epoch [45/120    avg_loss:0.025, val_acc:0.980]
Epoch [46/120    avg_loss:0.026, val_acc:0.978]
Epoch [47/120    avg_loss:0.032, val_acc:0.981]
Epoch [48/120    avg_loss:0.025, val_acc:0.982]
Epoch [49/120    avg_loss:0.019, val_acc:0.981]
Epoch [50/120    avg_loss:0.026, val_acc:0.981]
Epoch [51/120    avg_loss:0.034, val_acc:0.977]
Epoch [52/120    avg_loss:0.026, val_acc:0.978]
Epoch [53/120    avg_loss:0.016, val_acc:0.980]
Epoch [54/120    avg_loss:0.018, val_acc:0.982]
Epoch [55/120    avg_loss:0.023, val_acc:0.979]
Epoch [56/120    avg_loss:0.020, val_acc:0.981]
Epoch [57/120    avg_loss:0.030, val_acc:0.980]
Epoch [58/120    avg_loss:0.021, val_acc:0.978]
Epoch [59/120    avg_loss:0.017, val_acc:0.980]
Epoch [60/120    avg_loss:0.017, val_acc:0.980]
Epoch [61/120    avg_loss:0.022, val_acc:0.981]
Epoch [62/120    avg_loss:0.023, val_acc:0.978]
Epoch [63/120    avg_loss:0.022, val_acc:0.979]
Epoch [64/120    avg_loss:0.025, val_acc:0.980]
Epoch [65/120    avg_loss:0.024, val_acc:0.982]
Epoch [66/120    avg_loss:0.020, val_acc:0.983]
Epoch [67/120    avg_loss:0.017, val_acc:0.983]
Epoch [68/120    avg_loss:0.024, val_acc:0.983]
Epoch [69/120    avg_loss:0.028, val_acc:0.978]
Epoch [70/120    avg_loss:0.019, val_acc:0.979]
Epoch [71/120    avg_loss:0.015, val_acc:0.983]
Epoch [72/120    avg_loss:0.032, val_acc:0.984]
Epoch [73/120    avg_loss:0.025, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.983]
Epoch [75/120    avg_loss:0.024, val_acc:0.983]
Epoch [76/120    avg_loss:0.019, val_acc:0.984]
Epoch [77/120    avg_loss:0.016, val_acc:0.983]
Epoch [78/120    avg_loss:0.016, val_acc:0.985]
Epoch [79/120    avg_loss:0.021, val_acc:0.984]
Epoch [80/120    avg_loss:0.023, val_acc:0.984]
Epoch [81/120    avg_loss:0.026, val_acc:0.984]
Epoch [82/120    avg_loss:0.014, val_acc:0.983]
Epoch [83/120    avg_loss:0.029, val_acc:0.984]
Epoch [84/120    avg_loss:0.035, val_acc:0.983]
Epoch [85/120    avg_loss:0.021, val_acc:0.981]
Epoch [86/120    avg_loss:0.023, val_acc:0.982]
Epoch [87/120    avg_loss:0.023, val_acc:0.986]
Epoch [88/120    avg_loss:0.014, val_acc:0.986]
Epoch [89/120    avg_loss:0.021, val_acc:0.985]
Epoch [90/120    avg_loss:0.034, val_acc:0.985]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.014, val_acc:0.988]
Epoch [93/120    avg_loss:0.016, val_acc:0.986]
Epoch [94/120    avg_loss:0.015, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.014, val_acc:0.984]
Epoch [97/120    avg_loss:0.018, val_acc:0.983]
Epoch [98/120    avg_loss:0.017, val_acc:0.984]
Epoch [99/120    avg_loss:0.017, val_acc:0.985]
Epoch [100/120    avg_loss:0.014, val_acc:0.985]
Epoch [101/120    avg_loss:0.016, val_acc:0.986]
Epoch [102/120    avg_loss:0.015, val_acc:0.986]
Epoch [103/120    avg_loss:0.018, val_acc:0.983]
Epoch [104/120    avg_loss:0.016, val_acc:0.983]
Epoch [105/120    avg_loss:0.016, val_acc:0.981]
Epoch [106/120    avg_loss:0.020, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.984]
Epoch [108/120    avg_loss:0.029, val_acc:0.984]
Epoch [109/120    avg_loss:0.013, val_acc:0.983]
Epoch [110/120    avg_loss:0.025, val_acc:0.983]
Epoch [111/120    avg_loss:0.023, val_acc:0.984]
Epoch [112/120    avg_loss:0.014, val_acc:0.984]
Epoch [113/120    avg_loss:0.035, val_acc:0.984]
Epoch [114/120    avg_loss:0.019, val_acc:0.985]
Epoch [115/120    avg_loss:0.013, val_acc:0.985]
Epoch [116/120    avg_loss:0.015, val_acc:0.985]
Epoch [117/120    avg_loss:0.020, val_acc:0.986]
Epoch [118/120    avg_loss:0.018, val_acc:0.986]
Epoch [119/120    avg_loss:0.018, val_acc:0.986]
Epoch [120/120    avg_loss:0.018, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     0     0     0    11     1    20    15]
 [    0     0 17997     0    33     0    55     0     4     1]
 [    0     0     0  1978     1     0     0     0    50     7]
 [    0    17     0     0  2931     0     1     0    18     5]
 [    0     0     0   205     0  1087     0     0    13     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     2     0     0     0     0     0  1285     0     3]
 [    0     3     0    19    26     0     0     0  3523     0]
 [    0     0     0     0    14     8     0     0     0   897]]

Accuracy:
98.71785602390764

F1 scores:
[       nan 0.99462575 0.99742289 0.93345918 0.98075958 0.90583333
 0.99317927 0.99767081 0.97874705 0.97130482]

Kappa:
0.983025811152962
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff344372780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.192, val_acc:0.677]
Epoch [2/120    avg_loss:0.806, val_acc:0.678]
Epoch [3/120    avg_loss:0.548, val_acc:0.670]
Epoch [4/120    avg_loss:0.407, val_acc:0.765]
Epoch [5/120    avg_loss:0.449, val_acc:0.823]
Epoch [6/120    avg_loss:0.407, val_acc:0.723]
Epoch [7/120    avg_loss:0.410, val_acc:0.729]
Epoch [8/120    avg_loss:0.308, val_acc:0.892]
Epoch [9/120    avg_loss:0.230, val_acc:0.860]
Epoch [10/120    avg_loss:0.286, val_acc:0.878]
Epoch [11/120    avg_loss:0.237, val_acc:0.874]
Epoch [12/120    avg_loss:0.181, val_acc:0.909]
Epoch [13/120    avg_loss:0.171, val_acc:0.926]
Epoch [14/120    avg_loss:0.118, val_acc:0.934]
Epoch [15/120    avg_loss:0.124, val_acc:0.929]
Epoch [16/120    avg_loss:0.088, val_acc:0.938]
Epoch [17/120    avg_loss:0.083, val_acc:0.945]
Epoch [18/120    avg_loss:0.069, val_acc:0.913]
Epoch [19/120    avg_loss:0.191, val_acc:0.913]
Epoch [20/120    avg_loss:0.220, val_acc:0.911]
Epoch [21/120    avg_loss:0.133, val_acc:0.926]
Epoch [22/120    avg_loss:0.097, val_acc:0.938]
Epoch [23/120    avg_loss:0.082, val_acc:0.941]
Epoch [24/120    avg_loss:0.096, val_acc:0.912]
Epoch [25/120    avg_loss:0.161, val_acc:0.951]
Epoch [26/120    avg_loss:0.067, val_acc:0.947]
Epoch [27/120    avg_loss:0.073, val_acc:0.963]
Epoch [28/120    avg_loss:0.053, val_acc:0.929]
Epoch [29/120    avg_loss:0.042, val_acc:0.953]
Epoch [30/120    avg_loss:0.094, val_acc:0.948]
Epoch [31/120    avg_loss:0.050, val_acc:0.963]
Epoch [32/120    avg_loss:0.083, val_acc:0.938]
Epoch [33/120    avg_loss:0.045, val_acc:0.967]
Epoch [34/120    avg_loss:0.056, val_acc:0.948]
Epoch [35/120    avg_loss:0.053, val_acc:0.938]
Epoch [36/120    avg_loss:0.026, val_acc:0.946]
Epoch [37/120    avg_loss:0.105, val_acc:0.955]
Epoch [38/120    avg_loss:0.050, val_acc:0.964]
Epoch [39/120    avg_loss:0.035, val_acc:0.967]
Epoch [40/120    avg_loss:0.025, val_acc:0.970]
Epoch [41/120    avg_loss:0.032, val_acc:0.969]
Epoch [42/120    avg_loss:0.020, val_acc:0.965]
Epoch [43/120    avg_loss:0.020, val_acc:0.970]
Epoch [44/120    avg_loss:0.019, val_acc:0.966]
Epoch [45/120    avg_loss:0.037, val_acc:0.971]
Epoch [46/120    avg_loss:0.032, val_acc:0.961]
Epoch [47/120    avg_loss:0.042, val_acc:0.970]
Epoch [48/120    avg_loss:0.022, val_acc:0.963]
Epoch [49/120    avg_loss:0.021, val_acc:0.973]
Epoch [50/120    avg_loss:0.023, val_acc:0.973]
Epoch [51/120    avg_loss:0.016, val_acc:0.972]
Epoch [52/120    avg_loss:0.033, val_acc:0.969]
Epoch [53/120    avg_loss:0.021, val_acc:0.961]
Epoch [54/120    avg_loss:0.033, val_acc:0.969]
Epoch [55/120    avg_loss:0.019, val_acc:0.969]
Epoch [56/120    avg_loss:0.030, val_acc:0.963]
Epoch [57/120    avg_loss:0.029, val_acc:0.958]
Epoch [58/120    avg_loss:0.028, val_acc:0.968]
Epoch [59/120    avg_loss:0.035, val_acc:0.978]
Epoch [60/120    avg_loss:0.020, val_acc:0.973]
Epoch [61/120    avg_loss:0.008, val_acc:0.979]
Epoch [62/120    avg_loss:0.013, val_acc:0.980]
Epoch [63/120    avg_loss:0.010, val_acc:0.973]
Epoch [64/120    avg_loss:0.018, val_acc:0.979]
Epoch [65/120    avg_loss:0.010, val_acc:0.988]
Epoch [66/120    avg_loss:0.016, val_acc:0.982]
Epoch [67/120    avg_loss:0.024, val_acc:0.973]
Epoch [68/120    avg_loss:0.020, val_acc:0.964]
Epoch [69/120    avg_loss:0.022, val_acc:0.971]
Epoch [70/120    avg_loss:0.009, val_acc:0.972]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.008, val_acc:0.980]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.013, val_acc:0.973]
Epoch [75/120    avg_loss:0.017, val_acc:0.973]
Epoch [76/120    avg_loss:0.003, val_acc:0.982]
Epoch [77/120    avg_loss:0.032, val_acc:0.975]
Epoch [78/120    avg_loss:0.015, val_acc:0.976]
Epoch [79/120    avg_loss:0.006, val_acc:0.978]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.982]
Epoch [82/120    avg_loss:0.008, val_acc:0.982]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.005, val_acc:0.982]
Epoch [85/120    avg_loss:0.006, val_acc:0.981]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.980]
Epoch [89/120    avg_loss:0.007, val_acc:0.982]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.005, val_acc:0.981]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.005, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.981]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.004, val_acc:0.981]
Epoch [99/120    avg_loss:0.016, val_acc:0.981]
Epoch [100/120    avg_loss:0.004, val_acc:0.981]
Epoch [101/120    avg_loss:0.005, val_acc:0.982]
Epoch [102/120    avg_loss:0.005, val_acc:0.982]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.004, val_acc:0.982]
Epoch [107/120    avg_loss:0.006, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.005, val_acc:0.982]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.004, val_acc:0.982]
Epoch [116/120    avg_loss:0.005, val_acc:0.982]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.004, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0     9    17     0    12]
 [    0     0 18053     0    26     0    11     0     0     0]
 [    0     0     0  1962     0     0     0     0    65     9]
 [    0    17     0     0  2949     0     1     0     3     2]
 [    0     0     0     0     0  1133   170     0     0     2]
 [    0     0     2     0     0     0  4872     0     4     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    12     0    16    37     0     0     0  3500     6]
 [    0     0     0     0    14    34     0     0     0   871]]

Accuracy:
98.86486877304606

F1 scores:
[       nan 0.99478802 0.99892101 0.97757848 0.98332778 0.91666667
 0.98018308 0.99267823 0.9799804  0.95556775]

Kappa:
0.984957866562147
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb040746d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.254, val_acc:0.623]
Epoch [2/120    avg_loss:0.744, val_acc:0.671]
Epoch [3/120    avg_loss:0.615, val_acc:0.738]
Epoch [4/120    avg_loss:0.448, val_acc:0.751]
Epoch [5/120    avg_loss:0.402, val_acc:0.805]
Epoch [6/120    avg_loss:0.435, val_acc:0.732]
Epoch [7/120    avg_loss:0.315, val_acc:0.828]
Epoch [8/120    avg_loss:0.291, val_acc:0.787]
Epoch [9/120    avg_loss:0.285, val_acc:0.868]
Epoch [10/120    avg_loss:0.288, val_acc:0.883]
Epoch [11/120    avg_loss:0.347, val_acc:0.888]
Epoch [12/120    avg_loss:0.248, val_acc:0.839]
Epoch [13/120    avg_loss:0.163, val_acc:0.819]
Epoch [14/120    avg_loss:0.189, val_acc:0.889]
Epoch [15/120    avg_loss:0.202, val_acc:0.748]
Epoch [16/120    avg_loss:0.217, val_acc:0.866]
Epoch [17/120    avg_loss:0.176, val_acc:0.874]
Epoch [18/120    avg_loss:0.105, val_acc:0.928]
Epoch [19/120    avg_loss:0.159, val_acc:0.847]
Epoch [20/120    avg_loss:0.155, val_acc:0.928]
Epoch [21/120    avg_loss:0.118, val_acc:0.948]
Epoch [22/120    avg_loss:0.106, val_acc:0.934]
Epoch [23/120    avg_loss:0.087, val_acc:0.941]
Epoch [24/120    avg_loss:0.081, val_acc:0.901]
Epoch [25/120    avg_loss:0.131, val_acc:0.833]
Epoch [26/120    avg_loss:0.131, val_acc:0.935]
Epoch [27/120    avg_loss:0.101, val_acc:0.943]
Epoch [28/120    avg_loss:0.070, val_acc:0.933]
Epoch [29/120    avg_loss:0.131, val_acc:0.890]
Epoch [30/120    avg_loss:0.088, val_acc:0.953]
Epoch [31/120    avg_loss:0.067, val_acc:0.963]
Epoch [32/120    avg_loss:0.057, val_acc:0.940]
Epoch [33/120    avg_loss:0.097, val_acc:0.952]
Epoch [34/120    avg_loss:0.049, val_acc:0.927]
Epoch [35/120    avg_loss:0.078, val_acc:0.958]
Epoch [36/120    avg_loss:0.050, val_acc:0.930]
Epoch [37/120    avg_loss:0.073, val_acc:0.929]
Epoch [38/120    avg_loss:0.059, val_acc:0.971]
Epoch [39/120    avg_loss:0.047, val_acc:0.957]
Epoch [40/120    avg_loss:0.052, val_acc:0.953]
Epoch [41/120    avg_loss:0.067, val_acc:0.964]
Epoch [42/120    avg_loss:0.027, val_acc:0.972]
Epoch [43/120    avg_loss:0.036, val_acc:0.972]
Epoch [44/120    avg_loss:0.038, val_acc:0.952]
Epoch [45/120    avg_loss:0.050, val_acc:0.932]
Epoch [46/120    avg_loss:0.047, val_acc:0.922]
Epoch [47/120    avg_loss:0.044, val_acc:0.964]
Epoch [48/120    avg_loss:0.027, val_acc:0.971]
Epoch [49/120    avg_loss:0.026, val_acc:0.969]
Epoch [50/120    avg_loss:0.047, val_acc:0.972]
Epoch [51/120    avg_loss:0.023, val_acc:0.973]
Epoch [52/120    avg_loss:0.040, val_acc:0.963]
Epoch [53/120    avg_loss:0.047, val_acc:0.960]
Epoch [54/120    avg_loss:0.203, val_acc:0.935]
Epoch [55/120    avg_loss:0.084, val_acc:0.951]
Epoch [56/120    avg_loss:0.041, val_acc:0.938]
Epoch [57/120    avg_loss:0.043, val_acc:0.968]
Epoch [58/120    avg_loss:0.076, val_acc:0.957]
Epoch [59/120    avg_loss:0.044, val_acc:0.971]
Epoch [60/120    avg_loss:0.068, val_acc:0.921]
Epoch [61/120    avg_loss:0.060, val_acc:0.963]
Epoch [62/120    avg_loss:0.068, val_acc:0.964]
Epoch [63/120    avg_loss:0.034, val_acc:0.961]
Epoch [64/120    avg_loss:0.032, val_acc:0.976]
Epoch [65/120    avg_loss:0.030, val_acc:0.941]
Epoch [66/120    avg_loss:0.062, val_acc:0.952]
Epoch [67/120    avg_loss:0.043, val_acc:0.978]
Epoch [68/120    avg_loss:0.031, val_acc:0.959]
Epoch [69/120    avg_loss:0.011, val_acc:0.978]
Epoch [70/120    avg_loss:0.023, val_acc:0.975]
Epoch [71/120    avg_loss:0.020, val_acc:0.969]
Epoch [72/120    avg_loss:0.041, val_acc:0.965]
Epoch [73/120    avg_loss:0.038, val_acc:0.973]
Epoch [74/120    avg_loss:0.022, val_acc:0.973]
Epoch [75/120    avg_loss:0.015, val_acc:0.975]
Epoch [76/120    avg_loss:0.029, val_acc:0.912]
Epoch [77/120    avg_loss:0.025, val_acc:0.978]
Epoch [78/120    avg_loss:0.019, val_acc:0.951]
Epoch [79/120    avg_loss:0.030, val_acc:0.972]
Epoch [80/120    avg_loss:0.018, val_acc:0.978]
Epoch [81/120    avg_loss:0.023, val_acc:0.977]
Epoch [82/120    avg_loss:0.017, val_acc:0.983]
Epoch [83/120    avg_loss:0.006, val_acc:0.982]
Epoch [84/120    avg_loss:0.020, val_acc:0.980]
Epoch [85/120    avg_loss:0.011, val_acc:0.979]
Epoch [86/120    avg_loss:0.024, val_acc:0.969]
Epoch [87/120    avg_loss:0.019, val_acc:0.977]
Epoch [88/120    avg_loss:0.022, val_acc:0.973]
Epoch [89/120    avg_loss:0.023, val_acc:0.963]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.011, val_acc:0.980]
Epoch [92/120    avg_loss:0.031, val_acc:0.960]
Epoch [93/120    avg_loss:0.081, val_acc:0.963]
Epoch [94/120    avg_loss:0.040, val_acc:0.975]
Epoch [95/120    avg_loss:0.026, val_acc:0.977]
Epoch [96/120    avg_loss:0.016, val_acc:0.978]
Epoch [97/120    avg_loss:0.013, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.981]
Epoch [99/120    avg_loss:0.011, val_acc:0.982]
Epoch [100/120    avg_loss:0.014, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.005, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.014, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     1     0     0    36     1    19     1]
 [    0     0 18075     0    13     0     2     0     0     0]
 [    0     0     0  2007     0     0     0     0    27     2]
 [    0    27     1     0  2933     0     1     0    10     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     3     0     0     0  4852     0    23     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    18     1    24    17     0     0     0  3511     0]
 [    0     0     0     0    14     7     0     0     0   898]]

Accuracy:
99.39748873303931

F1 scores:
[       nan 0.99198506 0.99944706 0.98672566 0.98604808 0.99655832
 0.9933463  0.99961255 0.9805893  0.98572997]

Kappa:
0.9920171815233473
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4c8af3780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.294, val_acc:0.588]
Epoch [2/120    avg_loss:0.765, val_acc:0.533]
Epoch [3/120    avg_loss:0.545, val_acc:0.726]
Epoch [4/120    avg_loss:0.567, val_acc:0.756]
Epoch [5/120    avg_loss:0.407, val_acc:0.798]
Epoch [6/120    avg_loss:0.378, val_acc:0.826]
Epoch [7/120    avg_loss:0.378, val_acc:0.815]
Epoch [8/120    avg_loss:0.295, val_acc:0.862]
Epoch [9/120    avg_loss:0.241, val_acc:0.907]
Epoch [10/120    avg_loss:0.307, val_acc:0.882]
Epoch [11/120    avg_loss:0.228, val_acc:0.909]
Epoch [12/120    avg_loss:0.206, val_acc:0.866]
Epoch [13/120    avg_loss:0.143, val_acc:0.909]
Epoch [14/120    avg_loss:0.181, val_acc:0.890]
Epoch [15/120    avg_loss:0.289, val_acc:0.918]
Epoch [16/120    avg_loss:0.126, val_acc:0.901]
Epoch [17/120    avg_loss:0.180, val_acc:0.942]
Epoch [18/120    avg_loss:0.122, val_acc:0.932]
Epoch [19/120    avg_loss:0.131, val_acc:0.915]
Epoch [20/120    avg_loss:0.114, val_acc:0.921]
Epoch [21/120    avg_loss:0.072, val_acc:0.928]
Epoch [22/120    avg_loss:0.093, val_acc:0.936]
Epoch [23/120    avg_loss:0.096, val_acc:0.910]
Epoch [24/120    avg_loss:0.086, val_acc:0.928]
Epoch [25/120    avg_loss:0.078, val_acc:0.955]
Epoch [26/120    avg_loss:0.105, val_acc:0.941]
Epoch [27/120    avg_loss:0.117, val_acc:0.940]
Epoch [28/120    avg_loss:0.107, val_acc:0.945]
Epoch [29/120    avg_loss:0.071, val_acc:0.962]
Epoch [30/120    avg_loss:0.066, val_acc:0.957]
Epoch [31/120    avg_loss:0.078, val_acc:0.951]
Epoch [32/120    avg_loss:0.072, val_acc:0.962]
Epoch [33/120    avg_loss:0.052, val_acc:0.958]
Epoch [34/120    avg_loss:0.028, val_acc:0.961]
Epoch [35/120    avg_loss:0.023, val_acc:0.962]
Epoch [36/120    avg_loss:0.058, val_acc:0.947]
Epoch [37/120    avg_loss:0.028, val_acc:0.965]
Epoch [38/120    avg_loss:0.035, val_acc:0.964]
Epoch [39/120    avg_loss:0.040, val_acc:0.966]
Epoch [40/120    avg_loss:0.029, val_acc:0.924]
Epoch [41/120    avg_loss:0.048, val_acc:0.945]
Epoch [42/120    avg_loss:0.052, val_acc:0.965]
Epoch [43/120    avg_loss:0.041, val_acc:0.966]
Epoch [44/120    avg_loss:0.060, val_acc:0.940]
Epoch [45/120    avg_loss:0.030, val_acc:0.966]
Epoch [46/120    avg_loss:0.280, val_acc:0.943]
Epoch [47/120    avg_loss:0.094, val_acc:0.940]
Epoch [48/120    avg_loss:0.067, val_acc:0.955]
Epoch [49/120    avg_loss:0.021, val_acc:0.956]
Epoch [50/120    avg_loss:0.056, val_acc:0.947]
Epoch [51/120    avg_loss:0.123, val_acc:0.928]
Epoch [52/120    avg_loss:0.049, val_acc:0.960]
Epoch [53/120    avg_loss:0.031, val_acc:0.955]
Epoch [54/120    avg_loss:0.045, val_acc:0.959]
Epoch [55/120    avg_loss:0.025, val_acc:0.828]
Epoch [56/120    avg_loss:0.048, val_acc:0.956]
Epoch [57/120    avg_loss:0.029, val_acc:0.970]
Epoch [58/120    avg_loss:0.036, val_acc:0.964]
Epoch [59/120    avg_loss:0.027, val_acc:0.959]
Epoch [60/120    avg_loss:0.019, val_acc:0.968]
Epoch [61/120    avg_loss:0.023, val_acc:0.965]
Epoch [62/120    avg_loss:0.022, val_acc:0.971]
Epoch [63/120    avg_loss:0.041, val_acc:0.974]
Epoch [64/120    avg_loss:0.020, val_acc:0.953]
Epoch [65/120    avg_loss:0.029, val_acc:0.955]
Epoch [66/120    avg_loss:0.031, val_acc:0.932]
Epoch [67/120    avg_loss:0.047, val_acc:0.976]
Epoch [68/120    avg_loss:0.023, val_acc:0.967]
Epoch [69/120    avg_loss:0.010, val_acc:0.970]
Epoch [70/120    avg_loss:0.024, val_acc:0.975]
Epoch [71/120    avg_loss:0.026, val_acc:0.956]
Epoch [72/120    avg_loss:0.013, val_acc:0.972]
Epoch [73/120    avg_loss:0.013, val_acc:0.970]
Epoch [74/120    avg_loss:0.013, val_acc:0.962]
Epoch [75/120    avg_loss:0.015, val_acc:0.971]
Epoch [76/120    avg_loss:0.017, val_acc:0.976]
Epoch [77/120    avg_loss:0.014, val_acc:0.968]
Epoch [78/120    avg_loss:0.013, val_acc:0.976]
Epoch [79/120    avg_loss:0.007, val_acc:0.977]
Epoch [80/120    avg_loss:0.018, val_acc:0.965]
Epoch [81/120    avg_loss:0.014, val_acc:0.973]
Epoch [82/120    avg_loss:0.020, val_acc:0.956]
Epoch [83/120    avg_loss:0.021, val_acc:0.971]
Epoch [84/120    avg_loss:0.013, val_acc:0.975]
Epoch [85/120    avg_loss:0.023, val_acc:0.974]
Epoch [86/120    avg_loss:0.028, val_acc:0.933]
Epoch [87/120    avg_loss:0.015, val_acc:0.973]
Epoch [88/120    avg_loss:0.031, val_acc:0.973]
Epoch [89/120    avg_loss:0.019, val_acc:0.980]
Epoch [90/120    avg_loss:0.013, val_acc:0.978]
Epoch [91/120    avg_loss:0.010, val_acc:0.972]
Epoch [92/120    avg_loss:0.015, val_acc:0.982]
Epoch [93/120    avg_loss:0.027, val_acc:0.910]
Epoch [94/120    avg_loss:0.027, val_acc:0.985]
Epoch [95/120    avg_loss:0.017, val_acc:0.976]
Epoch [96/120    avg_loss:0.008, val_acc:0.977]
Epoch [97/120    avg_loss:0.024, val_acc:0.949]
Epoch [98/120    avg_loss:0.012, val_acc:0.950]
Epoch [99/120    avg_loss:0.147, val_acc:0.966]
Epoch [100/120    avg_loss:0.023, val_acc:0.967]
Epoch [101/120    avg_loss:0.019, val_acc:0.969]
Epoch [102/120    avg_loss:0.032, val_acc:0.953]
Epoch [103/120    avg_loss:0.045, val_acc:0.961]
Epoch [104/120    avg_loss:0.032, val_acc:0.966]
Epoch [105/120    avg_loss:0.014, val_acc:0.976]
Epoch [106/120    avg_loss:0.022, val_acc:0.968]
Epoch [107/120    avg_loss:0.015, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.979]
Epoch [110/120    avg_loss:0.006, val_acc:0.979]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.005, val_acc:0.980]
Epoch [113/120    avg_loss:0.004, val_acc:0.981]
Epoch [114/120    avg_loss:0.003, val_acc:0.980]
Epoch [115/120    avg_loss:0.003, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.008, val_acc:0.980]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.003, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     0     0     0     7     5     2     6]
 [    0     0 18072     0    11     0     7     0     0     0]
 [    0     7     0  1973     0     0     0     0    50     6]
 [    0    21     2     0  2937     0     4     0     6     2]
 [    0     9     0     0     0  1254     0     0     0    42]
 [    0     0     0     0     0     0  4861     0    17     0]
 [    0     4     0     0     0     0     0  1284     0     2]
 [    0    24     0     9    22     0     0     0  3511     5]
 [    0     0     0     0     2     5     0     0     0   912]]

Accuracy:
99.33241751620756

F1 scores:
[       nan 0.99341545 0.99944696 0.98208064 0.98822342 0.97815913
 0.99641283 0.99573478 0.98113735 0.96304118]

Kappa:
0.9911545465476793
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a2dbda7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.203, val_acc:0.713]
Epoch [2/120    avg_loss:0.766, val_acc:0.614]
Epoch [3/120    avg_loss:0.728, val_acc:0.713]
Epoch [4/120    avg_loss:0.497, val_acc:0.802]
Epoch [5/120    avg_loss:0.412, val_acc:0.850]
Epoch [6/120    avg_loss:0.408, val_acc:0.857]
Epoch [7/120    avg_loss:0.311, val_acc:0.877]
Epoch [8/120    avg_loss:0.377, val_acc:0.883]
Epoch [9/120    avg_loss:0.238, val_acc:0.863]
Epoch [10/120    avg_loss:0.215, val_acc:0.854]
Epoch [11/120    avg_loss:0.265, val_acc:0.848]
Epoch [12/120    avg_loss:0.207, val_acc:0.914]
Epoch [13/120    avg_loss:0.197, val_acc:0.918]
Epoch [14/120    avg_loss:0.183, val_acc:0.915]
Epoch [15/120    avg_loss:0.164, val_acc:0.895]
Epoch [16/120    avg_loss:0.149, val_acc:0.904]
Epoch [17/120    avg_loss:0.128, val_acc:0.932]
Epoch [18/120    avg_loss:0.107, val_acc:0.946]
Epoch [19/120    avg_loss:0.071, val_acc:0.951]
Epoch [20/120    avg_loss:0.139, val_acc:0.952]
Epoch [21/120    avg_loss:0.060, val_acc:0.922]
Epoch [22/120    avg_loss:0.064, val_acc:0.945]
Epoch [23/120    avg_loss:0.100, val_acc:0.923]
Epoch [24/120    avg_loss:0.126, val_acc:0.917]
Epoch [25/120    avg_loss:0.092, val_acc:0.928]
Epoch [26/120    avg_loss:0.067, val_acc:0.960]
Epoch [27/120    avg_loss:0.133, val_acc:0.898]
Epoch [28/120    avg_loss:0.100, val_acc:0.936]
Epoch [29/120    avg_loss:0.057, val_acc:0.947]
Epoch [30/120    avg_loss:0.085, val_acc:0.932]
Epoch [31/120    avg_loss:0.073, val_acc:0.958]
Epoch [32/120    avg_loss:0.051, val_acc:0.954]
Epoch [33/120    avg_loss:0.086, val_acc:0.957]
Epoch [34/120    avg_loss:0.055, val_acc:0.958]
Epoch [35/120    avg_loss:0.029, val_acc:0.960]
Epoch [36/120    avg_loss:0.045, val_acc:0.961]
Epoch [37/120    avg_loss:0.040, val_acc:0.965]
Epoch [38/120    avg_loss:0.037, val_acc:0.974]
Epoch [39/120    avg_loss:0.046, val_acc:0.965]
Epoch [40/120    avg_loss:0.046, val_acc:0.968]
Epoch [41/120    avg_loss:0.039, val_acc:0.958]
Epoch [42/120    avg_loss:0.053, val_acc:0.963]
Epoch [43/120    avg_loss:0.043, val_acc:0.873]
Epoch [44/120    avg_loss:0.072, val_acc:0.970]
Epoch [45/120    avg_loss:0.035, val_acc:0.956]
Epoch [46/120    avg_loss:0.035, val_acc:0.973]
Epoch [47/120    avg_loss:0.022, val_acc:0.971]
Epoch [48/120    avg_loss:0.019, val_acc:0.975]
Epoch [49/120    avg_loss:0.032, val_acc:0.973]
Epoch [50/120    avg_loss:0.036, val_acc:0.954]
Epoch [51/120    avg_loss:0.016, val_acc:0.956]
Epoch [52/120    avg_loss:0.031, val_acc:0.966]
Epoch [53/120    avg_loss:0.046, val_acc:0.964]
Epoch [54/120    avg_loss:0.101, val_acc:0.973]
Epoch [55/120    avg_loss:0.088, val_acc:0.904]
Epoch [56/120    avg_loss:0.041, val_acc:0.958]
Epoch [57/120    avg_loss:0.030, val_acc:0.972]
Epoch [58/120    avg_loss:0.018, val_acc:0.973]
Epoch [59/120    avg_loss:0.025, val_acc:0.979]
Epoch [60/120    avg_loss:0.029, val_acc:0.981]
Epoch [61/120    avg_loss:0.021, val_acc:0.977]
Epoch [62/120    avg_loss:0.017, val_acc:0.983]
Epoch [63/120    avg_loss:0.018, val_acc:0.982]
Epoch [64/120    avg_loss:0.015, val_acc:0.976]
Epoch [65/120    avg_loss:0.009, val_acc:0.980]
Epoch [66/120    avg_loss:0.013, val_acc:0.982]
Epoch [67/120    avg_loss:0.016, val_acc:0.952]
Epoch [68/120    avg_loss:0.021, val_acc:0.979]
Epoch [69/120    avg_loss:0.009, val_acc:0.968]
Epoch [70/120    avg_loss:0.011, val_acc:0.975]
Epoch [71/120    avg_loss:0.020, val_acc:0.978]
Epoch [72/120    avg_loss:0.014, val_acc:0.973]
Epoch [73/120    avg_loss:0.049, val_acc:0.976]
Epoch [74/120    avg_loss:0.021, val_acc:0.983]
Epoch [75/120    avg_loss:0.014, val_acc:0.977]
Epoch [76/120    avg_loss:0.033, val_acc:0.975]
Epoch [77/120    avg_loss:0.025, val_acc:0.983]
Epoch [78/120    avg_loss:0.014, val_acc:0.973]
Epoch [79/120    avg_loss:0.004, val_acc:0.980]
Epoch [80/120    avg_loss:0.015, val_acc:0.979]
Epoch [81/120    avg_loss:0.016, val_acc:0.983]
Epoch [82/120    avg_loss:0.007, val_acc:0.959]
Epoch [83/120    avg_loss:0.014, val_acc:0.977]
Epoch [84/120    avg_loss:0.009, val_acc:0.978]
Epoch [85/120    avg_loss:0.015, val_acc:0.979]
Epoch [86/120    avg_loss:0.005, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.970]
Epoch [88/120    avg_loss:0.006, val_acc:0.979]
Epoch [89/120    avg_loss:0.009, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.012, val_acc:0.938]
Epoch [95/120    avg_loss:0.016, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.082, val_acc:0.979]
Epoch [98/120    avg_loss:0.041, val_acc:0.959]
Epoch [99/120    avg_loss:0.030, val_acc:0.976]
Epoch [100/120    avg_loss:0.011, val_acc:0.976]
Epoch [101/120    avg_loss:0.024, val_acc:0.971]
Epoch [102/120    avg_loss:0.030, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.963]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.013, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.982]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.019, val_acc:0.985]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     2     0     0     3     0     7     1]
 [    0     0 18060     0    19     0     5     0     6     0]
 [    0     1     0  1977     0     0     0     0    54     4]
 [    0    27     0     0  2937     0     3     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4865     0     8     0]
 [    0     3     0     0     0     0     0  1286     0     1]
 [    0    60     0     0    30     0     0     0  3480     1]
 [    0     0     0     1    14    30     0     0     0   874]]

Accuracy:
99.3010869303256

F1 scores:
[       nan 0.99196415 0.99903195 0.98456175 0.98359009 0.98863636
 0.99753947 0.9984472  0.97602019 0.97111111]

Kappa:
0.990739202907454
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d11f44748>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.293, val_acc:0.717]
Epoch [2/120    avg_loss:0.727, val_acc:0.626]
Epoch [3/120    avg_loss:0.638, val_acc:0.715]
Epoch [4/120    avg_loss:0.508, val_acc:0.696]
Epoch [5/120    avg_loss:0.481, val_acc:0.806]
Epoch [6/120    avg_loss:0.386, val_acc:0.854]
Epoch [7/120    avg_loss:0.340, val_acc:0.854]
Epoch [8/120    avg_loss:0.324, val_acc:0.825]
Epoch [9/120    avg_loss:0.240, val_acc:0.919]
Epoch [10/120    avg_loss:0.179, val_acc:0.898]
Epoch [11/120    avg_loss:0.285, val_acc:0.896]
Epoch [12/120    avg_loss:0.277, val_acc:0.894]
Epoch [13/120    avg_loss:0.261, val_acc:0.938]
Epoch [14/120    avg_loss:0.172, val_acc:0.836]
Epoch [15/120    avg_loss:0.159, val_acc:0.944]
Epoch [16/120    avg_loss:0.157, val_acc:0.920]
Epoch [17/120    avg_loss:0.171, val_acc:0.930]
Epoch [18/120    avg_loss:0.115, val_acc:0.950]
Epoch [19/120    avg_loss:0.098, val_acc:0.949]
Epoch [20/120    avg_loss:0.108, val_acc:0.970]
Epoch [21/120    avg_loss:0.106, val_acc:0.940]
Epoch [22/120    avg_loss:0.102, val_acc:0.940]
Epoch [23/120    avg_loss:0.084, val_acc:0.973]
Epoch [24/120    avg_loss:0.118, val_acc:0.935]
Epoch [25/120    avg_loss:0.132, val_acc:0.960]
Epoch [26/120    avg_loss:0.067, val_acc:0.962]
Epoch [27/120    avg_loss:0.079, val_acc:0.978]
Epoch [28/120    avg_loss:0.088, val_acc:0.966]
Epoch [29/120    avg_loss:0.053, val_acc:0.972]
Epoch [30/120    avg_loss:0.047, val_acc:0.962]
Epoch [31/120    avg_loss:0.065, val_acc:0.965]
Epoch [32/120    avg_loss:0.057, val_acc:0.971]
Epoch [33/120    avg_loss:0.048, val_acc:0.953]
Epoch [34/120    avg_loss:0.057, val_acc:0.961]
Epoch [35/120    avg_loss:0.079, val_acc:0.958]
Epoch [36/120    avg_loss:0.026, val_acc:0.988]
Epoch [37/120    avg_loss:0.027, val_acc:0.967]
Epoch [38/120    avg_loss:0.045, val_acc:0.981]
Epoch [39/120    avg_loss:0.038, val_acc:0.977]
Epoch [40/120    avg_loss:0.042, val_acc:0.986]
Epoch [41/120    avg_loss:0.051, val_acc:0.982]
Epoch [42/120    avg_loss:0.038, val_acc:0.963]
Epoch [43/120    avg_loss:0.067, val_acc:0.948]
Epoch [44/120    avg_loss:0.166, val_acc:0.938]
Epoch [45/120    avg_loss:0.082, val_acc:0.932]
Epoch [46/120    avg_loss:0.033, val_acc:0.985]
Epoch [47/120    avg_loss:0.047, val_acc:0.979]
Epoch [48/120    avg_loss:0.037, val_acc:0.975]
Epoch [49/120    avg_loss:0.020, val_acc:0.969]
Epoch [50/120    avg_loss:0.032, val_acc:0.986]
Epoch [51/120    avg_loss:0.021, val_acc:0.990]
Epoch [52/120    avg_loss:0.048, val_acc:0.991]
Epoch [53/120    avg_loss:0.025, val_acc:0.990]
Epoch [54/120    avg_loss:0.013, val_acc:0.991]
Epoch [55/120    avg_loss:0.014, val_acc:0.991]
Epoch [56/120    avg_loss:0.013, val_acc:0.991]
Epoch [57/120    avg_loss:0.012, val_acc:0.991]
Epoch [58/120    avg_loss:0.011, val_acc:0.991]
Epoch [59/120    avg_loss:0.026, val_acc:0.991]
Epoch [60/120    avg_loss:0.012, val_acc:0.991]
Epoch [61/120    avg_loss:0.017, val_acc:0.991]
Epoch [62/120    avg_loss:0.018, val_acc:0.990]
Epoch [63/120    avg_loss:0.019, val_acc:0.991]
Epoch [64/120    avg_loss:0.008, val_acc:0.991]
Epoch [65/120    avg_loss:0.012, val_acc:0.991]
Epoch [66/120    avg_loss:0.021, val_acc:0.989]
Epoch [67/120    avg_loss:0.011, val_acc:0.991]
Epoch [68/120    avg_loss:0.014, val_acc:0.991]
Epoch [69/120    avg_loss:0.012, val_acc:0.991]
Epoch [70/120    avg_loss:0.012, val_acc:0.991]
Epoch [71/120    avg_loss:0.021, val_acc:0.990]
Epoch [72/120    avg_loss:0.021, val_acc:0.990]
Epoch [73/120    avg_loss:0.011, val_acc:0.991]
Epoch [74/120    avg_loss:0.016, val_acc:0.990]
Epoch [75/120    avg_loss:0.009, val_acc:0.991]
Epoch [76/120    avg_loss:0.013, val_acc:0.991]
Epoch [77/120    avg_loss:0.012, val_acc:0.987]
Epoch [78/120    avg_loss:0.017, val_acc:0.988]
Epoch [79/120    avg_loss:0.010, val_acc:0.990]
Epoch [80/120    avg_loss:0.014, val_acc:0.991]
Epoch [81/120    avg_loss:0.011, val_acc:0.991]
Epoch [82/120    avg_loss:0.014, val_acc:0.992]
Epoch [83/120    avg_loss:0.012, val_acc:0.991]
Epoch [84/120    avg_loss:0.014, val_acc:0.991]
Epoch [85/120    avg_loss:0.013, val_acc:0.991]
Epoch [86/120    avg_loss:0.012, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.990]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.016, val_acc:0.990]
Epoch [91/120    avg_loss:0.017, val_acc:0.991]
Epoch [92/120    avg_loss:0.013, val_acc:0.990]
Epoch [93/120    avg_loss:0.014, val_acc:0.991]
Epoch [94/120    avg_loss:0.015, val_acc:0.991]
Epoch [95/120    avg_loss:0.017, val_acc:0.988]
Epoch [96/120    avg_loss:0.028, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.016, val_acc:0.988]
Epoch [99/120    avg_loss:0.013, val_acc:0.990]
Epoch [100/120    avg_loss:0.015, val_acc:0.990]
Epoch [101/120    avg_loss:0.015, val_acc:0.991]
Epoch [102/120    avg_loss:0.007, val_acc:0.991]
Epoch [103/120    avg_loss:0.010, val_acc:0.991]
Epoch [104/120    avg_loss:0.011, val_acc:0.991]
Epoch [105/120    avg_loss:0.015, val_acc:0.991]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.010, val_acc:0.991]
Epoch [108/120    avg_loss:0.017, val_acc:0.991]
Epoch [109/120    avg_loss:0.018, val_acc:0.991]
Epoch [110/120    avg_loss:0.006, val_acc:0.991]
Epoch [111/120    avg_loss:0.013, val_acc:0.991]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.016, val_acc:0.991]
Epoch [114/120    avg_loss:0.009, val_acc:0.991]
Epoch [115/120    avg_loss:0.008, val_acc:0.991]
Epoch [116/120    avg_loss:0.012, val_acc:0.991]
Epoch [117/120    avg_loss:0.019, val_acc:0.991]
Epoch [118/120    avg_loss:0.009, val_acc:0.991]
Epoch [119/120    avg_loss:0.020, val_acc:0.991]
Epoch [120/120    avg_loss:0.011, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     0     0     0     1]
 [    0     6 17878     0    60     0   143     0     3     0]
 [    0     0     0  2025     0     0     0     0     7     4]
 [    0    23     0     2  2925     0     0     0    20     2]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4863     0     7     8]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0     7    29     0     0     0  3530     3]
 [    0     0     0     6    14    24     0     0     0   875]]

Accuracy:
99.10346323476249

F1 scores:
[       nan 0.99751823 0.99410587 0.9936212  0.975      0.99050513
 0.98401457 1.         0.98907257 0.96525097]

Kappa:
0.988146350575226
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f789e36d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.248, val_acc:0.587]
Epoch [2/120    avg_loss:0.690, val_acc:0.690]
Epoch [3/120    avg_loss:0.626, val_acc:0.758]
Epoch [4/120    avg_loss:0.397, val_acc:0.767]
Epoch [5/120    avg_loss:0.477, val_acc:0.812]
Epoch [6/120    avg_loss:0.554, val_acc:0.863]
Epoch [7/120    avg_loss:0.407, val_acc:0.860]
Epoch [8/120    avg_loss:0.318, val_acc:0.872]
Epoch [9/120    avg_loss:0.268, val_acc:0.873]
Epoch [10/120    avg_loss:0.358, val_acc:0.807]
Epoch [11/120    avg_loss:0.236, val_acc:0.870]
Epoch [12/120    avg_loss:0.242, val_acc:0.846]
Epoch [13/120    avg_loss:0.196, val_acc:0.876]
Epoch [14/120    avg_loss:0.198, val_acc:0.922]
Epoch [15/120    avg_loss:0.171, val_acc:0.898]
Epoch [16/120    avg_loss:0.155, val_acc:0.900]
Epoch [17/120    avg_loss:0.131, val_acc:0.863]
Epoch [18/120    avg_loss:0.150, val_acc:0.924]
Epoch [19/120    avg_loss:0.080, val_acc:0.952]
Epoch [20/120    avg_loss:0.092, val_acc:0.884]
Epoch [21/120    avg_loss:0.112, val_acc:0.937]
Epoch [22/120    avg_loss:0.088, val_acc:0.954]
Epoch [23/120    avg_loss:0.071, val_acc:0.930]
Epoch [24/120    avg_loss:0.154, val_acc:0.941]
Epoch [25/120    avg_loss:0.100, val_acc:0.963]
Epoch [26/120    avg_loss:0.079, val_acc:0.950]
Epoch [27/120    avg_loss:0.077, val_acc:0.958]
Epoch [28/120    avg_loss:0.097, val_acc:0.888]
Epoch [29/120    avg_loss:0.106, val_acc:0.935]
Epoch [30/120    avg_loss:0.143, val_acc:0.965]
Epoch [31/120    avg_loss:0.038, val_acc:0.966]
Epoch [32/120    avg_loss:0.071, val_acc:0.957]
Epoch [33/120    avg_loss:0.039, val_acc:0.969]
Epoch [34/120    avg_loss:0.041, val_acc:0.975]
Epoch [35/120    avg_loss:0.067, val_acc:0.972]
Epoch [36/120    avg_loss:0.056, val_acc:0.980]
Epoch [37/120    avg_loss:0.038, val_acc:0.977]
Epoch [38/120    avg_loss:0.060, val_acc:0.969]
Epoch [39/120    avg_loss:0.151, val_acc:0.937]
Epoch [40/120    avg_loss:0.065, val_acc:0.965]
Epoch [41/120    avg_loss:0.039, val_acc:0.977]
Epoch [42/120    avg_loss:0.025, val_acc:0.977]
Epoch [43/120    avg_loss:0.034, val_acc:0.968]
Epoch [44/120    avg_loss:0.017, val_acc:0.975]
Epoch [45/120    avg_loss:0.017, val_acc:0.979]
Epoch [46/120    avg_loss:0.018, val_acc:0.978]
Epoch [47/120    avg_loss:0.024, val_acc:0.971]
Epoch [48/120    avg_loss:0.034, val_acc:0.962]
Epoch [49/120    avg_loss:0.027, val_acc:0.982]
Epoch [50/120    avg_loss:0.049, val_acc:0.982]
Epoch [51/120    avg_loss:0.038, val_acc:0.961]
Epoch [52/120    avg_loss:0.039, val_acc:0.979]
Epoch [53/120    avg_loss:0.029, val_acc:0.966]
Epoch [54/120    avg_loss:0.025, val_acc:0.981]
Epoch [55/120    avg_loss:0.022, val_acc:0.977]
Epoch [56/120    avg_loss:0.023, val_acc:0.982]
Epoch [57/120    avg_loss:0.017, val_acc:0.968]
Epoch [58/120    avg_loss:0.013, val_acc:0.985]
Epoch [59/120    avg_loss:0.042, val_acc:0.972]
Epoch [60/120    avg_loss:0.032, val_acc:0.982]
Epoch [61/120    avg_loss:0.025, val_acc:0.977]
Epoch [62/120    avg_loss:0.015, val_acc:0.987]
Epoch [63/120    avg_loss:0.014, val_acc:0.986]
Epoch [64/120    avg_loss:0.012, val_acc:0.983]
Epoch [65/120    avg_loss:0.029, val_acc:0.982]
Epoch [66/120    avg_loss:0.024, val_acc:0.987]
Epoch [67/120    avg_loss:0.018, val_acc:0.942]
Epoch [68/120    avg_loss:0.050, val_acc:0.962]
Epoch [69/120    avg_loss:0.035, val_acc:0.983]
Epoch [70/120    avg_loss:0.028, val_acc:0.980]
Epoch [71/120    avg_loss:0.021, val_acc:0.978]
Epoch [72/120    avg_loss:0.013, val_acc:0.985]
Epoch [73/120    avg_loss:0.021, val_acc:0.983]
Epoch [74/120    avg_loss:0.017, val_acc:0.973]
Epoch [75/120    avg_loss:0.015, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.013, val_acc:0.991]
Epoch [78/120    avg_loss:0.005, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.014, val_acc:0.987]
Epoch [81/120    avg_loss:0.036, val_acc:0.987]
Epoch [82/120    avg_loss:0.020, val_acc:0.985]
Epoch [83/120    avg_loss:0.013, val_acc:0.988]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.017, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.986]
Epoch [88/120    avg_loss:0.059, val_acc:0.957]
Epoch [89/120    avg_loss:0.172, val_acc:0.964]
Epoch [90/120    avg_loss:0.042, val_acc:0.966]
Epoch [91/120    avg_loss:0.036, val_acc:0.975]
Epoch [92/120    avg_loss:0.017, val_acc:0.980]
Epoch [93/120    avg_loss:0.013, val_acc:0.981]
Epoch [94/120    avg_loss:0.024, val_acc:0.984]
Epoch [95/120    avg_loss:0.013, val_acc:0.984]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.984]
Epoch [99/120    avg_loss:0.032, val_acc:0.984]
Epoch [100/120    avg_loss:0.011, val_acc:0.986]
Epoch [101/120    avg_loss:0.013, val_acc:0.985]
Epoch [102/120    avg_loss:0.012, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.982]
Epoch [105/120    avg_loss:0.015, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.020, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.014, val_acc:0.983]
Epoch [112/120    avg_loss:0.014, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.012, val_acc:0.983]
Epoch [118/120    avg_loss:0.016, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     0     0     2     4     0     0]
 [    0     3 17998     0    53     0    36     0     0     0]
 [    0     2     0  2009     0     0     0     0    19     6]
 [    0    43     1     0  2915     0     1     0    10     2]
 [    0     0     0     0    38  1184     0     0    80     3]
 [    0     0     0     0     0     0  4856     0    20     2]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     5     0     3    45     0     0     0  3518     0]
 [    0     0     0     0    14    20     0     0     0   885]]

Accuracy:
99.00465138698094

F1 scores:
[       nan 0.99543025 0.99742304 0.99258893 0.96571145 0.94380231
 0.99375831 0.99806427 0.97478526 0.97359736]

Kappa:
0.9868202435811613
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75f6bcd748>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.324, val_acc:0.680]
Epoch [2/120    avg_loss:0.847, val_acc:0.683]
Epoch [3/120    avg_loss:0.669, val_acc:0.664]
Epoch [4/120    avg_loss:0.543, val_acc:0.748]
Epoch [5/120    avg_loss:0.451, val_acc:0.738]
Epoch [6/120    avg_loss:0.494, val_acc:0.854]
Epoch [7/120    avg_loss:0.445, val_acc:0.830]
Epoch [8/120    avg_loss:0.286, val_acc:0.878]
Epoch [9/120    avg_loss:0.216, val_acc:0.901]
Epoch [10/120    avg_loss:0.307, val_acc:0.879]
Epoch [11/120    avg_loss:0.228, val_acc:0.925]
Epoch [12/120    avg_loss:0.217, val_acc:0.928]
Epoch [13/120    avg_loss:0.203, val_acc:0.944]
Epoch [14/120    avg_loss:0.121, val_acc:0.947]
Epoch [15/120    avg_loss:0.219, val_acc:0.920]
Epoch [16/120    avg_loss:0.133, val_acc:0.938]
Epoch [17/120    avg_loss:0.183, val_acc:0.925]
Epoch [18/120    avg_loss:0.128, val_acc:0.940]
Epoch [19/120    avg_loss:0.144, val_acc:0.932]
Epoch [20/120    avg_loss:0.122, val_acc:0.921]
Epoch [21/120    avg_loss:0.205, val_acc:0.953]
Epoch [22/120    avg_loss:0.118, val_acc:0.912]
Epoch [23/120    avg_loss:0.094, val_acc:0.949]
Epoch [24/120    avg_loss:0.064, val_acc:0.953]
Epoch [25/120    avg_loss:0.096, val_acc:0.972]
Epoch [26/120    avg_loss:0.092, val_acc:0.967]
Epoch [27/120    avg_loss:0.068, val_acc:0.934]
Epoch [28/120    avg_loss:0.128, val_acc:0.966]
Epoch [29/120    avg_loss:0.053, val_acc:0.976]
Epoch [30/120    avg_loss:0.082, val_acc:0.971]
Epoch [31/120    avg_loss:0.060, val_acc:0.977]
Epoch [32/120    avg_loss:0.081, val_acc:0.918]
Epoch [33/120    avg_loss:0.081, val_acc:0.965]
Epoch [34/120    avg_loss:0.104, val_acc:0.944]
Epoch [35/120    avg_loss:0.051, val_acc:0.973]
Epoch [36/120    avg_loss:0.031, val_acc:0.976]
Epoch [37/120    avg_loss:0.050, val_acc:0.951]
Epoch [38/120    avg_loss:0.048, val_acc:0.977]
Epoch [39/120    avg_loss:0.085, val_acc:0.969]
Epoch [40/120    avg_loss:0.040, val_acc:0.966]
Epoch [41/120    avg_loss:0.041, val_acc:0.971]
Epoch [42/120    avg_loss:0.046, val_acc:0.973]
Epoch [43/120    avg_loss:0.028, val_acc:0.972]
Epoch [44/120    avg_loss:0.057, val_acc:0.977]
Epoch [45/120    avg_loss:0.041, val_acc:0.976]
Epoch [46/120    avg_loss:0.038, val_acc:0.974]
Epoch [47/120    avg_loss:0.022, val_acc:0.984]
Epoch [48/120    avg_loss:0.009, val_acc:0.981]
Epoch [49/120    avg_loss:0.014, val_acc:0.982]
Epoch [50/120    avg_loss:0.019, val_acc:0.981]
Epoch [51/120    avg_loss:0.036, val_acc:0.978]
Epoch [52/120    avg_loss:0.052, val_acc:0.956]
Epoch [53/120    avg_loss:0.038, val_acc:0.982]
Epoch [54/120    avg_loss:0.022, val_acc:0.977]
Epoch [55/120    avg_loss:0.013, val_acc:0.983]
Epoch [56/120    avg_loss:0.017, val_acc:0.982]
Epoch [57/120    avg_loss:0.028, val_acc:0.977]
Epoch [58/120    avg_loss:0.035, val_acc:0.966]
Epoch [59/120    avg_loss:0.104, val_acc:0.972]
Epoch [60/120    avg_loss:0.035, val_acc:0.980]
Epoch [61/120    avg_loss:0.023, val_acc:0.980]
Epoch [62/120    avg_loss:0.018, val_acc:0.982]
Epoch [63/120    avg_loss:0.015, val_acc:0.983]
Epoch [64/120    avg_loss:0.014, val_acc:0.984]
Epoch [65/120    avg_loss:0.012, val_acc:0.984]
Epoch [66/120    avg_loss:0.017, val_acc:0.985]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.015, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.983]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.985]
Epoch [72/120    avg_loss:0.018, val_acc:0.983]
Epoch [73/120    avg_loss:0.011, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.023, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.983]
Epoch [78/120    avg_loss:0.006, val_acc:0.985]
Epoch [79/120    avg_loss:0.016, val_acc:0.985]
Epoch [80/120    avg_loss:0.008, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.985]
Epoch [84/120    avg_loss:0.013, val_acc:0.987]
Epoch [85/120    avg_loss:0.014, val_acc:0.985]
Epoch [86/120    avg_loss:0.014, val_acc:0.987]
Epoch [87/120    avg_loss:0.010, val_acc:0.987]
Epoch [88/120    avg_loss:0.016, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.016, val_acc:0.986]
Epoch [91/120    avg_loss:0.012, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.014, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.022, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.013, val_acc:0.988]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.012, val_acc:0.987]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.010, val_acc:0.987]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.024, val_acc:0.987]
Epoch [110/120    avg_loss:0.010, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.015, val_acc:0.987]
Epoch [115/120    avg_loss:0.009, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.024, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6424     0     0     0     0     1     6     1     0]
 [    0     0 18029     0    30     0    30     0     1     0]
 [    0     0     0  2009     0     0     0     0    17    10]
 [    0    17     0     0  2939     0     0     0    14     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4865     0     9     0]
 [    0     5     0     0     0     0     0  1283     0     2]
 [    0    11     0     8    19     0     0     0  3533     0]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.44568963439616

F1 scores:
[       nan 0.99681899 0.99820059 0.99136442 0.98393036 0.98901099
 0.99549826 0.99495929 0.98880493 0.96849088]

Kappa:
0.9926590521546268
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3413a3c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.283, val_acc:0.669]
Epoch [2/120    avg_loss:0.764, val_acc:0.580]
Epoch [3/120    avg_loss:0.584, val_acc:0.759]
Epoch [4/120    avg_loss:0.520, val_acc:0.796]
Epoch [5/120    avg_loss:0.438, val_acc:0.751]
Epoch [6/120    avg_loss:0.404, val_acc:0.758]
Epoch [7/120    avg_loss:0.330, val_acc:0.858]
Epoch [8/120    avg_loss:0.327, val_acc:0.853]
Epoch [9/120    avg_loss:0.266, val_acc:0.733]
Epoch [10/120    avg_loss:0.279, val_acc:0.871]
Epoch [11/120    avg_loss:0.212, val_acc:0.879]
Epoch [12/120    avg_loss:0.188, val_acc:0.877]
Epoch [13/120    avg_loss:0.265, val_acc:0.899]
Epoch [14/120    avg_loss:0.193, val_acc:0.915]
Epoch [15/120    avg_loss:0.153, val_acc:0.911]
Epoch [16/120    avg_loss:0.129, val_acc:0.888]
Epoch [17/120    avg_loss:0.085, val_acc:0.945]
Epoch [18/120    avg_loss:0.109, val_acc:0.930]
Epoch [19/120    avg_loss:0.079, val_acc:0.949]
Epoch [20/120    avg_loss:0.125, val_acc:0.868]
Epoch [21/120    avg_loss:0.105, val_acc:0.938]
Epoch [22/120    avg_loss:0.143, val_acc:0.957]
Epoch [23/120    avg_loss:0.104, val_acc:0.944]
Epoch [24/120    avg_loss:0.124, val_acc:0.946]
Epoch [25/120    avg_loss:0.090, val_acc:0.963]
Epoch [26/120    avg_loss:0.061, val_acc:0.948]
Epoch [27/120    avg_loss:0.121, val_acc:0.954]
Epoch [28/120    avg_loss:0.205, val_acc:0.863]
Epoch [29/120    avg_loss:0.226, val_acc:0.941]
Epoch [30/120    avg_loss:0.121, val_acc:0.906]
Epoch [31/120    avg_loss:0.142, val_acc:0.938]
Epoch [32/120    avg_loss:0.091, val_acc:0.960]
Epoch [33/120    avg_loss:0.057, val_acc:0.963]
Epoch [34/120    avg_loss:0.048, val_acc:0.968]
Epoch [35/120    avg_loss:0.047, val_acc:0.965]
Epoch [36/120    avg_loss:0.036, val_acc:0.968]
Epoch [37/120    avg_loss:0.072, val_acc:0.922]
Epoch [38/120    avg_loss:0.089, val_acc:0.963]
Epoch [39/120    avg_loss:0.053, val_acc:0.962]
Epoch [40/120    avg_loss:0.028, val_acc:0.968]
Epoch [41/120    avg_loss:0.031, val_acc:0.964]
Epoch [42/120    avg_loss:0.031, val_acc:0.970]
Epoch [43/120    avg_loss:0.038, val_acc:0.974]
Epoch [44/120    avg_loss:0.034, val_acc:0.970]
Epoch [45/120    avg_loss:0.042, val_acc:0.954]
Epoch [46/120    avg_loss:0.044, val_acc:0.966]
Epoch [47/120    avg_loss:0.039, val_acc:0.971]
Epoch [48/120    avg_loss:0.030, val_acc:0.956]
Epoch [49/120    avg_loss:0.071, val_acc:0.966]
Epoch [50/120    avg_loss:0.051, val_acc:0.963]
Epoch [51/120    avg_loss:0.065, val_acc:0.958]
Epoch [52/120    avg_loss:0.031, val_acc:0.971]
Epoch [53/120    avg_loss:0.015, val_acc:0.970]
Epoch [54/120    avg_loss:0.025, val_acc:0.975]
Epoch [55/120    avg_loss:0.012, val_acc:0.973]
Epoch [56/120    avg_loss:0.034, val_acc:0.978]
Epoch [57/120    avg_loss:0.018, val_acc:0.975]
Epoch [58/120    avg_loss:0.016, val_acc:0.976]
Epoch [59/120    avg_loss:0.029, val_acc:0.921]
Epoch [60/120    avg_loss:0.023, val_acc:0.973]
Epoch [61/120    avg_loss:0.016, val_acc:0.971]
Epoch [62/120    avg_loss:0.019, val_acc:0.975]
Epoch [63/120    avg_loss:0.022, val_acc:0.953]
Epoch [64/120    avg_loss:0.016, val_acc:0.976]
Epoch [65/120    avg_loss:0.025, val_acc:0.975]
Epoch [66/120    avg_loss:0.035, val_acc:0.973]
Epoch [67/120    avg_loss:0.018, val_acc:0.968]
Epoch [68/120    avg_loss:0.009, val_acc:0.977]
Epoch [69/120    avg_loss:0.017, val_acc:0.977]
Epoch [70/120    avg_loss:0.017, val_acc:0.975]
Epoch [71/120    avg_loss:0.007, val_acc:0.979]
Epoch [72/120    avg_loss:0.007, val_acc:0.979]
Epoch [73/120    avg_loss:0.012, val_acc:0.978]
Epoch [74/120    avg_loss:0.008, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.978]
Epoch [76/120    avg_loss:0.011, val_acc:0.979]
Epoch [77/120    avg_loss:0.013, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.978]
Epoch [79/120    avg_loss:0.009, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.978]
Epoch [81/120    avg_loss:0.008, val_acc:0.980]
Epoch [82/120    avg_loss:0.006, val_acc:0.980]
Epoch [83/120    avg_loss:0.008, val_acc:0.978]
Epoch [84/120    avg_loss:0.009, val_acc:0.979]
Epoch [85/120    avg_loss:0.008, val_acc:0.979]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.005, val_acc:0.979]
Epoch [89/120    avg_loss:0.015, val_acc:0.978]
Epoch [90/120    avg_loss:0.008, val_acc:0.978]
Epoch [91/120    avg_loss:0.010, val_acc:0.978]
Epoch [92/120    avg_loss:0.006, val_acc:0.978]
Epoch [93/120    avg_loss:0.006, val_acc:0.978]
Epoch [94/120    avg_loss:0.007, val_acc:0.978]
Epoch [95/120    avg_loss:0.005, val_acc:0.977]
Epoch [96/120    avg_loss:0.006, val_acc:0.978]
Epoch [97/120    avg_loss:0.004, val_acc:0.978]
Epoch [98/120    avg_loss:0.007, val_acc:0.978]
Epoch [99/120    avg_loss:0.012, val_acc:0.978]
Epoch [100/120    avg_loss:0.006, val_acc:0.978]
Epoch [101/120    avg_loss:0.008, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.978]
Epoch [103/120    avg_loss:0.006, val_acc:0.978]
Epoch [104/120    avg_loss:0.008, val_acc:0.978]
Epoch [105/120    avg_loss:0.006, val_acc:0.978]
Epoch [106/120    avg_loss:0.004, val_acc:0.978]
Epoch [107/120    avg_loss:0.009, val_acc:0.978]
Epoch [108/120    avg_loss:0.012, val_acc:0.978]
Epoch [109/120    avg_loss:0.006, val_acc:0.978]
Epoch [110/120    avg_loss:0.011, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.032, val_acc:0.979]
Epoch [114/120    avg_loss:0.004, val_acc:0.979]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     0     0     0    22     3     8     0]
 [    0     0 18054     0    29     0     7     0     0     0]
 [    0     0     0  2015     0     0     0     0    11    10]
 [    0    15     0     3  2942     0     3     0     9     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0    10     0     0     0  4865     0     3     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0    14    26     0     0     0  3529     0]
 [    0     0     0     0    14    20     0     0     0   885]]

Accuracy:
99.49389053575302

F1 scores:
[       nan 0.99610834 0.99872766 0.9906588  0.98345312 0.99201217
 0.99539642 0.99883856 0.98976301 0.97520661]

Kappa:
0.9932958313217979
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ad63c9710>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.480, val_acc:0.590]
Epoch [2/120    avg_loss:0.782, val_acc:0.740]
Epoch [3/120    avg_loss:0.567, val_acc:0.780]
Epoch [4/120    avg_loss:0.504, val_acc:0.793]
Epoch [5/120    avg_loss:0.457, val_acc:0.789]
Epoch [6/120    avg_loss:0.395, val_acc:0.873]
Epoch [7/120    avg_loss:0.408, val_acc:0.763]
Epoch [8/120    avg_loss:0.338, val_acc:0.878]
Epoch [9/120    avg_loss:0.326, val_acc:0.844]
Epoch [10/120    avg_loss:0.297, val_acc:0.883]
Epoch [11/120    avg_loss:0.209, val_acc:0.909]
Epoch [12/120    avg_loss:0.216, val_acc:0.914]
Epoch [13/120    avg_loss:0.180, val_acc:0.880]
Epoch [14/120    avg_loss:0.157, val_acc:0.865]
Epoch [15/120    avg_loss:0.146, val_acc:0.904]
Epoch [16/120    avg_loss:0.128, val_acc:0.916]
Epoch [17/120    avg_loss:0.127, val_acc:0.935]
Epoch [18/120    avg_loss:0.118, val_acc:0.950]
Epoch [19/120    avg_loss:0.172, val_acc:0.932]
Epoch [20/120    avg_loss:0.109, val_acc:0.914]
Epoch [21/120    avg_loss:0.091, val_acc:0.955]
Epoch [22/120    avg_loss:0.065, val_acc:0.945]
Epoch [23/120    avg_loss:0.056, val_acc:0.902]
Epoch [24/120    avg_loss:0.083, val_acc:0.929]
Epoch [25/120    avg_loss:0.092, val_acc:0.940]
Epoch [26/120    avg_loss:0.096, val_acc:0.947]
Epoch [27/120    avg_loss:0.088, val_acc:0.945]
Epoch [28/120    avg_loss:0.070, val_acc:0.940]
Epoch [29/120    avg_loss:0.049, val_acc:0.968]
Epoch [30/120    avg_loss:0.045, val_acc:0.958]
Epoch [31/120    avg_loss:0.066, val_acc:0.966]
Epoch [32/120    avg_loss:0.056, val_acc:0.873]
Epoch [33/120    avg_loss:0.038, val_acc:0.968]
Epoch [34/120    avg_loss:0.044, val_acc:0.957]
Epoch [35/120    avg_loss:0.023, val_acc:0.968]
Epoch [36/120    avg_loss:0.043, val_acc:0.966]
Epoch [37/120    avg_loss:0.036, val_acc:0.967]
Epoch [38/120    avg_loss:0.031, val_acc:0.965]
Epoch [39/120    avg_loss:0.040, val_acc:0.961]
Epoch [40/120    avg_loss:0.062, val_acc:0.966]
Epoch [41/120    avg_loss:0.075, val_acc:0.970]
Epoch [42/120    avg_loss:0.051, val_acc:0.966]
Epoch [43/120    avg_loss:0.038, val_acc:0.960]
Epoch [44/120    avg_loss:0.043, val_acc:0.971]
Epoch [45/120    avg_loss:0.024, val_acc:0.968]
Epoch [46/120    avg_loss:0.016, val_acc:0.978]
Epoch [47/120    avg_loss:0.031, val_acc:0.971]
Epoch [48/120    avg_loss:0.021, val_acc:0.976]
Epoch [49/120    avg_loss:0.016, val_acc:0.979]
Epoch [50/120    avg_loss:0.060, val_acc:0.972]
Epoch [51/120    avg_loss:0.023, val_acc:0.973]
Epoch [52/120    avg_loss:0.020, val_acc:0.961]
Epoch [53/120    avg_loss:0.023, val_acc:0.972]
Epoch [54/120    avg_loss:0.044, val_acc:0.954]
Epoch [55/120    avg_loss:0.034, val_acc:0.970]
Epoch [56/120    avg_loss:0.020, val_acc:0.973]
Epoch [57/120    avg_loss:0.018, val_acc:0.982]
Epoch [58/120    avg_loss:0.028, val_acc:0.967]
Epoch [59/120    avg_loss:0.036, val_acc:0.978]
Epoch [60/120    avg_loss:0.082, val_acc:0.917]
Epoch [61/120    avg_loss:0.120, val_acc:0.919]
Epoch [62/120    avg_loss:0.082, val_acc:0.959]
Epoch [63/120    avg_loss:0.034, val_acc:0.976]
Epoch [64/120    avg_loss:0.098, val_acc:0.939]
Epoch [65/120    avg_loss:0.043, val_acc:0.961]
Epoch [66/120    avg_loss:0.031, val_acc:0.947]
Epoch [67/120    avg_loss:0.022, val_acc:0.966]
Epoch [68/120    avg_loss:0.051, val_acc:0.971]
Epoch [69/120    avg_loss:0.028, val_acc:0.977]
Epoch [70/120    avg_loss:0.015, val_acc:0.982]
Epoch [71/120    avg_loss:0.011, val_acc:0.977]
Epoch [72/120    avg_loss:0.019, val_acc:0.973]
Epoch [73/120    avg_loss:0.015, val_acc:0.979]
Epoch [74/120    avg_loss:0.015, val_acc:0.975]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.013, val_acc:0.980]
Epoch [77/120    avg_loss:0.016, val_acc:0.977]
Epoch [78/120    avg_loss:0.049, val_acc:0.970]
Epoch [79/120    avg_loss:0.028, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.983]
Epoch [82/120    avg_loss:0.015, val_acc:0.974]
Epoch [83/120    avg_loss:0.018, val_acc:0.977]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.024, val_acc:0.971]
Epoch [88/120    avg_loss:0.010, val_acc:0.966]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.982]
Epoch [91/120    avg_loss:0.020, val_acc:0.983]
Epoch [92/120    avg_loss:0.009, val_acc:0.975]
Epoch [93/120    avg_loss:0.011, val_acc:0.970]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.015, val_acc:0.977]
Epoch [98/120    avg_loss:0.042, val_acc:0.974]
Epoch [99/120    avg_loss:0.015, val_acc:0.977]
Epoch [100/120    avg_loss:0.020, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.982]
Epoch [102/120    avg_loss:0.012, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.014, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     0 18021     0    63     0     5     0     1     0]
 [    0     3     0  1994     0     0     0     0    33     6]
 [    0    28     0     0  2917     0     1     0    13    13]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     3     0     0     0  4853     0    22     0]
 [    0     3     0     0     0     0     0  1287     0     0]
 [    0     1     0    21    33     0     0     0  3515     1]
 [    0     0     0     0     5    28     0     0     0   886]]

Accuracy:
99.31313715566482

F1 scores:
[       nan 0.99728661 0.99800631 0.98444828 0.97395659 0.98861912
 0.99681627 0.99883586 0.9825297  0.969896  ]

Kappa:
0.9909049605146552
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f09c2f4f748>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.355, val_acc:0.507]
Epoch [2/120    avg_loss:0.730, val_acc:0.677]
Epoch [3/120    avg_loss:0.547, val_acc:0.717]
Epoch [4/120    avg_loss:0.442, val_acc:0.677]
Epoch [5/120    avg_loss:0.486, val_acc:0.842]
Epoch [6/120    avg_loss:0.420, val_acc:0.850]
Epoch [7/120    avg_loss:0.367, val_acc:0.825]
Epoch [8/120    avg_loss:0.324, val_acc:0.905]
Epoch [9/120    avg_loss:0.254, val_acc:0.906]
Epoch [10/120    avg_loss:0.238, val_acc:0.917]
Epoch [11/120    avg_loss:0.206, val_acc:0.918]
Epoch [12/120    avg_loss:0.224, val_acc:0.917]
Epoch [13/120    avg_loss:0.254, val_acc:0.916]
Epoch [14/120    avg_loss:0.199, val_acc:0.913]
Epoch [15/120    avg_loss:0.196, val_acc:0.923]
Epoch [16/120    avg_loss:0.211, val_acc:0.937]
Epoch [17/120    avg_loss:0.170, val_acc:0.947]
Epoch [18/120    avg_loss:0.199, val_acc:0.933]
Epoch [19/120    avg_loss:0.178, val_acc:0.921]
Epoch [20/120    avg_loss:0.202, val_acc:0.897]
Epoch [21/120    avg_loss:0.118, val_acc:0.957]
Epoch [22/120    avg_loss:0.119, val_acc:0.932]
Epoch [23/120    avg_loss:0.110, val_acc:0.897]
Epoch [24/120    avg_loss:0.066, val_acc:0.938]
Epoch [25/120    avg_loss:0.051, val_acc:0.968]
Epoch [26/120    avg_loss:0.062, val_acc:0.957]
Epoch [27/120    avg_loss:0.054, val_acc:0.951]
Epoch [28/120    avg_loss:0.057, val_acc:0.963]
Epoch [29/120    avg_loss:0.065, val_acc:0.947]
Epoch [30/120    avg_loss:0.062, val_acc:0.963]
Epoch [31/120    avg_loss:0.066, val_acc:0.873]
Epoch [32/120    avg_loss:0.044, val_acc:0.955]
Epoch [33/120    avg_loss:0.043, val_acc:0.960]
Epoch [34/120    avg_loss:0.031, val_acc:0.970]
Epoch [35/120    avg_loss:0.039, val_acc:0.973]
Epoch [36/120    avg_loss:0.034, val_acc:0.971]
Epoch [37/120    avg_loss:0.056, val_acc:0.974]
Epoch [38/120    avg_loss:0.034, val_acc:0.973]
Epoch [39/120    avg_loss:0.043, val_acc:0.947]
Epoch [40/120    avg_loss:0.032, val_acc:0.975]
Epoch [41/120    avg_loss:0.047, val_acc:0.966]
Epoch [42/120    avg_loss:0.085, val_acc:0.959]
Epoch [43/120    avg_loss:0.044, val_acc:0.978]
Epoch [44/120    avg_loss:0.017, val_acc:0.972]
Epoch [45/120    avg_loss:0.022, val_acc:0.971]
Epoch [46/120    avg_loss:0.016, val_acc:0.973]
Epoch [47/120    avg_loss:0.014, val_acc:0.972]
Epoch [48/120    avg_loss:0.019, val_acc:0.978]
Epoch [49/120    avg_loss:0.025, val_acc:0.979]
Epoch [50/120    avg_loss:0.017, val_acc:0.979]
Epoch [51/120    avg_loss:0.061, val_acc:0.963]
Epoch [52/120    avg_loss:0.030, val_acc:0.978]
Epoch [53/120    avg_loss:0.021, val_acc:0.944]
Epoch [54/120    avg_loss:0.029, val_acc:0.966]
Epoch [55/120    avg_loss:0.021, val_acc:0.972]
Epoch [56/120    avg_loss:0.025, val_acc:0.974]
Epoch [57/120    avg_loss:0.051, val_acc:0.974]
Epoch [58/120    avg_loss:0.023, val_acc:0.969]
Epoch [59/120    avg_loss:0.011, val_acc:0.978]
Epoch [60/120    avg_loss:0.026, val_acc:0.974]
Epoch [61/120    avg_loss:0.025, val_acc:0.967]
Epoch [62/120    avg_loss:0.022, val_acc:0.978]
Epoch [63/120    avg_loss:0.026, val_acc:0.975]
Epoch [64/120    avg_loss:0.016, val_acc:0.982]
Epoch [65/120    avg_loss:0.013, val_acc:0.983]
Epoch [66/120    avg_loss:0.030, val_acc:0.983]
Epoch [67/120    avg_loss:0.013, val_acc:0.982]
Epoch [68/120    avg_loss:0.014, val_acc:0.982]
Epoch [69/120    avg_loss:0.028, val_acc:0.984]
Epoch [70/120    avg_loss:0.014, val_acc:0.982]
Epoch [71/120    avg_loss:0.007, val_acc:0.982]
Epoch [72/120    avg_loss:0.022, val_acc:0.981]
Epoch [73/120    avg_loss:0.015, val_acc:0.982]
Epoch [74/120    avg_loss:0.006, val_acc:0.983]
Epoch [75/120    avg_loss:0.008, val_acc:0.983]
Epoch [76/120    avg_loss:0.012, val_acc:0.983]
Epoch [77/120    avg_loss:0.010, val_acc:0.983]
Epoch [78/120    avg_loss:0.007, val_acc:0.983]
Epoch [79/120    avg_loss:0.014, val_acc:0.983]
Epoch [80/120    avg_loss:0.008, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.021, val_acc:0.982]
Epoch [83/120    avg_loss:0.013, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.982]
Epoch [87/120    avg_loss:0.011, val_acc:0.983]
Epoch [88/120    avg_loss:0.010, val_acc:0.982]
Epoch [89/120    avg_loss:0.014, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.005, val_acc:0.982]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.983]
Epoch [103/120    avg_loss:0.012, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.016, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.015, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.018, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.983]
Epoch [113/120    avg_loss:0.019, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.021, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     0     0     0     0     0     6]
 [    0     0 17928     0   110     0    52     0     0     0]
 [    0     0     0  2010     0     0     0     0    11    15]
 [    0    20     0     0  2935     0     0     0    14     3]
 [    0     0     0     0     0  1300     0     0     0     5]
 [    0     0     0     0     0     0  4868     0    10     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    18     0    10    28     0     0     0  3512     3]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
99.17094449666209

F1 scores:
[       nan 0.99658809 0.99550225 0.99112426 0.96880673 0.98934551
 0.99367218 0.9992242  0.98679404 0.9613079 ]

Kappa:
0.9890337421217171
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d9b0a4860>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.327, val_acc:0.432]
Epoch [2/120    avg_loss:0.785, val_acc:0.729]
Epoch [3/120    avg_loss:0.621, val_acc:0.708]
Epoch [4/120    avg_loss:0.522, val_acc:0.703]
Epoch [5/120    avg_loss:0.421, val_acc:0.802]
Epoch [6/120    avg_loss:0.373, val_acc:0.842]
Epoch [7/120    avg_loss:0.303, val_acc:0.807]
Epoch [8/120    avg_loss:0.295, val_acc:0.786]
Epoch [9/120    avg_loss:0.377, val_acc:0.816]
Epoch [10/120    avg_loss:0.199, val_acc:0.904]
Epoch [11/120    avg_loss:0.242, val_acc:0.849]
Epoch [12/120    avg_loss:0.138, val_acc:0.877]
Epoch [13/120    avg_loss:0.297, val_acc:0.900]
Epoch [14/120    avg_loss:0.186, val_acc:0.893]
Epoch [15/120    avg_loss:0.173, val_acc:0.907]
Epoch [16/120    avg_loss:0.135, val_acc:0.922]
Epoch [17/120    avg_loss:0.101, val_acc:0.907]
Epoch [18/120    avg_loss:0.129, val_acc:0.938]
Epoch [19/120    avg_loss:0.133, val_acc:0.908]
Epoch [20/120    avg_loss:0.063, val_acc:0.905]
Epoch [21/120    avg_loss:0.078, val_acc:0.919]
Epoch [22/120    avg_loss:0.148, val_acc:0.946]
Epoch [23/120    avg_loss:0.113, val_acc:0.907]
Epoch [24/120    avg_loss:0.079, val_acc:0.952]
Epoch [25/120    avg_loss:0.058, val_acc:0.959]
Epoch [26/120    avg_loss:0.087, val_acc:0.917]
Epoch [27/120    avg_loss:0.074, val_acc:0.938]
Epoch [28/120    avg_loss:0.071, val_acc:0.945]
Epoch [29/120    avg_loss:0.049, val_acc:0.958]
Epoch [30/120    avg_loss:0.100, val_acc:0.943]
Epoch [31/120    avg_loss:0.070, val_acc:0.902]
Epoch [32/120    avg_loss:0.097, val_acc:0.946]
Epoch [33/120    avg_loss:0.092, val_acc:0.955]
Epoch [34/120    avg_loss:0.042, val_acc:0.954]
Epoch [35/120    avg_loss:0.068, val_acc:0.926]
Epoch [36/120    avg_loss:0.066, val_acc:0.934]
Epoch [37/120    avg_loss:0.060, val_acc:0.880]
Epoch [38/120    avg_loss:0.070, val_acc:0.967]
Epoch [39/120    avg_loss:0.056, val_acc:0.912]
Epoch [40/120    avg_loss:0.086, val_acc:0.931]
Epoch [41/120    avg_loss:0.065, val_acc:0.964]
Epoch [42/120    avg_loss:0.055, val_acc:0.966]
Epoch [43/120    avg_loss:0.031, val_acc:0.965]
Epoch [44/120    avg_loss:0.054, val_acc:0.944]
Epoch [45/120    avg_loss:0.050, val_acc:0.950]
Epoch [46/120    avg_loss:0.064, val_acc:0.958]
Epoch [47/120    avg_loss:0.043, val_acc:0.975]
Epoch [48/120    avg_loss:0.025, val_acc:0.971]
Epoch [49/120    avg_loss:0.023, val_acc:0.976]
Epoch [50/120    avg_loss:0.042, val_acc:0.971]
Epoch [51/120    avg_loss:0.023, val_acc:0.970]
Epoch [52/120    avg_loss:0.022, val_acc:0.980]
Epoch [53/120    avg_loss:0.030, val_acc:0.973]
Epoch [54/120    avg_loss:0.032, val_acc:0.968]
Epoch [55/120    avg_loss:0.041, val_acc:0.979]
Epoch [56/120    avg_loss:0.022, val_acc:0.973]
Epoch [57/120    avg_loss:0.070, val_acc:0.947]
Epoch [58/120    avg_loss:0.110, val_acc:0.955]
Epoch [59/120    avg_loss:0.085, val_acc:0.968]
Epoch [60/120    avg_loss:0.035, val_acc:0.963]
Epoch [61/120    avg_loss:0.030, val_acc:0.962]
Epoch [62/120    avg_loss:0.026, val_acc:0.911]
Epoch [63/120    avg_loss:0.023, val_acc:0.972]
Epoch [64/120    avg_loss:0.035, val_acc:0.967]
Epoch [65/120    avg_loss:0.020, val_acc:0.978]
Epoch [66/120    avg_loss:0.018, val_acc:0.981]
Epoch [67/120    avg_loss:0.018, val_acc:0.983]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.008, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.013, val_acc:0.983]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.010, val_acc:0.981]
Epoch [74/120    avg_loss:0.015, val_acc:0.983]
Epoch [75/120    avg_loss:0.012, val_acc:0.983]
Epoch [76/120    avg_loss:0.015, val_acc:0.981]
Epoch [77/120    avg_loss:0.005, val_acc:0.983]
Epoch [78/120    avg_loss:0.020, val_acc:0.980]
Epoch [79/120    avg_loss:0.013, val_acc:0.982]
Epoch [80/120    avg_loss:0.023, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.981]
Epoch [82/120    avg_loss:0.014, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.018, val_acc:0.983]
Epoch [86/120    avg_loss:0.012, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.013, val_acc:0.983]
Epoch [92/120    avg_loss:0.014, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.983]
Epoch [95/120    avg_loss:0.017, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.017, val_acc:0.983]
Epoch [98/120    avg_loss:0.015, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.011, val_acc:0.983]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.007, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.024, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     0     0    17     0     1     0]
 [    0     1 18015     0    39     0    35     0     0     0]
 [    0     5     0  1985     0     0     0     0    39     7]
 [    0    23     0     0  2937     0     0     0    12     0]
 [    0     0     0     0    13  1252     0     0     0    40]
 [    0     0     1     0     0     0  4860     0    17     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    15     0     0    36     0     0     0  3520     0]
 [    0     0     0     0    14     3     0     0     0   902]]

Accuracy:
99.22878557829031

F1 scores:
[       nan 0.99519007 0.99789509 0.98731659 0.97720845 0.978125
 0.99284985 0.9992242  0.98324022 0.96470588]

Kappa:
0.9897878943622425
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f370308d6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.273, val_acc:0.508]
Epoch [2/120    avg_loss:0.799, val_acc:0.538]
Epoch [3/120    avg_loss:0.659, val_acc:0.702]
Epoch [4/120    avg_loss:0.511, val_acc:0.733]
Epoch [5/120    avg_loss:0.427, val_acc:0.881]
Epoch [6/120    avg_loss:0.443, val_acc:0.814]
Epoch [7/120    avg_loss:0.337, val_acc:0.790]
Epoch [8/120    avg_loss:0.338, val_acc:0.881]
Epoch [9/120    avg_loss:0.233, val_acc:0.895]
Epoch [10/120    avg_loss:0.280, val_acc:0.798]
Epoch [11/120    avg_loss:0.207, val_acc:0.851]
Epoch [12/120    avg_loss:0.314, val_acc:0.872]
Epoch [13/120    avg_loss:0.269, val_acc:0.862]
Epoch [14/120    avg_loss:0.167, val_acc:0.925]
Epoch [15/120    avg_loss:0.154, val_acc:0.877]
Epoch [16/120    avg_loss:0.156, val_acc:0.877]
Epoch [17/120    avg_loss:0.125, val_acc:0.941]
Epoch [18/120    avg_loss:0.147, val_acc:0.915]
Epoch [19/120    avg_loss:0.174, val_acc:0.898]
Epoch [20/120    avg_loss:0.074, val_acc:0.878]
Epoch [21/120    avg_loss:0.113, val_acc:0.924]
Epoch [22/120    avg_loss:0.069, val_acc:0.942]
Epoch [23/120    avg_loss:0.068, val_acc:0.931]
Epoch [24/120    avg_loss:0.084, val_acc:0.946]
Epoch [25/120    avg_loss:0.083, val_acc:0.910]
Epoch [26/120    avg_loss:0.060, val_acc:0.935]
Epoch [27/120    avg_loss:0.063, val_acc:0.956]
Epoch [28/120    avg_loss:0.070, val_acc:0.966]
Epoch [29/120    avg_loss:0.065, val_acc:0.965]
Epoch [30/120    avg_loss:0.100, val_acc:0.951]
Epoch [31/120    avg_loss:0.213, val_acc:0.936]
Epoch [32/120    avg_loss:0.089, val_acc:0.959]
Epoch [33/120    avg_loss:0.044, val_acc:0.966]
Epoch [34/120    avg_loss:0.079, val_acc:0.942]
Epoch [35/120    avg_loss:0.077, val_acc:0.966]
Epoch [36/120    avg_loss:0.054, val_acc:0.967]
Epoch [37/120    avg_loss:0.053, val_acc:0.959]
Epoch [38/120    avg_loss:0.058, val_acc:0.970]
Epoch [39/120    avg_loss:0.059, val_acc:0.951]
Epoch [40/120    avg_loss:0.043, val_acc:0.967]
Epoch [41/120    avg_loss:0.038, val_acc:0.966]
Epoch [42/120    avg_loss:0.048, val_acc:0.961]
Epoch [43/120    avg_loss:0.074, val_acc:0.962]
Epoch [44/120    avg_loss:0.021, val_acc:0.970]
Epoch [45/120    avg_loss:0.030, val_acc:0.970]
Epoch [46/120    avg_loss:0.021, val_acc:0.971]
Epoch [47/120    avg_loss:0.022, val_acc:0.971]
Epoch [48/120    avg_loss:0.034, val_acc:0.971]
Epoch [49/120    avg_loss:0.068, val_acc:0.977]
Epoch [50/120    avg_loss:0.011, val_acc:0.977]
Epoch [51/120    avg_loss:0.018, val_acc:0.967]
Epoch [52/120    avg_loss:0.025, val_acc:0.953]
Epoch [53/120    avg_loss:0.030, val_acc:0.962]
Epoch [54/120    avg_loss:0.029, val_acc:0.973]
Epoch [55/120    avg_loss:0.027, val_acc:0.980]
Epoch [56/120    avg_loss:0.031, val_acc:0.959]
Epoch [57/120    avg_loss:0.029, val_acc:0.962]
Epoch [58/120    avg_loss:0.033, val_acc:0.960]
Epoch [59/120    avg_loss:0.018, val_acc:0.972]
Epoch [60/120    avg_loss:0.035, val_acc:0.981]
Epoch [61/120    avg_loss:0.027, val_acc:0.971]
Epoch [62/120    avg_loss:0.033, val_acc:0.940]
Epoch [63/120    avg_loss:0.022, val_acc:0.973]
Epoch [64/120    avg_loss:0.021, val_acc:0.973]
Epoch [65/120    avg_loss:0.020, val_acc:0.975]
Epoch [66/120    avg_loss:0.032, val_acc:0.971]
Epoch [67/120    avg_loss:0.062, val_acc:0.955]
Epoch [68/120    avg_loss:0.022, val_acc:0.984]
Epoch [69/120    avg_loss:0.036, val_acc:0.971]
Epoch [70/120    avg_loss:0.058, val_acc:0.967]
Epoch [71/120    avg_loss:0.024, val_acc:0.971]
Epoch [72/120    avg_loss:0.027, val_acc:0.977]
Epoch [73/120    avg_loss:0.026, val_acc:0.976]
Epoch [74/120    avg_loss:0.075, val_acc:0.968]
Epoch [75/120    avg_loss:0.048, val_acc:0.979]
Epoch [76/120    avg_loss:0.034, val_acc:0.973]
Epoch [77/120    avg_loss:0.022, val_acc:0.968]
Epoch [78/120    avg_loss:0.018, val_acc:0.977]
Epoch [79/120    avg_loss:0.036, val_acc:0.973]
Epoch [80/120    avg_loss:0.025, val_acc:0.966]
Epoch [81/120    avg_loss:0.014, val_acc:0.978]
Epoch [82/120    avg_loss:0.017, val_acc:0.977]
Epoch [83/120    avg_loss:0.007, val_acc:0.978]
Epoch [84/120    avg_loss:0.006, val_acc:0.978]
Epoch [85/120    avg_loss:0.014, val_acc:0.977]
Epoch [86/120    avg_loss:0.016, val_acc:0.972]
Epoch [87/120    avg_loss:0.014, val_acc:0.976]
Epoch [88/120    avg_loss:0.010, val_acc:0.977]
Epoch [89/120    avg_loss:0.008, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.975]
Epoch [91/120    avg_loss:0.012, val_acc:0.976]
Epoch [92/120    avg_loss:0.007, val_acc:0.976]
Epoch [93/120    avg_loss:0.011, val_acc:0.974]
Epoch [94/120    avg_loss:0.007, val_acc:0.979]
Epoch [95/120    avg_loss:0.007, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.011, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.005, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.980]
Epoch [102/120    avg_loss:0.005, val_acc:0.980]
Epoch [103/120    avg_loss:0.006, val_acc:0.980]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.980]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.980]
Epoch [110/120    avg_loss:0.007, val_acc:0.980]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.007, val_acc:0.980]
Epoch [113/120    avg_loss:0.016, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.006, val_acc:0.980]
Epoch [118/120    avg_loss:0.015, val_acc:0.981]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     0     0    16     3     0     0]
 [    0     0 18015     0    48     0    27     0     0     0]
 [    0     0     0  2012     0     0     0     0    21     3]
 [    0    13     1     0  2950     0     0     0     7     1]
 [    0     0     0     0     0  1299     0     0     0     6]
 [    0     0     0     0     0     0  4863     0    15     0]
 [    0     2     0     0     0     0     1  1282     0     5]
 [    0     3     0     0    34     0     0     0  3530     4]
 [    0     0     0     3    14    12     0     0     0   890]]

Accuracy:
99.42399922878558

F1 scores:
[       nan 0.99712353 0.99789509 0.99333498 0.98039216 0.99311927
 0.99397036 0.99572816 0.98824188 0.97374179]

Kappa:
0.9923736705753959
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a880a7748>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.341, val_acc:0.677]
Epoch [2/120    avg_loss:0.661, val_acc:0.743]
Epoch [3/120    avg_loss:0.615, val_acc:0.731]
Epoch [4/120    avg_loss:0.566, val_acc:0.748]
Epoch [5/120    avg_loss:0.401, val_acc:0.831]
Epoch [6/120    avg_loss:0.363, val_acc:0.855]
Epoch [7/120    avg_loss:0.456, val_acc:0.862]
Epoch [8/120    avg_loss:0.310, val_acc:0.892]
Epoch [9/120    avg_loss:0.330, val_acc:0.847]
Epoch [10/120    avg_loss:0.229, val_acc:0.890]
Epoch [11/120    avg_loss:0.219, val_acc:0.894]
Epoch [12/120    avg_loss:0.278, val_acc:0.899]
Epoch [13/120    avg_loss:0.169, val_acc:0.899]
Epoch [14/120    avg_loss:0.192, val_acc:0.900]
Epoch [15/120    avg_loss:0.177, val_acc:0.913]
Epoch [16/120    avg_loss:0.171, val_acc:0.920]
Epoch [17/120    avg_loss:0.161, val_acc:0.898]
Epoch [18/120    avg_loss:0.156, val_acc:0.932]
Epoch [19/120    avg_loss:0.153, val_acc:0.866]
Epoch [20/120    avg_loss:0.123, val_acc:0.929]
Epoch [21/120    avg_loss:0.163, val_acc:0.915]
Epoch [22/120    avg_loss:0.093, val_acc:0.947]
Epoch [23/120    avg_loss:0.132, val_acc:0.952]
Epoch [24/120    avg_loss:0.164, val_acc:0.941]
Epoch [25/120    avg_loss:0.144, val_acc:0.943]
Epoch [26/120    avg_loss:0.105, val_acc:0.919]
Epoch [27/120    avg_loss:0.104, val_acc:0.929]
Epoch [28/120    avg_loss:0.122, val_acc:0.929]
Epoch [29/120    avg_loss:0.067, val_acc:0.966]
Epoch [30/120    avg_loss:0.082, val_acc:0.937]
Epoch [31/120    avg_loss:0.110, val_acc:0.949]
Epoch [32/120    avg_loss:0.069, val_acc:0.938]
Epoch [33/120    avg_loss:0.072, val_acc:0.956]
Epoch [34/120    avg_loss:0.068, val_acc:0.934]
Epoch [35/120    avg_loss:0.071, val_acc:0.954]
Epoch [36/120    avg_loss:0.053, val_acc:0.962]
Epoch [37/120    avg_loss:0.069, val_acc:0.965]
Epoch [38/120    avg_loss:0.066, val_acc:0.932]
Epoch [39/120    avg_loss:0.046, val_acc:0.963]
Epoch [40/120    avg_loss:0.033, val_acc:0.964]
Epoch [41/120    avg_loss:0.045, val_acc:0.964]
Epoch [42/120    avg_loss:0.062, val_acc:0.959]
Epoch [43/120    avg_loss:0.034, val_acc:0.970]
Epoch [44/120    avg_loss:0.021, val_acc:0.975]
Epoch [45/120    avg_loss:0.055, val_acc:0.979]
Epoch [46/120    avg_loss:0.031, val_acc:0.979]
Epoch [47/120    avg_loss:0.024, val_acc:0.981]
Epoch [48/120    avg_loss:0.020, val_acc:0.979]
Epoch [49/120    avg_loss:0.033, val_acc:0.974]
Epoch [50/120    avg_loss:0.029, val_acc:0.977]
Epoch [51/120    avg_loss:0.025, val_acc:0.978]
Epoch [52/120    avg_loss:0.017, val_acc:0.980]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.023, val_acc:0.982]
Epoch [55/120    avg_loss:0.017, val_acc:0.981]
Epoch [56/120    avg_loss:0.026, val_acc:0.978]
Epoch [57/120    avg_loss:0.030, val_acc:0.976]
Epoch [58/120    avg_loss:0.024, val_acc:0.978]
Epoch [59/120    avg_loss:0.026, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.983]
Epoch [61/120    avg_loss:0.020, val_acc:0.980]
Epoch [62/120    avg_loss:0.027, val_acc:0.981]
Epoch [63/120    avg_loss:0.025, val_acc:0.982]
Epoch [64/120    avg_loss:0.019, val_acc:0.983]
Epoch [65/120    avg_loss:0.015, val_acc:0.983]
Epoch [66/120    avg_loss:0.014, val_acc:0.983]
Epoch [67/120    avg_loss:0.025, val_acc:0.983]
Epoch [68/120    avg_loss:0.017, val_acc:0.981]
Epoch [69/120    avg_loss:0.026, val_acc:0.983]
Epoch [70/120    avg_loss:0.016, val_acc:0.987]
Epoch [71/120    avg_loss:0.023, val_acc:0.984]
Epoch [72/120    avg_loss:0.025, val_acc:0.979]
Epoch [73/120    avg_loss:0.017, val_acc:0.985]
Epoch [74/120    avg_loss:0.019, val_acc:0.982]
Epoch [75/120    avg_loss:0.017, val_acc:0.984]
Epoch [76/120    avg_loss:0.015, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.984]
Epoch [78/120    avg_loss:0.019, val_acc:0.982]
Epoch [79/120    avg_loss:0.025, val_acc:0.980]
Epoch [80/120    avg_loss:0.016, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.984]
Epoch [82/120    avg_loss:0.018, val_acc:0.983]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.984]
Epoch [87/120    avg_loss:0.016, val_acc:0.985]
Epoch [88/120    avg_loss:0.015, val_acc:0.985]
Epoch [89/120    avg_loss:0.019, val_acc:0.985]
Epoch [90/120    avg_loss:0.021, val_acc:0.986]
Epoch [91/120    avg_loss:0.015, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.022, val_acc:0.985]
Epoch [95/120    avg_loss:0.016, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.985]
Epoch [97/120    avg_loss:0.021, val_acc:0.985]
Epoch [98/120    avg_loss:0.015, val_acc:0.985]
Epoch [99/120    avg_loss:0.016, val_acc:0.985]
Epoch [100/120    avg_loss:0.018, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.023, val_acc:0.985]
Epoch [103/120    avg_loss:0.023, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.985]
Epoch [105/120    avg_loss:0.019, val_acc:0.985]
Epoch [106/120    avg_loss:0.021, val_acc:0.985]
Epoch [107/120    avg_loss:0.019, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.018, val_acc:0.985]
Epoch [111/120    avg_loss:0.018, val_acc:0.985]
Epoch [112/120    avg_loss:0.015, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.985]
Epoch [114/120    avg_loss:0.015, val_acc:0.985]
Epoch [115/120    avg_loss:0.021, val_acc:0.985]
Epoch [116/120    avg_loss:0.015, val_acc:0.985]
Epoch [117/120    avg_loss:0.019, val_acc:0.985]
Epoch [118/120    avg_loss:0.014, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.017, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     0     1     0    15     6     8    11]
 [    0     0 17990     0    67     0    29     0     4     0]
 [    0     2     0  2004     0     0     0     0    27     3]
 [    0    29     0     1  2928     0     0     0    14     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     4     0     0     0  4834     0    40     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0     1    31     0     0     0  3537     0]
 [    0     0     0     9    14     9     0     0     0   887]]

Accuracy:
99.20709517267973

F1 scores:
[       nan 0.99424393 0.99711784 0.98938534 0.97388991 0.99579671
 0.99097991 0.99767981 0.98236356 0.97365532]

Kappa:
0.9895049448202318
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdebeb547b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.202, val_acc:0.614]
Epoch [2/120    avg_loss:0.812, val_acc:0.696]
Epoch [3/120    avg_loss:0.559, val_acc:0.813]
Epoch [4/120    avg_loss:0.438, val_acc:0.825]
Epoch [5/120    avg_loss:0.392, val_acc:0.894]
Epoch [6/120    avg_loss:0.293, val_acc:0.857]
Epoch [7/120    avg_loss:0.339, val_acc:0.878]
Epoch [8/120    avg_loss:0.341, val_acc:0.852]
Epoch [9/120    avg_loss:0.304, val_acc:0.808]
Epoch [10/120    avg_loss:0.263, val_acc:0.906]
Epoch [11/120    avg_loss:0.212, val_acc:0.884]
Epoch [12/120    avg_loss:0.161, val_acc:0.929]
Epoch [13/120    avg_loss:0.208, val_acc:0.894]
Epoch [14/120    avg_loss:0.253, val_acc:0.922]
Epoch [15/120    avg_loss:0.157, val_acc:0.934]
Epoch [16/120    avg_loss:0.139, val_acc:0.939]
Epoch [17/120    avg_loss:0.125, val_acc:0.945]
Epoch [18/120    avg_loss:0.105, val_acc:0.956]
Epoch [19/120    avg_loss:0.127, val_acc:0.912]
Epoch [20/120    avg_loss:0.344, val_acc:0.920]
Epoch [21/120    avg_loss:0.105, val_acc:0.938]
Epoch [22/120    avg_loss:0.117, val_acc:0.951]
Epoch [23/120    avg_loss:0.084, val_acc:0.959]
Epoch [24/120    avg_loss:0.101, val_acc:0.931]
Epoch [25/120    avg_loss:0.102, val_acc:0.938]
Epoch [26/120    avg_loss:0.106, val_acc:0.960]
Epoch [27/120    avg_loss:0.112, val_acc:0.963]
Epoch [28/120    avg_loss:0.064, val_acc:0.943]
Epoch [29/120    avg_loss:0.061, val_acc:0.965]
Epoch [30/120    avg_loss:0.062, val_acc:0.965]
Epoch [31/120    avg_loss:0.043, val_acc:0.902]
Epoch [32/120    avg_loss:0.060, val_acc:0.941]
Epoch [33/120    avg_loss:0.057, val_acc:0.954]
Epoch [34/120    avg_loss:0.071, val_acc:0.939]
Epoch [35/120    avg_loss:0.093, val_acc:0.964]
Epoch [36/120    avg_loss:0.038, val_acc:0.965]
Epoch [37/120    avg_loss:0.058, val_acc:0.958]
Epoch [38/120    avg_loss:0.209, val_acc:0.929]
Epoch [39/120    avg_loss:0.053, val_acc:0.967]
Epoch [40/120    avg_loss:0.101, val_acc:0.964]
Epoch [41/120    avg_loss:0.042, val_acc:0.965]
Epoch [42/120    avg_loss:0.043, val_acc:0.980]
Epoch [43/120    avg_loss:0.141, val_acc:0.961]
Epoch [44/120    avg_loss:0.077, val_acc:0.935]
Epoch [45/120    avg_loss:0.122, val_acc:0.974]
Epoch [46/120    avg_loss:0.051, val_acc:0.970]
Epoch [47/120    avg_loss:0.046, val_acc:0.970]
Epoch [48/120    avg_loss:0.036, val_acc:0.963]
Epoch [49/120    avg_loss:0.038, val_acc:0.952]
Epoch [50/120    avg_loss:0.025, val_acc:0.980]
Epoch [51/120    avg_loss:0.029, val_acc:0.975]
Epoch [52/120    avg_loss:0.020, val_acc:0.968]
Epoch [53/120    avg_loss:0.026, val_acc:0.930]
Epoch [54/120    avg_loss:0.023, val_acc:0.985]
Epoch [55/120    avg_loss:0.015, val_acc:0.985]
Epoch [56/120    avg_loss:0.017, val_acc:0.981]
Epoch [57/120    avg_loss:0.013, val_acc:0.976]
Epoch [58/120    avg_loss:0.027, val_acc:0.964]
Epoch [59/120    avg_loss:0.017, val_acc:0.982]
Epoch [60/120    avg_loss:0.028, val_acc:0.987]
Epoch [61/120    avg_loss:0.010, val_acc:0.984]
Epoch [62/120    avg_loss:0.014, val_acc:0.985]
Epoch [63/120    avg_loss:0.017, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.986]
Epoch [65/120    avg_loss:0.039, val_acc:0.980]
Epoch [66/120    avg_loss:0.016, val_acc:0.980]
Epoch [67/120    avg_loss:0.013, val_acc:0.975]
Epoch [68/120    avg_loss:0.031, val_acc:0.983]
Epoch [69/120    avg_loss:0.015, val_acc:0.972]
Epoch [70/120    avg_loss:0.015, val_acc:0.980]
Epoch [71/120    avg_loss:0.009, val_acc:0.985]
Epoch [72/120    avg_loss:0.047, val_acc:0.973]
Epoch [73/120    avg_loss:0.027, val_acc:0.984]
Epoch [74/120    avg_loss:0.020, val_acc:0.983]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.022, val_acc:0.983]
Epoch [77/120    avg_loss:0.020, val_acc:0.984]
Epoch [78/120    avg_loss:0.020, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.016, val_acc:0.983]
Epoch [83/120    avg_loss:0.014, val_acc:0.983]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.013, val_acc:0.985]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.015, val_acc:0.985]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.019, val_acc:0.985]
Epoch [100/120    avg_loss:0.015, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.013, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.017, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.013, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.019, val_acc:0.985]
Epoch [114/120    avg_loss:0.014, val_acc:0.985]
Epoch [115/120    avg_loss:0.020, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     0     0     3     0     0     0]
 [    0     0 17972     0    83     0    34     0     1     0]
 [    0     0     0  2013     0     0     0     0     9    14]
 [    0    36     0     0  2918     0     0     0    18     0]
 [    0     0     0     0     0  1273     0     0     0    32]
 [    0     0     0     0     0     0  4868     0    10     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     7     0    14    31     0     0     0  3517     2]
 [    0     0     0     0    14    18     0     0     0   887]]

Accuracy:
99.20950521774758

F1 scores:
[       nan 0.99643521 0.99672786 0.99089343 0.96975739 0.9807396
 0.99519575 0.9992242  0.98708953 0.95581897]

Kappa:
0.9895383933433624
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0e5d4a9748>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.301, val_acc:0.671]
Epoch [2/120    avg_loss:0.750, val_acc:0.658]
Epoch [3/120    avg_loss:0.693, val_acc:0.699]
Epoch [4/120    avg_loss:0.597, val_acc:0.773]
Epoch [5/120    avg_loss:0.352, val_acc:0.859]
Epoch [6/120    avg_loss:0.473, val_acc:0.844]
Epoch [7/120    avg_loss:0.443, val_acc:0.829]
Epoch [8/120    avg_loss:0.397, val_acc:0.894]
Epoch [9/120    avg_loss:0.300, val_acc:0.905]
Epoch [10/120    avg_loss:0.334, val_acc:0.894]
Epoch [11/120    avg_loss:0.292, val_acc:0.886]
Epoch [12/120    avg_loss:0.199, val_acc:0.933]
Epoch [13/120    avg_loss:0.273, val_acc:0.883]
Epoch [14/120    avg_loss:0.170, val_acc:0.916]
Epoch [15/120    avg_loss:0.176, val_acc:0.904]
Epoch [16/120    avg_loss:0.112, val_acc:0.941]
Epoch [17/120    avg_loss:0.210, val_acc:0.893]
Epoch [18/120    avg_loss:0.169, val_acc:0.915]
Epoch [19/120    avg_loss:0.129, val_acc:0.938]
Epoch [20/120    avg_loss:0.111, val_acc:0.959]
Epoch [21/120    avg_loss:0.118, val_acc:0.939]
Epoch [22/120    avg_loss:0.109, val_acc:0.944]
Epoch [23/120    avg_loss:0.149, val_acc:0.908]
Epoch [24/120    avg_loss:0.073, val_acc:0.944]
Epoch [25/120    avg_loss:0.091, val_acc:0.889]
Epoch [26/120    avg_loss:0.107, val_acc:0.953]
Epoch [27/120    avg_loss:0.078, val_acc:0.950]
Epoch [28/120    avg_loss:0.229, val_acc:0.943]
Epoch [29/120    avg_loss:0.099, val_acc:0.933]
Epoch [30/120    avg_loss:0.079, val_acc:0.962]
Epoch [31/120    avg_loss:0.082, val_acc:0.938]
Epoch [32/120    avg_loss:0.085, val_acc:0.893]
Epoch [33/120    avg_loss:0.055, val_acc:0.936]
Epoch [34/120    avg_loss:0.054, val_acc:0.963]
Epoch [35/120    avg_loss:0.050, val_acc:0.954]
Epoch [36/120    avg_loss:0.031, val_acc:0.957]
Epoch [37/120    avg_loss:0.024, val_acc:0.968]
Epoch [38/120    avg_loss:0.045, val_acc:0.970]
Epoch [39/120    avg_loss:0.023, val_acc:0.971]
Epoch [40/120    avg_loss:0.027, val_acc:0.962]
Epoch [41/120    avg_loss:0.035, val_acc:0.974]
Epoch [42/120    avg_loss:0.052, val_acc:0.927]
Epoch [43/120    avg_loss:0.079, val_acc:0.959]
Epoch [44/120    avg_loss:0.063, val_acc:0.945]
Epoch [45/120    avg_loss:0.058, val_acc:0.899]
Epoch [46/120    avg_loss:0.051, val_acc:0.973]
Epoch [47/120    avg_loss:0.038, val_acc:0.965]
Epoch [48/120    avg_loss:0.024, val_acc:0.965]
Epoch [49/120    avg_loss:0.115, val_acc:0.951]
Epoch [50/120    avg_loss:0.076, val_acc:0.965]
Epoch [51/120    avg_loss:0.024, val_acc:0.968]
Epoch [52/120    avg_loss:0.036, val_acc:0.970]
Epoch [53/120    avg_loss:0.031, val_acc:0.971]
Epoch [54/120    avg_loss:0.033, val_acc:0.975]
Epoch [55/120    avg_loss:0.028, val_acc:0.967]
Epoch [56/120    avg_loss:0.043, val_acc:0.973]
Epoch [57/120    avg_loss:0.069, val_acc:0.961]
Epoch [58/120    avg_loss:0.024, val_acc:0.967]
Epoch [59/120    avg_loss:0.030, val_acc:0.961]
Epoch [60/120    avg_loss:0.026, val_acc:0.971]
Epoch [61/120    avg_loss:0.018, val_acc:0.976]
Epoch [62/120    avg_loss:0.029, val_acc:0.978]
Epoch [63/120    avg_loss:0.022, val_acc:0.974]
Epoch [64/120    avg_loss:0.013, val_acc:0.977]
Epoch [65/120    avg_loss:0.023, val_acc:0.981]
Epoch [66/120    avg_loss:0.014, val_acc:0.984]
Epoch [67/120    avg_loss:0.015, val_acc:0.976]
Epoch [68/120    avg_loss:0.028, val_acc:0.976]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.017, val_acc:0.976]
Epoch [71/120    avg_loss:0.007, val_acc:0.984]
Epoch [72/120    avg_loss:0.023, val_acc:0.981]
Epoch [73/120    avg_loss:0.013, val_acc:0.970]
Epoch [74/120    avg_loss:0.025, val_acc:0.982]
Epoch [75/120    avg_loss:0.012, val_acc:0.983]
Epoch [76/120    avg_loss:0.028, val_acc:0.969]
Epoch [77/120    avg_loss:0.121, val_acc:0.968]
Epoch [78/120    avg_loss:0.027, val_acc:0.974]
Epoch [79/120    avg_loss:0.022, val_acc:0.978]
Epoch [80/120    avg_loss:0.014, val_acc:0.986]
Epoch [81/120    avg_loss:0.004, val_acc:0.985]
Epoch [82/120    avg_loss:0.014, val_acc:0.980]
Epoch [83/120    avg_loss:0.016, val_acc:0.985]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.023, val_acc:0.956]
Epoch [87/120    avg_loss:0.023, val_acc:0.981]
Epoch [88/120    avg_loss:0.020, val_acc:0.984]
Epoch [89/120    avg_loss:0.010, val_acc:0.989]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.064, val_acc:0.977]
Epoch [92/120    avg_loss:0.021, val_acc:0.938]
Epoch [93/120    avg_loss:0.028, val_acc:0.976]
Epoch [94/120    avg_loss:0.019, val_acc:0.959]
Epoch [95/120    avg_loss:0.050, val_acc:0.946]
Epoch [96/120    avg_loss:0.015, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.009, val_acc:0.986]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.990]
Epoch [101/120    avg_loss:0.018, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.025, val_acc:0.988]
Epoch [104/120    avg_loss:0.021, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.042, val_acc:0.979]
Epoch [107/120    avg_loss:0.022, val_acc:0.969]
Epoch [108/120    avg_loss:0.027, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.012, val_acc:0.989]
Epoch [111/120    avg_loss:0.017, val_acc:0.967]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.981]
Epoch [114/120    avg_loss:0.021, val_acc:0.973]
Epoch [115/120    avg_loss:0.015, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.031, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     2     0    12     0     0     0]
 [    0     0 18029     0    61     0     0     0     0     0]
 [    0     0     0  2020     0     0     0     0    14     2]
 [    0    17     0     0  2945     0     1     0     9     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4874     0     0     4]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     3     0     0    75     0    11     0  3478     4]
 [    0     0     0     4    14     9     0     0     0   892]]

Accuracy:
99.40953895837852

F1 scores:
[       nan 0.9973582  0.99831114 0.99507389 0.97050585 0.99579671
 0.99703385 0.99961225 0.98359729 0.97860669]

Kappa:
0.9921812776640261
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ab520a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.374, val_acc:0.657]
Epoch [2/120    avg_loss:0.698, val_acc:0.750]
Epoch [3/120    avg_loss:0.704, val_acc:0.759]
Epoch [4/120    avg_loss:0.508, val_acc:0.782]
Epoch [5/120    avg_loss:0.516, val_acc:0.841]
Epoch [6/120    avg_loss:0.496, val_acc:0.852]
Epoch [7/120    avg_loss:0.351, val_acc:0.850]
Epoch [8/120    avg_loss:0.329, val_acc:0.857]
Epoch [9/120    avg_loss:0.315, val_acc:0.851]
Epoch [10/120    avg_loss:0.290, val_acc:0.890]
Epoch [11/120    avg_loss:0.227, val_acc:0.905]
Epoch [12/120    avg_loss:0.208, val_acc:0.893]
Epoch [13/120    avg_loss:0.206, val_acc:0.931]
Epoch [14/120    avg_loss:0.230, val_acc:0.897]
Epoch [15/120    avg_loss:0.258, val_acc:0.935]
Epoch [16/120    avg_loss:0.142, val_acc:0.930]
Epoch [17/120    avg_loss:0.153, val_acc:0.901]
Epoch [18/120    avg_loss:0.120, val_acc:0.946]
Epoch [19/120    avg_loss:0.142, val_acc:0.875]
Epoch [20/120    avg_loss:0.135, val_acc:0.921]
Epoch [21/120    avg_loss:0.165, val_acc:0.944]
Epoch [22/120    avg_loss:0.104, val_acc:0.929]
Epoch [23/120    avg_loss:0.079, val_acc:0.938]
Epoch [24/120    avg_loss:0.120, val_acc:0.944]
Epoch [25/120    avg_loss:0.073, val_acc:0.926]
Epoch [26/120    avg_loss:0.073, val_acc:0.952]
Epoch [27/120    avg_loss:0.076, val_acc:0.935]
Epoch [28/120    avg_loss:0.069, val_acc:0.942]
Epoch [29/120    avg_loss:0.085, val_acc:0.946]
Epoch [30/120    avg_loss:0.074, val_acc:0.952]
Epoch [31/120    avg_loss:0.188, val_acc:0.910]
Epoch [32/120    avg_loss:0.086, val_acc:0.947]
Epoch [33/120    avg_loss:0.040, val_acc:0.976]
Epoch [34/120    avg_loss:0.060, val_acc:0.943]
Epoch [35/120    avg_loss:0.044, val_acc:0.962]
Epoch [36/120    avg_loss:0.061, val_acc:0.945]
Epoch [37/120    avg_loss:0.033, val_acc:0.944]
Epoch [38/120    avg_loss:0.032, val_acc:0.969]
Epoch [39/120    avg_loss:0.071, val_acc:0.904]
Epoch [40/120    avg_loss:0.082, val_acc:0.950]
Epoch [41/120    avg_loss:0.082, val_acc:0.969]
Epoch [42/120    avg_loss:0.067, val_acc:0.969]
Epoch [43/120    avg_loss:0.056, val_acc:0.951]
Epoch [44/120    avg_loss:0.045, val_acc:0.969]
Epoch [45/120    avg_loss:0.040, val_acc:0.961]
Epoch [46/120    avg_loss:0.055, val_acc:0.973]
Epoch [47/120    avg_loss:0.021, val_acc:0.978]
Epoch [48/120    avg_loss:0.020, val_acc:0.979]
Epoch [49/120    avg_loss:0.023, val_acc:0.977]
Epoch [50/120    avg_loss:0.018, val_acc:0.977]
Epoch [51/120    avg_loss:0.019, val_acc:0.976]
Epoch [52/120    avg_loss:0.016, val_acc:0.974]
Epoch [53/120    avg_loss:0.018, val_acc:0.974]
Epoch [54/120    avg_loss:0.017, val_acc:0.973]
Epoch [55/120    avg_loss:0.018, val_acc:0.975]
Epoch [56/120    avg_loss:0.018, val_acc:0.975]
Epoch [57/120    avg_loss:0.022, val_acc:0.978]
Epoch [58/120    avg_loss:0.025, val_acc:0.980]
Epoch [59/120    avg_loss:0.020, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.980]
Epoch [61/120    avg_loss:0.022, val_acc:0.978]
Epoch [62/120    avg_loss:0.010, val_acc:0.981]
Epoch [63/120    avg_loss:0.020, val_acc:0.978]
Epoch [64/120    avg_loss:0.021, val_acc:0.980]
Epoch [65/120    avg_loss:0.017, val_acc:0.980]
Epoch [66/120    avg_loss:0.024, val_acc:0.981]
Epoch [67/120    avg_loss:0.020, val_acc:0.976]
Epoch [68/120    avg_loss:0.017, val_acc:0.979]
Epoch [69/120    avg_loss:0.010, val_acc:0.979]
Epoch [70/120    avg_loss:0.024, val_acc:0.975]
Epoch [71/120    avg_loss:0.019, val_acc:0.980]
Epoch [72/120    avg_loss:0.024, val_acc:0.980]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.018, val_acc:0.981]
Epoch [75/120    avg_loss:0.015, val_acc:0.981]
Epoch [76/120    avg_loss:0.019, val_acc:0.981]
Epoch [77/120    avg_loss:0.015, val_acc:0.981]
Epoch [78/120    avg_loss:0.013, val_acc:0.981]
Epoch [79/120    avg_loss:0.012, val_acc:0.981]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.018, val_acc:0.981]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.014, val_acc:0.981]
Epoch [84/120    avg_loss:0.020, val_acc:0.982]
Epoch [85/120    avg_loss:0.013, val_acc:0.982]
Epoch [86/120    avg_loss:0.017, val_acc:0.981]
Epoch [87/120    avg_loss:0.008, val_acc:0.981]
Epoch [88/120    avg_loss:0.019, val_acc:0.981]
Epoch [89/120    avg_loss:0.030, val_acc:0.980]
Epoch [90/120    avg_loss:0.022, val_acc:0.980]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.013, val_acc:0.980]
Epoch [93/120    avg_loss:0.012, val_acc:0.980]
Epoch [94/120    avg_loss:0.016, val_acc:0.981]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.014, val_acc:0.980]
Epoch [97/120    avg_loss:0.009, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.980]
Epoch [101/120    avg_loss:0.018, val_acc:0.980]
Epoch [102/120    avg_loss:0.014, val_acc:0.980]
Epoch [103/120    avg_loss:0.023, val_acc:0.980]
Epoch [104/120    avg_loss:0.015, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.013, val_acc:0.980]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.980]
Epoch [109/120    avg_loss:0.015, val_acc:0.980]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.018, val_acc:0.980]
Epoch [113/120    avg_loss:0.016, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.013, val_acc:0.980]
Epoch [116/120    avg_loss:0.009, val_acc:0.980]
Epoch [117/120    avg_loss:0.015, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.980]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     1     4     0     0    14     5    21     0]
 [    0     0 17998     0    81     0    11     0     0     0]
 [    0     0     0  2010     0     0     0     0    18     8]
 [    0    30     0     0  2912     0     0     0    30     0]
 [    0     0     0     0     0  1301     0     0     0     4]
 [    0     0     0     0     0     0  4871     0     0     7]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     0     0     9    35     0     0     0  3520     7]
 [    0     0     0     1    14    17     0     0     0   887]]

Accuracy:
99.23601571349384

F1 scores:
[       nan 0.99416297 0.99742304 0.99014778 0.96840705 0.9919939
 0.99672601 0.99806576 0.98324022 0.96834061]

Kappa:
0.9898875578035332
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fadcf0fe748>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.263, val_acc:0.632]
Epoch [2/120    avg_loss:0.748, val_acc:0.707]
Epoch [3/120    avg_loss:0.686, val_acc:0.741]
Epoch [4/120    avg_loss:0.603, val_acc:0.777]
Epoch [5/120    avg_loss:0.418, val_acc:0.854]
Epoch [6/120    avg_loss:0.445, val_acc:0.844]
Epoch [7/120    avg_loss:0.362, val_acc:0.889]
Epoch [8/120    avg_loss:0.270, val_acc:0.815]
Epoch [9/120    avg_loss:0.317, val_acc:0.897]
Epoch [10/120    avg_loss:0.270, val_acc:0.849]
Epoch [11/120    avg_loss:0.254, val_acc:0.885]
Epoch [12/120    avg_loss:0.261, val_acc:0.921]
Epoch [13/120    avg_loss:0.148, val_acc:0.931]
Epoch [14/120    avg_loss:0.287, val_acc:0.872]
Epoch [15/120    avg_loss:0.244, val_acc:0.921]
Epoch [16/120    avg_loss:0.202, val_acc:0.901]
Epoch [17/120    avg_loss:0.202, val_acc:0.920]
Epoch [18/120    avg_loss:0.198, val_acc:0.925]
Epoch [19/120    avg_loss:0.134, val_acc:0.931]
Epoch [20/120    avg_loss:0.081, val_acc:0.947]
Epoch [21/120    avg_loss:0.110, val_acc:0.947]
Epoch [22/120    avg_loss:0.107, val_acc:0.914]
Epoch [23/120    avg_loss:0.124, val_acc:0.885]
Epoch [24/120    avg_loss:0.235, val_acc:0.926]
Epoch [25/120    avg_loss:0.090, val_acc:0.950]
Epoch [26/120    avg_loss:0.101, val_acc:0.947]
Epoch [27/120    avg_loss:0.121, val_acc:0.940]
Epoch [28/120    avg_loss:0.073, val_acc:0.953]
Epoch [29/120    avg_loss:0.065, val_acc:0.953]
Epoch [30/120    avg_loss:0.065, val_acc:0.959]
Epoch [31/120    avg_loss:0.058, val_acc:0.959]
Epoch [32/120    avg_loss:0.056, val_acc:0.959]
Epoch [33/120    avg_loss:0.061, val_acc:0.961]
Epoch [34/120    avg_loss:0.036, val_acc:0.966]
Epoch [35/120    avg_loss:0.061, val_acc:0.964]
Epoch [36/120    avg_loss:0.060, val_acc:0.948]
Epoch [37/120    avg_loss:0.237, val_acc:0.939]
Epoch [38/120    avg_loss:0.118, val_acc:0.929]
Epoch [39/120    avg_loss:0.090, val_acc:0.960]
Epoch [40/120    avg_loss:0.074, val_acc:0.903]
Epoch [41/120    avg_loss:0.041, val_acc:0.960]
Epoch [42/120    avg_loss:0.082, val_acc:0.952]
Epoch [43/120    avg_loss:0.089, val_acc:0.971]
Epoch [44/120    avg_loss:0.057, val_acc:0.958]
Epoch [45/120    avg_loss:0.049, val_acc:0.949]
Epoch [46/120    avg_loss:0.025, val_acc:0.971]
Epoch [47/120    avg_loss:0.058, val_acc:0.959]
Epoch [48/120    avg_loss:0.075, val_acc:0.938]
Epoch [49/120    avg_loss:0.081, val_acc:0.967]
Epoch [50/120    avg_loss:0.061, val_acc:0.877]
Epoch [51/120    avg_loss:0.066, val_acc:0.952]
Epoch [52/120    avg_loss:0.026, val_acc:0.961]
Epoch [53/120    avg_loss:0.039, val_acc:0.966]
Epoch [54/120    avg_loss:0.024, val_acc:0.967]
Epoch [55/120    avg_loss:0.039, val_acc:0.963]
Epoch [56/120    avg_loss:0.043, val_acc:0.957]
Epoch [57/120    avg_loss:0.054, val_acc:0.973]
Epoch [58/120    avg_loss:0.021, val_acc:0.974]
Epoch [59/120    avg_loss:0.044, val_acc:0.973]
Epoch [60/120    avg_loss:0.019, val_acc:0.975]
Epoch [61/120    avg_loss:0.030, val_acc:0.924]
Epoch [62/120    avg_loss:0.016, val_acc:0.974]
Epoch [63/120    avg_loss:0.042, val_acc:0.971]
Epoch [64/120    avg_loss:0.032, val_acc:0.915]
Epoch [65/120    avg_loss:0.032, val_acc:0.971]
Epoch [66/120    avg_loss:0.021, val_acc:0.965]
Epoch [67/120    avg_loss:0.031, val_acc:0.968]
Epoch [68/120    avg_loss:0.030, val_acc:0.971]
Epoch [69/120    avg_loss:0.023, val_acc:0.979]
Epoch [70/120    avg_loss:0.027, val_acc:0.974]
Epoch [71/120    avg_loss:0.018, val_acc:0.975]
Epoch [72/120    avg_loss:0.012, val_acc:0.974]
Epoch [73/120    avg_loss:0.019, val_acc:0.979]
Epoch [74/120    avg_loss:0.029, val_acc:0.959]
Epoch [75/120    avg_loss:0.023, val_acc:0.972]
Epoch [76/120    avg_loss:0.019, val_acc:0.971]
Epoch [77/120    avg_loss:0.018, val_acc:0.966]
Epoch [78/120    avg_loss:0.042, val_acc:0.948]
Epoch [79/120    avg_loss:0.018, val_acc:0.980]
Epoch [80/120    avg_loss:0.008, val_acc:0.978]
Epoch [81/120    avg_loss:0.009, val_acc:0.974]
Epoch [82/120    avg_loss:0.009, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.011, val_acc:0.978]
Epoch [85/120    avg_loss:0.025, val_acc:0.978]
Epoch [86/120    avg_loss:0.007, val_acc:0.978]
Epoch [87/120    avg_loss:0.013, val_acc:0.957]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.974]
Epoch [90/120    avg_loss:0.017, val_acc:0.975]
Epoch [91/120    avg_loss:0.016, val_acc:0.974]
Epoch [92/120    avg_loss:0.011, val_acc:0.978]
Epoch [93/120    avg_loss:0.017, val_acc:0.974]
Epoch [94/120    avg_loss:0.035, val_acc:0.962]
Epoch [95/120    avg_loss:0.037, val_acc:0.973]
Epoch [96/120    avg_loss:0.017, val_acc:0.974]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.013, val_acc:0.974]
Epoch [99/120    avg_loss:0.026, val_acc:0.975]
Epoch [100/120    avg_loss:0.018, val_acc:0.950]
Epoch [101/120    avg_loss:0.032, val_acc:0.981]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.982]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.017, val_acc:0.981]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     0     3     0     0     1     0     1]
 [    0     0 18008     0    81     0     1     0     0     0]
 [    0     0     0  2009     0     0     0     0    24     3]
 [    0    46     0     0  2899     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4855     0    12    11]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0     9    19     0     0     0  3541     0]
 [    0     0     2     0    14    36     0     0     0   867]]

Accuracy:
99.29144675005423

F1 scores:
[       nan 0.9958937  0.99767313 0.99111988 0.96826987 0.98639456
 0.99753442 0.99883676 0.98703833 0.96173045]

Kappa:
0.9906188402394247
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3a2ad9710>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.294, val_acc:0.534]
Epoch [2/120    avg_loss:0.821, val_acc:0.691]
Epoch [3/120    avg_loss:0.543, val_acc:0.691]
Epoch [4/120    avg_loss:0.621, val_acc:0.782]
Epoch [5/120    avg_loss:0.379, val_acc:0.695]
Epoch [6/120    avg_loss:0.467, val_acc:0.857]
Epoch [7/120    avg_loss:0.319, val_acc:0.885]
Epoch [8/120    avg_loss:0.275, val_acc:0.867]
Epoch [9/120    avg_loss:0.206, val_acc:0.893]
Epoch [10/120    avg_loss:0.298, val_acc:0.877]
Epoch [11/120    avg_loss:0.181, val_acc:0.908]
Epoch [12/120    avg_loss:0.166, val_acc:0.886]
Epoch [13/120    avg_loss:0.286, val_acc:0.906]
Epoch [14/120    avg_loss:0.186, val_acc:0.902]
Epoch [15/120    avg_loss:0.154, val_acc:0.908]
Epoch [16/120    avg_loss:0.121, val_acc:0.918]
Epoch [17/120    avg_loss:0.122, val_acc:0.923]
Epoch [18/120    avg_loss:0.149, val_acc:0.883]
Epoch [19/120    avg_loss:0.121, val_acc:0.928]
Epoch [20/120    avg_loss:0.091, val_acc:0.951]
Epoch [21/120    avg_loss:0.121, val_acc:0.927]
Epoch [22/120    avg_loss:0.126, val_acc:0.928]
Epoch [23/120    avg_loss:0.063, val_acc:0.927]
Epoch [24/120    avg_loss:0.170, val_acc:0.921]
Epoch [25/120    avg_loss:0.148, val_acc:0.921]
Epoch [26/120    avg_loss:0.099, val_acc:0.941]
Epoch [27/120    avg_loss:0.095, val_acc:0.945]
Epoch [28/120    avg_loss:0.048, val_acc:0.938]
Epoch [29/120    avg_loss:0.062, val_acc:0.931]
Epoch [30/120    avg_loss:0.076, val_acc:0.922]
Epoch [31/120    avg_loss:0.104, val_acc:0.964]
Epoch [32/120    avg_loss:0.075, val_acc:0.965]
Epoch [33/120    avg_loss:0.036, val_acc:0.950]
Epoch [34/120    avg_loss:0.037, val_acc:0.964]
Epoch [35/120    avg_loss:0.029, val_acc:0.972]
Epoch [36/120    avg_loss:0.035, val_acc:0.974]
Epoch [37/120    avg_loss:0.058, val_acc:0.955]
Epoch [38/120    avg_loss:0.036, val_acc:0.974]
Epoch [39/120    avg_loss:0.027, val_acc:0.959]
Epoch [40/120    avg_loss:0.026, val_acc:0.961]
Epoch [41/120    avg_loss:0.050, val_acc:0.937]
Epoch [42/120    avg_loss:0.033, val_acc:0.980]
Epoch [43/120    avg_loss:0.018, val_acc:0.975]
Epoch [44/120    avg_loss:0.023, val_acc:0.976]
Epoch [45/120    avg_loss:0.051, val_acc:0.960]
Epoch [46/120    avg_loss:0.016, val_acc:0.977]
Epoch [47/120    avg_loss:0.046, val_acc:0.958]
Epoch [48/120    avg_loss:0.033, val_acc:0.981]
Epoch [49/120    avg_loss:0.013, val_acc:0.980]
Epoch [50/120    avg_loss:0.012, val_acc:0.978]
Epoch [51/120    avg_loss:0.019, val_acc:0.980]
Epoch [52/120    avg_loss:0.013, val_acc:0.980]
Epoch [53/120    avg_loss:0.018, val_acc:0.981]
Epoch [54/120    avg_loss:0.011, val_acc:0.977]
Epoch [55/120    avg_loss:0.013, val_acc:0.982]
Epoch [56/120    avg_loss:0.010, val_acc:0.981]
Epoch [57/120    avg_loss:0.022, val_acc:0.954]
Epoch [58/120    avg_loss:0.030, val_acc:0.976]
Epoch [59/120    avg_loss:0.017, val_acc:0.979]
Epoch [60/120    avg_loss:0.012, val_acc:0.982]
Epoch [61/120    avg_loss:0.017, val_acc:0.979]
Epoch [62/120    avg_loss:0.019, val_acc:0.973]
Epoch [63/120    avg_loss:0.023, val_acc:0.979]
Epoch [64/120    avg_loss:0.010, val_acc:0.981]
Epoch [65/120    avg_loss:0.016, val_acc:0.981]
Epoch [66/120    avg_loss:0.007, val_acc:0.986]
Epoch [67/120    avg_loss:0.005, val_acc:0.978]
Epoch [68/120    avg_loss:0.017, val_acc:0.982]
Epoch [69/120    avg_loss:0.016, val_acc:0.967]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.017, val_acc:0.959]
Epoch [72/120    avg_loss:0.064, val_acc:0.954]
Epoch [73/120    avg_loss:0.098, val_acc:0.943]
Epoch [74/120    avg_loss:0.105, val_acc:0.963]
Epoch [75/120    avg_loss:0.058, val_acc:0.972]
Epoch [76/120    avg_loss:0.028, val_acc:0.952]
Epoch [77/120    avg_loss:0.026, val_acc:0.975]
Epoch [78/120    avg_loss:0.038, val_acc:0.965]
Epoch [79/120    avg_loss:0.032, val_acc:0.981]
Epoch [80/120    avg_loss:0.014, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.982]
Epoch [82/120    avg_loss:0.014, val_acc:0.973]
Epoch [83/120    avg_loss:0.029, val_acc:0.965]
Epoch [84/120    avg_loss:0.018, val_acc:0.980]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.014, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.985]
Epoch [100/120    avg_loss:0.004, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.013, val_acc:0.985]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.984]
Epoch [116/120    avg_loss:0.010, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     0     0     0     9     0     0    33]
 [    0     6 17981     0    84     0    19     0     0     0]
 [    0     0     0  2011     0     0     0     0    14    11]
 [    0    35     1     0  2916     0     0     0    16     4]
 [    0     0     0     0     0  1302     0     0     0     3]
 [    0     0     0     0     0     0  4873     0     2     3]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0     0    38     0     0     0  3528     3]
 [    0     0     0     0     4    53     0     0     0   862]]

Accuracy:
99.17576458679777

F1 scores:
[       nan 0.99339293 0.99695054 0.99382258 0.96973728 0.97894737
 0.99662542 0.9992242  0.98948254 0.93695652]

Kappa:
0.9890922575514618
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4273edc780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.435, val_acc:0.620]
Epoch [2/120    avg_loss:0.869, val_acc:0.727]
Epoch [3/120    avg_loss:0.587, val_acc:0.747]
Epoch [4/120    avg_loss:0.478, val_acc:0.771]
Epoch [5/120    avg_loss:0.613, val_acc:0.779]
Epoch [6/120    avg_loss:0.393, val_acc:0.859]
Epoch [7/120    avg_loss:0.406, val_acc:0.857]
Epoch [8/120    avg_loss:0.267, val_acc:0.902]
Epoch [9/120    avg_loss:0.285, val_acc:0.932]
Epoch [10/120    avg_loss:0.353, val_acc:0.842]
Epoch [11/120    avg_loss:0.308, val_acc:0.888]
Epoch [12/120    avg_loss:0.180, val_acc:0.877]
Epoch [13/120    avg_loss:0.190, val_acc:0.925]
Epoch [14/120    avg_loss:0.232, val_acc:0.917]
Epoch [15/120    avg_loss:0.121, val_acc:0.917]
Epoch [16/120    avg_loss:0.120, val_acc:0.923]
Epoch [17/120    avg_loss:0.158, val_acc:0.902]
Epoch [18/120    avg_loss:0.199, val_acc:0.880]
Epoch [19/120    avg_loss:0.189, val_acc:0.861]
Epoch [20/120    avg_loss:0.133, val_acc:0.947]
Epoch [21/120    avg_loss:0.140, val_acc:0.933]
Epoch [22/120    avg_loss:0.140, val_acc:0.961]
Epoch [23/120    avg_loss:0.298, val_acc:0.844]
Epoch [24/120    avg_loss:0.177, val_acc:0.952]
Epoch [25/120    avg_loss:0.087, val_acc:0.928]
Epoch [26/120    avg_loss:0.092, val_acc:0.953]
Epoch [27/120    avg_loss:0.105, val_acc:0.944]
Epoch [28/120    avg_loss:0.183, val_acc:0.939]
Epoch [29/120    avg_loss:0.116, val_acc:0.946]
Epoch [30/120    avg_loss:0.077, val_acc:0.963]
Epoch [31/120    avg_loss:0.056, val_acc:0.941]
Epoch [32/120    avg_loss:0.068, val_acc:0.942]
Epoch [33/120    avg_loss:0.093, val_acc:0.945]
Epoch [34/120    avg_loss:0.072, val_acc:0.975]
Epoch [35/120    avg_loss:0.051, val_acc:0.974]
Epoch [36/120    avg_loss:0.047, val_acc:0.967]
Epoch [37/120    avg_loss:0.078, val_acc:0.962]
Epoch [38/120    avg_loss:0.061, val_acc:0.949]
Epoch [39/120    avg_loss:0.070, val_acc:0.972]
Epoch [40/120    avg_loss:0.036, val_acc:0.958]
Epoch [41/120    avg_loss:0.047, val_acc:0.970]
Epoch [42/120    avg_loss:0.029, val_acc:0.978]
Epoch [43/120    avg_loss:0.063, val_acc:0.882]
Epoch [44/120    avg_loss:0.054, val_acc:0.967]
Epoch [45/120    avg_loss:0.054, val_acc:0.971]
Epoch [46/120    avg_loss:0.033, val_acc:0.968]
Epoch [47/120    avg_loss:0.061, val_acc:0.957]
Epoch [48/120    avg_loss:0.045, val_acc:0.977]
Epoch [49/120    avg_loss:0.069, val_acc:0.972]
Epoch [50/120    avg_loss:0.043, val_acc:0.973]
Epoch [51/120    avg_loss:0.049, val_acc:0.967]
Epoch [52/120    avg_loss:0.037, val_acc:0.977]
Epoch [53/120    avg_loss:0.021, val_acc:0.977]
Epoch [54/120    avg_loss:0.016, val_acc:0.983]
Epoch [55/120    avg_loss:0.010, val_acc:0.981]
Epoch [56/120    avg_loss:0.028, val_acc:0.979]
Epoch [57/120    avg_loss:0.026, val_acc:0.980]
Epoch [58/120    avg_loss:0.033, val_acc:0.957]
Epoch [59/120    avg_loss:0.016, val_acc:0.980]
Epoch [60/120    avg_loss:0.023, val_acc:0.979]
Epoch [61/120    avg_loss:0.026, val_acc:0.967]
Epoch [62/120    avg_loss:0.025, val_acc:0.974]
Epoch [63/120    avg_loss:0.025, val_acc:0.952]
Epoch [64/120    avg_loss:0.032, val_acc:0.983]
Epoch [65/120    avg_loss:0.009, val_acc:0.987]
Epoch [66/120    avg_loss:0.017, val_acc:0.971]
Epoch [67/120    avg_loss:0.026, val_acc:0.961]
Epoch [68/120    avg_loss:0.015, val_acc:0.981]
Epoch [69/120    avg_loss:0.016, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.021, val_acc:0.980]
Epoch [72/120    avg_loss:0.019, val_acc:0.981]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.983]
Epoch [77/120    avg_loss:0.011, val_acc:0.974]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.143, val_acc:0.955]
Epoch [80/120    avg_loss:0.101, val_acc:0.943]
Epoch [81/120    avg_loss:0.086, val_acc:0.974]
Epoch [82/120    avg_loss:0.030, val_acc:0.972]
Epoch [83/120    avg_loss:0.057, val_acc:0.972]
Epoch [84/120    avg_loss:0.023, val_acc:0.979]
Epoch [85/120    avg_loss:0.014, val_acc:0.980]
Epoch [86/120    avg_loss:0.014, val_acc:0.979]
Epoch [87/120    avg_loss:0.019, val_acc:0.967]
Epoch [88/120    avg_loss:0.015, val_acc:0.982]
Epoch [89/120    avg_loss:0.016, val_acc:0.982]
Epoch [90/120    avg_loss:0.016, val_acc:0.981]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.010, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.985]
Epoch [102/120    avg_loss:0.027, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.013, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.017, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.015, val_acc:0.985]
Epoch [113/120    avg_loss:0.020, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.015, val_acc:0.984]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6430     0     0     2     0     0     0     0     0]
 [    0     0 18016     0    53     0    21     0     0     0]
 [    0     0     0  2021     0     0     0     0    10     5]
 [    0    29     1     0  2921     0     0     0    16     5]
 [    0     1     0     0    52  1248     0     0     0     4]
 [    0     0     0     0     0     0  4866     0     0    12]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     1     0     2    36     0     0     0  3529     3]
 [    0     0     0     0    14    10     0     0     0   895]]

Accuracy:
99.32759742607188

F1 scores:
[       nan 0.99744047 0.99792284 0.99581178 0.96561983 0.97385876
 0.99662058 0.9992242  0.99045748 0.9701897 ]

Kappa:
0.991096537896068
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbb9ed10780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.464, val_acc:0.634]
Epoch [2/120    avg_loss:0.841, val_acc:0.733]
Epoch [3/120    avg_loss:0.640, val_acc:0.766]
Epoch [4/120    avg_loss:0.528, val_acc:0.632]
Epoch [5/120    avg_loss:0.417, val_acc:0.836]
Epoch [6/120    avg_loss:0.476, val_acc:0.715]
Epoch [7/120    avg_loss:0.393, val_acc:0.867]
Epoch [8/120    avg_loss:0.328, val_acc:0.817]
Epoch [9/120    avg_loss:0.272, val_acc:0.864]
Epoch [10/120    avg_loss:0.340, val_acc:0.820]
Epoch [11/120    avg_loss:0.246, val_acc:0.874]
Epoch [12/120    avg_loss:0.229, val_acc:0.905]
Epoch [13/120    avg_loss:0.284, val_acc:0.886]
Epoch [14/120    avg_loss:0.199, val_acc:0.894]
Epoch [15/120    avg_loss:0.166, val_acc:0.929]
Epoch [16/120    avg_loss:0.227, val_acc:0.917]
Epoch [17/120    avg_loss:0.139, val_acc:0.905]
Epoch [18/120    avg_loss:0.118, val_acc:0.916]
Epoch [19/120    avg_loss:0.251, val_acc:0.902]
Epoch [20/120    avg_loss:0.131, val_acc:0.941]
Epoch [21/120    avg_loss:0.123, val_acc:0.918]
Epoch [22/120    avg_loss:0.168, val_acc:0.936]
Epoch [23/120    avg_loss:0.172, val_acc:0.885]
Epoch [24/120    avg_loss:0.111, val_acc:0.951]
Epoch [25/120    avg_loss:0.106, val_acc:0.939]
Epoch [26/120    avg_loss:0.102, val_acc:0.941]
Epoch [27/120    avg_loss:0.102, val_acc:0.952]
Epoch [28/120    avg_loss:0.117, val_acc:0.915]
Epoch [29/120    avg_loss:0.165, val_acc:0.929]
Epoch [30/120    avg_loss:0.164, val_acc:0.943]
Epoch [31/120    avg_loss:0.066, val_acc:0.966]
Epoch [32/120    avg_loss:0.079, val_acc:0.934]
Epoch [33/120    avg_loss:0.126, val_acc:0.931]
Epoch [34/120    avg_loss:0.100, val_acc:0.942]
Epoch [35/120    avg_loss:0.052, val_acc:0.968]
Epoch [36/120    avg_loss:0.050, val_acc:0.972]
Epoch [37/120    avg_loss:0.041, val_acc:0.963]
Epoch [38/120    avg_loss:0.063, val_acc:0.966]
Epoch [39/120    avg_loss:0.067, val_acc:0.962]
Epoch [40/120    avg_loss:0.034, val_acc:0.947]
Epoch [41/120    avg_loss:0.048, val_acc:0.966]
Epoch [42/120    avg_loss:0.070, val_acc:0.971]
Epoch [43/120    avg_loss:0.048, val_acc:0.976]
Epoch [44/120    avg_loss:0.038, val_acc:0.955]
Epoch [45/120    avg_loss:0.035, val_acc:0.963]
Epoch [46/120    avg_loss:0.116, val_acc:0.951]
Epoch [47/120    avg_loss:0.064, val_acc:0.969]
Epoch [48/120    avg_loss:0.048, val_acc:0.946]
Epoch [49/120    avg_loss:0.033, val_acc:0.923]
Epoch [50/120    avg_loss:0.081, val_acc:0.949]
Epoch [51/120    avg_loss:0.057, val_acc:0.981]
Epoch [52/120    avg_loss:0.040, val_acc:0.980]
Epoch [53/120    avg_loss:0.037, val_acc:0.974]
Epoch [54/120    avg_loss:0.038, val_acc:0.980]
Epoch [55/120    avg_loss:0.061, val_acc:0.952]
Epoch [56/120    avg_loss:0.033, val_acc:0.980]
Epoch [57/120    avg_loss:0.027, val_acc:0.986]
Epoch [58/120    avg_loss:0.029, val_acc:0.983]
Epoch [59/120    avg_loss:0.031, val_acc:0.977]
Epoch [60/120    avg_loss:0.054, val_acc:0.967]
Epoch [61/120    avg_loss:0.045, val_acc:0.958]
Epoch [62/120    avg_loss:0.035, val_acc:0.974]
Epoch [63/120    avg_loss:0.033, val_acc:0.981]
Epoch [64/120    avg_loss:0.037, val_acc:0.977]
Epoch [65/120    avg_loss:0.022, val_acc:0.977]
Epoch [66/120    avg_loss:0.028, val_acc:0.958]
Epoch [67/120    avg_loss:0.028, val_acc:0.986]
Epoch [68/120    avg_loss:0.013, val_acc:0.987]
Epoch [69/120    avg_loss:0.057, val_acc:0.982]
Epoch [70/120    avg_loss:0.029, val_acc:0.980]
Epoch [71/120    avg_loss:0.015, val_acc:0.988]
Epoch [72/120    avg_loss:0.021, val_acc:0.979]
Epoch [73/120    avg_loss:0.022, val_acc:0.978]
Epoch [74/120    avg_loss:0.030, val_acc:0.978]
Epoch [75/120    avg_loss:0.029, val_acc:0.980]
Epoch [76/120    avg_loss:0.037, val_acc:0.986]
Epoch [77/120    avg_loss:0.031, val_acc:0.978]
Epoch [78/120    avg_loss:0.083, val_acc:0.929]
Epoch [79/120    avg_loss:0.123, val_acc:0.953]
Epoch [80/120    avg_loss:0.030, val_acc:0.978]
Epoch [81/120    avg_loss:0.025, val_acc:0.981]
Epoch [82/120    avg_loss:0.015, val_acc:0.983]
Epoch [83/120    avg_loss:0.033, val_acc:0.940]
Epoch [84/120    avg_loss:0.052, val_acc:0.976]
Epoch [85/120    avg_loss:0.019, val_acc:0.983]
Epoch [86/120    avg_loss:0.013, val_acc:0.984]
Epoch [87/120    avg_loss:0.018, val_acc:0.984]
Epoch [88/120    avg_loss:0.012, val_acc:0.983]
Epoch [89/120    avg_loss:0.022, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.013, val_acc:0.987]
Epoch [92/120    avg_loss:0.015, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.013, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.989]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.990]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.990]
Epoch [103/120    avg_loss:0.014, val_acc:0.989]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.014, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.991]
Epoch [114/120    avg_loss:0.014, val_acc:0.991]
Epoch [115/120    avg_loss:0.013, val_acc:0.991]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     0     0     0     0     0     3]
 [    0     5 18041     0    34     0     7     0     0     3]
 [    0     0     0  2032     0     0     0     0     1     3]
 [    0    31     6     0  2911     0     0     0    14    10]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4861     0     7    10]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     2     0     7    27     0     0     0  3531     4]
 [    0     0     0     5    14    36     0     0     0   864]]

Accuracy:
99.44568963439616

F1 scores:
[       nan 0.99682146 0.99847801 0.99607843 0.97717355 0.98639456
 0.99753745 0.99961225 0.99129702 0.95101816]

Kappa:
0.992658338664244
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e156ad748>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.286, val_acc:0.682]
Epoch [2/120    avg_loss:0.767, val_acc:0.700]
Epoch [3/120    avg_loss:0.676, val_acc:0.733]
Epoch [4/120    avg_loss:0.519, val_acc:0.782]
Epoch [5/120    avg_loss:0.505, val_acc:0.848]
Epoch [6/120    avg_loss:0.367, val_acc:0.870]
Epoch [7/120    avg_loss:0.430, val_acc:0.679]
Epoch [8/120    avg_loss:0.344, val_acc:0.918]
Epoch [9/120    avg_loss:0.295, val_acc:0.905]
Epoch [10/120    avg_loss:0.242, val_acc:0.832]
Epoch [11/120    avg_loss:0.272, val_acc:0.938]
Epoch [12/120    avg_loss:0.206, val_acc:0.910]
Epoch [13/120    avg_loss:0.242, val_acc:0.796]
Epoch [14/120    avg_loss:0.245, val_acc:0.875]
Epoch [15/120    avg_loss:0.213, val_acc:0.944]
Epoch [16/120    avg_loss:0.153, val_acc:0.889]
Epoch [17/120    avg_loss:0.122, val_acc:0.848]
Epoch [18/120    avg_loss:0.127, val_acc:0.952]
Epoch [19/120    avg_loss:0.102, val_acc:0.920]
Epoch [20/120    avg_loss:0.341, val_acc:0.929]
Epoch [21/120    avg_loss:0.148, val_acc:0.936]
Epoch [22/120    avg_loss:0.095, val_acc:0.960]
Epoch [23/120    avg_loss:0.088, val_acc:0.935]
Epoch [24/120    avg_loss:0.058, val_acc:0.927]
Epoch [25/120    avg_loss:0.092, val_acc:0.945]
Epoch [26/120    avg_loss:0.130, val_acc:0.963]
Epoch [27/120    avg_loss:0.084, val_acc:0.930]
Epoch [28/120    avg_loss:0.107, val_acc:0.964]
Epoch [29/120    avg_loss:0.078, val_acc:0.950]
Epoch [30/120    avg_loss:0.105, val_acc:0.957]
Epoch [31/120    avg_loss:0.094, val_acc:0.980]
Epoch [32/120    avg_loss:0.072, val_acc:0.954]
Epoch [33/120    avg_loss:0.170, val_acc:0.947]
Epoch [34/120    avg_loss:0.100, val_acc:0.976]
Epoch [35/120    avg_loss:0.035, val_acc:0.979]
Epoch [36/120    avg_loss:0.049, val_acc:0.971]
Epoch [37/120    avg_loss:0.057, val_acc:0.975]
Epoch [38/120    avg_loss:0.034, val_acc:0.972]
Epoch [39/120    avg_loss:0.055, val_acc:0.974]
Epoch [40/120    avg_loss:0.026, val_acc:0.969]
Epoch [41/120    avg_loss:0.057, val_acc:0.956]
Epoch [42/120    avg_loss:0.023, val_acc:0.988]
Epoch [43/120    avg_loss:0.060, val_acc:0.957]
Epoch [44/120    avg_loss:0.062, val_acc:0.976]
Epoch [45/120    avg_loss:0.054, val_acc:0.978]
Epoch [46/120    avg_loss:0.145, val_acc:0.974]
Epoch [47/120    avg_loss:0.093, val_acc:0.963]
Epoch [48/120    avg_loss:0.054, val_acc:0.974]
Epoch [49/120    avg_loss:0.028, val_acc:0.961]
Epoch [50/120    avg_loss:0.041, val_acc:0.965]
Epoch [51/120    avg_loss:0.029, val_acc:0.979]
Epoch [52/120    avg_loss:0.033, val_acc:0.973]
Epoch [53/120    avg_loss:0.030, val_acc:0.986]
Epoch [54/120    avg_loss:0.027, val_acc:0.987]
Epoch [55/120    avg_loss:0.022, val_acc:0.986]
Epoch [56/120    avg_loss:0.014, val_acc:0.988]
Epoch [57/120    avg_loss:0.015, val_acc:0.986]
Epoch [58/120    avg_loss:0.018, val_acc:0.986]
Epoch [59/120    avg_loss:0.019, val_acc:0.988]
Epoch [60/120    avg_loss:0.013, val_acc:0.988]
Epoch [61/120    avg_loss:0.011, val_acc:0.987]
Epoch [62/120    avg_loss:0.008, val_acc:0.987]
Epoch [63/120    avg_loss:0.017, val_acc:0.987]
Epoch [64/120    avg_loss:0.010, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.019, val_acc:0.986]
Epoch [67/120    avg_loss:0.021, val_acc:0.987]
Epoch [68/120    avg_loss:0.012, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.987]
Epoch [70/120    avg_loss:0.021, val_acc:0.986]
Epoch [71/120    avg_loss:0.013, val_acc:0.986]
Epoch [72/120    avg_loss:0.014, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.988]
Epoch [74/120    avg_loss:0.012, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.035, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.987]
Epoch [79/120    avg_loss:0.011, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.989]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.009, val_acc:0.989]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.989]
Epoch [88/120    avg_loss:0.011, val_acc:0.990]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.011, val_acc:0.989]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.014, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.017, val_acc:0.987]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.011, val_acc:0.988]
Epoch [99/120    avg_loss:0.017, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.015, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.009, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.012, val_acc:0.988]
Epoch [112/120    avg_loss:0.012, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.010, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.016, val_acc:0.989]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.011, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     3     0    22     0     0     0]
 [    0     0 18037     0    37     0    16     0     0     0]
 [    0     0     0  2014     0     0     0     0    18     4]
 [    0    21     8     0  2911     0     1     0    31     0]
 [    0     0     0     0     0  1299     0     0     0     6]
 [    0     0     1     0     0     0  4858     0     5    14]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     2     0     1    30     0     0     0  3534     4]
 [    0     0     0     2    14    28     0     0     0   875]]

Accuracy:
99.3516978767503

F1 scores:
[       nan 0.99626808 0.99828426 0.99383173 0.97569968 0.98708207
 0.99396419 0.99961225 0.98728873 0.95995612]

Kappa:
0.9914136504358316
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f111cd710>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.488, val_acc:0.667]
Epoch [2/120    avg_loss:0.855, val_acc:0.740]
Epoch [3/120    avg_loss:0.585, val_acc:0.844]
Epoch [4/120    avg_loss:0.638, val_acc:0.695]
Epoch [5/120    avg_loss:0.463, val_acc:0.765]
Epoch [6/120    avg_loss:0.445, val_acc:0.848]
Epoch [7/120    avg_loss:0.340, val_acc:0.835]
Epoch [8/120    avg_loss:0.362, val_acc:0.845]
Epoch [9/120    avg_loss:0.385, val_acc:0.877]
Epoch [10/120    avg_loss:0.225, val_acc:0.859]
Epoch [11/120    avg_loss:0.242, val_acc:0.912]
Epoch [12/120    avg_loss:0.209, val_acc:0.909]
Epoch [13/120    avg_loss:0.305, val_acc:0.925]
Epoch [14/120    avg_loss:0.172, val_acc:0.898]
Epoch [15/120    avg_loss:0.162, val_acc:0.850]
Epoch [16/120    avg_loss:0.166, val_acc:0.928]
Epoch [17/120    avg_loss:0.184, val_acc:0.905]
Epoch [18/120    avg_loss:0.124, val_acc:0.940]
Epoch [19/120    avg_loss:0.133, val_acc:0.934]
Epoch [20/120    avg_loss:0.166, val_acc:0.938]
Epoch [21/120    avg_loss:0.162, val_acc:0.932]
Epoch [22/120    avg_loss:0.196, val_acc:0.945]
Epoch [23/120    avg_loss:0.180, val_acc:0.956]
Epoch [24/120    avg_loss:0.127, val_acc:0.947]
Epoch [25/120    avg_loss:0.134, val_acc:0.950]
Epoch [26/120    avg_loss:0.097, val_acc:0.959]
Epoch [27/120    avg_loss:0.056, val_acc:0.958]
Epoch [28/120    avg_loss:0.062, val_acc:0.954]
Epoch [29/120    avg_loss:0.066, val_acc:0.934]
Epoch [30/120    avg_loss:0.044, val_acc:0.959]
Epoch [31/120    avg_loss:0.052, val_acc:0.971]
Epoch [32/120    avg_loss:0.060, val_acc:0.959]
Epoch [33/120    avg_loss:0.090, val_acc:0.955]
Epoch [34/120    avg_loss:0.068, val_acc:0.974]
Epoch [35/120    avg_loss:0.040, val_acc:0.940]
Epoch [36/120    avg_loss:0.035, val_acc:0.971]
Epoch [37/120    avg_loss:0.077, val_acc:0.940]
Epoch [38/120    avg_loss:0.068, val_acc:0.976]
Epoch [39/120    avg_loss:0.060, val_acc:0.963]
Epoch [40/120    avg_loss:0.047, val_acc:0.972]
Epoch [41/120    avg_loss:0.038, val_acc:0.976]
Epoch [42/120    avg_loss:0.041, val_acc:0.960]
Epoch [43/120    avg_loss:0.031, val_acc:0.980]
Epoch [44/120    avg_loss:0.021, val_acc:0.970]
Epoch [45/120    avg_loss:0.048, val_acc:0.970]
Epoch [46/120    avg_loss:0.039, val_acc:0.980]
Epoch [47/120    avg_loss:0.071, val_acc:0.962]
Epoch [48/120    avg_loss:0.026, val_acc:0.979]
Epoch [49/120    avg_loss:0.027, val_acc:0.983]
Epoch [50/120    avg_loss:0.030, val_acc:0.980]
Epoch [51/120    avg_loss:0.020, val_acc:0.971]
Epoch [52/120    avg_loss:0.027, val_acc:0.976]
Epoch [53/120    avg_loss:0.028, val_acc:0.981]
Epoch [54/120    avg_loss:0.019, val_acc:0.986]
Epoch [55/120    avg_loss:0.028, val_acc:0.976]
Epoch [56/120    avg_loss:0.016, val_acc:0.976]
Epoch [57/120    avg_loss:0.016, val_acc:0.978]
Epoch [58/120    avg_loss:0.016, val_acc:0.980]
Epoch [59/120    avg_loss:0.016, val_acc:0.986]
Epoch [60/120    avg_loss:0.024, val_acc:0.966]
Epoch [61/120    avg_loss:0.024, val_acc:0.984]
Epoch [62/120    avg_loss:0.026, val_acc:0.976]
Epoch [63/120    avg_loss:0.029, val_acc:0.983]
Epoch [64/120    avg_loss:0.017, val_acc:0.983]
Epoch [65/120    avg_loss:0.009, val_acc:0.985]
Epoch [66/120    avg_loss:0.012, val_acc:0.982]
Epoch [67/120    avg_loss:0.015, val_acc:0.982]
Epoch [68/120    avg_loss:0.006, val_acc:0.986]
Epoch [69/120    avg_loss:0.014, val_acc:0.981]
Epoch [70/120    avg_loss:0.022, val_acc:0.963]
Epoch [71/120    avg_loss:0.016, val_acc:0.982]
Epoch [72/120    avg_loss:0.012, val_acc:0.979]
Epoch [73/120    avg_loss:0.100, val_acc:0.898]
Epoch [74/120    avg_loss:0.070, val_acc:0.971]
Epoch [75/120    avg_loss:0.039, val_acc:0.971]
Epoch [76/120    avg_loss:0.023, val_acc:0.983]
Epoch [77/120    avg_loss:0.020, val_acc:0.977]
Epoch [78/120    avg_loss:0.036, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.926]
Epoch [80/120    avg_loss:0.018, val_acc:0.981]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.015, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.985]
Epoch [84/120    avg_loss:0.013, val_acc:0.984]
Epoch [85/120    avg_loss:0.005, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.017, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.983]
Epoch [91/120    avg_loss:0.014, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.985]
Epoch [93/120    avg_loss:0.010, val_acc:0.985]
Epoch [94/120    avg_loss:0.016, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.014, val_acc:0.985]
Epoch [99/120    avg_loss:0.013, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.004, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.011, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     0 17993     0    94     0     3     0     0     0]
 [    0     0     0  2010     0     0     0     0    15    11]
 [    0    39     0     0  2920     0     0     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0     0    18]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     5     0     0    29     0     0     0  3535     2]
 [    0     0     0     0    14    21     0     0     0   884]]

Accuracy:
99.35892801195382

F1 scores:
[       nan 0.99659126 0.99731175 0.9935739  0.96865152 0.99201824
 0.99784416 0.9992242  0.99102888 0.96296296]

Kappa:
0.99151423087706
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf73a44780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.393, val_acc:0.447]
Epoch [2/120    avg_loss:0.761, val_acc:0.665]
Epoch [3/120    avg_loss:0.629, val_acc:0.778]
Epoch [4/120    avg_loss:0.509, val_acc:0.810]
Epoch [5/120    avg_loss:0.453, val_acc:0.754]
Epoch [6/120    avg_loss:0.454, val_acc:0.816]
Epoch [7/120    avg_loss:0.465, val_acc:0.820]
Epoch [8/120    avg_loss:0.279, val_acc:0.864]
Epoch [9/120    avg_loss:0.357, val_acc:0.789]
Epoch [10/120    avg_loss:0.264, val_acc:0.847]
Epoch [11/120    avg_loss:0.159, val_acc:0.867]
Epoch [12/120    avg_loss:0.223, val_acc:0.854]
Epoch [13/120    avg_loss:0.258, val_acc:0.903]
Epoch [14/120    avg_loss:0.148, val_acc:0.930]
Epoch [15/120    avg_loss:0.171, val_acc:0.920]
Epoch [16/120    avg_loss:0.167, val_acc:0.943]
Epoch [17/120    avg_loss:0.123, val_acc:0.930]
Epoch [18/120    avg_loss:0.089, val_acc:0.937]
Epoch [19/120    avg_loss:0.098, val_acc:0.926]
Epoch [20/120    avg_loss:0.128, val_acc:0.929]
Epoch [21/120    avg_loss:0.104, val_acc:0.929]
Epoch [22/120    avg_loss:0.159, val_acc:0.843]
Epoch [23/120    avg_loss:0.252, val_acc:0.902]
Epoch [24/120    avg_loss:0.118, val_acc:0.947]
Epoch [25/120    avg_loss:0.127, val_acc:0.953]
Epoch [26/120    avg_loss:0.103, val_acc:0.962]
Epoch [27/120    avg_loss:0.132, val_acc:0.954]
Epoch [28/120    avg_loss:0.065, val_acc:0.963]
Epoch [29/120    avg_loss:0.076, val_acc:0.965]
Epoch [30/120    avg_loss:0.072, val_acc:0.942]
Epoch [31/120    avg_loss:0.046, val_acc:0.947]
Epoch [32/120    avg_loss:0.054, val_acc:0.947]
Epoch [33/120    avg_loss:0.081, val_acc:0.958]
Epoch [34/120    avg_loss:0.050, val_acc:0.962]
Epoch [35/120    avg_loss:0.065, val_acc:0.941]
Epoch [36/120    avg_loss:0.066, val_acc:0.967]
Epoch [37/120    avg_loss:0.035, val_acc:0.955]
Epoch [38/120    avg_loss:0.042, val_acc:0.973]
Epoch [39/120    avg_loss:0.031, val_acc:0.969]
Epoch [40/120    avg_loss:0.080, val_acc:0.955]
Epoch [41/120    avg_loss:0.207, val_acc:0.930]
Epoch [42/120    avg_loss:0.079, val_acc:0.914]
Epoch [43/120    avg_loss:0.063, val_acc:0.967]
Epoch [44/120    avg_loss:0.030, val_acc:0.971]
Epoch [45/120    avg_loss:0.043, val_acc:0.968]
Epoch [46/120    avg_loss:0.042, val_acc:0.963]
Epoch [47/120    avg_loss:0.050, val_acc:0.967]
Epoch [48/120    avg_loss:0.024, val_acc:0.955]
Epoch [49/120    avg_loss:0.073, val_acc:0.946]
Epoch [50/120    avg_loss:0.051, val_acc:0.971]
Epoch [51/120    avg_loss:0.041, val_acc:0.976]
Epoch [52/120    avg_loss:0.019, val_acc:0.967]
Epoch [53/120    avg_loss:0.062, val_acc:0.942]
Epoch [54/120    avg_loss:0.088, val_acc:0.969]
Epoch [55/120    avg_loss:0.034, val_acc:0.961]
Epoch [56/120    avg_loss:0.053, val_acc:0.958]
Epoch [57/120    avg_loss:0.029, val_acc:0.969]
Epoch [58/120    avg_loss:0.036, val_acc:0.975]
Epoch [59/120    avg_loss:0.020, val_acc:0.967]
Epoch [60/120    avg_loss:0.023, val_acc:0.968]
Epoch [61/120    avg_loss:0.039, val_acc:0.969]
Epoch [62/120    avg_loss:0.013, val_acc:0.980]
Epoch [63/120    avg_loss:0.015, val_acc:0.969]
Epoch [64/120    avg_loss:0.009, val_acc:0.970]
Epoch [65/120    avg_loss:0.033, val_acc:0.966]
Epoch [66/120    avg_loss:0.025, val_acc:0.982]
Epoch [67/120    avg_loss:0.025, val_acc:0.955]
Epoch [68/120    avg_loss:0.044, val_acc:0.976]
Epoch [69/120    avg_loss:0.010, val_acc:0.979]
Epoch [70/120    avg_loss:0.012, val_acc:0.936]
Epoch [71/120    avg_loss:0.014, val_acc:0.979]
Epoch [72/120    avg_loss:0.035, val_acc:0.942]
Epoch [73/120    avg_loss:0.025, val_acc:0.979]
Epoch [74/120    avg_loss:0.008, val_acc:0.981]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.975]
Epoch [77/120    avg_loss:0.012, val_acc:0.976]
Epoch [78/120    avg_loss:0.018, val_acc:0.977]
Epoch [79/120    avg_loss:0.008, val_acc:0.983]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.970]
Epoch [82/120    avg_loss:0.006, val_acc:0.978]
Epoch [83/120    avg_loss:0.011, val_acc:0.973]
Epoch [84/120    avg_loss:0.011, val_acc:0.979]
Epoch [85/120    avg_loss:0.219, val_acc:0.860]
Epoch [86/120    avg_loss:0.184, val_acc:0.941]
Epoch [87/120    avg_loss:0.065, val_acc:0.961]
Epoch [88/120    avg_loss:0.031, val_acc:0.976]
Epoch [89/120    avg_loss:0.013, val_acc:0.973]
Epoch [90/120    avg_loss:0.068, val_acc:0.960]
Epoch [91/120    avg_loss:0.035, val_acc:0.963]
Epoch [92/120    avg_loss:0.032, val_acc:0.965]
Epoch [93/120    avg_loss:0.020, val_acc:0.977]
Epoch [94/120    avg_loss:0.022, val_acc:0.979]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.007, val_acc:0.977]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.015, val_acc:0.979]
Epoch [100/120    avg_loss:0.006, val_acc:0.978]
Epoch [101/120    avg_loss:0.011, val_acc:0.974]
Epoch [102/120    avg_loss:0.010, val_acc:0.974]
Epoch [103/120    avg_loss:0.008, val_acc:0.976]
Epoch [104/120    avg_loss:0.019, val_acc:0.979]
Epoch [105/120    avg_loss:0.007, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.979]
Epoch [107/120    avg_loss:0.006, val_acc:0.979]
Epoch [108/120    avg_loss:0.006, val_acc:0.978]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.008, val_acc:0.978]
Epoch [113/120    avg_loss:0.012, val_acc:0.979]
Epoch [114/120    avg_loss:0.009, val_acc:0.979]
Epoch [115/120    avg_loss:0.005, val_acc:0.979]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.005, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.006, val_acc:0.979]
Epoch [120/120    avg_loss:0.011, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     3     0    11    15     3     0]
 [    0     0 17976     0    55     0    59     0     0     0]
 [    0     0     0  2019     0     0     0     0    13     4]
 [    0    34     0     0  2924     0     0     0    14     0]
 [    0     0     6     0    17  1268     0     0     0    14]
 [    0     0     0     0     0     0  4865     0     2    11]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0     2     0     6    21     0     0     0  3540     2]
 [    0     0     0     0    14    19     0     0     0   886]]

Accuracy:
99.20468512761188

F1 scores:
[       nan 0.99471557 0.99667332 0.99433637 0.97369297 0.97839506
 0.99154183 0.99227799 0.99118018 0.96252037]

Kappa:
0.9894736876415976
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f910e2d2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.429, val_acc:0.686]
Epoch [2/120    avg_loss:0.973, val_acc:0.668]
Epoch [3/120    avg_loss:0.703, val_acc:0.770]
Epoch [4/120    avg_loss:0.499, val_acc:0.690]
Epoch [5/120    avg_loss:0.455, val_acc:0.763]
Epoch [6/120    avg_loss:0.370, val_acc:0.868]
Epoch [7/120    avg_loss:0.405, val_acc:0.867]
Epoch [8/120    avg_loss:0.334, val_acc:0.865]
Epoch [9/120    avg_loss:0.298, val_acc:0.798]
Epoch [10/120    avg_loss:0.328, val_acc:0.855]
Epoch [11/120    avg_loss:0.333, val_acc:0.884]
Epoch [12/120    avg_loss:0.245, val_acc:0.822]
Epoch [13/120    avg_loss:0.232, val_acc:0.866]
Epoch [14/120    avg_loss:0.244, val_acc:0.898]
Epoch [15/120    avg_loss:0.252, val_acc:0.807]
Epoch [16/120    avg_loss:0.137, val_acc:0.864]
Epoch [17/120    avg_loss:0.185, val_acc:0.909]
Epoch [18/120    avg_loss:0.133, val_acc:0.931]
Epoch [19/120    avg_loss:0.182, val_acc:0.927]
Epoch [20/120    avg_loss:0.152, val_acc:0.874]
Epoch [21/120    avg_loss:0.141, val_acc:0.933]
Epoch [22/120    avg_loss:0.101, val_acc:0.914]
Epoch [23/120    avg_loss:0.106, val_acc:0.911]
Epoch [24/120    avg_loss:0.160, val_acc:0.901]
Epoch [25/120    avg_loss:0.106, val_acc:0.926]
Epoch [26/120    avg_loss:0.095, val_acc:0.935]
Epoch [27/120    avg_loss:0.127, val_acc:0.897]
Epoch [28/120    avg_loss:0.071, val_acc:0.930]
Epoch [29/120    avg_loss:0.103, val_acc:0.938]
Epoch [30/120    avg_loss:0.099, val_acc:0.857]
Epoch [31/120    avg_loss:0.080, val_acc:0.929]
Epoch [32/120    avg_loss:0.065, val_acc:0.933]
Epoch [33/120    avg_loss:0.055, val_acc:0.957]
Epoch [34/120    avg_loss:0.050, val_acc:0.930]
Epoch [35/120    avg_loss:0.056, val_acc:0.949]
Epoch [36/120    avg_loss:0.074, val_acc:0.957]
Epoch [37/120    avg_loss:0.061, val_acc:0.916]
Epoch [38/120    avg_loss:0.044, val_acc:0.959]
Epoch [39/120    avg_loss:0.053, val_acc:0.967]
Epoch [40/120    avg_loss:0.130, val_acc:0.923]
Epoch [41/120    avg_loss:0.104, val_acc:0.956]
Epoch [42/120    avg_loss:0.057, val_acc:0.933]
Epoch [43/120    avg_loss:0.059, val_acc:0.936]
Epoch [44/120    avg_loss:0.060, val_acc:0.964]
Epoch [45/120    avg_loss:0.048, val_acc:0.942]
Epoch [46/120    avg_loss:0.039, val_acc:0.962]
Epoch [47/120    avg_loss:0.040, val_acc:0.948]
Epoch [48/120    avg_loss:0.027, val_acc:0.968]
Epoch [49/120    avg_loss:0.027, val_acc:0.965]
Epoch [50/120    avg_loss:0.024, val_acc:0.959]
Epoch [51/120    avg_loss:0.022, val_acc:0.969]
Epoch [52/120    avg_loss:0.016, val_acc:0.974]
Epoch [53/120    avg_loss:0.019, val_acc:0.976]
Epoch [54/120    avg_loss:0.026, val_acc:0.964]
Epoch [55/120    avg_loss:0.027, val_acc:0.949]
Epoch [56/120    avg_loss:0.043, val_acc:0.974]
Epoch [57/120    avg_loss:0.065, val_acc:0.938]
Epoch [58/120    avg_loss:0.023, val_acc:0.966]
Epoch [59/120    avg_loss:0.025, val_acc:0.975]
Epoch [60/120    avg_loss:0.061, val_acc:0.963]
Epoch [61/120    avg_loss:0.030, val_acc:0.970]
Epoch [62/120    avg_loss:0.022, val_acc:0.963]
Epoch [63/120    avg_loss:0.010, val_acc:0.977]
Epoch [64/120    avg_loss:0.014, val_acc:0.976]
Epoch [65/120    avg_loss:0.010, val_acc:0.976]
Epoch [66/120    avg_loss:0.015, val_acc:0.967]
Epoch [67/120    avg_loss:0.018, val_acc:0.968]
Epoch [68/120    avg_loss:0.013, val_acc:0.974]
Epoch [69/120    avg_loss:0.014, val_acc:0.980]
Epoch [70/120    avg_loss:0.015, val_acc:0.971]
Epoch [71/120    avg_loss:0.030, val_acc:0.946]
Epoch [72/120    avg_loss:0.021, val_acc:0.942]
Epoch [73/120    avg_loss:0.020, val_acc:0.969]
Epoch [74/120    avg_loss:0.013, val_acc:0.974]
Epoch [75/120    avg_loss:0.020, val_acc:0.973]
Epoch [76/120    avg_loss:0.026, val_acc:0.973]
Epoch [77/120    avg_loss:0.017, val_acc:0.970]
Epoch [78/120    avg_loss:0.019, val_acc:0.972]
Epoch [79/120    avg_loss:0.007, val_acc:0.976]
Epoch [80/120    avg_loss:0.028, val_acc:0.941]
Epoch [81/120    avg_loss:0.043, val_acc:0.978]
Epoch [82/120    avg_loss:0.012, val_acc:0.976]
Epoch [83/120    avg_loss:0.010, val_acc:0.980]
Epoch [84/120    avg_loss:0.007, val_acc:0.979]
Epoch [85/120    avg_loss:0.008, val_acc:0.980]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.011, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.980]
Epoch [90/120    avg_loss:0.018, val_acc:0.980]
Epoch [91/120    avg_loss:0.012, val_acc:0.980]
Epoch [92/120    avg_loss:0.008, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.015, val_acc:0.980]
Epoch [95/120    avg_loss:0.011, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.007, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.014, val_acc:0.980]
Epoch [100/120    avg_loss:0.015, val_acc:0.980]
Epoch [101/120    avg_loss:0.004, val_acc:0.979]
Epoch [102/120    avg_loss:0.005, val_acc:0.980]
Epoch [103/120    avg_loss:0.004, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.004, val_acc:0.981]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.007, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.005, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.005, val_acc:0.980]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.003, val_acc:0.980]
Epoch [116/120    avg_loss:0.004, val_acc:0.980]
Epoch [117/120    avg_loss:0.013, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.005, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6415     0     0     8     0     0     9     0     0]
 [    0     0 18021     0    55     0    11     0     3     0]
 [    0     0     0  2017     0     0     0     0    12     7]
 [    0    32     0     0  2926     0     0     0    14     0]
 [    0     0     0     0     0  1243    52     0     0    10]
 [    0     0     0     0     0     0  4859     0     5    14]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     3     0     1    25     0     0     0  3540     2]
 [    0     0     0     2    14     9     0     0     0   894]]

Accuracy:
99.3010869303256

F1 scores:
[       nan 0.99596336 0.99808922 0.99457594 0.97533333 0.97223309
 0.99163265 0.99574797 0.99090273 0.96753247]

Kappa:
0.9907448503620631
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a4d898748>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.405, val_acc:0.621]
Epoch [2/120    avg_loss:0.809, val_acc:0.732]
Epoch [3/120    avg_loss:0.594, val_acc:0.791]
Epoch [4/120    avg_loss:0.685, val_acc:0.732]
Epoch [5/120    avg_loss:0.443, val_acc:0.852]
Epoch [6/120    avg_loss:0.524, val_acc:0.871]
Epoch [7/120    avg_loss:0.355, val_acc:0.853]
Epoch [8/120    avg_loss:0.373, val_acc:0.887]
Epoch [9/120    avg_loss:0.408, val_acc:0.868]
Epoch [10/120    avg_loss:0.369, val_acc:0.805]
Epoch [11/120    avg_loss:0.293, val_acc:0.886]
Epoch [12/120    avg_loss:0.271, val_acc:0.905]
Epoch [13/120    avg_loss:0.217, val_acc:0.928]
Epoch [14/120    avg_loss:0.300, val_acc:0.914]
Epoch [15/120    avg_loss:0.231, val_acc:0.901]
Epoch [16/120    avg_loss:0.232, val_acc:0.918]
Epoch [17/120    avg_loss:0.173, val_acc:0.929]
Epoch [18/120    avg_loss:0.278, val_acc:0.734]
Epoch [19/120    avg_loss:0.232, val_acc:0.918]
Epoch [20/120    avg_loss:0.190, val_acc:0.926]
Epoch [21/120    avg_loss:0.151, val_acc:0.923]
Epoch [22/120    avg_loss:0.153, val_acc:0.941]
Epoch [23/120    avg_loss:0.115, val_acc:0.963]
Epoch [24/120    avg_loss:0.190, val_acc:0.947]
Epoch [25/120    avg_loss:0.103, val_acc:0.960]
Epoch [26/120    avg_loss:0.125, val_acc:0.961]
Epoch [27/120    avg_loss:0.091, val_acc:0.972]
Epoch [28/120    avg_loss:0.088, val_acc:0.972]
Epoch [29/120    avg_loss:0.061, val_acc:0.963]
Epoch [30/120    avg_loss:0.075, val_acc:0.965]
Epoch [31/120    avg_loss:0.089, val_acc:0.902]
Epoch [32/120    avg_loss:0.071, val_acc:0.950]
Epoch [33/120    avg_loss:0.079, val_acc:0.818]
Epoch [34/120    avg_loss:0.108, val_acc:0.966]
Epoch [35/120    avg_loss:0.043, val_acc:0.974]
Epoch [36/120    avg_loss:0.123, val_acc:0.947]
Epoch [37/120    avg_loss:0.140, val_acc:0.966]
Epoch [38/120    avg_loss:0.072, val_acc:0.967]
Epoch [39/120    avg_loss:0.061, val_acc:0.978]
Epoch [40/120    avg_loss:0.073, val_acc:0.931]
Epoch [41/120    avg_loss:0.040, val_acc:0.973]
Epoch [42/120    avg_loss:0.063, val_acc:0.966]
Epoch [43/120    avg_loss:0.072, val_acc:0.967]
Epoch [44/120    avg_loss:0.049, val_acc:0.971]
Epoch [45/120    avg_loss:0.059, val_acc:0.959]
Epoch [46/120    avg_loss:0.079, val_acc:0.958]
Epoch [47/120    avg_loss:0.074, val_acc:0.972]
Epoch [48/120    avg_loss:0.050, val_acc:0.976]
Epoch [49/120    avg_loss:0.046, val_acc:0.931]
Epoch [50/120    avg_loss:0.048, val_acc:0.968]
Epoch [51/120    avg_loss:0.030, val_acc:0.972]
Epoch [52/120    avg_loss:0.030, val_acc:0.980]
Epoch [53/120    avg_loss:0.041, val_acc:0.966]
Epoch [54/120    avg_loss:0.038, val_acc:0.976]
Epoch [55/120    avg_loss:0.029, val_acc:0.974]
Epoch [56/120    avg_loss:0.051, val_acc:0.978]
Epoch [57/120    avg_loss:0.150, val_acc:0.947]
Epoch [58/120    avg_loss:0.071, val_acc:0.971]
Epoch [59/120    avg_loss:0.039, val_acc:0.976]
Epoch [60/120    avg_loss:0.054, val_acc:0.972]
Epoch [61/120    avg_loss:0.076, val_acc:0.970]
Epoch [62/120    avg_loss:0.030, val_acc:0.973]
Epoch [63/120    avg_loss:0.036, val_acc:0.944]
Epoch [64/120    avg_loss:0.059, val_acc:0.958]
Epoch [65/120    avg_loss:0.026, val_acc:0.983]
Epoch [66/120    avg_loss:0.027, val_acc:0.974]
Epoch [67/120    avg_loss:0.010, val_acc:0.985]
Epoch [68/120    avg_loss:0.022, val_acc:0.985]
Epoch [69/120    avg_loss:0.016, val_acc:0.982]
Epoch [70/120    avg_loss:0.054, val_acc:0.976]
Epoch [71/120    avg_loss:0.036, val_acc:0.983]
Epoch [72/120    avg_loss:0.028, val_acc:0.979]
Epoch [73/120    avg_loss:0.024, val_acc:0.981]
Epoch [74/120    avg_loss:0.018, val_acc:0.982]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.024, val_acc:0.981]
Epoch [77/120    avg_loss:0.018, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.980]
Epoch [79/120    avg_loss:0.016, val_acc:0.983]
Epoch [80/120    avg_loss:0.014, val_acc:0.984]
Epoch [81/120    avg_loss:0.019, val_acc:0.958]
Epoch [82/120    avg_loss:0.043, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.017, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.987]
Epoch [94/120    avg_loss:0.038, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.014, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.987]
Epoch [110/120    avg_loss:0.013, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.989]
Epoch [119/120    avg_loss:0.016, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     0     5     0     0     0     0     0]
 [    0     0 18061     0    25     0     3     0     1     0]
 [    0     0     0  2027     0     0     0     0     0     9]
 [    0    37     1     3  2897     0     0     0    30     4]
 [    0     0     0     0     0  1293     0     0     0    12]
 [    0     0     0     0     0     0  4847     0     0    31]
 [    0     0     0     0     0     0     0  1284     0     6]
 [    0     2     0     2    23     0     0     0  3534    10]
 [    0     0     0     0    14     9     0     0     0   896]]

Accuracy:
99.45291976959969

F1 scores:
[       nan 0.99658862 0.99917017 0.99655851 0.97607817 0.99194476
 0.99650493 0.997669   0.99047085 0.94965554]

Kappa:
0.9927533349630726
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ccf8f6748>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.301, val_acc:0.602]
Epoch [2/120    avg_loss:0.826, val_acc:0.783]
Epoch [3/120    avg_loss:0.627, val_acc:0.716]
Epoch [4/120    avg_loss:0.605, val_acc:0.829]
Epoch [5/120    avg_loss:0.455, val_acc:0.881]
Epoch [6/120    avg_loss:0.435, val_acc:0.872]
Epoch [7/120    avg_loss:0.364, val_acc:0.847]
Epoch [8/120    avg_loss:0.363, val_acc:0.861]
Epoch [9/120    avg_loss:0.312, val_acc:0.866]
Epoch [10/120    avg_loss:0.287, val_acc:0.892]
Epoch [11/120    avg_loss:0.303, val_acc:0.903]
Epoch [12/120    avg_loss:0.218, val_acc:0.862]
Epoch [13/120    avg_loss:0.262, val_acc:0.885]
Epoch [14/120    avg_loss:0.204, val_acc:0.928]
Epoch [15/120    avg_loss:0.224, val_acc:0.878]
Epoch [16/120    avg_loss:0.230, val_acc:0.862]
Epoch [17/120    avg_loss:0.248, val_acc:0.936]
Epoch [18/120    avg_loss:0.160, val_acc:0.924]
Epoch [19/120    avg_loss:0.131, val_acc:0.932]
Epoch [20/120    avg_loss:0.112, val_acc:0.913]
Epoch [21/120    avg_loss:0.108, val_acc:0.955]
Epoch [22/120    avg_loss:0.110, val_acc:0.941]
Epoch [23/120    avg_loss:0.111, val_acc:0.931]
Epoch [24/120    avg_loss:0.182, val_acc:0.945]
Epoch [25/120    avg_loss:0.100, val_acc:0.959]
Epoch [26/120    avg_loss:0.112, val_acc:0.923]
Epoch [27/120    avg_loss:0.085, val_acc:0.966]
Epoch [28/120    avg_loss:0.093, val_acc:0.944]
Epoch [29/120    avg_loss:0.073, val_acc:0.966]
Epoch [30/120    avg_loss:0.082, val_acc:0.968]
Epoch [31/120    avg_loss:0.063, val_acc:0.964]
Epoch [32/120    avg_loss:0.051, val_acc:0.974]
Epoch [33/120    avg_loss:0.064, val_acc:0.964]
Epoch [34/120    avg_loss:0.239, val_acc:0.898]
Epoch [35/120    avg_loss:0.113, val_acc:0.947]
Epoch [36/120    avg_loss:0.098, val_acc:0.931]
Epoch [37/120    avg_loss:0.106, val_acc:0.944]
Epoch [38/120    avg_loss:0.067, val_acc:0.962]
Epoch [39/120    avg_loss:0.058, val_acc:0.947]
Epoch [40/120    avg_loss:0.038, val_acc:0.977]
Epoch [41/120    avg_loss:0.048, val_acc:0.964]
Epoch [42/120    avg_loss:0.064, val_acc:0.948]
Epoch [43/120    avg_loss:0.050, val_acc:0.978]
Epoch [44/120    avg_loss:0.044, val_acc:0.972]
Epoch [45/120    avg_loss:0.042, val_acc:0.981]
Epoch [46/120    avg_loss:0.031, val_acc:0.975]
Epoch [47/120    avg_loss:0.056, val_acc:0.978]
Epoch [48/120    avg_loss:0.033, val_acc:0.923]
Epoch [49/120    avg_loss:0.105, val_acc:0.978]
Epoch [50/120    avg_loss:0.025, val_acc:0.976]
Epoch [51/120    avg_loss:0.035, val_acc:0.961]
Epoch [52/120    avg_loss:0.036, val_acc:0.980]
Epoch [53/120    avg_loss:0.019, val_acc:0.975]
Epoch [54/120    avg_loss:0.048, val_acc:0.969]
Epoch [55/120    avg_loss:0.028, val_acc:0.984]
Epoch [56/120    avg_loss:0.023, val_acc:0.979]
Epoch [57/120    avg_loss:0.021, val_acc:0.979]
Epoch [58/120    avg_loss:0.027, val_acc:0.978]
Epoch [59/120    avg_loss:0.017, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.981]
Epoch [61/120    avg_loss:0.023, val_acc:0.983]
Epoch [62/120    avg_loss:0.022, val_acc:0.980]
Epoch [63/120    avg_loss:0.028, val_acc:0.962]
Epoch [64/120    avg_loss:0.022, val_acc:0.985]
Epoch [65/120    avg_loss:0.020, val_acc:0.980]
Epoch [66/120    avg_loss:0.014, val_acc:0.984]
Epoch [67/120    avg_loss:0.008, val_acc:0.987]
Epoch [68/120    avg_loss:0.045, val_acc:0.974]
Epoch [69/120    avg_loss:0.032, val_acc:0.974]
Epoch [70/120    avg_loss:0.073, val_acc:0.966]
Epoch [71/120    avg_loss:0.076, val_acc:0.884]
Epoch [72/120    avg_loss:0.099, val_acc:0.930]
Epoch [73/120    avg_loss:0.139, val_acc:0.952]
Epoch [74/120    avg_loss:0.060, val_acc:0.982]
Epoch [75/120    avg_loss:0.055, val_acc:0.983]
Epoch [76/120    avg_loss:0.025, val_acc:0.982]
Epoch [77/120    avg_loss:0.027, val_acc:0.980]
Epoch [78/120    avg_loss:0.023, val_acc:0.984]
Epoch [79/120    avg_loss:0.022, val_acc:0.978]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.013, val_acc:0.988]
Epoch [82/120    avg_loss:0.011, val_acc:0.991]
Epoch [83/120    avg_loss:0.007, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.007, val_acc:0.991]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.989]
Epoch [88/120    avg_loss:0.007, val_acc:0.989]
Epoch [89/120    avg_loss:0.010, val_acc:0.989]
Epoch [90/120    avg_loss:0.011, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.012, val_acc:0.989]
Epoch [94/120    avg_loss:0.012, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.008, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.989]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.010, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.015, val_acc:0.990]
Epoch [106/120    avg_loss:0.009, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.020, val_acc:0.991]
Epoch [110/120    avg_loss:0.015, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.008, val_acc:0.991]
Epoch [115/120    avg_loss:0.010, val_acc:0.991]
Epoch [116/120    avg_loss:0.007, val_acc:0.991]
Epoch [117/120    avg_loss:0.015, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.011, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     4     0     0    14     0     0]
 [    0     0 18051     0    39     0     0     0     0     0]
 [    0     0     0  2023     0     0     0     0     9     4]
 [    0    19     8     0  2925     0     0     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4821     0    36    18]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     4     0     0    26     0     0     0  3541     0]
 [    0     0     0     1    14    58     0     0     0   846]]

Accuracy:
99.33241751620756

F1 scores:
[       nan 0.99681405 0.99861695 0.99655172 0.97826087 0.97826087
 0.99412311 0.99460293 0.98676327 0.94683828]

Kappa:
0.9911575178462744
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ff0dcf780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.402, val_acc:0.629]
Epoch [2/120    avg_loss:0.872, val_acc:0.732]
Epoch [3/120    avg_loss:0.678, val_acc:0.797]
Epoch [4/120    avg_loss:0.468, val_acc:0.783]
Epoch [5/120    avg_loss:0.492, val_acc:0.815]
Epoch [6/120    avg_loss:0.515, val_acc:0.783]
Epoch [7/120    avg_loss:0.559, val_acc:0.803]
Epoch [8/120    avg_loss:0.328, val_acc:0.895]
Epoch [9/120    avg_loss:0.370, val_acc:0.837]
Epoch [10/120    avg_loss:0.262, val_acc:0.920]
Epoch [11/120    avg_loss:0.256, val_acc:0.916]
Epoch [12/120    avg_loss:0.250, val_acc:0.832]
Epoch [13/120    avg_loss:0.272, val_acc:0.846]
Epoch [14/120    avg_loss:0.179, val_acc:0.912]
Epoch [15/120    avg_loss:0.205, val_acc:0.902]
Epoch [16/120    avg_loss:0.178, val_acc:0.913]
Epoch [17/120    avg_loss:0.157, val_acc:0.905]
Epoch [18/120    avg_loss:0.193, val_acc:0.945]
Epoch [19/120    avg_loss:0.159, val_acc:0.940]
Epoch [20/120    avg_loss:0.120, val_acc:0.886]
Epoch [21/120    avg_loss:0.144, val_acc:0.950]
Epoch [22/120    avg_loss:0.134, val_acc:0.957]
Epoch [23/120    avg_loss:0.079, val_acc:0.948]
Epoch [24/120    avg_loss:0.073, val_acc:0.963]
Epoch [25/120    avg_loss:0.159, val_acc:0.914]
Epoch [26/120    avg_loss:0.155, val_acc:0.943]
Epoch [27/120    avg_loss:0.083, val_acc:0.918]
Epoch [28/120    avg_loss:0.151, val_acc:0.912]
Epoch [29/120    avg_loss:0.099, val_acc:0.970]
Epoch [30/120    avg_loss:0.060, val_acc:0.935]
Epoch [31/120    avg_loss:0.053, val_acc:0.950]
Epoch [32/120    avg_loss:0.052, val_acc:0.956]
Epoch [33/120    avg_loss:0.067, val_acc:0.955]
Epoch [34/120    avg_loss:0.065, val_acc:0.955]
Epoch [35/120    avg_loss:0.155, val_acc:0.925]
Epoch [36/120    avg_loss:0.092, val_acc:0.954]
Epoch [37/120    avg_loss:0.088, val_acc:0.965]
Epoch [38/120    avg_loss:0.083, val_acc:0.930]
Epoch [39/120    avg_loss:0.071, val_acc:0.967]
Epoch [40/120    avg_loss:0.058, val_acc:0.974]
Epoch [41/120    avg_loss:0.041, val_acc:0.970]
Epoch [42/120    avg_loss:0.033, val_acc:0.968]
Epoch [43/120    avg_loss:0.047, val_acc:0.970]
Epoch [44/120    avg_loss:0.039, val_acc:0.963]
Epoch [45/120    avg_loss:0.071, val_acc:0.957]
Epoch [46/120    avg_loss:0.023, val_acc:0.964]
Epoch [47/120    avg_loss:0.033, val_acc:0.959]
Epoch [48/120    avg_loss:0.023, val_acc:0.973]
Epoch [49/120    avg_loss:0.031, val_acc:0.979]
Epoch [50/120    avg_loss:0.031, val_acc:0.982]
Epoch [51/120    avg_loss:0.030, val_acc:0.975]
Epoch [52/120    avg_loss:0.018, val_acc:0.985]
Epoch [53/120    avg_loss:0.022, val_acc:0.975]
Epoch [54/120    avg_loss:0.026, val_acc:0.979]
Epoch [55/120    avg_loss:0.014, val_acc:0.982]
Epoch [56/120    avg_loss:0.024, val_acc:0.977]
Epoch [57/120    avg_loss:0.054, val_acc:0.976]
Epoch [58/120    avg_loss:0.021, val_acc:0.985]
Epoch [59/120    avg_loss:0.021, val_acc:0.980]
Epoch [60/120    avg_loss:0.040, val_acc:0.975]
Epoch [61/120    avg_loss:0.055, val_acc:0.976]
Epoch [62/120    avg_loss:0.073, val_acc:0.979]
Epoch [63/120    avg_loss:0.021, val_acc:0.979]
Epoch [64/120    avg_loss:0.014, val_acc:0.986]
Epoch [65/120    avg_loss:0.021, val_acc:0.987]
Epoch [66/120    avg_loss:0.016, val_acc:0.989]
Epoch [67/120    avg_loss:0.017, val_acc:0.937]
Epoch [68/120    avg_loss:0.014, val_acc:0.987]
Epoch [69/120    avg_loss:0.017, val_acc:0.983]
Epoch [70/120    avg_loss:0.030, val_acc:0.981]
Epoch [71/120    avg_loss:0.027, val_acc:0.984]
Epoch [72/120    avg_loss:0.020, val_acc:0.961]
Epoch [73/120    avg_loss:0.019, val_acc:0.985]
Epoch [74/120    avg_loss:0.014, val_acc:0.990]
Epoch [75/120    avg_loss:0.026, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.990]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.037, val_acc:0.983]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.024, val_acc:0.970]
Epoch [82/120    avg_loss:0.031, val_acc:0.982]
Epoch [83/120    avg_loss:0.036, val_acc:0.975]
Epoch [84/120    avg_loss:0.028, val_acc:0.979]
Epoch [85/120    avg_loss:0.054, val_acc:0.976]
Epoch [86/120    avg_loss:0.039, val_acc:0.978]
Epoch [87/120    avg_loss:0.023, val_acc:0.980]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.989]
Epoch [90/120    avg_loss:0.011, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.014, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.990]
Epoch [95/120    avg_loss:0.015, val_acc:0.989]
Epoch [96/120    avg_loss:0.007, val_acc:0.990]
Epoch [97/120    avg_loss:0.011, val_acc:0.990]
Epoch [98/120    avg_loss:0.011, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.011, val_acc:0.991]
Epoch [101/120    avg_loss:0.005, val_acc:0.991]
Epoch [102/120    avg_loss:0.008, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.992]
Epoch [105/120    avg_loss:0.015, val_acc:0.992]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.017, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.013, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.004, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.008, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.991]
Epoch [116/120    avg_loss:0.014, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.003, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     0     7     0     0     0     0     0]
 [    0     0 17967     0    79     0    44     0     0     0]
 [    0     0     0  2018     0     0     0     0    11     7]
 [    0    33     1     0  2900     0     0     0    28    10]
 [    0     0     0     0     0  1289     0     0     0    16]
 [    0     0     0     0     0     0  4854     0     0    24]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     3     0     0    32     0     0     0  3535     1]
 [    0     0     0     0    13    42     0     0     0   864]]

Accuracy:
99.1492540910515

F1 scores:
[       nan 0.99666486 0.9965611  0.99555994 0.96618357 0.97799697
 0.99304419 0.9992242  0.98950315 0.93760174]

Kappa:
0.9887421576028284
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91797257f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.423, val_acc:0.604]
Epoch [2/120    avg_loss:1.002, val_acc:0.660]
Epoch [3/120    avg_loss:0.693, val_acc:0.739]
Epoch [4/120    avg_loss:0.558, val_acc:0.761]
Epoch [5/120    avg_loss:0.501, val_acc:0.851]
Epoch [6/120    avg_loss:0.408, val_acc:0.799]
Epoch [7/120    avg_loss:0.373, val_acc:0.786]
Epoch [8/120    avg_loss:0.421, val_acc:0.869]
Epoch [9/120    avg_loss:0.435, val_acc:0.826]
Epoch [10/120    avg_loss:0.346, val_acc:0.795]
Epoch [11/120    avg_loss:0.302, val_acc:0.894]
Epoch [12/120    avg_loss:0.317, val_acc:0.902]
Epoch [13/120    avg_loss:0.244, val_acc:0.890]
Epoch [14/120    avg_loss:0.189, val_acc:0.889]
Epoch [15/120    avg_loss:0.172, val_acc:0.874]
Epoch [16/120    avg_loss:0.167, val_acc:0.917]
Epoch [17/120    avg_loss:0.122, val_acc:0.905]
Epoch [18/120    avg_loss:0.134, val_acc:0.926]
Epoch [19/120    avg_loss:0.109, val_acc:0.919]
Epoch [20/120    avg_loss:0.146, val_acc:0.926]
Epoch [21/120    avg_loss:0.172, val_acc:0.926]
Epoch [22/120    avg_loss:0.122, val_acc:0.938]
Epoch [23/120    avg_loss:0.138, val_acc:0.911]
Epoch [24/120    avg_loss:0.142, val_acc:0.936]
Epoch [25/120    avg_loss:0.087, val_acc:0.949]
Epoch [26/120    avg_loss:0.135, val_acc:0.942]
Epoch [27/120    avg_loss:0.114, val_acc:0.949]
Epoch [28/120    avg_loss:0.102, val_acc:0.954]
Epoch [29/120    avg_loss:0.062, val_acc:0.954]
Epoch [30/120    avg_loss:0.113, val_acc:0.932]
Epoch [31/120    avg_loss:0.086, val_acc:0.947]
Epoch [32/120    avg_loss:0.059, val_acc:0.938]
Epoch [33/120    avg_loss:0.072, val_acc:0.964]
Epoch [34/120    avg_loss:0.071, val_acc:0.958]
Epoch [35/120    avg_loss:0.049, val_acc:0.964]
Epoch [36/120    avg_loss:0.055, val_acc:0.957]
Epoch [37/120    avg_loss:0.100, val_acc:0.943]
Epoch [38/120    avg_loss:0.054, val_acc:0.966]
Epoch [39/120    avg_loss:0.038, val_acc:0.958]
Epoch [40/120    avg_loss:0.040, val_acc:0.949]
Epoch [41/120    avg_loss:0.050, val_acc:0.973]
Epoch [42/120    avg_loss:0.137, val_acc:0.958]
Epoch [43/120    avg_loss:0.096, val_acc:0.960]
Epoch [44/120    avg_loss:0.116, val_acc:0.881]
Epoch [45/120    avg_loss:0.064, val_acc:0.958]
Epoch [46/120    avg_loss:0.034, val_acc:0.953]
Epoch [47/120    avg_loss:0.072, val_acc:0.967]
Epoch [48/120    avg_loss:0.026, val_acc:0.979]
Epoch [49/120    avg_loss:0.037, val_acc:0.967]
Epoch [50/120    avg_loss:0.047, val_acc:0.970]
Epoch [51/120    avg_loss:0.033, val_acc:0.962]
Epoch [52/120    avg_loss:0.048, val_acc:0.964]
Epoch [53/120    avg_loss:0.040, val_acc:0.976]
Epoch [54/120    avg_loss:0.026, val_acc:0.964]
Epoch [55/120    avg_loss:0.024, val_acc:0.961]
Epoch [56/120    avg_loss:0.109, val_acc:0.959]
Epoch [57/120    avg_loss:0.152, val_acc:0.913]
Epoch [58/120    avg_loss:0.144, val_acc:0.943]
Epoch [59/120    avg_loss:0.057, val_acc:0.960]
Epoch [60/120    avg_loss:0.030, val_acc:0.973]
Epoch [61/120    avg_loss:0.023, val_acc:0.957]
Epoch [62/120    avg_loss:0.029, val_acc:0.969]
Epoch [63/120    avg_loss:0.025, val_acc:0.972]
Epoch [64/120    avg_loss:0.014, val_acc:0.973]
Epoch [65/120    avg_loss:0.016, val_acc:0.973]
Epoch [66/120    avg_loss:0.014, val_acc:0.975]
Epoch [67/120    avg_loss:0.019, val_acc:0.976]
Epoch [68/120    avg_loss:0.025, val_acc:0.977]
Epoch [69/120    avg_loss:0.026, val_acc:0.976]
Epoch [70/120    avg_loss:0.019, val_acc:0.976]
Epoch [71/120    avg_loss:0.017, val_acc:0.978]
Epoch [72/120    avg_loss:0.024, val_acc:0.977]
Epoch [73/120    avg_loss:0.013, val_acc:0.977]
Epoch [74/120    avg_loss:0.026, val_acc:0.978]
Epoch [75/120    avg_loss:0.013, val_acc:0.978]
Epoch [76/120    avg_loss:0.013, val_acc:0.978]
Epoch [77/120    avg_loss:0.010, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.017, val_acc:0.978]
Epoch [80/120    avg_loss:0.013, val_acc:0.977]
Epoch [81/120    avg_loss:0.017, val_acc:0.977]
Epoch [82/120    avg_loss:0.011, val_acc:0.977]
Epoch [83/120    avg_loss:0.011, val_acc:0.977]
Epoch [84/120    avg_loss:0.018, val_acc:0.977]
Epoch [85/120    avg_loss:0.013, val_acc:0.977]
Epoch [86/120    avg_loss:0.011, val_acc:0.977]
Epoch [87/120    avg_loss:0.013, val_acc:0.977]
Epoch [88/120    avg_loss:0.019, val_acc:0.977]
Epoch [89/120    avg_loss:0.016, val_acc:0.977]
Epoch [90/120    avg_loss:0.033, val_acc:0.977]
Epoch [91/120    avg_loss:0.017, val_acc:0.977]
Epoch [92/120    avg_loss:0.013, val_acc:0.977]
Epoch [93/120    avg_loss:0.017, val_acc:0.977]
Epoch [94/120    avg_loss:0.042, val_acc:0.977]
Epoch [95/120    avg_loss:0.023, val_acc:0.977]
Epoch [96/120    avg_loss:0.009, val_acc:0.977]
Epoch [97/120    avg_loss:0.018, val_acc:0.977]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.011, val_acc:0.977]
Epoch [100/120    avg_loss:0.014, val_acc:0.977]
Epoch [101/120    avg_loss:0.011, val_acc:0.977]
Epoch [102/120    avg_loss:0.012, val_acc:0.977]
Epoch [103/120    avg_loss:0.013, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.977]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.015, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.977]
Epoch [108/120    avg_loss:0.015, val_acc:0.977]
Epoch [109/120    avg_loss:0.016, val_acc:0.977]
Epoch [110/120    avg_loss:0.015, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.977]
Epoch [112/120    avg_loss:0.018, val_acc:0.977]
Epoch [113/120    avg_loss:0.017, val_acc:0.977]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.016, val_acc:0.977]
Epoch [116/120    avg_loss:0.016, val_acc:0.977]
Epoch [117/120    avg_loss:0.020, val_acc:0.977]
Epoch [118/120    avg_loss:0.020, val_acc:0.977]
Epoch [119/120    avg_loss:0.012, val_acc:0.977]
Epoch [120/120    avg_loss:0.022, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     0     0     0     6     0     0]
 [    0     0 18032     0    58     0     0     0     0     0]
 [    0     1     0  2035     0     0     0     0     0     0]
 [    0    24    12     0  2905     0     1     0    30     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0    10     0     0     0  4830     0    19    19]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     1     0     0    27     0     0     0  3543     0]
 [    0     0     0     4    14    13     0     0     0   888]]

Accuracy:
99.41194900344637

F1 scores:
[       nan 0.9975163  0.99778663 0.99877301 0.97222222 0.99427699
 0.99495314 0.99651568 0.98925031 0.96996177]

Kappa:
0.992211349176321
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99d0645748>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.514, val_acc:0.628]
Epoch [2/120    avg_loss:0.865, val_acc:0.519]
Epoch [3/120    avg_loss:0.626, val_acc:0.705]
Epoch [4/120    avg_loss:0.550, val_acc:0.818]
Epoch [5/120    avg_loss:0.518, val_acc:0.779]
Epoch [6/120    avg_loss:0.369, val_acc:0.854]
Epoch [7/120    avg_loss:0.426, val_acc:0.882]
Epoch [8/120    avg_loss:0.360, val_acc:0.870]
Epoch [9/120    avg_loss:0.285, val_acc:0.879]
Epoch [10/120    avg_loss:0.282, val_acc:0.912]
Epoch [11/120    avg_loss:0.284, val_acc:0.899]
Epoch [12/120    avg_loss:0.310, val_acc:0.898]
Epoch [13/120    avg_loss:0.201, val_acc:0.884]
Epoch [14/120    avg_loss:0.227, val_acc:0.912]
Epoch [15/120    avg_loss:0.151, val_acc:0.913]
Epoch [16/120    avg_loss:0.323, val_acc:0.891]
Epoch [17/120    avg_loss:0.271, val_acc:0.908]
Epoch [18/120    avg_loss:0.243, val_acc:0.914]
Epoch [19/120    avg_loss:0.157, val_acc:0.927]
Epoch [20/120    avg_loss:0.180, val_acc:0.937]
Epoch [21/120    avg_loss:0.162, val_acc:0.939]
Epoch [22/120    avg_loss:0.173, val_acc:0.938]
Epoch [23/120    avg_loss:0.182, val_acc:0.933]
Epoch [24/120    avg_loss:0.112, val_acc:0.932]
Epoch [25/120    avg_loss:0.174, val_acc:0.940]
Epoch [26/120    avg_loss:0.142, val_acc:0.907]
Epoch [27/120    avg_loss:0.107, val_acc:0.919]
Epoch [28/120    avg_loss:0.116, val_acc:0.935]
Epoch [29/120    avg_loss:0.107, val_acc:0.952]
Epoch [30/120    avg_loss:0.076, val_acc:0.962]
Epoch [31/120    avg_loss:0.085, val_acc:0.932]
Epoch [32/120    avg_loss:0.139, val_acc:0.867]
Epoch [33/120    avg_loss:0.110, val_acc:0.944]
Epoch [34/120    avg_loss:0.070, val_acc:0.947]
Epoch [35/120    avg_loss:0.137, val_acc:0.954]
Epoch [36/120    avg_loss:0.152, val_acc:0.953]
Epoch [37/120    avg_loss:0.121, val_acc:0.920]
Epoch [38/120    avg_loss:0.082, val_acc:0.958]
Epoch [39/120    avg_loss:0.045, val_acc:0.955]
Epoch [40/120    avg_loss:0.060, val_acc:0.962]
Epoch [41/120    avg_loss:0.069, val_acc:0.971]
Epoch [42/120    avg_loss:0.035, val_acc:0.978]
Epoch [43/120    avg_loss:0.036, val_acc:0.972]
Epoch [44/120    avg_loss:0.037, val_acc:0.980]
Epoch [45/120    avg_loss:0.055, val_acc:0.984]
Epoch [46/120    avg_loss:0.041, val_acc:0.967]
Epoch [47/120    avg_loss:0.037, val_acc:0.980]
Epoch [48/120    avg_loss:0.043, val_acc:0.938]
Epoch [49/120    avg_loss:0.022, val_acc:0.983]
Epoch [50/120    avg_loss:0.035, val_acc:0.984]
Epoch [51/120    avg_loss:0.026, val_acc:0.985]
Epoch [52/120    avg_loss:0.038, val_acc:0.977]
Epoch [53/120    avg_loss:0.031, val_acc:0.988]
Epoch [54/120    avg_loss:0.029, val_acc:0.985]
Epoch [55/120    avg_loss:0.014, val_acc:0.987]
Epoch [56/120    avg_loss:0.017, val_acc:0.980]
Epoch [57/120    avg_loss:0.019, val_acc:0.980]
Epoch [58/120    avg_loss:0.021, val_acc:0.983]
Epoch [59/120    avg_loss:0.032, val_acc:0.892]
Epoch [60/120    avg_loss:0.016, val_acc:0.985]
Epoch [61/120    avg_loss:0.012, val_acc:0.985]
Epoch [62/120    avg_loss:0.025, val_acc:0.985]
Epoch [63/120    avg_loss:0.015, val_acc:0.978]
Epoch [64/120    avg_loss:0.031, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.989]
Epoch [66/120    avg_loss:0.119, val_acc:0.914]
Epoch [67/120    avg_loss:0.085, val_acc:0.964]
Epoch [68/120    avg_loss:0.062, val_acc:0.967]
Epoch [69/120    avg_loss:0.038, val_acc:0.954]
Epoch [70/120    avg_loss:0.139, val_acc:0.947]
Epoch [71/120    avg_loss:0.097, val_acc:0.961]
Epoch [72/120    avg_loss:0.069, val_acc:0.973]
Epoch [73/120    avg_loss:0.026, val_acc:0.970]
Epoch [74/120    avg_loss:0.052, val_acc:0.972]
Epoch [75/120    avg_loss:0.026, val_acc:0.983]
Epoch [76/120    avg_loss:0.030, val_acc:0.976]
Epoch [77/120    avg_loss:0.017, val_acc:0.986]
Epoch [78/120    avg_loss:0.021, val_acc:0.973]
Epoch [79/120    avg_loss:0.019, val_acc:0.990]
Epoch [80/120    avg_loss:0.029, val_acc:0.988]
Epoch [81/120    avg_loss:0.020, val_acc:0.988]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.008, val_acc:0.990]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.015, val_acc:0.988]
Epoch [86/120    avg_loss:0.014, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.989]
Epoch [88/120    avg_loss:0.010, val_acc:0.990]
Epoch [89/120    avg_loss:0.013, val_acc:0.990]
Epoch [90/120    avg_loss:0.015, val_acc:0.989]
Epoch [91/120    avg_loss:0.009, val_acc:0.990]
Epoch [92/120    avg_loss:0.009, val_acc:0.991]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.015, val_acc:0.988]
Epoch [95/120    avg_loss:0.013, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.991]
Epoch [97/120    avg_loss:0.011, val_acc:0.991]
Epoch [98/120    avg_loss:0.007, val_acc:0.991]
Epoch [99/120    avg_loss:0.016, val_acc:0.991]
Epoch [100/120    avg_loss:0.013, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.012, val_acc:0.991]
Epoch [103/120    avg_loss:0.012, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.012, val_acc:0.991]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.008, val_acc:0.991]
Epoch [109/120    avg_loss:0.008, val_acc:0.991]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.991]
Epoch [112/120    avg_loss:0.009, val_acc:0.991]
Epoch [113/120    avg_loss:0.007, val_acc:0.991]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.991]
Epoch [118/120    avg_loss:0.012, val_acc:0.991]
Epoch [119/120    avg_loss:0.009, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     7 17936     0   105     0    41     0     1     0]
 [    0     0     0  2027     0     0     0     0     7     2]
 [    0    16     0     0  2922     0     0     0    34     0]
 [    0     0     0    20     0  1204     0     0     0    81]
 [    0     0     0     0     0     0  4865     0     1    12]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0     0    26     0     0     0  3543     0]
 [    0     0     0     0    14     8     0     0     0   897]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.99806036 0.99572531 0.99289738 0.96770989 0.95669448
 0.99448078 1.         0.99007964 0.93877551]

Kappa:
0.9879808805281787
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e12890780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.351, val_acc:0.701]
Epoch [2/120    avg_loss:0.827, val_acc:0.742]
Epoch [3/120    avg_loss:0.734, val_acc:0.616]
Epoch [4/120    avg_loss:0.702, val_acc:0.589]
Epoch [5/120    avg_loss:0.609, val_acc:0.777]
Epoch [6/120    avg_loss:0.424, val_acc:0.839]
Epoch [7/120    avg_loss:0.433, val_acc:0.797]
Epoch [8/120    avg_loss:0.396, val_acc:0.820]
Epoch [9/120    avg_loss:0.274, val_acc:0.888]
Epoch [10/120    avg_loss:0.276, val_acc:0.823]
Epoch [11/120    avg_loss:0.292, val_acc:0.863]
Epoch [12/120    avg_loss:0.247, val_acc:0.811]
Epoch [13/120    avg_loss:0.326, val_acc:0.887]
Epoch [14/120    avg_loss:0.216, val_acc:0.918]
Epoch [15/120    avg_loss:0.269, val_acc:0.851]
Epoch [16/120    avg_loss:0.239, val_acc:0.909]
Epoch [17/120    avg_loss:0.248, val_acc:0.878]
Epoch [18/120    avg_loss:0.208, val_acc:0.950]
Epoch [19/120    avg_loss:0.128, val_acc:0.944]
Epoch [20/120    avg_loss:0.097, val_acc:0.957]
Epoch [21/120    avg_loss:0.159, val_acc:0.932]
Epoch [22/120    avg_loss:0.244, val_acc:0.926]
Epoch [23/120    avg_loss:0.156, val_acc:0.944]
Epoch [24/120    avg_loss:0.174, val_acc:0.937]
Epoch [25/120    avg_loss:0.242, val_acc:0.928]
Epoch [26/120    avg_loss:0.086, val_acc:0.938]
Epoch [27/120    avg_loss:0.069, val_acc:0.934]
Epoch [28/120    avg_loss:0.090, val_acc:0.921]
Epoch [29/120    avg_loss:0.060, val_acc:0.951]
Epoch [30/120    avg_loss:0.137, val_acc:0.909]
Epoch [31/120    avg_loss:0.117, val_acc:0.896]
Epoch [32/120    avg_loss:0.076, val_acc:0.943]
Epoch [33/120    avg_loss:0.067, val_acc:0.954]
Epoch [34/120    avg_loss:0.041, val_acc:0.961]
Epoch [35/120    avg_loss:0.041, val_acc:0.966]
Epoch [36/120    avg_loss:0.042, val_acc:0.964]
Epoch [37/120    avg_loss:0.033, val_acc:0.968]
Epoch [38/120    avg_loss:0.041, val_acc:0.968]
Epoch [39/120    avg_loss:0.030, val_acc:0.965]
Epoch [40/120    avg_loss:0.040, val_acc:0.970]
Epoch [41/120    avg_loss:0.071, val_acc:0.970]
Epoch [42/120    avg_loss:0.044, val_acc:0.966]
Epoch [43/120    avg_loss:0.029, val_acc:0.972]
Epoch [44/120    avg_loss:0.027, val_acc:0.971]
Epoch [45/120    avg_loss:0.042, val_acc:0.972]
Epoch [46/120    avg_loss:0.032, val_acc:0.977]
Epoch [47/120    avg_loss:0.023, val_acc:0.973]
Epoch [48/120    avg_loss:0.031, val_acc:0.970]
Epoch [49/120    avg_loss:0.031, val_acc:0.975]
Epoch [50/120    avg_loss:0.043, val_acc:0.969]
Epoch [51/120    avg_loss:0.027, val_acc:0.971]
Epoch [52/120    avg_loss:0.028, val_acc:0.971]
Epoch [53/120    avg_loss:0.026, val_acc:0.970]
Epoch [54/120    avg_loss:0.031, val_acc:0.971]
Epoch [55/120    avg_loss:0.026, val_acc:0.973]
Epoch [56/120    avg_loss:0.022, val_acc:0.974]
Epoch [57/120    avg_loss:0.025, val_acc:0.973]
Epoch [58/120    avg_loss:0.031, val_acc:0.979]
Epoch [59/120    avg_loss:0.030, val_acc:0.975]
Epoch [60/120    avg_loss:0.026, val_acc:0.973]
Epoch [61/120    avg_loss:0.025, val_acc:0.977]
Epoch [62/120    avg_loss:0.029, val_acc:0.978]
Epoch [63/120    avg_loss:0.032, val_acc:0.979]
Epoch [64/120    avg_loss:0.029, val_acc:0.977]
Epoch [65/120    avg_loss:0.028, val_acc:0.979]
Epoch [66/120    avg_loss:0.017, val_acc:0.977]
Epoch [67/120    avg_loss:0.029, val_acc:0.977]
Epoch [68/120    avg_loss:0.029, val_acc:0.980]
Epoch [69/120    avg_loss:0.017, val_acc:0.981]
Epoch [70/120    avg_loss:0.023, val_acc:0.977]
Epoch [71/120    avg_loss:0.016, val_acc:0.976]
Epoch [72/120    avg_loss:0.028, val_acc:0.979]
Epoch [73/120    avg_loss:0.024, val_acc:0.979]
Epoch [74/120    avg_loss:0.022, val_acc:0.979]
Epoch [75/120    avg_loss:0.018, val_acc:0.980]
Epoch [76/120    avg_loss:0.013, val_acc:0.981]
Epoch [77/120    avg_loss:0.014, val_acc:0.979]
Epoch [78/120    avg_loss:0.021, val_acc:0.978]
Epoch [79/120    avg_loss:0.023, val_acc:0.979]
Epoch [80/120    avg_loss:0.019, val_acc:0.982]
Epoch [81/120    avg_loss:0.030, val_acc:0.978]
Epoch [82/120    avg_loss:0.021, val_acc:0.979]
Epoch [83/120    avg_loss:0.026, val_acc:0.979]
Epoch [84/120    avg_loss:0.031, val_acc:0.979]
Epoch [85/120    avg_loss:0.017, val_acc:0.982]
Epoch [86/120    avg_loss:0.027, val_acc:0.977]
Epoch [87/120    avg_loss:0.017, val_acc:0.982]
Epoch [88/120    avg_loss:0.023, val_acc:0.981]
Epoch [89/120    avg_loss:0.025, val_acc:0.977]
Epoch [90/120    avg_loss:0.017, val_acc:0.979]
Epoch [91/120    avg_loss:0.018, val_acc:0.979]
Epoch [92/120    avg_loss:0.035, val_acc:0.977]
Epoch [93/120    avg_loss:0.020, val_acc:0.978]
Epoch [94/120    avg_loss:0.020, val_acc:0.978]
Epoch [95/120    avg_loss:0.015, val_acc:0.982]
Epoch [96/120    avg_loss:0.015, val_acc:0.983]
Epoch [97/120    avg_loss:0.014, val_acc:0.984]
Epoch [98/120    avg_loss:0.022, val_acc:0.979]
Epoch [99/120    avg_loss:0.026, val_acc:0.976]
Epoch [100/120    avg_loss:0.018, val_acc:0.978]
Epoch [101/120    avg_loss:0.022, val_acc:0.980]
Epoch [102/120    avg_loss:0.015, val_acc:0.981]
Epoch [103/120    avg_loss:0.017, val_acc:0.980]
Epoch [104/120    avg_loss:0.020, val_acc:0.980]
Epoch [105/120    avg_loss:0.015, val_acc:0.981]
Epoch [106/120    avg_loss:0.021, val_acc:0.983]
Epoch [107/120    avg_loss:0.030, val_acc:0.980]
Epoch [108/120    avg_loss:0.016, val_acc:0.980]
Epoch [109/120    avg_loss:0.016, val_acc:0.981]
Epoch [110/120    avg_loss:0.022, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.978]
Epoch [112/120    avg_loss:0.023, val_acc:0.979]
Epoch [113/120    avg_loss:0.021, val_acc:0.980]
Epoch [114/120    avg_loss:0.017, val_acc:0.981]
Epoch [115/120    avg_loss:0.012, val_acc:0.981]
Epoch [116/120    avg_loss:0.020, val_acc:0.981]
Epoch [117/120    avg_loss:0.018, val_acc:0.981]
Epoch [118/120    avg_loss:0.021, val_acc:0.981]
Epoch [119/120    avg_loss:0.022, val_acc:0.981]
Epoch [120/120    avg_loss:0.016, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     0 17923     0    80     0    87     0     0     0]
 [    0     0     0  2033     0     0     0     0     0     3]
 [    0    32     0     0  2896     0     1     0    33    10]
 [    0     7     0     0     0  1169   129     0     0     0]
 [    0     0     0     0     0     0  4860     0     9     9]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0     0    23     0     0     0  3544     2]
 [    0     0     0     0    14    34     0     0     0   871]]

Accuracy:
98.850408502639

F1 scores:
[       nan 0.99682294 0.99536279 0.99926272 0.96775272 0.93221691
 0.97639377 0.9992242  0.99035909 0.9592511 ]

Kappa:
0.984787978358668
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff83e148748>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.447, val_acc:0.654]
Epoch [2/120    avg_loss:0.889, val_acc:0.734]
Epoch [3/120    avg_loss:0.649, val_acc:0.685]
Epoch [4/120    avg_loss:0.661, val_acc:0.753]
Epoch [5/120    avg_loss:0.502, val_acc:0.770]
Epoch [6/120    avg_loss:0.461, val_acc:0.844]
Epoch [7/120    avg_loss:0.450, val_acc:0.861]
Epoch [8/120    avg_loss:0.360, val_acc:0.871]
Epoch [9/120    avg_loss:0.344, val_acc:0.910]
Epoch [10/120    avg_loss:0.351, val_acc:0.872]
Epoch [11/120    avg_loss:0.246, val_acc:0.932]
Epoch [12/120    avg_loss:0.264, val_acc:0.923]
Epoch [13/120    avg_loss:0.253, val_acc:0.923]
Epoch [14/120    avg_loss:0.249, val_acc:0.918]
Epoch [15/120    avg_loss:0.214, val_acc:0.947]
Epoch [16/120    avg_loss:0.156, val_acc:0.956]
Epoch [17/120    avg_loss:0.196, val_acc:0.941]
Epoch [18/120    avg_loss:0.170, val_acc:0.928]
Epoch [19/120    avg_loss:0.149, val_acc:0.931]
Epoch [20/120    avg_loss:0.117, val_acc:0.948]
Epoch [21/120    avg_loss:0.257, val_acc:0.930]
Epoch [22/120    avg_loss:0.151, val_acc:0.960]
Epoch [23/120    avg_loss:0.095, val_acc:0.965]
Epoch [24/120    avg_loss:0.094, val_acc:0.955]
Epoch [25/120    avg_loss:0.088, val_acc:0.945]
Epoch [26/120    avg_loss:0.110, val_acc:0.921]
Epoch [27/120    avg_loss:0.129, val_acc:0.910]
Epoch [28/120    avg_loss:0.248, val_acc:0.955]
Epoch [29/120    avg_loss:0.140, val_acc:0.959]
Epoch [30/120    avg_loss:0.095, val_acc:0.951]
Epoch [31/120    avg_loss:0.090, val_acc:0.976]
Epoch [32/120    avg_loss:0.054, val_acc:0.964]
Epoch [33/120    avg_loss:0.046, val_acc:0.967]
Epoch [34/120    avg_loss:0.056, val_acc:0.957]
Epoch [35/120    avg_loss:0.072, val_acc:0.957]
Epoch [36/120    avg_loss:0.049, val_acc:0.978]
Epoch [37/120    avg_loss:0.062, val_acc:0.954]
Epoch [38/120    avg_loss:0.051, val_acc:0.975]
Epoch [39/120    avg_loss:0.058, val_acc:0.956]
Epoch [40/120    avg_loss:0.042, val_acc:0.976]
Epoch [41/120    avg_loss:0.066, val_acc:0.981]
Epoch [42/120    avg_loss:0.079, val_acc:0.974]
Epoch [43/120    avg_loss:0.120, val_acc:0.957]
Epoch [44/120    avg_loss:0.084, val_acc:0.952]
Epoch [45/120    avg_loss:0.079, val_acc:0.975]
Epoch [46/120    avg_loss:0.047, val_acc:0.979]
Epoch [47/120    avg_loss:0.051, val_acc:0.971]
Epoch [48/120    avg_loss:0.031, val_acc:0.980]
Epoch [49/120    avg_loss:0.062, val_acc:0.976]
Epoch [50/120    avg_loss:0.033, val_acc:0.970]
Epoch [51/120    avg_loss:0.107, val_acc:0.942]
Epoch [52/120    avg_loss:0.055, val_acc:0.966]
Epoch [53/120    avg_loss:0.052, val_acc:0.965]
Epoch [54/120    avg_loss:0.045, val_acc:0.982]
Epoch [55/120    avg_loss:0.031, val_acc:0.984]
Epoch [56/120    avg_loss:0.065, val_acc:0.975]
Epoch [57/120    avg_loss:0.030, val_acc:0.967]
Epoch [58/120    avg_loss:0.035, val_acc:0.981]
Epoch [59/120    avg_loss:0.030, val_acc:0.976]
Epoch [60/120    avg_loss:0.014, val_acc:0.985]
Epoch [61/120    avg_loss:0.015, val_acc:0.984]
Epoch [62/120    avg_loss:0.008, val_acc:0.984]
Epoch [63/120    avg_loss:0.014, val_acc:0.985]
Epoch [64/120    avg_loss:0.033, val_acc:0.985]
Epoch [65/120    avg_loss:0.024, val_acc:0.978]
Epoch [66/120    avg_loss:0.010, val_acc:0.983]
Epoch [67/120    avg_loss:0.021, val_acc:0.982]
Epoch [68/120    avg_loss:0.033, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.989]
Epoch [70/120    avg_loss:0.015, val_acc:0.987]
Epoch [71/120    avg_loss:0.009, val_acc:0.987]
Epoch [72/120    avg_loss:0.022, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.987]
Epoch [74/120    avg_loss:0.010, val_acc:0.985]
Epoch [75/120    avg_loss:0.019, val_acc:0.987]
Epoch [76/120    avg_loss:0.034, val_acc:0.986]
Epoch [77/120    avg_loss:0.014, val_acc:0.985]
Epoch [78/120    avg_loss:0.030, val_acc:0.969]
Epoch [79/120    avg_loss:0.040, val_acc:0.984]
Epoch [80/120    avg_loss:0.017, val_acc:0.986]
Epoch [81/120    avg_loss:0.020, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.019, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.989]
Epoch [85/120    avg_loss:0.007, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.006, val_acc:0.990]
Epoch [88/120    avg_loss:0.006, val_acc:0.990]
Epoch [89/120    avg_loss:0.011, val_acc:0.991]
Epoch [90/120    avg_loss:0.009, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.008, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.991]
Epoch [95/120    avg_loss:0.008, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.019, val_acc:0.991]
Epoch [98/120    avg_loss:0.016, val_acc:0.991]
Epoch [99/120    avg_loss:0.005, val_acc:0.991]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.010, val_acc:0.991]
Epoch [102/120    avg_loss:0.010, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.991]
Epoch [104/120    avg_loss:0.006, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.010, val_acc:0.991]
Epoch [107/120    avg_loss:0.010, val_acc:0.991]
Epoch [108/120    avg_loss:0.008, val_acc:0.991]
Epoch [109/120    avg_loss:0.010, val_acc:0.991]
Epoch [110/120    avg_loss:0.009, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.014, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.991]
Epoch [116/120    avg_loss:0.006, val_acc:0.991]
Epoch [117/120    avg_loss:0.007, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.007, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     3     0     0     0     0     0]
 [    0     0 18019     0    57     0    14     0     0     0]
 [    0     0     0  2000     0     0     0     0    30     6]
 [    0    18     1     0  2924     0     0     0    26     3]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     1     0     0     0  4867     0     6     4]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     2     0     0    31     0     0     0  3538     0]
 [    0     0     0     1    14    27     0     0     0   877]]

Accuracy:
99.39989877810716

F1 scores:
[       nan 0.99821442 0.99797846 0.99083478 0.97450425 0.98899431
 0.99743826 0.99883586 0.9867522  0.96692393]

Kappa:
0.9920538971881043
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff108e2f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.391, val_acc:0.453]
Epoch [2/120    avg_loss:0.899, val_acc:0.659]
Epoch [3/120    avg_loss:0.741, val_acc:0.709]
Epoch [4/120    avg_loss:0.562, val_acc:0.617]
Epoch [5/120    avg_loss:0.442, val_acc:0.733]
Epoch [6/120    avg_loss:0.427, val_acc:0.767]
Epoch [7/120    avg_loss:0.366, val_acc:0.793]
Epoch [8/120    avg_loss:0.351, val_acc:0.840]
Epoch [9/120    avg_loss:0.320, val_acc:0.869]
Epoch [10/120    avg_loss:0.278, val_acc:0.874]
Epoch [11/120    avg_loss:0.386, val_acc:0.875]
Epoch [12/120    avg_loss:0.340, val_acc:0.878]
Epoch [13/120    avg_loss:0.239, val_acc:0.856]
Epoch [14/120    avg_loss:0.320, val_acc:0.895]
Epoch [15/120    avg_loss:0.219, val_acc:0.928]
Epoch [16/120    avg_loss:0.199, val_acc:0.774]
Epoch [17/120    avg_loss:0.229, val_acc:0.899]
Epoch [18/120    avg_loss:0.216, val_acc:0.883]
Epoch [19/120    avg_loss:0.205, val_acc:0.879]
Epoch [20/120    avg_loss:0.166, val_acc:0.872]
Epoch [21/120    avg_loss:0.174, val_acc:0.913]
Epoch [22/120    avg_loss:0.146, val_acc:0.883]
Epoch [23/120    avg_loss:0.098, val_acc:0.941]
Epoch [24/120    avg_loss:0.091, val_acc:0.945]
Epoch [25/120    avg_loss:0.076, val_acc:0.930]
Epoch [26/120    avg_loss:0.065, val_acc:0.915]
Epoch [27/120    avg_loss:0.148, val_acc:0.916]
Epoch [28/120    avg_loss:0.078, val_acc:0.927]
Epoch [29/120    avg_loss:0.198, val_acc:0.910]
Epoch [30/120    avg_loss:0.091, val_acc:0.929]
Epoch [31/120    avg_loss:0.092, val_acc:0.962]
Epoch [32/120    avg_loss:0.077, val_acc:0.951]
Epoch [33/120    avg_loss:0.070, val_acc:0.950]
Epoch [34/120    avg_loss:0.087, val_acc:0.949]
Epoch [35/120    avg_loss:0.072, val_acc:0.949]
Epoch [36/120    avg_loss:0.195, val_acc:0.939]
Epoch [37/120    avg_loss:0.114, val_acc:0.925]
Epoch [38/120    avg_loss:0.075, val_acc:0.901]
Epoch [39/120    avg_loss:0.079, val_acc:0.909]
Epoch [40/120    avg_loss:0.067, val_acc:0.959]
Epoch [41/120    avg_loss:0.074, val_acc:0.966]
Epoch [42/120    avg_loss:0.048, val_acc:0.947]
Epoch [43/120    avg_loss:0.051, val_acc:0.954]
Epoch [44/120    avg_loss:0.058, val_acc:0.967]
Epoch [45/120    avg_loss:0.045, val_acc:0.972]
Epoch [46/120    avg_loss:0.038, val_acc:0.954]
Epoch [47/120    avg_loss:0.041, val_acc:0.957]
Epoch [48/120    avg_loss:0.037, val_acc:0.965]
Epoch [49/120    avg_loss:0.068, val_acc:0.976]
Epoch [50/120    avg_loss:0.024, val_acc:0.969]
Epoch [51/120    avg_loss:0.047, val_acc:0.950]
Epoch [52/120    avg_loss:0.021, val_acc:0.974]
Epoch [53/120    avg_loss:0.008, val_acc:0.974]
Epoch [54/120    avg_loss:0.015, val_acc:0.966]
Epoch [55/120    avg_loss:0.014, val_acc:0.957]
Epoch [56/120    avg_loss:0.027, val_acc:0.961]
Epoch [57/120    avg_loss:0.046, val_acc:0.953]
Epoch [58/120    avg_loss:0.045, val_acc:0.972]
Epoch [59/120    avg_loss:0.041, val_acc:0.956]
Epoch [60/120    avg_loss:0.075, val_acc:0.959]
Epoch [61/120    avg_loss:0.022, val_acc:0.972]
Epoch [62/120    avg_loss:0.029, val_acc:0.972]
Epoch [63/120    avg_loss:0.017, val_acc:0.976]
Epoch [64/120    avg_loss:0.015, val_acc:0.977]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.016, val_acc:0.981]
Epoch [67/120    avg_loss:0.020, val_acc:0.980]
Epoch [68/120    avg_loss:0.009, val_acc:0.982]
Epoch [69/120    avg_loss:0.015, val_acc:0.980]
Epoch [70/120    avg_loss:0.021, val_acc:0.978]
Epoch [71/120    avg_loss:0.011, val_acc:0.977]
Epoch [72/120    avg_loss:0.006, val_acc:0.978]
Epoch [73/120    avg_loss:0.007, val_acc:0.980]
Epoch [74/120    avg_loss:0.008, val_acc:0.979]
Epoch [75/120    avg_loss:0.008, val_acc:0.979]
Epoch [76/120    avg_loss:0.014, val_acc:0.982]
Epoch [77/120    avg_loss:0.009, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.009, val_acc:0.980]
Epoch [82/120    avg_loss:0.018, val_acc:0.982]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.006, val_acc:0.983]
Epoch [85/120    avg_loss:0.015, val_acc:0.980]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.981]
Epoch [89/120    avg_loss:0.007, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.021, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.981]
Epoch [95/120    avg_loss:0.022, val_acc:0.981]
Epoch [96/120    avg_loss:0.021, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.014, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.021, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.005, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.016, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     0     2     0     9     0     0     0]
 [    0     0 17976     0    81     0    33     0     0     0]
 [    0     0     0  2031     0     0     0     0     0     5]
 [    0    26     4     0  2912     0     0     0    22     8]
 [    0     0     0     0     0  1298     0     0     0     7]
 [    0     0     0     0     0     0  4859     0     0    19]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0     0    38     0     0     0  3529     2]
 [    0     0     0     6    14    26     0     0     0   873]]

Accuracy:
99.26252620924012

F1 scores:
[       nan 0.99697228 0.99672858 0.99729929 0.96760259 0.9874477
 0.99376214 0.9992242  0.99101376 0.95149864]

Kappa:
0.9902400108037686
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74c23d37f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.398, val_acc:0.610]
Epoch [2/120    avg_loss:0.846, val_acc:0.665]
Epoch [3/120    avg_loss:0.582, val_acc:0.755]
Epoch [4/120    avg_loss:0.630, val_acc:0.830]
Epoch [5/120    avg_loss:0.441, val_acc:0.814]
Epoch [6/120    avg_loss:0.446, val_acc:0.795]
Epoch [7/120    avg_loss:0.407, val_acc:0.849]
Epoch [8/120    avg_loss:0.321, val_acc:0.868]
Epoch [9/120    avg_loss:0.242, val_acc:0.839]
Epoch [10/120    avg_loss:0.352, val_acc:0.788]
Epoch [11/120    avg_loss:0.423, val_acc:0.820]
Epoch [12/120    avg_loss:0.259, val_acc:0.895]
Epoch [13/120    avg_loss:0.230, val_acc:0.841]
Epoch [14/120    avg_loss:0.295, val_acc:0.838]
Epoch [15/120    avg_loss:0.158, val_acc:0.907]
Epoch [16/120    avg_loss:0.211, val_acc:0.901]
Epoch [17/120    avg_loss:0.284, val_acc:0.923]
Epoch [18/120    avg_loss:0.250, val_acc:0.914]
Epoch [19/120    avg_loss:0.198, val_acc:0.870]
Epoch [20/120    avg_loss:0.174, val_acc:0.862]
Epoch [21/120    avg_loss:0.212, val_acc:0.893]
Epoch [22/120    avg_loss:0.294, val_acc:0.909]
Epoch [23/120    avg_loss:0.152, val_acc:0.936]
Epoch [24/120    avg_loss:0.098, val_acc:0.944]
Epoch [25/120    avg_loss:0.118, val_acc:0.949]
Epoch [26/120    avg_loss:0.088, val_acc:0.943]
Epoch [27/120    avg_loss:0.079, val_acc:0.910]
Epoch [28/120    avg_loss:0.094, val_acc:0.931]
Epoch [29/120    avg_loss:0.080, val_acc:0.901]
Epoch [30/120    avg_loss:0.387, val_acc:0.917]
Epoch [31/120    avg_loss:0.183, val_acc:0.926]
Epoch [32/120    avg_loss:0.149, val_acc:0.919]
Epoch [33/120    avg_loss:0.093, val_acc:0.913]
Epoch [34/120    avg_loss:0.059, val_acc:0.933]
Epoch [35/120    avg_loss:0.079, val_acc:0.950]
Epoch [36/120    avg_loss:0.049, val_acc:0.950]
Epoch [37/120    avg_loss:0.057, val_acc:0.963]
Epoch [38/120    avg_loss:0.054, val_acc:0.944]
Epoch [39/120    avg_loss:0.064, val_acc:0.946]
Epoch [40/120    avg_loss:0.052, val_acc:0.961]
Epoch [41/120    avg_loss:0.055, val_acc:0.964]
Epoch [42/120    avg_loss:0.027, val_acc:0.968]
Epoch [43/120    avg_loss:0.026, val_acc:0.962]
Epoch [44/120    avg_loss:0.060, val_acc:0.949]
Epoch [45/120    avg_loss:0.071, val_acc:0.960]
Epoch [46/120    avg_loss:0.055, val_acc:0.958]
Epoch [47/120    avg_loss:0.030, val_acc:0.958]
Epoch [48/120    avg_loss:0.040, val_acc:0.963]
Epoch [49/120    avg_loss:0.048, val_acc:0.966]
Epoch [50/120    avg_loss:0.024, val_acc:0.949]
Epoch [51/120    avg_loss:0.033, val_acc:0.972]
Epoch [52/120    avg_loss:0.029, val_acc:0.968]
Epoch [53/120    avg_loss:0.042, val_acc:0.918]
Epoch [54/120    avg_loss:0.022, val_acc:0.969]
Epoch [55/120    avg_loss:0.020, val_acc:0.964]
Epoch [56/120    avg_loss:0.018, val_acc:0.974]
Epoch [57/120    avg_loss:0.033, val_acc:0.963]
Epoch [58/120    avg_loss:0.027, val_acc:0.972]
Epoch [59/120    avg_loss:0.030, val_acc:0.944]
Epoch [60/120    avg_loss:0.084, val_acc:0.952]
Epoch [61/120    avg_loss:0.183, val_acc:0.912]
Epoch [62/120    avg_loss:0.090, val_acc:0.965]
Epoch [63/120    avg_loss:0.066, val_acc:0.949]
Epoch [64/120    avg_loss:0.056, val_acc:0.955]
Epoch [65/120    avg_loss:0.036, val_acc:0.973]
Epoch [66/120    avg_loss:0.025, val_acc:0.974]
Epoch [67/120    avg_loss:0.051, val_acc:0.948]
Epoch [68/120    avg_loss:0.042, val_acc:0.967]
Epoch [69/120    avg_loss:0.022, val_acc:0.962]
Epoch [70/120    avg_loss:0.026, val_acc:0.967]
Epoch [71/120    avg_loss:0.045, val_acc:0.963]
Epoch [72/120    avg_loss:0.030, val_acc:0.973]
Epoch [73/120    avg_loss:0.014, val_acc:0.974]
Epoch [74/120    avg_loss:0.027, val_acc:0.969]
Epoch [75/120    avg_loss:0.024, val_acc:0.974]
Epoch [76/120    avg_loss:0.014, val_acc:0.961]
Epoch [77/120    avg_loss:0.017, val_acc:0.977]
Epoch [78/120    avg_loss:0.038, val_acc:0.970]
Epoch [79/120    avg_loss:0.023, val_acc:0.977]
Epoch [80/120    avg_loss:0.039, val_acc:0.967]
Epoch [81/120    avg_loss:0.048, val_acc:0.926]
Epoch [82/120    avg_loss:0.027, val_acc:0.955]
Epoch [83/120    avg_loss:0.045, val_acc:0.965]
Epoch [84/120    avg_loss:0.026, val_acc:0.976]
Epoch [85/120    avg_loss:0.022, val_acc:0.972]
Epoch [86/120    avg_loss:0.013, val_acc:0.977]
Epoch [87/120    avg_loss:0.015, val_acc:0.974]
Epoch [88/120    avg_loss:0.007, val_acc:0.973]
Epoch [89/120    avg_loss:0.013, val_acc:0.978]
Epoch [90/120    avg_loss:0.026, val_acc:0.975]
Epoch [91/120    avg_loss:0.013, val_acc:0.970]
Epoch [92/120    avg_loss:0.010, val_acc:0.969]
Epoch [93/120    avg_loss:0.009, val_acc:0.978]
Epoch [94/120    avg_loss:0.019, val_acc:0.979]
Epoch [95/120    avg_loss:0.022, val_acc:0.977]
Epoch [96/120    avg_loss:0.017, val_acc:0.971]
Epoch [97/120    avg_loss:0.011, val_acc:0.980]
Epoch [98/120    avg_loss:0.015, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.974]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.980]
Epoch [103/120    avg_loss:0.016, val_acc:0.975]
Epoch [104/120    avg_loss:0.008, val_acc:0.980]
Epoch [105/120    avg_loss:0.017, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.004, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.004, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.011, val_acc:0.979]
Epoch [113/120    avg_loss:0.011, val_acc:0.946]
Epoch [114/120    avg_loss:0.012, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.003, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6430     0     0     2     0     0     0     0     0]
 [    0     0 18037     0    41     0     7     0     0     5]
 [    0     1     0  2015     0     0     0     0    16     4]
 [    0    38     5     0  2906     0     0     0    21     2]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4855     0    11    12]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     0     0     0    29     0     0     0  3540     2]
 [    0     0     0     2    14    14     0     0     0   889]]

Accuracy:
99.44327958932833

F1 scores:
[       nan 0.99682195 0.99839477 0.99432519 0.97451375 0.99389779
 0.99691992 0.99883586 0.98896494 0.96735582]

Kappa:
0.9926266090434579
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc1bbdf9780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.391, val_acc:0.674]
Epoch [2/120    avg_loss:0.838, val_acc:0.722]
Epoch [3/120    avg_loss:0.648, val_acc:0.780]
Epoch [4/120    avg_loss:0.581, val_acc:0.766]
Epoch [5/120    avg_loss:0.488, val_acc:0.718]
Epoch [6/120    avg_loss:0.471, val_acc:0.758]
Epoch [7/120    avg_loss:0.425, val_acc:0.805]
Epoch [8/120    avg_loss:0.323, val_acc:0.832]
Epoch [9/120    avg_loss:0.455, val_acc:0.860]
Epoch [10/120    avg_loss:0.316, val_acc:0.834]
Epoch [11/120    avg_loss:0.266, val_acc:0.861]
Epoch [12/120    avg_loss:0.251, val_acc:0.886]
Epoch [13/120    avg_loss:0.247, val_acc:0.892]
Epoch [14/120    avg_loss:0.184, val_acc:0.847]
Epoch [15/120    avg_loss:0.244, val_acc:0.874]
Epoch [16/120    avg_loss:0.239, val_acc:0.899]
Epoch [17/120    avg_loss:0.161, val_acc:0.834]
Epoch [18/120    avg_loss:0.245, val_acc:0.891]
Epoch [19/120    avg_loss:0.170, val_acc:0.861]
Epoch [20/120    avg_loss:0.177, val_acc:0.893]
Epoch [21/120    avg_loss:0.112, val_acc:0.933]
Epoch [22/120    avg_loss:0.100, val_acc:0.942]
Epoch [23/120    avg_loss:0.210, val_acc:0.807]
Epoch [24/120    avg_loss:0.140, val_acc:0.941]
Epoch [25/120    avg_loss:0.165, val_acc:0.943]
Epoch [26/120    avg_loss:0.098, val_acc:0.937]
Epoch [27/120    avg_loss:0.151, val_acc:0.909]
Epoch [28/120    avg_loss:0.144, val_acc:0.907]
Epoch [29/120    avg_loss:0.105, val_acc:0.946]
Epoch [30/120    avg_loss:0.075, val_acc:0.953]
Epoch [31/120    avg_loss:0.092, val_acc:0.948]
Epoch [32/120    avg_loss:0.159, val_acc:0.911]
Epoch [33/120    avg_loss:0.096, val_acc:0.958]
Epoch [34/120    avg_loss:0.071, val_acc:0.949]
Epoch [35/120    avg_loss:0.082, val_acc:0.951]
Epoch [36/120    avg_loss:0.080, val_acc:0.937]
Epoch [37/120    avg_loss:0.052, val_acc:0.964]
Epoch [38/120    avg_loss:0.076, val_acc:0.965]
Epoch [39/120    avg_loss:0.055, val_acc:0.958]
Epoch [40/120    avg_loss:0.046, val_acc:0.952]
Epoch [41/120    avg_loss:0.042, val_acc:0.967]
Epoch [42/120    avg_loss:0.058, val_acc:0.957]
Epoch [43/120    avg_loss:0.145, val_acc:0.953]
Epoch [44/120    avg_loss:0.114, val_acc:0.960]
Epoch [45/120    avg_loss:0.078, val_acc:0.960]
Epoch [46/120    avg_loss:0.044, val_acc:0.972]
Epoch [47/120    avg_loss:0.044, val_acc:0.972]
Epoch [48/120    avg_loss:0.030, val_acc:0.971]
Epoch [49/120    avg_loss:0.022, val_acc:0.978]
Epoch [50/120    avg_loss:0.019, val_acc:0.964]
Epoch [51/120    avg_loss:0.031, val_acc:0.966]
Epoch [52/120    avg_loss:0.037, val_acc:0.966]
Epoch [53/120    avg_loss:0.034, val_acc:0.977]
Epoch [54/120    avg_loss:0.035, val_acc:0.914]
Epoch [55/120    avg_loss:0.044, val_acc:0.951]
Epoch [56/120    avg_loss:0.045, val_acc:0.966]
Epoch [57/120    avg_loss:0.038, val_acc:0.961]
Epoch [58/120    avg_loss:0.147, val_acc:0.947]
Epoch [59/120    avg_loss:0.111, val_acc:0.920]
Epoch [60/120    avg_loss:0.055, val_acc:0.947]
Epoch [61/120    avg_loss:0.060, val_acc:0.959]
Epoch [62/120    avg_loss:0.025, val_acc:0.964]
Epoch [63/120    avg_loss:0.024, val_acc:0.972]
Epoch [64/120    avg_loss:0.017, val_acc:0.975]
Epoch [65/120    avg_loss:0.018, val_acc:0.975]
Epoch [66/120    avg_loss:0.018, val_acc:0.975]
Epoch [67/120    avg_loss:0.013, val_acc:0.977]
Epoch [68/120    avg_loss:0.017, val_acc:0.975]
Epoch [69/120    avg_loss:0.026, val_acc:0.977]
Epoch [70/120    avg_loss:0.011, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.980]
Epoch [72/120    avg_loss:0.013, val_acc:0.979]
Epoch [73/120    avg_loss:0.025, val_acc:0.979]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.014, val_acc:0.982]
Epoch [76/120    avg_loss:0.009, val_acc:0.982]
Epoch [77/120    avg_loss:0.016, val_acc:0.978]
Epoch [78/120    avg_loss:0.020, val_acc:0.976]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.017, val_acc:0.980]
Epoch [81/120    avg_loss:0.015, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.015, val_acc:0.983]
Epoch [85/120    avg_loss:0.012, val_acc:0.983]
Epoch [86/120    avg_loss:0.015, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.017, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.013, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.017, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.982]
Epoch [108/120    avg_loss:0.018, val_acc:0.981]
Epoch [109/120    avg_loss:0.012, val_acc:0.982]
Epoch [110/120    avg_loss:0.012, val_acc:0.982]
Epoch [111/120    avg_loss:0.013, val_acc:0.982]
Epoch [112/120    avg_loss:0.009, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.013, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.982]
Epoch [117/120    avg_loss:0.013, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.007, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     0     0    21     0     3     0     0     0]
 [    0     0 18036     0    54     0     0     0     0     0]
 [    0     0     0  2017     0     0     0     0     9    10]
 [    0    14     8     0  2907     0     0     0    32    11]
 [    0     0     0     0     0  1271     7     0     0    27]
 [    0     0     1     0     0     0  4840     0     9    28]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0     0     0     0    44     0     0     0  3519     8]
 [    0     0     0     3    14    18     0     0     0   884]]

Accuracy:
99.21432530788326

F1 scores:
[       nan 0.99704372 0.99825654 0.99457594 0.96706587 0.97995374
 0.99506579 0.99805825 0.98571429 0.93446089]

Kappa:
0.9895959840642687
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9def6727f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.135, val_acc:0.643]
Epoch [2/120    avg_loss:0.640, val_acc:0.781]
Epoch [3/120    avg_loss:0.508, val_acc:0.725]
Epoch [4/120    avg_loss:0.386, val_acc:0.860]
Epoch [5/120    avg_loss:0.418, val_acc:0.830]
Epoch [6/120    avg_loss:0.357, val_acc:0.914]
Epoch [7/120    avg_loss:0.242, val_acc:0.926]
Epoch [8/120    avg_loss:0.213, val_acc:0.905]
Epoch [9/120    avg_loss:0.237, val_acc:0.910]
Epoch [10/120    avg_loss:0.172, val_acc:0.931]
Epoch [11/120    avg_loss:0.163, val_acc:0.914]
Epoch [12/120    avg_loss:0.188, val_acc:0.902]
Epoch [13/120    avg_loss:0.121, val_acc:0.943]
Epoch [14/120    avg_loss:0.173, val_acc:0.928]
Epoch [15/120    avg_loss:0.115, val_acc:0.940]
Epoch [16/120    avg_loss:0.094, val_acc:0.956]
Epoch [17/120    avg_loss:0.091, val_acc:0.964]
Epoch [18/120    avg_loss:0.058, val_acc:0.951]
Epoch [19/120    avg_loss:0.075, val_acc:0.961]
Epoch [20/120    avg_loss:0.100, val_acc:0.908]
Epoch [21/120    avg_loss:0.091, val_acc:0.959]
Epoch [22/120    avg_loss:0.057, val_acc:0.890]
Epoch [23/120    avg_loss:0.066, val_acc:0.963]
Epoch [24/120    avg_loss:0.063, val_acc:0.961]
Epoch [25/120    avg_loss:0.052, val_acc:0.960]
Epoch [26/120    avg_loss:0.034, val_acc:0.968]
Epoch [27/120    avg_loss:0.052, val_acc:0.956]
Epoch [28/120    avg_loss:0.035, val_acc:0.961]
Epoch [29/120    avg_loss:0.031, val_acc:0.979]
Epoch [30/120    avg_loss:0.027, val_acc:0.969]
Epoch [31/120    avg_loss:0.050, val_acc:0.947]
Epoch [32/120    avg_loss:0.039, val_acc:0.976]
Epoch [33/120    avg_loss:0.035, val_acc:0.976]
Epoch [34/120    avg_loss:0.046, val_acc:0.975]
Epoch [35/120    avg_loss:0.032, val_acc:0.970]
Epoch [36/120    avg_loss:0.034, val_acc:0.959]
Epoch [37/120    avg_loss:0.049, val_acc:0.963]
Epoch [38/120    avg_loss:0.098, val_acc:0.952]
Epoch [39/120    avg_loss:0.044, val_acc:0.969]
Epoch [40/120    avg_loss:0.025, val_acc:0.964]
Epoch [41/120    avg_loss:0.022, val_acc:0.979]
Epoch [42/120    avg_loss:0.016, val_acc:0.979]
Epoch [43/120    avg_loss:0.023, val_acc:0.975]
Epoch [44/120    avg_loss:0.030, val_acc:0.975]
Epoch [45/120    avg_loss:0.036, val_acc:0.963]
Epoch [46/120    avg_loss:0.025, val_acc:0.976]
Epoch [47/120    avg_loss:0.020, val_acc:0.953]
Epoch [48/120    avg_loss:0.050, val_acc:0.964]
Epoch [49/120    avg_loss:0.025, val_acc:0.978]
Epoch [50/120    avg_loss:0.021, val_acc:0.976]
Epoch [51/120    avg_loss:0.014, val_acc:0.979]
Epoch [52/120    avg_loss:0.027, val_acc:0.977]
Epoch [53/120    avg_loss:0.013, val_acc:0.966]
Epoch [54/120    avg_loss:0.025, val_acc:0.979]
Epoch [55/120    avg_loss:0.015, val_acc:0.982]
Epoch [56/120    avg_loss:0.025, val_acc:0.984]
Epoch [57/120    avg_loss:0.017, val_acc:0.975]
Epoch [58/120    avg_loss:0.021, val_acc:0.981]
Epoch [59/120    avg_loss:0.020, val_acc:0.971]
Epoch [60/120    avg_loss:0.018, val_acc:0.983]
Epoch [61/120    avg_loss:0.019, val_acc:0.938]
Epoch [62/120    avg_loss:0.017, val_acc:0.967]
Epoch [63/120    avg_loss:0.010, val_acc:0.983]
Epoch [64/120    avg_loss:0.010, val_acc:0.983]
Epoch [65/120    avg_loss:0.017, val_acc:0.976]
Epoch [66/120    avg_loss:0.010, val_acc:0.980]
Epoch [67/120    avg_loss:0.007, val_acc:0.966]
Epoch [68/120    avg_loss:0.005, val_acc:0.981]
Epoch [69/120    avg_loss:0.007, val_acc:0.980]
Epoch [70/120    avg_loss:0.009, val_acc:0.981]
Epoch [71/120    avg_loss:0.005, val_acc:0.983]
Epoch [72/120    avg_loss:0.014, val_acc:0.983]
Epoch [73/120    avg_loss:0.005, val_acc:0.984]
Epoch [74/120    avg_loss:0.007, val_acc:0.984]
Epoch [75/120    avg_loss:0.006, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.984]
Epoch [78/120    avg_loss:0.004, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.985]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.014, val_acc:0.982]
Epoch [82/120    avg_loss:0.004, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.983]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.005, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.003, val_acc:0.985]
Epoch [91/120    avg_loss:0.005, val_acc:0.984]
Epoch [92/120    avg_loss:0.004, val_acc:0.985]
Epoch [93/120    avg_loss:0.004, val_acc:0.985]
Epoch [94/120    avg_loss:0.004, val_acc:0.985]
Epoch [95/120    avg_loss:0.003, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.012, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.011, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.014, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     8     0     0     5     1    40     3]
 [    0     0 18041     0     1     0    45     0     3     0]
 [    0     0     0  1967     0     0     0     0    67     2]
 [    0    23     1     0  2926     0    12     0     4     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4876     0     1     0]
 [    0    15     0     0     0     0     0  1272     0     3]
 [    0    25     0    49    13     0    21     0  3463     0]
 [    0     0     0     0     0    20     0     0     0   899]]

Accuracy:
99.11069336996601

F1 scores:
[       nan 0.99067599 0.99858855 0.96896552 0.98985115 0.99239544
 0.99135915 0.99258681 0.96880683 0.98144105]

Kappa:
0.9882207952724676
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fade480d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.377, val_acc:0.652]
Epoch [2/120    avg_loss:0.642, val_acc:0.637]
Epoch [3/120    avg_loss:0.504, val_acc:0.627]
Epoch [4/120    avg_loss:0.429, val_acc:0.771]
Epoch [5/120    avg_loss:0.361, val_acc:0.752]
Epoch [6/120    avg_loss:0.298, val_acc:0.861]
Epoch [7/120    avg_loss:0.247, val_acc:0.873]
Epoch [8/120    avg_loss:0.203, val_acc:0.900]
Epoch [9/120    avg_loss:0.177, val_acc:0.906]
Epoch [10/120    avg_loss:0.173, val_acc:0.839]
Epoch [11/120    avg_loss:0.185, val_acc:0.924]
Epoch [12/120    avg_loss:0.176, val_acc:0.891]
Epoch [13/120    avg_loss:0.138, val_acc:0.947]
Epoch [14/120    avg_loss:0.087, val_acc:0.938]
Epoch [15/120    avg_loss:0.171, val_acc:0.905]
Epoch [16/120    avg_loss:0.140, val_acc:0.941]
Epoch [17/120    avg_loss:0.115, val_acc:0.933]
Epoch [18/120    avg_loss:0.093, val_acc:0.935]
Epoch [19/120    avg_loss:0.128, val_acc:0.945]
Epoch [20/120    avg_loss:0.091, val_acc:0.916]
Epoch [21/120    avg_loss:0.055, val_acc:0.941]
Epoch [22/120    avg_loss:0.092, val_acc:0.897]
Epoch [23/120    avg_loss:0.194, val_acc:0.943]
Epoch [24/120    avg_loss:0.060, val_acc:0.956]
Epoch [25/120    avg_loss:0.055, val_acc:0.955]
Epoch [26/120    avg_loss:0.094, val_acc:0.927]
Epoch [27/120    avg_loss:0.049, val_acc:0.959]
Epoch [28/120    avg_loss:0.048, val_acc:0.941]
Epoch [29/120    avg_loss:0.043, val_acc:0.952]
Epoch [30/120    avg_loss:0.042, val_acc:0.956]
Epoch [31/120    avg_loss:0.036, val_acc:0.964]
Epoch [32/120    avg_loss:0.026, val_acc:0.967]
Epoch [33/120    avg_loss:0.036, val_acc:0.971]
Epoch [34/120    avg_loss:0.042, val_acc:0.956]
Epoch [35/120    avg_loss:0.036, val_acc:0.965]
Epoch [36/120    avg_loss:0.040, val_acc:0.972]
Epoch [37/120    avg_loss:0.075, val_acc:0.905]
Epoch [38/120    avg_loss:0.062, val_acc:0.972]
Epoch [39/120    avg_loss:0.030, val_acc:0.969]
Epoch [40/120    avg_loss:0.033, val_acc:0.961]
Epoch [41/120    avg_loss:0.048, val_acc:0.963]
Epoch [42/120    avg_loss:0.033, val_acc:0.966]
Epoch [43/120    avg_loss:0.043, val_acc:0.974]
Epoch [44/120    avg_loss:0.025, val_acc:0.976]
Epoch [45/120    avg_loss:0.027, val_acc:0.950]
Epoch [46/120    avg_loss:0.019, val_acc:0.979]
Epoch [47/120    avg_loss:0.016, val_acc:0.985]
Epoch [48/120    avg_loss:0.008, val_acc:0.981]
Epoch [49/120    avg_loss:0.019, val_acc:0.976]
Epoch [50/120    avg_loss:0.019, val_acc:0.983]
Epoch [51/120    avg_loss:0.018, val_acc:0.974]
Epoch [52/120    avg_loss:0.026, val_acc:0.981]
Epoch [53/120    avg_loss:0.011, val_acc:0.985]
Epoch [54/120    avg_loss:0.017, val_acc:0.984]
Epoch [55/120    avg_loss:0.015, val_acc:0.984]
Epoch [56/120    avg_loss:0.015, val_acc:0.984]
Epoch [57/120    avg_loss:0.010, val_acc:0.974]
Epoch [58/120    avg_loss:0.019, val_acc:0.953]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.017, val_acc:0.971]
Epoch [61/120    avg_loss:0.015, val_acc:0.986]
Epoch [62/120    avg_loss:0.014, val_acc:0.986]
Epoch [63/120    avg_loss:0.012, val_acc:0.984]
Epoch [64/120    avg_loss:0.015, val_acc:0.984]
Epoch [65/120    avg_loss:0.019, val_acc:0.974]
Epoch [66/120    avg_loss:0.011, val_acc:0.981]
Epoch [67/120    avg_loss:0.026, val_acc:0.975]
Epoch [68/120    avg_loss:0.012, val_acc:0.988]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.013, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.981]
Epoch [72/120    avg_loss:0.008, val_acc:0.979]
Epoch [73/120    avg_loss:0.018, val_acc:0.954]
Epoch [74/120    avg_loss:0.019, val_acc:0.987]
Epoch [75/120    avg_loss:0.020, val_acc:0.965]
Epoch [76/120    avg_loss:0.011, val_acc:0.978]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.990]
Epoch [80/120    avg_loss:0.009, val_acc:0.978]
Epoch [81/120    avg_loss:0.014, val_acc:0.980]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.017, val_acc:0.979]
Epoch [86/120    avg_loss:0.009, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.989]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.020, val_acc:0.978]
Epoch [91/120    avg_loss:0.056, val_acc:0.978]
Epoch [92/120    avg_loss:0.115, val_acc:0.979]
Epoch [93/120    avg_loss:0.019, val_acc:0.981]
Epoch [94/120    avg_loss:0.023, val_acc:0.983]
Epoch [95/120    avg_loss:0.015, val_acc:0.984]
Epoch [96/120    avg_loss:0.012, val_acc:0.984]
Epoch [97/120    avg_loss:0.017, val_acc:0.984]
Epoch [98/120    avg_loss:0.014, val_acc:0.984]
Epoch [99/120    avg_loss:0.013, val_acc:0.985]
Epoch [100/120    avg_loss:0.011, val_acc:0.985]
Epoch [101/120    avg_loss:0.017, val_acc:0.985]
Epoch [102/120    avg_loss:0.013, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.013, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.014, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6341     0     7     0     0     0    12    62    10]
 [    0     0 17991     0    13     0    83     0     3     0]
 [    0     0     0  1913     0     0     0     0   117     6]
 [    0    21     0     0  2948     0     1     0     1     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0     7     0     0     0     0     2  1280     0     1]
 [    0     3     0    36    40     0     0     0  3492     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
98.92270985467428

F1 scores:
[       nan 0.99047173 0.99695223 0.95841683 0.98710866 0.99618321
 0.99013325 0.99147947 0.96384212 0.98483207]

Kappa:
0.9857394605345908
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc46e1ba710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.186, val_acc:0.747]
Epoch [2/120    avg_loss:0.705, val_acc:0.691]
Epoch [3/120    avg_loss:0.464, val_acc:0.797]
Epoch [4/120    avg_loss:0.411, val_acc:0.797]
Epoch [5/120    avg_loss:0.409, val_acc:0.833]
Epoch [6/120    avg_loss:0.321, val_acc:0.846]
Epoch [7/120    avg_loss:0.246, val_acc:0.874]
Epoch [8/120    avg_loss:0.256, val_acc:0.873]
Epoch [9/120    avg_loss:0.193, val_acc:0.909]
Epoch [10/120    avg_loss:0.135, val_acc:0.902]
Epoch [11/120    avg_loss:0.167, val_acc:0.939]
Epoch [12/120    avg_loss:0.209, val_acc:0.829]
Epoch [13/120    avg_loss:0.109, val_acc:0.935]
Epoch [14/120    avg_loss:0.163, val_acc:0.917]
Epoch [15/120    avg_loss:0.127, val_acc:0.944]
Epoch [16/120    avg_loss:0.094, val_acc:0.935]
Epoch [17/120    avg_loss:0.129, val_acc:0.909]
Epoch [18/120    avg_loss:0.091, val_acc:0.942]
Epoch [19/120    avg_loss:0.083, val_acc:0.929]
Epoch [20/120    avg_loss:0.096, val_acc:0.922]
Epoch [21/120    avg_loss:0.148, val_acc:0.935]
Epoch [22/120    avg_loss:0.085, val_acc:0.890]
Epoch [23/120    avg_loss:0.095, val_acc:0.949]
Epoch [24/120    avg_loss:0.136, val_acc:0.937]
Epoch [25/120    avg_loss:0.052, val_acc:0.963]
Epoch [26/120    avg_loss:0.056, val_acc:0.958]
Epoch [27/120    avg_loss:0.049, val_acc:0.971]
Epoch [28/120    avg_loss:0.054, val_acc:0.967]
Epoch [29/120    avg_loss:0.099, val_acc:0.960]
Epoch [30/120    avg_loss:0.041, val_acc:0.950]
Epoch [31/120    avg_loss:0.038, val_acc:0.968]
Epoch [32/120    avg_loss:0.031, val_acc:0.972]
Epoch [33/120    avg_loss:0.040, val_acc:0.959]
Epoch [34/120    avg_loss:0.045, val_acc:0.966]
Epoch [35/120    avg_loss:0.041, val_acc:0.968]
Epoch [36/120    avg_loss:0.032, val_acc:0.970]
Epoch [37/120    avg_loss:0.033, val_acc:0.963]
Epoch [38/120    avg_loss:0.041, val_acc:0.967]
Epoch [39/120    avg_loss:0.134, val_acc:0.953]
Epoch [40/120    avg_loss:0.045, val_acc:0.954]
Epoch [41/120    avg_loss:0.035, val_acc:0.962]
Epoch [42/120    avg_loss:0.042, val_acc:0.963]
Epoch [43/120    avg_loss:0.031, val_acc:0.968]
Epoch [44/120    avg_loss:0.031, val_acc:0.978]
Epoch [45/120    avg_loss:0.019, val_acc:0.973]
Epoch [46/120    avg_loss:0.021, val_acc:0.975]
Epoch [47/120    avg_loss:0.018, val_acc:0.978]
Epoch [48/120    avg_loss:0.024, val_acc:0.966]
Epoch [49/120    avg_loss:0.018, val_acc:0.963]
Epoch [50/120    avg_loss:0.047, val_acc:0.970]
Epoch [51/120    avg_loss:0.031, val_acc:0.973]
Epoch [52/120    avg_loss:0.072, val_acc:0.952]
Epoch [53/120    avg_loss:0.042, val_acc:0.961]
Epoch [54/120    avg_loss:0.023, val_acc:0.967]
Epoch [55/120    avg_loss:0.025, val_acc:0.955]
Epoch [56/120    avg_loss:0.023, val_acc:0.977]
Epoch [57/120    avg_loss:0.042, val_acc:0.968]
Epoch [58/120    avg_loss:0.026, val_acc:0.975]
Epoch [59/120    avg_loss:0.027, val_acc:0.973]
Epoch [60/120    avg_loss:0.011, val_acc:0.975]
Epoch [61/120    avg_loss:0.015, val_acc:0.976]
Epoch [62/120    avg_loss:0.016, val_acc:0.975]
Epoch [63/120    avg_loss:0.017, val_acc:0.980]
Epoch [64/120    avg_loss:0.015, val_acc:0.979]
Epoch [65/120    avg_loss:0.012, val_acc:0.979]
Epoch [66/120    avg_loss:0.011, val_acc:0.978]
Epoch [67/120    avg_loss:0.009, val_acc:0.979]
Epoch [68/120    avg_loss:0.012, val_acc:0.978]
Epoch [69/120    avg_loss:0.010, val_acc:0.980]
Epoch [70/120    avg_loss:0.020, val_acc:0.978]
Epoch [71/120    avg_loss:0.010, val_acc:0.979]
Epoch [72/120    avg_loss:0.011, val_acc:0.979]
Epoch [73/120    avg_loss:0.009, val_acc:0.980]
Epoch [74/120    avg_loss:0.009, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.981]
Epoch [77/120    avg_loss:0.014, val_acc:0.980]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.980]
Epoch [80/120    avg_loss:0.009, val_acc:0.982]
Epoch [81/120    avg_loss:0.010, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.013, val_acc:0.982]
Epoch [85/120    avg_loss:0.008, val_acc:0.979]
Epoch [86/120    avg_loss:0.013, val_acc:0.981]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.981]
Epoch [90/120    avg_loss:0.018, val_acc:0.979]
Epoch [91/120    avg_loss:0.008, val_acc:0.981]
Epoch [92/120    avg_loss:0.005, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.981]
Epoch [97/120    avg_loss:0.012, val_acc:0.982]
Epoch [98/120    avg_loss:0.009, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.014, val_acc:0.980]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     8     0     0     2     9    37     3]
 [    0     0 17914     0    16     0   156     0     4     0]
 [    0     0     0  1934     0     0     0     0    98     4]
 [    0    13     0     0  2946     0     3     0     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4865     0    12     0]
 [    0     9     0     0     0     0     0  1279     0     2]
 [    0    10     0    59    29     0     2     0  3471     0]
 [    0     1     0     0     0     6     0     0     0   912]]

Accuracy:
98.80943773648568

F1 scores:
[       nan 0.99283377 0.99508402 0.95813723 0.98809324 0.99770642
 0.98223299 0.99224205 0.96416667 0.98969072]

Kappa:
0.9842534060769298
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8b8735710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.162, val_acc:0.426]
Epoch [2/120    avg_loss:0.676, val_acc:0.733]
Epoch [3/120    avg_loss:0.562, val_acc:0.829]
Epoch [4/120    avg_loss:0.448, val_acc:0.862]
Epoch [5/120    avg_loss:0.348, val_acc:0.837]
Epoch [6/120    avg_loss:0.306, val_acc:0.843]
Epoch [7/120    avg_loss:0.245, val_acc:0.871]
Epoch [8/120    avg_loss:0.239, val_acc:0.902]
Epoch [9/120    avg_loss:0.209, val_acc:0.915]
Epoch [10/120    avg_loss:0.169, val_acc:0.943]
Epoch [11/120    avg_loss:0.130, val_acc:0.941]
Epoch [12/120    avg_loss:0.201, val_acc:0.902]
Epoch [13/120    avg_loss:0.197, val_acc:0.928]
Epoch [14/120    avg_loss:0.159, val_acc:0.922]
Epoch [15/120    avg_loss:0.113, val_acc:0.950]
Epoch [16/120    avg_loss:0.087, val_acc:0.956]
Epoch [17/120    avg_loss:0.110, val_acc:0.874]
Epoch [18/120    avg_loss:0.105, val_acc:0.907]
Epoch [19/120    avg_loss:0.084, val_acc:0.955]
Epoch [20/120    avg_loss:0.093, val_acc:0.904]
Epoch [21/120    avg_loss:0.098, val_acc:0.944]
Epoch [22/120    avg_loss:0.187, val_acc:0.936]
Epoch [23/120    avg_loss:0.112, val_acc:0.947]
Epoch [24/120    avg_loss:0.065, val_acc:0.970]
Epoch [25/120    avg_loss:0.059, val_acc:0.947]
Epoch [26/120    avg_loss:0.061, val_acc:0.972]
Epoch [27/120    avg_loss:0.057, val_acc:0.962]
Epoch [28/120    avg_loss:0.093, val_acc:0.968]
Epoch [29/120    avg_loss:0.041, val_acc:0.963]
Epoch [30/120    avg_loss:0.043, val_acc:0.969]
Epoch [31/120    avg_loss:0.027, val_acc:0.964]
Epoch [32/120    avg_loss:0.030, val_acc:0.971]
Epoch [33/120    avg_loss:0.036, val_acc:0.964]
Epoch [34/120    avg_loss:0.062, val_acc:0.927]
Epoch [35/120    avg_loss:0.049, val_acc:0.981]
Epoch [36/120    avg_loss:0.050, val_acc:0.955]
Epoch [37/120    avg_loss:0.027, val_acc:0.971]
Epoch [38/120    avg_loss:0.031, val_acc:0.978]
Epoch [39/120    avg_loss:0.036, val_acc:0.973]
Epoch [40/120    avg_loss:0.020, val_acc:0.964]
Epoch [41/120    avg_loss:0.027, val_acc:0.978]
Epoch [42/120    avg_loss:0.022, val_acc:0.966]
Epoch [43/120    avg_loss:0.034, val_acc:0.974]
Epoch [44/120    avg_loss:0.046, val_acc:0.963]
Epoch [45/120    avg_loss:0.066, val_acc:0.953]
Epoch [46/120    avg_loss:0.204, val_acc:0.940]
Epoch [47/120    avg_loss:0.119, val_acc:0.962]
Epoch [48/120    avg_loss:0.043, val_acc:0.968]
Epoch [49/120    avg_loss:0.023, val_acc:0.972]
Epoch [50/120    avg_loss:0.028, val_acc:0.976]
Epoch [51/120    avg_loss:0.016, val_acc:0.978]
Epoch [52/120    avg_loss:0.023, val_acc:0.977]
Epoch [53/120    avg_loss:0.019, val_acc:0.977]
Epoch [54/120    avg_loss:0.023, val_acc:0.976]
Epoch [55/120    avg_loss:0.027, val_acc:0.977]
Epoch [56/120    avg_loss:0.023, val_acc:0.977]
Epoch [57/120    avg_loss:0.019, val_acc:0.978]
Epoch [58/120    avg_loss:0.021, val_acc:0.978]
Epoch [59/120    avg_loss:0.017, val_acc:0.979]
Epoch [60/120    avg_loss:0.018, val_acc:0.978]
Epoch [61/120    avg_loss:0.014, val_acc:0.983]
Epoch [62/120    avg_loss:0.018, val_acc:0.983]
Epoch [63/120    avg_loss:0.020, val_acc:0.981]
Epoch [64/120    avg_loss:0.015, val_acc:0.980]
Epoch [65/120    avg_loss:0.019, val_acc:0.982]
Epoch [66/120    avg_loss:0.013, val_acc:0.983]
Epoch [67/120    avg_loss:0.015, val_acc:0.983]
Epoch [68/120    avg_loss:0.018, val_acc:0.982]
Epoch [69/120    avg_loss:0.017, val_acc:0.981]
Epoch [70/120    avg_loss:0.016, val_acc:0.979]
Epoch [71/120    avg_loss:0.018, val_acc:0.979]
Epoch [72/120    avg_loss:0.014, val_acc:0.984]
Epoch [73/120    avg_loss:0.013, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.984]
Epoch [76/120    avg_loss:0.021, val_acc:0.981]
Epoch [77/120    avg_loss:0.015, val_acc:0.983]
Epoch [78/120    avg_loss:0.022, val_acc:0.978]
Epoch [79/120    avg_loss:0.012, val_acc:0.984]
Epoch [80/120    avg_loss:0.019, val_acc:0.983]
Epoch [81/120    avg_loss:0.019, val_acc:0.979]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.012, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.982]
Epoch [86/120    avg_loss:0.015, val_acc:0.983]
Epoch [87/120    avg_loss:0.012, val_acc:0.984]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.985]
Epoch [91/120    avg_loss:0.031, val_acc:0.978]
Epoch [92/120    avg_loss:0.014, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.026, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.013, val_acc:0.984]
Epoch [99/120    avg_loss:0.014, val_acc:0.985]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.014, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.014, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0    10     0     0     0     1     3    13]
 [    0     0 18034     0    10     0    40     0     6     0]
 [    0     1     0  1959     0     0     0     0    72     4]
 [    0    21     2     0  2937     0     1     0     7     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     0     0  4853     0     3     0]
 [    0     8     0     0     0     0     0  1273     0     9]
 [    0    48     0    71    12     0     0     0  3440     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
99.08900296435543

F1 scores:
[       nan 0.99186992 0.99778688 0.96123651 0.99038948 0.99618321
 0.99324601 0.99297972 0.9687412  0.97847147]

Kappa:
0.9879318536853908
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f775af48748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.196, val_acc:0.532]
Epoch [2/120    avg_loss:0.668, val_acc:0.662]
Epoch [3/120    avg_loss:0.438, val_acc:0.763]
Epoch [4/120    avg_loss:0.393, val_acc:0.825]
Epoch [5/120    avg_loss:0.356, val_acc:0.879]
Epoch [6/120    avg_loss:0.276, val_acc:0.855]
Epoch [7/120    avg_loss:0.235, val_acc:0.878]
Epoch [8/120    avg_loss:0.230, val_acc:0.851]
Epoch [9/120    avg_loss:0.178, val_acc:0.889]
Epoch [10/120    avg_loss:0.183, val_acc:0.838]
Epoch [11/120    avg_loss:0.186, val_acc:0.939]
Epoch [12/120    avg_loss:0.135, val_acc:0.952]
Epoch [13/120    avg_loss:0.095, val_acc:0.943]
Epoch [14/120    avg_loss:0.129, val_acc:0.921]
Epoch [15/120    avg_loss:0.121, val_acc:0.942]
Epoch [16/120    avg_loss:0.102, val_acc:0.941]
Epoch [17/120    avg_loss:0.078, val_acc:0.938]
Epoch [18/120    avg_loss:0.082, val_acc:0.929]
Epoch [19/120    avg_loss:0.083, val_acc:0.964]
Epoch [20/120    avg_loss:0.108, val_acc:0.942]
Epoch [21/120    avg_loss:0.123, val_acc:0.954]
Epoch [22/120    avg_loss:0.080, val_acc:0.960]
Epoch [23/120    avg_loss:0.081, val_acc:0.950]
Epoch [24/120    avg_loss:0.049, val_acc:0.967]
Epoch [25/120    avg_loss:0.050, val_acc:0.966]
Epoch [26/120    avg_loss:0.036, val_acc:0.955]
Epoch [27/120    avg_loss:0.046, val_acc:0.972]
Epoch [28/120    avg_loss:0.032, val_acc:0.917]
Epoch [29/120    avg_loss:0.032, val_acc:0.971]
Epoch [30/120    avg_loss:0.098, val_acc:0.903]
Epoch [31/120    avg_loss:0.133, val_acc:0.950]
Epoch [32/120    avg_loss:0.052, val_acc:0.965]
Epoch [33/120    avg_loss:0.032, val_acc:0.972]
Epoch [34/120    avg_loss:0.026, val_acc:0.955]
Epoch [35/120    avg_loss:0.028, val_acc:0.970]
Epoch [36/120    avg_loss:0.021, val_acc:0.976]
Epoch [37/120    avg_loss:0.015, val_acc:0.979]
Epoch [38/120    avg_loss:0.022, val_acc:0.974]
Epoch [39/120    avg_loss:0.036, val_acc:0.963]
Epoch [40/120    avg_loss:0.035, val_acc:0.977]
Epoch [41/120    avg_loss:0.038, val_acc:0.984]
Epoch [42/120    avg_loss:0.022, val_acc:0.976]
Epoch [43/120    avg_loss:0.019, val_acc:0.988]
Epoch [44/120    avg_loss:0.011, val_acc:0.980]
Epoch [45/120    avg_loss:0.011, val_acc:0.983]
Epoch [46/120    avg_loss:0.013, val_acc:0.981]
Epoch [47/120    avg_loss:0.023, val_acc:0.981]
Epoch [48/120    avg_loss:0.030, val_acc:0.981]
Epoch [49/120    avg_loss:0.036, val_acc:0.963]
Epoch [50/120    avg_loss:0.173, val_acc:0.841]
Epoch [51/120    avg_loss:0.154, val_acc:0.963]
Epoch [52/120    avg_loss:0.046, val_acc:0.976]
Epoch [53/120    avg_loss:0.064, val_acc:0.968]
Epoch [54/120    avg_loss:0.027, val_acc:0.968]
Epoch [55/120    avg_loss:0.066, val_acc:0.959]
Epoch [56/120    avg_loss:0.025, val_acc:0.966]
Epoch [57/120    avg_loss:0.049, val_acc:0.976]
Epoch [58/120    avg_loss:0.024, val_acc:0.981]
Epoch [59/120    avg_loss:0.015, val_acc:0.980]
Epoch [60/120    avg_loss:0.011, val_acc:0.982]
Epoch [61/120    avg_loss:0.020, val_acc:0.981]
Epoch [62/120    avg_loss:0.015, val_acc:0.982]
Epoch [63/120    avg_loss:0.014, val_acc:0.983]
Epoch [64/120    avg_loss:0.013, val_acc:0.983]
Epoch [65/120    avg_loss:0.018, val_acc:0.981]
Epoch [66/120    avg_loss:0.017, val_acc:0.981]
Epoch [67/120    avg_loss:0.013, val_acc:0.985]
Epoch [68/120    avg_loss:0.009, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.984]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.983]
Epoch [73/120    avg_loss:0.018, val_acc:0.983]
Epoch [74/120    avg_loss:0.014, val_acc:0.982]
Epoch [75/120    avg_loss:0.012, val_acc:0.982]
Epoch [76/120    avg_loss:0.019, val_acc:0.983]
Epoch [77/120    avg_loss:0.015, val_acc:0.983]
Epoch [78/120    avg_loss:0.030, val_acc:0.984]
Epoch [79/120    avg_loss:0.017, val_acc:0.984]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.027, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.984]
Epoch [83/120    avg_loss:0.013, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.984]
Epoch [85/120    avg_loss:0.019, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.983]
Epoch [87/120    avg_loss:0.014, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.014, val_acc:0.984]
Epoch [90/120    avg_loss:0.017, val_acc:0.984]
Epoch [91/120    avg_loss:0.020, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.015, val_acc:0.983]
Epoch [94/120    avg_loss:0.020, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.983]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.015, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.016, val_acc:0.983]
Epoch [105/120    avg_loss:0.017, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.016, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.983]
Epoch [110/120    avg_loss:0.015, val_acc:0.983]
Epoch [111/120    avg_loss:0.019, val_acc:0.983]
Epoch [112/120    avg_loss:0.020, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.015, val_acc:0.983]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.011, val_acc:0.983]
Epoch [117/120    avg_loss:0.015, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     7     0     0     9    13    18     6]
 [    0     0 17702     0    11     0   374     0     3     0]
 [    0     0     0  1925     0     0     0     0   106     5]
 [    0    12     3     0  2945     0     3     0     5     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0     9     0     0     0     0     1  1280     0     0]
 [    0     2     0    35    38     0     0     0  3496     0]
 [    0     0     0     0     1    35     0     0     0   883]]

Accuracy:
98.30814836237438

F1 scores:
[       nan 0.99407823 0.98902143 0.96177867 0.98709569 0.98676749
 0.96164086 0.99109563 0.97124601 0.97193176]

Kappa:
0.9776692813450184
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f54d09557b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.210, val_acc:0.706]
Epoch [2/120    avg_loss:0.582, val_acc:0.581]
Epoch [3/120    avg_loss:0.500, val_acc:0.706]
Epoch [4/120    avg_loss:0.388, val_acc:0.795]
Epoch [5/120    avg_loss:0.361, val_acc:0.882]
Epoch [6/120    avg_loss:0.247, val_acc:0.850]
Epoch [7/120    avg_loss:0.294, val_acc:0.857]
Epoch [8/120    avg_loss:0.195, val_acc:0.896]
Epoch [9/120    avg_loss:0.274, val_acc:0.790]
Epoch [10/120    avg_loss:0.230, val_acc:0.903]
Epoch [11/120    avg_loss:0.181, val_acc:0.927]
Epoch [12/120    avg_loss:0.145, val_acc:0.910]
Epoch [13/120    avg_loss:0.119, val_acc:0.934]
Epoch [14/120    avg_loss:0.104, val_acc:0.945]
Epoch [15/120    avg_loss:0.165, val_acc:0.905]
Epoch [16/120    avg_loss:0.127, val_acc:0.924]
Epoch [17/120    avg_loss:0.105, val_acc:0.899]
Epoch [18/120    avg_loss:0.142, val_acc:0.859]
Epoch [19/120    avg_loss:0.115, val_acc:0.941]
Epoch [20/120    avg_loss:0.099, val_acc:0.938]
Epoch [21/120    avg_loss:0.056, val_acc:0.937]
Epoch [22/120    avg_loss:0.065, val_acc:0.949]
Epoch [23/120    avg_loss:0.061, val_acc:0.964]
Epoch [24/120    avg_loss:0.074, val_acc:0.957]
Epoch [25/120    avg_loss:0.037, val_acc:0.959]
Epoch [26/120    avg_loss:0.039, val_acc:0.953]
Epoch [27/120    avg_loss:0.039, val_acc:0.962]
Epoch [28/120    avg_loss:0.064, val_acc:0.951]
Epoch [29/120    avg_loss:0.035, val_acc:0.961]
Epoch [30/120    avg_loss:0.052, val_acc:0.953]
Epoch [31/120    avg_loss:0.036, val_acc:0.951]
Epoch [32/120    avg_loss:0.067, val_acc:0.959]
Epoch [33/120    avg_loss:0.054, val_acc:0.950]
Epoch [34/120    avg_loss:0.032, val_acc:0.928]
Epoch [35/120    avg_loss:0.036, val_acc:0.942]
Epoch [36/120    avg_loss:0.048, val_acc:0.943]
Epoch [37/120    avg_loss:0.041, val_acc:0.955]
Epoch [38/120    avg_loss:0.026, val_acc:0.963]
Epoch [39/120    avg_loss:0.029, val_acc:0.961]
Epoch [40/120    avg_loss:0.020, val_acc:0.962]
Epoch [41/120    avg_loss:0.014, val_acc:0.964]
Epoch [42/120    avg_loss:0.023, val_acc:0.965]
Epoch [43/120    avg_loss:0.023, val_acc:0.964]
Epoch [44/120    avg_loss:0.015, val_acc:0.965]
Epoch [45/120    avg_loss:0.013, val_acc:0.965]
Epoch [46/120    avg_loss:0.011, val_acc:0.965]
Epoch [47/120    avg_loss:0.018, val_acc:0.966]
Epoch [48/120    avg_loss:0.033, val_acc:0.967]
Epoch [49/120    avg_loss:0.016, val_acc:0.966]
Epoch [50/120    avg_loss:0.013, val_acc:0.968]
Epoch [51/120    avg_loss:0.020, val_acc:0.968]
Epoch [52/120    avg_loss:0.026, val_acc:0.967]
Epoch [53/120    avg_loss:0.014, val_acc:0.969]
Epoch [54/120    avg_loss:0.013, val_acc:0.968]
Epoch [55/120    avg_loss:0.017, val_acc:0.967]
Epoch [56/120    avg_loss:0.022, val_acc:0.967]
Epoch [57/120    avg_loss:0.020, val_acc:0.972]
Epoch [58/120    avg_loss:0.013, val_acc:0.969]
Epoch [59/120    avg_loss:0.022, val_acc:0.968]
Epoch [60/120    avg_loss:0.021, val_acc:0.969]
Epoch [61/120    avg_loss:0.010, val_acc:0.970]
Epoch [62/120    avg_loss:0.011, val_acc:0.968]
Epoch [63/120    avg_loss:0.016, val_acc:0.970]
Epoch [64/120    avg_loss:0.017, val_acc:0.971]
Epoch [65/120    avg_loss:0.013, val_acc:0.971]
Epoch [66/120    avg_loss:0.017, val_acc:0.970]
Epoch [67/120    avg_loss:0.014, val_acc:0.972]
Epoch [68/120    avg_loss:0.014, val_acc:0.975]
Epoch [69/120    avg_loss:0.011, val_acc:0.973]
Epoch [70/120    avg_loss:0.016, val_acc:0.971]
Epoch [71/120    avg_loss:0.010, val_acc:0.972]
Epoch [72/120    avg_loss:0.011, val_acc:0.974]
Epoch [73/120    avg_loss:0.014, val_acc:0.970]
Epoch [74/120    avg_loss:0.009, val_acc:0.970]
Epoch [75/120    avg_loss:0.027, val_acc:0.973]
Epoch [76/120    avg_loss:0.015, val_acc:0.972]
Epoch [77/120    avg_loss:0.009, val_acc:0.972]
Epoch [78/120    avg_loss:0.011, val_acc:0.974]
Epoch [79/120    avg_loss:0.009, val_acc:0.972]
Epoch [80/120    avg_loss:0.014, val_acc:0.973]
Epoch [81/120    avg_loss:0.012, val_acc:0.971]
Epoch [82/120    avg_loss:0.010, val_acc:0.972]
Epoch [83/120    avg_loss:0.018, val_acc:0.972]
Epoch [84/120    avg_loss:0.009, val_acc:0.972]
Epoch [85/120    avg_loss:0.016, val_acc:0.972]
Epoch [86/120    avg_loss:0.012, val_acc:0.972]
Epoch [87/120    avg_loss:0.010, val_acc:0.972]
Epoch [88/120    avg_loss:0.009, val_acc:0.972]
Epoch [89/120    avg_loss:0.014, val_acc:0.972]
Epoch [90/120    avg_loss:0.013, val_acc:0.973]
Epoch [91/120    avg_loss:0.022, val_acc:0.972]
Epoch [92/120    avg_loss:0.006, val_acc:0.972]
Epoch [93/120    avg_loss:0.012, val_acc:0.972]
Epoch [94/120    avg_loss:0.011, val_acc:0.972]
Epoch [95/120    avg_loss:0.016, val_acc:0.972]
Epoch [96/120    avg_loss:0.018, val_acc:0.972]
Epoch [97/120    avg_loss:0.009, val_acc:0.972]
Epoch [98/120    avg_loss:0.009, val_acc:0.972]
Epoch [99/120    avg_loss:0.010, val_acc:0.972]
Epoch [100/120    avg_loss:0.011, val_acc:0.972]
Epoch [101/120    avg_loss:0.011, val_acc:0.972]
Epoch [102/120    avg_loss:0.011, val_acc:0.972]
Epoch [103/120    avg_loss:0.011, val_acc:0.972]
Epoch [104/120    avg_loss:0.009, val_acc:0.972]
Epoch [105/120    avg_loss:0.017, val_acc:0.972]
Epoch [106/120    avg_loss:0.013, val_acc:0.972]
Epoch [107/120    avg_loss:0.013, val_acc:0.972]
Epoch [108/120    avg_loss:0.011, val_acc:0.972]
Epoch [109/120    avg_loss:0.012, val_acc:0.972]
Epoch [110/120    avg_loss:0.011, val_acc:0.972]
Epoch [111/120    avg_loss:0.014, val_acc:0.972]
Epoch [112/120    avg_loss:0.012, val_acc:0.972]
Epoch [113/120    avg_loss:0.020, val_acc:0.972]
Epoch [114/120    avg_loss:0.010, val_acc:0.972]
Epoch [115/120    avg_loss:0.010, val_acc:0.972]
Epoch [116/120    avg_loss:0.008, val_acc:0.972]
Epoch [117/120    avg_loss:0.010, val_acc:0.972]
Epoch [118/120    avg_loss:0.009, val_acc:0.972]
Epoch [119/120    avg_loss:0.018, val_acc:0.972]
Epoch [120/120    avg_loss:0.008, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     2     0     0     0     1    29     2]
 [    0     0 17951     0    23     0   113     0     3     0]
 [    0     0     0  1979     0     0     0     0    55     2]
 [    0    17     1     0  2948     0     1     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4858     0     7     0]
 [    0     9     0     0     0     0     0  1281     0     0]
 [    0    18     0    90    34     0     0     0  3429     0]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.96368062082762

F1 scores:
[       nan 0.99394128 0.99575648 0.96372048 0.98644805 0.99808795
 0.98639594 0.99611198 0.96645998 0.99347826]

Kappa:
0.9862864572273627
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d31a12780>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.082, val_acc:0.706]
Epoch [2/120    avg_loss:0.633, val_acc:0.683]
Epoch [3/120    avg_loss:0.521, val_acc:0.732]
Epoch [4/120    avg_loss:0.401, val_acc:0.785]
Epoch [5/120    avg_loss:0.328, val_acc:0.786]
Epoch [6/120    avg_loss:0.344, val_acc:0.901]
Epoch [7/120    avg_loss:0.283, val_acc:0.851]
Epoch [8/120    avg_loss:0.214, val_acc:0.870]
Epoch [9/120    avg_loss:0.294, val_acc:0.829]
Epoch [10/120    avg_loss:0.206, val_acc:0.924]
Epoch [11/120    avg_loss:0.187, val_acc:0.916]
Epoch [12/120    avg_loss:0.215, val_acc:0.921]
Epoch [13/120    avg_loss:0.151, val_acc:0.937]
Epoch [14/120    avg_loss:0.138, val_acc:0.945]
Epoch [15/120    avg_loss:0.097, val_acc:0.955]
Epoch [16/120    avg_loss:0.075, val_acc:0.953]
Epoch [17/120    avg_loss:0.091, val_acc:0.957]
Epoch [18/120    avg_loss:0.132, val_acc:0.965]
Epoch [19/120    avg_loss:0.073, val_acc:0.967]
Epoch [20/120    avg_loss:0.067, val_acc:0.962]
Epoch [21/120    avg_loss:0.066, val_acc:0.969]
Epoch [22/120    avg_loss:0.046, val_acc:0.977]
Epoch [23/120    avg_loss:0.028, val_acc:0.974]
Epoch [24/120    avg_loss:0.041, val_acc:0.968]
Epoch [25/120    avg_loss:0.052, val_acc:0.968]
Epoch [26/120    avg_loss:0.073, val_acc:0.959]
Epoch [27/120    avg_loss:0.047, val_acc:0.955]
Epoch [28/120    avg_loss:0.026, val_acc:0.979]
Epoch [29/120    avg_loss:0.032, val_acc:0.980]
Epoch [30/120    avg_loss:0.046, val_acc:0.954]
Epoch [31/120    avg_loss:0.149, val_acc:0.950]
Epoch [32/120    avg_loss:0.061, val_acc:0.967]
Epoch [33/120    avg_loss:0.044, val_acc:0.962]
Epoch [34/120    avg_loss:0.047, val_acc:0.966]
Epoch [35/120    avg_loss:0.027, val_acc:0.976]
Epoch [36/120    avg_loss:0.025, val_acc:0.935]
Epoch [37/120    avg_loss:0.030, val_acc:0.977]
Epoch [38/120    avg_loss:0.034, val_acc:0.947]
Epoch [39/120    avg_loss:0.034, val_acc:0.980]
Epoch [40/120    avg_loss:0.019, val_acc:0.984]
Epoch [41/120    avg_loss:0.023, val_acc:0.981]
Epoch [42/120    avg_loss:0.040, val_acc:0.979]
Epoch [43/120    avg_loss:0.028, val_acc:0.981]
Epoch [44/120    avg_loss:0.030, val_acc:0.979]
Epoch [45/120    avg_loss:0.022, val_acc:0.979]
Epoch [46/120    avg_loss:0.025, val_acc:0.971]
Epoch [47/120    avg_loss:0.029, val_acc:0.985]
Epoch [48/120    avg_loss:0.020, val_acc:0.983]
Epoch [49/120    avg_loss:0.020, val_acc:0.990]
Epoch [50/120    avg_loss:0.010, val_acc:0.988]
Epoch [51/120    avg_loss:0.020, val_acc:0.987]
Epoch [52/120    avg_loss:0.022, val_acc:0.989]
Epoch [53/120    avg_loss:0.011, val_acc:0.989]
Epoch [54/120    avg_loss:0.024, val_acc:0.943]
Epoch [55/120    avg_loss:0.056, val_acc:0.956]
Epoch [56/120    avg_loss:0.021, val_acc:0.980]
Epoch [57/120    avg_loss:0.021, val_acc:0.987]
Epoch [58/120    avg_loss:0.025, val_acc:0.988]
Epoch [59/120    avg_loss:0.017, val_acc:0.985]
Epoch [60/120    avg_loss:0.011, val_acc:0.990]
Epoch [61/120    avg_loss:0.018, val_acc:0.974]
Epoch [62/120    avg_loss:0.016, val_acc:0.988]
Epoch [63/120    avg_loss:0.014, val_acc:0.993]
Epoch [64/120    avg_loss:0.013, val_acc:0.984]
Epoch [65/120    avg_loss:0.009, val_acc:0.984]
Epoch [66/120    avg_loss:0.013, val_acc:0.988]
Epoch [67/120    avg_loss:0.012, val_acc:0.990]
Epoch [68/120    avg_loss:0.021, val_acc:0.993]
Epoch [69/120    avg_loss:0.015, val_acc:0.974]
Epoch [70/120    avg_loss:0.007, val_acc:0.983]
Epoch [71/120    avg_loss:0.010, val_acc:0.990]
Epoch [72/120    avg_loss:0.006, val_acc:0.993]
Epoch [73/120    avg_loss:0.005, val_acc:0.993]
Epoch [74/120    avg_loss:0.004, val_acc:0.992]
Epoch [75/120    avg_loss:0.005, val_acc:0.993]
Epoch [76/120    avg_loss:0.005, val_acc:0.990]
Epoch [77/120    avg_loss:0.014, val_acc:0.990]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.991]
Epoch [80/120    avg_loss:0.007, val_acc:0.993]
Epoch [81/120    avg_loss:0.010, val_acc:0.979]
Epoch [82/120    avg_loss:0.005, val_acc:0.990]
Epoch [83/120    avg_loss:0.008, val_acc:0.991]
Epoch [84/120    avg_loss:0.003, val_acc:0.992]
Epoch [85/120    avg_loss:0.019, val_acc:0.956]
Epoch [86/120    avg_loss:0.014, val_acc:0.989]
Epoch [87/120    avg_loss:0.009, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.993]
Epoch [89/120    avg_loss:0.008, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.003, val_acc:0.992]
Epoch [94/120    avg_loss:0.009, val_acc:0.991]
Epoch [95/120    avg_loss:0.005, val_acc:0.993]
Epoch [96/120    avg_loss:0.004, val_acc:0.992]
Epoch [97/120    avg_loss:0.007, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.992]
Epoch [99/120    avg_loss:0.005, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.992]
Epoch [101/120    avg_loss:0.003, val_acc:0.992]
Epoch [102/120    avg_loss:0.004, val_acc:0.992]
Epoch [103/120    avg_loss:0.003, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.992]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.003, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.993]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.013, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.003, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     8     0     0     0    12    15     3]
 [    0     0 17868     0    20     0   202     0     0     0]
 [    0     2     0  1931     0     0     0     0   100     3]
 [    0     5     0     0  2965     0     0     0     0     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4874     0     0     0]
 [    0    17     0     0     0     0     1  1270     0     2]
 [    0     4     0    21    48     0    13     0  3483     2]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
98.80943773648568

F1 scores:
[       nan 0.99486541 0.99371559 0.96646647 0.98751041 0.99618321
 0.97792937 0.98755832 0.97168364 0.98804348]

Kappa:
0.9842588133398797
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb9381d97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.206, val_acc:0.602]
Epoch [2/120    avg_loss:0.664, val_acc:0.640]
Epoch [3/120    avg_loss:0.519, val_acc:0.748]
Epoch [4/120    avg_loss:0.446, val_acc:0.780]
Epoch [5/120    avg_loss:0.377, val_acc:0.785]
Epoch [6/120    avg_loss:0.287, val_acc:0.887]
Epoch [7/120    avg_loss:0.265, val_acc:0.914]
Epoch [8/120    avg_loss:0.250, val_acc:0.868]
Epoch [9/120    avg_loss:0.247, val_acc:0.850]
Epoch [10/120    avg_loss:0.280, val_acc:0.920]
Epoch [11/120    avg_loss:0.183, val_acc:0.882]
Epoch [12/120    avg_loss:0.129, val_acc:0.891]
Epoch [13/120    avg_loss:0.162, val_acc:0.926]
Epoch [14/120    avg_loss:0.131, val_acc:0.917]
Epoch [15/120    avg_loss:0.125, val_acc:0.893]
Epoch [16/120    avg_loss:0.103, val_acc:0.938]
Epoch [17/120    avg_loss:0.083, val_acc:0.929]
Epoch [18/120    avg_loss:0.108, val_acc:0.942]
Epoch [19/120    avg_loss:0.153, val_acc:0.916]
Epoch [20/120    avg_loss:0.090, val_acc:0.944]
Epoch [21/120    avg_loss:0.071, val_acc:0.947]
Epoch [22/120    avg_loss:0.088, val_acc:0.950]
Epoch [23/120    avg_loss:0.072, val_acc:0.943]
Epoch [24/120    avg_loss:0.061, val_acc:0.956]
Epoch [25/120    avg_loss:0.048, val_acc:0.948]
Epoch [26/120    avg_loss:0.054, val_acc:0.960]
Epoch [27/120    avg_loss:0.102, val_acc:0.942]
Epoch [28/120    avg_loss:0.082, val_acc:0.961]
Epoch [29/120    avg_loss:0.079, val_acc:0.942]
Epoch [30/120    avg_loss:0.093, val_acc:0.955]
Epoch [31/120    avg_loss:0.062, val_acc:0.955]
Epoch [32/120    avg_loss:0.055, val_acc:0.962]
Epoch [33/120    avg_loss:0.035, val_acc:0.967]
Epoch [34/120    avg_loss:0.032, val_acc:0.963]
Epoch [35/120    avg_loss:0.053, val_acc:0.964]
Epoch [36/120    avg_loss:0.038, val_acc:0.968]
Epoch [37/120    avg_loss:0.031, val_acc:0.969]
Epoch [38/120    avg_loss:0.038, val_acc:0.960]
Epoch [39/120    avg_loss:0.028, val_acc:0.970]
Epoch [40/120    avg_loss:0.045, val_acc:0.965]
Epoch [41/120    avg_loss:0.057, val_acc:0.965]
Epoch [42/120    avg_loss:0.035, val_acc:0.960]
Epoch [43/120    avg_loss:0.021, val_acc:0.978]
Epoch [44/120    avg_loss:0.033, val_acc:0.956]
Epoch [45/120    avg_loss:0.025, val_acc:0.975]
Epoch [46/120    avg_loss:0.017, val_acc:0.975]
Epoch [47/120    avg_loss:0.024, val_acc:0.976]
Epoch [48/120    avg_loss:0.040, val_acc:0.981]
Epoch [49/120    avg_loss:0.043, val_acc:0.967]
Epoch [50/120    avg_loss:0.034, val_acc:0.969]
Epoch [51/120    avg_loss:0.055, val_acc:0.975]
Epoch [52/120    avg_loss:0.090, val_acc:0.960]
Epoch [53/120    avg_loss:0.054, val_acc:0.978]
Epoch [54/120    avg_loss:0.027, val_acc:0.969]
Epoch [55/120    avg_loss:0.026, val_acc:0.979]
Epoch [56/120    avg_loss:0.016, val_acc:0.979]
Epoch [57/120    avg_loss:0.032, val_acc:0.966]
Epoch [58/120    avg_loss:0.026, val_acc:0.975]
Epoch [59/120    avg_loss:0.021, val_acc:0.976]
Epoch [60/120    avg_loss:0.010, val_acc:0.984]
Epoch [61/120    avg_loss:0.013, val_acc:0.983]
Epoch [62/120    avg_loss:0.017, val_acc:0.986]
Epoch [63/120    avg_loss:0.015, val_acc:0.983]
Epoch [64/120    avg_loss:0.013, val_acc:0.985]
Epoch [65/120    avg_loss:0.009, val_acc:0.983]
Epoch [66/120    avg_loss:0.008, val_acc:0.980]
Epoch [67/120    avg_loss:0.010, val_acc:0.976]
Epoch [68/120    avg_loss:0.022, val_acc:0.957]
Epoch [69/120    avg_loss:0.013, val_acc:0.983]
Epoch [70/120    avg_loss:0.014, val_acc:0.983]
Epoch [71/120    avg_loss:0.013, val_acc:0.984]
Epoch [72/120    avg_loss:0.020, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.980]
Epoch [75/120    avg_loss:0.014, val_acc:0.975]
Epoch [76/120    avg_loss:0.014, val_acc:0.986]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.010, val_acc:0.989]
Epoch [79/120    avg_loss:0.005, val_acc:0.989]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.007, val_acc:0.992]
Epoch [82/120    avg_loss:0.003, val_acc:0.992]
Epoch [83/120    avg_loss:0.011, val_acc:0.993]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.993]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.010, val_acc:0.992]
Epoch [89/120    avg_loss:0.008, val_acc:0.993]
Epoch [90/120    avg_loss:0.007, val_acc:0.993]
Epoch [91/120    avg_loss:0.012, val_acc:0.991]
Epoch [92/120    avg_loss:0.013, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.991]
Epoch [101/120    avg_loss:0.009, val_acc:0.991]
Epoch [102/120    avg_loss:0.003, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.008, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.993]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.993]
Epoch [110/120    avg_loss:0.005, val_acc:0.993]
Epoch [111/120    avg_loss:0.006, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.993]
Epoch [113/120    avg_loss:0.006, val_acc:0.993]
Epoch [114/120    avg_loss:0.009, val_acc:0.993]
Epoch [115/120    avg_loss:0.008, val_acc:0.993]
Epoch [116/120    avg_loss:0.004, val_acc:0.993]
Epoch [117/120    avg_loss:0.005, val_acc:0.993]
Epoch [118/120    avg_loss:0.005, val_acc:0.993]
Epoch [119/120    avg_loss:0.009, val_acc:0.993]
Epoch [120/120    avg_loss:0.005, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     3     0     0     0    10     9     3]
 [    0     0 18046     0     3     0    41     0     0     0]
 [    0     0     0  1961     0     0     0     0    74     1]
 [    0    15     0     0  2955     0     0     0     2     0]
 [    0     0     0     8     0  1297     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    10     0     0     0     0     0  1280     0     0]
 [    0     7     0    43    31     0     1     0  3489     0]
 [    0     0     0     0     1    16     0     0     0   902]]

Accuracy:
99.33000747113971

F1 scores:
[       nan 0.99557144 0.99878238 0.96815601 0.99127809 0.9908327
 0.99571341 0.99224806 0.97662701 0.98849315]

Kappa:
0.9911257876074736
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f504c40e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.196, val_acc:0.595]
Epoch [2/120    avg_loss:0.600, val_acc:0.711]
Epoch [3/120    avg_loss:0.458, val_acc:0.808]
Epoch [4/120    avg_loss:0.338, val_acc:0.833]
Epoch [5/120    avg_loss:0.356, val_acc:0.867]
Epoch [6/120    avg_loss:0.312, val_acc:0.884]
Epoch [7/120    avg_loss:0.308, val_acc:0.904]
Epoch [8/120    avg_loss:0.257, val_acc:0.822]
Epoch [9/120    avg_loss:0.222, val_acc:0.920]
Epoch [10/120    avg_loss:0.230, val_acc:0.896]
Epoch [11/120    avg_loss:0.222, val_acc:0.888]
Epoch [12/120    avg_loss:0.176, val_acc:0.918]
Epoch [13/120    avg_loss:0.156, val_acc:0.882]
Epoch [14/120    avg_loss:0.145, val_acc:0.939]
Epoch [15/120    avg_loss:0.127, val_acc:0.937]
Epoch [16/120    avg_loss:0.129, val_acc:0.947]
Epoch [17/120    avg_loss:0.115, val_acc:0.930]
Epoch [18/120    avg_loss:0.079, val_acc:0.948]
Epoch [19/120    avg_loss:0.080, val_acc:0.926]
Epoch [20/120    avg_loss:0.057, val_acc:0.952]
Epoch [21/120    avg_loss:0.149, val_acc:0.922]
Epoch [22/120    avg_loss:0.093, val_acc:0.953]
Epoch [23/120    avg_loss:0.087, val_acc:0.956]
Epoch [24/120    avg_loss:0.110, val_acc:0.953]
Epoch [25/120    avg_loss:0.086, val_acc:0.946]
Epoch [26/120    avg_loss:0.072, val_acc:0.954]
Epoch [27/120    avg_loss:0.068, val_acc:0.957]
Epoch [28/120    avg_loss:0.060, val_acc:0.950]
Epoch [29/120    avg_loss:0.051, val_acc:0.954]
Epoch [30/120    avg_loss:0.069, val_acc:0.948]
Epoch [31/120    avg_loss:0.100, val_acc:0.929]
Epoch [32/120    avg_loss:0.110, val_acc:0.949]
Epoch [33/120    avg_loss:0.048, val_acc:0.972]
Epoch [34/120    avg_loss:0.034, val_acc:0.966]
Epoch [35/120    avg_loss:0.043, val_acc:0.951]
Epoch [36/120    avg_loss:0.040, val_acc:0.975]
Epoch [37/120    avg_loss:0.035, val_acc:0.954]
Epoch [38/120    avg_loss:0.040, val_acc:0.948]
Epoch [39/120    avg_loss:0.047, val_acc:0.967]
Epoch [40/120    avg_loss:0.025, val_acc:0.975]
Epoch [41/120    avg_loss:0.026, val_acc:0.974]
Epoch [42/120    avg_loss:0.051, val_acc:0.972]
Epoch [43/120    avg_loss:0.048, val_acc:0.961]
Epoch [44/120    avg_loss:0.029, val_acc:0.981]
Epoch [45/120    avg_loss:0.025, val_acc:0.976]
Epoch [46/120    avg_loss:0.042, val_acc:0.974]
Epoch [47/120    avg_loss:0.029, val_acc:0.975]
Epoch [48/120    avg_loss:0.020, val_acc:0.972]
Epoch [49/120    avg_loss:0.030, val_acc:0.964]
Epoch [50/120    avg_loss:0.098, val_acc:0.943]
Epoch [51/120    avg_loss:0.068, val_acc:0.960]
Epoch [52/120    avg_loss:0.031, val_acc:0.978]
Epoch [53/120    avg_loss:0.027, val_acc:0.966]
Epoch [54/120    avg_loss:0.024, val_acc:0.975]
Epoch [55/120    avg_loss:0.016, val_acc:0.974]
Epoch [56/120    avg_loss:0.025, val_acc:0.965]
Epoch [57/120    avg_loss:0.038, val_acc:0.976]
Epoch [58/120    avg_loss:0.017, val_acc:0.976]
Epoch [59/120    avg_loss:0.012, val_acc:0.976]
Epoch [60/120    avg_loss:0.023, val_acc:0.974]
Epoch [61/120    avg_loss:0.021, val_acc:0.975]
Epoch [62/120    avg_loss:0.022, val_acc:0.975]
Epoch [63/120    avg_loss:0.023, val_acc:0.976]
Epoch [64/120    avg_loss:0.013, val_acc:0.971]
Epoch [65/120    avg_loss:0.021, val_acc:0.978]
Epoch [66/120    avg_loss:0.012, val_acc:0.976]
Epoch [67/120    avg_loss:0.031, val_acc:0.975]
Epoch [68/120    avg_loss:0.013, val_acc:0.975]
Epoch [69/120    avg_loss:0.013, val_acc:0.976]
Epoch [70/120    avg_loss:0.012, val_acc:0.978]
Epoch [71/120    avg_loss:0.009, val_acc:0.978]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.009, val_acc:0.977]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.010, val_acc:0.977]
Epoch [76/120    avg_loss:0.012, val_acc:0.977]
Epoch [77/120    avg_loss:0.013, val_acc:0.978]
Epoch [78/120    avg_loss:0.011, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.977]
Epoch [80/120    avg_loss:0.009, val_acc:0.977]
Epoch [81/120    avg_loss:0.012, val_acc:0.977]
Epoch [82/120    avg_loss:0.012, val_acc:0.977]
Epoch [83/120    avg_loss:0.013, val_acc:0.978]
Epoch [84/120    avg_loss:0.010, val_acc:0.978]
Epoch [85/120    avg_loss:0.009, val_acc:0.978]
Epoch [86/120    avg_loss:0.024, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.978]
Epoch [88/120    avg_loss:0.008, val_acc:0.978]
Epoch [89/120    avg_loss:0.012, val_acc:0.978]
Epoch [90/120    avg_loss:0.019, val_acc:0.978]
Epoch [91/120    avg_loss:0.010, val_acc:0.978]
Epoch [92/120    avg_loss:0.019, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.978]
Epoch [94/120    avg_loss:0.011, val_acc:0.978]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.978]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.007, val_acc:0.978]
Epoch [99/120    avg_loss:0.010, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.978]
Epoch [101/120    avg_loss:0.019, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.978]
Epoch [103/120    avg_loss:0.011, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.013, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.978]
Epoch [109/120    avg_loss:0.015, val_acc:0.978]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.021, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.978]
Epoch [114/120    avg_loss:0.012, val_acc:0.978]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.978]
Epoch [117/120    avg_loss:0.015, val_acc:0.978]
Epoch [118/120    avg_loss:0.015, val_acc:0.978]
Epoch [119/120    avg_loss:0.014, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     3     0     0     0     0    31     0]
 [    0     0 18014     0     9     0    63     0     4     0]
 [    0     5     0  1948     0     0     0     0    81     2]
 [    0    22     0     2  2933     0     8     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4863     0     0     0]
 [    0     8     0     0     0     0     1  1281     0     0]
 [    0    27     0    47    30     0     0     0  3467     0]
 [    0     0     0     0     1     9     0     0     0   909]]

Accuracy:
99.09623309955896

F1 scores:
[       nan 0.99255352 0.99748055 0.96531219 0.98671152 0.99656357
 0.99113421 0.99649942 0.96857103 0.99235808]

Kappa:
0.9880306257945725
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d0539b748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.187, val_acc:0.700]
Epoch [2/120    avg_loss:0.577, val_acc:0.518]
Epoch [3/120    avg_loss:0.565, val_acc:0.773]
Epoch [4/120    avg_loss:0.495, val_acc:0.827]
Epoch [5/120    avg_loss:0.318, val_acc:0.773]
Epoch [6/120    avg_loss:0.314, val_acc:0.854]
Epoch [7/120    avg_loss:0.313, val_acc:0.893]
Epoch [8/120    avg_loss:0.257, val_acc:0.790]
Epoch [9/120    avg_loss:0.240, val_acc:0.916]
Epoch [10/120    avg_loss:0.228, val_acc:0.820]
Epoch [11/120    avg_loss:0.161, val_acc:0.919]
Epoch [12/120    avg_loss:0.137, val_acc:0.903]
Epoch [13/120    avg_loss:0.158, val_acc:0.914]
Epoch [14/120    avg_loss:0.152, val_acc:0.917]
Epoch [15/120    avg_loss:0.118, val_acc:0.933]
Epoch [16/120    avg_loss:0.109, val_acc:0.941]
Epoch [17/120    avg_loss:0.172, val_acc:0.930]
Epoch [18/120    avg_loss:0.144, val_acc:0.933]
Epoch [19/120    avg_loss:0.125, val_acc:0.937]
Epoch [20/120    avg_loss:0.072, val_acc:0.952]
Epoch [21/120    avg_loss:0.064, val_acc:0.954]
Epoch [22/120    avg_loss:0.064, val_acc:0.924]
Epoch [23/120    avg_loss:0.075, val_acc:0.949]
Epoch [24/120    avg_loss:0.068, val_acc:0.947]
Epoch [25/120    avg_loss:0.094, val_acc:0.902]
Epoch [26/120    avg_loss:0.128, val_acc:0.946]
Epoch [27/120    avg_loss:0.076, val_acc:0.907]
Epoch [28/120    avg_loss:0.053, val_acc:0.917]
Epoch [29/120    avg_loss:0.046, val_acc:0.961]
Epoch [30/120    avg_loss:0.047, val_acc:0.962]
Epoch [31/120    avg_loss:0.036, val_acc:0.970]
Epoch [32/120    avg_loss:0.041, val_acc:0.938]
Epoch [33/120    avg_loss:0.046, val_acc:0.918]
Epoch [34/120    avg_loss:0.064, val_acc:0.940]
Epoch [35/120    avg_loss:0.064, val_acc:0.957]
Epoch [36/120    avg_loss:0.035, val_acc:0.959]
Epoch [37/120    avg_loss:0.044, val_acc:0.961]
Epoch [38/120    avg_loss:0.030, val_acc:0.967]
Epoch [39/120    avg_loss:0.051, val_acc:0.971]
Epoch [40/120    avg_loss:0.058, val_acc:0.972]
Epoch [41/120    avg_loss:0.059, val_acc:0.957]
Epoch [42/120    avg_loss:0.037, val_acc:0.980]
Epoch [43/120    avg_loss:0.015, val_acc:0.978]
Epoch [44/120    avg_loss:0.016, val_acc:0.982]
Epoch [45/120    avg_loss:0.035, val_acc:0.922]
Epoch [46/120    avg_loss:0.017, val_acc:0.978]
Epoch [47/120    avg_loss:0.016, val_acc:0.984]
Epoch [48/120    avg_loss:0.015, val_acc:0.980]
Epoch [49/120    avg_loss:0.037, val_acc:0.968]
Epoch [50/120    avg_loss:0.028, val_acc:0.975]
Epoch [51/120    avg_loss:0.016, val_acc:0.979]
Epoch [52/120    avg_loss:0.016, val_acc:0.977]
Epoch [53/120    avg_loss:0.011, val_acc:0.984]
Epoch [54/120    avg_loss:0.020, val_acc:0.983]
Epoch [55/120    avg_loss:0.133, val_acc:0.930]
Epoch [56/120    avg_loss:0.056, val_acc:0.965]
Epoch [57/120    avg_loss:0.027, val_acc:0.965]
Epoch [58/120    avg_loss:0.022, val_acc:0.965]
Epoch [59/120    avg_loss:0.018, val_acc:0.954]
Epoch [60/120    avg_loss:0.023, val_acc:0.975]
Epoch [61/120    avg_loss:0.013, val_acc:0.981]
Epoch [62/120    avg_loss:0.016, val_acc:0.982]
Epoch [63/120    avg_loss:0.029, val_acc:0.983]
Epoch [64/120    avg_loss:0.019, val_acc:0.977]
Epoch [65/120    avg_loss:0.014, val_acc:0.965]
Epoch [66/120    avg_loss:0.034, val_acc:0.961]
Epoch [67/120    avg_loss:0.029, val_acc:0.976]
Epoch [68/120    avg_loss:0.012, val_acc:0.982]
Epoch [69/120    avg_loss:0.017, val_acc:0.983]
Epoch [70/120    avg_loss:0.009, val_acc:0.985]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.018, val_acc:0.984]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.013, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.012, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.015, val_acc:0.985]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.015, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     3     0     0     0     2     4     0]
 [    0     0 17964     0    13     0   110     0     3     0]
 [    0     4     0  1939     0     0     0     0    92     1]
 [    0     8     6     0  2948     0     4     0     1     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0    15     0     0     0     0     1  1273     0     1]
 [    0     0     0    27    31     0     4     0  3509     0]
 [    0     0     0     0     0    18     0     0     0   901]]

Accuracy:
99.14684404598366

F1 scores:
[       nan 0.9972054  0.9963118  0.96828964 0.98859826 0.99315068
 0.98784687 0.99259259 0.97743733 0.98631637]

Kappa:
0.9887074104517076
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ecfb66748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.236, val_acc:0.690]
Epoch [2/120    avg_loss:0.647, val_acc:0.725]
Epoch [3/120    avg_loss:0.485, val_acc:0.807]
Epoch [4/120    avg_loss:0.467, val_acc:0.869]
Epoch [5/120    avg_loss:0.383, val_acc:0.834]
Epoch [6/120    avg_loss:0.326, val_acc:0.852]
Epoch [7/120    avg_loss:0.289, val_acc:0.929]
Epoch [8/120    avg_loss:0.238, val_acc:0.830]
Epoch [9/120    avg_loss:0.170, val_acc:0.917]
Epoch [10/120    avg_loss:0.234, val_acc:0.882]
Epoch [11/120    avg_loss:0.173, val_acc:0.928]
Epoch [12/120    avg_loss:0.169, val_acc:0.925]
Epoch [13/120    avg_loss:0.136, val_acc:0.946]
Epoch [14/120    avg_loss:0.135, val_acc:0.948]
Epoch [15/120    avg_loss:0.122, val_acc:0.918]
Epoch [16/120    avg_loss:0.143, val_acc:0.957]
Epoch [17/120    avg_loss:0.104, val_acc:0.943]
Epoch [18/120    avg_loss:0.103, val_acc:0.913]
Epoch [19/120    avg_loss:0.075, val_acc:0.953]
Epoch [20/120    avg_loss:0.073, val_acc:0.942]
Epoch [21/120    avg_loss:0.078, val_acc:0.959]
Epoch [22/120    avg_loss:0.065, val_acc:0.964]
Epoch [23/120    avg_loss:0.049, val_acc:0.970]
Epoch [24/120    avg_loss:0.074, val_acc:0.950]
Epoch [25/120    avg_loss:0.121, val_acc:0.940]
Epoch [26/120    avg_loss:0.101, val_acc:0.933]
Epoch [27/120    avg_loss:0.063, val_acc:0.958]
Epoch [28/120    avg_loss:0.083, val_acc:0.969]
Epoch [29/120    avg_loss:0.068, val_acc:0.956]
Epoch [30/120    avg_loss:0.038, val_acc:0.964]
Epoch [31/120    avg_loss:0.038, val_acc:0.977]
Epoch [32/120    avg_loss:0.032, val_acc:0.978]
Epoch [33/120    avg_loss:0.069, val_acc:0.973]
Epoch [34/120    avg_loss:0.040, val_acc:0.965]
Epoch [35/120    avg_loss:0.055, val_acc:0.973]
Epoch [36/120    avg_loss:0.051, val_acc:0.953]
Epoch [37/120    avg_loss:0.048, val_acc:0.974]
Epoch [38/120    avg_loss:0.037, val_acc:0.971]
Epoch [39/120    avg_loss:0.027, val_acc:0.980]
Epoch [40/120    avg_loss:0.017, val_acc:0.973]
Epoch [41/120    avg_loss:0.049, val_acc:0.978]
Epoch [42/120    avg_loss:0.034, val_acc:0.985]
Epoch [43/120    avg_loss:0.052, val_acc:0.978]
Epoch [44/120    avg_loss:0.035, val_acc:0.963]
Epoch [45/120    avg_loss:0.048, val_acc:0.958]
Epoch [46/120    avg_loss:0.041, val_acc:0.948]
Epoch [47/120    avg_loss:0.030, val_acc:0.980]
Epoch [48/120    avg_loss:0.022, val_acc:0.983]
Epoch [49/120    avg_loss:0.013, val_acc:0.982]
Epoch [50/120    avg_loss:0.013, val_acc:0.990]
Epoch [51/120    avg_loss:0.008, val_acc:0.988]
Epoch [52/120    avg_loss:0.014, val_acc:0.988]
Epoch [53/120    avg_loss:0.011, val_acc:0.985]
Epoch [54/120    avg_loss:0.015, val_acc:0.988]
Epoch [55/120    avg_loss:0.021, val_acc:0.991]
Epoch [56/120    avg_loss:0.015, val_acc:0.987]
Epoch [57/120    avg_loss:0.024, val_acc:0.992]
Epoch [58/120    avg_loss:0.015, val_acc:0.984]
Epoch [59/120    avg_loss:0.010, val_acc:0.988]
Epoch [60/120    avg_loss:0.006, val_acc:0.990]
Epoch [61/120    avg_loss:0.009, val_acc:0.981]
Epoch [62/120    avg_loss:0.021, val_acc:0.984]
Epoch [63/120    avg_loss:0.019, val_acc:0.979]
Epoch [64/120    avg_loss:0.025, val_acc:0.984]
Epoch [65/120    avg_loss:0.011, val_acc:0.987]
Epoch [66/120    avg_loss:0.012, val_acc:0.990]
Epoch [67/120    avg_loss:0.022, val_acc:0.989]
Epoch [68/120    avg_loss:0.015, val_acc:0.983]
Epoch [69/120    avg_loss:0.008, val_acc:0.982]
Epoch [70/120    avg_loss:0.016, val_acc:0.988]
Epoch [71/120    avg_loss:0.010, val_acc:0.991]
Epoch [72/120    avg_loss:0.009, val_acc:0.992]
Epoch [73/120    avg_loss:0.011, val_acc:0.994]
Epoch [74/120    avg_loss:0.009, val_acc:0.993]
Epoch [75/120    avg_loss:0.008, val_acc:0.993]
Epoch [76/120    avg_loss:0.010, val_acc:0.993]
Epoch [77/120    avg_loss:0.010, val_acc:0.994]
Epoch [78/120    avg_loss:0.006, val_acc:0.994]
Epoch [79/120    avg_loss:0.011, val_acc:0.993]
Epoch [80/120    avg_loss:0.010, val_acc:0.994]
Epoch [81/120    avg_loss:0.008, val_acc:0.995]
Epoch [82/120    avg_loss:0.013, val_acc:0.994]
Epoch [83/120    avg_loss:0.004, val_acc:0.994]
Epoch [84/120    avg_loss:0.007, val_acc:0.994]
Epoch [85/120    avg_loss:0.007, val_acc:0.994]
Epoch [86/120    avg_loss:0.005, val_acc:0.994]
Epoch [87/120    avg_loss:0.003, val_acc:0.994]
Epoch [88/120    avg_loss:0.007, val_acc:0.994]
Epoch [89/120    avg_loss:0.013, val_acc:0.993]
Epoch [90/120    avg_loss:0.020, val_acc:0.993]
Epoch [91/120    avg_loss:0.009, val_acc:0.993]
Epoch [92/120    avg_loss:0.009, val_acc:0.994]
Epoch [93/120    avg_loss:0.005, val_acc:0.995]
Epoch [94/120    avg_loss:0.006, val_acc:0.995]
Epoch [95/120    avg_loss:0.010, val_acc:0.993]
Epoch [96/120    avg_loss:0.008, val_acc:0.994]
Epoch [97/120    avg_loss:0.005, val_acc:0.994]
Epoch [98/120    avg_loss:0.008, val_acc:0.994]
Epoch [99/120    avg_loss:0.004, val_acc:0.994]
Epoch [100/120    avg_loss:0.005, val_acc:0.995]
Epoch [101/120    avg_loss:0.005, val_acc:0.995]
Epoch [102/120    avg_loss:0.006, val_acc:0.995]
Epoch [103/120    avg_loss:0.018, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.005, val_acc:0.995]
Epoch [106/120    avg_loss:0.005, val_acc:0.995]
Epoch [107/120    avg_loss:0.007, val_acc:0.995]
Epoch [108/120    avg_loss:0.008, val_acc:0.995]
Epoch [109/120    avg_loss:0.007, val_acc:0.993]
Epoch [110/120    avg_loss:0.009, val_acc:0.994]
Epoch [111/120    avg_loss:0.006, val_acc:0.994]
Epoch [112/120    avg_loss:0.004, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.007, val_acc:0.994]
Epoch [115/120    avg_loss:0.004, val_acc:0.995]
Epoch [116/120    avg_loss:0.006, val_acc:0.995]
Epoch [117/120    avg_loss:0.004, val_acc:0.995]
Epoch [118/120    avg_loss:0.004, val_acc:0.995]
Epoch [119/120    avg_loss:0.007, val_acc:0.993]
Epoch [120/120    avg_loss:0.006, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     0     0     0    18     0     1]
 [    0     4 18034     0    18     0    34     0     0     0]
 [    0     0     0  2000     0     0     0     0    28     8]
 [    0    18     0     0  2946     0     0     0     6     2]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4863     0    15     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     7     0    24    30     0     0     0  3503     7]
 [    0     0     0     0     0    13     0     0     0   906]]

Accuracy:
99.43363940905695

F1 scores:
[       nan 0.99627156 0.99844978 0.98522167 0.98759638 0.99466056
 0.99498721 0.99268387 0.98357434 0.98211382]

Kappa:
0.9925002223546775
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0445b20780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.131, val_acc:0.614]
Epoch [2/120    avg_loss:0.612, val_acc:0.688]
Epoch [3/120    avg_loss:0.510, val_acc:0.772]
Epoch [4/120    avg_loss:0.332, val_acc:0.850]
Epoch [5/120    avg_loss:0.334, val_acc:0.802]
Epoch [6/120    avg_loss:0.372, val_acc:0.897]
Epoch [7/120    avg_loss:0.241, val_acc:0.870]
Epoch [8/120    avg_loss:0.237, val_acc:0.843]
Epoch [9/120    avg_loss:0.244, val_acc:0.870]
Epoch [10/120    avg_loss:0.176, val_acc:0.917]
Epoch [11/120    avg_loss:0.171, val_acc:0.839]
Epoch [12/120    avg_loss:0.150, val_acc:0.931]
Epoch [13/120    avg_loss:0.098, val_acc:0.944]
Epoch [14/120    avg_loss:0.098, val_acc:0.919]
Epoch [15/120    avg_loss:0.122, val_acc:0.941]
Epoch [16/120    avg_loss:0.102, val_acc:0.903]
Epoch [17/120    avg_loss:0.131, val_acc:0.901]
Epoch [18/120    avg_loss:0.105, val_acc:0.946]
Epoch [19/120    avg_loss:0.055, val_acc:0.952]
Epoch [20/120    avg_loss:0.072, val_acc:0.957]
Epoch [21/120    avg_loss:0.057, val_acc:0.956]
Epoch [22/120    avg_loss:0.071, val_acc:0.940]
Epoch [23/120    avg_loss:0.069, val_acc:0.945]
Epoch [24/120    avg_loss:0.166, val_acc:0.930]
Epoch [25/120    avg_loss:0.064, val_acc:0.930]
Epoch [26/120    avg_loss:0.110, val_acc:0.953]
Epoch [27/120    avg_loss:0.068, val_acc:0.941]
Epoch [28/120    avg_loss:0.059, val_acc:0.955]
Epoch [29/120    avg_loss:0.038, val_acc:0.965]
Epoch [30/120    avg_loss:0.030, val_acc:0.964]
Epoch [31/120    avg_loss:0.068, val_acc:0.877]
Epoch [32/120    avg_loss:0.112, val_acc:0.959]
Epoch [33/120    avg_loss:0.195, val_acc:0.943]
Epoch [34/120    avg_loss:0.054, val_acc:0.959]
Epoch [35/120    avg_loss:0.048, val_acc:0.948]
Epoch [36/120    avg_loss:0.039, val_acc:0.968]
Epoch [37/120    avg_loss:0.041, val_acc:0.959]
Epoch [38/120    avg_loss:0.061, val_acc:0.914]
Epoch [39/120    avg_loss:0.114, val_acc:0.951]
Epoch [40/120    avg_loss:0.042, val_acc:0.974]
Epoch [41/120    avg_loss:0.033, val_acc:0.974]
Epoch [42/120    avg_loss:0.033, val_acc:0.977]
Epoch [43/120    avg_loss:0.034, val_acc:0.970]
Epoch [44/120    avg_loss:0.037, val_acc:0.968]
Epoch [45/120    avg_loss:0.033, val_acc:0.971]
Epoch [46/120    avg_loss:0.019, val_acc:0.970]
Epoch [47/120    avg_loss:0.033, val_acc:0.973]
Epoch [48/120    avg_loss:0.018, val_acc:0.968]
Epoch [49/120    avg_loss:0.014, val_acc:0.981]
Epoch [50/120    avg_loss:0.021, val_acc:0.953]
Epoch [51/120    avg_loss:0.021, val_acc:0.971]
Epoch [52/120    avg_loss:0.014, val_acc:0.977]
Epoch [53/120    avg_loss:0.034, val_acc:0.973]
Epoch [54/120    avg_loss:0.030, val_acc:0.978]
Epoch [55/120    avg_loss:0.015, val_acc:0.968]
Epoch [56/120    avg_loss:0.015, val_acc:0.980]
Epoch [57/120    avg_loss:0.010, val_acc:0.982]
Epoch [58/120    avg_loss:0.009, val_acc:0.981]
Epoch [59/120    avg_loss:0.013, val_acc:0.983]
Epoch [60/120    avg_loss:0.015, val_acc:0.970]
Epoch [61/120    avg_loss:0.026, val_acc:0.969]
Epoch [62/120    avg_loss:0.015, val_acc:0.982]
Epoch [63/120    avg_loss:0.011, val_acc:0.981]
Epoch [64/120    avg_loss:0.006, val_acc:0.983]
Epoch [65/120    avg_loss:0.007, val_acc:0.981]
Epoch [66/120    avg_loss:0.011, val_acc:0.978]
Epoch [67/120    avg_loss:0.020, val_acc:0.981]
Epoch [68/120    avg_loss:0.028, val_acc:0.963]
Epoch [69/120    avg_loss:0.028, val_acc:0.986]
Epoch [70/120    avg_loss:0.018, val_acc:0.984]
Epoch [71/120    avg_loss:0.012, val_acc:0.981]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.981]
Epoch [76/120    avg_loss:0.010, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.007, val_acc:0.987]
Epoch [79/120    avg_loss:0.019, val_acc:0.983]
Epoch [80/120    avg_loss:0.020, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.983]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.007, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.981]
Epoch [85/120    avg_loss:0.006, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.979]
Epoch [88/120    avg_loss:0.004, val_acc:0.981]
Epoch [89/120    avg_loss:0.005, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.004, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.979]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.028, val_acc:0.968]
Epoch [96/120    avg_loss:0.057, val_acc:0.958]
Epoch [97/120    avg_loss:0.038, val_acc:0.971]
Epoch [98/120    avg_loss:0.009, val_acc:0.979]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.978]
Epoch [101/120    avg_loss:0.014, val_acc:0.973]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.975]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.015, val_acc:0.981]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.017, val_acc:0.983]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.023, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6361     0     3     0     0     4    13    49     2]
 [    0     2 18018     0    14     0    55     0     0     1]
 [    0     0     0  1995     0     0     0     0    36     5]
 [    0    14     0     2  2950     0     0     0     5     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     2     0     0     0     0     1  1287     0     0]
 [    0    35     0    19    23     0     0     0  3484    10]
 [    0     0     0     0     2    17     0     0     0   900]]

Accuracy:
99.24083580362954

F1 scores:
[       nan 0.99034719 0.99800598 0.98397041 0.98976682 0.99352874
 0.99388753 0.99382239 0.97522743 0.97932535]

Kappa:
0.9899488456236746
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7446279780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.376, val_acc:0.390]
Epoch [2/120    avg_loss:0.667, val_acc:0.683]
Epoch [3/120    avg_loss:0.585, val_acc:0.666]
Epoch [4/120    avg_loss:0.340, val_acc:0.742]
Epoch [5/120    avg_loss:0.385, val_acc:0.818]
Epoch [6/120    avg_loss:0.276, val_acc:0.869]
Epoch [7/120    avg_loss:0.271, val_acc:0.874]
Epoch [8/120    avg_loss:0.201, val_acc:0.875]
Epoch [9/120    avg_loss:0.191, val_acc:0.927]
Epoch [10/120    avg_loss:0.157, val_acc:0.939]
Epoch [11/120    avg_loss:0.134, val_acc:0.886]
Epoch [12/120    avg_loss:0.195, val_acc:0.927]
Epoch [13/120    avg_loss:0.169, val_acc:0.943]
Epoch [14/120    avg_loss:0.167, val_acc:0.950]
Epoch [15/120    avg_loss:0.132, val_acc:0.921]
Epoch [16/120    avg_loss:0.120, val_acc:0.972]
Epoch [17/120    avg_loss:0.099, val_acc:0.963]
Epoch [18/120    avg_loss:0.073, val_acc:0.959]
Epoch [19/120    avg_loss:0.078, val_acc:0.958]
Epoch [20/120    avg_loss:0.081, val_acc:0.958]
Epoch [21/120    avg_loss:0.060, val_acc:0.948]
Epoch [22/120    avg_loss:0.080, val_acc:0.959]
Epoch [23/120    avg_loss:0.070, val_acc:0.966]
Epoch [24/120    avg_loss:0.057, val_acc:0.944]
Epoch [25/120    avg_loss:0.099, val_acc:0.952]
Epoch [26/120    avg_loss:0.036, val_acc:0.965]
Epoch [27/120    avg_loss:0.033, val_acc:0.974]
Epoch [28/120    avg_loss:0.041, val_acc:0.980]
Epoch [29/120    avg_loss:0.024, val_acc:0.971]
Epoch [30/120    avg_loss:0.060, val_acc:0.976]
Epoch [31/120    avg_loss:0.058, val_acc:0.973]
Epoch [32/120    avg_loss:0.047, val_acc:0.971]
Epoch [33/120    avg_loss:0.027, val_acc:0.966]
Epoch [34/120    avg_loss:0.028, val_acc:0.953]
Epoch [35/120    avg_loss:0.019, val_acc:0.973]
Epoch [36/120    avg_loss:0.020, val_acc:0.974]
Epoch [37/120    avg_loss:0.016, val_acc:0.976]
Epoch [38/120    avg_loss:0.015, val_acc:0.979]
Epoch [39/120    avg_loss:0.021, val_acc:0.976]
Epoch [40/120    avg_loss:0.020, val_acc:0.981]
Epoch [41/120    avg_loss:0.016, val_acc:0.958]
Epoch [42/120    avg_loss:0.088, val_acc:0.964]
Epoch [43/120    avg_loss:0.058, val_acc:0.979]
Epoch [44/120    avg_loss:0.027, val_acc:0.985]
Epoch [45/120    avg_loss:0.051, val_acc:0.959]
Epoch [46/120    avg_loss:0.027, val_acc:0.985]
Epoch [47/120    avg_loss:0.025, val_acc:0.980]
Epoch [48/120    avg_loss:0.014, val_acc:0.983]
Epoch [49/120    avg_loss:0.012, val_acc:0.981]
Epoch [50/120    avg_loss:0.014, val_acc:0.982]
Epoch [51/120    avg_loss:0.016, val_acc:0.986]
Epoch [52/120    avg_loss:0.018, val_acc:0.985]
Epoch [53/120    avg_loss:0.007, val_acc:0.986]
Epoch [54/120    avg_loss:0.017, val_acc:0.980]
Epoch [55/120    avg_loss:0.022, val_acc:0.976]
Epoch [56/120    avg_loss:0.024, val_acc:0.977]
Epoch [57/120    avg_loss:0.017, val_acc:0.976]
Epoch [58/120    avg_loss:0.009, val_acc:0.983]
Epoch [59/120    avg_loss:0.023, val_acc:0.981]
Epoch [60/120    avg_loss:0.013, val_acc:0.983]
Epoch [61/120    avg_loss:0.020, val_acc:0.982]
Epoch [62/120    avg_loss:0.011, val_acc:0.985]
Epoch [63/120    avg_loss:0.008, val_acc:0.985]
Epoch [64/120    avg_loss:0.017, val_acc:0.985]
Epoch [65/120    avg_loss:0.004, val_acc:0.988]
Epoch [66/120    avg_loss:0.010, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.986]
Epoch [68/120    avg_loss:0.004, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.987]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.008, val_acc:0.985]
Epoch [72/120    avg_loss:0.004, val_acc:0.986]
Epoch [73/120    avg_loss:0.005, val_acc:0.986]
Epoch [74/120    avg_loss:0.004, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.987]
Epoch [76/120    avg_loss:0.002, val_acc:0.986]
Epoch [77/120    avg_loss:0.013, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.003, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.979]
Epoch [82/120    avg_loss:0.006, val_acc:0.982]
Epoch [83/120    avg_loss:0.004, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.987]
Epoch [85/120    avg_loss:0.003, val_acc:0.986]
Epoch [86/120    avg_loss:0.004, val_acc:0.986]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.003, val_acc:0.986]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.003, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.003, val_acc:0.987]
Epoch [101/120    avg_loss:0.003, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.003, val_acc:0.987]
Epoch [104/120    avg_loss:0.003, val_acc:0.987]
Epoch [105/120    avg_loss:0.002, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.003, val_acc:0.987]
Epoch [108/120    avg_loss:0.002, val_acc:0.987]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.987]
Epoch [112/120    avg_loss:0.002, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     0     0     0     3     1     1     0]
 [    0     0 18011     0    13     0    66     0     0     0]
 [    0     0     0  1981     0     0     0     0    51     4]
 [    0    26     0     2  2938     0     1     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4869     0     6     0]
 [    0     7     0     0     0     0     0  1283     0     0]
 [    0    11     0     7    30     0     0     0  3520     3]
 [    0     0     0     0     1    21     0     0     0   897]]

Accuracy:
99.3685681922252

F1 scores:
[       nan 0.99620243 0.99772878 0.98410333 0.98689956 0.99201824
 0.99195274 0.996892   0.98406486 0.98409216]

Kappa:
0.9916385633053834
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7df5cb7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.306, val_acc:0.627]
Epoch [2/120    avg_loss:0.676, val_acc:0.711]
Epoch [3/120    avg_loss:0.523, val_acc:0.818]
Epoch [4/120    avg_loss:0.396, val_acc:0.775]
Epoch [5/120    avg_loss:0.336, val_acc:0.843]
Epoch [6/120    avg_loss:0.263, val_acc:0.897]
Epoch [7/120    avg_loss:0.206, val_acc:0.868]
Epoch [8/120    avg_loss:0.245, val_acc:0.917]
Epoch [9/120    avg_loss:0.192, val_acc:0.917]
Epoch [10/120    avg_loss:0.191, val_acc:0.901]
Epoch [11/120    avg_loss:0.183, val_acc:0.942]
Epoch [12/120    avg_loss:0.115, val_acc:0.936]
Epoch [13/120    avg_loss:0.094, val_acc:0.943]
Epoch [14/120    avg_loss:0.097, val_acc:0.958]
Epoch [15/120    avg_loss:0.096, val_acc:0.957]
Epoch [16/120    avg_loss:0.110, val_acc:0.934]
Epoch [17/120    avg_loss:0.108, val_acc:0.956]
Epoch [18/120    avg_loss:0.106, val_acc:0.950]
Epoch [19/120    avg_loss:0.082, val_acc:0.956]
Epoch [20/120    avg_loss:0.083, val_acc:0.922]
Epoch [21/120    avg_loss:0.046, val_acc:0.960]
Epoch [22/120    avg_loss:0.047, val_acc:0.971]
Epoch [23/120    avg_loss:0.088, val_acc:0.948]
Epoch [24/120    avg_loss:0.089, val_acc:0.955]
Epoch [25/120    avg_loss:0.058, val_acc:0.958]
Epoch [26/120    avg_loss:0.081, val_acc:0.936]
Epoch [27/120    avg_loss:0.065, val_acc:0.967]
Epoch [28/120    avg_loss:0.062, val_acc:0.969]
Epoch [29/120    avg_loss:0.043, val_acc:0.947]
Epoch [30/120    avg_loss:0.056, val_acc:0.973]
Epoch [31/120    avg_loss:0.038, val_acc:0.976]
Epoch [32/120    avg_loss:0.042, val_acc:0.977]
Epoch [33/120    avg_loss:0.038, val_acc:0.973]
Epoch [34/120    avg_loss:0.027, val_acc:0.974]
Epoch [35/120    avg_loss:0.030, val_acc:0.978]
Epoch [36/120    avg_loss:0.022, val_acc:0.984]
Epoch [37/120    avg_loss:0.021, val_acc:0.985]
Epoch [38/120    avg_loss:0.040, val_acc:0.967]
Epoch [39/120    avg_loss:0.034, val_acc:0.935]
Epoch [40/120    avg_loss:0.232, val_acc:0.948]
Epoch [41/120    avg_loss:0.075, val_acc:0.970]
Epoch [42/120    avg_loss:0.068, val_acc:0.945]
Epoch [43/120    avg_loss:0.085, val_acc:0.917]
Epoch [44/120    avg_loss:0.067, val_acc:0.954]
Epoch [45/120    avg_loss:0.040, val_acc:0.968]
Epoch [46/120    avg_loss:0.039, val_acc:0.980]
Epoch [47/120    avg_loss:0.037, val_acc:0.980]
Epoch [48/120    avg_loss:0.042, val_acc:0.978]
Epoch [49/120    avg_loss:0.025, val_acc:0.972]
Epoch [50/120    avg_loss:0.024, val_acc:0.978]
Epoch [51/120    avg_loss:0.022, val_acc:0.985]
Epoch [52/120    avg_loss:0.012, val_acc:0.981]
Epoch [53/120    avg_loss:0.017, val_acc:0.983]
Epoch [54/120    avg_loss:0.018, val_acc:0.981]
Epoch [55/120    avg_loss:0.012, val_acc:0.981]
Epoch [56/120    avg_loss:0.022, val_acc:0.982]
Epoch [57/120    avg_loss:0.013, val_acc:0.981]
Epoch [58/120    avg_loss:0.017, val_acc:0.981]
Epoch [59/120    avg_loss:0.010, val_acc:0.983]
Epoch [60/120    avg_loss:0.009, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.981]
Epoch [62/120    avg_loss:0.014, val_acc:0.980]
Epoch [63/120    avg_loss:0.010, val_acc:0.981]
Epoch [64/120    avg_loss:0.007, val_acc:0.983]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.018, val_acc:0.983]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.008, val_acc:0.983]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.008, val_acc:0.983]
Epoch [71/120    avg_loss:0.008, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.983]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.983]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.015, val_acc:0.983]
Epoch [80/120    avg_loss:0.016, val_acc:0.983]
Epoch [81/120    avg_loss:0.007, val_acc:0.983]
Epoch [82/120    avg_loss:0.011, val_acc:0.983]
Epoch [83/120    avg_loss:0.009, val_acc:0.983]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.012, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.019, val_acc:0.983]
Epoch [93/120    avg_loss:0.013, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.016, val_acc:0.983]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.009, val_acc:0.983]
Epoch [104/120    avg_loss:0.014, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.011, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.011, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.011, val_acc:0.983]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     5     0     0     9     4    17     2]
 [    0     0 17803     0    42     0   245     0     0     0]
 [    0     0     0  2027     0     0     0     0     5     4]
 [    0    22     0     4  2937     0     1     0     7     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     2  1280     0     8]
 [    0     1     0    40    45     0     0     0  3479     6]
 [    0     0     0     0     3    16     0     0     0   900]]

Accuracy:
98.82148796182489

F1 scores:
[       nan 0.99533074 0.99200401 0.98589494 0.97916319 0.99390708
 0.97433337 0.99456099 0.98290719 0.97826087]

Kappa:
0.9844318509705251
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa03934c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.202, val_acc:0.594]
Epoch [2/120    avg_loss:0.610, val_acc:0.603]
Epoch [3/120    avg_loss:0.532, val_acc:0.782]
Epoch [4/120    avg_loss:0.382, val_acc:0.843]
Epoch [5/120    avg_loss:0.442, val_acc:0.779]
Epoch [6/120    avg_loss:0.356, val_acc:0.743]
Epoch [7/120    avg_loss:0.259, val_acc:0.833]
Epoch [8/120    avg_loss:0.253, val_acc:0.920]
Epoch [9/120    avg_loss:0.187, val_acc:0.911]
Epoch [10/120    avg_loss:0.187, val_acc:0.895]
Epoch [11/120    avg_loss:0.199, val_acc:0.933]
Epoch [12/120    avg_loss:0.116, val_acc:0.913]
Epoch [13/120    avg_loss:0.162, val_acc:0.907]
Epoch [14/120    avg_loss:0.203, val_acc:0.900]
Epoch [15/120    avg_loss:0.122, val_acc:0.936]
Epoch [16/120    avg_loss:0.082, val_acc:0.940]
Epoch [17/120    avg_loss:0.148, val_acc:0.935]
Epoch [18/120    avg_loss:0.087, val_acc:0.937]
Epoch [19/120    avg_loss:0.076, val_acc:0.952]
Epoch [20/120    avg_loss:0.074, val_acc:0.966]
Epoch [21/120    avg_loss:0.104, val_acc:0.927]
Epoch [22/120    avg_loss:0.097, val_acc:0.936]
Epoch [23/120    avg_loss:0.056, val_acc:0.962]
Epoch [24/120    avg_loss:0.048, val_acc:0.956]
Epoch [25/120    avg_loss:0.040, val_acc:0.964]
Epoch [26/120    avg_loss:0.054, val_acc:0.974]
Epoch [27/120    avg_loss:0.033, val_acc:0.962]
Epoch [28/120    avg_loss:0.052, val_acc:0.980]
Epoch [29/120    avg_loss:0.049, val_acc:0.963]
Epoch [30/120    avg_loss:0.071, val_acc:0.956]
Epoch [31/120    avg_loss:0.098, val_acc:0.941]
Epoch [32/120    avg_loss:0.041, val_acc:0.971]
Epoch [33/120    avg_loss:0.049, val_acc:0.964]
Epoch [34/120    avg_loss:0.039, val_acc:0.959]
Epoch [35/120    avg_loss:0.063, val_acc:0.965]
Epoch [36/120    avg_loss:0.030, val_acc:0.973]
Epoch [37/120    avg_loss:0.030, val_acc:0.959]
Epoch [38/120    avg_loss:0.021, val_acc:0.979]
Epoch [39/120    avg_loss:0.044, val_acc:0.973]
Epoch [40/120    avg_loss:0.024, val_acc:0.978]
Epoch [41/120    avg_loss:0.033, val_acc:0.986]
Epoch [42/120    avg_loss:0.047, val_acc:0.968]
Epoch [43/120    avg_loss:0.020, val_acc:0.984]
Epoch [44/120    avg_loss:0.022, val_acc:0.972]
Epoch [45/120    avg_loss:0.019, val_acc:0.971]
Epoch [46/120    avg_loss:0.037, val_acc:0.978]
Epoch [47/120    avg_loss:0.044, val_acc:0.966]
Epoch [48/120    avg_loss:0.027, val_acc:0.986]
Epoch [49/120    avg_loss:0.035, val_acc:0.972]
Epoch [50/120    avg_loss:0.028, val_acc:0.980]
Epoch [51/120    avg_loss:0.024, val_acc:0.986]
Epoch [52/120    avg_loss:0.019, val_acc:0.973]
Epoch [53/120    avg_loss:0.014, val_acc:0.984]
Epoch [54/120    avg_loss:0.016, val_acc:0.981]
Epoch [55/120    avg_loss:0.029, val_acc:0.974]
Epoch [56/120    avg_loss:0.031, val_acc:0.972]
Epoch [57/120    avg_loss:0.021, val_acc:0.984]
Epoch [58/120    avg_loss:0.021, val_acc:0.970]
Epoch [59/120    avg_loss:0.039, val_acc:0.982]
Epoch [60/120    avg_loss:0.018, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.985]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.011, val_acc:0.992]
Epoch [64/120    avg_loss:0.013, val_acc:0.993]
Epoch [65/120    avg_loss:0.019, val_acc:0.984]
Epoch [66/120    avg_loss:0.025, val_acc:0.990]
Epoch [67/120    avg_loss:0.028, val_acc:0.985]
Epoch [68/120    avg_loss:0.017, val_acc:0.979]
Epoch [69/120    avg_loss:0.013, val_acc:0.986]
Epoch [70/120    avg_loss:0.006, val_acc:0.991]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.024, val_acc:0.980]
Epoch [73/120    avg_loss:0.025, val_acc:0.990]
Epoch [74/120    avg_loss:0.010, val_acc:0.991]
Epoch [75/120    avg_loss:0.008, val_acc:0.990]
Epoch [76/120    avg_loss:0.007, val_acc:0.992]
Epoch [77/120    avg_loss:0.006, val_acc:0.990]
Epoch [78/120    avg_loss:0.008, val_acc:0.991]
Epoch [79/120    avg_loss:0.009, val_acc:0.993]
Epoch [80/120    avg_loss:0.006, val_acc:0.993]
Epoch [81/120    avg_loss:0.003, val_acc:0.993]
Epoch [82/120    avg_loss:0.004, val_acc:0.993]
Epoch [83/120    avg_loss:0.003, val_acc:0.993]
Epoch [84/120    avg_loss:0.004, val_acc:0.993]
Epoch [85/120    avg_loss:0.013, val_acc:0.993]
Epoch [86/120    avg_loss:0.007, val_acc:0.993]
Epoch [87/120    avg_loss:0.004, val_acc:0.993]
Epoch [88/120    avg_loss:0.003, val_acc:0.993]
Epoch [89/120    avg_loss:0.007, val_acc:0.994]
Epoch [90/120    avg_loss:0.005, val_acc:0.993]
Epoch [91/120    avg_loss:0.010, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.993]
Epoch [93/120    avg_loss:0.012, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.993]
Epoch [96/120    avg_loss:0.006, val_acc:0.993]
Epoch [97/120    avg_loss:0.004, val_acc:0.993]
Epoch [98/120    avg_loss:0.003, val_acc:0.993]
Epoch [99/120    avg_loss:0.004, val_acc:0.994]
Epoch [100/120    avg_loss:0.006, val_acc:0.994]
Epoch [101/120    avg_loss:0.006, val_acc:0.993]
Epoch [102/120    avg_loss:0.005, val_acc:0.993]
Epoch [103/120    avg_loss:0.007, val_acc:0.994]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.005, val_acc:0.993]
Epoch [106/120    avg_loss:0.006, val_acc:0.993]
Epoch [107/120    avg_loss:0.007, val_acc:0.993]
Epoch [108/120    avg_loss:0.004, val_acc:0.994]
Epoch [109/120    avg_loss:0.003, val_acc:0.995]
Epoch [110/120    avg_loss:0.003, val_acc:0.995]
Epoch [111/120    avg_loss:0.010, val_acc:0.993]
Epoch [112/120    avg_loss:0.007, val_acc:0.994]
Epoch [113/120    avg_loss:0.009, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.006, val_acc:0.993]
Epoch [116/120    avg_loss:0.003, val_acc:0.993]
Epoch [117/120    avg_loss:0.004, val_acc:0.993]
Epoch [118/120    avg_loss:0.007, val_acc:0.995]
Epoch [119/120    avg_loss:0.006, val_acc:0.994]
Epoch [120/120    avg_loss:0.007, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     0     0     0     3     0     0]
 [    0     0 18077     0     8     0     5     0     0     0]
 [    0     0     0  1986     0     0     0     0    43     7]
 [    0    15     3     3  2942     0     1     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0     1     0     0     0     0     2  1284     0     3]
 [    0     6     0    21    34     0     0     0  3509     1]
 [    0     0     0     0     1    27     0     0     0   891]]

Accuracy:
99.53004121177067

F1 scores:
[       nan 0.99805946 0.99955764 0.98171033 0.98774551 0.98976109
 0.99887307 0.99650757 0.98373984 0.9785832 ]

Kappa:
0.9937730565741763
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7facfe423748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.234, val_acc:0.443]
Epoch [2/120    avg_loss:0.706, val_acc:0.652]
Epoch [3/120    avg_loss:0.515, val_acc:0.645]
Epoch [4/120    avg_loss:0.470, val_acc:0.799]
Epoch [5/120    avg_loss:0.389, val_acc:0.834]
Epoch [6/120    avg_loss:0.307, val_acc:0.865]
Epoch [7/120    avg_loss:0.292, val_acc:0.869]
Epoch [8/120    avg_loss:0.278, val_acc:0.864]
Epoch [9/120    avg_loss:0.213, val_acc:0.895]
Epoch [10/120    avg_loss:0.161, val_acc:0.928]
Epoch [11/120    avg_loss:0.203, val_acc:0.864]
Epoch [12/120    avg_loss:0.115, val_acc:0.937]
Epoch [13/120    avg_loss:0.169, val_acc:0.900]
Epoch [14/120    avg_loss:0.196, val_acc:0.945]
Epoch [15/120    avg_loss:0.139, val_acc:0.924]
Epoch [16/120    avg_loss:0.154, val_acc:0.938]
Epoch [17/120    avg_loss:0.117, val_acc:0.946]
Epoch [18/120    avg_loss:0.136, val_acc:0.894]
Epoch [19/120    avg_loss:0.098, val_acc:0.949]
Epoch [20/120    avg_loss:0.083, val_acc:0.959]
Epoch [21/120    avg_loss:0.080, val_acc:0.942]
Epoch [22/120    avg_loss:0.096, val_acc:0.946]
Epoch [23/120    avg_loss:0.059, val_acc:0.953]
Epoch [24/120    avg_loss:0.085, val_acc:0.952]
Epoch [25/120    avg_loss:0.066, val_acc:0.966]
Epoch [26/120    avg_loss:0.086, val_acc:0.947]
Epoch [27/120    avg_loss:0.092, val_acc:0.894]
Epoch [28/120    avg_loss:0.063, val_acc:0.918]
Epoch [29/120    avg_loss:0.054, val_acc:0.962]
Epoch [30/120    avg_loss:0.050, val_acc:0.963]
Epoch [31/120    avg_loss:0.044, val_acc:0.969]
Epoch [32/120    avg_loss:0.063, val_acc:0.943]
Epoch [33/120    avg_loss:0.034, val_acc:0.968]
Epoch [34/120    avg_loss:0.033, val_acc:0.966]
Epoch [35/120    avg_loss:0.036, val_acc:0.974]
Epoch [36/120    avg_loss:0.030, val_acc:0.963]
Epoch [37/120    avg_loss:0.031, val_acc:0.964]
Epoch [38/120    avg_loss:0.051, val_acc:0.958]
Epoch [39/120    avg_loss:0.051, val_acc:0.974]
Epoch [40/120    avg_loss:0.025, val_acc:0.976]
Epoch [41/120    avg_loss:0.023, val_acc:0.978]
Epoch [42/120    avg_loss:0.058, val_acc:0.970]
Epoch [43/120    avg_loss:0.051, val_acc:0.963]
Epoch [44/120    avg_loss:0.033, val_acc:0.965]
Epoch [45/120    avg_loss:0.023, val_acc:0.978]
Epoch [46/120    avg_loss:0.022, val_acc:0.962]
Epoch [47/120    avg_loss:0.024, val_acc:0.980]
Epoch [48/120    avg_loss:0.037, val_acc:0.981]
Epoch [49/120    avg_loss:0.041, val_acc:0.962]
Epoch [50/120    avg_loss:0.045, val_acc:0.984]
Epoch [51/120    avg_loss:0.030, val_acc:0.968]
Epoch [52/120    avg_loss:0.011, val_acc:0.979]
Epoch [53/120    avg_loss:0.014, val_acc:0.981]
Epoch [54/120    avg_loss:0.020, val_acc:0.980]
Epoch [55/120    avg_loss:0.017, val_acc:0.983]
Epoch [56/120    avg_loss:0.011, val_acc:0.982]
Epoch [57/120    avg_loss:0.011, val_acc:0.983]
Epoch [58/120    avg_loss:0.020, val_acc:0.949]
Epoch [59/120    avg_loss:0.024, val_acc:0.978]
Epoch [60/120    avg_loss:0.015, val_acc:0.985]
Epoch [61/120    avg_loss:0.023, val_acc:0.978]
Epoch [62/120    avg_loss:0.013, val_acc:0.972]
Epoch [63/120    avg_loss:0.012, val_acc:0.981]
Epoch [64/120    avg_loss:0.018, val_acc:0.982]
Epoch [65/120    avg_loss:0.015, val_acc:0.977]
Epoch [66/120    avg_loss:0.023, val_acc:0.978]
Epoch [67/120    avg_loss:0.013, val_acc:0.986]
Epoch [68/120    avg_loss:0.007, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.987]
Epoch [70/120    avg_loss:0.005, val_acc:0.988]
Epoch [71/120    avg_loss:0.009, val_acc:0.979]
Epoch [72/120    avg_loss:0.006, val_acc:0.980]
Epoch [73/120    avg_loss:0.019, val_acc:0.980]
Epoch [74/120    avg_loss:0.009, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.983]
Epoch [78/120    avg_loss:0.005, val_acc:0.979]
Epoch [79/120    avg_loss:0.009, val_acc:0.977]
Epoch [80/120    avg_loss:0.005, val_acc:0.985]
Epoch [81/120    avg_loss:0.021, val_acc:0.977]
Epoch [82/120    avg_loss:0.008, val_acc:0.982]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.987]
Epoch [85/120    avg_loss:0.032, val_acc:0.983]
Epoch [86/120    avg_loss:0.023, val_acc:0.969]
Epoch [87/120    avg_loss:0.010, val_acc:0.978]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.015, val_acc:0.981]
Epoch [93/120    avg_loss:0.024, val_acc:0.981]
Epoch [94/120    avg_loss:0.044, val_acc:0.982]
Epoch [95/120    avg_loss:0.041, val_acc:0.978]
Epoch [96/120    avg_loss:0.030, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.983]
Epoch [100/120    avg_loss:0.022, val_acc:0.978]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.015, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     1     0     0     1     0    40     0]
 [    0     0 17987     0    24     0    79     0     0     0]
 [    0     2     0  1971     0     0     0     0    62     1]
 [    0    24     0     3  2938     0     0     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4860     0    17     0]
 [    0     3     0     0     0     0     0  1283     0     4]
 [    0    15     0    10    28     0     0     0  3517     1]
 [    0     0     0     0     8    11     0     0     0   900]]

Accuracy:
99.17576458679777

F1 scores:
[       nan 0.99331572 0.99711736 0.98035315 0.98425461 0.99580313
 0.99001833 0.99727944 0.97504852 0.98630137]

Kappa:
0.9890893493140287
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b64633710>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.334, val_acc:0.620]
Epoch [2/120    avg_loss:0.703, val_acc:0.792]
Epoch [3/120    avg_loss:0.552, val_acc:0.802]
Epoch [4/120    avg_loss:0.429, val_acc:0.777]
Epoch [5/120    avg_loss:0.374, val_acc:0.888]
Epoch [6/120    avg_loss:0.280, val_acc:0.839]
Epoch [7/120    avg_loss:0.354, val_acc:0.918]
Epoch [8/120    avg_loss:0.186, val_acc:0.919]
Epoch [9/120    avg_loss:0.152, val_acc:0.903]
Epoch [10/120    avg_loss:0.194, val_acc:0.914]
Epoch [11/120    avg_loss:0.168, val_acc:0.910]
Epoch [12/120    avg_loss:0.170, val_acc:0.933]
Epoch [13/120    avg_loss:0.111, val_acc:0.936]
Epoch [14/120    avg_loss:0.103, val_acc:0.954]
Epoch [15/120    avg_loss:0.104, val_acc:0.923]
Epoch [16/120    avg_loss:0.140, val_acc:0.963]
Epoch [17/120    avg_loss:0.124, val_acc:0.969]
Epoch [18/120    avg_loss:0.116, val_acc:0.940]
Epoch [19/120    avg_loss:0.084, val_acc:0.960]
Epoch [20/120    avg_loss:0.061, val_acc:0.973]
Epoch [21/120    avg_loss:0.055, val_acc:0.954]
Epoch [22/120    avg_loss:0.067, val_acc:0.976]
Epoch [23/120    avg_loss:0.067, val_acc:0.960]
Epoch [24/120    avg_loss:0.180, val_acc:0.958]
Epoch [25/120    avg_loss:0.073, val_acc:0.959]
Epoch [26/120    avg_loss:0.067, val_acc:0.962]
Epoch [27/120    avg_loss:0.043, val_acc:0.963]
Epoch [28/120    avg_loss:0.036, val_acc:0.974]
Epoch [29/120    avg_loss:0.030, val_acc:0.982]
Epoch [30/120    avg_loss:0.050, val_acc:0.975]
Epoch [31/120    avg_loss:0.058, val_acc:0.954]
Epoch [32/120    avg_loss:0.042, val_acc:0.961]
Epoch [33/120    avg_loss:0.053, val_acc:0.979]
Epoch [34/120    avg_loss:0.025, val_acc:0.985]
Epoch [35/120    avg_loss:0.027, val_acc:0.982]
Epoch [36/120    avg_loss:0.022, val_acc:0.985]
Epoch [37/120    avg_loss:0.030, val_acc:0.986]
Epoch [38/120    avg_loss:0.025, val_acc:0.972]
Epoch [39/120    avg_loss:0.033, val_acc:0.978]
Epoch [40/120    avg_loss:0.035, val_acc:0.985]
Epoch [41/120    avg_loss:0.048, val_acc:0.980]
Epoch [42/120    avg_loss:0.022, val_acc:0.985]
Epoch [43/120    avg_loss:0.019, val_acc:0.984]
Epoch [44/120    avg_loss:0.023, val_acc:0.985]
Epoch [45/120    avg_loss:0.046, val_acc:0.893]
Epoch [46/120    avg_loss:0.056, val_acc:0.985]
Epoch [47/120    avg_loss:0.023, val_acc:0.980]
Epoch [48/120    avg_loss:0.013, val_acc:0.988]
Epoch [49/120    avg_loss:0.014, val_acc:0.986]
Epoch [50/120    avg_loss:0.027, val_acc:0.983]
Epoch [51/120    avg_loss:0.031, val_acc:0.981]
Epoch [52/120    avg_loss:0.013, val_acc:0.987]
Epoch [53/120    avg_loss:0.023, val_acc:0.974]
Epoch [54/120    avg_loss:0.010, val_acc:0.988]
Epoch [55/120    avg_loss:0.014, val_acc:0.989]
Epoch [56/120    avg_loss:0.037, val_acc:0.976]
Epoch [57/120    avg_loss:0.043, val_acc:0.983]
Epoch [58/120    avg_loss:0.014, val_acc:0.991]
Epoch [59/120    avg_loss:0.012, val_acc:0.988]
Epoch [60/120    avg_loss:0.007, val_acc:0.991]
Epoch [61/120    avg_loss:0.021, val_acc:0.945]
Epoch [62/120    avg_loss:0.066, val_acc:0.963]
Epoch [63/120    avg_loss:0.052, val_acc:0.985]
Epoch [64/120    avg_loss:0.023, val_acc:0.983]
Epoch [65/120    avg_loss:0.015, val_acc:0.980]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.013, val_acc:0.987]
Epoch [68/120    avg_loss:0.047, val_acc:0.943]
Epoch [69/120    avg_loss:0.022, val_acc:0.988]
Epoch [70/120    avg_loss:0.022, val_acc:0.969]
Epoch [71/120    avg_loss:0.012, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.011, val_acc:0.978]
Epoch [74/120    avg_loss:0.013, val_acc:0.984]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.989]
Epoch [77/120    avg_loss:0.009, val_acc:0.989]
Epoch [78/120    avg_loss:0.008, val_acc:0.990]
Epoch [79/120    avg_loss:0.007, val_acc:0.992]
Epoch [80/120    avg_loss:0.007, val_acc:0.992]
Epoch [81/120    avg_loss:0.010, val_acc:0.991]
Epoch [82/120    avg_loss:0.010, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.991]
Epoch [84/120    avg_loss:0.004, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.008, val_acc:0.992]
Epoch [88/120    avg_loss:0.007, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.008, val_acc:0.993]
Epoch [92/120    avg_loss:0.004, val_acc:0.993]
Epoch [93/120    avg_loss:0.006, val_acc:0.993]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.991]
Epoch [96/120    avg_loss:0.009, val_acc:0.993]
Epoch [97/120    avg_loss:0.007, val_acc:0.993]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.993]
Epoch [100/120    avg_loss:0.006, val_acc:0.993]
Epoch [101/120    avg_loss:0.005, val_acc:0.993]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.006, val_acc:0.992]
Epoch [105/120    avg_loss:0.004, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.013, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.009, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     0     0     3     0     9     1]
 [    0     0 18057     0    12     0    21     0     0     0]
 [    0     0     0  1996     0     0     0     0    35     5]
 [    0    25     0     3  2934     0     1     0     9     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0    13     0     0     0     0     2  1268     0     7]
 [    0     8     0    44    46     0     0     0  3470     3]
 [    0     0     0     0     8    22     0     0     0   889]]

Accuracy:
99.32759742607188

F1 scores:
[       nan 0.99542529 0.99903179 0.97867124 0.9825854  0.99164134
 0.99703507 0.99139953 0.97829151 0.9747807 ]

Kappa:
0.9910924059551519
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29e682c7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.185, val_acc:0.638]
Epoch [2/120    avg_loss:0.611, val_acc:0.760]
Epoch [3/120    avg_loss:0.426, val_acc:0.857]
Epoch [4/120    avg_loss:0.390, val_acc:0.733]
Epoch [5/120    avg_loss:0.323, val_acc:0.873]
Epoch [6/120    avg_loss:0.235, val_acc:0.877]
Epoch [7/120    avg_loss:0.286, val_acc:0.878]
Epoch [8/120    avg_loss:0.209, val_acc:0.859]
Epoch [9/120    avg_loss:0.229, val_acc:0.887]
Epoch [10/120    avg_loss:0.190, val_acc:0.925]
Epoch [11/120    avg_loss:0.167, val_acc:0.859]
Epoch [12/120    avg_loss:0.196, val_acc:0.929]
Epoch [13/120    avg_loss:0.133, val_acc:0.929]
Epoch [14/120    avg_loss:0.116, val_acc:0.917]
Epoch [15/120    avg_loss:0.091, val_acc:0.945]
Epoch [16/120    avg_loss:0.112, val_acc:0.937]
Epoch [17/120    avg_loss:0.094, val_acc:0.942]
Epoch [18/120    avg_loss:0.133, val_acc:0.922]
Epoch [19/120    avg_loss:0.088, val_acc:0.946]
Epoch [20/120    avg_loss:0.079, val_acc:0.938]
Epoch [21/120    avg_loss:0.088, val_acc:0.918]
Epoch [22/120    avg_loss:0.073, val_acc:0.934]
Epoch [23/120    avg_loss:0.058, val_acc:0.928]
Epoch [24/120    avg_loss:0.062, val_acc:0.948]
Epoch [25/120    avg_loss:0.045, val_acc:0.965]
Epoch [26/120    avg_loss:0.032, val_acc:0.963]
Epoch [27/120    avg_loss:0.064, val_acc:0.944]
Epoch [28/120    avg_loss:0.068, val_acc:0.953]
Epoch [29/120    avg_loss:0.075, val_acc:0.948]
Epoch [30/120    avg_loss:0.042, val_acc:0.964]
Epoch [31/120    avg_loss:0.030, val_acc:0.968]
Epoch [32/120    avg_loss:0.046, val_acc:0.960]
Epoch [33/120    avg_loss:0.039, val_acc:0.962]
Epoch [34/120    avg_loss:0.045, val_acc:0.934]
Epoch [35/120    avg_loss:0.044, val_acc:0.966]
Epoch [36/120    avg_loss:0.031, val_acc:0.969]
Epoch [37/120    avg_loss:0.021, val_acc:0.935]
Epoch [38/120    avg_loss:0.035, val_acc:0.943]
Epoch [39/120    avg_loss:0.074, val_acc:0.948]
Epoch [40/120    avg_loss:0.053, val_acc:0.965]
Epoch [41/120    avg_loss:0.046, val_acc:0.968]
Epoch [42/120    avg_loss:0.026, val_acc:0.959]
Epoch [43/120    avg_loss:0.036, val_acc:0.972]
Epoch [44/120    avg_loss:0.051, val_acc:0.958]
Epoch [45/120    avg_loss:0.030, val_acc:0.971]
Epoch [46/120    avg_loss:0.014, val_acc:0.977]
Epoch [47/120    avg_loss:0.021, val_acc:0.977]
Epoch [48/120    avg_loss:0.020, val_acc:0.963]
Epoch [49/120    avg_loss:0.019, val_acc:0.975]
Epoch [50/120    avg_loss:0.029, val_acc:0.972]
Epoch [51/120    avg_loss:0.017, val_acc:0.978]
Epoch [52/120    avg_loss:0.020, val_acc:0.942]
Epoch [53/120    avg_loss:0.011, val_acc:0.977]
Epoch [54/120    avg_loss:0.013, val_acc:0.974]
Epoch [55/120    avg_loss:0.016, val_acc:0.968]
Epoch [56/120    avg_loss:0.020, val_acc:0.970]
Epoch [57/120    avg_loss:0.019, val_acc:0.980]
Epoch [58/120    avg_loss:0.014, val_acc:0.976]
Epoch [59/120    avg_loss:0.008, val_acc:0.976]
Epoch [60/120    avg_loss:0.014, val_acc:0.975]
Epoch [61/120    avg_loss:0.014, val_acc:0.974]
Epoch [62/120    avg_loss:0.006, val_acc:0.976]
Epoch [63/120    avg_loss:0.017, val_acc:0.973]
Epoch [64/120    avg_loss:0.023, val_acc:0.969]
Epoch [65/120    avg_loss:0.017, val_acc:0.961]
Epoch [66/120    avg_loss:0.022, val_acc:0.975]
Epoch [67/120    avg_loss:0.026, val_acc:0.976]
Epoch [68/120    avg_loss:0.012, val_acc:0.973]
Epoch [69/120    avg_loss:0.062, val_acc:0.954]
Epoch [70/120    avg_loss:0.044, val_acc:0.971]
Epoch [71/120    avg_loss:0.016, val_acc:0.976]
Epoch [72/120    avg_loss:0.013, val_acc:0.978]
Epoch [73/120    avg_loss:0.008, val_acc:0.978]
Epoch [74/120    avg_loss:0.008, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.978]
Epoch [76/120    avg_loss:0.009, val_acc:0.978]
Epoch [77/120    avg_loss:0.012, val_acc:0.978]
Epoch [78/120    avg_loss:0.008, val_acc:0.978]
Epoch [79/120    avg_loss:0.007, val_acc:0.978]
Epoch [80/120    avg_loss:0.011, val_acc:0.978]
Epoch [81/120    avg_loss:0.006, val_acc:0.978]
Epoch [82/120    avg_loss:0.011, val_acc:0.977]
Epoch [83/120    avg_loss:0.008, val_acc:0.977]
Epoch [84/120    avg_loss:0.012, val_acc:0.977]
Epoch [85/120    avg_loss:0.007, val_acc:0.977]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.006, val_acc:0.977]
Epoch [89/120    avg_loss:0.007, val_acc:0.977]
Epoch [90/120    avg_loss:0.011, val_acc:0.976]
Epoch [91/120    avg_loss:0.015, val_acc:0.976]
Epoch [92/120    avg_loss:0.008, val_acc:0.977]
Epoch [93/120    avg_loss:0.010, val_acc:0.977]
Epoch [94/120    avg_loss:0.014, val_acc:0.976]
Epoch [95/120    avg_loss:0.009, val_acc:0.976]
Epoch [96/120    avg_loss:0.009, val_acc:0.976]
Epoch [97/120    avg_loss:0.014, val_acc:0.976]
Epoch [98/120    avg_loss:0.007, val_acc:0.976]
Epoch [99/120    avg_loss:0.009, val_acc:0.976]
Epoch [100/120    avg_loss:0.009, val_acc:0.976]
Epoch [101/120    avg_loss:0.006, val_acc:0.976]
Epoch [102/120    avg_loss:0.006, val_acc:0.976]
Epoch [103/120    avg_loss:0.007, val_acc:0.976]
Epoch [104/120    avg_loss:0.017, val_acc:0.976]
Epoch [105/120    avg_loss:0.015, val_acc:0.976]
Epoch [106/120    avg_loss:0.007, val_acc:0.976]
Epoch [107/120    avg_loss:0.007, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.976]
Epoch [109/120    avg_loss:0.017, val_acc:0.976]
Epoch [110/120    avg_loss:0.008, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.976]
Epoch [112/120    avg_loss:0.009, val_acc:0.976]
Epoch [113/120    avg_loss:0.006, val_acc:0.976]
Epoch [114/120    avg_loss:0.008, val_acc:0.976]
Epoch [115/120    avg_loss:0.006, val_acc:0.976]
Epoch [116/120    avg_loss:0.005, val_acc:0.976]
Epoch [117/120    avg_loss:0.007, val_acc:0.976]
Epoch [118/120    avg_loss:0.011, val_acc:0.976]
Epoch [119/120    avg_loss:0.007, val_acc:0.976]
Epoch [120/120    avg_loss:0.007, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     0     0     0    11     0     3]
 [    0     0 17937     0    23     0   129     0     1     0]
 [    0     0     1  2011     0     0     0     0    20     4]
 [    0    32     0     0  2936     0     0     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     5     0     0     0     0     2  1283     0     0]
 [    0    14     0    41    48     0     0     0  3465     3]
 [    0     0     0     0     0    36     0     0     0   883]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.99496163 0.99572555 0.98385519 0.98210403 0.98639456
 0.98675028 0.99303406 0.98144739 0.97461369]

Kappa:
0.9879791499850404
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5820135710>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.224, val_acc:0.574]
Epoch [2/120    avg_loss:0.626, val_acc:0.818]
Epoch [3/120    avg_loss:0.483, val_acc:0.696]
Epoch [4/120    avg_loss:0.367, val_acc:0.797]
Epoch [5/120    avg_loss:0.251, val_acc:0.886]
Epoch [6/120    avg_loss:0.272, val_acc:0.902]
Epoch [7/120    avg_loss:0.188, val_acc:0.882]
Epoch [8/120    avg_loss:0.204, val_acc:0.845]
Epoch [9/120    avg_loss:0.188, val_acc:0.927]
Epoch [10/120    avg_loss:0.189, val_acc:0.912]
Epoch [11/120    avg_loss:0.126, val_acc:0.921]
Epoch [12/120    avg_loss:0.218, val_acc:0.912]
Epoch [13/120    avg_loss:0.147, val_acc:0.945]
Epoch [14/120    avg_loss:0.092, val_acc:0.947]
Epoch [15/120    avg_loss:0.079, val_acc:0.939]
Epoch [16/120    avg_loss:0.036, val_acc:0.938]
Epoch [17/120    avg_loss:0.042, val_acc:0.945]
Epoch [18/120    avg_loss:0.069, val_acc:0.949]
Epoch [19/120    avg_loss:0.066, val_acc:0.950]
Epoch [20/120    avg_loss:0.089, val_acc:0.927]
Epoch [21/120    avg_loss:0.069, val_acc:0.953]
Epoch [22/120    avg_loss:0.077, val_acc:0.942]
Epoch [23/120    avg_loss:0.079, val_acc:0.938]
Epoch [24/120    avg_loss:0.050, val_acc:0.969]
Epoch [25/120    avg_loss:0.052, val_acc:0.938]
Epoch [26/120    avg_loss:0.055, val_acc:0.953]
Epoch [27/120    avg_loss:0.060, val_acc:0.954]
Epoch [28/120    avg_loss:0.072, val_acc:0.954]
Epoch [29/120    avg_loss:0.078, val_acc:0.949]
Epoch [30/120    avg_loss:0.071, val_acc:0.947]
Epoch [31/120    avg_loss:0.071, val_acc:0.948]
Epoch [32/120    avg_loss:0.053, val_acc:0.966]
Epoch [33/120    avg_loss:0.038, val_acc:0.958]
Epoch [34/120    avg_loss:0.041, val_acc:0.953]
Epoch [35/120    avg_loss:0.052, val_acc:0.959]
Epoch [36/120    avg_loss:0.048, val_acc:0.958]
Epoch [37/120    avg_loss:0.112, val_acc:0.967]
Epoch [38/120    avg_loss:0.025, val_acc:0.970]
Epoch [39/120    avg_loss:0.023, val_acc:0.968]
Epoch [40/120    avg_loss:0.019, val_acc:0.966]
Epoch [41/120    avg_loss:0.028, val_acc:0.972]
Epoch [42/120    avg_loss:0.023, val_acc:0.975]
Epoch [43/120    avg_loss:0.016, val_acc:0.973]
Epoch [44/120    avg_loss:0.020, val_acc:0.973]
Epoch [45/120    avg_loss:0.018, val_acc:0.973]
Epoch [46/120    avg_loss:0.016, val_acc:0.975]
Epoch [47/120    avg_loss:0.028, val_acc:0.973]
Epoch [48/120    avg_loss:0.020, val_acc:0.972]
Epoch [49/120    avg_loss:0.012, val_acc:0.973]
Epoch [50/120    avg_loss:0.020, val_acc:0.973]
Epoch [51/120    avg_loss:0.020, val_acc:0.974]
Epoch [52/120    avg_loss:0.016, val_acc:0.974]
Epoch [53/120    avg_loss:0.014, val_acc:0.974]
Epoch [54/120    avg_loss:0.024, val_acc:0.975]
Epoch [55/120    avg_loss:0.021, val_acc:0.974]
Epoch [56/120    avg_loss:0.022, val_acc:0.970]
Epoch [57/120    avg_loss:0.022, val_acc:0.971]
Epoch [58/120    avg_loss:0.013, val_acc:0.973]
Epoch [59/120    avg_loss:0.014, val_acc:0.973]
Epoch [60/120    avg_loss:0.014, val_acc:0.974]
Epoch [61/120    avg_loss:0.018, val_acc:0.974]
Epoch [62/120    avg_loss:0.015, val_acc:0.978]
Epoch [63/120    avg_loss:0.013, val_acc:0.977]
Epoch [64/120    avg_loss:0.014, val_acc:0.977]
Epoch [65/120    avg_loss:0.023, val_acc:0.977]
Epoch [66/120    avg_loss:0.020, val_acc:0.975]
Epoch [67/120    avg_loss:0.015, val_acc:0.977]
Epoch [68/120    avg_loss:0.024, val_acc:0.973]
Epoch [69/120    avg_loss:0.021, val_acc:0.973]
Epoch [70/120    avg_loss:0.014, val_acc:0.974]
Epoch [71/120    avg_loss:0.014, val_acc:0.974]
Epoch [72/120    avg_loss:0.013, val_acc:0.975]
Epoch [73/120    avg_loss:0.013, val_acc:0.979]
Epoch [74/120    avg_loss:0.013, val_acc:0.977]
Epoch [75/120    avg_loss:0.014, val_acc:0.976]
Epoch [76/120    avg_loss:0.019, val_acc:0.974]
Epoch [77/120    avg_loss:0.021, val_acc:0.974]
Epoch [78/120    avg_loss:0.012, val_acc:0.977]
Epoch [79/120    avg_loss:0.011, val_acc:0.978]
Epoch [80/120    avg_loss:0.017, val_acc:0.980]
Epoch [81/120    avg_loss:0.021, val_acc:0.976]
Epoch [82/120    avg_loss:0.017, val_acc:0.976]
Epoch [83/120    avg_loss:0.010, val_acc:0.976]
Epoch [84/120    avg_loss:0.013, val_acc:0.974]
Epoch [85/120    avg_loss:0.009, val_acc:0.978]
Epoch [86/120    avg_loss:0.010, val_acc:0.977]
Epoch [87/120    avg_loss:0.022, val_acc:0.974]
Epoch [88/120    avg_loss:0.013, val_acc:0.978]
Epoch [89/120    avg_loss:0.018, val_acc:0.977]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.011, val_acc:0.978]
Epoch [92/120    avg_loss:0.017, val_acc:0.978]
Epoch [93/120    avg_loss:0.016, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.978]
Epoch [95/120    avg_loss:0.013, val_acc:0.978]
Epoch [96/120    avg_loss:0.024, val_acc:0.978]
Epoch [97/120    avg_loss:0.015, val_acc:0.977]
Epoch [98/120    avg_loss:0.011, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.012, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.977]
Epoch [103/120    avg_loss:0.013, val_acc:0.977]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.012, val_acc:0.978]
Epoch [106/120    avg_loss:0.013, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.013, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.978]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.023, val_acc:0.978]
Epoch [118/120    avg_loss:0.023, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     2     0     0     2     0     0     1]
 [    0     3 17894     0    40     0   153     0     0     0]
 [    0     0     0  1973     0     0     0     0    57     6]
 [    0    23     0     0  2931     0     0     0    15     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4868     0    10     0]
 [    0     9     0     0     0     0     2  1276     0     3]
 [    0     3     0     7    34     0     0     0  3527     0]
 [    0     0     0     0     6    21     0     0     0   892]]

Accuracy:
99.0359819728629

F1 scores:
[       nan 0.99666589 0.99455313 0.98208064 0.97977603 0.99201824
 0.98313642 0.99454404 0.98245125 0.97807018]

Kappa:
0.9872504959243441
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79e5c90748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.210, val_acc:0.583]
Epoch [2/120    avg_loss:0.616, val_acc:0.698]
Epoch [3/120    avg_loss:0.458, val_acc:0.782]
Epoch [4/120    avg_loss:0.390, val_acc:0.771]
Epoch [5/120    avg_loss:0.317, val_acc:0.727]
Epoch [6/120    avg_loss:0.270, val_acc:0.890]
Epoch [7/120    avg_loss:0.194, val_acc:0.905]
Epoch [8/120    avg_loss:0.306, val_acc:0.853]
Epoch [9/120    avg_loss:0.307, val_acc:0.882]
Epoch [10/120    avg_loss:0.156, val_acc:0.905]
Epoch [11/120    avg_loss:0.129, val_acc:0.882]
Epoch [12/120    avg_loss:0.152, val_acc:0.903]
Epoch [13/120    avg_loss:0.122, val_acc:0.877]
Epoch [14/120    avg_loss:0.118, val_acc:0.938]
Epoch [15/120    avg_loss:0.089, val_acc:0.943]
Epoch [16/120    avg_loss:0.083, val_acc:0.907]
Epoch [17/120    avg_loss:0.101, val_acc:0.952]
Epoch [18/120    avg_loss:0.104, val_acc:0.953]
Epoch [19/120    avg_loss:0.083, val_acc:0.906]
Epoch [20/120    avg_loss:0.072, val_acc:0.956]
Epoch [21/120    avg_loss:0.075, val_acc:0.953]
Epoch [22/120    avg_loss:0.124, val_acc:0.929]
Epoch [23/120    avg_loss:0.065, val_acc:0.920]
Epoch [24/120    avg_loss:0.061, val_acc:0.962]
Epoch [25/120    avg_loss:0.048, val_acc:0.940]
Epoch [26/120    avg_loss:0.057, val_acc:0.927]
Epoch [27/120    avg_loss:0.069, val_acc:0.935]
Epoch [28/120    avg_loss:0.070, val_acc:0.948]
Epoch [29/120    avg_loss:0.065, val_acc:0.956]
Epoch [30/120    avg_loss:0.047, val_acc:0.934]
Epoch [31/120    avg_loss:0.039, val_acc:0.961]
Epoch [32/120    avg_loss:0.044, val_acc:0.968]
Epoch [33/120    avg_loss:0.035, val_acc:0.973]
Epoch [34/120    avg_loss:0.022, val_acc:0.977]
Epoch [35/120    avg_loss:0.027, val_acc:0.969]
Epoch [36/120    avg_loss:0.033, val_acc:0.973]
Epoch [37/120    avg_loss:0.023, val_acc:0.958]
Epoch [38/120    avg_loss:0.020, val_acc:0.970]
Epoch [39/120    avg_loss:0.023, val_acc:0.973]
Epoch [40/120    avg_loss:0.020, val_acc:0.979]
Epoch [41/120    avg_loss:0.022, val_acc:0.974]
Epoch [42/120    avg_loss:0.044, val_acc:0.965]
Epoch [43/120    avg_loss:0.057, val_acc:0.965]
Epoch [44/120    avg_loss:0.023, val_acc:0.973]
Epoch [45/120    avg_loss:0.045, val_acc:0.956]
Epoch [46/120    avg_loss:0.020, val_acc:0.966]
Epoch [47/120    avg_loss:0.019, val_acc:0.971]
Epoch [48/120    avg_loss:0.041, val_acc:0.967]
Epoch [49/120    avg_loss:0.054, val_acc:0.979]
Epoch [50/120    avg_loss:0.027, val_acc:0.949]
Epoch [51/120    avg_loss:0.034, val_acc:0.960]
Epoch [52/120    avg_loss:0.016, val_acc:0.962]
Epoch [53/120    avg_loss:0.050, val_acc:0.983]
Epoch [54/120    avg_loss:0.013, val_acc:0.981]
Epoch [55/120    avg_loss:0.010, val_acc:0.981]
Epoch [56/120    avg_loss:0.021, val_acc:0.966]
Epoch [57/120    avg_loss:0.041, val_acc:0.976]
Epoch [58/120    avg_loss:0.014, val_acc:0.978]
Epoch [59/120    avg_loss:0.013, val_acc:0.978]
Epoch [60/120    avg_loss:0.007, val_acc:0.979]
Epoch [61/120    avg_loss:0.015, val_acc:0.982]
Epoch [62/120    avg_loss:0.008, val_acc:0.983]
Epoch [63/120    avg_loss:0.008, val_acc:0.981]
Epoch [64/120    avg_loss:0.005, val_acc:0.982]
Epoch [65/120    avg_loss:0.008, val_acc:0.984]
Epoch [66/120    avg_loss:0.007, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.981]
Epoch [68/120    avg_loss:0.017, val_acc:0.985]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.009, val_acc:0.978]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.980]
Epoch [73/120    avg_loss:0.007, val_acc:0.977]
Epoch [74/120    avg_loss:0.031, val_acc:0.979]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.015, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.982]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.013, val_acc:0.981]
Epoch [81/120    avg_loss:0.006, val_acc:0.985]
Epoch [82/120    avg_loss:0.011, val_acc:0.982]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.006, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.989]
Epoch [89/120    avg_loss:0.002, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.980]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.003, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.987]
Epoch [96/120    avg_loss:0.013, val_acc:0.973]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.988]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.002, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.002, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.002, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.002, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     6     0     0     1    14    18     0]
 [    0     6 18065     0    19     0     0     0     0     0]
 [    0     0     0  1988     0     0     0     0    41     7]
 [    0    29     0     3  2930     0     1     0     7     2]
 [    0     0     0     6     0  1299     0     0     0     0]
 [    0     0     7     0     0     0  4857     0    14     0]
 [    0     2     0     0     0     0     0  1288     0     0]
 [    0    11     0    11    29     0     0     0  3512     8]
 [    0     2     0    11     5    20     0     0     0   881]]

Accuracy:
99.32518738100403

F1 scores:
[       nan 0.99308738 0.99911509 0.97906919 0.98404702 0.99009146
 0.99763788 0.99382716 0.98059472 0.96973032]

Kappa:
0.9910600302619319
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a0b404748>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.339, val_acc:0.638]
Epoch [2/120    avg_loss:0.706, val_acc:0.666]
Epoch [3/120    avg_loss:0.530, val_acc:0.792]
Epoch [4/120    avg_loss:0.421, val_acc:0.827]
Epoch [5/120    avg_loss:0.360, val_acc:0.855]
Epoch [6/120    avg_loss:0.252, val_acc:0.859]
Epoch [7/120    avg_loss:0.242, val_acc:0.902]
Epoch [8/120    avg_loss:0.209, val_acc:0.869]
Epoch [9/120    avg_loss:0.253, val_acc:0.902]
Epoch [10/120    avg_loss:0.204, val_acc:0.902]
Epoch [11/120    avg_loss:0.180, val_acc:0.926]
Epoch [12/120    avg_loss:0.122, val_acc:0.927]
Epoch [13/120    avg_loss:0.123, val_acc:0.926]
Epoch [14/120    avg_loss:0.109, val_acc:0.926]
Epoch [15/120    avg_loss:0.097, val_acc:0.955]
Epoch [16/120    avg_loss:0.085, val_acc:0.943]
Epoch [17/120    avg_loss:0.080, val_acc:0.956]
Epoch [18/120    avg_loss:0.057, val_acc:0.951]
Epoch [19/120    avg_loss:0.152, val_acc:0.951]
Epoch [20/120    avg_loss:0.069, val_acc:0.955]
Epoch [21/120    avg_loss:0.076, val_acc:0.926]
Epoch [22/120    avg_loss:0.049, val_acc:0.961]
Epoch [23/120    avg_loss:0.052, val_acc:0.973]
Epoch [24/120    avg_loss:0.049, val_acc:0.944]
Epoch [25/120    avg_loss:0.031, val_acc:0.964]
Epoch [26/120    avg_loss:0.033, val_acc:0.938]
Epoch [27/120    avg_loss:0.041, val_acc:0.970]
Epoch [28/120    avg_loss:0.051, val_acc:0.961]
Epoch [29/120    avg_loss:0.032, val_acc:0.955]
Epoch [30/120    avg_loss:0.044, val_acc:0.967]
Epoch [31/120    avg_loss:0.035, val_acc:0.961]
Epoch [32/120    avg_loss:0.046, val_acc:0.970]
Epoch [33/120    avg_loss:0.074, val_acc:0.967]
Epoch [34/120    avg_loss:0.016, val_acc:0.972]
Epoch [35/120    avg_loss:0.031, val_acc:0.957]
Epoch [36/120    avg_loss:0.086, val_acc:0.926]
Epoch [37/120    avg_loss:0.061, val_acc:0.973]
Epoch [38/120    avg_loss:0.024, val_acc:0.974]
Epoch [39/120    avg_loss:0.019, val_acc:0.977]
Epoch [40/120    avg_loss:0.012, val_acc:0.979]
Epoch [41/120    avg_loss:0.016, val_acc:0.978]
Epoch [42/120    avg_loss:0.026, val_acc:0.981]
Epoch [43/120    avg_loss:0.024, val_acc:0.979]
Epoch [44/120    avg_loss:0.014, val_acc:0.980]
Epoch [45/120    avg_loss:0.017, val_acc:0.979]
Epoch [46/120    avg_loss:0.019, val_acc:0.979]
Epoch [47/120    avg_loss:0.020, val_acc:0.980]
Epoch [48/120    avg_loss:0.011, val_acc:0.979]
Epoch [49/120    avg_loss:0.016, val_acc:0.981]
Epoch [50/120    avg_loss:0.012, val_acc:0.979]
Epoch [51/120    avg_loss:0.011, val_acc:0.980]
Epoch [52/120    avg_loss:0.013, val_acc:0.980]
Epoch [53/120    avg_loss:0.014, val_acc:0.981]
Epoch [54/120    avg_loss:0.016, val_acc:0.981]
Epoch [55/120    avg_loss:0.019, val_acc:0.978]
Epoch [56/120    avg_loss:0.018, val_acc:0.979]
Epoch [57/120    avg_loss:0.019, val_acc:0.980]
Epoch [58/120    avg_loss:0.020, val_acc:0.979]
Epoch [59/120    avg_loss:0.011, val_acc:0.979]
Epoch [60/120    avg_loss:0.013, val_acc:0.979]
Epoch [61/120    avg_loss:0.016, val_acc:0.981]
Epoch [62/120    avg_loss:0.009, val_acc:0.981]
Epoch [63/120    avg_loss:0.019, val_acc:0.979]
Epoch [64/120    avg_loss:0.012, val_acc:0.979]
Epoch [65/120    avg_loss:0.013, val_acc:0.979]
Epoch [66/120    avg_loss:0.012, val_acc:0.979]
Epoch [67/120    avg_loss:0.011, val_acc:0.981]
Epoch [68/120    avg_loss:0.014, val_acc:0.980]
Epoch [69/120    avg_loss:0.008, val_acc:0.981]
Epoch [70/120    avg_loss:0.015, val_acc:0.982]
Epoch [71/120    avg_loss:0.008, val_acc:0.982]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.017, val_acc:0.979]
Epoch [74/120    avg_loss:0.010, val_acc:0.981]
Epoch [75/120    avg_loss:0.014, val_acc:0.979]
Epoch [76/120    avg_loss:0.009, val_acc:0.980]
Epoch [77/120    avg_loss:0.013, val_acc:0.979]
Epoch [78/120    avg_loss:0.010, val_acc:0.979]
Epoch [79/120    avg_loss:0.011, val_acc:0.979]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.017, val_acc:0.980]
Epoch [82/120    avg_loss:0.008, val_acc:0.980]
Epoch [83/120    avg_loss:0.015, val_acc:0.979]
Epoch [84/120    avg_loss:0.010, val_acc:0.978]
Epoch [85/120    avg_loss:0.009, val_acc:0.979]
Epoch [86/120    avg_loss:0.011, val_acc:0.979]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.009, val_acc:0.979]
Epoch [89/120    avg_loss:0.014, val_acc:0.980]
Epoch [90/120    avg_loss:0.010, val_acc:0.980]
Epoch [91/120    avg_loss:0.009, val_acc:0.980]
Epoch [92/120    avg_loss:0.016, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.007, val_acc:0.980]
Epoch [97/120    avg_loss:0.007, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.013, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.980]
Epoch [104/120    avg_loss:0.015, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.981]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.008, val_acc:0.980]
Epoch [116/120    avg_loss:0.008, val_acc:0.980]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     0     3     0     0     0     0     2]
 [    0     7 17995     0    52     0    34     0     2     0]
 [    0     1     0  2031     0     0     0     0     1     3]
 [    0    35     0     1  2920     0     0     0    15     1]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     2     0     0     0  4853     0    23     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    12     0    35    36     0     0     0  3488     0]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.25770611910443

F1 scores:
[       nan 0.99535388 0.99731205 0.99000731 0.97382024 0.98900265
 0.99395801 1.         0.98253521 0.97282307]

Kappa:
0.9901735593732808
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f786f845780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.323, val_acc:0.709]
Epoch [2/120    avg_loss:0.798, val_acc:0.672]
Epoch [3/120    avg_loss:0.471, val_acc:0.776]
Epoch [4/120    avg_loss:0.436, val_acc:0.829]
Epoch [5/120    avg_loss:0.342, val_acc:0.899]
Epoch [6/120    avg_loss:0.264, val_acc:0.910]
Epoch [7/120    avg_loss:0.205, val_acc:0.918]
Epoch [8/120    avg_loss:0.187, val_acc:0.901]
Epoch [9/120    avg_loss:0.236, val_acc:0.878]
Epoch [10/120    avg_loss:0.201, val_acc:0.848]
Epoch [11/120    avg_loss:0.160, val_acc:0.929]
Epoch [12/120    avg_loss:0.124, val_acc:0.956]
Epoch [13/120    avg_loss:0.111, val_acc:0.945]
Epoch [14/120    avg_loss:0.158, val_acc:0.928]
Epoch [15/120    avg_loss:0.214, val_acc:0.932]
Epoch [16/120    avg_loss:0.073, val_acc:0.928]
Epoch [17/120    avg_loss:0.077, val_acc:0.951]
Epoch [18/120    avg_loss:0.100, val_acc:0.920]
Epoch [19/120    avg_loss:0.116, val_acc:0.938]
Epoch [20/120    avg_loss:0.055, val_acc:0.963]
Epoch [21/120    avg_loss:0.054, val_acc:0.960]
Epoch [22/120    avg_loss:0.050, val_acc:0.970]
Epoch [23/120    avg_loss:0.053, val_acc:0.942]
Epoch [24/120    avg_loss:0.041, val_acc:0.972]
Epoch [25/120    avg_loss:0.066, val_acc:0.953]
Epoch [26/120    avg_loss:0.062, val_acc:0.954]
Epoch [27/120    avg_loss:0.124, val_acc:0.954]
Epoch [28/120    avg_loss:0.044, val_acc:0.970]
Epoch [29/120    avg_loss:0.042, val_acc:0.962]
Epoch [30/120    avg_loss:0.064, val_acc:0.974]
Epoch [31/120    avg_loss:0.040, val_acc:0.972]
Epoch [32/120    avg_loss:0.059, val_acc:0.968]
Epoch [33/120    avg_loss:0.063, val_acc:0.977]
Epoch [34/120    avg_loss:0.030, val_acc:0.972]
Epoch [35/120    avg_loss:0.026, val_acc:0.975]
Epoch [36/120    avg_loss:0.045, val_acc:0.959]
Epoch [37/120    avg_loss:0.051, val_acc:0.974]
Epoch [38/120    avg_loss:0.055, val_acc:0.976]
Epoch [39/120    avg_loss:0.031, val_acc:0.975]
Epoch [40/120    avg_loss:0.031, val_acc:0.924]
Epoch [41/120    avg_loss:0.028, val_acc:0.981]
Epoch [42/120    avg_loss:0.015, val_acc:0.987]
Epoch [43/120    avg_loss:0.022, val_acc:0.986]
Epoch [44/120    avg_loss:0.024, val_acc:0.983]
Epoch [45/120    avg_loss:0.013, val_acc:0.988]
Epoch [46/120    avg_loss:0.008, val_acc:0.990]
Epoch [47/120    avg_loss:0.007, val_acc:0.987]
Epoch [48/120    avg_loss:0.011, val_acc:0.953]
Epoch [49/120    avg_loss:0.018, val_acc:0.988]
Epoch [50/120    avg_loss:0.058, val_acc:0.944]
Epoch [51/120    avg_loss:0.056, val_acc:0.974]
Epoch [52/120    avg_loss:0.069, val_acc:0.981]
Epoch [53/120    avg_loss:0.022, val_acc:0.973]
Epoch [54/120    avg_loss:0.029, val_acc:0.968]
Epoch [55/120    avg_loss:0.029, val_acc:0.982]
Epoch [56/120    avg_loss:0.013, val_acc:0.981]
Epoch [57/120    avg_loss:0.035, val_acc:0.970]
Epoch [58/120    avg_loss:0.093, val_acc:0.976]
Epoch [59/120    avg_loss:0.039, val_acc:0.985]
Epoch [60/120    avg_loss:0.018, val_acc:0.986]
Epoch [61/120    avg_loss:0.014, val_acc:0.986]
Epoch [62/120    avg_loss:0.010, val_acc:0.986]
Epoch [63/120    avg_loss:0.012, val_acc:0.987]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.990]
Epoch [66/120    avg_loss:0.009, val_acc:0.988]
Epoch [67/120    avg_loss:0.011, val_acc:0.990]
Epoch [68/120    avg_loss:0.011, val_acc:0.991]
Epoch [69/120    avg_loss:0.009, val_acc:0.991]
Epoch [70/120    avg_loss:0.009, val_acc:0.990]
Epoch [71/120    avg_loss:0.012, val_acc:0.990]
Epoch [72/120    avg_loss:0.010, val_acc:0.991]
Epoch [73/120    avg_loss:0.008, val_acc:0.990]
Epoch [74/120    avg_loss:0.015, val_acc:0.989]
Epoch [75/120    avg_loss:0.018, val_acc:0.990]
Epoch [76/120    avg_loss:0.009, val_acc:0.989]
Epoch [77/120    avg_loss:0.006, val_acc:0.990]
Epoch [78/120    avg_loss:0.011, val_acc:0.988]
Epoch [79/120    avg_loss:0.011, val_acc:0.991]
Epoch [80/120    avg_loss:0.011, val_acc:0.990]
Epoch [81/120    avg_loss:0.010, val_acc:0.991]
Epoch [82/120    avg_loss:0.013, val_acc:0.988]
Epoch [83/120    avg_loss:0.014, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.010, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.991]
Epoch [88/120    avg_loss:0.008, val_acc:0.991]
Epoch [89/120    avg_loss:0.013, val_acc:0.992]
Epoch [90/120    avg_loss:0.006, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.993]
Epoch [92/120    avg_loss:0.007, val_acc:0.991]
Epoch [93/120    avg_loss:0.023, val_acc:0.990]
Epoch [94/120    avg_loss:0.009, val_acc:0.993]
Epoch [95/120    avg_loss:0.009, val_acc:0.992]
Epoch [96/120    avg_loss:0.011, val_acc:0.992]
Epoch [97/120    avg_loss:0.008, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.016, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.008, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.008, val_acc:0.992]
Epoch [105/120    avg_loss:0.009, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.009, val_acc:0.992]
Epoch [114/120    avg_loss:0.007, val_acc:0.992]
Epoch [115/120    avg_loss:0.014, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     4     0    24    12     0     0]
 [    0     0 18037     0    24     0    22     0     7     0]
 [    0     0     0  2028     0     0     0     0     2     6]
 [    0    24     0     0  2933     0     0     0    12     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0     0     0     0     0     0     1  1288     0     1]
 [    0     2     0     7    40     0     0     0  3522     0]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.4312293639891

F1 scores:
[       nan 0.99486381 0.99853295 0.9963154  0.97978954 0.9893859
 0.99489796 0.99459459 0.98974287 0.97120709]

Kappa:
0.9924679536141776
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff5a39887b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.204, val_acc:0.718]
Epoch [2/120    avg_loss:0.672, val_acc:0.653]
Epoch [3/120    avg_loss:0.498, val_acc:0.767]
Epoch [4/120    avg_loss:0.423, val_acc:0.869]
Epoch [5/120    avg_loss:0.373, val_acc:0.785]
Epoch [6/120    avg_loss:0.268, val_acc:0.887]
Epoch [7/120    avg_loss:0.298, val_acc:0.907]
Epoch [8/120    avg_loss:0.211, val_acc:0.900]
Epoch [9/120    avg_loss:0.180, val_acc:0.865]
Epoch [10/120    avg_loss:0.166, val_acc:0.897]
Epoch [11/120    avg_loss:0.189, val_acc:0.933]
Epoch [12/120    avg_loss:0.171, val_acc:0.933]
Epoch [13/120    avg_loss:0.098, val_acc:0.931]
Epoch [14/120    avg_loss:0.103, val_acc:0.918]
Epoch [15/120    avg_loss:0.092, val_acc:0.923]
Epoch [16/120    avg_loss:0.095, val_acc:0.954]
Epoch [17/120    avg_loss:0.089, val_acc:0.943]
Epoch [18/120    avg_loss:0.071, val_acc:0.965]
Epoch [19/120    avg_loss:0.049, val_acc:0.972]
Epoch [20/120    avg_loss:0.102, val_acc:0.942]
Epoch [21/120    avg_loss:0.092, val_acc:0.946]
Epoch [22/120    avg_loss:0.073, val_acc:0.963]
Epoch [23/120    avg_loss:0.104, val_acc:0.924]
Epoch [24/120    avg_loss:0.087, val_acc:0.965]
Epoch [25/120    avg_loss:0.067, val_acc:0.955]
Epoch [26/120    avg_loss:0.038, val_acc:0.954]
Epoch [27/120    avg_loss:0.043, val_acc:0.972]
Epoch [28/120    avg_loss:0.047, val_acc:0.967]
Epoch [29/120    avg_loss:0.091, val_acc:0.965]
Epoch [30/120    avg_loss:0.035, val_acc:0.976]
Epoch [31/120    avg_loss:0.029, val_acc:0.986]
Epoch [32/120    avg_loss:0.025, val_acc:0.980]
Epoch [33/120    avg_loss:0.016, val_acc:0.984]
Epoch [34/120    avg_loss:0.017, val_acc:0.975]
Epoch [35/120    avg_loss:0.058, val_acc:0.977]
Epoch [36/120    avg_loss:0.037, val_acc:0.969]
Epoch [37/120    avg_loss:0.020, val_acc:0.981]
Epoch [38/120    avg_loss:0.022, val_acc:0.976]
Epoch [39/120    avg_loss:0.021, val_acc:0.981]
Epoch [40/120    avg_loss:0.024, val_acc:0.972]
Epoch [41/120    avg_loss:0.016, val_acc:0.982]
Epoch [42/120    avg_loss:0.014, val_acc:0.979]
Epoch [43/120    avg_loss:0.024, val_acc:0.981]
Epoch [44/120    avg_loss:0.011, val_acc:0.981]
Epoch [45/120    avg_loss:0.016, val_acc:0.983]
Epoch [46/120    avg_loss:0.010, val_acc:0.986]
Epoch [47/120    avg_loss:0.013, val_acc:0.983]
Epoch [48/120    avg_loss:0.013, val_acc:0.984]
Epoch [49/120    avg_loss:0.012, val_acc:0.984]
Epoch [50/120    avg_loss:0.013, val_acc:0.984]
Epoch [51/120    avg_loss:0.009, val_acc:0.984]
Epoch [52/120    avg_loss:0.022, val_acc:0.984]
Epoch [53/120    avg_loss:0.010, val_acc:0.986]
Epoch [54/120    avg_loss:0.016, val_acc:0.985]
Epoch [55/120    avg_loss:0.010, val_acc:0.983]
Epoch [56/120    avg_loss:0.008, val_acc:0.984]
Epoch [57/120    avg_loss:0.009, val_acc:0.985]
Epoch [58/120    avg_loss:0.009, val_acc:0.987]
Epoch [59/120    avg_loss:0.009, val_acc:0.986]
Epoch [60/120    avg_loss:0.018, val_acc:0.986]
Epoch [61/120    avg_loss:0.014, val_acc:0.986]
Epoch [62/120    avg_loss:0.006, val_acc:0.986]
Epoch [63/120    avg_loss:0.009, val_acc:0.986]
Epoch [64/120    avg_loss:0.006, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.986]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.986]
Epoch [68/120    avg_loss:0.011, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.987]
Epoch [70/120    avg_loss:0.008, val_acc:0.988]
Epoch [71/120    avg_loss:0.014, val_acc:0.986]
Epoch [72/120    avg_loss:0.010, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.988]
Epoch [76/120    avg_loss:0.014, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.987]
Epoch [78/120    avg_loss:0.006, val_acc:0.987]
Epoch [79/120    avg_loss:0.013, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.012, val_acc:0.990]
Epoch [82/120    avg_loss:0.011, val_acc:0.990]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.990]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.991]
Epoch [90/120    avg_loss:0.007, val_acc:0.991]
Epoch [91/120    avg_loss:0.011, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.989]
Epoch [93/120    avg_loss:0.011, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.991]
Epoch [96/120    avg_loss:0.009, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.989]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.020, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.989]
Epoch [104/120    avg_loss:0.010, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.010, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.010, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.011, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     4     0     0    11     0     0     1]
 [    0     0 18049     0    17     0    22     0     0     2]
 [    0     2     0  2009     0     0     0     0    17     8]
 [    0    26     0     0  2922     0     1     0    13    10]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4874     0     0     2]
 [    0     0     0     0     0     0     3  1282     0     5]
 [    0     2     0    10    19     0     0     0  3537     3]
 [    0     0     0     2     2    16     0     0     0   899]]

Accuracy:
99.51799098643144

F1 scores:
[       nan 0.99642802 0.99881022 0.98941148 0.98516521 0.99390708
 0.99581163 0.99688958 0.9910339  0.97241752]

Kappa:
0.9936155765444155
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79d54907f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.278, val_acc:0.552]
Epoch [2/120    avg_loss:0.708, val_acc:0.746]
Epoch [3/120    avg_loss:0.569, val_acc:0.720]
Epoch [4/120    avg_loss:0.430, val_acc:0.741]
Epoch [5/120    avg_loss:0.349, val_acc:0.905]
Epoch [6/120    avg_loss:0.277, val_acc:0.863]
Epoch [7/120    avg_loss:0.318, val_acc:0.912]
Epoch [8/120    avg_loss:0.228, val_acc:0.916]
Epoch [9/120    avg_loss:0.191, val_acc:0.898]
Epoch [10/120    avg_loss:0.235, val_acc:0.887]
Epoch [11/120    avg_loss:0.158, val_acc:0.936]
Epoch [12/120    avg_loss:0.122, val_acc:0.913]
Epoch [13/120    avg_loss:0.123, val_acc:0.948]
Epoch [14/120    avg_loss:0.135, val_acc:0.943]
Epoch [15/120    avg_loss:0.093, val_acc:0.910]
Epoch [16/120    avg_loss:0.096, val_acc:0.927]
Epoch [17/120    avg_loss:0.171, val_acc:0.911]
Epoch [18/120    avg_loss:0.079, val_acc:0.928]
Epoch [19/120    avg_loss:0.068, val_acc:0.959]
Epoch [20/120    avg_loss:0.087, val_acc:0.934]
Epoch [21/120    avg_loss:0.209, val_acc:0.946]
Epoch [22/120    avg_loss:0.100, val_acc:0.946]
Epoch [23/120    avg_loss:0.065, val_acc:0.964]
Epoch [24/120    avg_loss:0.052, val_acc:0.962]
Epoch [25/120    avg_loss:0.117, val_acc:0.938]
Epoch [26/120    avg_loss:0.064, val_acc:0.943]
Epoch [27/120    avg_loss:0.088, val_acc:0.962]
Epoch [28/120    avg_loss:0.073, val_acc:0.970]
Epoch [29/120    avg_loss:0.034, val_acc:0.923]
Epoch [30/120    avg_loss:0.042, val_acc:0.970]
Epoch [31/120    avg_loss:0.034, val_acc:0.965]
Epoch [32/120    avg_loss:0.044, val_acc:0.969]
Epoch [33/120    avg_loss:0.041, val_acc:0.961]
Epoch [34/120    avg_loss:0.022, val_acc:0.976]
Epoch [35/120    avg_loss:0.026, val_acc:0.976]
Epoch [36/120    avg_loss:0.043, val_acc:0.979]
Epoch [37/120    avg_loss:0.028, val_acc:0.965]
Epoch [38/120    avg_loss:0.026, val_acc:0.972]
Epoch [39/120    avg_loss:0.019, val_acc:0.977]
Epoch [40/120    avg_loss:0.008, val_acc:0.972]
Epoch [41/120    avg_loss:0.029, val_acc:0.969]
Epoch [42/120    avg_loss:0.014, val_acc:0.975]
Epoch [43/120    avg_loss:0.017, val_acc:0.966]
Epoch [44/120    avg_loss:0.038, val_acc:0.961]
Epoch [45/120    avg_loss:0.026, val_acc:0.976]
Epoch [46/120    avg_loss:0.032, val_acc:0.981]
Epoch [47/120    avg_loss:0.022, val_acc:0.979]
Epoch [48/120    avg_loss:0.028, val_acc:0.971]
Epoch [49/120    avg_loss:0.011, val_acc:0.978]
Epoch [50/120    avg_loss:0.032, val_acc:0.979]
Epoch [51/120    avg_loss:0.014, val_acc:0.981]
Epoch [52/120    avg_loss:0.011, val_acc:0.979]
Epoch [53/120    avg_loss:0.020, val_acc:0.971]
Epoch [54/120    avg_loss:0.037, val_acc:0.978]
Epoch [55/120    avg_loss:0.028, val_acc:0.977]
Epoch [56/120    avg_loss:0.041, val_acc:0.955]
Epoch [57/120    avg_loss:0.182, val_acc:0.874]
Epoch [58/120    avg_loss:0.250, val_acc:0.924]
Epoch [59/120    avg_loss:0.059, val_acc:0.964]
Epoch [60/120    avg_loss:0.050, val_acc:0.975]
Epoch [61/120    avg_loss:0.027, val_acc:0.972]
Epoch [62/120    avg_loss:0.020, val_acc:0.976]
Epoch [63/120    avg_loss:0.024, val_acc:0.980]
Epoch [64/120    avg_loss:0.029, val_acc:0.954]
Epoch [65/120    avg_loss:0.025, val_acc:0.974]
Epoch [66/120    avg_loss:0.015, val_acc:0.980]
Epoch [67/120    avg_loss:0.017, val_acc:0.977]
Epoch [68/120    avg_loss:0.014, val_acc:0.979]
Epoch [69/120    avg_loss:0.009, val_acc:0.981]
Epoch [70/120    avg_loss:0.013, val_acc:0.982]
Epoch [71/120    avg_loss:0.016, val_acc:0.981]
Epoch [72/120    avg_loss:0.013, val_acc:0.979]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.015, val_acc:0.983]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.009, val_acc:0.980]
Epoch [77/120    avg_loss:0.013, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.981]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.981]
Epoch [84/120    avg_loss:0.014, val_acc:0.981]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.983]
Epoch [87/120    avg_loss:0.013, val_acc:0.981]
Epoch [88/120    avg_loss:0.009, val_acc:0.981]
Epoch [89/120    avg_loss:0.013, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.981]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.013, val_acc:0.980]
Epoch [93/120    avg_loss:0.007, val_acc:0.982]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.981]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.016, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.015, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.011, val_acc:0.984]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.013, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0    10    18     4     0]
 [    0     0 18037     0    25     0    28     0     0     0]
 [    0    10     0  1984     0     0     0     0    40     2]
 [    0    32     1     0  2920     0     0     0    13     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4856     0    21     1]
 [    0     1     0     0     0     0     2  1284     0     3]
 [    0    28     0    11    27     0     0     0  3491    14]
 [    0     0     0     2     9    26     0     0     0   882]]

Accuracy:
99.19504494734052

F1 scores:
[       nan 0.99201736 0.99850531 0.98388297 0.98101797 0.99013657
 0.99365664 0.99074074 0.97787115 0.96551724]

Kappa:
0.9893389672223128
creating ./logs/logs-2022-01-19PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa5351a16d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.270, val_acc:0.611]
Epoch [2/120    avg_loss:0.680, val_acc:0.675]
Epoch [3/120    avg_loss:0.524, val_acc:0.758]
Epoch [4/120    avg_loss:0.397, val_acc:0.802]
Epoch [5/120    avg_loss:0.314, val_acc:0.834]
Epoch [6/120    avg_loss:0.293, val_acc:0.907]
Epoch [7/120    avg_loss:0.240, val_acc:0.878]
Epoch [8/120    avg_loss:0.205, val_acc:0.914]
Epoch [9/120    avg_loss:0.270, val_acc:0.898]
Epoch [10/120    avg_loss:0.182, val_acc:0.873]
Epoch [11/120    avg_loss:0.154, val_acc:0.861]
Epoch [12/120    avg_loss:0.124, val_acc:0.919]
Epoch [13/120    avg_loss:0.115, val_acc:0.943]
Epoch [14/120    avg_loss:0.082, val_acc:0.896]
Epoch [15/120    avg_loss:0.088, val_acc:0.948]
Epoch [16/120    avg_loss:0.105, val_acc:0.949]
Epoch [17/120    avg_loss:0.126, val_acc:0.949]
Epoch [18/120    avg_loss:0.126, val_acc:0.938]
Epoch [19/120    avg_loss:0.106, val_acc:0.948]
Epoch [20/120    avg_loss:0.069, val_acc:0.957]
Epoch [21/120    avg_loss:0.077, val_acc:0.950]
Epoch [22/120    avg_loss:0.063, val_acc:0.939]
Epoch [23/120    avg_loss:0.073, val_acc:0.971]
Epoch [24/120    avg_loss:0.041, val_acc:0.956]
Epoch [25/120    avg_loss:0.031, val_acc:0.973]
Epoch [26/120    avg_loss:0.029, val_acc:0.967]
Epoch [27/120    avg_loss:0.044, val_acc:0.960]
Epoch [28/120    avg_loss:0.066, val_acc:0.957]
Epoch [29/120    avg_loss:0.042, val_acc:0.970]
Epoch [30/120    avg_loss:0.068, val_acc:0.936]
Epoch [31/120    avg_loss:0.091, val_acc:0.957]
Epoch [32/120    avg_loss:0.038, val_acc:0.957]
Epoch [33/120    avg_loss:0.053, val_acc:0.959]
Epoch [34/120    avg_loss:0.040, val_acc:0.970]
Epoch [35/120    avg_loss:0.033, val_acc:0.969]
Epoch [36/120    avg_loss:0.036, val_acc:0.971]
Epoch [37/120    avg_loss:0.042, val_acc:0.978]
Epoch [38/120    avg_loss:0.052, val_acc:0.976]
Epoch [39/120    avg_loss:0.046, val_acc:0.977]
Epoch [40/120    avg_loss:0.029, val_acc:0.981]
Epoch [41/120    avg_loss:0.021, val_acc:0.979]
Epoch [42/120    avg_loss:0.038, val_acc:0.971]
Epoch [43/120    avg_loss:0.020, val_acc:0.976]
Epoch [44/120    avg_loss:0.022, val_acc:0.974]
Epoch [45/120    avg_loss:0.032, val_acc:0.984]
Epoch [46/120    avg_loss:0.020, val_acc:0.960]
Epoch [47/120    avg_loss:0.031, val_acc:0.976]
Epoch [48/120    avg_loss:0.016, val_acc:0.977]
Epoch [49/120    avg_loss:0.044, val_acc:0.937]
Epoch [50/120    avg_loss:0.047, val_acc:0.972]
Epoch [51/120    avg_loss:0.019, val_acc:0.984]
Epoch [52/120    avg_loss:0.016, val_acc:0.988]
Epoch [53/120    avg_loss:0.012, val_acc:0.986]
Epoch [54/120    avg_loss:0.018, val_acc:0.976]
Epoch [55/120    avg_loss:0.012, val_acc:0.985]
Epoch [56/120    avg_loss:0.025, val_acc:0.985]
Epoch [57/120    avg_loss:0.012, val_acc:0.986]
Epoch [58/120    avg_loss:0.025, val_acc:0.985]
Epoch [59/120    avg_loss:0.063, val_acc:0.958]
Epoch [60/120    avg_loss:0.042, val_acc:0.973]
Epoch [61/120    avg_loss:0.026, val_acc:0.981]
Epoch [62/120    avg_loss:0.033, val_acc:0.981]
Epoch [63/120    avg_loss:0.019, val_acc:0.986]
Epoch [64/120    avg_loss:0.039, val_acc:0.976]
Epoch [65/120    avg_loss:0.018, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.013, val_acc:0.987]
Epoch [68/120    avg_loss:0.008, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.986]
Epoch [70/120    avg_loss:0.006, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.986]
Epoch [80/120    avg_loss:0.012, val_acc:0.986]
Epoch [81/120    avg_loss:0.015, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.986]
Epoch [86/120    avg_loss:0.018, val_acc:0.986]
Epoch [87/120    avg_loss:0.012, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.986]
Epoch [90/120    avg_loss:0.004, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.011, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6378     0     0     0     0    16    31     4     3]
 [    0     0 18022     0    24     0    44     0     0     0]
 [    0     4     0  2023     0     0     0     0     7     2]
 [    0    20     2     0  2931     0     1     0    17     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4877     0     0     1]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     1     0     3    38     0     0     0  3528     1]
 [    0     0     0    14    14    33     0     0     0   858]]

Accuracy:
99.3179572458005

F1 scores:
[       nan 0.99384496 0.99806169 0.99263984 0.98043151 0.98751419
 0.99368378 0.98735148 0.99003788 0.96026861]

Kappa:
0.9909694410787214
