creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f20e43956d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.230, val_acc:0.656]
Epoch [2/120    avg_loss:0.725, val_acc:0.594]
Epoch [3/120    avg_loss:0.482, val_acc:0.749]
Epoch [4/120    avg_loss:0.464, val_acc:0.859]
Epoch [5/120    avg_loss:0.377, val_acc:0.789]
Epoch [6/120    avg_loss:0.278, val_acc:0.845]
Epoch [7/120    avg_loss:0.272, val_acc:0.889]
Epoch [8/120    avg_loss:0.208, val_acc:0.893]
Epoch [9/120    avg_loss:0.224, val_acc:0.916]
Epoch [10/120    avg_loss:0.207, val_acc:0.935]
Epoch [11/120    avg_loss:0.224, val_acc:0.899]
Epoch [12/120    avg_loss:0.121, val_acc:0.938]
Epoch [13/120    avg_loss:0.116, val_acc:0.909]
Epoch [14/120    avg_loss:0.116, val_acc:0.909]
Epoch [15/120    avg_loss:0.138, val_acc:0.923]
Epoch [16/120    avg_loss:0.121, val_acc:0.946]
Epoch [17/120    avg_loss:0.117, val_acc:0.934]
Epoch [18/120    avg_loss:0.081, val_acc:0.955]
Epoch [19/120    avg_loss:0.051, val_acc:0.958]
Epoch [20/120    avg_loss:0.070, val_acc:0.946]
Epoch [21/120    avg_loss:0.076, val_acc:0.955]
Epoch [22/120    avg_loss:0.054, val_acc:0.952]
Epoch [23/120    avg_loss:0.059, val_acc:0.960]
Epoch [24/120    avg_loss:0.081, val_acc:0.963]
Epoch [25/120    avg_loss:0.055, val_acc:0.965]
Epoch [26/120    avg_loss:0.052, val_acc:0.966]
Epoch [27/120    avg_loss:0.048, val_acc:0.972]
Epoch [28/120    avg_loss:0.113, val_acc:0.942]
Epoch [29/120    avg_loss:0.165, val_acc:0.900]
Epoch [30/120    avg_loss:0.095, val_acc:0.948]
Epoch [31/120    avg_loss:0.050, val_acc:0.964]
Epoch [32/120    avg_loss:0.046, val_acc:0.970]
Epoch [33/120    avg_loss:0.072, val_acc:0.970]
Epoch [34/120    avg_loss:0.096, val_acc:0.962]
Epoch [35/120    avg_loss:0.040, val_acc:0.974]
Epoch [36/120    avg_loss:0.039, val_acc:0.965]
Epoch [37/120    avg_loss:0.020, val_acc:0.979]
Epoch [38/120    avg_loss:0.018, val_acc:0.976]
Epoch [39/120    avg_loss:0.038, val_acc:0.966]
Epoch [40/120    avg_loss:0.034, val_acc:0.975]
Epoch [41/120    avg_loss:0.022, val_acc:0.975]
Epoch [42/120    avg_loss:0.026, val_acc:0.981]
Epoch [43/120    avg_loss:0.036, val_acc:0.969]
Epoch [44/120    avg_loss:0.031, val_acc:0.893]
Epoch [45/120    avg_loss:0.024, val_acc:0.977]
Epoch [46/120    avg_loss:0.022, val_acc:0.973]
Epoch [47/120    avg_loss:0.010, val_acc:0.981]
Epoch [48/120    avg_loss:0.014, val_acc:0.983]
Epoch [49/120    avg_loss:0.025, val_acc:0.981]
Epoch [50/120    avg_loss:0.014, val_acc:0.985]
Epoch [51/120    avg_loss:0.023, val_acc:0.943]
Epoch [52/120    avg_loss:0.028, val_acc:0.979]
Epoch [53/120    avg_loss:0.039, val_acc:0.974]
Epoch [54/120    avg_loss:0.016, val_acc:0.979]
Epoch [55/120    avg_loss:0.018, val_acc:0.981]
Epoch [56/120    avg_loss:0.010, val_acc:0.976]
Epoch [57/120    avg_loss:0.009, val_acc:0.986]
Epoch [58/120    avg_loss:0.011, val_acc:0.983]
Epoch [59/120    avg_loss:0.010, val_acc:0.982]
Epoch [60/120    avg_loss:0.014, val_acc:0.970]
Epoch [61/120    avg_loss:0.011, val_acc:0.982]
Epoch [62/120    avg_loss:0.056, val_acc:0.920]
Epoch [63/120    avg_loss:0.013, val_acc:0.982]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.008, val_acc:0.986]
Epoch [66/120    avg_loss:0.010, val_acc:0.985]
Epoch [67/120    avg_loss:0.040, val_acc:0.972]
Epoch [68/120    avg_loss:0.059, val_acc:0.975]
Epoch [69/120    avg_loss:0.019, val_acc:0.979]
Epoch [70/120    avg_loss:0.014, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.004, val_acc:0.983]
Epoch [73/120    avg_loss:0.007, val_acc:0.983]
Epoch [74/120    avg_loss:0.022, val_acc:0.983]
Epoch [75/120    avg_loss:0.014, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.014, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.005, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.004, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.003, val_acc:0.989]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.003, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.011, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.988]
Epoch [108/120    avg_loss:0.003, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.003, val_acc:0.988]
Epoch [112/120    avg_loss:0.003, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.003, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     0     0    14     0     2     2]
 [    0     0 17951     0    45     0    90     0     0     4]
 [    0     0     0  2007     0     0     0     0    19    10]
 [    0    26     0     0  2932     0     0     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4863     0    13     1]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     1     0    15    29     0     0     0  3525     1]
 [    0     0     0     0    14    26     0     0     0   879]]

Accuracy:
99.20227508254405

F1 scores:
[       nan 0.99650431 0.99611564 0.98915722 0.97863818 0.99013657
 0.98771199 0.9984472  0.98684211 0.9669967 ]

Kappa:
0.9894447969869092
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f69e47a5780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.256, val_acc:0.465]
Epoch [2/120    avg_loss:0.704, val_acc:0.735]
Epoch [3/120    avg_loss:0.538, val_acc:0.660]
Epoch [4/120    avg_loss:0.439, val_acc:0.869]
Epoch [5/120    avg_loss:0.296, val_acc:0.861]
Epoch [6/120    avg_loss:0.335, val_acc:0.872]
Epoch [7/120    avg_loss:0.242, val_acc:0.920]
Epoch [8/120    avg_loss:0.171, val_acc:0.898]
Epoch [9/120    avg_loss:0.158, val_acc:0.941]
Epoch [10/120    avg_loss:0.168, val_acc:0.949]
Epoch [11/120    avg_loss:0.147, val_acc:0.952]
Epoch [12/120    avg_loss:0.142, val_acc:0.926]
Epoch [13/120    avg_loss:0.132, val_acc:0.933]
Epoch [14/120    avg_loss:0.192, val_acc:0.938]
Epoch [15/120    avg_loss:0.091, val_acc:0.943]
Epoch [16/120    avg_loss:0.110, val_acc:0.939]
Epoch [17/120    avg_loss:0.126, val_acc:0.912]
Epoch [18/120    avg_loss:0.109, val_acc:0.939]
Epoch [19/120    avg_loss:0.113, val_acc:0.965]
Epoch [20/120    avg_loss:0.098, val_acc:0.954]
Epoch [21/120    avg_loss:0.088, val_acc:0.962]
Epoch [22/120    avg_loss:0.050, val_acc:0.964]
Epoch [23/120    avg_loss:0.072, val_acc:0.935]
Epoch [24/120    avg_loss:0.090, val_acc:0.944]
Epoch [25/120    avg_loss:0.042, val_acc:0.973]
Epoch [26/120    avg_loss:0.030, val_acc:0.976]
Epoch [27/120    avg_loss:0.049, val_acc:0.957]
Epoch [28/120    avg_loss:0.052, val_acc:0.977]
Epoch [29/120    avg_loss:0.026, val_acc:0.959]
Epoch [30/120    avg_loss:0.032, val_acc:0.968]
Epoch [31/120    avg_loss:0.036, val_acc:0.950]
Epoch [32/120    avg_loss:0.040, val_acc:0.973]
Epoch [33/120    avg_loss:0.036, val_acc:0.955]
Epoch [34/120    avg_loss:0.046, val_acc:0.967]
Epoch [35/120    avg_loss:0.043, val_acc:0.976]
Epoch [36/120    avg_loss:0.027, val_acc:0.976]
Epoch [37/120    avg_loss:0.019, val_acc:0.980]
Epoch [38/120    avg_loss:0.034, val_acc:0.972]
Epoch [39/120    avg_loss:0.019, val_acc:0.982]
Epoch [40/120    avg_loss:0.014, val_acc:0.970]
Epoch [41/120    avg_loss:0.030, val_acc:0.983]
Epoch [42/120    avg_loss:0.042, val_acc:0.980]
Epoch [43/120    avg_loss:0.033, val_acc:0.981]
Epoch [44/120    avg_loss:0.017, val_acc:0.982]
Epoch [45/120    avg_loss:0.015, val_acc:0.983]
Epoch [46/120    avg_loss:0.015, val_acc:0.984]
Epoch [47/120    avg_loss:0.011, val_acc:0.985]
Epoch [48/120    avg_loss:0.024, val_acc:0.986]
Epoch [49/120    avg_loss:0.016, val_acc:0.974]
Epoch [50/120    avg_loss:0.029, val_acc:0.981]
Epoch [51/120    avg_loss:0.035, val_acc:0.979]
Epoch [52/120    avg_loss:0.073, val_acc:0.920]
Epoch [53/120    avg_loss:0.083, val_acc:0.975]
Epoch [54/120    avg_loss:0.045, val_acc:0.927]
Epoch [55/120    avg_loss:0.078, val_acc:0.949]
Epoch [56/120    avg_loss:0.038, val_acc:0.987]
Epoch [57/120    avg_loss:0.020, val_acc:0.983]
Epoch [58/120    avg_loss:0.030, val_acc:0.980]
Epoch [59/120    avg_loss:0.027, val_acc:0.986]
Epoch [60/120    avg_loss:0.029, val_acc:0.985]
Epoch [61/120    avg_loss:0.018, val_acc:0.986]
Epoch [62/120    avg_loss:0.041, val_acc:0.982]
Epoch [63/120    avg_loss:0.044, val_acc:0.964]
Epoch [64/120    avg_loss:0.081, val_acc:0.976]
Epoch [65/120    avg_loss:0.028, val_acc:0.979]
Epoch [66/120    avg_loss:0.016, val_acc:0.977]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.030, val_acc:0.927]
Epoch [69/120    avg_loss:0.019, val_acc:0.986]
Epoch [70/120    avg_loss:0.012, val_acc:0.987]
Epoch [71/120    avg_loss:0.014, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.986]
Epoch [74/120    avg_loss:0.006, val_acc:0.986]
Epoch [75/120    avg_loss:0.007, val_acc:0.987]
Epoch [76/120    avg_loss:0.007, val_acc:0.987]
Epoch [77/120    avg_loss:0.025, val_acc:0.986]
Epoch [78/120    avg_loss:0.016, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.987]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.006, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.009, val_acc:0.990]
Epoch [85/120    avg_loss:0.005, val_acc:0.990]
Epoch [86/120    avg_loss:0.017, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.007, val_acc:0.991]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.992]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.013, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.010, val_acc:0.991]
Epoch [108/120    avg_loss:0.014, val_acc:0.991]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.991]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.004, val_acc:0.992]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     4     0    24     0     9     0]
 [    0     2 18032     0    41     0    15     0     0     0]
 [    0     0     0  2000     1     0     0     0    27     8]
 [    0    18     0     0  2935     0     0     0    16     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4869     0     9     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    14     0     6    41     0     0     0  3505     5]
 [    0     0     0     0    13    28     0     0     0   878]]

Accuracy:
99.31072711059697

F1 scores:
[       nan 0.99447943 0.99839433 0.9896091  0.97719327 0.9893859
 0.99509503 0.9992242  0.98220541 0.96749311]

Kappa:
0.9908723452417028
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f360f1c57f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.320, val_acc:0.451]
Epoch [2/120    avg_loss:0.632, val_acc:0.713]
Epoch [3/120    avg_loss:0.460, val_acc:0.831]
Epoch [4/120    avg_loss:0.353, val_acc:0.887]
Epoch [5/120    avg_loss:0.383, val_acc:0.887]
Epoch [6/120    avg_loss:0.269, val_acc:0.863]
Epoch [7/120    avg_loss:0.296, val_acc:0.910]
Epoch [8/120    avg_loss:0.185, val_acc:0.915]
Epoch [9/120    avg_loss:0.130, val_acc:0.896]
Epoch [10/120    avg_loss:0.182, val_acc:0.943]
Epoch [11/120    avg_loss:0.143, val_acc:0.916]
Epoch [12/120    avg_loss:0.144, val_acc:0.870]
Epoch [13/120    avg_loss:0.142, val_acc:0.886]
Epoch [14/120    avg_loss:0.095, val_acc:0.930]
Epoch [15/120    avg_loss:0.097, val_acc:0.926]
Epoch [16/120    avg_loss:0.147, val_acc:0.923]
Epoch [17/120    avg_loss:0.071, val_acc:0.949]
Epoch [18/120    avg_loss:0.089, val_acc:0.961]
Epoch [19/120    avg_loss:0.078, val_acc:0.957]
Epoch [20/120    avg_loss:0.109, val_acc:0.942]
Epoch [21/120    avg_loss:0.091, val_acc:0.916]
Epoch [22/120    avg_loss:0.049, val_acc:0.952]
Epoch [23/120    avg_loss:0.033, val_acc:0.943]
Epoch [24/120    avg_loss:0.040, val_acc:0.972]
Epoch [25/120    avg_loss:0.045, val_acc:0.974]
Epoch [26/120    avg_loss:0.039, val_acc:0.971]
Epoch [27/120    avg_loss:0.031, val_acc:0.978]
Epoch [28/120    avg_loss:0.045, val_acc:0.946]
Epoch [29/120    avg_loss:0.045, val_acc:0.967]
Epoch [30/120    avg_loss:0.091, val_acc:0.960]
Epoch [31/120    avg_loss:0.071, val_acc:0.975]
Epoch [32/120    avg_loss:0.048, val_acc:0.965]
Epoch [33/120    avg_loss:0.030, val_acc:0.980]
Epoch [34/120    avg_loss:0.024, val_acc:0.976]
Epoch [35/120    avg_loss:0.022, val_acc:0.981]
Epoch [36/120    avg_loss:0.025, val_acc:0.973]
Epoch [37/120    avg_loss:0.028, val_acc:0.952]
Epoch [38/120    avg_loss:0.015, val_acc:0.978]
Epoch [39/120    avg_loss:0.017, val_acc:0.961]
Epoch [40/120    avg_loss:0.027, val_acc:0.986]
Epoch [41/120    avg_loss:0.022, val_acc:0.976]
Epoch [42/120    avg_loss:0.017, val_acc:0.981]
Epoch [43/120    avg_loss:0.015, val_acc:0.961]
Epoch [44/120    avg_loss:0.021, val_acc:0.972]
Epoch [45/120    avg_loss:0.014, val_acc:0.979]
Epoch [46/120    avg_loss:0.008, val_acc:0.986]
Epoch [47/120    avg_loss:0.012, val_acc:0.984]
Epoch [48/120    avg_loss:0.026, val_acc:0.943]
Epoch [49/120    avg_loss:0.030, val_acc:0.986]
Epoch [50/120    avg_loss:0.011, val_acc:0.976]
Epoch [51/120    avg_loss:0.009, val_acc:0.987]
Epoch [52/120    avg_loss:0.010, val_acc:0.986]
Epoch [53/120    avg_loss:0.008, val_acc:0.983]
Epoch [54/120    avg_loss:0.011, val_acc:0.981]
Epoch [55/120    avg_loss:0.010, val_acc:0.984]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.012, val_acc:0.981]
Epoch [58/120    avg_loss:0.008, val_acc:0.986]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.008, val_acc:0.985]
Epoch [61/120    avg_loss:0.009, val_acc:0.988]
Epoch [62/120    avg_loss:0.008, val_acc:0.981]
Epoch [63/120    avg_loss:0.012, val_acc:0.984]
Epoch [64/120    avg_loss:0.007, val_acc:0.984]
Epoch [65/120    avg_loss:0.016, val_acc:0.985]
Epoch [66/120    avg_loss:0.011, val_acc:0.981]
Epoch [67/120    avg_loss:0.008, val_acc:0.985]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.987]
Epoch [71/120    avg_loss:0.012, val_acc:0.981]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.005, val_acc:0.987]
Epoch [74/120    avg_loss:0.005, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.987]
Epoch [76/120    avg_loss:0.004, val_acc:0.986]
Epoch [77/120    avg_loss:0.007, val_acc:0.987]
Epoch [78/120    avg_loss:0.007, val_acc:0.987]
Epoch [79/120    avg_loss:0.004, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.004, val_acc:0.987]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.988]
Epoch [85/120    avg_loss:0.005, val_acc:0.987]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.003, val_acc:0.988]
Epoch [88/120    avg_loss:0.003, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.990]
Epoch [90/120    avg_loss:0.004, val_acc:0.989]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.003, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.011, val_acc:0.987]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.988]
Epoch [101/120    avg_loss:0.014, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.003, val_acc:0.989]
Epoch [113/120    avg_loss:0.008, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.002, val_acc:0.990]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.013, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.002, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     4     0     0    34     0     0]
 [    0     1 18041     0    28     0    20     0     0     0]
 [    0     0     0  2010     1     0     0     0    24     1]
 [    0    25     2     1  2931     0     0     0    13     0]
 [    0     0    32     0    88  1185     0     0     0     0]
 [    0     0     4     0     0     0  4864     0    10     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0    10     0     9    36     0     0     0  3513     3]
 [    0     0     1     5     9    15     0     0     0   889]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.99416932 0.99756704 0.98990396 0.96589224 0.94610778
 0.99651711 0.98660543 0.98527556 0.9812362 ]

Kappa:
0.9879603639684121
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f630ed72780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.275, val_acc:0.465]
Epoch [2/120    avg_loss:0.754, val_acc:0.733]
Epoch [3/120    avg_loss:0.481, val_acc:0.791]
Epoch [4/120    avg_loss:0.376, val_acc:0.845]
Epoch [5/120    avg_loss:0.338, val_acc:0.866]
Epoch [6/120    avg_loss:0.272, val_acc:0.785]
Epoch [7/120    avg_loss:0.259, val_acc:0.904]
Epoch [8/120    avg_loss:0.176, val_acc:0.926]
Epoch [9/120    avg_loss:0.190, val_acc:0.932]
Epoch [10/120    avg_loss:0.171, val_acc:0.889]
Epoch [11/120    avg_loss:0.153, val_acc:0.939]
Epoch [12/120    avg_loss:0.108, val_acc:0.945]
Epoch [13/120    avg_loss:0.117, val_acc:0.955]
Epoch [14/120    avg_loss:0.102, val_acc:0.918]
Epoch [15/120    avg_loss:0.141, val_acc:0.958]
Epoch [16/120    avg_loss:0.067, val_acc:0.965]
Epoch [17/120    avg_loss:0.077, val_acc:0.974]
Epoch [18/120    avg_loss:0.101, val_acc:0.948]
Epoch [19/120    avg_loss:0.121, val_acc:0.947]
Epoch [20/120    avg_loss:0.128, val_acc:0.959]
Epoch [21/120    avg_loss:0.067, val_acc:0.963]
Epoch [22/120    avg_loss:0.064, val_acc:0.954]
Epoch [23/120    avg_loss:0.050, val_acc:0.970]
Epoch [24/120    avg_loss:0.026, val_acc:0.974]
Epoch [25/120    avg_loss:0.040, val_acc:0.976]
Epoch [26/120    avg_loss:0.047, val_acc:0.980]
Epoch [27/120    avg_loss:0.023, val_acc:0.976]
Epoch [28/120    avg_loss:0.025, val_acc:0.978]
Epoch [29/120    avg_loss:0.031, val_acc:0.980]
Epoch [30/120    avg_loss:0.022, val_acc:0.976]
Epoch [31/120    avg_loss:0.060, val_acc:0.978]
Epoch [32/120    avg_loss:0.018, val_acc:0.981]
Epoch [33/120    avg_loss:0.030, val_acc:0.983]
Epoch [34/120    avg_loss:0.025, val_acc:0.983]
Epoch [35/120    avg_loss:0.014, val_acc:0.985]
Epoch [36/120    avg_loss:0.023, val_acc:0.959]
Epoch [37/120    avg_loss:0.040, val_acc:0.945]
Epoch [38/120    avg_loss:0.092, val_acc:0.969]
Epoch [39/120    avg_loss:0.066, val_acc:0.964]
Epoch [40/120    avg_loss:0.057, val_acc:0.961]
Epoch [41/120    avg_loss:0.066, val_acc:0.961]
Epoch [42/120    avg_loss:0.022, val_acc:0.984]
Epoch [43/120    avg_loss:0.020, val_acc:0.986]
Epoch [44/120    avg_loss:0.022, val_acc:0.979]
Epoch [45/120    avg_loss:0.021, val_acc:0.969]
Epoch [46/120    avg_loss:0.035, val_acc:0.976]
Epoch [47/120    avg_loss:0.032, val_acc:0.978]
Epoch [48/120    avg_loss:0.012, val_acc:0.984]
Epoch [49/120    avg_loss:0.009, val_acc:0.989]
Epoch [50/120    avg_loss:0.022, val_acc:0.985]
Epoch [51/120    avg_loss:0.015, val_acc:0.975]
Epoch [52/120    avg_loss:0.015, val_acc:0.987]
Epoch [53/120    avg_loss:0.012, val_acc:0.986]
Epoch [54/120    avg_loss:0.008, val_acc:0.987]
Epoch [55/120    avg_loss:0.017, val_acc:0.984]
Epoch [56/120    avg_loss:0.014, val_acc:0.988]
Epoch [57/120    avg_loss:0.128, val_acc:0.955]
Epoch [58/120    avg_loss:0.045, val_acc:0.984]
Epoch [59/120    avg_loss:0.017, val_acc:0.985]
Epoch [60/120    avg_loss:0.014, val_acc:0.978]
Epoch [61/120    avg_loss:0.033, val_acc:0.982]
Epoch [62/120    avg_loss:0.013, val_acc:0.985]
Epoch [63/120    avg_loss:0.011, val_acc:0.986]
Epoch [64/120    avg_loss:0.022, val_acc:0.984]
Epoch [65/120    avg_loss:0.007, val_acc:0.986]
Epoch [66/120    avg_loss:0.011, val_acc:0.986]
Epoch [67/120    avg_loss:0.016, val_acc:0.985]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.987]
Epoch [72/120    avg_loss:0.006, val_acc:0.988]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.005, val_acc:0.987]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.987]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.014, val_acc:0.987]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.005, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     0     0     0     0     6     0     1]
 [    0     0 17976     0    12     0   102     0     0     0]
 [    0     8     0  2011     1     0     0     0    13     3]
 [    0    24     4     0  2930     0     0     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     0     2]
 [    0     1     0     0     0     0     0  1287     0     2]
 [    0     1     0    12    47     0     0     0  3506     5]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.28662665991854

F1 scores:
[       nan 0.99681949 0.99672858 0.99088445 0.98058902 0.99088838
 0.98944805 0.99651568 0.98718851 0.97133407]

Kappa:
0.9905574443993684
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f415e07c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.274, val_acc:0.661]
Epoch [2/120    avg_loss:0.780, val_acc:0.743]
Epoch [3/120    avg_loss:0.564, val_acc:0.831]
Epoch [4/120    avg_loss:0.346, val_acc:0.873]
Epoch [5/120    avg_loss:0.297, val_acc:0.897]
Epoch [6/120    avg_loss:0.280, val_acc:0.898]
Epoch [7/120    avg_loss:0.274, val_acc:0.906]
Epoch [8/120    avg_loss:0.332, val_acc:0.882]
Epoch [9/120    avg_loss:0.269, val_acc:0.850]
Epoch [10/120    avg_loss:0.159, val_acc:0.878]
Epoch [11/120    avg_loss:0.191, val_acc:0.931]
Epoch [12/120    avg_loss:0.132, val_acc:0.915]
Epoch [13/120    avg_loss:0.112, val_acc:0.917]
Epoch [14/120    avg_loss:0.166, val_acc:0.946]
Epoch [15/120    avg_loss:0.125, val_acc:0.943]
Epoch [16/120    avg_loss:0.113, val_acc:0.927]
Epoch [17/120    avg_loss:0.074, val_acc:0.953]
Epoch [18/120    avg_loss:0.075, val_acc:0.915]
Epoch [19/120    avg_loss:0.078, val_acc:0.956]
Epoch [20/120    avg_loss:0.064, val_acc:0.947]
Epoch [21/120    avg_loss:0.077, val_acc:0.952]
Epoch [22/120    avg_loss:0.045, val_acc:0.962]
Epoch [23/120    avg_loss:0.040, val_acc:0.959]
Epoch [24/120    avg_loss:0.083, val_acc:0.968]
Epoch [25/120    avg_loss:0.033, val_acc:0.970]
Epoch [26/120    avg_loss:0.050, val_acc:0.968]
Epoch [27/120    avg_loss:0.062, val_acc:0.965]
Epoch [28/120    avg_loss:0.032, val_acc:0.966]
Epoch [29/120    avg_loss:0.039, val_acc:0.935]
Epoch [30/120    avg_loss:0.046, val_acc:0.948]
Epoch [31/120    avg_loss:0.024, val_acc:0.967]
Epoch [32/120    avg_loss:0.041, val_acc:0.981]
Epoch [33/120    avg_loss:0.016, val_acc:0.979]
Epoch [34/120    avg_loss:0.040, val_acc:0.971]
Epoch [35/120    avg_loss:0.031, val_acc:0.932]
Epoch [36/120    avg_loss:0.079, val_acc:0.964]
Epoch [37/120    avg_loss:0.053, val_acc:0.948]
Epoch [38/120    avg_loss:0.037, val_acc:0.955]
Epoch [39/120    avg_loss:0.040, val_acc:0.962]
Epoch [40/120    avg_loss:0.019, val_acc:0.973]
Epoch [41/120    avg_loss:0.016, val_acc:0.974]
Epoch [42/120    avg_loss:0.018, val_acc:0.982]
Epoch [43/120    avg_loss:0.067, val_acc:0.942]
Epoch [44/120    avg_loss:0.038, val_acc:0.979]
Epoch [45/120    avg_loss:0.017, val_acc:0.979]
Epoch [46/120    avg_loss:0.014, val_acc:0.980]
Epoch [47/120    avg_loss:0.021, val_acc:0.980]
Epoch [48/120    avg_loss:0.017, val_acc:0.980]
Epoch [49/120    avg_loss:0.020, val_acc:0.963]
Epoch [50/120    avg_loss:0.015, val_acc:0.980]
Epoch [51/120    avg_loss:0.007, val_acc:0.973]
Epoch [52/120    avg_loss:0.011, val_acc:0.986]
Epoch [53/120    avg_loss:0.019, val_acc:0.983]
Epoch [54/120    avg_loss:0.015, val_acc:0.985]
Epoch [55/120    avg_loss:0.016, val_acc:0.975]
Epoch [56/120    avg_loss:0.016, val_acc:0.974]
Epoch [57/120    avg_loss:0.007, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.979]
Epoch [59/120    avg_loss:0.013, val_acc:0.988]
Epoch [60/120    avg_loss:0.043, val_acc:0.965]
Epoch [61/120    avg_loss:0.028, val_acc:0.974]
Epoch [62/120    avg_loss:0.010, val_acc:0.983]
Epoch [63/120    avg_loss:0.044, val_acc:0.952]
Epoch [64/120    avg_loss:0.044, val_acc:0.960]
Epoch [65/120    avg_loss:0.044, val_acc:0.976]
Epoch [66/120    avg_loss:0.067, val_acc:0.973]
Epoch [67/120    avg_loss:0.034, val_acc:0.975]
Epoch [68/120    avg_loss:0.027, val_acc:0.966]
Epoch [69/120    avg_loss:0.014, val_acc:0.986]
Epoch [70/120    avg_loss:0.017, val_acc:0.986]
Epoch [71/120    avg_loss:0.021, val_acc:0.989]
Epoch [72/120    avg_loss:0.010, val_acc:0.988]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.039, val_acc:0.958]
Epoch [75/120    avg_loss:0.040, val_acc:0.977]
Epoch [76/120    avg_loss:0.013, val_acc:0.985]
Epoch [77/120    avg_loss:0.032, val_acc:0.942]
Epoch [78/120    avg_loss:0.013, val_acc:0.984]
Epoch [79/120    avg_loss:0.012, val_acc:0.965]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.030, val_acc:0.983]
Epoch [82/120    avg_loss:0.020, val_acc:0.973]
Epoch [83/120    avg_loss:0.012, val_acc:0.988]
Epoch [84/120    avg_loss:0.017, val_acc:0.982]
Epoch [85/120    avg_loss:0.015, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.989]
Epoch [89/120    avg_loss:0.010, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.991]
Epoch [91/120    avg_loss:0.006, val_acc:0.991]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.005, val_acc:0.991]
Epoch [94/120    avg_loss:0.004, val_acc:0.991]
Epoch [95/120    avg_loss:0.003, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.012, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.992]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.992]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.992]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.010, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.991]
Epoch [117/120    avg_loss:0.007, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     0     0     0     7     0     2]
 [    0     0 18060     0    11     0    19     0     0     0]
 [    0     0     0  2005     0     0     0     0    21    10]
 [    0    35     3     0  2916     0     0     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4868     0    10     0]
 [    0     0     0     0     0     4     2  1279     0     5]
 [    0     4     0     7    40     0     0     0  3513     7]
 [    0     0     0     2    14    27     0     0     0   876]]

Accuracy:
99.40230882317499

F1 scores:
[       nan 0.99627734 0.99908721 0.99012346 0.97967411 0.98826202
 0.99682605 0.99301242 0.9849993  0.96316658]

Kappa:
0.9920819016309705
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd11e571780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.280, val_acc:0.634]
Epoch [2/120    avg_loss:0.679, val_acc:0.744]
Epoch [3/120    avg_loss:0.576, val_acc:0.801]
Epoch [4/120    avg_loss:0.425, val_acc:0.856]
Epoch [5/120    avg_loss:0.379, val_acc:0.813]
Epoch [6/120    avg_loss:0.298, val_acc:0.922]
Epoch [7/120    avg_loss:0.267, val_acc:0.904]
Epoch [8/120    avg_loss:0.263, val_acc:0.920]
Epoch [9/120    avg_loss:0.237, val_acc:0.901]
Epoch [10/120    avg_loss:0.237, val_acc:0.910]
Epoch [11/120    avg_loss:0.184, val_acc:0.914]
Epoch [12/120    avg_loss:0.119, val_acc:0.940]
Epoch [13/120    avg_loss:0.135, val_acc:0.945]
Epoch [14/120    avg_loss:0.126, val_acc:0.964]
Epoch [15/120    avg_loss:0.089, val_acc:0.876]
Epoch [16/120    avg_loss:0.114, val_acc:0.954]
Epoch [17/120    avg_loss:0.116, val_acc:0.960]
Epoch [18/120    avg_loss:0.075, val_acc:0.953]
Epoch [19/120    avg_loss:0.090, val_acc:0.952]
Epoch [20/120    avg_loss:0.057, val_acc:0.964]
Epoch [21/120    avg_loss:0.071, val_acc:0.960]
Epoch [22/120    avg_loss:0.097, val_acc:0.971]
Epoch [23/120    avg_loss:0.070, val_acc:0.948]
Epoch [24/120    avg_loss:0.064, val_acc:0.967]
Epoch [25/120    avg_loss:0.057, val_acc:0.940]
Epoch [26/120    avg_loss:0.060, val_acc:0.980]
Epoch [27/120    avg_loss:0.038, val_acc:0.968]
Epoch [28/120    avg_loss:0.055, val_acc:0.970]
Epoch [29/120    avg_loss:0.079, val_acc:0.980]
Epoch [30/120    avg_loss:0.051, val_acc:0.970]
Epoch [31/120    avg_loss:0.031, val_acc:0.979]
Epoch [32/120    avg_loss:0.032, val_acc:0.973]
Epoch [33/120    avg_loss:0.033, val_acc:0.967]
Epoch [34/120    avg_loss:0.040, val_acc:0.974]
Epoch [35/120    avg_loss:0.043, val_acc:0.953]
Epoch [36/120    avg_loss:0.033, val_acc:0.979]
Epoch [37/120    avg_loss:0.027, val_acc:0.983]
Epoch [38/120    avg_loss:0.034, val_acc:0.972]
Epoch [39/120    avg_loss:0.036, val_acc:0.971]
Epoch [40/120    avg_loss:0.020, val_acc:0.980]
Epoch [41/120    avg_loss:0.056, val_acc:0.964]
Epoch [42/120    avg_loss:0.038, val_acc:0.977]
Epoch [43/120    avg_loss:0.045, val_acc:0.964]
Epoch [44/120    avg_loss:0.044, val_acc:0.980]
Epoch [45/120    avg_loss:0.018, val_acc:0.984]
Epoch [46/120    avg_loss:0.023, val_acc:0.978]
Epoch [47/120    avg_loss:0.021, val_acc:0.983]
Epoch [48/120    avg_loss:0.030, val_acc:0.977]
Epoch [49/120    avg_loss:0.028, val_acc:0.989]
Epoch [50/120    avg_loss:0.021, val_acc:0.985]
Epoch [51/120    avg_loss:0.027, val_acc:0.978]
Epoch [52/120    avg_loss:0.027, val_acc:0.968]
Epoch [53/120    avg_loss:0.035, val_acc:0.985]
Epoch [54/120    avg_loss:0.020, val_acc:0.981]
Epoch [55/120    avg_loss:0.011, val_acc:0.987]
Epoch [56/120    avg_loss:0.014, val_acc:0.972]
Epoch [57/120    avg_loss:0.016, val_acc:0.973]
Epoch [58/120    avg_loss:0.016, val_acc:0.985]
Epoch [59/120    avg_loss:0.017, val_acc:0.984]
Epoch [60/120    avg_loss:0.022, val_acc:0.982]
Epoch [61/120    avg_loss:0.079, val_acc:0.967]
Epoch [62/120    avg_loss:0.053, val_acc:0.981]
Epoch [63/120    avg_loss:0.032, val_acc:0.982]
Epoch [64/120    avg_loss:0.019, val_acc:0.983]
Epoch [65/120    avg_loss:0.015, val_acc:0.985]
Epoch [66/120    avg_loss:0.022, val_acc:0.984]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.982]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.010, val_acc:0.982]
Epoch [72/120    avg_loss:0.013, val_acc:0.982]
Epoch [73/120    avg_loss:0.015, val_acc:0.983]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.006, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.012, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.013, val_acc:0.983]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.013, val_acc:0.984]
Epoch [87/120    avg_loss:0.014, val_acc:0.983]
Epoch [88/120    avg_loss:0.012, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.983]
Epoch [90/120    avg_loss:0.012, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.012, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.013, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.014, val_acc:0.983]
Epoch [99/120    avg_loss:0.015, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.010, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.014, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.010, val_acc:0.983]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     2     0     9     0     0    20]
 [    0     0 17959     0    50     0    80     0     0     1]
 [    0     0     0  2000     0     0     0     0    23    13]
 [    0    21     9     0  2921     0     0     0    19     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4868     0     0    10]
 [    0     0     0     0     0     0     4  1284     0     2]
 [    0     0     0     0    46     0     0     0  3510    15]
 [    0     0     0     0    15    41     0     0     0   863]]

Accuracy:
99.07936278408407

F1 scores:
[       nan 0.99595457 0.99611737 0.99108028 0.97269397 0.98453414
 0.98953146 0.997669   0.9855398  0.93550136]

Kappa:
0.9878176067429298
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efcd0941898>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.299, val_acc:0.624]
Epoch [2/120    avg_loss:0.780, val_acc:0.609]
Epoch [3/120    avg_loss:0.650, val_acc:0.795]
Epoch [4/120    avg_loss:0.435, val_acc:0.807]
Epoch [5/120    avg_loss:0.451, val_acc:0.836]
Epoch [6/120    avg_loss:0.335, val_acc:0.878]
Epoch [7/120    avg_loss:0.239, val_acc:0.932]
Epoch [8/120    avg_loss:0.220, val_acc:0.924]
Epoch [9/120    avg_loss:0.200, val_acc:0.920]
Epoch [10/120    avg_loss:0.185, val_acc:0.911]
Epoch [11/120    avg_loss:0.147, val_acc:0.946]
Epoch [12/120    avg_loss:0.118, val_acc:0.957]
Epoch [13/120    avg_loss:0.096, val_acc:0.960]
Epoch [14/120    avg_loss:0.138, val_acc:0.949]
Epoch [15/120    avg_loss:0.223, val_acc:0.931]
Epoch [16/120    avg_loss:0.113, val_acc:0.930]
Epoch [17/120    avg_loss:0.096, val_acc:0.957]
Epoch [18/120    avg_loss:0.079, val_acc:0.946]
Epoch [19/120    avg_loss:0.068, val_acc:0.966]
Epoch [20/120    avg_loss:0.069, val_acc:0.956]
Epoch [21/120    avg_loss:0.115, val_acc:0.943]
Epoch [22/120    avg_loss:0.095, val_acc:0.947]
Epoch [23/120    avg_loss:0.129, val_acc:0.949]
Epoch [24/120    avg_loss:0.098, val_acc:0.967]
Epoch [25/120    avg_loss:0.052, val_acc:0.957]
Epoch [26/120    avg_loss:0.038, val_acc:0.971]
Epoch [27/120    avg_loss:0.040, val_acc:0.973]
Epoch [28/120    avg_loss:0.052, val_acc:0.955]
Epoch [29/120    avg_loss:0.031, val_acc:0.975]
Epoch [30/120    avg_loss:0.021, val_acc:0.967]
Epoch [31/120    avg_loss:0.027, val_acc:0.979]
Epoch [32/120    avg_loss:0.037, val_acc:0.976]
Epoch [33/120    avg_loss:0.028, val_acc:0.980]
Epoch [34/120    avg_loss:0.036, val_acc:0.970]
Epoch [35/120    avg_loss:0.018, val_acc:0.969]
Epoch [36/120    avg_loss:0.017, val_acc:0.971]
Epoch [37/120    avg_loss:0.026, val_acc:0.969]
Epoch [38/120    avg_loss:0.016, val_acc:0.977]
Epoch [39/120    avg_loss:0.018, val_acc:0.985]
Epoch [40/120    avg_loss:0.015, val_acc:0.973]
Epoch [41/120    avg_loss:0.018, val_acc:0.978]
Epoch [42/120    avg_loss:0.018, val_acc:0.980]
Epoch [43/120    avg_loss:0.015, val_acc:0.981]
Epoch [44/120    avg_loss:0.011, val_acc:0.985]
Epoch [45/120    avg_loss:0.008, val_acc:0.986]
Epoch [46/120    avg_loss:0.027, val_acc:0.979]
Epoch [47/120    avg_loss:0.047, val_acc:0.954]
Epoch [48/120    avg_loss:0.031, val_acc:0.974]
Epoch [49/120    avg_loss:0.025, val_acc:0.978]
Epoch [50/120    avg_loss:0.017, val_acc:0.970]
Epoch [51/120    avg_loss:0.010, val_acc:0.982]
Epoch [52/120    avg_loss:0.009, val_acc:0.981]
Epoch [53/120    avg_loss:0.064, val_acc:0.967]
Epoch [54/120    avg_loss:0.040, val_acc:0.977]
Epoch [55/120    avg_loss:0.016, val_acc:0.985]
Epoch [56/120    avg_loss:0.017, val_acc:0.979]
Epoch [57/120    avg_loss:0.022, val_acc:0.976]
Epoch [58/120    avg_loss:0.015, val_acc:0.980]
Epoch [59/120    avg_loss:0.008, val_acc:0.983]
Epoch [60/120    avg_loss:0.014, val_acc:0.983]
Epoch [61/120    avg_loss:0.007, val_acc:0.983]
Epoch [62/120    avg_loss:0.010, val_acc:0.982]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.007, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.985]
Epoch [66/120    avg_loss:0.014, val_acc:0.981]
Epoch [67/120    avg_loss:0.010, val_acc:0.984]
Epoch [68/120    avg_loss:0.007, val_acc:0.985]
Epoch [69/120    avg_loss:0.005, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.983]
Epoch [71/120    avg_loss:0.006, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.018, val_acc:0.985]
Epoch [74/120    avg_loss:0.005, val_acc:0.985]
Epoch [75/120    avg_loss:0.005, val_acc:0.985]
Epoch [76/120    avg_loss:0.005, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.005, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.013, val_acc:0.985]
Epoch [85/120    avg_loss:0.009, val_acc:0.985]
Epoch [86/120    avg_loss:0.005, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.014, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     0     2     0    18     0     0     0]
 [    0     0 17994     0    48     0    48     0     0     0]
 [    0     0     0  2025     0     0     0     0     8     3]
 [    0    31     0     0  2920     0     0     0    21     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4857     0     0    21]
 [    0     0     0     0     0     0     2  1284     0     4]
 [    0     0     0     4    62     0     0     0  3482    23]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.19263490227267

F1 scores:
[       nan 0.99603883 0.99733954 0.99630996 0.97042207 0.99012158
 0.99092115 0.997669   0.98333804 0.95089045]

Kappa:
0.9893135253200143
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feaec83e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.285, val_acc:0.706]
Epoch [2/120    avg_loss:0.605, val_acc:0.789]
Epoch [3/120    avg_loss:0.558, val_acc:0.729]
Epoch [4/120    avg_loss:0.485, val_acc:0.858]
Epoch [5/120    avg_loss:0.383, val_acc:0.894]
Epoch [6/120    avg_loss:0.314, val_acc:0.859]
Epoch [7/120    avg_loss:0.260, val_acc:0.898]
Epoch [8/120    avg_loss:0.190, val_acc:0.915]
Epoch [9/120    avg_loss:0.209, val_acc:0.880]
Epoch [10/120    avg_loss:0.212, val_acc:0.902]
Epoch [11/120    avg_loss:0.160, val_acc:0.902]
Epoch [12/120    avg_loss:0.164, val_acc:0.924]
Epoch [13/120    avg_loss:0.123, val_acc:0.949]
Epoch [14/120    avg_loss:0.147, val_acc:0.895]
Epoch [15/120    avg_loss:0.141, val_acc:0.866]
Epoch [16/120    avg_loss:0.088, val_acc:0.956]
Epoch [17/120    avg_loss:0.126, val_acc:0.952]
Epoch [18/120    avg_loss:0.104, val_acc:0.963]
Epoch [19/120    avg_loss:0.100, val_acc:0.952]
Epoch [20/120    avg_loss:0.112, val_acc:0.903]
Epoch [21/120    avg_loss:0.121, val_acc:0.951]
Epoch [22/120    avg_loss:0.094, val_acc:0.955]
Epoch [23/120    avg_loss:0.064, val_acc:0.962]
Epoch [24/120    avg_loss:0.051, val_acc:0.937]
Epoch [25/120    avg_loss:0.049, val_acc:0.955]
Epoch [26/120    avg_loss:0.084, val_acc:0.962]
Epoch [27/120    avg_loss:0.055, val_acc:0.973]
Epoch [28/120    avg_loss:0.045, val_acc:0.965]
Epoch [29/120    avg_loss:0.035, val_acc:0.974]
Epoch [30/120    avg_loss:0.034, val_acc:0.978]
Epoch [31/120    avg_loss:0.031, val_acc:0.977]
Epoch [32/120    avg_loss:0.049, val_acc:0.967]
Epoch [33/120    avg_loss:0.047, val_acc:0.974]
Epoch [34/120    avg_loss:0.051, val_acc:0.974]
Epoch [35/120    avg_loss:0.024, val_acc:0.980]
Epoch [36/120    avg_loss:0.015, val_acc:0.978]
Epoch [37/120    avg_loss:0.021, val_acc:0.973]
Epoch [38/120    avg_loss:0.020, val_acc:0.980]
Epoch [39/120    avg_loss:0.025, val_acc:0.973]
Epoch [40/120    avg_loss:0.038, val_acc:0.979]
Epoch [41/120    avg_loss:0.027, val_acc:0.973]
Epoch [42/120    avg_loss:0.016, val_acc:0.979]
Epoch [43/120    avg_loss:0.032, val_acc:0.973]
Epoch [44/120    avg_loss:0.046, val_acc:0.985]
Epoch [45/120    avg_loss:0.013, val_acc:0.985]
Epoch [46/120    avg_loss:0.020, val_acc:0.961]
Epoch [47/120    avg_loss:0.040, val_acc:0.979]
Epoch [48/120    avg_loss:0.063, val_acc:0.978]
Epoch [49/120    avg_loss:0.069, val_acc:0.957]
Epoch [50/120    avg_loss:0.048, val_acc:0.976]
Epoch [51/120    avg_loss:0.024, val_acc:0.981]
Epoch [52/120    avg_loss:0.015, val_acc:0.984]
Epoch [53/120    avg_loss:0.016, val_acc:0.985]
Epoch [54/120    avg_loss:0.024, val_acc:0.976]
Epoch [55/120    avg_loss:0.023, val_acc:0.945]
Epoch [56/120    avg_loss:0.017, val_acc:0.944]
Epoch [57/120    avg_loss:0.029, val_acc:0.985]
Epoch [58/120    avg_loss:0.017, val_acc:0.985]
Epoch [59/120    avg_loss:0.013, val_acc:0.988]
Epoch [60/120    avg_loss:0.022, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.010, val_acc:0.988]
Epoch [64/120    avg_loss:0.012, val_acc:0.987]
Epoch [65/120    avg_loss:0.011, val_acc:0.987]
Epoch [66/120    avg_loss:0.005, val_acc:0.989]
Epoch [67/120    avg_loss:0.009, val_acc:0.989]
Epoch [68/120    avg_loss:0.009, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.989]
Epoch [70/120    avg_loss:0.010, val_acc:0.988]
Epoch [71/120    avg_loss:0.007, val_acc:0.989]
Epoch [72/120    avg_loss:0.006, val_acc:0.989]
Epoch [73/120    avg_loss:0.011, val_acc:0.987]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.012, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.004, val_acc:0.988]
Epoch [79/120    avg_loss:0.015, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.015, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.005, val_acc:0.989]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.008, val_acc:0.989]
Epoch [93/120    avg_loss:0.015, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.008, val_acc:0.991]
Epoch [99/120    avg_loss:0.013, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.015, val_acc:0.989]
Epoch [106/120    avg_loss:0.011, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.009, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     9     0     6    12     0     3]
 [    0     0 18026     0    39     0    19     0     0     6]
 [    0     0     0  2010     0     0     0     0    16    10]
 [    0    20    10     0  2904     0     0     0    32     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4854     0    10    14]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     2     0     0    33     0     0     0  3525    11]
 [    0     0     0     8    14    28     0     0     0   869]]

Accuracy:
99.2504759839009

F1 scores:
[       nan 0.9959552  0.99795161 0.99161322 0.97270139 0.9893859
 0.99497796 0.99420626 0.98546268 0.94405215]

Kappa:
0.9900751224359788
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4afe527b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.357, val_acc:0.586]
Epoch [2/120    avg_loss:0.673, val_acc:0.699]
Epoch [3/120    avg_loss:0.575, val_acc:0.817]
Epoch [4/120    avg_loss:0.446, val_acc:0.753]
Epoch [5/120    avg_loss:0.292, val_acc:0.872]
Epoch [6/120    avg_loss:0.384, val_acc:0.898]
Epoch [7/120    avg_loss:0.329, val_acc:0.786]
Epoch [8/120    avg_loss:0.295, val_acc:0.860]
Epoch [9/120    avg_loss:0.227, val_acc:0.904]
Epoch [10/120    avg_loss:0.171, val_acc:0.890]
Epoch [11/120    avg_loss:0.162, val_acc:0.941]
Epoch [12/120    avg_loss:0.151, val_acc:0.908]
Epoch [13/120    avg_loss:0.142, val_acc:0.887]
Epoch [14/120    avg_loss:0.111, val_acc:0.956]
Epoch [15/120    avg_loss:0.087, val_acc:0.957]
Epoch [16/120    avg_loss:0.114, val_acc:0.967]
Epoch [17/120    avg_loss:0.086, val_acc:0.944]
Epoch [18/120    avg_loss:0.061, val_acc:0.954]
Epoch [19/120    avg_loss:0.070, val_acc:0.949]
Epoch [20/120    avg_loss:0.057, val_acc:0.971]
Epoch [21/120    avg_loss:0.147, val_acc:0.943]
Epoch [22/120    avg_loss:0.153, val_acc:0.949]
Epoch [23/120    avg_loss:0.091, val_acc:0.928]
Epoch [24/120    avg_loss:0.090, val_acc:0.950]
Epoch [25/120    avg_loss:0.057, val_acc:0.974]
Epoch [26/120    avg_loss:0.052, val_acc:0.953]
Epoch [27/120    avg_loss:0.073, val_acc:0.963]
Epoch [28/120    avg_loss:0.040, val_acc:0.964]
Epoch [29/120    avg_loss:0.034, val_acc:0.974]
Epoch [30/120    avg_loss:0.036, val_acc:0.971]
Epoch [31/120    avg_loss:0.022, val_acc:0.986]
Epoch [32/120    avg_loss:0.022, val_acc:0.983]
Epoch [33/120    avg_loss:0.038, val_acc:0.977]
Epoch [34/120    avg_loss:0.026, val_acc:0.963]
Epoch [35/120    avg_loss:0.016, val_acc:0.988]
Epoch [36/120    avg_loss:0.022, val_acc:0.978]
Epoch [37/120    avg_loss:0.027, val_acc:0.974]
Epoch [38/120    avg_loss:0.084, val_acc:0.942]
Epoch [39/120    avg_loss:0.073, val_acc:0.974]
Epoch [40/120    avg_loss:0.053, val_acc:0.953]
Epoch [41/120    avg_loss:0.045, val_acc:0.971]
Epoch [42/120    avg_loss:0.031, val_acc:0.983]
Epoch [43/120    avg_loss:0.034, val_acc:0.962]
Epoch [44/120    avg_loss:0.019, val_acc:0.985]
Epoch [45/120    avg_loss:0.014, val_acc:0.985]
Epoch [46/120    avg_loss:0.023, val_acc:0.932]
Epoch [47/120    avg_loss:0.038, val_acc:0.984]
Epoch [48/120    avg_loss:0.024, val_acc:0.981]
Epoch [49/120    avg_loss:0.013, val_acc:0.983]
Epoch [50/120    avg_loss:0.014, val_acc:0.985]
Epoch [51/120    avg_loss:0.010, val_acc:0.987]
Epoch [52/120    avg_loss:0.010, val_acc:0.986]
Epoch [53/120    avg_loss:0.008, val_acc:0.986]
Epoch [54/120    avg_loss:0.016, val_acc:0.986]
Epoch [55/120    avg_loss:0.009, val_acc:0.987]
Epoch [56/120    avg_loss:0.011, val_acc:0.987]
Epoch [57/120    avg_loss:0.009, val_acc:0.988]
Epoch [58/120    avg_loss:0.013, val_acc:0.988]
Epoch [59/120    avg_loss:0.017, val_acc:0.987]
Epoch [60/120    avg_loss:0.007, val_acc:0.985]
Epoch [61/120    avg_loss:0.012, val_acc:0.989]
Epoch [62/120    avg_loss:0.009, val_acc:0.989]
Epoch [63/120    avg_loss:0.017, val_acc:0.987]
Epoch [64/120    avg_loss:0.009, val_acc:0.989]
Epoch [65/120    avg_loss:0.009, val_acc:0.991]
Epoch [66/120    avg_loss:0.011, val_acc:0.991]
Epoch [67/120    avg_loss:0.014, val_acc:0.991]
Epoch [68/120    avg_loss:0.011, val_acc:0.992]
Epoch [69/120    avg_loss:0.006, val_acc:0.992]
Epoch [70/120    avg_loss:0.016, val_acc:0.991]
Epoch [71/120    avg_loss:0.010, val_acc:0.991]
Epoch [72/120    avg_loss:0.005, val_acc:0.991]
Epoch [73/120    avg_loss:0.008, val_acc:0.991]
Epoch [74/120    avg_loss:0.009, val_acc:0.991]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.008, val_acc:0.991]
Epoch [77/120    avg_loss:0.007, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.992]
Epoch [79/120    avg_loss:0.005, val_acc:0.992]
Epoch [80/120    avg_loss:0.006, val_acc:0.992]
Epoch [81/120    avg_loss:0.008, val_acc:0.991]
Epoch [82/120    avg_loss:0.008, val_acc:0.992]
Epoch [83/120    avg_loss:0.007, val_acc:0.991]
Epoch [84/120    avg_loss:0.011, val_acc:0.991]
Epoch [85/120    avg_loss:0.012, val_acc:0.993]
Epoch [86/120    avg_loss:0.008, val_acc:0.993]
Epoch [87/120    avg_loss:0.005, val_acc:0.993]
Epoch [88/120    avg_loss:0.007, val_acc:0.994]
Epoch [89/120    avg_loss:0.010, val_acc:0.993]
Epoch [90/120    avg_loss:0.012, val_acc:0.991]
Epoch [91/120    avg_loss:0.011, val_acc:0.992]
Epoch [92/120    avg_loss:0.010, val_acc:0.991]
Epoch [93/120    avg_loss:0.007, val_acc:0.992]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.009, val_acc:0.993]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.008, val_acc:0.991]
Epoch [98/120    avg_loss:0.014, val_acc:0.991]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.008, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.012, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.992]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.013, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.991]
Epoch [120/120    avg_loss:0.010, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     2     0     0     2     0     2]
 [    0     8 18049     0    20     0    13     0     0     0]
 [    0     0     0  2013     2     0     0     0    19     2]
 [    0    31     8     0  2918     0     3     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4842     0    28     3]
 [    0     0     0     0     0     0     4  1285     0     1]
 [    0     2     0     3    39     0     0     0  3525     2]
 [    0     0     0     8    14    32     0     0     0   865]]

Accuracy:
99.36133805702167

F1 scores:
[       nan 0.99635631 0.99850631 0.99162562 0.97804592 0.98788796
 0.99425051 0.99728366 0.98532495 0.96432553]

Kappa:
0.9915391123916554
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd3707777f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.534, val_acc:0.595]
Epoch [2/120    avg_loss:0.741, val_acc:0.641]
Epoch [3/120    avg_loss:0.547, val_acc:0.740]
Epoch [4/120    avg_loss:0.386, val_acc:0.825]
Epoch [5/120    avg_loss:0.322, val_acc:0.842]
Epoch [6/120    avg_loss:0.300, val_acc:0.901]
Epoch [7/120    avg_loss:0.282, val_acc:0.863]
Epoch [8/120    avg_loss:0.208, val_acc:0.888]
Epoch [9/120    avg_loss:0.214, val_acc:0.921]
Epoch [10/120    avg_loss:0.168, val_acc:0.929]
Epoch [11/120    avg_loss:0.136, val_acc:0.930]
Epoch [12/120    avg_loss:0.147, val_acc:0.905]
Epoch [13/120    avg_loss:0.121, val_acc:0.946]
Epoch [14/120    avg_loss:0.124, val_acc:0.940]
Epoch [15/120    avg_loss:0.090, val_acc:0.936]
Epoch [16/120    avg_loss:0.139, val_acc:0.932]
Epoch [17/120    avg_loss:0.096, val_acc:0.936]
Epoch [18/120    avg_loss:0.177, val_acc:0.899]
Epoch [19/120    avg_loss:0.102, val_acc:0.922]
Epoch [20/120    avg_loss:0.068, val_acc:0.945]
Epoch [21/120    avg_loss:0.069, val_acc:0.948]
Epoch [22/120    avg_loss:0.052, val_acc:0.953]
Epoch [23/120    avg_loss:0.064, val_acc:0.948]
Epoch [24/120    avg_loss:0.046, val_acc:0.966]
Epoch [25/120    avg_loss:0.052, val_acc:0.954]
Epoch [26/120    avg_loss:0.109, val_acc:0.916]
Epoch [27/120    avg_loss:0.084, val_acc:0.952]
Epoch [28/120    avg_loss:0.092, val_acc:0.943]
Epoch [29/120    avg_loss:0.035, val_acc:0.966]
Epoch [30/120    avg_loss:0.025, val_acc:0.953]
Epoch [31/120    avg_loss:0.043, val_acc:0.959]
Epoch [32/120    avg_loss:0.037, val_acc:0.959]
Epoch [33/120    avg_loss:0.036, val_acc:0.958]
Epoch [34/120    avg_loss:0.023, val_acc:0.972]
Epoch [35/120    avg_loss:0.021, val_acc:0.981]
Epoch [36/120    avg_loss:0.035, val_acc:0.945]
Epoch [37/120    avg_loss:0.032, val_acc:0.975]
Epoch [38/120    avg_loss:0.016, val_acc:0.963]
Epoch [39/120    avg_loss:0.017, val_acc:0.969]
Epoch [40/120    avg_loss:0.040, val_acc:0.970]
Epoch [41/120    avg_loss:0.017, val_acc:0.982]
Epoch [42/120    avg_loss:0.035, val_acc:0.954]
Epoch [43/120    avg_loss:0.019, val_acc:0.981]
Epoch [44/120    avg_loss:0.012, val_acc:0.983]
Epoch [45/120    avg_loss:0.015, val_acc:0.978]
Epoch [46/120    avg_loss:0.042, val_acc:0.977]
Epoch [47/120    avg_loss:0.028, val_acc:0.982]
Epoch [48/120    avg_loss:0.009, val_acc:0.981]
Epoch [49/120    avg_loss:0.014, val_acc:0.983]
Epoch [50/120    avg_loss:0.025, val_acc:0.972]
Epoch [51/120    avg_loss:0.029, val_acc:0.967]
Epoch [52/120    avg_loss:0.017, val_acc:0.976]
Epoch [53/120    avg_loss:0.035, val_acc:0.943]
Epoch [54/120    avg_loss:0.056, val_acc:0.963]
Epoch [55/120    avg_loss:0.035, val_acc:0.952]
Epoch [56/120    avg_loss:0.049, val_acc:0.971]
Epoch [57/120    avg_loss:0.017, val_acc:0.969]
Epoch [58/120    avg_loss:0.014, val_acc:0.973]
Epoch [59/120    avg_loss:0.018, val_acc:0.978]
Epoch [60/120    avg_loss:0.017, val_acc:0.968]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.008, val_acc:0.984]
Epoch [63/120    avg_loss:0.008, val_acc:0.986]
Epoch [64/120    avg_loss:0.013, val_acc:0.989]
Epoch [65/120    avg_loss:0.034, val_acc:0.978]
Epoch [66/120    avg_loss:0.014, val_acc:0.985]
Epoch [67/120    avg_loss:0.110, val_acc:0.937]
Epoch [68/120    avg_loss:0.139, val_acc:0.946]
Epoch [69/120    avg_loss:0.039, val_acc:0.970]
Epoch [70/120    avg_loss:0.026, val_acc:0.966]
Epoch [71/120    avg_loss:0.035, val_acc:0.978]
Epoch [72/120    avg_loss:0.022, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.977]
Epoch [74/120    avg_loss:0.009, val_acc:0.983]
Epoch [75/120    avg_loss:0.014, val_acc:0.982]
Epoch [76/120    avg_loss:0.007, val_acc:0.979]
Epoch [77/120    avg_loss:0.017, val_acc:0.986]
Epoch [78/120    avg_loss:0.011, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.013, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.011, val_acc:0.987]
Epoch [97/120    avg_loss:0.010, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.011, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.010, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     2 17924     0    93     0    64     0     6     1]
 [    0     0     0  2004     0     0     0     0    19    13]
 [    0    36     0     0  2914     0     0     0    22     0]
 [    0     0     0     0     0  1302     0     0     0     3]
 [    0     0     0     0     0     0  4861     0     0    17]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0    20    28     0     0     0  3513     8]
 [    0     0     3     6    14    30     0     0     0   866]]

Accuracy:
99.06249246860916

F1 scores:
[       nan 0.99690019 0.99530777 0.98573537 0.96794552 0.98748578
 0.99173722 0.9992242  0.98527556 0.94696555]

Kappa:
0.9875990326595538
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c8199b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.613, val_acc:0.496]
Epoch [2/120    avg_loss:0.787, val_acc:0.747]
Epoch [3/120    avg_loss:0.506, val_acc:0.704]
Epoch [4/120    avg_loss:0.467, val_acc:0.808]
Epoch [5/120    avg_loss:0.348, val_acc:0.888]
Epoch [6/120    avg_loss:0.273, val_acc:0.912]
Epoch [7/120    avg_loss:0.263, val_acc:0.904]
Epoch [8/120    avg_loss:0.216, val_acc:0.910]
Epoch [9/120    avg_loss:0.185, val_acc:0.934]
Epoch [10/120    avg_loss:0.175, val_acc:0.937]
Epoch [11/120    avg_loss:0.325, val_acc:0.882]
Epoch [12/120    avg_loss:0.197, val_acc:0.893]
Epoch [13/120    avg_loss:0.136, val_acc:0.914]
Epoch [14/120    avg_loss:0.196, val_acc:0.904]
Epoch [15/120    avg_loss:0.120, val_acc:0.940]
Epoch [16/120    avg_loss:0.104, val_acc:0.943]
Epoch [17/120    avg_loss:0.176, val_acc:0.950]
Epoch [18/120    avg_loss:0.125, val_acc:0.937]
Epoch [19/120    avg_loss:0.075, val_acc:0.951]
Epoch [20/120    avg_loss:0.058, val_acc:0.972]
Epoch [21/120    avg_loss:0.084, val_acc:0.945]
Epoch [22/120    avg_loss:0.073, val_acc:0.965]
Epoch [23/120    avg_loss:0.117, val_acc:0.958]
Epoch [24/120    avg_loss:0.047, val_acc:0.972]
Epoch [25/120    avg_loss:0.058, val_acc:0.968]
Epoch [26/120    avg_loss:0.042, val_acc:0.953]
Epoch [27/120    avg_loss:0.043, val_acc:0.974]
Epoch [28/120    avg_loss:0.033, val_acc:0.922]
Epoch [29/120    avg_loss:0.057, val_acc:0.973]
Epoch [30/120    avg_loss:0.039, val_acc:0.978]
Epoch [31/120    avg_loss:0.019, val_acc:0.982]
Epoch [32/120    avg_loss:0.033, val_acc:0.976]
Epoch [33/120    avg_loss:0.020, val_acc:0.985]
Epoch [34/120    avg_loss:0.017, val_acc:0.986]
Epoch [35/120    avg_loss:0.025, val_acc:0.977]
Epoch [36/120    avg_loss:0.025, val_acc:0.978]
Epoch [37/120    avg_loss:0.032, val_acc:0.962]
Epoch [38/120    avg_loss:0.024, val_acc:0.977]
Epoch [39/120    avg_loss:0.020, val_acc:0.984]
Epoch [40/120    avg_loss:0.018, val_acc:0.986]
Epoch [41/120    avg_loss:0.017, val_acc:0.974]
Epoch [42/120    avg_loss:0.021, val_acc:0.986]
Epoch [43/120    avg_loss:0.026, val_acc:0.977]
Epoch [44/120    avg_loss:0.016, val_acc:0.980]
Epoch [45/120    avg_loss:0.028, val_acc:0.982]
Epoch [46/120    avg_loss:0.025, val_acc:0.981]
Epoch [47/120    avg_loss:0.041, val_acc:0.947]
Epoch [48/120    avg_loss:0.154, val_acc:0.957]
Epoch [49/120    avg_loss:0.039, val_acc:0.957]
Epoch [50/120    avg_loss:0.043, val_acc:0.977]
Epoch [51/120    avg_loss:0.023, val_acc:0.978]
Epoch [52/120    avg_loss:0.014, val_acc:0.985]
Epoch [53/120    avg_loss:0.024, val_acc:0.983]
Epoch [54/120    avg_loss:0.015, val_acc:0.983]
Epoch [55/120    avg_loss:0.009, val_acc:0.986]
Epoch [56/120    avg_loss:0.011, val_acc:0.986]
Epoch [57/120    avg_loss:0.008, val_acc:0.985]
Epoch [58/120    avg_loss:0.008, val_acc:0.986]
Epoch [59/120    avg_loss:0.008, val_acc:0.986]
Epoch [60/120    avg_loss:0.011, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.006, val_acc:0.986]
Epoch [63/120    avg_loss:0.007, val_acc:0.988]
Epoch [64/120    avg_loss:0.012, val_acc:0.987]
Epoch [65/120    avg_loss:0.009, val_acc:0.988]
Epoch [66/120    avg_loss:0.014, val_acc:0.986]
Epoch [67/120    avg_loss:0.007, val_acc:0.986]
Epoch [68/120    avg_loss:0.011, val_acc:0.986]
Epoch [69/120    avg_loss:0.011, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.986]
Epoch [73/120    avg_loss:0.011, val_acc:0.988]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.005, val_acc:0.987]
Epoch [78/120    avg_loss:0.014, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.981]
Epoch [80/120    avg_loss:0.008, val_acc:0.983]
Epoch [81/120    avg_loss:0.013, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.013, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.009, val_acc:0.987]
Epoch [97/120    avg_loss:0.013, val_acc:0.987]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.012, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.013, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.012, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.010, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     0     0    14     0     0     0]
 [    0     0 18011     0    35     0    44     0     0     0]
 [    0     0     0  2029     0     0     0     0     5     2]
 [    0    29     7     0  2909     0     0     0    24     3]
 [    0     0     0     0     0  1294     0     0     0    11]
 [    0     0     0     0     0     0  4865     0     0    13]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0     4     0    20    32     0     0     0  3510     5]
 [    0     0     0     1    14    32     0     0     0   872]]

Accuracy:
99.28180656978286

F1 scores:
[       nan 0.99635178 0.99761826 0.99314733 0.97584703 0.9836564
 0.99265456 0.99883586 0.98734177 0.95457033]

Kappa:
0.9904906370445687
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdce63c16d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.312, val_acc:0.689]
Epoch [2/120    avg_loss:0.734, val_acc:0.765]
Epoch [3/120    avg_loss:0.593, val_acc:0.764]
Epoch [4/120    avg_loss:0.473, val_acc:0.839]
Epoch [5/120    avg_loss:0.413, val_acc:0.877]
Epoch [6/120    avg_loss:0.294, val_acc:0.884]
Epoch [7/120    avg_loss:0.349, val_acc:0.890]
Epoch [8/120    avg_loss:0.330, val_acc:0.876]
Epoch [9/120    avg_loss:0.199, val_acc:0.869]
Epoch [10/120    avg_loss:0.195, val_acc:0.890]
Epoch [11/120    avg_loss:0.181, val_acc:0.949]
Epoch [12/120    avg_loss:0.176, val_acc:0.937]
Epoch [13/120    avg_loss:0.151, val_acc:0.941]
Epoch [14/120    avg_loss:0.082, val_acc:0.961]
Epoch [15/120    avg_loss:0.121, val_acc:0.962]
Epoch [16/120    avg_loss:0.079, val_acc:0.960]
Epoch [17/120    avg_loss:0.130, val_acc:0.943]
Epoch [18/120    avg_loss:0.158, val_acc:0.923]
Epoch [19/120    avg_loss:0.095, val_acc:0.960]
Epoch [20/120    avg_loss:0.058, val_acc:0.976]
Epoch [21/120    avg_loss:0.048, val_acc:0.962]
Epoch [22/120    avg_loss:0.052, val_acc:0.978]
Epoch [23/120    avg_loss:0.074, val_acc:0.849]
Epoch [24/120    avg_loss:0.075, val_acc:0.966]
Epoch [25/120    avg_loss:0.053, val_acc:0.973]
Epoch [26/120    avg_loss:0.065, val_acc:0.961]
Epoch [27/120    avg_loss:0.044, val_acc:0.976]
Epoch [28/120    avg_loss:0.046, val_acc:0.912]
Epoch [29/120    avg_loss:0.026, val_acc:0.984]
Epoch [30/120    avg_loss:0.024, val_acc:0.976]
Epoch [31/120    avg_loss:0.044, val_acc:0.979]
Epoch [32/120    avg_loss:0.036, val_acc:0.982]
Epoch [33/120    avg_loss:0.026, val_acc:0.958]
Epoch [34/120    avg_loss:0.025, val_acc:0.983]
Epoch [35/120    avg_loss:0.044, val_acc:0.969]
Epoch [36/120    avg_loss:0.040, val_acc:0.975]
Epoch [37/120    avg_loss:0.021, val_acc:0.985]
Epoch [38/120    avg_loss:0.014, val_acc:0.985]
Epoch [39/120    avg_loss:0.012, val_acc:0.983]
Epoch [40/120    avg_loss:0.023, val_acc:0.987]
Epoch [41/120    avg_loss:0.029, val_acc:0.979]
Epoch [42/120    avg_loss:0.014, val_acc:0.988]
Epoch [43/120    avg_loss:0.013, val_acc:0.987]
Epoch [44/120    avg_loss:0.011, val_acc:0.990]
Epoch [45/120    avg_loss:0.013, val_acc:0.989]
Epoch [46/120    avg_loss:0.011, val_acc:0.988]
Epoch [47/120    avg_loss:0.010, val_acc:0.989]
Epoch [48/120    avg_loss:0.008, val_acc:0.986]
Epoch [49/120    avg_loss:0.013, val_acc:0.984]
Epoch [50/120    avg_loss:0.013, val_acc:0.979]
Epoch [51/120    avg_loss:0.016, val_acc:0.976]
Epoch [52/120    avg_loss:0.023, val_acc:0.955]
Epoch [53/120    avg_loss:0.024, val_acc:0.986]
Epoch [54/120    avg_loss:0.029, val_acc:0.952]
Epoch [55/120    avg_loss:0.048, val_acc:0.985]
Epoch [56/120    avg_loss:0.024, val_acc:0.985]
Epoch [57/120    avg_loss:0.011, val_acc:0.985]
Epoch [58/120    avg_loss:0.014, val_acc:0.987]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.007, val_acc:0.990]
Epoch [61/120    avg_loss:0.010, val_acc:0.991]
Epoch [62/120    avg_loss:0.017, val_acc:0.987]
Epoch [63/120    avg_loss:0.010, val_acc:0.989]
Epoch [64/120    avg_loss:0.009, val_acc:0.990]
Epoch [65/120    avg_loss:0.009, val_acc:0.991]
Epoch [66/120    avg_loss:0.011, val_acc:0.989]
Epoch [67/120    avg_loss:0.011, val_acc:0.985]
Epoch [68/120    avg_loss:0.009, val_acc:0.985]
Epoch [69/120    avg_loss:0.008, val_acc:0.990]
Epoch [70/120    avg_loss:0.009, val_acc:0.991]
Epoch [71/120    avg_loss:0.008, val_acc:0.992]
Epoch [72/120    avg_loss:0.010, val_acc:0.992]
Epoch [73/120    avg_loss:0.008, val_acc:0.991]
Epoch [74/120    avg_loss:0.014, val_acc:0.991]
Epoch [75/120    avg_loss:0.007, val_acc:0.991]
Epoch [76/120    avg_loss:0.006, val_acc:0.992]
Epoch [77/120    avg_loss:0.014, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.993]
Epoch [79/120    avg_loss:0.013, val_acc:0.992]
Epoch [80/120    avg_loss:0.026, val_acc:0.991]
Epoch [81/120    avg_loss:0.007, val_acc:0.989]
Epoch [82/120    avg_loss:0.009, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.992]
Epoch [84/120    avg_loss:0.005, val_acc:0.992]
Epoch [85/120    avg_loss:0.008, val_acc:0.993]
Epoch [86/120    avg_loss:0.009, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.991]
Epoch [88/120    avg_loss:0.009, val_acc:0.991]
Epoch [89/120    avg_loss:0.009, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.007, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.008, val_acc:0.993]
Epoch [97/120    avg_loss:0.005, val_acc:0.993]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.008, val_acc:0.993]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.993]
Epoch [103/120    avg_loss:0.010, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.992]
Epoch [105/120    avg_loss:0.004, val_acc:0.993]
Epoch [106/120    avg_loss:0.007, val_acc:0.993]
Epoch [107/120    avg_loss:0.004, val_acc:0.993]
Epoch [108/120    avg_loss:0.005, val_acc:0.993]
Epoch [109/120    avg_loss:0.006, val_acc:0.993]
Epoch [110/120    avg_loss:0.005, val_acc:0.993]
Epoch [111/120    avg_loss:0.004, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.993]
Epoch [113/120    avg_loss:0.004, val_acc:0.993]
Epoch [114/120    avg_loss:0.006, val_acc:0.993]
Epoch [115/120    avg_loss:0.006, val_acc:0.993]
Epoch [116/120    avg_loss:0.005, val_acc:0.993]
Epoch [117/120    avg_loss:0.005, val_acc:0.993]
Epoch [118/120    avg_loss:0.006, val_acc:0.993]
Epoch [119/120    avg_loss:0.003, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     0     2     0    22     2     0     0]
 [    0     1 18033     0    23     0    31     0     0     2]
 [    0     0     0  2011     0     0     0     0    21     4]
 [    0    28     7     0  2912     0     3     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4871     0     0     7]
 [    0     0     0     0     0     0     3  1284     0     3]
 [    0     2     0     6    30     0     0     0  3527     6]
 [    0     0     0     6    14    22     0     0     0   877]]

Accuracy:
99.35651796688599

F1 scores:
[       nan 0.99557075 0.99822862 0.99088445 0.97833025 0.99164134
 0.9932708  0.99689441 0.98781683 0.96479648]

Kappa:
0.9914775201290598
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd12a037b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.312, val_acc:0.626]
Epoch [2/120    avg_loss:0.756, val_acc:0.636]
Epoch [3/120    avg_loss:0.561, val_acc:0.729]
Epoch [4/120    avg_loss:0.469, val_acc:0.846]
Epoch [5/120    avg_loss:0.511, val_acc:0.743]
Epoch [6/120    avg_loss:0.281, val_acc:0.802]
Epoch [7/120    avg_loss:0.255, val_acc:0.880]
Epoch [8/120    avg_loss:0.252, val_acc:0.900]
Epoch [9/120    avg_loss:0.234, val_acc:0.911]
Epoch [10/120    avg_loss:0.194, val_acc:0.919]
Epoch [11/120    avg_loss:0.215, val_acc:0.868]
Epoch [12/120    avg_loss:0.230, val_acc:0.902]
Epoch [13/120    avg_loss:0.188, val_acc:0.902]
Epoch [14/120    avg_loss:0.136, val_acc:0.910]
Epoch [15/120    avg_loss:0.108, val_acc:0.930]
Epoch [16/120    avg_loss:0.085, val_acc:0.938]
Epoch [17/120    avg_loss:0.086, val_acc:0.926]
Epoch [18/120    avg_loss:0.083, val_acc:0.942]
Epoch [19/120    avg_loss:0.060, val_acc:0.943]
Epoch [20/120    avg_loss:0.140, val_acc:0.891]
Epoch [21/120    avg_loss:0.125, val_acc:0.957]
Epoch [22/120    avg_loss:0.077, val_acc:0.958]
Epoch [23/120    avg_loss:0.073, val_acc:0.964]
Epoch [24/120    avg_loss:0.069, val_acc:0.971]
Epoch [25/120    avg_loss:0.093, val_acc:0.945]
Epoch [26/120    avg_loss:0.069, val_acc:0.962]
Epoch [27/120    avg_loss:0.077, val_acc:0.946]
Epoch [28/120    avg_loss:0.055, val_acc:0.955]
Epoch [29/120    avg_loss:0.042, val_acc:0.973]
Epoch [30/120    avg_loss:0.032, val_acc:0.965]
Epoch [31/120    avg_loss:0.034, val_acc:0.971]
Epoch [32/120    avg_loss:0.021, val_acc:0.980]
Epoch [33/120    avg_loss:0.027, val_acc:0.978]
Epoch [34/120    avg_loss:0.031, val_acc:0.982]
Epoch [35/120    avg_loss:0.043, val_acc:0.979]
Epoch [36/120    avg_loss:0.022, val_acc:0.971]
Epoch [37/120    avg_loss:0.023, val_acc:0.985]
Epoch [38/120    avg_loss:0.014, val_acc:0.971]
Epoch [39/120    avg_loss:0.023, val_acc:0.953]
Epoch [40/120    avg_loss:0.033, val_acc:0.964]
Epoch [41/120    avg_loss:0.014, val_acc:0.985]
Epoch [42/120    avg_loss:0.016, val_acc:0.979]
Epoch [43/120    avg_loss:0.025, val_acc:0.923]
Epoch [44/120    avg_loss:0.040, val_acc:0.976]
Epoch [45/120    avg_loss:0.021, val_acc:0.981]
Epoch [46/120    avg_loss:0.019, val_acc:0.987]
Epoch [47/120    avg_loss:0.012, val_acc:0.985]
Epoch [48/120    avg_loss:0.013, val_acc:0.968]
Epoch [49/120    avg_loss:0.015, val_acc:0.989]
Epoch [50/120    avg_loss:0.007, val_acc:0.987]
Epoch [51/120    avg_loss:0.020, val_acc:0.990]
Epoch [52/120    avg_loss:0.012, val_acc:0.990]
Epoch [53/120    avg_loss:0.017, val_acc:0.974]
Epoch [54/120    avg_loss:0.014, val_acc:0.987]
Epoch [55/120    avg_loss:0.015, val_acc:0.991]
Epoch [56/120    avg_loss:0.016, val_acc:0.986]
Epoch [57/120    avg_loss:0.012, val_acc:0.991]
Epoch [58/120    avg_loss:0.010, val_acc:0.987]
Epoch [59/120    avg_loss:0.007, val_acc:0.990]
Epoch [60/120    avg_loss:0.012, val_acc:0.991]
Epoch [61/120    avg_loss:0.014, val_acc:0.987]
Epoch [62/120    avg_loss:0.012, val_acc:0.985]
Epoch [63/120    avg_loss:0.008, val_acc:0.989]
Epoch [64/120    avg_loss:0.009, val_acc:0.984]
Epoch [65/120    avg_loss:0.019, val_acc:0.967]
Epoch [66/120    avg_loss:0.023, val_acc:0.985]
Epoch [67/120    avg_loss:0.006, val_acc:0.990]
Epoch [68/120    avg_loss:0.011, val_acc:0.984]
Epoch [69/120    avg_loss:0.019, val_acc:0.992]
Epoch [70/120    avg_loss:0.009, val_acc:0.994]
Epoch [71/120    avg_loss:0.007, val_acc:0.995]
Epoch [72/120    avg_loss:0.004, val_acc:0.995]
Epoch [73/120    avg_loss:0.009, val_acc:0.993]
Epoch [74/120    avg_loss:0.006, val_acc:0.994]
Epoch [75/120    avg_loss:0.004, val_acc:0.993]
Epoch [76/120    avg_loss:0.005, val_acc:0.994]
Epoch [77/120    avg_loss:0.009, val_acc:0.994]
Epoch [78/120    avg_loss:0.008, val_acc:0.994]
Epoch [79/120    avg_loss:0.008, val_acc:0.995]
Epoch [80/120    avg_loss:0.005, val_acc:0.993]
Epoch [81/120    avg_loss:0.005, val_acc:0.993]
Epoch [82/120    avg_loss:0.010, val_acc:0.993]
Epoch [83/120    avg_loss:0.005, val_acc:0.993]
Epoch [84/120    avg_loss:0.005, val_acc:0.993]
Epoch [85/120    avg_loss:0.003, val_acc:0.994]
Epoch [86/120    avg_loss:0.004, val_acc:0.993]
Epoch [87/120    avg_loss:0.003, val_acc:0.992]
Epoch [88/120    avg_loss:0.011, val_acc:0.992]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.004, val_acc:0.993]
Epoch [91/120    avg_loss:0.003, val_acc:0.993]
Epoch [92/120    avg_loss:0.007, val_acc:0.994]
Epoch [93/120    avg_loss:0.005, val_acc:0.994]
Epoch [94/120    avg_loss:0.005, val_acc:0.994]
Epoch [95/120    avg_loss:0.003, val_acc:0.994]
Epoch [96/120    avg_loss:0.005, val_acc:0.994]
Epoch [97/120    avg_loss:0.003, val_acc:0.994]
Epoch [98/120    avg_loss:0.008, val_acc:0.994]
Epoch [99/120    avg_loss:0.004, val_acc:0.994]
Epoch [100/120    avg_loss:0.004, val_acc:0.994]
Epoch [101/120    avg_loss:0.003, val_acc:0.994]
Epoch [102/120    avg_loss:0.019, val_acc:0.994]
Epoch [103/120    avg_loss:0.013, val_acc:0.994]
Epoch [104/120    avg_loss:0.006, val_acc:0.994]
Epoch [105/120    avg_loss:0.007, val_acc:0.994]
Epoch [106/120    avg_loss:0.007, val_acc:0.994]
Epoch [107/120    avg_loss:0.007, val_acc:0.994]
Epoch [108/120    avg_loss:0.004, val_acc:0.994]
Epoch [109/120    avg_loss:0.005, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.005, val_acc:0.994]
Epoch [112/120    avg_loss:0.007, val_acc:0.994]
Epoch [113/120    avg_loss:0.007, val_acc:0.994]
Epoch [114/120    avg_loss:0.005, val_acc:0.994]
Epoch [115/120    avg_loss:0.004, val_acc:0.994]
Epoch [116/120    avg_loss:0.004, val_acc:0.994]
Epoch [117/120    avg_loss:0.005, val_acc:0.994]
Epoch [118/120    avg_loss:0.004, val_acc:0.994]
Epoch [119/120    avg_loss:0.011, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6430     0     0     2     0     0     0     0     0]
 [    0     3 18003     0    78     0     6     0     0     0]
 [    0     0     0  2022     1     0     0     0    11     2]
 [    0    27     5     0  2921     0     0     0    19     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     3     0     0     0  4862     0    11     2]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     3     0    13    27     0     0     0  3518    10]
 [    0     0     0     9    14    21     0     0     0   875]]

Accuracy:
99.3516978767503

F1 scores:
[       nan 0.99728577 0.99736849 0.99117647 0.97123857 0.99125143
 0.99774266 1.         0.98681627 0.96685083]

Kappa:
0.9914168386615149
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff413ff17b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.260, val_acc:0.599]
Epoch [2/120    avg_loss:0.669, val_acc:0.729]
Epoch [3/120    avg_loss:0.523, val_acc:0.663]
Epoch [4/120    avg_loss:0.368, val_acc:0.787]
Epoch [5/120    avg_loss:0.387, val_acc:0.888]
Epoch [6/120    avg_loss:0.307, val_acc:0.861]
Epoch [7/120    avg_loss:0.217, val_acc:0.912]
Epoch [8/120    avg_loss:0.230, val_acc:0.888]
Epoch [9/120    avg_loss:0.189, val_acc:0.892]
Epoch [10/120    avg_loss:0.176, val_acc:0.866]
Epoch [11/120    avg_loss:0.172, val_acc:0.937]
Epoch [12/120    avg_loss:0.147, val_acc:0.892]
Epoch [13/120    avg_loss:0.135, val_acc:0.870]
Epoch [14/120    avg_loss:0.159, val_acc:0.938]
Epoch [15/120    avg_loss:0.102, val_acc:0.938]
Epoch [16/120    avg_loss:0.111, val_acc:0.938]
Epoch [17/120    avg_loss:0.103, val_acc:0.957]
Epoch [18/120    avg_loss:0.079, val_acc:0.940]
Epoch [19/120    avg_loss:0.050, val_acc:0.956]
Epoch [20/120    avg_loss:0.069, val_acc:0.962]
Epoch [21/120    avg_loss:0.059, val_acc:0.946]
Epoch [22/120    avg_loss:0.063, val_acc:0.954]
Epoch [23/120    avg_loss:0.050, val_acc:0.953]
Epoch [24/120    avg_loss:0.180, val_acc:0.917]
Epoch [25/120    avg_loss:0.192, val_acc:0.877]
Epoch [26/120    avg_loss:0.125, val_acc:0.948]
Epoch [27/120    avg_loss:0.066, val_acc:0.963]
Epoch [28/120    avg_loss:0.064, val_acc:0.931]
Epoch [29/120    avg_loss:0.051, val_acc:0.973]
Epoch [30/120    avg_loss:0.049, val_acc:0.943]
Epoch [31/120    avg_loss:0.039, val_acc:0.974]
Epoch [32/120    avg_loss:0.031, val_acc:0.977]
Epoch [33/120    avg_loss:0.037, val_acc:0.973]
Epoch [34/120    avg_loss:0.031, val_acc:0.970]
Epoch [35/120    avg_loss:0.029, val_acc:0.975]
Epoch [36/120    avg_loss:0.032, val_acc:0.973]
Epoch [37/120    avg_loss:0.031, val_acc:0.975]
Epoch [38/120    avg_loss:0.017, val_acc:0.982]
Epoch [39/120    avg_loss:0.030, val_acc:0.973]
Epoch [40/120    avg_loss:0.020, val_acc:0.971]
Epoch [41/120    avg_loss:0.060, val_acc:0.965]
Epoch [42/120    avg_loss:0.028, val_acc:0.941]
Epoch [43/120    avg_loss:0.064, val_acc:0.964]
Epoch [44/120    avg_loss:0.027, val_acc:0.982]
Epoch [45/120    avg_loss:0.032, val_acc:0.966]
Epoch [46/120    avg_loss:0.022, val_acc:0.967]
Epoch [47/120    avg_loss:0.011, val_acc:0.980]
Epoch [48/120    avg_loss:0.029, val_acc:0.969]
Epoch [49/120    avg_loss:0.018, val_acc:0.976]
Epoch [50/120    avg_loss:0.029, val_acc:0.969]
Epoch [51/120    avg_loss:0.048, val_acc:0.895]
Epoch [52/120    avg_loss:0.031, val_acc:0.973]
Epoch [53/120    avg_loss:0.010, val_acc:0.988]
Epoch [54/120    avg_loss:0.009, val_acc:0.987]
Epoch [55/120    avg_loss:0.011, val_acc:0.976]
Epoch [56/120    avg_loss:0.065, val_acc:0.970]
Epoch [57/120    avg_loss:0.032, val_acc:0.944]
Epoch [58/120    avg_loss:0.020, val_acc:0.983]
Epoch [59/120    avg_loss:0.019, val_acc:0.981]
Epoch [60/120    avg_loss:0.021, val_acc:0.985]
Epoch [61/120    avg_loss:0.017, val_acc:0.969]
Epoch [62/120    avg_loss:0.026, val_acc:0.907]
Epoch [63/120    avg_loss:0.018, val_acc:0.989]
Epoch [64/120    avg_loss:0.016, val_acc:0.982]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.033, val_acc:0.979]
Epoch [67/120    avg_loss:0.011, val_acc:0.988]
Epoch [68/120    avg_loss:0.007, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.988]
Epoch [70/120    avg_loss:0.009, val_acc:0.991]
Epoch [71/120    avg_loss:0.006, val_acc:0.986]
Epoch [72/120    avg_loss:0.011, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.982]
Epoch [74/120    avg_loss:0.018, val_acc:0.987]
Epoch [75/120    avg_loss:0.006, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.004, val_acc:0.990]
Epoch [80/120    avg_loss:0.010, val_acc:0.987]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.014, val_acc:0.979]
Epoch [83/120    avg_loss:0.007, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.007, val_acc:0.991]
Epoch [86/120    avg_loss:0.014, val_acc:0.991]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.010, val_acc:0.991]
Epoch [89/120    avg_loss:0.005, val_acc:0.991]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.003, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.991]
Epoch [95/120    avg_loss:0.009, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.003, val_acc:0.992]
Epoch [99/120    avg_loss:0.005, val_acc:0.992]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.991]
Epoch [104/120    avg_loss:0.010, val_acc:0.991]
Epoch [105/120    avg_loss:0.002, val_acc:0.991]
Epoch [106/120    avg_loss:0.003, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.003, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.008, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.002, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     4     0     0     5     0     0]
 [    0     9 18063     0    15     0     3     0     0     0]
 [    0     0     0  2028     0     0     0     0     2     6]
 [    0    36     9     0  2900     0     0     0    27     0]
 [    0     0     0     0     0  1298     0     0     7     0]
 [    0     0     2     0     0     0  4860     0     5    11]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     5     0    17    23     0     0     0  3525     1]
 [    0     0     0     2    14    23     0     0     0   880]]

Accuracy:
99.45532981466754

F1 scores:
[       nan 0.99542813 0.99894923 0.99338722 0.97840756 0.98857578
 0.99784416 0.99806576 0.98781    0.96862961]

Kappa:
0.9927835346072832
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ac3a677f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.366, val_acc:0.559]
Epoch [2/120    avg_loss:0.648, val_acc:0.615]
Epoch [3/120    avg_loss:0.483, val_acc:0.826]
Epoch [4/120    avg_loss:0.376, val_acc:0.866]
Epoch [5/120    avg_loss:0.377, val_acc:0.872]
Epoch [6/120    avg_loss:0.274, val_acc:0.893]
Epoch [7/120    avg_loss:0.246, val_acc:0.836]
Epoch [8/120    avg_loss:0.236, val_acc:0.922]
Epoch [9/120    avg_loss:0.226, val_acc:0.862]
Epoch [10/120    avg_loss:0.186, val_acc:0.924]
Epoch [11/120    avg_loss:0.168, val_acc:0.903]
Epoch [12/120    avg_loss:0.124, val_acc:0.920]
Epoch [13/120    avg_loss:0.115, val_acc:0.949]
Epoch [14/120    avg_loss:0.115, val_acc:0.944]
Epoch [15/120    avg_loss:0.125, val_acc:0.949]
Epoch [16/120    avg_loss:0.115, val_acc:0.951]
Epoch [17/120    avg_loss:0.181, val_acc:0.922]
Epoch [18/120    avg_loss:0.102, val_acc:0.917]
Epoch [19/120    avg_loss:0.112, val_acc:0.948]
Epoch [20/120    avg_loss:0.183, val_acc:0.896]
Epoch [21/120    avg_loss:0.203, val_acc:0.943]
Epoch [22/120    avg_loss:0.112, val_acc:0.957]
Epoch [23/120    avg_loss:0.084, val_acc:0.933]
Epoch [24/120    avg_loss:0.083, val_acc:0.960]
Epoch [25/120    avg_loss:0.056, val_acc:0.970]
Epoch [26/120    avg_loss:0.037, val_acc:0.972]
Epoch [27/120    avg_loss:0.040, val_acc:0.977]
Epoch [28/120    avg_loss:0.040, val_acc:0.954]
Epoch [29/120    avg_loss:0.036, val_acc:0.973]
Epoch [30/120    avg_loss:0.027, val_acc:0.962]
Epoch [31/120    avg_loss:0.048, val_acc:0.960]
Epoch [32/120    avg_loss:0.064, val_acc:0.960]
Epoch [33/120    avg_loss:0.112, val_acc:0.957]
Epoch [34/120    avg_loss:0.055, val_acc:0.960]
Epoch [35/120    avg_loss:0.038, val_acc:0.979]
Epoch [36/120    avg_loss:0.035, val_acc:0.978]
Epoch [37/120    avg_loss:0.033, val_acc:0.967]
Epoch [38/120    avg_loss:0.039, val_acc:0.967]
Epoch [39/120    avg_loss:0.043, val_acc:0.974]
Epoch [40/120    avg_loss:0.024, val_acc:0.977]
Epoch [41/120    avg_loss:0.024, val_acc:0.975]
Epoch [42/120    avg_loss:0.013, val_acc:0.985]
Epoch [43/120    avg_loss:0.011, val_acc:0.980]
Epoch [44/120    avg_loss:0.024, val_acc:0.983]
Epoch [45/120    avg_loss:0.010, val_acc:0.983]
Epoch [46/120    avg_loss:0.009, val_acc:0.987]
Epoch [47/120    avg_loss:0.012, val_acc:0.982]
Epoch [48/120    avg_loss:0.014, val_acc:0.985]
Epoch [49/120    avg_loss:0.022, val_acc:0.971]
Epoch [50/120    avg_loss:0.011, val_acc:0.984]
Epoch [51/120    avg_loss:0.013, val_acc:0.985]
Epoch [52/120    avg_loss:0.033, val_acc:0.979]
Epoch [53/120    avg_loss:0.022, val_acc:0.985]
Epoch [54/120    avg_loss:0.030, val_acc:0.980]
Epoch [55/120    avg_loss:0.039, val_acc:0.978]
Epoch [56/120    avg_loss:0.041, val_acc:0.962]
Epoch [57/120    avg_loss:0.031, val_acc:0.983]
Epoch [58/120    avg_loss:0.015, val_acc:0.983]
Epoch [59/120    avg_loss:0.021, val_acc:0.970]
Epoch [60/120    avg_loss:0.019, val_acc:0.982]
Epoch [61/120    avg_loss:0.011, val_acc:0.982]
Epoch [62/120    avg_loss:0.008, val_acc:0.984]
Epoch [63/120    avg_loss:0.008, val_acc:0.985]
Epoch [64/120    avg_loss:0.032, val_acc:0.983]
Epoch [65/120    avg_loss:0.009, val_acc:0.985]
Epoch [66/120    avg_loss:0.013, val_acc:0.987]
Epoch [67/120    avg_loss:0.007, val_acc:0.987]
Epoch [68/120    avg_loss:0.005, val_acc:0.987]
Epoch [69/120    avg_loss:0.010, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.987]
Epoch [72/120    avg_loss:0.007, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.010, val_acc:0.986]
Epoch [75/120    avg_loss:0.005, val_acc:0.987]
Epoch [76/120    avg_loss:0.006, val_acc:0.987]
Epoch [77/120    avg_loss:0.005, val_acc:0.987]
Epoch [78/120    avg_loss:0.005, val_acc:0.987]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.020, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.005, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.989]
Epoch [91/120    avg_loss:0.009, val_acc:0.987]
Epoch [92/120    avg_loss:0.009, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.011, val_acc:0.991]
Epoch [95/120    avg_loss:0.012, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.011, val_acc:0.991]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     0     4     0     1     2     0     0]
 [    0     0 18026     0    30     0    34     0     0     0]
 [    0     0     0  2021     0     0     0     0     6     9]
 [    0    24     6     0  2923     0     0     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0    11     7]
 [    0     0     0     0     0     0     3  1285     0     2]
 [    0     2     0     0    39     0     0     0  3514    16]
 [    0     0     0     0    14    32     0     0     0   873]]

Accuracy:
99.37097823729304

F1 scores:
[       nan 0.99743848 0.99806212 0.99630269 0.97726513 0.98788796
 0.99427169 0.99728366 0.98694004 0.95618839]

Kappa:
0.9916701795713302
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faef4acf7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.376, val_acc:0.441]
Epoch [2/120    avg_loss:0.772, val_acc:0.702]
Epoch [3/120    avg_loss:0.577, val_acc:0.741]
Epoch [4/120    avg_loss:0.499, val_acc:0.876]
Epoch [5/120    avg_loss:0.368, val_acc:0.799]
Epoch [6/120    avg_loss:0.298, val_acc:0.906]
Epoch [7/120    avg_loss:0.301, val_acc:0.911]
Epoch [8/120    avg_loss:0.339, val_acc:0.871]
Epoch [9/120    avg_loss:0.262, val_acc:0.857]
Epoch [10/120    avg_loss:0.213, val_acc:0.930]
Epoch [11/120    avg_loss:0.190, val_acc:0.875]
Epoch [12/120    avg_loss:0.212, val_acc:0.922]
Epoch [13/120    avg_loss:0.248, val_acc:0.934]
Epoch [14/120    avg_loss:0.128, val_acc:0.949]
Epoch [15/120    avg_loss:0.177, val_acc:0.931]
Epoch [16/120    avg_loss:0.147, val_acc:0.955]
Epoch [17/120    avg_loss:0.108, val_acc:0.963]
Epoch [18/120    avg_loss:0.124, val_acc:0.935]
Epoch [19/120    avg_loss:0.091, val_acc:0.892]
Epoch [20/120    avg_loss:0.222, val_acc:0.952]
Epoch [21/120    avg_loss:0.099, val_acc:0.973]
Epoch [22/120    avg_loss:0.068, val_acc:0.958]
Epoch [23/120    avg_loss:0.122, val_acc:0.949]
Epoch [24/120    avg_loss:0.092, val_acc:0.958]
Epoch [25/120    avg_loss:0.058, val_acc:0.935]
Epoch [26/120    avg_loss:0.047, val_acc:0.975]
Epoch [27/120    avg_loss:0.057, val_acc:0.930]
Epoch [28/120    avg_loss:0.039, val_acc:0.971]
Epoch [29/120    avg_loss:0.047, val_acc:0.966]
Epoch [30/120    avg_loss:0.066, val_acc:0.953]
Epoch [31/120    avg_loss:0.057, val_acc:0.970]
Epoch [32/120    avg_loss:0.045, val_acc:0.970]
Epoch [33/120    avg_loss:0.028, val_acc:0.971]
Epoch [34/120    avg_loss:0.046, val_acc:0.955]
Epoch [35/120    avg_loss:0.040, val_acc:0.974]
Epoch [36/120    avg_loss:0.030, val_acc:0.983]
Epoch [37/120    avg_loss:0.030, val_acc:0.983]
Epoch [38/120    avg_loss:0.022, val_acc:0.969]
Epoch [39/120    avg_loss:0.020, val_acc:0.979]
Epoch [40/120    avg_loss:0.014, val_acc:0.980]
Epoch [41/120    avg_loss:0.021, val_acc:0.976]
Epoch [42/120    avg_loss:0.027, val_acc:0.985]
Epoch [43/120    avg_loss:0.018, val_acc:0.979]
Epoch [44/120    avg_loss:0.034, val_acc:0.973]
Epoch [45/120    avg_loss:0.016, val_acc:0.985]
Epoch [46/120    avg_loss:0.013, val_acc:0.983]
Epoch [47/120    avg_loss:0.015, val_acc:0.987]
Epoch [48/120    avg_loss:0.020, val_acc:0.978]
Epoch [49/120    avg_loss:0.028, val_acc:0.972]
Epoch [50/120    avg_loss:0.077, val_acc:0.984]
Epoch [51/120    avg_loss:0.054, val_acc:0.954]
Epoch [52/120    avg_loss:0.031, val_acc:0.987]
Epoch [53/120    avg_loss:0.011, val_acc:0.987]
Epoch [54/120    avg_loss:0.019, val_acc:0.987]
Epoch [55/120    avg_loss:0.010, val_acc:0.985]
Epoch [56/120    avg_loss:0.009, val_acc:0.981]
Epoch [57/120    avg_loss:0.014, val_acc:0.985]
Epoch [58/120    avg_loss:0.010, val_acc:0.988]
Epoch [59/120    avg_loss:0.013, val_acc:0.986]
Epoch [60/120    avg_loss:0.020, val_acc:0.980]
Epoch [61/120    avg_loss:0.025, val_acc:0.983]
Epoch [62/120    avg_loss:0.013, val_acc:0.983]
Epoch [63/120    avg_loss:0.014, val_acc:0.979]
Epoch [64/120    avg_loss:0.021, val_acc:0.988]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.006, val_acc:0.984]
Epoch [67/120    avg_loss:0.017, val_acc:0.985]
Epoch [68/120    avg_loss:0.027, val_acc:0.987]
Epoch [69/120    avg_loss:0.010, val_acc:0.979]
Epoch [70/120    avg_loss:0.011, val_acc:0.989]
Epoch [71/120    avg_loss:0.012, val_acc:0.990]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.020, val_acc:0.955]
Epoch [74/120    avg_loss:0.028, val_acc:0.982]
Epoch [75/120    avg_loss:0.009, val_acc:0.990]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.015, val_acc:0.987]
Epoch [78/120    avg_loss:0.009, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.985]
Epoch [80/120    avg_loss:0.004, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.005, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.987]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.004, val_acc:0.991]
Epoch [87/120    avg_loss:0.004, val_acc:0.989]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.991]
Epoch [94/120    avg_loss:0.003, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.002, val_acc:0.990]
Epoch [97/120    avg_loss:0.029, val_acc:0.967]
Epoch [98/120    avg_loss:0.019, val_acc:0.990]
Epoch [99/120    avg_loss:0.027, val_acc:0.979]
Epoch [100/120    avg_loss:0.015, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.993]
Epoch [102/120    avg_loss:0.011, val_acc:0.989]
Epoch [103/120    avg_loss:0.016, val_acc:0.980]
Epoch [104/120    avg_loss:0.013, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.013, val_acc:0.974]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.003, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.002, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     0 18048     0    26     0    16     0     0     0]
 [    0     0     0  2024     0     0     0     0     7     5]
 [    0    24     8     0  2916     0     0     0    24     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4871     0     0     7]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0     0    35     0     0     0  3534     0]
 [    0     0     0     5    14    27     0     0     0   873]]

Accuracy:
99.51317089629576

F1 scores:
[       nan 0.99798293 0.99861672 0.99581796 0.97803119 0.98976109
 0.99764465 0.9992242  0.99047085 0.96677741]

Kappa:
0.9935511058171212
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1802f5780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.529, val_acc:0.559]
Epoch [2/120    avg_loss:0.756, val_acc:0.699]
Epoch [3/120    avg_loss:0.629, val_acc:0.783]
Epoch [4/120    avg_loss:0.460, val_acc:0.753]
Epoch [5/120    avg_loss:0.386, val_acc:0.800]
Epoch [6/120    avg_loss:0.298, val_acc:0.908]
Epoch [7/120    avg_loss:0.353, val_acc:0.857]
Epoch [8/120    avg_loss:0.340, val_acc:0.925]
Epoch [9/120    avg_loss:0.186, val_acc:0.925]
Epoch [10/120    avg_loss:0.164, val_acc:0.931]
Epoch [11/120    avg_loss:0.147, val_acc:0.926]
Epoch [12/120    avg_loss:0.136, val_acc:0.944]
Epoch [13/120    avg_loss:0.154, val_acc:0.918]
Epoch [14/120    avg_loss:0.110, val_acc:0.949]
Epoch [15/120    avg_loss:0.167, val_acc:0.918]
Epoch [16/120    avg_loss:0.155, val_acc:0.898]
Epoch [17/120    avg_loss:0.120, val_acc:0.963]
Epoch [18/120    avg_loss:0.092, val_acc:0.958]
Epoch [19/120    avg_loss:0.097, val_acc:0.958]
Epoch [20/120    avg_loss:0.074, val_acc:0.957]
Epoch [21/120    avg_loss:0.066, val_acc:0.980]
Epoch [22/120    avg_loss:0.070, val_acc:0.953]
Epoch [23/120    avg_loss:0.092, val_acc:0.955]
Epoch [24/120    avg_loss:0.201, val_acc:0.957]
Epoch [25/120    avg_loss:0.097, val_acc:0.946]
Epoch [26/120    avg_loss:0.057, val_acc:0.970]
Epoch [27/120    avg_loss:0.052, val_acc:0.948]
Epoch [28/120    avg_loss:0.087, val_acc:0.970]
Epoch [29/120    avg_loss:0.066, val_acc:0.980]
Epoch [30/120    avg_loss:0.052, val_acc:0.951]
Epoch [31/120    avg_loss:0.079, val_acc:0.975]
Epoch [32/120    avg_loss:0.040, val_acc:0.987]
Epoch [33/120    avg_loss:0.023, val_acc:0.975]
Epoch [34/120    avg_loss:0.041, val_acc:0.987]
Epoch [35/120    avg_loss:0.026, val_acc:0.983]
Epoch [36/120    avg_loss:0.026, val_acc:0.982]
Epoch [37/120    avg_loss:0.036, val_acc:0.982]
Epoch [38/120    avg_loss:0.052, val_acc:0.982]
Epoch [39/120    avg_loss:0.031, val_acc:0.982]
Epoch [40/120    avg_loss:0.024, val_acc:0.980]
Epoch [41/120    avg_loss:0.025, val_acc:0.986]
Epoch [42/120    avg_loss:0.012, val_acc:0.989]
Epoch [43/120    avg_loss:0.028, val_acc:0.984]
Epoch [44/120    avg_loss:0.018, val_acc:0.990]
Epoch [45/120    avg_loss:0.020, val_acc:0.986]
Epoch [46/120    avg_loss:0.014, val_acc:0.990]
Epoch [47/120    avg_loss:0.014, val_acc:0.990]
Epoch [48/120    avg_loss:0.010, val_acc:0.991]
Epoch [49/120    avg_loss:0.013, val_acc:0.978]
Epoch [50/120    avg_loss:0.009, val_acc:0.993]
Epoch [51/120    avg_loss:0.017, val_acc:0.993]
Epoch [52/120    avg_loss:0.013, val_acc:0.992]
Epoch [53/120    avg_loss:0.005, val_acc:0.993]
Epoch [54/120    avg_loss:0.009, val_acc:0.987]
Epoch [55/120    avg_loss:0.008, val_acc:0.991]
Epoch [56/120    avg_loss:0.016, val_acc:0.985]
Epoch [57/120    avg_loss:0.025, val_acc:0.987]
Epoch [58/120    avg_loss:0.015, val_acc:0.995]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.009, val_acc:0.993]
Epoch [61/120    avg_loss:0.008, val_acc:0.990]
Epoch [62/120    avg_loss:0.023, val_acc:0.960]
Epoch [63/120    avg_loss:0.040, val_acc:0.971]
Epoch [64/120    avg_loss:0.023, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.995]
Epoch [66/120    avg_loss:0.006, val_acc:0.997]
Epoch [67/120    avg_loss:0.007, val_acc:0.994]
Epoch [68/120    avg_loss:0.006, val_acc:0.992]
Epoch [69/120    avg_loss:0.008, val_acc:0.990]
Epoch [70/120    avg_loss:0.010, val_acc:0.991]
Epoch [71/120    avg_loss:0.012, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.995]
Epoch [73/120    avg_loss:0.022, val_acc:0.993]
Epoch [74/120    avg_loss:0.006, val_acc:0.996]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.041, val_acc:0.980]
Epoch [77/120    avg_loss:0.025, val_acc:0.972]
Epoch [78/120    avg_loss:0.039, val_acc:0.989]
Epoch [79/120    avg_loss:0.008, val_acc:0.993]
Epoch [80/120    avg_loss:0.008, val_acc:0.993]
Epoch [81/120    avg_loss:0.010, val_acc:0.995]
Epoch [82/120    avg_loss:0.008, val_acc:0.995]
Epoch [83/120    avg_loss:0.016, val_acc:0.993]
Epoch [84/120    avg_loss:0.006, val_acc:0.995]
Epoch [85/120    avg_loss:0.006, val_acc:0.995]
Epoch [86/120    avg_loss:0.007, val_acc:0.994]
Epoch [87/120    avg_loss:0.005, val_acc:0.996]
Epoch [88/120    avg_loss:0.005, val_acc:0.996]
Epoch [89/120    avg_loss:0.010, val_acc:0.996]
Epoch [90/120    avg_loss:0.005, val_acc:0.996]
Epoch [91/120    avg_loss:0.006, val_acc:0.996]
Epoch [92/120    avg_loss:0.007, val_acc:0.996]
Epoch [93/120    avg_loss:0.006, val_acc:0.996]
Epoch [94/120    avg_loss:0.008, val_acc:0.996]
Epoch [95/120    avg_loss:0.006, val_acc:0.996]
Epoch [96/120    avg_loss:0.005, val_acc:0.996]
Epoch [97/120    avg_loss:0.009, val_acc:0.996]
Epoch [98/120    avg_loss:0.007, val_acc:0.996]
Epoch [99/120    avg_loss:0.006, val_acc:0.996]
Epoch [100/120    avg_loss:0.006, val_acc:0.996]
Epoch [101/120    avg_loss:0.005, val_acc:0.996]
Epoch [102/120    avg_loss:0.007, val_acc:0.996]
Epoch [103/120    avg_loss:0.005, val_acc:0.996]
Epoch [104/120    avg_loss:0.004, val_acc:0.996]
Epoch [105/120    avg_loss:0.007, val_acc:0.996]
Epoch [106/120    avg_loss:0.005, val_acc:0.996]
Epoch [107/120    avg_loss:0.010, val_acc:0.996]
Epoch [108/120    avg_loss:0.006, val_acc:0.996]
Epoch [109/120    avg_loss:0.003, val_acc:0.996]
Epoch [110/120    avg_loss:0.007, val_acc:0.996]
Epoch [111/120    avg_loss:0.007, val_acc:0.996]
Epoch [112/120    avg_loss:0.007, val_acc:0.996]
Epoch [113/120    avg_loss:0.005, val_acc:0.996]
Epoch [114/120    avg_loss:0.007, val_acc:0.996]
Epoch [115/120    avg_loss:0.007, val_acc:0.996]
Epoch [116/120    avg_loss:0.009, val_acc:0.996]
Epoch [117/120    avg_loss:0.004, val_acc:0.996]
Epoch [118/120    avg_loss:0.004, val_acc:0.996]
Epoch [119/120    avg_loss:0.005, val_acc:0.996]
Epoch [120/120    avg_loss:0.005, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     9     0     0    10     0     0]
 [    0     4 18053     0    11     0    22     0     0     0]
 [    0     0     0  2025     0     0     0     0     7     4]
 [    0    23    14     0  2907     0     0     0    28     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4862     0     0    16]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     3     0     0    27     0     0     0  3540     1]
 [    0     0     0     7    14    18     0     0     0   880]]

Accuracy:
99.46496999493891

F1 scores:
[       nan 0.99619417 0.99858948 0.99557522 0.97878788 0.99276742
 0.99580133 0.99497487 0.99076406 0.96650192]

Kappa:
0.9929121198057127
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd25f050710>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.380, val_acc:0.458]
Epoch [2/120    avg_loss:0.886, val_acc:0.638]
Epoch [3/120    avg_loss:0.686, val_acc:0.765]
Epoch [4/120    avg_loss:0.521, val_acc:0.836]
Epoch [5/120    avg_loss:0.328, val_acc:0.861]
Epoch [6/120    avg_loss:0.475, val_acc:0.881]
Epoch [7/120    avg_loss:0.339, val_acc:0.899]
Epoch [8/120    avg_loss:0.239, val_acc:0.908]
Epoch [9/120    avg_loss:0.178, val_acc:0.899]
Epoch [10/120    avg_loss:0.214, val_acc:0.921]
Epoch [11/120    avg_loss:0.256, val_acc:0.906]
Epoch [12/120    avg_loss:0.156, val_acc:0.889]
Epoch [13/120    avg_loss:0.184, val_acc:0.935]
Epoch [14/120    avg_loss:0.146, val_acc:0.946]
Epoch [15/120    avg_loss:0.122, val_acc:0.923]
Epoch [16/120    avg_loss:0.138, val_acc:0.951]
Epoch [17/120    avg_loss:0.147, val_acc:0.965]
Epoch [18/120    avg_loss:0.126, val_acc:0.932]
Epoch [19/120    avg_loss:0.111, val_acc:0.943]
Epoch [20/120    avg_loss:0.077, val_acc:0.941]
Epoch [21/120    avg_loss:0.089, val_acc:0.968]
Epoch [22/120    avg_loss:0.094, val_acc:0.949]
Epoch [23/120    avg_loss:0.105, val_acc:0.944]
Epoch [24/120    avg_loss:0.061, val_acc:0.972]
Epoch [25/120    avg_loss:0.054, val_acc:0.967]
Epoch [26/120    avg_loss:0.066, val_acc:0.944]
Epoch [27/120    avg_loss:0.073, val_acc:0.921]
Epoch [28/120    avg_loss:0.047, val_acc:0.966]
Epoch [29/120    avg_loss:0.054, val_acc:0.977]
Epoch [30/120    avg_loss:0.040, val_acc:0.979]
Epoch [31/120    avg_loss:0.071, val_acc:0.976]
Epoch [32/120    avg_loss:0.077, val_acc:0.961]
Epoch [33/120    avg_loss:0.037, val_acc:0.976]
Epoch [34/120    avg_loss:0.045, val_acc:0.974]
Epoch [35/120    avg_loss:0.025, val_acc:0.966]
Epoch [36/120    avg_loss:0.028, val_acc:0.982]
Epoch [37/120    avg_loss:0.034, val_acc:0.982]
Epoch [38/120    avg_loss:0.023, val_acc:0.984]
Epoch [39/120    avg_loss:0.017, val_acc:0.982]
Epoch [40/120    avg_loss:0.023, val_acc:0.989]
Epoch [41/120    avg_loss:0.021, val_acc:0.946]
Epoch [42/120    avg_loss:0.035, val_acc:0.982]
Epoch [43/120    avg_loss:0.022, val_acc:0.974]
Epoch [44/120    avg_loss:0.015, val_acc:0.984]
Epoch [45/120    avg_loss:0.014, val_acc:0.990]
Epoch [46/120    avg_loss:0.011, val_acc:0.986]
Epoch [47/120    avg_loss:0.010, val_acc:0.988]
Epoch [48/120    avg_loss:0.019, val_acc:0.986]
Epoch [49/120    avg_loss:0.018, val_acc:0.986]
Epoch [50/120    avg_loss:0.013, val_acc:0.984]
Epoch [51/120    avg_loss:0.016, val_acc:0.967]
Epoch [52/120    avg_loss:0.023, val_acc:0.980]
Epoch [53/120    avg_loss:0.022, val_acc:0.983]
Epoch [54/120    avg_loss:0.022, val_acc:0.979]
Epoch [55/120    avg_loss:0.012, val_acc:0.987]
Epoch [56/120    avg_loss:0.010, val_acc:0.985]
Epoch [57/120    avg_loss:0.016, val_acc:0.986]
Epoch [58/120    avg_loss:0.013, val_acc:0.987]
Epoch [59/120    avg_loss:0.024, val_acc:0.993]
Epoch [60/120    avg_loss:0.008, val_acc:0.991]
Epoch [61/120    avg_loss:0.007, val_acc:0.992]
Epoch [62/120    avg_loss:0.009, val_acc:0.991]
Epoch [63/120    avg_loss:0.008, val_acc:0.991]
Epoch [64/120    avg_loss:0.008, val_acc:0.991]
Epoch [65/120    avg_loss:0.006, val_acc:0.991]
Epoch [66/120    avg_loss:0.007, val_acc:0.991]
Epoch [67/120    avg_loss:0.005, val_acc:0.991]
Epoch [68/120    avg_loss:0.008, val_acc:0.991]
Epoch [69/120    avg_loss:0.010, val_acc:0.992]
Epoch [70/120    avg_loss:0.005, val_acc:0.991]
Epoch [71/120    avg_loss:0.010, val_acc:0.991]
Epoch [72/120    avg_loss:0.008, val_acc:0.991]
Epoch [73/120    avg_loss:0.007, val_acc:0.991]
Epoch [74/120    avg_loss:0.007, val_acc:0.991]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.011, val_acc:0.991]
Epoch [77/120    avg_loss:0.005, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.991]
Epoch [80/120    avg_loss:0.008, val_acc:0.991]
Epoch [81/120    avg_loss:0.008, val_acc:0.991]
Epoch [82/120    avg_loss:0.004, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.009, val_acc:0.991]
Epoch [86/120    avg_loss:0.008, val_acc:0.991]
Epoch [87/120    avg_loss:0.005, val_acc:0.991]
Epoch [88/120    avg_loss:0.008, val_acc:0.991]
Epoch [89/120    avg_loss:0.004, val_acc:0.991]
Epoch [90/120    avg_loss:0.007, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.003, val_acc:0.991]
Epoch [93/120    avg_loss:0.014, val_acc:0.991]
Epoch [94/120    avg_loss:0.009, val_acc:0.991]
Epoch [95/120    avg_loss:0.005, val_acc:0.991]
Epoch [96/120    avg_loss:0.013, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.005, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.008, val_acc:0.991]
Epoch [102/120    avg_loss:0.008, val_acc:0.991]
Epoch [103/120    avg_loss:0.009, val_acc:0.991]
Epoch [104/120    avg_loss:0.009, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.008, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.010, val_acc:0.991]
Epoch [112/120    avg_loss:0.007, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.011, val_acc:0.991]
Epoch [120/120    avg_loss:0.007, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     0     5     0    15     1     0     0]
 [    0     0 18027     0    31     0    32     0     0     0]
 [    0     0     0  2032     0     0     0     0     0     4]
 [    0    35    17     0  2894     0     0     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4863     0     0    15]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0     2     0     0    20     0     0     0  3537    12]
 [    0     0     0    13    14    32     0     0     0   860]]

Accuracy:
99.32759742607188

F1 scores:
[       nan 0.99549689 0.99778602 0.99583435 0.97506739 0.98788796
 0.99366571 0.99767081 0.99158957 0.9476584 ]

Kappa:
0.9910943152588352
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbf0d7707b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.338, val_acc:0.587]
Epoch [2/120    avg_loss:0.741, val_acc:0.690]
Epoch [3/120    avg_loss:0.589, val_acc:0.796]
Epoch [4/120    avg_loss:0.433, val_acc:0.774]
Epoch [5/120    avg_loss:0.312, val_acc:0.888]
Epoch [6/120    avg_loss:0.341, val_acc:0.861]
Epoch [7/120    avg_loss:0.285, val_acc:0.910]
Epoch [8/120    avg_loss:0.231, val_acc:0.894]
Epoch [9/120    avg_loss:0.192, val_acc:0.932]
Epoch [10/120    avg_loss:0.175, val_acc:0.924]
Epoch [11/120    avg_loss:0.164, val_acc:0.917]
Epoch [12/120    avg_loss:0.167, val_acc:0.932]
Epoch [13/120    avg_loss:0.098, val_acc:0.958]
Epoch [14/120    avg_loss:0.100, val_acc:0.945]
Epoch [15/120    avg_loss:0.148, val_acc:0.943]
Epoch [16/120    avg_loss:0.184, val_acc:0.944]
Epoch [17/120    avg_loss:0.097, val_acc:0.937]
Epoch [18/120    avg_loss:0.108, val_acc:0.924]
Epoch [19/120    avg_loss:0.228, val_acc:0.927]
Epoch [20/120    avg_loss:0.153, val_acc:0.943]
Epoch [21/120    avg_loss:0.146, val_acc:0.955]
Epoch [22/120    avg_loss:0.079, val_acc:0.961]
Epoch [23/120    avg_loss:0.084, val_acc:0.969]
Epoch [24/120    avg_loss:0.050, val_acc:0.953]
Epoch [25/120    avg_loss:0.066, val_acc:0.955]
Epoch [26/120    avg_loss:0.055, val_acc:0.954]
Epoch [27/120    avg_loss:0.055, val_acc:0.924]
Epoch [28/120    avg_loss:0.033, val_acc:0.964]
Epoch [29/120    avg_loss:0.039, val_acc:0.943]
Epoch [30/120    avg_loss:0.034, val_acc:0.968]
Epoch [31/120    avg_loss:0.026, val_acc:0.980]
Epoch [32/120    avg_loss:0.082, val_acc:0.968]
Epoch [33/120    avg_loss:0.045, val_acc:0.939]
Epoch [34/120    avg_loss:0.076, val_acc:0.977]
Epoch [35/120    avg_loss:0.032, val_acc:0.965]
Epoch [36/120    avg_loss:0.058, val_acc:0.968]
Epoch [37/120    avg_loss:0.029, val_acc:0.978]
Epoch [38/120    avg_loss:0.042, val_acc:0.954]
Epoch [39/120    avg_loss:0.038, val_acc:0.980]
Epoch [40/120    avg_loss:0.025, val_acc:0.973]
Epoch [41/120    avg_loss:0.018, val_acc:0.979]
Epoch [42/120    avg_loss:0.019, val_acc:0.980]
Epoch [43/120    avg_loss:0.020, val_acc:0.983]
Epoch [44/120    avg_loss:0.022, val_acc:0.980]
Epoch [45/120    avg_loss:0.023, val_acc:0.980]
Epoch [46/120    avg_loss:0.018, val_acc:0.984]
Epoch [47/120    avg_loss:0.029, val_acc:0.969]
Epoch [48/120    avg_loss:0.033, val_acc:0.956]
Epoch [49/120    avg_loss:0.054, val_acc:0.965]
Epoch [50/120    avg_loss:0.065, val_acc:0.973]
Epoch [51/120    avg_loss:0.056, val_acc:0.973]
Epoch [52/120    avg_loss:0.112, val_acc:0.971]
Epoch [53/120    avg_loss:0.025, val_acc:0.894]
Epoch [54/120    avg_loss:0.026, val_acc:0.981]
Epoch [55/120    avg_loss:0.013, val_acc:0.986]
Epoch [56/120    avg_loss:0.043, val_acc:0.975]
Epoch [57/120    avg_loss:0.043, val_acc:0.960]
Epoch [58/120    avg_loss:0.023, val_acc:0.981]
Epoch [59/120    avg_loss:0.033, val_acc:0.980]
Epoch [60/120    avg_loss:0.022, val_acc:0.977]
Epoch [61/120    avg_loss:0.018, val_acc:0.985]
Epoch [62/120    avg_loss:0.022, val_acc:0.985]
Epoch [63/120    avg_loss:0.019, val_acc:0.983]
Epoch [64/120    avg_loss:0.011, val_acc:0.988]
Epoch [65/120    avg_loss:0.025, val_acc:0.978]
Epoch [66/120    avg_loss:0.020, val_acc:0.948]
Epoch [67/120    avg_loss:0.019, val_acc:0.988]
Epoch [68/120    avg_loss:0.011, val_acc:0.990]
Epoch [69/120    avg_loss:0.012, val_acc:0.990]
Epoch [70/120    avg_loss:0.010, val_acc:0.989]
Epoch [71/120    avg_loss:0.007, val_acc:0.989]
Epoch [72/120    avg_loss:0.006, val_acc:0.987]
Epoch [73/120    avg_loss:0.007, val_acc:0.987]
Epoch [74/120    avg_loss:0.006, val_acc:0.991]
Epoch [75/120    avg_loss:0.006, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.005, val_acc:0.986]
Epoch [78/120    avg_loss:0.018, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.102, val_acc:0.938]
Epoch [81/120    avg_loss:0.124, val_acc:0.957]
Epoch [82/120    avg_loss:0.053, val_acc:0.966]
Epoch [83/120    avg_loss:0.091, val_acc:0.972]
Epoch [84/120    avg_loss:0.029, val_acc:0.975]
Epoch [85/120    avg_loss:0.020, val_acc:0.979]
Epoch [86/120    avg_loss:0.014, val_acc:0.981]
Epoch [87/120    avg_loss:0.027, val_acc:0.978]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.007, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.983]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.004, val_acc:0.984]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     2     0    14    21     0     0]
 [    0     0 18041     0    44     0     5     0     0     0]
 [    0     0     0  2012     1     0     0     0    12    11]
 [    0    33     1     0  2915     0     0     0    21     2]
 [    0     0     0     0     0  1301     0     0     0     4]
 [    0     0     0     0     0     0  4867     0     0    11]
 [    0     0     0     0     0     0     3  1277     0    10]
 [    0     4     0     0    22     0     0     0  3542     3]
 [    0     0     0     0    14    17     0     0     0   888]]

Accuracy:
99.3854385077001

F1 scores:
[       nan 0.99424751 0.99861619 0.99407115 0.97654941 0.9919939
 0.99662128 0.98686244 0.99132382 0.96103896]

Kappa:
0.9918612960356824
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fedc15f8710>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.439, val_acc:0.448]
Epoch [2/120    avg_loss:0.779, val_acc:0.622]
Epoch [3/120    avg_loss:0.583, val_acc:0.755]
Epoch [4/120    avg_loss:0.570, val_acc:0.815]
Epoch [5/120    avg_loss:0.477, val_acc:0.865]
Epoch [6/120    avg_loss:0.412, val_acc:0.862]
Epoch [7/120    avg_loss:0.326, val_acc:0.811]
Epoch [8/120    avg_loss:0.305, val_acc:0.900]
Epoch [9/120    avg_loss:0.229, val_acc:0.916]
Epoch [10/120    avg_loss:0.164, val_acc:0.893]
Epoch [11/120    avg_loss:0.233, val_acc:0.883]
Epoch [12/120    avg_loss:0.201, val_acc:0.878]
Epoch [13/120    avg_loss:0.209, val_acc:0.889]
Epoch [14/120    avg_loss:0.172, val_acc:0.900]
Epoch [15/120    avg_loss:0.176, val_acc:0.924]
Epoch [16/120    avg_loss:0.135, val_acc:0.950]
Epoch [17/120    avg_loss:0.121, val_acc:0.916]
Epoch [18/120    avg_loss:0.127, val_acc:0.940]
Epoch [19/120    avg_loss:0.176, val_acc:0.927]
Epoch [20/120    avg_loss:0.102, val_acc:0.955]
Epoch [21/120    avg_loss:0.052, val_acc:0.955]
Epoch [22/120    avg_loss:0.052, val_acc:0.925]
Epoch [23/120    avg_loss:0.065, val_acc:0.946]
Epoch [24/120    avg_loss:0.110, val_acc:0.960]
Epoch [25/120    avg_loss:0.064, val_acc:0.955]
Epoch [26/120    avg_loss:0.033, val_acc:0.961]
Epoch [27/120    avg_loss:0.038, val_acc:0.970]
Epoch [28/120    avg_loss:0.133, val_acc:0.902]
Epoch [29/120    avg_loss:0.119, val_acc:0.959]
Epoch [30/120    avg_loss:0.067, val_acc:0.963]
Epoch [31/120    avg_loss:0.051, val_acc:0.962]
Epoch [32/120    avg_loss:0.038, val_acc:0.972]
Epoch [33/120    avg_loss:0.114, val_acc:0.925]
Epoch [34/120    avg_loss:0.097, val_acc:0.963]
Epoch [35/120    avg_loss:0.061, val_acc:0.968]
Epoch [36/120    avg_loss:0.037, val_acc:0.954]
Epoch [37/120    avg_loss:0.033, val_acc:0.961]
Epoch [38/120    avg_loss:0.025, val_acc:0.959]
Epoch [39/120    avg_loss:0.039, val_acc:0.961]
Epoch [40/120    avg_loss:0.024, val_acc:0.980]
Epoch [41/120    avg_loss:0.035, val_acc:0.970]
Epoch [42/120    avg_loss:0.018, val_acc:0.976]
Epoch [43/120    avg_loss:0.070, val_acc:0.961]
Epoch [44/120    avg_loss:0.089, val_acc:0.960]
Epoch [45/120    avg_loss:0.060, val_acc:0.967]
Epoch [46/120    avg_loss:0.032, val_acc:0.960]
Epoch [47/120    avg_loss:0.084, val_acc:0.954]
Epoch [48/120    avg_loss:0.033, val_acc:0.968]
Epoch [49/120    avg_loss:0.026, val_acc:0.980]
Epoch [50/120    avg_loss:0.015, val_acc:0.981]
Epoch [51/120    avg_loss:0.022, val_acc:0.980]
Epoch [52/120    avg_loss:0.014, val_acc:0.981]
Epoch [53/120    avg_loss:0.016, val_acc:0.983]
Epoch [54/120    avg_loss:0.008, val_acc:0.981]
Epoch [55/120    avg_loss:0.008, val_acc:0.979]
Epoch [56/120    avg_loss:0.022, val_acc:0.967]
Epoch [57/120    avg_loss:0.021, val_acc:0.974]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.008, val_acc:0.981]
Epoch [60/120    avg_loss:0.011, val_acc:0.981]
Epoch [61/120    avg_loss:0.011, val_acc:0.980]
Epoch [62/120    avg_loss:0.007, val_acc:0.976]
Epoch [63/120    avg_loss:0.010, val_acc:0.976]
Epoch [64/120    avg_loss:0.033, val_acc:0.970]
Epoch [65/120    avg_loss:0.010, val_acc:0.980]
Epoch [66/120    avg_loss:0.021, val_acc:0.977]
Epoch [67/120    avg_loss:0.019, val_acc:0.977]
Epoch [68/120    avg_loss:0.011, val_acc:0.979]
Epoch [69/120    avg_loss:0.012, val_acc:0.985]
Epoch [70/120    avg_loss:0.008, val_acc:0.983]
Epoch [71/120    avg_loss:0.022, val_acc:0.956]
Epoch [72/120    avg_loss:0.033, val_acc:0.981]
Epoch [73/120    avg_loss:0.017, val_acc:0.982]
Epoch [74/120    avg_loss:0.006, val_acc:0.983]
Epoch [75/120    avg_loss:0.008, val_acc:0.983]
Epoch [76/120    avg_loss:0.014, val_acc:0.981]
Epoch [77/120    avg_loss:0.005, val_acc:0.981]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.005, val_acc:0.985]
Epoch [80/120    avg_loss:0.008, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.005, val_acc:0.986]
Epoch [86/120    avg_loss:0.004, val_acc:0.986]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.004, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.003, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.010, val_acc:0.985]
Epoch [111/120    avg_loss:0.020, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     2     0     0    11     0     0]
 [    0     2 18061     0    16     0    11     0     0     0]
 [    0     0     0  2023     0     0     0     0     7     6]
 [    0    38    10     1  2890     0     1     0    31     1]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4857     0     5    16]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0    10    27     0     0     0  3530     2]
 [    0     0     0     3    14    28     0     0     0   874]]

Accuracy:
99.40471886824284

F1 scores:
[       nan 0.99573412 0.99892149 0.99337098 0.97618645 0.98900265
 0.99661434 0.99497876 0.98824188 0.95991214]

Kappa:
0.9921135556338029
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f37ea26a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.308, val_acc:0.661]
Epoch [2/120    avg_loss:0.677, val_acc:0.719]
Epoch [3/120    avg_loss:0.554, val_acc:0.757]
Epoch [4/120    avg_loss:0.468, val_acc:0.828]
Epoch [5/120    avg_loss:0.435, val_acc:0.816]
Epoch [6/120    avg_loss:0.377, val_acc:0.872]
Epoch [7/120    avg_loss:0.262, val_acc:0.920]
Epoch [8/120    avg_loss:0.256, val_acc:0.923]
Epoch [9/120    avg_loss:0.240, val_acc:0.892]
Epoch [10/120    avg_loss:0.225, val_acc:0.912]
Epoch [11/120    avg_loss:0.185, val_acc:0.938]
Epoch [12/120    avg_loss:0.166, val_acc:0.956]
Epoch [13/120    avg_loss:0.138, val_acc:0.928]
Epoch [14/120    avg_loss:0.192, val_acc:0.936]
Epoch [15/120    avg_loss:0.166, val_acc:0.954]
Epoch [16/120    avg_loss:0.155, val_acc:0.934]
Epoch [17/120    avg_loss:0.136, val_acc:0.951]
Epoch [18/120    avg_loss:0.150, val_acc:0.951]
Epoch [19/120    avg_loss:0.121, val_acc:0.953]
Epoch [20/120    avg_loss:0.107, val_acc:0.955]
Epoch [21/120    avg_loss:0.073, val_acc:0.951]
Epoch [22/120    avg_loss:0.058, val_acc:0.974]
Epoch [23/120    avg_loss:0.047, val_acc:0.978]
Epoch [24/120    avg_loss:0.059, val_acc:0.982]
Epoch [25/120    avg_loss:0.043, val_acc:0.957]
Epoch [26/120    avg_loss:0.051, val_acc:0.977]
Epoch [27/120    avg_loss:0.038, val_acc:0.977]
Epoch [28/120    avg_loss:0.072, val_acc:0.964]
Epoch [29/120    avg_loss:0.081, val_acc:0.984]
Epoch [30/120    avg_loss:0.063, val_acc:0.970]
Epoch [31/120    avg_loss:0.034, val_acc:0.966]
Epoch [32/120    avg_loss:0.145, val_acc:0.938]
Epoch [33/120    avg_loss:0.072, val_acc:0.944]
Epoch [34/120    avg_loss:0.078, val_acc:0.973]
Epoch [35/120    avg_loss:0.033, val_acc:0.978]
Epoch [36/120    avg_loss:0.033, val_acc:0.962]
Epoch [37/120    avg_loss:0.034, val_acc:0.983]
Epoch [38/120    avg_loss:0.037, val_acc:0.937]
Epoch [39/120    avg_loss:0.040, val_acc:0.982]
Epoch [40/120    avg_loss:0.049, val_acc:0.983]
Epoch [41/120    avg_loss:0.027, val_acc:0.978]
Epoch [42/120    avg_loss:0.022, val_acc:0.987]
Epoch [43/120    avg_loss:0.017, val_acc:0.989]
Epoch [44/120    avg_loss:0.028, val_acc:0.984]
Epoch [45/120    avg_loss:0.013, val_acc:0.990]
Epoch [46/120    avg_loss:0.012, val_acc:0.988]
Epoch [47/120    avg_loss:0.015, val_acc:0.983]
Epoch [48/120    avg_loss:0.031, val_acc:0.967]
Epoch [49/120    avg_loss:0.023, val_acc:0.990]
Epoch [50/120    avg_loss:0.028, val_acc:0.990]
Epoch [51/120    avg_loss:0.024, val_acc:0.987]
Epoch [52/120    avg_loss:0.017, val_acc:0.984]
Epoch [53/120    avg_loss:0.019, val_acc:0.988]
Epoch [54/120    avg_loss:0.015, val_acc:0.989]
Epoch [55/120    avg_loss:0.019, val_acc:0.989]
Epoch [56/120    avg_loss:0.013, val_acc:0.988]
Epoch [57/120    avg_loss:0.008, val_acc:0.991]
Epoch [58/120    avg_loss:0.008, val_acc:0.991]
Epoch [59/120    avg_loss:0.010, val_acc:0.990]
Epoch [60/120    avg_loss:0.018, val_acc:0.985]
Epoch [61/120    avg_loss:0.008, val_acc:0.990]
Epoch [62/120    avg_loss:0.009, val_acc:0.990]
Epoch [63/120    avg_loss:0.006, val_acc:0.989]
Epoch [64/120    avg_loss:0.006, val_acc:0.990]
Epoch [65/120    avg_loss:0.019, val_acc:0.975]
Epoch [66/120    avg_loss:0.053, val_acc:0.977]
Epoch [67/120    avg_loss:0.023, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.989]
Epoch [69/120    avg_loss:0.012, val_acc:0.990]
Epoch [70/120    avg_loss:0.006, val_acc:0.990]
Epoch [71/120    avg_loss:0.009, val_acc:0.978]
Epoch [72/120    avg_loss:0.011, val_acc:0.991]
Epoch [73/120    avg_loss:0.008, val_acc:0.991]
Epoch [74/120    avg_loss:0.004, val_acc:0.992]
Epoch [75/120    avg_loss:0.005, val_acc:0.992]
Epoch [76/120    avg_loss:0.017, val_acc:0.990]
Epoch [77/120    avg_loss:0.006, val_acc:0.992]
Epoch [78/120    avg_loss:0.007, val_acc:0.993]
Epoch [79/120    avg_loss:0.005, val_acc:0.994]
Epoch [80/120    avg_loss:0.005, val_acc:0.994]
Epoch [81/120    avg_loss:0.006, val_acc:0.994]
Epoch [82/120    avg_loss:0.004, val_acc:0.994]
Epoch [83/120    avg_loss:0.005, val_acc:0.994]
Epoch [84/120    avg_loss:0.008, val_acc:0.993]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.992]
Epoch [87/120    avg_loss:0.007, val_acc:0.993]
Epoch [88/120    avg_loss:0.006, val_acc:0.993]
Epoch [89/120    avg_loss:0.004, val_acc:0.993]
Epoch [90/120    avg_loss:0.004, val_acc:0.994]
Epoch [91/120    avg_loss:0.005, val_acc:0.994]
Epoch [92/120    avg_loss:0.009, val_acc:0.993]
Epoch [93/120    avg_loss:0.004, val_acc:0.994]
Epoch [94/120    avg_loss:0.005, val_acc:0.994]
Epoch [95/120    avg_loss:0.013, val_acc:0.993]
Epoch [96/120    avg_loss:0.006, val_acc:0.993]
Epoch [97/120    avg_loss:0.006, val_acc:0.994]
Epoch [98/120    avg_loss:0.003, val_acc:0.994]
Epoch [99/120    avg_loss:0.006, val_acc:0.994]
Epoch [100/120    avg_loss:0.004, val_acc:0.994]
Epoch [101/120    avg_loss:0.009, val_acc:0.994]
Epoch [102/120    avg_loss:0.005, val_acc:0.994]
Epoch [103/120    avg_loss:0.005, val_acc:0.993]
Epoch [104/120    avg_loss:0.003, val_acc:0.993]
Epoch [105/120    avg_loss:0.002, val_acc:0.993]
Epoch [106/120    avg_loss:0.003, val_acc:0.993]
Epoch [107/120    avg_loss:0.010, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.993]
Epoch [109/120    avg_loss:0.004, val_acc:0.993]
Epoch [110/120    avg_loss:0.003, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.007, val_acc:0.991]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6422     0     0     8     0     0     0     0     2]
 [    0     0 18035     0     5     0    50     0     0     0]
 [    0     0     0  2031     0     0     0     0     0     5]
 [    0    33    20     0  2891     0     3     0    25     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4863     0     0    15]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0     0     0    10    37     0     0     0  3524     0]
 [    0     0     0     2    14    29     0     0     0   874]]

Accuracy:
99.36374810208952

F1 scores:
[       nan 0.9966633  0.99792502 0.99583231 0.97553568 0.98862775
 0.99285423 0.99805825 0.98988764 0.96096756]

Kappa:
0.9915718227316886
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a60a6f780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.394, val_acc:0.537]
Epoch [2/120    avg_loss:0.787, val_acc:0.717]
Epoch [3/120    avg_loss:0.663, val_acc:0.839]
Epoch [4/120    avg_loss:0.421, val_acc:0.853]
Epoch [5/120    avg_loss:0.366, val_acc:0.897]
Epoch [6/120    avg_loss:0.410, val_acc:0.878]
Epoch [7/120    avg_loss:0.355, val_acc:0.857]
Epoch [8/120    avg_loss:0.282, val_acc:0.911]
Epoch [9/120    avg_loss:0.286, val_acc:0.915]
Epoch [10/120    avg_loss:0.206, val_acc:0.906]
Epoch [11/120    avg_loss:0.170, val_acc:0.920]
Epoch [12/120    avg_loss:0.189, val_acc:0.907]
Epoch [13/120    avg_loss:0.138, val_acc:0.945]
Epoch [14/120    avg_loss:0.133, val_acc:0.931]
Epoch [15/120    avg_loss:0.136, val_acc:0.943]
Epoch [16/120    avg_loss:0.101, val_acc:0.894]
Epoch [17/120    avg_loss:0.103, val_acc:0.954]
Epoch [18/120    avg_loss:0.175, val_acc:0.924]
Epoch [19/120    avg_loss:0.125, val_acc:0.946]
Epoch [20/120    avg_loss:0.093, val_acc:0.959]
Epoch [21/120    avg_loss:0.102, val_acc:0.950]
Epoch [22/120    avg_loss:0.069, val_acc:0.907]
Epoch [23/120    avg_loss:0.075, val_acc:0.969]
Epoch [24/120    avg_loss:0.180, val_acc:0.939]
Epoch [25/120    avg_loss:0.087, val_acc:0.967]
Epoch [26/120    avg_loss:0.076, val_acc:0.967]
Epoch [27/120    avg_loss:0.053, val_acc:0.970]
Epoch [28/120    avg_loss:0.032, val_acc:0.976]
Epoch [29/120    avg_loss:0.027, val_acc:0.955]
Epoch [30/120    avg_loss:0.037, val_acc:0.967]
Epoch [31/120    avg_loss:0.026, val_acc:0.974]
Epoch [32/120    avg_loss:0.072, val_acc:0.920]
Epoch [33/120    avg_loss:0.061, val_acc:0.961]
Epoch [34/120    avg_loss:0.050, val_acc:0.932]
Epoch [35/120    avg_loss:0.028, val_acc:0.973]
Epoch [36/120    avg_loss:0.025, val_acc:0.979]
Epoch [37/120    avg_loss:0.018, val_acc:0.982]
Epoch [38/120    avg_loss:0.022, val_acc:0.955]
Epoch [39/120    avg_loss:0.027, val_acc:0.977]
Epoch [40/120    avg_loss:0.030, val_acc:0.965]
Epoch [41/120    avg_loss:0.030, val_acc:0.938]
Epoch [42/120    avg_loss:0.041, val_acc:0.962]
Epoch [43/120    avg_loss:0.030, val_acc:0.956]
Epoch [44/120    avg_loss:0.048, val_acc:0.982]
Epoch [45/120    avg_loss:0.102, val_acc:0.936]
Epoch [46/120    avg_loss:0.073, val_acc:0.970]
Epoch [47/120    avg_loss:0.039, val_acc:0.962]
Epoch [48/120    avg_loss:0.025, val_acc:0.979]
Epoch [49/120    avg_loss:0.023, val_acc:0.970]
Epoch [50/120    avg_loss:0.019, val_acc:0.979]
Epoch [51/120    avg_loss:0.046, val_acc:0.979]
Epoch [52/120    avg_loss:0.022, val_acc:0.983]
Epoch [53/120    avg_loss:0.036, val_acc:0.968]
Epoch [54/120    avg_loss:0.031, val_acc:0.979]
Epoch [55/120    avg_loss:0.013, val_acc:0.979]
Epoch [56/120    avg_loss:0.012, val_acc:0.979]
Epoch [57/120    avg_loss:0.009, val_acc:0.987]
Epoch [58/120    avg_loss:0.015, val_acc:0.988]
Epoch [59/120    avg_loss:0.018, val_acc:0.967]
Epoch [60/120    avg_loss:0.022, val_acc:0.980]
Epoch [61/120    avg_loss:0.010, val_acc:0.991]
Epoch [62/120    avg_loss:0.010, val_acc:0.990]
Epoch [63/120    avg_loss:0.012, val_acc:0.988]
Epoch [64/120    avg_loss:0.006, val_acc:0.985]
Epoch [65/120    avg_loss:0.012, val_acc:0.991]
Epoch [66/120    avg_loss:0.011, val_acc:0.987]
Epoch [67/120    avg_loss:0.009, val_acc:0.991]
Epoch [68/120    avg_loss:0.011, val_acc:0.991]
Epoch [69/120    avg_loss:0.005, val_acc:0.991]
Epoch [70/120    avg_loss:0.007, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.014, val_acc:0.990]
Epoch [73/120    avg_loss:0.008, val_acc:0.977]
Epoch [74/120    avg_loss:0.012, val_acc:0.993]
Epoch [75/120    avg_loss:0.009, val_acc:0.989]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.025, val_acc:0.906]
Epoch [78/120    avg_loss:0.043, val_acc:0.979]
Epoch [79/120    avg_loss:0.009, val_acc:0.989]
Epoch [80/120    avg_loss:0.012, val_acc:0.994]
Epoch [81/120    avg_loss:0.018, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.987]
Epoch [84/120    avg_loss:0.004, val_acc:0.990]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.015, val_acc:0.979]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.003, val_acc:0.992]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.009, val_acc:0.975]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.003, val_acc:0.991]
Epoch [98/120    avg_loss:0.016, val_acc:0.989]
Epoch [99/120    avg_loss:0.003, val_acc:0.991]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.994]
Epoch [102/120    avg_loss:0.003, val_acc:0.995]
Epoch [103/120    avg_loss:0.003, val_acc:0.994]
Epoch [104/120    avg_loss:0.003, val_acc:0.994]
Epoch [105/120    avg_loss:0.004, val_acc:0.992]
Epoch [106/120    avg_loss:0.004, val_acc:0.993]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.992]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.008, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.003, val_acc:0.992]
Epoch [117/120    avg_loss:0.002, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6422     0     0     5     0     0     5     0     0]
 [    0     0 18034     0    27     0    24     0     0     5]
 [    0     0     0  2033     0     0     0     0     0     3]
 [    0    18    16     0  2904     0     0     0    34     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4852     0     0    26]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0     0    14     0     0     0  3548     7]
 [    0     0     0    15    14    37     0     0     0   853]]

Accuracy:
99.38302846263225

F1 scores:
[       nan 0.99766972 0.99800775 0.99559256 0.97843666 0.9852552
 0.9948739  0.99728997 0.99203132 0.93891029]

Kappa:
0.9918285206841004
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc10e3d37b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.354, val_acc:0.675]
Epoch [2/120    avg_loss:0.722, val_acc:0.665]
Epoch [3/120    avg_loss:0.559, val_acc:0.752]
Epoch [4/120    avg_loss:0.455, val_acc:0.804]
Epoch [5/120    avg_loss:0.410, val_acc:0.712]
Epoch [6/120    avg_loss:0.385, val_acc:0.744]
Epoch [7/120    avg_loss:0.334, val_acc:0.897]
Epoch [8/120    avg_loss:0.219, val_acc:0.894]
Epoch [9/120    avg_loss:0.186, val_acc:0.896]
Epoch [10/120    avg_loss:0.185, val_acc:0.909]
Epoch [11/120    avg_loss:0.192, val_acc:0.903]
Epoch [12/120    avg_loss:0.225, val_acc:0.868]
Epoch [13/120    avg_loss:0.165, val_acc:0.926]
Epoch [14/120    avg_loss:0.132, val_acc:0.926]
Epoch [15/120    avg_loss:0.146, val_acc:0.869]
Epoch [16/120    avg_loss:0.112, val_acc:0.956]
Epoch [17/120    avg_loss:0.106, val_acc:0.908]
Epoch [18/120    avg_loss:0.106, val_acc:0.953]
Epoch [19/120    avg_loss:0.057, val_acc:0.953]
Epoch [20/120    avg_loss:0.076, val_acc:0.935]
Epoch [21/120    avg_loss:0.095, val_acc:0.896]
Epoch [22/120    avg_loss:0.118, val_acc:0.918]
Epoch [23/120    avg_loss:0.260, val_acc:0.932]
Epoch [24/120    avg_loss:0.098, val_acc:0.945]
Epoch [25/120    avg_loss:0.083, val_acc:0.951]
Epoch [26/120    avg_loss:0.051, val_acc:0.959]
Epoch [27/120    avg_loss:0.051, val_acc:0.939]
Epoch [28/120    avg_loss:0.030, val_acc:0.974]
Epoch [29/120    avg_loss:0.059, val_acc:0.957]
Epoch [30/120    avg_loss:0.031, val_acc:0.967]
Epoch [31/120    avg_loss:0.047, val_acc:0.960]
Epoch [32/120    avg_loss:0.053, val_acc:0.938]
Epoch [33/120    avg_loss:0.050, val_acc:0.969]
Epoch [34/120    avg_loss:0.054, val_acc:0.950]
Epoch [35/120    avg_loss:0.043, val_acc:0.940]
Epoch [36/120    avg_loss:0.034, val_acc:0.971]
Epoch [37/120    avg_loss:0.041, val_acc:0.972]
Epoch [38/120    avg_loss:0.033, val_acc:0.973]
Epoch [39/120    avg_loss:0.021, val_acc:0.981]
Epoch [40/120    avg_loss:0.015, val_acc:0.969]
Epoch [41/120    avg_loss:0.042, val_acc:0.974]
Epoch [42/120    avg_loss:0.029, val_acc:0.964]
Epoch [43/120    avg_loss:0.028, val_acc:0.977]
Epoch [44/120    avg_loss:0.027, val_acc:0.958]
Epoch [45/120    avg_loss:0.021, val_acc:0.985]
Epoch [46/120    avg_loss:0.016, val_acc:0.978]
Epoch [47/120    avg_loss:0.020, val_acc:0.977]
Epoch [48/120    avg_loss:0.022, val_acc:0.979]
Epoch [49/120    avg_loss:0.014, val_acc:0.981]
Epoch [50/120    avg_loss:0.039, val_acc:0.932]
Epoch [51/120    avg_loss:0.064, val_acc:0.964]
Epoch [52/120    avg_loss:0.020, val_acc:0.980]
Epoch [53/120    avg_loss:0.022, val_acc:0.970]
Epoch [54/120    avg_loss:0.014, val_acc:0.980]
Epoch [55/120    avg_loss:0.010, val_acc:0.965]
Epoch [56/120    avg_loss:0.026, val_acc:0.965]
Epoch [57/120    avg_loss:0.026, val_acc:0.978]
Epoch [58/120    avg_loss:0.024, val_acc:0.971]
Epoch [59/120    avg_loss:0.018, val_acc:0.976]
Epoch [60/120    avg_loss:0.015, val_acc:0.980]
Epoch [61/120    avg_loss:0.011, val_acc:0.985]
Epoch [62/120    avg_loss:0.007, val_acc:0.986]
Epoch [63/120    avg_loss:0.015, val_acc:0.987]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.988]
Epoch [67/120    avg_loss:0.007, val_acc:0.988]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.006, val_acc:0.985]
Epoch [70/120    avg_loss:0.006, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.985]
Epoch [72/120    avg_loss:0.005, val_acc:0.985]
Epoch [73/120    avg_loss:0.006, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.986]
Epoch [75/120    avg_loss:0.009, val_acc:0.987]
Epoch [76/120    avg_loss:0.007, val_acc:0.987]
Epoch [77/120    avg_loss:0.009, val_acc:0.987]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.008, val_acc:0.986]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.006, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.014, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0    14     0    23     1     0     0]
 [    0     3 18004     0    39     0    44     0     0     0]
 [    0     0     0  2027     0     0     0     0     1     8]
 [    0    23    14     0  2900     0     2     0    33     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0     0    18]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     1     0     2    38     0     0     0  3530     0]
 [    0     0     0     0    14    34     0     0     0   871]]

Accuracy:
99.23842575856169

F1 scores:
[       nan 0.99494281 0.99723053 0.99729397 0.97038648 0.9871407
 0.9909267  0.99805976 0.98948844 0.95819582]

Kappa:
0.9899167036814551
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f671447c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.343, val_acc:0.533]
Epoch [2/120    avg_loss:0.680, val_acc:0.663]
Epoch [3/120    avg_loss:0.635, val_acc:0.808]
Epoch [4/120    avg_loss:0.406, val_acc:0.843]
Epoch [5/120    avg_loss:0.392, val_acc:0.854]
Epoch [6/120    avg_loss:0.332, val_acc:0.879]
Epoch [7/120    avg_loss:0.326, val_acc:0.914]
Epoch [8/120    avg_loss:0.231, val_acc:0.920]
Epoch [9/120    avg_loss:0.198, val_acc:0.922]
Epoch [10/120    avg_loss:0.197, val_acc:0.919]
Epoch [11/120    avg_loss:0.172, val_acc:0.915]
Epoch [12/120    avg_loss:0.155, val_acc:0.907]
Epoch [13/120    avg_loss:0.231, val_acc:0.920]
Epoch [14/120    avg_loss:0.166, val_acc:0.948]
Epoch [15/120    avg_loss:0.157, val_acc:0.926]
Epoch [16/120    avg_loss:0.118, val_acc:0.897]
Epoch [17/120    avg_loss:0.162, val_acc:0.947]
Epoch [18/120    avg_loss:0.097, val_acc:0.955]
Epoch [19/120    avg_loss:0.078, val_acc:0.955]
Epoch [20/120    avg_loss:0.103, val_acc:0.924]
Epoch [21/120    avg_loss:0.058, val_acc:0.949]
Epoch [22/120    avg_loss:0.120, val_acc:0.931]
Epoch [23/120    avg_loss:0.121, val_acc:0.909]
Epoch [24/120    avg_loss:0.122, val_acc:0.930]
Epoch [25/120    avg_loss:0.090, val_acc:0.924]
Epoch [26/120    avg_loss:0.058, val_acc:0.967]
Epoch [27/120    avg_loss:0.046, val_acc:0.967]
Epoch [28/120    avg_loss:0.037, val_acc:0.952]
Epoch [29/120    avg_loss:0.068, val_acc:0.960]
Epoch [30/120    avg_loss:0.051, val_acc:0.967]
Epoch [31/120    avg_loss:0.040, val_acc:0.977]
Epoch [32/120    avg_loss:0.044, val_acc:0.961]
Epoch [33/120    avg_loss:0.042, val_acc:0.981]
Epoch [34/120    avg_loss:0.021, val_acc:0.975]
Epoch [35/120    avg_loss:0.029, val_acc:0.970]
Epoch [36/120    avg_loss:0.019, val_acc:0.959]
Epoch [37/120    avg_loss:0.032, val_acc:0.976]
Epoch [38/120    avg_loss:0.035, val_acc:0.981]
Epoch [39/120    avg_loss:0.026, val_acc:0.980]
Epoch [40/120    avg_loss:0.016, val_acc:0.981]
Epoch [41/120    avg_loss:0.028, val_acc:0.978]
Epoch [42/120    avg_loss:0.020, val_acc:0.982]
Epoch [43/120    avg_loss:0.026, val_acc:0.967]
Epoch [44/120    avg_loss:0.048, val_acc:0.976]
Epoch [45/120    avg_loss:0.022, val_acc:0.986]
Epoch [46/120    avg_loss:0.016, val_acc:0.969]
Epoch [47/120    avg_loss:0.017, val_acc:0.980]
Epoch [48/120    avg_loss:0.026, val_acc:0.985]
Epoch [49/120    avg_loss:0.014, val_acc:0.989]
Epoch [50/120    avg_loss:0.019, val_acc:0.985]
Epoch [51/120    avg_loss:0.022, val_acc:0.979]
Epoch [52/120    avg_loss:0.009, val_acc:0.988]
Epoch [53/120    avg_loss:0.008, val_acc:0.991]
Epoch [54/120    avg_loss:0.009, val_acc:0.993]
Epoch [55/120    avg_loss:0.007, val_acc:0.991]
Epoch [56/120    avg_loss:0.012, val_acc:0.993]
Epoch [57/120    avg_loss:0.020, val_acc:0.985]
Epoch [58/120    avg_loss:0.012, val_acc:0.987]
Epoch [59/120    avg_loss:0.019, val_acc:0.987]
Epoch [60/120    avg_loss:0.010, val_acc:0.985]
Epoch [61/120    avg_loss:0.006, val_acc:0.991]
Epoch [62/120    avg_loss:0.006, val_acc:0.994]
Epoch [63/120    avg_loss:0.008, val_acc:0.991]
Epoch [64/120    avg_loss:0.009, val_acc:0.993]
Epoch [65/120    avg_loss:0.006, val_acc:0.987]
Epoch [66/120    avg_loss:0.006, val_acc:0.991]
Epoch [67/120    avg_loss:0.006, val_acc:0.991]
Epoch [68/120    avg_loss:0.068, val_acc:0.952]
Epoch [69/120    avg_loss:0.152, val_acc:0.958]
Epoch [70/120    avg_loss:0.120, val_acc:0.961]
Epoch [71/120    avg_loss:0.062, val_acc:0.972]
Epoch [72/120    avg_loss:0.037, val_acc:0.953]
Epoch [73/120    avg_loss:0.067, val_acc:0.953]
Epoch [74/120    avg_loss:0.111, val_acc:0.953]
Epoch [75/120    avg_loss:0.084, val_acc:0.975]
Epoch [76/120    avg_loss:0.042, val_acc:0.984]
Epoch [77/120    avg_loss:0.016, val_acc:0.982]
Epoch [78/120    avg_loss:0.021, val_acc:0.982]
Epoch [79/120    avg_loss:0.024, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.017, val_acc:0.985]
Epoch [82/120    avg_loss:0.019, val_acc:0.986]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.016, val_acc:0.985]
Epoch [85/120    avg_loss:0.012, val_acc:0.986]
Epoch [86/120    avg_loss:0.019, val_acc:0.985]
Epoch [87/120    avg_loss:0.024, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.986]
Epoch [89/120    avg_loss:0.016, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.013, val_acc:0.986]
Epoch [92/120    avg_loss:0.016, val_acc:0.986]
Epoch [93/120    avg_loss:0.019, val_acc:0.986]
Epoch [94/120    avg_loss:0.018, val_acc:0.987]
Epoch [95/120    avg_loss:0.013, val_acc:0.987]
Epoch [96/120    avg_loss:0.014, val_acc:0.987]
Epoch [97/120    avg_loss:0.017, val_acc:0.987]
Epoch [98/120    avg_loss:0.014, val_acc:0.987]
Epoch [99/120    avg_loss:0.012, val_acc:0.987]
Epoch [100/120    avg_loss:0.012, val_acc:0.987]
Epoch [101/120    avg_loss:0.017, val_acc:0.987]
Epoch [102/120    avg_loss:0.013, val_acc:0.987]
Epoch [103/120    avg_loss:0.012, val_acc:0.987]
Epoch [104/120    avg_loss:0.012, val_acc:0.987]
Epoch [105/120    avg_loss:0.017, val_acc:0.987]
Epoch [106/120    avg_loss:0.018, val_acc:0.987]
Epoch [107/120    avg_loss:0.013, val_acc:0.987]
Epoch [108/120    avg_loss:0.014, val_acc:0.987]
Epoch [109/120    avg_loss:0.013, val_acc:0.987]
Epoch [110/120    avg_loss:0.017, val_acc:0.987]
Epoch [111/120    avg_loss:0.011, val_acc:0.987]
Epoch [112/120    avg_loss:0.013, val_acc:0.987]
Epoch [113/120    avg_loss:0.021, val_acc:0.987]
Epoch [114/120    avg_loss:0.014, val_acc:0.987]
Epoch [115/120    avg_loss:0.013, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.015, val_acc:0.987]
Epoch [118/120    avg_loss:0.012, val_acc:0.987]
Epoch [119/120    avg_loss:0.013, val_acc:0.987]
Epoch [120/120    avg_loss:0.014, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     0     0     0     0     5     9]
 [    0     0 17973     0    48     0    66     0     3     0]
 [    0     0     0  2024     0     0     0     0     0    12]
 [    0    49    10     2  2879     0     0     0    30     2]
 [    0     0     0     0     0  1302     0     0     0     3]
 [    0     0     0     0     0     0  4851     0     0    27]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     2     0     0    35     0     0     0  3513    21]
 [    0     0     0     0    14    45     0     0     0   860]]

Accuracy:
99.07213264888054

F1 scores:
[       nan 0.99496163 0.99647936 0.99655342 0.96805649 0.98190045
 0.99050536 0.9992242  0.98652064 0.92722372]

Kappa:
0.9877195581182165
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9723ad87f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.515, val_acc:0.449]
Epoch [2/120    avg_loss:0.776, val_acc:0.688]
Epoch [3/120    avg_loss:0.577, val_acc:0.757]
Epoch [4/120    avg_loss:0.436, val_acc:0.816]
Epoch [5/120    avg_loss:0.427, val_acc:0.810]
Epoch [6/120    avg_loss:0.290, val_acc:0.909]
Epoch [7/120    avg_loss:0.346, val_acc:0.873]
Epoch [8/120    avg_loss:0.230, val_acc:0.940]
Epoch [9/120    avg_loss:0.323, val_acc:0.903]
Epoch [10/120    avg_loss:0.223, val_acc:0.922]
Epoch [11/120    avg_loss:0.220, val_acc:0.932]
Epoch [12/120    avg_loss:0.140, val_acc:0.919]
Epoch [13/120    avg_loss:0.189, val_acc:0.910]
Epoch [14/120    avg_loss:0.171, val_acc:0.945]
Epoch [15/120    avg_loss:0.129, val_acc:0.940]
Epoch [16/120    avg_loss:0.113, val_acc:0.943]
Epoch [17/120    avg_loss:0.140, val_acc:0.914]
Epoch [18/120    avg_loss:0.122, val_acc:0.948]
Epoch [19/120    avg_loss:0.068, val_acc:0.949]
Epoch [20/120    avg_loss:0.091, val_acc:0.952]
Epoch [21/120    avg_loss:0.080, val_acc:0.967]
Epoch [22/120    avg_loss:0.070, val_acc:0.973]
Epoch [23/120    avg_loss:0.044, val_acc:0.963]
Epoch [24/120    avg_loss:0.087, val_acc:0.953]
Epoch [25/120    avg_loss:0.085, val_acc:0.967]
Epoch [26/120    avg_loss:0.108, val_acc:0.966]
Epoch [27/120    avg_loss:0.060, val_acc:0.955]
Epoch [28/120    avg_loss:0.040, val_acc:0.967]
Epoch [29/120    avg_loss:0.053, val_acc:0.973]
Epoch [30/120    avg_loss:0.066, val_acc:0.978]
Epoch [31/120    avg_loss:0.038, val_acc:0.967]
Epoch [32/120    avg_loss:0.054, val_acc:0.972]
Epoch [33/120    avg_loss:0.025, val_acc:0.974]
Epoch [34/120    avg_loss:0.043, val_acc:0.952]
Epoch [35/120    avg_loss:0.036, val_acc:0.977]
Epoch [36/120    avg_loss:0.030, val_acc:0.976]
Epoch [37/120    avg_loss:0.039, val_acc:0.968]
Epoch [38/120    avg_loss:0.036, val_acc:0.983]
Epoch [39/120    avg_loss:0.021, val_acc:0.976]
Epoch [40/120    avg_loss:0.017, val_acc:0.985]
Epoch [41/120    avg_loss:0.057, val_acc:0.977]
Epoch [42/120    avg_loss:0.027, val_acc:0.973]
Epoch [43/120    avg_loss:0.053, val_acc:0.926]
Epoch [44/120    avg_loss:0.031, val_acc:0.981]
Epoch [45/120    avg_loss:0.020, val_acc:0.981]
Epoch [46/120    avg_loss:0.031, val_acc:0.980]
Epoch [47/120    avg_loss:0.016, val_acc:0.981]
Epoch [48/120    avg_loss:0.022, val_acc:0.985]
Epoch [49/120    avg_loss:0.015, val_acc:0.985]
Epoch [50/120    avg_loss:0.008, val_acc:0.984]
Epoch [51/120    avg_loss:0.017, val_acc:0.986]
Epoch [52/120    avg_loss:0.013, val_acc:0.985]
Epoch [53/120    avg_loss:0.011, val_acc:0.990]
Epoch [54/120    avg_loss:0.040, val_acc:0.985]
Epoch [55/120    avg_loss:0.018, val_acc:0.982]
Epoch [56/120    avg_loss:0.012, val_acc:0.974]
Epoch [57/120    avg_loss:0.025, val_acc:0.982]
Epoch [58/120    avg_loss:0.027, val_acc:0.984]
Epoch [59/120    avg_loss:0.013, val_acc:0.989]
Epoch [60/120    avg_loss:0.011, val_acc:0.986]
Epoch [61/120    avg_loss:0.022, val_acc:0.982]
Epoch [62/120    avg_loss:0.013, val_acc:0.984]
Epoch [63/120    avg_loss:0.020, val_acc:0.980]
Epoch [64/120    avg_loss:0.016, val_acc:0.982]
Epoch [65/120    avg_loss:0.014, val_acc:0.991]
Epoch [66/120    avg_loss:0.005, val_acc:0.987]
Epoch [67/120    avg_loss:0.007, val_acc:0.985]
Epoch [68/120    avg_loss:0.025, val_acc:0.987]
Epoch [69/120    avg_loss:0.012, val_acc:0.978]
Epoch [70/120    avg_loss:0.007, val_acc:0.986]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.011, val_acc:0.988]
Epoch [73/120    avg_loss:0.082, val_acc:0.907]
Epoch [74/120    avg_loss:0.074, val_acc:0.961]
Epoch [75/120    avg_loss:0.034, val_acc:0.989]
Epoch [76/120    avg_loss:0.040, val_acc:0.983]
Epoch [77/120    avg_loss:0.016, val_acc:0.963]
Epoch [78/120    avg_loss:0.012, val_acc:0.990]
Epoch [79/120    avg_loss:0.007, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.005, val_acc:0.990]
Epoch [83/120    avg_loss:0.008, val_acc:0.990]
Epoch [84/120    avg_loss:0.007, val_acc:0.989]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.991]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.004, val_acc:0.992]
Epoch [90/120    avg_loss:0.003, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.008, val_acc:0.992]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.008, val_acc:0.993]
Epoch [95/120    avg_loss:0.009, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.992]
Epoch [97/120    avg_loss:0.010, val_acc:0.991]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.009, val_acc:0.993]
Epoch [102/120    avg_loss:0.006, val_acc:0.993]
Epoch [103/120    avg_loss:0.008, val_acc:0.993]
Epoch [104/120    avg_loss:0.006, val_acc:0.994]
Epoch [105/120    avg_loss:0.011, val_acc:0.993]
Epoch [106/120    avg_loss:0.009, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.993]
Epoch [108/120    avg_loss:0.009, val_acc:0.993]
Epoch [109/120    avg_loss:0.008, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.993]
Epoch [111/120    avg_loss:0.006, val_acc:0.993]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.007, val_acc:0.991]
Epoch [114/120    avg_loss:0.009, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.992]
Epoch [119/120    avg_loss:0.004, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     0 18051     0    22     0    17     0     0     0]
 [    0     0     0  2018     0     0     0     0    13     5]
 [    0    38     9     0  2893     0     0     0    31     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4872     0     0     6]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     2     0     0    22     0     0     0  3547     0]
 [    0     0     0     5    14    48     0     0     0   852]]

Accuracy:
99.42881931892127

F1 scores:
[       nan 0.99690019 0.9986722  0.99433358 0.97686983 0.98194131
 0.99744088 0.9984472  0.99050545 0.95462185]

Kappa:
0.9924328545047006
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d6473a710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.247, val_acc:0.676]
Epoch [2/120    avg_loss:0.662, val_acc:0.727]
Epoch [3/120    avg_loss:0.539, val_acc:0.718]
Epoch [4/120    avg_loss:0.440, val_acc:0.784]
Epoch [5/120    avg_loss:0.380, val_acc:0.837]
Epoch [6/120    avg_loss:0.297, val_acc:0.858]
Epoch [7/120    avg_loss:0.271, val_acc:0.917]
Epoch [8/120    avg_loss:0.187, val_acc:0.866]
Epoch [9/120    avg_loss:0.194, val_acc:0.927]
Epoch [10/120    avg_loss:0.146, val_acc:0.940]
Epoch [11/120    avg_loss:0.144, val_acc:0.927]
Epoch [12/120    avg_loss:0.150, val_acc:0.929]
Epoch [13/120    avg_loss:0.087, val_acc:0.938]
Epoch [14/120    avg_loss:0.135, val_acc:0.940]
Epoch [15/120    avg_loss:0.137, val_acc:0.935]
Epoch [16/120    avg_loss:0.095, val_acc:0.959]
Epoch [17/120    avg_loss:0.069, val_acc:0.957]
Epoch [18/120    avg_loss:0.098, val_acc:0.928]
Epoch [19/120    avg_loss:0.107, val_acc:0.940]
Epoch [20/120    avg_loss:0.074, val_acc:0.966]
Epoch [21/120    avg_loss:0.055, val_acc:0.938]
Epoch [22/120    avg_loss:0.060, val_acc:0.942]
Epoch [23/120    avg_loss:0.038, val_acc:0.964]
Epoch [24/120    avg_loss:0.073, val_acc:0.970]
Epoch [25/120    avg_loss:0.075, val_acc:0.962]
Epoch [26/120    avg_loss:0.050, val_acc:0.971]
Epoch [27/120    avg_loss:0.030, val_acc:0.965]
Epoch [28/120    avg_loss:0.060, val_acc:0.960]
Epoch [29/120    avg_loss:0.043, val_acc:0.953]
Epoch [30/120    avg_loss:0.031, val_acc:0.978]
Epoch [31/120    avg_loss:0.033, val_acc:0.973]
Epoch [32/120    avg_loss:0.025, val_acc:0.982]
Epoch [33/120    avg_loss:0.021, val_acc:0.973]
Epoch [34/120    avg_loss:0.019, val_acc:0.976]
Epoch [35/120    avg_loss:0.014, val_acc:0.970]
Epoch [36/120    avg_loss:0.021, val_acc:0.961]
Epoch [37/120    avg_loss:0.044, val_acc:0.966]
Epoch [38/120    avg_loss:0.023, val_acc:0.980]
Epoch [39/120    avg_loss:0.011, val_acc:0.973]
Epoch [40/120    avg_loss:0.017, val_acc:0.969]
Epoch [41/120    avg_loss:0.033, val_acc:0.960]
Epoch [42/120    avg_loss:0.019, val_acc:0.964]
Epoch [43/120    avg_loss:0.014, val_acc:0.978]
Epoch [44/120    avg_loss:0.015, val_acc:0.982]
Epoch [45/120    avg_loss:0.011, val_acc:0.982]
Epoch [46/120    avg_loss:0.012, val_acc:0.980]
Epoch [47/120    avg_loss:0.018, val_acc:0.981]
Epoch [48/120    avg_loss:0.034, val_acc:0.979]
Epoch [49/120    avg_loss:0.026, val_acc:0.969]
Epoch [50/120    avg_loss:0.030, val_acc:0.965]
Epoch [51/120    avg_loss:0.027, val_acc:0.976]
Epoch [52/120    avg_loss:0.011, val_acc:0.983]
Epoch [53/120    avg_loss:0.019, val_acc:0.984]
Epoch [54/120    avg_loss:0.017, val_acc:0.976]
Epoch [55/120    avg_loss:0.015, val_acc:0.983]
Epoch [56/120    avg_loss:0.009, val_acc:0.983]
Epoch [57/120    avg_loss:0.007, val_acc:0.987]
Epoch [58/120    avg_loss:0.014, val_acc:0.981]
Epoch [59/120    avg_loss:0.024, val_acc:0.983]
Epoch [60/120    avg_loss:0.044, val_acc:0.975]
Epoch [61/120    avg_loss:0.027, val_acc:0.973]
Epoch [62/120    avg_loss:0.024, val_acc:0.979]
Epoch [63/120    avg_loss:0.014, val_acc:0.981]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.007, val_acc:0.983]
Epoch [66/120    avg_loss:0.014, val_acc:0.983]
Epoch [67/120    avg_loss:0.013, val_acc:0.973]
Epoch [68/120    avg_loss:0.012, val_acc:0.977]
Epoch [69/120    avg_loss:0.024, val_acc:0.975]
Epoch [70/120    avg_loss:0.025, val_acc:0.973]
Epoch [71/120    avg_loss:0.011, val_acc:0.975]
Epoch [72/120    avg_loss:0.010, val_acc:0.980]
Epoch [73/120    avg_loss:0.007, val_acc:0.984]
Epoch [74/120    avg_loss:0.008, val_acc:0.983]
Epoch [75/120    avg_loss:0.005, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.985]
Epoch [81/120    avg_loss:0.010, val_acc:0.984]
Epoch [82/120    avg_loss:0.005, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.985]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.005, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.004, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     4     0     0     0     0    11     1]
 [    0     1 18024     0     6     0    56     0     3     0]
 [    0     0     0  1982     0     0     0     0    53     1]
 [    0    29     1     0  2932     0     1     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4871     0     0     0]
 [    0    19     0     0     0     0     1  1268     0     2]
 [    0    41     0    71    13     0     0     0  3446     0]
 [    0     0     0     0     0    15     0     0     0   904]]

Accuracy:
99.16853445159424

F1 scores:
[       nan 0.99180708 0.99795139 0.96848278 0.99003883 0.99428571
 0.99337208 0.99139953 0.97207334 0.98797814]

Kappa:
0.9889872068823174
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f02236f2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.246, val_acc:0.421]
Epoch [2/120    avg_loss:0.653, val_acc:0.536]
Epoch [3/120    avg_loss:0.491, val_acc:0.772]
Epoch [4/120    avg_loss:0.483, val_acc:0.682]
Epoch [5/120    avg_loss:0.371, val_acc:0.819]
Epoch [6/120    avg_loss:0.272, val_acc:0.829]
Epoch [7/120    avg_loss:0.222, val_acc:0.776]
Epoch [8/120    avg_loss:0.236, val_acc:0.801]
Epoch [9/120    avg_loss:0.188, val_acc:0.892]
Epoch [10/120    avg_loss:0.212, val_acc:0.902]
Epoch [11/120    avg_loss:0.166, val_acc:0.899]
Epoch [12/120    avg_loss:0.153, val_acc:0.938]
Epoch [13/120    avg_loss:0.151, val_acc:0.935]
Epoch [14/120    avg_loss:0.099, val_acc:0.934]
Epoch [15/120    avg_loss:0.078, val_acc:0.965]
Epoch [16/120    avg_loss:0.102, val_acc:0.904]
Epoch [17/120    avg_loss:0.153, val_acc:0.963]
Epoch [18/120    avg_loss:0.093, val_acc:0.949]
Epoch [19/120    avg_loss:0.061, val_acc:0.959]
Epoch [20/120    avg_loss:0.050, val_acc:0.949]
Epoch [21/120    avg_loss:0.051, val_acc:0.967]
Epoch [22/120    avg_loss:0.061, val_acc:0.959]
Epoch [23/120    avg_loss:0.052, val_acc:0.975]
Epoch [24/120    avg_loss:0.077, val_acc:0.971]
Epoch [25/120    avg_loss:0.086, val_acc:0.924]
Epoch [26/120    avg_loss:0.116, val_acc:0.962]
Epoch [27/120    avg_loss:0.045, val_acc:0.970]
Epoch [28/120    avg_loss:0.059, val_acc:0.957]
Epoch [29/120    avg_loss:0.050, val_acc:0.968]
Epoch [30/120    avg_loss:0.042, val_acc:0.947]
Epoch [31/120    avg_loss:0.033, val_acc:0.974]
Epoch [32/120    avg_loss:0.029, val_acc:0.982]
Epoch [33/120    avg_loss:0.031, val_acc:0.965]
Epoch [34/120    avg_loss:0.051, val_acc:0.978]
Epoch [35/120    avg_loss:0.034, val_acc:0.981]
Epoch [36/120    avg_loss:0.023, val_acc:0.980]
Epoch [37/120    avg_loss:0.022, val_acc:0.970]
Epoch [38/120    avg_loss:0.026, val_acc:0.976]
Epoch [39/120    avg_loss:0.055, val_acc:0.971]
Epoch [40/120    avg_loss:0.039, val_acc:0.971]
Epoch [41/120    avg_loss:0.030, val_acc:0.975]
Epoch [42/120    avg_loss:0.031, val_acc:0.968]
Epoch [43/120    avg_loss:0.018, val_acc:0.984]
Epoch [44/120    avg_loss:0.017, val_acc:0.984]
Epoch [45/120    avg_loss:0.021, val_acc:0.980]
Epoch [46/120    avg_loss:0.025, val_acc:0.986]
Epoch [47/120    avg_loss:0.018, val_acc:0.982]
Epoch [48/120    avg_loss:0.015, val_acc:0.989]
Epoch [49/120    avg_loss:0.024, val_acc:0.986]
Epoch [50/120    avg_loss:0.011, val_acc:0.983]
Epoch [51/120    avg_loss:0.018, val_acc:0.985]
Epoch [52/120    avg_loss:0.054, val_acc:0.973]
Epoch [53/120    avg_loss:0.024, val_acc:0.983]
Epoch [54/120    avg_loss:0.023, val_acc:0.925]
Epoch [55/120    avg_loss:0.024, val_acc:0.976]
Epoch [56/120    avg_loss:0.016, val_acc:0.985]
Epoch [57/120    avg_loss:0.013, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.988]
Epoch [59/120    avg_loss:0.007, val_acc:0.988]
Epoch [60/120    avg_loss:0.016, val_acc:0.980]
Epoch [61/120    avg_loss:0.022, val_acc:0.983]
Epoch [62/120    avg_loss:0.015, val_acc:0.984]
Epoch [63/120    avg_loss:0.013, val_acc:0.985]
Epoch [64/120    avg_loss:0.007, val_acc:0.986]
Epoch [65/120    avg_loss:0.012, val_acc:0.986]
Epoch [66/120    avg_loss:0.010, val_acc:0.986]
Epoch [67/120    avg_loss:0.006, val_acc:0.986]
Epoch [68/120    avg_loss:0.007, val_acc:0.987]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.988]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.007, val_acc:0.988]
Epoch [74/120    avg_loss:0.004, val_acc:0.988]
Epoch [75/120    avg_loss:0.006, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.989]
Epoch [83/120    avg_loss:0.005, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.989]
Epoch [88/120    avg_loss:0.004, val_acc:0.988]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.989]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.989]
Epoch [105/120    avg_loss:0.007, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.009, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.011, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     6     0     0     3     2    21     0]
 [    0     1 18033     0     9     0    43     0     4     0]
 [    0     2     0  1987     0     0     0     0    47     0]
 [    0    13     1     2  2945     0     2     0     5     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     8     0     0     0     0     2  1272     0     8]
 [    0     7     0    43    35     0     0     0  3485     1]
 [    0     0     0     0     1     8     0     0     0   910]]

Accuracy:
99.32759742607188

F1 scores:
[       nan 0.99510223 0.99836678 0.9754541  0.98792352 0.99694423
 0.99479857 0.99219969 0.97714846 0.98805646]

Kappa:
0.9910953548902189
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f667829f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.437, val_acc:0.632]
Epoch [2/120    avg_loss:0.654, val_acc:0.598]
Epoch [3/120    avg_loss:0.527, val_acc:0.673]
Epoch [4/120    avg_loss:0.429, val_acc:0.711]
Epoch [5/120    avg_loss:0.343, val_acc:0.787]
Epoch [6/120    avg_loss:0.332, val_acc:0.828]
Epoch [7/120    avg_loss:0.271, val_acc:0.899]
Epoch [8/120    avg_loss:0.230, val_acc:0.843]
Epoch [9/120    avg_loss:0.208, val_acc:0.908]
Epoch [10/120    avg_loss:0.173, val_acc:0.933]
Epoch [11/120    avg_loss:0.136, val_acc:0.900]
Epoch [12/120    avg_loss:0.187, val_acc:0.856]
Epoch [13/120    avg_loss:0.130, val_acc:0.949]
Epoch [14/120    avg_loss:0.127, val_acc:0.931]
Epoch [15/120    avg_loss:0.130, val_acc:0.913]
Epoch [16/120    avg_loss:0.139, val_acc:0.950]
Epoch [17/120    avg_loss:0.113, val_acc:0.937]
Epoch [18/120    avg_loss:0.119, val_acc:0.957]
Epoch [19/120    avg_loss:0.090, val_acc:0.947]
Epoch [20/120    avg_loss:0.093, val_acc:0.932]
Epoch [21/120    avg_loss:0.071, val_acc:0.954]
Epoch [22/120    avg_loss:0.084, val_acc:0.958]
Epoch [23/120    avg_loss:0.060, val_acc:0.956]
Epoch [24/120    avg_loss:0.056, val_acc:0.976]
Epoch [25/120    avg_loss:0.040, val_acc:0.966]
Epoch [26/120    avg_loss:0.056, val_acc:0.961]
Epoch [27/120    avg_loss:0.051, val_acc:0.964]
Epoch [28/120    avg_loss:0.047, val_acc:0.951]
Epoch [29/120    avg_loss:0.063, val_acc:0.972]
Epoch [30/120    avg_loss:0.035, val_acc:0.973]
Epoch [31/120    avg_loss:0.026, val_acc:0.970]
Epoch [32/120    avg_loss:0.030, val_acc:0.978]
Epoch [33/120    avg_loss:0.026, val_acc:0.973]
Epoch [34/120    avg_loss:0.031, val_acc:0.962]
Epoch [35/120    avg_loss:0.026, val_acc:0.976]
Epoch [36/120    avg_loss:0.029, val_acc:0.956]
Epoch [37/120    avg_loss:0.045, val_acc:0.979]
Epoch [38/120    avg_loss:0.034, val_acc:0.973]
Epoch [39/120    avg_loss:0.032, val_acc:0.975]
Epoch [40/120    avg_loss:0.022, val_acc:0.977]
Epoch [41/120    avg_loss:0.020, val_acc:0.982]
Epoch [42/120    avg_loss:0.020, val_acc:0.972]
Epoch [43/120    avg_loss:0.062, val_acc:0.973]
Epoch [44/120    avg_loss:0.031, val_acc:0.970]
Epoch [45/120    avg_loss:0.036, val_acc:0.970]
Epoch [46/120    avg_loss:0.023, val_acc:0.976]
Epoch [47/120    avg_loss:0.014, val_acc:0.982]
Epoch [48/120    avg_loss:0.017, val_acc:0.978]
Epoch [49/120    avg_loss:0.022, val_acc:0.975]
Epoch [50/120    avg_loss:0.019, val_acc:0.982]
Epoch [51/120    avg_loss:0.009, val_acc:0.986]
Epoch [52/120    avg_loss:0.007, val_acc:0.980]
Epoch [53/120    avg_loss:0.022, val_acc:0.983]
Epoch [54/120    avg_loss:0.029, val_acc:0.976]
Epoch [55/120    avg_loss:0.015, val_acc:0.982]
Epoch [56/120    avg_loss:0.016, val_acc:0.979]
Epoch [57/120    avg_loss:0.018, val_acc:0.981]
Epoch [58/120    avg_loss:0.018, val_acc:0.983]
Epoch [59/120    avg_loss:0.028, val_acc:0.974]
Epoch [60/120    avg_loss:0.018, val_acc:0.983]
Epoch [61/120    avg_loss:0.016, val_acc:0.958]
Epoch [62/120    avg_loss:0.012, val_acc:0.986]
Epoch [63/120    avg_loss:0.018, val_acc:0.975]
Epoch [64/120    avg_loss:0.014, val_acc:0.983]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.010, val_acc:0.983]
Epoch [67/120    avg_loss:0.012, val_acc:0.986]
Epoch [68/120    avg_loss:0.011, val_acc:0.968]
Epoch [69/120    avg_loss:0.009, val_acc:0.988]
Epoch [70/120    avg_loss:0.009, val_acc:0.980]
Epoch [71/120    avg_loss:0.012, val_acc:0.988]
Epoch [72/120    avg_loss:0.006, val_acc:0.981]
Epoch [73/120    avg_loss:0.010, val_acc:0.983]
Epoch [74/120    avg_loss:0.014, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.988]
Epoch [77/120    avg_loss:0.031, val_acc:0.974]
Epoch [78/120    avg_loss:0.144, val_acc:0.934]
Epoch [79/120    avg_loss:0.132, val_acc:0.963]
Epoch [80/120    avg_loss:0.048, val_acc:0.980]
Epoch [81/120    avg_loss:0.034, val_acc:0.979]
Epoch [82/120    avg_loss:0.021, val_acc:0.984]
Epoch [83/120    avg_loss:0.021, val_acc:0.957]
Epoch [84/120    avg_loss:0.030, val_acc:0.969]
Epoch [85/120    avg_loss:0.026, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.988]
Epoch [87/120    avg_loss:0.016, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.972]
Epoch [89/120    avg_loss:0.018, val_acc:0.984]
Epoch [90/120    avg_loss:0.017, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.017, val_acc:0.982]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.987]
Epoch [98/120    avg_loss:0.014, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.003, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.003, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     3     0     0     0     0    12     1]
 [    0     0 18033     0     4     0    49     0     4     0]
 [    0     6     0  1959     0     0     0     0    70     1]
 [    0    17     4     0  2942     0     2     0     4     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4875     0     1     0]
 [    0    18     0     0     0     0     3  1266     0     3]
 [    0    16     0    46    29     0     0     0  3480     0]
 [    0     0     0     0     0     9     0     0     0   910]]

Accuracy:
99.26011616417227

F1 scores:
[       nan 0.99434328 0.99825625 0.96884273 0.98940642 0.99656357
 0.99418783 0.99061033 0.97451694 0.99074578]

Kappa:
0.9901997001120088
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f444fdb9710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.223, val_acc:0.569]
Epoch [2/120    avg_loss:0.626, val_acc:0.559]
Epoch [3/120    avg_loss:0.471, val_acc:0.577]
Epoch [4/120    avg_loss:0.414, val_acc:0.780]
Epoch [5/120    avg_loss:0.349, val_acc:0.808]
Epoch [6/120    avg_loss:0.315, val_acc:0.683]
Epoch [7/120    avg_loss:0.252, val_acc:0.813]
Epoch [8/120    avg_loss:0.226, val_acc:0.871]
Epoch [9/120    avg_loss:0.221, val_acc:0.894]
Epoch [10/120    avg_loss:0.173, val_acc:0.923]
Epoch [11/120    avg_loss:0.165, val_acc:0.906]
Epoch [12/120    avg_loss:0.116, val_acc:0.926]
Epoch [13/120    avg_loss:0.115, val_acc:0.873]
Epoch [14/120    avg_loss:0.113, val_acc:0.917]
Epoch [15/120    avg_loss:0.111, val_acc:0.937]
Epoch [16/120    avg_loss:0.088, val_acc:0.935]
Epoch [17/120    avg_loss:0.114, val_acc:0.931]
Epoch [18/120    avg_loss:0.079, val_acc:0.943]
Epoch [19/120    avg_loss:0.114, val_acc:0.932]
Epoch [20/120    avg_loss:0.096, val_acc:0.926]
Epoch [21/120    avg_loss:0.067, val_acc:0.940]
Epoch [22/120    avg_loss:0.085, val_acc:0.940]
Epoch [23/120    avg_loss:0.072, val_acc:0.926]
Epoch [24/120    avg_loss:0.065, val_acc:0.961]
Epoch [25/120    avg_loss:0.044, val_acc:0.957]
Epoch [26/120    avg_loss:0.080, val_acc:0.928]
Epoch [27/120    avg_loss:0.110, val_acc:0.944]
Epoch [28/120    avg_loss:0.075, val_acc:0.953]
Epoch [29/120    avg_loss:0.134, val_acc:0.944]
Epoch [30/120    avg_loss:0.171, val_acc:0.851]
Epoch [31/120    avg_loss:0.054, val_acc:0.963]
Epoch [32/120    avg_loss:0.048, val_acc:0.963]
Epoch [33/120    avg_loss:0.038, val_acc:0.962]
Epoch [34/120    avg_loss:0.029, val_acc:0.958]
Epoch [35/120    avg_loss:0.040, val_acc:0.953]
Epoch [36/120    avg_loss:0.033, val_acc:0.971]
Epoch [37/120    avg_loss:0.028, val_acc:0.966]
Epoch [38/120    avg_loss:0.052, val_acc:0.960]
Epoch [39/120    avg_loss:0.046, val_acc:0.972]
Epoch [40/120    avg_loss:0.023, val_acc:0.969]
Epoch [41/120    avg_loss:0.031, val_acc:0.964]
Epoch [42/120    avg_loss:0.022, val_acc:0.969]
Epoch [43/120    avg_loss:0.016, val_acc:0.965]
Epoch [44/120    avg_loss:0.014, val_acc:0.956]
Epoch [45/120    avg_loss:0.022, val_acc:0.968]
Epoch [46/120    avg_loss:0.029, val_acc:0.958]
Epoch [47/120    avg_loss:0.022, val_acc:0.968]
Epoch [48/120    avg_loss:0.013, val_acc:0.966]
Epoch [49/120    avg_loss:0.015, val_acc:0.970]
Epoch [50/120    avg_loss:0.040, val_acc:0.974]
Epoch [51/120    avg_loss:0.016, val_acc:0.971]
Epoch [52/120    avg_loss:0.025, val_acc:0.957]
Epoch [53/120    avg_loss:0.020, val_acc:0.971]
Epoch [54/120    avg_loss:0.024, val_acc:0.975]
Epoch [55/120    avg_loss:0.011, val_acc:0.978]
Epoch [56/120    avg_loss:0.007, val_acc:0.978]
Epoch [57/120    avg_loss:0.009, val_acc:0.975]
Epoch [58/120    avg_loss:0.007, val_acc:0.969]
Epoch [59/120    avg_loss:0.008, val_acc:0.975]
Epoch [60/120    avg_loss:0.014, val_acc:0.975]
Epoch [61/120    avg_loss:0.013, val_acc:0.974]
Epoch [62/120    avg_loss:0.009, val_acc:0.972]
Epoch [63/120    avg_loss:0.024, val_acc:0.960]
Epoch [64/120    avg_loss:0.025, val_acc:0.973]
Epoch [65/120    avg_loss:0.013, val_acc:0.978]
Epoch [66/120    avg_loss:0.008, val_acc:0.978]
Epoch [67/120    avg_loss:0.011, val_acc:0.975]
Epoch [68/120    avg_loss:0.015, val_acc:0.976]
Epoch [69/120    avg_loss:0.009, val_acc:0.974]
Epoch [70/120    avg_loss:0.009, val_acc:0.971]
Epoch [71/120    avg_loss:0.023, val_acc:0.968]
Epoch [72/120    avg_loss:0.050, val_acc:0.962]
Epoch [73/120    avg_loss:0.033, val_acc:0.970]
Epoch [74/120    avg_loss:0.033, val_acc:0.971]
Epoch [75/120    avg_loss:0.020, val_acc:0.975]
Epoch [76/120    avg_loss:0.011, val_acc:0.971]
Epoch [77/120    avg_loss:0.008, val_acc:0.979]
Epoch [78/120    avg_loss:0.006, val_acc:0.977]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.006, val_acc:0.975]
Epoch [81/120    avg_loss:0.004, val_acc:0.975]
Epoch [82/120    avg_loss:0.005, val_acc:0.974]
Epoch [83/120    avg_loss:0.007, val_acc:0.974]
Epoch [84/120    avg_loss:0.007, val_acc:0.978]
Epoch [85/120    avg_loss:0.005, val_acc:0.980]
Epoch [86/120    avg_loss:0.010, val_acc:0.979]
Epoch [87/120    avg_loss:0.009, val_acc:0.978]
Epoch [88/120    avg_loss:0.005, val_acc:0.977]
Epoch [89/120    avg_loss:0.008, val_acc:0.980]
Epoch [90/120    avg_loss:0.005, val_acc:0.980]
Epoch [91/120    avg_loss:0.004, val_acc:0.981]
Epoch [92/120    avg_loss:0.011, val_acc:0.974]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.004, val_acc:0.980]
Epoch [96/120    avg_loss:0.004, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.982]
Epoch [98/120    avg_loss:0.045, val_acc:0.974]
Epoch [99/120    avg_loss:0.013, val_acc:0.975]
Epoch [100/120    avg_loss:0.004, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.971]
Epoch [102/120    avg_loss:0.011, val_acc:0.973]
Epoch [103/120    avg_loss:0.008, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.971]
Epoch [105/120    avg_loss:0.010, val_acc:0.976]
Epoch [106/120    avg_loss:0.003, val_acc:0.979]
Epoch [107/120    avg_loss:0.006, val_acc:0.972]
Epoch [108/120    avg_loss:0.005, val_acc:0.978]
Epoch [109/120    avg_loss:0.003, val_acc:0.978]
Epoch [110/120    avg_loss:0.004, val_acc:0.979]
Epoch [111/120    avg_loss:0.005, val_acc:0.980]
Epoch [112/120    avg_loss:0.007, val_acc:0.979]
Epoch [113/120    avg_loss:0.006, val_acc:0.978]
Epoch [114/120    avg_loss:0.005, val_acc:0.978]
Epoch [115/120    avg_loss:0.002, val_acc:0.979]
Epoch [116/120    avg_loss:0.002, val_acc:0.981]
Epoch [117/120    avg_loss:0.005, val_acc:0.982]
Epoch [118/120    avg_loss:0.001, val_acc:0.982]
Epoch [119/120    avg_loss:0.003, val_acc:0.981]
Epoch [120/120    avg_loss:0.004, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     1     0     0     0     1     5     0]
 [    0     0 17886     0     8     0   192     0     4     0]
 [    0     0     0  1967     0     0     0     0    68     1]
 [    0    12     3     0  2951     0     3     0     1     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     9     0     0     0     0     0  1281     0     0]
 [    0    19     0    80    35     0     2     0  3435     0]
 [    0     0     0     0     0    11     0     0     0   908]]

Accuracy:
98.89860940399586

F1 scores:
[       nan 0.99635574 0.99424664 0.9632713  0.98927254 0.99580313
 0.98020697 0.99611198 0.96979108 0.99234973]

Kappa:
0.9854348698571244
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8522e287f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.208, val_acc:0.658]
Epoch [2/120    avg_loss:0.619, val_acc:0.695]
Epoch [3/120    avg_loss:0.549, val_acc:0.642]
Epoch [4/120    avg_loss:0.405, val_acc:0.785]
Epoch [5/120    avg_loss:0.332, val_acc:0.843]
Epoch [6/120    avg_loss:0.335, val_acc:0.823]
Epoch [7/120    avg_loss:0.261, val_acc:0.821]
Epoch [8/120    avg_loss:0.293, val_acc:0.888]
Epoch [9/120    avg_loss:0.209, val_acc:0.925]
Epoch [10/120    avg_loss:0.175, val_acc:0.911]
Epoch [11/120    avg_loss:0.147, val_acc:0.936]
Epoch [12/120    avg_loss:0.107, val_acc:0.941]
Epoch [13/120    avg_loss:0.153, val_acc:0.934]
Epoch [14/120    avg_loss:0.098, val_acc:0.902]
Epoch [15/120    avg_loss:0.183, val_acc:0.927]
Epoch [16/120    avg_loss:0.110, val_acc:0.929]
Epoch [17/120    avg_loss:0.084, val_acc:0.911]
Epoch [18/120    avg_loss:0.071, val_acc:0.933]
Epoch [19/120    avg_loss:0.071, val_acc:0.960]
Epoch [20/120    avg_loss:0.070, val_acc:0.925]
Epoch [21/120    avg_loss:0.149, val_acc:0.935]
Epoch [22/120    avg_loss:0.096, val_acc:0.922]
Epoch [23/120    avg_loss:0.082, val_acc:0.948]
Epoch [24/120    avg_loss:0.063, val_acc:0.959]
Epoch [25/120    avg_loss:0.050, val_acc:0.963]
Epoch [26/120    avg_loss:0.059, val_acc:0.944]
Epoch [27/120    avg_loss:0.058, val_acc:0.958]
Epoch [28/120    avg_loss:0.039, val_acc:0.938]
Epoch [29/120    avg_loss:0.051, val_acc:0.958]
Epoch [30/120    avg_loss:0.033, val_acc:0.969]
Epoch [31/120    avg_loss:0.044, val_acc:0.958]
Epoch [32/120    avg_loss:0.043, val_acc:0.959]
Epoch [33/120    avg_loss:0.043, val_acc:0.973]
Epoch [34/120    avg_loss:0.026, val_acc:0.975]
Epoch [35/120    avg_loss:0.049, val_acc:0.961]
Epoch [36/120    avg_loss:0.028, val_acc:0.978]
Epoch [37/120    avg_loss:0.024, val_acc:0.978]
Epoch [38/120    avg_loss:0.021, val_acc:0.967]
Epoch [39/120    avg_loss:0.026, val_acc:0.974]
Epoch [40/120    avg_loss:0.016, val_acc:0.981]
Epoch [41/120    avg_loss:0.016, val_acc:0.978]
Epoch [42/120    avg_loss:0.015, val_acc:0.973]
Epoch [43/120    avg_loss:0.025, val_acc:0.977]
Epoch [44/120    avg_loss:0.020, val_acc:0.975]
Epoch [45/120    avg_loss:0.029, val_acc:0.956]
Epoch [46/120    avg_loss:0.082, val_acc:0.960]
Epoch [47/120    avg_loss:0.029, val_acc:0.967]
Epoch [48/120    avg_loss:0.028, val_acc:0.978]
Epoch [49/120    avg_loss:0.030, val_acc:0.945]
Epoch [50/120    avg_loss:0.022, val_acc:0.979]
Epoch [51/120    avg_loss:0.015, val_acc:0.978]
Epoch [52/120    avg_loss:0.048, val_acc:0.966]
Epoch [53/120    avg_loss:0.020, val_acc:0.973]
Epoch [54/120    avg_loss:0.012, val_acc:0.973]
Epoch [55/120    avg_loss:0.010, val_acc:0.977]
Epoch [56/120    avg_loss:0.020, val_acc:0.981]
Epoch [57/120    avg_loss:0.013, val_acc:0.980]
Epoch [58/120    avg_loss:0.016, val_acc:0.983]
Epoch [59/120    avg_loss:0.008, val_acc:0.981]
Epoch [60/120    avg_loss:0.009, val_acc:0.982]
Epoch [61/120    avg_loss:0.010, val_acc:0.981]
Epoch [62/120    avg_loss:0.006, val_acc:0.980]
Epoch [63/120    avg_loss:0.009, val_acc:0.978]
Epoch [64/120    avg_loss:0.010, val_acc:0.980]
Epoch [65/120    avg_loss:0.012, val_acc:0.982]
Epoch [66/120    avg_loss:0.008, val_acc:0.980]
Epoch [67/120    avg_loss:0.009, val_acc:0.979]
Epoch [68/120    avg_loss:0.014, val_acc:0.981]
Epoch [69/120    avg_loss:0.009, val_acc:0.981]
Epoch [70/120    avg_loss:0.006, val_acc:0.979]
Epoch [71/120    avg_loss:0.008, val_acc:0.981]
Epoch [72/120    avg_loss:0.008, val_acc:0.981]
Epoch [73/120    avg_loss:0.011, val_acc:0.981]
Epoch [74/120    avg_loss:0.009, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.981]
Epoch [76/120    avg_loss:0.013, val_acc:0.981]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.009, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.980]
Epoch [80/120    avg_loss:0.009, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.012, val_acc:0.980]
Epoch [83/120    avg_loss:0.009, val_acc:0.980]
Epoch [84/120    avg_loss:0.006, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.011, val_acc:0.980]
Epoch [87/120    avg_loss:0.008, val_acc:0.980]
Epoch [88/120    avg_loss:0.007, val_acc:0.980]
Epoch [89/120    avg_loss:0.007, val_acc:0.980]
Epoch [90/120    avg_loss:0.010, val_acc:0.980]
Epoch [91/120    avg_loss:0.009, val_acc:0.980]
Epoch [92/120    avg_loss:0.009, val_acc:0.980]
Epoch [93/120    avg_loss:0.017, val_acc:0.980]
Epoch [94/120    avg_loss:0.011, val_acc:0.980]
Epoch [95/120    avg_loss:0.007, val_acc:0.980]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.010, val_acc:0.980]
Epoch [98/120    avg_loss:0.012, val_acc:0.980]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.016, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.011, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.980]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.980]
Epoch [107/120    avg_loss:0.006, val_acc:0.980]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.013, val_acc:0.980]
Epoch [111/120    avg_loss:0.009, val_acc:0.980]
Epoch [112/120    avg_loss:0.007, val_acc:0.980]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.008, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.008, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     5     0     0    10     0    23     7]
 [    0     4 17918     0    18     0   141     0     9     0]
 [    0     0     0  1959     0     0     0     0    75     2]
 [    0    17     3     1  2935     0     4     0     4     8]
 [    0     0     0     2     0  1302     0     0     0     1]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    12     0     0     0     0     2  1276     0     0]
 [    0     3     0    77    38     0     0     0  3453     0]
 [    0     0     0     0     0    11     0     0     0   908]]

Accuracy:
98.850408502639

F1 scores:
[       nan 0.99369895 0.99514037 0.96029412 0.98440382 0.99465241
 0.98416221 0.99454404 0.9679047  0.98428184]

Kappa:
0.9847941664627906
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f05c853a860>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.278, val_acc:0.726]
Epoch [2/120    avg_loss:0.694, val_acc:0.542]
Epoch [3/120    avg_loss:0.511, val_acc:0.758]
Epoch [4/120    avg_loss:0.465, val_acc:0.767]
Epoch [5/120    avg_loss:0.329, val_acc:0.879]
Epoch [6/120    avg_loss:0.258, val_acc:0.916]
Epoch [7/120    avg_loss:0.246, val_acc:0.924]
Epoch [8/120    avg_loss:0.174, val_acc:0.868]
Epoch [9/120    avg_loss:0.182, val_acc:0.932]
Epoch [10/120    avg_loss:0.129, val_acc:0.944]
Epoch [11/120    avg_loss:0.149, val_acc:0.875]
Epoch [12/120    avg_loss:0.134, val_acc:0.935]
Epoch [13/120    avg_loss:0.150, val_acc:0.921]
Epoch [14/120    avg_loss:0.109, val_acc:0.949]
Epoch [15/120    avg_loss:0.105, val_acc:0.961]
Epoch [16/120    avg_loss:0.076, val_acc:0.952]
Epoch [17/120    avg_loss:0.100, val_acc:0.938]
Epoch [18/120    avg_loss:0.109, val_acc:0.949]
Epoch [19/120    avg_loss:0.083, val_acc:0.947]
Epoch [20/120    avg_loss:0.066, val_acc:0.966]
Epoch [21/120    avg_loss:0.046, val_acc:0.953]
Epoch [22/120    avg_loss:0.057, val_acc:0.974]
Epoch [23/120    avg_loss:0.065, val_acc:0.963]
Epoch [24/120    avg_loss:0.096, val_acc:0.954]
Epoch [25/120    avg_loss:0.058, val_acc:0.973]
Epoch [26/120    avg_loss:0.034, val_acc:0.975]
Epoch [27/120    avg_loss:0.031, val_acc:0.980]
Epoch [28/120    avg_loss:0.027, val_acc:0.976]
Epoch [29/120    avg_loss:0.098, val_acc:0.958]
Epoch [30/120    avg_loss:0.047, val_acc:0.967]
Epoch [31/120    avg_loss:0.023, val_acc:0.979]
Epoch [32/120    avg_loss:0.036, val_acc:0.955]
Epoch [33/120    avg_loss:0.038, val_acc:0.976]
Epoch [34/120    avg_loss:0.091, val_acc:0.817]
Epoch [35/120    avg_loss:0.280, val_acc:0.907]
Epoch [36/120    avg_loss:0.088, val_acc:0.963]
Epoch [37/120    avg_loss:0.069, val_acc:0.958]
Epoch [38/120    avg_loss:0.112, val_acc:0.943]
Epoch [39/120    avg_loss:0.061, val_acc:0.961]
Epoch [40/120    avg_loss:0.034, val_acc:0.980]
Epoch [41/120    avg_loss:0.032, val_acc:0.983]
Epoch [42/120    avg_loss:0.041, val_acc:0.960]
Epoch [43/120    avg_loss:0.038, val_acc:0.954]
Epoch [44/120    avg_loss:0.056, val_acc:0.968]
Epoch [45/120    avg_loss:0.038, val_acc:0.984]
Epoch [46/120    avg_loss:0.038, val_acc:0.973]
Epoch [47/120    avg_loss:0.036, val_acc:0.973]
Epoch [48/120    avg_loss:0.024, val_acc:0.976]
Epoch [49/120    avg_loss:0.038, val_acc:0.943]
Epoch [50/120    avg_loss:0.043, val_acc:0.980]
Epoch [51/120    avg_loss:0.023, val_acc:0.981]
Epoch [52/120    avg_loss:0.015, val_acc:0.986]
Epoch [53/120    avg_loss:0.013, val_acc:0.986]
Epoch [54/120    avg_loss:0.009, val_acc:0.988]
Epoch [55/120    avg_loss:0.014, val_acc:0.980]
Epoch [56/120    avg_loss:0.016, val_acc:0.978]
Epoch [57/120    avg_loss:0.008, val_acc:0.989]
Epoch [58/120    avg_loss:0.012, val_acc:0.987]
Epoch [59/120    avg_loss:0.008, val_acc:0.987]
Epoch [60/120    avg_loss:0.009, val_acc:0.974]
Epoch [61/120    avg_loss:0.015, val_acc:0.988]
Epoch [62/120    avg_loss:0.011, val_acc:0.986]
Epoch [63/120    avg_loss:0.024, val_acc:0.988]
Epoch [64/120    avg_loss:0.023, val_acc:0.979]
Epoch [65/120    avg_loss:0.016, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.986]
Epoch [67/120    avg_loss:0.016, val_acc:0.987]
Epoch [68/120    avg_loss:0.017, val_acc:0.988]
Epoch [69/120    avg_loss:0.013, val_acc:0.980]
Epoch [70/120    avg_loss:0.014, val_acc:0.987]
Epoch [71/120    avg_loss:0.015, val_acc:0.987]
Epoch [72/120    avg_loss:0.010, val_acc:0.986]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.006, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.987]
Epoch [76/120    avg_loss:0.006, val_acc:0.987]
Epoch [77/120    avg_loss:0.007, val_acc:0.988]
Epoch [78/120    avg_loss:0.004, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.003, val_acc:0.988]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.004, val_acc:0.987]
Epoch [86/120    avg_loss:0.005, val_acc:0.987]
Epoch [87/120    avg_loss:0.004, val_acc:0.987]
Epoch [88/120    avg_loss:0.004, val_acc:0.987]
Epoch [89/120    avg_loss:0.005, val_acc:0.987]
Epoch [90/120    avg_loss:0.012, val_acc:0.987]
Epoch [91/120    avg_loss:0.013, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.016, val_acc:0.989]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.012, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.010, val_acc:0.989]
Epoch [109/120    avg_loss:0.009, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     9     0     0     0    12    20     1]
 [    0     0 17988     0    10     0    92     0     0     0]
 [    0     0     0  1978     0     0     0     0    57     1]
 [    0    15     2     4  2942     0     0     0     7     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4870     0     0     0]
 [    0     4     0     0     0     0     0  1286     0     0]
 [    0    20     0    67    20     0     0     0  3462     2]
 [    0     1     0     0     0     6     0     0     0   912]]

Accuracy:
99.1323837755766

F1 scores:
[       nan 0.99362463 0.99689648 0.96629213 0.98990579 0.99770642
 0.9898374  0.99381762 0.97288183 0.99292324]

Kappa:
0.9885150046692229
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8822e2a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.288, val_acc:0.661]
Epoch [2/120    avg_loss:0.643, val_acc:0.695]
Epoch [3/120    avg_loss:0.497, val_acc:0.655]
Epoch [4/120    avg_loss:0.422, val_acc:0.653]
Epoch [5/120    avg_loss:0.387, val_acc:0.787]
Epoch [6/120    avg_loss:0.301, val_acc:0.786]
Epoch [7/120    avg_loss:0.221, val_acc:0.912]
Epoch [8/120    avg_loss:0.184, val_acc:0.886]
Epoch [9/120    avg_loss:0.234, val_acc:0.892]
Epoch [10/120    avg_loss:0.141, val_acc:0.946]
Epoch [11/120    avg_loss:0.135, val_acc:0.938]
Epoch [12/120    avg_loss:0.135, val_acc:0.918]
Epoch [13/120    avg_loss:0.109, val_acc:0.934]
Epoch [14/120    avg_loss:0.118, val_acc:0.908]
Epoch [15/120    avg_loss:0.107, val_acc:0.943]
Epoch [16/120    avg_loss:0.115, val_acc:0.917]
Epoch [17/120    avg_loss:0.100, val_acc:0.941]
Epoch [18/120    avg_loss:0.108, val_acc:0.944]
Epoch [19/120    avg_loss:0.082, val_acc:0.952]
Epoch [20/120    avg_loss:0.068, val_acc:0.968]
Epoch [21/120    avg_loss:0.044, val_acc:0.967]
Epoch [22/120    avg_loss:0.032, val_acc:0.924]
Epoch [23/120    avg_loss:0.048, val_acc:0.963]
Epoch [24/120    avg_loss:0.036, val_acc:0.973]
Epoch [25/120    avg_loss:0.062, val_acc:0.951]
Epoch [26/120    avg_loss:0.081, val_acc:0.968]
Epoch [27/120    avg_loss:0.041, val_acc:0.963]
Epoch [28/120    avg_loss:0.054, val_acc:0.963]
Epoch [29/120    avg_loss:0.031, val_acc:0.978]
Epoch [30/120    avg_loss:0.023, val_acc:0.978]
Epoch [31/120    avg_loss:0.037, val_acc:0.969]
Epoch [32/120    avg_loss:0.029, val_acc:0.972]
Epoch [33/120    avg_loss:0.022, val_acc:0.978]
Epoch [34/120    avg_loss:0.022, val_acc:0.970]
Epoch [35/120    avg_loss:0.018, val_acc:0.978]
Epoch [36/120    avg_loss:0.011, val_acc:0.976]
Epoch [37/120    avg_loss:0.062, val_acc:0.963]
Epoch [38/120    avg_loss:0.041, val_acc:0.968]
Epoch [39/120    avg_loss:0.084, val_acc:0.958]
Epoch [40/120    avg_loss:0.044, val_acc:0.977]
Epoch [41/120    avg_loss:0.028, val_acc:0.973]
Epoch [42/120    avg_loss:0.061, val_acc:0.963]
Epoch [43/120    avg_loss:0.067, val_acc:0.973]
Epoch [44/120    avg_loss:0.025, val_acc:0.974]
Epoch [45/120    avg_loss:0.020, val_acc:0.975]
Epoch [46/120    avg_loss:0.018, val_acc:0.978]
Epoch [47/120    avg_loss:0.012, val_acc:0.979]
Epoch [48/120    avg_loss:0.020, val_acc:0.982]
Epoch [49/120    avg_loss:0.013, val_acc:0.979]
Epoch [50/120    avg_loss:0.012, val_acc:0.980]
Epoch [51/120    avg_loss:0.014, val_acc:0.983]
Epoch [52/120    avg_loss:0.015, val_acc:0.983]
Epoch [53/120    avg_loss:0.014, val_acc:0.982]
Epoch [54/120    avg_loss:0.017, val_acc:0.983]
Epoch [55/120    avg_loss:0.013, val_acc:0.985]
Epoch [56/120    avg_loss:0.016, val_acc:0.984]
Epoch [57/120    avg_loss:0.011, val_acc:0.986]
Epoch [58/120    avg_loss:0.008, val_acc:0.984]
Epoch [59/120    avg_loss:0.020, val_acc:0.983]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.008, val_acc:0.984]
Epoch [62/120    avg_loss:0.011, val_acc:0.983]
Epoch [63/120    avg_loss:0.011, val_acc:0.983]
Epoch [64/120    avg_loss:0.011, val_acc:0.983]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.011, val_acc:0.984]
Epoch [67/120    avg_loss:0.011, val_acc:0.983]
Epoch [68/120    avg_loss:0.010, val_acc:0.984]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.010, val_acc:0.984]
Epoch [72/120    avg_loss:0.014, val_acc:0.984]
Epoch [73/120    avg_loss:0.013, val_acc:0.984]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.015, val_acc:0.984]
Epoch [77/120    avg_loss:0.012, val_acc:0.984]
Epoch [78/120    avg_loss:0.012, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.015, val_acc:0.984]
Epoch [82/120    avg_loss:0.013, val_acc:0.985]
Epoch [83/120    avg_loss:0.014, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.013, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.020, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.012, val_acc:0.985]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.985]
Epoch [108/120    avg_loss:0.012, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.014, val_acc:0.985]
Epoch [113/120    avg_loss:0.010, val_acc:0.985]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.013, val_acc:0.985]
Epoch [117/120    avg_loss:0.016, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.016, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6336     0     8     0     0     3    12    52    21]
 [    0     0 17995     0    22     0    65     0     8     0]
 [    0     0     0  1963     0     0     0     0    70     3]
 [    0    23     0     2  2934     0     2     0     5     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0     5     0     0     0     0     3  1277     0     5]
 [    0     7     0    60    43     0     0     0  3461     0]
 [    0     0     0     0     0    15     0     0     0   904]]

Accuracy:
98.9347600800135

F1 scores:
[       nan 0.98976802 0.99736733 0.96485623 0.98274996 0.99428571
 0.99236797 0.99030632 0.9655461  0.97308934]

Kappa:
0.9859018255599798
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9231115780>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.300, val_acc:0.682]
Epoch [2/120    avg_loss:0.664, val_acc:0.556]
Epoch [3/120    avg_loss:0.479, val_acc:0.771]
Epoch [4/120    avg_loss:0.340, val_acc:0.836]
Epoch [5/120    avg_loss:0.276, val_acc:0.817]
Epoch [6/120    avg_loss:0.261, val_acc:0.874]
Epoch [7/120    avg_loss:0.235, val_acc:0.919]
Epoch [8/120    avg_loss:0.156, val_acc:0.897]
Epoch [9/120    avg_loss:0.188, val_acc:0.845]
Epoch [10/120    avg_loss:0.154, val_acc:0.926]
Epoch [11/120    avg_loss:0.126, val_acc:0.931]
Epoch [12/120    avg_loss:0.115, val_acc:0.938]
Epoch [13/120    avg_loss:0.122, val_acc:0.927]
Epoch [14/120    avg_loss:0.123, val_acc:0.956]
Epoch [15/120    avg_loss:0.102, val_acc:0.954]
Epoch [16/120    avg_loss:0.082, val_acc:0.932]
Epoch [17/120    avg_loss:0.057, val_acc:0.953]
Epoch [18/120    avg_loss:0.063, val_acc:0.943]
Epoch [19/120    avg_loss:0.073, val_acc:0.965]
Epoch [20/120    avg_loss:0.045, val_acc:0.964]
Epoch [21/120    avg_loss:0.045, val_acc:0.957]
Epoch [22/120    avg_loss:0.075, val_acc:0.957]
Epoch [23/120    avg_loss:0.061, val_acc:0.955]
Epoch [24/120    avg_loss:0.025, val_acc:0.969]
Epoch [25/120    avg_loss:0.030, val_acc:0.973]
Epoch [26/120    avg_loss:0.027, val_acc:0.972]
Epoch [27/120    avg_loss:0.031, val_acc:0.966]
Epoch [28/120    avg_loss:0.052, val_acc:0.960]
Epoch [29/120    avg_loss:0.026, val_acc:0.976]
Epoch [30/120    avg_loss:0.028, val_acc:0.972]
Epoch [31/120    avg_loss:0.030, val_acc:0.975]
Epoch [32/120    avg_loss:0.023, val_acc:0.971]
Epoch [33/120    avg_loss:0.015, val_acc:0.969]
Epoch [34/120    avg_loss:0.020, val_acc:0.980]
Epoch [35/120    avg_loss:0.043, val_acc:0.959]
Epoch [36/120    avg_loss:0.076, val_acc:0.975]
Epoch [37/120    avg_loss:0.029, val_acc:0.970]
Epoch [38/120    avg_loss:0.023, val_acc:0.976]
Epoch [39/120    avg_loss:0.043, val_acc:0.975]
Epoch [40/120    avg_loss:0.023, val_acc:0.975]
Epoch [41/120    avg_loss:0.021, val_acc:0.975]
Epoch [42/120    avg_loss:0.017, val_acc:0.965]
Epoch [43/120    avg_loss:0.010, val_acc:0.978]
Epoch [44/120    avg_loss:0.012, val_acc:0.978]
Epoch [45/120    avg_loss:0.010, val_acc:0.982]
Epoch [46/120    avg_loss:0.013, val_acc:0.968]
Epoch [47/120    avg_loss:0.009, val_acc:0.979]
Epoch [48/120    avg_loss:0.011, val_acc:0.973]
Epoch [49/120    avg_loss:0.027, val_acc:0.956]
Epoch [50/120    avg_loss:0.018, val_acc:0.951]
Epoch [51/120    avg_loss:0.012, val_acc:0.977]
Epoch [52/120    avg_loss:0.012, val_acc:0.976]
Epoch [53/120    avg_loss:0.010, val_acc:0.978]
Epoch [54/120    avg_loss:0.009, val_acc:0.978]
Epoch [55/120    avg_loss:0.019, val_acc:0.965]
Epoch [56/120    avg_loss:0.062, val_acc:0.884]
Epoch [57/120    avg_loss:0.191, val_acc:0.962]
Epoch [58/120    avg_loss:0.044, val_acc:0.978]
Epoch [59/120    avg_loss:0.048, val_acc:0.977]
Epoch [60/120    avg_loss:0.041, val_acc:0.977]
Epoch [61/120    avg_loss:0.019, val_acc:0.975]
Epoch [62/120    avg_loss:0.022, val_acc:0.978]
Epoch [63/120    avg_loss:0.020, val_acc:0.978]
Epoch [64/120    avg_loss:0.023, val_acc:0.978]
Epoch [65/120    avg_loss:0.018, val_acc:0.979]
Epoch [66/120    avg_loss:0.018, val_acc:0.979]
Epoch [67/120    avg_loss:0.021, val_acc:0.980]
Epoch [68/120    avg_loss:0.015, val_acc:0.978]
Epoch [69/120    avg_loss:0.013, val_acc:0.979]
Epoch [70/120    avg_loss:0.018, val_acc:0.980]
Epoch [71/120    avg_loss:0.019, val_acc:0.980]
Epoch [72/120    avg_loss:0.028, val_acc:0.980]
Epoch [73/120    avg_loss:0.015, val_acc:0.980]
Epoch [74/120    avg_loss:0.014, val_acc:0.980]
Epoch [75/120    avg_loss:0.013, val_acc:0.980]
Epoch [76/120    avg_loss:0.014, val_acc:0.980]
Epoch [77/120    avg_loss:0.019, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.982]
Epoch [79/120    avg_loss:0.022, val_acc:0.982]
Epoch [80/120    avg_loss:0.021, val_acc:0.983]
Epoch [81/120    avg_loss:0.014, val_acc:0.982]
Epoch [82/120    avg_loss:0.014, val_acc:0.982]
Epoch [83/120    avg_loss:0.015, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.020, val_acc:0.982]
Epoch [87/120    avg_loss:0.017, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.982]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.023, val_acc:0.982]
Epoch [93/120    avg_loss:0.013, val_acc:0.981]
Epoch [94/120    avg_loss:0.014, val_acc:0.981]
Epoch [95/120    avg_loss:0.016, val_acc:0.981]
Epoch [96/120    avg_loss:0.014, val_acc:0.981]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.013, val_acc:0.981]
Epoch [99/120    avg_loss:0.015, val_acc:0.981]
Epoch [100/120    avg_loss:0.013, val_acc:0.981]
Epoch [101/120    avg_loss:0.020, val_acc:0.981]
Epoch [102/120    avg_loss:0.014, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.019, val_acc:0.981]
Epoch [105/120    avg_loss:0.016, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.015, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.982]
Epoch [110/120    avg_loss:0.019, val_acc:0.982]
Epoch [111/120    avg_loss:0.017, val_acc:0.982]
Epoch [112/120    avg_loss:0.017, val_acc:0.982]
Epoch [113/120    avg_loss:0.017, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.982]
Epoch [115/120    avg_loss:0.016, val_acc:0.982]
Epoch [116/120    avg_loss:0.016, val_acc:0.982]
Epoch [117/120    avg_loss:0.013, val_acc:0.982]
Epoch [118/120    avg_loss:0.013, val_acc:0.982]
Epoch [119/120    avg_loss:0.016, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6361     0     6     0     0     9     0    53     3]
 [    0     0 17989     0    20     0    78     0     3     0]
 [    0     2     1  1940     0     0     0     0    90     3]
 [    0    11     0     1  2954     0     1     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    55     0     1     0     0     3  1230     0     1]
 [    0     0     0    67    41     0     3     0  3458     2]
 [    0     0     0     0     0    19     0     0     0   900]]

Accuracy:
98.84799845757117

F1 scores:
[       nan 0.98919213 0.99717295 0.9577882  0.98680474 0.99277292
 0.99045685 0.97619048 0.96363383 0.98306936]

Kappa:
0.9847495888358909
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f471a59e710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.250, val_acc:0.601]
Epoch [2/120    avg_loss:0.692, val_acc:0.677]
Epoch [3/120    avg_loss:0.522, val_acc:0.632]
Epoch [4/120    avg_loss:0.440, val_acc:0.697]
Epoch [5/120    avg_loss:0.362, val_acc:0.838]
Epoch [6/120    avg_loss:0.268, val_acc:0.819]
Epoch [7/120    avg_loss:0.239, val_acc:0.865]
Epoch [8/120    avg_loss:0.219, val_acc:0.853]
Epoch [9/120    avg_loss:0.175, val_acc:0.899]
Epoch [10/120    avg_loss:0.159, val_acc:0.884]
Epoch [11/120    avg_loss:0.154, val_acc:0.888]
Epoch [12/120    avg_loss:0.144, val_acc:0.914]
Epoch [13/120    avg_loss:0.189, val_acc:0.927]
Epoch [14/120    avg_loss:0.116, val_acc:0.954]
Epoch [15/120    avg_loss:0.082, val_acc:0.947]
Epoch [16/120    avg_loss:0.076, val_acc:0.953]
Epoch [17/120    avg_loss:0.087, val_acc:0.960]
Epoch [18/120    avg_loss:0.080, val_acc:0.854]
Epoch [19/120    avg_loss:0.176, val_acc:0.944]
Epoch [20/120    avg_loss:0.074, val_acc:0.959]
Epoch [21/120    avg_loss:0.063, val_acc:0.961]
Epoch [22/120    avg_loss:0.070, val_acc:0.960]
Epoch [23/120    avg_loss:0.085, val_acc:0.948]
Epoch [24/120    avg_loss:0.041, val_acc:0.962]
Epoch [25/120    avg_loss:0.044, val_acc:0.956]
Epoch [26/120    avg_loss:0.027, val_acc:0.968]
Epoch [27/120    avg_loss:0.081, val_acc:0.952]
Epoch [28/120    avg_loss:0.043, val_acc:0.946]
Epoch [29/120    avg_loss:0.054, val_acc:0.961]
Epoch [30/120    avg_loss:0.067, val_acc:0.959]
Epoch [31/120    avg_loss:0.057, val_acc:0.956]
Epoch [32/120    avg_loss:0.056, val_acc:0.938]
Epoch [33/120    avg_loss:0.103, val_acc:0.900]
Epoch [34/120    avg_loss:0.127, val_acc:0.954]
Epoch [35/120    avg_loss:0.043, val_acc:0.952]
Epoch [36/120    avg_loss:0.034, val_acc:0.948]
Epoch [37/120    avg_loss:0.028, val_acc:0.964]
Epoch [38/120    avg_loss:0.039, val_acc:0.964]
Epoch [39/120    avg_loss:0.048, val_acc:0.962]
Epoch [40/120    avg_loss:0.030, val_acc:0.968]
Epoch [41/120    avg_loss:0.022, val_acc:0.968]
Epoch [42/120    avg_loss:0.015, val_acc:0.970]
Epoch [43/120    avg_loss:0.024, val_acc:0.969]
Epoch [44/120    avg_loss:0.014, val_acc:0.972]
Epoch [45/120    avg_loss:0.017, val_acc:0.973]
Epoch [46/120    avg_loss:0.011, val_acc:0.975]
Epoch [47/120    avg_loss:0.012, val_acc:0.974]
Epoch [48/120    avg_loss:0.011, val_acc:0.974]
Epoch [49/120    avg_loss:0.017, val_acc:0.973]
Epoch [50/120    avg_loss:0.017, val_acc:0.973]
Epoch [51/120    avg_loss:0.024, val_acc:0.973]
Epoch [52/120    avg_loss:0.011, val_acc:0.972]
Epoch [53/120    avg_loss:0.017, val_acc:0.972]
Epoch [54/120    avg_loss:0.015, val_acc:0.972]
Epoch [55/120    avg_loss:0.011, val_acc:0.972]
Epoch [56/120    avg_loss:0.020, val_acc:0.969]
Epoch [57/120    avg_loss:0.013, val_acc:0.971]
Epoch [58/120    avg_loss:0.011, val_acc:0.971]
Epoch [59/120    avg_loss:0.011, val_acc:0.972]
Epoch [60/120    avg_loss:0.010, val_acc:0.973]
Epoch [61/120    avg_loss:0.014, val_acc:0.973]
Epoch [62/120    avg_loss:0.011, val_acc:0.972]
Epoch [63/120    avg_loss:0.010, val_acc:0.972]
Epoch [64/120    avg_loss:0.015, val_acc:0.973]
Epoch [65/120    avg_loss:0.012, val_acc:0.973]
Epoch [66/120    avg_loss:0.018, val_acc:0.973]
Epoch [67/120    avg_loss:0.008, val_acc:0.973]
Epoch [68/120    avg_loss:0.017, val_acc:0.973]
Epoch [69/120    avg_loss:0.008, val_acc:0.973]
Epoch [70/120    avg_loss:0.011, val_acc:0.973]
Epoch [71/120    avg_loss:0.013, val_acc:0.973]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.013, val_acc:0.973]
Epoch [74/120    avg_loss:0.014, val_acc:0.973]
Epoch [75/120    avg_loss:0.013, val_acc:0.973]
Epoch [76/120    avg_loss:0.020, val_acc:0.973]
Epoch [77/120    avg_loss:0.010, val_acc:0.973]
Epoch [78/120    avg_loss:0.013, val_acc:0.973]
Epoch [79/120    avg_loss:0.011, val_acc:0.973]
Epoch [80/120    avg_loss:0.016, val_acc:0.973]
Epoch [81/120    avg_loss:0.011, val_acc:0.973]
Epoch [82/120    avg_loss:0.014, val_acc:0.973]
Epoch [83/120    avg_loss:0.016, val_acc:0.973]
Epoch [84/120    avg_loss:0.013, val_acc:0.973]
Epoch [85/120    avg_loss:0.019, val_acc:0.973]
Epoch [86/120    avg_loss:0.011, val_acc:0.973]
Epoch [87/120    avg_loss:0.014, val_acc:0.973]
Epoch [88/120    avg_loss:0.014, val_acc:0.973]
Epoch [89/120    avg_loss:0.014, val_acc:0.973]
Epoch [90/120    avg_loss:0.011, val_acc:0.973]
Epoch [91/120    avg_loss:0.010, val_acc:0.973]
Epoch [92/120    avg_loss:0.017, val_acc:0.973]
Epoch [93/120    avg_loss:0.017, val_acc:0.973]
Epoch [94/120    avg_loss:0.011, val_acc:0.973]
Epoch [95/120    avg_loss:0.013, val_acc:0.973]
Epoch [96/120    avg_loss:0.010, val_acc:0.973]
Epoch [97/120    avg_loss:0.011, val_acc:0.973]
Epoch [98/120    avg_loss:0.006, val_acc:0.973]
Epoch [99/120    avg_loss:0.016, val_acc:0.973]
Epoch [100/120    avg_loss:0.012, val_acc:0.973]
Epoch [101/120    avg_loss:0.008, val_acc:0.973]
Epoch [102/120    avg_loss:0.017, val_acc:0.973]
Epoch [103/120    avg_loss:0.012, val_acc:0.973]
Epoch [104/120    avg_loss:0.012, val_acc:0.973]
Epoch [105/120    avg_loss:0.014, val_acc:0.973]
Epoch [106/120    avg_loss:0.011, val_acc:0.973]
Epoch [107/120    avg_loss:0.010, val_acc:0.973]
Epoch [108/120    avg_loss:0.010, val_acc:0.973]
Epoch [109/120    avg_loss:0.015, val_acc:0.973]
Epoch [110/120    avg_loss:0.012, val_acc:0.973]
Epoch [111/120    avg_loss:0.018, val_acc:0.973]
Epoch [112/120    avg_loss:0.014, val_acc:0.973]
Epoch [113/120    avg_loss:0.013, val_acc:0.973]
Epoch [114/120    avg_loss:0.011, val_acc:0.973]
Epoch [115/120    avg_loss:0.010, val_acc:0.973]
Epoch [116/120    avg_loss:0.017, val_acc:0.973]
Epoch [117/120    avg_loss:0.012, val_acc:0.973]
Epoch [118/120    avg_loss:0.008, val_acc:0.973]
Epoch [119/120    avg_loss:0.012, val_acc:0.973]
Epoch [120/120    avg_loss:0.016, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6415     0     0     0     0     0     0     6    11]
 [    0     0 18000     0     5     0    80     0     5     0]
 [    0    14     0  1925     0     0     0     0    95     2]
 [    0    25     4     2  2929     0     4     0     4     4]
 [    0     2     0     0     0  1303     0     0     0     0]
 [    0     0     3     0     0     0  4875     0     0     0]
 [    0     4     0     0     0     0     0  1284     0     2]
 [    0     3     0    82    38     0     0     0  3448     0]
 [    0     0     0     0     0     6     0     0     0   913]]

Accuracy:
99.03357192779505

F1 scores:
[       nan 0.99495929 0.9973128  0.95179234 0.98553163 0.99693956
 0.99115584 0.997669   0.96731659 0.98649379]

Kappa:
0.9872040819367244
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdbcfb1d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.232, val_acc:0.388]
Epoch [2/120    avg_loss:0.723, val_acc:0.602]
Epoch [3/120    avg_loss:0.495, val_acc:0.677]
Epoch [4/120    avg_loss:0.417, val_acc:0.742]
Epoch [5/120    avg_loss:0.367, val_acc:0.777]
Epoch [6/120    avg_loss:0.314, val_acc:0.734]
Epoch [7/120    avg_loss:0.303, val_acc:0.789]
Epoch [8/120    avg_loss:0.326, val_acc:0.804]
Epoch [9/120    avg_loss:0.280, val_acc:0.883]
Epoch [10/120    avg_loss:0.189, val_acc:0.917]
Epoch [11/120    avg_loss:0.188, val_acc:0.924]
Epoch [12/120    avg_loss:0.130, val_acc:0.890]
Epoch [13/120    avg_loss:0.132, val_acc:0.933]
Epoch [14/120    avg_loss:0.123, val_acc:0.930]
Epoch [15/120    avg_loss:0.159, val_acc:0.938]
Epoch [16/120    avg_loss:0.197, val_acc:0.936]
Epoch [17/120    avg_loss:0.114, val_acc:0.945]
Epoch [18/120    avg_loss:0.083, val_acc:0.927]
Epoch [19/120    avg_loss:0.100, val_acc:0.943]
Epoch [20/120    avg_loss:0.085, val_acc:0.941]
Epoch [21/120    avg_loss:0.075, val_acc:0.968]
Epoch [22/120    avg_loss:0.061, val_acc:0.970]
Epoch [23/120    avg_loss:0.052, val_acc:0.957]
Epoch [24/120    avg_loss:0.052, val_acc:0.938]
Epoch [25/120    avg_loss:0.080, val_acc:0.973]
Epoch [26/120    avg_loss:0.044, val_acc:0.976]
Epoch [27/120    avg_loss:0.077, val_acc:0.943]
Epoch [28/120    avg_loss:0.065, val_acc:0.974]
Epoch [29/120    avg_loss:0.043, val_acc:0.973]
Epoch [30/120    avg_loss:0.039, val_acc:0.978]
Epoch [31/120    avg_loss:0.025, val_acc:0.968]
Epoch [32/120    avg_loss:0.048, val_acc:0.967]
Epoch [33/120    avg_loss:0.078, val_acc:0.978]
Epoch [34/120    avg_loss:0.087, val_acc:0.958]
Epoch [35/120    avg_loss:0.051, val_acc:0.962]
Epoch [36/120    avg_loss:0.035, val_acc:0.981]
Epoch [37/120    avg_loss:0.019, val_acc:0.980]
Epoch [38/120    avg_loss:0.025, val_acc:0.979]
Epoch [39/120    avg_loss:0.023, val_acc:0.983]
Epoch [40/120    avg_loss:0.023, val_acc:0.968]
Epoch [41/120    avg_loss:0.021, val_acc:0.982]
Epoch [42/120    avg_loss:0.027, val_acc:0.975]
Epoch [43/120    avg_loss:0.032, val_acc:0.969]
Epoch [44/120    avg_loss:0.031, val_acc:0.973]
Epoch [45/120    avg_loss:0.021, val_acc:0.978]
Epoch [46/120    avg_loss:0.030, val_acc:0.978]
Epoch [47/120    avg_loss:0.026, val_acc:0.978]
Epoch [48/120    avg_loss:0.022, val_acc:0.980]
Epoch [49/120    avg_loss:0.023, val_acc:0.984]
Epoch [50/120    avg_loss:0.018, val_acc:0.982]
Epoch [51/120    avg_loss:0.017, val_acc:0.983]
Epoch [52/120    avg_loss:0.011, val_acc:0.981]
Epoch [53/120    avg_loss:0.012, val_acc:0.985]
Epoch [54/120    avg_loss:0.033, val_acc:0.981]
Epoch [55/120    avg_loss:0.014, val_acc:0.980]
Epoch [56/120    avg_loss:0.012, val_acc:0.984]
Epoch [57/120    avg_loss:0.011, val_acc:0.988]
Epoch [58/120    avg_loss:0.026, val_acc:0.987]
Epoch [59/120    avg_loss:0.016, val_acc:0.990]
Epoch [60/120    avg_loss:0.011, val_acc:0.990]
Epoch [61/120    avg_loss:0.011, val_acc:0.981]
Epoch [62/120    avg_loss:0.007, val_acc:0.985]
Epoch [63/120    avg_loss:0.008, val_acc:0.987]
Epoch [64/120    avg_loss:0.011, val_acc:0.975]
Epoch [65/120    avg_loss:0.020, val_acc:0.981]
Epoch [66/120    avg_loss:0.009, val_acc:0.981]
Epoch [67/120    avg_loss:0.008, val_acc:0.987]
Epoch [68/120    avg_loss:0.008, val_acc:0.988]
Epoch [69/120    avg_loss:0.014, val_acc:0.977]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.010, val_acc:0.979]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.006, val_acc:0.989]
Epoch [76/120    avg_loss:0.008, val_acc:0.990]
Epoch [77/120    avg_loss:0.008, val_acc:0.991]
Epoch [78/120    avg_loss:0.004, val_acc:0.991]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.009, val_acc:0.991]
Epoch [81/120    avg_loss:0.005, val_acc:0.991]
Epoch [82/120    avg_loss:0.005, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.005, val_acc:0.991]
Epoch [85/120    avg_loss:0.005, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.993]
Epoch [88/120    avg_loss:0.003, val_acc:0.993]
Epoch [89/120    avg_loss:0.004, val_acc:0.993]
Epoch [90/120    avg_loss:0.006, val_acc:0.993]
Epoch [91/120    avg_loss:0.004, val_acc:0.993]
Epoch [92/120    avg_loss:0.006, val_acc:0.993]
Epoch [93/120    avg_loss:0.005, val_acc:0.993]
Epoch [94/120    avg_loss:0.004, val_acc:0.992]
Epoch [95/120    avg_loss:0.004, val_acc:0.993]
Epoch [96/120    avg_loss:0.008, val_acc:0.993]
Epoch [97/120    avg_loss:0.005, val_acc:0.993]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.993]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.007, val_acc:0.993]
Epoch [103/120    avg_loss:0.003, val_acc:0.993]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.004, val_acc:0.993]
Epoch [108/120    avg_loss:0.003, val_acc:0.993]
Epoch [109/120    avg_loss:0.003, val_acc:0.993]
Epoch [110/120    avg_loss:0.003, val_acc:0.992]
Epoch [111/120    avg_loss:0.003, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.993]
Epoch [113/120    avg_loss:0.003, val_acc:0.993]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.003, val_acc:0.993]
Epoch [116/120    avg_loss:0.002, val_acc:0.993]
Epoch [117/120    avg_loss:0.004, val_acc:0.993]
Epoch [118/120    avg_loss:0.005, val_acc:0.993]
Epoch [119/120    avg_loss:0.005, val_acc:0.993]
Epoch [120/120    avg_loss:0.002, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     4     0     0     0     1    15     1]
 [    0     0 18029     0     9     0    49     0     3     0]
 [    0     2     0  1967     0     0     0     0    63     4]
 [    0    13     0     0  2952     0     2     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4873     0     0     0]
 [    0    18     0     0     0     0     2  1267     0     3]
 [    0    12     0    72    50     0     6     0  3431     0]
 [    0     0     0     0     0     7     0     0     0   912]]

Accuracy:
99.16612440652641

F1 scores:
[       nan 0.99487896 0.99817296 0.96445207 0.98679592 0.99732518
 0.99347604 0.99061767 0.96811512 0.99184339]

Kappa:
0.9889563926102336
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6bbca53780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.310, val_acc:0.644]
Epoch [2/120    avg_loss:0.745, val_acc:0.598]
Epoch [3/120    avg_loss:0.566, val_acc:0.708]
Epoch [4/120    avg_loss:0.376, val_acc:0.777]
Epoch [5/120    avg_loss:0.397, val_acc:0.754]
Epoch [6/120    avg_loss:0.345, val_acc:0.822]
Epoch [7/120    avg_loss:0.227, val_acc:0.918]
Epoch [8/120    avg_loss:0.246, val_acc:0.883]
Epoch [9/120    avg_loss:0.141, val_acc:0.890]
Epoch [10/120    avg_loss:0.147, val_acc:0.925]
Epoch [11/120    avg_loss:0.123, val_acc:0.914]
Epoch [12/120    avg_loss:0.125, val_acc:0.913]
Epoch [13/120    avg_loss:0.121, val_acc:0.940]
Epoch [14/120    avg_loss:0.124, val_acc:0.927]
Epoch [15/120    avg_loss:0.089, val_acc:0.915]
Epoch [16/120    avg_loss:0.087, val_acc:0.953]
Epoch [17/120    avg_loss:0.076, val_acc:0.955]
Epoch [18/120    avg_loss:0.085, val_acc:0.958]
Epoch [19/120    avg_loss:0.076, val_acc:0.930]
Epoch [20/120    avg_loss:0.071, val_acc:0.953]
Epoch [21/120    avg_loss:0.050, val_acc:0.959]
Epoch [22/120    avg_loss:0.035, val_acc:0.971]
Epoch [23/120    avg_loss:0.038, val_acc:0.973]
Epoch [24/120    avg_loss:0.035, val_acc:0.969]
Epoch [25/120    avg_loss:0.032, val_acc:0.970]
Epoch [26/120    avg_loss:0.024, val_acc:0.968]
Epoch [27/120    avg_loss:0.023, val_acc:0.963]
Epoch [28/120    avg_loss:0.060, val_acc:0.927]
Epoch [29/120    avg_loss:0.052, val_acc:0.966]
Epoch [30/120    avg_loss:0.053, val_acc:0.919]
Epoch [31/120    avg_loss:0.074, val_acc:0.965]
Epoch [32/120    avg_loss:0.049, val_acc:0.960]
Epoch [33/120    avg_loss:0.036, val_acc:0.960]
Epoch [34/120    avg_loss:0.032, val_acc:0.973]
Epoch [35/120    avg_loss:0.037, val_acc:0.950]
Epoch [36/120    avg_loss:0.040, val_acc:0.974]
Epoch [37/120    avg_loss:0.035, val_acc:0.978]
Epoch [38/120    avg_loss:0.026, val_acc:0.958]
Epoch [39/120    avg_loss:0.019, val_acc:0.978]
Epoch [40/120    avg_loss:0.023, val_acc:0.972]
Epoch [41/120    avg_loss:0.016, val_acc:0.980]
Epoch [42/120    avg_loss:0.015, val_acc:0.975]
Epoch [43/120    avg_loss:0.017, val_acc:0.975]
Epoch [44/120    avg_loss:0.007, val_acc:0.978]
Epoch [45/120    avg_loss:0.010, val_acc:0.976]
Epoch [46/120    avg_loss:0.025, val_acc:0.975]
Epoch [47/120    avg_loss:0.016, val_acc:0.980]
Epoch [48/120    avg_loss:0.019, val_acc:0.981]
Epoch [49/120    avg_loss:0.017, val_acc:0.981]
Epoch [50/120    avg_loss:0.007, val_acc:0.983]
Epoch [51/120    avg_loss:0.007, val_acc:0.980]
Epoch [52/120    avg_loss:0.007, val_acc:0.981]
Epoch [53/120    avg_loss:0.021, val_acc:0.959]
Epoch [54/120    avg_loss:0.018, val_acc:0.976]
Epoch [55/120    avg_loss:0.008, val_acc:0.976]
Epoch [56/120    avg_loss:0.014, val_acc:0.975]
Epoch [57/120    avg_loss:0.008, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.975]
Epoch [59/120    avg_loss:0.022, val_acc:0.977]
Epoch [60/120    avg_loss:0.031, val_acc:0.966]
Epoch [61/120    avg_loss:0.028, val_acc:0.979]
Epoch [62/120    avg_loss:0.013, val_acc:0.984]
Epoch [63/120    avg_loss:0.009, val_acc:0.979]
Epoch [64/120    avg_loss:0.013, val_acc:0.978]
Epoch [65/120    avg_loss:0.007, val_acc:0.976]
Epoch [66/120    avg_loss:0.015, val_acc:0.980]
Epoch [67/120    avg_loss:0.008, val_acc:0.981]
Epoch [68/120    avg_loss:0.007, val_acc:0.980]
Epoch [69/120    avg_loss:0.023, val_acc:0.973]
Epoch [70/120    avg_loss:0.020, val_acc:0.968]
Epoch [71/120    avg_loss:0.033, val_acc:0.973]
Epoch [72/120    avg_loss:0.013, val_acc:0.969]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.957]
Epoch [76/120    avg_loss:0.042, val_acc:0.977]
Epoch [77/120    avg_loss:0.009, val_acc:0.977]
Epoch [78/120    avg_loss:0.007, val_acc:0.979]
Epoch [79/120    avg_loss:0.008, val_acc:0.979]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.004, val_acc:0.984]
Epoch [84/120    avg_loss:0.005, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.011, val_acc:0.977]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.015, val_acc:0.980]
Epoch [90/120    avg_loss:0.014, val_acc:0.968]
Epoch [91/120    avg_loss:0.010, val_acc:0.979]
Epoch [92/120    avg_loss:0.031, val_acc:0.976]
Epoch [93/120    avg_loss:0.173, val_acc:0.945]
Epoch [94/120    avg_loss:0.066, val_acc:0.962]
Epoch [95/120    avg_loss:0.050, val_acc:0.967]
Epoch [96/120    avg_loss:0.041, val_acc:0.967]
Epoch [97/120    avg_loss:0.032, val_acc:0.971]
Epoch [98/120    avg_loss:0.033, val_acc:0.974]
Epoch [99/120    avg_loss:0.030, val_acc:0.973]
Epoch [100/120    avg_loss:0.031, val_acc:0.975]
Epoch [101/120    avg_loss:0.026, val_acc:0.973]
Epoch [102/120    avg_loss:0.022, val_acc:0.973]
Epoch [103/120    avg_loss:0.028, val_acc:0.972]
Epoch [104/120    avg_loss:0.016, val_acc:0.973]
Epoch [105/120    avg_loss:0.019, val_acc:0.973]
Epoch [106/120    avg_loss:0.018, val_acc:0.975]
Epoch [107/120    avg_loss:0.013, val_acc:0.976]
Epoch [108/120    avg_loss:0.019, val_acc:0.976]
Epoch [109/120    avg_loss:0.016, val_acc:0.976]
Epoch [110/120    avg_loss:0.019, val_acc:0.975]
Epoch [111/120    avg_loss:0.020, val_acc:0.975]
Epoch [112/120    avg_loss:0.012, val_acc:0.975]
Epoch [113/120    avg_loss:0.012, val_acc:0.975]
Epoch [114/120    avg_loss:0.017, val_acc:0.975]
Epoch [115/120    avg_loss:0.017, val_acc:0.976]
Epoch [116/120    avg_loss:0.012, val_acc:0.976]
Epoch [117/120    avg_loss:0.017, val_acc:0.976]
Epoch [118/120    avg_loss:0.013, val_acc:0.976]
Epoch [119/120    avg_loss:0.012, val_acc:0.976]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6364     0     3     0     0     0    42     8    15]
 [    0     0 18064     0     7     0    13     0     6     0]
 [    0     0     0  1969     0     0     1     0    59     7]
 [    0    27     2     2  2928     0     1     0    11     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     4     0     0     0     0     2  1273     0    11]
 [    0     8     0    13    52     0     0     0  3494     4]
 [    0     0     0     0     1    22     0     0     0   896]]

Accuracy:
99.22155544308679

F1 scores:
[       nan 0.99166342 0.99919794 0.97887149 0.98255034 0.99164134
 0.998158   0.97735125 0.97747937 0.96708041]

Kappa:
0.9896889249366767
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa7f53267f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.257, val_acc:0.668]
Epoch [2/120    avg_loss:0.750, val_acc:0.742]
Epoch [3/120    avg_loss:0.539, val_acc:0.790]
Epoch [4/120    avg_loss:0.424, val_acc:0.856]
Epoch [5/120    avg_loss:0.369, val_acc:0.873]
Epoch [6/120    avg_loss:0.290, val_acc:0.813]
Epoch [7/120    avg_loss:0.235, val_acc:0.890]
Epoch [8/120    avg_loss:0.169, val_acc:0.944]
Epoch [9/120    avg_loss:0.145, val_acc:0.928]
Epoch [10/120    avg_loss:0.144, val_acc:0.942]
Epoch [11/120    avg_loss:0.155, val_acc:0.918]
Epoch [12/120    avg_loss:0.213, val_acc:0.907]
Epoch [13/120    avg_loss:0.134, val_acc:0.929]
Epoch [14/120    avg_loss:0.132, val_acc:0.953]
Epoch [15/120    avg_loss:0.068, val_acc:0.922]
Epoch [16/120    avg_loss:0.090, val_acc:0.962]
Epoch [17/120    avg_loss:0.072, val_acc:0.957]
Epoch [18/120    avg_loss:0.073, val_acc:0.972]
Epoch [19/120    avg_loss:0.054, val_acc:0.965]
Epoch [20/120    avg_loss:0.060, val_acc:0.962]
Epoch [21/120    avg_loss:0.073, val_acc:0.968]
Epoch [22/120    avg_loss:0.042, val_acc:0.965]
Epoch [23/120    avg_loss:0.052, val_acc:0.969]
Epoch [24/120    avg_loss:0.037, val_acc:0.966]
Epoch [25/120    avg_loss:0.037, val_acc:0.976]
Epoch [26/120    avg_loss:0.035, val_acc:0.954]
Epoch [27/120    avg_loss:0.021, val_acc:0.976]
Epoch [28/120    avg_loss:0.107, val_acc:0.957]
Epoch [29/120    avg_loss:0.076, val_acc:0.961]
Epoch [30/120    avg_loss:0.029, val_acc:0.980]
Epoch [31/120    avg_loss:0.032, val_acc:0.965]
Epoch [32/120    avg_loss:0.039, val_acc:0.969]
Epoch [33/120    avg_loss:0.019, val_acc:0.963]
Epoch [34/120    avg_loss:0.027, val_acc:0.977]
Epoch [35/120    avg_loss:0.029, val_acc:0.956]
Epoch [36/120    avg_loss:0.051, val_acc:0.907]
Epoch [37/120    avg_loss:0.083, val_acc:0.975]
Epoch [38/120    avg_loss:0.035, val_acc:0.972]
Epoch [39/120    avg_loss:0.027, val_acc:0.980]
Epoch [40/120    avg_loss:0.021, val_acc:0.979]
Epoch [41/120    avg_loss:0.021, val_acc:0.985]
Epoch [42/120    avg_loss:0.016, val_acc:0.980]
Epoch [43/120    avg_loss:0.024, val_acc:0.976]
Epoch [44/120    avg_loss:0.024, val_acc:0.983]
Epoch [45/120    avg_loss:0.017, val_acc:0.976]
Epoch [46/120    avg_loss:0.014, val_acc:0.974]
Epoch [47/120    avg_loss:0.023, val_acc:0.971]
Epoch [48/120    avg_loss:0.037, val_acc:0.981]
Epoch [49/120    avg_loss:0.052, val_acc:0.974]
Epoch [50/120    avg_loss:0.036, val_acc:0.982]
Epoch [51/120    avg_loss:0.031, val_acc:0.969]
Epoch [52/120    avg_loss:0.017, val_acc:0.978]
Epoch [53/120    avg_loss:0.013, val_acc:0.984]
Epoch [54/120    avg_loss:0.008, val_acc:0.979]
Epoch [55/120    avg_loss:0.010, val_acc:0.980]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.008, val_acc:0.982]
Epoch [58/120    avg_loss:0.009, val_acc:0.982]
Epoch [59/120    avg_loss:0.006, val_acc:0.983]
Epoch [60/120    avg_loss:0.006, val_acc:0.989]
Epoch [61/120    avg_loss:0.009, val_acc:0.989]
Epoch [62/120    avg_loss:0.004, val_acc:0.988]
Epoch [63/120    avg_loss:0.007, val_acc:0.990]
Epoch [64/120    avg_loss:0.008, val_acc:0.984]
Epoch [65/120    avg_loss:0.008, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.984]
Epoch [67/120    avg_loss:0.016, val_acc:0.980]
Epoch [68/120    avg_loss:0.007, val_acc:0.980]
Epoch [69/120    avg_loss:0.007, val_acc:0.984]
Epoch [70/120    avg_loss:0.008, val_acc:0.985]
Epoch [71/120    avg_loss:0.010, val_acc:0.983]
Epoch [72/120    avg_loss:0.010, val_acc:0.982]
Epoch [73/120    avg_loss:0.006, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.007, val_acc:0.984]
Epoch [80/120    avg_loss:0.010, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.012, val_acc:0.984]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.006, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.018, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.004, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     7     0     0     0     6     7     0]
 [    0     0 18062     0    11     0    15     0     1     1]
 [    0     2     0  2019     0     0     0     0    14     1]
 [    0    21     0     4  2940     0     1     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4877     0     1     0]
 [    0     3     0     0     0     0     2  1285     0     0]
 [    0     7     0    75    35     0     0     0  3452     2]
 [    0     0     2     3     3    31     0     0     0   880]]

Accuracy:
99.37097823729304

F1 scores:
[       nan 0.99588413 0.99917022 0.97442085 0.98641168 0.98826202
 0.99805587 0.99573809 0.97901305 0.97615086]

Kappa:
0.9916677653930669
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1293fb780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.279, val_acc:0.399]
Epoch [2/120    avg_loss:0.678, val_acc:0.740]
Epoch [3/120    avg_loss:0.488, val_acc:0.818]
Epoch [4/120    avg_loss:0.398, val_acc:0.757]
Epoch [5/120    avg_loss:0.292, val_acc:0.912]
Epoch [6/120    avg_loss:0.254, val_acc:0.881]
Epoch [7/120    avg_loss:0.221, val_acc:0.856]
Epoch [8/120    avg_loss:0.284, val_acc:0.907]
Epoch [9/120    avg_loss:0.220, val_acc:0.929]
Epoch [10/120    avg_loss:0.254, val_acc:0.875]
Epoch [11/120    avg_loss:0.132, val_acc:0.955]
Epoch [12/120    avg_loss:0.140, val_acc:0.887]
Epoch [13/120    avg_loss:0.125, val_acc:0.963]
Epoch [14/120    avg_loss:0.116, val_acc:0.953]
Epoch [15/120    avg_loss:0.102, val_acc:0.962]
Epoch [16/120    avg_loss:0.062, val_acc:0.969]
Epoch [17/120    avg_loss:0.089, val_acc:0.960]
Epoch [18/120    avg_loss:0.084, val_acc:0.962]
Epoch [19/120    avg_loss:0.057, val_acc:0.914]
Epoch [20/120    avg_loss:0.062, val_acc:0.973]
Epoch [21/120    avg_loss:0.065, val_acc:0.963]
Epoch [22/120    avg_loss:0.042, val_acc:0.972]
Epoch [23/120    avg_loss:0.031, val_acc:0.975]
Epoch [24/120    avg_loss:0.038, val_acc:0.966]
Epoch [25/120    avg_loss:0.036, val_acc:0.980]
Epoch [26/120    avg_loss:0.028, val_acc:0.982]
Epoch [27/120    avg_loss:0.031, val_acc:0.963]
Epoch [28/120    avg_loss:0.034, val_acc:0.972]
Epoch [29/120    avg_loss:0.024, val_acc:0.981]
Epoch [30/120    avg_loss:0.015, val_acc:0.973]
Epoch [31/120    avg_loss:0.021, val_acc:0.983]
Epoch [32/120    avg_loss:0.029, val_acc:0.958]
Epoch [33/120    avg_loss:0.038, val_acc:0.976]
Epoch [34/120    avg_loss:0.044, val_acc:0.975]
Epoch [35/120    avg_loss:0.023, val_acc:0.986]
Epoch [36/120    avg_loss:0.039, val_acc:0.965]
Epoch [37/120    avg_loss:0.057, val_acc:0.968]
Epoch [38/120    avg_loss:0.029, val_acc:0.978]
Epoch [39/120    avg_loss:0.058, val_acc:0.963]
Epoch [40/120    avg_loss:0.054, val_acc:0.978]
Epoch [41/120    avg_loss:0.028, val_acc:0.979]
Epoch [42/120    avg_loss:0.030, val_acc:0.975]
Epoch [43/120    avg_loss:0.028, val_acc:0.977]
Epoch [44/120    avg_loss:0.028, val_acc:0.988]
Epoch [45/120    avg_loss:0.024, val_acc:0.979]
Epoch [46/120    avg_loss:0.045, val_acc:0.971]
Epoch [47/120    avg_loss:0.035, val_acc:0.982]
Epoch [48/120    avg_loss:0.020, val_acc:0.978]
Epoch [49/120    avg_loss:0.020, val_acc:0.979]
Epoch [50/120    avg_loss:0.016, val_acc:0.983]
Epoch [51/120    avg_loss:0.017, val_acc:0.978]
Epoch [52/120    avg_loss:0.014, val_acc:0.985]
Epoch [53/120    avg_loss:0.022, val_acc:0.983]
Epoch [54/120    avg_loss:0.034, val_acc:0.982]
Epoch [55/120    avg_loss:0.010, val_acc:0.988]
Epoch [56/120    avg_loss:0.022, val_acc:0.947]
Epoch [57/120    avg_loss:0.020, val_acc:0.985]
Epoch [58/120    avg_loss:0.012, val_acc:0.985]
Epoch [59/120    avg_loss:0.006, val_acc:0.986]
Epoch [60/120    avg_loss:0.006, val_acc:0.986]
Epoch [61/120    avg_loss:0.009, val_acc:0.986]
Epoch [62/120    avg_loss:0.011, val_acc:0.985]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.005, val_acc:0.985]
Epoch [65/120    avg_loss:0.006, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.986]
Epoch [67/120    avg_loss:0.012, val_acc:0.986]
Epoch [68/120    avg_loss:0.010, val_acc:0.986]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.007, val_acc:0.987]
Epoch [71/120    avg_loss:0.009, val_acc:0.987]
Epoch [72/120    avg_loss:0.006, val_acc:0.987]
Epoch [73/120    avg_loss:0.007, val_acc:0.987]
Epoch [74/120    avg_loss:0.006, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.987]
Epoch [76/120    avg_loss:0.010, val_acc:0.987]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.009, val_acc:0.987]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.987]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.004, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.011, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.012, val_acc:0.987]
Epoch [107/120    avg_loss:0.010, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.009, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6381     0     1     0     0     6    24    12     8]
 [    0     1 18042     0    11     0    35     0     1     0]
 [    0     2     0  1998     0     0     0     0    29     7]
 [    0    18     1     1  2939     0     1     0     9     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4874     0     3     0]
 [    0     2     0     0     0     0     0  1287     0     1]
 [    0    12     0    23    41     0     0     0  3484    11]
 [    0     0     0     0     8    16     0     0     0   895]]

Accuracy:
99.30590702046129

F1 scores:
[       nan 0.99330635 0.99861626 0.98447894 0.98442472 0.99390708
 0.99530325 0.98961938 0.98016599 0.97071584]

Kappa:
0.9908082271676703
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2846c437b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.290, val_acc:0.708]
Epoch [2/120    avg_loss:0.749, val_acc:0.562]
Epoch [3/120    avg_loss:0.527, val_acc:0.610]
Epoch [4/120    avg_loss:0.377, val_acc:0.795]
Epoch [5/120    avg_loss:0.442, val_acc:0.848]
Epoch [6/120    avg_loss:0.268, val_acc:0.882]
Epoch [7/120    avg_loss:0.185, val_acc:0.889]
Epoch [8/120    avg_loss:0.180, val_acc:0.897]
Epoch [9/120    avg_loss:0.157, val_acc:0.935]
Epoch [10/120    avg_loss:0.161, val_acc:0.930]
Epoch [11/120    avg_loss:0.146, val_acc:0.935]
Epoch [12/120    avg_loss:0.140, val_acc:0.926]
Epoch [13/120    avg_loss:0.128, val_acc:0.936]
Epoch [14/120    avg_loss:0.133, val_acc:0.948]
Epoch [15/120    avg_loss:0.067, val_acc:0.960]
Epoch [16/120    avg_loss:0.059, val_acc:0.943]
Epoch [17/120    avg_loss:0.058, val_acc:0.958]
Epoch [18/120    avg_loss:0.047, val_acc:0.956]
Epoch [19/120    avg_loss:0.048, val_acc:0.958]
Epoch [20/120    avg_loss:0.065, val_acc:0.971]
Epoch [21/120    avg_loss:0.042, val_acc:0.963]
Epoch [22/120    avg_loss:0.070, val_acc:0.955]
Epoch [23/120    avg_loss:0.086, val_acc:0.955]
Epoch [24/120    avg_loss:0.073, val_acc:0.960]
Epoch [25/120    avg_loss:0.048, val_acc:0.960]
Epoch [26/120    avg_loss:0.039, val_acc:0.966]
Epoch [27/120    avg_loss:0.024, val_acc:0.963]
Epoch [28/120    avg_loss:0.024, val_acc:0.964]
Epoch [29/120    avg_loss:0.013, val_acc:0.969]
Epoch [30/120    avg_loss:0.031, val_acc:0.967]
Epoch [31/120    avg_loss:0.023, val_acc:0.966]
Epoch [32/120    avg_loss:0.020, val_acc:0.972]
Epoch [33/120    avg_loss:0.037, val_acc:0.970]
Epoch [34/120    avg_loss:0.032, val_acc:0.969]
Epoch [35/120    avg_loss:0.028, val_acc:0.970]
Epoch [36/120    avg_loss:0.025, val_acc:0.979]
Epoch [37/120    avg_loss:0.026, val_acc:0.974]
Epoch [38/120    avg_loss:0.034, val_acc:0.957]
Epoch [39/120    avg_loss:0.087, val_acc:0.940]
Epoch [40/120    avg_loss:0.073, val_acc:0.945]
Epoch [41/120    avg_loss:0.096, val_acc:0.960]
Epoch [42/120    avg_loss:0.066, val_acc:0.958]
Epoch [43/120    avg_loss:0.039, val_acc:0.966]
Epoch [44/120    avg_loss:0.022, val_acc:0.979]
Epoch [45/120    avg_loss:0.016, val_acc:0.973]
Epoch [46/120    avg_loss:0.013, val_acc:0.976]
Epoch [47/120    avg_loss:0.021, val_acc:0.978]
Epoch [48/120    avg_loss:0.024, val_acc:0.978]
Epoch [49/120    avg_loss:0.013, val_acc:0.982]
Epoch [50/120    avg_loss:0.006, val_acc:0.978]
Epoch [51/120    avg_loss:0.014, val_acc:0.973]
Epoch [52/120    avg_loss:0.025, val_acc:0.984]
Epoch [53/120    avg_loss:0.010, val_acc:0.976]
Epoch [54/120    avg_loss:0.009, val_acc:0.984]
Epoch [55/120    avg_loss:0.015, val_acc:0.975]
Epoch [56/120    avg_loss:0.012, val_acc:0.977]
Epoch [57/120    avg_loss:0.017, val_acc:0.971]
Epoch [58/120    avg_loss:0.015, val_acc:0.981]
Epoch [59/120    avg_loss:0.028, val_acc:0.968]
Epoch [60/120    avg_loss:0.007, val_acc:0.983]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.015, val_acc:0.980]
Epoch [63/120    avg_loss:0.013, val_acc:0.983]
Epoch [64/120    avg_loss:0.024, val_acc:0.969]
Epoch [65/120    avg_loss:0.015, val_acc:0.984]
Epoch [66/120    avg_loss:0.008, val_acc:0.983]
Epoch [67/120    avg_loss:0.008, val_acc:0.986]
Epoch [68/120    avg_loss:0.006, val_acc:0.986]
Epoch [69/120    avg_loss:0.012, val_acc:0.958]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.976]
Epoch [74/120    avg_loss:0.007, val_acc:0.983]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.015, val_acc:0.981]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.006, val_acc:0.981]
Epoch [79/120    avg_loss:0.003, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.983]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.987]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.004, val_acc:0.988]
Epoch [85/120    avg_loss:0.004, val_acc:0.988]
Epoch [86/120    avg_loss:0.004, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.989]
Epoch [88/120    avg_loss:0.006, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.002, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.003, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.003, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.988]
Epoch [101/120    avg_loss:0.002, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.988]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.002, val_acc:0.988]
Epoch [105/120    avg_loss:0.002, val_acc:0.988]
Epoch [106/120    avg_loss:0.002, val_acc:0.988]
Epoch [107/120    avg_loss:0.003, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.003, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.003, val_acc:0.988]
Epoch [113/120    avg_loss:0.002, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     1     0     0     0     0     7     3]
 [    0     2 18076     0     7     0     2     0     2     1]
 [    0     0     0  1998     0     0     0     0    35     3]
 [    0    22     0     5  2936     0     0     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4871     0     7     0]
 [    0     6     0     0     0     0     0  1282     0     2]
 [    0     4     0    32    47     0     0     0  3487     1]
 [    0     0     0     0     0    20     0     0     0   899]]

Accuracy:
99.47461017521027

F1 scores:
[       nan 0.99650811 0.9996129  0.98133595 0.98490439 0.99239544
 0.99907702 0.99688958 0.98018271 0.98197706]

Kappa:
0.9930391422526548
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa2b1eb8748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.420, val_acc:0.574]
Epoch [2/120    avg_loss:0.704, val_acc:0.726]
Epoch [3/120    avg_loss:0.527, val_acc:0.728]
Epoch [4/120    avg_loss:0.402, val_acc:0.779]
Epoch [5/120    avg_loss:0.327, val_acc:0.806]
Epoch [6/120    avg_loss:0.222, val_acc:0.864]
Epoch [7/120    avg_loss:0.178, val_acc:0.846]
Epoch [8/120    avg_loss:0.215, val_acc:0.912]
Epoch [9/120    avg_loss:0.204, val_acc:0.874]
Epoch [10/120    avg_loss:0.215, val_acc:0.924]
Epoch [11/120    avg_loss:0.178, val_acc:0.919]
Epoch [12/120    avg_loss:0.249, val_acc:0.927]
Epoch [13/120    avg_loss:0.134, val_acc:0.919]
Epoch [14/120    avg_loss:0.174, val_acc:0.932]
Epoch [15/120    avg_loss:0.090, val_acc:0.958]
Epoch [16/120    avg_loss:0.076, val_acc:0.948]
Epoch [17/120    avg_loss:0.059, val_acc:0.963]
Epoch [18/120    avg_loss:0.066, val_acc:0.955]
Epoch [19/120    avg_loss:0.079, val_acc:0.972]
Epoch [20/120    avg_loss:0.062, val_acc:0.964]
Epoch [21/120    avg_loss:0.078, val_acc:0.942]
Epoch [22/120    avg_loss:0.069, val_acc:0.973]
Epoch [23/120    avg_loss:0.040, val_acc:0.979]
Epoch [24/120    avg_loss:0.072, val_acc:0.970]
Epoch [25/120    avg_loss:0.063, val_acc:0.978]
Epoch [26/120    avg_loss:0.033, val_acc:0.968]
Epoch [27/120    avg_loss:0.042, val_acc:0.964]
Epoch [28/120    avg_loss:0.081, val_acc:0.953]
Epoch [29/120    avg_loss:0.026, val_acc:0.973]
Epoch [30/120    avg_loss:0.045, val_acc:0.961]
Epoch [31/120    avg_loss:0.039, val_acc:0.976]
Epoch [32/120    avg_loss:0.047, val_acc:0.963]
Epoch [33/120    avg_loss:0.087, val_acc:0.971]
Epoch [34/120    avg_loss:0.031, val_acc:0.974]
Epoch [35/120    avg_loss:0.039, val_acc:0.981]
Epoch [36/120    avg_loss:0.026, val_acc:0.982]
Epoch [37/120    avg_loss:0.016, val_acc:0.979]
Epoch [38/120    avg_loss:0.029, val_acc:0.976]
Epoch [39/120    avg_loss:0.037, val_acc:0.975]
Epoch [40/120    avg_loss:0.023, val_acc:0.987]
Epoch [41/120    avg_loss:0.022, val_acc:0.983]
Epoch [42/120    avg_loss:0.036, val_acc:0.985]
Epoch [43/120    avg_loss:0.017, val_acc:0.984]
Epoch [44/120    avg_loss:0.020, val_acc:0.983]
Epoch [45/120    avg_loss:0.019, val_acc:0.978]
Epoch [46/120    avg_loss:0.012, val_acc:0.989]
Epoch [47/120    avg_loss:0.014, val_acc:0.983]
Epoch [48/120    avg_loss:0.011, val_acc:0.988]
Epoch [49/120    avg_loss:0.014, val_acc:0.991]
Epoch [50/120    avg_loss:0.011, val_acc:0.985]
Epoch [51/120    avg_loss:0.025, val_acc:0.985]
Epoch [52/120    avg_loss:0.057, val_acc:0.966]
Epoch [53/120    avg_loss:0.029, val_acc:0.968]
Epoch [54/120    avg_loss:0.032, val_acc:0.980]
Epoch [55/120    avg_loss:0.015, val_acc:0.972]
Epoch [56/120    avg_loss:0.022, val_acc:0.981]
Epoch [57/120    avg_loss:0.016, val_acc:0.980]
Epoch [58/120    avg_loss:0.010, val_acc:0.987]
Epoch [59/120    avg_loss:0.031, val_acc:0.980]
Epoch [60/120    avg_loss:0.012, val_acc:0.978]
Epoch [61/120    avg_loss:0.013, val_acc:0.987]
Epoch [62/120    avg_loss:0.012, val_acc:0.981]
Epoch [63/120    avg_loss:0.010, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.986]
Epoch [65/120    avg_loss:0.010, val_acc:0.987]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.008, val_acc:0.988]
Epoch [68/120    avg_loss:0.006, val_acc:0.989]
Epoch [69/120    avg_loss:0.008, val_acc:0.989]
Epoch [70/120    avg_loss:0.006, val_acc:0.988]
Epoch [71/120    avg_loss:0.006, val_acc:0.990]
Epoch [72/120    avg_loss:0.005, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.988]
Epoch [74/120    avg_loss:0.009, val_acc:0.989]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.990]
Epoch [77/120    avg_loss:0.004, val_acc:0.990]
Epoch [78/120    avg_loss:0.009, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.006, val_acc:0.990]
Epoch [83/120    avg_loss:0.006, val_acc:0.990]
Epoch [84/120    avg_loss:0.006, val_acc:0.990]
Epoch [85/120    avg_loss:0.013, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.009, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.009, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.009, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.009, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     2     0     0     9     6     8     9]
 [    0     0 17997     0    12     0    78     0     0     3]
 [    0     1     0  2013     0     0     0     0    17     5]
 [    0    12     0     3  2939     0     3     0    10     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     1     0     0     0     0     2  1280     0     7]
 [    0     5     0    40    47     0     0     0  3466    13]
 [    0     3     0     2     0    31     0     0     0   883]]

Accuracy:
99.19263490227267

F1 scores:
[       nan 0.9956427  0.99739526 0.98291016 0.98458961 0.98826202
 0.9905555  0.99378882 0.98020362 0.95770065]

Kappa:
0.9893132799909219
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a0b73f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.335, val_acc:0.694]
Epoch [2/120    avg_loss:0.734, val_acc:0.612]
Epoch [3/120    avg_loss:0.518, val_acc:0.732]
Epoch [4/120    avg_loss:0.455, val_acc:0.807]
Epoch [5/120    avg_loss:0.384, val_acc:0.828]
Epoch [6/120    avg_loss:0.323, val_acc:0.872]
Epoch [7/120    avg_loss:0.240, val_acc:0.911]
Epoch [8/120    avg_loss:0.258, val_acc:0.868]
Epoch [9/120    avg_loss:0.153, val_acc:0.930]
Epoch [10/120    avg_loss:0.149, val_acc:0.932]
Epoch [11/120    avg_loss:0.127, val_acc:0.958]
Epoch [12/120    avg_loss:0.088, val_acc:0.903]
Epoch [13/120    avg_loss:0.164, val_acc:0.932]
Epoch [14/120    avg_loss:0.120, val_acc:0.962]
Epoch [15/120    avg_loss:0.090, val_acc:0.932]
Epoch [16/120    avg_loss:0.075, val_acc:0.951]
Epoch [17/120    avg_loss:0.072, val_acc:0.978]
Epoch [18/120    avg_loss:0.081, val_acc:0.976]
Epoch [19/120    avg_loss:0.059, val_acc:0.963]
Epoch [20/120    avg_loss:0.056, val_acc:0.978]
Epoch [21/120    avg_loss:0.104, val_acc:0.892]
Epoch [22/120    avg_loss:0.080, val_acc:0.977]
Epoch [23/120    avg_loss:0.086, val_acc:0.967]
Epoch [24/120    avg_loss:0.042, val_acc:0.977]
Epoch [25/120    avg_loss:0.054, val_acc:0.956]
Epoch [26/120    avg_loss:0.076, val_acc:0.948]
Epoch [27/120    avg_loss:0.042, val_acc:0.974]
Epoch [28/120    avg_loss:0.047, val_acc:0.966]
Epoch [29/120    avg_loss:0.037, val_acc:0.973]
Epoch [30/120    avg_loss:0.032, val_acc:0.970]
Epoch [31/120    avg_loss:0.056, val_acc:0.972]
Epoch [32/120    avg_loss:0.053, val_acc:0.976]
Epoch [33/120    avg_loss:0.031, val_acc:0.973]
Epoch [34/120    avg_loss:0.040, val_acc:0.981]
Epoch [35/120    avg_loss:0.021, val_acc:0.983]
Epoch [36/120    avg_loss:0.016, val_acc:0.987]
Epoch [37/120    avg_loss:0.015, val_acc:0.985]
Epoch [38/120    avg_loss:0.025, val_acc:0.985]
Epoch [39/120    avg_loss:0.019, val_acc:0.983]
Epoch [40/120    avg_loss:0.017, val_acc:0.986]
Epoch [41/120    avg_loss:0.018, val_acc:0.988]
Epoch [42/120    avg_loss:0.019, val_acc:0.986]
Epoch [43/120    avg_loss:0.016, val_acc:0.986]
Epoch [44/120    avg_loss:0.012, val_acc:0.987]
Epoch [45/120    avg_loss:0.016, val_acc:0.987]
Epoch [46/120    avg_loss:0.017, val_acc:0.986]
Epoch [47/120    avg_loss:0.021, val_acc:0.986]
Epoch [48/120    avg_loss:0.015, val_acc:0.986]
Epoch [49/120    avg_loss:0.018, val_acc:0.987]
Epoch [50/120    avg_loss:0.017, val_acc:0.988]
Epoch [51/120    avg_loss:0.016, val_acc:0.988]
Epoch [52/120    avg_loss:0.017, val_acc:0.987]
Epoch [53/120    avg_loss:0.015, val_acc:0.987]
Epoch [54/120    avg_loss:0.014, val_acc:0.987]
Epoch [55/120    avg_loss:0.009, val_acc:0.988]
Epoch [56/120    avg_loss:0.014, val_acc:0.986]
Epoch [57/120    avg_loss:0.018, val_acc:0.987]
Epoch [58/120    avg_loss:0.012, val_acc:0.988]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.018, val_acc:0.988]
Epoch [61/120    avg_loss:0.018, val_acc:0.987]
Epoch [62/120    avg_loss:0.015, val_acc:0.987]
Epoch [63/120    avg_loss:0.016, val_acc:0.987]
Epoch [64/120    avg_loss:0.014, val_acc:0.987]
Epoch [65/120    avg_loss:0.022, val_acc:0.987]
Epoch [66/120    avg_loss:0.015, val_acc:0.985]
Epoch [67/120    avg_loss:0.015, val_acc:0.988]
Epoch [68/120    avg_loss:0.009, val_acc:0.988]
Epoch [69/120    avg_loss:0.012, val_acc:0.989]
Epoch [70/120    avg_loss:0.020, val_acc:0.988]
Epoch [71/120    avg_loss:0.017, val_acc:0.988]
Epoch [72/120    avg_loss:0.010, val_acc:0.987]
Epoch [73/120    avg_loss:0.012, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.987]
Epoch [78/120    avg_loss:0.020, val_acc:0.988]
Epoch [79/120    avg_loss:0.017, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.014, val_acc:0.988]
Epoch [82/120    avg_loss:0.015, val_acc:0.989]
Epoch [83/120    avg_loss:0.013, val_acc:0.989]
Epoch [84/120    avg_loss:0.014, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.021, val_acc:0.988]
Epoch [87/120    avg_loss:0.011, val_acc:0.989]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.012, val_acc:0.988]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.990]
Epoch [93/120    avg_loss:0.010, val_acc:0.990]
Epoch [94/120    avg_loss:0.015, val_acc:0.992]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.013, val_acc:0.987]
Epoch [101/120    avg_loss:0.016, val_acc:0.989]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.017, val_acc:0.991]
Epoch [104/120    avg_loss:0.013, val_acc:0.990]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.017, val_acc:0.989]
Epoch [108/120    avg_loss:0.011, val_acc:0.989]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.012, val_acc:0.989]
Epoch [115/120    avg_loss:0.009, val_acc:0.989]
Epoch [116/120    avg_loss:0.011, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.989]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6352     0     3     0     0    14    25    38     0]
 [    0     1 17972     0    33     0    83     0     1     0]
 [    0     0     0  2015     0     0     0     0    14     7]
 [    0    14     0     3  2939     0     0     0     9     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4875     0     2     0]
 [    0     9     0     0     0     0     2  1270     0     9]
 [    0     5     0    26    52     0     0     0  3485     3]
 [    0     0     0     0     2    14     0     0     0   903]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.99149301 0.99670022 0.98701935 0.97999333 0.99466463
 0.98964677 0.98259188 0.97893258 0.97727273]

Kappa:
0.9879776806282191
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f99e97780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.289, val_acc:0.493]
Epoch [2/120    avg_loss:0.674, val_acc:0.657]
Epoch [3/120    avg_loss:0.502, val_acc:0.689]
Epoch [4/120    avg_loss:0.393, val_acc:0.708]
Epoch [5/120    avg_loss:0.339, val_acc:0.832]
Epoch [6/120    avg_loss:0.267, val_acc:0.839]
Epoch [7/120    avg_loss:0.287, val_acc:0.849]
Epoch [8/120    avg_loss:0.177, val_acc:0.912]
Epoch [9/120    avg_loss:0.248, val_acc:0.925]
Epoch [10/120    avg_loss:0.163, val_acc:0.932]
Epoch [11/120    avg_loss:0.167, val_acc:0.905]
Epoch [12/120    avg_loss:0.166, val_acc:0.942]
Epoch [13/120    avg_loss:0.097, val_acc:0.923]
Epoch [14/120    avg_loss:0.151, val_acc:0.946]
Epoch [15/120    avg_loss:0.091, val_acc:0.961]
Epoch [16/120    avg_loss:0.107, val_acc:0.932]
Epoch [17/120    avg_loss:0.155, val_acc:0.916]
Epoch [18/120    avg_loss:0.099, val_acc:0.948]
Epoch [19/120    avg_loss:0.098, val_acc:0.949]
Epoch [20/120    avg_loss:0.098, val_acc:0.949]
Epoch [21/120    avg_loss:0.100, val_acc:0.936]
Epoch [22/120    avg_loss:0.099, val_acc:0.973]
Epoch [23/120    avg_loss:0.057, val_acc:0.913]
Epoch [24/120    avg_loss:0.049, val_acc:0.974]
Epoch [25/120    avg_loss:0.039, val_acc:0.971]
Epoch [26/120    avg_loss:0.037, val_acc:0.967]
Epoch [27/120    avg_loss:0.034, val_acc:0.976]
Epoch [28/120    avg_loss:0.038, val_acc:0.970]
Epoch [29/120    avg_loss:0.031, val_acc:0.977]
Epoch [30/120    avg_loss:0.025, val_acc:0.982]
Epoch [31/120    avg_loss:0.040, val_acc:0.978]
Epoch [32/120    avg_loss:0.028, val_acc:0.978]
Epoch [33/120    avg_loss:0.020, val_acc:0.983]
Epoch [34/120    avg_loss:0.020, val_acc:0.980]
Epoch [35/120    avg_loss:0.026, val_acc:0.975]
Epoch [36/120    avg_loss:0.044, val_acc:0.974]
Epoch [37/120    avg_loss:0.022, val_acc:0.978]
Epoch [38/120    avg_loss:0.018, val_acc:0.981]
Epoch [39/120    avg_loss:0.018, val_acc:0.978]
Epoch [40/120    avg_loss:0.026, val_acc:0.975]
Epoch [41/120    avg_loss:0.031, val_acc:0.966]
Epoch [42/120    avg_loss:0.053, val_acc:0.978]
Epoch [43/120    avg_loss:0.018, val_acc:0.981]
Epoch [44/120    avg_loss:0.013, val_acc:0.979]
Epoch [45/120    avg_loss:0.013, val_acc:0.983]
Epoch [46/120    avg_loss:0.034, val_acc:0.977]
Epoch [47/120    avg_loss:0.021, val_acc:0.980]
Epoch [48/120    avg_loss:0.012, val_acc:0.983]
Epoch [49/120    avg_loss:0.015, val_acc:0.983]
Epoch [50/120    avg_loss:0.014, val_acc:0.983]
Epoch [51/120    avg_loss:0.008, val_acc:0.983]
Epoch [52/120    avg_loss:0.011, val_acc:0.985]
Epoch [53/120    avg_loss:0.010, val_acc:0.984]
Epoch [54/120    avg_loss:0.010, val_acc:0.983]
Epoch [55/120    avg_loss:0.015, val_acc:0.984]
Epoch [56/120    avg_loss:0.009, val_acc:0.984]
Epoch [57/120    avg_loss:0.007, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.984]
Epoch [59/120    avg_loss:0.009, val_acc:0.984]
Epoch [60/120    avg_loss:0.017, val_acc:0.986]
Epoch [61/120    avg_loss:0.007, val_acc:0.985]
Epoch [62/120    avg_loss:0.009, val_acc:0.985]
Epoch [63/120    avg_loss:0.010, val_acc:0.985]
Epoch [64/120    avg_loss:0.007, val_acc:0.986]
Epoch [65/120    avg_loss:0.005, val_acc:0.985]
Epoch [66/120    avg_loss:0.007, val_acc:0.987]
Epoch [67/120    avg_loss:0.007, val_acc:0.987]
Epoch [68/120    avg_loss:0.007, val_acc:0.986]
Epoch [69/120    avg_loss:0.008, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.986]
Epoch [71/120    avg_loss:0.008, val_acc:0.985]
Epoch [72/120    avg_loss:0.010, val_acc:0.985]
Epoch [73/120    avg_loss:0.009, val_acc:0.987]
Epoch [74/120    avg_loss:0.007, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.987]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.015, val_acc:0.988]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.019, val_acc:0.983]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.987]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.011, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.011, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.019, val_acc:0.985]
Epoch [113/120    avg_loss:0.010, val_acc:0.985]
Epoch [114/120    avg_loss:0.016, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6430     0     0     0     0     0     0     0     2]
 [    0     2 18034     0    27     0    27     0     0     0]
 [    0     2     0  2010     0     0     0     0    19     5]
 [    0    24     0     4  2937     0     0     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4875     0     1     0]
 [    0     3     0     0     0     0     0  1279     0     8]
 [    0     9     0    22    51     0     0     0  3488     1]
 [    0     0     0     0     6    13     0     0     0   900]]

Accuracy:
99.43363940905695

F1 scores:
[       nan 0.99674469 0.99839451 0.98722986 0.9801435  0.99504384
 0.99693252 0.99571818 0.98447643 0.98092643]

Kappa:
0.9924992461595677
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3d3f0a6748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.405, val_acc:0.416]
Epoch [2/120    avg_loss:0.750, val_acc:0.701]
Epoch [3/120    avg_loss:0.511, val_acc:0.705]
Epoch [4/120    avg_loss:0.404, val_acc:0.778]
Epoch [5/120    avg_loss:0.375, val_acc:0.801]
Epoch [6/120    avg_loss:0.276, val_acc:0.895]
Epoch [7/120    avg_loss:0.224, val_acc:0.892]
Epoch [8/120    avg_loss:0.194, val_acc:0.879]
Epoch [9/120    avg_loss:0.143, val_acc:0.941]
Epoch [10/120    avg_loss:0.128, val_acc:0.910]
Epoch [11/120    avg_loss:0.121, val_acc:0.944]
Epoch [12/120    avg_loss:0.123, val_acc:0.906]
Epoch [13/120    avg_loss:0.211, val_acc:0.909]
Epoch [14/120    avg_loss:0.137, val_acc:0.948]
Epoch [15/120    avg_loss:0.072, val_acc:0.952]
Epoch [16/120    avg_loss:0.065, val_acc:0.966]
Epoch [17/120    avg_loss:0.052, val_acc:0.959]
Epoch [18/120    avg_loss:0.053, val_acc:0.969]
Epoch [19/120    avg_loss:0.044, val_acc:0.975]
Epoch [20/120    avg_loss:0.054, val_acc:0.965]
Epoch [21/120    avg_loss:0.085, val_acc:0.959]
Epoch [22/120    avg_loss:0.067, val_acc:0.953]
Epoch [23/120    avg_loss:0.090, val_acc:0.963]
Epoch [24/120    avg_loss:0.051, val_acc:0.965]
Epoch [25/120    avg_loss:0.050, val_acc:0.975]
Epoch [26/120    avg_loss:0.024, val_acc:0.974]
Epoch [27/120    avg_loss:0.060, val_acc:0.933]
Epoch [28/120    avg_loss:0.132, val_acc:0.962]
Epoch [29/120    avg_loss:0.059, val_acc:0.967]
Epoch [30/120    avg_loss:0.026, val_acc:0.971]
Epoch [31/120    avg_loss:0.033, val_acc:0.970]
Epoch [32/120    avg_loss:0.033, val_acc:0.969]
Epoch [33/120    avg_loss:0.025, val_acc:0.971]
Epoch [34/120    avg_loss:0.024, val_acc:0.978]
Epoch [35/120    avg_loss:0.020, val_acc:0.974]
Epoch [36/120    avg_loss:0.024, val_acc:0.976]
Epoch [37/120    avg_loss:0.031, val_acc:0.978]
Epoch [38/120    avg_loss:0.023, val_acc:0.983]
Epoch [39/120    avg_loss:0.024, val_acc:0.978]
Epoch [40/120    avg_loss:0.014, val_acc:0.980]
Epoch [41/120    avg_loss:0.023, val_acc:0.983]
Epoch [42/120    avg_loss:0.021, val_acc:0.983]
Epoch [43/120    avg_loss:0.016, val_acc:0.982]
Epoch [44/120    avg_loss:0.012, val_acc:0.974]
Epoch [45/120    avg_loss:0.006, val_acc:0.989]
Epoch [46/120    avg_loss:0.017, val_acc:0.984]
Epoch [47/120    avg_loss:0.013, val_acc:0.981]
Epoch [48/120    avg_loss:0.015, val_acc:0.977]
Epoch [49/120    avg_loss:0.014, val_acc:0.977]
Epoch [50/120    avg_loss:0.030, val_acc:0.976]
Epoch [51/120    avg_loss:0.031, val_acc:0.981]
Epoch [52/120    avg_loss:0.012, val_acc:0.982]
Epoch [53/120    avg_loss:0.015, val_acc:0.982]
Epoch [54/120    avg_loss:0.012, val_acc:0.978]
Epoch [55/120    avg_loss:0.009, val_acc:0.983]
Epoch [56/120    avg_loss:0.013, val_acc:0.980]
Epoch [57/120    avg_loss:0.009, val_acc:0.984]
Epoch [58/120    avg_loss:0.013, val_acc:0.984]
Epoch [59/120    avg_loss:0.009, val_acc:0.984]
Epoch [60/120    avg_loss:0.005, val_acc:0.983]
Epoch [61/120    avg_loss:0.007, val_acc:0.983]
Epoch [62/120    avg_loss:0.006, val_acc:0.984]
Epoch [63/120    avg_loss:0.004, val_acc:0.984]
Epoch [64/120    avg_loss:0.005, val_acc:0.985]
Epoch [65/120    avg_loss:0.005, val_acc:0.986]
Epoch [66/120    avg_loss:0.005, val_acc:0.985]
Epoch [67/120    avg_loss:0.005, val_acc:0.987]
Epoch [68/120    avg_loss:0.007, val_acc:0.985]
Epoch [69/120    avg_loss:0.006, val_acc:0.985]
Epoch [70/120    avg_loss:0.004, val_acc:0.985]
Epoch [71/120    avg_loss:0.004, val_acc:0.985]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.005, val_acc:0.985]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.005, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.004, val_acc:0.985]
Epoch [79/120    avg_loss:0.005, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.985]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.004, val_acc:0.985]
Epoch [87/120    avg_loss:0.006, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.985]
Epoch [103/120    avg_loss:0.004, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.985]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.003, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.016, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     0     0     0     0     0     1    20     3]
 [    0     2 18063     0     6     0    18     0     1     0]
 [    0     0     0  1998     0     0     0     0    33     5]
 [    0    16     0     3  2939     0     0     0    11     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     4     0     0     0     0     1  1283     0     2]
 [    0    12     0    11    43     0     0     0  3503     2]
 [    0     0     0     0     5    31     0     0     0   883]]

Accuracy:
99.43845949919263

F1 scores:
[       nan 0.9954948  0.99925317 0.98715415 0.98541492 0.98826202
 0.99805627 0.996892   0.98136994 0.97193176]

Kappa:
0.9925610536499564
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdaca6d1780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.380, val_acc:0.683]
Epoch [2/120    avg_loss:0.740, val_acc:0.743]
Epoch [3/120    avg_loss:0.494, val_acc:0.755]
Epoch [4/120    avg_loss:0.479, val_acc:0.795]
Epoch [5/120    avg_loss:0.329, val_acc:0.914]
Epoch [6/120    avg_loss:0.252, val_acc:0.860]
Epoch [7/120    avg_loss:0.197, val_acc:0.907]
Epoch [8/120    avg_loss:0.204, val_acc:0.914]
Epoch [9/120    avg_loss:0.173, val_acc:0.891]
Epoch [10/120    avg_loss:0.176, val_acc:0.936]
Epoch [11/120    avg_loss:0.156, val_acc:0.948]
Epoch [12/120    avg_loss:0.124, val_acc:0.951]
Epoch [13/120    avg_loss:0.093, val_acc:0.922]
Epoch [14/120    avg_loss:0.102, val_acc:0.924]
Epoch [15/120    avg_loss:0.096, val_acc:0.962]
Epoch [16/120    avg_loss:0.066, val_acc:0.932]
Epoch [17/120    avg_loss:0.090, val_acc:0.959]
Epoch [18/120    avg_loss:0.080, val_acc:0.958]
Epoch [19/120    avg_loss:0.088, val_acc:0.952]
Epoch [20/120    avg_loss:0.107, val_acc:0.948]
Epoch [21/120    avg_loss:0.085, val_acc:0.950]
Epoch [22/120    avg_loss:0.090, val_acc:0.966]
Epoch [23/120    avg_loss:0.043, val_acc:0.963]
Epoch [24/120    avg_loss:0.060, val_acc:0.935]
Epoch [25/120    avg_loss:0.044, val_acc:0.957]
Epoch [26/120    avg_loss:0.043, val_acc:0.968]
Epoch [27/120    avg_loss:0.074, val_acc:0.957]
Epoch [28/120    avg_loss:0.117, val_acc:0.944]
Epoch [29/120    avg_loss:0.082, val_acc:0.968]
Epoch [30/120    avg_loss:0.032, val_acc:0.976]
Epoch [31/120    avg_loss:0.031, val_acc:0.968]
Epoch [32/120    avg_loss:0.039, val_acc:0.969]
Epoch [33/120    avg_loss:0.032, val_acc:0.966]
Epoch [34/120    avg_loss:0.037, val_acc:0.973]
Epoch [35/120    avg_loss:0.017, val_acc:0.980]
Epoch [36/120    avg_loss:0.032, val_acc:0.965]
Epoch [37/120    avg_loss:0.033, val_acc:0.973]
Epoch [38/120    avg_loss:0.033, val_acc:0.965]
Epoch [39/120    avg_loss:0.049, val_acc:0.968]
Epoch [40/120    avg_loss:0.038, val_acc:0.978]
Epoch [41/120    avg_loss:0.043, val_acc:0.978]
Epoch [42/120    avg_loss:0.032, val_acc:0.959]
Epoch [43/120    avg_loss:0.086, val_acc:0.959]
Epoch [44/120    avg_loss:0.064, val_acc:0.974]
Epoch [45/120    avg_loss:0.045, val_acc:0.974]
Epoch [46/120    avg_loss:0.034, val_acc:0.948]
Epoch [47/120    avg_loss:0.025, val_acc:0.974]
Epoch [48/120    avg_loss:0.017, val_acc:0.980]
Epoch [49/120    avg_loss:0.044, val_acc:0.961]
Epoch [50/120    avg_loss:0.039, val_acc:0.972]
Epoch [51/120    avg_loss:0.030, val_acc:0.949]
Epoch [52/120    avg_loss:0.020, val_acc:0.977]
Epoch [53/120    avg_loss:0.008, val_acc:0.982]
Epoch [54/120    avg_loss:0.012, val_acc:0.985]
Epoch [55/120    avg_loss:0.008, val_acc:0.983]
Epoch [56/120    avg_loss:0.012, val_acc:0.982]
Epoch [57/120    avg_loss:0.017, val_acc:0.975]
Epoch [58/120    avg_loss:0.017, val_acc:0.978]
Epoch [59/120    avg_loss:0.022, val_acc:0.978]
Epoch [60/120    avg_loss:0.016, val_acc:0.976]
Epoch [61/120    avg_loss:0.011, val_acc:0.979]
Epoch [62/120    avg_loss:0.010, val_acc:0.985]
Epoch [63/120    avg_loss:0.007, val_acc:0.983]
Epoch [64/120    avg_loss:0.007, val_acc:0.983]
Epoch [65/120    avg_loss:0.009, val_acc:0.983]
Epoch [66/120    avg_loss:0.014, val_acc:0.986]
Epoch [67/120    avg_loss:0.008, val_acc:0.978]
Epoch [68/120    avg_loss:0.019, val_acc:0.980]
Epoch [69/120    avg_loss:0.015, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.988]
Epoch [71/120    avg_loss:0.007, val_acc:0.982]
Epoch [72/120    avg_loss:0.006, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.977]
Epoch [76/120    avg_loss:0.006, val_acc:0.983]
Epoch [77/120    avg_loss:0.015, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.979]
Epoch [80/120    avg_loss:0.017, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.989]
Epoch [82/120    avg_loss:0.005, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.988]
Epoch [84/120    avg_loss:0.004, val_acc:0.990]
Epoch [85/120    avg_loss:0.005, val_acc:0.990]
Epoch [86/120    avg_loss:0.017, val_acc:0.991]
Epoch [87/120    avg_loss:0.009, val_acc:0.991]
Epoch [88/120    avg_loss:0.019, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.983]
Epoch [92/120    avg_loss:0.016, val_acc:0.986]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.990]
Epoch [96/120    avg_loss:0.003, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.976]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.004, val_acc:0.983]
Epoch [103/120    avg_loss:0.004, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.004, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.002, val_acc:0.992]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.991]
Epoch [116/120    avg_loss:0.002, val_acc:0.992]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.002, val_acc:0.992]
Epoch [119/120    avg_loss:0.002, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0     4    20     6     2]
 [    0     0 18053     0    14     0    22     0     1     0]
 [    0     0     0  2017     0     0     0     0    18     1]
 [    0    22     0     3  2933     0     1     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     2     0     0     0     0     2  1285     0     1]
 [    0     4     0    23    31     0     0     0  3513     0]
 [    0     0     0     0     1    20     0     0     0   898]]

Accuracy:
99.49148049068518

F1 scores:
[       nan 0.99533437 0.99897629 0.98896788 0.98571669 0.99239544
 0.99703628 0.99036609 0.98652064 0.98627128]

Kappa:
0.9932647129111577
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd3308397b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.344, val_acc:0.439]
Epoch [2/120    avg_loss:0.679, val_acc:0.666]
Epoch [3/120    avg_loss:0.507, val_acc:0.689]
Epoch [4/120    avg_loss:0.483, val_acc:0.812]
Epoch [5/120    avg_loss:0.380, val_acc:0.810]
Epoch [6/120    avg_loss:0.360, val_acc:0.842]
Epoch [7/120    avg_loss:0.283, val_acc:0.838]
Epoch [8/120    avg_loss:0.229, val_acc:0.929]
Epoch [9/120    avg_loss:0.172, val_acc:0.933]
Epoch [10/120    avg_loss:0.146, val_acc:0.952]
Epoch [11/120    avg_loss:0.118, val_acc:0.917]
Epoch [12/120    avg_loss:0.111, val_acc:0.957]
Epoch [13/120    avg_loss:0.099, val_acc:0.927]
Epoch [14/120    avg_loss:0.100, val_acc:0.943]
Epoch [15/120    avg_loss:0.060, val_acc:0.942]
Epoch [16/120    avg_loss:0.059, val_acc:0.953]
Epoch [17/120    avg_loss:0.050, val_acc:0.950]
Epoch [18/120    avg_loss:0.070, val_acc:0.957]
Epoch [19/120    avg_loss:0.040, val_acc:0.965]
Epoch [20/120    avg_loss:0.049, val_acc:0.955]
Epoch [21/120    avg_loss:0.059, val_acc:0.953]
Epoch [22/120    avg_loss:0.044, val_acc:0.960]
Epoch [23/120    avg_loss:0.049, val_acc:0.950]
Epoch [24/120    avg_loss:0.029, val_acc:0.969]
Epoch [25/120    avg_loss:0.028, val_acc:0.971]
Epoch [26/120    avg_loss:0.026, val_acc:0.980]
Epoch [27/120    avg_loss:0.025, val_acc:0.974]
Epoch [28/120    avg_loss:0.038, val_acc:0.975]
Epoch [29/120    avg_loss:0.038, val_acc:0.964]
Epoch [30/120    avg_loss:0.035, val_acc:0.980]
Epoch [31/120    avg_loss:0.051, val_acc:0.930]
Epoch [32/120    avg_loss:0.096, val_acc:0.937]
Epoch [33/120    avg_loss:0.144, val_acc:0.957]
Epoch [34/120    avg_loss:0.048, val_acc:0.972]
Epoch [35/120    avg_loss:0.042, val_acc:0.969]
Epoch [36/120    avg_loss:0.027, val_acc:0.980]
Epoch [37/120    avg_loss:0.025, val_acc:0.978]
Epoch [38/120    avg_loss:0.017, val_acc:0.980]
Epoch [39/120    avg_loss:0.035, val_acc:0.976]
Epoch [40/120    avg_loss:0.021, val_acc:0.973]
Epoch [41/120    avg_loss:0.021, val_acc:0.965]
Epoch [42/120    avg_loss:0.028, val_acc:0.977]
Epoch [43/120    avg_loss:0.016, val_acc:0.979]
Epoch [44/120    avg_loss:0.012, val_acc:0.982]
Epoch [45/120    avg_loss:0.015, val_acc:0.978]
Epoch [46/120    avg_loss:0.025, val_acc:0.965]
Epoch [47/120    avg_loss:0.010, val_acc:0.983]
Epoch [48/120    avg_loss:0.017, val_acc:0.979]
Epoch [49/120    avg_loss:0.007, val_acc:0.986]
Epoch [50/120    avg_loss:0.010, val_acc:0.983]
Epoch [51/120    avg_loss:0.007, val_acc:0.984]
Epoch [52/120    avg_loss:0.009, val_acc:0.982]
Epoch [53/120    avg_loss:0.006, val_acc:0.984]
Epoch [54/120    avg_loss:0.022, val_acc:0.981]
Epoch [55/120    avg_loss:0.020, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.976]
Epoch [57/120    avg_loss:0.009, val_acc:0.973]
Epoch [58/120    avg_loss:0.012, val_acc:0.976]
Epoch [59/120    avg_loss:0.046, val_acc:0.986]
Epoch [60/120    avg_loss:0.016, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.008, val_acc:0.985]
Epoch [63/120    avg_loss:0.009, val_acc:0.986]
Epoch [64/120    avg_loss:0.006, val_acc:0.982]
Epoch [65/120    avg_loss:0.005, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.986]
Epoch [67/120    avg_loss:0.008, val_acc:0.983]
Epoch [68/120    avg_loss:0.008, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.980]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.005, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.989]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.005, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.008, val_acc:0.965]
Epoch [77/120    avg_loss:0.010, val_acc:0.981]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.004, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.988]
Epoch [82/120    avg_loss:0.003, val_acc:0.986]
Epoch [83/120    avg_loss:0.005, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.004, val_acc:0.987]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.003, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.003, val_acc:0.986]
Epoch [93/120    avg_loss:0.002, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.002, val_acc:0.986]
Epoch [97/120    avg_loss:0.004, val_acc:0.986]
Epoch [98/120    avg_loss:0.002, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.003, val_acc:0.986]
Epoch [102/120    avg_loss:0.003, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.003, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.002, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.002, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.002, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.002, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     1     0     0     1     7    17     0]
 [    0     0 18060     0     8     0    21     0     1     0]
 [    0     2     0  2011     1     0     0     0    15     7]
 [    0    25     1     6  2926     0     3     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4872     0     0     0]
 [    0     5     0     0     0     0     1  1283     0     1]
 [    0     5     0    12    42     0     0     0  3511     1]
 [    0     0     0     0     7    12     0     0     0   900]]

Accuracy:
99.47220013014244

F1 scores:
[       nan 0.9951068  0.99897669 0.98917855 0.98253862 0.99542334
 0.99672668 0.99457364 0.98540556 0.98468271]

Kappa:
0.9930078126800449
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1aa8125780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.293, val_acc:0.460]
Epoch [2/120    avg_loss:0.735, val_acc:0.596]
Epoch [3/120    avg_loss:0.490, val_acc:0.789]
Epoch [4/120    avg_loss:0.549, val_acc:0.834]
Epoch [5/120    avg_loss:0.395, val_acc:0.815]
Epoch [6/120    avg_loss:0.278, val_acc:0.918]
Epoch [7/120    avg_loss:0.239, val_acc:0.901]
Epoch [8/120    avg_loss:0.181, val_acc:0.952]
Epoch [9/120    avg_loss:0.211, val_acc:0.953]
Epoch [10/120    avg_loss:0.151, val_acc:0.967]
Epoch [11/120    avg_loss:0.137, val_acc:0.937]
Epoch [12/120    avg_loss:0.117, val_acc:0.929]
Epoch [13/120    avg_loss:0.178, val_acc:0.946]
Epoch [14/120    avg_loss:0.094, val_acc:0.955]
Epoch [15/120    avg_loss:0.076, val_acc:0.966]
Epoch [16/120    avg_loss:0.069, val_acc:0.964]
Epoch [17/120    avg_loss:0.040, val_acc:0.969]
Epoch [18/120    avg_loss:0.099, val_acc:0.974]
Epoch [19/120    avg_loss:0.082, val_acc:0.967]
Epoch [20/120    avg_loss:0.072, val_acc:0.974]
Epoch [21/120    avg_loss:0.067, val_acc:0.980]
Epoch [22/120    avg_loss:0.055, val_acc:0.973]
Epoch [23/120    avg_loss:0.051, val_acc:0.972]
Epoch [24/120    avg_loss:0.047, val_acc:0.980]
Epoch [25/120    avg_loss:0.032, val_acc:0.970]
Epoch [26/120    avg_loss:0.041, val_acc:0.972]
Epoch [27/120    avg_loss:0.086, val_acc:0.972]
Epoch [28/120    avg_loss:0.033, val_acc:0.985]
Epoch [29/120    avg_loss:0.028, val_acc:0.986]
Epoch [30/120    avg_loss:0.022, val_acc:0.980]
Epoch [31/120    avg_loss:0.035, val_acc:0.968]
Epoch [32/120    avg_loss:0.053, val_acc:0.980]
Epoch [33/120    avg_loss:0.029, val_acc:0.986]
Epoch [34/120    avg_loss:0.032, val_acc:0.944]
Epoch [35/120    avg_loss:0.025, val_acc:0.984]
Epoch [36/120    avg_loss:0.016, val_acc:0.975]
Epoch [37/120    avg_loss:0.025, val_acc:0.986]
Epoch [38/120    avg_loss:0.025, val_acc:0.983]
Epoch [39/120    avg_loss:0.017, val_acc:0.981]
Epoch [40/120    avg_loss:0.011, val_acc:0.988]
Epoch [41/120    avg_loss:0.024, val_acc:0.984]
Epoch [42/120    avg_loss:0.031, val_acc:0.986]
Epoch [43/120    avg_loss:0.037, val_acc:0.969]
Epoch [44/120    avg_loss:0.045, val_acc:0.976]
Epoch [45/120    avg_loss:0.064, val_acc:0.979]
Epoch [46/120    avg_loss:0.039, val_acc:0.984]
Epoch [47/120    avg_loss:0.016, val_acc:0.981]
Epoch [48/120    avg_loss:0.014, val_acc:0.986]
Epoch [49/120    avg_loss:0.023, val_acc:0.983]
Epoch [50/120    avg_loss:0.012, val_acc:0.987]
Epoch [51/120    avg_loss:0.016, val_acc:0.986]
Epoch [52/120    avg_loss:0.011, val_acc:0.988]
Epoch [53/120    avg_loss:0.010, val_acc:0.983]
Epoch [54/120    avg_loss:0.014, val_acc:0.988]
Epoch [55/120    avg_loss:0.034, val_acc:0.984]
Epoch [56/120    avg_loss:0.016, val_acc:0.981]
Epoch [57/120    avg_loss:0.010, val_acc:0.987]
Epoch [58/120    avg_loss:0.009, val_acc:0.986]
Epoch [59/120    avg_loss:0.008, val_acc:0.987]
Epoch [60/120    avg_loss:0.021, val_acc:0.970]
Epoch [61/120    avg_loss:0.013, val_acc:0.986]
Epoch [62/120    avg_loss:0.009, val_acc:0.986]
Epoch [63/120    avg_loss:0.005, val_acc:0.986]
Epoch [64/120    avg_loss:0.011, val_acc:0.986]
Epoch [65/120    avg_loss:0.013, val_acc:0.987]
Epoch [66/120    avg_loss:0.007, val_acc:0.986]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.007, val_acc:0.985]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.007, val_acc:0.985]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.004, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.005, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.987]
Epoch [76/120    avg_loss:0.004, val_acc:0.987]
Epoch [77/120    avg_loss:0.006, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.987]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.003, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.004, val_acc:0.987]
Epoch [88/120    avg_loss:0.005, val_acc:0.987]
Epoch [89/120    avg_loss:0.003, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.003, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.003, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.012, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     0     0     0     0     9     0     2]
 [    0     3 18069     0     7     0    11     0     0     0]
 [    0     0     0  1988     0     0     0     0    39     9]
 [    0    30     2     0  2923     0     0     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4866     0    10     0]
 [    0     0     0     0     0     0     7  1279     0     4]
 [    0     3     0    10    47     0     0     0  3508     3]
 [    0     0     0     2     9    35     0     0     0   873]]

Accuracy:
99.37097823729304

F1 scores:
[       nan 0.99635348 0.99930869 0.9851338  0.98120175 0.98676749
 0.99692686 0.99224205 0.98194542 0.96464088]

Kappa:
0.9916658582895109
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa574abf780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.399, val_acc:0.377]
Epoch [2/120    avg_loss:0.760, val_acc:0.772]
Epoch [3/120    avg_loss:0.554, val_acc:0.727]
Epoch [4/120    avg_loss:0.400, val_acc:0.795]
Epoch [5/120    avg_loss:0.341, val_acc:0.837]
Epoch [6/120    avg_loss:0.260, val_acc:0.892]
Epoch [7/120    avg_loss:0.232, val_acc:0.931]
Epoch [8/120    avg_loss:0.174, val_acc:0.905]
Epoch [9/120    avg_loss:0.135, val_acc:0.943]
Epoch [10/120    avg_loss:0.143, val_acc:0.937]
Epoch [11/120    avg_loss:0.128, val_acc:0.935]
Epoch [12/120    avg_loss:0.138, val_acc:0.912]
Epoch [13/120    avg_loss:0.191, val_acc:0.896]
Epoch [14/120    avg_loss:0.194, val_acc:0.936]
Epoch [15/120    avg_loss:0.102, val_acc:0.962]
Epoch [16/120    avg_loss:0.063, val_acc:0.952]
Epoch [17/120    avg_loss:0.065, val_acc:0.968]
Epoch [18/120    avg_loss:0.066, val_acc:0.967]
Epoch [19/120    avg_loss:0.045, val_acc:0.965]
Epoch [20/120    avg_loss:0.061, val_acc:0.972]
Epoch [21/120    avg_loss:0.049, val_acc:0.974]
Epoch [22/120    avg_loss:0.036, val_acc:0.966]
Epoch [23/120    avg_loss:0.042, val_acc:0.968]
Epoch [24/120    avg_loss:0.027, val_acc:0.981]
Epoch [25/120    avg_loss:0.037, val_acc:0.977]
Epoch [26/120    avg_loss:0.056, val_acc:0.943]
Epoch [27/120    avg_loss:0.030, val_acc:0.976]
Epoch [28/120    avg_loss:0.035, val_acc:0.981]
Epoch [29/120    avg_loss:0.029, val_acc:0.973]
Epoch [30/120    avg_loss:0.027, val_acc:0.974]
Epoch [31/120    avg_loss:0.020, val_acc:0.980]
Epoch [32/120    avg_loss:0.018, val_acc:0.978]
Epoch [33/120    avg_loss:0.020, val_acc:0.983]
Epoch [34/120    avg_loss:0.014, val_acc:0.983]
Epoch [35/120    avg_loss:0.035, val_acc:0.983]
Epoch [36/120    avg_loss:0.027, val_acc:0.981]
Epoch [37/120    avg_loss:0.039, val_acc:0.980]
Epoch [38/120    avg_loss:0.024, val_acc:0.969]
Epoch [39/120    avg_loss:0.020, val_acc:0.980]
Epoch [40/120    avg_loss:0.013, val_acc:0.983]
Epoch [41/120    avg_loss:0.018, val_acc:0.985]
Epoch [42/120    avg_loss:0.020, val_acc:0.979]
Epoch [43/120    avg_loss:0.018, val_acc:0.978]
Epoch [44/120    avg_loss:0.023, val_acc:0.979]
Epoch [45/120    avg_loss:0.008, val_acc:0.987]
Epoch [46/120    avg_loss:0.008, val_acc:0.986]
Epoch [47/120    avg_loss:0.009, val_acc:0.987]
Epoch [48/120    avg_loss:0.012, val_acc:0.970]
Epoch [49/120    avg_loss:0.020, val_acc:0.980]
Epoch [50/120    avg_loss:0.011, val_acc:0.987]
Epoch [51/120    avg_loss:0.020, val_acc:0.978]
Epoch [52/120    avg_loss:0.013, val_acc:0.984]
Epoch [53/120    avg_loss:0.006, val_acc:0.986]
Epoch [54/120    avg_loss:0.008, val_acc:0.991]
Epoch [55/120    avg_loss:0.008, val_acc:0.987]
Epoch [56/120    avg_loss:0.018, val_acc:0.963]
Epoch [57/120    avg_loss:0.055, val_acc:0.974]
Epoch [58/120    avg_loss:0.039, val_acc:0.982]
Epoch [59/120    avg_loss:0.027, val_acc:0.975]
Epoch [60/120    avg_loss:0.024, val_acc:0.979]
Epoch [61/120    avg_loss:0.021, val_acc:0.983]
Epoch [62/120    avg_loss:0.012, val_acc:0.990]
Epoch [63/120    avg_loss:0.011, val_acc:0.991]
Epoch [64/120    avg_loss:0.012, val_acc:0.987]
Epoch [65/120    avg_loss:0.005, val_acc:0.988]
Epoch [66/120    avg_loss:0.008, val_acc:0.988]
Epoch [67/120    avg_loss:0.012, val_acc:0.988]
Epoch [68/120    avg_loss:0.007, val_acc:0.987]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.005, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.004, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.989]
Epoch [76/120    avg_loss:0.004, val_acc:0.989]
Epoch [77/120    avg_loss:0.005, val_acc:0.989]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.989]
Epoch [81/120    avg_loss:0.006, val_acc:0.989]
Epoch [82/120    avg_loss:0.008, val_acc:0.989]
Epoch [83/120    avg_loss:0.009, val_acc:0.989]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.007, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.005, val_acc:0.989]
Epoch [88/120    avg_loss:0.004, val_acc:0.989]
Epoch [89/120    avg_loss:0.009, val_acc:0.989]
Epoch [90/120    avg_loss:0.006, val_acc:0.989]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.989]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.012, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     0     4     0    21     1    14     7]
 [    0     0 18041     0    17     0    21     0     5     6]
 [    0     0     0  2023     1     0     0     0    12     0]
 [    0    29     5     0  2914     0     0     0    23     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4860     0     5     0]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0     2     0     3    31     0     0     0  3531     4]
 [    0     0     0     0    12    21     0     0     0   886]]

Accuracy:
99.37097823729304

F1 scores:
[       nan 0.99392902 0.99814656 0.99606105 0.9793312  0.99201824
 0.99376342 0.99844841 0.98617512 0.9709589 ]

Kappa:
0.9916680352056376
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd9906217b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.328, val_acc:0.641]
Epoch [2/120    avg_loss:0.666, val_acc:0.761]
Epoch [3/120    avg_loss:0.546, val_acc:0.767]
Epoch [4/120    avg_loss:0.413, val_acc:0.783]
Epoch [5/120    avg_loss:0.390, val_acc:0.791]
Epoch [6/120    avg_loss:0.336, val_acc:0.835]
Epoch [7/120    avg_loss:0.237, val_acc:0.862]
Epoch [8/120    avg_loss:0.165, val_acc:0.920]
Epoch [9/120    avg_loss:0.164, val_acc:0.938]
Epoch [10/120    avg_loss:0.163, val_acc:0.923]
Epoch [11/120    avg_loss:0.105, val_acc:0.917]
Epoch [12/120    avg_loss:0.100, val_acc:0.943]
Epoch [13/120    avg_loss:0.100, val_acc:0.933]
Epoch [14/120    avg_loss:0.129, val_acc:0.918]
Epoch [15/120    avg_loss:0.105, val_acc:0.945]
Epoch [16/120    avg_loss:0.077, val_acc:0.947]
Epoch [17/120    avg_loss:0.099, val_acc:0.957]
Epoch [18/120    avg_loss:0.070, val_acc:0.943]
Epoch [19/120    avg_loss:0.045, val_acc:0.964]
Epoch [20/120    avg_loss:0.049, val_acc:0.958]
Epoch [21/120    avg_loss:0.045, val_acc:0.966]
Epoch [22/120    avg_loss:0.046, val_acc:0.955]
Epoch [23/120    avg_loss:0.060, val_acc:0.923]
Epoch [24/120    avg_loss:0.088, val_acc:0.962]
Epoch [25/120    avg_loss:0.062, val_acc:0.965]
Epoch [26/120    avg_loss:0.053, val_acc:0.956]
Epoch [27/120    avg_loss:0.036, val_acc:0.963]
Epoch [28/120    avg_loss:0.027, val_acc:0.971]
Epoch [29/120    avg_loss:0.026, val_acc:0.964]
Epoch [30/120    avg_loss:0.038, val_acc:0.969]
Epoch [31/120    avg_loss:0.029, val_acc:0.960]
Epoch [32/120    avg_loss:0.022, val_acc:0.971]
Epoch [33/120    avg_loss:0.013, val_acc:0.968]
Epoch [34/120    avg_loss:0.031, val_acc:0.975]
Epoch [35/120    avg_loss:0.048, val_acc:0.940]
Epoch [36/120    avg_loss:0.091, val_acc:0.940]
Epoch [37/120    avg_loss:0.093, val_acc:0.953]
Epoch [38/120    avg_loss:0.031, val_acc:0.975]
Epoch [39/120    avg_loss:0.031, val_acc:0.971]
Epoch [40/120    avg_loss:0.032, val_acc:0.979]
Epoch [41/120    avg_loss:0.033, val_acc:0.974]
Epoch [42/120    avg_loss:0.021, val_acc:0.982]
Epoch [43/120    avg_loss:0.014, val_acc:0.975]
Epoch [44/120    avg_loss:0.017, val_acc:0.974]
Epoch [45/120    avg_loss:0.027, val_acc:0.974]
Epoch [46/120    avg_loss:0.013, val_acc:0.975]
Epoch [47/120    avg_loss:0.023, val_acc:0.969]
Epoch [48/120    avg_loss:0.019, val_acc:0.978]
Epoch [49/120    avg_loss:0.028, val_acc:0.971]
Epoch [50/120    avg_loss:0.020, val_acc:0.974]
Epoch [51/120    avg_loss:0.027, val_acc:0.981]
Epoch [52/120    avg_loss:0.015, val_acc:0.980]
Epoch [53/120    avg_loss:0.011, val_acc:0.980]
Epoch [54/120    avg_loss:0.011, val_acc:0.982]
Epoch [55/120    avg_loss:0.019, val_acc:0.970]
Epoch [56/120    avg_loss:0.013, val_acc:0.984]
Epoch [57/120    avg_loss:0.012, val_acc:0.981]
Epoch [58/120    avg_loss:0.009, val_acc:0.983]
Epoch [59/120    avg_loss:0.012, val_acc:0.980]
Epoch [60/120    avg_loss:0.009, val_acc:0.980]
Epoch [61/120    avg_loss:0.006, val_acc:0.980]
Epoch [62/120    avg_loss:0.010, val_acc:0.965]
Epoch [63/120    avg_loss:0.006, val_acc:0.978]
Epoch [64/120    avg_loss:0.009, val_acc:0.981]
Epoch [65/120    avg_loss:0.024, val_acc:0.984]
Epoch [66/120    avg_loss:0.007, val_acc:0.982]
Epoch [67/120    avg_loss:0.010, val_acc:0.979]
Epoch [68/120    avg_loss:0.007, val_acc:0.982]
Epoch [69/120    avg_loss:0.004, val_acc:0.984]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.015, val_acc:0.957]
Epoch [72/120    avg_loss:0.011, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.016, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.005, val_acc:0.985]
Epoch [77/120    avg_loss:0.007, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.984]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.979]
Epoch [83/120    avg_loss:0.007, val_acc:0.987]
Epoch [84/120    avg_loss:0.003, val_acc:0.989]
Epoch [85/120    avg_loss:0.004, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.989]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.022, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.991]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.005, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.002, val_acc:0.989]
Epoch [98/120    avg_loss:0.003, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.990]
Epoch [102/120    avg_loss:0.009, val_acc:0.958]
Epoch [103/120    avg_loss:0.009, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.003, val_acc:0.987]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.003, val_acc:0.986]
Epoch [112/120    avg_loss:0.002, val_acc:0.987]
Epoch [113/120    avg_loss:0.003, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.002, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6428     0     0     1     0     0     0     0     3]
 [    0     0 18050     0    22     0    18     0     0     0]
 [    0     0     0  1987     0     0     0     0    43     6]
 [    0    22     0     0  2931     0     0     0    12     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     8     0     8    41     0     0     0  3503    11]
 [    0     0     0     0     3    19     0     0     0   897]]

Accuracy:
99.45050972453186

F1 scores:
[       nan 0.9973623  0.99889319 0.98585959 0.98190955 0.99277292
 0.99795334 0.9992242  0.9824709  0.97235772]

Kappa:
0.9927216956328463
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f186e1a77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.353, val_acc:0.659]
Epoch [2/120    avg_loss:0.743, val_acc:0.608]
Epoch [3/120    avg_loss:0.535, val_acc:0.683]
Epoch [4/120    avg_loss:0.473, val_acc:0.799]
Epoch [5/120    avg_loss:0.440, val_acc:0.878]
Epoch [6/120    avg_loss:0.311, val_acc:0.836]
Epoch [7/120    avg_loss:0.268, val_acc:0.922]
Epoch [8/120    avg_loss:0.187, val_acc:0.901]
Epoch [9/120    avg_loss:0.212, val_acc:0.890]
Epoch [10/120    avg_loss:0.144, val_acc:0.965]
Epoch [11/120    avg_loss:0.181, val_acc:0.953]
Epoch [12/120    avg_loss:0.115, val_acc:0.940]
Epoch [13/120    avg_loss:0.101, val_acc:0.956]
Epoch [14/120    avg_loss:0.096, val_acc:0.953]
Epoch [15/120    avg_loss:0.114, val_acc:0.921]
Epoch [16/120    avg_loss:0.086, val_acc:0.957]
Epoch [17/120    avg_loss:0.079, val_acc:0.972]
Epoch [18/120    avg_loss:0.089, val_acc:0.952]
Epoch [19/120    avg_loss:0.078, val_acc:0.963]
Epoch [20/120    avg_loss:0.085, val_acc:0.968]
Epoch [21/120    avg_loss:0.066, val_acc:0.969]
Epoch [22/120    avg_loss:0.044, val_acc:0.974]
Epoch [23/120    avg_loss:0.037, val_acc:0.982]
Epoch [24/120    avg_loss:0.048, val_acc:0.982]
Epoch [25/120    avg_loss:0.069, val_acc:0.947]
Epoch [26/120    avg_loss:0.051, val_acc:0.968]
Epoch [27/120    avg_loss:0.069, val_acc:0.976]
Epoch [28/120    avg_loss:0.048, val_acc:0.976]
Epoch [29/120    avg_loss:0.042, val_acc:0.944]
Epoch [30/120    avg_loss:0.034, val_acc:0.984]
Epoch [31/120    avg_loss:0.031, val_acc:0.986]
Epoch [32/120    avg_loss:0.067, val_acc:0.980]
Epoch [33/120    avg_loss:0.048, val_acc:0.977]
Epoch [34/120    avg_loss:0.078, val_acc:0.980]
Epoch [35/120    avg_loss:0.028, val_acc:0.983]
Epoch [36/120    avg_loss:0.020, val_acc:0.986]
Epoch [37/120    avg_loss:0.026, val_acc:0.980]
Epoch [38/120    avg_loss:0.034, val_acc:0.982]
Epoch [39/120    avg_loss:0.044, val_acc:0.984]
Epoch [40/120    avg_loss:0.030, val_acc:0.988]
Epoch [41/120    avg_loss:0.026, val_acc:0.989]
Epoch [42/120    avg_loss:0.026, val_acc:0.986]
Epoch [43/120    avg_loss:0.025, val_acc:0.986]
Epoch [44/120    avg_loss:0.027, val_acc:0.987]
Epoch [45/120    avg_loss:0.014, val_acc:0.989]
Epoch [46/120    avg_loss:0.014, val_acc:0.989]
Epoch [47/120    avg_loss:0.018, val_acc:0.990]
Epoch [48/120    avg_loss:0.024, val_acc:0.988]
Epoch [49/120    avg_loss:0.011, val_acc:0.990]
Epoch [50/120    avg_loss:0.015, val_acc:0.985]
Epoch [51/120    avg_loss:0.007, val_acc:0.989]
Epoch [52/120    avg_loss:0.012, val_acc:0.991]
Epoch [53/120    avg_loss:0.008, val_acc:0.991]
Epoch [54/120    avg_loss:0.007, val_acc:0.990]
Epoch [55/120    avg_loss:0.009, val_acc:0.990]
Epoch [56/120    avg_loss:0.013, val_acc:0.980]
Epoch [57/120    avg_loss:0.020, val_acc:0.991]
Epoch [58/120    avg_loss:0.008, val_acc:0.988]
Epoch [59/120    avg_loss:0.013, val_acc:0.989]
Epoch [60/120    avg_loss:0.008, val_acc:0.991]
Epoch [61/120    avg_loss:0.006, val_acc:0.991]
Epoch [62/120    avg_loss:0.005, val_acc:0.989]
Epoch [63/120    avg_loss:0.022, val_acc:0.983]
Epoch [64/120    avg_loss:0.011, val_acc:0.992]
Epoch [65/120    avg_loss:0.022, val_acc:0.979]
Epoch [66/120    avg_loss:0.030, val_acc:0.967]
Epoch [67/120    avg_loss:0.155, val_acc:0.962]
Epoch [68/120    avg_loss:0.058, val_acc:0.970]
Epoch [69/120    avg_loss:0.023, val_acc:0.986]
Epoch [70/120    avg_loss:0.015, val_acc:0.988]
Epoch [71/120    avg_loss:0.015, val_acc:0.987]
Epoch [72/120    avg_loss:0.036, val_acc:0.979]
Epoch [73/120    avg_loss:0.041, val_acc:0.977]
Epoch [74/120    avg_loss:0.016, val_acc:0.985]
Epoch [75/120    avg_loss:0.022, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.989]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.011, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.989]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.004, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.003, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6404     0     0     2     0    18     2     0     6]
 [    0     2 18059     0    12     0    16     0     0     1]
 [    0     0     0  2033     0     0     0     0     1     2]
 [    0    19     8     3  2922     0     0     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4874     0     0     4]
 [    0     4     0     0     0     0     0  1284     0     2]
 [    0     1     0    27    39     0     0     0  3480    24]
 [    0     0     0     0    11    24     0     0     0   884]]

Accuracy:
99.40230882317499

F1 scores:
[       nan 0.99580159 0.99892137 0.99194926 0.98086606 0.99088838
 0.9961169  0.99689441 0.9841629  0.95982628]

Kappa:
0.9920827057166388
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb488cc1748>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.317, val_acc:0.604]
Epoch [2/120    avg_loss:0.652, val_acc:0.611]
Epoch [3/120    avg_loss:0.471, val_acc:0.729]
Epoch [4/120    avg_loss:0.360, val_acc:0.837]
Epoch [5/120    avg_loss:0.326, val_acc:0.865]
Epoch [6/120    avg_loss:0.245, val_acc:0.929]
Epoch [7/120    avg_loss:0.203, val_acc:0.908]
Epoch [8/120    avg_loss:0.268, val_acc:0.913]
Epoch [9/120    avg_loss:0.226, val_acc:0.950]
Epoch [10/120    avg_loss:0.138, val_acc:0.953]
Epoch [11/120    avg_loss:0.126, val_acc:0.963]
Epoch [12/120    avg_loss:0.135, val_acc:0.945]
Epoch [13/120    avg_loss:0.102, val_acc:0.944]
Epoch [14/120    avg_loss:0.134, val_acc:0.925]
Epoch [15/120    avg_loss:0.108, val_acc:0.974]
Epoch [16/120    avg_loss:0.078, val_acc:0.959]
Epoch [17/120    avg_loss:0.070, val_acc:0.970]
Epoch [18/120    avg_loss:0.086, val_acc:0.968]
Epoch [19/120    avg_loss:0.061, val_acc:0.976]
Epoch [20/120    avg_loss:0.039, val_acc:0.973]
Epoch [21/120    avg_loss:0.041, val_acc:0.980]
Epoch [22/120    avg_loss:0.030, val_acc:0.976]
Epoch [23/120    avg_loss:0.045, val_acc:0.976]
Epoch [24/120    avg_loss:0.045, val_acc:0.963]
Epoch [25/120    avg_loss:0.038, val_acc:0.973]
Epoch [26/120    avg_loss:0.034, val_acc:0.968]
Epoch [27/120    avg_loss:0.046, val_acc:0.984]
Epoch [28/120    avg_loss:0.026, val_acc:0.986]
Epoch [29/120    avg_loss:0.025, val_acc:0.983]
Epoch [30/120    avg_loss:0.029, val_acc:0.977]
Epoch [31/120    avg_loss:0.031, val_acc:0.974]
Epoch [32/120    avg_loss:0.040, val_acc:0.979]
Epoch [33/120    avg_loss:0.086, val_acc:0.968]
Epoch [34/120    avg_loss:0.037, val_acc:0.980]
Epoch [35/120    avg_loss:0.049, val_acc:0.965]
Epoch [36/120    avg_loss:0.064, val_acc:0.975]
Epoch [37/120    avg_loss:0.075, val_acc:0.981]
Epoch [38/120    avg_loss:0.063, val_acc:0.979]
Epoch [39/120    avg_loss:0.101, val_acc:0.964]
Epoch [40/120    avg_loss:0.053, val_acc:0.977]
Epoch [41/120    avg_loss:0.047, val_acc:0.973]
Epoch [42/120    avg_loss:0.043, val_acc:0.985]
Epoch [43/120    avg_loss:0.020, val_acc:0.986]
Epoch [44/120    avg_loss:0.015, val_acc:0.986]
Epoch [45/120    avg_loss:0.028, val_acc:0.987]
Epoch [46/120    avg_loss:0.016, val_acc:0.989]
Epoch [47/120    avg_loss:0.022, val_acc:0.988]
Epoch [48/120    avg_loss:0.021, val_acc:0.990]
Epoch [49/120    avg_loss:0.013, val_acc:0.989]
Epoch [50/120    avg_loss:0.015, val_acc:0.989]
Epoch [51/120    avg_loss:0.018, val_acc:0.989]
Epoch [52/120    avg_loss:0.015, val_acc:0.990]
Epoch [53/120    avg_loss:0.015, val_acc:0.990]
Epoch [54/120    avg_loss:0.014, val_acc:0.990]
Epoch [55/120    avg_loss:0.014, val_acc:0.990]
Epoch [56/120    avg_loss:0.011, val_acc:0.989]
Epoch [57/120    avg_loss:0.022, val_acc:0.988]
Epoch [58/120    avg_loss:0.013, val_acc:0.989]
Epoch [59/120    avg_loss:0.018, val_acc:0.988]
Epoch [60/120    avg_loss:0.014, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.990]
Epoch [62/120    avg_loss:0.011, val_acc:0.990]
Epoch [63/120    avg_loss:0.023, val_acc:0.988]
Epoch [64/120    avg_loss:0.014, val_acc:0.989]
Epoch [65/120    avg_loss:0.011, val_acc:0.989]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.990]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.015, val_acc:0.988]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.015, val_acc:0.990]
Epoch [73/120    avg_loss:0.011, val_acc:0.990]
Epoch [74/120    avg_loss:0.012, val_acc:0.988]
Epoch [75/120    avg_loss:0.010, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.987]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.014, val_acc:0.988]
Epoch [79/120    avg_loss:0.020, val_acc:0.987]
Epoch [80/120    avg_loss:0.011, val_acc:0.991]
Epoch [81/120    avg_loss:0.030, val_acc:0.987]
Epoch [82/120    avg_loss:0.015, val_acc:0.988]
Epoch [83/120    avg_loss:0.014, val_acc:0.987]
Epoch [84/120    avg_loss:0.011, val_acc:0.987]
Epoch [85/120    avg_loss:0.014, val_acc:0.988]
Epoch [86/120    avg_loss:0.013, val_acc:0.989]
Epoch [87/120    avg_loss:0.011, val_acc:0.989]
Epoch [88/120    avg_loss:0.012, val_acc:0.989]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.015, val_acc:0.987]
Epoch [91/120    avg_loss:0.013, val_acc:0.988]
Epoch [92/120    avg_loss:0.013, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.990]
Epoch [94/120    avg_loss:0.010, val_acc:0.990]
Epoch [95/120    avg_loss:0.010, val_acc:0.990]
Epoch [96/120    avg_loss:0.013, val_acc:0.990]
Epoch [97/120    avg_loss:0.012, val_acc:0.991]
Epoch [98/120    avg_loss:0.012, val_acc:0.989]
Epoch [99/120    avg_loss:0.009, val_acc:0.989]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.009, val_acc:0.990]
Epoch [102/120    avg_loss:0.011, val_acc:0.990]
Epoch [103/120    avg_loss:0.010, val_acc:0.990]
Epoch [104/120    avg_loss:0.011, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.990]
Epoch [107/120    avg_loss:0.010, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.990]
Epoch [109/120    avg_loss:0.012, val_acc:0.990]
Epoch [110/120    avg_loss:0.012, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.013, val_acc:0.990]
Epoch [114/120    avg_loss:0.010, val_acc:0.990]
Epoch [115/120    avg_loss:0.009, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.012, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     0     1     0     4    16     0     8]
 [    0     0 17992     0    30     0    68     0     0     0]
 [    0     4     0  2019     2     0     0     0     2     9]
 [    0    31     5     0  2924     0     0     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4865     0    13     0]
 [    0     0     0     0     0     0     3  1282     0     5]
 [    0    13     0     7    51     0     0     0  3498     2]
 [    0     0     0     1    16    28     0     0     0   874]]

Accuracy:
99.20227508254405

F1 scores:
[       nan 0.99402313 0.99714579 0.99384691 0.97531688 0.9893859
 0.99103687 0.99072643 0.98590755 0.96202532]

Kappa:
0.9894398890358879
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4ea8b9780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.305, val_acc:0.572]
Epoch [2/120    avg_loss:0.817, val_acc:0.551]
Epoch [3/120    avg_loss:0.550, val_acc:0.690]
Epoch [4/120    avg_loss:0.429, val_acc:0.776]
Epoch [5/120    avg_loss:0.339, val_acc:0.847]
Epoch [6/120    avg_loss:0.255, val_acc:0.779]
Epoch [7/120    avg_loss:0.248, val_acc:0.885]
Epoch [8/120    avg_loss:0.201, val_acc:0.927]
Epoch [9/120    avg_loss:0.222, val_acc:0.906]
Epoch [10/120    avg_loss:0.172, val_acc:0.941]
Epoch [11/120    avg_loss:0.163, val_acc:0.952]
Epoch [12/120    avg_loss:0.140, val_acc:0.945]
Epoch [13/120    avg_loss:0.148, val_acc:0.946]
Epoch [14/120    avg_loss:0.107, val_acc:0.935]
Epoch [15/120    avg_loss:0.102, val_acc:0.959]
Epoch [16/120    avg_loss:0.110, val_acc:0.951]
Epoch [17/120    avg_loss:0.099, val_acc:0.964]
Epoch [18/120    avg_loss:0.055, val_acc:0.963]
Epoch [19/120    avg_loss:0.054, val_acc:0.957]
Epoch [20/120    avg_loss:0.037, val_acc:0.977]
Epoch [21/120    avg_loss:0.056, val_acc:0.967]
Epoch [22/120    avg_loss:0.038, val_acc:0.961]
Epoch [23/120    avg_loss:0.083, val_acc:0.959]
Epoch [24/120    avg_loss:0.066, val_acc:0.956]
Epoch [25/120    avg_loss:0.076, val_acc:0.965]
Epoch [26/120    avg_loss:0.063, val_acc:0.969]
Epoch [27/120    avg_loss:0.050, val_acc:0.970]
Epoch [28/120    avg_loss:0.055, val_acc:0.973]
Epoch [29/120    avg_loss:0.033, val_acc:0.969]
Epoch [30/120    avg_loss:0.030, val_acc:0.963]
Epoch [31/120    avg_loss:0.036, val_acc:0.983]
Epoch [32/120    avg_loss:0.074, val_acc:0.960]
Epoch [33/120    avg_loss:0.046, val_acc:0.973]
Epoch [34/120    avg_loss:0.049, val_acc:0.985]
Epoch [35/120    avg_loss:0.017, val_acc:0.985]
Epoch [36/120    avg_loss:0.028, val_acc:0.979]
Epoch [37/120    avg_loss:0.015, val_acc:0.984]
Epoch [38/120    avg_loss:0.013, val_acc:0.980]
Epoch [39/120    avg_loss:0.018, val_acc:0.986]
Epoch [40/120    avg_loss:0.012, val_acc:0.980]
Epoch [41/120    avg_loss:0.021, val_acc:0.980]
Epoch [42/120    avg_loss:0.031, val_acc:0.974]
Epoch [43/120    avg_loss:0.019, val_acc:0.987]
Epoch [44/120    avg_loss:0.010, val_acc:0.972]
Epoch [45/120    avg_loss:0.013, val_acc:0.980]
Epoch [46/120    avg_loss:0.014, val_acc:0.988]
Epoch [47/120    avg_loss:0.014, val_acc:0.981]
Epoch [48/120    avg_loss:0.017, val_acc:0.990]
Epoch [49/120    avg_loss:0.023, val_acc:0.983]
Epoch [50/120    avg_loss:0.009, val_acc:0.988]
Epoch [51/120    avg_loss:0.012, val_acc:0.986]
Epoch [52/120    avg_loss:0.020, val_acc:0.980]
Epoch [53/120    avg_loss:0.010, val_acc:0.986]
Epoch [54/120    avg_loss:0.007, val_acc:0.986]
Epoch [55/120    avg_loss:0.015, val_acc:0.983]
Epoch [56/120    avg_loss:0.012, val_acc:0.980]
Epoch [57/120    avg_loss:0.006, val_acc:0.986]
Epoch [58/120    avg_loss:0.009, val_acc:0.988]
Epoch [59/120    avg_loss:0.012, val_acc:0.988]
Epoch [60/120    avg_loss:0.018, val_acc:0.984]
Epoch [61/120    avg_loss:0.012, val_acc:0.988]
Epoch [62/120    avg_loss:0.007, val_acc:0.987]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.007, val_acc:0.988]
Epoch [65/120    avg_loss:0.008, val_acc:0.989]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.005, val_acc:0.989]
Epoch [69/120    avg_loss:0.005, val_acc:0.989]
Epoch [70/120    avg_loss:0.006, val_acc:0.989]
Epoch [71/120    avg_loss:0.005, val_acc:0.989]
Epoch [72/120    avg_loss:0.005, val_acc:0.989]
Epoch [73/120    avg_loss:0.004, val_acc:0.989]
Epoch [74/120    avg_loss:0.005, val_acc:0.989]
Epoch [75/120    avg_loss:0.005, val_acc:0.989]
Epoch [76/120    avg_loss:0.004, val_acc:0.989]
Epoch [77/120    avg_loss:0.004, val_acc:0.989]
Epoch [78/120    avg_loss:0.005, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.004, val_acc:0.989]
Epoch [81/120    avg_loss:0.005, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.989]
Epoch [83/120    avg_loss:0.004, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.005, val_acc:0.989]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.989]
Epoch [88/120    avg_loss:0.010, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.011, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.003, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.008, val_acc:0.989]
Epoch [100/120    avg_loss:0.016, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6420     0     0     1     0     0     6     0     5]
 [    0     0 18054     0    19     0    17     0     0     0]
 [    0     1     0  1986     1     0     0     0    39     9]
 [    0    25     0     0  2932     0     0     0    14     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4869     0     5     2]
 [    0     0     0     0     0     0     2  1284     0     4]
 [    0     2     0     2    44     0     0     0  3518     5]
 [    0     0     0     0    13    26     0     0     0   880]]

Accuracy:
99.40953895837852

F1 scores:
[       nan 0.99689441 0.99894871 0.98707753 0.98027416 0.99013657
 0.99713291 0.99534884 0.98446901 0.96438356]

Kappa:
0.9921784925205956
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79354dd710>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.391, val_acc:0.604]
Epoch [2/120    avg_loss:0.776, val_acc:0.495]
Epoch [3/120    avg_loss:0.536, val_acc:0.769]
Epoch [4/120    avg_loss:0.379, val_acc:0.829]
Epoch [5/120    avg_loss:0.324, val_acc:0.894]
Epoch [6/120    avg_loss:0.249, val_acc:0.853]
Epoch [7/120    avg_loss:0.238, val_acc:0.898]
Epoch [8/120    avg_loss:0.165, val_acc:0.921]
Epoch [9/120    avg_loss:0.128, val_acc:0.943]
Epoch [10/120    avg_loss:0.134, val_acc:0.938]
Epoch [11/120    avg_loss:0.159, val_acc:0.953]
Epoch [12/120    avg_loss:0.197, val_acc:0.927]
Epoch [13/120    avg_loss:0.132, val_acc:0.938]
Epoch [14/120    avg_loss:0.122, val_acc:0.954]
Epoch [15/120    avg_loss:0.084, val_acc:0.930]
Epoch [16/120    avg_loss:0.083, val_acc:0.968]
Epoch [17/120    avg_loss:0.060, val_acc:0.959]
Epoch [18/120    avg_loss:0.045, val_acc:0.962]
Epoch [19/120    avg_loss:0.116, val_acc:0.958]
Epoch [20/120    avg_loss:0.086, val_acc:0.912]
Epoch [21/120    avg_loss:0.121, val_acc:0.939]
Epoch [22/120    avg_loss:0.057, val_acc:0.954]
Epoch [23/120    avg_loss:0.047, val_acc:0.958]
Epoch [24/120    avg_loss:0.033, val_acc:0.968]
Epoch [25/120    avg_loss:0.030, val_acc:0.970]
Epoch [26/120    avg_loss:0.069, val_acc:0.938]
Epoch [27/120    avg_loss:0.045, val_acc:0.971]
Epoch [28/120    avg_loss:0.029, val_acc:0.977]
Epoch [29/120    avg_loss:0.022, val_acc:0.964]
Epoch [30/120    avg_loss:0.040, val_acc:0.963]
Epoch [31/120    avg_loss:0.026, val_acc:0.971]
Epoch [32/120    avg_loss:0.025, val_acc:0.974]
Epoch [33/120    avg_loss:0.022, val_acc:0.966]
Epoch [34/120    avg_loss:0.019, val_acc:0.977]
Epoch [35/120    avg_loss:0.020, val_acc:0.973]
Epoch [36/120    avg_loss:0.027, val_acc:0.959]
Epoch [37/120    avg_loss:0.025, val_acc:0.975]
Epoch [38/120    avg_loss:0.016, val_acc:0.979]
Epoch [39/120    avg_loss:0.029, val_acc:0.964]
Epoch [40/120    avg_loss:0.025, val_acc:0.979]
Epoch [41/120    avg_loss:0.040, val_acc:0.969]
Epoch [42/120    avg_loss:0.023, val_acc:0.973]
Epoch [43/120    avg_loss:0.011, val_acc:0.983]
Epoch [44/120    avg_loss:0.023, val_acc:0.982]
Epoch [45/120    avg_loss:0.014, val_acc:0.982]
Epoch [46/120    avg_loss:0.010, val_acc:0.988]
Epoch [47/120    avg_loss:0.008, val_acc:0.985]
Epoch [48/120    avg_loss:0.014, val_acc:0.986]
Epoch [49/120    avg_loss:0.020, val_acc:0.984]
Epoch [50/120    avg_loss:0.097, val_acc:0.944]
Epoch [51/120    avg_loss:0.114, val_acc:0.965]
Epoch [52/120    avg_loss:0.073, val_acc:0.979]
Epoch [53/120    avg_loss:0.058, val_acc:0.965]
Epoch [54/120    avg_loss:0.028, val_acc:0.973]
Epoch [55/120    avg_loss:0.039, val_acc:0.977]
Epoch [56/120    avg_loss:0.017, val_acc:0.981]
Epoch [57/120    avg_loss:0.019, val_acc:0.974]
Epoch [58/120    avg_loss:0.014, val_acc:0.981]
Epoch [59/120    avg_loss:0.015, val_acc:0.986]
Epoch [60/120    avg_loss:0.014, val_acc:0.987]
Epoch [61/120    avg_loss:0.009, val_acc:0.986]
Epoch [62/120    avg_loss:0.007, val_acc:0.988]
Epoch [63/120    avg_loss:0.012, val_acc:0.983]
Epoch [64/120    avg_loss:0.012, val_acc:0.983]
Epoch [65/120    avg_loss:0.011, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.985]
Epoch [67/120    avg_loss:0.009, val_acc:0.983]
Epoch [68/120    avg_loss:0.006, val_acc:0.984]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.011, val_acc:0.987]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.006, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.007, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.004, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.986]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.015, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.011, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     0     1     0     0]
 [    0     6 18006     0    27     0    51     0     0     0]
 [    0     7     0  2015     0     0     0     0     9     5]
 [    0    32     1     0  2920     0     0     0    18     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0    18     0]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     2     0    16    48     0     0     0  3499     6]
 [    0     0     0     1    10    20     0     0     0   888]]

Accuracy:
99.32036729086835

F1 scores:
[       nan 0.99628195 0.99764523 0.9906588  0.9770788  0.99239544
 0.99295127 0.99844841 0.98355587 0.97475302]

Kappa:
0.9910016744207909
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8b6c207780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.361, val_acc:0.713]
Epoch [2/120    avg_loss:0.713, val_acc:0.577]
Epoch [3/120    avg_loss:0.540, val_acc:0.747]
Epoch [4/120    avg_loss:0.383, val_acc:0.724]
Epoch [5/120    avg_loss:0.303, val_acc:0.794]
Epoch [6/120    avg_loss:0.281, val_acc:0.869]
Epoch [7/120    avg_loss:0.225, val_acc:0.855]
Epoch [8/120    avg_loss:0.152, val_acc:0.940]
Epoch [9/120    avg_loss:0.207, val_acc:0.930]
Epoch [10/120    avg_loss:0.165, val_acc:0.948]
Epoch [11/120    avg_loss:0.133, val_acc:0.903]
Epoch [12/120    avg_loss:0.218, val_acc:0.909]
Epoch [13/120    avg_loss:0.209, val_acc:0.958]
Epoch [14/120    avg_loss:0.114, val_acc:0.949]
Epoch [15/120    avg_loss:0.088, val_acc:0.951]
Epoch [16/120    avg_loss:0.070, val_acc:0.957]
Epoch [17/120    avg_loss:0.043, val_acc:0.972]
Epoch [18/120    avg_loss:0.036, val_acc:0.974]
Epoch [19/120    avg_loss:0.050, val_acc:0.973]
Epoch [20/120    avg_loss:0.064, val_acc:0.978]
Epoch [21/120    avg_loss:0.035, val_acc:0.980]
Epoch [22/120    avg_loss:0.052, val_acc:0.958]
Epoch [23/120    avg_loss:0.085, val_acc:0.966]
Epoch [24/120    avg_loss:0.097, val_acc:0.938]
Epoch [25/120    avg_loss:0.043, val_acc:0.980]
Epoch [26/120    avg_loss:0.032, val_acc:0.976]
Epoch [27/120    avg_loss:0.045, val_acc:0.972]
Epoch [28/120    avg_loss:0.095, val_acc:0.974]
Epoch [29/120    avg_loss:0.053, val_acc:0.970]
Epoch [30/120    avg_loss:0.058, val_acc:0.968]
Epoch [31/120    avg_loss:0.027, val_acc:0.983]
Epoch [32/120    avg_loss:0.024, val_acc:0.972]
Epoch [33/120    avg_loss:0.025, val_acc:0.982]
Epoch [34/120    avg_loss:0.023, val_acc:0.978]
Epoch [35/120    avg_loss:0.017, val_acc:0.990]
Epoch [36/120    avg_loss:0.028, val_acc:0.984]
Epoch [37/120    avg_loss:0.026, val_acc:0.982]
Epoch [38/120    avg_loss:0.023, val_acc:0.983]
Epoch [39/120    avg_loss:0.033, val_acc:0.962]
Epoch [40/120    avg_loss:0.034, val_acc:0.986]
Epoch [41/120    avg_loss:0.023, val_acc:0.985]
Epoch [42/120    avg_loss:0.012, val_acc:0.990]
Epoch [43/120    avg_loss:0.014, val_acc:0.985]
Epoch [44/120    avg_loss:0.019, val_acc:0.966]
Epoch [45/120    avg_loss:0.028, val_acc:0.983]
Epoch [46/120    avg_loss:0.034, val_acc:0.978]
Epoch [47/120    avg_loss:0.025, val_acc:0.964]
Epoch [48/120    avg_loss:0.028, val_acc:0.987]
Epoch [49/120    avg_loss:0.022, val_acc:0.990]
Epoch [50/120    avg_loss:0.010, val_acc:0.989]
Epoch [51/120    avg_loss:0.016, val_acc:0.992]
Epoch [52/120    avg_loss:0.018, val_acc:0.991]
Epoch [53/120    avg_loss:0.010, val_acc:0.989]
Epoch [54/120    avg_loss:0.013, val_acc:0.986]
Epoch [55/120    avg_loss:0.013, val_acc:0.989]
Epoch [56/120    avg_loss:0.005, val_acc:0.991]
Epoch [57/120    avg_loss:0.013, val_acc:0.986]
Epoch [58/120    avg_loss:0.012, val_acc:0.993]
Epoch [59/120    avg_loss:0.009, val_acc:0.977]
Epoch [60/120    avg_loss:0.012, val_acc:0.994]
Epoch [61/120    avg_loss:0.004, val_acc:0.997]
Epoch [62/120    avg_loss:0.007, val_acc:0.994]
Epoch [63/120    avg_loss:0.005, val_acc:0.993]
Epoch [64/120    avg_loss:0.007, val_acc:0.991]
Epoch [65/120    avg_loss:0.003, val_acc:0.992]
Epoch [66/120    avg_loss:0.004, val_acc:0.993]
Epoch [67/120    avg_loss:0.025, val_acc:0.991]
Epoch [68/120    avg_loss:0.019, val_acc:0.990]
Epoch [69/120    avg_loss:0.004, val_acc:0.994]
Epoch [70/120    avg_loss:0.009, val_acc:0.991]
Epoch [71/120    avg_loss:0.015, val_acc:0.986]
Epoch [72/120    avg_loss:0.006, val_acc:0.992]
Epoch [73/120    avg_loss:0.011, val_acc:0.996]
Epoch [74/120    avg_loss:0.006, val_acc:0.993]
Epoch [75/120    avg_loss:0.009, val_acc:0.995]
Epoch [76/120    avg_loss:0.005, val_acc:0.994]
Epoch [77/120    avg_loss:0.006, val_acc:0.997]
Epoch [78/120    avg_loss:0.006, val_acc:0.996]
Epoch [79/120    avg_loss:0.009, val_acc:0.995]
Epoch [80/120    avg_loss:0.005, val_acc:0.995]
Epoch [81/120    avg_loss:0.004, val_acc:0.995]
Epoch [82/120    avg_loss:0.004, val_acc:0.994]
Epoch [83/120    avg_loss:0.004, val_acc:0.994]
Epoch [84/120    avg_loss:0.006, val_acc:0.995]
Epoch [85/120    avg_loss:0.004, val_acc:0.995]
Epoch [86/120    avg_loss:0.005, val_acc:0.994]
Epoch [87/120    avg_loss:0.008, val_acc:0.995]
Epoch [88/120    avg_loss:0.003, val_acc:0.994]
Epoch [89/120    avg_loss:0.003, val_acc:0.994]
Epoch [90/120    avg_loss:0.006, val_acc:0.995]
Epoch [91/120    avg_loss:0.004, val_acc:0.995]
Epoch [92/120    avg_loss:0.005, val_acc:0.994]
Epoch [93/120    avg_loss:0.004, val_acc:0.994]
Epoch [94/120    avg_loss:0.003, val_acc:0.994]
Epoch [95/120    avg_loss:0.006, val_acc:0.994]
Epoch [96/120    avg_loss:0.005, val_acc:0.994]
Epoch [97/120    avg_loss:0.003, val_acc:0.994]
Epoch [98/120    avg_loss:0.004, val_acc:0.994]
Epoch [99/120    avg_loss:0.005, val_acc:0.994]
Epoch [100/120    avg_loss:0.004, val_acc:0.994]
Epoch [101/120    avg_loss:0.003, val_acc:0.994]
Epoch [102/120    avg_loss:0.004, val_acc:0.994]
Epoch [103/120    avg_loss:0.005, val_acc:0.994]
Epoch [104/120    avg_loss:0.003, val_acc:0.994]
Epoch [105/120    avg_loss:0.005, val_acc:0.994]
Epoch [106/120    avg_loss:0.005, val_acc:0.994]
Epoch [107/120    avg_loss:0.002, val_acc:0.994]
Epoch [108/120    avg_loss:0.004, val_acc:0.994]
Epoch [109/120    avg_loss:0.007, val_acc:0.994]
Epoch [110/120    avg_loss:0.006, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.003, val_acc:0.994]
Epoch [113/120    avg_loss:0.004, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.004, val_acc:0.994]
Epoch [117/120    avg_loss:0.005, val_acc:0.994]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.003, val_acc:0.994]
Epoch [120/120    avg_loss:0.004, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     2     0     0     4     0     0]
 [    0     0 18044     0    23     0    23     0     0     0]
 [    0     0     0  2031     2     0     0     0     0     3]
 [    0    33     4     0  2919     0     0     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4874     0     4     0]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     0     0     7    38     0     0     0  3524     2]
 [    0     0     0     2    15    40     0     0     0   862]]

Accuracy:
99.46496999493891

F1 scores:
[       nan 0.99697463 0.99861641 0.99656526 0.97772567 0.98490566
 0.99703385 0.99689922 0.99058327 0.96420582]

Kappa:
0.9929131770719172
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c891337b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.392, val_acc:0.711]
Epoch [2/120    avg_loss:0.714, val_acc:0.665]
Epoch [3/120    avg_loss:0.510, val_acc:0.812]
Epoch [4/120    avg_loss:0.399, val_acc:0.827]
Epoch [5/120    avg_loss:0.324, val_acc:0.870]
Epoch [6/120    avg_loss:0.268, val_acc:0.888]
Epoch [7/120    avg_loss:0.199, val_acc:0.903]
Epoch [8/120    avg_loss:0.210, val_acc:0.888]
Epoch [9/120    avg_loss:0.165, val_acc:0.896]
Epoch [10/120    avg_loss:0.181, val_acc:0.912]
Epoch [11/120    avg_loss:0.187, val_acc:0.927]
Epoch [12/120    avg_loss:0.140, val_acc:0.952]
Epoch [13/120    avg_loss:0.087, val_acc:0.955]
Epoch [14/120    avg_loss:0.083, val_acc:0.947]
Epoch [15/120    avg_loss:0.098, val_acc:0.958]
Epoch [16/120    avg_loss:0.063, val_acc:0.951]
Epoch [17/120    avg_loss:0.064, val_acc:0.941]
Epoch [18/120    avg_loss:0.087, val_acc:0.966]
Epoch [19/120    avg_loss:0.065, val_acc:0.960]
Epoch [20/120    avg_loss:0.031, val_acc:0.972]
Epoch [21/120    avg_loss:0.052, val_acc:0.968]
Epoch [22/120    avg_loss:0.035, val_acc:0.952]
Epoch [23/120    avg_loss:0.053, val_acc:0.952]
Epoch [24/120    avg_loss:0.052, val_acc:0.970]
Epoch [25/120    avg_loss:0.027, val_acc:0.977]
Epoch [26/120    avg_loss:0.041, val_acc:0.961]
Epoch [27/120    avg_loss:0.032, val_acc:0.956]
Epoch [28/120    avg_loss:0.042, val_acc:0.971]
Epoch [29/120    avg_loss:0.033, val_acc:0.963]
Epoch [30/120    avg_loss:0.021, val_acc:0.979]
Epoch [31/120    avg_loss:0.022, val_acc:0.982]
Epoch [32/120    avg_loss:0.012, val_acc:0.985]
Epoch [33/120    avg_loss:0.026, val_acc:0.974]
Epoch [34/120    avg_loss:0.022, val_acc:0.976]
Epoch [35/120    avg_loss:0.028, val_acc:0.974]
Epoch [36/120    avg_loss:0.022, val_acc:0.972]
Epoch [37/120    avg_loss:0.030, val_acc:0.961]
Epoch [38/120    avg_loss:0.089, val_acc:0.934]
Epoch [39/120    avg_loss:0.091, val_acc:0.958]
Epoch [40/120    avg_loss:0.111, val_acc:0.963]
Epoch [41/120    avg_loss:0.061, val_acc:0.958]
Epoch [42/120    avg_loss:0.051, val_acc:0.970]
Epoch [43/120    avg_loss:0.053, val_acc:0.967]
Epoch [44/120    avg_loss:0.049, val_acc:0.972]
Epoch [45/120    avg_loss:0.036, val_acc:0.973]
Epoch [46/120    avg_loss:0.017, val_acc:0.979]
Epoch [47/120    avg_loss:0.020, val_acc:0.978]
Epoch [48/120    avg_loss:0.011, val_acc:0.980]
Epoch [49/120    avg_loss:0.015, val_acc:0.979]
Epoch [50/120    avg_loss:0.011, val_acc:0.980]
Epoch [51/120    avg_loss:0.011, val_acc:0.980]
Epoch [52/120    avg_loss:0.014, val_acc:0.980]
Epoch [53/120    avg_loss:0.014, val_acc:0.981]
Epoch [54/120    avg_loss:0.009, val_acc:0.982]
Epoch [55/120    avg_loss:0.014, val_acc:0.982]
Epoch [56/120    avg_loss:0.011, val_acc:0.982]
Epoch [57/120    avg_loss:0.011, val_acc:0.982]
Epoch [58/120    avg_loss:0.008, val_acc:0.982]
Epoch [59/120    avg_loss:0.009, val_acc:0.982]
Epoch [60/120    avg_loss:0.010, val_acc:0.982]
Epoch [61/120    avg_loss:0.013, val_acc:0.982]
Epoch [62/120    avg_loss:0.014, val_acc:0.982]
Epoch [63/120    avg_loss:0.007, val_acc:0.982]
Epoch [64/120    avg_loss:0.015, val_acc:0.982]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.010, val_acc:0.982]
Epoch [67/120    avg_loss:0.010, val_acc:0.982]
Epoch [68/120    avg_loss:0.010, val_acc:0.982]
Epoch [69/120    avg_loss:0.013, val_acc:0.982]
Epoch [70/120    avg_loss:0.008, val_acc:0.982]
Epoch [71/120    avg_loss:0.012, val_acc:0.982]
Epoch [72/120    avg_loss:0.013, val_acc:0.982]
Epoch [73/120    avg_loss:0.008, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.982]
Epoch [75/120    avg_loss:0.010, val_acc:0.982]
Epoch [76/120    avg_loss:0.010, val_acc:0.982]
Epoch [77/120    avg_loss:0.014, val_acc:0.982]
Epoch [78/120    avg_loss:0.013, val_acc:0.982]
Epoch [79/120    avg_loss:0.013, val_acc:0.982]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.009, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.013, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.982]
Epoch [87/120    avg_loss:0.010, val_acc:0.982]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.015, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.012, val_acc:0.982]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.014, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.007, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.006, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.982]
Epoch [107/120    avg_loss:0.008, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.982]
Epoch [117/120    avg_loss:0.008, val_acc:0.982]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.018, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     2     0     3    14     2    11]
 [    0     0 18043     0    31     0     9     0     2     5]
 [    0     1     0  2017     0     0     0     0    11     7]
 [    0    24     0     0  2935     0     0     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     0     0  4843     0    19     2]
 [    0     0     0     0     0     0     2  1280     0     8]
 [    0     4     0    12    52     0     0     0  3494     9]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.27939652471501

F1 scores:
[       nan 0.99525698 0.99831245 0.99237392 0.97735598 0.9893859
 0.99496662 0.99071207 0.98256468 0.95429815]

Kappa:
0.9904560796223202
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f39d4a80828>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.345, val_acc:0.748]
Epoch [2/120    avg_loss:0.651, val_acc:0.787]
Epoch [3/120    avg_loss:0.448, val_acc:0.813]
Epoch [4/120    avg_loss:0.468, val_acc:0.784]
Epoch [5/120    avg_loss:0.309, val_acc:0.871]
Epoch [6/120    avg_loss:0.270, val_acc:0.928]
Epoch [7/120    avg_loss:0.264, val_acc:0.874]
Epoch [8/120    avg_loss:0.194, val_acc:0.894]
Epoch [9/120    avg_loss:0.161, val_acc:0.934]
Epoch [10/120    avg_loss:0.151, val_acc:0.953]
Epoch [11/120    avg_loss:0.110, val_acc:0.889]
Epoch [12/120    avg_loss:0.132, val_acc:0.954]
Epoch [13/120    avg_loss:0.156, val_acc:0.931]
Epoch [14/120    avg_loss:0.107, val_acc:0.946]
Epoch [15/120    avg_loss:0.063, val_acc:0.971]
Epoch [16/120    avg_loss:0.040, val_acc:0.975]
Epoch [17/120    avg_loss:0.064, val_acc:0.944]
Epoch [18/120    avg_loss:0.072, val_acc:0.972]
Epoch [19/120    avg_loss:0.039, val_acc:0.971]
Epoch [20/120    avg_loss:0.052, val_acc:0.972]
Epoch [21/120    avg_loss:0.063, val_acc:0.957]
Epoch [22/120    avg_loss:0.033, val_acc:0.974]
Epoch [23/120    avg_loss:0.106, val_acc:0.946]
Epoch [24/120    avg_loss:0.108, val_acc:0.942]
Epoch [25/120    avg_loss:0.057, val_acc:0.964]
Epoch [26/120    avg_loss:0.066, val_acc:0.872]
Epoch [27/120    avg_loss:0.046, val_acc:0.969]
Epoch [28/120    avg_loss:0.052, val_acc:0.973]
Epoch [29/120    avg_loss:0.037, val_acc:0.972]
Epoch [30/120    avg_loss:0.034, val_acc:0.978]
Epoch [31/120    avg_loss:0.029, val_acc:0.980]
Epoch [32/120    avg_loss:0.020, val_acc:0.980]
Epoch [33/120    avg_loss:0.014, val_acc:0.982]
Epoch [34/120    avg_loss:0.016, val_acc:0.981]
Epoch [35/120    avg_loss:0.026, val_acc:0.977]
Epoch [36/120    avg_loss:0.023, val_acc:0.977]
Epoch [37/120    avg_loss:0.018, val_acc:0.979]
Epoch [38/120    avg_loss:0.015, val_acc:0.981]
Epoch [39/120    avg_loss:0.016, val_acc:0.981]
Epoch [40/120    avg_loss:0.014, val_acc:0.980]
Epoch [41/120    avg_loss:0.014, val_acc:0.982]
Epoch [42/120    avg_loss:0.015, val_acc:0.983]
Epoch [43/120    avg_loss:0.017, val_acc:0.982]
Epoch [44/120    avg_loss:0.017, val_acc:0.983]
Epoch [45/120    avg_loss:0.019, val_acc:0.983]
Epoch [46/120    avg_loss:0.016, val_acc:0.984]
Epoch [47/120    avg_loss:0.014, val_acc:0.983]
Epoch [48/120    avg_loss:0.010, val_acc:0.982]
Epoch [49/120    avg_loss:0.013, val_acc:0.981]
Epoch [50/120    avg_loss:0.016, val_acc:0.982]
Epoch [51/120    avg_loss:0.011, val_acc:0.982]
Epoch [52/120    avg_loss:0.023, val_acc:0.982]
Epoch [53/120    avg_loss:0.025, val_acc:0.980]
Epoch [54/120    avg_loss:0.017, val_acc:0.983]
Epoch [55/120    avg_loss:0.013, val_acc:0.981]
Epoch [56/120    avg_loss:0.011, val_acc:0.981]
Epoch [57/120    avg_loss:0.010, val_acc:0.983]
Epoch [58/120    avg_loss:0.015, val_acc:0.983]
Epoch [59/120    avg_loss:0.015, val_acc:0.983]
Epoch [60/120    avg_loss:0.011, val_acc:0.983]
Epoch [61/120    avg_loss:0.016, val_acc:0.983]
Epoch [62/120    avg_loss:0.013, val_acc:0.983]
Epoch [63/120    avg_loss:0.015, val_acc:0.983]
Epoch [64/120    avg_loss:0.011, val_acc:0.983]
Epoch [65/120    avg_loss:0.022, val_acc:0.983]
Epoch [66/120    avg_loss:0.013, val_acc:0.983]
Epoch [67/120    avg_loss:0.011, val_acc:0.983]
Epoch [68/120    avg_loss:0.011, val_acc:0.983]
Epoch [69/120    avg_loss:0.016, val_acc:0.983]
Epoch [70/120    avg_loss:0.014, val_acc:0.983]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.016, val_acc:0.983]
Epoch [73/120    avg_loss:0.016, val_acc:0.983]
Epoch [74/120    avg_loss:0.011, val_acc:0.983]
Epoch [75/120    avg_loss:0.017, val_acc:0.983]
Epoch [76/120    avg_loss:0.012, val_acc:0.983]
Epoch [77/120    avg_loss:0.015, val_acc:0.983]
Epoch [78/120    avg_loss:0.007, val_acc:0.983]
Epoch [79/120    avg_loss:0.013, val_acc:0.983]
Epoch [80/120    avg_loss:0.015, val_acc:0.983]
Epoch [81/120    avg_loss:0.017, val_acc:0.983]
Epoch [82/120    avg_loss:0.015, val_acc:0.983]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.016, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.012, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.013, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.017, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.013, val_acc:0.983]
Epoch [99/120    avg_loss:0.029, val_acc:0.983]
Epoch [100/120    avg_loss:0.020, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.008, val_acc:0.983]
Epoch [104/120    avg_loss:0.011, val_acc:0.983]
Epoch [105/120    avg_loss:0.019, val_acc:0.983]
Epoch [106/120    avg_loss:0.015, val_acc:0.983]
Epoch [107/120    avg_loss:0.017, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.017, val_acc:0.983]
Epoch [110/120    avg_loss:0.012, val_acc:0.983]
Epoch [111/120    avg_loss:0.019, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.015, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.016, val_acc:0.983]
Epoch [120/120    avg_loss:0.019, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0    13     0    23     2]
 [    0     0 17887     0    74     0   129     0     0     0]
 [    0     2     0  2010     1     0     0     0    16     7]
 [    0    25     1     2  2930     0     1     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     3     0     0     0     0     3  1283     0     1]
 [    0     0     0     2    51     0     0     0  3504    14]
 [    0     0     0     0    14    33     0     0     0   872]]

Accuracy:
98.96368062082762

F1 scores:
[       nan 0.99471064 0.99432987 0.99259259 0.96987752 0.98751419
 0.9852555  0.99727944 0.98330293 0.96088154]

Kappa:
0.9862975405941635
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c7f86c7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.464, val_acc:0.503]
Epoch [2/120    avg_loss:0.786, val_acc:0.618]
Epoch [3/120    avg_loss:0.548, val_acc:0.684]
Epoch [4/120    avg_loss:0.344, val_acc:0.894]
Epoch [5/120    avg_loss:0.240, val_acc:0.907]
Epoch [6/120    avg_loss:0.280, val_acc:0.881]
Epoch [7/120    avg_loss:0.240, val_acc:0.916]
Epoch [8/120    avg_loss:0.193, val_acc:0.953]
Epoch [9/120    avg_loss:0.171, val_acc:0.948]
Epoch [10/120    avg_loss:0.181, val_acc:0.947]
Epoch [11/120    avg_loss:0.110, val_acc:0.964]
Epoch [12/120    avg_loss:0.076, val_acc:0.922]
Epoch [13/120    avg_loss:0.136, val_acc:0.949]
Epoch [14/120    avg_loss:0.163, val_acc:0.949]
Epoch [15/120    avg_loss:0.106, val_acc:0.964]
Epoch [16/120    avg_loss:0.088, val_acc:0.955]
Epoch [17/120    avg_loss:0.080, val_acc:0.947]
Epoch [18/120    avg_loss:0.078, val_acc:0.947]
Epoch [19/120    avg_loss:0.041, val_acc:0.977]
Epoch [20/120    avg_loss:0.062, val_acc:0.896]
Epoch [21/120    avg_loss:0.067, val_acc:0.973]
Epoch [22/120    avg_loss:0.076, val_acc:0.973]
Epoch [23/120    avg_loss:0.071, val_acc:0.953]
Epoch [24/120    avg_loss:0.068, val_acc:0.965]
Epoch [25/120    avg_loss:0.049, val_acc:0.957]
Epoch [26/120    avg_loss:0.073, val_acc:0.970]
Epoch [27/120    avg_loss:0.043, val_acc:0.971]
Epoch [28/120    avg_loss:0.043, val_acc:0.957]
Epoch [29/120    avg_loss:0.044, val_acc:0.972]
Epoch [30/120    avg_loss:0.047, val_acc:0.977]
Epoch [31/120    avg_loss:0.027, val_acc:0.979]
Epoch [32/120    avg_loss:0.027, val_acc:0.977]
Epoch [33/120    avg_loss:0.064, val_acc:0.963]
Epoch [34/120    avg_loss:0.056, val_acc:0.969]
Epoch [35/120    avg_loss:0.034, val_acc:0.982]
Epoch [36/120    avg_loss:0.027, val_acc:0.980]
Epoch [37/120    avg_loss:0.019, val_acc:0.980]
Epoch [38/120    avg_loss:0.017, val_acc:0.984]
Epoch [39/120    avg_loss:0.013, val_acc:0.990]
Epoch [40/120    avg_loss:0.015, val_acc:0.985]
Epoch [41/120    avg_loss:0.035, val_acc:0.985]
Epoch [42/120    avg_loss:0.022, val_acc:0.977]
Epoch [43/120    avg_loss:0.021, val_acc:0.986]
Epoch [44/120    avg_loss:0.016, val_acc:0.990]
Epoch [45/120    avg_loss:0.017, val_acc:0.973]
Epoch [46/120    avg_loss:0.014, val_acc:0.979]
Epoch [47/120    avg_loss:0.013, val_acc:0.986]
Epoch [48/120    avg_loss:0.011, val_acc:0.980]
Epoch [49/120    avg_loss:0.008, val_acc:0.983]
Epoch [50/120    avg_loss:0.021, val_acc:0.980]
Epoch [51/120    avg_loss:0.013, val_acc:0.977]
Epoch [52/120    avg_loss:0.016, val_acc:0.989]
Epoch [53/120    avg_loss:0.008, val_acc:0.990]
Epoch [54/120    avg_loss:0.008, val_acc:0.990]
Epoch [55/120    avg_loss:0.007, val_acc:0.989]
Epoch [56/120    avg_loss:0.005, val_acc:0.989]
Epoch [57/120    avg_loss:0.006, val_acc:0.990]
Epoch [58/120    avg_loss:0.011, val_acc:0.990]
Epoch [59/120    avg_loss:0.008, val_acc:0.988]
Epoch [60/120    avg_loss:0.006, val_acc:0.989]
Epoch [61/120    avg_loss:0.006, val_acc:0.989]
Epoch [62/120    avg_loss:0.006, val_acc:0.990]
Epoch [63/120    avg_loss:0.010, val_acc:0.990]
Epoch [64/120    avg_loss:0.013, val_acc:0.990]
Epoch [65/120    avg_loss:0.006, val_acc:0.990]
Epoch [66/120    avg_loss:0.008, val_acc:0.990]
Epoch [67/120    avg_loss:0.007, val_acc:0.990]
Epoch [68/120    avg_loss:0.004, val_acc:0.990]
Epoch [69/120    avg_loss:0.005, val_acc:0.990]
Epoch [70/120    avg_loss:0.004, val_acc:0.990]
Epoch [71/120    avg_loss:0.011, val_acc:0.990]
Epoch [72/120    avg_loss:0.005, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.989]
Epoch [74/120    avg_loss:0.006, val_acc:0.990]
Epoch [75/120    avg_loss:0.012, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.989]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.013, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.010, val_acc:0.990]
Epoch [89/120    avg_loss:0.005, val_acc:0.990]
Epoch [90/120    avg_loss:0.009, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.007, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.008, val_acc:0.989]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.009, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.011, val_acc:0.989]
Epoch [105/120    avg_loss:0.012, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.009, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.009, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.011, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.989]
Epoch [118/120    avg_loss:0.011, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     0     3     0    13     7    16     4]
 [    0     0 18001     0    44     0    43     0     0     2]
 [    0     0     0  2027     0     0     0     0     7     2]
 [    0    38    10     3  2894     0     1     0    24     2]
 [    0     0     0     0     0  1291     0     0    14     0]
 [    0     0     0     0     0     0  4870     0     0     8]
 [    0     0     0     0     0     0     8  1281     0     1]
 [    0     1     0     3    48     0     0     0  3518     1]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.16371436145856

F1 scores:
[       nan 0.99362364 0.99725769 0.99631359 0.96870293 0.98324448
 0.99256089 0.99379364 0.98405594 0.96471885]

Kappa:
0.9889277937682794
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f85b4998748>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.483, val_acc:0.387]
Epoch [2/120    avg_loss:0.899, val_acc:0.672]
Epoch [3/120    avg_loss:0.559, val_acc:0.670]
Epoch [4/120    avg_loss:0.493, val_acc:0.760]
Epoch [5/120    avg_loss:0.454, val_acc:0.842]
Epoch [6/120    avg_loss:0.275, val_acc:0.927]
Epoch [7/120    avg_loss:0.277, val_acc:0.893]
Epoch [8/120    avg_loss:0.212, val_acc:0.928]
Epoch [9/120    avg_loss:0.184, val_acc:0.934]
Epoch [10/120    avg_loss:0.191, val_acc:0.928]
Epoch [11/120    avg_loss:0.206, val_acc:0.930]
Epoch [12/120    avg_loss:0.169, val_acc:0.942]
Epoch [13/120    avg_loss:0.197, val_acc:0.925]
Epoch [14/120    avg_loss:0.149, val_acc:0.942]
Epoch [15/120    avg_loss:0.126, val_acc:0.952]
Epoch [16/120    avg_loss:0.093, val_acc:0.965]
Epoch [17/120    avg_loss:0.127, val_acc:0.957]
Epoch [18/120    avg_loss:0.064, val_acc:0.965]
Epoch [19/120    avg_loss:0.062, val_acc:0.965]
Epoch [20/120    avg_loss:0.089, val_acc:0.956]
Epoch [21/120    avg_loss:0.116, val_acc:0.957]
Epoch [22/120    avg_loss:0.057, val_acc:0.949]
Epoch [23/120    avg_loss:0.082, val_acc:0.960]
Epoch [24/120    avg_loss:0.039, val_acc:0.976]
Epoch [25/120    avg_loss:0.038, val_acc:0.967]
Epoch [26/120    avg_loss:0.027, val_acc:0.986]
Epoch [27/120    avg_loss:0.026, val_acc:0.982]
Epoch [28/120    avg_loss:0.030, val_acc:0.983]
Epoch [29/120    avg_loss:0.038, val_acc:0.969]
Epoch [30/120    avg_loss:0.043, val_acc:0.980]
Epoch [31/120    avg_loss:0.030, val_acc:0.983]
Epoch [32/120    avg_loss:0.025, val_acc:0.971]
Epoch [33/120    avg_loss:0.077, val_acc:0.956]
Epoch [34/120    avg_loss:0.055, val_acc:0.980]
Epoch [35/120    avg_loss:0.065, val_acc:0.968]
Epoch [36/120    avg_loss:0.062, val_acc:0.968]
Epoch [37/120    avg_loss:0.056, val_acc:0.980]
Epoch [38/120    avg_loss:0.024, val_acc:0.985]
Epoch [39/120    avg_loss:0.016, val_acc:0.982]
Epoch [40/120    avg_loss:0.016, val_acc:0.983]
Epoch [41/120    avg_loss:0.014, val_acc:0.985]
Epoch [42/120    avg_loss:0.018, val_acc:0.986]
Epoch [43/120    avg_loss:0.021, val_acc:0.987]
Epoch [44/120    avg_loss:0.012, val_acc:0.988]
Epoch [45/120    avg_loss:0.020, val_acc:0.986]
Epoch [46/120    avg_loss:0.012, val_acc:0.987]
Epoch [47/120    avg_loss:0.014, val_acc:0.987]
Epoch [48/120    avg_loss:0.013, val_acc:0.989]
Epoch [49/120    avg_loss:0.008, val_acc:0.990]
Epoch [50/120    avg_loss:0.014, val_acc:0.989]
Epoch [51/120    avg_loss:0.010, val_acc:0.988]
Epoch [52/120    avg_loss:0.018, val_acc:0.987]
Epoch [53/120    avg_loss:0.025, val_acc:0.985]
Epoch [54/120    avg_loss:0.012, val_acc:0.987]
Epoch [55/120    avg_loss:0.013, val_acc:0.988]
Epoch [56/120    avg_loss:0.012, val_acc:0.989]
Epoch [57/120    avg_loss:0.009, val_acc:0.989]
Epoch [58/120    avg_loss:0.012, val_acc:0.988]
Epoch [59/120    avg_loss:0.010, val_acc:0.989]
Epoch [60/120    avg_loss:0.014, val_acc:0.988]
Epoch [61/120    avg_loss:0.012, val_acc:0.989]
Epoch [62/120    avg_loss:0.008, val_acc:0.989]
Epoch [63/120    avg_loss:0.008, val_acc:0.989]
Epoch [64/120    avg_loss:0.014, val_acc:0.989]
Epoch [65/120    avg_loss:0.010, val_acc:0.989]
Epoch [66/120    avg_loss:0.012, val_acc:0.989]
Epoch [67/120    avg_loss:0.012, val_acc:0.989]
Epoch [68/120    avg_loss:0.009, val_acc:0.989]
Epoch [69/120    avg_loss:0.010, val_acc:0.989]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.008, val_acc:0.989]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.008, val_acc:0.989]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.010, val_acc:0.989]
Epoch [78/120    avg_loss:0.017, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.989]
Epoch [81/120    avg_loss:0.008, val_acc:0.989]
Epoch [82/120    avg_loss:0.009, val_acc:0.989]
Epoch [83/120    avg_loss:0.010, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.011, val_acc:0.989]
Epoch [88/120    avg_loss:0.006, val_acc:0.989]
Epoch [89/120    avg_loss:0.012, val_acc:0.989]
Epoch [90/120    avg_loss:0.008, val_acc:0.989]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.012, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.010, val_acc:0.989]
Epoch [96/120    avg_loss:0.010, val_acc:0.989]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.010, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.012, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.010, val_acc:0.989]
Epoch [105/120    avg_loss:0.010, val_acc:0.989]
Epoch [106/120    avg_loss:0.009, val_acc:0.989]
Epoch [107/120    avg_loss:0.013, val_acc:0.989]
Epoch [108/120    avg_loss:0.010, val_acc:0.989]
Epoch [109/120    avg_loss:0.010, val_acc:0.989]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.014, val_acc:0.989]
Epoch [112/120    avg_loss:0.014, val_acc:0.989]
Epoch [113/120    avg_loss:0.008, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.010, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.014, val_acc:0.989]
Epoch [118/120    avg_loss:0.012, val_acc:0.989]
Epoch [119/120    avg_loss:0.011, val_acc:0.989]
Epoch [120/120    avg_loss:0.010, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     0     0    16     0     3     0]
 [    0     2 18046     0    18     0    23     0     1     0]
 [    0     0     0  2032     0     0     0     0     0     4]
 [    0    35    14     0  2889     0     9     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4861     0    13     4]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     1     0    10    35     0     0     0  3510    15]
 [    0     0     0    13    15    44     0     0     0   847]]

Accuracy:
99.26975634444365

F1 scores:
[       nan 0.99557556 0.99839557 0.99340015 0.97453196 0.98342125
 0.99315558 0.99883586 0.9855398  0.94636872]

Kappa:
0.9903262414353047
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f11e69be828>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.402, val_acc:0.334]
Epoch [2/120    avg_loss:0.806, val_acc:0.735]
Epoch [3/120    avg_loss:0.457, val_acc:0.823]
Epoch [4/120    avg_loss:0.411, val_acc:0.871]
Epoch [5/120    avg_loss:0.350, val_acc:0.874]
Epoch [6/120    avg_loss:0.327, val_acc:0.849]
Epoch [7/120    avg_loss:0.252, val_acc:0.875]
Epoch [8/120    avg_loss:0.234, val_acc:0.911]
Epoch [9/120    avg_loss:0.167, val_acc:0.929]
Epoch [10/120    avg_loss:0.176, val_acc:0.939]
Epoch [11/120    avg_loss:0.187, val_acc:0.926]
Epoch [12/120    avg_loss:0.183, val_acc:0.932]
Epoch [13/120    avg_loss:0.105, val_acc:0.954]
Epoch [14/120    avg_loss:0.076, val_acc:0.936]
Epoch [15/120    avg_loss:0.095, val_acc:0.947]
Epoch [16/120    avg_loss:0.118, val_acc:0.950]
Epoch [17/120    avg_loss:0.107, val_acc:0.920]
Epoch [18/120    avg_loss:0.085, val_acc:0.950]
Epoch [19/120    avg_loss:0.080, val_acc:0.852]
Epoch [20/120    avg_loss:0.073, val_acc:0.962]
Epoch [21/120    avg_loss:0.070, val_acc:0.948]
Epoch [22/120    avg_loss:0.103, val_acc:0.963]
Epoch [23/120    avg_loss:0.130, val_acc:0.956]
Epoch [24/120    avg_loss:0.086, val_acc:0.912]
Epoch [25/120    avg_loss:0.042, val_acc:0.957]
Epoch [26/120    avg_loss:0.049, val_acc:0.942]
Epoch [27/120    avg_loss:0.036, val_acc:0.967]
Epoch [28/120    avg_loss:0.038, val_acc:0.971]
Epoch [29/120    avg_loss:0.042, val_acc:0.972]
Epoch [30/120    avg_loss:0.059, val_acc:0.924]
Epoch [31/120    avg_loss:0.048, val_acc:0.973]
Epoch [32/120    avg_loss:0.026, val_acc:0.977]
Epoch [33/120    avg_loss:0.025, val_acc:0.973]
Epoch [34/120    avg_loss:0.028, val_acc:0.976]
Epoch [35/120    avg_loss:0.073, val_acc:0.962]
Epoch [36/120    avg_loss:0.047, val_acc:0.963]
Epoch [37/120    avg_loss:0.037, val_acc:0.942]
Epoch [38/120    avg_loss:0.028, val_acc:0.977]
Epoch [39/120    avg_loss:0.041, val_acc:0.963]
Epoch [40/120    avg_loss:0.022, val_acc:0.975]
Epoch [41/120    avg_loss:0.029, val_acc:0.946]
Epoch [42/120    avg_loss:0.045, val_acc:0.967]
Epoch [43/120    avg_loss:0.023, val_acc:0.972]
Epoch [44/120    avg_loss:0.030, val_acc:0.972]
Epoch [45/120    avg_loss:0.021, val_acc:0.967]
Epoch [46/120    avg_loss:0.014, val_acc:0.985]
Epoch [47/120    avg_loss:0.017, val_acc:0.985]
Epoch [48/120    avg_loss:0.011, val_acc:0.982]
Epoch [49/120    avg_loss:0.018, val_acc:0.977]
Epoch [50/120    avg_loss:0.021, val_acc:0.974]
Epoch [51/120    avg_loss:0.019, val_acc:0.981]
Epoch [52/120    avg_loss:0.029, val_acc:0.960]
Epoch [53/120    avg_loss:0.017, val_acc:0.977]
Epoch [54/120    avg_loss:0.011, val_acc:0.980]
Epoch [55/120    avg_loss:0.016, val_acc:0.980]
Epoch [56/120    avg_loss:0.008, val_acc:0.978]
Epoch [57/120    avg_loss:0.010, val_acc:0.977]
Epoch [58/120    avg_loss:0.009, val_acc:0.980]
Epoch [59/120    avg_loss:0.005, val_acc:0.984]
Epoch [60/120    avg_loss:0.007, val_acc:0.977]
Epoch [61/120    avg_loss:0.006, val_acc:0.978]
Epoch [62/120    avg_loss:0.009, val_acc:0.978]
Epoch [63/120    avg_loss:0.005, val_acc:0.979]
Epoch [64/120    avg_loss:0.006, val_acc:0.979]
Epoch [65/120    avg_loss:0.008, val_acc:0.979]
Epoch [66/120    avg_loss:0.008, val_acc:0.977]
Epoch [67/120    avg_loss:0.008, val_acc:0.979]
Epoch [68/120    avg_loss:0.006, val_acc:0.980]
Epoch [69/120    avg_loss:0.009, val_acc:0.980]
Epoch [70/120    avg_loss:0.006, val_acc:0.980]
Epoch [71/120    avg_loss:0.004, val_acc:0.979]
Epoch [72/120    avg_loss:0.005, val_acc:0.978]
Epoch [73/120    avg_loss:0.004, val_acc:0.977]
Epoch [74/120    avg_loss:0.004, val_acc:0.977]
Epoch [75/120    avg_loss:0.004, val_acc:0.977]
Epoch [76/120    avg_loss:0.007, val_acc:0.978]
Epoch [77/120    avg_loss:0.005, val_acc:0.978]
Epoch [78/120    avg_loss:0.003, val_acc:0.978]
Epoch [79/120    avg_loss:0.005, val_acc:0.978]
Epoch [80/120    avg_loss:0.008, val_acc:0.977]
Epoch [81/120    avg_loss:0.005, val_acc:0.978]
Epoch [82/120    avg_loss:0.004, val_acc:0.978]
Epoch [83/120    avg_loss:0.005, val_acc:0.977]
Epoch [84/120    avg_loss:0.006, val_acc:0.978]
Epoch [85/120    avg_loss:0.004, val_acc:0.978]
Epoch [86/120    avg_loss:0.005, val_acc:0.978]
Epoch [87/120    avg_loss:0.006, val_acc:0.978]
Epoch [88/120    avg_loss:0.005, val_acc:0.978]
Epoch [89/120    avg_loss:0.005, val_acc:0.978]
Epoch [90/120    avg_loss:0.004, val_acc:0.978]
Epoch [91/120    avg_loss:0.005, val_acc:0.978]
Epoch [92/120    avg_loss:0.008, val_acc:0.978]
Epoch [93/120    avg_loss:0.009, val_acc:0.978]
Epoch [94/120    avg_loss:0.004, val_acc:0.978]
Epoch [95/120    avg_loss:0.006, val_acc:0.978]
Epoch [96/120    avg_loss:0.004, val_acc:0.978]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.007, val_acc:0.977]
Epoch [99/120    avg_loss:0.008, val_acc:0.977]
Epoch [100/120    avg_loss:0.005, val_acc:0.977]
Epoch [101/120    avg_loss:0.007, val_acc:0.977]
Epoch [102/120    avg_loss:0.007, val_acc:0.977]
Epoch [103/120    avg_loss:0.004, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.008, val_acc:0.977]
Epoch [106/120    avg_loss:0.004, val_acc:0.977]
Epoch [107/120    avg_loss:0.005, val_acc:0.977]
Epoch [108/120    avg_loss:0.005, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.977]
Epoch [110/120    avg_loss:0.008, val_acc:0.977]
Epoch [111/120    avg_loss:0.008, val_acc:0.977]
Epoch [112/120    avg_loss:0.006, val_acc:0.977]
Epoch [113/120    avg_loss:0.005, val_acc:0.977]
Epoch [114/120    avg_loss:0.006, val_acc:0.977]
Epoch [115/120    avg_loss:0.005, val_acc:0.977]
Epoch [116/120    avg_loss:0.004, val_acc:0.977]
Epoch [117/120    avg_loss:0.006, val_acc:0.977]
Epoch [118/120    avg_loss:0.007, val_acc:0.977]
Epoch [119/120    avg_loss:0.006, val_acc:0.977]
Epoch [120/120    avg_loss:0.006, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     0     0     9    13     0     8]
 [    0     0 18023     0    45     0    22     0     0     0]
 [    0     2     0  2029     2     0     0     0     0     3]
 [    0    32     8     0  2911     0     5     0    13     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4847     0     2    22]
 [    0     0     0     0     0     0     4  1284     0     2]
 [    0     3     0    15    33     0     0     0  3515     5]
 [    0     0     0     2    14    29     0     0     0   874]]

Accuracy:
99.26975634444365

F1 scores:
[       nan 0.9947945  0.99773029 0.99412053 0.97406726 0.98901099
 0.99272913 0.99265559 0.99000141 0.95206972]

Kappa:
0.9903300425431613
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c6ea35828>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.398, val_acc:0.379]
Epoch [2/120    avg_loss:0.790, val_acc:0.582]
Epoch [3/120    avg_loss:0.539, val_acc:0.763]
Epoch [4/120    avg_loss:0.491, val_acc:0.849]
Epoch [5/120    avg_loss:0.369, val_acc:0.880]
Epoch [6/120    avg_loss:0.302, val_acc:0.893]
Epoch [7/120    avg_loss:0.264, val_acc:0.884]
Epoch [8/120    avg_loss:0.173, val_acc:0.927]
Epoch [9/120    avg_loss:0.190, val_acc:0.901]
Epoch [10/120    avg_loss:0.136, val_acc:0.947]
Epoch [11/120    avg_loss:0.125, val_acc:0.945]
Epoch [12/120    avg_loss:0.176, val_acc:0.915]
Epoch [13/120    avg_loss:0.105, val_acc:0.956]
Epoch [14/120    avg_loss:0.115, val_acc:0.948]
Epoch [15/120    avg_loss:0.099, val_acc:0.952]
Epoch [16/120    avg_loss:0.063, val_acc:0.958]
Epoch [17/120    avg_loss:0.059, val_acc:0.956]
Epoch [18/120    avg_loss:0.080, val_acc:0.952]
Epoch [19/120    avg_loss:0.155, val_acc:0.917]
Epoch [20/120    avg_loss:0.102, val_acc:0.966]
Epoch [21/120    avg_loss:0.092, val_acc:0.924]
Epoch [22/120    avg_loss:0.071, val_acc:0.949]
Epoch [23/120    avg_loss:0.055, val_acc:0.972]
Epoch [24/120    avg_loss:0.051, val_acc:0.963]
Epoch [25/120    avg_loss:0.039, val_acc:0.966]
Epoch [26/120    avg_loss:0.031, val_acc:0.968]
Epoch [27/120    avg_loss:0.037, val_acc:0.962]
Epoch [28/120    avg_loss:0.023, val_acc:0.974]
Epoch [29/120    avg_loss:0.017, val_acc:0.972]
Epoch [30/120    avg_loss:0.022, val_acc:0.961]
Epoch [31/120    avg_loss:0.029, val_acc:0.977]
Epoch [32/120    avg_loss:0.030, val_acc:0.979]
Epoch [33/120    avg_loss:0.020, val_acc:0.976]
Epoch [34/120    avg_loss:0.013, val_acc:0.976]
Epoch [35/120    avg_loss:0.037, val_acc:0.978]
Epoch [36/120    avg_loss:0.024, val_acc:0.975]
Epoch [37/120    avg_loss:0.017, val_acc:0.979]
Epoch [38/120    avg_loss:0.025, val_acc:0.974]
Epoch [39/120    avg_loss:0.020, val_acc:0.982]
Epoch [40/120    avg_loss:0.020, val_acc:0.982]
Epoch [41/120    avg_loss:0.046, val_acc:0.963]
Epoch [42/120    avg_loss:0.053, val_acc:0.969]
Epoch [43/120    avg_loss:0.031, val_acc:0.980]
Epoch [44/120    avg_loss:0.028, val_acc:0.963]
Epoch [45/120    avg_loss:0.040, val_acc:0.973]
Epoch [46/120    avg_loss:0.014, val_acc:0.980]
Epoch [47/120    avg_loss:0.014, val_acc:0.977]
Epoch [48/120    avg_loss:0.009, val_acc:0.983]
Epoch [49/120    avg_loss:0.012, val_acc:0.968]
Epoch [50/120    avg_loss:0.010, val_acc:0.981]
Epoch [51/120    avg_loss:0.015, val_acc:0.975]
Epoch [52/120    avg_loss:0.011, val_acc:0.973]
Epoch [53/120    avg_loss:0.012, val_acc:0.982]
Epoch [54/120    avg_loss:0.011, val_acc:0.979]
Epoch [55/120    avg_loss:0.008, val_acc:0.986]
Epoch [56/120    avg_loss:0.010, val_acc:0.984]
Epoch [57/120    avg_loss:0.019, val_acc:0.972]
Epoch [58/120    avg_loss:0.010, val_acc:0.985]
Epoch [59/120    avg_loss:0.009, val_acc:0.982]
Epoch [60/120    avg_loss:0.007, val_acc:0.985]
Epoch [61/120    avg_loss:0.010, val_acc:0.981]
Epoch [62/120    avg_loss:0.011, val_acc:0.981]
Epoch [63/120    avg_loss:0.014, val_acc:0.979]
Epoch [64/120    avg_loss:0.011, val_acc:0.973]
Epoch [65/120    avg_loss:0.005, val_acc:0.986]
Epoch [66/120    avg_loss:0.014, val_acc:0.987]
Epoch [67/120    avg_loss:0.010, val_acc:0.968]
Epoch [68/120    avg_loss:0.007, val_acc:0.989]
Epoch [69/120    avg_loss:0.005, val_acc:0.990]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.982]
Epoch [72/120    avg_loss:0.014, val_acc:0.985]
Epoch [73/120    avg_loss:0.024, val_acc:0.980]
Epoch [74/120    avg_loss:0.007, val_acc:0.987]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.005, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.986]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.980]
Epoch [81/120    avg_loss:0.022, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.007, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.005, val_acc:0.988]
Epoch [86/120    avg_loss:0.002, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.003, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.003, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.990]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.003, val_acc:0.991]
Epoch [100/120    avg_loss:0.003, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.991]
Epoch [104/120    avg_loss:0.010, val_acc:0.989]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.003, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.003, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.003, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     5     0    14     0     0     0]
 [    0     0 18072     0    14     0     3     0     0     1]
 [    0     0     0  2035     0     0     0     0     0     1]
 [    0    23     8     0  2919     0     5     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4860     0     5     7]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     0     0     6    44     0     0     0  3521     0]
 [    0     0     0     1    14    29     0     0     0   875]]

Accuracy:
99.50594076109223

F1 scores:
[       nan 0.99673609 0.99911544 0.99803825 0.97821716 0.98901099
 0.9956976  0.9992242  0.98987911 0.97060455]

Kappa:
0.9934535713705367
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff2255a07b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.432, val_acc:0.511]
Epoch [2/120    avg_loss:0.806, val_acc:0.685]
Epoch [3/120    avg_loss:0.684, val_acc:0.619]
Epoch [4/120    avg_loss:0.509, val_acc:0.793]
Epoch [5/120    avg_loss:0.340, val_acc:0.873]
Epoch [6/120    avg_loss:0.288, val_acc:0.884]
Epoch [7/120    avg_loss:0.250, val_acc:0.878]
Epoch [8/120    avg_loss:0.194, val_acc:0.923]
Epoch [9/120    avg_loss:0.210, val_acc:0.899]
Epoch [10/120    avg_loss:0.201, val_acc:0.903]
Epoch [11/120    avg_loss:0.144, val_acc:0.957]
Epoch [12/120    avg_loss:0.101, val_acc:0.904]
Epoch [13/120    avg_loss:0.118, val_acc:0.838]
Epoch [14/120    avg_loss:0.106, val_acc:0.934]
Epoch [15/120    avg_loss:0.109, val_acc:0.946]
Epoch [16/120    avg_loss:0.067, val_acc:0.960]
Epoch [17/120    avg_loss:0.089, val_acc:0.961]
Epoch [18/120    avg_loss:0.074, val_acc:0.914]
Epoch [19/120    avg_loss:0.074, val_acc:0.951]
Epoch [20/120    avg_loss:0.072, val_acc:0.969]
Epoch [21/120    avg_loss:0.077, val_acc:0.969]
Epoch [22/120    avg_loss:0.061, val_acc:0.969]
Epoch [23/120    avg_loss:0.065, val_acc:0.953]
Epoch [24/120    avg_loss:0.062, val_acc:0.954]
Epoch [25/120    avg_loss:0.112, val_acc:0.957]
Epoch [26/120    avg_loss:0.087, val_acc:0.957]
Epoch [27/120    avg_loss:0.068, val_acc:0.974]
Epoch [28/120    avg_loss:0.049, val_acc:0.966]
Epoch [29/120    avg_loss:0.033, val_acc:0.981]
Epoch [30/120    avg_loss:0.044, val_acc:0.973]
Epoch [31/120    avg_loss:0.044, val_acc:0.964]
Epoch [32/120    avg_loss:0.041, val_acc:0.958]
Epoch [33/120    avg_loss:0.028, val_acc:0.957]
Epoch [34/120    avg_loss:0.029, val_acc:0.987]
Epoch [35/120    avg_loss:0.028, val_acc:0.980]
Epoch [36/120    avg_loss:0.021, val_acc:0.963]
Epoch [37/120    avg_loss:0.028, val_acc:0.968]
Epoch [38/120    avg_loss:0.023, val_acc:0.978]
Epoch [39/120    avg_loss:0.029, val_acc:0.989]
Epoch [40/120    avg_loss:0.024, val_acc:0.973]
Epoch [41/120    avg_loss:0.071, val_acc:0.974]
Epoch [42/120    avg_loss:0.086, val_acc:0.966]
Epoch [43/120    avg_loss:0.043, val_acc:0.982]
Epoch [44/120    avg_loss:0.046, val_acc:0.982]
Epoch [45/120    avg_loss:0.017, val_acc:0.980]
Epoch [46/120    avg_loss:0.023, val_acc:0.985]
Epoch [47/120    avg_loss:0.020, val_acc:0.991]
Epoch [48/120    avg_loss:0.020, val_acc:0.981]
Epoch [49/120    avg_loss:0.014, val_acc:0.986]
Epoch [50/120    avg_loss:0.007, val_acc:0.991]
Epoch [51/120    avg_loss:0.007, val_acc:0.989]
Epoch [52/120    avg_loss:0.018, val_acc:0.989]
Epoch [53/120    avg_loss:0.014, val_acc:0.983]
Epoch [54/120    avg_loss:0.007, val_acc:0.991]
Epoch [55/120    avg_loss:0.010, val_acc:0.993]
Epoch [56/120    avg_loss:0.014, val_acc:0.966]
Epoch [57/120    avg_loss:0.016, val_acc:0.989]
Epoch [58/120    avg_loss:0.006, val_acc:0.991]
Epoch [59/120    avg_loss:0.009, val_acc:0.992]
Epoch [60/120    avg_loss:0.017, val_acc:0.983]
Epoch [61/120    avg_loss:0.009, val_acc:0.991]
Epoch [62/120    avg_loss:0.009, val_acc:0.991]
Epoch [63/120    avg_loss:0.015, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.992]
Epoch [65/120    avg_loss:0.005, val_acc:0.993]
Epoch [66/120    avg_loss:0.013, val_acc:0.991]
Epoch [67/120    avg_loss:0.012, val_acc:0.989]
Epoch [68/120    avg_loss:0.007, val_acc:0.991]
Epoch [69/120    avg_loss:0.006, val_acc:0.993]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.008, val_acc:0.990]
Epoch [72/120    avg_loss:0.008, val_acc:0.983]
Epoch [73/120    avg_loss:0.015, val_acc:0.940]
Epoch [74/120    avg_loss:0.017, val_acc:0.991]
Epoch [75/120    avg_loss:0.006, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.995]
Epoch [77/120    avg_loss:0.010, val_acc:0.991]
Epoch [78/120    avg_loss:0.016, val_acc:0.992]
Epoch [79/120    avg_loss:0.047, val_acc:0.984]
Epoch [80/120    avg_loss:0.020, val_acc:0.988]
Epoch [81/120    avg_loss:0.037, val_acc:0.982]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.030, val_acc:0.976]
Epoch [84/120    avg_loss:0.036, val_acc:0.982]
Epoch [85/120    avg_loss:0.023, val_acc:0.985]
Epoch [86/120    avg_loss:0.020, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.991]
Epoch [88/120    avg_loss:0.006, val_acc:0.989]
Epoch [89/120    avg_loss:0.009, val_acc:0.989]
Epoch [90/120    avg_loss:0.012, val_acc:0.990]
Epoch [91/120    avg_loss:0.007, val_acc:0.991]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.990]
Epoch [94/120    avg_loss:0.011, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.991]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.005, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.003, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.004, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.013, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.003, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     3     0     0     6     0     0]
 [    0    10 18031     0    35     0    14     0     0     0]
 [    0     0     0  2023     0     0     0     0     8     5]
 [    0    34     8     0  2909     0     0     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4859     0     2    17]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     2     0     0    33     0     0     0  3521    15]
 [    0     0     0     4    18    50     0     0     0   847]]

Accuracy:
99.30349697539344

F1 scores:
[       nan 0.99573676 0.99814553 0.9958159  0.97453936 0.98120301
 0.99641136 0.99612703 0.98862839 0.93850416]

Kappa:
0.9907756753229395
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f157dc977f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.498, val_acc:0.679]
Epoch [2/120    avg_loss:0.783, val_acc:0.617]
Epoch [3/120    avg_loss:0.487, val_acc:0.768]
Epoch [4/120    avg_loss:0.391, val_acc:0.817]
Epoch [5/120    avg_loss:0.340, val_acc:0.889]
Epoch [6/120    avg_loss:0.306, val_acc:0.881]
Epoch [7/120    avg_loss:0.228, val_acc:0.891]
Epoch [8/120    avg_loss:0.194, val_acc:0.922]
Epoch [9/120    avg_loss:0.210, val_acc:0.913]
Epoch [10/120    avg_loss:0.185, val_acc:0.910]
Epoch [11/120    avg_loss:0.145, val_acc:0.892]
Epoch [12/120    avg_loss:0.136, val_acc:0.905]
Epoch [13/120    avg_loss:0.097, val_acc:0.959]
Epoch [14/120    avg_loss:0.082, val_acc:0.952]
Epoch [15/120    avg_loss:0.085, val_acc:0.952]
Epoch [16/120    avg_loss:0.108, val_acc:0.938]
Epoch [17/120    avg_loss:0.067, val_acc:0.955]
Epoch [18/120    avg_loss:0.057, val_acc:0.963]
Epoch [19/120    avg_loss:0.054, val_acc:0.965]
Epoch [20/120    avg_loss:0.050, val_acc:0.940]
Epoch [21/120    avg_loss:0.051, val_acc:0.959]
Epoch [22/120    avg_loss:0.068, val_acc:0.968]
Epoch [23/120    avg_loss:0.048, val_acc:0.971]
Epoch [24/120    avg_loss:0.049, val_acc:0.961]
Epoch [25/120    avg_loss:0.034, val_acc:0.978]
Epoch [26/120    avg_loss:0.031, val_acc:0.971]
Epoch [27/120    avg_loss:0.037, val_acc:0.967]
Epoch [28/120    avg_loss:0.093, val_acc:0.958]
Epoch [29/120    avg_loss:0.046, val_acc:0.964]
Epoch [30/120    avg_loss:0.028, val_acc:0.972]
Epoch [31/120    avg_loss:0.048, val_acc:0.963]
Epoch [32/120    avg_loss:0.032, val_acc:0.977]
Epoch [33/120    avg_loss:0.024, val_acc:0.975]
Epoch [34/120    avg_loss:0.019, val_acc:0.976]
Epoch [35/120    avg_loss:0.015, val_acc:0.980]
Epoch [36/120    avg_loss:0.020, val_acc:0.974]
Epoch [37/120    avg_loss:0.020, val_acc:0.975]
Epoch [38/120    avg_loss:0.014, val_acc:0.978]
Epoch [39/120    avg_loss:0.016, val_acc:0.974]
Epoch [40/120    avg_loss:0.024, val_acc:0.974]
Epoch [41/120    avg_loss:0.018, val_acc:0.980]
Epoch [42/120    avg_loss:0.015, val_acc:0.965]
Epoch [43/120    avg_loss:0.039, val_acc:0.974]
Epoch [44/120    avg_loss:0.130, val_acc:0.928]
Epoch [45/120    avg_loss:0.123, val_acc:0.964]
Epoch [46/120    avg_loss:0.078, val_acc:0.967]
Epoch [47/120    avg_loss:0.024, val_acc:0.964]
Epoch [48/120    avg_loss:0.035, val_acc:0.974]
Epoch [49/120    avg_loss:0.019, val_acc:0.982]
Epoch [50/120    avg_loss:0.025, val_acc:0.974]
Epoch [51/120    avg_loss:0.019, val_acc:0.983]
Epoch [52/120    avg_loss:0.017, val_acc:0.978]
Epoch [53/120    avg_loss:0.009, val_acc:0.987]
Epoch [54/120    avg_loss:0.014, val_acc:0.952]
Epoch [55/120    avg_loss:0.026, val_acc:0.969]
Epoch [56/120    avg_loss:0.011, val_acc:0.985]
Epoch [57/120    avg_loss:0.009, val_acc:0.982]
Epoch [58/120    avg_loss:0.007, val_acc:0.983]
Epoch [59/120    avg_loss:0.007, val_acc:0.985]
Epoch [60/120    avg_loss:0.025, val_acc:0.968]
Epoch [61/120    avg_loss:0.013, val_acc:0.982]
Epoch [62/120    avg_loss:0.013, val_acc:0.981]
Epoch [63/120    avg_loss:0.014, val_acc:0.978]
Epoch [64/120    avg_loss:0.008, val_acc:0.983]
Epoch [65/120    avg_loss:0.023, val_acc:0.981]
Epoch [66/120    avg_loss:0.010, val_acc:0.981]
Epoch [67/120    avg_loss:0.005, val_acc:0.982]
Epoch [68/120    avg_loss:0.005, val_acc:0.984]
Epoch [69/120    avg_loss:0.005, val_acc:0.984]
Epoch [70/120    avg_loss:0.005, val_acc:0.984]
Epoch [71/120    avg_loss:0.005, val_acc:0.984]
Epoch [72/120    avg_loss:0.003, val_acc:0.984]
Epoch [73/120    avg_loss:0.004, val_acc:0.985]
Epoch [74/120    avg_loss:0.005, val_acc:0.985]
Epoch [75/120    avg_loss:0.004, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.985]
Epoch [77/120    avg_loss:0.005, val_acc:0.985]
Epoch [78/120    avg_loss:0.004, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.985]
Epoch [81/120    avg_loss:0.004, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.003, val_acc:0.985]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.004, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.004, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.985]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.985]
Epoch [106/120    avg_loss:0.011, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.004, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     0     0    11     2     0     0]
 [    0     3 18050     0    37     0     0     0     0     0]
 [    0     0     0  2026     0     0     0     0     7     3]
 [    0    39     9     0  2901     0     1     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4870     0     0     2]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     4     0     4    36     0     0     0  3527     0]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
99.45773985973538

F1 scores:
[       nan 0.99542529 0.99847877 0.99655681 0.97348993 0.99126472
 0.99774636 0.99844961 0.98975726 0.97674419]

Kappa:
0.9928159881486613
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03e43fc7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.395, val_acc:0.694]
Epoch [2/120    avg_loss:0.744, val_acc:0.761]
Epoch [3/120    avg_loss:0.508, val_acc:0.806]
Epoch [4/120    avg_loss:0.312, val_acc:0.864]
Epoch [5/120    avg_loss:0.374, val_acc:0.852]
Epoch [6/120    avg_loss:0.254, val_acc:0.895]
Epoch [7/120    avg_loss:0.179, val_acc:0.938]
Epoch [8/120    avg_loss:0.163, val_acc:0.932]
Epoch [9/120    avg_loss:0.202, val_acc:0.908]
Epoch [10/120    avg_loss:0.135, val_acc:0.956]
Epoch [11/120    avg_loss:0.174, val_acc:0.919]
Epoch [12/120    avg_loss:0.129, val_acc:0.949]
Epoch [13/120    avg_loss:0.104, val_acc:0.967]
Epoch [14/120    avg_loss:0.111, val_acc:0.943]
Epoch [15/120    avg_loss:0.065, val_acc:0.970]
Epoch [16/120    avg_loss:0.037, val_acc:0.974]
Epoch [17/120    avg_loss:0.075, val_acc:0.951]
Epoch [18/120    avg_loss:0.098, val_acc:0.966]
Epoch [19/120    avg_loss:0.117, val_acc:0.958]
Epoch [20/120    avg_loss:0.067, val_acc:0.979]
Epoch [21/120    avg_loss:0.042, val_acc:0.975]
Epoch [22/120    avg_loss:0.048, val_acc:0.974]
Epoch [23/120    avg_loss:0.038, val_acc:0.978]
Epoch [24/120    avg_loss:0.034, val_acc:0.979]
Epoch [25/120    avg_loss:0.032, val_acc:0.964]
Epoch [26/120    avg_loss:0.046, val_acc:0.966]
Epoch [27/120    avg_loss:0.062, val_acc:0.975]
Epoch [28/120    avg_loss:0.052, val_acc:0.970]
Epoch [29/120    avg_loss:0.031, val_acc:0.960]
Epoch [30/120    avg_loss:0.026, val_acc:0.985]
Epoch [31/120    avg_loss:0.026, val_acc:0.976]
Epoch [32/120    avg_loss:0.059, val_acc:0.983]
Epoch [33/120    avg_loss:0.032, val_acc:0.970]
Epoch [34/120    avg_loss:0.032, val_acc:0.976]
Epoch [35/120    avg_loss:0.021, val_acc:0.968]
Epoch [36/120    avg_loss:0.017, val_acc:0.978]
Epoch [37/120    avg_loss:0.040, val_acc:0.964]
Epoch [38/120    avg_loss:0.068, val_acc:0.959]
Epoch [39/120    avg_loss:0.063, val_acc:0.972]
Epoch [40/120    avg_loss:0.031, val_acc:0.982]
Epoch [41/120    avg_loss:0.037, val_acc:0.983]
Epoch [42/120    avg_loss:0.026, val_acc:0.978]
Epoch [43/120    avg_loss:0.015, val_acc:0.984]
Epoch [44/120    avg_loss:0.012, val_acc:0.985]
Epoch [45/120    avg_loss:0.010, val_acc:0.984]
Epoch [46/120    avg_loss:0.011, val_acc:0.985]
Epoch [47/120    avg_loss:0.013, val_acc:0.986]
Epoch [48/120    avg_loss:0.007, val_acc:0.986]
Epoch [49/120    avg_loss:0.008, val_acc:0.986]
Epoch [50/120    avg_loss:0.008, val_acc:0.986]
Epoch [51/120    avg_loss:0.013, val_acc:0.985]
Epoch [52/120    avg_loss:0.009, val_acc:0.986]
Epoch [53/120    avg_loss:0.013, val_acc:0.986]
Epoch [54/120    avg_loss:0.011, val_acc:0.985]
Epoch [55/120    avg_loss:0.009, val_acc:0.987]
Epoch [56/120    avg_loss:0.018, val_acc:0.986]
Epoch [57/120    avg_loss:0.009, val_acc:0.987]
Epoch [58/120    avg_loss:0.011, val_acc:0.985]
Epoch [59/120    avg_loss:0.009, val_acc:0.986]
Epoch [60/120    avg_loss:0.009, val_acc:0.986]
Epoch [61/120    avg_loss:0.010, val_acc:0.986]
Epoch [62/120    avg_loss:0.009, val_acc:0.986]
Epoch [63/120    avg_loss:0.016, val_acc:0.988]
Epoch [64/120    avg_loss:0.010, val_acc:0.987]
Epoch [65/120    avg_loss:0.008, val_acc:0.988]
Epoch [66/120    avg_loss:0.009, val_acc:0.989]
Epoch [67/120    avg_loss:0.011, val_acc:0.988]
Epoch [68/120    avg_loss:0.011, val_acc:0.987]
Epoch [69/120    avg_loss:0.017, val_acc:0.987]
Epoch [70/120    avg_loss:0.009, val_acc:0.989]
Epoch [71/120    avg_loss:0.011, val_acc:0.989]
Epoch [72/120    avg_loss:0.010, val_acc:0.987]
Epoch [73/120    avg_loss:0.013, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.989]
Epoch [75/120    avg_loss:0.008, val_acc:0.989]
Epoch [76/120    avg_loss:0.019, val_acc:0.983]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.019, val_acc:0.987]
Epoch [79/120    avg_loss:0.012, val_acc:0.989]
Epoch [80/120    avg_loss:0.015, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.989]
Epoch [83/120    avg_loss:0.011, val_acc:0.989]
Epoch [84/120    avg_loss:0.005, val_acc:0.988]
Epoch [85/120    avg_loss:0.005, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.009, val_acc:0.989]
Epoch [91/120    avg_loss:0.009, val_acc:0.989]
Epoch [92/120    avg_loss:0.011, val_acc:0.989]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.007, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     0     0     0    14    12     0     0]
 [    0     1 18039     0    47     0     3     0     0     0]
 [    0     1     0  2032     1     0     0     0     0     2]
 [    0    34     8     0  2907     0     0     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4857     0     0    15]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     2     0     4    26     0     0     0  3520    19]
 [    0     0     0     8    14    23     0     0     0   874]]

Accuracy:
99.36133805702167

F1 scores:
[       nan 0.99502951 0.99820159 0.99607843 0.97435897 0.99126472
 0.99589912 0.99459459 0.98959798 0.9557135 ]

Kappa:
0.9915411363649798
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d1e1f6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.383, val_acc:0.461]
Epoch [2/120    avg_loss:0.757, val_acc:0.762]
Epoch [3/120    avg_loss:0.478, val_acc:0.778]
Epoch [4/120    avg_loss:0.463, val_acc:0.732]
Epoch [5/120    avg_loss:0.362, val_acc:0.888]
Epoch [6/120    avg_loss:0.284, val_acc:0.852]
Epoch [7/120    avg_loss:0.261, val_acc:0.906]
Epoch [8/120    avg_loss:0.191, val_acc:0.899]
Epoch [9/120    avg_loss:0.225, val_acc:0.884]
Epoch [10/120    avg_loss:0.157, val_acc:0.934]
Epoch [11/120    avg_loss:0.169, val_acc:0.922]
Epoch [12/120    avg_loss:0.157, val_acc:0.873]
Epoch [13/120    avg_loss:0.147, val_acc:0.936]
Epoch [14/120    avg_loss:0.109, val_acc:0.923]
Epoch [15/120    avg_loss:0.100, val_acc:0.949]
Epoch [16/120    avg_loss:0.106, val_acc:0.938]
Epoch [17/120    avg_loss:0.148, val_acc:0.936]
Epoch [18/120    avg_loss:0.118, val_acc:0.932]
Epoch [19/120    avg_loss:0.063, val_acc:0.939]
Epoch [20/120    avg_loss:0.080, val_acc:0.942]
Epoch [21/120    avg_loss:0.073, val_acc:0.939]
Epoch [22/120    avg_loss:0.058, val_acc:0.949]
Epoch [23/120    avg_loss:0.042, val_acc:0.956]
Epoch [24/120    avg_loss:0.062, val_acc:0.969]
Epoch [25/120    avg_loss:0.074, val_acc:0.946]
Epoch [26/120    avg_loss:0.092, val_acc:0.947]
Epoch [27/120    avg_loss:0.046, val_acc:0.953]
Epoch [28/120    avg_loss:0.036, val_acc:0.948]
Epoch [29/120    avg_loss:0.047, val_acc:0.968]
Epoch [30/120    avg_loss:0.030, val_acc:0.964]
Epoch [31/120    avg_loss:0.036, val_acc:0.946]
Epoch [32/120    avg_loss:0.048, val_acc:0.962]
Epoch [33/120    avg_loss:0.042, val_acc:0.967]
Epoch [34/120    avg_loss:0.036, val_acc:0.963]
Epoch [35/120    avg_loss:0.032, val_acc:0.961]
Epoch [36/120    avg_loss:0.033, val_acc:0.970]
Epoch [37/120    avg_loss:0.023, val_acc:0.979]
Epoch [38/120    avg_loss:0.020, val_acc:0.973]
Epoch [39/120    avg_loss:0.026, val_acc:0.972]
Epoch [40/120    avg_loss:0.017, val_acc:0.966]
Epoch [41/120    avg_loss:0.016, val_acc:0.975]
Epoch [42/120    avg_loss:0.011, val_acc:0.974]
Epoch [43/120    avg_loss:0.022, val_acc:0.972]
Epoch [44/120    avg_loss:0.008, val_acc:0.977]
Epoch [45/120    avg_loss:0.020, val_acc:0.972]
Epoch [46/120    avg_loss:0.020, val_acc:0.974]
Epoch [47/120    avg_loss:0.014, val_acc:0.977]
Epoch [48/120    avg_loss:0.018, val_acc:0.969]
Epoch [49/120    avg_loss:0.028, val_acc:0.966]
Epoch [50/120    avg_loss:0.014, val_acc:0.973]
Epoch [51/120    avg_loss:0.016, val_acc:0.978]
Epoch [52/120    avg_loss:0.012, val_acc:0.978]
Epoch [53/120    avg_loss:0.018, val_acc:0.980]
Epoch [54/120    avg_loss:0.014, val_acc:0.978]
Epoch [55/120    avg_loss:0.010, val_acc:0.980]
Epoch [56/120    avg_loss:0.010, val_acc:0.980]
Epoch [57/120    avg_loss:0.006, val_acc:0.981]
Epoch [58/120    avg_loss:0.011, val_acc:0.981]
Epoch [59/120    avg_loss:0.012, val_acc:0.981]
Epoch [60/120    avg_loss:0.009, val_acc:0.980]
Epoch [61/120    avg_loss:0.013, val_acc:0.980]
Epoch [62/120    avg_loss:0.014, val_acc:0.982]
Epoch [63/120    avg_loss:0.019, val_acc:0.980]
Epoch [64/120    avg_loss:0.015, val_acc:0.980]
Epoch [65/120    avg_loss:0.011, val_acc:0.979]
Epoch [66/120    avg_loss:0.012, val_acc:0.982]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.984]
Epoch [69/120    avg_loss:0.007, val_acc:0.981]
Epoch [70/120    avg_loss:0.007, val_acc:0.981]
Epoch [71/120    avg_loss:0.006, val_acc:0.982]
Epoch [72/120    avg_loss:0.008, val_acc:0.979]
Epoch [73/120    avg_loss:0.010, val_acc:0.980]
Epoch [74/120    avg_loss:0.007, val_acc:0.983]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.014, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.008, val_acc:0.980]
Epoch [81/120    avg_loss:0.007, val_acc:0.982]
Epoch [82/120    avg_loss:0.005, val_acc:0.982]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.007, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.005, val_acc:0.983]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.012, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.012, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     1     0    17     9     0     5]
 [    0     0 18026     0    33     0    31     0     0     0]
 [    0     2     0  2021     2     0     0     0     8     3]
 [    0    33    14     0  2896     0     0     0    28     1]
 [    0     0     0     1     0  1304     0     0     0     0]
 [    0     0     6     0     0     0  4863     0     0     9]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     0     0     6    40     0     0     0  3517     8]
 [    0     0     0     2    11    39     0     0     0   867]]

Accuracy:
99.2504759839009

F1 scores:
[       nan 0.99479288 0.99767545 0.99409739 0.97262804 0.98489426
 0.99336125 0.99574797 0.98736665 0.95695364]

Kappa:
0.9900730528098125
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f998ea757f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.360, val_acc:0.492]
Epoch [2/120    avg_loss:0.723, val_acc:0.722]
Epoch [3/120    avg_loss:0.577, val_acc:0.791]
Epoch [4/120    avg_loss:0.425, val_acc:0.768]
Epoch [5/120    avg_loss:0.323, val_acc:0.869]
Epoch [6/120    avg_loss:0.260, val_acc:0.884]
Epoch [7/120    avg_loss:0.250, val_acc:0.920]
Epoch [8/120    avg_loss:0.258, val_acc:0.819]
Epoch [9/120    avg_loss:0.182, val_acc:0.927]
Epoch [10/120    avg_loss:0.125, val_acc:0.936]
Epoch [11/120    avg_loss:0.135, val_acc:0.900]
Epoch [12/120    avg_loss:0.125, val_acc:0.934]
Epoch [13/120    avg_loss:0.096, val_acc:0.964]
Epoch [14/120    avg_loss:0.101, val_acc:0.952]
Epoch [15/120    avg_loss:0.104, val_acc:0.899]
Epoch [16/120    avg_loss:0.137, val_acc:0.930]
Epoch [17/120    avg_loss:0.075, val_acc:0.966]
Epoch [18/120    avg_loss:0.059, val_acc:0.964]
Epoch [19/120    avg_loss:0.052, val_acc:0.970]
Epoch [20/120    avg_loss:0.047, val_acc:0.972]
Epoch [21/120    avg_loss:0.079, val_acc:0.924]
Epoch [22/120    avg_loss:0.067, val_acc:0.974]
Epoch [23/120    avg_loss:0.037, val_acc:0.980]
Epoch [24/120    avg_loss:0.053, val_acc:0.971]
Epoch [25/120    avg_loss:0.051, val_acc:0.949]
Epoch [26/120    avg_loss:0.040, val_acc:0.979]
Epoch [27/120    avg_loss:0.052, val_acc:0.957]
Epoch [28/120    avg_loss:0.058, val_acc:0.979]
Epoch [29/120    avg_loss:0.023, val_acc:0.980]
Epoch [30/120    avg_loss:0.027, val_acc:0.983]
Epoch [31/120    avg_loss:0.022, val_acc:0.983]
Epoch [32/120    avg_loss:0.030, val_acc:0.981]
Epoch [33/120    avg_loss:0.016, val_acc:0.989]
Epoch [34/120    avg_loss:0.018, val_acc:0.985]
Epoch [35/120    avg_loss:0.013, val_acc:0.982]
Epoch [36/120    avg_loss:0.019, val_acc:0.975]
Epoch [37/120    avg_loss:0.026, val_acc:0.953]
Epoch [38/120    avg_loss:0.107, val_acc:0.968]
Epoch [39/120    avg_loss:0.111, val_acc:0.946]
Epoch [40/120    avg_loss:0.067, val_acc:0.972]
Epoch [41/120    avg_loss:0.038, val_acc:0.981]
Epoch [42/120    avg_loss:0.039, val_acc:0.977]
Epoch [43/120    avg_loss:0.035, val_acc:0.984]
Epoch [44/120    avg_loss:0.035, val_acc:0.964]
Epoch [45/120    avg_loss:0.024, val_acc:0.978]
Epoch [46/120    avg_loss:0.017, val_acc:0.984]
Epoch [47/120    avg_loss:0.012, val_acc:0.984]
Epoch [48/120    avg_loss:0.023, val_acc:0.985]
Epoch [49/120    avg_loss:0.010, val_acc:0.984]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.008, val_acc:0.985]
Epoch [52/120    avg_loss:0.013, val_acc:0.985]
Epoch [53/120    avg_loss:0.010, val_acc:0.986]
Epoch [54/120    avg_loss:0.011, val_acc:0.986]
Epoch [55/120    avg_loss:0.016, val_acc:0.985]
Epoch [56/120    avg_loss:0.009, val_acc:0.984]
Epoch [57/120    avg_loss:0.012, val_acc:0.984]
Epoch [58/120    avg_loss:0.013, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.015, val_acc:0.985]
Epoch [61/120    avg_loss:0.010, val_acc:0.985]
Epoch [62/120    avg_loss:0.007, val_acc:0.986]
Epoch [63/120    avg_loss:0.009, val_acc:0.986]
Epoch [64/120    avg_loss:0.007, val_acc:0.986]
Epoch [65/120    avg_loss:0.012, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.017, val_acc:0.985]
Epoch [68/120    avg_loss:0.011, val_acc:0.985]
Epoch [69/120    avg_loss:0.010, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.985]
Epoch [71/120    avg_loss:0.009, val_acc:0.985]
Epoch [72/120    avg_loss:0.010, val_acc:0.985]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.014, val_acc:0.985]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.017, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.011, val_acc:0.985]
Epoch [83/120    avg_loss:0.010, val_acc:0.985]
Epoch [84/120    avg_loss:0.014, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.014, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.014, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.021, val_acc:0.985]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.013, val_acc:0.985]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.013, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.014, val_acc:0.985]
Epoch [106/120    avg_loss:0.011, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.011, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.013, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     0     0    13     0     0     6]
 [    0     4 17993     0    48     0    43     0     2     0]
 [    0     0     0  2019     0     0     0     0     6    11]
 [    0    30    14     0  2896     0     0     0    27     5]
 [    0     0     0    32     0  1270     0     0     0     3]
 [    0     0     0     0     0     0  4855     0     7    16]
 [    0     0     0     0     0     0     6  1282     0     2]
 [    0     0     0    10    37     0     0     0  3505    19]
 [    0     0     0     0     9    30     0     0     0   880]]

Accuracy:
99.08418287421975

F1 scores:
[       nan 0.99588477 0.99692495 0.98559922 0.97148608 0.97504798
 0.9913221  0.99688958 0.9848272  0.9457281 ]

Kappa:
0.9878759903463313
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a7c867828>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.414, val_acc:0.547]
Epoch [2/120    avg_loss:0.821, val_acc:0.723]
Epoch [3/120    avg_loss:0.517, val_acc:0.784]
Epoch [4/120    avg_loss:0.541, val_acc:0.868]
Epoch [5/120    avg_loss:0.303, val_acc:0.914]
Epoch [6/120    avg_loss:0.244, val_acc:0.913]
Epoch [7/120    avg_loss:0.248, val_acc:0.895]
Epoch [8/120    avg_loss:0.249, val_acc:0.928]
Epoch [9/120    avg_loss:0.195, val_acc:0.869]
Epoch [10/120    avg_loss:0.161, val_acc:0.935]
Epoch [11/120    avg_loss:0.122, val_acc:0.957]
Epoch [12/120    avg_loss:0.105, val_acc:0.929]
Epoch [13/120    avg_loss:0.104, val_acc:0.955]
Epoch [14/120    avg_loss:0.181, val_acc:0.954]
Epoch [15/120    avg_loss:0.116, val_acc:0.941]
Epoch [16/120    avg_loss:0.093, val_acc:0.961]
Epoch [17/120    avg_loss:0.050, val_acc:0.965]
Epoch [18/120    avg_loss:0.047, val_acc:0.969]
Epoch [19/120    avg_loss:0.149, val_acc:0.927]
Epoch [20/120    avg_loss:0.177, val_acc:0.947]
Epoch [21/120    avg_loss:0.086, val_acc:0.962]
Epoch [22/120    avg_loss:0.054, val_acc:0.970]
Epoch [23/120    avg_loss:0.068, val_acc:0.972]
Epoch [24/120    avg_loss:0.045, val_acc:0.969]
Epoch [25/120    avg_loss:0.049, val_acc:0.969]
Epoch [26/120    avg_loss:0.060, val_acc:0.958]
Epoch [27/120    avg_loss:0.052, val_acc:0.954]
Epoch [28/120    avg_loss:0.044, val_acc:0.974]
Epoch [29/120    avg_loss:0.027, val_acc:0.977]
Epoch [30/120    avg_loss:0.027, val_acc:0.975]
Epoch [31/120    avg_loss:0.017, val_acc:0.981]
Epoch [32/120    avg_loss:0.036, val_acc:0.976]
Epoch [33/120    avg_loss:0.030, val_acc:0.978]
Epoch [34/120    avg_loss:0.019, val_acc:0.975]
Epoch [35/120    avg_loss:0.017, val_acc:0.974]
Epoch [36/120    avg_loss:0.013, val_acc:0.977]
Epoch [37/120    avg_loss:0.014, val_acc:0.978]
Epoch [38/120    avg_loss:0.031, val_acc:0.981]
Epoch [39/120    avg_loss:0.017, val_acc:0.980]
Epoch [40/120    avg_loss:0.025, val_acc:0.969]
Epoch [41/120    avg_loss:0.015, val_acc:0.983]
Epoch [42/120    avg_loss:0.011, val_acc:0.982]
Epoch [43/120    avg_loss:0.015, val_acc:0.977]
Epoch [44/120    avg_loss:0.018, val_acc:0.986]
Epoch [45/120    avg_loss:0.011, val_acc:0.978]
Epoch [46/120    avg_loss:0.014, val_acc:0.975]
Epoch [47/120    avg_loss:0.018, val_acc:0.981]
Epoch [48/120    avg_loss:0.014, val_acc:0.984]
Epoch [49/120    avg_loss:0.016, val_acc:0.980]
Epoch [50/120    avg_loss:0.018, val_acc:0.983]
Epoch [51/120    avg_loss:0.010, val_acc:0.986]
Epoch [52/120    avg_loss:0.011, val_acc:0.976]
Epoch [53/120    avg_loss:0.009, val_acc:0.986]
Epoch [54/120    avg_loss:0.011, val_acc:0.984]
Epoch [55/120    avg_loss:0.007, val_acc:0.987]
Epoch [56/120    avg_loss:0.009, val_acc:0.984]
Epoch [57/120    avg_loss:0.008, val_acc:0.985]
Epoch [58/120    avg_loss:0.006, val_acc:0.983]
Epoch [59/120    avg_loss:0.008, val_acc:0.986]
Epoch [60/120    avg_loss:0.007, val_acc:0.984]
Epoch [61/120    avg_loss:0.007, val_acc:0.990]
Epoch [62/120    avg_loss:0.009, val_acc:0.986]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.033, val_acc:0.974]
Epoch [65/120    avg_loss:0.021, val_acc:0.982]
Epoch [66/120    avg_loss:0.018, val_acc:0.987]
Epoch [67/120    avg_loss:0.015, val_acc:0.982]
Epoch [68/120    avg_loss:0.008, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.007, val_acc:0.985]
Epoch [71/120    avg_loss:0.006, val_acc:0.980]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.983]
Epoch [74/120    avg_loss:0.015, val_acc:0.977]
Epoch [75/120    avg_loss:0.012, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.987]
Epoch [77/120    avg_loss:0.006, val_acc:0.987]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.986]
Epoch [80/120    avg_loss:0.004, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.005, val_acc:0.986]
Epoch [84/120    avg_loss:0.004, val_acc:0.986]
Epoch [85/120    avg_loss:0.005, val_acc:0.986]
Epoch [86/120    avg_loss:0.004, val_acc:0.986]
Epoch [87/120    avg_loss:0.003, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.003, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.003, val_acc:0.987]
Epoch [102/120    avg_loss:0.003, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.003, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.002, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     0     0     0     0     7     0     0]
 [    0     4 18076     0     9     0     1     0     0     0]
 [    0     0     0  2021     1     0     0     0    10     4]
 [    0    42    19     0  2884     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4874     0     0     1]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     4     0    10    25     0     0     0  3532     0]
 [    0     0     0    14    14    72     0     0     0   819]]

Accuracy:
99.34928783168246

F1 scores:
[       nan 0.99558379 0.9990052  0.99044352 0.97679932 0.97315436
 0.99917999 0.99613003 0.98935574 0.93975904]

Kappa:
0.991375490977573
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f05510857b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.507, val_acc:0.663]
Epoch [2/120    avg_loss:0.821, val_acc:0.751]
Epoch [3/120    avg_loss:0.580, val_acc:0.605]
Epoch [4/120    avg_loss:0.519, val_acc:0.770]
Epoch [5/120    avg_loss:0.351, val_acc:0.855]
Epoch [6/120    avg_loss:0.335, val_acc:0.848]
Epoch [7/120    avg_loss:0.287, val_acc:0.887]
Epoch [8/120    avg_loss:0.195, val_acc:0.893]
Epoch [9/120    avg_loss:0.200, val_acc:0.917]
Epoch [10/120    avg_loss:0.140, val_acc:0.914]
Epoch [11/120    avg_loss:0.129, val_acc:0.910]
Epoch [12/120    avg_loss:0.120, val_acc:0.935]
Epoch [13/120    avg_loss:0.147, val_acc:0.946]
Epoch [14/120    avg_loss:0.145, val_acc:0.958]
Epoch [15/120    avg_loss:0.102, val_acc:0.926]
Epoch [16/120    avg_loss:0.146, val_acc:0.896]
Epoch [17/120    avg_loss:0.092, val_acc:0.961]
Epoch [18/120    avg_loss:0.080, val_acc:0.917]
Epoch [19/120    avg_loss:0.085, val_acc:0.964]
Epoch [20/120    avg_loss:0.049, val_acc:0.974]
Epoch [21/120    avg_loss:0.053, val_acc:0.942]
Epoch [22/120    avg_loss:0.054, val_acc:0.949]
Epoch [23/120    avg_loss:0.068, val_acc:0.941]
Epoch [24/120    avg_loss:0.083, val_acc:0.958]
Epoch [25/120    avg_loss:0.053, val_acc:0.964]
Epoch [26/120    avg_loss:0.055, val_acc:0.974]
Epoch [27/120    avg_loss:0.060, val_acc:0.946]
Epoch [28/120    avg_loss:0.044, val_acc:0.966]
Epoch [29/120    avg_loss:0.035, val_acc:0.970]
Epoch [30/120    avg_loss:0.032, val_acc:0.965]
Epoch [31/120    avg_loss:0.043, val_acc:0.959]
Epoch [32/120    avg_loss:0.024, val_acc:0.976]
Epoch [33/120    avg_loss:0.038, val_acc:0.966]
Epoch [34/120    avg_loss:0.033, val_acc:0.971]
Epoch [35/120    avg_loss:0.023, val_acc:0.975]
Epoch [36/120    avg_loss:0.020, val_acc:0.979]
Epoch [37/120    avg_loss:0.021, val_acc:0.975]
Epoch [38/120    avg_loss:0.017, val_acc:0.980]
Epoch [39/120    avg_loss:0.017, val_acc:0.973]
Epoch [40/120    avg_loss:0.020, val_acc:0.977]
Epoch [41/120    avg_loss:0.018, val_acc:0.985]
Epoch [42/120    avg_loss:0.019, val_acc:0.978]
Epoch [43/120    avg_loss:0.010, val_acc:0.980]
Epoch [44/120    avg_loss:0.013, val_acc:0.980]
Epoch [45/120    avg_loss:0.032, val_acc:0.968]
Epoch [46/120    avg_loss:0.016, val_acc:0.985]
Epoch [47/120    avg_loss:0.018, val_acc:0.980]
Epoch [48/120    avg_loss:0.011, val_acc:0.989]
Epoch [49/120    avg_loss:0.010, val_acc:0.988]
Epoch [50/120    avg_loss:0.020, val_acc:0.975]
Epoch [51/120    avg_loss:1.211, val_acc:0.716]
Epoch [52/120    avg_loss:0.743, val_acc:0.777]
Epoch [53/120    avg_loss:0.577, val_acc:0.736]
Epoch [54/120    avg_loss:0.456, val_acc:0.826]
Epoch [55/120    avg_loss:0.346, val_acc:0.867]
Epoch [56/120    avg_loss:0.257, val_acc:0.890]
Epoch [57/120    avg_loss:0.223, val_acc:0.888]
Epoch [58/120    avg_loss:0.209, val_acc:0.925]
Epoch [59/120    avg_loss:0.198, val_acc:0.928]
Epoch [60/120    avg_loss:0.188, val_acc:0.900]
Epoch [61/120    avg_loss:0.171, val_acc:0.920]
Epoch [62/120    avg_loss:0.170, val_acc:0.945]
Epoch [63/120    avg_loss:0.097, val_acc:0.949]
Epoch [64/120    avg_loss:0.102, val_acc:0.952]
Epoch [65/120    avg_loss:0.108, val_acc:0.952]
Epoch [66/120    avg_loss:0.097, val_acc:0.958]
Epoch [67/120    avg_loss:0.086, val_acc:0.957]
Epoch [68/120    avg_loss:0.095, val_acc:0.962]
Epoch [69/120    avg_loss:0.088, val_acc:0.961]
Epoch [70/120    avg_loss:0.086, val_acc:0.959]
Epoch [71/120    avg_loss:0.090, val_acc:0.959]
Epoch [72/120    avg_loss:0.083, val_acc:0.964]
Epoch [73/120    avg_loss:0.082, val_acc:0.964]
Epoch [74/120    avg_loss:0.072, val_acc:0.961]
Epoch [75/120    avg_loss:0.078, val_acc:0.961]
Epoch [76/120    avg_loss:0.081, val_acc:0.961]
Epoch [77/120    avg_loss:0.070, val_acc:0.962]
Epoch [78/120    avg_loss:0.085, val_acc:0.963]
Epoch [79/120    avg_loss:0.083, val_acc:0.964]
Epoch [80/120    avg_loss:0.070, val_acc:0.965]
Epoch [81/120    avg_loss:0.074, val_acc:0.964]
Epoch [82/120    avg_loss:0.069, val_acc:0.964]
Epoch [83/120    avg_loss:0.074, val_acc:0.964]
Epoch [84/120    avg_loss:0.067, val_acc:0.963]
Epoch [85/120    avg_loss:0.064, val_acc:0.964]
Epoch [86/120    avg_loss:0.072, val_acc:0.963]
Epoch [87/120    avg_loss:0.068, val_acc:0.964]
Epoch [88/120    avg_loss:0.078, val_acc:0.964]
Epoch [89/120    avg_loss:0.068, val_acc:0.964]
Epoch [90/120    avg_loss:0.079, val_acc:0.964]
Epoch [91/120    avg_loss:0.076, val_acc:0.964]
Epoch [92/120    avg_loss:0.065, val_acc:0.964]
Epoch [93/120    avg_loss:0.066, val_acc:0.964]
Epoch [94/120    avg_loss:0.056, val_acc:0.964]
Epoch [95/120    avg_loss:0.065, val_acc:0.964]
Epoch [96/120    avg_loss:0.073, val_acc:0.964]
Epoch [97/120    avg_loss:0.061, val_acc:0.964]
Epoch [98/120    avg_loss:0.076, val_acc:0.964]
Epoch [99/120    avg_loss:0.072, val_acc:0.964]
Epoch [100/120    avg_loss:0.071, val_acc:0.964]
Epoch [101/120    avg_loss:0.084, val_acc:0.964]
Epoch [102/120    avg_loss:0.082, val_acc:0.964]
Epoch [103/120    avg_loss:0.073, val_acc:0.964]
Epoch [104/120    avg_loss:0.098, val_acc:0.964]
Epoch [105/120    avg_loss:0.065, val_acc:0.964]
Epoch [106/120    avg_loss:0.075, val_acc:0.964]
Epoch [107/120    avg_loss:0.090, val_acc:0.964]
Epoch [108/120    avg_loss:0.075, val_acc:0.964]
Epoch [109/120    avg_loss:0.068, val_acc:0.964]
Epoch [110/120    avg_loss:0.066, val_acc:0.964]
Epoch [111/120    avg_loss:0.064, val_acc:0.964]
Epoch [112/120    avg_loss:0.071, val_acc:0.964]
Epoch [113/120    avg_loss:0.074, val_acc:0.964]
Epoch [114/120    avg_loss:0.061, val_acc:0.964]
Epoch [115/120    avg_loss:0.082, val_acc:0.964]
Epoch [116/120    avg_loss:0.072, val_acc:0.964]
Epoch [117/120    avg_loss:0.076, val_acc:0.964]
Epoch [118/120    avg_loss:0.060, val_acc:0.964]
Epoch [119/120    avg_loss:0.063, val_acc:0.964]
Epoch [120/120    avg_loss:0.062, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6152     0    74    18     0     0     0   188     0]
 [    0     4 17788     0    50     0   248     0     0     0]
 [    0    15     0  1931     0     0     0     0    81     9]
 [    0    60    31     0  2846     0     6     0    27     2]
 [    0     0     0     0     0  1299     0     0     0     6]
 [    0     0     0     0     0     0  4845     0     0    33]
 [    0     8     0     0     0     0     1  1281     0     0]
 [    0     0     0    11    68     0     0     0  3487     5]
 [    0     8     0    13    24    29     0     0     2   843]]

Accuracy:
97.53934398573253

F1 scores:
[       nan 0.97042353 0.99072656 0.9500615  0.95215791 0.98670718
 0.9711365  0.99649942 0.9480696  0.92790314]

Kappa:
0.9674988624926811
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe236aa7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.459, val_acc:0.548]
Epoch [2/120    avg_loss:0.854, val_acc:0.592]
Epoch [3/120    avg_loss:0.626, val_acc:0.741]
Epoch [4/120    avg_loss:0.404, val_acc:0.787]
Epoch [5/120    avg_loss:0.353, val_acc:0.854]
Epoch [6/120    avg_loss:0.295, val_acc:0.893]
Epoch [7/120    avg_loss:0.238, val_acc:0.885]
Epoch [8/120    avg_loss:0.237, val_acc:0.892]
Epoch [9/120    avg_loss:0.217, val_acc:0.914]
Epoch [10/120    avg_loss:0.182, val_acc:0.930]
Epoch [11/120    avg_loss:0.164, val_acc:0.932]
Epoch [12/120    avg_loss:0.225, val_acc:0.937]
Epoch [13/120    avg_loss:0.136, val_acc:0.880]
Epoch [14/120    avg_loss:0.107, val_acc:0.952]
Epoch [15/120    avg_loss:0.118, val_acc:0.928]
Epoch [16/120    avg_loss:0.073, val_acc:0.954]
Epoch [17/120    avg_loss:0.094, val_acc:0.945]
Epoch [18/120    avg_loss:0.068, val_acc:0.936]
Epoch [19/120    avg_loss:0.066, val_acc:0.966]
Epoch [20/120    avg_loss:0.055, val_acc:0.954]
Epoch [21/120    avg_loss:0.069, val_acc:0.963]
Epoch [22/120    avg_loss:0.050, val_acc:0.969]
Epoch [23/120    avg_loss:0.037, val_acc:0.968]
Epoch [24/120    avg_loss:0.041, val_acc:0.963]
Epoch [25/120    avg_loss:0.049, val_acc:0.969]
Epoch [26/120    avg_loss:0.042, val_acc:0.965]
Epoch [27/120    avg_loss:0.035, val_acc:0.971]
Epoch [28/120    avg_loss:0.031, val_acc:0.970]
Epoch [29/120    avg_loss:0.030, val_acc:0.974]
Epoch [30/120    avg_loss:0.035, val_acc:0.972]
Epoch [31/120    avg_loss:0.027, val_acc:0.974]
Epoch [32/120    avg_loss:0.017, val_acc:0.978]
Epoch [33/120    avg_loss:0.015, val_acc:0.975]
Epoch [34/120    avg_loss:0.028, val_acc:0.971]
Epoch [35/120    avg_loss:0.029, val_acc:0.979]
Epoch [36/120    avg_loss:0.056, val_acc:0.924]
Epoch [37/120    avg_loss:0.141, val_acc:0.947]
Epoch [38/120    avg_loss:0.091, val_acc:0.969]
Epoch [39/120    avg_loss:0.097, val_acc:0.965]
Epoch [40/120    avg_loss:0.063, val_acc:0.975]
Epoch [41/120    avg_loss:0.035, val_acc:0.963]
Epoch [42/120    avg_loss:0.025, val_acc:0.968]
Epoch [43/120    avg_loss:0.028, val_acc:0.977]
Epoch [44/120    avg_loss:0.016, val_acc:0.980]
Epoch [45/120    avg_loss:0.015, val_acc:0.985]
Epoch [46/120    avg_loss:0.017, val_acc:0.982]
Epoch [47/120    avg_loss:0.009, val_acc:0.983]
Epoch [48/120    avg_loss:0.013, val_acc:0.973]
Epoch [49/120    avg_loss:0.013, val_acc:0.974]
Epoch [50/120    avg_loss:0.024, val_acc:0.980]
Epoch [51/120    avg_loss:0.015, val_acc:0.986]
Epoch [52/120    avg_loss:0.024, val_acc:0.928]
Epoch [53/120    avg_loss:0.014, val_acc:0.981]
Epoch [54/120    avg_loss:0.010, val_acc:0.986]
Epoch [55/120    avg_loss:0.027, val_acc:0.980]
Epoch [56/120    avg_loss:0.013, val_acc:0.986]
Epoch [57/120    avg_loss:0.010, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.986]
Epoch [59/120    avg_loss:0.005, val_acc:0.988]
Epoch [60/120    avg_loss:0.009, val_acc:0.986]
Epoch [61/120    avg_loss:0.012, val_acc:0.980]
Epoch [62/120    avg_loss:0.007, val_acc:0.990]
Epoch [63/120    avg_loss:0.005, val_acc:0.990]
Epoch [64/120    avg_loss:0.003, val_acc:0.991]
Epoch [65/120    avg_loss:0.006, val_acc:0.988]
Epoch [66/120    avg_loss:0.006, val_acc:0.990]
Epoch [67/120    avg_loss:0.007, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.988]
Epoch [69/120    avg_loss:0.004, val_acc:0.991]
Epoch [70/120    avg_loss:0.004, val_acc:0.991]
Epoch [71/120    avg_loss:0.006, val_acc:0.989]
Epoch [72/120    avg_loss:0.005, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.988]
Epoch [74/120    avg_loss:0.003, val_acc:0.991]
Epoch [75/120    avg_loss:0.004, val_acc:0.989]
Epoch [76/120    avg_loss:0.013, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.987]
Epoch [78/120    avg_loss:0.005, val_acc:0.991]
Epoch [79/120    avg_loss:0.003, val_acc:0.990]
Epoch [80/120    avg_loss:0.004, val_acc:0.990]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.991]
Epoch [83/120    avg_loss:0.004, val_acc:0.990]
Epoch [84/120    avg_loss:0.007, val_acc:0.989]
Epoch [85/120    avg_loss:0.006, val_acc:0.993]
Epoch [86/120    avg_loss:0.004, val_acc:0.989]
Epoch [87/120    avg_loss:0.008, val_acc:0.993]
Epoch [88/120    avg_loss:0.002, val_acc:0.991]
Epoch [89/120    avg_loss:0.008, val_acc:0.973]
Epoch [90/120    avg_loss:0.011, val_acc:0.989]
Epoch [91/120    avg_loss:0.020, val_acc:0.981]
Epoch [92/120    avg_loss:0.012, val_acc:0.979]
Epoch [93/120    avg_loss:0.016, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.967]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.003, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.991]
Epoch [104/120    avg_loss:0.003, val_acc:0.992]
Epoch [105/120    avg_loss:0.002, val_acc:0.992]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.003, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.002, val_acc:0.991]
Epoch [114/120    avg_loss:0.003, val_acc:0.991]
Epoch [115/120    avg_loss:0.002, val_acc:0.991]
Epoch [116/120    avg_loss:0.003, val_acc:0.991]
Epoch [117/120    avg_loss:0.002, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.002, val_acc:0.991]
Epoch [120/120    avg_loss:0.002, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     6     0     0    17     0     0]
 [    0     0 18080     0     8     0     2     0     0     0]
 [    0     0     0  2029     2     0     0     0     1     4]
 [    0    43    20     0  2872     0     0     0    23    14]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4850     0     0    28]
 [    0     0     0     0     0     0     4  1284     0     2]
 [    0     5     0     0    26     0     0     0  3536     4]
 [    0     0     0     4    17    19     0     0     0   879]]

Accuracy:
99.39989877810716

F1 scores:
[       nan 0.99449143 0.99917104 0.99729663 0.97306454 0.99277292
 0.99650709 0.99112312 0.99172627 0.95027027]

Kappa:
0.9920475792730713
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2768aa7860>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.445, val_acc:0.459]
Epoch [2/120    avg_loss:0.909, val_acc:0.703]
Epoch [3/120    avg_loss:0.597, val_acc:0.688]
Epoch [4/120    avg_loss:0.459, val_acc:0.791]
Epoch [5/120    avg_loss:0.368, val_acc:0.843]
Epoch [6/120    avg_loss:0.287, val_acc:0.904]
Epoch [7/120    avg_loss:0.289, val_acc:0.896]
Epoch [8/120    avg_loss:0.194, val_acc:0.886]
Epoch [9/120    avg_loss:0.207, val_acc:0.914]
Epoch [10/120    avg_loss:0.143, val_acc:0.915]
Epoch [11/120    avg_loss:0.131, val_acc:0.923]
Epoch [12/120    avg_loss:0.104, val_acc:0.938]
Epoch [13/120    avg_loss:0.196, val_acc:0.887]
Epoch [14/120    avg_loss:0.279, val_acc:0.918]
Epoch [15/120    avg_loss:0.130, val_acc:0.943]
Epoch [16/120    avg_loss:0.123, val_acc:0.943]
Epoch [17/120    avg_loss:0.076, val_acc:0.950]
Epoch [18/120    avg_loss:0.072, val_acc:0.948]
Epoch [19/120    avg_loss:0.061, val_acc:0.947]
Epoch [20/120    avg_loss:0.110, val_acc:0.937]
Epoch [21/120    avg_loss:0.084, val_acc:0.957]
Epoch [22/120    avg_loss:0.094, val_acc:0.955]
Epoch [23/120    avg_loss:0.084, val_acc:0.933]
Epoch [24/120    avg_loss:0.070, val_acc:0.963]
Epoch [25/120    avg_loss:0.051, val_acc:0.964]
Epoch [26/120    avg_loss:0.041, val_acc:0.964]
Epoch [27/120    avg_loss:0.055, val_acc:0.969]
Epoch [28/120    avg_loss:0.049, val_acc:0.949]
Epoch [29/120    avg_loss:0.033, val_acc:0.958]
Epoch [30/120    avg_loss:0.042, val_acc:0.972]
Epoch [31/120    avg_loss:0.029, val_acc:0.972]
Epoch [32/120    avg_loss:0.024, val_acc:0.977]
Epoch [33/120    avg_loss:0.030, val_acc:0.978]
Epoch [34/120    avg_loss:0.026, val_acc:0.976]
Epoch [35/120    avg_loss:0.021, val_acc:0.963]
Epoch [36/120    avg_loss:0.037, val_acc:0.977]
Epoch [37/120    avg_loss:0.024, val_acc:0.975]
Epoch [38/120    avg_loss:0.016, val_acc:0.973]
Epoch [39/120    avg_loss:0.017, val_acc:0.978]
Epoch [40/120    avg_loss:0.045, val_acc:0.956]
Epoch [41/120    avg_loss:0.038, val_acc:0.970]
Epoch [42/120    avg_loss:0.013, val_acc:0.977]
Epoch [43/120    avg_loss:0.020, val_acc:0.969]
Epoch [44/120    avg_loss:0.013, val_acc:0.973]
Epoch [45/120    avg_loss:0.016, val_acc:0.971]
Epoch [46/120    avg_loss:0.022, val_acc:0.977]
Epoch [47/120    avg_loss:0.018, val_acc:0.981]
Epoch [48/120    avg_loss:0.015, val_acc:0.971]
Epoch [49/120    avg_loss:0.025, val_acc:0.970]
Epoch [50/120    avg_loss:0.011, val_acc:0.980]
Epoch [51/120    avg_loss:0.011, val_acc:0.981]
Epoch [52/120    avg_loss:0.010, val_acc:0.979]
Epoch [53/120    avg_loss:0.009, val_acc:0.982]
Epoch [54/120    avg_loss:0.009, val_acc:0.957]
Epoch [55/120    avg_loss:0.023, val_acc:0.977]
Epoch [56/120    avg_loss:0.010, val_acc:0.983]
Epoch [57/120    avg_loss:0.013, val_acc:0.977]
Epoch [58/120    avg_loss:0.007, val_acc:0.985]
Epoch [59/120    avg_loss:0.006, val_acc:0.985]
Epoch [60/120    avg_loss:0.015, val_acc:0.981]
Epoch [61/120    avg_loss:0.009, val_acc:0.968]
Epoch [62/120    avg_loss:0.035, val_acc:0.975]
Epoch [63/120    avg_loss:0.028, val_acc:0.984]
Epoch [64/120    avg_loss:0.027, val_acc:0.977]
Epoch [65/120    avg_loss:0.017, val_acc:0.976]
Epoch [66/120    avg_loss:0.014, val_acc:0.977]
Epoch [67/120    avg_loss:0.013, val_acc:0.980]
Epoch [68/120    avg_loss:0.006, val_acc:0.985]
Epoch [69/120    avg_loss:0.007, val_acc:0.984]
Epoch [70/120    avg_loss:0.005, val_acc:0.987]
Epoch [71/120    avg_loss:0.005, val_acc:0.985]
Epoch [72/120    avg_loss:0.005, val_acc:0.987]
Epoch [73/120    avg_loss:0.005, val_acc:0.984]
Epoch [74/120    avg_loss:0.006, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.005, val_acc:0.984]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.003, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.012, val_acc:0.975]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.006, val_acc:0.981]
Epoch [86/120    avg_loss:0.009, val_acc:0.981]
Epoch [87/120    avg_loss:0.006, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.978]
Epoch [89/120    avg_loss:0.018, val_acc:0.966]
Epoch [90/120    avg_loss:0.007, val_acc:0.977]
Epoch [91/120    avg_loss:0.022, val_acc:0.980]
Epoch [92/120    avg_loss:0.008, val_acc:0.980]
Epoch [93/120    avg_loss:0.006, val_acc:0.979]
Epoch [94/120    avg_loss:0.006, val_acc:0.981]
Epoch [95/120    avg_loss:0.004, val_acc:0.983]
Epoch [96/120    avg_loss:0.003, val_acc:0.983]
Epoch [97/120    avg_loss:0.004, val_acc:0.982]
Epoch [98/120    avg_loss:0.005, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.984]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.003, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.003, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.003, val_acc:0.984]
Epoch [109/120    avg_loss:0.003, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.984]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.003, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.002, val_acc:0.985]
Epoch [116/120    avg_loss:0.003, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.004, val_acc:0.984]
Epoch [120/120    avg_loss:0.003, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     2     0     0     0    10    12     0     0]
 [    0     1 18075     0     7     0     7     0     0     0]
 [    0     0     0  2027     0     0     0     0     9     0]
 [    0    40    20     0  2875     0     7     0    29     1]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     2     0     0     0  4867     0     0     9]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     2     0     0    20     0     0     0  3548     1]
 [    0     0     0     9    14    30     0     0     0   866]]

Accuracy:
99.42881931892127

F1 scores:
[       nan 0.99479935 0.99892232 0.99557957 0.9765625  0.9878696
 0.99641724 0.99420626 0.99147688 0.96168795]

Kappa:
0.992430056109279
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f31bcb3a7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.414, val_acc:0.446]
Epoch [2/120    avg_loss:0.802, val_acc:0.654]
Epoch [3/120    avg_loss:0.558, val_acc:0.759]
Epoch [4/120    avg_loss:0.446, val_acc:0.844]
Epoch [5/120    avg_loss:0.350, val_acc:0.817]
Epoch [6/120    avg_loss:0.268, val_acc:0.882]
Epoch [7/120    avg_loss:0.238, val_acc:0.896]
Epoch [8/120    avg_loss:0.241, val_acc:0.911]
Epoch [9/120    avg_loss:0.221, val_acc:0.913]
Epoch [10/120    avg_loss:0.235, val_acc:0.901]
Epoch [11/120    avg_loss:0.195, val_acc:0.884]
Epoch [12/120    avg_loss:0.129, val_acc:0.946]
Epoch [13/120    avg_loss:0.184, val_acc:0.916]
Epoch [14/120    avg_loss:0.128, val_acc:0.944]
Epoch [15/120    avg_loss:0.101, val_acc:0.951]
Epoch [16/120    avg_loss:0.105, val_acc:0.933]
Epoch [17/120    avg_loss:0.143, val_acc:0.956]
Epoch [18/120    avg_loss:0.067, val_acc:0.963]
Epoch [19/120    avg_loss:0.073, val_acc:0.960]
Epoch [20/120    avg_loss:0.056, val_acc:0.952]
Epoch [21/120    avg_loss:0.061, val_acc:0.966]
Epoch [22/120    avg_loss:0.042, val_acc:0.955]
Epoch [23/120    avg_loss:0.056, val_acc:0.968]
Epoch [24/120    avg_loss:0.038, val_acc:0.972]
Epoch [25/120    avg_loss:0.052, val_acc:0.969]
Epoch [26/120    avg_loss:0.057, val_acc:0.971]
Epoch [27/120    avg_loss:0.039, val_acc:0.973]
Epoch [28/120    avg_loss:0.033, val_acc:0.967]
Epoch [29/120    avg_loss:0.027, val_acc:0.969]
Epoch [30/120    avg_loss:0.026, val_acc:0.976]
Epoch [31/120    avg_loss:0.027, val_acc:0.974]
Epoch [32/120    avg_loss:0.018, val_acc:0.974]
Epoch [33/120    avg_loss:0.016, val_acc:0.980]
Epoch [34/120    avg_loss:0.019, val_acc:0.971]
Epoch [35/120    avg_loss:0.010, val_acc:0.977]
Epoch [36/120    avg_loss:0.014, val_acc:0.980]
Epoch [37/120    avg_loss:0.017, val_acc:0.980]
Epoch [38/120    avg_loss:0.023, val_acc:0.977]
Epoch [39/120    avg_loss:0.015, val_acc:0.981]
Epoch [40/120    avg_loss:0.019, val_acc:0.978]
Epoch [41/120    avg_loss:0.043, val_acc:0.962]
Epoch [42/120    avg_loss:0.027, val_acc:0.975]
Epoch [43/120    avg_loss:0.056, val_acc:0.960]
Epoch [44/120    avg_loss:0.028, val_acc:0.974]
Epoch [45/120    avg_loss:0.022, val_acc:0.972]
Epoch [46/120    avg_loss:0.026, val_acc:0.970]
Epoch [47/120    avg_loss:0.031, val_acc:0.967]
Epoch [48/120    avg_loss:0.031, val_acc:0.963]
Epoch [49/120    avg_loss:0.045, val_acc:0.957]
Epoch [50/120    avg_loss:0.058, val_acc:0.955]
Epoch [51/120    avg_loss:0.048, val_acc:0.972]
Epoch [52/120    avg_loss:0.035, val_acc:0.973]
Epoch [53/120    avg_loss:0.018, val_acc:0.979]
Epoch [54/120    avg_loss:0.011, val_acc:0.980]
Epoch [55/120    avg_loss:0.020, val_acc:0.982]
Epoch [56/120    avg_loss:0.018, val_acc:0.981]
Epoch [57/120    avg_loss:0.015, val_acc:0.980]
Epoch [58/120    avg_loss:0.013, val_acc:0.981]
Epoch [59/120    avg_loss:0.015, val_acc:0.981]
Epoch [60/120    avg_loss:0.015, val_acc:0.981]
Epoch [61/120    avg_loss:0.012, val_acc:0.981]
Epoch [62/120    avg_loss:0.013, val_acc:0.981]
Epoch [63/120    avg_loss:0.013, val_acc:0.982]
Epoch [64/120    avg_loss:0.008, val_acc:0.981]
Epoch [65/120    avg_loss:0.007, val_acc:0.982]
Epoch [66/120    avg_loss:0.009, val_acc:0.982]
Epoch [67/120    avg_loss:0.018, val_acc:0.981]
Epoch [68/120    avg_loss:0.013, val_acc:0.979]
Epoch [69/120    avg_loss:0.009, val_acc:0.980]
Epoch [70/120    avg_loss:0.010, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.982]
Epoch [72/120    avg_loss:0.012, val_acc:0.982]
Epoch [73/120    avg_loss:0.008, val_acc:0.982]
Epoch [74/120    avg_loss:0.009, val_acc:0.983]
Epoch [75/120    avg_loss:0.008, val_acc:0.982]
Epoch [76/120    avg_loss:0.014, val_acc:0.983]
Epoch [77/120    avg_loss:0.009, val_acc:0.982]
Epoch [78/120    avg_loss:0.008, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.980]
Epoch [80/120    avg_loss:0.014, val_acc:0.980]
Epoch [81/120    avg_loss:0.011, val_acc:0.980]
Epoch [82/120    avg_loss:0.007, val_acc:0.982]
Epoch [83/120    avg_loss:0.007, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.980]
Epoch [85/120    avg_loss:0.010, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.980]
Epoch [87/120    avg_loss:0.006, val_acc:0.981]
Epoch [88/120    avg_loss:0.010, val_acc:0.980]
Epoch [89/120    avg_loss:0.006, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.981]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.003, val_acc:0.981]
Epoch [95/120    avg_loss:0.007, val_acc:0.981]
Epoch [96/120    avg_loss:0.005, val_acc:0.981]
Epoch [97/120    avg_loss:0.012, val_acc:0.981]
Epoch [98/120    avg_loss:0.006, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.980]
Epoch [100/120    avg_loss:0.006, val_acc:0.980]
Epoch [101/120    avg_loss:0.009, val_acc:0.980]
Epoch [102/120    avg_loss:0.006, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.980]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.980]
Epoch [107/120    avg_loss:0.009, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.980]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.008, val_acc:0.980]
Epoch [116/120    avg_loss:0.009, val_acc:0.980]
Epoch [117/120    avg_loss:0.008, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     0     7     0     0     0     0     8]
 [    0     0 18042     0    37     0    11     0     0     0]
 [    0     0     0  2018     1     0     0     0    15     2]
 [    0    37    17     0  2886     0     7     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4863     0     0    15]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     3     0     0    31     0     0     0  3537     0]
 [    0     0     0     2    14    35     0     0     0   868]]

Accuracy:
99.35410792181814

F1 scores:
[       nan 0.9957328  0.99820189 0.99506903 0.97041022 0.98676749
 0.99661851 0.99961225 0.98964745 0.95752896]

Kappa:
0.9914439315677211
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa871df7780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.418, val_acc:0.624]
Epoch [2/120    avg_loss:0.817, val_acc:0.724]
Epoch [3/120    avg_loss:0.619, val_acc:0.770]
Epoch [4/120    avg_loss:0.465, val_acc:0.788]
Epoch [5/120    avg_loss:0.414, val_acc:0.885]
Epoch [6/120    avg_loss:0.309, val_acc:0.894]
Epoch [7/120    avg_loss:0.263, val_acc:0.909]
Epoch [8/120    avg_loss:0.209, val_acc:0.925]
Epoch [9/120    avg_loss:0.214, val_acc:0.929]
Epoch [10/120    avg_loss:0.137, val_acc:0.923]
Epoch [11/120    avg_loss:0.144, val_acc:0.905]
Epoch [12/120    avg_loss:0.161, val_acc:0.946]
Epoch [13/120    avg_loss:0.112, val_acc:0.931]
Epoch [14/120    avg_loss:0.137, val_acc:0.954]
Epoch [15/120    avg_loss:0.113, val_acc:0.957]
Epoch [16/120    avg_loss:0.085, val_acc:0.943]
Epoch [17/120    avg_loss:0.126, val_acc:0.958]
Epoch [18/120    avg_loss:0.170, val_acc:0.935]
Epoch [19/120    avg_loss:0.083, val_acc:0.949]
Epoch [20/120    avg_loss:0.168, val_acc:0.943]
Epoch [21/120    avg_loss:0.078, val_acc:0.968]
Epoch [22/120    avg_loss:0.068, val_acc:0.971]
Epoch [23/120    avg_loss:0.075, val_acc:0.966]
Epoch [24/120    avg_loss:0.077, val_acc:0.967]
Epoch [25/120    avg_loss:0.051, val_acc:0.973]
Epoch [26/120    avg_loss:0.064, val_acc:0.970]
Epoch [27/120    avg_loss:0.074, val_acc:0.940]
Epoch [28/120    avg_loss:0.054, val_acc:0.980]
Epoch [29/120    avg_loss:0.045, val_acc:0.976]
Epoch [30/120    avg_loss:0.038, val_acc:0.975]
Epoch [31/120    avg_loss:0.034, val_acc:0.972]
Epoch [32/120    avg_loss:0.031, val_acc:0.980]
Epoch [33/120    avg_loss:0.079, val_acc:0.977]
Epoch [34/120    avg_loss:0.071, val_acc:0.976]
Epoch [35/120    avg_loss:0.065, val_acc:0.964]
Epoch [36/120    avg_loss:0.026, val_acc:0.978]
Epoch [37/120    avg_loss:0.021, val_acc:0.980]
Epoch [38/120    avg_loss:0.019, val_acc:0.977]
Epoch [39/120    avg_loss:0.024, val_acc:0.974]
Epoch [40/120    avg_loss:0.028, val_acc:0.972]
Epoch [41/120    avg_loss:0.035, val_acc:0.973]
Epoch [42/120    avg_loss:0.036, val_acc:0.964]
Epoch [43/120    avg_loss:0.033, val_acc:0.978]
Epoch [44/120    avg_loss:0.037, val_acc:0.961]
Epoch [45/120    avg_loss:0.020, val_acc:0.982]
Epoch [46/120    avg_loss:0.018, val_acc:0.985]
Epoch [47/120    avg_loss:0.014, val_acc:0.984]
Epoch [48/120    avg_loss:0.021, val_acc:0.980]
Epoch [49/120    avg_loss:0.014, val_acc:0.983]
Epoch [50/120    avg_loss:0.024, val_acc:0.984]
Epoch [51/120    avg_loss:0.008, val_acc:0.985]
Epoch [52/120    avg_loss:0.011, val_acc:0.986]
Epoch [53/120    avg_loss:0.010, val_acc:0.993]
Epoch [54/120    avg_loss:0.017, val_acc:0.984]
Epoch [55/120    avg_loss:0.011, val_acc:0.989]
Epoch [56/120    avg_loss:0.008, val_acc:0.990]
Epoch [57/120    avg_loss:0.017, val_acc:0.978]
Epoch [58/120    avg_loss:0.009, val_acc:0.988]
Epoch [59/120    avg_loss:0.027, val_acc:0.986]
Epoch [60/120    avg_loss:0.019, val_acc:0.984]
Epoch [61/120    avg_loss:0.017, val_acc:0.984]
Epoch [62/120    avg_loss:0.013, val_acc:0.983]
Epoch [63/120    avg_loss:0.011, val_acc:0.990]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.019, val_acc:0.991]
Epoch [66/120    avg_loss:0.009, val_acc:0.990]
Epoch [67/120    avg_loss:0.010, val_acc:0.992]
Epoch [68/120    avg_loss:0.010, val_acc:0.992]
Epoch [69/120    avg_loss:0.009, val_acc:0.992]
Epoch [70/120    avg_loss:0.012, val_acc:0.991]
Epoch [71/120    avg_loss:0.008, val_acc:0.992]
Epoch [72/120    avg_loss:0.010, val_acc:0.991]
Epoch [73/120    avg_loss:0.006, val_acc:0.991]
Epoch [74/120    avg_loss:0.005, val_acc:0.992]
Epoch [75/120    avg_loss:0.006, val_acc:0.992]
Epoch [76/120    avg_loss:0.008, val_acc:0.992]
Epoch [77/120    avg_loss:0.003, val_acc:0.992]
Epoch [78/120    avg_loss:0.004, val_acc:0.992]
Epoch [79/120    avg_loss:0.004, val_acc:0.992]
Epoch [80/120    avg_loss:0.006, val_acc:0.992]
Epoch [81/120    avg_loss:0.004, val_acc:0.992]
Epoch [82/120    avg_loss:0.004, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.004, val_acc:0.992]
Epoch [85/120    avg_loss:0.009, val_acc:0.992]
Epoch [86/120    avg_loss:0.012, val_acc:0.992]
Epoch [87/120    avg_loss:0.007, val_acc:0.992]
Epoch [88/120    avg_loss:0.008, val_acc:0.992]
Epoch [89/120    avg_loss:0.005, val_acc:0.992]
Epoch [90/120    avg_loss:0.006, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.004, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.004, val_acc:0.992]
Epoch [98/120    avg_loss:0.004, val_acc:0.992]
Epoch [99/120    avg_loss:0.006, val_acc:0.992]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.006, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.004, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.992]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.004, val_acc:0.992]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     1     3     0    16     0     0     0]
 [    0     0 17947     0    51     0    92     0     0     0]
 [    0     0     0  2036     0     0     0     0     0     0]
 [    0    32    12     0  2897     0     1     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4857     0     0    21]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     4     0     0    28     0     0     0  3524    15]
 [    0     0     0    10    14    22     0     0     0   873]]

Accuracy:
99.14443400091582

F1 scores:
[       nan 0.99565217 0.9957003  0.9973059  0.97133277 0.99164134
 0.98649335 0.99883586 0.98919298 0.95514223]

Kappa:
0.988678771412886
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbcda33f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.508, val_acc:0.687]
Epoch [2/120    avg_loss:0.841, val_acc:0.599]
Epoch [3/120    avg_loss:0.570, val_acc:0.766]
Epoch [4/120    avg_loss:0.460, val_acc:0.699]
Epoch [5/120    avg_loss:0.423, val_acc:0.855]
Epoch [6/120    avg_loss:0.336, val_acc:0.840]
Epoch [7/120    avg_loss:0.318, val_acc:0.872]
Epoch [8/120    avg_loss:0.219, val_acc:0.859]
Epoch [9/120    avg_loss:0.212, val_acc:0.898]
Epoch [10/120    avg_loss:0.153, val_acc:0.903]
Epoch [11/120    avg_loss:0.152, val_acc:0.929]
Epoch [12/120    avg_loss:0.178, val_acc:0.924]
Epoch [13/120    avg_loss:0.158, val_acc:0.936]
Epoch [14/120    avg_loss:0.155, val_acc:0.909]
Epoch [15/120    avg_loss:0.172, val_acc:0.912]
Epoch [16/120    avg_loss:0.197, val_acc:0.911]
Epoch [17/120    avg_loss:0.136, val_acc:0.920]
Epoch [18/120    avg_loss:0.103, val_acc:0.961]
Epoch [19/120    avg_loss:0.076, val_acc:0.954]
Epoch [20/120    avg_loss:0.057, val_acc:0.958]
Epoch [21/120    avg_loss:0.046, val_acc:0.953]
Epoch [22/120    avg_loss:0.054, val_acc:0.963]
Epoch [23/120    avg_loss:0.087, val_acc:0.959]
Epoch [24/120    avg_loss:0.081, val_acc:0.957]
Epoch [25/120    avg_loss:0.101, val_acc:0.956]
Epoch [26/120    avg_loss:0.059, val_acc:0.955]
Epoch [27/120    avg_loss:0.052, val_acc:0.963]
Epoch [28/120    avg_loss:0.034, val_acc:0.964]
Epoch [29/120    avg_loss:0.046, val_acc:0.962]
Epoch [30/120    avg_loss:0.033, val_acc:0.964]
Epoch [31/120    avg_loss:0.077, val_acc:0.905]
Epoch [32/120    avg_loss:0.054, val_acc:0.963]
Epoch [33/120    avg_loss:0.028, val_acc:0.969]
Epoch [34/120    avg_loss:0.022, val_acc:0.970]
Epoch [35/120    avg_loss:0.055, val_acc:0.962]
Epoch [36/120    avg_loss:0.025, val_acc:0.962]
Epoch [37/120    avg_loss:0.018, val_acc:0.975]
Epoch [38/120    avg_loss:0.017, val_acc:0.977]
Epoch [39/120    avg_loss:0.019, val_acc:0.971]
Epoch [40/120    avg_loss:0.019, val_acc:0.970]
Epoch [41/120    avg_loss:0.015, val_acc:0.977]
Epoch [42/120    avg_loss:0.016, val_acc:0.977]
Epoch [43/120    avg_loss:0.023, val_acc:0.963]
Epoch [44/120    avg_loss:0.015, val_acc:0.981]
Epoch [45/120    avg_loss:0.013, val_acc:0.977]
Epoch [46/120    avg_loss:0.025, val_acc:0.964]
Epoch [47/120    avg_loss:0.020, val_acc:0.958]
Epoch [48/120    avg_loss:0.015, val_acc:0.977]
Epoch [49/120    avg_loss:0.009, val_acc:0.981]
Epoch [50/120    avg_loss:0.011, val_acc:0.974]
Epoch [51/120    avg_loss:0.010, val_acc:0.979]
Epoch [52/120    avg_loss:0.014, val_acc:0.981]
Epoch [53/120    avg_loss:0.019, val_acc:0.984]
Epoch [54/120    avg_loss:0.018, val_acc:0.970]
Epoch [55/120    avg_loss:0.009, val_acc:0.979]
Epoch [56/120    avg_loss:0.010, val_acc:0.979]
Epoch [57/120    avg_loss:0.016, val_acc:0.978]
Epoch [58/120    avg_loss:0.022, val_acc:0.974]
Epoch [59/120    avg_loss:0.017, val_acc:0.975]
Epoch [60/120    avg_loss:0.017, val_acc:0.981]
Epoch [61/120    avg_loss:0.019, val_acc:0.981]
Epoch [62/120    avg_loss:0.014, val_acc:0.979]
Epoch [63/120    avg_loss:0.011, val_acc:0.976]
Epoch [64/120    avg_loss:0.015, val_acc:0.976]
Epoch [65/120    avg_loss:0.009, val_acc:0.978]
Epoch [66/120    avg_loss:0.011, val_acc:0.980]
Epoch [67/120    avg_loss:0.005, val_acc:0.980]
Epoch [68/120    avg_loss:0.007, val_acc:0.981]
Epoch [69/120    avg_loss:0.007, val_acc:0.982]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.003, val_acc:0.984]
Epoch [72/120    avg_loss:0.005, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.984]
Epoch [74/120    avg_loss:0.008, val_acc:0.984]
Epoch [75/120    avg_loss:0.004, val_acc:0.984]
Epoch [76/120    avg_loss:0.004, val_acc:0.984]
Epoch [77/120    avg_loss:0.004, val_acc:0.984]
Epoch [78/120    avg_loss:0.004, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.984]
Epoch [80/120    avg_loss:0.003, val_acc:0.984]
Epoch [81/120    avg_loss:0.004, val_acc:0.983]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.984]
Epoch [84/120    avg_loss:0.005, val_acc:0.983]
Epoch [85/120    avg_loss:0.004, val_acc:0.984]
Epoch [86/120    avg_loss:0.004, val_acc:0.984]
Epoch [87/120    avg_loss:0.005, val_acc:0.984]
Epoch [88/120    avg_loss:0.004, val_acc:0.984]
Epoch [89/120    avg_loss:0.003, val_acc:0.984]
Epoch [90/120    avg_loss:0.004, val_acc:0.984]
Epoch [91/120    avg_loss:0.004, val_acc:0.984]
Epoch [92/120    avg_loss:0.003, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.005, val_acc:0.983]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.004, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.984]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.003, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.003, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0    11     0     4    15     0     0]
 [    0     0 17994     0    75     0    21     0     0     0]
 [    0     0     0  2032     1     0     0     0     0     3]
 [    0    28    11     0  2901     0     0     0    29     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4857     0     0    21]
 [    0     0     0     0     0     0     3  1286     0     1]
 [    0     0     0     3    36     0     0     0  3532     0]
 [    0     0     0     2    15    40     0     0     0   862]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.99549059 0.9970356  0.99779033 0.96523041 0.98490566
 0.99498105 0.99266692 0.99046551 0.95301271]

Kappa:
0.9897274403298656
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93ac2b9748>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.491, val_acc:0.491]
Epoch [2/120    avg_loss:0.864, val_acc:0.618]
Epoch [3/120    avg_loss:0.621, val_acc:0.786]
Epoch [4/120    avg_loss:0.473, val_acc:0.785]
Epoch [5/120    avg_loss:0.363, val_acc:0.808]
Epoch [6/120    avg_loss:0.280, val_acc:0.901]
Epoch [7/120    avg_loss:0.317, val_acc:0.883]
Epoch [8/120    avg_loss:0.217, val_acc:0.863]
Epoch [9/120    avg_loss:0.180, val_acc:0.886]
Epoch [10/120    avg_loss:0.160, val_acc:0.929]
Epoch [11/120    avg_loss:0.148, val_acc:0.929]
Epoch [12/120    avg_loss:0.121, val_acc:0.901]
Epoch [13/120    avg_loss:0.096, val_acc:0.949]
Epoch [14/120    avg_loss:0.084, val_acc:0.944]
Epoch [15/120    avg_loss:0.127, val_acc:0.946]
Epoch [16/120    avg_loss:0.130, val_acc:0.959]
Epoch [17/120    avg_loss:0.122, val_acc:0.958]
Epoch [18/120    avg_loss:0.088, val_acc:0.932]
Epoch [19/120    avg_loss:0.068, val_acc:0.967]
Epoch [20/120    avg_loss:0.052, val_acc:0.958]
Epoch [21/120    avg_loss:0.093, val_acc:0.936]
Epoch [22/120    avg_loss:0.050, val_acc:0.978]
Epoch [23/120    avg_loss:0.066, val_acc:0.940]
Epoch [24/120    avg_loss:0.086, val_acc:0.952]
Epoch [25/120    avg_loss:0.117, val_acc:0.965]
Epoch [26/120    avg_loss:0.079, val_acc:0.956]
Epoch [27/120    avg_loss:0.072, val_acc:0.926]
Epoch [28/120    avg_loss:0.053, val_acc:0.980]
Epoch [29/120    avg_loss:0.031, val_acc:0.978]
Epoch [30/120    avg_loss:0.038, val_acc:0.973]
Epoch [31/120    avg_loss:0.126, val_acc:0.935]
Epoch [32/120    avg_loss:0.111, val_acc:0.969]
Epoch [33/120    avg_loss:0.076, val_acc:0.974]
Epoch [34/120    avg_loss:0.040, val_acc:0.974]
Epoch [35/120    avg_loss:0.036, val_acc:0.965]
Epoch [36/120    avg_loss:0.037, val_acc:0.979]
Epoch [37/120    avg_loss:0.026, val_acc:0.976]
Epoch [38/120    avg_loss:0.033, val_acc:0.980]
Epoch [39/120    avg_loss:0.035, val_acc:0.941]
Epoch [40/120    avg_loss:0.031, val_acc:0.979]
Epoch [41/120    avg_loss:0.026, val_acc:0.984]
Epoch [42/120    avg_loss:0.014, val_acc:0.982]
Epoch [43/120    avg_loss:0.017, val_acc:0.982]
Epoch [44/120    avg_loss:0.020, val_acc:0.975]
Epoch [45/120    avg_loss:0.042, val_acc:0.984]
Epoch [46/120    avg_loss:0.027, val_acc:0.987]
Epoch [47/120    avg_loss:0.026, val_acc:0.968]
Epoch [48/120    avg_loss:0.024, val_acc:0.979]
Epoch [49/120    avg_loss:0.021, val_acc:0.953]
Epoch [50/120    avg_loss:0.017, val_acc:0.988]
Epoch [51/120    avg_loss:0.011, val_acc:0.991]
Epoch [52/120    avg_loss:0.031, val_acc:0.962]
Epoch [53/120    avg_loss:0.014, val_acc:0.990]
Epoch [54/120    avg_loss:0.014, val_acc:0.991]
Epoch [55/120    avg_loss:0.006, val_acc:0.991]
Epoch [56/120    avg_loss:0.013, val_acc:0.990]
Epoch [57/120    avg_loss:0.009, val_acc:0.991]
Epoch [58/120    avg_loss:0.008, val_acc:0.991]
Epoch [59/120    avg_loss:0.017, val_acc:0.986]
Epoch [60/120    avg_loss:0.010, val_acc:0.992]
Epoch [61/120    avg_loss:0.010, val_acc:0.991]
Epoch [62/120    avg_loss:0.007, val_acc:0.991]
Epoch [63/120    avg_loss:0.008, val_acc:0.990]
Epoch [64/120    avg_loss:0.009, val_acc:0.991]
Epoch [65/120    avg_loss:0.008, val_acc:0.989]
Epoch [66/120    avg_loss:0.007, val_acc:0.989]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.005, val_acc:0.989]
Epoch [69/120    avg_loss:0.005, val_acc:0.992]
Epoch [70/120    avg_loss:0.008, val_acc:0.991]
Epoch [71/120    avg_loss:0.007, val_acc:0.991]
Epoch [72/120    avg_loss:0.005, val_acc:0.991]
Epoch [73/120    avg_loss:0.004, val_acc:0.991]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.014, val_acc:0.989]
Epoch [77/120    avg_loss:0.044, val_acc:0.976]
Epoch [78/120    avg_loss:0.050, val_acc:0.890]
Epoch [79/120    avg_loss:0.171, val_acc:0.955]
Epoch [80/120    avg_loss:0.073, val_acc:0.961]
Epoch [81/120    avg_loss:0.048, val_acc:0.974]
Epoch [82/120    avg_loss:0.018, val_acc:0.984]
Epoch [83/120    avg_loss:0.050, val_acc:0.977]
Epoch [84/120    avg_loss:0.014, val_acc:0.986]
Epoch [85/120    avg_loss:0.016, val_acc:0.986]
Epoch [86/120    avg_loss:0.014, val_acc:0.987]
Epoch [87/120    avg_loss:0.011, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.012, val_acc:0.987]
Epoch [93/120    avg_loss:0.013, val_acc:0.986]
Epoch [94/120    avg_loss:0.016, val_acc:0.987]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.010, val_acc:0.987]
Epoch [106/120    avg_loss:0.012, val_acc:0.987]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.013, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.009, val_acc:0.987]
Epoch [119/120    avg_loss:0.009, val_acc:0.987]
Epoch [120/120    avg_loss:0.009, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     0     0     0     4     1     0     2]
 [    0     0 18011     0    50     0    29     0     0     0]
 [    0     0     0  2021     0     0     0     0     8     7]
 [    0    40    18     2  2875     0     7     0    28     2]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4863     0     0    15]
 [    0     1     0     0     0     0     2  1285     0     2]
 [    0     3     0     0    41     0     0     0  3523     4]
 [    0     0     0     5    14    24     0     0     0   876]]

Accuracy:
99.2504759839009

F1 scores:
[       nan 0.99604682 0.99731443 0.99458661 0.96606183 0.99012158
 0.99417357 0.99767081 0.98821879 0.95790049]

Kappa:
0.9900741132199612
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f82f78e07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.392, val_acc:0.644]
Epoch [2/120    avg_loss:0.783, val_acc:0.623]
Epoch [3/120    avg_loss:0.539, val_acc:0.792]
Epoch [4/120    avg_loss:0.494, val_acc:0.760]
Epoch [5/120    avg_loss:0.361, val_acc:0.858]
Epoch [6/120    avg_loss:0.384, val_acc:0.869]
Epoch [7/120    avg_loss:0.247, val_acc:0.907]
Epoch [8/120    avg_loss:0.209, val_acc:0.891]
Epoch [9/120    avg_loss:0.197, val_acc:0.917]
Epoch [10/120    avg_loss:0.212, val_acc:0.902]
Epoch [11/120    avg_loss:0.180, val_acc:0.939]
Epoch [12/120    avg_loss:0.127, val_acc:0.919]
Epoch [13/120    avg_loss:0.120, val_acc:0.928]
Epoch [14/120    avg_loss:0.133, val_acc:0.930]
Epoch [15/120    avg_loss:0.172, val_acc:0.943]
Epoch [16/120    avg_loss:0.138, val_acc:0.884]
Epoch [17/120    avg_loss:0.095, val_acc:0.967]
Epoch [18/120    avg_loss:0.168, val_acc:0.955]
Epoch [19/120    avg_loss:0.081, val_acc:0.963]
Epoch [20/120    avg_loss:0.078, val_acc:0.967]
Epoch [21/120    avg_loss:0.066, val_acc:0.947]
Epoch [22/120    avg_loss:0.160, val_acc:0.959]
Epoch [23/120    avg_loss:0.083, val_acc:0.968]
Epoch [24/120    avg_loss:0.065, val_acc:0.967]
Epoch [25/120    avg_loss:0.066, val_acc:0.974]
Epoch [26/120    avg_loss:0.082, val_acc:0.967]
Epoch [27/120    avg_loss:0.072, val_acc:0.957]
Epoch [28/120    avg_loss:0.050, val_acc:0.976]
Epoch [29/120    avg_loss:0.043, val_acc:0.979]
Epoch [30/120    avg_loss:0.030, val_acc:0.980]
Epoch [31/120    avg_loss:0.029, val_acc:0.977]
Epoch [32/120    avg_loss:0.028, val_acc:0.984]
Epoch [33/120    avg_loss:0.026, val_acc:0.980]
Epoch [34/120    avg_loss:0.021, val_acc:0.988]
Epoch [35/120    avg_loss:0.033, val_acc:0.981]
Epoch [36/120    avg_loss:0.049, val_acc:0.979]
Epoch [37/120    avg_loss:0.032, val_acc:0.981]
Epoch [38/120    avg_loss:0.025, val_acc:0.987]
Epoch [39/120    avg_loss:0.020, val_acc:0.985]
Epoch [40/120    avg_loss:0.014, val_acc:0.983]
Epoch [41/120    avg_loss:0.015, val_acc:0.984]
Epoch [42/120    avg_loss:0.024, val_acc:0.980]
Epoch [43/120    avg_loss:0.027, val_acc:0.989]
Epoch [44/120    avg_loss:0.022, val_acc:0.978]
Epoch [45/120    avg_loss:0.037, val_acc:0.981]
Epoch [46/120    avg_loss:0.025, val_acc:0.980]
Epoch [47/120    avg_loss:0.017, val_acc:0.985]
Epoch [48/120    avg_loss:0.017, val_acc:0.980]
Epoch [49/120    avg_loss:0.016, val_acc:0.983]
Epoch [50/120    avg_loss:0.011, val_acc:0.984]
Epoch [51/120    avg_loss:0.024, val_acc:0.984]
Epoch [52/120    avg_loss:0.013, val_acc:0.988]
Epoch [53/120    avg_loss:0.025, val_acc:0.988]
Epoch [54/120    avg_loss:0.030, val_acc:0.978]
Epoch [55/120    avg_loss:0.047, val_acc:0.986]
Epoch [56/120    avg_loss:0.021, val_acc:0.985]
Epoch [57/120    avg_loss:0.013, val_acc:0.988]
Epoch [58/120    avg_loss:0.015, val_acc:0.991]
Epoch [59/120    avg_loss:0.009, val_acc:0.990]
Epoch [60/120    avg_loss:0.008, val_acc:0.990]
Epoch [61/120    avg_loss:0.008, val_acc:0.990]
Epoch [62/120    avg_loss:0.008, val_acc:0.991]
Epoch [63/120    avg_loss:0.016, val_acc:0.990]
Epoch [64/120    avg_loss:0.007, val_acc:0.989]
Epoch [65/120    avg_loss:0.008, val_acc:0.989]
Epoch [66/120    avg_loss:0.008, val_acc:0.989]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.006, val_acc:0.989]
Epoch [69/120    avg_loss:0.011, val_acc:0.988]
Epoch [70/120    avg_loss:0.010, val_acc:0.989]
Epoch [71/120    avg_loss:0.014, val_acc:0.989]
Epoch [72/120    avg_loss:0.018, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.990]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.010, val_acc:0.991]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.007, val_acc:0.990]
Epoch [79/120    avg_loss:0.005, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.989]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.005, val_acc:0.991]
Epoch [83/120    avg_loss:0.013, val_acc:0.990]
Epoch [84/120    avg_loss:0.009, val_acc:0.989]
Epoch [85/120    avg_loss:0.008, val_acc:0.991]
Epoch [86/120    avg_loss:0.010, val_acc:0.994]
Epoch [87/120    avg_loss:0.007, val_acc:0.994]
Epoch [88/120    avg_loss:0.007, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.009, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.010, val_acc:0.992]
Epoch [93/120    avg_loss:0.007, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.006, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.008, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.992]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.007, val_acc:0.992]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.009, val_acc:0.992]
Epoch [107/120    avg_loss:0.007, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.992]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.010, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.015, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.010, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.007, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     0 18016     0     9     0    63     0     0     2]
 [    0     0     0  2024     0     0     0     0     7     5]
 [    0    50    21     2  2869     0     3     0    27     0]
 [    0     0     0     0     0  1301     0     0     0     4]
 [    0     0     0     0     0     0  4867     0     0    11]
 [    0     0     0     0     0     0     4  1283     0     3]
 [    0     4     0     1    21     0     0     0  3545     0]
 [    0     0     0     8    14    27     0     0     0   870]]

Accuracy:
99.31072711059697

F1 scores:
[       nan 0.99581979 0.99737039 0.99435028 0.97502124 0.98822636
 0.99174733 0.99727944 0.99160839 0.95920617]

Kappa:
0.9908700452227347
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f795b23d7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.391, val_acc:0.325]
Epoch [2/120    avg_loss:0.859, val_acc:0.565]
Epoch [3/120    avg_loss:0.589, val_acc:0.741]
Epoch [4/120    avg_loss:0.588, val_acc:0.762]
Epoch [5/120    avg_loss:0.365, val_acc:0.797]
Epoch [6/120    avg_loss:0.325, val_acc:0.835]
Epoch [7/120    avg_loss:0.258, val_acc:0.885]
Epoch [8/120    avg_loss:0.206, val_acc:0.923]
Epoch [9/120    avg_loss:0.286, val_acc:0.868]
Epoch [10/120    avg_loss:0.210, val_acc:0.942]
Epoch [11/120    avg_loss:0.143, val_acc:0.949]
Epoch [12/120    avg_loss:0.154, val_acc:0.948]
Epoch [13/120    avg_loss:0.127, val_acc:0.939]
Epoch [14/120    avg_loss:0.149, val_acc:0.913]
Epoch [15/120    avg_loss:0.131, val_acc:0.959]
Epoch [16/120    avg_loss:0.145, val_acc:0.941]
Epoch [17/120    avg_loss:0.079, val_acc:0.890]
Epoch [18/120    avg_loss:0.074, val_acc:0.969]
Epoch [19/120    avg_loss:0.089, val_acc:0.970]
Epoch [20/120    avg_loss:0.068, val_acc:0.967]
Epoch [21/120    avg_loss:0.059, val_acc:0.974]
Epoch [22/120    avg_loss:0.037, val_acc:0.980]
Epoch [23/120    avg_loss:0.032, val_acc:0.971]
Epoch [24/120    avg_loss:0.073, val_acc:0.937]
Epoch [25/120    avg_loss:0.082, val_acc:0.970]
Epoch [26/120    avg_loss:0.070, val_acc:0.971]
Epoch [27/120    avg_loss:0.067, val_acc:0.940]
Epoch [28/120    avg_loss:0.064, val_acc:0.965]
Epoch [29/120    avg_loss:0.059, val_acc:0.963]
Epoch [30/120    avg_loss:0.051, val_acc:0.971]
Epoch [31/120    avg_loss:0.034, val_acc:0.944]
Epoch [32/120    avg_loss:0.036, val_acc:0.982]
Epoch [33/120    avg_loss:0.060, val_acc:0.971]
Epoch [34/120    avg_loss:0.054, val_acc:0.980]
Epoch [35/120    avg_loss:0.033, val_acc:0.980]
Epoch [36/120    avg_loss:0.021, val_acc:0.988]
Epoch [37/120    avg_loss:0.021, val_acc:0.990]
Epoch [38/120    avg_loss:0.021, val_acc:0.979]
Epoch [39/120    avg_loss:0.021, val_acc:0.980]
Epoch [40/120    avg_loss:0.019, val_acc:0.985]
Epoch [41/120    avg_loss:0.030, val_acc:0.984]
Epoch [42/120    avg_loss:0.018, val_acc:0.988]
Epoch [43/120    avg_loss:0.013, val_acc:0.978]
Epoch [44/120    avg_loss:0.035, val_acc:0.969]
Epoch [45/120    avg_loss:0.020, val_acc:0.979]
Epoch [46/120    avg_loss:0.012, val_acc:0.987]
Epoch [47/120    avg_loss:0.024, val_acc:0.969]
Epoch [48/120    avg_loss:0.034, val_acc:0.983]
Epoch [49/120    avg_loss:0.020, val_acc:0.986]
Epoch [50/120    avg_loss:0.067, val_acc:0.973]
Epoch [51/120    avg_loss:0.038, val_acc:0.977]
Epoch [52/120    avg_loss:0.034, val_acc:0.977]
Epoch [53/120    avg_loss:0.029, val_acc:0.980]
Epoch [54/120    avg_loss:0.015, val_acc:0.983]
Epoch [55/120    avg_loss:0.021, val_acc:0.984]
Epoch [56/120    avg_loss:0.020, val_acc:0.984]
Epoch [57/120    avg_loss:0.018, val_acc:0.985]
Epoch [58/120    avg_loss:0.015, val_acc:0.986]
Epoch [59/120    avg_loss:0.016, val_acc:0.986]
Epoch [60/120    avg_loss:0.015, val_acc:0.987]
Epoch [61/120    avg_loss:0.013, val_acc:0.986]
Epoch [62/120    avg_loss:0.019, val_acc:0.982]
Epoch [63/120    avg_loss:0.011, val_acc:0.985]
Epoch [64/120    avg_loss:0.016, val_acc:0.985]
Epoch [65/120    avg_loss:0.010, val_acc:0.986]
Epoch [66/120    avg_loss:0.011, val_acc:0.985]
Epoch [67/120    avg_loss:0.015, val_acc:0.985]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.019, val_acc:0.985]
Epoch [70/120    avg_loss:0.013, val_acc:0.985]
Epoch [71/120    avg_loss:0.013, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.985]
Epoch [73/120    avg_loss:0.013, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.986]
Epoch [75/120    avg_loss:0.016, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.018, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.986]
Epoch [80/120    avg_loss:0.013, val_acc:0.986]
Epoch [81/120    avg_loss:0.018, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.015, val_acc:0.986]
Epoch [84/120    avg_loss:0.013, val_acc:0.986]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.013, val_acc:0.986]
Epoch [87/120    avg_loss:0.018, val_acc:0.986]
Epoch [88/120    avg_loss:0.014, val_acc:0.986]
Epoch [89/120    avg_loss:0.021, val_acc:0.986]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.012, val_acc:0.986]
Epoch [93/120    avg_loss:0.010, val_acc:0.986]
Epoch [94/120    avg_loss:0.023, val_acc:0.986]
Epoch [95/120    avg_loss:0.016, val_acc:0.986]
Epoch [96/120    avg_loss:0.015, val_acc:0.986]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.015, val_acc:0.986]
Epoch [99/120    avg_loss:0.017, val_acc:0.986]
Epoch [100/120    avg_loss:0.014, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.017, val_acc:0.986]
Epoch [104/120    avg_loss:0.016, val_acc:0.986]
Epoch [105/120    avg_loss:0.016, val_acc:0.986]
Epoch [106/120    avg_loss:0.016, val_acc:0.986]
Epoch [107/120    avg_loss:0.012, val_acc:0.986]
Epoch [108/120    avg_loss:0.028, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.018, val_acc:0.986]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.014, val_acc:0.986]
Epoch [117/120    avg_loss:0.012, val_acc:0.986]
Epoch [118/120    avg_loss:0.018, val_acc:0.986]
Epoch [119/120    avg_loss:0.017, val_acc:0.986]
Epoch [120/120    avg_loss:0.017, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     0     5     0     5     0     1     0]
 [    0     0 18023     0    47     0    20     0     0     0]
 [    0     0     0  2028     2     0     0     0     0     6]
 [    0    39    19     0  2876     0     5     0    33     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4869     0     1     8]
 [    0     0     0     0     0     0     6  1281     0     3]
 [    0     3     0     0    47     0     0     0  3520     1]
 [    0     0     0     0    17    55     0     0     0   847]]

Accuracy:
99.22155544308679

F1 scores:
[       nan 0.99588988 0.99761984 0.9980315  0.96413007 0.9793621
 0.99540018 0.99649942 0.98793152 0.94955157]

Kappa:
0.9896893826669659
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f710335f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.479, val_acc:0.516]
Epoch [2/120    avg_loss:0.889, val_acc:0.528]
Epoch [3/120    avg_loss:0.652, val_acc:0.766]
Epoch [4/120    avg_loss:0.427, val_acc:0.803]
Epoch [5/120    avg_loss:0.374, val_acc:0.876]
Epoch [6/120    avg_loss:0.337, val_acc:0.833]
Epoch [7/120    avg_loss:0.318, val_acc:0.865]
Epoch [8/120    avg_loss:0.326, val_acc:0.916]
Epoch [9/120    avg_loss:0.222, val_acc:0.909]
Epoch [10/120    avg_loss:0.202, val_acc:0.937]
Epoch [11/120    avg_loss:0.182, val_acc:0.929]
Epoch [12/120    avg_loss:0.150, val_acc:0.915]
Epoch [13/120    avg_loss:0.124, val_acc:0.954]
Epoch [14/120    avg_loss:0.104, val_acc:0.929]
Epoch [15/120    avg_loss:0.115, val_acc:0.921]
Epoch [16/120    avg_loss:0.136, val_acc:0.923]
Epoch [17/120    avg_loss:0.175, val_acc:0.918]
Epoch [18/120    avg_loss:0.131, val_acc:0.938]
Epoch [19/120    avg_loss:0.095, val_acc:0.940]
Epoch [20/120    avg_loss:0.102, val_acc:0.948]
Epoch [21/120    avg_loss:0.097, val_acc:0.955]
Epoch [22/120    avg_loss:0.079, val_acc:0.951]
Epoch [23/120    avg_loss:0.074, val_acc:0.948]
Epoch [24/120    avg_loss:0.073, val_acc:0.951]
Epoch [25/120    avg_loss:0.038, val_acc:0.964]
Epoch [26/120    avg_loss:0.060, val_acc:0.952]
Epoch [27/120    avg_loss:0.057, val_acc:0.945]
Epoch [28/120    avg_loss:0.091, val_acc:0.951]
Epoch [29/120    avg_loss:0.128, val_acc:0.953]
Epoch [30/120    avg_loss:0.067, val_acc:0.954]
Epoch [31/120    avg_loss:0.045, val_acc:0.961]
Epoch [32/120    avg_loss:0.020, val_acc:0.962]
Epoch [33/120    avg_loss:0.028, val_acc:0.972]
Epoch [34/120    avg_loss:0.021, val_acc:0.976]
Epoch [35/120    avg_loss:0.021, val_acc:0.976]
Epoch [36/120    avg_loss:0.020, val_acc:0.979]
Epoch [37/120    avg_loss:0.021, val_acc:0.979]
Epoch [38/120    avg_loss:0.029, val_acc:0.950]
Epoch [39/120    avg_loss:0.018, val_acc:0.975]
Epoch [40/120    avg_loss:0.016, val_acc:0.978]
Epoch [41/120    avg_loss:0.018, val_acc:0.978]
Epoch [42/120    avg_loss:0.024, val_acc:0.977]
Epoch [43/120    avg_loss:0.017, val_acc:0.975]
Epoch [44/120    avg_loss:0.019, val_acc:0.975]
Epoch [45/120    avg_loss:0.017, val_acc:0.979]
Epoch [46/120    avg_loss:0.017, val_acc:0.978]
Epoch [47/120    avg_loss:0.011, val_acc:0.969]
Epoch [48/120    avg_loss:0.012, val_acc:0.981]
Epoch [49/120    avg_loss:0.005, val_acc:0.981]
Epoch [50/120    avg_loss:0.010, val_acc:0.975]
Epoch [51/120    avg_loss:0.010, val_acc:0.983]
Epoch [52/120    avg_loss:0.007, val_acc:0.977]
Epoch [53/120    avg_loss:0.012, val_acc:0.977]
Epoch [54/120    avg_loss:0.020, val_acc:0.977]
Epoch [55/120    avg_loss:0.025, val_acc:0.974]
Epoch [56/120    avg_loss:0.021, val_acc:0.962]
Epoch [57/120    avg_loss:0.015, val_acc:0.977]
Epoch [58/120    avg_loss:0.015, val_acc:0.982]
Epoch [59/120    avg_loss:0.013, val_acc:0.971]
Epoch [60/120    avg_loss:0.013, val_acc:0.969]
Epoch [61/120    avg_loss:0.012, val_acc:0.975]
Epoch [62/120    avg_loss:0.015, val_acc:0.972]
Epoch [63/120    avg_loss:0.042, val_acc:0.978]
Epoch [64/120    avg_loss:0.028, val_acc:0.976]
Epoch [65/120    avg_loss:0.013, val_acc:0.979]
Epoch [66/120    avg_loss:0.016, val_acc:0.981]
Epoch [67/120    avg_loss:0.011, val_acc:0.981]
Epoch [68/120    avg_loss:0.009, val_acc:0.981]
Epoch [69/120    avg_loss:0.010, val_acc:0.979]
Epoch [70/120    avg_loss:0.008, val_acc:0.980]
Epoch [71/120    avg_loss:0.009, val_acc:0.980]
Epoch [72/120    avg_loss:0.008, val_acc:0.980]
Epoch [73/120    avg_loss:0.011, val_acc:0.980]
Epoch [74/120    avg_loss:0.006, val_acc:0.981]
Epoch [75/120    avg_loss:0.006, val_acc:0.981]
Epoch [76/120    avg_loss:0.008, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.981]
Epoch [78/120    avg_loss:0.006, val_acc:0.980]
Epoch [79/120    avg_loss:0.007, val_acc:0.981]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.007, val_acc:0.981]
Epoch [82/120    avg_loss:0.005, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.981]
Epoch [85/120    avg_loss:0.010, val_acc:0.981]
Epoch [86/120    avg_loss:0.006, val_acc:0.981]
Epoch [87/120    avg_loss:0.010, val_acc:0.981]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.007, val_acc:0.981]
Epoch [90/120    avg_loss:0.006, val_acc:0.981]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.006, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.981]
Epoch [97/120    avg_loss:0.006, val_acc:0.981]
Epoch [98/120    avg_loss:0.011, val_acc:0.981]
Epoch [99/120    avg_loss:0.007, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.016, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.004, val_acc:0.981]
Epoch [110/120    avg_loss:0.005, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.007, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     2     0     0     7     0     0]
 [    0     0 18028     0    12     0    50     0     0     0]
 [    0     0     0  2017     0     0     0     0    13     6]
 [    0    47    20     4  2873     0     0     0    28     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4858     0     0    20]
 [    0     0     0     0     0     0     0  1286     0     4]
 [    0     6     0     0    33     0     0     0  3531     1]
 [    0     0     0     5    14    22     0     0     0   878]]

Accuracy:
99.28662665991854

F1 scores:
[       nan 0.99519678 0.99773092 0.99310684 0.97290891 0.99087452
 0.99284692 0.99574139 0.98866023 0.95956284]

Kappa:
0.990550554402154
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbd0b978710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.544, val_acc:0.603]
Epoch [2/120    avg_loss:0.763, val_acc:0.484]
Epoch [3/120    avg_loss:0.526, val_acc:0.594]
Epoch [4/120    avg_loss:0.448, val_acc:0.693]
Epoch [5/120    avg_loss:0.363, val_acc:0.709]
Epoch [6/120    avg_loss:0.310, val_acc:0.804]
Epoch [7/120    avg_loss:0.284, val_acc:0.790]
Epoch [8/120    avg_loss:0.255, val_acc:0.685]
Epoch [9/120    avg_loss:0.241, val_acc:0.884]
Epoch [10/120    avg_loss:0.200, val_acc:0.907]
Epoch [11/120    avg_loss:0.196, val_acc:0.840]
Epoch [12/120    avg_loss:0.190, val_acc:0.935]
Epoch [13/120    avg_loss:0.165, val_acc:0.911]
Epoch [14/120    avg_loss:0.107, val_acc:0.942]
Epoch [15/120    avg_loss:0.111, val_acc:0.930]
Epoch [16/120    avg_loss:0.099, val_acc:0.920]
Epoch [17/120    avg_loss:0.109, val_acc:0.949]
Epoch [18/120    avg_loss:0.073, val_acc:0.923]
Epoch [19/120    avg_loss:0.103, val_acc:0.948]
Epoch [20/120    avg_loss:0.104, val_acc:0.951]
Epoch [21/120    avg_loss:0.056, val_acc:0.968]
Epoch [22/120    avg_loss:0.053, val_acc:0.953]
Epoch [23/120    avg_loss:0.110, val_acc:0.948]
Epoch [24/120    avg_loss:0.072, val_acc:0.964]
Epoch [25/120    avg_loss:0.078, val_acc:0.963]
Epoch [26/120    avg_loss:0.050, val_acc:0.962]
Epoch [27/120    avg_loss:0.035, val_acc:0.963]
Epoch [28/120    avg_loss:0.035, val_acc:0.965]
Epoch [29/120    avg_loss:0.048, val_acc:0.969]
Epoch [30/120    avg_loss:0.024, val_acc:0.975]
Epoch [31/120    avg_loss:0.021, val_acc:0.976]
Epoch [32/120    avg_loss:0.019, val_acc:0.973]
Epoch [33/120    avg_loss:0.028, val_acc:0.979]
Epoch [34/120    avg_loss:0.023, val_acc:0.954]
Epoch [35/120    avg_loss:0.047, val_acc:0.968]
Epoch [36/120    avg_loss:0.034, val_acc:0.954]
Epoch [37/120    avg_loss:0.069, val_acc:0.959]
Epoch [38/120    avg_loss:0.037, val_acc:0.975]
Epoch [39/120    avg_loss:0.024, val_acc:0.974]
Epoch [40/120    avg_loss:0.030, val_acc:0.970]
Epoch [41/120    avg_loss:0.015, val_acc:0.978]
Epoch [42/120    avg_loss:0.065, val_acc:0.961]
Epoch [43/120    avg_loss:0.038, val_acc:0.973]
Epoch [44/120    avg_loss:0.018, val_acc:0.979]
Epoch [45/120    avg_loss:0.016, val_acc:0.980]
Epoch [46/120    avg_loss:0.020, val_acc:0.979]
Epoch [47/120    avg_loss:0.037, val_acc:0.975]
Epoch [48/120    avg_loss:0.032, val_acc:0.977]
Epoch [49/120    avg_loss:0.021, val_acc:0.984]
Epoch [50/120    avg_loss:0.012, val_acc:0.984]
Epoch [51/120    avg_loss:0.020, val_acc:0.977]
Epoch [52/120    avg_loss:0.018, val_acc:0.985]
Epoch [53/120    avg_loss:0.011, val_acc:0.976]
Epoch [54/120    avg_loss:0.035, val_acc:0.975]
Epoch [55/120    avg_loss:0.016, val_acc:0.984]
Epoch [56/120    avg_loss:0.012, val_acc:0.984]
Epoch [57/120    avg_loss:0.010, val_acc:0.985]
Epoch [58/120    avg_loss:0.008, val_acc:0.987]
Epoch [59/120    avg_loss:0.026, val_acc:0.979]
Epoch [60/120    avg_loss:0.075, val_acc:0.975]
Epoch [61/120    avg_loss:0.028, val_acc:0.976]
Epoch [62/120    avg_loss:0.011, val_acc:0.984]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.013, val_acc:0.982]
Epoch [65/120    avg_loss:0.021, val_acc:0.970]
Epoch [66/120    avg_loss:0.021, val_acc:0.972]
Epoch [67/120    avg_loss:0.015, val_acc:0.981]
Epoch [68/120    avg_loss:0.016, val_acc:0.973]
Epoch [69/120    avg_loss:0.014, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.986]
Epoch [71/120    avg_loss:0.017, val_acc:0.979]
Epoch [72/120    avg_loss:0.011, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.987]
Epoch [74/120    avg_loss:0.006, val_acc:0.988]
Epoch [75/120    avg_loss:0.005, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.988]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.989]
Epoch [82/120    avg_loss:0.004, val_acc:0.988]
Epoch [83/120    avg_loss:0.004, val_acc:0.988]
Epoch [84/120    avg_loss:0.005, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.003, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.003, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.003, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.003, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     3     0     0     0     0    36     9]
 [    0     0 18056     0     2     0    28     0     4     0]
 [    0     4     0  1943     0     0     0     0    86     3]
 [    0    11     0     0  2955     0     0     0     2     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4863     0     0     0]
 [    0    26     0     0     0     0     2  1262     0     0]
 [    0     4     0    53    46     0     0     0  3468     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
99.16130431639071

F1 scores:
[       nan 0.99276884 0.99864495 0.96307311 0.98912134 0.99618321
 0.99539453 0.98902821 0.96776894 0.98590022]

Kappa:
0.9888884972128343
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f32ea20e710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.343, val_acc:0.432]
Epoch [2/120    avg_loss:0.746, val_acc:0.548]
Epoch [3/120    avg_loss:0.553, val_acc:0.745]
Epoch [4/120    avg_loss:0.415, val_acc:0.634]
Epoch [5/120    avg_loss:0.440, val_acc:0.802]
Epoch [6/120    avg_loss:0.323, val_acc:0.789]
Epoch [7/120    avg_loss:0.274, val_acc:0.817]
Epoch [8/120    avg_loss:0.225, val_acc:0.822]
Epoch [9/120    avg_loss:0.211, val_acc:0.901]
Epoch [10/120    avg_loss:0.201, val_acc:0.865]
Epoch [11/120    avg_loss:0.179, val_acc:0.923]
Epoch [12/120    avg_loss:0.142, val_acc:0.951]
Epoch [13/120    avg_loss:0.100, val_acc:0.923]
Epoch [14/120    avg_loss:0.110, val_acc:0.956]
Epoch [15/120    avg_loss:0.100, val_acc:0.951]
Epoch [16/120    avg_loss:0.066, val_acc:0.964]
Epoch [17/120    avg_loss:0.078, val_acc:0.942]
Epoch [18/120    avg_loss:0.072, val_acc:0.961]
Epoch [19/120    avg_loss:0.069, val_acc:0.954]
Epoch [20/120    avg_loss:0.069, val_acc:0.970]
Epoch [21/120    avg_loss:0.107, val_acc:0.937]
Epoch [22/120    avg_loss:0.092, val_acc:0.970]
Epoch [23/120    avg_loss:0.072, val_acc:0.954]
Epoch [24/120    avg_loss:0.068, val_acc:0.949]
Epoch [25/120    avg_loss:0.055, val_acc:0.965]
Epoch [26/120    avg_loss:0.051, val_acc:0.967]
Epoch [27/120    avg_loss:0.054, val_acc:0.961]
Epoch [28/120    avg_loss:0.041, val_acc:0.965]
Epoch [29/120    avg_loss:0.033, val_acc:0.968]
Epoch [30/120    avg_loss:0.040, val_acc:0.964]
Epoch [31/120    avg_loss:0.073, val_acc:0.951]
Epoch [32/120    avg_loss:0.048, val_acc:0.964]
Epoch [33/120    avg_loss:0.040, val_acc:0.976]
Epoch [34/120    avg_loss:0.035, val_acc:0.975]
Epoch [35/120    avg_loss:0.026, val_acc:0.980]
Epoch [36/120    avg_loss:0.037, val_acc:0.974]
Epoch [37/120    avg_loss:0.026, val_acc:0.979]
Epoch [38/120    avg_loss:0.038, val_acc:0.979]
Epoch [39/120    avg_loss:0.027, val_acc:0.980]
Epoch [40/120    avg_loss:0.028, val_acc:0.967]
Epoch [41/120    avg_loss:0.022, val_acc:0.970]
Epoch [42/120    avg_loss:0.022, val_acc:0.976]
Epoch [43/120    avg_loss:0.025, val_acc:0.975]
Epoch [44/120    avg_loss:0.044, val_acc:0.967]
Epoch [45/120    avg_loss:0.024, val_acc:0.979]
Epoch [46/120    avg_loss:0.015, val_acc:0.981]
Epoch [47/120    avg_loss:0.014, val_acc:0.977]
Epoch [48/120    avg_loss:0.018, val_acc:0.979]
Epoch [49/120    avg_loss:0.013, val_acc:0.983]
Epoch [50/120    avg_loss:0.028, val_acc:0.975]
Epoch [51/120    avg_loss:0.009, val_acc:0.981]
Epoch [52/120    avg_loss:0.021, val_acc:0.951]
Epoch [53/120    avg_loss:0.036, val_acc:0.976]
Epoch [54/120    avg_loss:0.040, val_acc:0.973]
Epoch [55/120    avg_loss:0.033, val_acc:0.965]
Epoch [56/120    avg_loss:0.016, val_acc:0.965]
Epoch [57/120    avg_loss:0.013, val_acc:0.980]
Epoch [58/120    avg_loss:0.018, val_acc:0.981]
Epoch [59/120    avg_loss:0.014, val_acc:0.986]
Epoch [60/120    avg_loss:0.018, val_acc:0.980]
Epoch [61/120    avg_loss:0.010, val_acc:0.989]
Epoch [62/120    avg_loss:0.012, val_acc:0.986]
Epoch [63/120    avg_loss:0.015, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.987]
Epoch [65/120    avg_loss:0.009, val_acc:0.989]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.007, val_acc:0.983]
Epoch [68/120    avg_loss:0.011, val_acc:0.985]
Epoch [69/120    avg_loss:0.007, val_acc:0.984]
Epoch [70/120    avg_loss:0.014, val_acc:0.984]
Epoch [71/120    avg_loss:0.028, val_acc:0.979]
Epoch [72/120    avg_loss:0.013, val_acc:0.984]
Epoch [73/120    avg_loss:0.008, val_acc:0.982]
Epoch [74/120    avg_loss:0.011, val_acc:0.987]
Epoch [75/120    avg_loss:0.005, val_acc:0.988]
Epoch [76/120    avg_loss:0.005, val_acc:0.989]
Epoch [77/120    avg_loss:0.006, val_acc:0.988]
Epoch [78/120    avg_loss:0.027, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.987]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.982]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.003, val_acc:0.988]
Epoch [93/120    avg_loss:0.014, val_acc:0.988]
Epoch [94/120    avg_loss:0.003, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.989]
Epoch [102/120    avg_loss:0.003, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.002, val_acc:0.989]
Epoch [108/120    avg_loss:0.003, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.989]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.003, val_acc:0.989]
Epoch [115/120    avg_loss:0.002, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.989]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.002, val_acc:0.988]
Epoch [120/120    avg_loss:0.002, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     1     0     0     0     9     1     5]
 [    0     1 18064     0     2     0    16     0     7     0]
 [    0     0     0  1976     0     0     0     0    58     2]
 [    0    18     5     0  2947     0     0     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4873     0     0     0]
 [    0    15     0     0     0     0     2  1267     0     6]
 [    0     5     0    43    39     0     0     0  3484     0]
 [    0     0     0     0     3     6     0     0     0   910]]

Accuracy:
99.39507868797146

F1 scores:
[       nan 0.99573213 0.99900453 0.97435897 0.98842864 0.99770642
 0.99764561 0.98752923 0.97823951 0.98805646]

Kappa:
0.9919852204896202
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cf8ccf7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.416, val_acc:0.324]
Epoch [2/120    avg_loss:0.741, val_acc:0.641]
Epoch [3/120    avg_loss:0.562, val_acc:0.621]
Epoch [4/120    avg_loss:0.438, val_acc:0.739]
Epoch [5/120    avg_loss:0.373, val_acc:0.794]
Epoch [6/120    avg_loss:0.309, val_acc:0.769]
Epoch [7/120    avg_loss:0.299, val_acc:0.837]
Epoch [8/120    avg_loss:0.300, val_acc:0.816]
Epoch [9/120    avg_loss:0.249, val_acc:0.904]
Epoch [10/120    avg_loss:0.247, val_acc:0.826]
Epoch [11/120    avg_loss:0.175, val_acc:0.932]
Epoch [12/120    avg_loss:0.216, val_acc:0.943]
Epoch [13/120    avg_loss:0.133, val_acc:0.924]
Epoch [14/120    avg_loss:0.124, val_acc:0.960]
Epoch [15/120    avg_loss:0.133, val_acc:0.948]
Epoch [16/120    avg_loss:0.103, val_acc:0.953]
Epoch [17/120    avg_loss:0.104, val_acc:0.971]
Epoch [18/120    avg_loss:0.088, val_acc:0.973]
Epoch [19/120    avg_loss:0.064, val_acc:0.968]
Epoch [20/120    avg_loss:0.067, val_acc:0.977]
Epoch [21/120    avg_loss:0.065, val_acc:0.970]
Epoch [22/120    avg_loss:0.056, val_acc:0.967]
Epoch [23/120    avg_loss:0.033, val_acc:0.980]
Epoch [24/120    avg_loss:0.062, val_acc:0.975]
Epoch [25/120    avg_loss:0.068, val_acc:0.976]
Epoch [26/120    avg_loss:0.042, val_acc:0.972]
Epoch [27/120    avg_loss:0.043, val_acc:0.982]
Epoch [28/120    avg_loss:0.099, val_acc:0.955]
Epoch [29/120    avg_loss:0.075, val_acc:0.979]
Epoch [30/120    avg_loss:0.041, val_acc:0.981]
Epoch [31/120    avg_loss:0.045, val_acc:0.968]
Epoch [32/120    avg_loss:0.026, val_acc:0.977]
Epoch [33/120    avg_loss:0.048, val_acc:0.978]
Epoch [34/120    avg_loss:0.022, val_acc:0.981]
Epoch [35/120    avg_loss:0.030, val_acc:0.982]
Epoch [36/120    avg_loss:0.029, val_acc:0.983]
Epoch [37/120    avg_loss:0.028, val_acc:0.978]
Epoch [38/120    avg_loss:0.033, val_acc:0.981]
Epoch [39/120    avg_loss:0.025, val_acc:0.974]
Epoch [40/120    avg_loss:0.018, val_acc:0.982]
Epoch [41/120    avg_loss:0.023, val_acc:0.990]
Epoch [42/120    avg_loss:0.016, val_acc:0.984]
Epoch [43/120    avg_loss:0.024, val_acc:0.934]
Epoch [44/120    avg_loss:0.022, val_acc:0.982]
Epoch [45/120    avg_loss:0.009, val_acc:0.986]
Epoch [46/120    avg_loss:0.019, val_acc:0.982]
Epoch [47/120    avg_loss:0.015, val_acc:0.987]
Epoch [48/120    avg_loss:0.018, val_acc:0.986]
Epoch [49/120    avg_loss:0.023, val_acc:0.982]
Epoch [50/120    avg_loss:0.030, val_acc:0.975]
Epoch [51/120    avg_loss:0.017, val_acc:0.989]
Epoch [52/120    avg_loss:0.035, val_acc:0.975]
Epoch [53/120    avg_loss:0.023, val_acc:0.985]
Epoch [54/120    avg_loss:0.017, val_acc:0.987]
Epoch [55/120    avg_loss:0.010, val_acc:0.988]
Epoch [56/120    avg_loss:0.007, val_acc:0.987]
Epoch [57/120    avg_loss:0.007, val_acc:0.988]
Epoch [58/120    avg_loss:0.010, val_acc:0.988]
Epoch [59/120    avg_loss:0.007, val_acc:0.987]
Epoch [60/120    avg_loss:0.009, val_acc:0.987]
Epoch [61/120    avg_loss:0.009, val_acc:0.988]
Epoch [62/120    avg_loss:0.010, val_acc:0.988]
Epoch [63/120    avg_loss:0.005, val_acc:0.988]
Epoch [64/120    avg_loss:0.009, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.990]
Epoch [66/120    avg_loss:0.007, val_acc:0.990]
Epoch [67/120    avg_loss:0.009, val_acc:0.990]
Epoch [68/120    avg_loss:0.007, val_acc:0.990]
Epoch [69/120    avg_loss:0.007, val_acc:0.990]
Epoch [70/120    avg_loss:0.009, val_acc:0.990]
Epoch [71/120    avg_loss:0.005, val_acc:0.990]
Epoch [72/120    avg_loss:0.010, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.006, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.005, val_acc:0.990]
Epoch [80/120    avg_loss:0.007, val_acc:0.989]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.990]
Epoch [83/120    avg_loss:0.005, val_acc:0.989]
Epoch [84/120    avg_loss:0.009, val_acc:0.990]
Epoch [85/120    avg_loss:0.005, val_acc:0.990]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.007, val_acc:0.989]
Epoch [89/120    avg_loss:0.008, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.989]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.011, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.008, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.006, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     0     2     0     0     0     0    22     0]
 [    0     0 18021     0    18     0    44     0     7     0]
 [    0     1     0  1950     0     0     0     0    83     2]
 [    0    11     0     8  2946     0     2     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0    15     0     0     0     0     0  1275     0     0]
 [    0     6     0    42    36     0     0     0  3487     0]
 [    0     0     0     0     0    13     0     0     0   906]]

Accuracy:
99.20950521774758

F1 scores:
[       nan 0.99557213 0.99778528 0.96582467 0.98660415 0.99504384
 0.99417833 0.99415205 0.97239264 0.99016393]

Kappa:
0.9895311809407572
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb6a2dad748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.343, val_acc:0.620]
Epoch [2/120    avg_loss:0.793, val_acc:0.558]
Epoch [3/120    avg_loss:0.573, val_acc:0.627]
Epoch [4/120    avg_loss:0.460, val_acc:0.775]
Epoch [5/120    avg_loss:0.358, val_acc:0.793]
Epoch [6/120    avg_loss:0.303, val_acc:0.836]
Epoch [7/120    avg_loss:0.244, val_acc:0.905]
Epoch [8/120    avg_loss:0.295, val_acc:0.905]
Epoch [9/120    avg_loss:0.215, val_acc:0.900]
Epoch [10/120    avg_loss:0.167, val_acc:0.924]
Epoch [11/120    avg_loss:0.158, val_acc:0.937]
Epoch [12/120    avg_loss:0.125, val_acc:0.948]
Epoch [13/120    avg_loss:0.105, val_acc:0.943]
Epoch [14/120    avg_loss:0.114, val_acc:0.911]
Epoch [15/120    avg_loss:0.154, val_acc:0.957]
Epoch [16/120    avg_loss:0.099, val_acc:0.961]
Epoch [17/120    avg_loss:0.075, val_acc:0.953]
Epoch [18/120    avg_loss:0.066, val_acc:0.950]
Epoch [19/120    avg_loss:0.107, val_acc:0.936]
Epoch [20/120    avg_loss:0.070, val_acc:0.959]
Epoch [21/120    avg_loss:0.043, val_acc:0.961]
Epoch [22/120    avg_loss:0.046, val_acc:0.960]
Epoch [23/120    avg_loss:0.097, val_acc:0.951]
Epoch [24/120    avg_loss:0.083, val_acc:0.975]
Epoch [25/120    avg_loss:0.056, val_acc:0.965]
Epoch [26/120    avg_loss:0.042, val_acc:0.956]
Epoch [27/120    avg_loss:0.036, val_acc:0.963]
Epoch [28/120    avg_loss:0.029, val_acc:0.973]
Epoch [29/120    avg_loss:0.043, val_acc:0.970]
Epoch [30/120    avg_loss:0.031, val_acc:0.963]
Epoch [31/120    avg_loss:0.058, val_acc:0.947]
Epoch [32/120    avg_loss:0.054, val_acc:0.962]
Epoch [33/120    avg_loss:0.051, val_acc:0.958]
Epoch [34/120    avg_loss:0.052, val_acc:0.970]
Epoch [35/120    avg_loss:0.038, val_acc:0.973]
Epoch [36/120    avg_loss:0.023, val_acc:0.979]
Epoch [37/120    avg_loss:0.029, val_acc:0.961]
Epoch [38/120    avg_loss:0.046, val_acc:0.952]
Epoch [39/120    avg_loss:0.049, val_acc:0.974]
Epoch [40/120    avg_loss:0.041, val_acc:0.978]
Epoch [41/120    avg_loss:0.028, val_acc:0.952]
Epoch [42/120    avg_loss:0.041, val_acc:0.980]
Epoch [43/120    avg_loss:0.062, val_acc:0.972]
Epoch [44/120    avg_loss:0.027, val_acc:0.980]
Epoch [45/120    avg_loss:0.028, val_acc:0.970]
Epoch [46/120    avg_loss:0.076, val_acc:0.962]
Epoch [47/120    avg_loss:0.033, val_acc:0.975]
Epoch [48/120    avg_loss:0.020, val_acc:0.982]
Epoch [49/120    avg_loss:0.014, val_acc:0.979]
Epoch [50/120    avg_loss:0.010, val_acc:0.984]
Epoch [51/120    avg_loss:0.024, val_acc:0.950]
Epoch [52/120    avg_loss:0.024, val_acc:0.974]
Epoch [53/120    avg_loss:0.021, val_acc:0.975]
Epoch [54/120    avg_loss:0.025, val_acc:0.971]
Epoch [55/120    avg_loss:0.013, val_acc:0.985]
Epoch [56/120    avg_loss:0.027, val_acc:0.979]
Epoch [57/120    avg_loss:0.022, val_acc:0.973]
Epoch [58/120    avg_loss:0.017, val_acc:0.981]
Epoch [59/120    avg_loss:0.010, val_acc:0.985]
Epoch [60/120    avg_loss:0.010, val_acc:0.979]
Epoch [61/120    avg_loss:0.014, val_acc:0.987]
Epoch [62/120    avg_loss:0.019, val_acc:0.984]
Epoch [63/120    avg_loss:0.024, val_acc:0.975]
Epoch [64/120    avg_loss:0.022, val_acc:0.978]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.008, val_acc:0.984]
Epoch [67/120    avg_loss:0.020, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.985]
Epoch [69/120    avg_loss:0.023, val_acc:0.951]
Epoch [70/120    avg_loss:0.045, val_acc:0.970]
Epoch [71/120    avg_loss:0.019, val_acc:0.974]
Epoch [72/120    avg_loss:0.016, val_acc:0.979]
Epoch [73/120    avg_loss:0.009, val_acc:0.970]
Epoch [74/120    avg_loss:0.011, val_acc:0.982]
Epoch [75/120    avg_loss:0.018, val_acc:0.951]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.008, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.985]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.004, val_acc:0.984]
Epoch [92/120    avg_loss:0.004, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.003, val_acc:0.984]
Epoch [98/120    avg_loss:0.004, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.003, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.004, val_acc:0.984]
Epoch [113/120    avg_loss:0.003, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     0     0     0     5     3     1]
 [    0     0 18024     0     7     0    58     0     1     0]
 [    0     2     0  1962     0     0     0     0    70     2]
 [    0    14     0     0  2955     0     1     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     0     0     0  4866     0     0     0]
 [    0    14     0     0     0     0     2  1273     0     1]
 [    0     7     0    39    44     0     0     0  3481     0]
 [    0     0     0     0     2    14     0     0     0   903]]

Accuracy:
99.27457643457933

F1 scores:
[       nan 0.9964319  0.99784089 0.97200892 0.98829431 0.99466463
 0.99255482 0.99143302 0.97671156 0.9890471 ]

Kappa:
0.9903919902765971
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa83af337b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.353, val_acc:0.697]
Epoch [2/120    avg_loss:0.707, val_acc:0.587]
Epoch [3/120    avg_loss:0.526, val_acc:0.733]
Epoch [4/120    avg_loss:0.503, val_acc:0.684]
Epoch [5/120    avg_loss:0.407, val_acc:0.773]
Epoch [6/120    avg_loss:0.343, val_acc:0.751]
Epoch [7/120    avg_loss:0.275, val_acc:0.785]
Epoch [8/120    avg_loss:0.204, val_acc:0.870]
Epoch [9/120    avg_loss:0.173, val_acc:0.877]
Epoch [10/120    avg_loss:0.146, val_acc:0.900]
Epoch [11/120    avg_loss:0.180, val_acc:0.890]
Epoch [12/120    avg_loss:0.162, val_acc:0.924]
Epoch [13/120    avg_loss:0.105, val_acc:0.889]
Epoch [14/120    avg_loss:0.133, val_acc:0.921]
Epoch [15/120    avg_loss:0.078, val_acc:0.947]
Epoch [16/120    avg_loss:0.077, val_acc:0.938]
Epoch [17/120    avg_loss:0.081, val_acc:0.942]
Epoch [18/120    avg_loss:0.082, val_acc:0.933]
Epoch [19/120    avg_loss:0.083, val_acc:0.947]
Epoch [20/120    avg_loss:0.107, val_acc:0.925]
Epoch [21/120    avg_loss:0.080, val_acc:0.932]
Epoch [22/120    avg_loss:0.061, val_acc:0.947]
Epoch [23/120    avg_loss:0.080, val_acc:0.960]
Epoch [24/120    avg_loss:0.037, val_acc:0.956]
Epoch [25/120    avg_loss:0.048, val_acc:0.955]
Epoch [26/120    avg_loss:0.033, val_acc:0.954]
Epoch [27/120    avg_loss:0.038, val_acc:0.961]
Epoch [28/120    avg_loss:0.070, val_acc:0.945]
Epoch [29/120    avg_loss:0.040, val_acc:0.960]
Epoch [30/120    avg_loss:0.038, val_acc:0.955]
Epoch [31/120    avg_loss:0.024, val_acc:0.964]
Epoch [32/120    avg_loss:0.025, val_acc:0.965]
Epoch [33/120    avg_loss:0.021, val_acc:0.953]
Epoch [34/120    avg_loss:0.026, val_acc:0.951]
Epoch [35/120    avg_loss:0.099, val_acc:0.941]
Epoch [36/120    avg_loss:0.047, val_acc:0.962]
Epoch [37/120    avg_loss:0.023, val_acc:0.968]
Epoch [38/120    avg_loss:0.029, val_acc:0.958]
Epoch [39/120    avg_loss:0.035, val_acc:0.961]
Epoch [40/120    avg_loss:0.036, val_acc:0.961]
Epoch [41/120    avg_loss:0.019, val_acc:0.966]
Epoch [42/120    avg_loss:0.025, val_acc:0.965]
Epoch [43/120    avg_loss:0.024, val_acc:0.956]
Epoch [44/120    avg_loss:0.020, val_acc:0.954]
Epoch [45/120    avg_loss:0.028, val_acc:0.970]
Epoch [46/120    avg_loss:0.020, val_acc:0.970]
Epoch [47/120    avg_loss:0.015, val_acc:0.970]
Epoch [48/120    avg_loss:0.027, val_acc:0.965]
Epoch [49/120    avg_loss:0.011, val_acc:0.968]
Epoch [50/120    avg_loss:0.020, val_acc:0.975]
Epoch [51/120    avg_loss:0.018, val_acc:0.961]
Epoch [52/120    avg_loss:0.024, val_acc:0.966]
Epoch [53/120    avg_loss:0.012, val_acc:0.973]
Epoch [54/120    avg_loss:0.013, val_acc:0.970]
Epoch [55/120    avg_loss:0.041, val_acc:0.966]
Epoch [56/120    avg_loss:0.085, val_acc:0.936]
Epoch [57/120    avg_loss:0.036, val_acc:0.951]
Epoch [58/120    avg_loss:0.027, val_acc:0.965]
Epoch [59/120    avg_loss:0.014, val_acc:0.970]
Epoch [60/120    avg_loss:0.019, val_acc:0.970]
Epoch [61/120    avg_loss:0.027, val_acc:0.975]
Epoch [62/120    avg_loss:0.015, val_acc:0.972]
Epoch [63/120    avg_loss:0.012, val_acc:0.970]
Epoch [64/120    avg_loss:0.016, val_acc:0.970]
Epoch [65/120    avg_loss:0.013, val_acc:0.959]
Epoch [66/120    avg_loss:0.013, val_acc:0.972]
Epoch [67/120    avg_loss:0.011, val_acc:0.973]
Epoch [68/120    avg_loss:0.054, val_acc:0.961]
Epoch [69/120    avg_loss:0.013, val_acc:0.968]
Epoch [70/120    avg_loss:0.016, val_acc:0.970]
Epoch [71/120    avg_loss:0.007, val_acc:0.972]
Epoch [72/120    avg_loss:0.018, val_acc:0.951]
Epoch [73/120    avg_loss:0.010, val_acc:0.974]
Epoch [74/120    avg_loss:0.009, val_acc:0.974]
Epoch [75/120    avg_loss:0.008, val_acc:0.977]
Epoch [76/120    avg_loss:0.006, val_acc:0.978]
Epoch [77/120    avg_loss:0.020, val_acc:0.973]
Epoch [78/120    avg_loss:0.006, val_acc:0.974]
Epoch [79/120    avg_loss:0.004, val_acc:0.976]
Epoch [80/120    avg_loss:0.003, val_acc:0.975]
Epoch [81/120    avg_loss:0.005, val_acc:0.976]
Epoch [82/120    avg_loss:0.004, val_acc:0.975]
Epoch [83/120    avg_loss:0.005, val_acc:0.976]
Epoch [84/120    avg_loss:0.006, val_acc:0.977]
Epoch [85/120    avg_loss:0.007, val_acc:0.977]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.007, val_acc:0.977]
Epoch [88/120    avg_loss:0.004, val_acc:0.976]
Epoch [89/120    avg_loss:0.005, val_acc:0.976]
Epoch [90/120    avg_loss:0.004, val_acc:0.976]
Epoch [91/120    avg_loss:0.006, val_acc:0.976]
Epoch [92/120    avg_loss:0.004, val_acc:0.977]
Epoch [93/120    avg_loss:0.009, val_acc:0.978]
Epoch [94/120    avg_loss:0.007, val_acc:0.978]
Epoch [95/120    avg_loss:0.005, val_acc:0.978]
Epoch [96/120    avg_loss:0.007, val_acc:0.977]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.005, val_acc:0.977]
Epoch [99/120    avg_loss:0.005, val_acc:0.977]
Epoch [100/120    avg_loss:0.006, val_acc:0.977]
Epoch [101/120    avg_loss:0.005, val_acc:0.975]
Epoch [102/120    avg_loss:0.008, val_acc:0.975]
Epoch [103/120    avg_loss:0.005, val_acc:0.977]
Epoch [104/120    avg_loss:0.004, val_acc:0.977]
Epoch [105/120    avg_loss:0.005, val_acc:0.977]
Epoch [106/120    avg_loss:0.004, val_acc:0.977]
Epoch [107/120    avg_loss:0.004, val_acc:0.977]
Epoch [108/120    avg_loss:0.003, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.977]
Epoch [110/120    avg_loss:0.007, val_acc:0.977]
Epoch [111/120    avg_loss:0.008, val_acc:0.977]
Epoch [112/120    avg_loss:0.003, val_acc:0.977]
Epoch [113/120    avg_loss:0.005, val_acc:0.977]
Epoch [114/120    avg_loss:0.005, val_acc:0.977]
Epoch [115/120    avg_loss:0.006, val_acc:0.977]
Epoch [116/120    avg_loss:0.004, val_acc:0.977]
Epoch [117/120    avg_loss:0.009, val_acc:0.977]
Epoch [118/120    avg_loss:0.005, val_acc:0.977]
Epoch [119/120    avg_loss:0.005, val_acc:0.977]
Epoch [120/120    avg_loss:0.004, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     1     0     0     0     0    16    17]
 [    0     0 18050     0     2     0    31     0     7     0]
 [    0     0     0  1948     0     0     0     0    88     0]
 [    0    27     7     0  2931     0     1     0     3     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     6     0     0     0     0     1  1281     0     2]
 [    0     7     0    72    35     0     0     0  3457     0]
 [    0     0     2     1     0    12     0     0     0   904]]

Accuracy:
99.17817463186562

F1 scores:
[       nan 0.99425019 0.9986445  0.96007886 0.98686869 0.99542334
 0.99662887 0.99649942 0.96807617 0.9799458 ]

Kappa:
0.989113594595287
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff756182710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.323, val_acc:0.340]
Epoch [2/120    avg_loss:0.775, val_acc:0.715]
Epoch [3/120    avg_loss:0.511, val_acc:0.655]
Epoch [4/120    avg_loss:0.398, val_acc:0.818]
Epoch [5/120    avg_loss:0.262, val_acc:0.876]
Epoch [6/120    avg_loss:0.233, val_acc:0.898]
Epoch [7/120    avg_loss:0.253, val_acc:0.850]
Epoch [8/120    avg_loss:0.257, val_acc:0.862]
Epoch [9/120    avg_loss:0.179, val_acc:0.886]
Epoch [10/120    avg_loss:0.145, val_acc:0.898]
Epoch [11/120    avg_loss:0.146, val_acc:0.931]
Epoch [12/120    avg_loss:0.152, val_acc:0.915]
Epoch [13/120    avg_loss:0.146, val_acc:0.950]
Epoch [14/120    avg_loss:0.123, val_acc:0.948]
Epoch [15/120    avg_loss:0.133, val_acc:0.957]
Epoch [16/120    avg_loss:0.099, val_acc:0.950]
Epoch [17/120    avg_loss:0.102, val_acc:0.943]
Epoch [18/120    avg_loss:0.071, val_acc:0.965]
Epoch [19/120    avg_loss:0.057, val_acc:0.964]
Epoch [20/120    avg_loss:0.046, val_acc:0.960]
Epoch [21/120    avg_loss:0.063, val_acc:0.959]
Epoch [22/120    avg_loss:0.096, val_acc:0.962]
Epoch [23/120    avg_loss:0.060, val_acc:0.956]
Epoch [24/120    avg_loss:0.041, val_acc:0.967]
Epoch [25/120    avg_loss:0.033, val_acc:0.962]
Epoch [26/120    avg_loss:0.034, val_acc:0.978]
Epoch [27/120    avg_loss:0.034, val_acc:0.967]
Epoch [28/120    avg_loss:0.049, val_acc:0.970]
Epoch [29/120    avg_loss:0.034, val_acc:0.970]
Epoch [30/120    avg_loss:0.059, val_acc:0.940]
Epoch [31/120    avg_loss:0.056, val_acc:0.967]
Epoch [32/120    avg_loss:0.028, val_acc:0.964]
Epoch [33/120    avg_loss:0.067, val_acc:0.964]
Epoch [34/120    avg_loss:0.038, val_acc:0.979]
Epoch [35/120    avg_loss:0.025, val_acc:0.982]
Epoch [36/120    avg_loss:0.019, val_acc:0.977]
Epoch [37/120    avg_loss:0.018, val_acc:0.976]
Epoch [38/120    avg_loss:0.016, val_acc:0.979]
Epoch [39/120    avg_loss:0.022, val_acc:0.974]
Epoch [40/120    avg_loss:0.018, val_acc:0.977]
Epoch [41/120    avg_loss:0.020, val_acc:0.980]
Epoch [42/120    avg_loss:0.020, val_acc:0.979]
Epoch [43/120    avg_loss:0.024, val_acc:0.909]
Epoch [44/120    avg_loss:0.022, val_acc:0.980]
Epoch [45/120    avg_loss:0.030, val_acc:0.971]
Epoch [46/120    avg_loss:0.022, val_acc:0.976]
Epoch [47/120    avg_loss:0.014, val_acc:0.984]
Epoch [48/120    avg_loss:0.015, val_acc:0.980]
Epoch [49/120    avg_loss:0.018, val_acc:0.982]
Epoch [50/120    avg_loss:0.009, val_acc:0.981]
Epoch [51/120    avg_loss:0.017, val_acc:0.982]
Epoch [52/120    avg_loss:0.009, val_acc:0.983]
Epoch [53/120    avg_loss:0.017, val_acc:0.984]
Epoch [54/120    avg_loss:0.009, val_acc:0.984]
Epoch [55/120    avg_loss:0.008, val_acc:0.983]
Epoch [56/120    avg_loss:0.019, val_acc:0.974]
Epoch [57/120    avg_loss:0.011, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.982]
Epoch [59/120    avg_loss:0.005, val_acc:0.983]
Epoch [60/120    avg_loss:0.012, val_acc:0.979]
Epoch [61/120    avg_loss:0.014, val_acc:0.985]
Epoch [62/120    avg_loss:0.019, val_acc:0.976]
Epoch [63/120    avg_loss:0.035, val_acc:0.981]
Epoch [64/120    avg_loss:0.014, val_acc:0.982]
Epoch [65/120    avg_loss:0.019, val_acc:0.979]
Epoch [66/120    avg_loss:0.012, val_acc:0.982]
Epoch [67/120    avg_loss:0.009, val_acc:0.985]
Epoch [68/120    avg_loss:0.011, val_acc:0.973]
Epoch [69/120    avg_loss:0.008, val_acc:0.985]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.988]
Epoch [72/120    avg_loss:0.005, val_acc:0.988]
Epoch [73/120    avg_loss:0.008, val_acc:0.983]
Epoch [74/120    avg_loss:0.006, val_acc:0.989]
Epoch [75/120    avg_loss:0.007, val_acc:0.987]
Epoch [76/120    avg_loss:0.003, val_acc:0.991]
Epoch [77/120    avg_loss:0.026, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.973]
Epoch [80/120    avg_loss:0.009, val_acc:0.987]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.009, val_acc:0.975]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.981]
Epoch [85/120    avg_loss:0.011, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.003, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.002, val_acc:0.989]
Epoch [97/120    avg_loss:0.003, val_acc:0.989]
Epoch [98/120    avg_loss:0.003, val_acc:0.989]
Epoch [99/120    avg_loss:0.003, val_acc:0.989]
Epoch [100/120    avg_loss:0.003, val_acc:0.989]
Epoch [101/120    avg_loss:0.002, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.989]
Epoch [103/120    avg_loss:0.003, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.989]
Epoch [106/120    avg_loss:0.009, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.989]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.003, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.002, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.002, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     3     0     0     0     4    19     4]
 [    0     0 18040     0     6     0    41     0     3     0]
 [    0     8     0  1976     0     0     0     0    50     2]
 [    0    20     0     0  2942     0     0     0     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     0     0  4868     0     0     0]
 [    0     7     0     0     0     0     3  1277     0     3]
 [    0     4     0    44    47     0     0     0  3476     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
99.28180656978286

F1 scores:
[       nan 0.99463994 0.99833979 0.97363883 0.98609016 0.99618321
 0.99448417 0.99338779 0.97558237 0.98804348]

Kappa:
0.9904872505251981
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fdcf55828>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.385, val_acc:0.574]
Epoch [2/120    avg_loss:0.738, val_acc:0.491]
Epoch [3/120    avg_loss:0.539, val_acc:0.556]
Epoch [4/120    avg_loss:0.448, val_acc:0.634]
Epoch [5/120    avg_loss:0.364, val_acc:0.710]
Epoch [6/120    avg_loss:0.352, val_acc:0.719]
Epoch [7/120    avg_loss:0.334, val_acc:0.783]
Epoch [8/120    avg_loss:0.258, val_acc:0.906]
Epoch [9/120    avg_loss:0.231, val_acc:0.877]
Epoch [10/120    avg_loss:0.199, val_acc:0.893]
Epoch [11/120    avg_loss:0.166, val_acc:0.905]
Epoch [12/120    avg_loss:0.177, val_acc:0.914]
Epoch [13/120    avg_loss:0.136, val_acc:0.931]
Epoch [14/120    avg_loss:0.111, val_acc:0.910]
Epoch [15/120    avg_loss:0.116, val_acc:0.932]
Epoch [16/120    avg_loss:0.126, val_acc:0.908]
Epoch [17/120    avg_loss:0.097, val_acc:0.934]
Epoch [18/120    avg_loss:0.078, val_acc:0.924]
Epoch [19/120    avg_loss:0.075, val_acc:0.956]
Epoch [20/120    avg_loss:0.058, val_acc:0.956]
Epoch [21/120    avg_loss:0.062, val_acc:0.965]
Epoch [22/120    avg_loss:0.058, val_acc:0.969]
Epoch [23/120    avg_loss:0.065, val_acc:0.929]
Epoch [24/120    avg_loss:0.066, val_acc:0.951]
Epoch [25/120    avg_loss:0.039, val_acc:0.966]
Epoch [26/120    avg_loss:0.061, val_acc:0.953]
Epoch [27/120    avg_loss:0.044, val_acc:0.968]
Epoch [28/120    avg_loss:0.041, val_acc:0.960]
Epoch [29/120    avg_loss:0.049, val_acc:0.966]
Epoch [30/120    avg_loss:0.024, val_acc:0.968]
Epoch [31/120    avg_loss:0.031, val_acc:0.965]
Epoch [32/120    avg_loss:0.037, val_acc:0.956]
Epoch [33/120    avg_loss:0.037, val_acc:0.970]
Epoch [34/120    avg_loss:0.037, val_acc:0.963]
Epoch [35/120    avg_loss:0.041, val_acc:0.951]
Epoch [36/120    avg_loss:0.034, val_acc:0.970]
Epoch [37/120    avg_loss:0.024, val_acc:0.951]
Epoch [38/120    avg_loss:0.025, val_acc:0.970]
Epoch [39/120    avg_loss:0.035, val_acc:0.964]
Epoch [40/120    avg_loss:0.024, val_acc:0.969]
Epoch [41/120    avg_loss:0.019, val_acc:0.957]
Epoch [42/120    avg_loss:0.021, val_acc:0.977]
Epoch [43/120    avg_loss:0.022, val_acc:0.973]
Epoch [44/120    avg_loss:0.354, val_acc:0.810]
Epoch [45/120    avg_loss:0.358, val_acc:0.828]
Epoch [46/120    avg_loss:0.217, val_acc:0.907]
Epoch [47/120    avg_loss:0.184, val_acc:0.905]
Epoch [48/120    avg_loss:0.185, val_acc:0.924]
Epoch [49/120    avg_loss:0.162, val_acc:0.927]
Epoch [50/120    avg_loss:0.181, val_acc:0.924]
Epoch [51/120    avg_loss:0.114, val_acc:0.961]
Epoch [52/120    avg_loss:0.061, val_acc:0.959]
Epoch [53/120    avg_loss:0.088, val_acc:0.963]
Epoch [54/120    avg_loss:0.085, val_acc:0.967]
Epoch [55/120    avg_loss:0.062, val_acc:0.924]
Epoch [56/120    avg_loss:0.053, val_acc:0.962]
Epoch [57/120    avg_loss:0.043, val_acc:0.961]
Epoch [58/120    avg_loss:0.035, val_acc:0.965]
Epoch [59/120    avg_loss:0.030, val_acc:0.969]
Epoch [60/120    avg_loss:0.033, val_acc:0.965]
Epoch [61/120    avg_loss:0.028, val_acc:0.967]
Epoch [62/120    avg_loss:0.036, val_acc:0.969]
Epoch [63/120    avg_loss:0.031, val_acc:0.967]
Epoch [64/120    avg_loss:0.026, val_acc:0.965]
Epoch [65/120    avg_loss:0.024, val_acc:0.964]
Epoch [66/120    avg_loss:0.026, val_acc:0.970]
Epoch [67/120    avg_loss:0.030, val_acc:0.967]
Epoch [68/120    avg_loss:0.028, val_acc:0.965]
Epoch [69/120    avg_loss:0.022, val_acc:0.965]
Epoch [70/120    avg_loss:0.024, val_acc:0.967]
Epoch [71/120    avg_loss:0.023, val_acc:0.966]
Epoch [72/120    avg_loss:0.024, val_acc:0.965]
Epoch [73/120    avg_loss:0.022, val_acc:0.965]
Epoch [74/120    avg_loss:0.018, val_acc:0.966]
Epoch [75/120    avg_loss:0.026, val_acc:0.967]
Epoch [76/120    avg_loss:0.028, val_acc:0.965]
Epoch [77/120    avg_loss:0.028, val_acc:0.967]
Epoch [78/120    avg_loss:0.024, val_acc:0.967]
Epoch [79/120    avg_loss:0.023, val_acc:0.967]
Epoch [80/120    avg_loss:0.028, val_acc:0.967]
Epoch [81/120    avg_loss:0.027, val_acc:0.966]
Epoch [82/120    avg_loss:0.031, val_acc:0.967]
Epoch [83/120    avg_loss:0.026, val_acc:0.967]
Epoch [84/120    avg_loss:0.027, val_acc:0.967]
Epoch [85/120    avg_loss:0.023, val_acc:0.967]
Epoch [86/120    avg_loss:0.025, val_acc:0.967]
Epoch [87/120    avg_loss:0.031, val_acc:0.967]
Epoch [88/120    avg_loss:0.023, val_acc:0.967]
Epoch [89/120    avg_loss:0.021, val_acc:0.967]
Epoch [90/120    avg_loss:0.035, val_acc:0.967]
Epoch [91/120    avg_loss:0.027, val_acc:0.967]
Epoch [92/120    avg_loss:0.021, val_acc:0.967]
Epoch [93/120    avg_loss:0.021, val_acc:0.967]
Epoch [94/120    avg_loss:0.026, val_acc:0.967]
Epoch [95/120    avg_loss:0.023, val_acc:0.967]
Epoch [96/120    avg_loss:0.025, val_acc:0.967]
Epoch [97/120    avg_loss:0.030, val_acc:0.967]
Epoch [98/120    avg_loss:0.020, val_acc:0.967]
Epoch [99/120    avg_loss:0.023, val_acc:0.967]
Epoch [100/120    avg_loss:0.024, val_acc:0.967]
Epoch [101/120    avg_loss:0.020, val_acc:0.967]
Epoch [102/120    avg_loss:0.022, val_acc:0.967]
Epoch [103/120    avg_loss:0.027, val_acc:0.967]
Epoch [104/120    avg_loss:0.029, val_acc:0.967]
Epoch [105/120    avg_loss:0.021, val_acc:0.967]
Epoch [106/120    avg_loss:0.022, val_acc:0.967]
Epoch [107/120    avg_loss:0.019, val_acc:0.967]
Epoch [108/120    avg_loss:0.028, val_acc:0.967]
Epoch [109/120    avg_loss:0.020, val_acc:0.967]
Epoch [110/120    avg_loss:0.025, val_acc:0.967]
Epoch [111/120    avg_loss:0.018, val_acc:0.967]
Epoch [112/120    avg_loss:0.030, val_acc:0.967]
Epoch [113/120    avg_loss:0.033, val_acc:0.967]
Epoch [114/120    avg_loss:0.022, val_acc:0.967]
Epoch [115/120    avg_loss:0.034, val_acc:0.967]
Epoch [116/120    avg_loss:0.026, val_acc:0.967]
Epoch [117/120    avg_loss:0.021, val_acc:0.967]
Epoch [118/120    avg_loss:0.030, val_acc:0.967]
Epoch [119/120    avg_loss:0.025, val_acc:0.967]
Epoch [120/120    avg_loss:0.024, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6386     9     0     0     0     0     0    26    11]
 [    0     5 18003     0    22     0    54     0     6     0]
 [    0    17     0  1950     0     0     0     0    69     0]
 [    0    44    18     0  2887     0     6     0    11     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    40     0     0     0  4806     0    32     0]
 [    0    23     0     0     0     0     1  1263     3     0]
 [    0    27     4    37    35     0     6     0  3462     0]
 [    0     4     0     0     1     8     0     2     0   904]]

Accuracy:
98.72990624924687

F1 scores:
[       nan 0.98716958 0.99563101 0.9694258  0.97583235 0.99694423
 0.98574505 0.98864971 0.9643454  0.9826087 ]

Kappa:
0.983169223026477
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6600e5a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.451, val_acc:0.602]
Epoch [2/120    avg_loss:0.757, val_acc:0.744]
Epoch [3/120    avg_loss:0.545, val_acc:0.714]
Epoch [4/120    avg_loss:0.398, val_acc:0.743]
Epoch [5/120    avg_loss:0.381, val_acc:0.763]
Epoch [6/120    avg_loss:0.281, val_acc:0.856]
Epoch [7/120    avg_loss:0.234, val_acc:0.867]
Epoch [8/120    avg_loss:0.218, val_acc:0.938]
Epoch [9/120    avg_loss:0.178, val_acc:0.893]
Epoch [10/120    avg_loss:0.184, val_acc:0.928]
Epoch [11/120    avg_loss:0.178, val_acc:0.951]
Epoch [12/120    avg_loss:0.121, val_acc:0.931]
Epoch [13/120    avg_loss:0.111, val_acc:0.944]
Epoch [14/120    avg_loss:0.080, val_acc:0.956]
Epoch [15/120    avg_loss:0.090, val_acc:0.900]
Epoch [16/120    avg_loss:0.060, val_acc:0.964]
Epoch [17/120    avg_loss:0.086, val_acc:0.961]
Epoch [18/120    avg_loss:0.062, val_acc:0.948]
Epoch [19/120    avg_loss:0.074, val_acc:0.967]
Epoch [20/120    avg_loss:0.051, val_acc:0.964]
Epoch [21/120    avg_loss:0.040, val_acc:0.965]
Epoch [22/120    avg_loss:0.048, val_acc:0.972]
Epoch [23/120    avg_loss:0.089, val_acc:0.963]
Epoch [24/120    avg_loss:0.091, val_acc:0.962]
Epoch [25/120    avg_loss:0.056, val_acc:0.968]
Epoch [26/120    avg_loss:0.040, val_acc:0.972]
Epoch [27/120    avg_loss:0.044, val_acc:0.963]
Epoch [28/120    avg_loss:0.046, val_acc:0.962]
Epoch [29/120    avg_loss:0.057, val_acc:0.962]
Epoch [30/120    avg_loss:0.046, val_acc:0.956]
Epoch [31/120    avg_loss:0.032, val_acc:0.972]
Epoch [32/120    avg_loss:0.065, val_acc:0.970]
Epoch [33/120    avg_loss:0.034, val_acc:0.951]
Epoch [34/120    avg_loss:0.058, val_acc:0.975]
Epoch [35/120    avg_loss:0.033, val_acc:0.976]
Epoch [36/120    avg_loss:0.023, val_acc:0.975]
Epoch [37/120    avg_loss:0.044, val_acc:0.976]
Epoch [38/120    avg_loss:0.038, val_acc:0.979]
Epoch [39/120    avg_loss:0.034, val_acc:0.961]
Epoch [40/120    avg_loss:0.046, val_acc:0.974]
Epoch [41/120    avg_loss:0.026, val_acc:0.979]
Epoch [42/120    avg_loss:0.029, val_acc:0.984]
Epoch [43/120    avg_loss:0.022, val_acc:0.986]
Epoch [44/120    avg_loss:0.012, val_acc:0.985]
Epoch [45/120    avg_loss:0.011, val_acc:0.985]
Epoch [46/120    avg_loss:0.012, val_acc:0.980]
Epoch [47/120    avg_loss:0.011, val_acc:0.985]
Epoch [48/120    avg_loss:0.013, val_acc:0.986]
Epoch [49/120    avg_loss:0.025, val_acc:0.987]
Epoch [50/120    avg_loss:0.012, val_acc:0.984]
Epoch [51/120    avg_loss:0.012, val_acc:0.984]
Epoch [52/120    avg_loss:0.016, val_acc:0.988]
Epoch [53/120    avg_loss:0.022, val_acc:0.974]
Epoch [54/120    avg_loss:0.020, val_acc:0.985]
Epoch [55/120    avg_loss:0.013, val_acc:0.981]
Epoch [56/120    avg_loss:0.030, val_acc:0.982]
Epoch [57/120    avg_loss:0.015, val_acc:0.984]
Epoch [58/120    avg_loss:0.009, val_acc:0.986]
Epoch [59/120    avg_loss:0.019, val_acc:0.984]
Epoch [60/120    avg_loss:0.009, val_acc:0.984]
Epoch [61/120    avg_loss:0.017, val_acc:0.984]
Epoch [62/120    avg_loss:0.021, val_acc:0.979]
Epoch [63/120    avg_loss:0.019, val_acc:0.984]
Epoch [64/120    avg_loss:0.008, val_acc:0.988]
Epoch [65/120    avg_loss:0.007, val_acc:0.987]
Epoch [66/120    avg_loss:0.006, val_acc:0.973]
Epoch [67/120    avg_loss:0.007, val_acc:0.986]
Epoch [68/120    avg_loss:0.010, val_acc:0.986]
Epoch [69/120    avg_loss:0.005, val_acc:0.989]
Epoch [70/120    avg_loss:0.004, val_acc:0.990]
Epoch [71/120    avg_loss:0.011, val_acc:0.983]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.008, val_acc:0.987]
Epoch [75/120    avg_loss:0.004, val_acc:0.988]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.987]
Epoch [79/120    avg_loss:0.008, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.989]
Epoch [81/120    avg_loss:0.005, val_acc:0.989]
Epoch [82/120    avg_loss:0.005, val_acc:0.991]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.002, val_acc:0.990]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.009, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.991]
Epoch [93/120    avg_loss:0.009, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.014, val_acc:0.990]
Epoch [97/120    avg_loss:0.017, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.015, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.014, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.993]
Epoch [112/120    avg_loss:0.004, val_acc:0.993]
Epoch [113/120    avg_loss:0.006, val_acc:0.993]
Epoch [114/120    avg_loss:0.006, val_acc:0.992]
Epoch [115/120    avg_loss:0.004, val_acc:0.992]
Epoch [116/120    avg_loss:0.002, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.993]
Epoch [119/120    avg_loss:0.003, val_acc:0.993]
Epoch [120/120    avg_loss:0.002, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     8     0     0     3     0     7     9]
 [    0     0 18058     0    11     0    17     0     4     0]
 [    0     1     0  1975     0     0     0     0    58     2]
 [    0    13     0     1  2946     0     1     0     6     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4852     0    26     0]
 [    0     5     0     0     0     0     5  1274     0     6]
 [    0     2     0    87    39     0     0     0  3442     1]
 [    0     0     0     0     0    12     0     0     0   907]]

Accuracy:
99.20709517267973

F1 scores:
[       nan 0.99626692 0.99911475 0.96177258 0.98726542 0.99542334
 0.99466995 0.99375975 0.96766938 0.98107085]

Kappa:
0.9894981371086492
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2466b61748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.305, val_acc:0.737]
Epoch [2/120    avg_loss:0.766, val_acc:0.656]
Epoch [3/120    avg_loss:0.475, val_acc:0.720]
Epoch [4/120    avg_loss:0.430, val_acc:0.770]
Epoch [5/120    avg_loss:0.329, val_acc:0.797]
Epoch [6/120    avg_loss:0.280, val_acc:0.881]
Epoch [7/120    avg_loss:0.247, val_acc:0.904]
Epoch [8/120    avg_loss:0.237, val_acc:0.880]
Epoch [9/120    avg_loss:0.186, val_acc:0.900]
Epoch [10/120    avg_loss:0.140, val_acc:0.934]
Epoch [11/120    avg_loss:0.152, val_acc:0.919]
Epoch [12/120    avg_loss:0.141, val_acc:0.914]
Epoch [13/120    avg_loss:0.095, val_acc:0.948]
Epoch [14/120    avg_loss:0.073, val_acc:0.928]
Epoch [15/120    avg_loss:0.091, val_acc:0.912]
Epoch [16/120    avg_loss:0.120, val_acc:0.910]
Epoch [17/120    avg_loss:0.073, val_acc:0.940]
Epoch [18/120    avg_loss:0.107, val_acc:0.947]
Epoch [19/120    avg_loss:0.066, val_acc:0.952]
Epoch [20/120    avg_loss:0.071, val_acc:0.964]
Epoch [21/120    avg_loss:0.055, val_acc:0.958]
Epoch [22/120    avg_loss:0.033, val_acc:0.965]
Epoch [23/120    avg_loss:0.107, val_acc:0.936]
Epoch [24/120    avg_loss:0.132, val_acc:0.942]
Epoch [25/120    avg_loss:0.069, val_acc:0.947]
Epoch [26/120    avg_loss:0.050, val_acc:0.973]
Epoch [27/120    avg_loss:0.038, val_acc:0.971]
Epoch [28/120    avg_loss:0.031, val_acc:0.969]
Epoch [29/120    avg_loss:0.029, val_acc:0.950]
Epoch [30/120    avg_loss:0.030, val_acc:0.961]
Epoch [31/120    avg_loss:0.060, val_acc:0.969]
Epoch [32/120    avg_loss:0.039, val_acc:0.970]
Epoch [33/120    avg_loss:0.025, val_acc:0.975]
Epoch [34/120    avg_loss:0.042, val_acc:0.959]
Epoch [35/120    avg_loss:0.038, val_acc:0.962]
Epoch [36/120    avg_loss:0.029, val_acc:0.974]
Epoch [37/120    avg_loss:0.033, val_acc:0.982]
Epoch [38/120    avg_loss:0.020, val_acc:0.975]
Epoch [39/120    avg_loss:0.016, val_acc:0.984]
Epoch [40/120    avg_loss:0.017, val_acc:0.974]
Epoch [41/120    avg_loss:0.013, val_acc:0.981]
Epoch [42/120    avg_loss:0.047, val_acc:0.975]
Epoch [43/120    avg_loss:0.030, val_acc:0.974]
Epoch [44/120    avg_loss:0.022, val_acc:0.977]
Epoch [45/120    avg_loss:0.018, val_acc:0.984]
Epoch [46/120    avg_loss:0.017, val_acc:0.987]
Epoch [47/120    avg_loss:0.012, val_acc:0.979]
Epoch [48/120    avg_loss:0.017, val_acc:0.983]
Epoch [49/120    avg_loss:0.015, val_acc:0.982]
Epoch [50/120    avg_loss:0.011, val_acc:0.980]
Epoch [51/120    avg_loss:0.020, val_acc:0.975]
Epoch [52/120    avg_loss:0.013, val_acc:0.988]
Epoch [53/120    avg_loss:0.016, val_acc:0.948]
Epoch [54/120    avg_loss:0.023, val_acc:0.982]
Epoch [55/120    avg_loss:0.014, val_acc:0.967]
Epoch [56/120    avg_loss:0.011, val_acc:0.984]
Epoch [57/120    avg_loss:0.007, val_acc:0.984]
Epoch [58/120    avg_loss:0.020, val_acc:0.984]
Epoch [59/120    avg_loss:0.029, val_acc:0.984]
Epoch [60/120    avg_loss:0.012, val_acc:0.984]
Epoch [61/120    avg_loss:0.007, val_acc:0.987]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.014, val_acc:0.977]
Epoch [64/120    avg_loss:0.027, val_acc:0.981]
Epoch [65/120    avg_loss:0.024, val_acc:0.965]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.005, val_acc:0.988]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.983]
Epoch [70/120    avg_loss:0.004, val_acc:0.986]
Epoch [71/120    avg_loss:0.008, val_acc:0.987]
Epoch [72/120    avg_loss:0.006, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.008, val_acc:0.987]
Epoch [75/120    avg_loss:0.008, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.982]
Epoch [77/120    avg_loss:0.009, val_acc:0.984]
Epoch [78/120    avg_loss:0.004, val_acc:0.985]
Epoch [79/120    avg_loss:0.005, val_acc:0.983]
Epoch [80/120    avg_loss:0.004, val_acc:0.987]
Epoch [81/120    avg_loss:0.003, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.004, val_acc:0.988]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.005, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.004, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.003, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.002, val_acc:0.989]
Epoch [97/120    avg_loss:0.003, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.989]
Epoch [100/120    avg_loss:0.003, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.989]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.003, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.003, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.003, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.002, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     9     0     0     0     0    28     0]
 [    0     0 18077     0     2     0     7     0     4     0]
 [    0     1     0  1947     0     0     0     0    87     1]
 [    0    17     0     0  2949     0     1     0     1     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0    16     0     0     0     0     0  1273     0     1]
 [    0     5     0    84    49     0     0     0  3433     0]
 [    0     0     0     0     0    11     0     0     0   908]]

Accuracy:
99.20709517267973

F1 scores:
[       nan 0.99409296 0.99961292 0.95534838 0.98760884 0.99580313
 0.99907815 0.99336715 0.96378439 0.99072559]

Kappa:
0.9894947865742971
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6446075748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.295, val_acc:0.423]
Epoch [2/120    avg_loss:0.775, val_acc:0.604]
Epoch [3/120    avg_loss:0.518, val_acc:0.634]
Epoch [4/120    avg_loss:0.394, val_acc:0.785]
Epoch [5/120    avg_loss:0.355, val_acc:0.774]
Epoch [6/120    avg_loss:0.258, val_acc:0.910]
Epoch [7/120    avg_loss:0.226, val_acc:0.863]
Epoch [8/120    avg_loss:0.186, val_acc:0.845]
Epoch [9/120    avg_loss:0.199, val_acc:0.921]
Epoch [10/120    avg_loss:0.170, val_acc:0.947]
Epoch [11/120    avg_loss:0.119, val_acc:0.933]
Epoch [12/120    avg_loss:0.137, val_acc:0.928]
Epoch [13/120    avg_loss:0.140, val_acc:0.906]
Epoch [14/120    avg_loss:0.092, val_acc:0.955]
Epoch [15/120    avg_loss:0.089, val_acc:0.965]
Epoch [16/120    avg_loss:0.147, val_acc:0.933]
Epoch [17/120    avg_loss:0.157, val_acc:0.956]
Epoch [18/120    avg_loss:0.143, val_acc:0.965]
Epoch [19/120    avg_loss:0.113, val_acc:0.958]
Epoch [20/120    avg_loss:0.084, val_acc:0.964]
Epoch [21/120    avg_loss:0.076, val_acc:0.918]
Epoch [22/120    avg_loss:0.063, val_acc:0.956]
Epoch [23/120    avg_loss:0.073, val_acc:0.882]
Epoch [24/120    avg_loss:0.070, val_acc:0.963]
Epoch [25/120    avg_loss:0.038, val_acc:0.975]
Epoch [26/120    avg_loss:0.051, val_acc:0.965]
Epoch [27/120    avg_loss:0.047, val_acc:0.968]
Epoch [28/120    avg_loss:0.037, val_acc:0.943]
Epoch [29/120    avg_loss:0.029, val_acc:0.967]
Epoch [30/120    avg_loss:0.053, val_acc:0.972]
Epoch [31/120    avg_loss:0.032, val_acc:0.967]
Epoch [32/120    avg_loss:0.028, val_acc:0.975]
Epoch [33/120    avg_loss:0.019, val_acc:0.970]
Epoch [34/120    avg_loss:0.039, val_acc:0.969]
Epoch [35/120    avg_loss:0.027, val_acc:0.964]
Epoch [36/120    avg_loss:0.017, val_acc:0.978]
Epoch [37/120    avg_loss:0.016, val_acc:0.976]
Epoch [38/120    avg_loss:0.031, val_acc:0.977]
Epoch [39/120    avg_loss:0.034, val_acc:0.966]
Epoch [40/120    avg_loss:0.017, val_acc:0.965]
Epoch [41/120    avg_loss:0.034, val_acc:0.968]
Epoch [42/120    avg_loss:0.028, val_acc:0.974]
Epoch [43/120    avg_loss:0.034, val_acc:0.979]
Epoch [44/120    avg_loss:0.017, val_acc:0.981]
Epoch [45/120    avg_loss:0.017, val_acc:0.976]
Epoch [46/120    avg_loss:0.016, val_acc:0.982]
Epoch [47/120    avg_loss:0.011, val_acc:0.985]
Epoch [48/120    avg_loss:0.018, val_acc:0.979]
Epoch [49/120    avg_loss:0.018, val_acc:0.971]
Epoch [50/120    avg_loss:0.015, val_acc:0.979]
Epoch [51/120    avg_loss:0.012, val_acc:0.981]
Epoch [52/120    avg_loss:0.009, val_acc:0.980]
Epoch [53/120    avg_loss:0.008, val_acc:0.983]
Epoch [54/120    avg_loss:0.011, val_acc:0.980]
Epoch [55/120    avg_loss:0.006, val_acc:0.985]
Epoch [56/120    avg_loss:0.025, val_acc:0.983]
Epoch [57/120    avg_loss:0.021, val_acc:0.978]
Epoch [58/120    avg_loss:0.007, val_acc:0.979]
Epoch [59/120    avg_loss:0.011, val_acc:0.982]
Epoch [60/120    avg_loss:0.008, val_acc:0.982]
Epoch [61/120    avg_loss:0.006, val_acc:0.981]
Epoch [62/120    avg_loss:0.009, val_acc:0.973]
Epoch [63/120    avg_loss:0.012, val_acc:0.983]
Epoch [64/120    avg_loss:0.007, val_acc:0.984]
Epoch [65/120    avg_loss:0.004, val_acc:0.983]
Epoch [66/120    avg_loss:0.042, val_acc:0.967]
Epoch [67/120    avg_loss:0.014, val_acc:0.975]
Epoch [68/120    avg_loss:0.006, val_acc:0.984]
Epoch [69/120    avg_loss:0.006, val_acc:0.985]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.983]
Epoch [72/120    avg_loss:0.006, val_acc:0.983]
Epoch [73/120    avg_loss:0.005, val_acc:0.983]
Epoch [74/120    avg_loss:0.006, val_acc:0.983]
Epoch [75/120    avg_loss:0.005, val_acc:0.984]
Epoch [76/120    avg_loss:0.005, val_acc:0.984]
Epoch [77/120    avg_loss:0.006, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.983]
Epoch [79/120    avg_loss:0.004, val_acc:0.982]
Epoch [80/120    avg_loss:0.006, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.982]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.007, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.982]
Epoch [85/120    avg_loss:0.005, val_acc:0.983]
Epoch [86/120    avg_loss:0.005, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.982]
Epoch [89/120    avg_loss:0.007, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.010, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.005, val_acc:0.982]
Epoch [101/120    avg_loss:0.007, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.004, val_acc:0.982]
Epoch [104/120    avg_loss:0.004, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.006, val_acc:0.982]
Epoch [107/120    avg_loss:0.005, val_acc:0.982]
Epoch [108/120    avg_loss:0.004, val_acc:0.982]
Epoch [109/120    avg_loss:0.005, val_acc:0.982]
Epoch [110/120    avg_loss:0.005, val_acc:0.982]
Epoch [111/120    avg_loss:0.005, val_acc:0.982]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.005, val_acc:0.982]
Epoch [114/120    avg_loss:0.004, val_acc:0.982]
Epoch [115/120    avg_loss:0.006, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.004, val_acc:0.982]
Epoch [118/120    avg_loss:0.006, val_acc:0.982]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     4     0     0     0     0     7     5]
 [    0     0 18037     0     9     0    38     0     6     0]
 [    0     2     0  1968     0     0     0     0    64     2]
 [    0    16     7     3  2938     0     1     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4872     0     0     0]
 [    0    32     0     0     0     0     0  1253     0     5]
 [    0     9     0    42    47     0     0     0  3472     1]
 [    0     0     0     0     0     9     0     0     0   910]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.9941892  0.99817377 0.97113249 0.98491452 0.99656357
 0.995403   0.98545026 0.97459649 0.98698482]

Kappa:
0.9897198473339146
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0adeb97748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.369, val_acc:0.595]
Epoch [2/120    avg_loss:0.722, val_acc:0.698]
Epoch [3/120    avg_loss:0.549, val_acc:0.760]
Epoch [4/120    avg_loss:0.446, val_acc:0.756]
Epoch [5/120    avg_loss:0.342, val_acc:0.832]
Epoch [6/120    avg_loss:0.281, val_acc:0.827]
Epoch [7/120    avg_loss:0.230, val_acc:0.896]
Epoch [8/120    avg_loss:0.219, val_acc:0.896]
Epoch [9/120    avg_loss:0.154, val_acc:0.908]
Epoch [10/120    avg_loss:0.186, val_acc:0.840]
Epoch [11/120    avg_loss:0.182, val_acc:0.902]
Epoch [12/120    avg_loss:0.162, val_acc:0.921]
Epoch [13/120    avg_loss:0.241, val_acc:0.931]
Epoch [14/120    avg_loss:0.121, val_acc:0.934]
Epoch [15/120    avg_loss:0.102, val_acc:0.960]
Epoch [16/120    avg_loss:0.077, val_acc:0.942]
Epoch [17/120    avg_loss:0.078, val_acc:0.938]
Epoch [18/120    avg_loss:0.052, val_acc:0.950]
Epoch [19/120    avg_loss:0.066, val_acc:0.961]
Epoch [20/120    avg_loss:0.062, val_acc:0.950]
Epoch [21/120    avg_loss:0.058, val_acc:0.973]
Epoch [22/120    avg_loss:0.049, val_acc:0.969]
Epoch [23/120    avg_loss:0.043, val_acc:0.963]
Epoch [24/120    avg_loss:0.039, val_acc:0.965]
Epoch [25/120    avg_loss:0.036, val_acc:0.978]
Epoch [26/120    avg_loss:0.018, val_acc:0.979]
Epoch [27/120    avg_loss:0.041, val_acc:0.969]
Epoch [28/120    avg_loss:0.045, val_acc:0.937]
Epoch [29/120    avg_loss:0.064, val_acc:0.953]
Epoch [30/120    avg_loss:0.053, val_acc:0.976]
Epoch [31/120    avg_loss:0.054, val_acc:0.970]
Epoch [32/120    avg_loss:0.038, val_acc:0.979]
Epoch [33/120    avg_loss:0.028, val_acc:0.981]
Epoch [34/120    avg_loss:0.021, val_acc:0.986]
Epoch [35/120    avg_loss:0.035, val_acc:0.969]
Epoch [36/120    avg_loss:0.025, val_acc:0.984]
Epoch [37/120    avg_loss:0.014, val_acc:0.987]
Epoch [38/120    avg_loss:0.010, val_acc:0.987]
Epoch [39/120    avg_loss:0.015, val_acc:0.983]
Epoch [40/120    avg_loss:0.025, val_acc:0.976]
Epoch [41/120    avg_loss:0.026, val_acc:0.978]
Epoch [42/120    avg_loss:0.032, val_acc:0.970]
Epoch [43/120    avg_loss:0.018, val_acc:0.984]
Epoch [44/120    avg_loss:0.012, val_acc:0.982]
Epoch [45/120    avg_loss:0.008, val_acc:0.983]
Epoch [46/120    avg_loss:0.021, val_acc:0.964]
Epoch [47/120    avg_loss:0.027, val_acc:0.983]
Epoch [48/120    avg_loss:0.018, val_acc:0.987]
Epoch [49/120    avg_loss:0.018, val_acc:0.971]
Epoch [50/120    avg_loss:0.032, val_acc:0.976]
Epoch [51/120    avg_loss:0.023, val_acc:0.981]
Epoch [52/120    avg_loss:0.014, val_acc:0.983]
Epoch [53/120    avg_loss:0.012, val_acc:0.983]
Epoch [54/120    avg_loss:0.018, val_acc:0.984]
Epoch [55/120    avg_loss:0.009, val_acc:0.986]
Epoch [56/120    avg_loss:0.013, val_acc:0.982]
Epoch [57/120    avg_loss:0.015, val_acc:0.974]
Epoch [58/120    avg_loss:0.013, val_acc:0.985]
Epoch [59/120    avg_loss:0.014, val_acc:0.986]
Epoch [60/120    avg_loss:0.019, val_acc:0.977]
Epoch [61/120    avg_loss:0.028, val_acc:0.979]
Epoch [62/120    avg_loss:0.011, val_acc:0.986]
Epoch [63/120    avg_loss:0.014, val_acc:0.989]
Epoch [64/120    avg_loss:0.009, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.010, val_acc:0.986]
Epoch [67/120    avg_loss:0.012, val_acc:0.985]
Epoch [68/120    avg_loss:0.010, val_acc:0.986]
Epoch [69/120    avg_loss:0.013, val_acc:0.987]
Epoch [70/120    avg_loss:0.006, val_acc:0.987]
Epoch [71/120    avg_loss:0.005, val_acc:0.986]
Epoch [72/120    avg_loss:0.005, val_acc:0.986]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.005, val_acc:0.986]
Epoch [75/120    avg_loss:0.009, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.009, val_acc:0.987]
Epoch [78/120    avg_loss:0.010, val_acc:0.987]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.004, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.004, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.005, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.011, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     1     1     0     0     0    30     6]
 [    0     2 18058     0    12     0    15     0     3     0]
 [    0     0     0  2015     0     0     0     0    19     2]
 [    0    21     1     0  2946     0     1     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4873     0     0     0]
 [    0     8     0     0     0     0     2  1273     0     7]
 [    0     9     0    28    68     0     0     0  3463     3]
 [    0     0     0     0     0    13     0     0     0   906]]

Accuracy:
99.37338828236088

F1 scores:
[       nan 0.99393751 0.99894894 0.9877451  0.98216369 0.99504384
 0.99764561 0.99336715 0.97700663 0.9831796 ]

Kappa:
0.991699596775709
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe220d3f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.393, val_acc:0.408]
Epoch [2/120    avg_loss:0.775, val_acc:0.593]
Epoch [3/120    avg_loss:0.591, val_acc:0.687]
Epoch [4/120    avg_loss:0.458, val_acc:0.755]
Epoch [5/120    avg_loss:0.344, val_acc:0.842]
Epoch [6/120    avg_loss:0.317, val_acc:0.800]
Epoch [7/120    avg_loss:0.273, val_acc:0.711]
Epoch [8/120    avg_loss:0.250, val_acc:0.899]
Epoch [9/120    avg_loss:0.163, val_acc:0.867]
Epoch [10/120    avg_loss:0.155, val_acc:0.893]
Epoch [11/120    avg_loss:0.148, val_acc:0.885]
Epoch [12/120    avg_loss:0.106, val_acc:0.931]
Epoch [13/120    avg_loss:0.081, val_acc:0.945]
Epoch [14/120    avg_loss:0.110, val_acc:0.932]
Epoch [15/120    avg_loss:0.116, val_acc:0.941]
Epoch [16/120    avg_loss:0.071, val_acc:0.946]
Epoch [17/120    avg_loss:0.053, val_acc:0.958]
Epoch [18/120    avg_loss:0.074, val_acc:0.946]
Epoch [19/120    avg_loss:0.070, val_acc:0.940]
Epoch [20/120    avg_loss:0.069, val_acc:0.940]
Epoch [21/120    avg_loss:0.067, val_acc:0.935]
Epoch [22/120    avg_loss:0.081, val_acc:0.966]
Epoch [23/120    avg_loss:0.054, val_acc:0.965]
Epoch [24/120    avg_loss:0.036, val_acc:0.968]
Epoch [25/120    avg_loss:0.029, val_acc:0.973]
Epoch [26/120    avg_loss:0.031, val_acc:0.930]
Epoch [27/120    avg_loss:0.036, val_acc:0.962]
Epoch [28/120    avg_loss:0.032, val_acc:0.982]
Epoch [29/120    avg_loss:0.031, val_acc:0.974]
Epoch [30/120    avg_loss:0.090, val_acc:0.968]
Epoch [31/120    avg_loss:0.054, val_acc:0.957]
Epoch [32/120    avg_loss:0.043, val_acc:0.954]
Epoch [33/120    avg_loss:0.022, val_acc:0.977]
Epoch [34/120    avg_loss:0.028, val_acc:0.954]
Epoch [35/120    avg_loss:0.039, val_acc:0.968]
Epoch [36/120    avg_loss:0.025, val_acc:0.980]
Epoch [37/120    avg_loss:0.016, val_acc:0.977]
Epoch [38/120    avg_loss:0.010, val_acc:0.979]
Epoch [39/120    avg_loss:0.020, val_acc:0.967]
Epoch [40/120    avg_loss:0.017, val_acc:0.980]
Epoch [41/120    avg_loss:0.014, val_acc:0.976]
Epoch [42/120    avg_loss:0.018, val_acc:0.981]
Epoch [43/120    avg_loss:0.010, val_acc:0.984]
Epoch [44/120    avg_loss:0.010, val_acc:0.984]
Epoch [45/120    avg_loss:0.015, val_acc:0.985]
Epoch [46/120    avg_loss:0.009, val_acc:0.985]
Epoch [47/120    avg_loss:0.008, val_acc:0.985]
Epoch [48/120    avg_loss:0.011, val_acc:0.985]
Epoch [49/120    avg_loss:0.009, val_acc:0.986]
Epoch [50/120    avg_loss:0.009, val_acc:0.985]
Epoch [51/120    avg_loss:0.007, val_acc:0.984]
Epoch [52/120    avg_loss:0.007, val_acc:0.984]
Epoch [53/120    avg_loss:0.007, val_acc:0.984]
Epoch [54/120    avg_loss:0.009, val_acc:0.986]
Epoch [55/120    avg_loss:0.008, val_acc:0.986]
Epoch [56/120    avg_loss:0.013, val_acc:0.986]
Epoch [57/120    avg_loss:0.005, val_acc:0.986]
Epoch [58/120    avg_loss:0.005, val_acc:0.986]
Epoch [59/120    avg_loss:0.006, val_acc:0.986]
Epoch [60/120    avg_loss:0.006, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.006, val_acc:0.986]
Epoch [63/120    avg_loss:0.011, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.985]
Epoch [65/120    avg_loss:0.004, val_acc:0.985]
Epoch [66/120    avg_loss:0.005, val_acc:0.985]
Epoch [67/120    avg_loss:0.008, val_acc:0.986]
Epoch [68/120    avg_loss:0.004, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.006, val_acc:0.984]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.005, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.983]
Epoch [74/120    avg_loss:0.009, val_acc:0.983]
Epoch [75/120    avg_loss:0.005, val_acc:0.983]
Epoch [76/120    avg_loss:0.006, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.004, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.004, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.013, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.012, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     2     0     0     1     1     9     8]
 [    0     2 18072     0     3     0    10     0     3     0]
 [    0     0     0  2011     0     0     0     0    21     4]
 [    0    22     0     6  2928     0     1     0    12     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4867     0     8     0]
 [    0     0     0     0     0     0     4  1278     0     8]
 [    0     2     0    15    25     0     0     0  3526     3]
 [    0     0     0     0     0    25     0     0     0   894]]

Accuracy:
99.51558094136361

F1 scores:
[       nan 0.99634781 0.99941933 0.98820639 0.98785425 0.99051233
 0.99723389 0.99493967 0.98629371 0.97226754]

Kappa:
0.9935822341218346
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2b091c2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.331, val_acc:0.719]
Epoch [2/120    avg_loss:0.763, val_acc:0.703]
Epoch [3/120    avg_loss:0.533, val_acc:0.764]
Epoch [4/120    avg_loss:0.437, val_acc:0.723]
Epoch [5/120    avg_loss:0.333, val_acc:0.843]
Epoch [6/120    avg_loss:0.270, val_acc:0.847]
Epoch [7/120    avg_loss:0.228, val_acc:0.891]
Epoch [8/120    avg_loss:0.184, val_acc:0.909]
Epoch [9/120    avg_loss:0.218, val_acc:0.921]
Epoch [10/120    avg_loss:0.238, val_acc:0.914]
Epoch [11/120    avg_loss:0.133, val_acc:0.910]
Epoch [12/120    avg_loss:0.125, val_acc:0.952]
Epoch [13/120    avg_loss:0.089, val_acc:0.962]
Epoch [14/120    avg_loss:0.119, val_acc:0.924]
Epoch [15/120    avg_loss:0.083, val_acc:0.958]
Epoch [16/120    avg_loss:0.072, val_acc:0.962]
Epoch [17/120    avg_loss:0.052, val_acc:0.923]
Epoch [18/120    avg_loss:0.061, val_acc:0.949]
Epoch [19/120    avg_loss:0.060, val_acc:0.965]
Epoch [20/120    avg_loss:0.058, val_acc:0.954]
Epoch [21/120    avg_loss:0.044, val_acc:0.966]
Epoch [22/120    avg_loss:0.054, val_acc:0.967]
Epoch [23/120    avg_loss:0.041, val_acc:0.970]
Epoch [24/120    avg_loss:0.052, val_acc:0.959]
Epoch [25/120    avg_loss:0.046, val_acc:0.950]
Epoch [26/120    avg_loss:0.048, val_acc:0.943]
Epoch [27/120    avg_loss:0.061, val_acc:0.968]
Epoch [28/120    avg_loss:0.075, val_acc:0.923]
Epoch [29/120    avg_loss:0.050, val_acc:0.968]
Epoch [30/120    avg_loss:0.042, val_acc:0.970]
Epoch [31/120    avg_loss:0.041, val_acc:0.971]
Epoch [32/120    avg_loss:0.018, val_acc:0.976]
Epoch [33/120    avg_loss:0.031, val_acc:0.960]
Epoch [34/120    avg_loss:0.038, val_acc:0.954]
Epoch [35/120    avg_loss:0.028, val_acc:0.976]
Epoch [36/120    avg_loss:0.028, val_acc:0.971]
Epoch [37/120    avg_loss:0.025, val_acc:0.954]
Epoch [38/120    avg_loss:0.014, val_acc:0.981]
Epoch [39/120    avg_loss:0.033, val_acc:0.966]
Epoch [40/120    avg_loss:0.040, val_acc:0.958]
Epoch [41/120    avg_loss:0.029, val_acc:0.979]
Epoch [42/120    avg_loss:0.014, val_acc:0.977]
Epoch [43/120    avg_loss:0.026, val_acc:0.981]
Epoch [44/120    avg_loss:0.016, val_acc:0.978]
Epoch [45/120    avg_loss:0.010, val_acc:0.981]
Epoch [46/120    avg_loss:0.015, val_acc:0.984]
Epoch [47/120    avg_loss:0.012, val_acc:0.974]
Epoch [48/120    avg_loss:0.014, val_acc:0.980]
Epoch [49/120    avg_loss:0.020, val_acc:0.981]
Epoch [50/120    avg_loss:0.013, val_acc:0.980]
Epoch [51/120    avg_loss:0.015, val_acc:0.979]
Epoch [52/120    avg_loss:0.011, val_acc:0.981]
Epoch [53/120    avg_loss:0.009, val_acc:0.982]
Epoch [54/120    avg_loss:0.031, val_acc:0.986]
Epoch [55/120    avg_loss:0.015, val_acc:0.983]
Epoch [56/120    avg_loss:0.031, val_acc:0.957]
Epoch [57/120    avg_loss:0.023, val_acc:0.976]
Epoch [58/120    avg_loss:0.013, val_acc:0.981]
Epoch [59/120    avg_loss:0.014, val_acc:0.976]
Epoch [60/120    avg_loss:0.008, val_acc:0.985]
Epoch [61/120    avg_loss:0.008, val_acc:0.981]
Epoch [62/120    avg_loss:0.010, val_acc:0.983]
Epoch [63/120    avg_loss:0.024, val_acc:0.983]
Epoch [64/120    avg_loss:0.019, val_acc:0.985]
Epoch [65/120    avg_loss:0.016, val_acc:0.972]
Epoch [66/120    avg_loss:0.046, val_acc:0.974]
Epoch [67/120    avg_loss:0.016, val_acc:0.978]
Epoch [68/120    avg_loss:0.012, val_acc:0.980]
Epoch [69/120    avg_loss:0.007, val_acc:0.980]
Epoch [70/120    avg_loss:0.007, val_acc:0.979]
Epoch [71/120    avg_loss:0.008, val_acc:0.979]
Epoch [72/120    avg_loss:0.008, val_acc:0.979]
Epoch [73/120    avg_loss:0.007, val_acc:0.978]
Epoch [74/120    avg_loss:0.006, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.007, val_acc:0.980]
Epoch [77/120    avg_loss:0.009, val_acc:0.981]
Epoch [78/120    avg_loss:0.008, val_acc:0.981]
Epoch [79/120    avg_loss:0.007, val_acc:0.981]
Epoch [80/120    avg_loss:0.005, val_acc:0.981]
Epoch [81/120    avg_loss:0.006, val_acc:0.981]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.011, val_acc:0.981]
Epoch [85/120    avg_loss:0.008, val_acc:0.981]
Epoch [86/120    avg_loss:0.005, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.006, val_acc:0.981]
Epoch [90/120    avg_loss:0.009, val_acc:0.981]
Epoch [91/120    avg_loss:0.005, val_acc:0.980]
Epoch [92/120    avg_loss:0.005, val_acc:0.980]
Epoch [93/120    avg_loss:0.005, val_acc:0.980]
Epoch [94/120    avg_loss:0.005, val_acc:0.980]
Epoch [95/120    avg_loss:0.008, val_acc:0.980]
Epoch [96/120    avg_loss:0.005, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.980]
Epoch [99/120    avg_loss:0.005, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.006, val_acc:0.980]
Epoch [102/120    avg_loss:0.004, val_acc:0.980]
Epoch [103/120    avg_loss:0.010, val_acc:0.980]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.015, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.980]
Epoch [107/120    avg_loss:0.005, val_acc:0.980]
Epoch [108/120    avg_loss:0.006, val_acc:0.980]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.004, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.007, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     1     1     0     0    10    13    12]
 [    0     0 18068     0     6     0    11     0     3     2]
 [    0     0     0  1989     0     0     0     0    38     9]
 [    0    25     1     0  2932     0     0     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0     5     0     0     0     0     2  1277     0     6]
 [    0    18     0     6    42     0     0     0  3495    10]
 [    0     0     0     0     3    28     0     0     0   888]]

Accuracy:
99.3516978767503

F1 scores:
[       nan 0.99339806 0.99936392 0.98660714 0.98455339 0.9893859
 0.99836166 0.99107489 0.97954036 0.96155929]

Kappa:
0.9914114998252467
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51415b6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.368, val_acc:0.701]
Epoch [2/120    avg_loss:0.759, val_acc:0.697]
Epoch [3/120    avg_loss:0.536, val_acc:0.702]
Epoch [4/120    avg_loss:0.367, val_acc:0.764]
Epoch [5/120    avg_loss:0.312, val_acc:0.822]
Epoch [6/120    avg_loss:0.227, val_acc:0.843]
Epoch [7/120    avg_loss:0.213, val_acc:0.878]
Epoch [8/120    avg_loss:0.255, val_acc:0.863]
Epoch [9/120    avg_loss:0.202, val_acc:0.902]
Epoch [10/120    avg_loss:0.153, val_acc:0.934]
Epoch [11/120    avg_loss:0.155, val_acc:0.945]
Epoch [12/120    avg_loss:0.134, val_acc:0.926]
Epoch [13/120    avg_loss:0.114, val_acc:0.945]
Epoch [14/120    avg_loss:0.068, val_acc:0.962]
Epoch [15/120    avg_loss:0.056, val_acc:0.964]
Epoch [16/120    avg_loss:0.066, val_acc:0.961]
Epoch [17/120    avg_loss:0.058, val_acc:0.955]
Epoch [18/120    avg_loss:0.062, val_acc:0.968]
Epoch [19/120    avg_loss:0.071, val_acc:0.966]
Epoch [20/120    avg_loss:0.057, val_acc:0.948]
Epoch [21/120    avg_loss:0.034, val_acc:0.968]
Epoch [22/120    avg_loss:0.043, val_acc:0.970]
Epoch [23/120    avg_loss:0.050, val_acc:0.969]
Epoch [24/120    avg_loss:0.030, val_acc:0.961]
Epoch [25/120    avg_loss:0.048, val_acc:0.961]
Epoch [26/120    avg_loss:0.057, val_acc:0.965]
Epoch [27/120    avg_loss:0.034, val_acc:0.974]
Epoch [28/120    avg_loss:0.031, val_acc:0.967]
Epoch [29/120    avg_loss:0.036, val_acc:0.978]
Epoch [30/120    avg_loss:0.039, val_acc:0.980]
Epoch [31/120    avg_loss:0.026, val_acc:0.974]
Epoch [32/120    avg_loss:0.019, val_acc:0.983]
Epoch [33/120    avg_loss:0.025, val_acc:0.975]
Epoch [34/120    avg_loss:0.037, val_acc:0.981]
Epoch [35/120    avg_loss:0.018, val_acc:0.981]
Epoch [36/120    avg_loss:0.030, val_acc:0.976]
Epoch [37/120    avg_loss:0.015, val_acc:0.984]
Epoch [38/120    avg_loss:0.013, val_acc:0.981]
Epoch [39/120    avg_loss:0.024, val_acc:0.976]
Epoch [40/120    avg_loss:0.031, val_acc:0.958]
Epoch [41/120    avg_loss:0.028, val_acc:0.971]
Epoch [42/120    avg_loss:0.020, val_acc:0.978]
Epoch [43/120    avg_loss:0.015, val_acc:0.983]
Epoch [44/120    avg_loss:0.012, val_acc:0.985]
Epoch [45/120    avg_loss:0.008, val_acc:0.986]
Epoch [46/120    avg_loss:0.020, val_acc:0.982]
Epoch [47/120    avg_loss:0.044, val_acc:0.964]
Epoch [48/120    avg_loss:0.029, val_acc:0.979]
Epoch [49/120    avg_loss:0.009, val_acc:0.983]
Epoch [50/120    avg_loss:0.016, val_acc:0.980]
Epoch [51/120    avg_loss:0.013, val_acc:0.983]
Epoch [52/120    avg_loss:0.007, val_acc:0.986]
Epoch [53/120    avg_loss:0.009, val_acc:0.985]
Epoch [54/120    avg_loss:0.011, val_acc:0.985]
Epoch [55/120    avg_loss:0.020, val_acc:0.982]
Epoch [56/120    avg_loss:0.019, val_acc:0.990]
Epoch [57/120    avg_loss:0.014, val_acc:0.985]
Epoch [58/120    avg_loss:0.018, val_acc:0.978]
Epoch [59/120    avg_loss:0.014, val_acc:0.980]
Epoch [60/120    avg_loss:0.015, val_acc:0.970]
Epoch [61/120    avg_loss:0.024, val_acc:0.981]
Epoch [62/120    avg_loss:0.021, val_acc:0.975]
Epoch [63/120    avg_loss:0.021, val_acc:0.985]
Epoch [64/120    avg_loss:0.010, val_acc:0.987]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.016, val_acc:0.976]
Epoch [67/120    avg_loss:0.009, val_acc:0.986]
Epoch [68/120    avg_loss:0.012, val_acc:0.980]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.005, val_acc:0.987]
Epoch [72/120    avg_loss:0.005, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.989]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.004, val_acc:0.988]
Epoch [78/120    avg_loss:0.004, val_acc:0.986]
Epoch [79/120    avg_loss:0.003, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.004, val_acc:0.986]
Epoch [82/120    avg_loss:0.005, val_acc:0.986]
Epoch [83/120    avg_loss:0.005, val_acc:0.986]
Epoch [84/120    avg_loss:0.003, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.004, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.003, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.004, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.003, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.003, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6430     0     0     0     0     0     0     0     2]
 [    0     5 18060     0    14     0     7     0     3     1]
 [    0     5     0  2003     1     0     0     0    24     3]
 [    0    27     0     2  2931     0     1     0     9     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0     4     0     0     0     0     2  1280     0     4]
 [    0     5     0     7    41     0     0     0  3515     3]
 [    0     0     0     0     1    23     0     0     0   895]]

Accuracy:
99.52281107656714

F1 scores:
[       nan 0.99628138 0.99917012 0.98962451 0.98355705 0.99126472
 0.998771   0.99610895 0.98680517 0.97867687]

Kappa:
0.9936783082550135
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf3ec9f780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.431, val_acc:0.652]
Epoch [2/120    avg_loss:0.826, val_acc:0.730]
Epoch [3/120    avg_loss:0.610, val_acc:0.687]
Epoch [4/120    avg_loss:0.476, val_acc:0.797]
Epoch [5/120    avg_loss:0.388, val_acc:0.821]
Epoch [6/120    avg_loss:0.386, val_acc:0.734]
Epoch [7/120    avg_loss:0.278, val_acc:0.759]
Epoch [8/120    avg_loss:0.249, val_acc:0.912]
Epoch [9/120    avg_loss:0.265, val_acc:0.813]
Epoch [10/120    avg_loss:0.179, val_acc:0.935]
Epoch [11/120    avg_loss:0.138, val_acc:0.944]
Epoch [12/120    avg_loss:0.143, val_acc:0.951]
Epoch [13/120    avg_loss:0.104, val_acc:0.933]
Epoch [14/120    avg_loss:0.102, val_acc:0.933]
Epoch [15/120    avg_loss:0.117, val_acc:0.946]
Epoch [16/120    avg_loss:0.096, val_acc:0.962]
Epoch [17/120    avg_loss:0.083, val_acc:0.948]
Epoch [18/120    avg_loss:0.078, val_acc:0.958]
Epoch [19/120    avg_loss:0.102, val_acc:0.959]
Epoch [20/120    avg_loss:0.055, val_acc:0.957]
Epoch [21/120    avg_loss:0.042, val_acc:0.961]
Epoch [22/120    avg_loss:0.060, val_acc:0.964]
Epoch [23/120    avg_loss:0.055, val_acc:0.963]
Epoch [24/120    avg_loss:0.044, val_acc:0.962]
Epoch [25/120    avg_loss:0.053, val_acc:0.964]
Epoch [26/120    avg_loss:0.035, val_acc:0.959]
Epoch [27/120    avg_loss:0.027, val_acc:0.973]
Epoch [28/120    avg_loss:0.034, val_acc:0.965]
Epoch [29/120    avg_loss:0.029, val_acc:0.973]
Epoch [30/120    avg_loss:0.037, val_acc:0.974]
Epoch [31/120    avg_loss:0.023, val_acc:0.969]
Epoch [32/120    avg_loss:0.042, val_acc:0.970]
Epoch [33/120    avg_loss:0.038, val_acc:0.970]
Epoch [34/120    avg_loss:0.059, val_acc:0.973]
Epoch [35/120    avg_loss:0.042, val_acc:0.973]
Epoch [36/120    avg_loss:0.025, val_acc:0.973]
Epoch [37/120    avg_loss:0.037, val_acc:0.970]
Epoch [38/120    avg_loss:0.031, val_acc:0.958]
Epoch [39/120    avg_loss:0.039, val_acc:0.970]
Epoch [40/120    avg_loss:0.058, val_acc:0.959]
Epoch [41/120    avg_loss:0.039, val_acc:0.955]
Epoch [42/120    avg_loss:0.045, val_acc:0.975]
Epoch [43/120    avg_loss:0.021, val_acc:0.977]
Epoch [44/120    avg_loss:0.021, val_acc:0.980]
Epoch [45/120    avg_loss:0.046, val_acc:0.967]
Epoch [46/120    avg_loss:0.017, val_acc:0.982]
Epoch [47/120    avg_loss:0.025, val_acc:0.974]
Epoch [48/120    avg_loss:0.014, val_acc:0.983]
Epoch [49/120    avg_loss:0.010, val_acc:0.982]
Epoch [50/120    avg_loss:0.017, val_acc:0.981]
Epoch [51/120    avg_loss:0.022, val_acc:0.964]
Epoch [52/120    avg_loss:0.024, val_acc:0.977]
Epoch [53/120    avg_loss:0.021, val_acc:0.976]
Epoch [54/120    avg_loss:0.013, val_acc:0.981]
Epoch [55/120    avg_loss:0.013, val_acc:0.981]
Epoch [56/120    avg_loss:0.010, val_acc:0.984]
Epoch [57/120    avg_loss:0.011, val_acc:0.984]
Epoch [58/120    avg_loss:0.010, val_acc:0.983]
Epoch [59/120    avg_loss:0.010, val_acc:0.978]
Epoch [60/120    avg_loss:0.009, val_acc:0.982]
Epoch [61/120    avg_loss:0.007, val_acc:0.984]
Epoch [62/120    avg_loss:0.016, val_acc:0.984]
Epoch [63/120    avg_loss:0.008, val_acc:0.986]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.006, val_acc:0.986]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.005, val_acc:0.986]
Epoch [68/120    avg_loss:0.006, val_acc:0.986]
Epoch [69/120    avg_loss:0.015, val_acc:0.986]
Epoch [70/120    avg_loss:0.029, val_acc:0.976]
Epoch [71/120    avg_loss:0.059, val_acc:0.965]
Epoch [72/120    avg_loss:0.023, val_acc:0.976]
Epoch [73/120    avg_loss:0.008, val_acc:0.984]
Epoch [74/120    avg_loss:0.008, val_acc:0.981]
Epoch [75/120    avg_loss:0.014, val_acc:0.986]
Epoch [76/120    avg_loss:0.014, val_acc:0.983]
Epoch [77/120    avg_loss:0.008, val_acc:0.980]
Epoch [78/120    avg_loss:0.007, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.012, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.016, val_acc:0.986]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.005, val_acc:0.989]
Epoch [88/120    avg_loss:0.006, val_acc:0.990]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.989]
Epoch [100/120    avg_loss:0.006, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.002, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     3     0     0     0     0    21     1]
 [    0     0 18051     0     6     0    31     0     2     0]
 [    0     0     0  2023     0     0     0     0     8     5]
 [    0    11     0     4  2948     0     1     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4871     0     6     0]
 [    0     7     0     0     0     0     3  1274     0     6]
 [    0     1     0    45    43     0     0     0  3481     1]
 [    0     0     0     0     4    29     0     0     0   886]]

Accuracy:
99.40471886824284

F1 scores:
[       nan 0.99657801 0.99889325 0.98418876 0.98710866 0.98901099
 0.99570728 0.99375975 0.98097788 0.97469747]

Kappa:
0.9921157195140972
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f57d2766748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.417, val_acc:0.466]
Epoch [2/120    avg_loss:0.811, val_acc:0.557]
Epoch [3/120    avg_loss:0.543, val_acc:0.680]
Epoch [4/120    avg_loss:0.392, val_acc:0.716]
Epoch [5/120    avg_loss:0.354, val_acc:0.836]
Epoch [6/120    avg_loss:0.309, val_acc:0.753]
Epoch [7/120    avg_loss:0.240, val_acc:0.833]
Epoch [8/120    avg_loss:0.331, val_acc:0.841]
Epoch [9/120    avg_loss:0.206, val_acc:0.858]
Epoch [10/120    avg_loss:0.171, val_acc:0.902]
Epoch [11/120    avg_loss:0.221, val_acc:0.873]
Epoch [12/120    avg_loss:0.169, val_acc:0.935]
Epoch [13/120    avg_loss:0.124, val_acc:0.900]
Epoch [14/120    avg_loss:0.094, val_acc:0.945]
Epoch [15/120    avg_loss:0.091, val_acc:0.892]
Epoch [16/120    avg_loss:0.095, val_acc:0.952]
Epoch [17/120    avg_loss:0.064, val_acc:0.960]
Epoch [18/120    avg_loss:0.067, val_acc:0.955]
Epoch [19/120    avg_loss:0.067, val_acc:0.926]
Epoch [20/120    avg_loss:0.082, val_acc:0.952]
Epoch [21/120    avg_loss:0.085, val_acc:0.942]
Epoch [22/120    avg_loss:0.067, val_acc:0.965]
Epoch [23/120    avg_loss:0.034, val_acc:0.965]
Epoch [24/120    avg_loss:0.046, val_acc:0.948]
Epoch [25/120    avg_loss:0.030, val_acc:0.964]
Epoch [26/120    avg_loss:0.057, val_acc:0.966]
Epoch [27/120    avg_loss:0.027, val_acc:0.965]
Epoch [28/120    avg_loss:0.024, val_acc:0.967]
Epoch [29/120    avg_loss:0.052, val_acc:0.961]
Epoch [30/120    avg_loss:0.052, val_acc:0.967]
Epoch [31/120    avg_loss:0.027, val_acc:0.961]
Epoch [32/120    avg_loss:0.032, val_acc:0.958]
Epoch [33/120    avg_loss:0.041, val_acc:0.947]
Epoch [34/120    avg_loss:0.056, val_acc:0.958]
Epoch [35/120    avg_loss:0.037, val_acc:0.970]
Epoch [36/120    avg_loss:0.020, val_acc:0.957]
Epoch [37/120    avg_loss:0.051, val_acc:0.965]
Epoch [38/120    avg_loss:0.041, val_acc:0.973]
Epoch [39/120    avg_loss:0.038, val_acc:0.959]
Epoch [40/120    avg_loss:0.036, val_acc:0.972]
Epoch [41/120    avg_loss:0.028, val_acc:0.965]
Epoch [42/120    avg_loss:0.025, val_acc:0.980]
Epoch [43/120    avg_loss:0.012, val_acc:0.977]
Epoch [44/120    avg_loss:0.013, val_acc:0.976]
Epoch [45/120    avg_loss:0.019, val_acc:0.980]
Epoch [46/120    avg_loss:0.020, val_acc:0.981]
Epoch [47/120    avg_loss:0.015, val_acc:0.982]
Epoch [48/120    avg_loss:0.011, val_acc:0.981]
Epoch [49/120    avg_loss:0.011, val_acc:0.980]
Epoch [50/120    avg_loss:0.018, val_acc:0.970]
Epoch [51/120    avg_loss:0.032, val_acc:0.969]
Epoch [52/120    avg_loss:0.054, val_acc:0.950]
Epoch [53/120    avg_loss:0.033, val_acc:0.965]
Epoch [54/120    avg_loss:0.024, val_acc:0.967]
Epoch [55/120    avg_loss:0.018, val_acc:0.972]
Epoch [56/120    avg_loss:0.012, val_acc:0.975]
Epoch [57/120    avg_loss:0.037, val_acc:0.971]
Epoch [58/120    avg_loss:0.027, val_acc:0.933]
Epoch [59/120    avg_loss:0.069, val_acc:0.974]
Epoch [60/120    avg_loss:0.038, val_acc:0.969]
Epoch [61/120    avg_loss:0.021, val_acc:0.975]
Epoch [62/120    avg_loss:0.022, val_acc:0.976]
Epoch [63/120    avg_loss:0.015, val_acc:0.980]
Epoch [64/120    avg_loss:0.009, val_acc:0.978]
Epoch [65/120    avg_loss:0.013, val_acc:0.981]
Epoch [66/120    avg_loss:0.020, val_acc:0.981]
Epoch [67/120    avg_loss:0.019, val_acc:0.981]
Epoch [68/120    avg_loss:0.011, val_acc:0.981]
Epoch [69/120    avg_loss:0.008, val_acc:0.981]
Epoch [70/120    avg_loss:0.008, val_acc:0.981]
Epoch [71/120    avg_loss:0.012, val_acc:0.982]
Epoch [72/120    avg_loss:0.010, val_acc:0.981]
Epoch [73/120    avg_loss:0.011, val_acc:0.980]
Epoch [74/120    avg_loss:0.013, val_acc:0.981]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.982]
Epoch [78/120    avg_loss:0.011, val_acc:0.982]
Epoch [79/120    avg_loss:0.013, val_acc:0.983]
Epoch [80/120    avg_loss:0.010, val_acc:0.981]
Epoch [81/120    avg_loss:0.016, val_acc:0.980]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.016, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.010, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.981]
Epoch [91/120    avg_loss:0.008, val_acc:0.981]
Epoch [92/120    avg_loss:0.009, val_acc:0.981]
Epoch [93/120    avg_loss:0.010, val_acc:0.981]
Epoch [94/120    avg_loss:0.012, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.983]
Epoch [96/120    avg_loss:0.012, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.007, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.008, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.016, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.982]
Epoch [115/120    avg_loss:0.006, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     2     0     0     6     0    31     2]
 [    0     2 18053     0    12     0    11     0    12     0]
 [    0     3     0  1993     0     0     0     0    37     3]
 [    0    12     3     1  2938     0     2     0    13     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4866     0    11     0]
 [    0     6     0     0     0     0     5  1277     0     2]
 [    0     2     0    33    55     0     0     0  3480     1]
 [    0     0     0     0    12    19     0     0     0   888]]

Accuracy:
99.27216638951148

F1 scores:
[       nan 0.99486301 0.99886574 0.98056581 0.98113208 0.99277292
 0.9963145  0.99493572 0.97274633 0.97689769]

Kappa:
0.9903593694776095
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f875926d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.339, val_acc:0.717]
Epoch [2/120    avg_loss:0.725, val_acc:0.612]
Epoch [3/120    avg_loss:0.564, val_acc:0.671]
Epoch [4/120    avg_loss:0.453, val_acc:0.756]
Epoch [5/120    avg_loss:0.381, val_acc:0.702]
Epoch [6/120    avg_loss:0.314, val_acc:0.832]
Epoch [7/120    avg_loss:0.276, val_acc:0.882]
Epoch [8/120    avg_loss:0.186, val_acc:0.905]
Epoch [9/120    avg_loss:0.132, val_acc:0.916]
Epoch [10/120    avg_loss:0.135, val_acc:0.937]
Epoch [11/120    avg_loss:0.150, val_acc:0.867]
Epoch [12/120    avg_loss:0.150, val_acc:0.930]
Epoch [13/120    avg_loss:0.093, val_acc:0.963]
Epoch [14/120    avg_loss:0.099, val_acc:0.938]
Epoch [15/120    avg_loss:0.078, val_acc:0.948]
Epoch [16/120    avg_loss:0.089, val_acc:0.937]
Epoch [17/120    avg_loss:0.061, val_acc:0.947]
Epoch [18/120    avg_loss:0.058, val_acc:0.885]
Epoch [19/120    avg_loss:0.056, val_acc:0.950]
Epoch [20/120    avg_loss:0.055, val_acc:0.972]
Epoch [21/120    avg_loss:0.064, val_acc:0.938]
Epoch [22/120    avg_loss:0.047, val_acc:0.928]
Epoch [23/120    avg_loss:0.087, val_acc:0.945]
Epoch [24/120    avg_loss:0.057, val_acc:0.970]
Epoch [25/120    avg_loss:0.080, val_acc:0.960]
Epoch [26/120    avg_loss:0.085, val_acc:0.933]
Epoch [27/120    avg_loss:0.073, val_acc:0.965]
Epoch [28/120    avg_loss:0.039, val_acc:0.951]
Epoch [29/120    avg_loss:0.031, val_acc:0.970]
Epoch [30/120    avg_loss:0.031, val_acc:0.959]
Epoch [31/120    avg_loss:0.058, val_acc:0.974]
Epoch [32/120    avg_loss:0.031, val_acc:0.975]
Epoch [33/120    avg_loss:0.034, val_acc:0.965]
Epoch [34/120    avg_loss:0.035, val_acc:0.963]
Epoch [35/120    avg_loss:0.020, val_acc:0.975]
Epoch [36/120    avg_loss:0.022, val_acc:0.953]
Epoch [37/120    avg_loss:0.032, val_acc:0.978]
Epoch [38/120    avg_loss:0.024, val_acc:0.965]
Epoch [39/120    avg_loss:0.025, val_acc:0.978]
Epoch [40/120    avg_loss:0.014, val_acc:0.984]
Epoch [41/120    avg_loss:0.011, val_acc:0.984]
Epoch [42/120    avg_loss:0.014, val_acc:0.977]
Epoch [43/120    avg_loss:0.014, val_acc:0.983]
Epoch [44/120    avg_loss:0.018, val_acc:0.981]
Epoch [45/120    avg_loss:0.025, val_acc:0.984]
Epoch [46/120    avg_loss:0.014, val_acc:0.989]
Epoch [47/120    avg_loss:0.012, val_acc:0.979]
Epoch [48/120    avg_loss:0.018, val_acc:0.988]
Epoch [49/120    avg_loss:0.011, val_acc:0.985]
Epoch [50/120    avg_loss:0.011, val_acc:0.992]
Epoch [51/120    avg_loss:0.010, val_acc:0.986]
Epoch [52/120    avg_loss:0.013, val_acc:0.986]
Epoch [53/120    avg_loss:0.018, val_acc:0.984]
Epoch [54/120    avg_loss:0.016, val_acc:0.987]
Epoch [55/120    avg_loss:0.009, val_acc:0.985]
Epoch [56/120    avg_loss:0.010, val_acc:0.986]
Epoch [57/120    avg_loss:0.008, val_acc:0.990]
Epoch [58/120    avg_loss:0.006, val_acc:0.990]
Epoch [59/120    avg_loss:0.008, val_acc:0.987]
Epoch [60/120    avg_loss:0.006, val_acc:0.986]
Epoch [61/120    avg_loss:0.009, val_acc:0.986]
Epoch [62/120    avg_loss:0.013, val_acc:0.978]
Epoch [63/120    avg_loss:0.004, val_acc:0.987]
Epoch [64/120    avg_loss:0.006, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.987]
Epoch [66/120    avg_loss:0.003, val_acc:0.987]
Epoch [67/120    avg_loss:0.004, val_acc:0.987]
Epoch [68/120    avg_loss:0.005, val_acc:0.990]
Epoch [69/120    avg_loss:0.002, val_acc:0.990]
Epoch [70/120    avg_loss:0.006, val_acc:0.991]
Epoch [71/120    avg_loss:0.005, val_acc:0.990]
Epoch [72/120    avg_loss:0.003, val_acc:0.991]
Epoch [73/120    avg_loss:0.004, val_acc:0.991]
Epoch [74/120    avg_loss:0.003, val_acc:0.990]
Epoch [75/120    avg_loss:0.004, val_acc:0.990]
Epoch [76/120    avg_loss:0.007, val_acc:0.990]
Epoch [77/120    avg_loss:0.006, val_acc:0.990]
Epoch [78/120    avg_loss:0.004, val_acc:0.990]
Epoch [79/120    avg_loss:0.005, val_acc:0.990]
Epoch [80/120    avg_loss:0.004, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.004, val_acc:0.990]
Epoch [83/120    avg_loss:0.004, val_acc:0.990]
Epoch [84/120    avg_loss:0.006, val_acc:0.990]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.004, val_acc:0.990]
Epoch [87/120    avg_loss:0.004, val_acc:0.990]
Epoch [88/120    avg_loss:0.004, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.003, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.009, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.004, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.003, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.003, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.003, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.990]
Epoch [113/120    avg_loss:0.003, val_acc:0.990]
Epoch [114/120    avg_loss:0.002, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6424     0     5     0     0     1     0     1     1]
 [    0     2 18047     0     8     0    33     0     0     0]
 [    0     0     0  2020     0     0     0     0    14     2]
 [    0    19     0     3  2942     0     1     0     6     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4871     0     0     0]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     1     0    19    44     0     0     0  3499     8]
 [    0     0     0     0     1    19     0     0     0   899]]

Accuracy:
99.51799098643144

F1 scores:
[       nan 0.99767045 0.99861664 0.98946853 0.98609016 0.99277292
 0.99550378 0.9984472  0.98688478 0.98144105]

Kappa:
0.9936155074693647
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f386a069860>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.370, val_acc:0.715]
Epoch [2/120    avg_loss:0.751, val_acc:0.669]
Epoch [3/120    avg_loss:0.503, val_acc:0.729]
Epoch [4/120    avg_loss:0.407, val_acc:0.831]
Epoch [5/120    avg_loss:0.343, val_acc:0.805]
Epoch [6/120    avg_loss:0.295, val_acc:0.884]
Epoch [7/120    avg_loss:0.194, val_acc:0.907]
Epoch [8/120    avg_loss:0.195, val_acc:0.915]
Epoch [9/120    avg_loss:0.176, val_acc:0.910]
Epoch [10/120    avg_loss:0.111, val_acc:0.927]
Epoch [11/120    avg_loss:0.138, val_acc:0.902]
Epoch [12/120    avg_loss:0.135, val_acc:0.926]
Epoch [13/120    avg_loss:0.097, val_acc:0.931]
Epoch [14/120    avg_loss:0.080, val_acc:0.949]
Epoch [15/120    avg_loss:0.073, val_acc:0.959]
Epoch [16/120    avg_loss:0.081, val_acc:0.937]
Epoch [17/120    avg_loss:0.105, val_acc:0.945]
Epoch [18/120    avg_loss:0.099, val_acc:0.928]
Epoch [19/120    avg_loss:0.072, val_acc:0.959]
Epoch [20/120    avg_loss:0.052, val_acc:0.954]
Epoch [21/120    avg_loss:0.053, val_acc:0.947]
Epoch [22/120    avg_loss:0.059, val_acc:0.971]
Epoch [23/120    avg_loss:0.041, val_acc:0.970]
Epoch [24/120    avg_loss:0.060, val_acc:0.959]
Epoch [25/120    avg_loss:0.045, val_acc:0.961]
Epoch [26/120    avg_loss:0.046, val_acc:0.964]
Epoch [27/120    avg_loss:0.030, val_acc:0.974]
Epoch [28/120    avg_loss:0.029, val_acc:0.971]
Epoch [29/120    avg_loss:0.012, val_acc:0.976]
Epoch [30/120    avg_loss:0.013, val_acc:0.978]
Epoch [31/120    avg_loss:0.020, val_acc:0.972]
Epoch [32/120    avg_loss:0.040, val_acc:0.971]
Epoch [33/120    avg_loss:0.028, val_acc:0.963]
Epoch [34/120    avg_loss:0.024, val_acc:0.977]
Epoch [35/120    avg_loss:0.024, val_acc:0.970]
Epoch [36/120    avg_loss:0.027, val_acc:0.963]
Epoch [37/120    avg_loss:0.040, val_acc:0.979]
Epoch [38/120    avg_loss:0.086, val_acc:0.961]
Epoch [39/120    avg_loss:0.044, val_acc:0.972]
Epoch [40/120    avg_loss:0.027, val_acc:0.976]
Epoch [41/120    avg_loss:0.021, val_acc:0.965]
Epoch [42/120    avg_loss:0.040, val_acc:0.963]
Epoch [43/120    avg_loss:0.019, val_acc:0.981]
Epoch [44/120    avg_loss:0.030, val_acc:0.978]
Epoch [45/120    avg_loss:0.022, val_acc:0.980]
Epoch [46/120    avg_loss:0.035, val_acc:0.964]
Epoch [47/120    avg_loss:0.044, val_acc:0.954]
Epoch [48/120    avg_loss:0.030, val_acc:0.971]
Epoch [49/120    avg_loss:0.018, val_acc:0.976]
Epoch [50/120    avg_loss:0.012, val_acc:0.981]
Epoch [51/120    avg_loss:0.012, val_acc:0.982]
Epoch [52/120    avg_loss:0.018, val_acc:0.981]
Epoch [53/120    avg_loss:0.018, val_acc:0.984]
Epoch [54/120    avg_loss:0.019, val_acc:0.970]
Epoch [55/120    avg_loss:0.015, val_acc:0.979]
Epoch [56/120    avg_loss:0.014, val_acc:0.978]
Epoch [57/120    avg_loss:0.016, val_acc:0.985]
Epoch [58/120    avg_loss:0.007, val_acc:0.982]
Epoch [59/120    avg_loss:0.020, val_acc:0.980]
Epoch [60/120    avg_loss:0.016, val_acc:0.983]
Epoch [61/120    avg_loss:0.014, val_acc:0.984]
Epoch [62/120    avg_loss:0.010, val_acc:0.979]
Epoch [63/120    avg_loss:0.006, val_acc:0.976]
Epoch [64/120    avg_loss:0.009, val_acc:0.985]
Epoch [65/120    avg_loss:0.012, val_acc:0.984]
Epoch [66/120    avg_loss:0.011, val_acc:0.984]
Epoch [67/120    avg_loss:0.020, val_acc:0.969]
Epoch [68/120    avg_loss:0.020, val_acc:0.982]
Epoch [69/120    avg_loss:0.008, val_acc:0.985]
Epoch [70/120    avg_loss:0.005, val_acc:0.989]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.005, val_acc:0.983]
Epoch [73/120    avg_loss:0.007, val_acc:0.987]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.008, val_acc:0.989]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.989]
Epoch [80/120    avg_loss:0.003, val_acc:0.989]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.004, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.984]
Epoch [84/120    avg_loss:0.004, val_acc:0.979]
Epoch [85/120    avg_loss:0.003, val_acc:0.987]
Epoch [86/120    avg_loss:0.003, val_acc:0.988]
Epoch [87/120    avg_loss:0.006, val_acc:0.981]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.017, val_acc:0.971]
Epoch [91/120    avg_loss:0.030, val_acc:0.973]
Epoch [92/120    avg_loss:0.049, val_acc:0.976]
Epoch [93/120    avg_loss:0.025, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     0     0     0     1]
 [    0     4 18066     0     3     0    17     0     0     0]
 [    0     2     0  1992     0     0     0     0    39     3]
 [    0    23     0     1  2936     0     2     0     9     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4869     0     9     0]
 [    0     5     0     0     0     0     2  1265     0    18]
 [    0     7     0    43    37     0     0     0  3480     4]
 [    0     0     0     4     0    46     0     0     0   869]]

Accuracy:
99.32518738100403

F1 scores:
[       nan 0.9967452  0.99933621 0.97742885 0.9872226  0.98268072
 0.99692875 0.99021526 0.97917839 0.95757576]

Kappa:
0.9910597913470985
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f432f7ba7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.371, val_acc:0.678]
Epoch [2/120    avg_loss:0.794, val_acc:0.494]
Epoch [3/120    avg_loss:0.545, val_acc:0.698]
Epoch [4/120    avg_loss:0.460, val_acc:0.718]
Epoch [5/120    avg_loss:0.346, val_acc:0.769]
Epoch [6/120    avg_loss:0.289, val_acc:0.856]
Epoch [7/120    avg_loss:0.281, val_acc:0.822]
Epoch [8/120    avg_loss:0.202, val_acc:0.927]
Epoch [9/120    avg_loss:0.211, val_acc:0.920]
Epoch [10/120    avg_loss:0.158, val_acc:0.914]
Epoch [11/120    avg_loss:0.156, val_acc:0.916]
Epoch [12/120    avg_loss:0.147, val_acc:0.921]
Epoch [13/120    avg_loss:0.097, val_acc:0.954]
Epoch [14/120    avg_loss:0.140, val_acc:0.916]
Epoch [15/120    avg_loss:0.094, val_acc:0.963]
Epoch [16/120    avg_loss:0.076, val_acc:0.965]
Epoch [17/120    avg_loss:0.068, val_acc:0.958]
Epoch [18/120    avg_loss:0.065, val_acc:0.964]
Epoch [19/120    avg_loss:0.066, val_acc:0.967]
Epoch [20/120    avg_loss:0.066, val_acc:0.968]
Epoch [21/120    avg_loss:0.052, val_acc:0.953]
Epoch [22/120    avg_loss:0.036, val_acc:0.965]
Epoch [23/120    avg_loss:0.030, val_acc:0.978]
Epoch [24/120    avg_loss:0.035, val_acc:0.959]
Epoch [25/120    avg_loss:0.049, val_acc:0.963]
Epoch [26/120    avg_loss:0.051, val_acc:0.967]
Epoch [27/120    avg_loss:0.032, val_acc:0.975]
Epoch [28/120    avg_loss:0.036, val_acc:0.951]
Epoch [29/120    avg_loss:0.026, val_acc:0.976]
Epoch [30/120    avg_loss:0.027, val_acc:0.977]
Epoch [31/120    avg_loss:0.020, val_acc:0.977]
Epoch [32/120    avg_loss:0.012, val_acc:0.979]
Epoch [33/120    avg_loss:0.024, val_acc:0.970]
Epoch [34/120    avg_loss:0.014, val_acc:0.978]
Epoch [35/120    avg_loss:0.021, val_acc:0.978]
Epoch [36/120    avg_loss:0.014, val_acc:0.980]
Epoch [37/120    avg_loss:0.013, val_acc:0.981]
Epoch [38/120    avg_loss:0.014, val_acc:0.976]
Epoch [39/120    avg_loss:0.008, val_acc:0.983]
Epoch [40/120    avg_loss:0.008, val_acc:0.982]
Epoch [41/120    avg_loss:0.010, val_acc:0.981]
Epoch [42/120    avg_loss:0.012, val_acc:0.977]
Epoch [43/120    avg_loss:0.027, val_acc:0.896]
Epoch [44/120    avg_loss:0.061, val_acc:0.961]
Epoch [45/120    avg_loss:0.026, val_acc:0.970]
Epoch [46/120    avg_loss:0.019, val_acc:0.975]
Epoch [47/120    avg_loss:0.026, val_acc:0.970]
Epoch [48/120    avg_loss:0.019, val_acc:0.981]
Epoch [49/120    avg_loss:0.016, val_acc:0.979]
Epoch [50/120    avg_loss:0.008, val_acc:0.981]
Epoch [51/120    avg_loss:0.006, val_acc:0.982]
Epoch [52/120    avg_loss:0.006, val_acc:0.981]
Epoch [53/120    avg_loss:0.007, val_acc:0.982]
Epoch [54/120    avg_loss:0.004, val_acc:0.984]
Epoch [55/120    avg_loss:0.005, val_acc:0.984]
Epoch [56/120    avg_loss:0.006, val_acc:0.984]
Epoch [57/120    avg_loss:0.007, val_acc:0.984]
Epoch [58/120    avg_loss:0.006, val_acc:0.985]
Epoch [59/120    avg_loss:0.008, val_acc:0.985]
Epoch [60/120    avg_loss:0.004, val_acc:0.984]
Epoch [61/120    avg_loss:0.003, val_acc:0.985]
Epoch [62/120    avg_loss:0.005, val_acc:0.985]
Epoch [63/120    avg_loss:0.006, val_acc:0.985]
Epoch [64/120    avg_loss:0.004, val_acc:0.985]
Epoch [65/120    avg_loss:0.004, val_acc:0.986]
Epoch [66/120    avg_loss:0.003, val_acc:0.986]
Epoch [67/120    avg_loss:0.004, val_acc:0.986]
Epoch [68/120    avg_loss:0.005, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.009, val_acc:0.986]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.986]
Epoch [76/120    avg_loss:0.006, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.004, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.003, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.004, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0    13     0     0     0     1    13    10]
 [    0     6 18063     0     7     0    11     0     3     0]
 [    0     0     0  2009     2     0     0     0    23     2]
 [    0    19     0     8  2933     0     0     0     7     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     2     0     0     0     0     2  1278     0     8]
 [    0     0     0    31    39     0     0     0  3498     3]
 [    0     0     0     0     0    23     0     0     0   896]]

Accuracy:
99.42640927385342

F1 scores:
[       nan 0.99502101 0.99925317 0.9807176  0.98538552 0.99126472
 0.99866926 0.99493967 0.98327477 0.97232773]

Kappa:
0.9924024280821645
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7febc8fdb7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.350, val_acc:0.456]
Epoch [2/120    avg_loss:0.782, val_acc:0.578]
Epoch [3/120    avg_loss:0.558, val_acc:0.691]
Epoch [4/120    avg_loss:0.417, val_acc:0.746]
Epoch [5/120    avg_loss:0.368, val_acc:0.741]
Epoch [6/120    avg_loss:0.289, val_acc:0.875]
Epoch [7/120    avg_loss:0.256, val_acc:0.888]
Epoch [8/120    avg_loss:0.273, val_acc:0.802]
Epoch [9/120    avg_loss:0.185, val_acc:0.901]
Epoch [10/120    avg_loss:0.187, val_acc:0.845]
Epoch [11/120    avg_loss:0.220, val_acc:0.930]
Epoch [12/120    avg_loss:0.157, val_acc:0.923]
Epoch [13/120    avg_loss:0.114, val_acc:0.943]
Epoch [14/120    avg_loss:0.105, val_acc:0.955]
Epoch [15/120    avg_loss:0.079, val_acc:0.969]
Epoch [16/120    avg_loss:0.131, val_acc:0.952]
Epoch [17/120    avg_loss:0.066, val_acc:0.959]
Epoch [18/120    avg_loss:0.067, val_acc:0.952]
Epoch [19/120    avg_loss:0.065, val_acc:0.929]
Epoch [20/120    avg_loss:0.080, val_acc:0.946]
Epoch [21/120    avg_loss:0.052, val_acc:0.960]
Epoch [22/120    avg_loss:0.056, val_acc:0.972]
Epoch [23/120    avg_loss:0.045, val_acc:0.959]
Epoch [24/120    avg_loss:0.031, val_acc:0.974]
Epoch [25/120    avg_loss:0.035, val_acc:0.971]
Epoch [26/120    avg_loss:0.035, val_acc:0.974]
Epoch [27/120    avg_loss:0.028, val_acc:0.976]
Epoch [28/120    avg_loss:0.037, val_acc:0.981]
Epoch [29/120    avg_loss:0.022, val_acc:0.981]
Epoch [30/120    avg_loss:0.024, val_acc:0.979]
Epoch [31/120    avg_loss:0.023, val_acc:0.978]
Epoch [32/120    avg_loss:0.015, val_acc:0.986]
Epoch [33/120    avg_loss:0.018, val_acc:0.983]
Epoch [34/120    avg_loss:0.021, val_acc:0.983]
Epoch [35/120    avg_loss:0.018, val_acc:0.986]
Epoch [36/120    avg_loss:0.033, val_acc:0.984]
Epoch [37/120    avg_loss:0.022, val_acc:0.984]
Epoch [38/120    avg_loss:0.015, val_acc:0.987]
Epoch [39/120    avg_loss:0.014, val_acc:0.985]
Epoch [40/120    avg_loss:0.015, val_acc:0.984]
Epoch [41/120    avg_loss:0.013, val_acc:0.986]
Epoch [42/120    avg_loss:0.015, val_acc:0.985]
Epoch [43/120    avg_loss:0.011, val_acc:0.981]
Epoch [44/120    avg_loss:0.010, val_acc:0.986]
Epoch [45/120    avg_loss:0.020, val_acc:0.988]
Epoch [46/120    avg_loss:0.012, val_acc:0.982]
Epoch [47/120    avg_loss:0.010, val_acc:0.986]
Epoch [48/120    avg_loss:0.012, val_acc:0.975]
Epoch [49/120    avg_loss:0.010, val_acc:0.982]
Epoch [50/120    avg_loss:0.016, val_acc:0.983]
Epoch [51/120    avg_loss:0.017, val_acc:0.963]
Epoch [52/120    avg_loss:0.017, val_acc:0.981]
Epoch [53/120    avg_loss:0.010, val_acc:0.987]
Epoch [54/120    avg_loss:0.015, val_acc:0.986]
Epoch [55/120    avg_loss:0.014, val_acc:0.990]
Epoch [56/120    avg_loss:0.012, val_acc:0.991]
Epoch [57/120    avg_loss:0.007, val_acc:0.990]
Epoch [58/120    avg_loss:0.014, val_acc:0.978]
Epoch [59/120    avg_loss:0.024, val_acc:0.986]
Epoch [60/120    avg_loss:0.008, val_acc:0.991]
Epoch [61/120    avg_loss:0.011, val_acc:0.990]
Epoch [62/120    avg_loss:0.008, val_acc:0.992]
Epoch [63/120    avg_loss:0.007, val_acc:0.987]
Epoch [64/120    avg_loss:0.009, val_acc:0.987]
Epoch [65/120    avg_loss:0.151, val_acc:0.968]
Epoch [66/120    avg_loss:0.041, val_acc:0.976]
Epoch [67/120    avg_loss:0.026, val_acc:0.981]
Epoch [68/120    avg_loss:0.035, val_acc:0.980]
Epoch [69/120    avg_loss:0.015, val_acc:0.982]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.063, val_acc:0.959]
Epoch [73/120    avg_loss:0.054, val_acc:0.980]
Epoch [74/120    avg_loss:0.028, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.989]
Epoch [76/120    avg_loss:0.009, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.007, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.989]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.010, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.008, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.007, val_acc:0.992]
Epoch [89/120    avg_loss:0.006, val_acc:0.992]
Epoch [90/120    avg_loss:0.006, val_acc:0.992]
Epoch [91/120    avg_loss:0.009, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.992]
Epoch [93/120    avg_loss:0.011, val_acc:0.991]
Epoch [94/120    avg_loss:0.008, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.991]
Epoch [101/120    avg_loss:0.006, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.009, val_acc:0.991]
Epoch [110/120    avg_loss:0.007, val_acc:0.991]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     0     0    24    19     6     6]
 [    0     4 18065     0     6     0    15     0     0     0]
 [    0     0     0  2005     0     0     0     0    27     4]
 [    0    28     0     3  2927     0     1     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     3     0     0     0     1     0  1272     0    14]
 [    0     2     0    31    51     0     0     0  3482     5]
 [    0     0     0     0     1    29     0     0     0   889]]

Accuracy:
99.29144675005423

F1 scores:
[       nan 0.99283824 0.99928089 0.98404908 0.98270942 0.98863636
 0.99581419 0.98566447 0.98098324 0.96788242]

Kappa:
0.9906143402164171
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feac76757b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.419, val_acc:0.376]
Epoch [2/120    avg_loss:0.768, val_acc:0.568]
Epoch [3/120    avg_loss:0.547, val_acc:0.731]
Epoch [4/120    avg_loss:0.449, val_acc:0.790]
Epoch [5/120    avg_loss:0.366, val_acc:0.771]
Epoch [6/120    avg_loss:0.270, val_acc:0.897]
Epoch [7/120    avg_loss:0.249, val_acc:0.905]
Epoch [8/120    avg_loss:0.165, val_acc:0.927]
Epoch [9/120    avg_loss:0.155, val_acc:0.961]
Epoch [10/120    avg_loss:0.113, val_acc:0.947]
Epoch [11/120    avg_loss:0.118, val_acc:0.949]
Epoch [12/120    avg_loss:0.117, val_acc:0.954]
Epoch [13/120    avg_loss:0.105, val_acc:0.964]
Epoch [14/120    avg_loss:0.102, val_acc:0.959]
Epoch [15/120    avg_loss:0.071, val_acc:0.947]
Epoch [16/120    avg_loss:0.081, val_acc:0.949]
Epoch [17/120    avg_loss:0.118, val_acc:0.968]
Epoch [18/120    avg_loss:0.107, val_acc:0.963]
Epoch [19/120    avg_loss:0.070, val_acc:0.961]
Epoch [20/120    avg_loss:0.066, val_acc:0.982]
Epoch [21/120    avg_loss:0.044, val_acc:0.976]
Epoch [22/120    avg_loss:0.041, val_acc:0.972]
Epoch [23/120    avg_loss:0.030, val_acc:0.981]
Epoch [24/120    avg_loss:0.040, val_acc:0.961]
Epoch [25/120    avg_loss:0.050, val_acc:0.968]
Epoch [26/120    avg_loss:0.058, val_acc:0.963]
Epoch [27/120    avg_loss:0.027, val_acc:0.983]
Epoch [28/120    avg_loss:0.025, val_acc:0.986]
Epoch [29/120    avg_loss:0.037, val_acc:0.961]
Epoch [30/120    avg_loss:0.111, val_acc:0.971]
Epoch [31/120    avg_loss:0.039, val_acc:0.981]
Epoch [32/120    avg_loss:0.041, val_acc:0.978]
Epoch [33/120    avg_loss:0.038, val_acc:0.965]
Epoch [34/120    avg_loss:0.036, val_acc:0.982]
Epoch [35/120    avg_loss:0.034, val_acc:0.981]
Epoch [36/120    avg_loss:0.028, val_acc:0.987]
Epoch [37/120    avg_loss:0.021, val_acc:0.988]
Epoch [38/120    avg_loss:0.018, val_acc:0.983]
Epoch [39/120    avg_loss:0.018, val_acc:0.986]
Epoch [40/120    avg_loss:0.021, val_acc:0.981]
Epoch [41/120    avg_loss:0.014, val_acc:0.985]
Epoch [42/120    avg_loss:0.008, val_acc:0.986]
Epoch [43/120    avg_loss:0.039, val_acc:0.970]
Epoch [44/120    avg_loss:0.049, val_acc:0.982]
Epoch [45/120    avg_loss:0.033, val_acc:0.986]
Epoch [46/120    avg_loss:0.013, val_acc:0.987]
Epoch [47/120    avg_loss:0.016, val_acc:0.989]
Epoch [48/120    avg_loss:0.028, val_acc:0.981]
Epoch [49/120    avg_loss:0.018, val_acc:0.986]
Epoch [50/120    avg_loss:0.014, val_acc:0.991]
Epoch [51/120    avg_loss:0.009, val_acc:0.991]
Epoch [52/120    avg_loss:0.027, val_acc:0.979]
Epoch [53/120    avg_loss:0.020, val_acc:0.990]
Epoch [54/120    avg_loss:0.012, val_acc:0.987]
Epoch [55/120    avg_loss:0.013, val_acc:0.985]
Epoch [56/120    avg_loss:0.028, val_acc:0.986]
Epoch [57/120    avg_loss:0.014, val_acc:0.989]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.011, val_acc:0.991]
Epoch [60/120    avg_loss:0.011, val_acc:0.986]
Epoch [61/120    avg_loss:0.015, val_acc:0.987]
Epoch [62/120    avg_loss:0.014, val_acc:0.985]
Epoch [63/120    avg_loss:0.015, val_acc:0.971]
Epoch [64/120    avg_loss:0.015, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.991]
Epoch [66/120    avg_loss:0.007, val_acc:0.988]
Epoch [67/120    avg_loss:0.009, val_acc:0.990]
Epoch [68/120    avg_loss:0.011, val_acc:0.981]
Epoch [69/120    avg_loss:0.053, val_acc:0.966]
Epoch [70/120    avg_loss:0.046, val_acc:0.984]
Epoch [71/120    avg_loss:0.122, val_acc:0.969]
Epoch [72/120    avg_loss:0.058, val_acc:0.980]
Epoch [73/120    avg_loss:0.024, val_acc:0.985]
Epoch [74/120    avg_loss:0.016, val_acc:0.981]
Epoch [75/120    avg_loss:0.020, val_acc:0.989]
Epoch [76/120    avg_loss:0.028, val_acc:0.973]
Epoch [77/120    avg_loss:0.048, val_acc:0.960]
Epoch [78/120    avg_loss:0.034, val_acc:0.984]
Epoch [79/120    avg_loss:0.031, val_acc:0.992]
Epoch [80/120    avg_loss:0.011, val_acc:0.992]
Epoch [81/120    avg_loss:0.013, val_acc:0.991]
Epoch [82/120    avg_loss:0.011, val_acc:0.992]
Epoch [83/120    avg_loss:0.012, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.011, val_acc:0.991]
Epoch [88/120    avg_loss:0.008, val_acc:0.992]
Epoch [89/120    avg_loss:0.011, val_acc:0.991]
Epoch [90/120    avg_loss:0.018, val_acc:0.992]
Epoch [91/120    avg_loss:0.011, val_acc:0.989]
Epoch [92/120    avg_loss:0.009, val_acc:0.992]
Epoch [93/120    avg_loss:0.007, val_acc:0.992]
Epoch [94/120    avg_loss:0.009, val_acc:0.992]
Epoch [95/120    avg_loss:0.008, val_acc:0.992]
Epoch [96/120    avg_loss:0.010, val_acc:0.992]
Epoch [97/120    avg_loss:0.007, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.013, val_acc:0.991]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.007, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.991]
Epoch [104/120    avg_loss:0.009, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.012, val_acc:0.991]
Epoch [109/120    avg_loss:0.008, val_acc:0.992]
Epoch [110/120    avg_loss:0.012, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.007, val_acc:0.992]
Epoch [113/120    avg_loss:0.011, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.006, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.012, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     0     2     0     1     0    10     2]
 [    0     4 18065     0     8     0    13     0     0     0]
 [    0     0     0  2014     1     0     0     0    15     6]
 [    0    25     2     0  2925     0     0     0    19     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     0     0  4865     0     1     2]
 [    0     0     0     0     0     0     9  1275     0     6]
 [    0     0     0     2    51     0     0     0  3514     4]
 [    0     0     0     0    12    28     0     0     0   879]]

Accuracy:
99.4360494541248

F1 scores:
[       nan 0.99658332 0.99897697 0.994077   0.97973539 0.9893859
 0.99631374 0.99415205 0.98569425 0.96646509]

Kappa:
0.992527889359629
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f55b7143748>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.575, val_acc:0.326]
Epoch [2/120    avg_loss:0.855, val_acc:0.509]
Epoch [3/120    avg_loss:0.602, val_acc:0.667]
Epoch [4/120    avg_loss:0.463, val_acc:0.812]
Epoch [5/120    avg_loss:0.381, val_acc:0.791]
Epoch [6/120    avg_loss:0.317, val_acc:0.778]
Epoch [7/120    avg_loss:0.268, val_acc:0.867]
Epoch [8/120    avg_loss:0.170, val_acc:0.947]
Epoch [9/120    avg_loss:0.141, val_acc:0.925]
Epoch [10/120    avg_loss:0.136, val_acc:0.923]
Epoch [11/120    avg_loss:0.159, val_acc:0.912]
Epoch [12/120    avg_loss:0.119, val_acc:0.914]
Epoch [13/120    avg_loss:0.088, val_acc:0.953]
Epoch [14/120    avg_loss:0.090, val_acc:0.970]
Epoch [15/120    avg_loss:0.067, val_acc:0.966]
Epoch [16/120    avg_loss:0.062, val_acc:0.925]
Epoch [17/120    avg_loss:0.050, val_acc:0.965]
Epoch [18/120    avg_loss:0.047, val_acc:0.962]
Epoch [19/120    avg_loss:0.049, val_acc:0.966]
Epoch [20/120    avg_loss:0.046, val_acc:0.965]
Epoch [21/120    avg_loss:0.041, val_acc:0.966]
Epoch [22/120    avg_loss:0.045, val_acc:0.970]
Epoch [23/120    avg_loss:0.048, val_acc:0.970]
Epoch [24/120    avg_loss:0.038, val_acc:0.975]
Epoch [25/120    avg_loss:0.032, val_acc:0.968]
Epoch [26/120    avg_loss:0.088, val_acc:0.946]
Epoch [27/120    avg_loss:0.060, val_acc:0.967]
Epoch [28/120    avg_loss:0.044, val_acc:0.962]
Epoch [29/120    avg_loss:0.051, val_acc:0.974]
Epoch [30/120    avg_loss:0.031, val_acc:0.979]
Epoch [31/120    avg_loss:0.029, val_acc:0.970]
Epoch [32/120    avg_loss:0.024, val_acc:0.975]
Epoch [33/120    avg_loss:0.020, val_acc:0.968]
Epoch [34/120    avg_loss:0.023, val_acc:0.981]
Epoch [35/120    avg_loss:0.034, val_acc:0.976]
Epoch [36/120    avg_loss:0.022, val_acc:0.982]
Epoch [37/120    avg_loss:0.014, val_acc:0.976]
Epoch [38/120    avg_loss:0.017, val_acc:0.984]
Epoch [39/120    avg_loss:0.012, val_acc:0.984]
Epoch [40/120    avg_loss:0.011, val_acc:0.983]
Epoch [41/120    avg_loss:0.011, val_acc:0.984]
Epoch [42/120    avg_loss:0.011, val_acc:0.977]
Epoch [43/120    avg_loss:0.012, val_acc:0.985]
Epoch [44/120    avg_loss:0.013, val_acc:0.980]
Epoch [45/120    avg_loss:0.017, val_acc:0.984]
Epoch [46/120    avg_loss:0.008, val_acc:0.985]
Epoch [47/120    avg_loss:0.011, val_acc:0.984]
Epoch [48/120    avg_loss:0.009, val_acc:0.988]
Epoch [49/120    avg_loss:0.007, val_acc:0.982]
Epoch [50/120    avg_loss:0.017, val_acc:0.977]
Epoch [51/120    avg_loss:0.024, val_acc:0.972]
Epoch [52/120    avg_loss:0.014, val_acc:0.981]
Epoch [53/120    avg_loss:0.013, val_acc:0.977]
Epoch [54/120    avg_loss:0.018, val_acc:0.965]
Epoch [55/120    avg_loss:0.008, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.984]
Epoch [57/120    avg_loss:0.009, val_acc:0.984]
Epoch [58/120    avg_loss:0.018, val_acc:0.976]
Epoch [59/120    avg_loss:0.016, val_acc:0.983]
Epoch [60/120    avg_loss:0.007, val_acc:0.982]
Epoch [61/120    avg_loss:0.007, val_acc:0.981]
Epoch [62/120    avg_loss:0.007, val_acc:0.981]
Epoch [63/120    avg_loss:0.010, val_acc:0.982]
Epoch [64/120    avg_loss:0.010, val_acc:0.982]
Epoch [65/120    avg_loss:0.008, val_acc:0.984]
Epoch [66/120    avg_loss:0.007, val_acc:0.984]
Epoch [67/120    avg_loss:0.005, val_acc:0.985]
Epoch [68/120    avg_loss:0.005, val_acc:0.985]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.005, val_acc:0.985]
Epoch [71/120    avg_loss:0.013, val_acc:0.984]
Epoch [72/120    avg_loss:0.004, val_acc:0.984]
Epoch [73/120    avg_loss:0.004, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.984]
Epoch [75/120    avg_loss:0.005, val_acc:0.984]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.004, val_acc:0.984]
Epoch [78/120    avg_loss:0.003, val_acc:0.984]
Epoch [79/120    avg_loss:0.007, val_acc:0.984]
Epoch [80/120    avg_loss:0.004, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.984]
Epoch [83/120    avg_loss:0.006, val_acc:0.984]
Epoch [84/120    avg_loss:0.004, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.005, val_acc:0.984]
Epoch [88/120    avg_loss:0.004, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.003, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.004, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.003, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.003, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.003, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     0     0     0    16     2     0]
 [    0     3 18068     0     8     0     7     0     4     0]
 [    0     0     0  2016     0     0     0     0    11     9]
 [    0    28     6     0  2916     0     0     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4872     0     3     0]
 [    0     2     0     0     0     0     9  1275     0     4]
 [    0     3     0    16    46     0     0     0  3501     5]
 [    0     0     0     0     4    21     0     0     0   894]]

Accuracy:
99.44086954426048

F1 scores:
[       nan 0.9958081  0.99914287 0.99115044 0.98082745 0.99201824
 0.99774729 0.98798915 0.9842564  0.97651557]

Kappa:
0.9925920420180818
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f43a8ec6898>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.416, val_acc:0.635]
Epoch [2/120    avg_loss:0.859, val_acc:0.688]
Epoch [3/120    avg_loss:0.569, val_acc:0.755]
Epoch [4/120    avg_loss:0.399, val_acc:0.771]
Epoch [5/120    avg_loss:0.328, val_acc:0.807]
Epoch [6/120    avg_loss:0.276, val_acc:0.882]
Epoch [7/120    avg_loss:0.218, val_acc:0.889]
Epoch [8/120    avg_loss:0.190, val_acc:0.939]
Epoch [9/120    avg_loss:0.230, val_acc:0.915]
Epoch [10/120    avg_loss:0.169, val_acc:0.902]
Epoch [11/120    avg_loss:0.208, val_acc:0.941]
Epoch [12/120    avg_loss:0.096, val_acc:0.938]
Epoch [13/120    avg_loss:0.105, val_acc:0.932]
Epoch [14/120    avg_loss:0.093, val_acc:0.961]
Epoch [15/120    avg_loss:0.117, val_acc:0.959]
Epoch [16/120    avg_loss:0.073, val_acc:0.967]
Epoch [17/120    avg_loss:0.051, val_acc:0.957]
Epoch [18/120    avg_loss:0.063, val_acc:0.962]
Epoch [19/120    avg_loss:0.044, val_acc:0.972]
Epoch [20/120    avg_loss:0.041, val_acc:0.965]
Epoch [21/120    avg_loss:0.051, val_acc:0.959]
Epoch [22/120    avg_loss:0.082, val_acc:0.965]
Epoch [23/120    avg_loss:0.041, val_acc:0.955]
Epoch [24/120    avg_loss:0.034, val_acc:0.970]
Epoch [25/120    avg_loss:0.044, val_acc:0.958]
Epoch [26/120    avg_loss:0.042, val_acc:0.967]
Epoch [27/120    avg_loss:0.030, val_acc:0.973]
Epoch [28/120    avg_loss:0.030, val_acc:0.969]
Epoch [29/120    avg_loss:0.028, val_acc:0.974]
Epoch [30/120    avg_loss:0.013, val_acc:0.976]
Epoch [31/120    avg_loss:0.022, val_acc:0.969]
Epoch [32/120    avg_loss:0.082, val_acc:0.965]
Epoch [33/120    avg_loss:0.047, val_acc:0.966]
Epoch [34/120    avg_loss:0.033, val_acc:0.972]
Epoch [35/120    avg_loss:0.053, val_acc:0.961]
Epoch [36/120    avg_loss:0.040, val_acc:0.965]
Epoch [37/120    avg_loss:0.022, val_acc:0.974]
Epoch [38/120    avg_loss:0.027, val_acc:0.962]
Epoch [39/120    avg_loss:0.018, val_acc:0.974]
Epoch [40/120    avg_loss:0.013, val_acc:0.970]
Epoch [41/120    avg_loss:0.008, val_acc:0.977]
Epoch [42/120    avg_loss:0.010, val_acc:0.976]
Epoch [43/120    avg_loss:0.012, val_acc:0.974]
Epoch [44/120    avg_loss:0.013, val_acc:0.977]
Epoch [45/120    avg_loss:0.024, val_acc:0.975]
Epoch [46/120    avg_loss:0.014, val_acc:0.980]
Epoch [47/120    avg_loss:0.023, val_acc:0.975]
Epoch [48/120    avg_loss:0.010, val_acc:0.976]
Epoch [49/120    avg_loss:0.015, val_acc:0.976]
Epoch [50/120    avg_loss:0.016, val_acc:0.974]
Epoch [51/120    avg_loss:0.012, val_acc:0.976]
Epoch [52/120    avg_loss:0.012, val_acc:0.975]
Epoch [53/120    avg_loss:0.012, val_acc:0.975]
Epoch [54/120    avg_loss:0.012, val_acc:0.976]
Epoch [55/120    avg_loss:0.011, val_acc:0.980]
Epoch [56/120    avg_loss:0.006, val_acc:0.977]
Epoch [57/120    avg_loss:0.008, val_acc:0.976]
Epoch [58/120    avg_loss:0.009, val_acc:0.976]
Epoch [59/120    avg_loss:0.012, val_acc:0.976]
Epoch [60/120    avg_loss:0.011, val_acc:0.976]
Epoch [61/120    avg_loss:0.012, val_acc:0.983]
Epoch [62/120    avg_loss:0.009, val_acc:0.981]
Epoch [63/120    avg_loss:0.010, val_acc:0.979]
Epoch [64/120    avg_loss:0.017, val_acc:0.955]
Epoch [65/120    avg_loss:0.017, val_acc:0.976]
Epoch [66/120    avg_loss:0.022, val_acc:0.971]
Epoch [67/120    avg_loss:0.008, val_acc:0.978]
Epoch [68/120    avg_loss:0.011, val_acc:0.975]
Epoch [69/120    avg_loss:0.016, val_acc:0.981]
Epoch [70/120    avg_loss:0.004, val_acc:0.980]
Epoch [71/120    avg_loss:0.006, val_acc:0.981]
Epoch [72/120    avg_loss:0.005, val_acc:0.980]
Epoch [73/120    avg_loss:0.004, val_acc:0.980]
Epoch [74/120    avg_loss:0.010, val_acc:0.977]
Epoch [75/120    avg_loss:0.008, val_acc:0.979]
Epoch [76/120    avg_loss:0.007, val_acc:0.979]
Epoch [77/120    avg_loss:0.006, val_acc:0.979]
Epoch [78/120    avg_loss:0.005, val_acc:0.979]
Epoch [79/120    avg_loss:0.004, val_acc:0.979]
Epoch [80/120    avg_loss:0.005, val_acc:0.979]
Epoch [81/120    avg_loss:0.004, val_acc:0.979]
Epoch [82/120    avg_loss:0.006, val_acc:0.979]
Epoch [83/120    avg_loss:0.009, val_acc:0.977]
Epoch [84/120    avg_loss:0.005, val_acc:0.978]
Epoch [85/120    avg_loss:0.006, val_acc:0.978]
Epoch [86/120    avg_loss:0.003, val_acc:0.980]
Epoch [87/120    avg_loss:0.004, val_acc:0.979]
Epoch [88/120    avg_loss:0.004, val_acc:0.979]
Epoch [89/120    avg_loss:0.007, val_acc:0.979]
Epoch [90/120    avg_loss:0.004, val_acc:0.979]
Epoch [91/120    avg_loss:0.005, val_acc:0.979]
Epoch [92/120    avg_loss:0.004, val_acc:0.979]
Epoch [93/120    avg_loss:0.004, val_acc:0.979]
Epoch [94/120    avg_loss:0.005, val_acc:0.979]
Epoch [95/120    avg_loss:0.005, val_acc:0.979]
Epoch [96/120    avg_loss:0.006, val_acc:0.979]
Epoch [97/120    avg_loss:0.003, val_acc:0.979]
Epoch [98/120    avg_loss:0.005, val_acc:0.979]
Epoch [99/120    avg_loss:0.002, val_acc:0.979]
Epoch [100/120    avg_loss:0.003, val_acc:0.979]
Epoch [101/120    avg_loss:0.004, val_acc:0.979]
Epoch [102/120    avg_loss:0.005, val_acc:0.979]
Epoch [103/120    avg_loss:0.005, val_acc:0.979]
Epoch [104/120    avg_loss:0.005, val_acc:0.979]
Epoch [105/120    avg_loss:0.004, val_acc:0.979]
Epoch [106/120    avg_loss:0.005, val_acc:0.979]
Epoch [107/120    avg_loss:0.006, val_acc:0.979]
Epoch [108/120    avg_loss:0.004, val_acc:0.979]
Epoch [109/120    avg_loss:0.003, val_acc:0.979]
Epoch [110/120    avg_loss:0.005, val_acc:0.979]
Epoch [111/120    avg_loss:0.007, val_acc:0.979]
Epoch [112/120    avg_loss:0.005, val_acc:0.979]
Epoch [113/120    avg_loss:0.004, val_acc:0.979]
Epoch [114/120    avg_loss:0.003, val_acc:0.979]
Epoch [115/120    avg_loss:0.005, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.005, val_acc:0.979]
Epoch [118/120    avg_loss:0.005, val_acc:0.979]
Epoch [119/120    avg_loss:0.004, val_acc:0.979]
Epoch [120/120    avg_loss:0.004, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     0     0     0     1]
 [    0     0 18058     0    11     0    21     0     0     0]
 [    0     5     0  1996     0     0     0     0    34     1]
 [    0    25     1     0  2917     0     1     0    27     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4866     0    10     0]
 [    0     5     0     0     0     0     3  1281     0     1]
 [    0     5     0     9    48     0     0     0  3506     3]
 [    0     0     0     1     5    27     0     0     0   886]]

Accuracy:
99.40471886824284

F1 scores:
[       nan 0.99682244 0.99903184 0.98762989 0.98001008 0.98976109
 0.99621251 0.99649942 0.9809737  0.97792494]

Kappa:
0.9921135221530261
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f35a86e36d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.520, val_acc:0.682]
Epoch [2/120    avg_loss:0.880, val_acc:0.601]
Epoch [3/120    avg_loss:0.625, val_acc:0.686]
Epoch [4/120    avg_loss:0.448, val_acc:0.763]
Epoch [5/120    avg_loss:0.344, val_acc:0.768]
Epoch [6/120    avg_loss:0.239, val_acc:0.871]
Epoch [7/120    avg_loss:0.230, val_acc:0.898]
Epoch [8/120    avg_loss:0.218, val_acc:0.909]
Epoch [9/120    avg_loss:0.154, val_acc:0.922]
Epoch [10/120    avg_loss:0.144, val_acc:0.951]
Epoch [11/120    avg_loss:0.099, val_acc:0.926]
Epoch [12/120    avg_loss:0.094, val_acc:0.953]
Epoch [13/120    avg_loss:0.085, val_acc:0.951]
Epoch [14/120    avg_loss:0.117, val_acc:0.954]
Epoch [15/120    avg_loss:0.078, val_acc:0.958]
Epoch [16/120    avg_loss:0.073, val_acc:0.967]
Epoch [17/120    avg_loss:0.049, val_acc:0.962]
Epoch [18/120    avg_loss:0.057, val_acc:0.949]
Epoch [19/120    avg_loss:0.060, val_acc:0.961]
Epoch [20/120    avg_loss:0.032, val_acc:0.980]
Epoch [21/120    avg_loss:0.025, val_acc:0.979]
Epoch [22/120    avg_loss:0.036, val_acc:0.970]
Epoch [23/120    avg_loss:0.047, val_acc:0.932]
Epoch [24/120    avg_loss:0.066, val_acc:0.979]
Epoch [25/120    avg_loss:0.031, val_acc:0.984]
Epoch [26/120    avg_loss:0.039, val_acc:0.979]
Epoch [27/120    avg_loss:0.046, val_acc:0.964]
Epoch [28/120    avg_loss:0.046, val_acc:0.971]
Epoch [29/120    avg_loss:0.032, val_acc:0.987]
Epoch [30/120    avg_loss:0.023, val_acc:0.987]
Epoch [31/120    avg_loss:0.022, val_acc:0.988]
Epoch [32/120    avg_loss:0.022, val_acc:0.986]
Epoch [33/120    avg_loss:0.024, val_acc:0.977]
Epoch [34/120    avg_loss:0.019, val_acc:0.986]
Epoch [35/120    avg_loss:0.017, val_acc:0.989]
Epoch [36/120    avg_loss:0.028, val_acc:0.987]
Epoch [37/120    avg_loss:0.033, val_acc:0.969]
Epoch [38/120    avg_loss:0.016, val_acc:0.987]
Epoch [39/120    avg_loss:0.012, val_acc:0.991]
Epoch [40/120    avg_loss:0.013, val_acc:0.988]
Epoch [41/120    avg_loss:0.011, val_acc:0.992]
Epoch [42/120    avg_loss:0.029, val_acc:0.992]
Epoch [43/120    avg_loss:0.008, val_acc:0.990]
Epoch [44/120    avg_loss:0.014, val_acc:0.963]
Epoch [45/120    avg_loss:0.012, val_acc:0.988]
Epoch [46/120    avg_loss:0.013, val_acc:0.992]
Epoch [47/120    avg_loss:0.010, val_acc:0.991]
Epoch [48/120    avg_loss:0.010, val_acc:0.992]
Epoch [49/120    avg_loss:0.005, val_acc:0.992]
Epoch [50/120    avg_loss:0.005, val_acc:0.992]
Epoch [51/120    avg_loss:0.014, val_acc:0.989]
Epoch [52/120    avg_loss:0.010, val_acc:0.996]
Epoch [53/120    avg_loss:0.008, val_acc:0.988]
Epoch [54/120    avg_loss:0.015, val_acc:0.986]
Epoch [55/120    avg_loss:0.009, val_acc:0.994]
Epoch [56/120    avg_loss:0.020, val_acc:0.994]
Epoch [57/120    avg_loss:0.011, val_acc:0.979]
Epoch [58/120    avg_loss:0.016, val_acc:0.990]
Epoch [59/120    avg_loss:0.008, val_acc:0.993]
Epoch [60/120    avg_loss:0.008, val_acc:0.992]
Epoch [61/120    avg_loss:0.007, val_acc:0.993]
Epoch [62/120    avg_loss:0.053, val_acc:0.978]
Epoch [63/120    avg_loss:0.026, val_acc:0.987]
Epoch [64/120    avg_loss:0.012, val_acc:0.992]
Epoch [65/120    avg_loss:0.009, val_acc:0.991]
Epoch [66/120    avg_loss:0.010, val_acc:0.994]
Epoch [67/120    avg_loss:0.006, val_acc:0.994]
Epoch [68/120    avg_loss:0.006, val_acc:0.995]
Epoch [69/120    avg_loss:0.005, val_acc:0.996]
Epoch [70/120    avg_loss:0.004, val_acc:0.995]
Epoch [71/120    avg_loss:0.005, val_acc:0.995]
Epoch [72/120    avg_loss:0.007, val_acc:0.996]
Epoch [73/120    avg_loss:0.007, val_acc:0.994]
Epoch [74/120    avg_loss:0.007, val_acc:0.994]
Epoch [75/120    avg_loss:0.011, val_acc:0.995]
Epoch [76/120    avg_loss:0.005, val_acc:0.995]
Epoch [77/120    avg_loss:0.004, val_acc:0.995]
Epoch [78/120    avg_loss:0.005, val_acc:0.994]
Epoch [79/120    avg_loss:0.004, val_acc:0.994]
Epoch [80/120    avg_loss:0.005, val_acc:0.994]
Epoch [81/120    avg_loss:0.005, val_acc:0.993]
Epoch [82/120    avg_loss:0.008, val_acc:0.993]
Epoch [83/120    avg_loss:0.006, val_acc:0.994]
Epoch [84/120    avg_loss:0.011, val_acc:0.994]
Epoch [85/120    avg_loss:0.008, val_acc:0.995]
Epoch [86/120    avg_loss:0.004, val_acc:0.995]
Epoch [87/120    avg_loss:0.006, val_acc:0.995]
Epoch [88/120    avg_loss:0.007, val_acc:0.995]
Epoch [89/120    avg_loss:0.006, val_acc:0.995]
Epoch [90/120    avg_loss:0.005, val_acc:0.995]
Epoch [91/120    avg_loss:0.004, val_acc:0.995]
Epoch [92/120    avg_loss:0.005, val_acc:0.995]
Epoch [93/120    avg_loss:0.005, val_acc:0.995]
Epoch [94/120    avg_loss:0.005, val_acc:0.995]
Epoch [95/120    avg_loss:0.005, val_acc:0.995]
Epoch [96/120    avg_loss:0.007, val_acc:0.995]
Epoch [97/120    avg_loss:0.005, val_acc:0.995]
Epoch [98/120    avg_loss:0.003, val_acc:0.995]
Epoch [99/120    avg_loss:0.005, val_acc:0.995]
Epoch [100/120    avg_loss:0.006, val_acc:0.995]
Epoch [101/120    avg_loss:0.008, val_acc:0.995]
Epoch [102/120    avg_loss:0.004, val_acc:0.995]
Epoch [103/120    avg_loss:0.004, val_acc:0.995]
Epoch [104/120    avg_loss:0.006, val_acc:0.995]
Epoch [105/120    avg_loss:0.005, val_acc:0.995]
Epoch [106/120    avg_loss:0.004, val_acc:0.995]
Epoch [107/120    avg_loss:0.004, val_acc:0.995]
Epoch [108/120    avg_loss:0.005, val_acc:0.995]
Epoch [109/120    avg_loss:0.004, val_acc:0.995]
Epoch [110/120    avg_loss:0.006, val_acc:0.995]
Epoch [111/120    avg_loss:0.006, val_acc:0.995]
Epoch [112/120    avg_loss:0.006, val_acc:0.995]
Epoch [113/120    avg_loss:0.006, val_acc:0.995]
Epoch [114/120    avg_loss:0.007, val_acc:0.995]
Epoch [115/120    avg_loss:0.006, val_acc:0.995]
Epoch [116/120    avg_loss:0.005, val_acc:0.995]
Epoch [117/120    avg_loss:0.005, val_acc:0.995]
Epoch [118/120    avg_loss:0.006, val_acc:0.995]
Epoch [119/120    avg_loss:0.006, val_acc:0.995]
Epoch [120/120    avg_loss:0.007, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0    14    21     0     3]
 [    0     0 18045     0    14     0    31     0     0     0]
 [    0     0     0  2018     0     0     0     0    17     1]
 [    0    32     8     0  2918     0     0     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4866     0    12     0]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     0     0     2    43     0     0     0  3524     2]
 [    0     0     0     0     8    33     0     0     0   878]]

Accuracy:
99.37820837249657

F1 scores:
[       nan 0.99455592 0.9985336  0.99506903 0.98001679 0.98751419
 0.99387255 0.99076212 0.98752977 0.97339246]

Kappa:
0.9917641498235389
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc510147b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.334, val_acc:0.432]
Epoch [2/120    avg_loss:0.765, val_acc:0.578]
Epoch [3/120    avg_loss:0.604, val_acc:0.728]
Epoch [4/120    avg_loss:0.462, val_acc:0.743]
Epoch [5/120    avg_loss:0.374, val_acc:0.738]
Epoch [6/120    avg_loss:0.283, val_acc:0.739]
Epoch [7/120    avg_loss:0.242, val_acc:0.908]
Epoch [8/120    avg_loss:0.182, val_acc:0.922]
Epoch [9/120    avg_loss:0.191, val_acc:0.872]
Epoch [10/120    avg_loss:0.183, val_acc:0.901]
Epoch [11/120    avg_loss:0.127, val_acc:0.932]
Epoch [12/120    avg_loss:0.118, val_acc:0.960]
Epoch [13/120    avg_loss:0.122, val_acc:0.925]
Epoch [14/120    avg_loss:0.115, val_acc:0.959]
Epoch [15/120    avg_loss:0.091, val_acc:0.947]
Epoch [16/120    avg_loss:0.062, val_acc:0.967]
Epoch [17/120    avg_loss:0.057, val_acc:0.976]
Epoch [18/120    avg_loss:0.058, val_acc:0.957]
Epoch [19/120    avg_loss:0.050, val_acc:0.971]
Epoch [20/120    avg_loss:0.088, val_acc:0.935]
Epoch [21/120    avg_loss:0.066, val_acc:0.976]
Epoch [22/120    avg_loss:0.046, val_acc:0.973]
Epoch [23/120    avg_loss:0.054, val_acc:0.982]
Epoch [24/120    avg_loss:0.065, val_acc:0.970]
Epoch [25/120    avg_loss:0.041, val_acc:0.967]
Epoch [26/120    avg_loss:0.039, val_acc:0.977]
Epoch [27/120    avg_loss:0.040, val_acc:0.977]
Epoch [28/120    avg_loss:0.023, val_acc:0.976]
Epoch [29/120    avg_loss:0.034, val_acc:0.968]
Epoch [30/120    avg_loss:0.029, val_acc:0.980]
Epoch [31/120    avg_loss:0.025, val_acc:0.980]
Epoch [32/120    avg_loss:0.022, val_acc:0.975]
Epoch [33/120    avg_loss:0.027, val_acc:0.979]
Epoch [34/120    avg_loss:0.022, val_acc:0.981]
Epoch [35/120    avg_loss:0.020, val_acc:0.974]
Epoch [36/120    avg_loss:0.030, val_acc:0.953]
Epoch [37/120    avg_loss:0.034, val_acc:0.973]
Epoch [38/120    avg_loss:0.019, val_acc:0.980]
Epoch [39/120    avg_loss:0.015, val_acc:0.980]
Epoch [40/120    avg_loss:0.011, val_acc:0.983]
Epoch [41/120    avg_loss:0.008, val_acc:0.983]
Epoch [42/120    avg_loss:0.009, val_acc:0.984]
Epoch [43/120    avg_loss:0.011, val_acc:0.982]
Epoch [44/120    avg_loss:0.009, val_acc:0.983]
Epoch [45/120    avg_loss:0.009, val_acc:0.983]
Epoch [46/120    avg_loss:0.013, val_acc:0.983]
Epoch [47/120    avg_loss:0.016, val_acc:0.982]
Epoch [48/120    avg_loss:0.009, val_acc:0.984]
Epoch [49/120    avg_loss:0.009, val_acc:0.984]
Epoch [50/120    avg_loss:0.010, val_acc:0.984]
Epoch [51/120    avg_loss:0.008, val_acc:0.985]
Epoch [52/120    avg_loss:0.009, val_acc:0.983]
Epoch [53/120    avg_loss:0.011, val_acc:0.983]
Epoch [54/120    avg_loss:0.009, val_acc:0.985]
Epoch [55/120    avg_loss:0.009, val_acc:0.983]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.012, val_acc:0.984]
Epoch [58/120    avg_loss:0.009, val_acc:0.985]
Epoch [59/120    avg_loss:0.013, val_acc:0.982]
Epoch [60/120    avg_loss:0.013, val_acc:0.981]
Epoch [61/120    avg_loss:0.013, val_acc:0.982]
Epoch [62/120    avg_loss:0.010, val_acc:0.983]
Epoch [63/120    avg_loss:0.011, val_acc:0.983]
Epoch [64/120    avg_loss:0.012, val_acc:0.983]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.007, val_acc:0.984]
Epoch [67/120    avg_loss:0.007, val_acc:0.984]
Epoch [68/120    avg_loss:0.008, val_acc:0.983]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.008, val_acc:0.983]
Epoch [72/120    avg_loss:0.008, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.983]
Epoch [74/120    avg_loss:0.008, val_acc:0.983]
Epoch [75/120    avg_loss:0.009, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.007, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.007, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.006, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.983]
Epoch [88/120    avg_loss:0.013, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.008, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     1     0     0     7     3     1     2]
 [    0     0 18034     0    23     0    33     0     0     0]
 [    0     3     0  2014     0     0     0     0    16     3]
 [    0    26     7     2  2906     0     9     0    20     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     2  1281     0     7]
 [    0     2     0    13    35     0     0     0  3520     1]
 [    0     0     0    19    10    35     0     0     0   855]]

Accuracy:
99.32036729086835

F1 scores:
[       nan 0.99650648 0.99825634 0.98604651 0.97746384 0.98676749
 0.99479963 0.995338   0.98765432 0.95584125]

Kappa:
0.9909982870549947
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd4453a97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.439, val_acc:0.589]
Epoch [2/120    avg_loss:0.841, val_acc:0.508]
Epoch [3/120    avg_loss:0.621, val_acc:0.697]
Epoch [4/120    avg_loss:0.447, val_acc:0.755]
Epoch [5/120    avg_loss:0.367, val_acc:0.750]
Epoch [6/120    avg_loss:0.280, val_acc:0.844]
Epoch [7/120    avg_loss:0.234, val_acc:0.856]
Epoch [8/120    avg_loss:0.196, val_acc:0.918]
Epoch [9/120    avg_loss:0.172, val_acc:0.885]
Epoch [10/120    avg_loss:0.120, val_acc:0.911]
Epoch [11/120    avg_loss:0.151, val_acc:0.909]
Epoch [12/120    avg_loss:0.119, val_acc:0.901]
Epoch [13/120    avg_loss:0.094, val_acc:0.953]
Epoch [14/120    avg_loss:0.064, val_acc:0.958]
Epoch [15/120    avg_loss:0.077, val_acc:0.958]
Epoch [16/120    avg_loss:0.089, val_acc:0.951]
Epoch [17/120    avg_loss:0.102, val_acc:0.946]
Epoch [18/120    avg_loss:0.075, val_acc:0.962]
Epoch [19/120    avg_loss:0.039, val_acc:0.970]
Epoch [20/120    avg_loss:0.057, val_acc:0.962]
Epoch [21/120    avg_loss:0.039, val_acc:0.978]
Epoch [22/120    avg_loss:0.054, val_acc:0.965]
Epoch [23/120    avg_loss:0.037, val_acc:0.969]
Epoch [24/120    avg_loss:0.033, val_acc:0.975]
Epoch [25/120    avg_loss:0.033, val_acc:0.960]
Epoch [26/120    avg_loss:0.027, val_acc:0.973]
Epoch [27/120    avg_loss:0.018, val_acc:0.976]
Epoch [28/120    avg_loss:0.016, val_acc:0.982]
Epoch [29/120    avg_loss:0.015, val_acc:0.981]
Epoch [30/120    avg_loss:0.028, val_acc:0.930]
Epoch [31/120    avg_loss:0.030, val_acc:0.977]
Epoch [32/120    avg_loss:0.018, val_acc:0.972]
Epoch [33/120    avg_loss:0.015, val_acc:0.981]
Epoch [34/120    avg_loss:0.010, val_acc:0.979]
Epoch [35/120    avg_loss:0.016, val_acc:0.964]
Epoch [36/120    avg_loss:0.047, val_acc:0.968]
Epoch [37/120    avg_loss:0.018, val_acc:0.982]
Epoch [38/120    avg_loss:0.020, val_acc:0.974]
Epoch [39/120    avg_loss:0.019, val_acc:0.973]
Epoch [40/120    avg_loss:0.027, val_acc:0.981]
Epoch [41/120    avg_loss:0.022, val_acc:0.981]
Epoch [42/120    avg_loss:0.020, val_acc:0.982]
Epoch [43/120    avg_loss:0.017, val_acc:0.972]
Epoch [44/120    avg_loss:0.024, val_acc:0.972]
Epoch [45/120    avg_loss:0.043, val_acc:0.972]
Epoch [46/120    avg_loss:0.035, val_acc:0.978]
Epoch [47/120    avg_loss:0.020, val_acc:0.977]
Epoch [48/120    avg_loss:0.014, val_acc:0.978]
Epoch [49/120    avg_loss:0.017, val_acc:0.979]
Epoch [50/120    avg_loss:0.012, val_acc:0.987]
Epoch [51/120    avg_loss:0.019, val_acc:0.974]
Epoch [52/120    avg_loss:0.015, val_acc:0.977]
Epoch [53/120    avg_loss:0.026, val_acc:0.981]
Epoch [54/120    avg_loss:0.016, val_acc:0.974]
Epoch [55/120    avg_loss:0.013, val_acc:0.979]
Epoch [56/120    avg_loss:0.009, val_acc:0.983]
Epoch [57/120    avg_loss:0.010, val_acc:0.984]
Epoch [58/120    avg_loss:0.010, val_acc:0.978]
Epoch [59/120    avg_loss:0.006, val_acc:0.985]
Epoch [60/120    avg_loss:0.007, val_acc:0.982]
Epoch [61/120    avg_loss:0.010, val_acc:0.984]
Epoch [62/120    avg_loss:0.006, val_acc:0.977]
Epoch [63/120    avg_loss:0.009, val_acc:0.983]
Epoch [64/120    avg_loss:0.010, val_acc:0.984]
Epoch [65/120    avg_loss:0.006, val_acc:0.985]
Epoch [66/120    avg_loss:0.005, val_acc:0.985]
Epoch [67/120    avg_loss:0.004, val_acc:0.986]
Epoch [68/120    avg_loss:0.006, val_acc:0.985]
Epoch [69/120    avg_loss:0.004, val_acc:0.985]
Epoch [70/120    avg_loss:0.004, val_acc:0.986]
Epoch [71/120    avg_loss:0.004, val_acc:0.984]
Epoch [72/120    avg_loss:0.003, val_acc:0.985]
Epoch [73/120    avg_loss:0.008, val_acc:0.987]
Epoch [74/120    avg_loss:0.003, val_acc:0.986]
Epoch [75/120    avg_loss:0.005, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.986]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.003, val_acc:0.985]
Epoch [80/120    avg_loss:0.004, val_acc:0.985]
Epoch [81/120    avg_loss:0.004, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.984]
Epoch [83/120    avg_loss:0.003, val_acc:0.986]
Epoch [84/120    avg_loss:0.004, val_acc:0.986]
Epoch [85/120    avg_loss:0.004, val_acc:0.986]
Epoch [86/120    avg_loss:0.012, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.982]
Epoch [89/120    avg_loss:0.004, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.003, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.003, val_acc:0.987]
Epoch [96/120    avg_loss:0.003, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.987]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.002, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.003, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     0     2     0     0    19     0     0]
 [    0     0 18069     0     5     0    13     0     1     2]
 [    0     9     0  2015     0     0     0     0    10     2]
 [    0    27     4     0  2923     0     1     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     1     0     5    59     0     0     0  3497     9]
 [    0     0     0     4    14    21     0     0     0   880]]

Accuracy:
99.45050972453186

F1 scores:
[       nan 0.99549689 0.99930869 0.99261084 0.97841004 0.99201824
 0.99826051 0.99152542 0.9856257  0.97130243]

Kappa:
0.9927200095070461
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fa2aa47b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.429, val_acc:0.405]
Epoch [2/120    avg_loss:0.703, val_acc:0.660]
Epoch [3/120    avg_loss:0.503, val_acc:0.740]
Epoch [4/120    avg_loss:0.447, val_acc:0.733]
Epoch [5/120    avg_loss:0.330, val_acc:0.736]
Epoch [6/120    avg_loss:0.317, val_acc:0.779]
Epoch [7/120    avg_loss:0.254, val_acc:0.879]
Epoch [8/120    avg_loss:0.200, val_acc:0.916]
Epoch [9/120    avg_loss:0.158, val_acc:0.861]
Epoch [10/120    avg_loss:0.189, val_acc:0.949]
Epoch [11/120    avg_loss:0.142, val_acc:0.938]
Epoch [12/120    avg_loss:0.141, val_acc:0.949]
Epoch [13/120    avg_loss:0.154, val_acc:0.952]
Epoch [14/120    avg_loss:0.081, val_acc:0.960]
Epoch [15/120    avg_loss:0.080, val_acc:0.951]
Epoch [16/120    avg_loss:0.081, val_acc:0.957]
Epoch [17/120    avg_loss:0.146, val_acc:0.905]
Epoch [18/120    avg_loss:0.126, val_acc:0.936]
Epoch [19/120    avg_loss:0.079, val_acc:0.959]
Epoch [20/120    avg_loss:0.064, val_acc:0.959]
Epoch [21/120    avg_loss:0.056, val_acc:0.967]
Epoch [22/120    avg_loss:0.067, val_acc:0.934]
Epoch [23/120    avg_loss:0.042, val_acc:0.969]
Epoch [24/120    avg_loss:0.072, val_acc:0.952]
Epoch [25/120    avg_loss:0.078, val_acc:0.972]
Epoch [26/120    avg_loss:0.080, val_acc:0.970]
Epoch [27/120    avg_loss:0.044, val_acc:0.974]
Epoch [28/120    avg_loss:0.042, val_acc:0.964]
Epoch [29/120    avg_loss:0.032, val_acc:0.971]
Epoch [30/120    avg_loss:0.046, val_acc:0.967]
Epoch [31/120    avg_loss:0.046, val_acc:0.959]
Epoch [32/120    avg_loss:0.032, val_acc:0.978]
Epoch [33/120    avg_loss:0.062, val_acc:0.968]
Epoch [34/120    avg_loss:0.087, val_acc:0.970]
Epoch [35/120    avg_loss:0.034, val_acc:0.976]
Epoch [36/120    avg_loss:0.016, val_acc:0.978]
Epoch [37/120    avg_loss:0.013, val_acc:0.980]
Epoch [38/120    avg_loss:0.032, val_acc:0.975]
Epoch [39/120    avg_loss:0.054, val_acc:0.980]
Epoch [40/120    avg_loss:0.025, val_acc:0.981]
Epoch [41/120    avg_loss:0.024, val_acc:0.980]
Epoch [42/120    avg_loss:0.025, val_acc:0.969]
Epoch [43/120    avg_loss:0.020, val_acc:0.969]
Epoch [44/120    avg_loss:0.014, val_acc:0.976]
Epoch [45/120    avg_loss:0.013, val_acc:0.978]
Epoch [46/120    avg_loss:0.021, val_acc:0.959]
Epoch [47/120    avg_loss:0.016, val_acc:0.985]
Epoch [48/120    avg_loss:0.011, val_acc:0.981]
Epoch [49/120    avg_loss:0.021, val_acc:0.981]
Epoch [50/120    avg_loss:0.017, val_acc:0.971]
Epoch [51/120    avg_loss:0.013, val_acc:0.979]
Epoch [52/120    avg_loss:0.014, val_acc:0.979]
Epoch [53/120    avg_loss:0.013, val_acc:0.981]
Epoch [54/120    avg_loss:0.019, val_acc:0.986]
Epoch [55/120    avg_loss:0.011, val_acc:0.982]
Epoch [56/120    avg_loss:0.008, val_acc:0.980]
Epoch [57/120    avg_loss:0.011, val_acc:0.982]
Epoch [58/120    avg_loss:0.010, val_acc:0.977]
Epoch [59/120    avg_loss:0.016, val_acc:0.977]
Epoch [60/120    avg_loss:0.009, val_acc:0.973]
Epoch [61/120    avg_loss:0.011, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.978]
Epoch [63/120    avg_loss:0.023, val_acc:0.984]
Epoch [64/120    avg_loss:0.010, val_acc:0.984]
Epoch [65/120    avg_loss:0.006, val_acc:0.988]
Epoch [66/120    avg_loss:0.004, val_acc:0.987]
Epoch [67/120    avg_loss:0.007, val_acc:0.984]
Epoch [68/120    avg_loss:0.007, val_acc:0.982]
Epoch [69/120    avg_loss:0.007, val_acc:0.978]
Epoch [70/120    avg_loss:0.015, val_acc:0.974]
Epoch [71/120    avg_loss:0.008, val_acc:0.976]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.981]
Epoch [75/120    avg_loss:0.005, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.976]
Epoch [77/120    avg_loss:0.018, val_acc:0.981]
Epoch [78/120    avg_loss:0.092, val_acc:0.936]
Epoch [79/120    avg_loss:0.140, val_acc:0.952]
Epoch [80/120    avg_loss:0.071, val_acc:0.932]
Epoch [81/120    avg_loss:0.028, val_acc:0.976]
Epoch [82/120    avg_loss:0.022, val_acc:0.968]
Epoch [83/120    avg_loss:0.029, val_acc:0.981]
Epoch [84/120    avg_loss:0.020, val_acc:0.976]
Epoch [85/120    avg_loss:0.018, val_acc:0.973]
Epoch [86/120    avg_loss:0.021, val_acc:0.980]
Epoch [87/120    avg_loss:0.014, val_acc:0.981]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.009, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.981]
Epoch [91/120    avg_loss:0.007, val_acc:0.982]
Epoch [92/120    avg_loss:0.006, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.982]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.010, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.006, val_acc:0.981]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.006, val_acc:0.981]
Epoch [106/120    avg_loss:0.006, val_acc:0.981]
Epoch [107/120    avg_loss:0.009, val_acc:0.981]
Epoch [108/120    avg_loss:0.007, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.981]
Epoch [110/120    avg_loss:0.010, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.004, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     0     0    14    13     0     4]
 [    0     2 18047     0    33     0     7     0     1     0]
 [    0     8     0  2000     1     0     0     0    22     5]
 [    0    26     2     1  2930     0     0     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4853     0    24     0]
 [    0     0     0     0     0     0    16  1273     0     1]
 [    0     3     0    20    38     0     0     0  3497    13]
 [    0     0     0     0    15    27     0     0     0   877]]

Accuracy:
99.25288602896875

F1 scores:
[       nan 0.99456184 0.99872717 0.98595021 0.97846051 0.98976109
 0.99365274 0.98835404 0.9812009  0.96426608]

Kappa:
0.9901044605353324
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f18d644d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.483, val_acc:0.515]
Epoch [2/120    avg_loss:0.952, val_acc:0.687]
Epoch [3/120    avg_loss:0.671, val_acc:0.668]
Epoch [4/120    avg_loss:0.531, val_acc:0.715]
Epoch [5/120    avg_loss:0.408, val_acc:0.807]
Epoch [6/120    avg_loss:0.332, val_acc:0.863]
Epoch [7/120    avg_loss:0.248, val_acc:0.903]
Epoch [8/120    avg_loss:0.224, val_acc:0.814]
Epoch [9/120    avg_loss:0.176, val_acc:0.856]
Epoch [10/120    avg_loss:0.121, val_acc:0.897]
Epoch [11/120    avg_loss:0.125, val_acc:0.928]
Epoch [12/120    avg_loss:0.144, val_acc:0.939]
Epoch [13/120    avg_loss:0.142, val_acc:0.926]
Epoch [14/120    avg_loss:0.121, val_acc:0.922]
Epoch [15/120    avg_loss:0.083, val_acc:0.950]
Epoch [16/120    avg_loss:0.086, val_acc:0.963]
Epoch [17/120    avg_loss:0.065, val_acc:0.949]
Epoch [18/120    avg_loss:0.046, val_acc:0.960]
Epoch [19/120    avg_loss:0.070, val_acc:0.947]
Epoch [20/120    avg_loss:0.038, val_acc:0.974]
Epoch [21/120    avg_loss:0.050, val_acc:0.969]
Epoch [22/120    avg_loss:0.054, val_acc:0.979]
Epoch [23/120    avg_loss:0.046, val_acc:0.980]
Epoch [24/120    avg_loss:0.051, val_acc:0.966]
Epoch [25/120    avg_loss:0.046, val_acc:0.976]
Epoch [26/120    avg_loss:0.033, val_acc:0.976]
Epoch [27/120    avg_loss:0.023, val_acc:0.979]
Epoch [28/120    avg_loss:0.036, val_acc:0.972]
Epoch [29/120    avg_loss:0.023, val_acc:0.980]
Epoch [30/120    avg_loss:0.044, val_acc:0.970]
Epoch [31/120    avg_loss:0.043, val_acc:0.971]
Epoch [32/120    avg_loss:0.038, val_acc:0.954]
Epoch [33/120    avg_loss:0.036, val_acc:0.971]
Epoch [34/120    avg_loss:0.031, val_acc:0.980]
Epoch [35/120    avg_loss:0.021, val_acc:0.971]
Epoch [36/120    avg_loss:0.017, val_acc:0.982]
Epoch [37/120    avg_loss:0.034, val_acc:0.985]
Epoch [38/120    avg_loss:0.051, val_acc:0.974]
Epoch [39/120    avg_loss:0.027, val_acc:0.968]
Epoch [40/120    avg_loss:0.030, val_acc:0.976]
Epoch [41/120    avg_loss:0.029, val_acc:0.985]
Epoch [42/120    avg_loss:0.013, val_acc:0.980]
Epoch [43/120    avg_loss:0.020, val_acc:0.973]
Epoch [44/120    avg_loss:0.030, val_acc:0.971]
Epoch [45/120    avg_loss:0.013, val_acc:0.985]
Epoch [46/120    avg_loss:0.014, val_acc:0.977]
Epoch [47/120    avg_loss:0.011, val_acc:0.982]
Epoch [48/120    avg_loss:0.015, val_acc:0.985]
Epoch [49/120    avg_loss:0.014, val_acc:0.984]
Epoch [50/120    avg_loss:0.011, val_acc:0.987]
Epoch [51/120    avg_loss:0.010, val_acc:0.990]
Epoch [52/120    avg_loss:0.007, val_acc:0.986]
Epoch [53/120    avg_loss:0.006, val_acc:0.988]
Epoch [54/120    avg_loss:0.006, val_acc:0.989]
Epoch [55/120    avg_loss:0.010, val_acc:0.984]
Epoch [56/120    avg_loss:0.005, val_acc:0.984]
Epoch [57/120    avg_loss:0.005, val_acc:0.990]
Epoch [58/120    avg_loss:0.014, val_acc:0.989]
Epoch [59/120    avg_loss:0.018, val_acc:0.987]
Epoch [60/120    avg_loss:0.018, val_acc:0.986]
Epoch [61/120    avg_loss:0.019, val_acc:0.989]
Epoch [62/120    avg_loss:0.010, val_acc:0.989]
Epoch [63/120    avg_loss:0.008, val_acc:0.986]
Epoch [64/120    avg_loss:0.007, val_acc:0.979]
Epoch [65/120    avg_loss:0.007, val_acc:0.990]
Epoch [66/120    avg_loss:0.008, val_acc:0.984]
Epoch [67/120    avg_loss:0.008, val_acc:0.991]
Epoch [68/120    avg_loss:0.005, val_acc:0.982]
Epoch [69/120    avg_loss:0.030, val_acc:0.979]
Epoch [70/120    avg_loss:0.014, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.007, val_acc:0.990]
Epoch [74/120    avg_loss:0.006, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.983]
Epoch [77/120    avg_loss:0.004, val_acc:0.989]
Epoch [78/120    avg_loss:0.004, val_acc:0.990]
Epoch [79/120    avg_loss:0.008, val_acc:0.990]
Epoch [80/120    avg_loss:0.010, val_acc:0.991]
Epoch [81/120    avg_loss:0.003, val_acc:0.991]
Epoch [82/120    avg_loss:0.018, val_acc:0.941]
Epoch [83/120    avg_loss:0.016, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.990]
Epoch [87/120    avg_loss:0.004, val_acc:0.990]
Epoch [88/120    avg_loss:0.003, val_acc:0.991]
Epoch [89/120    avg_loss:0.003, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.003, val_acc:0.990]
Epoch [92/120    avg_loss:0.003, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.017, val_acc:0.982]
Epoch [100/120    avg_loss:0.012, val_acc:0.986]
Epoch [101/120    avg_loss:0.016, val_acc:0.991]
Epoch [102/120    avg_loss:0.031, val_acc:0.973]
Epoch [103/120    avg_loss:0.040, val_acc:0.973]
Epoch [104/120    avg_loss:0.045, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.983]
Epoch [106/120    avg_loss:0.023, val_acc:0.980]
Epoch [107/120    avg_loss:0.047, val_acc:0.984]
Epoch [108/120    avg_loss:0.023, val_acc:0.983]
Epoch [109/120    avg_loss:0.018, val_acc:0.980]
Epoch [110/120    avg_loss:0.012, val_acc:0.988]
Epoch [111/120    avg_loss:0.018, val_acc:0.986]
Epoch [112/120    avg_loss:0.015, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.993]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.013, val_acc:0.898]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     0 13048     0     1     0  5041     0     0     0]
 [    0     0     0  2024     0     0     0     0     5     7]
 [    0    38    43     0  2872     0     0     0    13     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4865     0    13     0]
 [    0     2     0     0     0     0     0  1288     0     0]
 [    0     2     0    14    57     0     0     0  3497     1]
 [    0     0     0     8     6    30     0     0     0   875]]

Accuracy:
87.25809172631529

F1 scores:
[       nan 0.9967457  0.83691992 0.99167075 0.97224103 0.98863636
 0.65814394 0.9992242  0.98520918 0.96792035]

Kappa:
0.8393080100504636
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d1418d7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.435, val_acc:0.430]
Epoch [2/120    avg_loss:0.816, val_acc:0.706]
Epoch [3/120    avg_loss:0.521, val_acc:0.741]
Epoch [4/120    avg_loss:0.406, val_acc:0.695]
Epoch [5/120    avg_loss:0.334, val_acc:0.771]
Epoch [6/120    avg_loss:0.286, val_acc:0.857]
Epoch [7/120    avg_loss:0.217, val_acc:0.882]
Epoch [8/120    avg_loss:0.162, val_acc:0.861]
Epoch [9/120    avg_loss:0.182, val_acc:0.843]
Epoch [10/120    avg_loss:0.141, val_acc:0.944]
Epoch [11/120    avg_loss:0.088, val_acc:0.937]
Epoch [12/120    avg_loss:0.084, val_acc:0.954]
Epoch [13/120    avg_loss:0.094, val_acc:0.956]
Epoch [14/120    avg_loss:0.081, val_acc:0.960]
Epoch [15/120    avg_loss:0.103, val_acc:0.929]
Epoch [16/120    avg_loss:0.070, val_acc:0.958]
Epoch [17/120    avg_loss:0.060, val_acc:0.965]
Epoch [18/120    avg_loss:0.051, val_acc:0.964]
Epoch [19/120    avg_loss:0.049, val_acc:0.978]
Epoch [20/120    avg_loss:0.042, val_acc:0.981]
Epoch [21/120    avg_loss:0.067, val_acc:0.968]
Epoch [22/120    avg_loss:0.072, val_acc:0.956]
Epoch [23/120    avg_loss:0.043, val_acc:0.972]
Epoch [24/120    avg_loss:0.036, val_acc:0.974]
Epoch [25/120    avg_loss:0.075, val_acc:0.954]
Epoch [26/120    avg_loss:0.038, val_acc:0.948]
Epoch [27/120    avg_loss:0.046, val_acc:0.968]
Epoch [28/120    avg_loss:0.023, val_acc:0.977]
Epoch [29/120    avg_loss:0.027, val_acc:0.979]
Epoch [30/120    avg_loss:0.021, val_acc:0.974]
Epoch [31/120    avg_loss:0.028, val_acc:0.975]
Epoch [32/120    avg_loss:0.043, val_acc:0.933]
Epoch [33/120    avg_loss:0.031, val_acc:0.978]
Epoch [34/120    avg_loss:0.028, val_acc:0.981]
Epoch [35/120    avg_loss:0.011, val_acc:0.981]
Epoch [36/120    avg_loss:0.010, val_acc:0.981]
Epoch [37/120    avg_loss:0.014, val_acc:0.981]
Epoch [38/120    avg_loss:0.015, val_acc:0.980]
Epoch [39/120    avg_loss:0.016, val_acc:0.981]
Epoch [40/120    avg_loss:0.016, val_acc:0.981]
Epoch [41/120    avg_loss:0.010, val_acc:0.981]
Epoch [42/120    avg_loss:0.011, val_acc:0.983]
Epoch [43/120    avg_loss:0.014, val_acc:0.982]
Epoch [44/120    avg_loss:0.009, val_acc:0.986]
Epoch [45/120    avg_loss:0.012, val_acc:0.986]
Epoch [46/120    avg_loss:0.012, val_acc:0.984]
Epoch [47/120    avg_loss:0.010, val_acc:0.984]
Epoch [48/120    avg_loss:0.012, val_acc:0.981]
Epoch [49/120    avg_loss:0.007, val_acc:0.983]
Epoch [50/120    avg_loss:0.011, val_acc:0.984]
Epoch [51/120    avg_loss:0.008, val_acc:0.986]
Epoch [52/120    avg_loss:0.012, val_acc:0.985]
Epoch [53/120    avg_loss:0.013, val_acc:0.982]
Epoch [54/120    avg_loss:0.008, val_acc:0.984]
Epoch [55/120    avg_loss:0.009, val_acc:0.983]
Epoch [56/120    avg_loss:0.012, val_acc:0.983]
Epoch [57/120    avg_loss:0.010, val_acc:0.983]
Epoch [58/120    avg_loss:0.009, val_acc:0.983]
Epoch [59/120    avg_loss:0.011, val_acc:0.984]
Epoch [60/120    avg_loss:0.009, val_acc:0.984]
Epoch [61/120    avg_loss:0.011, val_acc:0.984]
Epoch [62/120    avg_loss:0.008, val_acc:0.985]
Epoch [63/120    avg_loss:0.008, val_acc:0.984]
Epoch [64/120    avg_loss:0.014, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.985]
Epoch [66/120    avg_loss:0.009, val_acc:0.985]
Epoch [67/120    avg_loss:0.007, val_acc:0.985]
Epoch [68/120    avg_loss:0.007, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.985]
Epoch [70/120    avg_loss:0.009, val_acc:0.985]
Epoch [71/120    avg_loss:0.009, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.985]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.014, val_acc:0.986]
Epoch [77/120    avg_loss:0.007, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.986]
Epoch [80/120    avg_loss:0.012, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.986]
Epoch [90/120    avg_loss:0.012, val_acc:0.986]
Epoch [91/120    avg_loss:0.015, val_acc:0.985]
Epoch [92/120    avg_loss:0.013, val_acc:0.985]
Epoch [93/120    avg_loss:0.012, val_acc:0.985]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.013, val_acc:0.985]
Epoch [101/120    avg_loss:0.013, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.013, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.986]
Epoch [116/120    avg_loss:0.011, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.012, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     0     0     0    24     7     3]
 [    0     4 18047     0    26     0     9     0     4     0]
 [    0     4     0  2011     1     0     0     0    14     6]
 [    0    25    10     2  2904     0     0     0    22     9]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4856     0    14     0]
 [    0     0     0     0     0     0     7  1277     0     6]
 [    0     0     0    21    58     0     0     0  3473    19]
 [    0     0     0     0    10    26     0     0     0   883]]

Accuracy:
99.1829947220013

F1 scores:
[       nan 0.99479126 0.99831282 0.98820639 0.97270139 0.99013657
 0.99610256 0.9857198  0.97762139 0.95718157]

Kappa:
0.9891782622816654
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7d54a6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.436, val_acc:0.477]
Epoch [2/120    avg_loss:0.782, val_acc:0.641]
Epoch [3/120    avg_loss:0.606, val_acc:0.729]
Epoch [4/120    avg_loss:0.483, val_acc:0.760]
Epoch [5/120    avg_loss:0.363, val_acc:0.764]
Epoch [6/120    avg_loss:0.298, val_acc:0.899]
Epoch [7/120    avg_loss:0.240, val_acc:0.905]
Epoch [8/120    avg_loss:0.217, val_acc:0.916]
Epoch [9/120    avg_loss:0.138, val_acc:0.938]
Epoch [10/120    avg_loss:0.151, val_acc:0.937]
Epoch [11/120    avg_loss:0.116, val_acc:0.956]
Epoch [12/120    avg_loss:0.126, val_acc:0.947]
Epoch [13/120    avg_loss:0.109, val_acc:0.964]
Epoch [14/120    avg_loss:0.122, val_acc:0.957]
Epoch [15/120    avg_loss:0.152, val_acc:0.953]
Epoch [16/120    avg_loss:0.142, val_acc:0.967]
Epoch [17/120    avg_loss:0.068, val_acc:0.974]
Epoch [18/120    avg_loss:0.074, val_acc:0.953]
Epoch [19/120    avg_loss:0.052, val_acc:0.976]
Epoch [20/120    avg_loss:0.043, val_acc:0.970]
Epoch [21/120    avg_loss:0.049, val_acc:0.971]
Epoch [22/120    avg_loss:0.049, val_acc:0.972]
Epoch [23/120    avg_loss:0.107, val_acc:0.949]
Epoch [24/120    avg_loss:0.060, val_acc:0.977]
Epoch [25/120    avg_loss:0.037, val_acc:0.965]
Epoch [26/120    avg_loss:0.028, val_acc:0.984]
Epoch [27/120    avg_loss:0.027, val_acc:0.979]
Epoch [28/120    avg_loss:0.016, val_acc:0.984]
Epoch [29/120    avg_loss:0.017, val_acc:0.983]
Epoch [30/120    avg_loss:0.032, val_acc:0.977]
Epoch [31/120    avg_loss:0.031, val_acc:0.979]
Epoch [32/120    avg_loss:0.017, val_acc:0.987]
Epoch [33/120    avg_loss:0.019, val_acc:0.989]
Epoch [34/120    avg_loss:0.019, val_acc:0.984]
Epoch [35/120    avg_loss:0.018, val_acc:0.984]
Epoch [36/120    avg_loss:0.027, val_acc:0.984]
Epoch [37/120    avg_loss:0.040, val_acc:0.976]
Epoch [38/120    avg_loss:0.086, val_acc:0.951]
Epoch [39/120    avg_loss:0.101, val_acc:0.966]
Epoch [40/120    avg_loss:0.027, val_acc:0.983]
Epoch [41/120    avg_loss:0.022, val_acc:0.988]
Epoch [42/120    avg_loss:0.018, val_acc:0.983]
Epoch [43/120    avg_loss:0.008, val_acc:0.990]
Epoch [44/120    avg_loss:0.015, val_acc:0.975]
Epoch [45/120    avg_loss:0.017, val_acc:0.984]
Epoch [46/120    avg_loss:0.014, val_acc:0.989]
Epoch [47/120    avg_loss:0.011, val_acc:0.980]
Epoch [48/120    avg_loss:0.019, val_acc:0.987]
Epoch [49/120    avg_loss:0.006, val_acc:0.989]
Epoch [50/120    avg_loss:0.010, val_acc:0.987]
Epoch [51/120    avg_loss:0.023, val_acc:0.984]
Epoch [52/120    avg_loss:0.011, val_acc:0.991]
Epoch [53/120    avg_loss:0.009, val_acc:0.984]
Epoch [54/120    avg_loss:0.008, val_acc:0.990]
Epoch [55/120    avg_loss:0.010, val_acc:0.987]
Epoch [56/120    avg_loss:0.011, val_acc:0.990]
Epoch [57/120    avg_loss:0.008, val_acc:0.990]
Epoch [58/120    avg_loss:0.012, val_acc:0.990]
Epoch [59/120    avg_loss:0.020, val_acc:0.965]
Epoch [60/120    avg_loss:0.014, val_acc:0.988]
Epoch [61/120    avg_loss:0.009, val_acc:0.988]
Epoch [62/120    avg_loss:0.005, val_acc:0.993]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.006, val_acc:0.990]
Epoch [65/120    avg_loss:0.004, val_acc:0.994]
Epoch [66/120    avg_loss:0.035, val_acc:0.976]
Epoch [67/120    avg_loss:0.042, val_acc:0.987]
Epoch [68/120    avg_loss:0.016, val_acc:0.979]
Epoch [69/120    avg_loss:0.010, val_acc:0.993]
Epoch [70/120    avg_loss:0.013, val_acc:0.987]
Epoch [71/120    avg_loss:0.009, val_acc:0.992]
Epoch [72/120    avg_loss:0.009, val_acc:0.990]
Epoch [73/120    avg_loss:0.007, val_acc:0.993]
Epoch [74/120    avg_loss:0.005, val_acc:0.992]
Epoch [75/120    avg_loss:0.004, val_acc:0.994]
Epoch [76/120    avg_loss:0.004, val_acc:0.994]
Epoch [77/120    avg_loss:0.007, val_acc:0.992]
Epoch [78/120    avg_loss:0.006, val_acc:0.995]
Epoch [79/120    avg_loss:0.006, val_acc:0.991]
Epoch [80/120    avg_loss:0.008, val_acc:0.989]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.992]
Epoch [83/120    avg_loss:0.004, val_acc:0.995]
Epoch [84/120    avg_loss:0.003, val_acc:0.994]
Epoch [85/120    avg_loss:0.003, val_acc:0.991]
Epoch [86/120    avg_loss:0.003, val_acc:0.994]
Epoch [87/120    avg_loss:0.004, val_acc:0.990]
Epoch [88/120    avg_loss:0.004, val_acc:0.990]
Epoch [89/120    avg_loss:0.002, val_acc:0.990]
Epoch [90/120    avg_loss:0.003, val_acc:0.992]
Epoch [91/120    avg_loss:0.007, val_acc:0.993]
Epoch [92/120    avg_loss:0.003, val_acc:0.990]
Epoch [93/120    avg_loss:0.002, val_acc:0.992]
Epoch [94/120    avg_loss:0.006, val_acc:0.991]
Epoch [95/120    avg_loss:0.003, val_acc:0.994]
Epoch [96/120    avg_loss:0.017, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.003, val_acc:0.991]
Epoch [107/120    avg_loss:0.002, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.002, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.003, val_acc:0.992]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.003, val_acc:0.992]
Epoch [119/120    avg_loss:0.003, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     2     0     0     4     9     8]
 [    0     7 18052     0    22     0     9     0     0     0]
 [    0     4     0  2008     3     0     0     0    18     3]
 [    0    16     2     0  2940     0     0     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4872     0     0     0]
 [    0     2     0     0     0     0     2  1275     0    11]
 [    0     3     0    15    44     0     0     0  3506     3]
 [    0     0     0    19    14    36     0     0     0   850]]

Accuracy:
99.3348275612754

F1 scores:
[       nan 0.99572749 0.99872752 0.98479647 0.98049025 0.98639456
 0.99825838 0.99260413 0.98510818 0.94760312]

Kappa:
0.9911886677732101
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5389e2d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.412, val_acc:0.482]
Epoch [2/120    avg_loss:0.836, val_acc:0.641]
Epoch [3/120    avg_loss:0.651, val_acc:0.560]
Epoch [4/120    avg_loss:0.479, val_acc:0.766]
Epoch [5/120    avg_loss:0.339, val_acc:0.839]
Epoch [6/120    avg_loss:0.298, val_acc:0.824]
Epoch [7/120    avg_loss:0.261, val_acc:0.862]
Epoch [8/120    avg_loss:0.208, val_acc:0.929]
Epoch [9/120    avg_loss:0.203, val_acc:0.945]
Epoch [10/120    avg_loss:0.135, val_acc:0.945]
Epoch [11/120    avg_loss:0.146, val_acc:0.931]
Epoch [12/120    avg_loss:0.114, val_acc:0.967]
Epoch [13/120    avg_loss:0.132, val_acc:0.947]
Epoch [14/120    avg_loss:0.101, val_acc:0.962]
Epoch [15/120    avg_loss:0.098, val_acc:0.941]
Epoch [16/120    avg_loss:0.079, val_acc:0.976]
Epoch [17/120    avg_loss:0.063, val_acc:0.976]
Epoch [18/120    avg_loss:0.069, val_acc:0.975]
Epoch [19/120    avg_loss:0.078, val_acc:0.968]
Epoch [20/120    avg_loss:0.043, val_acc:0.977]
Epoch [21/120    avg_loss:0.046, val_acc:0.977]
Epoch [22/120    avg_loss:0.042, val_acc:0.984]
Epoch [23/120    avg_loss:0.036, val_acc:0.975]
Epoch [24/120    avg_loss:0.050, val_acc:0.961]
Epoch [25/120    avg_loss:0.068, val_acc:0.972]
Epoch [26/120    avg_loss:0.050, val_acc:0.985]
Epoch [27/120    avg_loss:0.041, val_acc:0.944]
Epoch [28/120    avg_loss:0.040, val_acc:0.977]
Epoch [29/120    avg_loss:0.024, val_acc:0.988]
Epoch [30/120    avg_loss:0.032, val_acc:0.984]
Epoch [31/120    avg_loss:0.039, val_acc:0.975]
Epoch [32/120    avg_loss:0.032, val_acc:0.989]
Epoch [33/120    avg_loss:0.021, val_acc:0.990]
Epoch [34/120    avg_loss:0.026, val_acc:0.990]
Epoch [35/120    avg_loss:0.015, val_acc:0.994]
Epoch [36/120    avg_loss:0.025, val_acc:0.990]
Epoch [37/120    avg_loss:0.015, val_acc:0.990]
Epoch [38/120    avg_loss:0.013, val_acc:0.990]
Epoch [39/120    avg_loss:0.020, val_acc:0.990]
Epoch [40/120    avg_loss:0.020, val_acc:0.996]
Epoch [41/120    avg_loss:0.013, val_acc:0.985]
Epoch [42/120    avg_loss:0.012, val_acc:0.991]
Epoch [43/120    avg_loss:0.009, val_acc:0.990]
Epoch [44/120    avg_loss:0.014, val_acc:0.993]
Epoch [45/120    avg_loss:0.014, val_acc:0.990]
Epoch [46/120    avg_loss:0.015, val_acc:0.990]
Epoch [47/120    avg_loss:0.014, val_acc:0.944]
Epoch [48/120    avg_loss:0.024, val_acc:0.990]
Epoch [49/120    avg_loss:0.008, val_acc:0.992]
Epoch [50/120    avg_loss:0.007, val_acc:0.993]
Epoch [51/120    avg_loss:0.022, val_acc:0.988]
Epoch [52/120    avg_loss:0.013, val_acc:0.988]
Epoch [53/120    avg_loss:0.008, val_acc:0.992]
Epoch [54/120    avg_loss:0.010, val_acc:0.994]
Epoch [55/120    avg_loss:0.009, val_acc:0.994]
Epoch [56/120    avg_loss:0.008, val_acc:0.994]
Epoch [57/120    avg_loss:0.008, val_acc:0.994]
Epoch [58/120    avg_loss:0.004, val_acc:0.995]
Epoch [59/120    avg_loss:0.009, val_acc:0.994]
Epoch [60/120    avg_loss:0.007, val_acc:0.995]
Epoch [61/120    avg_loss:0.009, val_acc:0.992]
Epoch [62/120    avg_loss:0.008, val_acc:0.993]
Epoch [63/120    avg_loss:0.006, val_acc:0.994]
Epoch [64/120    avg_loss:0.006, val_acc:0.993]
Epoch [65/120    avg_loss:0.006, val_acc:0.993]
Epoch [66/120    avg_loss:0.006, val_acc:0.995]
Epoch [67/120    avg_loss:0.010, val_acc:0.994]
Epoch [68/120    avg_loss:0.008, val_acc:0.994]
Epoch [69/120    avg_loss:0.004, val_acc:0.994]
Epoch [70/120    avg_loss:0.006, val_acc:0.994]
Epoch [71/120    avg_loss:0.006, val_acc:0.994]
Epoch [72/120    avg_loss:0.005, val_acc:0.994]
Epoch [73/120    avg_loss:0.006, val_acc:0.994]
Epoch [74/120    avg_loss:0.007, val_acc:0.994]
Epoch [75/120    avg_loss:0.007, val_acc:0.994]
Epoch [76/120    avg_loss:0.005, val_acc:0.994]
Epoch [77/120    avg_loss:0.006, val_acc:0.994]
Epoch [78/120    avg_loss:0.007, val_acc:0.994]
Epoch [79/120    avg_loss:0.004, val_acc:0.994]
Epoch [80/120    avg_loss:0.005, val_acc:0.994]
Epoch [81/120    avg_loss:0.005, val_acc:0.994]
Epoch [82/120    avg_loss:0.007, val_acc:0.994]
Epoch [83/120    avg_loss:0.008, val_acc:0.994]
Epoch [84/120    avg_loss:0.005, val_acc:0.994]
Epoch [85/120    avg_loss:0.004, val_acc:0.994]
Epoch [86/120    avg_loss:0.004, val_acc:0.994]
Epoch [87/120    avg_loss:0.007, val_acc:0.994]
Epoch [88/120    avg_loss:0.006, val_acc:0.994]
Epoch [89/120    avg_loss:0.007, val_acc:0.994]
Epoch [90/120    avg_loss:0.005, val_acc:0.994]
Epoch [91/120    avg_loss:0.006, val_acc:0.994]
Epoch [92/120    avg_loss:0.004, val_acc:0.994]
Epoch [93/120    avg_loss:0.007, val_acc:0.994]
Epoch [94/120    avg_loss:0.006, val_acc:0.994]
Epoch [95/120    avg_loss:0.005, val_acc:0.994]
Epoch [96/120    avg_loss:0.007, val_acc:0.994]
Epoch [97/120    avg_loss:0.005, val_acc:0.994]
Epoch [98/120    avg_loss:0.004, val_acc:0.994]
Epoch [99/120    avg_loss:0.005, val_acc:0.994]
Epoch [100/120    avg_loss:0.006, val_acc:0.994]
Epoch [101/120    avg_loss:0.003, val_acc:0.994]
Epoch [102/120    avg_loss:0.005, val_acc:0.994]
Epoch [103/120    avg_loss:0.008, val_acc:0.994]
Epoch [104/120    avg_loss:0.006, val_acc:0.994]
Epoch [105/120    avg_loss:0.006, val_acc:0.994]
Epoch [106/120    avg_loss:0.005, val_acc:0.994]
Epoch [107/120    avg_loss:0.009, val_acc:0.994]
Epoch [108/120    avg_loss:0.006, val_acc:0.994]
Epoch [109/120    avg_loss:0.008, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.006, val_acc:0.994]
Epoch [112/120    avg_loss:0.007, val_acc:0.994]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.006, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.994]
Epoch [116/120    avg_loss:0.005, val_acc:0.994]
Epoch [117/120    avg_loss:0.006, val_acc:0.994]
Epoch [118/120    avg_loss:0.006, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     0     0    23     7     0     0]
 [    0     1 18025     0    49     0    11     0     0     4]
 [    0     0     0  2006     2     0     0     0    26     2]
 [    0    44    16     0  2887     0     0     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     0     0  4842     0     0    26]
 [    0     0     0     0     0     0     6  1284     0     0]
 [    0     0     0     0    38     0     0     0  3521    12]
 [    0     0     0     7    14    27     0     0     0   871]]

Accuracy:
99.15648422625503

F1 scores:
[       nan 0.99417657 0.99748208 0.99086194 0.96846696 0.98976109
 0.99221311 0.99496319 0.98586028 0.94983642]

Kappa:
0.9888276050613398
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8706f9d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.570, val_acc:0.651]
Epoch [2/120    avg_loss:0.868, val_acc:0.654]
Epoch [3/120    avg_loss:0.660, val_acc:0.708]
Epoch [4/120    avg_loss:0.437, val_acc:0.789]
Epoch [5/120    avg_loss:0.311, val_acc:0.775]
Epoch [6/120    avg_loss:0.356, val_acc:0.809]
Epoch [7/120    avg_loss:0.256, val_acc:0.897]
Epoch [8/120    avg_loss:0.199, val_acc:0.927]
Epoch [9/120    avg_loss:0.180, val_acc:0.892]
Epoch [10/120    avg_loss:0.104, val_acc:0.956]
Epoch [11/120    avg_loss:0.110, val_acc:0.895]
Epoch [12/120    avg_loss:0.124, val_acc:0.913]
Epoch [13/120    avg_loss:0.123, val_acc:0.961]
Epoch [14/120    avg_loss:0.075, val_acc:0.950]
Epoch [15/120    avg_loss:0.056, val_acc:0.951]
Epoch [16/120    avg_loss:0.053, val_acc:0.971]
Epoch [17/120    avg_loss:0.051, val_acc:0.970]
Epoch [18/120    avg_loss:0.085, val_acc:0.964]
Epoch [19/120    avg_loss:0.080, val_acc:0.967]
Epoch [20/120    avg_loss:0.086, val_acc:0.944]
Epoch [21/120    avg_loss:0.068, val_acc:0.985]
Epoch [22/120    avg_loss:0.047, val_acc:0.966]
Epoch [23/120    avg_loss:0.056, val_acc:0.970]
Epoch [24/120    avg_loss:0.027, val_acc:0.982]
Epoch [25/120    avg_loss:0.035, val_acc:0.984]
Epoch [26/120    avg_loss:0.034, val_acc:0.966]
Epoch [27/120    avg_loss:0.041, val_acc:0.967]
Epoch [28/120    avg_loss:0.030, val_acc:0.981]
Epoch [29/120    avg_loss:0.023, val_acc:0.988]
Epoch [30/120    avg_loss:0.024, val_acc:0.984]
Epoch [31/120    avg_loss:0.020, val_acc:0.984]
Epoch [32/120    avg_loss:0.026, val_acc:0.984]
Epoch [33/120    avg_loss:0.028, val_acc:0.990]
Epoch [34/120    avg_loss:0.028, val_acc:0.977]
Epoch [35/120    avg_loss:0.029, val_acc:0.981]
Epoch [36/120    avg_loss:0.041, val_acc:0.980]
Epoch [37/120    avg_loss:0.027, val_acc:0.986]
Epoch [38/120    avg_loss:0.020, val_acc:0.983]
Epoch [39/120    avg_loss:0.026, val_acc:0.952]
Epoch [40/120    avg_loss:0.033, val_acc:0.990]
Epoch [41/120    avg_loss:0.016, val_acc:0.982]
Epoch [42/120    avg_loss:0.010, val_acc:0.990]
Epoch [43/120    avg_loss:0.006, val_acc:0.994]
Epoch [44/120    avg_loss:0.010, val_acc:0.990]
Epoch [45/120    avg_loss:0.008, val_acc:0.987]
Epoch [46/120    avg_loss:0.007, val_acc:0.990]
Epoch [47/120    avg_loss:0.004, val_acc:0.990]
Epoch [48/120    avg_loss:0.006, val_acc:0.990]
Epoch [49/120    avg_loss:0.007, val_acc:0.993]
Epoch [50/120    avg_loss:0.008, val_acc:0.991]
Epoch [51/120    avg_loss:0.012, val_acc:0.992]
Epoch [52/120    avg_loss:0.028, val_acc:0.992]
Epoch [53/120    avg_loss:0.014, val_acc:0.990]
Epoch [54/120    avg_loss:0.012, val_acc:0.992]
Epoch [55/120    avg_loss:0.008, val_acc:0.991]
Epoch [56/120    avg_loss:0.008, val_acc:0.981]
Epoch [57/120    avg_loss:0.012, val_acc:0.988]
Epoch [58/120    avg_loss:0.004, val_acc:0.989]
Epoch [59/120    avg_loss:0.006, val_acc:0.989]
Epoch [60/120    avg_loss:0.008, val_acc:0.990]
Epoch [61/120    avg_loss:0.004, val_acc:0.990]
Epoch [62/120    avg_loss:0.003, val_acc:0.992]
Epoch [63/120    avg_loss:0.003, val_acc:0.990]
Epoch [64/120    avg_loss:0.004, val_acc:0.990]
Epoch [65/120    avg_loss:0.005, val_acc:0.991]
Epoch [66/120    avg_loss:0.005, val_acc:0.992]
Epoch [67/120    avg_loss:0.007, val_acc:0.993]
Epoch [68/120    avg_loss:0.005, val_acc:0.993]
Epoch [69/120    avg_loss:0.003, val_acc:0.993]
Epoch [70/120    avg_loss:0.006, val_acc:0.993]
Epoch [71/120    avg_loss:0.005, val_acc:0.993]
Epoch [72/120    avg_loss:0.004, val_acc:0.993]
Epoch [73/120    avg_loss:0.004, val_acc:0.993]
Epoch [74/120    avg_loss:0.003, val_acc:0.993]
Epoch [75/120    avg_loss:0.007, val_acc:0.993]
Epoch [76/120    avg_loss:0.008, val_acc:0.993]
Epoch [77/120    avg_loss:0.005, val_acc:0.993]
Epoch [78/120    avg_loss:0.007, val_acc:0.993]
Epoch [79/120    avg_loss:0.006, val_acc:0.993]
Epoch [80/120    avg_loss:0.005, val_acc:0.993]
Epoch [81/120    avg_loss:0.005, val_acc:0.993]
Epoch [82/120    avg_loss:0.003, val_acc:0.993]
Epoch [83/120    avg_loss:0.005, val_acc:0.993]
Epoch [84/120    avg_loss:0.005, val_acc:0.993]
Epoch [85/120    avg_loss:0.004, val_acc:0.993]
Epoch [86/120    avg_loss:0.003, val_acc:0.993]
Epoch [87/120    avg_loss:0.007, val_acc:0.993]
Epoch [88/120    avg_loss:0.004, val_acc:0.993]
Epoch [89/120    avg_loss:0.005, val_acc:0.993]
Epoch [90/120    avg_loss:0.003, val_acc:0.993]
Epoch [91/120    avg_loss:0.004, val_acc:0.993]
Epoch [92/120    avg_loss:0.005, val_acc:0.993]
Epoch [93/120    avg_loss:0.004, val_acc:0.993]
Epoch [94/120    avg_loss:0.006, val_acc:0.993]
Epoch [95/120    avg_loss:0.006, val_acc:0.993]
Epoch [96/120    avg_loss:0.006, val_acc:0.993]
Epoch [97/120    avg_loss:0.004, val_acc:0.993]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.006, val_acc:0.993]
Epoch [100/120    avg_loss:0.006, val_acc:0.993]
Epoch [101/120    avg_loss:0.005, val_acc:0.993]
Epoch [102/120    avg_loss:0.008, val_acc:0.993]
Epoch [103/120    avg_loss:0.006, val_acc:0.993]
Epoch [104/120    avg_loss:0.006, val_acc:0.993]
Epoch [105/120    avg_loss:0.005, val_acc:0.993]
Epoch [106/120    avg_loss:0.010, val_acc:0.993]
Epoch [107/120    avg_loss:0.005, val_acc:0.993]
Epoch [108/120    avg_loss:0.006, val_acc:0.993]
Epoch [109/120    avg_loss:0.005, val_acc:0.993]
Epoch [110/120    avg_loss:0.005, val_acc:0.993]
Epoch [111/120    avg_loss:0.005, val_acc:0.993]
Epoch [112/120    avg_loss:0.004, val_acc:0.993]
Epoch [113/120    avg_loss:0.005, val_acc:0.993]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.003, val_acc:0.993]
Epoch [116/120    avg_loss:0.004, val_acc:0.993]
Epoch [117/120    avg_loss:0.006, val_acc:0.993]
Epoch [118/120    avg_loss:0.004, val_acc:0.993]
Epoch [119/120    avg_loss:0.005, val_acc:0.993]
Epoch [120/120    avg_loss:0.004, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0     5     0     2    15     0     0]
 [    0     1 18029     0    33     0    27     0     0     0]
 [    0     1     0  2028     2     0     0     0     3     2]
 [    0    30     8     0  2907     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4872     0     1     2]
 [    0     0     0     0     0     0     2  1284     0     4]
 [    0     0     0    10    43     0     0     0  3517     1]
 [    0     0     0     1    14    40     0     0     0   864]]

Accuracy:
99.33241751620756

F1 scores:
[       nan 0.9958055  0.9980072  0.99533742 0.97289157 0.98490566
 0.99621716 0.99188876 0.98806012 0.96428571]

Kappa:
0.9911587397154854
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f42dd109710>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.519, val_acc:0.697]
Epoch [2/120    avg_loss:0.817, val_acc:0.768]
Epoch [3/120    avg_loss:0.557, val_acc:0.837]
Epoch [4/120    avg_loss:0.402, val_acc:0.835]
Epoch [5/120    avg_loss:0.371, val_acc:0.827]
Epoch [6/120    avg_loss:0.247, val_acc:0.924]
Epoch [7/120    avg_loss:0.159, val_acc:0.910]
Epoch [8/120    avg_loss:0.165, val_acc:0.949]
Epoch [9/120    avg_loss:0.176, val_acc:0.899]
Epoch [10/120    avg_loss:0.158, val_acc:0.910]
Epoch [11/120    avg_loss:0.166, val_acc:0.941]
Epoch [12/120    avg_loss:0.099, val_acc:0.948]
Epoch [13/120    avg_loss:0.092, val_acc:0.961]
Epoch [14/120    avg_loss:0.096, val_acc:0.943]
Epoch [15/120    avg_loss:0.088, val_acc:0.954]
Epoch [16/120    avg_loss:0.071, val_acc:0.962]
Epoch [17/120    avg_loss:0.069, val_acc:0.933]
Epoch [18/120    avg_loss:0.055, val_acc:0.976]
Epoch [19/120    avg_loss:0.065, val_acc:0.950]
Epoch [20/120    avg_loss:0.074, val_acc:0.961]
Epoch [21/120    avg_loss:0.058, val_acc:0.965]
Epoch [22/120    avg_loss:0.041, val_acc:0.963]
Epoch [23/120    avg_loss:0.080, val_acc:0.956]
Epoch [24/120    avg_loss:0.046, val_acc:0.972]
Epoch [25/120    avg_loss:0.036, val_acc:0.977]
Epoch [26/120    avg_loss:0.046, val_acc:0.974]
Epoch [27/120    avg_loss:0.027, val_acc:0.967]
Epoch [28/120    avg_loss:0.024, val_acc:0.968]
Epoch [29/120    avg_loss:0.018, val_acc:0.974]
Epoch [30/120    avg_loss:0.022, val_acc:0.974]
Epoch [31/120    avg_loss:0.020, val_acc:0.978]
Epoch [32/120    avg_loss:0.022, val_acc:0.968]
Epoch [33/120    avg_loss:0.020, val_acc:0.983]
Epoch [34/120    avg_loss:0.018, val_acc:0.975]
Epoch [35/120    avg_loss:0.016, val_acc:0.977]
Epoch [36/120    avg_loss:0.013, val_acc:0.954]
Epoch [37/120    avg_loss:0.051, val_acc:0.976]
Epoch [38/120    avg_loss:0.043, val_acc:0.953]
Epoch [39/120    avg_loss:0.046, val_acc:0.974]
Epoch [40/120    avg_loss:0.016, val_acc:0.981]
Epoch [41/120    avg_loss:0.015, val_acc:0.977]
Epoch [42/120    avg_loss:0.015, val_acc:0.980]
Epoch [43/120    avg_loss:0.019, val_acc:0.981]
Epoch [44/120    avg_loss:0.014, val_acc:0.983]
Epoch [45/120    avg_loss:0.011, val_acc:0.982]
Epoch [46/120    avg_loss:0.013, val_acc:0.986]
Epoch [47/120    avg_loss:0.010, val_acc:0.976]
Epoch [48/120    avg_loss:0.021, val_acc:0.974]
Epoch [49/120    avg_loss:0.010, val_acc:0.983]
Epoch [50/120    avg_loss:0.066, val_acc:0.956]
Epoch [51/120    avg_loss:0.067, val_acc:0.969]
Epoch [52/120    avg_loss:0.160, val_acc:0.965]
Epoch [53/120    avg_loss:0.069, val_acc:0.955]
Epoch [54/120    avg_loss:0.050, val_acc:0.968]
Epoch [55/120    avg_loss:0.035, val_acc:0.960]
Epoch [56/120    avg_loss:0.043, val_acc:0.975]
Epoch [57/120    avg_loss:0.017, val_acc:0.973]
Epoch [58/120    avg_loss:0.015, val_acc:0.976]
Epoch [59/120    avg_loss:0.017, val_acc:0.967]
Epoch [60/120    avg_loss:0.019, val_acc:0.976]
Epoch [61/120    avg_loss:0.013, val_acc:0.981]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.010, val_acc:0.981]
Epoch [64/120    avg_loss:0.012, val_acc:0.981]
Epoch [65/120    avg_loss:0.014, val_acc:0.980]
Epoch [66/120    avg_loss:0.014, val_acc:0.982]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.984]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.008, val_acc:0.983]
Epoch [72/120    avg_loss:0.012, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.984]
Epoch [74/120    avg_loss:0.007, val_acc:0.984]
Epoch [75/120    avg_loss:0.006, val_acc:0.984]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.984]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.009, val_acc:0.984]
Epoch [83/120    avg_loss:0.012, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.015, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.018, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     0     0    14    10     0     6]
 [    0     0 18066     0    10     0    13     0     1     0]
 [    0     0     0  2028     3     0     0     0     0     5]
 [    0    44    20     0  2881     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4858     0     0    12]
 [    0     0     0     0     0     0     7  1281     0     2]
 [    0     0     0    18    42     0     0     0  3501    10]
 [    0     0     0     4    15    37     0     0     0   863]]

Accuracy:
99.25770611910443

F1 scores:
[       nan 0.99425377 0.9985629  0.99265786 0.97281783 0.98602191
 0.99447288 0.99263851 0.98619718 0.94991745]

Kappa:
0.99016357412384
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e38145780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.495, val_acc:0.668]
Epoch [2/120    avg_loss:0.865, val_acc:0.626]
Epoch [3/120    avg_loss:0.564, val_acc:0.755]
Epoch [4/120    avg_loss:0.396, val_acc:0.779]
Epoch [5/120    avg_loss:0.344, val_acc:0.788]
Epoch [6/120    avg_loss:0.221, val_acc:0.826]
Epoch [7/120    avg_loss:0.253, val_acc:0.911]
Epoch [8/120    avg_loss:0.164, val_acc:0.915]
Epoch [9/120    avg_loss:0.238, val_acc:0.903]
Epoch [10/120    avg_loss:0.214, val_acc:0.949]
Epoch [11/120    avg_loss:0.173, val_acc:0.915]
Epoch [12/120    avg_loss:0.179, val_acc:0.952]
Epoch [13/120    avg_loss:0.126, val_acc:0.962]
Epoch [14/120    avg_loss:0.122, val_acc:0.942]
Epoch [15/120    avg_loss:0.076, val_acc:0.968]
Epoch [16/120    avg_loss:0.044, val_acc:0.964]
Epoch [17/120    avg_loss:0.067, val_acc:0.967]
Epoch [18/120    avg_loss:0.069, val_acc:0.970]
Epoch [19/120    avg_loss:0.044, val_acc:0.977]
Epoch [20/120    avg_loss:0.043, val_acc:0.975]
Epoch [21/120    avg_loss:0.041, val_acc:0.970]
Epoch [22/120    avg_loss:0.037, val_acc:0.978]
Epoch [23/120    avg_loss:0.038, val_acc:0.979]
Epoch [24/120    avg_loss:0.037, val_acc:0.979]
Epoch [25/120    avg_loss:0.038, val_acc:0.978]
Epoch [26/120    avg_loss:0.055, val_acc:0.956]
Epoch [27/120    avg_loss:0.084, val_acc:0.928]
Epoch [28/120    avg_loss:0.036, val_acc:0.977]
Epoch [29/120    avg_loss:0.021, val_acc:0.984]
Epoch [30/120    avg_loss:0.021, val_acc:0.984]
Epoch [31/120    avg_loss:0.025, val_acc:0.981]
Epoch [32/120    avg_loss:0.024, val_acc:0.984]
Epoch [33/120    avg_loss:0.009, val_acc:0.990]
Epoch [34/120    avg_loss:0.020, val_acc:0.989]
Epoch [35/120    avg_loss:0.012, val_acc:0.990]
Epoch [36/120    avg_loss:0.023, val_acc:0.991]
Epoch [37/120    avg_loss:0.012, val_acc:0.990]
Epoch [38/120    avg_loss:0.016, val_acc:0.990]
Epoch [39/120    avg_loss:0.008, val_acc:0.991]
Epoch [40/120    avg_loss:0.009, val_acc:0.992]
Epoch [41/120    avg_loss:0.011, val_acc:0.991]
Epoch [42/120    avg_loss:0.016, val_acc:0.985]
Epoch [43/120    avg_loss:0.014, val_acc:0.970]
Epoch [44/120    avg_loss:0.011, val_acc:0.991]
Epoch [45/120    avg_loss:0.021, val_acc:0.989]
Epoch [46/120    avg_loss:0.007, val_acc:0.991]
Epoch [47/120    avg_loss:0.028, val_acc:0.981]
Epoch [48/120    avg_loss:0.019, val_acc:0.986]
Epoch [49/120    avg_loss:0.007, val_acc:0.990]
Epoch [50/120    avg_loss:0.007, val_acc:0.989]
Epoch [51/120    avg_loss:0.012, val_acc:0.973]
Epoch [52/120    avg_loss:0.011, val_acc:0.990]
Epoch [53/120    avg_loss:0.009, val_acc:0.990]
Epoch [54/120    avg_loss:0.015, val_acc:0.991]
Epoch [55/120    avg_loss:0.007, val_acc:0.992]
Epoch [56/120    avg_loss:0.007, val_acc:0.994]
Epoch [57/120    avg_loss:0.004, val_acc:0.994]
Epoch [58/120    avg_loss:0.004, val_acc:0.993]
Epoch [59/120    avg_loss:0.007, val_acc:0.992]
Epoch [60/120    avg_loss:0.005, val_acc:0.992]
Epoch [61/120    avg_loss:0.005, val_acc:0.992]
Epoch [62/120    avg_loss:0.005, val_acc:0.992]
Epoch [63/120    avg_loss:0.005, val_acc:0.993]
Epoch [64/120    avg_loss:0.005, val_acc:0.993]
Epoch [65/120    avg_loss:0.005, val_acc:0.993]
Epoch [66/120    avg_loss:0.007, val_acc:0.992]
Epoch [67/120    avg_loss:0.004, val_acc:0.993]
Epoch [68/120    avg_loss:0.005, val_acc:0.993]
Epoch [69/120    avg_loss:0.006, val_acc:0.993]
Epoch [70/120    avg_loss:0.005, val_acc:0.993]
Epoch [71/120    avg_loss:0.005, val_acc:0.994]
Epoch [72/120    avg_loss:0.004, val_acc:0.994]
Epoch [73/120    avg_loss:0.004, val_acc:0.994]
Epoch [74/120    avg_loss:0.005, val_acc:0.994]
Epoch [75/120    avg_loss:0.003, val_acc:0.994]
Epoch [76/120    avg_loss:0.004, val_acc:0.994]
Epoch [77/120    avg_loss:0.004, val_acc:0.994]
Epoch [78/120    avg_loss:0.004, val_acc:0.994]
Epoch [79/120    avg_loss:0.004, val_acc:0.994]
Epoch [80/120    avg_loss:0.004, val_acc:0.994]
Epoch [81/120    avg_loss:0.004, val_acc:0.994]
Epoch [82/120    avg_loss:0.004, val_acc:0.994]
Epoch [83/120    avg_loss:0.004, val_acc:0.994]
Epoch [84/120    avg_loss:0.004, val_acc:0.994]
Epoch [85/120    avg_loss:0.007, val_acc:0.994]
Epoch [86/120    avg_loss:0.005, val_acc:0.994]
Epoch [87/120    avg_loss:0.004, val_acc:0.994]
Epoch [88/120    avg_loss:0.006, val_acc:0.994]
Epoch [89/120    avg_loss:0.006, val_acc:0.994]
Epoch [90/120    avg_loss:0.005, val_acc:0.994]
Epoch [91/120    avg_loss:0.003, val_acc:0.994]
Epoch [92/120    avg_loss:0.008, val_acc:0.994]
Epoch [93/120    avg_loss:0.004, val_acc:0.994]
Epoch [94/120    avg_loss:0.008, val_acc:0.993]
Epoch [95/120    avg_loss:0.005, val_acc:0.993]
Epoch [96/120    avg_loss:0.004, val_acc:0.993]
Epoch [97/120    avg_loss:0.005, val_acc:0.993]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.005, val_acc:0.993]
Epoch [100/120    avg_loss:0.003, val_acc:0.993]
Epoch [101/120    avg_loss:0.004, val_acc:0.993]
Epoch [102/120    avg_loss:0.004, val_acc:0.993]
Epoch [103/120    avg_loss:0.005, val_acc:0.993]
Epoch [104/120    avg_loss:0.004, val_acc:0.994]
Epoch [105/120    avg_loss:0.004, val_acc:0.994]
Epoch [106/120    avg_loss:0.006, val_acc:0.994]
Epoch [107/120    avg_loss:0.005, val_acc:0.994]
Epoch [108/120    avg_loss:0.004, val_acc:0.994]
Epoch [109/120    avg_loss:0.004, val_acc:0.994]
Epoch [110/120    avg_loss:0.006, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.003, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.993]
Epoch [114/120    avg_loss:0.006, val_acc:0.993]
Epoch [115/120    avg_loss:0.005, val_acc:0.993]
Epoch [116/120    avg_loss:0.004, val_acc:0.993]
Epoch [117/120    avg_loss:0.005, val_acc:0.993]
Epoch [118/120    avg_loss:0.004, val_acc:0.993]
Epoch [119/120    avg_loss:0.004, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     0     4     0    19    18     0     0]
 [    0     7 18036     0    32     0    13     0     2     0]
 [    0     0     0  2020     2     0     0     0     8     6]
 [    0    27    12     0  2902     0     4     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4867     0     4     4]
 [    0     0     0     0     0     0     2  1282     0     6]
 [    0     3     0     9    35     0     0     0  3523     1]
 [    0     0     0     2    14    29     0     0     0   874]]

Accuracy:
99.29385679512207

F1 scores:
[       nan 0.99393468 0.99809081 0.9933612  0.97366214 0.98901099
 0.99499131 0.98996139 0.98752628 0.96574586]

Kappa:
0.9906472010616091
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd2a1b5e6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.510, val_acc:0.377]
Epoch [2/120    avg_loss:0.843, val_acc:0.600]
Epoch [3/120    avg_loss:0.638, val_acc:0.687]
Epoch [4/120    avg_loss:0.497, val_acc:0.756]
Epoch [5/120    avg_loss:0.440, val_acc:0.763]
Epoch [6/120    avg_loss:0.303, val_acc:0.825]
Epoch [7/120    avg_loss:0.203, val_acc:0.918]
Epoch [8/120    avg_loss:0.225, val_acc:0.933]
Epoch [9/120    avg_loss:0.177, val_acc:0.961]
Epoch [10/120    avg_loss:0.171, val_acc:0.931]
Epoch [11/120    avg_loss:0.123, val_acc:0.948]
Epoch [12/120    avg_loss:0.109, val_acc:0.950]
Epoch [13/120    avg_loss:0.115, val_acc:0.955]
Epoch [14/120    avg_loss:0.086, val_acc:0.974]
Epoch [15/120    avg_loss:0.083, val_acc:0.964]
Epoch [16/120    avg_loss:0.066, val_acc:0.970]
Epoch [17/120    avg_loss:0.060, val_acc:0.959]
Epoch [18/120    avg_loss:0.059, val_acc:0.972]
Epoch [19/120    avg_loss:0.046, val_acc:0.970]
Epoch [20/120    avg_loss:0.052, val_acc:0.969]
Epoch [21/120    avg_loss:0.040, val_acc:0.979]
Epoch [22/120    avg_loss:0.038, val_acc:0.974]
Epoch [23/120    avg_loss:0.039, val_acc:0.977]
Epoch [24/120    avg_loss:0.039, val_acc:0.970]
Epoch [25/120    avg_loss:0.046, val_acc:0.977]
Epoch [26/120    avg_loss:0.043, val_acc:0.953]
Epoch [27/120    avg_loss:0.031, val_acc:0.982]
Epoch [28/120    avg_loss:0.022, val_acc:0.978]
Epoch [29/120    avg_loss:0.030, val_acc:0.975]
Epoch [30/120    avg_loss:0.022, val_acc:0.984]
Epoch [31/120    avg_loss:0.043, val_acc:0.966]
Epoch [32/120    avg_loss:0.056, val_acc:0.968]
Epoch [33/120    avg_loss:0.075, val_acc:0.945]
Epoch [34/120    avg_loss:0.058, val_acc:0.982]
Epoch [35/120    avg_loss:0.057, val_acc:0.964]
Epoch [36/120    avg_loss:0.060, val_acc:0.984]
Epoch [37/120    avg_loss:0.052, val_acc:0.964]
Epoch [38/120    avg_loss:0.036, val_acc:0.987]
Epoch [39/120    avg_loss:0.016, val_acc:0.985]
Epoch [40/120    avg_loss:0.015, val_acc:0.986]
Epoch [41/120    avg_loss:0.016, val_acc:0.989]
Epoch [42/120    avg_loss:0.012, val_acc:0.987]
Epoch [43/120    avg_loss:0.009, val_acc:0.985]
Epoch [44/120    avg_loss:0.013, val_acc:0.983]
Epoch [45/120    avg_loss:0.022, val_acc:0.989]
Epoch [46/120    avg_loss:0.017, val_acc:0.984]
Epoch [47/120    avg_loss:0.012, val_acc:0.984]
Epoch [48/120    avg_loss:0.017, val_acc:0.984]
Epoch [49/120    avg_loss:0.011, val_acc:0.984]
Epoch [50/120    avg_loss:0.012, val_acc:0.985]
Epoch [51/120    avg_loss:0.013, val_acc:0.990]
Epoch [52/120    avg_loss:0.008, val_acc:0.984]
Epoch [53/120    avg_loss:0.014, val_acc:0.989]
Epoch [54/120    avg_loss:0.009, val_acc:0.989]
Epoch [55/120    avg_loss:0.013, val_acc:0.985]
Epoch [56/120    avg_loss:0.011, val_acc:0.987]
Epoch [57/120    avg_loss:0.008, val_acc:0.983]
Epoch [58/120    avg_loss:0.012, val_acc:0.984]
Epoch [59/120    avg_loss:0.010, val_acc:0.982]
Epoch [60/120    avg_loss:0.008, val_acc:0.986]
Epoch [61/120    avg_loss:0.010, val_acc:0.989]
Epoch [62/120    avg_loss:0.008, val_acc:0.990]
Epoch [63/120    avg_loss:0.014, val_acc:0.990]
Epoch [64/120    avg_loss:0.006, val_acc:0.988]
Epoch [65/120    avg_loss:0.004, val_acc:0.988]
Epoch [66/120    avg_loss:0.006, val_acc:0.990]
Epoch [67/120    avg_loss:0.005, val_acc:0.982]
Epoch [68/120    avg_loss:0.006, val_acc:0.989]
Epoch [69/120    avg_loss:0.004, val_acc:0.990]
Epoch [70/120    avg_loss:0.008, val_acc:0.990]
Epoch [71/120    avg_loss:0.004, val_acc:0.990]
Epoch [72/120    avg_loss:0.012, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.990]
Epoch [74/120    avg_loss:0.004, val_acc:0.989]
Epoch [75/120    avg_loss:0.004, val_acc:0.990]
Epoch [76/120    avg_loss:0.004, val_acc:0.986]
Epoch [77/120    avg_loss:0.004, val_acc:0.989]
Epoch [78/120    avg_loss:0.003, val_acc:0.990]
Epoch [79/120    avg_loss:0.004, val_acc:0.990]
Epoch [80/120    avg_loss:0.003, val_acc:0.990]
Epoch [81/120    avg_loss:0.003, val_acc:0.990]
Epoch [82/120    avg_loss:0.003, val_acc:0.990]
Epoch [83/120    avg_loss:0.004, val_acc:0.991]
Epoch [84/120    avg_loss:0.005, val_acc:0.978]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.003, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.002, val_acc:0.989]
Epoch [90/120    avg_loss:0.003, val_acc:0.989]
Epoch [91/120    avg_loss:0.003, val_acc:0.988]
Epoch [92/120    avg_loss:0.003, val_acc:0.989]
Epoch [93/120    avg_loss:0.003, val_acc:0.990]
Epoch [94/120    avg_loss:0.003, val_acc:0.989]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.002, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.003, val_acc:0.990]
Epoch [101/120    avg_loss:0.003, val_acc:0.990]
Epoch [102/120    avg_loss:0.003, val_acc:0.990]
Epoch [103/120    avg_loss:0.002, val_acc:0.990]
Epoch [104/120    avg_loss:0.003, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.002, val_acc:0.990]
Epoch [108/120    avg_loss:0.002, val_acc:0.990]
Epoch [109/120    avg_loss:0.003, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.001, val_acc:0.990]
Epoch [113/120    avg_loss:0.003, val_acc:0.990]
Epoch [114/120    avg_loss:0.002, val_acc:0.990]
Epoch [115/120    avg_loss:0.002, val_acc:0.990]
Epoch [116/120    avg_loss:0.002, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.990]
Epoch [119/120    avg_loss:0.003, val_acc:0.990]
Epoch [120/120    avg_loss:0.002, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     5     0    14    16     0     0]
 [    0     0 18069     0    18     0     3     0     0     0]
 [    0     0     0  2030     0     0     0     0     6     0]
 [    0    29    11     0  2904     0     0     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4862     0     0    13]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     3     0     1    28     0     0     0  3537     2]
 [    0     0     0     1    14    32     0     0     0   872]]

Accuracy:
99.44809967946401

F1 scores:
[       nan 0.99479045 0.99903243 0.99803343 0.9776132  0.98788796
 0.99641357 0.99306091 0.99047886 0.96566999]

Kappa:
0.9926876511313708
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8bd921710>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.458, val_acc:0.625]
Epoch [2/120    avg_loss:0.822, val_acc:0.646]
Epoch [3/120    avg_loss:0.565, val_acc:0.739]
Epoch [4/120    avg_loss:0.489, val_acc:0.741]
Epoch [5/120    avg_loss:0.345, val_acc:0.878]
Epoch [6/120    avg_loss:0.279, val_acc:0.921]
Epoch [7/120    avg_loss:0.294, val_acc:0.877]
Epoch [8/120    avg_loss:0.194, val_acc:0.935]
Epoch [9/120    avg_loss:0.128, val_acc:0.948]
Epoch [10/120    avg_loss:0.143, val_acc:0.916]
Epoch [11/120    avg_loss:0.145, val_acc:0.960]
Epoch [12/120    avg_loss:0.109, val_acc:0.954]
Epoch [13/120    avg_loss:0.071, val_acc:0.959]
Epoch [14/120    avg_loss:0.080, val_acc:0.965]
Epoch [15/120    avg_loss:0.089, val_acc:0.962]
Epoch [16/120    avg_loss:0.078, val_acc:0.969]
Epoch [17/120    avg_loss:0.081, val_acc:0.970]
Epoch [18/120    avg_loss:0.056, val_acc:0.957]
Epoch [19/120    avg_loss:0.070, val_acc:0.949]
Epoch [20/120    avg_loss:0.049, val_acc:0.966]
Epoch [21/120    avg_loss:0.038, val_acc:0.971]
Epoch [22/120    avg_loss:0.046, val_acc:0.970]
Epoch [23/120    avg_loss:0.100, val_acc:0.943]
Epoch [24/120    avg_loss:0.062, val_acc:0.964]
Epoch [25/120    avg_loss:0.082, val_acc:0.968]
Epoch [26/120    avg_loss:0.068, val_acc:0.965]
Epoch [27/120    avg_loss:0.075, val_acc:0.950]
Epoch [28/120    avg_loss:0.069, val_acc:0.974]
Epoch [29/120    avg_loss:0.038, val_acc:0.970]
Epoch [30/120    avg_loss:0.039, val_acc:0.971]
Epoch [31/120    avg_loss:0.061, val_acc:0.979]
Epoch [32/120    avg_loss:0.038, val_acc:0.979]
Epoch [33/120    avg_loss:0.041, val_acc:0.969]
Epoch [34/120    avg_loss:0.027, val_acc:0.980]
Epoch [35/120    avg_loss:0.021, val_acc:0.988]
Epoch [36/120    avg_loss:0.020, val_acc:0.984]
Epoch [37/120    avg_loss:0.013, val_acc:0.990]
Epoch [38/120    avg_loss:0.028, val_acc:0.971]
Epoch [39/120    avg_loss:0.059, val_acc:0.978]
Epoch [40/120    avg_loss:0.024, val_acc:0.982]
Epoch [41/120    avg_loss:0.029, val_acc:0.986]
Epoch [42/120    avg_loss:0.016, val_acc:0.985]
Epoch [43/120    avg_loss:0.019, val_acc:0.976]
Epoch [44/120    avg_loss:0.018, val_acc:0.987]
Epoch [45/120    avg_loss:0.025, val_acc:0.987]
Epoch [46/120    avg_loss:0.017, val_acc:0.979]
Epoch [47/120    avg_loss:0.014, val_acc:0.985]
Epoch [48/120    avg_loss:0.012, val_acc:0.981]
Epoch [49/120    avg_loss:0.011, val_acc:0.986]
Epoch [50/120    avg_loss:0.011, val_acc:0.984]
Epoch [51/120    avg_loss:0.007, val_acc:0.984]
Epoch [52/120    avg_loss:0.007, val_acc:0.987]
Epoch [53/120    avg_loss:0.010, val_acc:0.988]
Epoch [54/120    avg_loss:0.009, val_acc:0.987]
Epoch [55/120    avg_loss:0.008, val_acc:0.988]
Epoch [56/120    avg_loss:0.007, val_acc:0.989]
Epoch [57/120    avg_loss:0.007, val_acc:0.989]
Epoch [58/120    avg_loss:0.004, val_acc:0.989]
Epoch [59/120    avg_loss:0.008, val_acc:0.989]
Epoch [60/120    avg_loss:0.008, val_acc:0.988]
Epoch [61/120    avg_loss:0.009, val_acc:0.987]
Epoch [62/120    avg_loss:0.006, val_acc:0.988]
Epoch [63/120    avg_loss:0.005, val_acc:0.988]
Epoch [64/120    avg_loss:0.013, val_acc:0.988]
Epoch [65/120    avg_loss:0.007, val_acc:0.989]
Epoch [66/120    avg_loss:0.005, val_acc:0.988]
Epoch [67/120    avg_loss:0.006, val_acc:0.988]
Epoch [68/120    avg_loss:0.009, val_acc:0.988]
Epoch [69/120    avg_loss:0.005, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.006, val_acc:0.988]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.005, val_acc:0.988]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.005, val_acc:0.988]
Epoch [79/120    avg_loss:0.010, val_acc:0.988]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.010, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.003, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6424     0     0     5     0     0     3     0     0]
 [    0     4 18049     0    21     0    16     0     0     0]
 [    0     0     0  2016     1     0     0     0    19     0]
 [    0    28    10     0  2902     0     5     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     0     5]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     1     0     6    44     0     0     0  3514     6]
 [    0     0     0     5    19    43     0     0     0   852]]

Accuracy:
99.34928783168246

F1 scores:
[       nan 0.99681899 0.99858917 0.99237017 0.97317237 0.98379193
 0.99713526 0.99806277 0.98555602 0.95622896]

Kappa:
0.9913796903238441
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d8f0bc7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.472, val_acc:0.429]
Epoch [2/120    avg_loss:0.917, val_acc:0.528]
Epoch [3/120    avg_loss:0.610, val_acc:0.704]
Epoch [4/120    avg_loss:0.465, val_acc:0.813]
Epoch [5/120    avg_loss:0.403, val_acc:0.836]
Epoch [6/120    avg_loss:0.303, val_acc:0.848]
Epoch [7/120    avg_loss:0.290, val_acc:0.877]
Epoch [8/120    avg_loss:0.166, val_acc:0.934]
Epoch [9/120    avg_loss:0.127, val_acc:0.949]
Epoch [10/120    avg_loss:0.176, val_acc:0.960]
Epoch [11/120    avg_loss:0.106, val_acc:0.940]
Epoch [12/120    avg_loss:0.116, val_acc:0.958]
Epoch [13/120    avg_loss:0.093, val_acc:0.955]
Epoch [14/120    avg_loss:0.061, val_acc:0.927]
Epoch [15/120    avg_loss:0.093, val_acc:0.940]
Epoch [16/120    avg_loss:0.083, val_acc:0.970]
Epoch [17/120    avg_loss:0.054, val_acc:0.970]
Epoch [18/120    avg_loss:0.069, val_acc:0.971]
Epoch [19/120    avg_loss:0.062, val_acc:0.966]
Epoch [20/120    avg_loss:0.076, val_acc:0.964]
Epoch [21/120    avg_loss:0.050, val_acc:0.967]
Epoch [22/120    avg_loss:0.051, val_acc:0.977]
Epoch [23/120    avg_loss:0.055, val_acc:0.951]
Epoch [24/120    avg_loss:0.043, val_acc:0.971]
Epoch [25/120    avg_loss:0.044, val_acc:0.966]
Epoch [26/120    avg_loss:0.035, val_acc:0.964]
Epoch [27/120    avg_loss:0.044, val_acc:0.954]
Epoch [28/120    avg_loss:0.036, val_acc:0.974]
Epoch [29/120    avg_loss:0.048, val_acc:0.967]
Epoch [30/120    avg_loss:0.038, val_acc:0.974]
Epoch [31/120    avg_loss:0.027, val_acc:0.953]
Epoch [32/120    avg_loss:0.028, val_acc:0.984]
Epoch [33/120    avg_loss:0.026, val_acc:0.987]
Epoch [34/120    avg_loss:0.021, val_acc:0.980]
Epoch [35/120    avg_loss:0.011, val_acc:0.984]
Epoch [36/120    avg_loss:0.018, val_acc:0.984]
Epoch [37/120    avg_loss:0.018, val_acc:0.983]
Epoch [38/120    avg_loss:0.016, val_acc:0.980]
Epoch [39/120    avg_loss:0.013, val_acc:0.989]
Epoch [40/120    avg_loss:0.036, val_acc:0.976]
Epoch [41/120    avg_loss:0.037, val_acc:0.972]
Epoch [42/120    avg_loss:0.030, val_acc:0.970]
Epoch [43/120    avg_loss:0.019, val_acc:0.984]
Epoch [44/120    avg_loss:0.017, val_acc:0.986]
Epoch [45/120    avg_loss:0.012, val_acc:0.984]
Epoch [46/120    avg_loss:0.011, val_acc:0.989]
Epoch [47/120    avg_loss:0.011, val_acc:0.984]
Epoch [48/120    avg_loss:0.013, val_acc:0.977]
Epoch [49/120    avg_loss:0.011, val_acc:0.990]
Epoch [50/120    avg_loss:0.013, val_acc:0.988]
Epoch [51/120    avg_loss:0.011, val_acc:0.987]
Epoch [52/120    avg_loss:0.011, val_acc:0.985]
Epoch [53/120    avg_loss:0.012, val_acc:0.983]
Epoch [54/120    avg_loss:0.011, val_acc:0.989]
Epoch [55/120    avg_loss:0.008, val_acc:0.991]
Epoch [56/120    avg_loss:0.006, val_acc:0.989]
Epoch [57/120    avg_loss:0.008, val_acc:0.990]
Epoch [58/120    avg_loss:0.006, val_acc:0.992]
Epoch [59/120    avg_loss:0.004, val_acc:0.992]
Epoch [60/120    avg_loss:0.011, val_acc:0.984]
Epoch [61/120    avg_loss:0.082, val_acc:0.976]
Epoch [62/120    avg_loss:0.041, val_acc:0.972]
Epoch [63/120    avg_loss:0.020, val_acc:0.975]
Epoch [64/120    avg_loss:0.021, val_acc:0.982]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.015, val_acc:0.990]
Epoch [67/120    avg_loss:0.007, val_acc:0.989]
Epoch [68/120    avg_loss:0.011, val_acc:0.990]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.026, val_acc:0.983]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.012, val_acc:0.983]
Epoch [73/120    avg_loss:0.018, val_acc:0.984]
Epoch [74/120    avg_loss:0.008, val_acc:0.985]
Epoch [75/120    avg_loss:0.016, val_acc:0.986]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.990]
Epoch [82/120    avg_loss:0.008, val_acc:0.990]
Epoch [83/120    avg_loss:0.007, val_acc:0.990]
Epoch [84/120    avg_loss:0.007, val_acc:0.992]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.006, val_acc:0.990]
Epoch [88/120    avg_loss:0.004, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.011, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.009, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.008, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.004, val_acc:0.992]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.993]
Epoch [112/120    avg_loss:0.006, val_acc:0.993]
Epoch [113/120    avg_loss:0.005, val_acc:0.993]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.006, val_acc:0.993]
Epoch [116/120    avg_loss:0.002, val_acc:0.993]
Epoch [117/120    avg_loss:0.005, val_acc:0.993]
Epoch [118/120    avg_loss:0.003, val_acc:0.993]
Epoch [119/120    avg_loss:0.004, val_acc:0.993]
Epoch [120/120    avg_loss:0.003, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6404     0     0     4     0     7    17     0     0]
 [    0     0 17993     0    41     0    48     0     4     4]
 [    0     3     0  2024     0     0     0     0     3     6]
 [    0    30     5     0  2902     0     7     0    27     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4862     0    11     5]
 [    0     0     0     0     0     0     4  1284     0     2]
 [    0     0     0     5    29     0     0     0  3537     0]
 [    0     0     0     1    14    28     0     0     0   876]]

Accuracy:
99.26252620924012

F1 scores:
[       nan 0.99525993 0.99717358 0.99557304 0.97349883 0.9893859
 0.99163777 0.99112312 0.98895568 0.96635411]

Kappa:
0.9902376433305023
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdeb61407b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.527, val_acc:0.339]
Epoch [2/120    avg_loss:0.895, val_acc:0.573]
Epoch [3/120    avg_loss:0.623, val_acc:0.753]
Epoch [4/120    avg_loss:0.445, val_acc:0.832]
Epoch [5/120    avg_loss:0.324, val_acc:0.874]
Epoch [6/120    avg_loss:0.286, val_acc:0.905]
Epoch [7/120    avg_loss:0.203, val_acc:0.928]
Epoch [8/120    avg_loss:0.183, val_acc:0.921]
Epoch [9/120    avg_loss:0.163, val_acc:0.942]
Epoch [10/120    avg_loss:0.116, val_acc:0.939]
Epoch [11/120    avg_loss:0.103, val_acc:0.932]
Epoch [12/120    avg_loss:0.114, val_acc:0.936]
Epoch [13/120    avg_loss:0.064, val_acc:0.951]
Epoch [14/120    avg_loss:0.071, val_acc:0.940]
Epoch [15/120    avg_loss:0.061, val_acc:0.957]
Epoch [16/120    avg_loss:0.072, val_acc:0.968]
Epoch [17/120    avg_loss:0.058, val_acc:0.959]
Epoch [18/120    avg_loss:0.073, val_acc:0.964]
Epoch [19/120    avg_loss:0.067, val_acc:0.960]
Epoch [20/120    avg_loss:0.045, val_acc:0.976]
Epoch [21/120    avg_loss:0.045, val_acc:0.971]
Epoch [22/120    avg_loss:0.035, val_acc:0.966]
Epoch [23/120    avg_loss:0.042, val_acc:0.970]
Epoch [24/120    avg_loss:0.043, val_acc:0.965]
Epoch [25/120    avg_loss:0.026, val_acc:0.972]
Epoch [26/120    avg_loss:0.037, val_acc:0.972]
Epoch [27/120    avg_loss:0.050, val_acc:0.920]
Epoch [28/120    avg_loss:0.044, val_acc:0.965]
Epoch [29/120    avg_loss:0.038, val_acc:0.964]
Epoch [30/120    avg_loss:0.024, val_acc:0.974]
Epoch [31/120    avg_loss:0.019, val_acc:0.966]
Epoch [32/120    avg_loss:0.022, val_acc:0.971]
Epoch [33/120    avg_loss:0.016, val_acc:0.976]
Epoch [34/120    avg_loss:0.019, val_acc:0.975]
Epoch [35/120    avg_loss:0.021, val_acc:0.978]
Epoch [36/120    avg_loss:0.029, val_acc:0.975]
Epoch [37/120    avg_loss:0.044, val_acc:0.963]
Epoch [38/120    avg_loss:0.072, val_acc:0.958]
Epoch [39/120    avg_loss:0.022, val_acc:0.976]
Epoch [40/120    avg_loss:0.024, val_acc:0.973]
Epoch [41/120    avg_loss:0.024, val_acc:0.976]
Epoch [42/120    avg_loss:0.026, val_acc:0.908]
Epoch [43/120    avg_loss:0.033, val_acc:0.966]
Epoch [44/120    avg_loss:0.059, val_acc:0.933]
Epoch [45/120    avg_loss:0.032, val_acc:0.981]
Epoch [46/120    avg_loss:0.021, val_acc:0.981]
Epoch [47/120    avg_loss:0.012, val_acc:0.981]
Epoch [48/120    avg_loss:0.011, val_acc:0.984]
Epoch [49/120    avg_loss:0.017, val_acc:0.970]
Epoch [50/120    avg_loss:0.007, val_acc:0.982]
Epoch [51/120    avg_loss:0.008, val_acc:0.977]
Epoch [52/120    avg_loss:0.015, val_acc:0.971]
Epoch [53/120    avg_loss:0.013, val_acc:0.977]
Epoch [54/120    avg_loss:0.011, val_acc:0.984]
Epoch [55/120    avg_loss:0.020, val_acc:0.968]
Epoch [56/120    avg_loss:0.023, val_acc:0.983]
Epoch [57/120    avg_loss:0.011, val_acc:0.979]
Epoch [58/120    avg_loss:0.008, val_acc:0.983]
Epoch [59/120    avg_loss:0.007, val_acc:0.982]
Epoch [60/120    avg_loss:0.011, val_acc:0.984]
Epoch [61/120    avg_loss:0.005, val_acc:0.983]
Epoch [62/120    avg_loss:0.015, val_acc:0.974]
Epoch [63/120    avg_loss:0.112, val_acc:0.952]
Epoch [64/120    avg_loss:0.113, val_acc:0.952]
Epoch [65/120    avg_loss:0.040, val_acc:0.956]
Epoch [66/120    avg_loss:0.029, val_acc:0.974]
Epoch [67/120    avg_loss:0.017, val_acc:0.974]
Epoch [68/120    avg_loss:0.011, val_acc:0.977]
Epoch [69/120    avg_loss:0.009, val_acc:0.980]
Epoch [70/120    avg_loss:0.023, val_acc:0.984]
Epoch [71/120    avg_loss:0.033, val_acc:0.975]
Epoch [72/120    avg_loss:0.027, val_acc:0.979]
Epoch [73/120    avg_loss:0.033, val_acc:0.975]
Epoch [74/120    avg_loss:0.034, val_acc:0.965]
Epoch [75/120    avg_loss:0.014, val_acc:0.982]
Epoch [76/120    avg_loss:0.010, val_acc:0.983]
Epoch [77/120    avg_loss:0.011, val_acc:0.977]
Epoch [78/120    avg_loss:0.035, val_acc:0.972]
Epoch [79/120    avg_loss:0.021, val_acc:0.977]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.977]
Epoch [82/120    avg_loss:0.009, val_acc:0.982]
Epoch [83/120    avg_loss:0.009, val_acc:0.980]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.975]
Epoch [89/120    avg_loss:0.003, val_acc:0.982]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.007, val_acc:0.985]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.003, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.003, val_acc:0.984]
Epoch [102/120    avg_loss:0.011, val_acc:0.974]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.003, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.002, val_acc:0.986]
Epoch [108/120    avg_loss:0.002, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.002, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.002, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6430     0     0     0     0     0     2     0     0]
 [    0     0 18076     0    11     0     0     0     0     3]
 [    0     0     0  2031     3     0     0     0     0     2]
 [    0    43    13     0  2889     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     0     0     0  4843     0     7     8]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0    12    19     0     0     0  3537     1]
 [    0     0     0     5    14    38     0     0     0   862]]

Accuracy:
99.44568963439616

F1 scores:
[       nan 0.99635857 0.99870162 0.99461312 0.97799594 0.98564955
 0.99639955 0.99922541 0.99047886 0.96044568]

Kappa:
0.9926529060674318
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f648bc74828>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.443, val_acc:0.703]
Epoch [2/120    avg_loss:0.903, val_acc:0.698]
Epoch [3/120    avg_loss:0.555, val_acc:0.689]
Epoch [4/120    avg_loss:0.463, val_acc:0.780]
Epoch [5/120    avg_loss:0.391, val_acc:0.788]
Epoch [6/120    avg_loss:0.269, val_acc:0.793]
Epoch [7/120    avg_loss:0.265, val_acc:0.924]
Epoch [8/120    avg_loss:0.163, val_acc:0.940]
Epoch [9/120    avg_loss:0.119, val_acc:0.920]
Epoch [10/120    avg_loss:0.215, val_acc:0.884]
Epoch [11/120    avg_loss:0.211, val_acc:0.929]
Epoch [12/120    avg_loss:0.123, val_acc:0.939]
Epoch [13/120    avg_loss:0.125, val_acc:0.943]
Epoch [14/120    avg_loss:0.108, val_acc:0.910]
Epoch [15/120    avg_loss:0.106, val_acc:0.954]
Epoch [16/120    avg_loss:0.068, val_acc:0.970]
Epoch [17/120    avg_loss:0.064, val_acc:0.943]
Epoch [18/120    avg_loss:0.047, val_acc:0.959]
Epoch [19/120    avg_loss:0.051, val_acc:0.974]
Epoch [20/120    avg_loss:0.064, val_acc:0.959]
Epoch [21/120    avg_loss:0.151, val_acc:0.944]
Epoch [22/120    avg_loss:0.104, val_acc:0.936]
Epoch [23/120    avg_loss:0.080, val_acc:0.934]
Epoch [24/120    avg_loss:0.071, val_acc:0.965]
Epoch [25/120    avg_loss:0.048, val_acc:0.973]
Epoch [26/120    avg_loss:0.074, val_acc:0.948]
Epoch [27/120    avg_loss:0.057, val_acc:0.964]
Epoch [28/120    avg_loss:0.087, val_acc:0.934]
Epoch [29/120    avg_loss:0.055, val_acc:0.963]
Epoch [30/120    avg_loss:0.068, val_acc:0.962]
Epoch [31/120    avg_loss:0.047, val_acc:0.970]
Epoch [32/120    avg_loss:0.029, val_acc:0.977]
Epoch [33/120    avg_loss:0.027, val_acc:0.975]
Epoch [34/120    avg_loss:0.044, val_acc:0.959]
Epoch [35/120    avg_loss:0.045, val_acc:0.960]
Epoch [36/120    avg_loss:0.032, val_acc:0.969]
Epoch [37/120    avg_loss:0.018, val_acc:0.982]
Epoch [38/120    avg_loss:0.018, val_acc:0.966]
Epoch [39/120    avg_loss:0.025, val_acc:0.959]
Epoch [40/120    avg_loss:0.019, val_acc:0.981]
Epoch [41/120    avg_loss:0.033, val_acc:0.982]
Epoch [42/120    avg_loss:0.013, val_acc:0.984]
Epoch [43/120    avg_loss:0.013, val_acc:0.979]
Epoch [44/120    avg_loss:0.013, val_acc:0.977]
Epoch [45/120    avg_loss:0.014, val_acc:0.984]
Epoch [46/120    avg_loss:0.048, val_acc:0.970]
Epoch [47/120    avg_loss:0.028, val_acc:0.986]
Epoch [48/120    avg_loss:0.025, val_acc:0.969]
Epoch [49/120    avg_loss:0.025, val_acc:0.975]
Epoch [50/120    avg_loss:0.028, val_acc:0.984]
Epoch [51/120    avg_loss:0.019, val_acc:0.986]
Epoch [52/120    avg_loss:0.008, val_acc:0.982]
Epoch [53/120    avg_loss:0.013, val_acc:0.977]
Epoch [54/120    avg_loss:0.007, val_acc:0.987]
Epoch [55/120    avg_loss:0.008, val_acc:0.978]
Epoch [56/120    avg_loss:0.019, val_acc:0.931]
Epoch [57/120    avg_loss:0.032, val_acc:0.951]
Epoch [58/120    avg_loss:0.015, val_acc:0.976]
Epoch [59/120    avg_loss:0.024, val_acc:0.973]
Epoch [60/120    avg_loss:0.012, val_acc:0.985]
Epoch [61/120    avg_loss:0.014, val_acc:0.984]
Epoch [62/120    avg_loss:0.013, val_acc:0.983]
Epoch [63/120    avg_loss:0.015, val_acc:0.986]
Epoch [64/120    avg_loss:0.012, val_acc:0.986]
Epoch [65/120    avg_loss:0.011, val_acc:0.976]
Epoch [66/120    avg_loss:0.012, val_acc:0.984]
Epoch [67/120    avg_loss:0.005, val_acc:0.989]
Epoch [68/120    avg_loss:0.010, val_acc:0.973]
Epoch [69/120    avg_loss:0.012, val_acc:0.963]
Epoch [70/120    avg_loss:0.014, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.983]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.005, val_acc:0.990]
Epoch [74/120    avg_loss:0.006, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.989]
Epoch [76/120    avg_loss:0.004, val_acc:0.990]
Epoch [77/120    avg_loss:0.007, val_acc:0.979]
Epoch [78/120    avg_loss:0.006, val_acc:0.991]
Epoch [79/120    avg_loss:0.003, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.983]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.990]
Epoch [84/120    avg_loss:0.002, val_acc:0.989]
Epoch [85/120    avg_loss:0.003, val_acc:0.987]
Epoch [86/120    avg_loss:0.003, val_acc:0.987]
Epoch [87/120    avg_loss:0.003, val_acc:0.987]
Epoch [88/120    avg_loss:0.003, val_acc:0.990]
Epoch [89/120    avg_loss:0.003, val_acc:0.990]
Epoch [90/120    avg_loss:0.002, val_acc:0.990]
Epoch [91/120    avg_loss:0.002, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.003, val_acc:0.990]
Epoch [94/120    avg_loss:0.002, val_acc:0.990]
Epoch [95/120    avg_loss:0.003, val_acc:0.990]
Epoch [96/120    avg_loss:0.002, val_acc:0.990]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.004, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.002, val_acc:0.990]
Epoch [101/120    avg_loss:0.002, val_acc:0.990]
Epoch [102/120    avg_loss:0.002, val_acc:0.990]
Epoch [103/120    avg_loss:0.003, val_acc:0.990]
Epoch [104/120    avg_loss:0.003, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.003, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.990]
Epoch [109/120    avg_loss:0.002, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.002, val_acc:0.990]
Epoch [113/120    avg_loss:0.003, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.002, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.002, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.003, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     5     0     9     0     4     0]
 [    0     0 18080     0     8     0     2     0     0     0]
 [    0     0     0  2027     3     0     0     0     1     5]
 [    0    30    14     0  2901     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4871     0     0     3]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     1     0     0    29     0     0     0  3539     2]
 [    0     0     0     5    14    22     0     0     0   878]]

Accuracy:
99.53968139204203

F1 scores:
[       nan 0.99619477 0.99922626 0.99655851 0.97808496 0.99164134
 0.99795124 0.99883586 0.99103892 0.97123894]

Kappa:
0.9938995094034793
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd25970c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.452, val_acc:0.395]
Epoch [2/120    avg_loss:0.878, val_acc:0.579]
Epoch [3/120    avg_loss:0.599, val_acc:0.710]
Epoch [4/120    avg_loss:0.477, val_acc:0.769]
Epoch [5/120    avg_loss:0.384, val_acc:0.753]
Epoch [6/120    avg_loss:0.311, val_acc:0.882]
Epoch [7/120    avg_loss:0.256, val_acc:0.902]
Epoch [8/120    avg_loss:0.207, val_acc:0.926]
Epoch [9/120    avg_loss:0.162, val_acc:0.921]
Epoch [10/120    avg_loss:0.132, val_acc:0.927]
Epoch [11/120    avg_loss:0.112, val_acc:0.954]
Epoch [12/120    avg_loss:0.118, val_acc:0.946]
Epoch [13/120    avg_loss:0.113, val_acc:0.931]
Epoch [14/120    avg_loss:0.101, val_acc:0.945]
Epoch [15/120    avg_loss:0.072, val_acc:0.927]
Epoch [16/120    avg_loss:0.090, val_acc:0.943]
Epoch [17/120    avg_loss:0.111, val_acc:0.943]
Epoch [18/120    avg_loss:0.065, val_acc:0.962]
Epoch [19/120    avg_loss:0.054, val_acc:0.965]
Epoch [20/120    avg_loss:0.054, val_acc:0.967]
Epoch [21/120    avg_loss:0.045, val_acc:0.947]
Epoch [22/120    avg_loss:0.056, val_acc:0.970]
Epoch [23/120    avg_loss:0.040, val_acc:0.965]
Epoch [24/120    avg_loss:0.047, val_acc:0.966]
Epoch [25/120    avg_loss:0.048, val_acc:0.975]
Epoch [26/120    avg_loss:0.031, val_acc:0.969]
Epoch [27/120    avg_loss:0.022, val_acc:0.970]
Epoch [28/120    avg_loss:0.015, val_acc:0.972]
Epoch [29/120    avg_loss:0.045, val_acc:0.962]
Epoch [30/120    avg_loss:0.081, val_acc:0.969]
Epoch [31/120    avg_loss:0.046, val_acc:0.970]
Epoch [32/120    avg_loss:0.049, val_acc:0.964]
Epoch [33/120    avg_loss:0.032, val_acc:0.951]
Epoch [34/120    avg_loss:0.025, val_acc:0.976]
Epoch [35/120    avg_loss:0.021, val_acc:0.977]
Epoch [36/120    avg_loss:0.034, val_acc:0.983]
Epoch [37/120    avg_loss:0.018, val_acc:0.975]
Epoch [38/120    avg_loss:0.027, val_acc:0.983]
Epoch [39/120    avg_loss:0.031, val_acc:0.982]
Epoch [40/120    avg_loss:0.028, val_acc:0.977]
Epoch [41/120    avg_loss:0.029, val_acc:0.976]
Epoch [42/120    avg_loss:0.014, val_acc:0.981]
Epoch [43/120    avg_loss:0.016, val_acc:0.981]
Epoch [44/120    avg_loss:0.028, val_acc:0.980]
Epoch [45/120    avg_loss:0.016, val_acc:0.979]
Epoch [46/120    avg_loss:0.013, val_acc:0.983]
Epoch [47/120    avg_loss:0.013, val_acc:0.980]
Epoch [48/120    avg_loss:0.018, val_acc:0.981]
Epoch [49/120    avg_loss:0.011, val_acc:0.985]
Epoch [50/120    avg_loss:0.009, val_acc:0.985]
Epoch [51/120    avg_loss:0.008, val_acc:0.984]
Epoch [52/120    avg_loss:0.009, val_acc:0.981]
Epoch [53/120    avg_loss:0.026, val_acc:0.869]
Epoch [54/120    avg_loss:0.031, val_acc:0.981]
Epoch [55/120    avg_loss:0.014, val_acc:0.976]
Epoch [56/120    avg_loss:0.011, val_acc:0.984]
Epoch [57/120    avg_loss:0.037, val_acc:0.964]
Epoch [58/120    avg_loss:0.147, val_acc:0.932]
Epoch [59/120    avg_loss:0.123, val_acc:0.940]
Epoch [60/120    avg_loss:0.106, val_acc:0.948]
Epoch [61/120    avg_loss:0.060, val_acc:0.958]
Epoch [62/120    avg_loss:0.031, val_acc:0.970]
Epoch [63/120    avg_loss:0.038, val_acc:0.970]
Epoch [64/120    avg_loss:0.025, val_acc:0.972]
Epoch [65/120    avg_loss:0.017, val_acc:0.977]
Epoch [66/120    avg_loss:0.016, val_acc:0.976]
Epoch [67/120    avg_loss:0.024, val_acc:0.977]
Epoch [68/120    avg_loss:0.016, val_acc:0.979]
Epoch [69/120    avg_loss:0.015, val_acc:0.978]
Epoch [70/120    avg_loss:0.015, val_acc:0.978]
Epoch [71/120    avg_loss:0.011, val_acc:0.979]
Epoch [72/120    avg_loss:0.014, val_acc:0.980]
Epoch [73/120    avg_loss:0.010, val_acc:0.980]
Epoch [74/120    avg_loss:0.011, val_acc:0.981]
Epoch [75/120    avg_loss:0.010, val_acc:0.981]
Epoch [76/120    avg_loss:0.015, val_acc:0.979]
Epoch [77/120    avg_loss:0.009, val_acc:0.979]
Epoch [78/120    avg_loss:0.015, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.980]
Epoch [80/120    avg_loss:0.015, val_acc:0.981]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.016, val_acc:0.982]
Epoch [84/120    avg_loss:0.014, val_acc:0.982]
Epoch [85/120    avg_loss:0.011, val_acc:0.982]
Epoch [86/120    avg_loss:0.014, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.982]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.982]
Epoch [92/120    avg_loss:0.009, val_acc:0.982]
Epoch [93/120    avg_loss:0.011, val_acc:0.982]
Epoch [94/120    avg_loss:0.013, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.982]
Epoch [97/120    avg_loss:0.012, val_acc:0.982]
Epoch [98/120    avg_loss:0.010, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.030, val_acc:0.982]
Epoch [106/120    avg_loss:0.007, val_acc:0.982]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.012, val_acc:0.982]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.013, val_acc:0.982]
Epoch [115/120    avg_loss:0.020, val_acc:0.982]
Epoch [116/120    avg_loss:0.016, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.018, val_acc:0.982]
Epoch [119/120    avg_loss:0.015, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     2     0     0    21     0     0]
 [    0     3 18023     0    23     0    41     0     0     0]
 [    0     1     0  2029     3     0     0     0     2     1]
 [    0    43    15     0  2883     0     5     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4858     0     0    11]
 [    0     0     0     0     0     0     6  1281     0     3]
 [    0     4     0    10    51     0     0     0  3499     7]
 [    0     0     0     2    16    40     0     0     0   861]]

Accuracy:
99.16853445159424

F1 scores:
[       nan 0.99426001 0.99748181 0.99533971 0.96907563 0.98490566
 0.99264405 0.98842593 0.98591152 0.95560488]

Kappa:
0.9889871239802942
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f54ffd907b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.542, val_acc:0.451]
Epoch [2/120    avg_loss:0.978, val_acc:0.456]
Epoch [3/120    avg_loss:0.687, val_acc:0.673]
Epoch [4/120    avg_loss:0.554, val_acc:0.801]
Epoch [5/120    avg_loss:0.404, val_acc:0.889]
Epoch [6/120    avg_loss:0.327, val_acc:0.884]
Epoch [7/120    avg_loss:0.272, val_acc:0.882]
Epoch [8/120    avg_loss:0.245, val_acc:0.931]
Epoch [9/120    avg_loss:0.131, val_acc:0.931]
Epoch [10/120    avg_loss:0.166, val_acc:0.932]
Epoch [11/120    avg_loss:0.124, val_acc:0.964]
Epoch [12/120    avg_loss:0.135, val_acc:0.942]
Epoch [13/120    avg_loss:0.133, val_acc:0.952]
Epoch [14/120    avg_loss:0.137, val_acc:0.947]
Epoch [15/120    avg_loss:0.093, val_acc:0.951]
Epoch [16/120    avg_loss:0.066, val_acc:0.949]
Epoch [17/120    avg_loss:0.064, val_acc:0.963]
Epoch [18/120    avg_loss:0.050, val_acc:0.963]
Epoch [19/120    avg_loss:0.050, val_acc:0.960]
Epoch [20/120    avg_loss:0.062, val_acc:0.956]
Epoch [21/120    avg_loss:0.078, val_acc:0.956]
Epoch [22/120    avg_loss:0.077, val_acc:0.955]
Epoch [23/120    avg_loss:0.069, val_acc:0.925]
Epoch [24/120    avg_loss:0.163, val_acc:0.959]
Epoch [25/120    avg_loss:0.076, val_acc:0.967]
Epoch [26/120    avg_loss:0.049, val_acc:0.969]
Epoch [27/120    avg_loss:0.039, val_acc:0.970]
Epoch [28/120    avg_loss:0.039, val_acc:0.968]
Epoch [29/120    avg_loss:0.031, val_acc:0.968]
Epoch [30/120    avg_loss:0.030, val_acc:0.971]
Epoch [31/120    avg_loss:0.040, val_acc:0.970]
Epoch [32/120    avg_loss:0.027, val_acc:0.975]
Epoch [33/120    avg_loss:0.034, val_acc:0.973]
Epoch [34/120    avg_loss:0.035, val_acc:0.975]
Epoch [35/120    avg_loss:0.037, val_acc:0.974]
Epoch [36/120    avg_loss:0.028, val_acc:0.976]
Epoch [37/120    avg_loss:0.028, val_acc:0.976]
Epoch [38/120    avg_loss:0.038, val_acc:0.976]
Epoch [39/120    avg_loss:0.032, val_acc:0.978]
Epoch [40/120    avg_loss:0.023, val_acc:0.978]
Epoch [41/120    avg_loss:0.037, val_acc:0.977]
Epoch [42/120    avg_loss:0.020, val_acc:0.978]
Epoch [43/120    avg_loss:0.019, val_acc:0.979]
Epoch [44/120    avg_loss:0.018, val_acc:0.978]
Epoch [45/120    avg_loss:0.032, val_acc:0.977]
Epoch [46/120    avg_loss:0.021, val_acc:0.977]
Epoch [47/120    avg_loss:0.019, val_acc:0.973]
Epoch [48/120    avg_loss:0.027, val_acc:0.977]
Epoch [49/120    avg_loss:0.018, val_acc:0.978]
Epoch [50/120    avg_loss:0.019, val_acc:0.977]
Epoch [51/120    avg_loss:0.018, val_acc:0.977]
Epoch [52/120    avg_loss:0.024, val_acc:0.976]
Epoch [53/120    avg_loss:0.019, val_acc:0.977]
Epoch [54/120    avg_loss:0.019, val_acc:0.979]
Epoch [55/120    avg_loss:0.021, val_acc:0.978]
Epoch [56/120    avg_loss:0.021, val_acc:0.979]
Epoch [57/120    avg_loss:0.020, val_acc:0.978]
Epoch [58/120    avg_loss:0.020, val_acc:0.978]
Epoch [59/120    avg_loss:0.016, val_acc:0.980]
Epoch [60/120    avg_loss:0.016, val_acc:0.982]
Epoch [61/120    avg_loss:0.018, val_acc:0.979]
Epoch [62/120    avg_loss:0.022, val_acc:0.977]
Epoch [63/120    avg_loss:0.021, val_acc:0.976]
Epoch [64/120    avg_loss:0.014, val_acc:0.977]
Epoch [65/120    avg_loss:0.017, val_acc:0.977]
Epoch [66/120    avg_loss:0.015, val_acc:0.977]
Epoch [67/120    avg_loss:0.016, val_acc:0.978]
Epoch [68/120    avg_loss:0.024, val_acc:0.977]
Epoch [69/120    avg_loss:0.020, val_acc:0.977]
Epoch [70/120    avg_loss:0.020, val_acc:0.979]
Epoch [71/120    avg_loss:0.017, val_acc:0.980]
Epoch [72/120    avg_loss:0.024, val_acc:0.981]
Epoch [73/120    avg_loss:0.016, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.981]
Epoch [75/120    avg_loss:0.029, val_acc:0.975]
Epoch [76/120    avg_loss:0.019, val_acc:0.977]
Epoch [77/120    avg_loss:0.016, val_acc:0.978]
Epoch [78/120    avg_loss:0.019, val_acc:0.977]
Epoch [79/120    avg_loss:0.015, val_acc:0.981]
Epoch [80/120    avg_loss:0.022, val_acc:0.974]
Epoch [81/120    avg_loss:0.016, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.020, val_acc:0.980]
Epoch [84/120    avg_loss:0.016, val_acc:0.980]
Epoch [85/120    avg_loss:0.015, val_acc:0.981]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.016, val_acc:0.981]
Epoch [88/120    avg_loss:0.012, val_acc:0.980]
Epoch [89/120    avg_loss:0.011, val_acc:0.980]
Epoch [90/120    avg_loss:0.015, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.979]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.014, val_acc:0.980]
Epoch [94/120    avg_loss:0.016, val_acc:0.981]
Epoch [95/120    avg_loss:0.011, val_acc:0.981]
Epoch [96/120    avg_loss:0.015, val_acc:0.980]
Epoch [97/120    avg_loss:0.016, val_acc:0.981]
Epoch [98/120    avg_loss:0.019, val_acc:0.981]
Epoch [99/120    avg_loss:0.014, val_acc:0.981]
Epoch [100/120    avg_loss:0.012, val_acc:0.981]
Epoch [101/120    avg_loss:0.015, val_acc:0.981]
Epoch [102/120    avg_loss:0.019, val_acc:0.981]
Epoch [103/120    avg_loss:0.015, val_acc:0.981]
Epoch [104/120    avg_loss:0.016, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.981]
Epoch [106/120    avg_loss:0.015, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.014, val_acc:0.981]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.981]
Epoch [111/120    avg_loss:0.014, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.981]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.012, val_acc:0.981]
Epoch [115/120    avg_loss:0.015, val_acc:0.981]
Epoch [116/120    avg_loss:0.014, val_acc:0.981]
Epoch [117/120    avg_loss:0.016, val_acc:0.981]
Epoch [118/120    avg_loss:0.016, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.015, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6383     0     5     8     0     5     1    21     9]
 [    0    21 17996     0    61     0     9     0     3     0]
 [    0     0     0  2031     2     0     0     0     0     3]
 [    0    43    20     1  2872     0     5     0    28     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4838     0     0    40]
 [    0     0     0     0     0     0     6  1276     0     8]
 [    0     2     0    10    49     0     0     0  3496    14]
 [    0     0     0    12    17    36     0     0     0   854]]

Accuracy:
98.9347600800135

F1 scores:
[       nan 0.99107212 0.99684263 0.99194139 0.96037452 0.98639456
 0.99332717 0.9941566  0.98216042 0.92324324]

Kappa:
0.9858978063397071
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff21cc0a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.541, val_acc:0.370]
Epoch [2/120    avg_loss:0.938, val_acc:0.718]
Epoch [3/120    avg_loss:0.677, val_acc:0.713]
Epoch [4/120    avg_loss:0.448, val_acc:0.817]
Epoch [5/120    avg_loss:0.380, val_acc:0.865]
Epoch [6/120    avg_loss:0.330, val_acc:0.873]
Epoch [7/120    avg_loss:0.275, val_acc:0.908]
Epoch [8/120    avg_loss:0.241, val_acc:0.915]
Epoch [9/120    avg_loss:0.139, val_acc:0.937]
Epoch [10/120    avg_loss:0.135, val_acc:0.923]
Epoch [11/120    avg_loss:0.162, val_acc:0.936]
Epoch [12/120    avg_loss:0.164, val_acc:0.911]
Epoch [13/120    avg_loss:0.134, val_acc:0.944]
Epoch [14/120    avg_loss:0.100, val_acc:0.960]
Epoch [15/120    avg_loss:0.168, val_acc:0.950]
Epoch [16/120    avg_loss:0.125, val_acc:0.952]
Epoch [17/120    avg_loss:0.089, val_acc:0.963]
Epoch [18/120    avg_loss:0.055, val_acc:0.967]
Epoch [19/120    avg_loss:0.064, val_acc:0.957]
Epoch [20/120    avg_loss:0.061, val_acc:0.962]
Epoch [21/120    avg_loss:0.048, val_acc:0.955]
Epoch [22/120    avg_loss:0.107, val_acc:0.965]
Epoch [23/120    avg_loss:0.059, val_acc:0.965]
Epoch [24/120    avg_loss:0.051, val_acc:0.977]
Epoch [25/120    avg_loss:0.046, val_acc:0.972]
Epoch [26/120    avg_loss:0.061, val_acc:0.967]
Epoch [27/120    avg_loss:0.026, val_acc:0.978]
Epoch [28/120    avg_loss:0.020, val_acc:0.981]
Epoch [29/120    avg_loss:0.025, val_acc:0.977]
Epoch [30/120    avg_loss:0.024, val_acc:0.977]
Epoch [31/120    avg_loss:0.019, val_acc:0.971]
Epoch [32/120    avg_loss:0.029, val_acc:0.981]
Epoch [33/120    avg_loss:0.021, val_acc:0.970]
Epoch [34/120    avg_loss:0.031, val_acc:0.977]
Epoch [35/120    avg_loss:0.022, val_acc:0.978]
Epoch [36/120    avg_loss:0.013, val_acc:0.980]
Epoch [37/120    avg_loss:0.019, val_acc:0.985]
Epoch [38/120    avg_loss:0.015, val_acc:0.984]
Epoch [39/120    avg_loss:0.017, val_acc:0.978]
Epoch [40/120    avg_loss:0.020, val_acc:0.984]
Epoch [41/120    avg_loss:0.015, val_acc:0.983]
Epoch [42/120    avg_loss:0.015, val_acc:0.984]
Epoch [43/120    avg_loss:0.019, val_acc:0.976]
Epoch [44/120    avg_loss:0.033, val_acc:0.981]
Epoch [45/120    avg_loss:0.038, val_acc:0.973]
Epoch [46/120    avg_loss:0.028, val_acc:0.982]
Epoch [47/120    avg_loss:0.013, val_acc:0.978]
Epoch [48/120    avg_loss:0.010, val_acc:0.985]
Epoch [49/120    avg_loss:0.015, val_acc:0.977]
Epoch [50/120    avg_loss:0.011, val_acc:0.982]
Epoch [51/120    avg_loss:0.010, val_acc:0.986]
Epoch [52/120    avg_loss:0.011, val_acc:0.983]
Epoch [53/120    avg_loss:0.010, val_acc:0.984]
Epoch [54/120    avg_loss:0.008, val_acc:0.983]
Epoch [55/120    avg_loss:0.006, val_acc:0.985]
Epoch [56/120    avg_loss:0.005, val_acc:0.983]
Epoch [57/120    avg_loss:0.010, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.013, val_acc:0.970]
Epoch [62/120    avg_loss:0.022, val_acc:0.982]
Epoch [63/120    avg_loss:0.014, val_acc:0.981]
Epoch [64/120    avg_loss:0.009, val_acc:0.977]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.009, val_acc:0.984]
Epoch [69/120    avg_loss:0.008, val_acc:0.969]
Epoch [70/120    avg_loss:0.011, val_acc:0.986]
Epoch [71/120    avg_loss:0.005, val_acc:0.985]
Epoch [72/120    avg_loss:0.007, val_acc:0.983]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.011, val_acc:0.987]
Epoch [75/120    avg_loss:0.006, val_acc:0.987]
Epoch [76/120    avg_loss:0.006, val_acc:0.987]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.003, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.004, val_acc:0.987]
Epoch [82/120    avg_loss:0.003, val_acc:0.986]
Epoch [83/120    avg_loss:0.005, val_acc:0.986]
Epoch [84/120    avg_loss:0.003, val_acc:0.986]
Epoch [85/120    avg_loss:0.004, val_acc:0.986]
Epoch [86/120    avg_loss:0.003, val_acc:0.986]
Epoch [87/120    avg_loss:0.003, val_acc:0.986]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.003, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.003, val_acc:0.986]
Epoch [97/120    avg_loss:0.004, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.004, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.002, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.003, val_acc:0.986]
Epoch [106/120    avg_loss:0.003, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.003, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.003, val_acc:0.986]
Epoch [117/120    avg_loss:0.003, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     0     5     0     0     0     0     0]
 [    0     0 18074     0    15     0     1     0     0     0]
 [    0     0     0  2026     0     0     0     0     7     3]
 [    0    38    19     0  2884     0     3     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4855     0     0    19]
 [    0     0     0     0     0     0     3  1283     0     4]
 [    0     4     0     0    32     0     0     0  3535     0]
 [    0     0     0    15    14    36     0     0     0   854]]

Accuracy:
99.39748873303931

F1 scores:
[       nan 0.99635687 0.99892226 0.99386804 0.97399527 0.98639456
 0.99691992 0.99727944 0.99005741 0.94941634]

Kappa:
0.9920149488993601
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6ce6f6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.558, val_acc:0.659]
Epoch [2/120    avg_loss:0.933, val_acc:0.669]
Epoch [3/120    avg_loss:0.706, val_acc:0.674]
Epoch [4/120    avg_loss:0.515, val_acc:0.773]
Epoch [5/120    avg_loss:0.399, val_acc:0.809]
Epoch [6/120    avg_loss:0.354, val_acc:0.807]
Epoch [7/120    avg_loss:0.343, val_acc:0.806]
Epoch [8/120    avg_loss:0.254, val_acc:0.883]
Epoch [9/120    avg_loss:0.220, val_acc:0.924]
Epoch [10/120    avg_loss:0.184, val_acc:0.874]
Epoch [11/120    avg_loss:0.124, val_acc:0.944]
Epoch [12/120    avg_loss:0.098, val_acc:0.944]
Epoch [13/120    avg_loss:0.109, val_acc:0.935]
Epoch [14/120    avg_loss:0.120, val_acc:0.942]
Epoch [15/120    avg_loss:0.087, val_acc:0.961]
Epoch [16/120    avg_loss:0.102, val_acc:0.952]
Epoch [17/120    avg_loss:0.066, val_acc:0.954]
Epoch [18/120    avg_loss:0.061, val_acc:0.945]
Epoch [19/120    avg_loss:0.063, val_acc:0.950]
Epoch [20/120    avg_loss:0.057, val_acc:0.952]
Epoch [21/120    avg_loss:0.056, val_acc:0.945]
Epoch [22/120    avg_loss:0.075, val_acc:0.951]
Epoch [23/120    avg_loss:0.080, val_acc:0.951]
Epoch [24/120    avg_loss:0.056, val_acc:0.969]
Epoch [25/120    avg_loss:0.046, val_acc:0.944]
Epoch [26/120    avg_loss:0.055, val_acc:0.969]
Epoch [27/120    avg_loss:0.035, val_acc:0.963]
Epoch [28/120    avg_loss:0.047, val_acc:0.966]
Epoch [29/120    avg_loss:0.103, val_acc:0.952]
Epoch [30/120    avg_loss:0.074, val_acc:0.953]
Epoch [31/120    avg_loss:0.042, val_acc:0.955]
Epoch [32/120    avg_loss:0.032, val_acc:0.964]
Epoch [33/120    avg_loss:0.035, val_acc:0.964]
Epoch [34/120    avg_loss:0.069, val_acc:0.950]
Epoch [35/120    avg_loss:0.039, val_acc:0.957]
Epoch [36/120    avg_loss:0.040, val_acc:0.971]
Epoch [37/120    avg_loss:0.032, val_acc:0.959]
Epoch [38/120    avg_loss:0.027, val_acc:0.976]
Epoch [39/120    avg_loss:0.024, val_acc:0.967]
Epoch [40/120    avg_loss:0.028, val_acc:0.952]
Epoch [41/120    avg_loss:0.035, val_acc:0.962]
Epoch [42/120    avg_loss:0.071, val_acc:0.956]
Epoch [43/120    avg_loss:0.040, val_acc:0.967]
Epoch [44/120    avg_loss:0.019, val_acc:0.970]
Epoch [45/120    avg_loss:0.021, val_acc:0.967]
Epoch [46/120    avg_loss:0.017, val_acc:0.977]
Epoch [47/120    avg_loss:0.014, val_acc:0.981]
Epoch [48/120    avg_loss:0.010, val_acc:0.984]
Epoch [49/120    avg_loss:0.019, val_acc:0.980]
Epoch [50/120    avg_loss:0.017, val_acc:0.982]
Epoch [51/120    avg_loss:0.029, val_acc:0.977]
Epoch [52/120    avg_loss:0.014, val_acc:0.983]
Epoch [53/120    avg_loss:0.011, val_acc:0.980]
Epoch [54/120    avg_loss:0.006, val_acc:0.984]
Epoch [55/120    avg_loss:0.010, val_acc:0.982]
Epoch [56/120    avg_loss:0.006, val_acc:0.984]
Epoch [57/120    avg_loss:0.009, val_acc:0.978]
Epoch [58/120    avg_loss:0.009, val_acc:0.984]
Epoch [59/120    avg_loss:0.005, val_acc:0.981]
Epoch [60/120    avg_loss:0.006, val_acc:0.979]
Epoch [61/120    avg_loss:0.006, val_acc:0.982]
Epoch [62/120    avg_loss:0.007, val_acc:0.984]
Epoch [63/120    avg_loss:0.014, val_acc:0.982]
Epoch [64/120    avg_loss:0.020, val_acc:0.977]
Epoch [65/120    avg_loss:0.012, val_acc:0.980]
Epoch [66/120    avg_loss:0.012, val_acc:0.981]
Epoch [67/120    avg_loss:0.008, val_acc:0.984]
Epoch [68/120    avg_loss:0.054, val_acc:0.977]
Epoch [69/120    avg_loss:0.039, val_acc:0.969]
Epoch [70/120    avg_loss:0.040, val_acc:0.961]
Epoch [71/120    avg_loss:0.020, val_acc:0.979]
Epoch [72/120    avg_loss:0.010, val_acc:0.977]
Epoch [73/120    avg_loss:0.010, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.006, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.982]
Epoch [79/120    avg_loss:0.005, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.984]
Epoch [81/120    avg_loss:0.003, val_acc:0.985]
Epoch [82/120    avg_loss:0.004, val_acc:0.983]
Epoch [83/120    avg_loss:0.003, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.976]
Epoch [85/120    avg_loss:0.009, val_acc:0.978]
Epoch [86/120    avg_loss:0.006, val_acc:0.981]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.979]
Epoch [91/120    avg_loss:0.006, val_acc:0.980]
Epoch [92/120    avg_loss:0.011, val_acc:0.974]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.003, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.990]
Epoch [100/120    avg_loss:0.002, val_acc:0.990]
Epoch [101/120    avg_loss:0.003, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.002, val_acc:0.987]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.003, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.003, val_acc:0.984]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.002, val_acc:0.989]
Epoch [117/120    avg_loss:0.002, val_acc:0.990]
Epoch [118/120    avg_loss:0.002, val_acc:0.990]
Epoch [119/120    avg_loss:0.003, val_acc:0.990]
Epoch [120/120    avg_loss:0.003, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0     8     0    14     0     0     0]
 [    0     0 18069     0    14     0     7     0     0     0]
 [    0     0     0  2029     1     0     0     0     1     5]
 [    0    39    20     0  2885     0     0     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4868     0     0     2]
 [    0     0     0     0     0     0     6  1282     0     2]
 [    0     3     0     0    25     0     0     0  3543     0]
 [    0     0     0     5    14    34     0     0     0   866]]

Accuracy:
99.4312293639891

F1 scores:
[       nan 0.9950326  0.99864592 0.9970516  0.97482683 0.9871407
 0.99621406 0.99688958 0.99202016 0.96544036]

Kappa:
0.9924619544586827
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc70805c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.523, val_acc:0.454]
Epoch [2/120    avg_loss:0.946, val_acc:0.712]
Epoch [3/120    avg_loss:0.668, val_acc:0.687]
Epoch [4/120    avg_loss:0.503, val_acc:0.760]
Epoch [5/120    avg_loss:0.450, val_acc:0.791]
Epoch [6/120    avg_loss:0.321, val_acc:0.839]
Epoch [7/120    avg_loss:0.247, val_acc:0.912]
Epoch [8/120    avg_loss:0.196, val_acc:0.932]
Epoch [9/120    avg_loss:0.148, val_acc:0.948]
Epoch [10/120    avg_loss:0.179, val_acc:0.945]
Epoch [11/120    avg_loss:0.126, val_acc:0.965]
Epoch [12/120    avg_loss:0.114, val_acc:0.960]
Epoch [13/120    avg_loss:0.137, val_acc:0.924]
Epoch [14/120    avg_loss:0.099, val_acc:0.935]
Epoch [15/120    avg_loss:0.083, val_acc:0.953]
Epoch [16/120    avg_loss:0.059, val_acc:0.959]
Epoch [17/120    avg_loss:0.075, val_acc:0.962]
Epoch [18/120    avg_loss:0.098, val_acc:0.938]
Epoch [19/120    avg_loss:0.074, val_acc:0.977]
Epoch [20/120    avg_loss:0.041, val_acc:0.974]
Epoch [21/120    avg_loss:0.049, val_acc:0.972]
Epoch [22/120    avg_loss:0.039, val_acc:0.980]
Epoch [23/120    avg_loss:0.035, val_acc:0.982]
Epoch [24/120    avg_loss:0.036, val_acc:0.964]
Epoch [25/120    avg_loss:0.049, val_acc:0.977]
Epoch [26/120    avg_loss:0.022, val_acc:0.981]
Epoch [27/120    avg_loss:0.037, val_acc:0.977]
Epoch [28/120    avg_loss:0.055, val_acc:0.955]
Epoch [29/120    avg_loss:0.064, val_acc:0.957]
Epoch [30/120    avg_loss:0.087, val_acc:0.952]
Epoch [31/120    avg_loss:0.048, val_acc:0.974]
Epoch [32/120    avg_loss:0.021, val_acc:0.981]
Epoch [33/120    avg_loss:0.017, val_acc:0.978]
Epoch [34/120    avg_loss:0.021, val_acc:0.980]
Epoch [35/120    avg_loss:0.029, val_acc:0.977]
Epoch [36/120    avg_loss:0.045, val_acc:0.944]
Epoch [37/120    avg_loss:0.026, val_acc:0.971]
Epoch [38/120    avg_loss:0.018, val_acc:0.977]
Epoch [39/120    avg_loss:0.025, val_acc:0.978]
Epoch [40/120    avg_loss:0.010, val_acc:0.979]
Epoch [41/120    avg_loss:0.015, val_acc:0.980]
Epoch [42/120    avg_loss:0.012, val_acc:0.979]
Epoch [43/120    avg_loss:0.013, val_acc:0.979]
Epoch [44/120    avg_loss:0.018, val_acc:0.977]
Epoch [45/120    avg_loss:0.014, val_acc:0.981]
Epoch [46/120    avg_loss:0.010, val_acc:0.982]
Epoch [47/120    avg_loss:0.013, val_acc:0.984]
Epoch [48/120    avg_loss:0.012, val_acc:0.981]
Epoch [49/120    avg_loss:0.011, val_acc:0.982]
Epoch [50/120    avg_loss:0.012, val_acc:0.981]
Epoch [51/120    avg_loss:0.011, val_acc:0.983]
Epoch [52/120    avg_loss:0.013, val_acc:0.981]
Epoch [53/120    avg_loss:0.008, val_acc:0.982]
Epoch [54/120    avg_loss:0.016, val_acc:0.982]
Epoch [55/120    avg_loss:0.009, val_acc:0.982]
Epoch [56/120    avg_loss:0.009, val_acc:0.981]
Epoch [57/120    avg_loss:0.009, val_acc:0.982]
Epoch [58/120    avg_loss:0.010, val_acc:0.984]
Epoch [59/120    avg_loss:0.009, val_acc:0.983]
Epoch [60/120    avg_loss:0.010, val_acc:0.982]
Epoch [61/120    avg_loss:0.011, val_acc:0.982]
Epoch [62/120    avg_loss:0.012, val_acc:0.982]
Epoch [63/120    avg_loss:0.011, val_acc:0.984]
Epoch [64/120    avg_loss:0.013, val_acc:0.984]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.010, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.013, val_acc:0.983]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.008, val_acc:0.985]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.008, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.983]
Epoch [74/120    avg_loss:0.008, val_acc:0.983]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.983]
Epoch [77/120    avg_loss:0.021, val_acc:0.979]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.016, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.013, val_acc:0.984]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.007, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.985]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.006, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     0     1     0     0]
 [    0     0 18040     0    37     0    13     0     0     0]
 [    0     0     0  2025     3     0     0     0     0     8]
 [    0    47    18     0  2879     0     0     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4854     0     0    23]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     3     0     5    33     0     0     0  3527     3]
 [    0     0     0     6    17    50     0     0     0   846]]

Accuracy:
99.28180656978286

F1 scores:
[       nan 0.99605049 0.99809123 0.99459725 0.9691971  0.98120301
 0.99599877 0.99883676 0.98989615 0.94052251]

Kappa:
0.9904859010032641
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f542fb68710>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.620, val_acc:0.393]
Epoch [2/120    avg_loss:0.904, val_acc:0.677]
Epoch [3/120    avg_loss:0.630, val_acc:0.702]
Epoch [4/120    avg_loss:0.476, val_acc:0.774]
Epoch [5/120    avg_loss:0.412, val_acc:0.806]
Epoch [6/120    avg_loss:0.286, val_acc:0.853]
Epoch [7/120    avg_loss:0.286, val_acc:0.902]
Epoch [8/120    avg_loss:0.245, val_acc:0.822]
Epoch [9/120    avg_loss:0.198, val_acc:0.933]
Epoch [10/120    avg_loss:0.177, val_acc:0.925]
Epoch [11/120    avg_loss:0.220, val_acc:0.905]
Epoch [12/120    avg_loss:0.181, val_acc:0.935]
Epoch [13/120    avg_loss:0.138, val_acc:0.958]
Epoch [14/120    avg_loss:0.121, val_acc:0.951]
Epoch [15/120    avg_loss:0.102, val_acc:0.963]
Epoch [16/120    avg_loss:0.070, val_acc:0.968]
Epoch [17/120    avg_loss:0.067, val_acc:0.970]
Epoch [18/120    avg_loss:0.085, val_acc:0.889]
Epoch [19/120    avg_loss:0.141, val_acc:0.911]
Epoch [20/120    avg_loss:0.107, val_acc:0.925]
Epoch [21/120    avg_loss:0.091, val_acc:0.964]
Epoch [22/120    avg_loss:0.132, val_acc:0.905]
Epoch [23/120    avg_loss:0.291, val_acc:0.906]
Epoch [24/120    avg_loss:0.116, val_acc:0.959]
Epoch [25/120    avg_loss:0.062, val_acc:0.965]
Epoch [26/120    avg_loss:0.071, val_acc:0.965]
Epoch [27/120    avg_loss:0.052, val_acc:0.967]
Epoch [28/120    avg_loss:0.051, val_acc:0.979]
Epoch [29/120    avg_loss:0.040, val_acc:0.977]
Epoch [30/120    avg_loss:0.037, val_acc:0.974]
Epoch [31/120    avg_loss:0.071, val_acc:0.960]
Epoch [32/120    avg_loss:0.032, val_acc:0.984]
Epoch [33/120    avg_loss:0.026, val_acc:0.975]
Epoch [34/120    avg_loss:0.020, val_acc:0.977]
Epoch [35/120    avg_loss:0.027, val_acc:0.977]
Epoch [36/120    avg_loss:0.021, val_acc:0.978]
Epoch [37/120    avg_loss:0.025, val_acc:0.978]
Epoch [38/120    avg_loss:0.030, val_acc:0.977]
Epoch [39/120    avg_loss:0.026, val_acc:0.981]
Epoch [40/120    avg_loss:0.015, val_acc:0.979]
Epoch [41/120    avg_loss:0.021, val_acc:0.983]
Epoch [42/120    avg_loss:0.015, val_acc:0.988]
Epoch [43/120    avg_loss:0.021, val_acc:0.982]
Epoch [44/120    avg_loss:0.018, val_acc:0.984]
Epoch [45/120    avg_loss:0.019, val_acc:0.979]
Epoch [46/120    avg_loss:0.022, val_acc:0.983]
Epoch [47/120    avg_loss:0.016, val_acc:0.987]
Epoch [48/120    avg_loss:0.010, val_acc:0.985]
Epoch [49/120    avg_loss:0.016, val_acc:0.985]
Epoch [50/120    avg_loss:0.022, val_acc:0.932]
Epoch [51/120    avg_loss:0.018, val_acc:0.984]
Epoch [52/120    avg_loss:0.009, val_acc:0.983]
Epoch [53/120    avg_loss:0.007, val_acc:0.985]
Epoch [54/120    avg_loss:0.011, val_acc:0.985]
Epoch [55/120    avg_loss:0.016, val_acc:0.984]
Epoch [56/120    avg_loss:0.013, val_acc:0.983]
Epoch [57/120    avg_loss:0.014, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.985]
Epoch [59/120    avg_loss:0.008, val_acc:0.986]
Epoch [60/120    avg_loss:0.009, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.010, val_acc:0.986]
Epoch [63/120    avg_loss:0.007, val_acc:0.985]
Epoch [64/120    avg_loss:0.006, val_acc:0.984]
Epoch [65/120    avg_loss:0.011, val_acc:0.984]
Epoch [66/120    avg_loss:0.013, val_acc:0.984]
Epoch [67/120    avg_loss:0.010, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.984]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.007, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.984]
Epoch [73/120    avg_loss:0.008, val_acc:0.984]
Epoch [74/120    avg_loss:0.008, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.984]
Epoch [77/120    avg_loss:0.005, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.006, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.011, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0    10     4     0     2     0     0     0]
 [    0     2 18016     0    61     0    11     0     0     0]
 [    0     0     0  2034     1     0     0     0     0     1]
 [    0    39    18     0  2876     0     7     0    31     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4845     0     0    32]
 [    0     0     0     0     0     0     9  1278     0     3]
 [    0     2     0     0    33     0     0     0  3536     0]
 [    0     0     0     4    13    43     0     0     0   859]]

Accuracy:
99.20950521774758

F1 scores:
[       nan 0.99542316 0.99742561 0.99608227 0.96510067 0.98379193
 0.99364233 0.9953271  0.99075371 0.94655647]

Kappa:
0.9895316188576532
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f669fa88780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.584, val_acc:0.345]
Epoch [2/120    avg_loss:0.930, val_acc:0.523]
Epoch [3/120    avg_loss:0.629, val_acc:0.626]
Epoch [4/120    avg_loss:0.531, val_acc:0.819]
Epoch [5/120    avg_loss:0.403, val_acc:0.819]
Epoch [6/120    avg_loss:0.281, val_acc:0.834]
Epoch [7/120    avg_loss:0.296, val_acc:0.840]
Epoch [8/120    avg_loss:0.261, val_acc:0.897]
Epoch [9/120    avg_loss:0.276, val_acc:0.860]
Epoch [10/120    avg_loss:0.275, val_acc:0.917]
Epoch [11/120    avg_loss:0.180, val_acc:0.912]
Epoch [12/120    avg_loss:0.140, val_acc:0.936]
Epoch [13/120    avg_loss:0.108, val_acc:0.954]
Epoch [14/120    avg_loss:0.066, val_acc:0.946]
Epoch [15/120    avg_loss:0.070, val_acc:0.949]
Epoch [16/120    avg_loss:0.094, val_acc:0.953]
Epoch [17/120    avg_loss:0.077, val_acc:0.957]
Epoch [18/120    avg_loss:0.053, val_acc:0.962]
Epoch [19/120    avg_loss:0.060, val_acc:0.958]
Epoch [20/120    avg_loss:0.054, val_acc:0.958]
Epoch [21/120    avg_loss:0.076, val_acc:0.953]
Epoch [22/120    avg_loss:0.062, val_acc:0.961]
Epoch [23/120    avg_loss:0.060, val_acc:0.965]
Epoch [24/120    avg_loss:0.042, val_acc:0.959]
Epoch [25/120    avg_loss:0.041, val_acc:0.964]
Epoch [26/120    avg_loss:0.025, val_acc:0.971]
Epoch [27/120    avg_loss:0.041, val_acc:0.963]
Epoch [28/120    avg_loss:0.053, val_acc:0.967]
Epoch [29/120    avg_loss:0.036, val_acc:0.964]
Epoch [30/120    avg_loss:0.034, val_acc:0.968]
Epoch [31/120    avg_loss:0.019, val_acc:0.974]
Epoch [32/120    avg_loss:0.023, val_acc:0.970]
Epoch [33/120    avg_loss:0.047, val_acc:0.964]
Epoch [34/120    avg_loss:0.026, val_acc:0.974]
Epoch [35/120    avg_loss:0.015, val_acc:0.971]
Epoch [36/120    avg_loss:0.034, val_acc:0.964]
Epoch [37/120    avg_loss:0.041, val_acc:0.971]
Epoch [38/120    avg_loss:0.030, val_acc:0.934]
Epoch [39/120    avg_loss:0.042, val_acc:0.960]
Epoch [40/120    avg_loss:0.025, val_acc:0.971]
Epoch [41/120    avg_loss:0.035, val_acc:0.957]
Epoch [42/120    avg_loss:0.097, val_acc:0.962]
Epoch [43/120    avg_loss:0.043, val_acc:0.954]
Epoch [44/120    avg_loss:0.039, val_acc:0.961]
Epoch [45/120    avg_loss:0.047, val_acc:0.970]
Epoch [46/120    avg_loss:0.018, val_acc:0.974]
Epoch [47/120    avg_loss:0.020, val_acc:0.974]
Epoch [48/120    avg_loss:0.013, val_acc:0.977]
Epoch [49/120    avg_loss:0.009, val_acc:0.973]
Epoch [50/120    avg_loss:0.041, val_acc:0.964]
Epoch [51/120    avg_loss:0.030, val_acc:0.975]
Epoch [52/120    avg_loss:0.023, val_acc:0.970]
Epoch [53/120    avg_loss:0.017, val_acc:0.980]
Epoch [54/120    avg_loss:0.013, val_acc:0.971]
Epoch [55/120    avg_loss:0.008, val_acc:0.980]
Epoch [56/120    avg_loss:0.019, val_acc:0.977]
Epoch [57/120    avg_loss:0.017, val_acc:0.973]
Epoch [58/120    avg_loss:0.006, val_acc:0.980]
Epoch [59/120    avg_loss:0.007, val_acc:0.978]
Epoch [60/120    avg_loss:0.011, val_acc:0.971]
Epoch [61/120    avg_loss:0.005, val_acc:0.979]
Epoch [62/120    avg_loss:0.013, val_acc:0.978]
Epoch [63/120    avg_loss:0.011, val_acc:0.981]
Epoch [64/120    avg_loss:0.006, val_acc:0.978]
Epoch [65/120    avg_loss:0.007, val_acc:0.975]
Epoch [66/120    avg_loss:0.005, val_acc:0.979]
Epoch [67/120    avg_loss:0.013, val_acc:0.970]
Epoch [68/120    avg_loss:0.010, val_acc:0.963]
Epoch [69/120    avg_loss:0.010, val_acc:0.981]
Epoch [70/120    avg_loss:0.011, val_acc:0.977]
Epoch [71/120    avg_loss:0.006, val_acc:0.980]
Epoch [72/120    avg_loss:0.006, val_acc:0.979]
Epoch [73/120    avg_loss:0.005, val_acc:0.979]
Epoch [74/120    avg_loss:0.006, val_acc:0.977]
Epoch [75/120    avg_loss:0.006, val_acc:0.981]
Epoch [76/120    avg_loss:0.005, val_acc:0.977]
Epoch [77/120    avg_loss:0.009, val_acc:0.980]
Epoch [78/120    avg_loss:0.007, val_acc:0.959]
Epoch [79/120    avg_loss:0.007, val_acc:0.977]
Epoch [80/120    avg_loss:0.004, val_acc:0.978]
Epoch [81/120    avg_loss:0.006, val_acc:0.975]
Epoch [82/120    avg_loss:0.006, val_acc:0.980]
Epoch [83/120    avg_loss:0.005, val_acc:0.982]
Epoch [84/120    avg_loss:0.004, val_acc:0.982]
Epoch [85/120    avg_loss:0.004, val_acc:0.980]
Epoch [86/120    avg_loss:0.006, val_acc:0.980]
Epoch [87/120    avg_loss:0.006, val_acc:0.979]
Epoch [88/120    avg_loss:0.008, val_acc:0.977]
Epoch [89/120    avg_loss:0.005, val_acc:0.981]
Epoch [90/120    avg_loss:0.005, val_acc:0.979]
Epoch [91/120    avg_loss:0.003, val_acc:0.983]
Epoch [92/120    avg_loss:0.003, val_acc:0.983]
Epoch [93/120    avg_loss:0.005, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.004, val_acc:0.977]
Epoch [96/120    avg_loss:0.004, val_acc:0.982]
Epoch [97/120    avg_loss:0.002, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.977]
Epoch [99/120    avg_loss:0.005, val_acc:0.982]
Epoch [100/120    avg_loss:0.002, val_acc:0.984]
Epoch [101/120    avg_loss:0.002, val_acc:0.982]
Epoch [102/120    avg_loss:0.003, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.982]
Epoch [104/120    avg_loss:0.018, val_acc:0.978]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.004, val_acc:0.980]
Epoch [107/120    avg_loss:0.004, val_acc:0.983]
Epoch [108/120    avg_loss:0.003, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.003, val_acc:0.984]
Epoch [112/120    avg_loss:0.003, val_acc:0.979]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.003, val_acc:0.984]
Epoch [116/120    avg_loss:0.002, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.018, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.964]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     0     4     0     0    16     0     1]
 [    0     0 18034     0    56     0     0     0     0     0]
 [    0     0     0  2030     0     0     0     0     0     6]
 [    0    37    12     0  2889     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     0     0  4842     0     0    13]
 [    0     0     0     0     0     0     7  1281     0     2]
 [    0     0     0     0    38     0     0     0  3533     0]
 [    0     0     0    19    14    41     0     0     0   845]]

Accuracy:
99.22155544308679

F1 scores:
[       nan 0.99549689 0.99748334 0.99388005 0.96735309 0.98453414
 0.99486337 0.9903363  0.99088487 0.9462486 ]

Kappa:
0.9896874900926994
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71e2f75898>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.566, val_acc:0.385]
Epoch [2/120    avg_loss:1.010, val_acc:0.666]
Epoch [3/120    avg_loss:0.723, val_acc:0.740]
Epoch [4/120    avg_loss:0.533, val_acc:0.758]
Epoch [5/120    avg_loss:0.438, val_acc:0.811]
Epoch [6/120    avg_loss:0.332, val_acc:0.879]
Epoch [7/120    avg_loss:0.331, val_acc:0.887]
Epoch [8/120    avg_loss:0.219, val_acc:0.927]
Epoch [9/120    avg_loss:0.171, val_acc:0.875]
Epoch [10/120    avg_loss:0.211, val_acc:0.952]
Epoch [11/120    avg_loss:0.133, val_acc:0.953]
Epoch [12/120    avg_loss:0.126, val_acc:0.955]
Epoch [13/120    avg_loss:0.159, val_acc:0.927]
Epoch [14/120    avg_loss:0.126, val_acc:0.959]
Epoch [15/120    avg_loss:0.099, val_acc:0.967]
Epoch [16/120    avg_loss:0.084, val_acc:0.969]
Epoch [17/120    avg_loss:0.069, val_acc:0.970]
Epoch [18/120    avg_loss:0.062, val_acc:0.964]
Epoch [19/120    avg_loss:0.038, val_acc:0.973]
Epoch [20/120    avg_loss:0.053, val_acc:0.968]
Epoch [21/120    avg_loss:0.047, val_acc:0.970]
Epoch [22/120    avg_loss:0.047, val_acc:0.972]
Epoch [23/120    avg_loss:0.100, val_acc:0.964]
Epoch [24/120    avg_loss:0.066, val_acc:0.961]
Epoch [25/120    avg_loss:0.069, val_acc:0.967]
Epoch [26/120    avg_loss:0.053, val_acc:0.973]
Epoch [27/120    avg_loss:0.041, val_acc:0.972]
Epoch [28/120    avg_loss:0.023, val_acc:0.980]
Epoch [29/120    avg_loss:0.028, val_acc:0.983]
Epoch [30/120    avg_loss:0.055, val_acc:0.960]
Epoch [31/120    avg_loss:0.045, val_acc:0.964]
Epoch [32/120    avg_loss:0.033, val_acc:0.970]
Epoch [33/120    avg_loss:0.025, val_acc:0.978]
Epoch [34/120    avg_loss:0.020, val_acc:0.984]
Epoch [35/120    avg_loss:0.034, val_acc:0.979]
Epoch [36/120    avg_loss:0.053, val_acc:0.979]
Epoch [37/120    avg_loss:0.020, val_acc:0.977]
Epoch [38/120    avg_loss:0.023, val_acc:0.982]
Epoch [39/120    avg_loss:0.023, val_acc:0.950]
Epoch [40/120    avg_loss:0.023, val_acc:0.979]
Epoch [41/120    avg_loss:0.028, val_acc:0.977]
Epoch [42/120    avg_loss:0.020, val_acc:0.984]
Epoch [43/120    avg_loss:0.018, val_acc:0.984]
Epoch [44/120    avg_loss:0.030, val_acc:0.977]
Epoch [45/120    avg_loss:0.020, val_acc:0.984]
Epoch [46/120    avg_loss:0.027, val_acc:0.972]
Epoch [47/120    avg_loss:0.030, val_acc:0.983]
Epoch [48/120    avg_loss:0.024, val_acc:0.980]
Epoch [49/120    avg_loss:0.019, val_acc:0.986]
Epoch [50/120    avg_loss:0.009, val_acc:0.985]
Epoch [51/120    avg_loss:0.016, val_acc:0.970]
Epoch [52/120    avg_loss:0.013, val_acc:0.985]
Epoch [53/120    avg_loss:0.011, val_acc:0.982]
Epoch [54/120    avg_loss:0.013, val_acc:0.983]
Epoch [55/120    avg_loss:0.012, val_acc:0.990]
Epoch [56/120    avg_loss:0.008, val_acc:0.984]
Epoch [57/120    avg_loss:0.015, val_acc:0.976]
Epoch [58/120    avg_loss:0.012, val_acc:0.985]
Epoch [59/120    avg_loss:0.006, val_acc:0.984]
Epoch [60/120    avg_loss:0.010, val_acc:0.987]
Epoch [61/120    avg_loss:0.006, val_acc:0.986]
Epoch [62/120    avg_loss:0.016, val_acc:0.984]
Epoch [63/120    avg_loss:0.015, val_acc:0.984]
Epoch [64/120    avg_loss:0.006, val_acc:0.990]
Epoch [65/120    avg_loss:0.005, val_acc:0.990]
Epoch [66/120    avg_loss:0.006, val_acc:0.986]
Epoch [67/120    avg_loss:0.018, val_acc:0.970]
Epoch [68/120    avg_loss:0.010, val_acc:0.984]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.988]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.008, val_acc:0.989]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.005, val_acc:0.987]
Epoch [80/120    avg_loss:0.005, val_acc:0.985]
Epoch [81/120    avg_loss:0.004, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.989]
Epoch [85/120    avg_loss:0.004, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.003, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.003, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.003, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6428     0     0     4     0     0     0     0     0]
 [    0     5 17968     0    98     0    19     0     0     0]
 [    0     0     0  2030     2     0     0     0     1     3]
 [    0    43    18     0  2877     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4847     0     0    31]
 [    0     0     0     0     0     0     1  1285     0     4]
 [    0     2     0     0    47     0     0     0  3522     0]
 [    0     0     0     2    15    35     0     0     0   867]]

Accuracy:
99.12274359530524

F1 scores:
[       nan 0.9958172  0.9961193  0.99803343 0.95660848 0.98676749
 0.9940525  0.99805825 0.98918691 0.95065789]

Kappa:
0.98838878737516
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa47c9227b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.488, val_acc:0.653]
Epoch [2/120    avg_loss:0.915, val_acc:0.636]
Epoch [3/120    avg_loss:0.600, val_acc:0.787]
Epoch [4/120    avg_loss:0.487, val_acc:0.799]
Epoch [5/120    avg_loss:0.438, val_acc:0.898]
Epoch [6/120    avg_loss:0.318, val_acc:0.904]
Epoch [7/120    avg_loss:0.246, val_acc:0.916]
Epoch [8/120    avg_loss:0.225, val_acc:0.925]
Epoch [9/120    avg_loss:0.200, val_acc:0.930]
Epoch [10/120    avg_loss:0.156, val_acc:0.913]
Epoch [11/120    avg_loss:0.133, val_acc:0.951]
Epoch [12/120    avg_loss:0.115, val_acc:0.953]
Epoch [13/120    avg_loss:0.098, val_acc:0.957]
Epoch [14/120    avg_loss:0.128, val_acc:0.950]
Epoch [15/120    avg_loss:0.093, val_acc:0.944]
Epoch [16/120    avg_loss:0.062, val_acc:0.970]
Epoch [17/120    avg_loss:0.058, val_acc:0.968]
Epoch [18/120    avg_loss:0.073, val_acc:0.957]
Epoch [19/120    avg_loss:0.062, val_acc:0.964]
Epoch [20/120    avg_loss:0.083, val_acc:0.951]
Epoch [21/120    avg_loss:0.066, val_acc:0.975]
Epoch [22/120    avg_loss:0.078, val_acc:0.961]
Epoch [23/120    avg_loss:0.049, val_acc:0.970]
Epoch [24/120    avg_loss:0.037, val_acc:0.976]
Epoch [25/120    avg_loss:0.027, val_acc:0.970]
Epoch [26/120    avg_loss:0.029, val_acc:0.980]
Epoch [27/120    avg_loss:0.086, val_acc:0.957]
Epoch [28/120    avg_loss:0.036, val_acc:0.970]
Epoch [29/120    avg_loss:0.028, val_acc:0.980]
Epoch [30/120    avg_loss:0.026, val_acc:0.978]
Epoch [31/120    avg_loss:0.016, val_acc:0.977]
Epoch [32/120    avg_loss:0.032, val_acc:0.977]
Epoch [33/120    avg_loss:0.035, val_acc:0.970]
Epoch [34/120    avg_loss:0.022, val_acc:0.978]
Epoch [35/120    avg_loss:0.024, val_acc:0.975]
Epoch [36/120    avg_loss:0.015, val_acc:0.978]
Epoch [37/120    avg_loss:0.010, val_acc:0.982]
Epoch [38/120    avg_loss:0.013, val_acc:0.981]
Epoch [39/120    avg_loss:0.009, val_acc:0.981]
Epoch [40/120    avg_loss:0.012, val_acc:0.980]
Epoch [41/120    avg_loss:0.011, val_acc:0.984]
Epoch [42/120    avg_loss:0.013, val_acc:0.985]
Epoch [43/120    avg_loss:0.011, val_acc:0.975]
Epoch [44/120    avg_loss:0.010, val_acc:0.982]
Epoch [45/120    avg_loss:0.008, val_acc:0.984]
Epoch [46/120    avg_loss:0.007, val_acc:0.982]
Epoch [47/120    avg_loss:0.014, val_acc:0.977]
Epoch [48/120    avg_loss:0.007, val_acc:0.983]
Epoch [49/120    avg_loss:0.009, val_acc:0.978]
Epoch [50/120    avg_loss:0.012, val_acc:0.982]
Epoch [51/120    avg_loss:0.010, val_acc:0.982]
Epoch [52/120    avg_loss:0.006, val_acc:0.985]
Epoch [53/120    avg_loss:0.009, val_acc:0.981]
Epoch [54/120    avg_loss:0.009, val_acc:0.976]
Epoch [55/120    avg_loss:0.009, val_acc:0.986]
Epoch [56/120    avg_loss:0.015, val_acc:0.986]
Epoch [57/120    avg_loss:0.019, val_acc:0.977]
Epoch [58/120    avg_loss:0.013, val_acc:0.977]
Epoch [59/120    avg_loss:0.019, val_acc:0.981]
Epoch [60/120    avg_loss:0.006, val_acc:0.984]
Epoch [61/120    avg_loss:0.013, val_acc:0.972]
Epoch [62/120    avg_loss:0.010, val_acc:0.984]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.006, val_acc:0.984]
Epoch [65/120    avg_loss:0.005, val_acc:0.987]
Epoch [66/120    avg_loss:0.007, val_acc:0.983]
Epoch [67/120    avg_loss:0.007, val_acc:0.984]
Epoch [68/120    avg_loss:0.004, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.983]
Epoch [70/120    avg_loss:0.006, val_acc:0.981]
Epoch [71/120    avg_loss:0.031, val_acc:0.979]
Epoch [72/120    avg_loss:0.075, val_acc:0.961]
Epoch [73/120    avg_loss:0.083, val_acc:0.944]
Epoch [74/120    avg_loss:0.091, val_acc:0.942]
Epoch [75/120    avg_loss:0.112, val_acc:0.960]
Epoch [76/120    avg_loss:0.069, val_acc:0.953]
Epoch [77/120    avg_loss:0.044, val_acc:0.975]
Epoch [78/120    avg_loss:0.027, val_acc:0.980]
Epoch [79/120    avg_loss:0.012, val_acc:0.980]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.008, val_acc:0.979]
Epoch [83/120    avg_loss:0.011, val_acc:0.978]
Epoch [84/120    avg_loss:0.013, val_acc:0.979]
Epoch [85/120    avg_loss:0.008, val_acc:0.979]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.980]
Epoch [88/120    avg_loss:0.007, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.980]
Epoch [90/120    avg_loss:0.010, val_acc:0.980]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.011, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.980]
Epoch [96/120    avg_loss:0.008, val_acc:0.980]
Epoch [97/120    avg_loss:0.010, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.980]
Epoch [99/120    avg_loss:0.006, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.980]
Epoch [101/120    avg_loss:0.006, val_acc:0.980]
Epoch [102/120    avg_loss:0.011, val_acc:0.980]
Epoch [103/120    avg_loss:0.008, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.980]
Epoch [109/120    avg_loss:0.010, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.014, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.007, val_acc:0.980]
Epoch [116/120    avg_loss:0.014, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     1     0     0    18     0     0]
 [    0     0 18019     0    48     0     8     0    15     0]
 [    0     0     0  2015     0     0     0     0    14     7]
 [    0    41    20     1  2874     0     7     0    28     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     0     0  4837     0     0    31]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     2     0     6    35     0     0     0  3519     9]
 [    0     0     0    24    16    40     0     0     0   839]]

Accuracy:
99.07454269394837

F1 scores:
[       nan 0.99518932 0.99720524 0.98726115 0.9667003  0.98490566
 0.99404028 0.99229584 0.98474885 0.92912514]

Kappa:
0.9877429502791155
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96fef35780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.587, val_acc:0.682]
Epoch [2/120    avg_loss:0.933, val_acc:0.617]
Epoch [3/120    avg_loss:0.695, val_acc:0.654]
Epoch [4/120    avg_loss:0.512, val_acc:0.723]
Epoch [5/120    avg_loss:0.399, val_acc:0.779]
Epoch [6/120    avg_loss:0.374, val_acc:0.863]
Epoch [7/120    avg_loss:0.282, val_acc:0.914]
Epoch [8/120    avg_loss:0.250, val_acc:0.908]
Epoch [9/120    avg_loss:0.226, val_acc:0.912]
Epoch [10/120    avg_loss:0.203, val_acc:0.939]
Epoch [11/120    avg_loss:0.134, val_acc:0.921]
Epoch [12/120    avg_loss:0.119, val_acc:0.954]
Epoch [13/120    avg_loss:0.167, val_acc:0.916]
Epoch [14/120    avg_loss:0.128, val_acc:0.957]
Epoch [15/120    avg_loss:0.148, val_acc:0.948]
Epoch [16/120    avg_loss:0.110, val_acc:0.964]
Epoch [17/120    avg_loss:0.104, val_acc:0.967]
Epoch [18/120    avg_loss:0.096, val_acc:0.954]
Epoch [19/120    avg_loss:0.167, val_acc:0.934]
Epoch [20/120    avg_loss:0.121, val_acc:0.963]
Epoch [21/120    avg_loss:0.110, val_acc:0.958]
Epoch [22/120    avg_loss:0.071, val_acc:0.963]
Epoch [23/120    avg_loss:0.061, val_acc:0.967]
Epoch [24/120    avg_loss:0.057, val_acc:0.950]
Epoch [25/120    avg_loss:0.130, val_acc:0.959]
Epoch [26/120    avg_loss:0.077, val_acc:0.961]
Epoch [27/120    avg_loss:0.076, val_acc:0.963]
Epoch [28/120    avg_loss:0.063, val_acc:0.970]
Epoch [29/120    avg_loss:0.071, val_acc:0.964]
Epoch [30/120    avg_loss:0.078, val_acc:0.964]
Epoch [31/120    avg_loss:0.066, val_acc:0.957]
Epoch [32/120    avg_loss:0.058, val_acc:0.967]
Epoch [33/120    avg_loss:0.047, val_acc:0.961]
Epoch [34/120    avg_loss:0.057, val_acc:0.962]
Epoch [35/120    avg_loss:0.054, val_acc:0.957]
Epoch [36/120    avg_loss:0.034, val_acc:0.972]
Epoch [37/120    avg_loss:0.024, val_acc:0.979]
Epoch [38/120    avg_loss:0.020, val_acc:0.978]
Epoch [39/120    avg_loss:0.013, val_acc:0.975]
Epoch [40/120    avg_loss:0.023, val_acc:0.973]
Epoch [41/120    avg_loss:0.016, val_acc:0.964]
Epoch [42/120    avg_loss:0.018, val_acc:0.980]
Epoch [43/120    avg_loss:0.014, val_acc:0.980]
Epoch [44/120    avg_loss:0.019, val_acc:0.973]
Epoch [45/120    avg_loss:0.024, val_acc:0.980]
Epoch [46/120    avg_loss:0.042, val_acc:0.967]
Epoch [47/120    avg_loss:0.021, val_acc:0.978]
Epoch [48/120    avg_loss:0.014, val_acc:0.971]
Epoch [49/120    avg_loss:0.016, val_acc:0.976]
Epoch [50/120    avg_loss:0.007, val_acc:0.975]
Epoch [51/120    avg_loss:0.007, val_acc:0.981]
Epoch [52/120    avg_loss:0.008, val_acc:0.978]
Epoch [53/120    avg_loss:0.008, val_acc:0.978]
Epoch [54/120    avg_loss:0.009, val_acc:0.979]
Epoch [55/120    avg_loss:0.009, val_acc:0.977]
Epoch [56/120    avg_loss:0.017, val_acc:0.978]
Epoch [57/120    avg_loss:0.030, val_acc:0.975]
Epoch [58/120    avg_loss:0.012, val_acc:0.981]
Epoch [59/120    avg_loss:0.011, val_acc:0.977]
Epoch [60/120    avg_loss:0.011, val_acc:0.979]
Epoch [61/120    avg_loss:0.007, val_acc:0.979]
Epoch [62/120    avg_loss:0.009, val_acc:0.977]
Epoch [63/120    avg_loss:0.008, val_acc:0.979]
Epoch [64/120    avg_loss:0.010, val_acc:0.979]
Epoch [65/120    avg_loss:0.006, val_acc:0.979]
Epoch [66/120    avg_loss:0.005, val_acc:0.981]
Epoch [67/120    avg_loss:0.006, val_acc:0.981]
Epoch [68/120    avg_loss:0.006, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.979]
Epoch [70/120    avg_loss:0.006, val_acc:0.982]
Epoch [71/120    avg_loss:0.007, val_acc:0.982]
Epoch [72/120    avg_loss:0.012, val_acc:0.970]
Epoch [73/120    avg_loss:0.013, val_acc:0.983]
Epoch [74/120    avg_loss:0.006, val_acc:0.977]
Epoch [75/120    avg_loss:0.005, val_acc:0.979]
Epoch [76/120    avg_loss:0.007, val_acc:0.982]
Epoch [77/120    avg_loss:0.013, val_acc:0.980]
Epoch [78/120    avg_loss:0.009, val_acc:0.979]
Epoch [79/120    avg_loss:0.004, val_acc:0.979]
Epoch [80/120    avg_loss:0.004, val_acc:0.980]
Epoch [81/120    avg_loss:0.005, val_acc:0.979]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.981]
Epoch [84/120    avg_loss:0.003, val_acc:0.982]
Epoch [85/120    avg_loss:0.003, val_acc:0.982]
Epoch [86/120    avg_loss:0.005, val_acc:0.983]
Epoch [87/120    avg_loss:0.003, val_acc:0.982]
Epoch [88/120    avg_loss:0.005, val_acc:0.980]
Epoch [89/120    avg_loss:0.002, val_acc:0.984]
Epoch [90/120    avg_loss:0.003, val_acc:0.984]
Epoch [91/120    avg_loss:0.002, val_acc:0.988]
Epoch [92/120    avg_loss:0.003, val_acc:0.982]
Epoch [93/120    avg_loss:0.003, val_acc:0.983]
Epoch [94/120    avg_loss:0.003, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.978]
Epoch [96/120    avg_loss:0.004, val_acc:0.982]
Epoch [97/120    avg_loss:0.003, val_acc:0.979]
Epoch [98/120    avg_loss:0.004, val_acc:0.983]
Epoch [99/120    avg_loss:0.002, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.982]
Epoch [101/120    avg_loss:0.002, val_acc:0.982]
Epoch [102/120    avg_loss:0.003, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.983]
Epoch [105/120    avg_loss:0.004, val_acc:0.984]
Epoch [106/120    avg_loss:0.003, val_acc:0.984]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.002, val_acc:0.985]
Epoch [109/120    avg_loss:0.003, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.002, val_acc:0.985]
Epoch [115/120    avg_loss:0.002, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     0     5     0     0     0     0     0]
 [    0     0 18062     0     9     0    19     0     0     0]
 [    0     0     0  2035     0     0     0     0     0     1]
 [    0    39    20     1  2878     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4852     0     0    26]
 [    0     0     0     0     0     0     3  1286     0     1]
 [    0     3     0     0    26     0     0     0  3542     0]
 [    0     0     0     4    14    26     0     0     0   875]]

Accuracy:
99.44327958932833

F1 scores:
[       nan 0.99635687 0.99867301 0.99852797 0.97493225 0.99013657
 0.99436418 0.9984472  0.99215686 0.96048299]

Kappa:
0.9926230950778789
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd032f98780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.521, val_acc:0.294]
Epoch [2/120    avg_loss:0.905, val_acc:0.678]
Epoch [3/120    avg_loss:0.577, val_acc:0.757]
Epoch [4/120    avg_loss:0.403, val_acc:0.771]
Epoch [5/120    avg_loss:0.415, val_acc:0.785]
Epoch [6/120    avg_loss:0.350, val_acc:0.904]
Epoch [7/120    avg_loss:0.316, val_acc:0.948]
Epoch [8/120    avg_loss:0.204, val_acc:0.948]
Epoch [9/120    avg_loss:0.165, val_acc:0.951]
Epoch [10/120    avg_loss:0.168, val_acc:0.943]
Epoch [11/120    avg_loss:0.149, val_acc:0.876]
Epoch [12/120    avg_loss:0.135, val_acc:0.976]
Epoch [13/120    avg_loss:0.177, val_acc:0.933]
Epoch [14/120    avg_loss:0.129, val_acc:0.957]
Epoch [15/120    avg_loss:0.086, val_acc:0.957]
Epoch [16/120    avg_loss:0.102, val_acc:0.971]
Epoch [17/120    avg_loss:0.076, val_acc:0.961]
Epoch [18/120    avg_loss:0.072, val_acc:0.937]
Epoch [19/120    avg_loss:0.108, val_acc:0.905]
Epoch [20/120    avg_loss:0.113, val_acc:0.965]
Epoch [21/120    avg_loss:0.087, val_acc:0.975]
Epoch [22/120    avg_loss:0.055, val_acc:0.976]
Epoch [23/120    avg_loss:0.086, val_acc:0.934]
Epoch [24/120    avg_loss:0.048, val_acc:0.977]
Epoch [25/120    avg_loss:0.043, val_acc:0.974]
Epoch [26/120    avg_loss:0.030, val_acc:0.968]
Epoch [27/120    avg_loss:0.031, val_acc:0.977]
Epoch [28/120    avg_loss:0.043, val_acc:0.974]
Epoch [29/120    avg_loss:0.043, val_acc:0.973]
Epoch [30/120    avg_loss:0.042, val_acc:0.958]
Epoch [31/120    avg_loss:0.019, val_acc:0.982]
Epoch [32/120    avg_loss:0.013, val_acc:0.985]
Epoch [33/120    avg_loss:0.017, val_acc:0.987]
Epoch [34/120    avg_loss:0.026, val_acc:0.976]
Epoch [35/120    avg_loss:0.016, val_acc:0.984]
Epoch [36/120    avg_loss:0.022, val_acc:0.983]
Epoch [37/120    avg_loss:0.020, val_acc:0.984]
Epoch [38/120    avg_loss:0.015, val_acc:0.989]
Epoch [39/120    avg_loss:0.020, val_acc:0.979]
Epoch [40/120    avg_loss:0.039, val_acc:0.968]
Epoch [41/120    avg_loss:0.085, val_acc:0.970]
Epoch [42/120    avg_loss:0.068, val_acc:0.967]
Epoch [43/120    avg_loss:0.044, val_acc:0.983]
Epoch [44/120    avg_loss:0.033, val_acc:0.976]
Epoch [45/120    avg_loss:0.029, val_acc:0.977]
Epoch [46/120    avg_loss:0.039, val_acc:0.981]
Epoch [47/120    avg_loss:0.031, val_acc:0.984]
Epoch [48/120    avg_loss:0.047, val_acc:0.953]
Epoch [49/120    avg_loss:0.043, val_acc:0.981]
Epoch [50/120    avg_loss:0.017, val_acc:0.980]
Epoch [51/120    avg_loss:0.025, val_acc:0.985]
Epoch [52/120    avg_loss:0.009, val_acc:0.987]
Epoch [53/120    avg_loss:0.012, val_acc:0.989]
Epoch [54/120    avg_loss:0.010, val_acc:0.988]
Epoch [55/120    avg_loss:0.017, val_acc:0.988]
Epoch [56/120    avg_loss:0.012, val_acc:0.989]
Epoch [57/120    avg_loss:0.010, val_acc:0.989]
Epoch [58/120    avg_loss:0.013, val_acc:0.990]
Epoch [59/120    avg_loss:0.012, val_acc:0.988]
Epoch [60/120    avg_loss:0.018, val_acc:0.990]
Epoch [61/120    avg_loss:0.008, val_acc:0.990]
Epoch [62/120    avg_loss:0.010, val_acc:0.990]
Epoch [63/120    avg_loss:0.012, val_acc:0.990]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.008, val_acc:0.989]
Epoch [66/120    avg_loss:0.013, val_acc:0.989]
Epoch [67/120    avg_loss:0.010, val_acc:0.989]
Epoch [68/120    avg_loss:0.011, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.989]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.012, val_acc:0.989]
Epoch [72/120    avg_loss:0.009, val_acc:0.989]
Epoch [73/120    avg_loss:0.006, val_acc:0.989]
Epoch [74/120    avg_loss:0.005, val_acc:0.989]
Epoch [75/120    avg_loss:0.007, val_acc:0.989]
Epoch [76/120    avg_loss:0.011, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.008, val_acc:0.989]
Epoch [79/120    avg_loss:0.012, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.989]
Epoch [82/120    avg_loss:0.008, val_acc:0.989]
Epoch [83/120    avg_loss:0.009, val_acc:0.989]
Epoch [84/120    avg_loss:0.007, val_acc:0.989]
Epoch [85/120    avg_loss:0.011, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.989]
Epoch [88/120    avg_loss:0.009, val_acc:0.989]
Epoch [89/120    avg_loss:0.004, val_acc:0.989]
Epoch [90/120    avg_loss:0.008, val_acc:0.989]
Epoch [91/120    avg_loss:0.013, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.010, val_acc:0.989]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.010, val_acc:0.989]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.010, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.010, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.010, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.989]
Epoch [113/120    avg_loss:0.008, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.011, val_acc:0.989]
Epoch [118/120    avg_loss:0.013, val_acc:0.989]
Epoch [119/120    avg_loss:0.012, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     1     5     0    12    27     0     0]
 [    0     0 18061     0    17     0    12     0     0     0]
 [    0     0     0  2027     0     0     0     0     5     4]
 [    0    41    20     4  2878     0     1     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4856     0     0    22]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0     4     0     0    34     0     0     0  3511    22]
 [    0     0     0    14    14    37     0     0     0   854]]

Accuracy:
99.20709517267973

F1 scores:
[       nan 0.99300373 0.99864532 0.99314062 0.9722973  0.98602191
 0.99498002 0.98770177 0.98692902 0.93640351]

Kappa:
0.9894953269615597
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd25ca48780>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.482, val_acc:0.681]
Epoch [2/120    avg_loss:0.826, val_acc:0.448]
Epoch [3/120    avg_loss:0.614, val_acc:0.752]
Epoch [4/120    avg_loss:0.470, val_acc:0.685]
Epoch [5/120    avg_loss:0.386, val_acc:0.723]
Epoch [6/120    avg_loss:0.309, val_acc:0.774]
Epoch [7/120    avg_loss:0.335, val_acc:0.802]
Epoch [8/120    avg_loss:0.340, val_acc:0.839]
Epoch [9/120    avg_loss:0.266, val_acc:0.764]
Epoch [10/120    avg_loss:0.213, val_acc:0.909]
Epoch [11/120    avg_loss:0.187, val_acc:0.927]
Epoch [12/120    avg_loss:0.145, val_acc:0.915]
Epoch [13/120    avg_loss:0.138, val_acc:0.903]
Epoch [14/120    avg_loss:0.126, val_acc:0.943]
Epoch [15/120    avg_loss:0.112, val_acc:0.906]
Epoch [16/120    avg_loss:0.091, val_acc:0.937]
Epoch [17/120    avg_loss:0.099, val_acc:0.919]
Epoch [18/120    avg_loss:0.106, val_acc:0.938]
Epoch [19/120    avg_loss:0.089, val_acc:0.948]
Epoch [20/120    avg_loss:0.073, val_acc:0.940]
Epoch [21/120    avg_loss:0.065, val_acc:0.953]
Epoch [22/120    avg_loss:0.092, val_acc:0.922]
Epoch [23/120    avg_loss:0.078, val_acc:0.955]
Epoch [24/120    avg_loss:0.041, val_acc:0.956]
Epoch [25/120    avg_loss:0.051, val_acc:0.960]
Epoch [26/120    avg_loss:0.080, val_acc:0.932]
Epoch [27/120    avg_loss:0.114, val_acc:0.957]
Epoch [28/120    avg_loss:0.067, val_acc:0.932]
Epoch [29/120    avg_loss:0.051, val_acc:0.959]
Epoch [30/120    avg_loss:0.032, val_acc:0.960]
Epoch [31/120    avg_loss:0.033, val_acc:0.965]
Epoch [32/120    avg_loss:0.036, val_acc:0.967]
Epoch [33/120    avg_loss:0.014, val_acc:0.975]
Epoch [34/120    avg_loss:0.024, val_acc:0.978]
Epoch [35/120    avg_loss:0.031, val_acc:0.958]
Epoch [36/120    avg_loss:0.027, val_acc:0.973]
Epoch [37/120    avg_loss:0.022, val_acc:0.968]
Epoch [38/120    avg_loss:0.020, val_acc:0.975]
Epoch [39/120    avg_loss:0.026, val_acc:0.981]
Epoch [40/120    avg_loss:0.051, val_acc:0.969]
Epoch [41/120    avg_loss:0.022, val_acc:0.972]
Epoch [42/120    avg_loss:0.055, val_acc:0.969]
Epoch [43/120    avg_loss:0.023, val_acc:0.976]
Epoch [44/120    avg_loss:0.040, val_acc:0.975]
Epoch [45/120    avg_loss:0.025, val_acc:0.977]
Epoch [46/120    avg_loss:0.020, val_acc:0.980]
Epoch [47/120    avg_loss:0.013, val_acc:0.980]
Epoch [48/120    avg_loss:0.009, val_acc:0.981]
Epoch [49/120    avg_loss:0.015, val_acc:0.976]
Epoch [50/120    avg_loss:0.020, val_acc:0.972]
Epoch [51/120    avg_loss:0.019, val_acc:0.979]
Epoch [52/120    avg_loss:0.011, val_acc:0.978]
Epoch [53/120    avg_loss:0.014, val_acc:0.983]
Epoch [54/120    avg_loss:0.015, val_acc:0.983]
Epoch [55/120    avg_loss:0.014, val_acc:0.980]
Epoch [56/120    avg_loss:0.011, val_acc:0.978]
Epoch [57/120    avg_loss:0.013, val_acc:0.977]
Epoch [58/120    avg_loss:0.015, val_acc:0.979]
Epoch [59/120    avg_loss:0.016, val_acc:0.973]
Epoch [60/120    avg_loss:0.014, val_acc:0.975]
Epoch [61/120    avg_loss:0.057, val_acc:0.963]
Epoch [62/120    avg_loss:0.024, val_acc:0.973]
Epoch [63/120    avg_loss:0.011, val_acc:0.974]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.009, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.978]
Epoch [67/120    avg_loss:0.008, val_acc:0.978]
Epoch [68/120    avg_loss:0.017, val_acc:0.983]
Epoch [69/120    avg_loss:0.012, val_acc:0.982]
Epoch [70/120    avg_loss:0.009, val_acc:0.982]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.008, val_acc:0.982]
Epoch [73/120    avg_loss:0.007, val_acc:0.983]
Epoch [74/120    avg_loss:0.005, val_acc:0.983]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.982]
Epoch [77/120    avg_loss:0.007, val_acc:0.982]
Epoch [78/120    avg_loss:0.009, val_acc:0.982]
Epoch [79/120    avg_loss:0.006, val_acc:0.983]
Epoch [80/120    avg_loss:0.005, val_acc:0.983]
Epoch [81/120    avg_loss:0.007, val_acc:0.983]
Epoch [82/120    avg_loss:0.006, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.005, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.982]
Epoch [88/120    avg_loss:0.003, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.005, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.983]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.005, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     5     0     0     0     0     6     7]
 [    0     0 18062     0     5     0    18     0     5     0]
 [    0     3     0  1944     0     0     0     0    89     0]
 [    0     5     0     0  2965     0     0     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4865     0     0     0]
 [    0    14     0     0     0     0     3  1265     2     6]
 [    0     6     0    44    39     0     0     0  3482     0]
 [    0     0     0     0     0    19     0     0     0   900]]

Accuracy:
99.29867688525776

F1 scores:
[       nan 0.99642691 0.99886631 0.96500372 0.991473   0.99277292
 0.99651782 0.99021526 0.97303339 0.98253275]

Kappa:
0.9907077959790491
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f06c2eec7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.435, val_acc:0.698]
Epoch [2/120    avg_loss:0.806, val_acc:0.435]
Epoch [3/120    avg_loss:0.710, val_acc:0.595]
Epoch [4/120    avg_loss:0.528, val_acc:0.694]
Epoch [5/120    avg_loss:0.385, val_acc:0.746]
Epoch [6/120    avg_loss:0.330, val_acc:0.843]
Epoch [7/120    avg_loss:0.279, val_acc:0.868]
Epoch [8/120    avg_loss:0.239, val_acc:0.882]
Epoch [9/120    avg_loss:0.204, val_acc:0.856]
Epoch [10/120    avg_loss:0.172, val_acc:0.929]
Epoch [11/120    avg_loss:0.165, val_acc:0.922]
Epoch [12/120    avg_loss:0.149, val_acc:0.961]
Epoch [13/120    avg_loss:0.120, val_acc:0.943]
Epoch [14/120    avg_loss:0.125, val_acc:0.951]
Epoch [15/120    avg_loss:0.179, val_acc:0.890]
Epoch [16/120    avg_loss:0.106, val_acc:0.935]
Epoch [17/120    avg_loss:0.082, val_acc:0.956]
Epoch [18/120    avg_loss:0.088, val_acc:0.930]
Epoch [19/120    avg_loss:0.075, val_acc:0.838]
Epoch [20/120    avg_loss:0.069, val_acc:0.971]
Epoch [21/120    avg_loss:0.073, val_acc:0.964]
Epoch [22/120    avg_loss:0.052, val_acc:0.974]
Epoch [23/120    avg_loss:0.043, val_acc:0.967]
Epoch [24/120    avg_loss:0.069, val_acc:0.931]
Epoch [25/120    avg_loss:0.069, val_acc:0.952]
Epoch [26/120    avg_loss:0.043, val_acc:0.953]
Epoch [27/120    avg_loss:0.055, val_acc:0.962]
Epoch [28/120    avg_loss:0.044, val_acc:0.968]
Epoch [29/120    avg_loss:0.039, val_acc:0.972]
Epoch [30/120    avg_loss:0.022, val_acc:0.980]
Epoch [31/120    avg_loss:0.021, val_acc:0.974]
Epoch [32/120    avg_loss:0.019, val_acc:0.983]
Epoch [33/120    avg_loss:0.022, val_acc:0.973]
Epoch [34/120    avg_loss:0.018, val_acc:0.986]
Epoch [35/120    avg_loss:0.011, val_acc:0.978]
Epoch [36/120    avg_loss:0.016, val_acc:0.975]
Epoch [37/120    avg_loss:0.016, val_acc:0.984]
Epoch [38/120    avg_loss:0.021, val_acc:0.976]
Epoch [39/120    avg_loss:0.022, val_acc:0.980]
Epoch [40/120    avg_loss:0.024, val_acc:0.983]
Epoch [41/120    avg_loss:0.015, val_acc:0.970]
Epoch [42/120    avg_loss:0.024, val_acc:0.980]
Epoch [43/120    avg_loss:0.018, val_acc:0.980]
Epoch [44/120    avg_loss:0.029, val_acc:0.978]
Epoch [45/120    avg_loss:0.016, val_acc:0.978]
Epoch [46/120    avg_loss:0.009, val_acc:0.991]
Epoch [47/120    avg_loss:0.009, val_acc:0.987]
Epoch [48/120    avg_loss:0.014, val_acc:0.983]
Epoch [49/120    avg_loss:0.009, val_acc:0.989]
Epoch [50/120    avg_loss:0.009, val_acc:0.988]
Epoch [51/120    avg_loss:0.010, val_acc:0.982]
Epoch [52/120    avg_loss:0.016, val_acc:0.977]
Epoch [53/120    avg_loss:0.042, val_acc:0.973]
Epoch [54/120    avg_loss:0.018, val_acc:0.980]
Epoch [55/120    avg_loss:0.015, val_acc:0.975]
Epoch [56/120    avg_loss:0.020, val_acc:0.986]
Epoch [57/120    avg_loss:0.016, val_acc:0.988]
Epoch [58/120    avg_loss:0.017, val_acc:0.983]
Epoch [59/120    avg_loss:0.025, val_acc:0.976]
Epoch [60/120    avg_loss:0.016, val_acc:0.982]
Epoch [61/120    avg_loss:0.011, val_acc:0.985]
Epoch [62/120    avg_loss:0.006, val_acc:0.984]
Epoch [63/120    avg_loss:0.010, val_acc:0.984]
Epoch [64/120    avg_loss:0.008, val_acc:0.986]
Epoch [65/120    avg_loss:0.009, val_acc:0.987]
Epoch [66/120    avg_loss:0.009, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.986]
Epoch [68/120    avg_loss:0.007, val_acc:0.987]
Epoch [69/120    avg_loss:0.007, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.988]
Epoch [71/120    avg_loss:0.007, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.008, val_acc:0.988]
Epoch [74/120    avg_loss:0.004, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.005, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.018, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     0     0     0     0     7    12]
 [    0     0 18066     0    11     0    11     0     2     0]
 [    0     4     0  1954     0     0     0     0    76     2]
 [    0    21     0     1  2939     0     2     0     5     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    54     0     0     0  4824     0     0     0]
 [    0    19     0     0     0     0     2  1266     0     3]
 [    0     3     0    41    44     0     0     0  3483     0]
 [    0     0     0     0     0    12     0     0     0   907]]

Accuracy:
99.19022485720483

F1 scores:
[       nan 0.99488055 0.9978459  0.96924603 0.98524975 0.99542334
 0.99289904 0.99061033 0.97508399 0.98213319]

Kappa:
0.9892657222390323
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f877592b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.429, val_acc:0.623]
Epoch [2/120    avg_loss:0.842, val_acc:0.439]
Epoch [3/120    avg_loss:0.608, val_acc:0.752]
Epoch [4/120    avg_loss:0.433, val_acc:0.773]
Epoch [5/120    avg_loss:0.369, val_acc:0.830]
Epoch [6/120    avg_loss:0.326, val_acc:0.786]
Epoch [7/120    avg_loss:0.266, val_acc:0.914]
Epoch [8/120    avg_loss:0.203, val_acc:0.890]
Epoch [9/120    avg_loss:0.203, val_acc:0.907]
Epoch [10/120    avg_loss:0.216, val_acc:0.917]
Epoch [11/120    avg_loss:0.151, val_acc:0.934]
Epoch [12/120    avg_loss:0.144, val_acc:0.814]
Epoch [13/120    avg_loss:0.110, val_acc:0.941]
Epoch [14/120    avg_loss:0.111, val_acc:0.917]
Epoch [15/120    avg_loss:0.137, val_acc:0.953]
Epoch [16/120    avg_loss:0.067, val_acc:0.960]
Epoch [17/120    avg_loss:0.073, val_acc:0.953]
Epoch [18/120    avg_loss:0.052, val_acc:0.968]
Epoch [19/120    avg_loss:0.067, val_acc:0.953]
Epoch [20/120    avg_loss:0.110, val_acc:0.974]
Epoch [21/120    avg_loss:0.069, val_acc:0.941]
Epoch [22/120    avg_loss:0.054, val_acc:0.977]
Epoch [23/120    avg_loss:0.044, val_acc:0.973]
Epoch [24/120    avg_loss:0.042, val_acc:0.958]
Epoch [25/120    avg_loss:0.047, val_acc:0.966]
Epoch [26/120    avg_loss:0.047, val_acc:0.963]
Epoch [27/120    avg_loss:0.034, val_acc:0.970]
Epoch [28/120    avg_loss:0.022, val_acc:0.978]
Epoch [29/120    avg_loss:0.034, val_acc:0.976]
Epoch [30/120    avg_loss:0.052, val_acc:0.972]
Epoch [31/120    avg_loss:0.048, val_acc:0.976]
Epoch [32/120    avg_loss:0.029, val_acc:0.978]
Epoch [33/120    avg_loss:0.024, val_acc:0.973]
Epoch [34/120    avg_loss:0.018, val_acc:0.978]
Epoch [35/120    avg_loss:0.014, val_acc:0.978]
Epoch [36/120    avg_loss:0.013, val_acc:0.982]
Epoch [37/120    avg_loss:0.020, val_acc:0.979]
Epoch [38/120    avg_loss:0.013, val_acc:0.977]
Epoch [39/120    avg_loss:0.016, val_acc:0.981]
Epoch [40/120    avg_loss:0.029, val_acc:0.969]
Epoch [41/120    avg_loss:0.018, val_acc:0.933]
Epoch [42/120    avg_loss:0.041, val_acc:0.978]
Epoch [43/120    avg_loss:0.029, val_acc:0.978]
Epoch [44/120    avg_loss:0.042, val_acc:0.975]
Epoch [45/120    avg_loss:0.044, val_acc:0.983]
Epoch [46/120    avg_loss:0.018, val_acc:0.975]
Epoch [47/120    avg_loss:0.022, val_acc:0.973]
Epoch [48/120    avg_loss:0.015, val_acc:0.980]
Epoch [49/120    avg_loss:0.011, val_acc:0.983]
Epoch [50/120    avg_loss:0.009, val_acc:0.988]
Epoch [51/120    avg_loss:0.013, val_acc:0.984]
Epoch [52/120    avg_loss:0.008, val_acc:0.983]
Epoch [53/120    avg_loss:0.009, val_acc:0.984]
Epoch [54/120    avg_loss:0.008, val_acc:0.983]
Epoch [55/120    avg_loss:0.012, val_acc:0.984]
Epoch [56/120    avg_loss:0.015, val_acc:0.983]
Epoch [57/120    avg_loss:0.016, val_acc:0.987]
Epoch [58/120    avg_loss:0.011, val_acc:0.987]
Epoch [59/120    avg_loss:0.007, val_acc:0.980]
Epoch [60/120    avg_loss:0.007, val_acc:0.985]
Epoch [61/120    avg_loss:0.009, val_acc:0.982]
Epoch [62/120    avg_loss:0.006, val_acc:0.991]
Epoch [63/120    avg_loss:0.012, val_acc:0.986]
Epoch [64/120    avg_loss:0.007, val_acc:0.986]
Epoch [65/120    avg_loss:0.007, val_acc:0.989]
Epoch [66/120    avg_loss:0.012, val_acc:0.987]
Epoch [67/120    avg_loss:0.015, val_acc:0.982]
Epoch [68/120    avg_loss:0.007, val_acc:0.986]
Epoch [69/120    avg_loss:0.004, val_acc:0.989]
Epoch [70/120    avg_loss:0.006, val_acc:0.984]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.005, val_acc:0.989]
Epoch [73/120    avg_loss:0.004, val_acc:0.989]
Epoch [74/120    avg_loss:0.005, val_acc:0.985]
Epoch [75/120    avg_loss:0.015, val_acc:0.978]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.005, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.991]
Epoch [80/120    avg_loss:0.006, val_acc:0.989]
Epoch [81/120    avg_loss:0.002, val_acc:0.988]
Epoch [82/120    avg_loss:0.004, val_acc:0.990]
Epoch [83/120    avg_loss:0.002, val_acc:0.990]
Epoch [84/120    avg_loss:0.003, val_acc:0.990]
Epoch [85/120    avg_loss:0.003, val_acc:0.991]
Epoch [86/120    avg_loss:0.004, val_acc:0.991]
Epoch [87/120    avg_loss:0.003, val_acc:0.991]
Epoch [88/120    avg_loss:0.004, val_acc:0.990]
Epoch [89/120    avg_loss:0.004, val_acc:0.991]
Epoch [90/120    avg_loss:0.003, val_acc:0.990]
Epoch [91/120    avg_loss:0.010, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.003, val_acc:0.988]
Epoch [95/120    avg_loss:0.002, val_acc:0.989]
Epoch [96/120    avg_loss:0.002, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.002, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.002, val_acc:0.991]
Epoch [104/120    avg_loss:0.003, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.002, val_acc:0.992]
Epoch [109/120    avg_loss:0.003, val_acc:0.992]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.002, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.003, val_acc:0.991]
Epoch [115/120    avg_loss:0.003, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.002, val_acc:0.991]
Epoch [118/120    avg_loss:0.003, val_acc:0.991]
Epoch [119/120    avg_loss:0.002, val_acc:0.991]
Epoch [120/120    avg_loss:0.003, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     3     0     0     0     0    26     4]
 [    0     1 18060     0     7     0    15     0     7     0]
 [    0     0     0  1989     0     0     0     0    47     0]
 [    0    28     2     1  2930     0     3     0     4     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4874     0     0     0]
 [    0     8     0     0     0     0     0  1278     0     4]
 [    0     6     0    66    32     0     0     0  3467     0]
 [    0     0     0     0     4    10     0     0     0   905]]

Accuracy:
99.31072711059697

F1 scores:
[       nan 0.99409663 0.99900431 0.97142857 0.98570227 0.99618321
 0.99774821 0.9953271  0.97360292 0.98583878]

Kappa:
0.9908690189361282
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a5e94b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.432, val_acc:0.492]
Epoch [2/120    avg_loss:0.775, val_acc:0.430]
Epoch [3/120    avg_loss:0.605, val_acc:0.811]
Epoch [4/120    avg_loss:0.438, val_acc:0.657]
Epoch [5/120    avg_loss:0.367, val_acc:0.782]
Epoch [6/120    avg_loss:0.261, val_acc:0.869]
Epoch [7/120    avg_loss:0.229, val_acc:0.895]
Epoch [8/120    avg_loss:0.196, val_acc:0.913]
Epoch [9/120    avg_loss:0.201, val_acc:0.927]
Epoch [10/120    avg_loss:0.154, val_acc:0.906]
Epoch [11/120    avg_loss:0.181, val_acc:0.853]
Epoch [12/120    avg_loss:0.144, val_acc:0.919]
Epoch [13/120    avg_loss:0.119, val_acc:0.951]
Epoch [14/120    avg_loss:0.097, val_acc:0.959]
Epoch [15/120    avg_loss:0.108, val_acc:0.958]
Epoch [16/120    avg_loss:0.072, val_acc:0.954]
Epoch [17/120    avg_loss:0.062, val_acc:0.963]
Epoch [18/120    avg_loss:0.064, val_acc:0.966]
Epoch [19/120    avg_loss:0.066, val_acc:0.972]
Epoch [20/120    avg_loss:0.047, val_acc:0.977]
Epoch [21/120    avg_loss:0.038, val_acc:0.968]
Epoch [22/120    avg_loss:0.049, val_acc:0.939]
Epoch [23/120    avg_loss:0.049, val_acc:0.958]
Epoch [24/120    avg_loss:0.036, val_acc:0.979]
Epoch [25/120    avg_loss:0.036, val_acc:0.969]
Epoch [26/120    avg_loss:0.037, val_acc:0.978]
Epoch [27/120    avg_loss:0.037, val_acc:0.977]
Epoch [28/120    avg_loss:0.026, val_acc:0.982]
Epoch [29/120    avg_loss:0.023, val_acc:0.980]
Epoch [30/120    avg_loss:0.019, val_acc:0.986]
Epoch [31/120    avg_loss:0.024, val_acc:0.967]
Epoch [32/120    avg_loss:0.045, val_acc:0.982]
Epoch [33/120    avg_loss:0.033, val_acc:0.970]
Epoch [34/120    avg_loss:0.031, val_acc:0.976]
Epoch [35/120    avg_loss:0.022, val_acc:0.985]
Epoch [36/120    avg_loss:0.035, val_acc:0.984]
Epoch [37/120    avg_loss:0.020, val_acc:0.989]
Epoch [38/120    avg_loss:0.017, val_acc:0.982]
Epoch [39/120    avg_loss:0.012, val_acc:0.979]
Epoch [40/120    avg_loss:0.008, val_acc:0.990]
Epoch [41/120    avg_loss:0.013, val_acc:0.986]
Epoch [42/120    avg_loss:0.009, val_acc:0.979]
Epoch [43/120    avg_loss:0.020, val_acc:0.973]
Epoch [44/120    avg_loss:0.011, val_acc:0.989]
Epoch [45/120    avg_loss:0.011, val_acc:0.968]
Epoch [46/120    avg_loss:0.017, val_acc:0.981]
Epoch [47/120    avg_loss:0.014, val_acc:0.985]
Epoch [48/120    avg_loss:0.025, val_acc:0.982]
Epoch [49/120    avg_loss:0.014, val_acc:0.989]
Epoch [50/120    avg_loss:0.020, val_acc:0.978]
Epoch [51/120    avg_loss:0.017, val_acc:0.987]
Epoch [52/120    avg_loss:0.013, val_acc:0.982]
Epoch [53/120    avg_loss:0.009, val_acc:0.987]
Epoch [54/120    avg_loss:0.006, val_acc:0.987]
Epoch [55/120    avg_loss:0.007, val_acc:0.987]
Epoch [56/120    avg_loss:0.007, val_acc:0.988]
Epoch [57/120    avg_loss:0.005, val_acc:0.990]
Epoch [58/120    avg_loss:0.005, val_acc:0.989]
Epoch [59/120    avg_loss:0.006, val_acc:0.989]
Epoch [60/120    avg_loss:0.007, val_acc:0.988]
Epoch [61/120    avg_loss:0.005, val_acc:0.989]
Epoch [62/120    avg_loss:0.004, val_acc:0.989]
Epoch [63/120    avg_loss:0.005, val_acc:0.989]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.008, val_acc:0.988]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.011, val_acc:0.988]
Epoch [68/120    avg_loss:0.008, val_acc:0.989]
Epoch [69/120    avg_loss:0.008, val_acc:0.989]
Epoch [70/120    avg_loss:0.006, val_acc:0.989]
Epoch [71/120    avg_loss:0.007, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.989]
Epoch [74/120    avg_loss:0.006, val_acc:0.989]
Epoch [75/120    avg_loss:0.005, val_acc:0.989]
Epoch [76/120    avg_loss:0.011, val_acc:0.989]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.989]
Epoch [79/120    avg_loss:0.004, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.989]
Epoch [82/120    avg_loss:0.005, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.004, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.989]
Epoch [87/120    avg_loss:0.005, val_acc:0.989]
Epoch [88/120    avg_loss:0.006, val_acc:0.989]
Epoch [89/120    avg_loss:0.007, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.989]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.990]
Epoch [97/120    avg_loss:0.004, val_acc:0.990]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0     0     0     1    11     2     8]
 [    0     0 18021     0    13     0    53     0     3     0]
 [    0     2     0  1964     0     0     0     0    68     2]
 [    0    18     0     0  2944     0     2     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4870     0     0     0]
 [    0     9     0     0     0     0     0  1274     0     7]
 [    0     9     0    44    35     0     0     0  3483     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
99.24565589376522

F1 scores:
[       nan 0.99534161 0.99786816 0.97131553 0.98725687 0.99618321
 0.99347205 0.98951456 0.97672462 0.98376623]

Kappa:
0.9900105119496125
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe7d89f748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.448, val_acc:0.613]
Epoch [2/120    avg_loss:0.799, val_acc:0.612]
Epoch [3/120    avg_loss:0.652, val_acc:0.573]
Epoch [4/120    avg_loss:0.476, val_acc:0.705]
Epoch [5/120    avg_loss:0.405, val_acc:0.733]
Epoch [6/120    avg_loss:0.347, val_acc:0.762]
Epoch [7/120    avg_loss:0.293, val_acc:0.808]
Epoch [8/120    avg_loss:0.264, val_acc:0.823]
Epoch [9/120    avg_loss:0.252, val_acc:0.786]
Epoch [10/120    avg_loss:0.216, val_acc:0.894]
Epoch [11/120    avg_loss:0.186, val_acc:0.897]
Epoch [12/120    avg_loss:0.142, val_acc:0.919]
Epoch [13/120    avg_loss:0.134, val_acc:0.917]
Epoch [14/120    avg_loss:0.123, val_acc:0.909]
Epoch [15/120    avg_loss:0.158, val_acc:0.922]
Epoch [16/120    avg_loss:0.122, val_acc:0.943]
Epoch [17/120    avg_loss:0.088, val_acc:0.934]
Epoch [18/120    avg_loss:0.069, val_acc:0.935]
Epoch [19/120    avg_loss:0.066, val_acc:0.958]
Epoch [20/120    avg_loss:0.059, val_acc:0.958]
Epoch [21/120    avg_loss:0.067, val_acc:0.955]
Epoch [22/120    avg_loss:0.056, val_acc:0.963]
Epoch [23/120    avg_loss:0.072, val_acc:0.924]
Epoch [24/120    avg_loss:0.101, val_acc:0.942]
Epoch [25/120    avg_loss:0.049, val_acc:0.951]
Epoch [26/120    avg_loss:0.050, val_acc:0.950]
Epoch [27/120    avg_loss:0.092, val_acc:0.955]
Epoch [28/120    avg_loss:0.051, val_acc:0.968]
Epoch [29/120    avg_loss:0.034, val_acc:0.970]
Epoch [30/120    avg_loss:0.034, val_acc:0.974]
Epoch [31/120    avg_loss:0.024, val_acc:0.970]
Epoch [32/120    avg_loss:0.026, val_acc:0.976]
Epoch [33/120    avg_loss:0.027, val_acc:0.965]
Epoch [34/120    avg_loss:0.075, val_acc:0.963]
Epoch [35/120    avg_loss:0.046, val_acc:0.978]
Epoch [36/120    avg_loss:0.044, val_acc:0.968]
Epoch [37/120    avg_loss:0.022, val_acc:0.969]
Epoch [38/120    avg_loss:0.030, val_acc:0.972]
Epoch [39/120    avg_loss:0.038, val_acc:0.972]
Epoch [40/120    avg_loss:0.030, val_acc:0.976]
Epoch [41/120    avg_loss:0.039, val_acc:0.977]
Epoch [42/120    avg_loss:0.023, val_acc:0.969]
Epoch [43/120    avg_loss:0.020, val_acc:0.978]
Epoch [44/120    avg_loss:0.026, val_acc:0.968]
Epoch [45/120    avg_loss:0.039, val_acc:0.961]
Epoch [46/120    avg_loss:0.033, val_acc:0.974]
Epoch [47/120    avg_loss:0.031, val_acc:0.955]
Epoch [48/120    avg_loss:0.027, val_acc:0.976]
Epoch [49/120    avg_loss:0.020, val_acc:0.979]
Epoch [50/120    avg_loss:0.018, val_acc:0.979]
Epoch [51/120    avg_loss:0.010, val_acc:0.986]
Epoch [52/120    avg_loss:0.010, val_acc:0.980]
Epoch [53/120    avg_loss:0.009, val_acc:0.986]
Epoch [54/120    avg_loss:0.014, val_acc:0.985]
Epoch [55/120    avg_loss:0.013, val_acc:0.978]
Epoch [56/120    avg_loss:0.013, val_acc:0.976]
Epoch [57/120    avg_loss:0.016, val_acc:0.983]
Epoch [58/120    avg_loss:0.010, val_acc:0.983]
Epoch [59/120    avg_loss:0.060, val_acc:0.974]
Epoch [60/120    avg_loss:0.033, val_acc:0.971]
Epoch [61/120    avg_loss:0.018, val_acc:0.984]
Epoch [62/120    avg_loss:0.013, val_acc:0.989]
Epoch [63/120    avg_loss:0.009, val_acc:0.985]
Epoch [64/120    avg_loss:0.013, val_acc:0.978]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.985]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.005, val_acc:0.985]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.014, val_acc:0.980]
Epoch [71/120    avg_loss:0.008, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.968]
Epoch [74/120    avg_loss:0.008, val_acc:0.983]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.005, val_acc:0.987]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.004, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.002, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.005, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.004, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.988]
Epoch [91/120    avg_loss:0.003, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.986]
Epoch [96/120    avg_loss:0.002, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.003, val_acc:0.987]
Epoch [103/120    avg_loss:0.003, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.003, val_acc:0.988]
Epoch [107/120    avg_loss:0.003, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6420     0     3     0     0     0     0     3     6]
 [    0     0 18031     0     6     0    47     0     6     0]
 [    0     2     0  1995     0     0     0     0    37     2]
 [    0    22     0     0  2939     0     1     0     7     3]
 [    0     0     0   152     0  1153     0     0     0     0]
 [    0     0    10     0     2     0  4866     0     0     0]
 [    0     3     0     0     0     0     0  1284     0     3]
 [    0     4     0    56    38     0     0     0  3473     0]
 [    0     2     0     0     0    19     0     0     0   898]]

Accuracy:
98.95404044055624

F1 scores:
[       nan 0.99650757 0.99809028 0.94059406 0.98673829 0.93096488
 0.99387255 0.997669   0.9787234  0.98088476]

Kappa:
0.9861461554252732
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e940cd748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.388, val_acc:0.715]
Epoch [2/120    avg_loss:0.744, val_acc:0.728]
Epoch [3/120    avg_loss:0.590, val_acc:0.688]
Epoch [4/120    avg_loss:0.431, val_acc:0.756]
Epoch [5/120    avg_loss:0.387, val_acc:0.804]
Epoch [6/120    avg_loss:0.322, val_acc:0.806]
Epoch [7/120    avg_loss:0.233, val_acc:0.840]
Epoch [8/120    avg_loss:0.199, val_acc:0.948]
Epoch [9/120    avg_loss:0.195, val_acc:0.929]
Epoch [10/120    avg_loss:0.178, val_acc:0.926]
Epoch [11/120    avg_loss:0.151, val_acc:0.919]
Epoch [12/120    avg_loss:0.203, val_acc:0.953]
Epoch [13/120    avg_loss:0.126, val_acc:0.946]
Epoch [14/120    avg_loss:0.117, val_acc:0.932]
Epoch [15/120    avg_loss:0.078, val_acc:0.963]
Epoch [16/120    avg_loss:0.075, val_acc:0.954]
Epoch [17/120    avg_loss:0.083, val_acc:0.949]
Epoch [18/120    avg_loss:0.060, val_acc:0.968]
Epoch [19/120    avg_loss:0.043, val_acc:0.971]
Epoch [20/120    avg_loss:0.055, val_acc:0.951]
Epoch [21/120    avg_loss:0.107, val_acc:0.963]
Epoch [22/120    avg_loss:0.061, val_acc:0.968]
Epoch [23/120    avg_loss:0.042, val_acc:0.968]
Epoch [24/120    avg_loss:0.077, val_acc:0.952]
Epoch [25/120    avg_loss:0.047, val_acc:0.977]
Epoch [26/120    avg_loss:0.045, val_acc:0.973]
Epoch [27/120    avg_loss:0.032, val_acc:0.975]
Epoch [28/120    avg_loss:0.030, val_acc:0.969]
Epoch [29/120    avg_loss:0.043, val_acc:0.969]
Epoch [30/120    avg_loss:0.040, val_acc:0.972]
Epoch [31/120    avg_loss:0.030, val_acc:0.974]
Epoch [32/120    avg_loss:0.040, val_acc:0.974]
Epoch [33/120    avg_loss:0.039, val_acc:0.975]
Epoch [34/120    avg_loss:0.024, val_acc:0.973]
Epoch [35/120    avg_loss:0.020, val_acc:0.974]
Epoch [36/120    avg_loss:0.085, val_acc:0.922]
Epoch [37/120    avg_loss:0.049, val_acc:0.970]
Epoch [38/120    avg_loss:0.019, val_acc:0.977]
Epoch [39/120    avg_loss:0.014, val_acc:0.982]
Epoch [40/120    avg_loss:0.019, val_acc:0.972]
Epoch [41/120    avg_loss:0.017, val_acc:0.976]
Epoch [42/120    avg_loss:0.016, val_acc:0.979]
Epoch [43/120    avg_loss:0.020, val_acc:0.978]
Epoch [44/120    avg_loss:0.036, val_acc:0.958]
Epoch [45/120    avg_loss:0.037, val_acc:0.971]
Epoch [46/120    avg_loss:0.018, val_acc:0.966]
Epoch [47/120    avg_loss:0.018, val_acc:0.980]
Epoch [48/120    avg_loss:0.031, val_acc:0.917]
Epoch [49/120    avg_loss:0.029, val_acc:0.973]
Epoch [50/120    avg_loss:0.014, val_acc:0.983]
Epoch [51/120    avg_loss:0.026, val_acc:0.976]
Epoch [52/120    avg_loss:0.014, val_acc:0.978]
Epoch [53/120    avg_loss:0.016, val_acc:0.965]
Epoch [54/120    avg_loss:0.014, val_acc:0.983]
Epoch [55/120    avg_loss:0.008, val_acc:0.983]
Epoch [56/120    avg_loss:0.007, val_acc:0.985]
Epoch [57/120    avg_loss:0.010, val_acc:0.983]
Epoch [58/120    avg_loss:0.019, val_acc:0.979]
Epoch [59/120    avg_loss:0.012, val_acc:0.982]
Epoch [60/120    avg_loss:0.013, val_acc:0.973]
Epoch [61/120    avg_loss:0.039, val_acc:0.982]
Epoch [62/120    avg_loss:0.012, val_acc:0.986]
Epoch [63/120    avg_loss:0.016, val_acc:0.976]
Epoch [64/120    avg_loss:0.021, val_acc:0.980]
Epoch [65/120    avg_loss:0.012, val_acc:0.981]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.978]
Epoch [68/120    avg_loss:0.012, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.985]
Epoch [70/120    avg_loss:0.009, val_acc:0.987]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.007, val_acc:0.986]
Epoch [73/120    avg_loss:0.016, val_acc:0.978]
Epoch [74/120    avg_loss:0.034, val_acc:0.958]
Epoch [75/120    avg_loss:0.100, val_acc:0.954]
Epoch [76/120    avg_loss:0.048, val_acc:0.949]
Epoch [77/120    avg_loss:0.026, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.981]
Epoch [79/120    avg_loss:0.012, val_acc:0.984]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.979]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.979]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.004, val_acc:0.986]
Epoch [86/120    avg_loss:0.010, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.008, val_acc:0.989]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.030, val_acc:0.946]
Epoch [94/120    avg_loss:0.037, val_acc:0.976]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.007, val_acc:0.981]
Epoch [97/120    avg_loss:0.012, val_acc:0.974]
Epoch [98/120    avg_loss:0.020, val_acc:0.983]
Epoch [99/120    avg_loss:0.015, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.011, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.982]
Epoch [105/120    avg_loss:0.003, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.003, val_acc:0.985]
Epoch [109/120    avg_loss:0.003, val_acc:0.985]
Epoch [110/120    avg_loss:0.003, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.017, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.002, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.002, val_acc:0.984]
Epoch [120/120    avg_loss:0.003, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     5     0     0     0     0     9     6]
 [    0     3 18031     0    14     0    38     0     4     0]
 [    0     9     0  1956     0     0     0     0    69     2]
 [    0    19     0     0  2945     0     1     0     4     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     0     0  4856     0     0     0]
 [    0     6     0     0     0     0     3  1280     0     1]
 [    0     3     0    64    44     0     0     0  3460     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
99.1829947220013

F1 scores:
[       nan 0.99534306 0.9977589  0.96330953 0.98577406 0.99618321
 0.99345336 0.99610895 0.9723198  0.98804348]

Kappa:
0.9891780235709614
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03a2e2d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.429, val_acc:0.393]
Epoch [2/120    avg_loss:0.780, val_acc:0.637]
Epoch [3/120    avg_loss:0.587, val_acc:0.791]
Epoch [4/120    avg_loss:0.455, val_acc:0.732]
Epoch [5/120    avg_loss:0.320, val_acc:0.777]
Epoch [6/120    avg_loss:0.303, val_acc:0.891]
Epoch [7/120    avg_loss:0.249, val_acc:0.901]
Epoch [8/120    avg_loss:0.250, val_acc:0.902]
Epoch [9/120    avg_loss:0.218, val_acc:0.883]
Epoch [10/120    avg_loss:0.180, val_acc:0.902]
Epoch [11/120    avg_loss:0.151, val_acc:0.924]
Epoch [12/120    avg_loss:0.126, val_acc:0.937]
Epoch [13/120    avg_loss:0.134, val_acc:0.922]
Epoch [14/120    avg_loss:0.102, val_acc:0.937]
Epoch [15/120    avg_loss:0.083, val_acc:0.951]
Epoch [16/120    avg_loss:0.094, val_acc:0.919]
Epoch [17/120    avg_loss:0.088, val_acc:0.947]
Epoch [18/120    avg_loss:0.082, val_acc:0.948]
Epoch [19/120    avg_loss:0.067, val_acc:0.968]
Epoch [20/120    avg_loss:0.068, val_acc:0.946]
Epoch [21/120    avg_loss:0.069, val_acc:0.972]
Epoch [22/120    avg_loss:0.035, val_acc:0.973]
Epoch [23/120    avg_loss:0.042, val_acc:0.969]
Epoch [24/120    avg_loss:0.087, val_acc:0.949]
Epoch [25/120    avg_loss:0.058, val_acc:0.960]
Epoch [26/120    avg_loss:0.055, val_acc:0.972]
Epoch [27/120    avg_loss:0.044, val_acc:0.961]
Epoch [28/120    avg_loss:0.041, val_acc:0.922]
Epoch [29/120    avg_loss:0.052, val_acc:0.952]
Epoch [30/120    avg_loss:0.049, val_acc:0.954]
Epoch [31/120    avg_loss:0.036, val_acc:0.966]
Epoch [32/120    avg_loss:0.033, val_acc:0.973]
Epoch [33/120    avg_loss:0.046, val_acc:0.966]
Epoch [34/120    avg_loss:0.030, val_acc:0.957]
Epoch [35/120    avg_loss:0.028, val_acc:0.970]
Epoch [36/120    avg_loss:0.023, val_acc:0.981]
Epoch [37/120    avg_loss:0.022, val_acc:0.977]
Epoch [38/120    avg_loss:0.022, val_acc:0.970]
Epoch [39/120    avg_loss:0.019, val_acc:0.978]
Epoch [40/120    avg_loss:0.013, val_acc:0.973]
Epoch [41/120    avg_loss:0.024, val_acc:0.972]
Epoch [42/120    avg_loss:0.017, val_acc:0.974]
Epoch [43/120    avg_loss:0.023, val_acc:0.966]
Epoch [44/120    avg_loss:0.042, val_acc:0.966]
Epoch [45/120    avg_loss:0.021, val_acc:0.963]
Epoch [46/120    avg_loss:0.014, val_acc:0.978]
Epoch [47/120    avg_loss:0.019, val_acc:0.978]
Epoch [48/120    avg_loss:0.022, val_acc:0.970]
Epoch [49/120    avg_loss:0.016, val_acc:0.978]
Epoch [50/120    avg_loss:0.017, val_acc:0.981]
Epoch [51/120    avg_loss:0.010, val_acc:0.981]
Epoch [52/120    avg_loss:0.008, val_acc:0.980]
Epoch [53/120    avg_loss:0.015, val_acc:0.980]
Epoch [54/120    avg_loss:0.007, val_acc:0.980]
Epoch [55/120    avg_loss:0.008, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.979]
Epoch [57/120    avg_loss:0.009, val_acc:0.981]
Epoch [58/120    avg_loss:0.008, val_acc:0.981]
Epoch [59/120    avg_loss:0.011, val_acc:0.979]
Epoch [60/120    avg_loss:0.010, val_acc:0.980]
Epoch [61/120    avg_loss:0.007, val_acc:0.982]
Epoch [62/120    avg_loss:0.006, val_acc:0.979]
Epoch [63/120    avg_loss:0.011, val_acc:0.981]
Epoch [64/120    avg_loss:0.007, val_acc:0.980]
Epoch [65/120    avg_loss:0.009, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.982]
Epoch [67/120    avg_loss:0.020, val_acc:0.979]
Epoch [68/120    avg_loss:0.010, val_acc:0.979]
Epoch [69/120    avg_loss:0.013, val_acc:0.982]
Epoch [70/120    avg_loss:0.008, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.980]
Epoch [72/120    avg_loss:0.008, val_acc:0.980]
Epoch [73/120    avg_loss:0.009, val_acc:0.979]
Epoch [74/120    avg_loss:0.009, val_acc:0.980]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.006, val_acc:0.983]
Epoch [77/120    avg_loss:0.005, val_acc:0.983]
Epoch [78/120    avg_loss:0.011, val_acc:0.982]
Epoch [79/120    avg_loss:0.009, val_acc:0.983]
Epoch [80/120    avg_loss:0.007, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.981]
Epoch [82/120    avg_loss:0.005, val_acc:0.983]
Epoch [83/120    avg_loss:0.009, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.983]
Epoch [85/120    avg_loss:0.006, val_acc:0.980]
Epoch [86/120    avg_loss:0.010, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.982]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.982]
Epoch [91/120    avg_loss:0.006, val_acc:0.982]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.007, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.010, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     0     0     0     0     0     3]
 [    0     0 18065     0     2     0    20     0     3     0]
 [    0     3     0  1992     0     0     0     0    40     1]
 [    0    18     3     0  2944     0     4     0     1     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     0     0  4856     0     0     0]
 [    0    21     0     0     0     0     2  1260     0     7]
 [    0    18     0    78    33     0     3     0  3439     0]
 [    0     0     0     0     0    16     0     0     0   903]]

Accuracy:
99.27698647964718

F1 scores:
[       nan 0.99512422 0.99861802 0.97028738 0.98941354 0.99390708
 0.9947762  0.98823529 0.97504962 0.98419619]

Kappa:
0.9904184839348764
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4ffec2f780>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.389, val_acc:0.665]
Epoch [2/120    avg_loss:0.774, val_acc:0.518]
Epoch [3/120    avg_loss:0.588, val_acc:0.648]
Epoch [4/120    avg_loss:0.476, val_acc:0.808]
Epoch [5/120    avg_loss:0.373, val_acc:0.729]
Epoch [6/120    avg_loss:0.366, val_acc:0.778]
Epoch [7/120    avg_loss:0.284, val_acc:0.798]
Epoch [8/120    avg_loss:0.303, val_acc:0.730]
Epoch [9/120    avg_loss:0.323, val_acc:0.847]
Epoch [10/120    avg_loss:0.200, val_acc:0.854]
Epoch [11/120    avg_loss:0.198, val_acc:0.922]
Epoch [12/120    avg_loss:0.205, val_acc:0.876]
Epoch [13/120    avg_loss:0.144, val_acc:0.948]
Epoch [14/120    avg_loss:0.099, val_acc:0.953]
Epoch [15/120    avg_loss:0.102, val_acc:0.950]
Epoch [16/120    avg_loss:0.085, val_acc:0.963]
Epoch [17/120    avg_loss:0.091, val_acc:0.956]
Epoch [18/120    avg_loss:0.083, val_acc:0.957]
Epoch [19/120    avg_loss:0.065, val_acc:0.968]
Epoch [20/120    avg_loss:0.123, val_acc:0.962]
Epoch [21/120    avg_loss:0.098, val_acc:0.906]
Epoch [22/120    avg_loss:0.064, val_acc:0.977]
Epoch [23/120    avg_loss:0.053, val_acc:0.962]
Epoch [24/120    avg_loss:0.038, val_acc:0.976]
Epoch [25/120    avg_loss:0.050, val_acc:0.970]
Epoch [26/120    avg_loss:0.056, val_acc:0.966]
Epoch [27/120    avg_loss:0.043, val_acc:0.964]
Epoch [28/120    avg_loss:0.023, val_acc:0.982]
Epoch [29/120    avg_loss:0.040, val_acc:0.978]
Epoch [30/120    avg_loss:0.039, val_acc:0.975]
Epoch [31/120    avg_loss:0.040, val_acc:0.978]
Epoch [32/120    avg_loss:0.037, val_acc:0.974]
Epoch [33/120    avg_loss:0.027, val_acc:0.980]
Epoch [34/120    avg_loss:0.026, val_acc:0.968]
Epoch [35/120    avg_loss:0.027, val_acc:0.984]
Epoch [36/120    avg_loss:0.021, val_acc:0.983]
Epoch [37/120    avg_loss:0.026, val_acc:0.974]
Epoch [38/120    avg_loss:0.056, val_acc:0.986]
Epoch [39/120    avg_loss:0.049, val_acc:0.964]
Epoch [40/120    avg_loss:0.046, val_acc:0.978]
Epoch [41/120    avg_loss:0.018, val_acc:0.983]
Epoch [42/120    avg_loss:0.018, val_acc:0.986]
Epoch [43/120    avg_loss:0.022, val_acc:0.987]
Epoch [44/120    avg_loss:0.040, val_acc:0.955]
Epoch [45/120    avg_loss:0.034, val_acc:0.983]
Epoch [46/120    avg_loss:0.019, val_acc:0.988]
Epoch [47/120    avg_loss:0.024, val_acc:0.973]
Epoch [48/120    avg_loss:0.019, val_acc:0.985]
Epoch [49/120    avg_loss:0.019, val_acc:0.975]
Epoch [50/120    avg_loss:0.021, val_acc:0.990]
Epoch [51/120    avg_loss:0.018, val_acc:0.976]
Epoch [52/120    avg_loss:0.012, val_acc:0.988]
Epoch [53/120    avg_loss:0.008, val_acc:0.989]
Epoch [54/120    avg_loss:0.010, val_acc:0.977]
Epoch [55/120    avg_loss:0.011, val_acc:0.990]
Epoch [56/120    avg_loss:0.010, val_acc:0.991]
Epoch [57/120    avg_loss:0.008, val_acc:0.990]
Epoch [58/120    avg_loss:0.006, val_acc:0.989]
Epoch [59/120    avg_loss:0.038, val_acc:0.977]
Epoch [60/120    avg_loss:0.013, val_acc:0.987]
Epoch [61/120    avg_loss:0.014, val_acc:0.982]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.007, val_acc:0.987]
Epoch [64/120    avg_loss:0.006, val_acc:0.989]
Epoch [65/120    avg_loss:0.006, val_acc:0.990]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.016, val_acc:0.984]
Epoch [68/120    avg_loss:0.007, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.975]
Epoch [70/120    avg_loss:0.007, val_acc:0.985]
Epoch [71/120    avg_loss:0.011, val_acc:0.988]
Epoch [72/120    avg_loss:0.007, val_acc:0.988]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.005, val_acc:0.988]
Epoch [75/120    avg_loss:0.004, val_acc:0.990]
Epoch [76/120    avg_loss:0.003, val_acc:0.991]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.005, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.990]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.003, val_acc:0.990]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.005, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.005, val_acc:0.993]
Epoch [91/120    avg_loss:0.003, val_acc:0.993]
Epoch [92/120    avg_loss:0.005, val_acc:0.993]
Epoch [93/120    avg_loss:0.005, val_acc:0.993]
Epoch [94/120    avg_loss:0.005, val_acc:0.993]
Epoch [95/120    avg_loss:0.005, val_acc:0.993]
Epoch [96/120    avg_loss:0.009, val_acc:0.993]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.005, val_acc:0.992]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.003, val_acc:0.992]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.003, val_acc:0.992]
Epoch [104/120    avg_loss:0.003, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.003, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.003, val_acc:0.993]
Epoch [109/120    avg_loss:0.003, val_acc:0.993]
Epoch [110/120    avg_loss:0.012, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.003, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     3     0     0     0     1    38     1]
 [    0     0 18067     0     5     0    11     0     7     0]
 [    0     0     0  1978     0     0     0     0    57     1]
 [    0    24     0     0  2943     0     1     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4874     0     0     0]
 [    0    17     0     0     0     0     2  1266     0     5]
 [    0     4     0    51    38     0     0     0  3478     0]
 [    0     0     0     0     0     7     0     0     0   912]]

Accuracy:
99.32277733593618

F1 scores:
[       nan 0.99316027 0.99925334 0.97246804 0.98791541 0.99732518
 0.99815687 0.99022292 0.97218728 0.99238303]

Kappa:
0.9910279285239354
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4085c1b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.414, val_acc:0.379]
Epoch [2/120    avg_loss:0.794, val_acc:0.723]
Epoch [3/120    avg_loss:0.532, val_acc:0.787]
Epoch [4/120    avg_loss:0.430, val_acc:0.779]
Epoch [5/120    avg_loss:0.393, val_acc:0.741]
Epoch [6/120    avg_loss:0.324, val_acc:0.755]
Epoch [7/120    avg_loss:0.262, val_acc:0.832]
Epoch [8/120    avg_loss:0.275, val_acc:0.811]
Epoch [9/120    avg_loss:0.260, val_acc:0.867]
Epoch [10/120    avg_loss:0.183, val_acc:0.868]
Epoch [11/120    avg_loss:0.158, val_acc:0.889]
Epoch [12/120    avg_loss:0.195, val_acc:0.912]
Epoch [13/120    avg_loss:0.125, val_acc:0.923]
Epoch [14/120    avg_loss:0.135, val_acc:0.948]
Epoch [15/120    avg_loss:0.107, val_acc:0.926]
Epoch [16/120    avg_loss:0.071, val_acc:0.947]
Epoch [17/120    avg_loss:0.077, val_acc:0.953]
Epoch [18/120    avg_loss:0.084, val_acc:0.921]
Epoch [19/120    avg_loss:0.072, val_acc:0.959]
Epoch [20/120    avg_loss:0.045, val_acc:0.958]
Epoch [21/120    avg_loss:0.073, val_acc:0.933]
Epoch [22/120    avg_loss:0.068, val_acc:0.959]
Epoch [23/120    avg_loss:0.077, val_acc:0.965]
Epoch [24/120    avg_loss:0.068, val_acc:0.961]
Epoch [25/120    avg_loss:0.060, val_acc:0.958]
Epoch [26/120    avg_loss:0.048, val_acc:0.962]
Epoch [27/120    avg_loss:0.046, val_acc:0.945]
Epoch [28/120    avg_loss:0.040, val_acc:0.973]
Epoch [29/120    avg_loss:0.044, val_acc:0.968]
Epoch [30/120    avg_loss:0.020, val_acc:0.981]
Epoch [31/120    avg_loss:0.024, val_acc:0.973]
Epoch [32/120    avg_loss:0.029, val_acc:0.968]
Epoch [33/120    avg_loss:0.017, val_acc:0.972]
Epoch [34/120    avg_loss:0.015, val_acc:0.986]
Epoch [35/120    avg_loss:0.017, val_acc:0.985]
Epoch [36/120    avg_loss:0.027, val_acc:0.973]
Epoch [37/120    avg_loss:0.045, val_acc:0.951]
Epoch [38/120    avg_loss:0.052, val_acc:0.969]
Epoch [39/120    avg_loss:0.037, val_acc:0.971]
Epoch [40/120    avg_loss:0.035, val_acc:0.983]
Epoch [41/120    avg_loss:0.025, val_acc:0.977]
Epoch [42/120    avg_loss:0.068, val_acc:0.938]
Epoch [43/120    avg_loss:0.058, val_acc:0.975]
Epoch [44/120    avg_loss:0.029, val_acc:0.974]
Epoch [45/120    avg_loss:0.019, val_acc:0.985]
Epoch [46/120    avg_loss:0.018, val_acc:0.975]
Epoch [47/120    avg_loss:0.017, val_acc:0.971]
Epoch [48/120    avg_loss:0.017, val_acc:0.975]
Epoch [49/120    avg_loss:0.011, val_acc:0.980]
Epoch [50/120    avg_loss:0.014, val_acc:0.983]
Epoch [51/120    avg_loss:0.008, val_acc:0.980]
Epoch [52/120    avg_loss:0.008, val_acc:0.984]
Epoch [53/120    avg_loss:0.008, val_acc:0.985]
Epoch [54/120    avg_loss:0.008, val_acc:0.985]
Epoch [55/120    avg_loss:0.010, val_acc:0.986]
Epoch [56/120    avg_loss:0.010, val_acc:0.986]
Epoch [57/120    avg_loss:0.011, val_acc:0.986]
Epoch [58/120    avg_loss:0.006, val_acc:0.986]
Epoch [59/120    avg_loss:0.009, val_acc:0.986]
Epoch [60/120    avg_loss:0.008, val_acc:0.987]
Epoch [61/120    avg_loss:0.009, val_acc:0.985]
Epoch [62/120    avg_loss:0.008, val_acc:0.985]
Epoch [63/120    avg_loss:0.010, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.010, val_acc:0.985]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.011, val_acc:0.985]
Epoch [70/120    avg_loss:0.006, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.007, val_acc:0.988]
Epoch [73/120    avg_loss:0.016, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.987]
Epoch [75/120    avg_loss:0.009, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.009, val_acc:0.987]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.011, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.010, val_acc:0.987]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.013, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.015, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     6     0     0     0     0     6    10]
 [    0     0 18032     0    19     0    36     0     3     0]
 [    0     0     0  1988     0     0     0     0    45     3]
 [    0    12     0     0  2946     0     2     0     2    10]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    17     0     0     0     0     4  1264     0     5]
 [    0    15     0    35    41     0     0     0  3480     0]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
99.3348275612754

F1 scores:
[       nan 0.99487816 0.99839433 0.97810578 0.98561392 0.99808795
 0.99571341 0.98981989 0.97931617 0.9822676 ]

Kappa:
0.9911911271522856
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f58a5d907b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 80528==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.386, val_acc:0.581]
Epoch [2/120    avg_loss:0.747, val_acc:0.674]
Epoch [3/120    avg_loss:0.499, val_acc:0.699]
Epoch [4/120    avg_loss:0.447, val_acc:0.722]
Epoch [5/120    avg_loss:0.369, val_acc:0.784]
Epoch [6/120    avg_loss:0.333, val_acc:0.787]
Epoch [7/120    avg_loss:0.284, val_acc:0.840]
Epoch [8/120    avg_loss:0.224, val_acc:0.905]
Epoch [9/120    avg_loss:0.178, val_acc:0.921]
Epoch [10/120    avg_loss:0.152, val_acc:0.812]
Epoch [11/120    avg_loss:0.180, val_acc:0.936]
Epoch [12/120    avg_loss:0.148, val_acc:0.938]
Epoch [13/120    avg_loss:0.123, val_acc:0.931]
Epoch [14/120    avg_loss:0.130, val_acc:0.927]
Epoch [15/120    avg_loss:0.126, val_acc:0.921]
Epoch [16/120    avg_loss:0.169, val_acc:0.952]
Epoch [17/120    avg_loss:0.088, val_acc:0.948]
Epoch [18/120    avg_loss:0.064, val_acc:0.964]
Epoch [19/120    avg_loss:0.057, val_acc:0.956]
Epoch [20/120    avg_loss:0.037, val_acc:0.958]
Epoch [21/120    avg_loss:0.052, val_acc:0.968]
Epoch [22/120    avg_loss:0.060, val_acc:0.941]
Epoch [23/120    avg_loss:0.069, val_acc:0.950]
Epoch [24/120    avg_loss:0.050, val_acc:0.947]
Epoch [25/120    avg_loss:0.044, val_acc:0.946]
Epoch [26/120    avg_loss:0.032, val_acc:0.972]
Epoch [27/120    avg_loss:0.051, val_acc:0.968]
Epoch [28/120    avg_loss:0.028, val_acc:0.972]
Epoch [29/120    avg_loss:0.043, val_acc:0.954]
Epoch [30/120    avg_loss:0.069, val_acc:0.961]
Epoch [31/120    avg_loss:0.037, val_acc:0.972]
Epoch [32/120    avg_loss:0.036, val_acc:0.970]
Epoch [33/120    avg_loss:0.040, val_acc:0.971]
Epoch [34/120    avg_loss:0.025, val_acc:0.983]
Epoch [35/120    avg_loss:0.030, val_acc:0.982]
Epoch [36/120    avg_loss:0.023, val_acc:0.972]
Epoch [37/120    avg_loss:0.016, val_acc:0.983]
Epoch [38/120    avg_loss:0.020, val_acc:0.974]
Epoch [39/120    avg_loss:0.015, val_acc:0.978]
Epoch [40/120    avg_loss:0.027, val_acc:0.980]
Epoch [41/120    avg_loss:0.022, val_acc:0.978]
Epoch [42/120    avg_loss:0.020, val_acc:0.984]
Epoch [43/120    avg_loss:0.021, val_acc:0.981]
Epoch [44/120    avg_loss:0.020, val_acc:0.978]
Epoch [45/120    avg_loss:0.016, val_acc:0.983]
Epoch [46/120    avg_loss:0.014, val_acc:0.986]
Epoch [47/120    avg_loss:0.024, val_acc:0.973]
Epoch [48/120    avg_loss:0.031, val_acc:0.972]
Epoch [49/120    avg_loss:0.017, val_acc:0.975]
Epoch [50/120    avg_loss:0.009, val_acc:0.985]
Epoch [51/120    avg_loss:0.013, val_acc:0.975]
Epoch [52/120    avg_loss:0.013, val_acc:0.980]
Epoch [53/120    avg_loss:0.026, val_acc:0.977]
Epoch [54/120    avg_loss:0.039, val_acc:0.972]
Epoch [55/120    avg_loss:0.030, val_acc:0.973]
Epoch [56/120    avg_loss:0.019, val_acc:0.976]
Epoch [57/120    avg_loss:0.014, val_acc:0.988]
Epoch [58/120    avg_loss:0.012, val_acc:0.978]
Epoch [59/120    avg_loss:0.014, val_acc:0.981]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.012, val_acc:0.980]
Epoch [62/120    avg_loss:0.009, val_acc:0.986]
Epoch [63/120    avg_loss:0.031, val_acc:0.985]
Epoch [64/120    avg_loss:0.023, val_acc:0.978]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.013, val_acc:0.978]
Epoch [67/120    avg_loss:0.013, val_acc:0.991]
Epoch [68/120    avg_loss:0.008, val_acc:0.985]
Epoch [69/120    avg_loss:0.005, val_acc:0.990]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.018, val_acc:0.979]
Epoch [72/120    avg_loss:0.016, val_acc:0.989]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.006, val_acc:0.990]
Epoch [75/120    avg_loss:0.004, val_acc:0.991]
Epoch [76/120    avg_loss:0.018, val_acc:0.973]
Epoch [77/120    avg_loss:0.012, val_acc:0.983]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.989]
Epoch [80/120    avg_loss:0.014, val_acc:0.988]
Epoch [81/120    avg_loss:0.013, val_acc:0.987]
Epoch [82/120    avg_loss:0.013, val_acc:0.974]
Epoch [83/120    avg_loss:0.010, val_acc:0.993]
Epoch [84/120    avg_loss:0.004, val_acc:0.990]
Epoch [85/120    avg_loss:0.010, val_acc:0.991]
Epoch [86/120    avg_loss:0.008, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.994]
Epoch [88/120    avg_loss:0.005, val_acc:0.993]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.989]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.991]
Epoch [93/120    avg_loss:0.002, val_acc:0.995]
Epoch [94/120    avg_loss:0.003, val_acc:0.992]
Epoch [95/120    avg_loss:0.009, val_acc:0.971]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.002, val_acc:0.993]
Epoch [99/120    avg_loss:0.002, val_acc:0.993]
Epoch [100/120    avg_loss:0.002, val_acc:0.992]
Epoch [101/120    avg_loss:0.002, val_acc:0.993]
Epoch [102/120    avg_loss:0.004, val_acc:0.993]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.002, val_acc:0.992]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.993]
Epoch [109/120    avg_loss:0.002, val_acc:0.993]
Epoch [110/120    avg_loss:0.001, val_acc:0.993]
Epoch [111/120    avg_loss:0.003, val_acc:0.994]
Epoch [112/120    avg_loss:0.003, val_acc:0.993]
Epoch [113/120    avg_loss:0.001, val_acc:0.993]
Epoch [114/120    avg_loss:0.002, val_acc:0.993]
Epoch [115/120    avg_loss:0.002, val_acc:0.993]
Epoch [116/120    avg_loss:0.002, val_acc:0.993]
Epoch [117/120    avg_loss:0.001, val_acc:0.993]
Epoch [118/120    avg_loss:0.002, val_acc:0.993]
Epoch [119/120    avg_loss:0.002, val_acc:0.994]
Epoch [120/120    avg_loss:0.002, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     1     0     0     0     1     3     0]
 [    0     0 18045     0    10     0    27     0     8     0]
 [    0     3     0  1973     0     0     0     0    57     3]
 [    0    27     0     1  2938     0     1     0     4     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     0     0     0  4859     0     2     0]
 [    0     8     0     0     0     0     1  1276     0     5]
 [    0     7     0    48    32     0     0     0  3484     0]
 [    0     0     0     0     1    12     0     0     0   906]]

Accuracy:
99.32518738100403

F1 scores:
[       nan 0.99612523 0.99828502 0.97216063 0.98706535 0.99542334
 0.99508499 0.9941566  0.97741619 0.98800436]

Kappa:
0.9910600548688359
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b96873748>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.501, val_acc:0.535]
Epoch [2/120    avg_loss:0.815, val_acc:0.497]
Epoch [3/120    avg_loss:0.546, val_acc:0.646]
Epoch [4/120    avg_loss:0.424, val_acc:0.746]
Epoch [5/120    avg_loss:0.349, val_acc:0.770]
Epoch [6/120    avg_loss:0.292, val_acc:0.873]
Epoch [7/120    avg_loss:0.250, val_acc:0.897]
Epoch [8/120    avg_loss:0.200, val_acc:0.878]
Epoch [9/120    avg_loss:0.184, val_acc:0.932]
Epoch [10/120    avg_loss:0.148, val_acc:0.953]
Epoch [11/120    avg_loss:0.148, val_acc:0.913]
Epoch [12/120    avg_loss:0.163, val_acc:0.910]
Epoch [13/120    avg_loss:0.120, val_acc:0.953]
Epoch [14/120    avg_loss:0.085, val_acc:0.959]
Epoch [15/120    avg_loss:0.072, val_acc:0.938]
Epoch [16/120    avg_loss:0.145, val_acc:0.948]
Epoch [17/120    avg_loss:0.103, val_acc:0.953]
Epoch [18/120    avg_loss:0.111, val_acc:0.956]
Epoch [19/120    avg_loss:0.096, val_acc:0.962]
Epoch [20/120    avg_loss:0.083, val_acc:0.963]
Epoch [21/120    avg_loss:0.084, val_acc:0.974]
Epoch [22/120    avg_loss:0.059, val_acc:0.975]
Epoch [23/120    avg_loss:0.032, val_acc:0.976]
Epoch [24/120    avg_loss:0.026, val_acc:0.976]
Epoch [25/120    avg_loss:0.039, val_acc:0.983]
Epoch [26/120    avg_loss:0.045, val_acc:0.962]
Epoch [27/120    avg_loss:0.055, val_acc:0.974]
Epoch [28/120    avg_loss:0.028, val_acc:0.980]
Epoch [29/120    avg_loss:0.024, val_acc:0.979]
Epoch [30/120    avg_loss:0.028, val_acc:0.945]
Epoch [31/120    avg_loss:0.017, val_acc:0.980]
Epoch [32/120    avg_loss:0.015, val_acc:0.984]
Epoch [33/120    avg_loss:0.019, val_acc:0.983]
Epoch [34/120    avg_loss:0.017, val_acc:0.981]
Epoch [35/120    avg_loss:0.035, val_acc:0.968]
Epoch [36/120    avg_loss:0.023, val_acc:0.978]
Epoch [37/120    avg_loss:0.027, val_acc:0.980]
Epoch [38/120    avg_loss:0.023, val_acc:0.978]
Epoch [39/120    avg_loss:0.013, val_acc:0.983]
Epoch [40/120    avg_loss:0.038, val_acc:0.978]
Epoch [41/120    avg_loss:0.028, val_acc:0.980]
Epoch [42/120    avg_loss:0.029, val_acc:0.971]
Epoch [43/120    avg_loss:0.021, val_acc:0.980]
Epoch [44/120    avg_loss:0.015, val_acc:0.985]
Epoch [45/120    avg_loss:0.023, val_acc:0.984]
Epoch [46/120    avg_loss:0.014, val_acc:0.985]
Epoch [47/120    avg_loss:0.011, val_acc:0.987]
Epoch [48/120    avg_loss:0.016, val_acc:0.988]
Epoch [49/120    avg_loss:0.025, val_acc:0.980]
Epoch [50/120    avg_loss:0.014, val_acc:0.983]
Epoch [51/120    avg_loss:0.018, val_acc:0.984]
Epoch [52/120    avg_loss:0.018, val_acc:0.981]
Epoch [53/120    avg_loss:0.020, val_acc:0.983]
Epoch [54/120    avg_loss:0.020, val_acc:0.991]
Epoch [55/120    avg_loss:0.008, val_acc:0.994]
Epoch [56/120    avg_loss:0.008, val_acc:0.990]
Epoch [57/120    avg_loss:0.011, val_acc:0.987]
Epoch [58/120    avg_loss:0.006, val_acc:0.989]
Epoch [59/120    avg_loss:0.006, val_acc:0.991]
Epoch [60/120    avg_loss:0.013, val_acc:0.978]
Epoch [61/120    avg_loss:0.010, val_acc:0.987]
Epoch [62/120    avg_loss:0.013, val_acc:0.977]
Epoch [63/120    avg_loss:0.017, val_acc:0.988]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.006, val_acc:0.991]
Epoch [66/120    avg_loss:0.005, val_acc:0.987]
Epoch [67/120    avg_loss:0.008, val_acc:0.989]
Epoch [68/120    avg_loss:0.005, val_acc:0.990]
Epoch [69/120    avg_loss:0.005, val_acc:0.991]
Epoch [70/120    avg_loss:0.005, val_acc:0.992]
Epoch [71/120    avg_loss:0.005, val_acc:0.991]
Epoch [72/120    avg_loss:0.005, val_acc:0.992]
Epoch [73/120    avg_loss:0.003, val_acc:0.992]
Epoch [74/120    avg_loss:0.007, val_acc:0.992]
Epoch [75/120    avg_loss:0.004, val_acc:0.992]
Epoch [76/120    avg_loss:0.007, val_acc:0.993]
Epoch [77/120    avg_loss:0.004, val_acc:0.993]
Epoch [78/120    avg_loss:0.003, val_acc:0.993]
Epoch [79/120    avg_loss:0.007, val_acc:0.992]
Epoch [80/120    avg_loss:0.004, val_acc:0.991]
Epoch [81/120    avg_loss:0.005, val_acc:0.992]
Epoch [82/120    avg_loss:0.003, val_acc:0.992]
Epoch [83/120    avg_loss:0.004, val_acc:0.992]
Epoch [84/120    avg_loss:0.003, val_acc:0.992]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.005, val_acc:0.993]
Epoch [87/120    avg_loss:0.005, val_acc:0.993]
Epoch [88/120    avg_loss:0.003, val_acc:0.993]
Epoch [89/120    avg_loss:0.003, val_acc:0.993]
Epoch [90/120    avg_loss:0.003, val_acc:0.993]
Epoch [91/120    avg_loss:0.003, val_acc:0.993]
Epoch [92/120    avg_loss:0.005, val_acc:0.993]
Epoch [93/120    avg_loss:0.004, val_acc:0.993]
Epoch [94/120    avg_loss:0.004, val_acc:0.993]
Epoch [95/120    avg_loss:0.006, val_acc:0.993]
Epoch [96/120    avg_loss:0.003, val_acc:0.993]
Epoch [97/120    avg_loss:0.005, val_acc:0.993]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.007, val_acc:0.993]
Epoch [100/120    avg_loss:0.003, val_acc:0.993]
Epoch [101/120    avg_loss:0.004, val_acc:0.993]
Epoch [102/120    avg_loss:0.004, val_acc:0.993]
Epoch [103/120    avg_loss:0.005, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.993]
Epoch [105/120    avg_loss:0.004, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.993]
Epoch [107/120    avg_loss:0.003, val_acc:0.993]
Epoch [108/120    avg_loss:0.003, val_acc:0.993]
Epoch [109/120    avg_loss:0.005, val_acc:0.993]
Epoch [110/120    avg_loss:0.004, val_acc:0.993]
Epoch [111/120    avg_loss:0.006, val_acc:0.993]
Epoch [112/120    avg_loss:0.003, val_acc:0.993]
Epoch [113/120    avg_loss:0.006, val_acc:0.993]
Epoch [114/120    avg_loss:0.005, val_acc:0.993]
Epoch [115/120    avg_loss:0.006, val_acc:0.993]
Epoch [116/120    avg_loss:0.005, val_acc:0.993]
Epoch [117/120    avg_loss:0.006, val_acc:0.993]
Epoch [118/120    avg_loss:0.004, val_acc:0.993]
Epoch [119/120    avg_loss:0.004, val_acc:0.993]
Epoch [120/120    avg_loss:0.005, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6420     0     2     0     0     0     7     2     1]
 [    0     3 18058     0    14     0    12     0     3     0]
 [    0     6     0  2008     2     0     0     0    17     3]
 [    0    16     0     1  2943     0     0     0     9     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4877     0     1     0]
 [    0     0     0     0     0     0     1  1286     0     3]
 [    0     0     0    10    50     0     0     0  3510     1]
 [    0     0     0     2     0    24     0     0     0   893]]

Accuracy:
99.53486130190635

F1 scores:
[       nan 0.99712666 0.99911475 0.98940626 0.98411637 0.99088838
 0.99856675 0.99574139 0.98692535 0.97970378]

Kappa:
0.9938386958737618
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe7753e17b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.458, val_acc:0.597]
Epoch [2/120    avg_loss:0.788, val_acc:0.636]
Epoch [3/120    avg_loss:0.520, val_acc:0.788]
Epoch [4/120    avg_loss:0.419, val_acc:0.764]
Epoch [5/120    avg_loss:0.347, val_acc:0.767]
Epoch [6/120    avg_loss:0.289, val_acc:0.863]
Epoch [7/120    avg_loss:0.211, val_acc:0.892]
Epoch [8/120    avg_loss:0.190, val_acc:0.903]
Epoch [9/120    avg_loss:0.179, val_acc:0.893]
Epoch [10/120    avg_loss:0.156, val_acc:0.937]
Epoch [11/120    avg_loss:0.110, val_acc:0.942]
Epoch [12/120    avg_loss:0.100, val_acc:0.932]
Epoch [13/120    avg_loss:0.089, val_acc:0.943]
Epoch [14/120    avg_loss:0.068, val_acc:0.947]
Epoch [15/120    avg_loss:0.071, val_acc:0.845]
Epoch [16/120    avg_loss:0.104, val_acc:0.926]
Epoch [17/120    avg_loss:0.058, val_acc:0.956]
Epoch [18/120    avg_loss:0.057, val_acc:0.970]
Epoch [19/120    avg_loss:0.037, val_acc:0.973]
Epoch [20/120    avg_loss:0.045, val_acc:0.945]
Epoch [21/120    avg_loss:0.042, val_acc:0.963]
Epoch [22/120    avg_loss:0.085, val_acc:0.948]
Epoch [23/120    avg_loss:0.074, val_acc:0.921]
Epoch [24/120    avg_loss:0.054, val_acc:0.956]
Epoch [25/120    avg_loss:0.062, val_acc:0.976]
Epoch [26/120    avg_loss:0.031, val_acc:0.963]
Epoch [27/120    avg_loss:0.038, val_acc:0.977]
Epoch [28/120    avg_loss:0.028, val_acc:0.973]
Epoch [29/120    avg_loss:0.032, val_acc:0.971]
Epoch [30/120    avg_loss:0.034, val_acc:0.975]
Epoch [31/120    avg_loss:0.029, val_acc:0.960]
Epoch [32/120    avg_loss:0.039, val_acc:0.962]
Epoch [33/120    avg_loss:0.025, val_acc:0.977]
Epoch [34/120    avg_loss:0.017, val_acc:0.983]
Epoch [35/120    avg_loss:0.015, val_acc:0.978]
Epoch [36/120    avg_loss:0.020, val_acc:0.982]
Epoch [37/120    avg_loss:0.014, val_acc:0.968]
Epoch [38/120    avg_loss:0.020, val_acc:0.979]
Epoch [39/120    avg_loss:0.015, val_acc:0.978]
Epoch [40/120    avg_loss:0.015, val_acc:0.989]
Epoch [41/120    avg_loss:0.013, val_acc:0.987]
Epoch [42/120    avg_loss:0.018, val_acc:0.984]
Epoch [43/120    avg_loss:0.011, val_acc:0.986]
Epoch [44/120    avg_loss:0.028, val_acc:0.976]
Epoch [45/120    avg_loss:0.016, val_acc:0.972]
Epoch [46/120    avg_loss:0.061, val_acc:0.973]
Epoch [47/120    avg_loss:0.033, val_acc:0.958]
Epoch [48/120    avg_loss:0.022, val_acc:0.978]
Epoch [49/120    avg_loss:0.019, val_acc:0.982]
Epoch [50/120    avg_loss:0.009, val_acc:0.988]
Epoch [51/120    avg_loss:0.024, val_acc:0.981]
Epoch [52/120    avg_loss:0.021, val_acc:0.978]
Epoch [53/120    avg_loss:0.023, val_acc:0.976]
Epoch [54/120    avg_loss:0.011, val_acc:0.979]
Epoch [55/120    avg_loss:0.012, val_acc:0.983]
Epoch [56/120    avg_loss:0.015, val_acc:0.986]
Epoch [57/120    avg_loss:0.018, val_acc:0.984]
Epoch [58/120    avg_loss:0.009, val_acc:0.986]
Epoch [59/120    avg_loss:0.005, val_acc:0.986]
Epoch [60/120    avg_loss:0.011, val_acc:0.988]
Epoch [61/120    avg_loss:0.007, val_acc:0.988]
Epoch [62/120    avg_loss:0.005, val_acc:0.988]
Epoch [63/120    avg_loss:0.013, val_acc:0.987]
Epoch [64/120    avg_loss:0.006, val_acc:0.988]
Epoch [65/120    avg_loss:0.010, val_acc:0.987]
Epoch [66/120    avg_loss:0.008, val_acc:0.988]
Epoch [67/120    avg_loss:0.009, val_acc:0.988]
Epoch [68/120    avg_loss:0.006, val_acc:0.988]
Epoch [69/120    avg_loss:0.005, val_acc:0.988]
Epoch [70/120    avg_loss:0.003, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.013, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.988]
Epoch [74/120    avg_loss:0.006, val_acc:0.988]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.009, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     1     0     0     0     2    27     0]
 [    0     0 18061     0    14     0     9     0     6     0]
 [    0     2     0  2005     0     0     0     0    25     4]
 [    0    15     0     3  2944     0     0     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0    13     0     0     0     0     1  1272     0     4]
 [    0     2     0    26    52     0     0     0  3488     3]
 [    0     0     0     0     3    18     0     0     0   898]]

Accuracy:
99.40953895837852

F1 scores:
[       nan 0.9951811  0.99919781 0.98501597 0.98379282 0.99315068
 0.99846327 0.99219969 0.97812675 0.98249453]

Kappa:
0.9921785001511984
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94e28dc780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.455, val_acc:0.341]
Epoch [2/120    avg_loss:0.851, val_acc:0.750]
Epoch [3/120    avg_loss:0.658, val_acc:0.596]
Epoch [4/120    avg_loss:0.478, val_acc:0.797]
Epoch [5/120    avg_loss:0.365, val_acc:0.851]
Epoch [6/120    avg_loss:0.280, val_acc:0.895]
Epoch [7/120    avg_loss:0.276, val_acc:0.838]
Epoch [8/120    avg_loss:0.253, val_acc:0.834]
Epoch [9/120    avg_loss:0.194, val_acc:0.934]
Epoch [10/120    avg_loss:0.136, val_acc:0.927]
Epoch [11/120    avg_loss:0.147, val_acc:0.926]
Epoch [12/120    avg_loss:0.156, val_acc:0.925]
Epoch [13/120    avg_loss:0.109, val_acc:0.944]
Epoch [14/120    avg_loss:0.150, val_acc:0.948]
Epoch [15/120    avg_loss:0.114, val_acc:0.940]
Epoch [16/120    avg_loss:0.087, val_acc:0.962]
Epoch [17/120    avg_loss:0.082, val_acc:0.963]
Epoch [18/120    avg_loss:0.080, val_acc:0.952]
Epoch [19/120    avg_loss:0.085, val_acc:0.943]
Epoch [20/120    avg_loss:0.092, val_acc:0.952]
Epoch [21/120    avg_loss:0.046, val_acc:0.961]
Epoch [22/120    avg_loss:0.059, val_acc:0.951]
Epoch [23/120    avg_loss:0.047, val_acc:0.965]
Epoch [24/120    avg_loss:0.050, val_acc:0.961]
Epoch [25/120    avg_loss:0.063, val_acc:0.963]
Epoch [26/120    avg_loss:0.044, val_acc:0.968]
Epoch [27/120    avg_loss:0.048, val_acc:0.962]
Epoch [28/120    avg_loss:0.050, val_acc:0.959]
Epoch [29/120    avg_loss:0.033, val_acc:0.964]
Epoch [30/120    avg_loss:0.026, val_acc:0.977]
Epoch [31/120    avg_loss:0.021, val_acc:0.978]
Epoch [32/120    avg_loss:0.036, val_acc:0.973]
Epoch [33/120    avg_loss:0.037, val_acc:0.973]
Epoch [34/120    avg_loss:0.028, val_acc:0.978]
Epoch [35/120    avg_loss:0.065, val_acc:0.950]
Epoch [36/120    avg_loss:0.031, val_acc:0.974]
Epoch [37/120    avg_loss:0.018, val_acc:0.980]
Epoch [38/120    avg_loss:0.016, val_acc:0.973]
Epoch [39/120    avg_loss:0.021, val_acc:0.965]
Epoch [40/120    avg_loss:0.037, val_acc:0.958]
Epoch [41/120    avg_loss:0.033, val_acc:0.976]
Epoch [42/120    avg_loss:0.034, val_acc:0.978]
Epoch [43/120    avg_loss:0.020, val_acc:0.970]
Epoch [44/120    avg_loss:0.019, val_acc:0.971]
Epoch [45/120    avg_loss:0.012, val_acc:0.978]
Epoch [46/120    avg_loss:0.034, val_acc:0.976]
Epoch [47/120    avg_loss:0.030, val_acc:0.969]
Epoch [48/120    avg_loss:0.027, val_acc:0.969]
Epoch [49/120    avg_loss:0.031, val_acc:0.977]
Epoch [50/120    avg_loss:0.014, val_acc:0.979]
Epoch [51/120    avg_loss:0.019, val_acc:0.983]
Epoch [52/120    avg_loss:0.016, val_acc:0.986]
Epoch [53/120    avg_loss:0.008, val_acc:0.987]
Epoch [54/120    avg_loss:0.011, val_acc:0.985]
Epoch [55/120    avg_loss:0.012, val_acc:0.984]
Epoch [56/120    avg_loss:0.014, val_acc:0.985]
Epoch [57/120    avg_loss:0.007, val_acc:0.984]
Epoch [58/120    avg_loss:0.009, val_acc:0.985]
Epoch [59/120    avg_loss:0.007, val_acc:0.983]
Epoch [60/120    avg_loss:0.008, val_acc:0.985]
Epoch [61/120    avg_loss:0.007, val_acc:0.987]
Epoch [62/120    avg_loss:0.008, val_acc:0.986]
Epoch [63/120    avg_loss:0.010, val_acc:0.984]
Epoch [64/120    avg_loss:0.010, val_acc:0.985]
Epoch [65/120    avg_loss:0.006, val_acc:0.985]
Epoch [66/120    avg_loss:0.007, val_acc:0.986]
Epoch [67/120    avg_loss:0.007, val_acc:0.986]
Epoch [68/120    avg_loss:0.006, val_acc:0.987]
Epoch [69/120    avg_loss:0.006, val_acc:0.987]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.006, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.005, val_acc:0.985]
Epoch [77/120    avg_loss:0.005, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.987]
Epoch [120/120    avg_loss:0.009, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     0     0     0     0     1    14     1]
 [    0     2 18077     0     7     0     0     0     4     0]
 [    0     3     0  2010     1     0     0     0    19     3]
 [    0    24     0     2  2930     0     3     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4875     0     0     0]
 [    0     7     0     0     0     0     2  1277     0     4]
 [    0     6     0    21    52     0     0     0  3491     1]
 [    0     0     0     1     0    23     0     0     0   895]]

Accuracy:
99.47702022027812

F1 scores:
[       nan 0.99550039 0.99955764 0.98771499 0.98289165 0.99126472
 0.99918016 0.99454829 0.98172103 0.98189797]

Kappa:
0.9930705266371311
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f32de37b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.489, val_acc:0.683]
Epoch [2/120    avg_loss:0.848, val_acc:0.495]
Epoch [3/120    avg_loss:0.598, val_acc:0.573]
Epoch [4/120    avg_loss:0.452, val_acc:0.747]
Epoch [5/120    avg_loss:0.381, val_acc:0.621]
Epoch [6/120    avg_loss:0.320, val_acc:0.755]
Epoch [7/120    avg_loss:0.274, val_acc:0.801]
Epoch [8/120    avg_loss:0.255, val_acc:0.797]
Epoch [9/120    avg_loss:0.238, val_acc:0.787]
Epoch [10/120    avg_loss:0.190, val_acc:0.866]
Epoch [11/120    avg_loss:0.146, val_acc:0.922]
Epoch [12/120    avg_loss:0.114, val_acc:0.887]
Epoch [13/120    avg_loss:0.132, val_acc:0.924]
Epoch [14/120    avg_loss:0.131, val_acc:0.952]
Epoch [15/120    avg_loss:0.117, val_acc:0.953]
Epoch [16/120    avg_loss:0.176, val_acc:0.934]
Epoch [17/120    avg_loss:0.131, val_acc:0.940]
Epoch [18/120    avg_loss:0.115, val_acc:0.944]
Epoch [19/120    avg_loss:0.098, val_acc:0.961]
Epoch [20/120    avg_loss:0.079, val_acc:0.947]
Epoch [21/120    avg_loss:0.056, val_acc:0.943]
Epoch [22/120    avg_loss:0.065, val_acc:0.941]
Epoch [23/120    avg_loss:0.050, val_acc:0.957]
Epoch [24/120    avg_loss:0.037, val_acc:0.976]
Epoch [25/120    avg_loss:0.038, val_acc:0.956]
Epoch [26/120    avg_loss:0.042, val_acc:0.969]
Epoch [27/120    avg_loss:0.040, val_acc:0.968]
Epoch [28/120    avg_loss:0.048, val_acc:0.973]
Epoch [29/120    avg_loss:0.029, val_acc:0.976]
Epoch [30/120    avg_loss:0.025, val_acc:0.978]
Epoch [31/120    avg_loss:0.020, val_acc:0.973]
Epoch [32/120    avg_loss:0.020, val_acc:0.970]
Epoch [33/120    avg_loss:0.039, val_acc:0.978]
Epoch [34/120    avg_loss:0.031, val_acc:0.982]
Epoch [35/120    avg_loss:0.017, val_acc:0.986]
Epoch [36/120    avg_loss:0.024, val_acc:0.978]
Epoch [37/120    avg_loss:0.026, val_acc:0.960]
Epoch [38/120    avg_loss:0.022, val_acc:0.976]
Epoch [39/120    avg_loss:0.030, val_acc:0.968]
Epoch [40/120    avg_loss:0.028, val_acc:0.983]
Epoch [41/120    avg_loss:0.023, val_acc:0.983]
Epoch [42/120    avg_loss:0.030, val_acc:0.973]
Epoch [43/120    avg_loss:0.047, val_acc:0.966]
Epoch [44/120    avg_loss:0.036, val_acc:0.973]
Epoch [45/120    avg_loss:0.037, val_acc:0.978]
Epoch [46/120    avg_loss:0.038, val_acc:0.978]
Epoch [47/120    avg_loss:0.023, val_acc:0.979]
Epoch [48/120    avg_loss:0.036, val_acc:0.985]
Epoch [49/120    avg_loss:0.017, val_acc:0.986]
Epoch [50/120    avg_loss:0.014, val_acc:0.983]
Epoch [51/120    avg_loss:0.013, val_acc:0.983]
Epoch [52/120    avg_loss:0.011, val_acc:0.985]
Epoch [53/120    avg_loss:0.014, val_acc:0.985]
Epoch [54/120    avg_loss:0.009, val_acc:0.985]
Epoch [55/120    avg_loss:0.008, val_acc:0.985]
Epoch [56/120    avg_loss:0.011, val_acc:0.984]
Epoch [57/120    avg_loss:0.018, val_acc:0.982]
Epoch [58/120    avg_loss:0.013, val_acc:0.983]
Epoch [59/120    avg_loss:0.010, val_acc:0.984]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.010, val_acc:0.983]
Epoch [64/120    avg_loss:0.009, val_acc:0.983]
Epoch [65/120    avg_loss:0.008, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.983]
Epoch [67/120    avg_loss:0.009, val_acc:0.983]
Epoch [68/120    avg_loss:0.008, val_acc:0.983]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.009, val_acc:0.983]
Epoch [71/120    avg_loss:0.007, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.007, val_acc:0.983]
Epoch [74/120    avg_loss:0.009, val_acc:0.983]
Epoch [75/120    avg_loss:0.009, val_acc:0.983]
Epoch [76/120    avg_loss:0.014, val_acc:0.983]
Epoch [77/120    avg_loss:0.009, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.005, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.008, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.012, val_acc:0.983]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.009, val_acc:0.983]
Epoch [93/120    avg_loss:0.012, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.983]
Epoch [99/120    avg_loss:0.011, val_acc:0.983]
Epoch [100/120    avg_loss:0.018, val_acc:0.983]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.009, val_acc:0.983]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.012, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     4     0     0    12     1    42     3]
 [    0     3 18070     0     4     0     9     0     4     0]
 [    0     2     0  1981     0     0     0     0    49     4]
 [    0    22     4     3  2931     0     3     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4875     0     0     0]
 [    0     7     0     0     0     0     6  1266     0    11]
 [    0     3     0    63    39     0     0     0  3464     2]
 [    0     0     0     0     0    25     0     0     0   894]]

Accuracy:
99.18781481213699

F1 scores:
[       nan 0.99228912 0.99925346 0.96941522 0.98587286 0.99051233
 0.9966268  0.99022292 0.97057999 0.97491821]

Kappa:
0.9892400662290908
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f62c2eb77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.601, val_acc:0.554]
Epoch [2/120    avg_loss:0.798, val_acc:0.670]
Epoch [3/120    avg_loss:0.598, val_acc:0.760]
Epoch [4/120    avg_loss:0.480, val_acc:0.741]
Epoch [5/120    avg_loss:0.403, val_acc:0.752]
Epoch [6/120    avg_loss:0.336, val_acc:0.863]
Epoch [7/120    avg_loss:0.245, val_acc:0.889]
Epoch [8/120    avg_loss:0.180, val_acc:0.907]
Epoch [9/120    avg_loss:0.177, val_acc:0.942]
Epoch [10/120    avg_loss:0.182, val_acc:0.914]
Epoch [11/120    avg_loss:0.118, val_acc:0.960]
Epoch [12/120    avg_loss:0.109, val_acc:0.932]
Epoch [13/120    avg_loss:0.153, val_acc:0.915]
Epoch [14/120    avg_loss:0.183, val_acc:0.950]
Epoch [15/120    avg_loss:0.111, val_acc:0.945]
Epoch [16/120    avg_loss:0.119, val_acc:0.944]
Epoch [17/120    avg_loss:0.112, val_acc:0.952]
Epoch [18/120    avg_loss:0.114, val_acc:0.947]
Epoch [19/120    avg_loss:0.079, val_acc:0.949]
Epoch [20/120    avg_loss:0.062, val_acc:0.966]
Epoch [21/120    avg_loss:0.048, val_acc:0.932]
Epoch [22/120    avg_loss:0.048, val_acc:0.966]
Epoch [23/120    avg_loss:0.035, val_acc:0.968]
Epoch [24/120    avg_loss:0.033, val_acc:0.975]
Epoch [25/120    avg_loss:0.040, val_acc:0.977]
Epoch [26/120    avg_loss:0.041, val_acc:0.976]
Epoch [27/120    avg_loss:0.062, val_acc:0.963]
Epoch [28/120    avg_loss:0.060, val_acc:0.972]
Epoch [29/120    avg_loss:0.040, val_acc:0.976]
Epoch [30/120    avg_loss:0.036, val_acc:0.955]
Epoch [31/120    avg_loss:0.036, val_acc:0.974]
Epoch [32/120    avg_loss:0.032, val_acc:0.970]
Epoch [33/120    avg_loss:0.027, val_acc:0.972]
Epoch [34/120    avg_loss:0.020, val_acc:0.983]
Epoch [35/120    avg_loss:0.021, val_acc:0.979]
Epoch [36/120    avg_loss:0.026, val_acc:0.965]
Epoch [37/120    avg_loss:0.021, val_acc:0.981]
Epoch [38/120    avg_loss:0.024, val_acc:0.978]
Epoch [39/120    avg_loss:0.028, val_acc:0.958]
Epoch [40/120    avg_loss:0.044, val_acc:0.983]
Epoch [41/120    avg_loss:0.030, val_acc:0.975]
Epoch [42/120    avg_loss:0.014, val_acc:0.967]
Epoch [43/120    avg_loss:0.028, val_acc:0.972]
Epoch [44/120    avg_loss:0.023, val_acc:0.972]
Epoch [45/120    avg_loss:0.025, val_acc:0.983]
Epoch [46/120    avg_loss:0.016, val_acc:0.975]
Epoch [47/120    avg_loss:0.021, val_acc:0.982]
Epoch [48/120    avg_loss:0.007, val_acc:0.983]
Epoch [49/120    avg_loss:0.010, val_acc:0.983]
Epoch [50/120    avg_loss:0.010, val_acc:0.983]
Epoch [51/120    avg_loss:0.007, val_acc:0.984]
Epoch [52/120    avg_loss:0.011, val_acc:0.985]
Epoch [53/120    avg_loss:0.006, val_acc:0.984]
Epoch [54/120    avg_loss:0.008, val_acc:0.985]
Epoch [55/120    avg_loss:0.007, val_acc:0.986]
Epoch [56/120    avg_loss:0.007, val_acc:0.985]
Epoch [57/120    avg_loss:0.006, val_acc:0.985]
Epoch [58/120    avg_loss:0.008, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.986]
Epoch [60/120    avg_loss:0.006, val_acc:0.985]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.005, val_acc:0.986]
Epoch [63/120    avg_loss:0.005, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.007, val_acc:0.987]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.008, val_acc:0.988]
Epoch [68/120    avg_loss:0.008, val_acc:0.988]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.988]
Epoch [71/120    avg_loss:0.007, val_acc:0.987]
Epoch [72/120    avg_loss:0.009, val_acc:0.987]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.012, val_acc:0.983]
Epoch [76/120    avg_loss:0.005, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.005, val_acc:0.987]
Epoch [87/120    avg_loss:0.013, val_acc:0.988]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.014, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.017, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.012, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     0     0     0     0     0     3]
 [    0     1 18043     0    14     0    29     0     3     0]
 [    0     1     0  2010     1     0     0     0    23     1]
 [    0    27     0     3  2923     0     5     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0     8     0     0     0     0     0  1278     0     4]
 [    0     9     0    18    46     0     0     0  3497     1]
 [    0     0     0     0     0    18     0     0     0   901]]

Accuracy:
99.44327958932833

F1 scores:
[       nan 0.99620361 0.99864397 0.98844357 0.98153123 0.99315068
 0.99632203 0.9953271  0.98410018 0.98469945]

Kappa:
0.9926258437342742
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15eb1777b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.405, val_acc:0.674]
Epoch [2/120    avg_loss:0.752, val_acc:0.541]
Epoch [3/120    avg_loss:0.536, val_acc:0.672]
Epoch [4/120    avg_loss:0.400, val_acc:0.652]
Epoch [5/120    avg_loss:0.375, val_acc:0.737]
Epoch [6/120    avg_loss:0.268, val_acc:0.826]
Epoch [7/120    avg_loss:0.233, val_acc:0.854]
Epoch [8/120    avg_loss:0.167, val_acc:0.927]
Epoch [9/120    avg_loss:0.152, val_acc:0.917]
Epoch [10/120    avg_loss:0.143, val_acc:0.932]
Epoch [11/120    avg_loss:0.109, val_acc:0.936]
Epoch [12/120    avg_loss:0.100, val_acc:0.949]
Epoch [13/120    avg_loss:0.088, val_acc:0.932]
Epoch [14/120    avg_loss:0.087, val_acc:0.944]
Epoch [15/120    avg_loss:0.095, val_acc:0.951]
Epoch [16/120    avg_loss:0.112, val_acc:0.952]
Epoch [17/120    avg_loss:0.104, val_acc:0.961]
Epoch [18/120    avg_loss:0.051, val_acc:0.965]
Epoch [19/120    avg_loss:0.046, val_acc:0.940]
Epoch [20/120    avg_loss:0.044, val_acc:0.970]
Epoch [21/120    avg_loss:0.035, val_acc:0.973]
Epoch [22/120    avg_loss:0.052, val_acc:0.958]
Epoch [23/120    avg_loss:0.071, val_acc:0.969]
Epoch [24/120    avg_loss:0.052, val_acc:0.969]
Epoch [25/120    avg_loss:0.044, val_acc:0.963]
Epoch [26/120    avg_loss:0.029, val_acc:0.958]
Epoch [27/120    avg_loss:0.026, val_acc:0.973]
Epoch [28/120    avg_loss:0.023, val_acc:0.978]
Epoch [29/120    avg_loss:0.024, val_acc:0.959]
Epoch [30/120    avg_loss:0.024, val_acc:0.981]
Epoch [31/120    avg_loss:0.019, val_acc:0.973]
Epoch [32/120    avg_loss:0.027, val_acc:0.969]
Epoch [33/120    avg_loss:0.033, val_acc:0.969]
Epoch [34/120    avg_loss:0.028, val_acc:0.973]
Epoch [35/120    avg_loss:0.022, val_acc:0.978]
Epoch [36/120    avg_loss:0.038, val_acc:0.978]
Epoch [37/120    avg_loss:0.017, val_acc:0.974]
Epoch [38/120    avg_loss:0.035, val_acc:0.971]
Epoch [39/120    avg_loss:0.017, val_acc:0.977]
Epoch [40/120    avg_loss:0.013, val_acc:0.982]
Epoch [41/120    avg_loss:0.012, val_acc:0.977]
Epoch [42/120    avg_loss:0.033, val_acc:0.968]
Epoch [43/120    avg_loss:0.018, val_acc:0.970]
Epoch [44/120    avg_loss:0.105, val_acc:0.962]
Epoch [45/120    avg_loss:0.105, val_acc:0.966]
Epoch [46/120    avg_loss:0.063, val_acc:0.948]
Epoch [47/120    avg_loss:0.064, val_acc:0.973]
Epoch [48/120    avg_loss:0.075, val_acc:0.955]
Epoch [49/120    avg_loss:0.045, val_acc:0.973]
Epoch [50/120    avg_loss:0.029, val_acc:0.971]
Epoch [51/120    avg_loss:0.018, val_acc:0.984]
Epoch [52/120    avg_loss:0.016, val_acc:0.978]
Epoch [53/120    avg_loss:0.017, val_acc:0.983]
Epoch [54/120    avg_loss:0.024, val_acc:0.951]
Epoch [55/120    avg_loss:0.019, val_acc:0.983]
Epoch [56/120    avg_loss:0.023, val_acc:0.973]
Epoch [57/120    avg_loss:0.018, val_acc:0.984]
Epoch [58/120    avg_loss:0.010, val_acc:0.983]
Epoch [59/120    avg_loss:0.010, val_acc:0.982]
Epoch [60/120    avg_loss:0.011, val_acc:0.978]
Epoch [61/120    avg_loss:0.011, val_acc:0.988]
Epoch [62/120    avg_loss:0.016, val_acc:0.981]
Epoch [63/120    avg_loss:0.011, val_acc:0.973]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.007, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.983]
Epoch [67/120    avg_loss:0.019, val_acc:0.981]
Epoch [68/120    avg_loss:0.007, val_acc:0.988]
Epoch [69/120    avg_loss:0.012, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.988]
Epoch [71/120    avg_loss:0.007, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.006, val_acc:0.981]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.004, val_acc:0.987]
Epoch [78/120    avg_loss:0.004, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.984]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.987]
Epoch [82/120    avg_loss:0.004, val_acc:0.988]
Epoch [83/120    avg_loss:0.003, val_acc:0.988]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.004, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.991]
Epoch [87/120    avg_loss:0.004, val_acc:0.991]
Epoch [88/120    avg_loss:0.004, val_acc:0.990]
Epoch [89/120    avg_loss:0.003, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.003, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.003, val_acc:0.990]
Epoch [97/120    avg_loss:0.003, val_acc:0.988]
Epoch [98/120    avg_loss:0.002, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.003, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.003, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.003, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     0     0     2     0    30     2]
 [    0     2 18055     0    12     0    20     0     1     0]
 [    0     1     0  1980     0     0     0     0    51     4]
 [    0    23     0     2  2932     0     2     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4875     0     0     0]
 [    0     8     0     0     0     0     2  1276     0     4]
 [    0     2     0    23    41     0     0     0  3494    11]
 [    0     0     0     0     9    20     0     0     0   890]]

Accuracy:
99.30590702046129

F1 scores:
[       nan 0.9945593  0.99894877 0.97995546 0.98290312 0.99239544
 0.99703446 0.99454404 0.97597765 0.9726776 ]

Kappa:
0.990805584397292
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f226b446780>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.444, val_acc:0.388]
Epoch [2/120    avg_loss:0.831, val_acc:0.751]
Epoch [3/120    avg_loss:0.622, val_acc:0.742]
Epoch [4/120    avg_loss:0.501, val_acc:0.793]
Epoch [5/120    avg_loss:0.401, val_acc:0.817]
Epoch [6/120    avg_loss:0.332, val_acc:0.818]
Epoch [7/120    avg_loss:0.269, val_acc:0.805]
Epoch [8/120    avg_loss:0.263, val_acc:0.806]
Epoch [9/120    avg_loss:0.222, val_acc:0.912]
Epoch [10/120    avg_loss:0.143, val_acc:0.911]
Epoch [11/120    avg_loss:0.150, val_acc:0.932]
Epoch [12/120    avg_loss:0.159, val_acc:0.927]
Epoch [13/120    avg_loss:0.113, val_acc:0.946]
Epoch [14/120    avg_loss:0.104, val_acc:0.940]
Epoch [15/120    avg_loss:0.094, val_acc:0.966]
Epoch [16/120    avg_loss:0.085, val_acc:0.967]
Epoch [17/120    avg_loss:0.076, val_acc:0.947]
Epoch [18/120    avg_loss:0.078, val_acc:0.965]
Epoch [19/120    avg_loss:0.080, val_acc:0.964]
Epoch [20/120    avg_loss:0.068, val_acc:0.963]
Epoch [21/120    avg_loss:0.064, val_acc:0.957]
Epoch [22/120    avg_loss:0.094, val_acc:0.953]
Epoch [23/120    avg_loss:0.061, val_acc:0.964]
Epoch [24/120    avg_loss:0.068, val_acc:0.971]
Epoch [25/120    avg_loss:0.051, val_acc:0.951]
Epoch [26/120    avg_loss:0.056, val_acc:0.957]
Epoch [27/120    avg_loss:0.059, val_acc:0.968]
Epoch [28/120    avg_loss:0.066, val_acc:0.960]
Epoch [29/120    avg_loss:0.042, val_acc:0.980]
Epoch [30/120    avg_loss:0.048, val_acc:0.954]
Epoch [31/120    avg_loss:0.036, val_acc:0.973]
Epoch [32/120    avg_loss:0.021, val_acc:0.983]
Epoch [33/120    avg_loss:0.026, val_acc:0.956]
Epoch [34/120    avg_loss:0.021, val_acc:0.978]
Epoch [35/120    avg_loss:0.018, val_acc:0.976]
Epoch [36/120    avg_loss:0.015, val_acc:0.975]
Epoch [37/120    avg_loss:0.013, val_acc:0.982]
Epoch [38/120    avg_loss:0.019, val_acc:0.982]
Epoch [39/120    avg_loss:0.021, val_acc:0.981]
Epoch [40/120    avg_loss:0.015, val_acc:0.964]
Epoch [41/120    avg_loss:0.019, val_acc:0.977]
Epoch [42/120    avg_loss:0.022, val_acc:0.970]
Epoch [43/120    avg_loss:0.027, val_acc:0.975]
Epoch [44/120    avg_loss:0.035, val_acc:0.983]
Epoch [45/120    avg_loss:0.060, val_acc:0.958]
Epoch [46/120    avg_loss:0.035, val_acc:0.973]
Epoch [47/120    avg_loss:0.024, val_acc:0.978]
Epoch [48/120    avg_loss:0.034, val_acc:0.973]
Epoch [49/120    avg_loss:0.028, val_acc:0.973]
Epoch [50/120    avg_loss:0.021, val_acc:0.982]
Epoch [51/120    avg_loss:0.010, val_acc:0.978]
Epoch [52/120    avg_loss:0.015, val_acc:0.977]
Epoch [53/120    avg_loss:0.013, val_acc:0.983]
Epoch [54/120    avg_loss:0.021, val_acc:0.973]
Epoch [55/120    avg_loss:0.012, val_acc:0.979]
Epoch [56/120    avg_loss:0.009, val_acc:0.988]
Epoch [57/120    avg_loss:0.008, val_acc:0.981]
Epoch [58/120    avg_loss:0.012, val_acc:0.988]
Epoch [59/120    avg_loss:0.006, val_acc:0.986]
Epoch [60/120    avg_loss:0.007, val_acc:0.989]
Epoch [61/120    avg_loss:0.011, val_acc:0.984]
Epoch [62/120    avg_loss:0.007, val_acc:0.991]
Epoch [63/120    avg_loss:0.029, val_acc:0.972]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.018, val_acc:0.972]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.009, val_acc:0.985]
Epoch [68/120    avg_loss:0.005, val_acc:0.990]
Epoch [69/120    avg_loss:0.008, val_acc:0.993]
Epoch [70/120    avg_loss:0.006, val_acc:0.986]
Epoch [71/120    avg_loss:0.003, val_acc:0.989]
Epoch [72/120    avg_loss:0.004, val_acc:0.985]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.004, val_acc:0.986]
Epoch [75/120    avg_loss:0.008, val_acc:0.984]
Epoch [76/120    avg_loss:0.004, val_acc:0.988]
Epoch [77/120    avg_loss:0.004, val_acc:0.988]
Epoch [78/120    avg_loss:0.013, val_acc:0.975]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.003, val_acc:0.988]
Epoch [85/120    avg_loss:0.003, val_acc:0.988]
Epoch [86/120    avg_loss:0.002, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.003, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.003, val_acc:0.989]
Epoch [91/120    avg_loss:0.004, val_acc:0.989]
Epoch [92/120    avg_loss:0.003, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.989]
Epoch [94/120    avg_loss:0.003, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.003, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.003, val_acc:0.989]
Epoch [103/120    avg_loss:0.003, val_acc:0.989]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.003, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.002, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6386     0     0     0     0     7    32     6     1]
 [    0     0 18046     0    40     0     2     0     2     0]
 [    0     2     0  1980     1     0     0     0    50     3]
 [    0    24     0     6  2931     0     0     0    10     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4870     0     7     0]
 [    0     2     0     0     0     0     7  1274     0     7]
 [    0     8     0    16    46     0     0     0  3500     1]
 [    0     0     0     0     1    20     0     0     0   898]]

Accuracy:
99.26975634444365

F1 scores:
[       nan 0.99362066 0.99875474 0.98068351 0.9784677  0.99239544
 0.99754199 0.98151002 0.97956899 0.98142077]

Kappa:
0.9903288846856788
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2f3807f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.494, val_acc:0.593]
Epoch [2/120    avg_loss:0.824, val_acc:0.453]
Epoch [3/120    avg_loss:0.620, val_acc:0.633]
Epoch [4/120    avg_loss:0.452, val_acc:0.743]
Epoch [5/120    avg_loss:0.381, val_acc:0.772]
Epoch [6/120    avg_loss:0.311, val_acc:0.752]
Epoch [7/120    avg_loss:0.251, val_acc:0.857]
Epoch [8/120    avg_loss:0.211, val_acc:0.826]
Epoch [9/120    avg_loss:0.229, val_acc:0.838]
Epoch [10/120    avg_loss:0.250, val_acc:0.918]
Epoch [11/120    avg_loss:0.194, val_acc:0.894]
Epoch [12/120    avg_loss:0.153, val_acc:0.949]
Epoch [13/120    avg_loss:0.138, val_acc:0.946]
Epoch [14/120    avg_loss:0.106, val_acc:0.961]
Epoch [15/120    avg_loss:0.086, val_acc:0.940]
Epoch [16/120    avg_loss:0.070, val_acc:0.953]
Epoch [17/120    avg_loss:0.078, val_acc:0.970]
Epoch [18/120    avg_loss:0.055, val_acc:0.966]
Epoch [19/120    avg_loss:0.049, val_acc:0.976]
Epoch [20/120    avg_loss:0.036, val_acc:0.978]
Epoch [21/120    avg_loss:0.037, val_acc:0.978]
Epoch [22/120    avg_loss:0.055, val_acc:0.949]
Epoch [23/120    avg_loss:0.088, val_acc:0.970]
Epoch [24/120    avg_loss:0.058, val_acc:0.957]
Epoch [25/120    avg_loss:0.034, val_acc:0.978]
Epoch [26/120    avg_loss:0.039, val_acc:0.963]
Epoch [27/120    avg_loss:0.063, val_acc:0.973]
Epoch [28/120    avg_loss:0.059, val_acc:0.959]
Epoch [29/120    avg_loss:0.056, val_acc:0.973]
Epoch [30/120    avg_loss:0.027, val_acc:0.983]
Epoch [31/120    avg_loss:0.020, val_acc:0.979]
Epoch [32/120    avg_loss:0.030, val_acc:0.985]
Epoch [33/120    avg_loss:0.025, val_acc:0.983]
Epoch [34/120    avg_loss:0.037, val_acc:0.987]
Epoch [35/120    avg_loss:0.019, val_acc:0.989]
Epoch [36/120    avg_loss:0.018, val_acc:0.973]
Epoch [37/120    avg_loss:0.018, val_acc:0.986]
Epoch [38/120    avg_loss:0.021, val_acc:0.983]
Epoch [39/120    avg_loss:0.017, val_acc:0.984]
Epoch [40/120    avg_loss:0.009, val_acc:0.985]
Epoch [41/120    avg_loss:0.021, val_acc:0.922]
Epoch [42/120    avg_loss:0.016, val_acc:0.986]
Epoch [43/120    avg_loss:0.009, val_acc:0.987]
Epoch [44/120    avg_loss:0.014, val_acc:0.988]
Epoch [45/120    avg_loss:0.017, val_acc:0.990]
Epoch [46/120    avg_loss:0.029, val_acc:0.980]
Epoch [47/120    avg_loss:0.036, val_acc:0.980]
Epoch [48/120    avg_loss:0.025, val_acc:0.958]
Epoch [49/120    avg_loss:0.030, val_acc:0.983]
Epoch [50/120    avg_loss:0.018, val_acc:0.985]
Epoch [51/120    avg_loss:0.013, val_acc:0.991]
Epoch [52/120    avg_loss:0.013, val_acc:0.987]
Epoch [53/120    avg_loss:0.015, val_acc:0.988]
Epoch [54/120    avg_loss:0.029, val_acc:0.984]
Epoch [55/120    avg_loss:0.012, val_acc:0.989]
Epoch [56/120    avg_loss:0.015, val_acc:0.987]
Epoch [57/120    avg_loss:0.010, val_acc:0.988]
Epoch [58/120    avg_loss:0.006, val_acc:0.990]
Epoch [59/120    avg_loss:0.010, val_acc:0.989]
Epoch [60/120    avg_loss:0.008, val_acc:0.993]
Epoch [61/120    avg_loss:0.008, val_acc:0.991]
Epoch [62/120    avg_loss:0.010, val_acc:0.989]
Epoch [63/120    avg_loss:0.009, val_acc:0.988]
Epoch [64/120    avg_loss:0.010, val_acc:0.983]
Epoch [65/120    avg_loss:0.012, val_acc:0.989]
Epoch [66/120    avg_loss:0.009, val_acc:0.989]
Epoch [67/120    avg_loss:0.012, val_acc:0.991]
Epoch [68/120    avg_loss:0.008, val_acc:0.991]
Epoch [69/120    avg_loss:0.006, val_acc:0.993]
Epoch [70/120    avg_loss:0.017, val_acc:0.990]
Epoch [71/120    avg_loss:0.009, val_acc:0.992]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.006, val_acc:0.994]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.012, val_acc:0.989]
Epoch [76/120    avg_loss:0.007, val_acc:0.993]
Epoch [77/120    avg_loss:0.006, val_acc:0.988]
Epoch [78/120    avg_loss:0.011, val_acc:0.994]
Epoch [79/120    avg_loss:0.007, val_acc:0.995]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.005, val_acc:0.992]
Epoch [83/120    avg_loss:0.004, val_acc:0.993]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.010, val_acc:0.992]
Epoch [86/120    avg_loss:0.008, val_acc:0.991]
Epoch [87/120    avg_loss:0.014, val_acc:0.990]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.991]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.004, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.009, val_acc:0.993]
Epoch [100/120    avg_loss:0.004, val_acc:0.993]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.004, val_acc:0.992]
Epoch [103/120    avg_loss:0.003, val_acc:0.992]
Epoch [104/120    avg_loss:0.002, val_acc:0.992]
Epoch [105/120    avg_loss:0.004, val_acc:0.992]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.002, val_acc:0.992]
Epoch [108/120    avg_loss:0.003, val_acc:0.992]
Epoch [109/120    avg_loss:0.003, val_acc:0.992]
Epoch [110/120    avg_loss:0.002, val_acc:0.992]
Epoch [111/120    avg_loss:0.003, val_acc:0.992]
Epoch [112/120    avg_loss:0.002, val_acc:0.992]
Epoch [113/120    avg_loss:0.002, val_acc:0.992]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.003, val_acc:0.992]
Epoch [116/120    avg_loss:0.002, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.004, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     2     0     0     0     1     0     0]
 [    0     0 18066     0     8     0    12     0     4     0]
 [    0     0     0  2015     0     0     0     0    19     2]
 [    0    19     1     5  2945     0     0     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     8     0     0     0     0     3  1276     0     3]
 [    0     5     0    16    46     0     0     0  3503     1]
 [    0     0     0     0     7    30     0     0     0   882]]

Accuracy:
99.5324512568385

F1 scores:
[       nan 0.99728535 0.99930857 0.9891998  0.98527936 0.98863636
 0.99846484 0.9941566  0.98689956 0.97620365]

Kappa:
0.9938057022374923
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd5ecad0710>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.438, val_acc:0.695]
Epoch [2/120    avg_loss:0.805, val_acc:0.581]
Epoch [3/120    avg_loss:0.590, val_acc:0.697]
Epoch [4/120    avg_loss:0.489, val_acc:0.682]
Epoch [5/120    avg_loss:0.424, val_acc:0.719]
Epoch [6/120    avg_loss:0.358, val_acc:0.796]
Epoch [7/120    avg_loss:0.366, val_acc:0.839]
Epoch [8/120    avg_loss:0.258, val_acc:0.874]
Epoch [9/120    avg_loss:0.231, val_acc:0.943]
Epoch [10/120    avg_loss:0.151, val_acc:0.930]
Epoch [11/120    avg_loss:0.120, val_acc:0.963]
Epoch [12/120    avg_loss:0.109, val_acc:0.944]
Epoch [13/120    avg_loss:0.095, val_acc:0.936]
Epoch [14/120    avg_loss:0.143, val_acc:0.951]
Epoch [15/120    avg_loss:0.101, val_acc:0.953]
Epoch [16/120    avg_loss:0.066, val_acc:0.978]
Epoch [17/120    avg_loss:0.058, val_acc:0.961]
Epoch [18/120    avg_loss:0.068, val_acc:0.968]
Epoch [19/120    avg_loss:0.059, val_acc:0.979]
Epoch [20/120    avg_loss:0.046, val_acc:0.977]
Epoch [21/120    avg_loss:0.034, val_acc:0.986]
Epoch [22/120    avg_loss:0.049, val_acc:0.983]
Epoch [23/120    avg_loss:0.073, val_acc:0.969]
Epoch [24/120    avg_loss:0.046, val_acc:0.984]
Epoch [25/120    avg_loss:0.030, val_acc:0.983]
Epoch [26/120    avg_loss:0.027, val_acc:0.978]
Epoch [27/120    avg_loss:0.022, val_acc:0.988]
Epoch [28/120    avg_loss:0.023, val_acc:0.987]
Epoch [29/120    avg_loss:0.017, val_acc:0.988]
Epoch [30/120    avg_loss:0.013, val_acc:0.979]
Epoch [31/120    avg_loss:0.016, val_acc:0.987]
Epoch [32/120    avg_loss:0.011, val_acc:0.985]
Epoch [33/120    avg_loss:0.019, val_acc:0.981]
Epoch [34/120    avg_loss:0.019, val_acc:0.983]
Epoch [35/120    avg_loss:0.022, val_acc:0.983]
Epoch [36/120    avg_loss:0.025, val_acc:0.980]
Epoch [37/120    avg_loss:0.027, val_acc:0.973]
Epoch [38/120    avg_loss:0.023, val_acc:0.972]
Epoch [39/120    avg_loss:0.025, val_acc:0.987]
Epoch [40/120    avg_loss:0.020, val_acc:0.983]
Epoch [41/120    avg_loss:0.015, val_acc:0.984]
Epoch [42/120    avg_loss:0.028, val_acc:0.982]
Epoch [43/120    avg_loss:0.019, val_acc:0.989]
Epoch [44/120    avg_loss:0.018, val_acc:0.989]
Epoch [45/120    avg_loss:0.011, val_acc:0.990]
Epoch [46/120    avg_loss:0.009, val_acc:0.991]
Epoch [47/120    avg_loss:0.011, val_acc:0.990]
Epoch [48/120    avg_loss:0.011, val_acc:0.992]
Epoch [49/120    avg_loss:0.007, val_acc:0.991]
Epoch [50/120    avg_loss:0.009, val_acc:0.989]
Epoch [51/120    avg_loss:0.012, val_acc:0.993]
Epoch [52/120    avg_loss:0.010, val_acc:0.992]
Epoch [53/120    avg_loss:0.007, val_acc:0.992]
Epoch [54/120    avg_loss:0.006, val_acc:0.991]
Epoch [55/120    avg_loss:0.007, val_acc:0.992]
Epoch [56/120    avg_loss:0.007, val_acc:0.992]
Epoch [57/120    avg_loss:0.014, val_acc:0.991]
Epoch [58/120    avg_loss:0.009, val_acc:0.991]
Epoch [59/120    avg_loss:0.008, val_acc:0.992]
Epoch [60/120    avg_loss:0.004, val_acc:0.992]
Epoch [61/120    avg_loss:0.007, val_acc:0.992]
Epoch [62/120    avg_loss:0.009, val_acc:0.991]
Epoch [63/120    avg_loss:0.007, val_acc:0.991]
Epoch [64/120    avg_loss:0.007, val_acc:0.992]
Epoch [65/120    avg_loss:0.009, val_acc:0.992]
Epoch [66/120    avg_loss:0.007, val_acc:0.992]
Epoch [67/120    avg_loss:0.007, val_acc:0.992]
Epoch [68/120    avg_loss:0.006, val_acc:0.992]
Epoch [69/120    avg_loss:0.007, val_acc:0.992]
Epoch [70/120    avg_loss:0.006, val_acc:0.992]
Epoch [71/120    avg_loss:0.006, val_acc:0.992]
Epoch [72/120    avg_loss:0.006, val_acc:0.992]
Epoch [73/120    avg_loss:0.009, val_acc:0.992]
Epoch [74/120    avg_loss:0.006, val_acc:0.991]
Epoch [75/120    avg_loss:0.008, val_acc:0.991]
Epoch [76/120    avg_loss:0.007, val_acc:0.992]
Epoch [77/120    avg_loss:0.006, val_acc:0.992]
Epoch [78/120    avg_loss:0.007, val_acc:0.992]
Epoch [79/120    avg_loss:0.006, val_acc:0.992]
Epoch [80/120    avg_loss:0.009, val_acc:0.992]
Epoch [81/120    avg_loss:0.006, val_acc:0.992]
Epoch [82/120    avg_loss:0.006, val_acc:0.992]
Epoch [83/120    avg_loss:0.010, val_acc:0.992]
Epoch [84/120    avg_loss:0.005, val_acc:0.992]
Epoch [85/120    avg_loss:0.007, val_acc:0.992]
Epoch [86/120    avg_loss:0.007, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.008, val_acc:0.992]
Epoch [89/120    avg_loss:0.005, val_acc:0.992]
Epoch [90/120    avg_loss:0.007, val_acc:0.992]
Epoch [91/120    avg_loss:0.006, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.992]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.992]
Epoch [97/120    avg_loss:0.011, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.007, val_acc:0.992]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.006, val_acc:0.992]
Epoch [103/120    avg_loss:0.015, val_acc:0.992]
Epoch [104/120    avg_loss:0.006, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.012, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.011, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.992]
Epoch [117/120    avg_loss:0.012, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.992]
Epoch [119/120    avg_loss:0.009, val_acc:0.992]
Epoch [120/120    avg_loss:0.015, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     0     0     0     0    14     1     1]
 [    0     2 18010     0    19     0    56     0     3     0]
 [    0     3     0  1998     0     0     0     0    31     4]
 [    0    35     1     3  2914     0     7     0    10     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     5     0     0     0     0     3  1276     0     6]
 [    0     5     0    12    42     0     0     0  3508     4]
 [    0     0     0     0     0    20     0     0     0   899]]

Accuracy:
99.30349697539344

F1 scores:
[       nan 0.99488293 0.99775629 0.98691035 0.97998991 0.99239544
 0.99328039 0.98914729 0.98483998 0.97983651]

Kappa:
0.9907777800221644
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f66539197b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 88528==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.440, val_acc:0.706]
Epoch [2/120    avg_loss:0.823, val_acc:0.623]
Epoch [3/120    avg_loss:0.601, val_acc:0.728]
Epoch [4/120    avg_loss:0.465, val_acc:0.748]
Epoch [5/120    avg_loss:0.366, val_acc:0.783]
Epoch [6/120    avg_loss:0.352, val_acc:0.772]
Epoch [7/120    avg_loss:0.331, val_acc:0.806]
Epoch [8/120    avg_loss:0.268, val_acc:0.868]
Epoch [9/120    avg_loss:0.197, val_acc:0.916]
Epoch [10/120    avg_loss:0.188, val_acc:0.936]
Epoch [11/120    avg_loss:0.145, val_acc:0.917]
Epoch [12/120    avg_loss:0.147, val_acc:0.867]
Epoch [13/120    avg_loss:0.135, val_acc:0.943]
Epoch [14/120    avg_loss:0.087, val_acc:0.941]
Epoch [15/120    avg_loss:0.103, val_acc:0.867]
Epoch [16/120    avg_loss:0.102, val_acc:0.931]
Epoch [17/120    avg_loss:0.089, val_acc:0.963]
Epoch [18/120    avg_loss:0.051, val_acc:0.964]
Epoch [19/120    avg_loss:0.079, val_acc:0.966]
Epoch [20/120    avg_loss:0.070, val_acc:0.946]
Epoch [21/120    avg_loss:0.046, val_acc:0.968]
Epoch [22/120    avg_loss:0.045, val_acc:0.963]
Epoch [23/120    avg_loss:0.046, val_acc:0.965]
Epoch [24/120    avg_loss:0.061, val_acc:0.980]
Epoch [25/120    avg_loss:0.049, val_acc:0.949]
Epoch [26/120    avg_loss:0.034, val_acc:0.967]
Epoch [27/120    avg_loss:0.020, val_acc:0.979]
Epoch [28/120    avg_loss:0.030, val_acc:0.962]
Epoch [29/120    avg_loss:0.072, val_acc:0.965]
Epoch [30/120    avg_loss:0.041, val_acc:0.965]
Epoch [31/120    avg_loss:0.040, val_acc:0.968]
Epoch [32/120    avg_loss:0.053, val_acc:0.968]
Epoch [33/120    avg_loss:0.022, val_acc:0.977]
Epoch [34/120    avg_loss:0.035, val_acc:0.983]
Epoch [35/120    avg_loss:0.017, val_acc:0.984]
Epoch [36/120    avg_loss:0.018, val_acc:0.981]
Epoch [37/120    avg_loss:0.015, val_acc:0.981]
Epoch [38/120    avg_loss:0.015, val_acc:0.980]
Epoch [39/120    avg_loss:0.024, val_acc:0.983]
Epoch [40/120    avg_loss:0.025, val_acc:0.969]
Epoch [41/120    avg_loss:0.029, val_acc:0.973]
Epoch [42/120    avg_loss:0.030, val_acc:0.973]
Epoch [43/120    avg_loss:0.035, val_acc:0.942]
Epoch [44/120    avg_loss:0.056, val_acc:0.945]
Epoch [45/120    avg_loss:0.041, val_acc:0.972]
Epoch [46/120    avg_loss:0.049, val_acc:0.963]
Epoch [47/120    avg_loss:0.033, val_acc:0.970]
Epoch [48/120    avg_loss:0.040, val_acc:0.954]
Epoch [49/120    avg_loss:0.028, val_acc:0.973]
Epoch [50/120    avg_loss:0.017, val_acc:0.980]
Epoch [51/120    avg_loss:0.013, val_acc:0.983]
Epoch [52/120    avg_loss:0.016, val_acc:0.983]
Epoch [53/120    avg_loss:0.015, val_acc:0.985]
Epoch [54/120    avg_loss:0.016, val_acc:0.986]
Epoch [55/120    avg_loss:0.013, val_acc:0.985]
Epoch [56/120    avg_loss:0.011, val_acc:0.987]
Epoch [57/120    avg_loss:0.011, val_acc:0.988]
Epoch [58/120    avg_loss:0.009, val_acc:0.988]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.007, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.988]
Epoch [62/120    avg_loss:0.012, val_acc:0.988]
Epoch [63/120    avg_loss:0.013, val_acc:0.988]
Epoch [64/120    avg_loss:0.013, val_acc:0.988]
Epoch [65/120    avg_loss:0.010, val_acc:0.988]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.988]
Epoch [68/120    avg_loss:0.009, val_acc:0.987]
Epoch [69/120    avg_loss:0.010, val_acc:0.988]
Epoch [70/120    avg_loss:0.013, val_acc:0.988]
Epoch [71/120    avg_loss:0.007, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.012, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.011, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.988]
Epoch [83/120    avg_loss:0.007, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.012, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.012, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.013, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     3     0     0     7    13     5     5]
 [    0     4 18076     0     9     0     0     0     1     0]
 [    0     2     0  2008     0     0     0     0    23     3]
 [    0    25     0     4  2932     0     0     0     9     2]
 [    0     0     0     1     0  1304     0     0     0     0]
 [    0     0    20     0     0     0  4858     0     0     0]
 [    0     1     0     0     0     4     5  1272     0     8]
 [    0     5     0    24    57     0     0     0  3482     3]
 [    0     0     0     1     3    16     0     0     0   899]]

Accuracy:
99.36615814715735

F1 scores:
[       nan 0.99456015 0.99906041 0.98503802 0.98175121 0.99201217
 0.99671728 0.98796117 0.98208997 0.97770527]

Kappa:
0.9916010267060146
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6bbe448780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.556, val_acc:0.636]
Epoch [2/120    avg_loss:0.853, val_acc:0.547]
Epoch [3/120    avg_loss:0.578, val_acc:0.723]
Epoch [4/120    avg_loss:0.407, val_acc:0.736]
Epoch [5/120    avg_loss:0.322, val_acc:0.782]
Epoch [6/120    avg_loss:0.269, val_acc:0.878]
Epoch [7/120    avg_loss:0.201, val_acc:0.808]
Epoch [8/120    avg_loss:0.185, val_acc:0.898]
Epoch [9/120    avg_loss:0.139, val_acc:0.922]
Epoch [10/120    avg_loss:0.135, val_acc:0.960]
Epoch [11/120    avg_loss:0.084, val_acc:0.962]
Epoch [12/120    avg_loss:0.084, val_acc:0.930]
Epoch [13/120    avg_loss:0.110, val_acc:0.938]
Epoch [14/120    avg_loss:0.107, val_acc:0.951]
Epoch [15/120    avg_loss:0.077, val_acc:0.971]
Epoch [16/120    avg_loss:0.060, val_acc:0.966]
Epoch [17/120    avg_loss:0.066, val_acc:0.977]
Epoch [18/120    avg_loss:0.042, val_acc:0.968]
Epoch [19/120    avg_loss:0.059, val_acc:0.951]
Epoch [20/120    avg_loss:0.056, val_acc:0.943]
Epoch [21/120    avg_loss:0.032, val_acc:0.947]
Epoch [22/120    avg_loss:0.079, val_acc:0.965]
Epoch [23/120    avg_loss:0.051, val_acc:0.970]
Epoch [24/120    avg_loss:0.033, val_acc:0.980]
Epoch [25/120    avg_loss:0.044, val_acc:0.945]
Epoch [26/120    avg_loss:0.035, val_acc:0.985]
Epoch [27/120    avg_loss:0.030, val_acc:0.982]
Epoch [28/120    avg_loss:0.049, val_acc:0.980]
Epoch [29/120    avg_loss:0.031, val_acc:0.989]
Epoch [30/120    avg_loss:0.035, val_acc:0.987]
Epoch [31/120    avg_loss:0.027, val_acc:0.989]
Epoch [32/120    avg_loss:0.028, val_acc:0.978]
Epoch [33/120    avg_loss:0.031, val_acc:0.953]
Epoch [34/120    avg_loss:0.024, val_acc:0.980]
Epoch [35/120    avg_loss:0.019, val_acc:0.988]
Epoch [36/120    avg_loss:0.051, val_acc:0.981]
Epoch [37/120    avg_loss:0.040, val_acc:0.983]
Epoch [38/120    avg_loss:0.027, val_acc:0.981]
Epoch [39/120    avg_loss:0.034, val_acc:0.973]
Epoch [40/120    avg_loss:0.023, val_acc:0.971]
Epoch [41/120    avg_loss:0.018, val_acc:0.988]
Epoch [42/120    avg_loss:0.033, val_acc:0.972]
Epoch [43/120    avg_loss:0.043, val_acc:0.979]
Epoch [44/120    avg_loss:0.029, val_acc:0.975]
Epoch [45/120    avg_loss:0.019, val_acc:0.989]
Epoch [46/120    avg_loss:0.011, val_acc:0.992]
Epoch [47/120    avg_loss:0.012, val_acc:0.991]
Epoch [48/120    avg_loss:0.014, val_acc:0.990]
Epoch [49/120    avg_loss:0.014, val_acc:0.991]
Epoch [50/120    avg_loss:0.010, val_acc:0.991]
Epoch [51/120    avg_loss:0.016, val_acc:0.992]
Epoch [52/120    avg_loss:0.015, val_acc:0.992]
Epoch [53/120    avg_loss:0.010, val_acc:0.992]
Epoch [54/120    avg_loss:0.012, val_acc:0.993]
Epoch [55/120    avg_loss:0.012, val_acc:0.993]
Epoch [56/120    avg_loss:0.007, val_acc:0.994]
Epoch [57/120    avg_loss:0.011, val_acc:0.993]
Epoch [58/120    avg_loss:0.012, val_acc:0.993]
Epoch [59/120    avg_loss:0.012, val_acc:0.992]
Epoch [60/120    avg_loss:0.010, val_acc:0.990]
Epoch [61/120    avg_loss:0.016, val_acc:0.993]
Epoch [62/120    avg_loss:0.010, val_acc:0.991]
Epoch [63/120    avg_loss:0.012, val_acc:0.992]
Epoch [64/120    avg_loss:0.011, val_acc:0.993]
Epoch [65/120    avg_loss:0.009, val_acc:0.992]
Epoch [66/120    avg_loss:0.010, val_acc:0.993]
Epoch [67/120    avg_loss:0.010, val_acc:0.993]
Epoch [68/120    avg_loss:0.012, val_acc:0.993]
Epoch [69/120    avg_loss:0.006, val_acc:0.993]
Epoch [70/120    avg_loss:0.007, val_acc:0.993]
Epoch [71/120    avg_loss:0.013, val_acc:0.993]
Epoch [72/120    avg_loss:0.009, val_acc:0.993]
Epoch [73/120    avg_loss:0.009, val_acc:0.993]
Epoch [74/120    avg_loss:0.010, val_acc:0.993]
Epoch [75/120    avg_loss:0.008, val_acc:0.993]
Epoch [76/120    avg_loss:0.008, val_acc:0.993]
Epoch [77/120    avg_loss:0.007, val_acc:0.993]
Epoch [78/120    avg_loss:0.009, val_acc:0.993]
Epoch [79/120    avg_loss:0.008, val_acc:0.993]
Epoch [80/120    avg_loss:0.009, val_acc:0.993]
Epoch [81/120    avg_loss:0.009, val_acc:0.993]
Epoch [82/120    avg_loss:0.010, val_acc:0.993]
Epoch [83/120    avg_loss:0.012, val_acc:0.993]
Epoch [84/120    avg_loss:0.014, val_acc:0.993]
Epoch [85/120    avg_loss:0.006, val_acc:0.993]
Epoch [86/120    avg_loss:0.012, val_acc:0.993]
Epoch [87/120    avg_loss:0.009, val_acc:0.993]
Epoch [88/120    avg_loss:0.009, val_acc:0.993]
Epoch [89/120    avg_loss:0.009, val_acc:0.993]
Epoch [90/120    avg_loss:0.007, val_acc:0.993]
Epoch [91/120    avg_loss:0.012, val_acc:0.993]
Epoch [92/120    avg_loss:0.008, val_acc:0.993]
Epoch [93/120    avg_loss:0.008, val_acc:0.993]
Epoch [94/120    avg_loss:0.010, val_acc:0.993]
Epoch [95/120    avg_loss:0.007, val_acc:0.993]
Epoch [96/120    avg_loss:0.010, val_acc:0.993]
Epoch [97/120    avg_loss:0.009, val_acc:0.993]
Epoch [98/120    avg_loss:0.011, val_acc:0.993]
Epoch [99/120    avg_loss:0.008, val_acc:0.993]
Epoch [100/120    avg_loss:0.006, val_acc:0.993]
Epoch [101/120    avg_loss:0.009, val_acc:0.993]
Epoch [102/120    avg_loss:0.009, val_acc:0.993]
Epoch [103/120    avg_loss:0.009, val_acc:0.993]
Epoch [104/120    avg_loss:0.011, val_acc:0.993]
Epoch [105/120    avg_loss:0.007, val_acc:0.993]
Epoch [106/120    avg_loss:0.008, val_acc:0.993]
Epoch [107/120    avg_loss:0.008, val_acc:0.993]
Epoch [108/120    avg_loss:0.006, val_acc:0.993]
Epoch [109/120    avg_loss:0.012, val_acc:0.993]
Epoch [110/120    avg_loss:0.010, val_acc:0.993]
Epoch [111/120    avg_loss:0.009, val_acc:0.993]
Epoch [112/120    avg_loss:0.014, val_acc:0.993]
Epoch [113/120    avg_loss:0.007, val_acc:0.993]
Epoch [114/120    avg_loss:0.012, val_acc:0.993]
Epoch [115/120    avg_loss:0.008, val_acc:0.993]
Epoch [116/120    avg_loss:0.011, val_acc:0.993]
Epoch [117/120    avg_loss:0.007, val_acc:0.993]
Epoch [118/120    avg_loss:0.007, val_acc:0.993]
Epoch [119/120    avg_loss:0.008, val_acc:0.993]
Epoch [120/120    avg_loss:0.008, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0    14     0     0    13    18     4     3]
 [    0     6 18056     0    18     0     9     0     1     0]
 [    0     7     0  2002     0     0     0     0    19     8]
 [    0    26     9     2  2914     0     1     0    18     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4867     0    11     0]
 [    0     0     0     0     0     0     4  1281     0     5]
 [    0     0     0     7    44     0     0     0  3519     1]
 [    0     0     0     0    10    48     0     0     0   861]]

Accuracy:
99.25770611910443

F1 scores:
[       nan 0.99291884 0.99881068 0.98596405 0.9781806  0.98194131
 0.99611134 0.98957126 0.98530029 0.95719844]

Kappa:
0.9901671592559407
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3683597b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.495, val_acc:0.652]
Epoch [2/120    avg_loss:0.888, val_acc:0.491]
Epoch [3/120    avg_loss:0.629, val_acc:0.638]
Epoch [4/120    avg_loss:0.431, val_acc:0.644]
Epoch [5/120    avg_loss:0.358, val_acc:0.773]
Epoch [6/120    avg_loss:0.262, val_acc:0.807]
Epoch [7/120    avg_loss:0.239, val_acc:0.866]
Epoch [8/120    avg_loss:0.195, val_acc:0.908]
Epoch [9/120    avg_loss:0.217, val_acc:0.859]
Epoch [10/120    avg_loss:0.200, val_acc:0.889]
Epoch [11/120    avg_loss:0.121, val_acc:0.942]
Epoch [12/120    avg_loss:0.105, val_acc:0.890]
Epoch [13/120    avg_loss:0.078, val_acc:0.959]
Epoch [14/120    avg_loss:0.061, val_acc:0.960]
Epoch [15/120    avg_loss:0.053, val_acc:0.972]
Epoch [16/120    avg_loss:0.057, val_acc:0.925]
Epoch [17/120    avg_loss:0.079, val_acc:0.960]
Epoch [18/120    avg_loss:0.063, val_acc:0.957]
Epoch [19/120    avg_loss:0.053, val_acc:0.953]
Epoch [20/120    avg_loss:0.053, val_acc:0.958]
Epoch [21/120    avg_loss:0.062, val_acc:0.958]
Epoch [22/120    avg_loss:0.079, val_acc:0.955]
Epoch [23/120    avg_loss:0.116, val_acc:0.950]
Epoch [24/120    avg_loss:0.069, val_acc:0.972]
Epoch [25/120    avg_loss:0.031, val_acc:0.980]
Epoch [26/120    avg_loss:0.036, val_acc:0.979]
Epoch [27/120    avg_loss:0.031, val_acc:0.980]
Epoch [28/120    avg_loss:0.027, val_acc:0.976]
Epoch [29/120    avg_loss:0.031, val_acc:0.977]
Epoch [30/120    avg_loss:0.044, val_acc:0.975]
Epoch [31/120    avg_loss:0.037, val_acc:0.970]
Epoch [32/120    avg_loss:0.050, val_acc:0.977]
Epoch [33/120    avg_loss:0.045, val_acc:0.974]
Epoch [34/120    avg_loss:0.044, val_acc:0.975]
Epoch [35/120    avg_loss:0.023, val_acc:0.984]
Epoch [36/120    avg_loss:0.017, val_acc:0.978]
Epoch [37/120    avg_loss:0.021, val_acc:0.973]
Epoch [38/120    avg_loss:0.015, val_acc:0.977]
Epoch [39/120    avg_loss:0.026, val_acc:0.983]
Epoch [40/120    avg_loss:0.018, val_acc:0.979]
Epoch [41/120    avg_loss:0.037, val_acc:0.984]
Epoch [42/120    avg_loss:0.016, val_acc:0.985]
Epoch [43/120    avg_loss:0.013, val_acc:0.983]
Epoch [44/120    avg_loss:0.017, val_acc:0.983]
Epoch [45/120    avg_loss:0.042, val_acc:0.978]
Epoch [46/120    avg_loss:0.017, val_acc:0.983]
Epoch [47/120    avg_loss:0.012, val_acc:0.981]
Epoch [48/120    avg_loss:0.014, val_acc:0.984]
Epoch [49/120    avg_loss:0.010, val_acc:0.984]
Epoch [50/120    avg_loss:0.007, val_acc:0.983]
Epoch [51/120    avg_loss:0.006, val_acc:0.988]
Epoch [52/120    avg_loss:0.011, val_acc:0.976]
Epoch [53/120    avg_loss:0.017, val_acc:0.978]
Epoch [54/120    avg_loss:0.014, val_acc:0.986]
Epoch [55/120    avg_loss:0.007, val_acc:0.984]
Epoch [56/120    avg_loss:0.011, val_acc:0.986]
Epoch [57/120    avg_loss:0.006, val_acc:0.987]
Epoch [58/120    avg_loss:0.005, val_acc:0.987]
Epoch [59/120    avg_loss:0.010, val_acc:0.982]
Epoch [60/120    avg_loss:0.016, val_acc:0.974]
Epoch [61/120    avg_loss:0.010, val_acc:0.986]
Epoch [62/120    avg_loss:0.005, val_acc:0.984]
Epoch [63/120    avg_loss:0.006, val_acc:0.984]
Epoch [64/120    avg_loss:0.007, val_acc:0.982]
Epoch [65/120    avg_loss:0.009, val_acc:0.985]
Epoch [66/120    avg_loss:0.005, val_acc:0.988]
Epoch [67/120    avg_loss:0.006, val_acc:0.987]
Epoch [68/120    avg_loss:0.005, val_acc:0.987]
Epoch [69/120    avg_loss:0.005, val_acc:0.986]
Epoch [70/120    avg_loss:0.006, val_acc:0.986]
Epoch [71/120    avg_loss:0.004, val_acc:0.986]
Epoch [72/120    avg_loss:0.003, val_acc:0.987]
Epoch [73/120    avg_loss:0.007, val_acc:0.987]
Epoch [74/120    avg_loss:0.003, val_acc:0.988]
Epoch [75/120    avg_loss:0.004, val_acc:0.988]
Epoch [76/120    avg_loss:0.005, val_acc:0.988]
Epoch [77/120    avg_loss:0.003, val_acc:0.988]
Epoch [78/120    avg_loss:0.004, val_acc:0.987]
Epoch [79/120    avg_loss:0.004, val_acc:0.987]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.003, val_acc:0.986]
Epoch [88/120    avg_loss:0.004, val_acc:0.987]
Epoch [89/120    avg_loss:0.004, val_acc:0.987]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.003, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.003, val_acc:0.986]
Epoch [96/120    avg_loss:0.003, val_acc:0.986]
Epoch [97/120    avg_loss:0.003, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.986]
Epoch [100/120    avg_loss:0.003, val_acc:0.986]
Epoch [101/120    avg_loss:0.003, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.003, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.003, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.002, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6422     0     0     0     0     8     1     1     0]
 [    0     8 18066     0     8     0     6     0     2     0]
 [    0     0     0  2020     3     0     0     0    12     1]
 [    0    26     4     0  2922     0     2     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4862     0    10     0]
 [    0     2     0     0     0     0     2  1281     0     5]
 [    0     1     0     5    55     0     0     0  3506     4]
 [    0     0     0     2    14    23     0     0     0   880]]

Accuracy:
99.44809967946401

F1 scores:
[       nan 0.99635405 0.99905989 0.99433916 0.97823904 0.99126472
 0.99651568 0.99611198 0.98483146 0.97291321]

Kappa:
0.9926874630719424
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f07d0ee67b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.414, val_acc:0.395]
Epoch [2/120    avg_loss:0.866, val_acc:0.742]
Epoch [3/120    avg_loss:0.659, val_acc:0.708]
Epoch [4/120    avg_loss:0.510, val_acc:0.748]
Epoch [5/120    avg_loss:0.391, val_acc:0.729]
Epoch [6/120    avg_loss:0.297, val_acc:0.758]
Epoch [7/120    avg_loss:0.259, val_acc:0.871]
Epoch [8/120    avg_loss:0.204, val_acc:0.939]
Epoch [9/120    avg_loss:0.184, val_acc:0.933]
Epoch [10/120    avg_loss:0.135, val_acc:0.916]
Epoch [11/120    avg_loss:0.148, val_acc:0.953]
Epoch [12/120    avg_loss:0.087, val_acc:0.958]
Epoch [13/120    avg_loss:0.094, val_acc:0.952]
Epoch [14/120    avg_loss:0.079, val_acc:0.946]
Epoch [15/120    avg_loss:0.105, val_acc:0.958]
Epoch [16/120    avg_loss:0.061, val_acc:0.960]
Epoch [17/120    avg_loss:0.057, val_acc:0.972]
Epoch [18/120    avg_loss:0.034, val_acc:0.957]
Epoch [19/120    avg_loss:0.034, val_acc:0.974]
Epoch [20/120    avg_loss:0.031, val_acc:0.974]
Epoch [21/120    avg_loss:0.041, val_acc:0.960]
Epoch [22/120    avg_loss:0.044, val_acc:0.969]
Epoch [23/120    avg_loss:0.043, val_acc:0.973]
Epoch [24/120    avg_loss:0.031, val_acc:0.974]
Epoch [25/120    avg_loss:0.025, val_acc:0.975]
Epoch [26/120    avg_loss:0.052, val_acc:0.964]
Epoch [27/120    avg_loss:0.038, val_acc:0.976]
Epoch [28/120    avg_loss:0.021, val_acc:0.973]
Epoch [29/120    avg_loss:0.019, val_acc:0.987]
Epoch [30/120    avg_loss:0.026, val_acc:0.972]
Epoch [31/120    avg_loss:0.027, val_acc:0.978]
Epoch [32/120    avg_loss:0.019, val_acc:0.973]
Epoch [33/120    avg_loss:0.015, val_acc:0.978]
Epoch [34/120    avg_loss:0.015, val_acc:0.978]
Epoch [35/120    avg_loss:0.014, val_acc:0.946]
Epoch [36/120    avg_loss:0.020, val_acc:0.960]
Epoch [37/120    avg_loss:0.030, val_acc:0.976]
Epoch [38/120    avg_loss:0.015, val_acc:0.980]
Epoch [39/120    avg_loss:0.025, val_acc:0.981]
Epoch [40/120    avg_loss:0.026, val_acc:0.963]
Epoch [41/120    avg_loss:0.029, val_acc:0.970]
Epoch [42/120    avg_loss:0.094, val_acc:0.948]
Epoch [43/120    avg_loss:0.067, val_acc:0.967]
Epoch [44/120    avg_loss:0.019, val_acc:0.973]
Epoch [45/120    avg_loss:0.013, val_acc:0.976]
Epoch [46/120    avg_loss:0.015, val_acc:0.978]
Epoch [47/120    avg_loss:0.020, val_acc:0.978]
Epoch [48/120    avg_loss:0.015, val_acc:0.978]
Epoch [49/120    avg_loss:0.016, val_acc:0.978]
Epoch [50/120    avg_loss:0.016, val_acc:0.982]
Epoch [51/120    avg_loss:0.015, val_acc:0.981]
Epoch [52/120    avg_loss:0.012, val_acc:0.979]
Epoch [53/120    avg_loss:0.014, val_acc:0.982]
Epoch [54/120    avg_loss:0.013, val_acc:0.982]
Epoch [55/120    avg_loss:0.014, val_acc:0.982]
Epoch [56/120    avg_loss:0.016, val_acc:0.982]
Epoch [57/120    avg_loss:0.010, val_acc:0.982]
Epoch [58/120    avg_loss:0.010, val_acc:0.982]
Epoch [59/120    avg_loss:0.012, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.982]
Epoch [61/120    avg_loss:0.013, val_acc:0.982]
Epoch [62/120    avg_loss:0.009, val_acc:0.982]
Epoch [63/120    avg_loss:0.016, val_acc:0.982]
Epoch [64/120    avg_loss:0.013, val_acc:0.982]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.009, val_acc:0.982]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.010, val_acc:0.983]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.008, val_acc:0.983]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.013, val_acc:0.983]
Epoch [74/120    avg_loss:0.013, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.983]
Epoch [76/120    avg_loss:0.008, val_acc:0.983]
Epoch [77/120    avg_loss:0.010, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.014, val_acc:0.983]
Epoch [82/120    avg_loss:0.012, val_acc:0.983]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.013, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.018, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.015, val_acc:0.983]
Epoch [94/120    avg_loss:0.013, val_acc:0.983]
Epoch [95/120    avg_loss:0.015, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.017, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.014, val_acc:0.983]
Epoch [107/120    avg_loss:0.012, val_acc:0.983]
Epoch [108/120    avg_loss:0.014, val_acc:0.983]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.020, val_acc:0.983]
Epoch [114/120    avg_loss:0.014, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.015, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     0     0     0    14    23     3     5]
 [    0     1 18068     0    16     0     3     0     1     1]
 [    0     7     0  2012     0     0     0     0    11     6]
 [    0    36     4     0  2907     0     1     0    22     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4870     0     2     0]
 [    0     0     0     0     0     0     0  1281     0     9]
 [    0     8     0     9    49     0     0     0  3496     9]
 [    0     0     0     0    12    39     0     0     0   868]]

Accuracy:
99.27939652471501

F1 scores:
[       nan 0.99246368 0.99911524 0.99186591 0.9761585  0.98527746
 0.9973377  0.98766384 0.98395722 0.95437053]

Kappa:
0.9904530030564285
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2b52dff828>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.531, val_acc:0.405]
Epoch [2/120    avg_loss:0.922, val_acc:0.505]
Epoch [3/120    avg_loss:0.652, val_acc:0.747]
Epoch [4/120    avg_loss:0.442, val_acc:0.685]
Epoch [5/120    avg_loss:0.375, val_acc:0.723]
Epoch [6/120    avg_loss:0.312, val_acc:0.847]
Epoch [7/120    avg_loss:0.235, val_acc:0.872]
Epoch [8/120    avg_loss:0.214, val_acc:0.908]
Epoch [9/120    avg_loss:0.172, val_acc:0.890]
Epoch [10/120    avg_loss:0.127, val_acc:0.941]
Epoch [11/120    avg_loss:0.099, val_acc:0.934]
Epoch [12/120    avg_loss:0.103, val_acc:0.913]
Epoch [13/120    avg_loss:0.089, val_acc:0.956]
Epoch [14/120    avg_loss:0.069, val_acc:0.963]
Epoch [15/120    avg_loss:0.076, val_acc:0.959]
Epoch [16/120    avg_loss:0.070, val_acc:0.964]
Epoch [17/120    avg_loss:0.169, val_acc:0.948]
Epoch [18/120    avg_loss:0.067, val_acc:0.966]
Epoch [19/120    avg_loss:0.069, val_acc:0.946]
Epoch [20/120    avg_loss:0.046, val_acc:0.970]
Epoch [21/120    avg_loss:0.039, val_acc:0.973]
Epoch [22/120    avg_loss:0.030, val_acc:0.969]
Epoch [23/120    avg_loss:0.043, val_acc:0.978]
Epoch [24/120    avg_loss:0.053, val_acc:0.966]
Epoch [25/120    avg_loss:0.044, val_acc:0.971]
Epoch [26/120    avg_loss:0.041, val_acc:0.966]
Epoch [27/120    avg_loss:0.023, val_acc:0.979]
Epoch [28/120    avg_loss:0.024, val_acc:0.982]
Epoch [29/120    avg_loss:0.017, val_acc:0.978]
Epoch [30/120    avg_loss:0.019, val_acc:0.980]
Epoch [31/120    avg_loss:0.028, val_acc:0.979]
Epoch [32/120    avg_loss:0.017, val_acc:0.978]
Epoch [33/120    avg_loss:0.018, val_acc:0.981]
Epoch [34/120    avg_loss:0.015, val_acc:0.981]
Epoch [35/120    avg_loss:0.015, val_acc:0.973]
Epoch [36/120    avg_loss:0.011, val_acc:0.984]
Epoch [37/120    avg_loss:0.013, val_acc:0.980]
Epoch [38/120    avg_loss:0.035, val_acc:0.969]
Epoch [39/120    avg_loss:0.078, val_acc:0.968]
Epoch [40/120    avg_loss:0.052, val_acc:0.966]
Epoch [41/120    avg_loss:0.026, val_acc:0.978]
Epoch [42/120    avg_loss:0.019, val_acc:0.973]
Epoch [43/120    avg_loss:0.024, val_acc:0.976]
Epoch [44/120    avg_loss:0.018, val_acc:0.978]
Epoch [45/120    avg_loss:0.016, val_acc:0.982]
Epoch [46/120    avg_loss:0.017, val_acc:0.984]
Epoch [47/120    avg_loss:0.011, val_acc:0.981]
Epoch [48/120    avg_loss:0.012, val_acc:0.982]
Epoch [49/120    avg_loss:0.017, val_acc:0.962]
Epoch [50/120    avg_loss:0.015, val_acc:0.984]
Epoch [51/120    avg_loss:0.019, val_acc:0.972]
Epoch [52/120    avg_loss:0.010, val_acc:0.984]
Epoch [53/120    avg_loss:0.009, val_acc:0.984]
Epoch [54/120    avg_loss:0.008, val_acc:0.983]
Epoch [55/120    avg_loss:0.006, val_acc:0.980]
Epoch [56/120    avg_loss:0.012, val_acc:0.983]
Epoch [57/120    avg_loss:0.010, val_acc:0.978]
Epoch [58/120    avg_loss:0.007, val_acc:0.983]
Epoch [59/120    avg_loss:0.008, val_acc:0.982]
Epoch [60/120    avg_loss:0.008, val_acc:0.985]
Epoch [61/120    avg_loss:0.008, val_acc:0.983]
Epoch [62/120    avg_loss:0.009, val_acc:0.984]
Epoch [63/120    avg_loss:0.007, val_acc:0.976]
Epoch [64/120    avg_loss:0.015, val_acc:0.982]
Epoch [65/120    avg_loss:0.009, val_acc:0.986]
Epoch [66/120    avg_loss:0.014, val_acc:0.983]
Epoch [67/120    avg_loss:0.006, val_acc:0.987]
Epoch [68/120    avg_loss:0.011, val_acc:0.978]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.004, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.005, val_acc:0.984]
Epoch [75/120    avg_loss:0.005, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.987]
Epoch [77/120    avg_loss:0.005, val_acc:0.980]
Epoch [78/120    avg_loss:0.011, val_acc:0.984]
Epoch [79/120    avg_loss:0.004, val_acc:0.986]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.012, val_acc:0.982]
Epoch [82/120    avg_loss:0.034, val_acc:0.976]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.019, val_acc:0.969]
Epoch [86/120    avg_loss:0.017, val_acc:0.981]
Epoch [87/120    avg_loss:0.012, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.985]
Epoch [94/120    avg_loss:0.004, val_acc:0.984]
Epoch [95/120    avg_loss:0.004, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.003, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     0     0     2     0     0     4]
 [    0    10 18065     0     9     0     6     0     0     0]
 [    0     1     0  2016     3     0     0     0    15     1]
 [    0    31     7     0  2911     0     0     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4875     0     2     0]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0     5     0    11    49     0     1     0  3503     2]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.4360494541248

F1 scores:
[       nan 0.99589306 0.99908747 0.99237017 0.97717355 0.98863636
 0.99856616 0.99727944 0.98481867 0.96899225]

Kappa:
0.9925277792096414
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff4770a8710>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.499, val_acc:0.431]
Epoch [2/120    avg_loss:0.855, val_acc:0.677]
Epoch [3/120    avg_loss:0.649, val_acc:0.664]
Epoch [4/120    avg_loss:0.455, val_acc:0.768]
Epoch [5/120    avg_loss:0.371, val_acc:0.811]
Epoch [6/120    avg_loss:0.276, val_acc:0.786]
Epoch [7/120    avg_loss:0.251, val_acc:0.875]
Epoch [8/120    avg_loss:0.202, val_acc:0.914]
Epoch [9/120    avg_loss:0.169, val_acc:0.796]
Epoch [10/120    avg_loss:0.159, val_acc:0.916]
Epoch [11/120    avg_loss:0.122, val_acc:0.949]
Epoch [12/120    avg_loss:0.110, val_acc:0.928]
Epoch [13/120    avg_loss:0.133, val_acc:0.921]
Epoch [14/120    avg_loss:0.090, val_acc:0.959]
Epoch [15/120    avg_loss:0.101, val_acc:0.947]
Epoch [16/120    avg_loss:0.093, val_acc:0.955]
Epoch [17/120    avg_loss:0.052, val_acc:0.953]
Epoch [18/120    avg_loss:0.047, val_acc:0.967]
Epoch [19/120    avg_loss:0.034, val_acc:0.965]
Epoch [20/120    avg_loss:0.030, val_acc:0.972]
Epoch [21/120    avg_loss:0.030, val_acc:0.976]
Epoch [22/120    avg_loss:0.044, val_acc:0.972]
Epoch [23/120    avg_loss:0.039, val_acc:0.976]
Epoch [24/120    avg_loss:0.025, val_acc:0.974]
Epoch [25/120    avg_loss:0.032, val_acc:0.966]
Epoch [26/120    avg_loss:0.023, val_acc:0.976]
Epoch [27/120    avg_loss:0.039, val_acc:0.956]
Epoch [28/120    avg_loss:0.029, val_acc:0.965]
Epoch [29/120    avg_loss:0.039, val_acc:0.974]
Epoch [30/120    avg_loss:0.018, val_acc:0.978]
Epoch [31/120    avg_loss:0.024, val_acc:0.968]
Epoch [32/120    avg_loss:0.018, val_acc:0.978]
Epoch [33/120    avg_loss:0.012, val_acc:0.977]
Epoch [34/120    avg_loss:0.023, val_acc:0.970]
Epoch [35/120    avg_loss:0.021, val_acc:0.982]
Epoch [36/120    avg_loss:0.011, val_acc:0.982]
Epoch [37/120    avg_loss:0.015, val_acc:0.981]
Epoch [38/120    avg_loss:0.018, val_acc:0.977]
Epoch [39/120    avg_loss:0.032, val_acc:0.981]
Epoch [40/120    avg_loss:0.016, val_acc:0.975]
Epoch [41/120    avg_loss:0.033, val_acc:0.977]
Epoch [42/120    avg_loss:0.037, val_acc:0.980]
Epoch [43/120    avg_loss:0.019, val_acc:0.973]
Epoch [44/120    avg_loss:0.015, val_acc:0.970]
Epoch [45/120    avg_loss:0.013, val_acc:0.985]
Epoch [46/120    avg_loss:0.009, val_acc:0.983]
Epoch [47/120    avg_loss:0.010, val_acc:0.982]
Epoch [48/120    avg_loss:0.022, val_acc:0.976]
Epoch [49/120    avg_loss:0.014, val_acc:0.980]
Epoch [50/120    avg_loss:0.008, val_acc:0.984]
Epoch [51/120    avg_loss:0.014, val_acc:0.959]
Epoch [52/120    avg_loss:0.010, val_acc:0.983]
Epoch [53/120    avg_loss:0.010, val_acc:0.980]
Epoch [54/120    avg_loss:0.008, val_acc:0.981]
Epoch [55/120    avg_loss:0.013, val_acc:0.982]
Epoch [56/120    avg_loss:0.021, val_acc:0.958]
Epoch [57/120    avg_loss:0.024, val_acc:0.980]
Epoch [58/120    avg_loss:0.012, val_acc:0.985]
Epoch [59/120    avg_loss:0.007, val_acc:0.987]
Epoch [60/120    avg_loss:0.009, val_acc:0.988]
Epoch [61/120    avg_loss:0.008, val_acc:0.982]
Epoch [62/120    avg_loss:0.010, val_acc:0.987]
Epoch [63/120    avg_loss:0.012, val_acc:0.974]
Epoch [64/120    avg_loss:0.012, val_acc:0.985]
Epoch [65/120    avg_loss:0.010, val_acc:0.984]
Epoch [66/120    avg_loss:0.009, val_acc:0.979]
Epoch [67/120    avg_loss:0.007, val_acc:0.988]
Epoch [68/120    avg_loss:0.007, val_acc:0.989]
Epoch [69/120    avg_loss:0.004, val_acc:0.989]
Epoch [70/120    avg_loss:0.006, val_acc:0.985]
Epoch [71/120    avg_loss:0.005, val_acc:0.987]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.003, val_acc:0.991]
Epoch [74/120    avg_loss:0.004, val_acc:0.989]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.005, val_acc:0.987]
Epoch [79/120    avg_loss:0.005, val_acc:0.985]
Epoch [80/120    avg_loss:0.024, val_acc:0.959]
Epoch [81/120    avg_loss:0.013, val_acc:0.988]
Epoch [82/120    avg_loss:0.004, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.021, val_acc:0.982]
Epoch [85/120    avg_loss:0.017, val_acc:0.973]
Epoch [86/120    avg_loss:0.031, val_acc:0.980]
Epoch [87/120    avg_loss:0.018, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.009, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     0     0     0     5    11     0     4]
 [    0     0 18056     0    21     0    13     0     0     0]
 [    0     1     0  2000     3     0     0     0    31     1]
 [    0    25     2     1  2920     0     0     0    24     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     3     0     0     0     0     2  1280     0     5]
 [    0     0     0     7    37     0     0     0  3526     1]
 [    0     0     0     0    11    31     0     0     0   877]]

Accuracy:
99.42399922878558

F1 scores:
[       nan 0.99619358 0.99900409 0.98911968 0.97920858 0.98826202
 0.99795417 0.99186362 0.9860179  0.97066962]

Kappa:
0.9923698266094906
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc82e90828>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.510, val_acc:0.374]
Epoch [2/120    avg_loss:0.888, val_acc:0.539]
Epoch [3/120    avg_loss:0.651, val_acc:0.581]
Epoch [4/120    avg_loss:0.514, val_acc:0.732]
Epoch [5/120    avg_loss:0.380, val_acc:0.784]
Epoch [6/120    avg_loss:0.364, val_acc:0.828]
Epoch [7/120    avg_loss:0.280, val_acc:0.808]
Epoch [8/120    avg_loss:0.254, val_acc:0.885]
Epoch [9/120    avg_loss:0.211, val_acc:0.897]
Epoch [10/120    avg_loss:0.199, val_acc:0.931]
Epoch [11/120    avg_loss:0.114, val_acc:0.951]
Epoch [12/120    avg_loss:0.137, val_acc:0.926]
Epoch [13/120    avg_loss:0.117, val_acc:0.953]
Epoch [14/120    avg_loss:0.131, val_acc:0.921]
Epoch [15/120    avg_loss:0.110, val_acc:0.957]
Epoch [16/120    avg_loss:0.081, val_acc:0.963]
Epoch [17/120    avg_loss:0.068, val_acc:0.967]
Epoch [18/120    avg_loss:0.065, val_acc:0.971]
Epoch [19/120    avg_loss:0.055, val_acc:0.967]
Epoch [20/120    avg_loss:0.042, val_acc:0.953]
Epoch [21/120    avg_loss:0.060, val_acc:0.972]
Epoch [22/120    avg_loss:0.037, val_acc:0.973]
Epoch [23/120    avg_loss:0.036, val_acc:0.969]
Epoch [24/120    avg_loss:0.033, val_acc:0.978]
Epoch [25/120    avg_loss:0.023, val_acc:0.971]
Epoch [26/120    avg_loss:0.020, val_acc:0.979]
Epoch [27/120    avg_loss:0.026, val_acc:0.976]
Epoch [28/120    avg_loss:0.031, val_acc:0.983]
Epoch [29/120    avg_loss:0.029, val_acc:0.978]
Epoch [30/120    avg_loss:0.033, val_acc:0.978]
Epoch [31/120    avg_loss:0.025, val_acc:0.978]
Epoch [32/120    avg_loss:0.027, val_acc:0.977]
Epoch [33/120    avg_loss:0.018, val_acc:0.978]
Epoch [34/120    avg_loss:0.049, val_acc:0.967]
Epoch [35/120    avg_loss:0.053, val_acc:0.964]
Epoch [36/120    avg_loss:0.021, val_acc:0.977]
Epoch [37/120    avg_loss:0.014, val_acc:0.979]
Epoch [38/120    avg_loss:0.014, val_acc:0.981]
Epoch [39/120    avg_loss:0.011, val_acc:0.978]
Epoch [40/120    avg_loss:0.015, val_acc:0.980]
Epoch [41/120    avg_loss:0.013, val_acc:0.982]
Epoch [42/120    avg_loss:0.011, val_acc:0.984]
Epoch [43/120    avg_loss:0.011, val_acc:0.980]
Epoch [44/120    avg_loss:0.012, val_acc:0.980]
Epoch [45/120    avg_loss:0.010, val_acc:0.980]
Epoch [46/120    avg_loss:0.008, val_acc:0.980]
Epoch [47/120    avg_loss:0.010, val_acc:0.980]
Epoch [48/120    avg_loss:0.009, val_acc:0.981]
Epoch [49/120    avg_loss:0.008, val_acc:0.981]
Epoch [50/120    avg_loss:0.008, val_acc:0.983]
Epoch [51/120    avg_loss:0.010, val_acc:0.981]
Epoch [52/120    avg_loss:0.008, val_acc:0.981]
Epoch [53/120    avg_loss:0.007, val_acc:0.981]
Epoch [54/120    avg_loss:0.011, val_acc:0.982]
Epoch [55/120    avg_loss:0.008, val_acc:0.981]
Epoch [56/120    avg_loss:0.007, val_acc:0.981]
Epoch [57/120    avg_loss:0.009, val_acc:0.981]
Epoch [58/120    avg_loss:0.005, val_acc:0.981]
Epoch [59/120    avg_loss:0.007, val_acc:0.981]
Epoch [60/120    avg_loss:0.008, val_acc:0.981]
Epoch [61/120    avg_loss:0.007, val_acc:0.981]
Epoch [62/120    avg_loss:0.008, val_acc:0.981]
Epoch [63/120    avg_loss:0.006, val_acc:0.981]
Epoch [64/120    avg_loss:0.010, val_acc:0.981]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.007, val_acc:0.981]
Epoch [67/120    avg_loss:0.008, val_acc:0.981]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.010, val_acc:0.981]
Epoch [70/120    avg_loss:0.005, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.007, val_acc:0.981]
Epoch [73/120    avg_loss:0.007, val_acc:0.981]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.981]
Epoch [77/120    avg_loss:0.008, val_acc:0.981]
Epoch [78/120    avg_loss:0.009, val_acc:0.981]
Epoch [79/120    avg_loss:0.007, val_acc:0.981]
Epoch [80/120    avg_loss:0.010, val_acc:0.981]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.006, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.012, val_acc:0.981]
Epoch [85/120    avg_loss:0.014, val_acc:0.981]
Epoch [86/120    avg_loss:0.008, val_acc:0.981]
Epoch [87/120    avg_loss:0.013, val_acc:0.981]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.010, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.009, val_acc:0.981]
Epoch [93/120    avg_loss:0.007, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.007, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.981]
Epoch [98/120    avg_loss:0.006, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.008, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.981]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.008, val_acc:0.981]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.011, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.006, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     0     0     0     0     0    31     2]
 [    0     0 18029     0    33     0    23     0     5     0]
 [    0     0     0  2030     0     0     0     0     3     3]
 [    0    32     2     0  2910     0     1     0    26     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     0     0  4846     0     9     0]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0     1     0     2    37     0     0     0  3522     9]
 [    0     0     0     2     3    41     0     0     0   873]]

Accuracy:
99.28662665991854

F1 scores:
[       nan 0.9948694  0.99762063 0.997543   0.97732997 0.98453414
 0.99405128 0.99727944 0.98283801 0.96357616]

Kappa:
0.9905513140256604
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d50fe4780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.533, val_acc:0.569]
Epoch [2/120    avg_loss:0.883, val_acc:0.561]
Epoch [3/120    avg_loss:0.610, val_acc:0.697]
Epoch [4/120    avg_loss:0.453, val_acc:0.757]
Epoch [5/120    avg_loss:0.368, val_acc:0.773]
Epoch [6/120    avg_loss:0.307, val_acc:0.803]
Epoch [7/120    avg_loss:0.228, val_acc:0.851]
Epoch [8/120    avg_loss:0.214, val_acc:0.932]
Epoch [9/120    avg_loss:0.178, val_acc:0.910]
Epoch [10/120    avg_loss:0.142, val_acc:0.934]
Epoch [11/120    avg_loss:0.111, val_acc:0.948]
Epoch [12/120    avg_loss:0.106, val_acc:0.951]
Epoch [13/120    avg_loss:0.086, val_acc:0.943]
Epoch [14/120    avg_loss:0.168, val_acc:0.928]
Epoch [15/120    avg_loss:0.129, val_acc:0.961]
Epoch [16/120    avg_loss:0.061, val_acc:0.957]
Epoch [17/120    avg_loss:0.053, val_acc:0.959]
Epoch [18/120    avg_loss:0.067, val_acc:0.969]
Epoch [19/120    avg_loss:0.048, val_acc:0.977]
Epoch [20/120    avg_loss:0.037, val_acc:0.973]
Epoch [21/120    avg_loss:0.059, val_acc:0.971]
Epoch [22/120    avg_loss:0.047, val_acc:0.962]
Epoch [23/120    avg_loss:0.062, val_acc:0.972]
Epoch [24/120    avg_loss:0.040, val_acc:0.969]
Epoch [25/120    avg_loss:0.036, val_acc:0.949]
Epoch [26/120    avg_loss:0.045, val_acc:0.972]
Epoch [27/120    avg_loss:0.031, val_acc:0.981]
Epoch [28/120    avg_loss:0.024, val_acc:0.980]
Epoch [29/120    avg_loss:0.034, val_acc:0.980]
Epoch [30/120    avg_loss:0.047, val_acc:0.961]
Epoch [31/120    avg_loss:0.029, val_acc:0.981]
Epoch [32/120    avg_loss:0.029, val_acc:0.983]
Epoch [33/120    avg_loss:0.016, val_acc:0.984]
Epoch [34/120    avg_loss:0.011, val_acc:0.983]
Epoch [35/120    avg_loss:0.021, val_acc:0.986]
Epoch [36/120    avg_loss:0.022, val_acc:0.984]
Epoch [37/120    avg_loss:0.035, val_acc:0.976]
Epoch [38/120    avg_loss:0.023, val_acc:0.978]
Epoch [39/120    avg_loss:0.028, val_acc:0.980]
Epoch [40/120    avg_loss:0.019, val_acc:0.981]
Epoch [41/120    avg_loss:0.014, val_acc:0.981]
Epoch [42/120    avg_loss:0.014, val_acc:0.986]
Epoch [43/120    avg_loss:0.010, val_acc:0.987]
Epoch [44/120    avg_loss:0.013, val_acc:0.976]
Epoch [45/120    avg_loss:0.021, val_acc:0.985]
Epoch [46/120    avg_loss:0.014, val_acc:0.984]
Epoch [47/120    avg_loss:0.009, val_acc:0.984]
Epoch [48/120    avg_loss:0.008, val_acc:0.988]
Epoch [49/120    avg_loss:0.009, val_acc:0.988]
Epoch [50/120    avg_loss:0.011, val_acc:0.986]
Epoch [51/120    avg_loss:0.010, val_acc:0.987]
Epoch [52/120    avg_loss:0.012, val_acc:0.988]
Epoch [53/120    avg_loss:0.009, val_acc:0.988]
Epoch [54/120    avg_loss:0.010, val_acc:0.988]
Epoch [55/120    avg_loss:0.008, val_acc:0.984]
Epoch [56/120    avg_loss:0.011, val_acc:0.989]
Epoch [57/120    avg_loss:0.007, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.986]
Epoch [59/120    avg_loss:0.008, val_acc:0.986]
Epoch [60/120    avg_loss:0.009, val_acc:0.985]
Epoch [61/120    avg_loss:0.007, val_acc:0.987]
Epoch [62/120    avg_loss:0.011, val_acc:0.987]
Epoch [63/120    avg_loss:0.055, val_acc:0.973]
Epoch [64/120    avg_loss:0.035, val_acc:0.982]
Epoch [65/120    avg_loss:0.014, val_acc:0.987]
Epoch [66/120    avg_loss:0.014, val_acc:0.985]
Epoch [67/120    avg_loss:0.006, val_acc:0.987]
Epoch [68/120    avg_loss:0.007, val_acc:0.991]
Epoch [69/120    avg_loss:0.006, val_acc:0.989]
Epoch [70/120    avg_loss:0.007, val_acc:0.986]
Epoch [71/120    avg_loss:0.006, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.006, val_acc:0.989]
Epoch [75/120    avg_loss:0.006, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.989]
Epoch [77/120    avg_loss:0.006, val_acc:0.990]
Epoch [78/120    avg_loss:0.004, val_acc:0.991]
Epoch [79/120    avg_loss:0.003, val_acc:0.992]
Epoch [80/120    avg_loss:0.007, val_acc:0.991]
Epoch [81/120    avg_loss:0.004, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.986]
Epoch [83/120    avg_loss:0.003, val_acc:0.990]
Epoch [84/120    avg_loss:0.003, val_acc:0.989]
Epoch [85/120    avg_loss:0.005, val_acc:0.989]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.016, val_acc:0.989]
Epoch [89/120    avg_loss:0.012, val_acc:0.980]
Epoch [90/120    avg_loss:0.015, val_acc:0.979]
Epoch [91/120    avg_loss:0.016, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.003, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.003, val_acc:0.990]
Epoch [103/120    avg_loss:0.003, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.990]
Epoch [106/120    avg_loss:0.003, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.990]
Epoch [109/120    avg_loss:0.003, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.002, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     0     0     0     0     2     0     3]
 [    0     0 18062     0     7     0    20     0     0     1]
 [    0     3     0  2004     1     0     0     0    25     3]
 [    0    39     3     0  2916     0     1     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4866     0     0     4]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     2     0    13    37     0     0     0  3515     4]
 [    0     0     0     1     4    31     0     0     0   883]]

Accuracy:
99.45291976959969

F1 scores:
[       nan 0.99620243 0.99892155 0.98865318 0.9823143  0.98826202
 0.9964165  0.99844961 0.98680517 0.97193176]

Kappa:
0.99275143443688
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f39dafb77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.590, val_acc:0.428]
Epoch [2/120    avg_loss:0.921, val_acc:0.496]
Epoch [3/120    avg_loss:0.651, val_acc:0.684]
Epoch [4/120    avg_loss:0.515, val_acc:0.742]
Epoch [5/120    avg_loss:0.367, val_acc:0.817]
Epoch [6/120    avg_loss:0.322, val_acc:0.796]
Epoch [7/120    avg_loss:0.242, val_acc:0.867]
Epoch [8/120    avg_loss:0.203, val_acc:0.928]
Epoch [9/120    avg_loss:0.167, val_acc:0.897]
Epoch [10/120    avg_loss:0.176, val_acc:0.934]
Epoch [11/120    avg_loss:0.094, val_acc:0.944]
Epoch [12/120    avg_loss:0.113, val_acc:0.949]
Epoch [13/120    avg_loss:0.132, val_acc:0.938]
Epoch [14/120    avg_loss:0.102, val_acc:0.966]
Epoch [15/120    avg_loss:0.089, val_acc:0.954]
Epoch [16/120    avg_loss:0.081, val_acc:0.965]
Epoch [17/120    avg_loss:0.057, val_acc:0.970]
Epoch [18/120    avg_loss:0.043, val_acc:0.971]
Epoch [19/120    avg_loss:0.052, val_acc:0.954]
Epoch [20/120    avg_loss:0.061, val_acc:0.976]
Epoch [21/120    avg_loss:0.035, val_acc:0.972]
Epoch [22/120    avg_loss:0.039, val_acc:0.976]
Epoch [23/120    avg_loss:0.037, val_acc:0.981]
Epoch [24/120    avg_loss:0.034, val_acc:0.981]
Epoch [25/120    avg_loss:0.033, val_acc:0.963]
Epoch [26/120    avg_loss:0.029, val_acc:0.976]
Epoch [27/120    avg_loss:0.031, val_acc:0.984]
Epoch [28/120    avg_loss:0.022, val_acc:0.984]
Epoch [29/120    avg_loss:0.028, val_acc:0.984]
Epoch [30/120    avg_loss:0.016, val_acc:0.982]
Epoch [31/120    avg_loss:0.023, val_acc:0.985]
Epoch [32/120    avg_loss:0.019, val_acc:0.986]
Epoch [33/120    avg_loss:0.017, val_acc:0.981]
Epoch [34/120    avg_loss:0.016, val_acc:0.984]
Epoch [35/120    avg_loss:0.023, val_acc:0.984]
Epoch [36/120    avg_loss:0.021, val_acc:0.982]
Epoch [37/120    avg_loss:0.012, val_acc:0.984]
Epoch [38/120    avg_loss:0.020, val_acc:0.988]
Epoch [39/120    avg_loss:0.011, val_acc:0.986]
Epoch [40/120    avg_loss:0.007, val_acc:0.990]
Epoch [41/120    avg_loss:0.008, val_acc:0.991]
Epoch [42/120    avg_loss:0.012, val_acc:0.987]
Epoch [43/120    avg_loss:0.007, val_acc:0.991]
Epoch [44/120    avg_loss:0.008, val_acc:0.991]
Epoch [45/120    avg_loss:0.007, val_acc:0.991]
Epoch [46/120    avg_loss:0.022, val_acc:0.984]
Epoch [47/120    avg_loss:0.028, val_acc:0.975]
Epoch [48/120    avg_loss:0.031, val_acc:0.974]
Epoch [49/120    avg_loss:0.023, val_acc:0.978]
Epoch [50/120    avg_loss:0.016, val_acc:0.986]
Epoch [51/120    avg_loss:0.009, val_acc:0.990]
Epoch [52/120    avg_loss:0.020, val_acc:0.985]
Epoch [53/120    avg_loss:0.011, val_acc:0.982]
Epoch [54/120    avg_loss:0.008, val_acc:0.991]
Epoch [55/120    avg_loss:0.008, val_acc:0.986]
Epoch [56/120    avg_loss:0.009, val_acc:0.995]
Epoch [57/120    avg_loss:0.008, val_acc:0.994]
Epoch [58/120    avg_loss:0.017, val_acc:0.986]
Epoch [59/120    avg_loss:0.013, val_acc:0.989]
Epoch [60/120    avg_loss:0.009, val_acc:0.991]
Epoch [61/120    avg_loss:0.011, val_acc:0.989]
Epoch [62/120    avg_loss:0.011, val_acc:0.987]
Epoch [63/120    avg_loss:0.009, val_acc:0.991]
Epoch [64/120    avg_loss:0.007, val_acc:0.987]
Epoch [65/120    avg_loss:0.009, val_acc:0.984]
Epoch [66/120    avg_loss:0.005, val_acc:0.986]
Epoch [67/120    avg_loss:0.009, val_acc:0.982]
Epoch [68/120    avg_loss:0.017, val_acc:0.991]
Epoch [69/120    avg_loss:0.013, val_acc:0.992]
Epoch [70/120    avg_loss:0.009, val_acc:0.995]
Epoch [71/120    avg_loss:0.006, val_acc:0.994]
Epoch [72/120    avg_loss:0.006, val_acc:0.994]
Epoch [73/120    avg_loss:0.006, val_acc:0.995]
Epoch [74/120    avg_loss:0.006, val_acc:0.994]
Epoch [75/120    avg_loss:0.007, val_acc:0.996]
Epoch [76/120    avg_loss:0.004, val_acc:0.995]
Epoch [77/120    avg_loss:0.003, val_acc:0.995]
Epoch [78/120    avg_loss:0.004, val_acc:0.995]
Epoch [79/120    avg_loss:0.004, val_acc:0.996]
Epoch [80/120    avg_loss:0.006, val_acc:0.996]
Epoch [81/120    avg_loss:0.004, val_acc:0.995]
Epoch [82/120    avg_loss:0.007, val_acc:0.994]
Epoch [83/120    avg_loss:0.005, val_acc:0.995]
Epoch [84/120    avg_loss:0.006, val_acc:0.995]
Epoch [85/120    avg_loss:0.005, val_acc:0.995]
Epoch [86/120    avg_loss:0.004, val_acc:0.996]
Epoch [87/120    avg_loss:0.008, val_acc:0.996]
Epoch [88/120    avg_loss:0.004, val_acc:0.996]
Epoch [89/120    avg_loss:0.005, val_acc:0.996]
Epoch [90/120    avg_loss:0.003, val_acc:0.994]
Epoch [91/120    avg_loss:0.006, val_acc:0.996]
Epoch [92/120    avg_loss:0.005, val_acc:0.996]
Epoch [93/120    avg_loss:0.004, val_acc:0.996]
Epoch [94/120    avg_loss:0.006, val_acc:0.996]
Epoch [95/120    avg_loss:0.003, val_acc:0.996]
Epoch [96/120    avg_loss:0.004, val_acc:0.996]
Epoch [97/120    avg_loss:0.004, val_acc:0.996]
Epoch [98/120    avg_loss:0.003, val_acc:0.996]
Epoch [99/120    avg_loss:0.003, val_acc:0.996]
Epoch [100/120    avg_loss:0.006, val_acc:0.996]
Epoch [101/120    avg_loss:0.004, val_acc:0.996]
Epoch [102/120    avg_loss:0.007, val_acc:0.994]
Epoch [103/120    avg_loss:0.003, val_acc:0.995]
Epoch [104/120    avg_loss:0.006, val_acc:0.995]
Epoch [105/120    avg_loss:0.004, val_acc:0.996]
Epoch [106/120    avg_loss:0.003, val_acc:0.996]
Epoch [107/120    avg_loss:0.004, val_acc:0.996]
Epoch [108/120    avg_loss:0.003, val_acc:0.996]
Epoch [109/120    avg_loss:0.004, val_acc:0.995]
Epoch [110/120    avg_loss:0.006, val_acc:0.995]
Epoch [111/120    avg_loss:0.003, val_acc:0.995]
Epoch [112/120    avg_loss:0.003, val_acc:0.996]
Epoch [113/120    avg_loss:0.003, val_acc:0.996]
Epoch [114/120    avg_loss:0.005, val_acc:0.995]
Epoch [115/120    avg_loss:0.006, val_acc:0.995]
Epoch [116/120    avg_loss:0.004, val_acc:0.995]
Epoch [117/120    avg_loss:0.004, val_acc:0.995]
Epoch [118/120    avg_loss:0.003, val_acc:0.996]
Epoch [119/120    avg_loss:0.003, val_acc:0.996]
Epoch [120/120    avg_loss:0.002, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     9 18074     0     4     0     0     0     3     0]
 [    0     0     0  2010     2     0     0     0    21     3]
 [    0    29     5     0  2922     0     0     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4872     0     0     1]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     0     0    13    45     0     0     0  3502    11]
 [    0     0     0     0     8    32     0     0     0   879]]

Accuracy:
99.49148049068518

F1 scores:
[       nan 0.99705472 0.99928125 0.99039172 0.9816899  0.98788796
 0.99917966 0.9984472  0.98467595 0.96859504]

Kappa:
0.9932616612825494
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb2f32ac780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.572, val_acc:0.496]
Epoch [2/120    avg_loss:0.937, val_acc:0.475]
Epoch [3/120    avg_loss:0.678, val_acc:0.772]
Epoch [4/120    avg_loss:0.447, val_acc:0.666]
Epoch [5/120    avg_loss:0.383, val_acc:0.769]
Epoch [6/120    avg_loss:0.280, val_acc:0.805]
Epoch [7/120    avg_loss:0.250, val_acc:0.868]
Epoch [8/120    avg_loss:0.256, val_acc:0.882]
Epoch [9/120    avg_loss:0.217, val_acc:0.876]
Epoch [10/120    avg_loss:0.150, val_acc:0.947]
Epoch [11/120    avg_loss:0.121, val_acc:0.903]
Epoch [12/120    avg_loss:0.100, val_acc:0.947]
Epoch [13/120    avg_loss:0.060, val_acc:0.961]
Epoch [14/120    avg_loss:0.083, val_acc:0.958]
Epoch [15/120    avg_loss:0.080, val_acc:0.944]
Epoch [16/120    avg_loss:0.087, val_acc:0.947]
Epoch [17/120    avg_loss:0.086, val_acc:0.959]
Epoch [18/120    avg_loss:0.062, val_acc:0.959]
Epoch [19/120    avg_loss:0.057, val_acc:0.950]
Epoch [20/120    avg_loss:0.056, val_acc:0.963]
Epoch [21/120    avg_loss:0.051, val_acc:0.962]
Epoch [22/120    avg_loss:0.040, val_acc:0.976]
Epoch [23/120    avg_loss:0.042, val_acc:0.976]
Epoch [24/120    avg_loss:0.039, val_acc:0.981]
Epoch [25/120    avg_loss:0.030, val_acc:0.959]
Epoch [26/120    avg_loss:0.026, val_acc:0.966]
Epoch [27/120    avg_loss:0.022, val_acc:0.977]
Epoch [28/120    avg_loss:0.033, val_acc:0.979]
Epoch [29/120    avg_loss:0.026, val_acc:0.975]
Epoch [30/120    avg_loss:0.032, val_acc:0.971]
Epoch [31/120    avg_loss:0.028, val_acc:0.974]
Epoch [32/120    avg_loss:0.019, val_acc:0.980]
Epoch [33/120    avg_loss:0.023, val_acc:0.978]
Epoch [34/120    avg_loss:0.043, val_acc:0.972]
Epoch [35/120    avg_loss:0.059, val_acc:0.979]
Epoch [36/120    avg_loss:0.025, val_acc:0.972]
Epoch [37/120    avg_loss:0.029, val_acc:0.980]
Epoch [38/120    avg_loss:0.017, val_acc:0.985]
Epoch [39/120    avg_loss:0.012, val_acc:0.985]
Epoch [40/120    avg_loss:0.013, val_acc:0.984]
Epoch [41/120    avg_loss:0.016, val_acc:0.984]
Epoch [42/120    avg_loss:0.012, val_acc:0.985]
Epoch [43/120    avg_loss:0.015, val_acc:0.987]
Epoch [44/120    avg_loss:0.013, val_acc:0.988]
Epoch [45/120    avg_loss:0.006, val_acc:0.988]
Epoch [46/120    avg_loss:0.010, val_acc:0.985]
Epoch [47/120    avg_loss:0.011, val_acc:0.990]
Epoch [48/120    avg_loss:0.010, val_acc:0.987]
Epoch [49/120    avg_loss:0.010, val_acc:0.986]
Epoch [50/120    avg_loss:0.014, val_acc:0.988]
Epoch [51/120    avg_loss:0.012, val_acc:0.990]
Epoch [52/120    avg_loss:0.012, val_acc:0.987]
Epoch [53/120    avg_loss:0.009, val_acc:0.990]
Epoch [54/120    avg_loss:0.010, val_acc:0.985]
Epoch [55/120    avg_loss:0.010, val_acc:0.986]
Epoch [56/120    avg_loss:0.012, val_acc:0.990]
Epoch [57/120    avg_loss:0.011, val_acc:0.989]
Epoch [58/120    avg_loss:0.008, val_acc:0.987]
Epoch [59/120    avg_loss:0.009, val_acc:0.991]
Epoch [60/120    avg_loss:0.008, val_acc:0.990]
Epoch [61/120    avg_loss:0.008, val_acc:0.989]
Epoch [62/120    avg_loss:0.007, val_acc:0.989]
Epoch [63/120    avg_loss:0.007, val_acc:0.989]
Epoch [64/120    avg_loss:0.007, val_acc:0.989]
Epoch [65/120    avg_loss:0.009, val_acc:0.989]
Epoch [66/120    avg_loss:0.008, val_acc:0.989]
Epoch [67/120    avg_loss:0.007, val_acc:0.990]
Epoch [68/120    avg_loss:0.007, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.991]
Epoch [70/120    avg_loss:0.007, val_acc:0.990]
Epoch [71/120    avg_loss:0.010, val_acc:0.990]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.011, val_acc:0.990]
Epoch [75/120    avg_loss:0.010, val_acc:0.991]
Epoch [76/120    avg_loss:0.008, val_acc:0.992]
Epoch [77/120    avg_loss:0.009, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.006, val_acc:0.989]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.010, val_acc:0.990]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.011, val_acc:0.991]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.989]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.013, val_acc:0.989]
Epoch [92/120    avg_loss:0.008, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.990]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.009, val_acc:0.990]
Epoch [98/120    avg_loss:0.009, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.009, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6422     0     0     0     0     8     0     0     2]
 [    0     0 18043     0    13     0    21     0     8     5]
 [    0     0     0  2032     1     0     0     0     2     1]
 [    0    33     4     0  2909     0     0     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     0     0     0     0     0     4  1285     0     1]
 [    0     0     4    10    46     0     0     0  3499    12]
 [    0     0     0     2     3    27     0     0     0   887]]

Accuracy:
99.4360494541248

F1 scores:
[       nan 0.9966633  0.99845056 0.99607843 0.97880215 0.98976109
 0.99652636 0.99805825 0.98480158 0.9709907 ]

Kappa:
0.9925300568449867
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e6fd7f780>
supervision:full
center_pixel:True
Network :
Number of parameter: 98128==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.482, val_acc:0.693]
Epoch [2/120    avg_loss:0.844, val_acc:0.532]
Epoch [3/120    avg_loss:0.545, val_acc:0.753]
Epoch [4/120    avg_loss:0.453, val_acc:0.809]
Epoch [5/120    avg_loss:0.359, val_acc:0.863]
Epoch [6/120    avg_loss:0.332, val_acc:0.850]
Epoch [7/120    avg_loss:0.273, val_acc:0.853]
Epoch [8/120    avg_loss:0.216, val_acc:0.936]
Epoch [9/120    avg_loss:0.148, val_acc:0.932]
Epoch [10/120    avg_loss:0.139, val_acc:0.910]
Epoch [11/120    avg_loss:0.124, val_acc:0.960]
Epoch [12/120    avg_loss:0.101, val_acc:0.914]
Epoch [13/120    avg_loss:0.103, val_acc:0.938]
Epoch [14/120    avg_loss:0.106, val_acc:0.939]
Epoch [15/120    avg_loss:0.057, val_acc:0.968]
Epoch [16/120    avg_loss:0.087, val_acc:0.967]
Epoch [17/120    avg_loss:0.051, val_acc:0.960]
Epoch [18/120    avg_loss:0.069, val_acc:0.943]
Epoch [19/120    avg_loss:0.097, val_acc:0.945]
Epoch [20/120    avg_loss:0.095, val_acc:0.947]
Epoch [21/120    avg_loss:0.058, val_acc:0.974]
Epoch [22/120    avg_loss:0.034, val_acc:0.980]
Epoch [23/120    avg_loss:0.038, val_acc:0.973]
Epoch [24/120    avg_loss:0.038, val_acc:0.968]
Epoch [25/120    avg_loss:0.037, val_acc:0.978]
Epoch [26/120    avg_loss:0.035, val_acc:0.977]
Epoch [27/120    avg_loss:0.063, val_acc:0.943]
Epoch [28/120    avg_loss:0.059, val_acc:0.953]
Epoch [29/120    avg_loss:0.051, val_acc:0.938]
Epoch [30/120    avg_loss:0.112, val_acc:0.941]
Epoch [31/120    avg_loss:0.070, val_acc:0.973]
Epoch [32/120    avg_loss:0.053, val_acc:0.962]
Epoch [33/120    avg_loss:0.033, val_acc:0.976]
Epoch [34/120    avg_loss:0.031, val_acc:0.974]
Epoch [35/120    avg_loss:0.027, val_acc:0.963]
Epoch [36/120    avg_loss:0.026, val_acc:0.981]
Epoch [37/120    avg_loss:0.016, val_acc:0.982]
Epoch [38/120    avg_loss:0.014, val_acc:0.983]
Epoch [39/120    avg_loss:0.010, val_acc:0.983]
Epoch [40/120    avg_loss:0.012, val_acc:0.984]
Epoch [41/120    avg_loss:0.014, val_acc:0.982]
Epoch [42/120    avg_loss:0.009, val_acc:0.983]
Epoch [43/120    avg_loss:0.014, val_acc:0.983]
Epoch [44/120    avg_loss:0.011, val_acc:0.983]
Epoch [45/120    avg_loss:0.011, val_acc:0.983]
Epoch [46/120    avg_loss:0.018, val_acc:0.983]
Epoch [47/120    avg_loss:0.012, val_acc:0.983]
Epoch [48/120    avg_loss:0.014, val_acc:0.983]
Epoch [49/120    avg_loss:0.014, val_acc:0.983]
Epoch [50/120    avg_loss:0.011, val_acc:0.984]
Epoch [51/120    avg_loss:0.013, val_acc:0.983]
Epoch [52/120    avg_loss:0.011, val_acc:0.982]
Epoch [53/120    avg_loss:0.013, val_acc:0.983]
Epoch [54/120    avg_loss:0.011, val_acc:0.984]
Epoch [55/120    avg_loss:0.022, val_acc:0.983]
Epoch [56/120    avg_loss:0.013, val_acc:0.983]
Epoch [57/120    avg_loss:0.016, val_acc:0.983]
Epoch [58/120    avg_loss:0.011, val_acc:0.983]
Epoch [59/120    avg_loss:0.009, val_acc:0.984]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.011, val_acc:0.983]
Epoch [62/120    avg_loss:0.011, val_acc:0.985]
Epoch [63/120    avg_loss:0.009, val_acc:0.983]
Epoch [64/120    avg_loss:0.014, val_acc:0.983]
Epoch [65/120    avg_loss:0.012, val_acc:0.981]
Epoch [66/120    avg_loss:0.010, val_acc:0.985]
Epoch [67/120    avg_loss:0.014, val_acc:0.983]
Epoch [68/120    avg_loss:0.010, val_acc:0.984]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.010, val_acc:0.983]
Epoch [73/120    avg_loss:0.015, val_acc:0.984]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.007, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.983]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.012, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.011, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.023, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     0     0     0    17     1    17     7]
 [    0     4 18046     0    23     0    12     0     5     0]
 [    0     6     0  2017     0     0     0     0     9     4]
 [    0    32     4     0  2915     0     0     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4867     0     0     2]
 [    0     0     0     0     0     0     2  1279     0     9]
 [    0     0     0     7    47     0     0     0  3515     2]
 [    0     0     0     0    16    27     0     0     0   876]]

Accuracy:
99.3179572458005

F1 scores:
[       nan 0.99347015 0.99842319 0.99359606 0.97605893 0.98976109
 0.99570376 0.99533074 0.98486971 0.96316658]

Kappa:
0.9909653785572428
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbedc7257f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.619, val_acc:0.634]
Epoch [2/120    avg_loss:0.963, val_acc:0.713]
Epoch [3/120    avg_loss:0.709, val_acc:0.653]
Epoch [4/120    avg_loss:0.499, val_acc:0.778]
Epoch [5/120    avg_loss:0.392, val_acc:0.819]
Epoch [6/120    avg_loss:0.349, val_acc:0.801]
Epoch [7/120    avg_loss:0.276, val_acc:0.854]
Epoch [8/120    avg_loss:0.220, val_acc:0.919]
Epoch [9/120    avg_loss:0.196, val_acc:0.907]
Epoch [10/120    avg_loss:0.190, val_acc:0.920]
Epoch [11/120    avg_loss:0.185, val_acc:0.887]
Epoch [12/120    avg_loss:0.125, val_acc:0.938]
Epoch [13/120    avg_loss:0.099, val_acc:0.941]
Epoch [14/120    avg_loss:0.087, val_acc:0.946]
Epoch [15/120    avg_loss:0.074, val_acc:0.959]
Epoch [16/120    avg_loss:0.074, val_acc:0.965]
Epoch [17/120    avg_loss:0.070, val_acc:0.960]
Epoch [18/120    avg_loss:0.072, val_acc:0.961]
Epoch [19/120    avg_loss:0.067, val_acc:0.934]
Epoch [20/120    avg_loss:0.056, val_acc:0.970]
Epoch [21/120    avg_loss:0.092, val_acc:0.969]
Epoch [22/120    avg_loss:0.124, val_acc:0.939]
Epoch [23/120    avg_loss:0.125, val_acc:0.972]
Epoch [24/120    avg_loss:0.066, val_acc:0.975]
Epoch [25/120    avg_loss:0.045, val_acc:0.971]
Epoch [26/120    avg_loss:0.041, val_acc:0.969]
Epoch [27/120    avg_loss:0.037, val_acc:0.975]
Epoch [28/120    avg_loss:0.051, val_acc:0.971]
Epoch [29/120    avg_loss:0.046, val_acc:0.971]
Epoch [30/120    avg_loss:0.083, val_acc:0.964]
Epoch [31/120    avg_loss:0.043, val_acc:0.972]
Epoch [32/120    avg_loss:0.045, val_acc:0.972]
Epoch [33/120    avg_loss:0.040, val_acc:0.975]
Epoch [34/120    avg_loss:0.050, val_acc:0.959]
Epoch [35/120    avg_loss:0.034, val_acc:0.967]
Epoch [36/120    avg_loss:0.025, val_acc:0.966]
Epoch [37/120    avg_loss:0.023, val_acc:0.977]
Epoch [38/120    avg_loss:0.039, val_acc:0.976]
Epoch [39/120    avg_loss:0.020, val_acc:0.982]
Epoch [40/120    avg_loss:0.018, val_acc:0.975]
Epoch [41/120    avg_loss:0.023, val_acc:0.980]
Epoch [42/120    avg_loss:0.021, val_acc:0.976]
Epoch [43/120    avg_loss:0.017, val_acc:0.982]
Epoch [44/120    avg_loss:0.016, val_acc:0.989]
Epoch [45/120    avg_loss:0.007, val_acc:0.987]
Epoch [46/120    avg_loss:0.007, val_acc:0.985]
Epoch [47/120    avg_loss:0.027, val_acc:0.970]
Epoch [48/120    avg_loss:0.039, val_acc:0.975]
Epoch [49/120    avg_loss:0.013, val_acc:0.985]
Epoch [50/120    avg_loss:0.011, val_acc:0.984]
Epoch [51/120    avg_loss:0.011, val_acc:0.986]
Epoch [52/120    avg_loss:0.020, val_acc:0.982]
Epoch [53/120    avg_loss:0.013, val_acc:0.982]
Epoch [54/120    avg_loss:0.018, val_acc:0.981]
Epoch [55/120    avg_loss:0.008, val_acc:0.985]
Epoch [56/120    avg_loss:0.008, val_acc:0.987]
Epoch [57/120    avg_loss:0.005, val_acc:0.987]
Epoch [58/120    avg_loss:0.008, val_acc:0.987]
Epoch [59/120    avg_loss:0.006, val_acc:0.987]
Epoch [60/120    avg_loss:0.005, val_acc:0.987]
Epoch [61/120    avg_loss:0.005, val_acc:0.987]
Epoch [62/120    avg_loss:0.011, val_acc:0.989]
Epoch [63/120    avg_loss:0.006, val_acc:0.987]
Epoch [64/120    avg_loss:0.005, val_acc:0.989]
Epoch [65/120    avg_loss:0.004, val_acc:0.987]
Epoch [66/120    avg_loss:0.009, val_acc:0.988]
Epoch [67/120    avg_loss:0.006, val_acc:0.988]
Epoch [68/120    avg_loss:0.005, val_acc:0.988]
Epoch [69/120    avg_loss:0.006, val_acc:0.987]
Epoch [70/120    avg_loss:0.005, val_acc:0.987]
Epoch [71/120    avg_loss:0.006, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.987]
Epoch [73/120    avg_loss:0.006, val_acc:0.987]
Epoch [74/120    avg_loss:0.005, val_acc:0.986]
Epoch [75/120    avg_loss:0.010, val_acc:0.989]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.004, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.013, val_acc:0.986]
Epoch [81/120    avg_loss:0.004, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.989]
Epoch [88/120    avg_loss:0.005, val_acc:0.989]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.987]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6417     0     0     4     0     0    11     0     0]
 [    0     2 18063     0    21     0     4     0     0     0]
 [    0     0     0  2031     0     0     0     0     1     4]
 [    0    36    15     0  2894     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4849     0     3    13]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     1     0    12    36     0     0     0  3522     0]
 [    0     0     0     2    14    34     0     0     0   869]]

Accuracy:
99.3854385077001

F1 scores:
[       nan 0.99581006 0.99847987 0.99534428 0.97424676 0.9871407
 0.99640399 0.99497876 0.98877035 0.96288089]

Kappa:
0.9918564944178692
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe601650748>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.481, val_acc:0.466]
Epoch [2/120    avg_loss:0.983, val_acc:0.585]
Epoch [3/120    avg_loss:0.742, val_acc:0.689]
Epoch [4/120    avg_loss:0.561, val_acc:0.760]
Epoch [5/120    avg_loss:0.430, val_acc:0.822]
Epoch [6/120    avg_loss:0.400, val_acc:0.851]
Epoch [7/120    avg_loss:0.336, val_acc:0.843]
Epoch [8/120    avg_loss:0.248, val_acc:0.905]
Epoch [9/120    avg_loss:0.174, val_acc:0.916]
Epoch [10/120    avg_loss:0.162, val_acc:0.914]
Epoch [11/120    avg_loss:0.116, val_acc:0.927]
Epoch [12/120    avg_loss:0.103, val_acc:0.940]
Epoch [13/120    avg_loss:0.092, val_acc:0.966]
Epoch [14/120    avg_loss:0.091, val_acc:0.960]
Epoch [15/120    avg_loss:0.106, val_acc:0.959]
Epoch [16/120    avg_loss:0.081, val_acc:0.965]
Epoch [17/120    avg_loss:0.056, val_acc:0.966]
Epoch [18/120    avg_loss:0.059, val_acc:0.970]
Epoch [19/120    avg_loss:0.080, val_acc:0.957]
Epoch [20/120    avg_loss:0.058, val_acc:0.972]
Epoch [21/120    avg_loss:0.053, val_acc:0.969]
Epoch [22/120    avg_loss:0.040, val_acc:0.967]
Epoch [23/120    avg_loss:0.045, val_acc:0.948]
Epoch [24/120    avg_loss:0.056, val_acc:0.969]
Epoch [25/120    avg_loss:0.025, val_acc:0.984]
Epoch [26/120    avg_loss:0.029, val_acc:0.984]
Epoch [27/120    avg_loss:0.027, val_acc:0.984]
Epoch [28/120    avg_loss:0.031, val_acc:0.946]
Epoch [29/120    avg_loss:0.038, val_acc:0.981]
Epoch [30/120    avg_loss:0.039, val_acc:0.917]
Epoch [31/120    avg_loss:0.059, val_acc:0.969]
Epoch [32/120    avg_loss:0.053, val_acc:0.970]
Epoch [33/120    avg_loss:0.026, val_acc:0.985]
Epoch [34/120    avg_loss:0.017, val_acc:0.984]
Epoch [35/120    avg_loss:0.020, val_acc:0.984]
Epoch [36/120    avg_loss:0.018, val_acc:0.981]
Epoch [37/120    avg_loss:0.011, val_acc:0.985]
Epoch [38/120    avg_loss:0.017, val_acc:0.984]
Epoch [39/120    avg_loss:0.012, val_acc:0.984]
Epoch [40/120    avg_loss:0.012, val_acc:0.982]
Epoch [41/120    avg_loss:0.011, val_acc:0.983]
Epoch [42/120    avg_loss:0.008, val_acc:0.984]
Epoch [43/120    avg_loss:0.012, val_acc:0.974]
Epoch [44/120    avg_loss:0.017, val_acc:0.990]
Epoch [45/120    avg_loss:0.028, val_acc:0.969]
Epoch [46/120    avg_loss:0.034, val_acc:0.982]
Epoch [47/120    avg_loss:0.024, val_acc:0.978]
Epoch [48/120    avg_loss:0.024, val_acc:0.970]
Epoch [49/120    avg_loss:0.043, val_acc:0.969]
Epoch [50/120    avg_loss:0.035, val_acc:0.978]
Epoch [51/120    avg_loss:0.025, val_acc:0.971]
Epoch [52/120    avg_loss:0.017, val_acc:0.984]
Epoch [53/120    avg_loss:0.014, val_acc:0.985]
Epoch [54/120    avg_loss:0.008, val_acc:0.988]
Epoch [55/120    avg_loss:0.009, val_acc:0.983]
Epoch [56/120    avg_loss:0.010, val_acc:0.988]
Epoch [57/120    avg_loss:0.009, val_acc:0.987]
Epoch [58/120    avg_loss:0.007, val_acc:0.987]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.005, val_acc:0.988]
Epoch [61/120    avg_loss:0.006, val_acc:0.987]
Epoch [62/120    avg_loss:0.005, val_acc:0.987]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.005, val_acc:0.986]
Epoch [65/120    avg_loss:0.005, val_acc:0.988]
Epoch [66/120    avg_loss:0.007, val_acc:0.987]
Epoch [67/120    avg_loss:0.010, val_acc:0.986]
Epoch [68/120    avg_loss:0.004, val_acc:0.987]
Epoch [69/120    avg_loss:0.007, val_acc:0.988]
Epoch [70/120    avg_loss:0.007, val_acc:0.987]
Epoch [71/120    avg_loss:0.006, val_acc:0.987]
Epoch [72/120    avg_loss:0.007, val_acc:0.987]
Epoch [73/120    avg_loss:0.005, val_acc:0.987]
Epoch [74/120    avg_loss:0.005, val_acc:0.987]
Epoch [75/120    avg_loss:0.003, val_acc:0.987]
Epoch [76/120    avg_loss:0.005, val_acc:0.987]
Epoch [77/120    avg_loss:0.012, val_acc:0.987]
Epoch [78/120    avg_loss:0.003, val_acc:0.987]
Epoch [79/120    avg_loss:0.005, val_acc:0.987]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.003, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.005, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     0     2     0    24    20     6     0]
 [    0     0 18024     0    43     0    21     0     2     0]
 [    0     0     0  2019     1     0     0     0    13     3]
 [    0    34    11     0  2903     0     1     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4860     0     0    13]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     1     0    10    45     0     0     0  3515     0]
 [    0     0     0     2    13    35     0     0     0   869]]

Accuracy:
99.20468512761188

F1 scores:
[       nan 0.99322799 0.99773042 0.99286944 0.9710654  0.98676749
 0.99325567 0.99153195 0.98597475 0.96341463]

Kappa:
0.9894679109452075
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f95d562a780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.541, val_acc:0.701]
Epoch [2/120    avg_loss:0.884, val_acc:0.484]
Epoch [3/120    avg_loss:0.639, val_acc:0.712]
Epoch [4/120    avg_loss:0.493, val_acc:0.772]
Epoch [5/120    avg_loss:0.425, val_acc:0.734]
Epoch [6/120    avg_loss:0.379, val_acc:0.822]
Epoch [7/120    avg_loss:0.245, val_acc:0.850]
Epoch [8/120    avg_loss:0.211, val_acc:0.880]
Epoch [9/120    avg_loss:0.235, val_acc:0.911]
Epoch [10/120    avg_loss:0.177, val_acc:0.909]
Epoch [11/120    avg_loss:0.128, val_acc:0.942]
Epoch [12/120    avg_loss:0.112, val_acc:0.952]
Epoch [13/120    avg_loss:0.115, val_acc:0.951]
Epoch [14/120    avg_loss:0.083, val_acc:0.955]
Epoch [15/120    avg_loss:0.099, val_acc:0.946]
Epoch [16/120    avg_loss:0.071, val_acc:0.957]
Epoch [17/120    avg_loss:0.068, val_acc:0.960]
Epoch [18/120    avg_loss:0.072, val_acc:0.963]
Epoch [19/120    avg_loss:0.069, val_acc:0.961]
Epoch [20/120    avg_loss:0.054, val_acc:0.939]
Epoch [21/120    avg_loss:0.078, val_acc:0.965]
Epoch [22/120    avg_loss:0.051, val_acc:0.959]
Epoch [23/120    avg_loss:0.106, val_acc:0.953]
Epoch [24/120    avg_loss:0.065, val_acc:0.953]
Epoch [25/120    avg_loss:0.057, val_acc:0.968]
Epoch [26/120    avg_loss:0.072, val_acc:0.957]
Epoch [27/120    avg_loss:0.048, val_acc:0.969]
Epoch [28/120    avg_loss:0.035, val_acc:0.969]
Epoch [29/120    avg_loss:0.022, val_acc:0.966]
Epoch [30/120    avg_loss:0.034, val_acc:0.973]
Epoch [31/120    avg_loss:0.031, val_acc:0.975]
Epoch [32/120    avg_loss:0.050, val_acc:0.965]
Epoch [33/120    avg_loss:0.045, val_acc:0.971]
Epoch [34/120    avg_loss:0.032, val_acc:0.964]
Epoch [35/120    avg_loss:0.026, val_acc:0.979]
Epoch [36/120    avg_loss:0.020, val_acc:0.980]
Epoch [37/120    avg_loss:0.016, val_acc:0.979]
Epoch [38/120    avg_loss:0.011, val_acc:0.980]
Epoch [39/120    avg_loss:0.015, val_acc:0.979]
Epoch [40/120    avg_loss:0.015, val_acc:0.984]
Epoch [41/120    avg_loss:0.065, val_acc:0.966]
Epoch [42/120    avg_loss:0.048, val_acc:0.980]
Epoch [43/120    avg_loss:0.026, val_acc:0.970]
Epoch [44/120    avg_loss:0.019, val_acc:0.978]
Epoch [45/120    avg_loss:0.013, val_acc:0.983]
Epoch [46/120    avg_loss:0.024, val_acc:0.976]
Epoch [47/120    avg_loss:0.022, val_acc:0.975]
Epoch [48/120    avg_loss:0.017, val_acc:0.979]
Epoch [49/120    avg_loss:0.011, val_acc:0.981]
Epoch [50/120    avg_loss:0.013, val_acc:0.978]
Epoch [51/120    avg_loss:0.025, val_acc:0.980]
Epoch [52/120    avg_loss:0.018, val_acc:0.982]
Epoch [53/120    avg_loss:0.011, val_acc:0.984]
Epoch [54/120    avg_loss:0.008, val_acc:0.984]
Epoch [55/120    avg_loss:0.007, val_acc:0.984]
Epoch [56/120    avg_loss:0.009, val_acc:0.984]
Epoch [57/120    avg_loss:0.009, val_acc:0.985]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.008, val_acc:0.984]
Epoch [60/120    avg_loss:0.006, val_acc:0.985]
Epoch [61/120    avg_loss:0.006, val_acc:0.986]
Epoch [62/120    avg_loss:0.008, val_acc:0.985]
Epoch [63/120    avg_loss:0.009, val_acc:0.985]
Epoch [64/120    avg_loss:0.006, val_acc:0.986]
Epoch [65/120    avg_loss:0.006, val_acc:0.986]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.010, val_acc:0.984]
Epoch [68/120    avg_loss:0.007, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.984]
Epoch [70/120    avg_loss:0.004, val_acc:0.984]
Epoch [71/120    avg_loss:0.005, val_acc:0.985]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.985]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.005, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.007, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.987]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.011, val_acc:0.987]
Epoch [99/120    avg_loss:0.005, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.003, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.003, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     2     0     0    10     8     0     3]
 [    0     1 18040     0    42     0     7     0     0     0]
 [    0     0     0  2030     0     0     0     0     1     5]
 [    0    45    19     0  2878     0     4     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4858     0     0    18]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     0     0     8    51     0     0     0  3505     7]
 [    0     0     0     2    14    53     0     0     0   850]]

Accuracy:
99.20227508254405

F1 scores:
[       nan 0.99464577 0.99803602 0.99558607 0.96625818 0.98009763
 0.99559381 0.99574468 0.98690694 0.94287299]

Kappa:
0.9894329730814824
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7462dcd7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.556, val_acc:0.356]
Epoch [2/120    avg_loss:1.040, val_acc:0.384]
Epoch [3/120    avg_loss:0.777, val_acc:0.659]
Epoch [4/120    avg_loss:0.587, val_acc:0.709]
Epoch [5/120    avg_loss:0.399, val_acc:0.775]
Epoch [6/120    avg_loss:0.320, val_acc:0.797]
Epoch [7/120    avg_loss:0.286, val_acc:0.791]
Epoch [8/120    avg_loss:0.252, val_acc:0.858]
Epoch [9/120    avg_loss:0.209, val_acc:0.813]
Epoch [10/120    avg_loss:0.232, val_acc:0.908]
Epoch [11/120    avg_loss:0.160, val_acc:0.897]
Epoch [12/120    avg_loss:0.120, val_acc:0.932]
Epoch [13/120    avg_loss:0.116, val_acc:0.937]
Epoch [14/120    avg_loss:0.099, val_acc:0.942]
Epoch [15/120    avg_loss:0.063, val_acc:0.962]
Epoch [16/120    avg_loss:0.066, val_acc:0.951]
Epoch [17/120    avg_loss:0.072, val_acc:0.964]
Epoch [18/120    avg_loss:0.063, val_acc:0.934]
Epoch [19/120    avg_loss:0.054, val_acc:0.965]
Epoch [20/120    avg_loss:0.045, val_acc:0.971]
Epoch [21/120    avg_loss:0.039, val_acc:0.971]
Epoch [22/120    avg_loss:0.042, val_acc:0.970]
Epoch [23/120    avg_loss:0.042, val_acc:0.965]
Epoch [24/120    avg_loss:0.045, val_acc:0.966]
Epoch [25/120    avg_loss:0.033, val_acc:0.966]
Epoch [26/120    avg_loss:0.051, val_acc:0.948]
Epoch [27/120    avg_loss:0.039, val_acc:0.962]
Epoch [28/120    avg_loss:0.035, val_acc:0.955]
Epoch [29/120    avg_loss:0.036, val_acc:0.924]
Epoch [30/120    avg_loss:0.040, val_acc:0.965]
Epoch [31/120    avg_loss:0.079, val_acc:0.963]
Epoch [32/120    avg_loss:0.059, val_acc:0.961]
Epoch [33/120    avg_loss:0.029, val_acc:0.961]
Epoch [34/120    avg_loss:0.030, val_acc:0.969]
Epoch [35/120    avg_loss:0.025, val_acc:0.968]
Epoch [36/120    avg_loss:0.037, val_acc:0.967]
Epoch [37/120    avg_loss:0.018, val_acc:0.967]
Epoch [38/120    avg_loss:0.013, val_acc:0.971]
Epoch [39/120    avg_loss:0.021, val_acc:0.969]
Epoch [40/120    avg_loss:0.018, val_acc:0.969]
Epoch [41/120    avg_loss:0.015, val_acc:0.967]
Epoch [42/120    avg_loss:0.016, val_acc:0.970]
Epoch [43/120    avg_loss:0.010, val_acc:0.972]
Epoch [44/120    avg_loss:0.022, val_acc:0.970]
Epoch [45/120    avg_loss:0.025, val_acc:0.969]
Epoch [46/120    avg_loss:0.015, val_acc:0.969]
Epoch [47/120    avg_loss:0.020, val_acc:0.970]
Epoch [48/120    avg_loss:0.011, val_acc:0.970]
Epoch [49/120    avg_loss:0.018, val_acc:0.969]
Epoch [50/120    avg_loss:0.013, val_acc:0.972]
Epoch [51/120    avg_loss:0.014, val_acc:0.972]
Epoch [52/120    avg_loss:0.018, val_acc:0.971]
Epoch [53/120    avg_loss:0.014, val_acc:0.972]
Epoch [54/120    avg_loss:0.016, val_acc:0.973]
Epoch [55/120    avg_loss:0.013, val_acc:0.973]
Epoch [56/120    avg_loss:0.011, val_acc:0.972]
Epoch [57/120    avg_loss:0.016, val_acc:0.972]
Epoch [58/120    avg_loss:0.015, val_acc:0.971]
Epoch [59/120    avg_loss:0.015, val_acc:0.971]
Epoch [60/120    avg_loss:0.014, val_acc:0.972]
Epoch [61/120    avg_loss:0.013, val_acc:0.973]
Epoch [62/120    avg_loss:0.009, val_acc:0.974]
Epoch [63/120    avg_loss:0.012, val_acc:0.975]
Epoch [64/120    avg_loss:0.013, val_acc:0.975]
Epoch [65/120    avg_loss:0.009, val_acc:0.972]
Epoch [66/120    avg_loss:0.012, val_acc:0.972]
Epoch [67/120    avg_loss:0.013, val_acc:0.973]
Epoch [68/120    avg_loss:0.010, val_acc:0.974]
Epoch [69/120    avg_loss:0.014, val_acc:0.972]
Epoch [70/120    avg_loss:0.013, val_acc:0.976]
Epoch [71/120    avg_loss:0.011, val_acc:0.971]
Epoch [72/120    avg_loss:0.010, val_acc:0.973]
Epoch [73/120    avg_loss:0.014, val_acc:0.974]
Epoch [74/120    avg_loss:0.012, val_acc:0.973]
Epoch [75/120    avg_loss:0.013, val_acc:0.976]
Epoch [76/120    avg_loss:0.013, val_acc:0.972]
Epoch [77/120    avg_loss:0.012, val_acc:0.972]
Epoch [78/120    avg_loss:0.010, val_acc:0.973]
Epoch [79/120    avg_loss:0.012, val_acc:0.974]
Epoch [80/120    avg_loss:0.012, val_acc:0.975]
Epoch [81/120    avg_loss:0.012, val_acc:0.973]
Epoch [82/120    avg_loss:0.011, val_acc:0.976]
Epoch [83/120    avg_loss:0.009, val_acc:0.977]
Epoch [84/120    avg_loss:0.016, val_acc:0.978]
Epoch [85/120    avg_loss:0.012, val_acc:0.977]
Epoch [86/120    avg_loss:0.012, val_acc:0.972]
Epoch [87/120    avg_loss:0.007, val_acc:0.972]
Epoch [88/120    avg_loss:0.008, val_acc:0.975]
Epoch [89/120    avg_loss:0.017, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.976]
Epoch [91/120    avg_loss:0.016, val_acc:0.975]
Epoch [92/120    avg_loss:0.017, val_acc:0.973]
Epoch [93/120    avg_loss:0.008, val_acc:0.973]
Epoch [94/120    avg_loss:0.009, val_acc:0.976]
Epoch [95/120    avg_loss:0.010, val_acc:0.972]
Epoch [96/120    avg_loss:0.013, val_acc:0.972]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.007, val_acc:0.975]
Epoch [99/120    avg_loss:0.011, val_acc:0.976]
Epoch [100/120    avg_loss:0.012, val_acc:0.975]
Epoch [101/120    avg_loss:0.013, val_acc:0.976]
Epoch [102/120    avg_loss:0.009, val_acc:0.976]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.010, val_acc:0.976]
Epoch [105/120    avg_loss:0.011, val_acc:0.976]
Epoch [106/120    avg_loss:0.013, val_acc:0.976]
Epoch [107/120    avg_loss:0.009, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.976]
Epoch [109/120    avg_loss:0.009, val_acc:0.977]
Epoch [110/120    avg_loss:0.010, val_acc:0.976]
Epoch [111/120    avg_loss:0.008, val_acc:0.976]
Epoch [112/120    avg_loss:0.008, val_acc:0.976]
Epoch [113/120    avg_loss:0.010, val_acc:0.976]
Epoch [114/120    avg_loss:0.009, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.977]
Epoch [116/120    avg_loss:0.011, val_acc:0.977]
Epoch [117/120    avg_loss:0.008, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.977]
Epoch [119/120    avg_loss:0.011, val_acc:0.977]
Epoch [120/120    avg_loss:0.010, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     4 17983     0    70     0    32     0     1     0]
 [    0     0     0  2031     3     0     0     0     0     2]
 [    0    42    15     0  2895     0     0     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4861     0     1     8]
 [    0     0     0     0     0     0     7  1281     0     2]
 [    0     0     0     2    56     0     0     0  3498    15]
 [    0     0     0     4    14    34     0     0     0   867]]

Accuracy:
99.18058467693346

F1 scores:
[       nan 0.99643687 0.99639849 0.99729929 0.96339434 0.9871407
 0.99427286 0.99649942 0.98660274 0.95642581]

Kappa:
0.9891513573248706
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f439e436748>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.549, val_acc:0.401]
Epoch [2/120    avg_loss:0.915, val_acc:0.735]
Epoch [3/120    avg_loss:0.695, val_acc:0.625]
Epoch [4/120    avg_loss:0.527, val_acc:0.819]
Epoch [5/120    avg_loss:0.395, val_acc:0.765]
Epoch [6/120    avg_loss:0.336, val_acc:0.922]
Epoch [7/120    avg_loss:0.259, val_acc:0.903]
Epoch [8/120    avg_loss:0.223, val_acc:0.929]
Epoch [9/120    avg_loss:0.174, val_acc:0.917]
Epoch [10/120    avg_loss:0.164, val_acc:0.950]
Epoch [11/120    avg_loss:0.131, val_acc:0.952]
Epoch [12/120    avg_loss:0.108, val_acc:0.952]
Epoch [13/120    avg_loss:0.112, val_acc:0.937]
Epoch [14/120    avg_loss:0.138, val_acc:0.953]
Epoch [15/120    avg_loss:0.083, val_acc:0.970]
Epoch [16/120    avg_loss:0.079, val_acc:0.949]
Epoch [17/120    avg_loss:0.088, val_acc:0.974]
Epoch [18/120    avg_loss:0.056, val_acc:0.970]
Epoch [19/120    avg_loss:0.052, val_acc:0.962]
Epoch [20/120    avg_loss:0.047, val_acc:0.969]
Epoch [21/120    avg_loss:0.056, val_acc:0.979]
Epoch [22/120    avg_loss:0.043, val_acc:0.954]
Epoch [23/120    avg_loss:0.038, val_acc:0.981]
Epoch [24/120    avg_loss:0.025, val_acc:0.978]
Epoch [25/120    avg_loss:0.033, val_acc:0.972]
Epoch [26/120    avg_loss:0.054, val_acc:0.976]
Epoch [27/120    avg_loss:0.032, val_acc:0.968]
Epoch [28/120    avg_loss:0.053, val_acc:0.961]
Epoch [29/120    avg_loss:0.038, val_acc:0.983]
Epoch [30/120    avg_loss:0.026, val_acc:0.982]
Epoch [31/120    avg_loss:0.023, val_acc:0.985]
Epoch [32/120    avg_loss:0.012, val_acc:0.990]
Epoch [33/120    avg_loss:0.013, val_acc:0.985]
Epoch [34/120    avg_loss:0.018, val_acc:0.981]
Epoch [35/120    avg_loss:0.016, val_acc:0.988]
Epoch [36/120    avg_loss:0.016, val_acc:0.983]
Epoch [37/120    avg_loss:0.014, val_acc:0.982]
Epoch [38/120    avg_loss:0.018, val_acc:0.983]
Epoch [39/120    avg_loss:0.009, val_acc:0.989]
Epoch [40/120    avg_loss:0.011, val_acc:0.984]
Epoch [41/120    avg_loss:0.009, val_acc:0.989]
Epoch [42/120    avg_loss:0.012, val_acc:0.984]
Epoch [43/120    avg_loss:0.022, val_acc:0.939]
Epoch [44/120    avg_loss:0.018, val_acc:0.984]
Epoch [45/120    avg_loss:0.012, val_acc:0.987]
Epoch [46/120    avg_loss:0.008, val_acc:0.987]
Epoch [47/120    avg_loss:0.009, val_acc:0.988]
Epoch [48/120    avg_loss:0.010, val_acc:0.987]
Epoch [49/120    avg_loss:0.008, val_acc:0.988]
Epoch [50/120    avg_loss:0.008, val_acc:0.987]
Epoch [51/120    avg_loss:0.009, val_acc:0.987]
Epoch [52/120    avg_loss:0.008, val_acc:0.987]
Epoch [53/120    avg_loss:0.009, val_acc:0.987]
Epoch [54/120    avg_loss:0.010, val_acc:0.989]
Epoch [55/120    avg_loss:0.010, val_acc:0.989]
Epoch [56/120    avg_loss:0.007, val_acc:0.989]
Epoch [57/120    avg_loss:0.007, val_acc:0.989]
Epoch [58/120    avg_loss:0.007, val_acc:0.989]
Epoch [59/120    avg_loss:0.007, val_acc:0.989]
Epoch [60/120    avg_loss:0.006, val_acc:0.989]
Epoch [61/120    avg_loss:0.008, val_acc:0.989]
Epoch [62/120    avg_loss:0.009, val_acc:0.989]
Epoch [63/120    avg_loss:0.010, val_acc:0.989]
Epoch [64/120    avg_loss:0.009, val_acc:0.989]
Epoch [65/120    avg_loss:0.007, val_acc:0.989]
Epoch [66/120    avg_loss:0.009, val_acc:0.989]
Epoch [67/120    avg_loss:0.009, val_acc:0.989]
Epoch [68/120    avg_loss:0.014, val_acc:0.989]
Epoch [69/120    avg_loss:0.004, val_acc:0.989]
Epoch [70/120    avg_loss:0.006, val_acc:0.989]
Epoch [71/120    avg_loss:0.009, val_acc:0.989]
Epoch [72/120    avg_loss:0.009, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.989]
Epoch [74/120    avg_loss:0.005, val_acc:0.989]
Epoch [75/120    avg_loss:0.006, val_acc:0.989]
Epoch [76/120    avg_loss:0.009, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.015, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.005, val_acc:0.989]
Epoch [81/120    avg_loss:0.008, val_acc:0.989]
Epoch [82/120    avg_loss:0.006, val_acc:0.989]
Epoch [83/120    avg_loss:0.009, val_acc:0.989]
Epoch [84/120    avg_loss:0.011, val_acc:0.989]
Epoch [85/120    avg_loss:0.007, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.989]
Epoch [87/120    avg_loss:0.005, val_acc:0.989]
Epoch [88/120    avg_loss:0.007, val_acc:0.989]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.009, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.989]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     3     0     0    10    11     0     3]
 [    0     9 17997     0    53     0    28     0     3     0]
 [    0     0     0  2028     1     0     0     0     5     2]
 [    0    35    11     0  2899     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4865     0     1    12]
 [    0     0     0     0     0     0     2  1284     0     4]
 [    0     2     0    17    42     0     0     0  3508     2]
 [    0     0     0     6    15    32     0     0     0   866]]

Accuracy:
99.19022485720483

F1 scores:
[       nan 0.99433362 0.99711895 0.99168704 0.96924106 0.98788796
 0.99458244 0.9934236  0.98608573 0.9579646 ]

Kappa:
0.9892795254182138
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc95666780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.583, val_acc:0.303]
Epoch [2/120    avg_loss:0.923, val_acc:0.496]
Epoch [3/120    avg_loss:0.688, val_acc:0.539]
Epoch [4/120    avg_loss:0.514, val_acc:0.749]
Epoch [5/120    avg_loss:0.409, val_acc:0.764]
Epoch [6/120    avg_loss:0.344, val_acc:0.872]
Epoch [7/120    avg_loss:0.307, val_acc:0.862]
Epoch [8/120    avg_loss:0.223, val_acc:0.915]
Epoch [9/120    avg_loss:0.202, val_acc:0.884]
Epoch [10/120    avg_loss:0.181, val_acc:0.916]
Epoch [11/120    avg_loss:0.115, val_acc:0.942]
Epoch [12/120    avg_loss:0.131, val_acc:0.947]
Epoch [13/120    avg_loss:0.100, val_acc:0.957]
Epoch [14/120    avg_loss:0.107, val_acc:0.951]
Epoch [15/120    avg_loss:0.135, val_acc:0.934]
Epoch [16/120    avg_loss:0.115, val_acc:0.951]
Epoch [17/120    avg_loss:0.105, val_acc:0.959]
Epoch [18/120    avg_loss:0.090, val_acc:0.966]
Epoch [19/120    avg_loss:0.065, val_acc:0.940]
Epoch [20/120    avg_loss:0.070, val_acc:0.959]
Epoch [21/120    avg_loss:0.053, val_acc:0.972]
Epoch [22/120    avg_loss:0.087, val_acc:0.951]
Epoch [23/120    avg_loss:0.062, val_acc:0.973]
Epoch [24/120    avg_loss:0.044, val_acc:0.976]
Epoch [25/120    avg_loss:0.028, val_acc:0.979]
Epoch [26/120    avg_loss:0.020, val_acc:0.978]
Epoch [27/120    avg_loss:0.022, val_acc:0.978]
Epoch [28/120    avg_loss:0.037, val_acc:0.977]
Epoch [29/120    avg_loss:0.033, val_acc:0.972]
Epoch [30/120    avg_loss:0.023, val_acc:0.982]
Epoch [31/120    avg_loss:0.022, val_acc:0.983]
Epoch [32/120    avg_loss:0.029, val_acc:0.979]
Epoch [33/120    avg_loss:0.021, val_acc:0.970]
Epoch [34/120    avg_loss:0.042, val_acc:0.960]
Epoch [35/120    avg_loss:0.043, val_acc:0.965]
Epoch [36/120    avg_loss:0.067, val_acc:0.968]
Epoch [37/120    avg_loss:0.069, val_acc:0.967]
Epoch [38/120    avg_loss:0.043, val_acc:0.981]
Epoch [39/120    avg_loss:0.020, val_acc:0.981]
Epoch [40/120    avg_loss:0.017, val_acc:0.983]
Epoch [41/120    avg_loss:0.014, val_acc:0.988]
Epoch [42/120    avg_loss:0.034, val_acc:0.976]
Epoch [43/120    avg_loss:0.038, val_acc:0.972]
Epoch [44/120    avg_loss:0.022, val_acc:0.984]
Epoch [45/120    avg_loss:0.015, val_acc:0.983]
Epoch [46/120    avg_loss:0.046, val_acc:0.973]
Epoch [47/120    avg_loss:0.029, val_acc:0.966]
Epoch [48/120    avg_loss:0.015, val_acc:0.978]
Epoch [49/120    avg_loss:0.024, val_acc:0.982]
Epoch [50/120    avg_loss:0.023, val_acc:0.967]
Epoch [51/120    avg_loss:0.038, val_acc:0.951]
Epoch [52/120    avg_loss:0.028, val_acc:0.972]
Epoch [53/120    avg_loss:0.031, val_acc:0.976]
Epoch [54/120    avg_loss:0.014, val_acc:0.982]
Epoch [55/120    avg_loss:0.014, val_acc:0.983]
Epoch [56/120    avg_loss:0.011, val_acc:0.985]
Epoch [57/120    avg_loss:0.018, val_acc:0.988]
Epoch [58/120    avg_loss:0.007, val_acc:0.988]
Epoch [59/120    avg_loss:0.010, val_acc:0.985]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.011, val_acc:0.986]
Epoch [62/120    avg_loss:0.009, val_acc:0.987]
Epoch [63/120    avg_loss:0.009, val_acc:0.988]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.010, val_acc:0.988]
Epoch [66/120    avg_loss:0.010, val_acc:0.989]
Epoch [67/120    avg_loss:0.007, val_acc:0.989]
Epoch [68/120    avg_loss:0.011, val_acc:0.988]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.006, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.988]
Epoch [72/120    avg_loss:0.007, val_acc:0.987]
Epoch [73/120    avg_loss:0.011, val_acc:0.988]
Epoch [74/120    avg_loss:0.010, val_acc:0.986]
Epoch [75/120    avg_loss:0.011, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.005, val_acc:0.990]
Epoch [80/120    avg_loss:0.008, val_acc:0.989]
Epoch [81/120    avg_loss:0.007, val_acc:0.989]
Epoch [82/120    avg_loss:0.012, val_acc:0.990]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.990]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.991]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.991]
Epoch [94/120    avg_loss:0.005, val_acc:0.991]
Epoch [95/120    avg_loss:0.007, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.007, val_acc:0.991]
Epoch [99/120    avg_loss:0.006, val_acc:0.991]
Epoch [100/120    avg_loss:0.008, val_acc:0.991]
Epoch [101/120    avg_loss:0.006, val_acc:0.991]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.014, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.006, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     0     0     6     0     0     7]
 [    0     0 18048     0    40     0     2     0     0     0]
 [    0     0     0  2021     1     0     0     0    13     1]
 [    0    25     9     0  2920     0     0     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4846     0    12    12]
 [    0     0     0     0     0     0     5  1280     0     5]
 [    0     1     0     0    66     0     0     0  3492    12]
 [    0     0     0     0    16    48     0     0     1   854]]

Accuracy:
99.25770611910443

F1 scores:
[       nan 0.99697134 0.99836814 0.99630269 0.97090607 0.98194131
 0.99537845 0.99610895 0.98269312 0.94364641]

Kappa:
0.9901670472487726
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f643790f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.524, val_acc:0.364]
Epoch [2/120    avg_loss:0.881, val_acc:0.460]
Epoch [3/120    avg_loss:0.749, val_acc:0.707]
Epoch [4/120    avg_loss:0.550, val_acc:0.778]
Epoch [5/120    avg_loss:0.411, val_acc:0.841]
Epoch [6/120    avg_loss:0.330, val_acc:0.827]
Epoch [7/120    avg_loss:0.232, val_acc:0.924]
Epoch [8/120    avg_loss:0.178, val_acc:0.917]
Epoch [9/120    avg_loss:0.180, val_acc:0.944]
Epoch [10/120    avg_loss:0.149, val_acc:0.901]
Epoch [11/120    avg_loss:0.132, val_acc:0.956]
Epoch [12/120    avg_loss:0.119, val_acc:0.952]
Epoch [13/120    avg_loss:0.130, val_acc:0.916]
Epoch [14/120    avg_loss:0.099, val_acc:0.959]
Epoch [15/120    avg_loss:0.077, val_acc:0.963]
Epoch [16/120    avg_loss:0.064, val_acc:0.967]
Epoch [17/120    avg_loss:0.066, val_acc:0.970]
Epoch [18/120    avg_loss:0.082, val_acc:0.971]
Epoch [19/120    avg_loss:0.053, val_acc:0.958]
Epoch [20/120    avg_loss:0.048, val_acc:0.953]
Epoch [21/120    avg_loss:0.084, val_acc:0.941]
Epoch [22/120    avg_loss:0.096, val_acc:0.945]
Epoch [23/120    avg_loss:0.054, val_acc:0.972]
Epoch [24/120    avg_loss:0.047, val_acc:0.972]
Epoch [25/120    avg_loss:0.060, val_acc:0.921]
Epoch [26/120    avg_loss:0.041, val_acc:0.965]
Epoch [27/120    avg_loss:0.067, val_acc:0.957]
Epoch [28/120    avg_loss:0.034, val_acc:0.970]
Epoch [29/120    avg_loss:0.043, val_acc:0.911]
Epoch [30/120    avg_loss:0.031, val_acc:0.971]
Epoch [31/120    avg_loss:0.029, val_acc:0.977]
Epoch [32/120    avg_loss:0.022, val_acc:0.978]
Epoch [33/120    avg_loss:0.029, val_acc:0.974]
Epoch [34/120    avg_loss:0.029, val_acc:0.966]
Epoch [35/120    avg_loss:0.018, val_acc:0.981]
Epoch [36/120    avg_loss:0.016, val_acc:0.981]
Epoch [37/120    avg_loss:0.028, val_acc:0.980]
Epoch [38/120    avg_loss:0.021, val_acc:0.984]
Epoch [39/120    avg_loss:0.017, val_acc:0.981]
Epoch [40/120    avg_loss:0.023, val_acc:0.979]
Epoch [41/120    avg_loss:0.015, val_acc:0.978]
Epoch [42/120    avg_loss:0.013, val_acc:0.985]
Epoch [43/120    avg_loss:0.009, val_acc:0.983]
Epoch [44/120    avg_loss:0.010, val_acc:0.985]
Epoch [45/120    avg_loss:0.011, val_acc:0.983]
Epoch [46/120    avg_loss:0.016, val_acc:0.984]
Epoch [47/120    avg_loss:0.014, val_acc:0.980]
Epoch [48/120    avg_loss:0.009, val_acc:0.988]
Epoch [49/120    avg_loss:0.013, val_acc:0.990]
Epoch [50/120    avg_loss:0.005, val_acc:0.987]
Epoch [51/120    avg_loss:0.009, val_acc:0.986]
Epoch [52/120    avg_loss:0.013, val_acc:0.986]
Epoch [53/120    avg_loss:0.010, val_acc:0.985]
Epoch [54/120    avg_loss:0.012, val_acc:0.980]
Epoch [55/120    avg_loss:0.017, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.982]
Epoch [57/120    avg_loss:0.009, val_acc:0.981]
Epoch [58/120    avg_loss:0.017, val_acc:0.981]
Epoch [59/120    avg_loss:0.019, val_acc:0.974]
Epoch [60/120    avg_loss:0.005, val_acc:0.984]
Epoch [61/120    avg_loss:0.006, val_acc:0.985]
Epoch [62/120    avg_loss:0.006, val_acc:0.985]
Epoch [63/120    avg_loss:0.005, val_acc:0.984]
Epoch [64/120    avg_loss:0.007, val_acc:0.985]
Epoch [65/120    avg_loss:0.005, val_acc:0.986]
Epoch [66/120    avg_loss:0.004, val_acc:0.986]
Epoch [67/120    avg_loss:0.004, val_acc:0.986]
Epoch [68/120    avg_loss:0.007, val_acc:0.987]
Epoch [69/120    avg_loss:0.006, val_acc:0.987]
Epoch [70/120    avg_loss:0.004, val_acc:0.989]
Epoch [71/120    avg_loss:0.003, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.008, val_acc:0.990]
Epoch [74/120    avg_loss:0.005, val_acc:0.988]
Epoch [75/120    avg_loss:0.004, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.987]
Epoch [77/120    avg_loss:0.005, val_acc:0.987]
Epoch [78/120    avg_loss:0.004, val_acc:0.988]
Epoch [79/120    avg_loss:0.004, val_acc:0.988]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.004, val_acc:0.988]
Epoch [82/120    avg_loss:0.003, val_acc:0.988]
Epoch [83/120    avg_loss:0.010, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.990]
Epoch [85/120    avg_loss:0.003, val_acc:0.990]
Epoch [86/120    avg_loss:0.004, val_acc:0.989]
Epoch [87/120    avg_loss:0.005, val_acc:0.989]
Epoch [88/120    avg_loss:0.005, val_acc:0.989]
Epoch [89/120    avg_loss:0.004, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.003, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.009, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.006, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.990]
Epoch [102/120    avg_loss:0.003, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.003, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.003, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.003, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     6     0     0     0     3     0]
 [    0     0 18042     0    29     0    18     0     1     0]
 [    0     2     0  2020     1     0     0     0     7     6]
 [    0    26    13     0  2916     0     0     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4861     0     0    14]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     2     0     1    61     0     0     0  3495    12]
 [    0     0     0     0    14    32     0     0     0   873]]

Accuracy:
99.3516978767503

F1 scores:
[       nan 0.99697322 0.9982295  0.99580971 0.97216203 0.98788796
 0.99631072 0.99961225 0.98533972 0.95723684]

Kappa:
0.9914125459468986
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcc7ecc1780>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.523, val_acc:0.394]
Epoch [2/120    avg_loss:0.984, val_acc:0.714]
Epoch [3/120    avg_loss:0.738, val_acc:0.692]
Epoch [4/120    avg_loss:0.571, val_acc:0.766]
Epoch [5/120    avg_loss:0.400, val_acc:0.822]
Epoch [6/120    avg_loss:0.311, val_acc:0.917]
Epoch [7/120    avg_loss:0.253, val_acc:0.891]
Epoch [8/120    avg_loss:0.248, val_acc:0.933]
Epoch [9/120    avg_loss:0.205, val_acc:0.954]
Epoch [10/120    avg_loss:0.147, val_acc:0.955]
Epoch [11/120    avg_loss:0.113, val_acc:0.970]
Epoch [12/120    avg_loss:0.117, val_acc:0.971]
Epoch [13/120    avg_loss:0.095, val_acc:0.969]
Epoch [14/120    avg_loss:0.076, val_acc:0.930]
Epoch [15/120    avg_loss:0.127, val_acc:0.972]
Epoch [16/120    avg_loss:0.101, val_acc:0.965]
Epoch [17/120    avg_loss:0.085, val_acc:0.966]
Epoch [18/120    avg_loss:0.063, val_acc:0.972]
Epoch [19/120    avg_loss:0.040, val_acc:0.977]
Epoch [20/120    avg_loss:0.041, val_acc:0.987]
Epoch [21/120    avg_loss:0.052, val_acc:0.963]
Epoch [22/120    avg_loss:0.040, val_acc:0.990]
Epoch [23/120    avg_loss:0.051, val_acc:0.987]
Epoch [24/120    avg_loss:0.078, val_acc:0.962]
Epoch [25/120    avg_loss:0.049, val_acc:0.987]
Epoch [26/120    avg_loss:0.045, val_acc:0.981]
Epoch [27/120    avg_loss:0.065, val_acc:0.975]
Epoch [28/120    avg_loss:0.039, val_acc:0.986]
Epoch [29/120    avg_loss:0.027, val_acc:0.988]
Epoch [30/120    avg_loss:0.040, val_acc:0.988]
Epoch [31/120    avg_loss:0.020, val_acc:0.989]
Epoch [32/120    avg_loss:0.038, val_acc:0.966]
Epoch [33/120    avg_loss:0.030, val_acc:0.989]
Epoch [34/120    avg_loss:0.016, val_acc:0.990]
Epoch [35/120    avg_loss:0.024, val_acc:0.984]
Epoch [36/120    avg_loss:0.012, val_acc:0.991]
Epoch [37/120    avg_loss:0.018, val_acc:0.989]
Epoch [38/120    avg_loss:0.013, val_acc:0.991]
Epoch [39/120    avg_loss:0.013, val_acc:0.991]
Epoch [40/120    avg_loss:0.013, val_acc:0.989]
Epoch [41/120    avg_loss:0.015, val_acc:0.992]
Epoch [42/120    avg_loss:0.010, val_acc:0.990]
Epoch [43/120    avg_loss:0.008, val_acc:0.993]
Epoch [44/120    avg_loss:0.021, val_acc:0.975]
Epoch [45/120    avg_loss:0.012, val_acc:0.990]
Epoch [46/120    avg_loss:0.022, val_acc:0.977]
Epoch [47/120    avg_loss:0.017, val_acc:0.987]
Epoch [48/120    avg_loss:0.008, val_acc:0.992]
Epoch [49/120    avg_loss:0.011, val_acc:0.989]
Epoch [50/120    avg_loss:0.035, val_acc:0.982]
Epoch [51/120    avg_loss:0.008, val_acc:0.987]
Epoch [52/120    avg_loss:0.009, val_acc:0.990]
Epoch [53/120    avg_loss:0.008, val_acc:0.989]
Epoch [54/120    avg_loss:0.011, val_acc:0.992]
Epoch [55/120    avg_loss:0.009, val_acc:0.991]
Epoch [56/120    avg_loss:0.010, val_acc:0.989]
Epoch [57/120    avg_loss:0.010, val_acc:0.991]
Epoch [58/120    avg_loss:0.005, val_acc:0.991]
Epoch [59/120    avg_loss:0.007, val_acc:0.991]
Epoch [60/120    avg_loss:0.006, val_acc:0.991]
Epoch [61/120    avg_loss:0.007, val_acc:0.991]
Epoch [62/120    avg_loss:0.006, val_acc:0.991]
Epoch [63/120    avg_loss:0.012, val_acc:0.991]
Epoch [64/120    avg_loss:0.005, val_acc:0.991]
Epoch [65/120    avg_loss:0.006, val_acc:0.991]
Epoch [66/120    avg_loss:0.006, val_acc:0.991]
Epoch [67/120    avg_loss:0.004, val_acc:0.991]
Epoch [68/120    avg_loss:0.005, val_acc:0.991]
Epoch [69/120    avg_loss:0.006, val_acc:0.991]
Epoch [70/120    avg_loss:0.007, val_acc:0.991]
Epoch [71/120    avg_loss:0.005, val_acc:0.991]
Epoch [72/120    avg_loss:0.006, val_acc:0.991]
Epoch [73/120    avg_loss:0.005, val_acc:0.991]
Epoch [74/120    avg_loss:0.006, val_acc:0.991]
Epoch [75/120    avg_loss:0.004, val_acc:0.991]
Epoch [76/120    avg_loss:0.007, val_acc:0.991]
Epoch [77/120    avg_loss:0.004, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.010, val_acc:0.991]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.005, val_acc:0.991]
Epoch [82/120    avg_loss:0.007, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.004, val_acc:0.991]
Epoch [86/120    avg_loss:0.006, val_acc:0.991]
Epoch [87/120    avg_loss:0.005, val_acc:0.991]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.004, val_acc:0.991]
Epoch [91/120    avg_loss:0.003, val_acc:0.991]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.005, val_acc:0.991]
Epoch [95/120    avg_loss:0.003, val_acc:0.991]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.008, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.006, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.010, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.007, val_acc:0.991]
Epoch [110/120    avg_loss:0.009, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.007, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.009, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     2     0     0    15     0     0     5]
 [    0     2 18010     0    54     0    21     0     3     0]
 [    0     0     0  2024     1     0     0     0     0    11]
 [    0    34     8     0  2898     0     5     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4856     0     3    18]
 [    0     0     0     0     0     0     8  1278     0     4]
 [    0     2     0     0    35     0     0     0  3512    22]
 [    0     0     0     5    14    52     0     0     0   848]]

Accuracy:
99.15166413611935

F1 scores:
[       nan 0.99534161 0.99753524 0.99532825 0.97020422 0.98046582
 0.99274251 0.9953271  0.98707139 0.92829776]

Kappa:
0.9887678667506697
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fadcc695710>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.677, val_acc:0.457]
Epoch [2/120    avg_loss:0.917, val_acc:0.487]
Epoch [3/120    avg_loss:0.654, val_acc:0.731]
Epoch [4/120    avg_loss:0.495, val_acc:0.718]
Epoch [5/120    avg_loss:0.363, val_acc:0.806]
Epoch [6/120    avg_loss:0.333, val_acc:0.816]
Epoch [7/120    avg_loss:0.316, val_acc:0.818]
Epoch [8/120    avg_loss:0.269, val_acc:0.838]
Epoch [9/120    avg_loss:0.220, val_acc:0.879]
Epoch [10/120    avg_loss:0.196, val_acc:0.923]
Epoch [11/120    avg_loss:0.152, val_acc:0.923]
Epoch [12/120    avg_loss:0.146, val_acc:0.934]
Epoch [13/120    avg_loss:0.158, val_acc:0.899]
Epoch [14/120    avg_loss:0.105, val_acc:0.938]
Epoch [15/120    avg_loss:0.102, val_acc:0.957]
Epoch [16/120    avg_loss:0.092, val_acc:0.971]
Epoch [17/120    avg_loss:0.076, val_acc:0.954]
Epoch [18/120    avg_loss:0.103, val_acc:0.959]
Epoch [19/120    avg_loss:0.070, val_acc:0.969]
Epoch [20/120    avg_loss:0.062, val_acc:0.966]
Epoch [21/120    avg_loss:0.095, val_acc:0.951]
Epoch [22/120    avg_loss:0.072, val_acc:0.928]
Epoch [23/120    avg_loss:0.069, val_acc:0.972]
Epoch [24/120    avg_loss:0.042, val_acc:0.974]
Epoch [25/120    avg_loss:0.025, val_acc:0.972]
Epoch [26/120    avg_loss:0.026, val_acc:0.978]
Epoch [27/120    avg_loss:0.028, val_acc:0.975]
Epoch [28/120    avg_loss:0.031, val_acc:0.959]
Epoch [29/120    avg_loss:0.040, val_acc:0.968]
Epoch [30/120    avg_loss:0.043, val_acc:0.968]
Epoch [31/120    avg_loss:0.032, val_acc:0.969]
Epoch [32/120    avg_loss:0.023, val_acc:0.978]
Epoch [33/120    avg_loss:0.021, val_acc:0.984]
Epoch [34/120    avg_loss:0.044, val_acc:0.970]
Epoch [35/120    avg_loss:0.029, val_acc:0.984]
Epoch [36/120    avg_loss:0.024, val_acc:0.976]
Epoch [37/120    avg_loss:0.023, val_acc:0.981]
Epoch [38/120    avg_loss:0.019, val_acc:0.982]
Epoch [39/120    avg_loss:0.028, val_acc:0.980]
Epoch [40/120    avg_loss:0.015, val_acc:0.984]
Epoch [41/120    avg_loss:0.011, val_acc:0.982]
Epoch [42/120    avg_loss:0.015, val_acc:0.984]
Epoch [43/120    avg_loss:0.015, val_acc:0.982]
Epoch [44/120    avg_loss:0.011, val_acc:0.980]
Epoch [45/120    avg_loss:0.016, val_acc:0.983]
Epoch [46/120    avg_loss:0.012, val_acc:0.985]
Epoch [47/120    avg_loss:0.010, val_acc:0.985]
Epoch [48/120    avg_loss:0.022, val_acc:0.959]
Epoch [49/120    avg_loss:0.037, val_acc:0.980]
Epoch [50/120    avg_loss:0.019, val_acc:0.981]
Epoch [51/120    avg_loss:0.020, val_acc:0.979]
Epoch [52/120    avg_loss:0.022, val_acc:0.966]
Epoch [53/120    avg_loss:0.021, val_acc:0.978]
Epoch [54/120    avg_loss:0.044, val_acc:0.981]
Epoch [55/120    avg_loss:0.017, val_acc:0.982]
Epoch [56/120    avg_loss:0.010, val_acc:0.984]
Epoch [57/120    avg_loss:0.010, val_acc:0.980]
Epoch [58/120    avg_loss:0.012, val_acc:0.979]
Epoch [59/120    avg_loss:0.006, val_acc:0.985]
Epoch [60/120    avg_loss:0.007, val_acc:0.985]
Epoch [61/120    avg_loss:0.006, val_acc:0.989]
Epoch [62/120    avg_loss:0.009, val_acc:0.957]
Epoch [63/120    avg_loss:0.029, val_acc:0.982]
Epoch [64/120    avg_loss:0.010, val_acc:0.984]
Epoch [65/120    avg_loss:0.008, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.982]
Epoch [67/120    avg_loss:0.006, val_acc:0.975]
Epoch [68/120    avg_loss:0.013, val_acc:0.982]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.006, val_acc:0.985]
Epoch [71/120    avg_loss:0.006, val_acc:0.984]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.008, val_acc:0.984]
Epoch [74/120    avg_loss:0.004, val_acc:0.989]
Epoch [75/120    avg_loss:0.004, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.006, val_acc:0.987]
Epoch [78/120    avg_loss:0.011, val_acc:0.974]
Epoch [79/120    avg_loss:0.008, val_acc:0.986]
Epoch [80/120    avg_loss:0.006, val_acc:0.986]
Epoch [81/120    avg_loss:0.004, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.045, val_acc:0.959]
Epoch [86/120    avg_loss:0.050, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.982]
Epoch [89/120    avg_loss:0.018, val_acc:0.981]
Epoch [90/120    avg_loss:0.035, val_acc:0.983]
Epoch [91/120    avg_loss:0.015, val_acc:0.982]
Epoch [92/120    avg_loss:0.019, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.975]
Epoch [94/120    avg_loss:0.020, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.979]
Epoch [96/120    avg_loss:0.010, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.003, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     4     0     0    27     0     0]
 [    0     0 18051     0    31     0     8     0     0     0]
 [    0     8     0  2007     3     0     0     0    11     7]
 [    0    35    11     0  2900     0     0     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4866     0     3     8]
 [    0     0     0     0     0     0     5  1284     0     1]
 [    0     2     0     1    38     0     0     0  3527     3]
 [    0     0     0     4    14    33     0     0     0   868]]

Accuracy:
99.31554720073265

F1 scores:
[       nan 0.99409846 0.99858933 0.99160079 0.97282791 0.98751419
 0.99743774 0.98731257 0.988232   0.96124031]

Kappa:
0.9909331159380308
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2ffff06d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 109328==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:1.506, val_acc:0.328]
Epoch [2/120    avg_loss:0.937, val_acc:0.704]
Epoch [3/120    avg_loss:0.652, val_acc:0.651]
Epoch [4/120    avg_loss:0.445, val_acc:0.817]
Epoch [5/120    avg_loss:0.378, val_acc:0.781]
Epoch [6/120    avg_loss:0.353, val_acc:0.799]
Epoch [7/120    avg_loss:0.337, val_acc:0.904]
Epoch [8/120    avg_loss:0.208, val_acc:0.932]
Epoch [9/120    avg_loss:0.147, val_acc:0.941]
Epoch [10/120    avg_loss:0.147, val_acc:0.880]
Epoch [11/120    avg_loss:0.167, val_acc:0.901]
Epoch [12/120    avg_loss:0.148, val_acc:0.859]
Epoch [13/120    avg_loss:0.109, val_acc:0.938]
Epoch [14/120    avg_loss:0.117, val_acc:0.943]
Epoch [15/120    avg_loss:0.108, val_acc:0.953]
Epoch [16/120    avg_loss:0.072, val_acc:0.962]
Epoch [17/120    avg_loss:0.059, val_acc:0.965]
Epoch [18/120    avg_loss:0.059, val_acc:0.963]
Epoch [19/120    avg_loss:0.042, val_acc:0.957]
Epoch [20/120    avg_loss:0.045, val_acc:0.974]
Epoch [21/120    avg_loss:0.081, val_acc:0.963]
Epoch [22/120    avg_loss:0.046, val_acc:0.950]
Epoch [23/120    avg_loss:0.072, val_acc:0.966]
Epoch [24/120    avg_loss:0.054, val_acc:0.971]
Epoch [25/120    avg_loss:0.030, val_acc:0.977]
Epoch [26/120    avg_loss:0.025, val_acc:0.978]
Epoch [27/120    avg_loss:0.031, val_acc:0.966]
Epoch [28/120    avg_loss:0.025, val_acc:0.978]
Epoch [29/120    avg_loss:0.025, val_acc:0.978]
Epoch [30/120    avg_loss:0.029, val_acc:0.975]
Epoch [31/120    avg_loss:0.021, val_acc:0.978]
Epoch [32/120    avg_loss:0.023, val_acc:0.972]
Epoch [33/120    avg_loss:0.018, val_acc:0.966]
Epoch [34/120    avg_loss:0.024, val_acc:0.975]
Epoch [35/120    avg_loss:0.029, val_acc:0.976]
Epoch [36/120    avg_loss:0.020, val_acc:0.984]
Epoch [37/120    avg_loss:0.031, val_acc:0.979]
Epoch [38/120    avg_loss:0.037, val_acc:0.979]
Epoch [39/120    avg_loss:0.013, val_acc:0.981]
Epoch [40/120    avg_loss:0.011, val_acc:0.978]
Epoch [41/120    avg_loss:0.011, val_acc:0.980]
Epoch [42/120    avg_loss:0.012, val_acc:0.977]
Epoch [43/120    avg_loss:0.012, val_acc:0.986]
Epoch [44/120    avg_loss:0.010, val_acc:0.984]
Epoch [45/120    avg_loss:0.007, val_acc:0.983]
Epoch [46/120    avg_loss:0.006, val_acc:0.981]
Epoch [47/120    avg_loss:0.005, val_acc:0.984]
Epoch [48/120    avg_loss:0.007, val_acc:0.985]
Epoch [49/120    avg_loss:0.006, val_acc:0.986]
Epoch [50/120    avg_loss:0.005, val_acc:0.984]
Epoch [51/120    avg_loss:0.010, val_acc:0.984]
Epoch [52/120    avg_loss:0.006, val_acc:0.984]
Epoch [53/120    avg_loss:0.011, val_acc:0.983]
Epoch [54/120    avg_loss:0.008, val_acc:0.983]
Epoch [55/120    avg_loss:0.014, val_acc:0.977]
Epoch [56/120    avg_loss:0.015, val_acc:0.984]
Epoch [57/120    avg_loss:0.023, val_acc:0.972]
Epoch [58/120    avg_loss:0.019, val_acc:0.978]
Epoch [59/120    avg_loss:0.022, val_acc:0.981]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.984]
Epoch [62/120    avg_loss:0.006, val_acc:0.984]
Epoch [63/120    avg_loss:0.016, val_acc:0.980]
Epoch [64/120    avg_loss:0.008, val_acc:0.986]
Epoch [65/120    avg_loss:0.006, val_acc:0.987]
Epoch [66/120    avg_loss:0.019, val_acc:0.974]
Epoch [67/120    avg_loss:0.007, val_acc:0.985]
Epoch [68/120    avg_loss:0.005, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.980]
Epoch [70/120    avg_loss:0.017, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.982]
Epoch [72/120    avg_loss:0.006, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.984]
Epoch [74/120    avg_loss:0.005, val_acc:0.982]
Epoch [75/120    avg_loss:0.004, val_acc:0.983]
Epoch [76/120    avg_loss:0.003, val_acc:0.984]
Epoch [77/120    avg_loss:0.003, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.983]
Epoch [79/120    avg_loss:0.003, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.980]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.003, val_acc:0.985]
Epoch [86/120    avg_loss:0.004, val_acc:0.984]
Epoch [87/120    avg_loss:0.005, val_acc:0.984]
Epoch [88/120    avg_loss:0.004, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.979]
Epoch [90/120    avg_loss:0.009, val_acc:0.981]
Epoch [91/120    avg_loss:0.005, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.979]
Epoch [93/120    avg_loss:0.006, val_acc:0.980]
Epoch [94/120    avg_loss:0.004, val_acc:0.981]
Epoch [95/120    avg_loss:0.003, val_acc:0.982]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.004, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.003, val_acc:0.985]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.003, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.003, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.002, val_acc:0.984]
Epoch [116/120    avg_loss:0.003, val_acc:0.984]
Epoch [117/120    avg_loss:0.003, val_acc:0.984]
Epoch [118/120    avg_loss:0.003, val_acc:0.984]
Epoch [119/120    avg_loss:0.004, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     3     0    12    18     0     4]
 [    0     0 18047     0    24     0    19     0     0     0]
 [    0     0     0  2028     1     0     0     0     3     4]
 [    0    41    12     0  2907     0     0     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4867     0     0     5]
 [    0     0     0     0     0     0     6  1280     0     4]
 [    0     2     0     0    55     0     0     0  3514     0]
 [    0     0     0     2    12    28     0     0     0   877]]

Accuracy:
99.34205769647893

F1 scores:
[       nan 0.99378399 0.99831282 0.99754058 0.97321727 0.9893859
 0.99509303 0.98918083 0.98985915 0.96745725]

Kappa:
0.9912841208493196
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f31d5d7c748>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.664, val_acc:0.353]
Epoch [2/120    avg_loss:0.994, val_acc:0.482]
Epoch [3/120    avg_loss:0.754, val_acc:0.699]
Epoch [4/120    avg_loss:0.584, val_acc:0.734]
Epoch [5/120    avg_loss:0.440, val_acc:0.833]
Epoch [6/120    avg_loss:0.343, val_acc:0.832]
Epoch [7/120    avg_loss:0.325, val_acc:0.897]
Epoch [8/120    avg_loss:0.223, val_acc:0.932]
Epoch [9/120    avg_loss:0.226, val_acc:0.873]
Epoch [10/120    avg_loss:0.196, val_acc:0.928]
Epoch [11/120    avg_loss:0.184, val_acc:0.945]
Epoch [12/120    avg_loss:0.116, val_acc:0.963]
Epoch [13/120    avg_loss:0.103, val_acc:0.939]
Epoch [14/120    avg_loss:0.133, val_acc:0.956]
Epoch [15/120    avg_loss:0.096, val_acc:0.977]
Epoch [16/120    avg_loss:0.064, val_acc:0.974]
Epoch [17/120    avg_loss:0.066, val_acc:0.961]
Epoch [18/120    avg_loss:0.090, val_acc:0.964]
Epoch [19/120    avg_loss:0.051, val_acc:0.980]
Epoch [20/120    avg_loss:0.039, val_acc:0.981]
Epoch [21/120    avg_loss:0.040, val_acc:0.966]
Epoch [22/120    avg_loss:0.046, val_acc:0.968]
Epoch [23/120    avg_loss:0.090, val_acc:0.974]
Epoch [24/120    avg_loss:0.058, val_acc:0.974]
Epoch [25/120    avg_loss:0.049, val_acc:0.978]
Epoch [26/120    avg_loss:0.044, val_acc:0.979]
Epoch [27/120    avg_loss:0.029, val_acc:0.978]
Epoch [28/120    avg_loss:0.023, val_acc:0.985]
Epoch [29/120    avg_loss:0.017, val_acc:0.984]
Epoch [30/120    avg_loss:0.015, val_acc:0.984]
Epoch [31/120    avg_loss:0.020, val_acc:0.983]
Epoch [32/120    avg_loss:0.024, val_acc:0.979]
Epoch [33/120    avg_loss:0.027, val_acc:0.985]
Epoch [34/120    avg_loss:0.019, val_acc:0.986]
Epoch [35/120    avg_loss:0.016, val_acc:0.984]
Epoch [36/120    avg_loss:0.016, val_acc:0.984]
Epoch [37/120    avg_loss:0.021, val_acc:0.987]
Epoch [38/120    avg_loss:0.027, val_acc:0.986]
Epoch [39/120    avg_loss:0.021, val_acc:0.976]
Epoch [40/120    avg_loss:0.015, val_acc:0.986]
Epoch [41/120    avg_loss:0.033, val_acc:0.955]
Epoch [42/120    avg_loss:0.086, val_acc:0.972]
Epoch [43/120    avg_loss:0.094, val_acc:0.959]
Epoch [44/120    avg_loss:0.050, val_acc:0.972]
Epoch [45/120    avg_loss:0.023, val_acc:0.985]
Epoch [46/120    avg_loss:0.030, val_acc:0.980]
Epoch [47/120    avg_loss:0.018, val_acc:0.986]
Epoch [48/120    avg_loss:0.022, val_acc:0.984]
Epoch [49/120    avg_loss:0.016, val_acc:0.986]
Epoch [50/120    avg_loss:0.013, val_acc:0.987]
Epoch [51/120    avg_loss:0.016, val_acc:0.987]
Epoch [52/120    avg_loss:0.013, val_acc:0.988]
Epoch [53/120    avg_loss:0.017, val_acc:0.988]
Epoch [54/120    avg_loss:0.011, val_acc:0.988]
Epoch [55/120    avg_loss:0.010, val_acc:0.990]
Epoch [56/120    avg_loss:0.007, val_acc:0.990]
Epoch [57/120    avg_loss:0.006, val_acc:0.990]
Epoch [58/120    avg_loss:0.012, val_acc:0.982]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.013, val_acc:0.987]
Epoch [61/120    avg_loss:0.016, val_acc:0.989]
Epoch [62/120    avg_loss:0.031, val_acc:0.980]
Epoch [63/120    avg_loss:0.021, val_acc:0.989]
Epoch [64/120    avg_loss:0.011, val_acc:0.989]
Epoch [65/120    avg_loss:0.008, val_acc:0.991]
Epoch [66/120    avg_loss:0.012, val_acc:0.989]
Epoch [67/120    avg_loss:0.010, val_acc:0.987]
Epoch [68/120    avg_loss:0.006, val_acc:0.990]
Epoch [69/120    avg_loss:0.015, val_acc:0.987]
Epoch [70/120    avg_loss:0.014, val_acc:0.978]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.010, val_acc:0.992]
Epoch [73/120    avg_loss:0.009, val_acc:0.991]
Epoch [74/120    avg_loss:0.005, val_acc:0.990]
Epoch [75/120    avg_loss:0.008, val_acc:0.989]
Epoch [76/120    avg_loss:0.014, val_acc:0.989]
Epoch [77/120    avg_loss:0.021, val_acc:0.975]
Epoch [78/120    avg_loss:0.027, val_acc:0.970]
Epoch [79/120    avg_loss:0.018, val_acc:0.983]
Epoch [80/120    avg_loss:0.033, val_acc:0.975]
Epoch [81/120    avg_loss:0.016, val_acc:0.989]
Epoch [82/120    avg_loss:0.013, val_acc:0.990]
Epoch [83/120    avg_loss:0.006, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.005, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.991]
Epoch [87/120    avg_loss:0.005, val_acc:0.991]
Epoch [88/120    avg_loss:0.007, val_acc:0.991]
Epoch [89/120    avg_loss:0.009, val_acc:0.991]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.004, val_acc:0.991]
Epoch [92/120    avg_loss:0.004, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.991]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.003, val_acc:0.992]
Epoch [108/120    avg_loss:0.003, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.004, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.003, val_acc:0.992]
Epoch [119/120    avg_loss:0.003, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0    13     0     0     9     0     0]
 [    0     0 18046     0    28     0    16     0     0     0]
 [    0     0     0  2033     2     0     0     0     1     0]
 [    0    35    20     1  2886     0     3     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0     0    18]
 [    0     0     0     0     0     0     6  1282     0     2]
 [    0     5     0     0    31     0     0     0  3535     0]
 [    0     0     0    10    16    32     0     0     0   861]]

Accuracy:
99.33723760634324

F1 scores:
[       nan 0.99518708 0.99822989 0.99656863 0.97041022 0.98788796
 0.99559562 0.99341341 0.99102888 0.95666667]

Kappa:
0.9912198664148679
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0c24f17b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.628, val_acc:0.332]
Epoch [2/120    avg_loss:0.995, val_acc:0.720]
Epoch [3/120    avg_loss:0.768, val_acc:0.711]
Epoch [4/120    avg_loss:0.608, val_acc:0.733]
Epoch [5/120    avg_loss:0.428, val_acc:0.716]
Epoch [6/120    avg_loss:0.377, val_acc:0.784]
Epoch [7/120    avg_loss:0.326, val_acc:0.781]
Epoch [8/120    avg_loss:0.276, val_acc:0.828]
Epoch [9/120    avg_loss:0.233, val_acc:0.887]
Epoch [10/120    avg_loss:0.238, val_acc:0.924]
Epoch [11/120    avg_loss:0.197, val_acc:0.895]
Epoch [12/120    avg_loss:0.160, val_acc:0.946]
Epoch [13/120    avg_loss:0.101, val_acc:0.952]
Epoch [14/120    avg_loss:0.122, val_acc:0.942]
Epoch [15/120    avg_loss:0.123, val_acc:0.932]
Epoch [16/120    avg_loss:0.086, val_acc:0.957]
Epoch [17/120    avg_loss:0.124, val_acc:0.952]
Epoch [18/120    avg_loss:0.080, val_acc:0.950]
Epoch [19/120    avg_loss:0.076, val_acc:0.935]
Epoch [20/120    avg_loss:0.098, val_acc:0.951]
Epoch [21/120    avg_loss:0.085, val_acc:0.907]
Epoch [22/120    avg_loss:0.056, val_acc:0.968]
Epoch [23/120    avg_loss:0.040, val_acc:0.960]
Epoch [24/120    avg_loss:0.046, val_acc:0.967]
Epoch [25/120    avg_loss:0.046, val_acc:0.938]
Epoch [26/120    avg_loss:0.083, val_acc:0.966]
Epoch [27/120    avg_loss:0.055, val_acc:0.963]
Epoch [28/120    avg_loss:0.034, val_acc:0.970]
Epoch [29/120    avg_loss:0.037, val_acc:0.963]
Epoch [30/120    avg_loss:0.054, val_acc:0.974]
Epoch [31/120    avg_loss:0.034, val_acc:0.966]
Epoch [32/120    avg_loss:0.027, val_acc:0.975]
Epoch [33/120    avg_loss:0.023, val_acc:0.976]
Epoch [34/120    avg_loss:0.024, val_acc:0.969]
Epoch [35/120    avg_loss:0.029, val_acc:0.959]
Epoch [36/120    avg_loss:0.021, val_acc:0.977]
Epoch [37/120    avg_loss:0.021, val_acc:0.966]
Epoch [38/120    avg_loss:0.015, val_acc:0.977]
Epoch [39/120    avg_loss:0.014, val_acc:0.977]
Epoch [40/120    avg_loss:0.015, val_acc:0.979]
Epoch [41/120    avg_loss:0.017, val_acc:0.959]
Epoch [42/120    avg_loss:0.012, val_acc:0.980]
Epoch [43/120    avg_loss:0.019, val_acc:0.982]
Epoch [44/120    avg_loss:0.013, val_acc:0.978]
Epoch [45/120    avg_loss:0.014, val_acc:0.979]
Epoch [46/120    avg_loss:0.010, val_acc:0.980]
Epoch [47/120    avg_loss:0.021, val_acc:0.972]
Epoch [48/120    avg_loss:0.015, val_acc:0.977]
Epoch [49/120    avg_loss:0.012, val_acc:0.974]
Epoch [50/120    avg_loss:0.016, val_acc:0.973]
Epoch [51/120    avg_loss:0.017, val_acc:0.980]
Epoch [52/120    avg_loss:0.015, val_acc:0.978]
Epoch [53/120    avg_loss:0.018, val_acc:0.928]
Epoch [54/120    avg_loss:0.034, val_acc:0.975]
Epoch [55/120    avg_loss:0.017, val_acc:0.974]
Epoch [56/120    avg_loss:0.014, val_acc:0.978]
Epoch [57/120    avg_loss:0.008, val_acc:0.979]
Epoch [58/120    avg_loss:0.009, val_acc:0.979]
Epoch [59/120    avg_loss:0.006, val_acc:0.980]
Epoch [60/120    avg_loss:0.007, val_acc:0.980]
Epoch [61/120    avg_loss:0.009, val_acc:0.980]
Epoch [62/120    avg_loss:0.008, val_acc:0.979]
Epoch [63/120    avg_loss:0.005, val_acc:0.979]
Epoch [64/120    avg_loss:0.007, val_acc:0.980]
Epoch [65/120    avg_loss:0.005, val_acc:0.980]
Epoch [66/120    avg_loss:0.007, val_acc:0.980]
Epoch [67/120    avg_loss:0.005, val_acc:0.980]
Epoch [68/120    avg_loss:0.008, val_acc:0.981]
Epoch [69/120    avg_loss:0.008, val_acc:0.980]
Epoch [70/120    avg_loss:0.006, val_acc:0.980]
Epoch [71/120    avg_loss:0.005, val_acc:0.980]
Epoch [72/120    avg_loss:0.008, val_acc:0.981]
Epoch [73/120    avg_loss:0.005, val_acc:0.981]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.009, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.005, val_acc:0.981]
Epoch [79/120    avg_loss:0.005, val_acc:0.981]
Epoch [80/120    avg_loss:0.008, val_acc:0.981]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.008, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.005, val_acc:0.981]
Epoch [85/120    avg_loss:0.005, val_acc:0.981]
Epoch [86/120    avg_loss:0.006, val_acc:0.981]
Epoch [87/120    avg_loss:0.006, val_acc:0.981]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.981]
Epoch [90/120    avg_loss:0.005, val_acc:0.981]
Epoch [91/120    avg_loss:0.005, val_acc:0.981]
Epoch [92/120    avg_loss:0.005, val_acc:0.981]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.006, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.006, val_acc:0.981]
Epoch [98/120    avg_loss:0.004, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.006, val_acc:0.981]
Epoch [102/120    avg_loss:0.005, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.981]
Epoch [104/120    avg_loss:0.004, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.006, val_acc:0.981]
Epoch [107/120    avg_loss:0.006, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.007, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.005, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.005, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     3     0    19    14     0     0]
 [    0     0 18038     0    33     0    19     0     0     0]
 [    0     0     0  2030     2     0     0     0     1     3]
 [    0    40    20     0  2884     0     0     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4848     0     0    30]
 [    0     0     0     0     0     0     6  1284     0     0]
 [    0     0     0     0    62     0     0     0  3509     0]
 [    0     0     0     5    14    41     0     0     0   859]]

Accuracy:
99.18058467693346

F1 scores:
[       nan 0.99409388 0.99800819 0.99729796 0.96616415 0.98453414
 0.99242579 0.99227202 0.98719932 0.94864716]

Kappa:
0.9891463322613571
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f477dc146a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.573, val_acc:0.339]
Epoch [2/120    avg_loss:1.012, val_acc:0.679]
Epoch [3/120    avg_loss:0.717, val_acc:0.665]
Epoch [4/120    avg_loss:0.557, val_acc:0.758]
Epoch [5/120    avg_loss:0.467, val_acc:0.767]
Epoch [6/120    avg_loss:0.378, val_acc:0.802]
Epoch [7/120    avg_loss:0.308, val_acc:0.880]
Epoch [8/120    avg_loss:0.276, val_acc:0.905]
Epoch [9/120    avg_loss:0.251, val_acc:0.927]
Epoch [10/120    avg_loss:0.221, val_acc:0.937]
Epoch [11/120    avg_loss:0.165, val_acc:0.959]
Epoch [12/120    avg_loss:0.141, val_acc:0.951]
Epoch [13/120    avg_loss:0.137, val_acc:0.950]
Epoch [14/120    avg_loss:0.087, val_acc:0.964]
Epoch [15/120    avg_loss:0.088, val_acc:0.931]
Epoch [16/120    avg_loss:0.116, val_acc:0.960]
Epoch [17/120    avg_loss:0.078, val_acc:0.971]
Epoch [18/120    avg_loss:0.092, val_acc:0.967]
Epoch [19/120    avg_loss:0.097, val_acc:0.964]
Epoch [20/120    avg_loss:0.074, val_acc:0.963]
Epoch [21/120    avg_loss:0.098, val_acc:0.971]
Epoch [22/120    avg_loss:0.087, val_acc:0.968]
Epoch [23/120    avg_loss:0.047, val_acc:0.978]
Epoch [24/120    avg_loss:0.050, val_acc:0.972]
Epoch [25/120    avg_loss:0.040, val_acc:0.980]
Epoch [26/120    avg_loss:0.045, val_acc:0.957]
Epoch [27/120    avg_loss:0.040, val_acc:0.961]
Epoch [28/120    avg_loss:0.037, val_acc:0.985]
Epoch [29/120    avg_loss:0.041, val_acc:0.980]
Epoch [30/120    avg_loss:0.040, val_acc:0.978]
Epoch [31/120    avg_loss:0.030, val_acc:0.982]
Epoch [32/120    avg_loss:0.034, val_acc:0.984]
Epoch [33/120    avg_loss:0.017, val_acc:0.985]
Epoch [34/120    avg_loss:0.022, val_acc:0.983]
Epoch [35/120    avg_loss:0.018, val_acc:0.984]
Epoch [36/120    avg_loss:0.046, val_acc:0.984]
Epoch [37/120    avg_loss:0.032, val_acc:0.952]
Epoch [38/120    avg_loss:0.026, val_acc:0.982]
Epoch [39/120    avg_loss:0.027, val_acc:0.975]
Epoch [40/120    avg_loss:0.029, val_acc:0.987]
Epoch [41/120    avg_loss:0.020, val_acc:0.982]
Epoch [42/120    avg_loss:0.041, val_acc:0.975]
Epoch [43/120    avg_loss:0.019, val_acc:0.987]
Epoch [44/120    avg_loss:0.017, val_acc:0.985]
Epoch [45/120    avg_loss:0.010, val_acc:0.987]
Epoch [46/120    avg_loss:0.012, val_acc:0.987]
Epoch [47/120    avg_loss:0.015, val_acc:0.988]
Epoch [48/120    avg_loss:0.016, val_acc:0.978]
Epoch [49/120    avg_loss:0.030, val_acc:0.982]
Epoch [50/120    avg_loss:0.018, val_acc:0.985]
Epoch [51/120    avg_loss:0.015, val_acc:0.984]
Epoch [52/120    avg_loss:0.018, val_acc:0.988]
Epoch [53/120    avg_loss:0.012, val_acc:0.988]
Epoch [54/120    avg_loss:0.005, val_acc:0.991]
Epoch [55/120    avg_loss:0.010, val_acc:0.989]
Epoch [56/120    avg_loss:0.013, val_acc:0.986]
Epoch [57/120    avg_loss:0.007, val_acc:0.992]
Epoch [58/120    avg_loss:0.010, val_acc:0.992]
Epoch [59/120    avg_loss:0.006, val_acc:0.989]
Epoch [60/120    avg_loss:0.009, val_acc:0.994]
Epoch [61/120    avg_loss:0.006, val_acc:0.992]
Epoch [62/120    avg_loss:0.009, val_acc:0.983]
Epoch [63/120    avg_loss:0.014, val_acc:0.989]
Epoch [64/120    avg_loss:0.011, val_acc:0.983]
Epoch [65/120    avg_loss:0.007, val_acc:0.990]
Epoch [66/120    avg_loss:0.006, val_acc:0.990]
Epoch [67/120    avg_loss:0.014, val_acc:0.986]
Epoch [68/120    avg_loss:0.081, val_acc:0.930]
Epoch [69/120    avg_loss:0.147, val_acc:0.976]
Epoch [70/120    avg_loss:0.085, val_acc:0.976]
Epoch [71/120    avg_loss:0.072, val_acc:0.979]
Epoch [72/120    avg_loss:0.048, val_acc:0.973]
Epoch [73/120    avg_loss:0.037, val_acc:0.984]
Epoch [74/120    avg_loss:0.016, val_acc:0.986]
Epoch [75/120    avg_loss:0.016, val_acc:0.988]
Epoch [76/120    avg_loss:0.015, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.013, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.012, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.012, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.013, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6386     0     0     1     0    31     3     0    11]
 [    0     7 18025     0    41     0    15     0     2     0]
 [    0     5     0  2028     0     0     0     0     0     3]
 [    0    49    20     3  2871     0     0     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4850     0     0    28]
 [    0     0     0     0     0     0     5  1282     0     3]
 [    0     1     0     0    38     0     0     0  3510    22]
 [    0     0     0     4    14    35     0     0     0   866]]

Accuracy:
99.10828332489818

F1 scores:
[       nan 0.99161491 0.99764771 0.9963154  0.96715513 0.98676749
 0.99192146 0.99572816 0.98706412 0.93520518]

Kappa:
0.9881903671543644
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f114c4f1ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.569, val_acc:0.297]
Epoch [2/120    avg_loss:0.929, val_acc:0.576]
Epoch [3/120    avg_loss:0.793, val_acc:0.720]
Epoch [4/120    avg_loss:0.627, val_acc:0.733]
Epoch [5/120    avg_loss:0.453, val_acc:0.762]
Epoch [6/120    avg_loss:0.365, val_acc:0.788]
Epoch [7/120    avg_loss:0.267, val_acc:0.891]
Epoch [8/120    avg_loss:0.243, val_acc:0.909]
Epoch [9/120    avg_loss:0.204, val_acc:0.953]
Epoch [10/120    avg_loss:0.199, val_acc:0.955]
Epoch [11/120    avg_loss:0.136, val_acc:0.961]
Epoch [12/120    avg_loss:0.142, val_acc:0.904]
Epoch [13/120    avg_loss:0.168, val_acc:0.930]
Epoch [14/120    avg_loss:0.140, val_acc:0.943]
Epoch [15/120    avg_loss:0.127, val_acc:0.946]
Epoch [16/120    avg_loss:0.097, val_acc:0.957]
Epoch [17/120    avg_loss:0.098, val_acc:0.967]
Epoch [18/120    avg_loss:0.062, val_acc:0.966]
Epoch [19/120    avg_loss:0.062, val_acc:0.971]
Epoch [20/120    avg_loss:0.046, val_acc:0.969]
Epoch [21/120    avg_loss:0.061, val_acc:0.970]
Epoch [22/120    avg_loss:0.039, val_acc:0.976]
Epoch [23/120    avg_loss:0.048, val_acc:0.961]
Epoch [24/120    avg_loss:0.045, val_acc:0.960]
Epoch [25/120    avg_loss:0.043, val_acc:0.979]
Epoch [26/120    avg_loss:0.047, val_acc:0.953]
Epoch [27/120    avg_loss:0.035, val_acc:0.978]
Epoch [28/120    avg_loss:0.042, val_acc:0.975]
Epoch [29/120    avg_loss:0.059, val_acc:0.985]
Epoch [30/120    avg_loss:0.029, val_acc:0.983]
Epoch [31/120    avg_loss:0.021, val_acc:0.987]
Epoch [32/120    avg_loss:0.027, val_acc:0.989]
Epoch [33/120    avg_loss:0.026, val_acc:0.976]
Epoch [34/120    avg_loss:0.027, val_acc:0.965]
Epoch [35/120    avg_loss:0.024, val_acc:0.986]
Epoch [36/120    avg_loss:0.022, val_acc:0.986]
Epoch [37/120    avg_loss:0.023, val_acc:0.984]
Epoch [38/120    avg_loss:0.014, val_acc:0.990]
Epoch [39/120    avg_loss:0.014, val_acc:0.985]
Epoch [40/120    avg_loss:0.016, val_acc:0.984]
Epoch [41/120    avg_loss:0.022, val_acc:0.985]
Epoch [42/120    avg_loss:0.032, val_acc:0.984]
Epoch [43/120    avg_loss:0.010, val_acc:0.987]
Epoch [44/120    avg_loss:0.018, val_acc:0.971]
Epoch [45/120    avg_loss:0.023, val_acc:0.967]
Epoch [46/120    avg_loss:0.023, val_acc:0.984]
Epoch [47/120    avg_loss:0.020, val_acc:0.985]
Epoch [48/120    avg_loss:0.008, val_acc:0.986]
Epoch [49/120    avg_loss:0.007, val_acc:0.986]
Epoch [50/120    avg_loss:0.008, val_acc:0.990]
Epoch [51/120    avg_loss:0.010, val_acc:0.984]
Epoch [52/120    avg_loss:0.011, val_acc:0.990]
Epoch [53/120    avg_loss:0.005, val_acc:0.989]
Epoch [54/120    avg_loss:0.007, val_acc:0.990]
Epoch [55/120    avg_loss:0.006, val_acc:0.988]
Epoch [56/120    avg_loss:0.007, val_acc:0.991]
Epoch [57/120    avg_loss:0.007, val_acc:0.981]
Epoch [58/120    avg_loss:0.009, val_acc:0.984]
Epoch [59/120    avg_loss:0.007, val_acc:0.986]
Epoch [60/120    avg_loss:0.009, val_acc:0.984]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.005, val_acc:0.989]
Epoch [63/120    avg_loss:0.006, val_acc:0.991]
Epoch [64/120    avg_loss:0.005, val_acc:0.990]
Epoch [65/120    avg_loss:0.004, val_acc:0.991]
Epoch [66/120    avg_loss:0.020, val_acc:0.988]
Epoch [67/120    avg_loss:0.004, val_acc:0.991]
Epoch [68/120    avg_loss:0.008, val_acc:0.991]
Epoch [69/120    avg_loss:0.005, val_acc:0.990]
Epoch [70/120    avg_loss:0.006, val_acc:0.988]
Epoch [71/120    avg_loss:0.005, val_acc:0.991]
Epoch [72/120    avg_loss:0.006, val_acc:0.987]
Epoch [73/120    avg_loss:0.011, val_acc:0.990]
Epoch [74/120    avg_loss:0.011, val_acc:0.955]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.007, val_acc:0.987]
Epoch [78/120    avg_loss:0.006, val_acc:0.982]
Epoch [79/120    avg_loss:0.011, val_acc:0.987]
Epoch [80/120    avg_loss:0.013, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.989]
Epoch [82/120    avg_loss:0.004, val_acc:0.990]
Epoch [83/120    avg_loss:0.003, val_acc:0.990]
Epoch [84/120    avg_loss:0.003, val_acc:0.991]
Epoch [85/120    avg_loss:0.004, val_acc:0.991]
Epoch [86/120    avg_loss:0.004, val_acc:0.991]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.003, val_acc:0.991]
Epoch [90/120    avg_loss:0.003, val_acc:0.991]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.003, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.003, val_acc:0.990]
Epoch [96/120    avg_loss:0.003, val_acc:0.990]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.002, val_acc:0.990]
Epoch [100/120    avg_loss:0.003, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.003, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.003, val_acc:0.991]
Epoch [106/120    avg_loss:0.002, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.002, val_acc:0.991]
Epoch [109/120    avg_loss:0.003, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.991]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.003, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.002, val_acc:0.991]
Epoch [118/120    avg_loss:0.010, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     0     2     0     0    14     0     0]
 [    0     0 18065     0    17     0     8     0     0     0]
 [    0     0     0  2030     1     0     0     0     0     5]
 [    0    40    20     0  2877     0     7     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4870     0     0     8]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     2     0     0    37     0     0     0  3532     0]
 [    0     0     0     3    14    34     0     0     0   868]]

Accuracy:
99.41194900344637

F1 scores:
[       nan 0.99550039 0.99875605 0.99778815 0.97195946 0.9871407
 0.99723559 0.99305019 0.9906044  0.96444444]

Kappa:
0.9922077527632946
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe0d254b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.551, val_acc:0.386]
Epoch [2/120    avg_loss:0.969, val_acc:0.726]
Epoch [3/120    avg_loss:0.705, val_acc:0.729]
Epoch [4/120    avg_loss:0.466, val_acc:0.796]
Epoch [5/120    avg_loss:0.398, val_acc:0.871]
Epoch [6/120    avg_loss:0.322, val_acc:0.878]
Epoch [7/120    avg_loss:0.255, val_acc:0.897]
Epoch [8/120    avg_loss:0.202, val_acc:0.919]
Epoch [9/120    avg_loss:0.132, val_acc:0.951]
Epoch [10/120    avg_loss:0.149, val_acc:0.956]
Epoch [11/120    avg_loss:0.101, val_acc:0.966]
Epoch [12/120    avg_loss:0.087, val_acc:0.965]
Epoch [13/120    avg_loss:0.067, val_acc:0.937]
Epoch [14/120    avg_loss:0.074, val_acc:0.957]
Epoch [15/120    avg_loss:0.087, val_acc:0.973]
Epoch [16/120    avg_loss:0.067, val_acc:0.965]
Epoch [17/120    avg_loss:0.060, val_acc:0.967]
Epoch [18/120    avg_loss:0.041, val_acc:0.963]
Epoch [19/120    avg_loss:0.070, val_acc:0.970]
Epoch [20/120    avg_loss:0.080, val_acc:0.962]
Epoch [21/120    avg_loss:0.063, val_acc:0.963]
Epoch [22/120    avg_loss:0.047, val_acc:0.970]
Epoch [23/120    avg_loss:0.047, val_acc:0.968]
Epoch [24/120    avg_loss:0.034, val_acc:0.972]
Epoch [25/120    avg_loss:0.026, val_acc:0.977]
Epoch [26/120    avg_loss:0.036, val_acc:0.976]
Epoch [27/120    avg_loss:0.024, val_acc:0.975]
Epoch [28/120    avg_loss:0.016, val_acc:0.980]
Epoch [29/120    avg_loss:0.018, val_acc:0.985]
Epoch [30/120    avg_loss:0.018, val_acc:0.984]
Epoch [31/120    avg_loss:0.021, val_acc:0.969]
Epoch [32/120    avg_loss:0.019, val_acc:0.983]
Epoch [33/120    avg_loss:0.016, val_acc:0.986]
Epoch [34/120    avg_loss:0.015, val_acc:0.980]
Epoch [35/120    avg_loss:0.017, val_acc:0.977]
Epoch [36/120    avg_loss:0.038, val_acc:0.977]
Epoch [37/120    avg_loss:0.018, val_acc:0.979]
Epoch [38/120    avg_loss:0.015, val_acc:0.985]
Epoch [39/120    avg_loss:0.014, val_acc:0.984]
Epoch [40/120    avg_loss:0.019, val_acc:0.980]
Epoch [41/120    avg_loss:0.020, val_acc:0.977]
Epoch [42/120    avg_loss:0.017, val_acc:0.981]
Epoch [43/120    avg_loss:0.015, val_acc:0.974]
Epoch [44/120    avg_loss:0.014, val_acc:0.979]
Epoch [45/120    avg_loss:0.018, val_acc:0.984]
Epoch [46/120    avg_loss:0.011, val_acc:0.985]
Epoch [47/120    avg_loss:0.012, val_acc:0.985]
Epoch [48/120    avg_loss:0.008, val_acc:0.985]
Epoch [49/120    avg_loss:0.008, val_acc:0.985]
Epoch [50/120    avg_loss:0.010, val_acc:0.986]
Epoch [51/120    avg_loss:0.008, val_acc:0.986]
Epoch [52/120    avg_loss:0.006, val_acc:0.986]
Epoch [53/120    avg_loss:0.012, val_acc:0.984]
Epoch [54/120    avg_loss:0.007, val_acc:0.987]
Epoch [55/120    avg_loss:0.011, val_acc:0.988]
Epoch [56/120    avg_loss:0.005, val_acc:0.987]
Epoch [57/120    avg_loss:0.010, val_acc:0.988]
Epoch [58/120    avg_loss:0.007, val_acc:0.988]
Epoch [59/120    avg_loss:0.010, val_acc:0.987]
Epoch [60/120    avg_loss:0.005, val_acc:0.988]
Epoch [61/120    avg_loss:0.007, val_acc:0.988]
Epoch [62/120    avg_loss:0.007, val_acc:0.988]
Epoch [63/120    avg_loss:0.008, val_acc:0.988]
Epoch [64/120    avg_loss:0.007, val_acc:0.988]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.006, val_acc:0.988]
Epoch [67/120    avg_loss:0.007, val_acc:0.988]
Epoch [68/120    avg_loss:0.010, val_acc:0.988]
Epoch [69/120    avg_loss:0.007, val_acc:0.987]
Epoch [70/120    avg_loss:0.008, val_acc:0.987]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.006, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.004, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.987]
Epoch [77/120    avg_loss:0.006, val_acc:0.987]
Epoch [78/120    avg_loss:0.005, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.009, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     0     0     0    13     0     0]
 [    0     0 18058     0    20     0    12     0     0     0]
 [    0     0     0  2030     0     0     0     0     0     6]
 [    0    41    20     0  2883     0     0     0    27     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4842     0     0    36]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0     3     0     1    29     0     0     0  3532     6]
 [    0     0     0     7    14    34     0     0     0   864]]

Accuracy:
99.33723760634324

F1 scores:
[       nan 0.99557968 0.99856226 0.99656357 0.97431565 0.9871407
 0.99506782 0.99304482 0.99074334 0.94066413]

Kappa:
0.9912193295372507
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f130adbf780>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.648, val_acc:0.502]
Epoch [2/120    avg_loss:1.008, val_acc:0.566]
Epoch [3/120    avg_loss:0.722, val_acc:0.778]
Epoch [4/120    avg_loss:0.537, val_acc:0.780]
Epoch [5/120    avg_loss:0.378, val_acc:0.842]
Epoch [6/120    avg_loss:0.356, val_acc:0.831]
Epoch [7/120    avg_loss:0.284, val_acc:0.897]
Epoch [8/120    avg_loss:0.223, val_acc:0.905]
Epoch [9/120    avg_loss:0.206, val_acc:0.923]
Epoch [10/120    avg_loss:0.151, val_acc:0.925]
Epoch [11/120    avg_loss:0.137, val_acc:0.916]
Epoch [12/120    avg_loss:0.193, val_acc:0.943]
Epoch [13/120    avg_loss:0.134, val_acc:0.947]
Epoch [14/120    avg_loss:0.100, val_acc:0.949]
Epoch [15/120    avg_loss:0.077, val_acc:0.957]
Epoch [16/120    avg_loss:0.080, val_acc:0.938]
Epoch [17/120    avg_loss:0.115, val_acc:0.955]
Epoch [18/120    avg_loss:0.090, val_acc:0.966]
Epoch [19/120    avg_loss:0.093, val_acc:0.952]
Epoch [20/120    avg_loss:0.056, val_acc:0.965]
Epoch [21/120    avg_loss:0.041, val_acc:0.971]
Epoch [22/120    avg_loss:0.063, val_acc:0.954]
Epoch [23/120    avg_loss:0.051, val_acc:0.969]
Epoch [24/120    avg_loss:0.053, val_acc:0.967]
Epoch [25/120    avg_loss:0.048, val_acc:0.966]
Epoch [26/120    avg_loss:0.033, val_acc:0.977]
Epoch [27/120    avg_loss:0.024, val_acc:0.972]
Epoch [28/120    avg_loss:0.053, val_acc:0.960]
Epoch [29/120    avg_loss:0.054, val_acc:0.973]
Epoch [30/120    avg_loss:0.039, val_acc:0.972]
Epoch [31/120    avg_loss:0.038, val_acc:0.972]
Epoch [32/120    avg_loss:0.032, val_acc:0.963]
Epoch [33/120    avg_loss:0.025, val_acc:0.972]
Epoch [34/120    avg_loss:0.021, val_acc:0.972]
Epoch [35/120    avg_loss:0.025, val_acc:0.962]
Epoch [36/120    avg_loss:0.016, val_acc:0.980]
Epoch [37/120    avg_loss:0.028, val_acc:0.927]
Epoch [38/120    avg_loss:0.041, val_acc:0.977]
Epoch [39/120    avg_loss:0.018, val_acc:0.982]
Epoch [40/120    avg_loss:0.018, val_acc:0.986]
Epoch [41/120    avg_loss:0.018, val_acc:0.974]
Epoch [42/120    avg_loss:0.015, val_acc:0.981]
Epoch [43/120    avg_loss:0.017, val_acc:0.981]
Epoch [44/120    avg_loss:0.015, val_acc:0.971]
Epoch [45/120    avg_loss:0.012, val_acc:0.982]
Epoch [46/120    avg_loss:0.012, val_acc:0.986]
Epoch [47/120    avg_loss:0.009, val_acc:0.981]
Epoch [48/120    avg_loss:0.008, val_acc:0.982]
Epoch [49/120    avg_loss:0.008, val_acc:0.986]
Epoch [50/120    avg_loss:0.012, val_acc:0.990]
Epoch [51/120    avg_loss:0.012, val_acc:0.985]
Epoch [52/120    avg_loss:0.009, val_acc:0.987]
Epoch [53/120    avg_loss:0.017, val_acc:0.980]
Epoch [54/120    avg_loss:0.014, val_acc:0.981]
Epoch [55/120    avg_loss:0.010, val_acc:0.985]
Epoch [56/120    avg_loss:0.007, val_acc:0.988]
Epoch [57/120    avg_loss:0.006, val_acc:0.990]
Epoch [58/120    avg_loss:0.008, val_acc:0.990]
Epoch [59/120    avg_loss:0.010, val_acc:0.991]
Epoch [60/120    avg_loss:0.009, val_acc:0.991]
Epoch [61/120    avg_loss:0.013, val_acc:0.991]
Epoch [62/120    avg_loss:0.010, val_acc:0.984]
Epoch [63/120    avg_loss:0.015, val_acc:0.981]
Epoch [64/120    avg_loss:0.014, val_acc:0.984]
Epoch [65/120    avg_loss:0.007, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.984]
Epoch [67/120    avg_loss:0.009, val_acc:0.982]
Epoch [68/120    avg_loss:0.012, val_acc:0.988]
Epoch [69/120    avg_loss:0.010, val_acc:0.991]
Epoch [70/120    avg_loss:0.020, val_acc:0.984]
Epoch [71/120    avg_loss:0.033, val_acc:0.981]
Epoch [72/120    avg_loss:0.023, val_acc:0.976]
Epoch [73/120    avg_loss:0.016, val_acc:0.983]
Epoch [74/120    avg_loss:0.016, val_acc:0.988]
Epoch [75/120    avg_loss:0.008, val_acc:0.989]
Epoch [76/120    avg_loss:0.010, val_acc:0.989]
Epoch [77/120    avg_loss:0.010, val_acc:0.990]
Epoch [78/120    avg_loss:0.009, val_acc:0.991]
Epoch [79/120    avg_loss:0.008, val_acc:0.992]
Epoch [80/120    avg_loss:0.036, val_acc:0.982]
Epoch [81/120    avg_loss:0.011, val_acc:0.991]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.010, val_acc:0.991]
Epoch [85/120    avg_loss:0.007, val_acc:0.992]
Epoch [86/120    avg_loss:0.008, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.006, val_acc:0.993]
Epoch [89/120    avg_loss:0.008, val_acc:0.993]
Epoch [90/120    avg_loss:0.006, val_acc:0.993]
Epoch [91/120    avg_loss:0.009, val_acc:0.993]
Epoch [92/120    avg_loss:0.012, val_acc:0.992]
Epoch [93/120    avg_loss:0.009, val_acc:0.992]
Epoch [94/120    avg_loss:0.006, val_acc:0.993]
Epoch [95/120    avg_loss:0.007, val_acc:0.993]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.993]
Epoch [98/120    avg_loss:0.006, val_acc:0.993]
Epoch [99/120    avg_loss:0.006, val_acc:0.993]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.004, val_acc:0.993]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.006, val_acc:0.993]
Epoch [104/120    avg_loss:0.008, val_acc:0.993]
Epoch [105/120    avg_loss:0.008, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.993]
Epoch [109/120    avg_loss:0.008, val_acc:0.993]
Epoch [110/120    avg_loss:0.010, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.007, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.010, val_acc:0.993]
Epoch [116/120    avg_loss:0.006, val_acc:0.993]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.992]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.007, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6432     0     0     0     0     0     0     0     0]
 [    0     2 18023     0    48     0    15     0     2     0]
 [    0     0     0  2028     2     0     0     0     4     2]
 [    0    46    19     0  2873     0     6     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4866     0     0    12]
 [    0     0     0     0     0     0     2  1282     0     6]
 [    0     3     0     1    44     0     0     0  3518     5]
 [    0     0     0     8    14    42     0     0     0   855]]

Accuracy:
99.2504759839009

F1 scores:
[       nan 0.9960511  0.99761984 0.99582617 0.96522762 0.9841629
 0.9964165  0.99688958 0.98778605 0.95052807]

Kappa:
0.9900723046481715
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f25d0c837f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.633, val_acc:0.505]
Epoch [2/120    avg_loss:1.002, val_acc:0.579]
Epoch [3/120    avg_loss:0.801, val_acc:0.688]
Epoch [4/120    avg_loss:0.613, val_acc:0.691]
Epoch [5/120    avg_loss:0.447, val_acc:0.744]
Epoch [6/120    avg_loss:0.400, val_acc:0.766]
Epoch [7/120    avg_loss:0.298, val_acc:0.870]
Epoch [8/120    avg_loss:0.262, val_acc:0.872]
Epoch [9/120    avg_loss:0.187, val_acc:0.851]
Epoch [10/120    avg_loss:0.204, val_acc:0.925]
Epoch [11/120    avg_loss:0.197, val_acc:0.910]
Epoch [12/120    avg_loss:0.143, val_acc:0.944]
Epoch [13/120    avg_loss:0.140, val_acc:0.915]
Epoch [14/120    avg_loss:0.116, val_acc:0.933]
Epoch [15/120    avg_loss:0.117, val_acc:0.951]
Epoch [16/120    avg_loss:0.177, val_acc:0.903]
Epoch [17/120    avg_loss:0.136, val_acc:0.955]
Epoch [18/120    avg_loss:0.065, val_acc:0.972]
Epoch [19/120    avg_loss:0.047, val_acc:0.972]
Epoch [20/120    avg_loss:0.048, val_acc:0.975]
Epoch [21/120    avg_loss:0.045, val_acc:0.978]
Epoch [22/120    avg_loss:0.045, val_acc:0.972]
Epoch [23/120    avg_loss:0.076, val_acc:0.972]
Epoch [24/120    avg_loss:0.086, val_acc:0.958]
Epoch [25/120    avg_loss:0.060, val_acc:0.970]
Epoch [26/120    avg_loss:0.048, val_acc:0.966]
Epoch [27/120    avg_loss:0.061, val_acc:0.970]
Epoch [28/120    avg_loss:0.051, val_acc:0.977]
Epoch [29/120    avg_loss:0.035, val_acc:0.975]
Epoch [30/120    avg_loss:0.029, val_acc:0.983]
Epoch [31/120    avg_loss:0.032, val_acc:0.980]
Epoch [32/120    avg_loss:0.029, val_acc:0.972]
Epoch [33/120    avg_loss:0.023, val_acc:0.984]
Epoch [34/120    avg_loss:0.021, val_acc:0.979]
Epoch [35/120    avg_loss:0.026, val_acc:0.970]
Epoch [36/120    avg_loss:0.029, val_acc:0.987]
Epoch [37/120    avg_loss:0.017, val_acc:0.988]
Epoch [38/120    avg_loss:0.017, val_acc:0.987]
Epoch [39/120    avg_loss:0.015, val_acc:0.987]
Epoch [40/120    avg_loss:0.015, val_acc:0.990]
Epoch [41/120    avg_loss:0.011, val_acc:0.991]
Epoch [42/120    avg_loss:0.020, val_acc:0.976]
Epoch [43/120    avg_loss:0.013, val_acc:0.984]
Epoch [44/120    avg_loss:0.011, val_acc:0.988]
Epoch [45/120    avg_loss:0.010, val_acc:0.989]
Epoch [46/120    avg_loss:0.008, val_acc:0.986]
Epoch [47/120    avg_loss:0.017, val_acc:0.987]
Epoch [48/120    avg_loss:0.031, val_acc:0.936]
Epoch [49/120    avg_loss:0.028, val_acc:0.982]
Epoch [50/120    avg_loss:0.023, val_acc:0.978]
Epoch [51/120    avg_loss:0.026, val_acc:0.973]
Epoch [52/120    avg_loss:0.019, val_acc:0.984]
Epoch [53/120    avg_loss:0.021, val_acc:0.975]
Epoch [54/120    avg_loss:0.015, val_acc:0.985]
Epoch [55/120    avg_loss:0.011, val_acc:0.988]
Epoch [56/120    avg_loss:0.007, val_acc:0.989]
Epoch [57/120    avg_loss:0.008, val_acc:0.990]
Epoch [58/120    avg_loss:0.008, val_acc:0.990]
Epoch [59/120    avg_loss:0.010, val_acc:0.990]
Epoch [60/120    avg_loss:0.009, val_acc:0.990]
Epoch [61/120    avg_loss:0.010, val_acc:0.990]
Epoch [62/120    avg_loss:0.008, val_acc:0.990]
Epoch [63/120    avg_loss:0.008, val_acc:0.991]
Epoch [64/120    avg_loss:0.009, val_acc:0.991]
Epoch [65/120    avg_loss:0.008, val_acc:0.991]
Epoch [66/120    avg_loss:0.008, val_acc:0.991]
Epoch [67/120    avg_loss:0.006, val_acc:0.990]
Epoch [68/120    avg_loss:0.007, val_acc:0.990]
Epoch [69/120    avg_loss:0.010, val_acc:0.990]
Epoch [70/120    avg_loss:0.013, val_acc:0.990]
Epoch [71/120    avg_loss:0.011, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.990]
Epoch [73/120    avg_loss:0.006, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.990]
Epoch [75/120    avg_loss:0.006, val_acc:0.990]
Epoch [76/120    avg_loss:0.007, val_acc:0.990]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.010, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.006, val_acc:0.990]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.010, val_acc:0.990]
Epoch [85/120    avg_loss:0.006, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.008, val_acc:0.990]
Epoch [89/120    avg_loss:0.012, val_acc:0.990]
Epoch [90/120    avg_loss:0.012, val_acc:0.990]
Epoch [91/120    avg_loss:0.010, val_acc:0.990]
Epoch [92/120    avg_loss:0.008, val_acc:0.990]
Epoch [93/120    avg_loss:0.012, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.007, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.008, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.007, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0     3     0     2    14     0     3]
 [    0     7 18026     0    52     0     2     0     2     1]
 [    0     0     0  2031     0     0     0     0     1     4]
 [    0    36    20     1  2877     0     6     0    28     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4857     0     0    21]
 [    0     0     0     0     0     0     0  1286     0     4]
 [    0     3     0     1    39     0     0     0  3528     0]
 [    0     0     0     4    14    39     0     0     0   862]]

Accuracy:
99.2504759839009

F1 scores:
[       nan 0.99472377 0.99767545 0.99729929 0.96592244 0.98527746
 0.99681888 0.99305019 0.98962132 0.94829483]

Kappa:
0.9900733353916492
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7019a3b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.608, val_acc:0.604]
Epoch [2/120    avg_loss:0.999, val_acc:0.643]
Epoch [3/120    avg_loss:0.729, val_acc:0.738]
Epoch [4/120    avg_loss:0.569, val_acc:0.705]
Epoch [5/120    avg_loss:0.480, val_acc:0.751]
Epoch [6/120    avg_loss:0.420, val_acc:0.776]
Epoch [7/120    avg_loss:0.319, val_acc:0.774]
Epoch [8/120    avg_loss:0.292, val_acc:0.897]
Epoch [9/120    avg_loss:0.231, val_acc:0.841]
Epoch [10/120    avg_loss:0.177, val_acc:0.922]
Epoch [11/120    avg_loss:0.146, val_acc:0.935]
Epoch [12/120    avg_loss:0.173, val_acc:0.938]
Epoch [13/120    avg_loss:0.120, val_acc:0.937]
Epoch [14/120    avg_loss:0.127, val_acc:0.947]
Epoch [15/120    avg_loss:0.101, val_acc:0.961]
Epoch [16/120    avg_loss:0.095, val_acc:0.956]
Epoch [17/120    avg_loss:0.095, val_acc:0.956]
Epoch [18/120    avg_loss:0.072, val_acc:0.972]
Epoch [19/120    avg_loss:0.060, val_acc:0.959]
Epoch [20/120    avg_loss:0.057, val_acc:0.973]
Epoch [21/120    avg_loss:0.057, val_acc:0.980]
Epoch [22/120    avg_loss:0.053, val_acc:0.978]
Epoch [23/120    avg_loss:0.068, val_acc:0.959]
Epoch [24/120    avg_loss:0.049, val_acc:0.973]
Epoch [25/120    avg_loss:0.060, val_acc:0.966]
Epoch [26/120    avg_loss:0.048, val_acc:0.977]
Epoch [27/120    avg_loss:0.051, val_acc:0.978]
Epoch [28/120    avg_loss:0.047, val_acc:0.974]
Epoch [29/120    avg_loss:0.025, val_acc:0.983]
Epoch [30/120    avg_loss:0.030, val_acc:0.956]
Epoch [31/120    avg_loss:0.031, val_acc:0.957]
Epoch [32/120    avg_loss:0.035, val_acc:0.982]
Epoch [33/120    avg_loss:0.029, val_acc:0.984]
Epoch [34/120    avg_loss:0.037, val_acc:0.963]
Epoch [35/120    avg_loss:0.018, val_acc:0.965]
Epoch [36/120    avg_loss:0.011, val_acc:0.988]
Epoch [37/120    avg_loss:0.014, val_acc:0.989]
Epoch [38/120    avg_loss:0.024, val_acc:0.985]
Epoch [39/120    avg_loss:0.024, val_acc:0.988]
Epoch [40/120    avg_loss:0.029, val_acc:0.980]
Epoch [41/120    avg_loss:0.021, val_acc:0.984]
Epoch [42/120    avg_loss:0.013, val_acc:0.984]
Epoch [43/120    avg_loss:0.016, val_acc:0.985]
Epoch [44/120    avg_loss:0.011, val_acc:0.983]
Epoch [45/120    avg_loss:0.014, val_acc:0.985]
Epoch [46/120    avg_loss:0.011, val_acc:0.980]
Epoch [47/120    avg_loss:0.016, val_acc:0.978]
Epoch [48/120    avg_loss:0.023, val_acc:0.987]
Epoch [49/120    avg_loss:0.013, val_acc:0.987]
Epoch [50/120    avg_loss:0.011, val_acc:0.984]
Epoch [51/120    avg_loss:0.009, val_acc:0.983]
Epoch [52/120    avg_loss:0.008, val_acc:0.984]
Epoch [53/120    avg_loss:0.010, val_acc:0.987]
Epoch [54/120    avg_loss:0.007, val_acc:0.986]
Epoch [55/120    avg_loss:0.007, val_acc:0.988]
Epoch [56/120    avg_loss:0.005, val_acc:0.990]
Epoch [57/120    avg_loss:0.009, val_acc:0.991]
Epoch [58/120    avg_loss:0.006, val_acc:0.991]
Epoch [59/120    avg_loss:0.005, val_acc:0.990]
Epoch [60/120    avg_loss:0.007, val_acc:0.989]
Epoch [61/120    avg_loss:0.007, val_acc:0.988]
Epoch [62/120    avg_loss:0.006, val_acc:0.986]
Epoch [63/120    avg_loss:0.005, val_acc:0.987]
Epoch [64/120    avg_loss:0.004, val_acc:0.987]
Epoch [65/120    avg_loss:0.009, val_acc:0.989]
Epoch [66/120    avg_loss:0.008, val_acc:0.990]
Epoch [67/120    avg_loss:0.009, val_acc:0.990]
Epoch [68/120    avg_loss:0.005, val_acc:0.989]
Epoch [69/120    avg_loss:0.008, val_acc:0.990]
Epoch [70/120    avg_loss:0.006, val_acc:0.991]
Epoch [71/120    avg_loss:0.005, val_acc:0.991]
Epoch [72/120    avg_loss:0.006, val_acc:0.991]
Epoch [73/120    avg_loss:0.007, val_acc:0.991]
Epoch [74/120    avg_loss:0.007, val_acc:0.991]
Epoch [75/120    avg_loss:0.006, val_acc:0.991]
Epoch [76/120    avg_loss:0.005, val_acc:0.991]
Epoch [77/120    avg_loss:0.005, val_acc:0.991]
Epoch [78/120    avg_loss:0.006, val_acc:0.991]
Epoch [79/120    avg_loss:0.006, val_acc:0.991]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.004, val_acc:0.991]
Epoch [82/120    avg_loss:0.008, val_acc:0.991]
Epoch [83/120    avg_loss:0.010, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.008, val_acc:0.991]
Epoch [86/120    avg_loss:0.009, val_acc:0.991]
Epoch [87/120    avg_loss:0.008, val_acc:0.991]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.991]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.010, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.009, val_acc:0.991]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.006, val_acc:0.991]
Epoch [99/120    avg_loss:0.006, val_acc:0.991]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.006, val_acc:0.991]
Epoch [102/120    avg_loss:0.007, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.006, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.991]
Epoch [106/120    avg_loss:0.005, val_acc:0.991]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.007, val_acc:0.991]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.991]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.006, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.008, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     0     0     4     1     0     1]
 [    0     0 18052     0    27     0    11     0     0     0]
 [    0     1     0  2033     0     0     0     0     0     2]
 [    0    42    20     0  2875     0     7     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4854     0     0    24]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     4     0     0    33     0     0     0  3534     0]
 [    0     0     0     4    14    43     0     0     0   858]]

Accuracy:
99.35410792181814

F1 scores:
[       nan 0.99589306 0.99839611 0.99828137 0.97111974 0.98379193
 0.99507995 0.99883676 0.99088742 0.95121951]

Kappa:
0.9914424407336103
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb1b239c748>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.594, val_acc:0.522]
Epoch [2/120    avg_loss:0.966, val_acc:0.650]
Epoch [3/120    avg_loss:0.702, val_acc:0.717]
Epoch [4/120    avg_loss:0.546, val_acc:0.723]
Epoch [5/120    avg_loss:0.433, val_acc:0.811]
Epoch [6/120    avg_loss:0.350, val_acc:0.830]
Epoch [7/120    avg_loss:0.320, val_acc:0.878]
Epoch [8/120    avg_loss:0.256, val_acc:0.902]
Epoch [9/120    avg_loss:0.279, val_acc:0.921]
Epoch [10/120    avg_loss:0.184, val_acc:0.935]
Epoch [11/120    avg_loss:0.161, val_acc:0.949]
Epoch [12/120    avg_loss:0.167, val_acc:0.927]
Epoch [13/120    avg_loss:0.139, val_acc:0.950]
Epoch [14/120    avg_loss:0.104, val_acc:0.963]
Epoch [15/120    avg_loss:0.089, val_acc:0.944]
Epoch [16/120    avg_loss:0.071, val_acc:0.950]
Epoch [17/120    avg_loss:0.080, val_acc:0.966]
Epoch [18/120    avg_loss:0.060, val_acc:0.972]
Epoch [19/120    avg_loss:0.062, val_acc:0.927]
Epoch [20/120    avg_loss:0.062, val_acc:0.973]
Epoch [21/120    avg_loss:0.046, val_acc:0.966]
Epoch [22/120    avg_loss:0.041, val_acc:0.978]
Epoch [23/120    avg_loss:0.035, val_acc:0.981]
Epoch [24/120    avg_loss:0.026, val_acc:0.981]
Epoch [25/120    avg_loss:0.053, val_acc:0.970]
Epoch [26/120    avg_loss:0.037, val_acc:0.974]
Epoch [27/120    avg_loss:0.042, val_acc:0.947]
Epoch [28/120    avg_loss:0.054, val_acc:0.972]
Epoch [29/120    avg_loss:0.058, val_acc:0.970]
Epoch [30/120    avg_loss:0.045, val_acc:0.963]
Epoch [31/120    avg_loss:0.033, val_acc:0.979]
Epoch [32/120    avg_loss:0.032, val_acc:0.960]
Epoch [33/120    avg_loss:0.035, val_acc:0.975]
Epoch [34/120    avg_loss:0.026, val_acc:0.979]
Epoch [35/120    avg_loss:0.019, val_acc:0.976]
Epoch [36/120    avg_loss:0.019, val_acc:0.984]
Epoch [37/120    avg_loss:0.014, val_acc:0.984]
Epoch [38/120    avg_loss:0.016, val_acc:0.981]
Epoch [39/120    avg_loss:0.047, val_acc:0.979]
Epoch [40/120    avg_loss:0.041, val_acc:0.959]
Epoch [41/120    avg_loss:0.089, val_acc:0.977]
Epoch [42/120    avg_loss:0.058, val_acc:0.973]
Epoch [43/120    avg_loss:0.035, val_acc:0.977]
Epoch [44/120    avg_loss:0.032, val_acc:0.982]
Epoch [45/120    avg_loss:0.033, val_acc:0.976]
Epoch [46/120    avg_loss:0.050, val_acc:0.978]
Epoch [47/120    avg_loss:0.025, val_acc:0.964]
Epoch [48/120    avg_loss:0.023, val_acc:0.978]
Epoch [49/120    avg_loss:0.013, val_acc:0.978]
Epoch [50/120    avg_loss:0.011, val_acc:0.979]
Epoch [51/120    avg_loss:0.012, val_acc:0.981]
Epoch [52/120    avg_loss:0.009, val_acc:0.985]
Epoch [53/120    avg_loss:0.008, val_acc:0.985]
Epoch [54/120    avg_loss:0.009, val_acc:0.985]
Epoch [55/120    avg_loss:0.013, val_acc:0.984]
Epoch [56/120    avg_loss:0.009, val_acc:0.985]
Epoch [57/120    avg_loss:0.007, val_acc:0.986]
Epoch [58/120    avg_loss:0.007, val_acc:0.985]
Epoch [59/120    avg_loss:0.009, val_acc:0.986]
Epoch [60/120    avg_loss:0.010, val_acc:0.987]
Epoch [61/120    avg_loss:0.006, val_acc:0.987]
Epoch [62/120    avg_loss:0.009, val_acc:0.987]
Epoch [63/120    avg_loss:0.010, val_acc:0.986]
Epoch [64/120    avg_loss:0.007, val_acc:0.987]
Epoch [65/120    avg_loss:0.009, val_acc:0.986]
Epoch [66/120    avg_loss:0.011, val_acc:0.987]
Epoch [67/120    avg_loss:0.013, val_acc:0.985]
Epoch [68/120    avg_loss:0.008, val_acc:0.985]
Epoch [69/120    avg_loss:0.008, val_acc:0.986]
Epoch [70/120    avg_loss:0.006, val_acc:0.985]
Epoch [71/120    avg_loss:0.008, val_acc:0.985]
Epoch [72/120    avg_loss:0.007, val_acc:0.986]
Epoch [73/120    avg_loss:0.005, val_acc:0.987]
Epoch [74/120    avg_loss:0.007, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.007, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.009, val_acc:0.989]
Epoch [90/120    avg_loss:0.006, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.004, val_acc:0.989]
Epoch [112/120    avg_loss:0.004, val_acc:0.989]
Epoch [113/120    avg_loss:0.008, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     4     0     4    24     0     0]
 [    0     0 18036     0    48     0     6     0     0     0]
 [    0     0     0  2014     1     0     0     0    16     5]
 [    0    36    19     1  2876     0     7     0    33     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4845     0     0    27]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0     2     0     0    33     0     0     0  3535     1]
 [    0     0     0    17    14    37     0     0     0   851]]

Accuracy:
99.16612440652641

F1 scores:
[       nan 0.99456099 0.99781472 0.99016716 0.96704775 0.98602191
 0.99466229 0.98884186 0.9881202  0.94241417]

Kappa:
0.9889545615681811
creating ./logs/logs-2022-01-20PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ba02697b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 122128==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:1.688, val_acc:0.365]
Epoch [2/120    avg_loss:0.974, val_acc:0.640]
Epoch [3/120    avg_loss:0.695, val_acc:0.770]
Epoch [4/120    avg_loss:0.473, val_acc:0.730]
Epoch [5/120    avg_loss:0.501, val_acc:0.864]
Epoch [6/120    avg_loss:0.371, val_acc:0.887]
Epoch [7/120    avg_loss:0.275, val_acc:0.897]
Epoch [8/120    avg_loss:0.274, val_acc:0.904]
Epoch [9/120    avg_loss:0.194, val_acc:0.927]
Epoch [10/120    avg_loss:0.136, val_acc:0.956]
Epoch [11/120    avg_loss:0.156, val_acc:0.941]
Epoch [12/120    avg_loss:0.130, val_acc:0.944]
Epoch [13/120    avg_loss:0.110, val_acc:0.931]
Epoch [14/120    avg_loss:0.112, val_acc:0.945]
Epoch [15/120    avg_loss:0.149, val_acc:0.948]
Epoch [16/120    avg_loss:0.070, val_acc:0.955]
Epoch [17/120    avg_loss:0.070, val_acc:0.962]
Epoch [18/120    avg_loss:0.066, val_acc:0.953]
Epoch [19/120    avg_loss:0.050, val_acc:0.962]
Epoch [20/120    avg_loss:0.036, val_acc:0.975]
Epoch [21/120    avg_loss:0.047, val_acc:0.969]
Epoch [22/120    avg_loss:0.075, val_acc:0.961]
Epoch [23/120    avg_loss:0.041, val_acc:0.966]
Epoch [24/120    avg_loss:0.053, val_acc:0.973]
Epoch [25/120    avg_loss:0.058, val_acc:0.929]
Epoch [26/120    avg_loss:0.099, val_acc:0.960]
Epoch [27/120    avg_loss:0.057, val_acc:0.972]
Epoch [28/120    avg_loss:0.041, val_acc:0.950]
Epoch [29/120    avg_loss:0.035, val_acc:0.973]
Epoch [30/120    avg_loss:0.043, val_acc:0.978]
Epoch [31/120    avg_loss:0.031, val_acc:0.977]
Epoch [32/120    avg_loss:0.027, val_acc:0.927]
Epoch [33/120    avg_loss:0.039, val_acc:0.974]
Epoch [34/120    avg_loss:0.047, val_acc:0.969]
Epoch [35/120    avg_loss:0.029, val_acc:0.979]
Epoch [36/120    avg_loss:0.018, val_acc:0.975]
Epoch [37/120    avg_loss:0.015, val_acc:0.982]
Epoch [38/120    avg_loss:0.014, val_acc:0.980]
Epoch [39/120    avg_loss:0.014, val_acc:0.984]
Epoch [40/120    avg_loss:0.014, val_acc:0.982]
Epoch [41/120    avg_loss:0.023, val_acc:0.974]
Epoch [42/120    avg_loss:0.038, val_acc:0.978]
Epoch [43/120    avg_loss:0.010, val_acc:0.978]
Epoch [44/120    avg_loss:0.013, val_acc:0.984]
Epoch [45/120    avg_loss:0.012, val_acc:0.984]
Epoch [46/120    avg_loss:0.013, val_acc:0.973]
Epoch [47/120    avg_loss:0.013, val_acc:0.985]
Epoch [48/120    avg_loss:0.012, val_acc:0.987]
Epoch [49/120    avg_loss:0.021, val_acc:0.972]
Epoch [50/120    avg_loss:0.034, val_acc:0.977]
Epoch [51/120    avg_loss:0.016, val_acc:0.979]
Epoch [52/120    avg_loss:0.008, val_acc:0.985]
Epoch [53/120    avg_loss:0.007, val_acc:0.981]
Epoch [54/120    avg_loss:0.013, val_acc:0.983]
Epoch [55/120    avg_loss:0.015, val_acc:0.981]
Epoch [56/120    avg_loss:0.011, val_acc:0.980]
Epoch [57/120    avg_loss:0.007, val_acc:0.984]
Epoch [58/120    avg_loss:0.007, val_acc:0.988]
Epoch [59/120    avg_loss:0.008, val_acc:0.985]
Epoch [60/120    avg_loss:0.008, val_acc:0.982]
Epoch [61/120    avg_loss:0.011, val_acc:0.986]
Epoch [62/120    avg_loss:0.013, val_acc:0.981]
Epoch [63/120    avg_loss:0.008, val_acc:0.984]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.984]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.016, val_acc:0.972]
Epoch [68/120    avg_loss:0.011, val_acc:0.984]
Epoch [69/120    avg_loss:0.006, val_acc:0.985]
Epoch [70/120    avg_loss:0.008, val_acc:0.983]
Epoch [71/120    avg_loss:0.006, val_acc:0.985]
Epoch [72/120    avg_loss:0.003, val_acc:0.985]
Epoch [73/120    avg_loss:0.006, val_acc:0.987]
Epoch [74/120    avg_loss:0.007, val_acc:0.986]
Epoch [75/120    avg_loss:0.005, val_acc:0.986]
Epoch [76/120    avg_loss:0.005, val_acc:0.986]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.986]
Epoch [80/120    avg_loss:0.004, val_acc:0.987]
Epoch [81/120    avg_loss:0.003, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.987]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.004, val_acc:0.987]
Epoch [85/120    avg_loss:0.003, val_acc:0.987]
Epoch [86/120    avg_loss:0.003, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.987]
Epoch [89/120    avg_loss:0.004, val_acc:0.987]
Epoch [90/120    avg_loss:0.003, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.003, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.003, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     1     0     0     0]
 [    0     4 18060     0    21     0     5     0     0     0]
 [    0     0     0  2005     3     0     0     0     5    23]
 [    0    47    19     0  2878     0     1     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4860     0     0    18]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     4     0     0    30     0     0     0  3518    19]
 [    0     0     0     0    14    40     0     0     0   865]]

Accuracy:
99.31313715566482

F1 scores:
[       nan 0.99566496 0.99864525 0.99232863 0.97262589 0.98490566
 0.99722992 0.9984472  0.98806347 0.93716143]

Kappa:
0.9908990485263444
