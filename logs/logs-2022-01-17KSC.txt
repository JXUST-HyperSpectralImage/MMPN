creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1de170d30>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.589, val_acc:0.348]
Epoch [2/120    avg_loss:2.444, val_acc:0.416]
Epoch [3/120    avg_loss:2.293, val_acc:0.520]
Epoch [4/120    avg_loss:2.137, val_acc:0.584]
Epoch [5/120    avg_loss:1.961, val_acc:0.600]
Epoch [6/120    avg_loss:1.744, val_acc:0.641]
Epoch [7/120    avg_loss:1.540, val_acc:0.672]
Epoch [8/120    avg_loss:1.306, val_acc:0.744]
Epoch [9/120    avg_loss:1.172, val_acc:0.801]
Epoch [10/120    avg_loss:0.999, val_acc:0.807]
Epoch [11/120    avg_loss:0.874, val_acc:0.846]
Epoch [12/120    avg_loss:0.834, val_acc:0.840]
Epoch [13/120    avg_loss:0.726, val_acc:0.854]
Epoch [14/120    avg_loss:0.617, val_acc:0.854]
Epoch [15/120    avg_loss:0.650, val_acc:0.865]
Epoch [16/120    avg_loss:0.589, val_acc:0.879]
Epoch [17/120    avg_loss:0.526, val_acc:0.861]
Epoch [18/120    avg_loss:0.517, val_acc:0.908]
Epoch [19/120    avg_loss:0.472, val_acc:0.893]
Epoch [20/120    avg_loss:0.465, val_acc:0.885]
Epoch [21/120    avg_loss:0.443, val_acc:0.891]
Epoch [22/120    avg_loss:0.422, val_acc:0.904]
Epoch [23/120    avg_loss:0.345, val_acc:0.910]
Epoch [24/120    avg_loss:0.350, val_acc:0.912]
Epoch [25/120    avg_loss:0.367, val_acc:0.914]
Epoch [26/120    avg_loss:0.325, val_acc:0.922]
Epoch [27/120    avg_loss:0.336, val_acc:0.904]
Epoch [28/120    avg_loss:0.317, val_acc:0.877]
Epoch [29/120    avg_loss:0.379, val_acc:0.930]
Epoch [30/120    avg_loss:0.319, val_acc:0.895]
Epoch [31/120    avg_loss:0.291, val_acc:0.922]
Epoch [32/120    avg_loss:0.273, val_acc:0.922]
Epoch [33/120    avg_loss:0.324, val_acc:0.900]
Epoch [34/120    avg_loss:0.337, val_acc:0.896]
Epoch [35/120    avg_loss:0.264, val_acc:0.893]
Epoch [36/120    avg_loss:0.329, val_acc:0.908]
Epoch [37/120    avg_loss:0.349, val_acc:0.926]
Epoch [38/120    avg_loss:0.232, val_acc:0.920]
Epoch [39/120    avg_loss:0.247, val_acc:0.932]
Epoch [40/120    avg_loss:0.248, val_acc:0.936]
Epoch [41/120    avg_loss:0.205, val_acc:0.941]
Epoch [42/120    avg_loss:0.229, val_acc:0.943]
Epoch [43/120    avg_loss:0.248, val_acc:0.951]
Epoch [44/120    avg_loss:0.253, val_acc:0.949]
Epoch [45/120    avg_loss:0.180, val_acc:0.951]
Epoch [46/120    avg_loss:0.224, val_acc:0.916]
Epoch [47/120    avg_loss:0.205, val_acc:0.947]
Epoch [48/120    avg_loss:0.163, val_acc:0.951]
Epoch [49/120    avg_loss:0.157, val_acc:0.959]
Epoch [50/120    avg_loss:0.157, val_acc:0.943]
Epoch [51/120    avg_loss:0.200, val_acc:0.934]
Epoch [52/120    avg_loss:0.166, val_acc:0.975]
Epoch [53/120    avg_loss:0.149, val_acc:0.924]
Epoch [54/120    avg_loss:0.196, val_acc:0.932]
Epoch [55/120    avg_loss:0.161, val_acc:0.947]
Epoch [56/120    avg_loss:0.146, val_acc:0.951]
Epoch [57/120    avg_loss:0.132, val_acc:0.930]
Epoch [58/120    avg_loss:0.107, val_acc:0.955]
Epoch [59/120    avg_loss:0.270, val_acc:0.922]
Epoch [60/120    avg_loss:0.227, val_acc:0.953]
Epoch [61/120    avg_loss:0.156, val_acc:0.959]
Epoch [62/120    avg_loss:0.105, val_acc:0.961]
Epoch [63/120    avg_loss:0.180, val_acc:0.932]
Epoch [64/120    avg_loss:0.190, val_acc:0.949]
Epoch [65/120    avg_loss:0.159, val_acc:0.949]
Epoch [66/120    avg_loss:0.137, val_acc:0.967]
Epoch [67/120    avg_loss:0.114, val_acc:0.963]
Epoch [68/120    avg_loss:0.101, val_acc:0.975]
Epoch [69/120    avg_loss:0.084, val_acc:0.980]
Epoch [70/120    avg_loss:0.080, val_acc:0.971]
Epoch [71/120    avg_loss:0.083, val_acc:0.971]
Epoch [72/120    avg_loss:0.078, val_acc:0.982]
Epoch [73/120    avg_loss:0.068, val_acc:0.980]
Epoch [74/120    avg_loss:0.069, val_acc:0.980]
Epoch [75/120    avg_loss:0.069, val_acc:0.977]
Epoch [76/120    avg_loss:0.067, val_acc:0.980]
Epoch [77/120    avg_loss:0.068, val_acc:0.980]
Epoch [78/120    avg_loss:0.069, val_acc:0.980]
Epoch [79/120    avg_loss:0.058, val_acc:0.980]
Epoch [80/120    avg_loss:0.064, val_acc:0.979]
Epoch [81/120    avg_loss:0.066, val_acc:0.979]
Epoch [82/120    avg_loss:0.065, val_acc:0.980]
Epoch [83/120    avg_loss:0.059, val_acc:0.979]
Epoch [84/120    avg_loss:0.063, val_acc:0.980]
Epoch [85/120    avg_loss:0.063, val_acc:0.975]
Epoch [86/120    avg_loss:0.057, val_acc:0.975]
Epoch [87/120    avg_loss:0.065, val_acc:0.975]
Epoch [88/120    avg_loss:0.059, val_acc:0.975]
Epoch [89/120    avg_loss:0.058, val_acc:0.977]
Epoch [90/120    avg_loss:0.062, val_acc:0.977]
Epoch [91/120    avg_loss:0.045, val_acc:0.977]
Epoch [92/120    avg_loss:0.052, val_acc:0.979]
Epoch [93/120    avg_loss:0.049, val_acc:0.979]
Epoch [94/120    avg_loss:0.043, val_acc:0.979]
Epoch [95/120    avg_loss:0.053, val_acc:0.980]
Epoch [96/120    avg_loss:0.047, val_acc:0.980]
Epoch [97/120    avg_loss:0.055, val_acc:0.979]
Epoch [98/120    avg_loss:0.054, val_acc:0.979]
Epoch [99/120    avg_loss:0.057, val_acc:0.979]
Epoch [100/120    avg_loss:0.051, val_acc:0.979]
Epoch [101/120    avg_loss:0.058, val_acc:0.979]
Epoch [102/120    avg_loss:0.064, val_acc:0.979]
Epoch [103/120    avg_loss:0.062, val_acc:0.979]
Epoch [104/120    avg_loss:0.049, val_acc:0.979]
Epoch [105/120    avg_loss:0.058, val_acc:0.979]
Epoch [106/120    avg_loss:0.062, val_acc:0.979]
Epoch [107/120    avg_loss:0.059, val_acc:0.979]
Epoch [108/120    avg_loss:0.062, val_acc:0.979]
Epoch [109/120    avg_loss:0.052, val_acc:0.979]
Epoch [110/120    avg_loss:0.051, val_acc:0.979]
Epoch [111/120    avg_loss:0.053, val_acc:0.980]
Epoch [112/120    avg_loss:0.054, val_acc:0.980]
Epoch [113/120    avg_loss:0.046, val_acc:0.980]
Epoch [114/120    avg_loss:0.060, val_acc:0.980]
Epoch [115/120    avg_loss:0.052, val_acc:0.980]
Epoch [116/120    avg_loss:0.054, val_acc:0.980]
Epoch [117/120    avg_loss:0.054, val_acc:0.980]
Epoch [118/120    avg_loss:0.045, val_acc:0.980]
Epoch [119/120    avg_loss:0.056, val_acc:0.980]
Epoch [120/120    avg_loss:0.056, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 222   3   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  26 118   0   0   1   0   0   0   0   0]
 [  0   8   0   0   0   0 198   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.31556503198294

F1 scores:
[       nan 0.99419448 0.93932584 0.98230088 0.91983122 0.86764706
 0.98019802 0.85082873 0.99359795 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9812400237468054
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7305339eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.188]
Epoch [2/120    avg_loss:2.569, val_acc:0.342]
Epoch [3/120    avg_loss:2.489, val_acc:0.449]
Epoch [4/120    avg_loss:2.385, val_acc:0.432]
Epoch [5/120    avg_loss:2.254, val_acc:0.453]
Epoch [6/120    avg_loss:2.109, val_acc:0.498]
Epoch [7/120    avg_loss:1.949, val_acc:0.588]
Epoch [8/120    avg_loss:1.790, val_acc:0.668]
Epoch [9/120    avg_loss:1.608, val_acc:0.703]
Epoch [10/120    avg_loss:1.415, val_acc:0.746]
Epoch [11/120    avg_loss:1.237, val_acc:0.764]
Epoch [12/120    avg_loss:1.108, val_acc:0.828]
Epoch [13/120    avg_loss:0.934, val_acc:0.832]
Epoch [14/120    avg_loss:0.822, val_acc:0.840]
Epoch [15/120    avg_loss:0.722, val_acc:0.838]
Epoch [16/120    avg_loss:0.750, val_acc:0.885]
Epoch [17/120    avg_loss:0.683, val_acc:0.836]
Epoch [18/120    avg_loss:0.602, val_acc:0.887]
Epoch [19/120    avg_loss:0.542, val_acc:0.885]
Epoch [20/120    avg_loss:0.540, val_acc:0.863]
Epoch [21/120    avg_loss:0.494, val_acc:0.906]
Epoch [22/120    avg_loss:0.470, val_acc:0.869]
Epoch [23/120    avg_loss:0.379, val_acc:0.922]
Epoch [24/120    avg_loss:0.414, val_acc:0.904]
Epoch [25/120    avg_loss:0.365, val_acc:0.910]
Epoch [26/120    avg_loss:0.364, val_acc:0.904]
Epoch [27/120    avg_loss:0.338, val_acc:0.912]
Epoch [28/120    avg_loss:0.390, val_acc:0.918]
Epoch [29/120    avg_loss:0.334, val_acc:0.934]
Epoch [30/120    avg_loss:0.366, val_acc:0.887]
Epoch [31/120    avg_loss:0.369, val_acc:0.924]
Epoch [32/120    avg_loss:0.267, val_acc:0.891]
Epoch [33/120    avg_loss:0.268, val_acc:0.959]
Epoch [34/120    avg_loss:0.228, val_acc:0.936]
Epoch [35/120    avg_loss:0.246, val_acc:0.947]
Epoch [36/120    avg_loss:0.218, val_acc:0.922]
Epoch [37/120    avg_loss:0.293, val_acc:0.941]
Epoch [38/120    avg_loss:0.208, val_acc:0.900]
Epoch [39/120    avg_loss:0.217, val_acc:0.957]
Epoch [40/120    avg_loss:0.240, val_acc:0.939]
Epoch [41/120    avg_loss:0.257, val_acc:0.881]
Epoch [42/120    avg_loss:0.241, val_acc:0.965]
Epoch [43/120    avg_loss:0.166, val_acc:0.961]
Epoch [44/120    avg_loss:0.204, val_acc:0.945]
Epoch [45/120    avg_loss:0.193, val_acc:0.959]
Epoch [46/120    avg_loss:0.213, val_acc:0.957]
Epoch [47/120    avg_loss:0.154, val_acc:0.957]
Epoch [48/120    avg_loss:0.198, val_acc:0.943]
Epoch [49/120    avg_loss:0.210, val_acc:0.945]
Epoch [50/120    avg_loss:0.158, val_acc:0.967]
Epoch [51/120    avg_loss:0.127, val_acc:0.967]
Epoch [52/120    avg_loss:0.118, val_acc:0.965]
Epoch [53/120    avg_loss:0.158, val_acc:0.953]
Epoch [54/120    avg_loss:0.135, val_acc:0.975]
Epoch [55/120    avg_loss:0.130, val_acc:0.973]
Epoch [56/120    avg_loss:0.127, val_acc:0.969]
Epoch [57/120    avg_loss:0.126, val_acc:0.951]
Epoch [58/120    avg_loss:0.133, val_acc:0.959]
Epoch [59/120    avg_loss:0.175, val_acc:0.965]
Epoch [60/120    avg_loss:0.103, val_acc:0.967]
Epoch [61/120    avg_loss:0.202, val_acc:0.912]
Epoch [62/120    avg_loss:0.119, val_acc:0.963]
Epoch [63/120    avg_loss:0.134, val_acc:0.967]
Epoch [64/120    avg_loss:0.144, val_acc:0.975]
Epoch [65/120    avg_loss:0.130, val_acc:0.941]
Epoch [66/120    avg_loss:0.166, val_acc:0.957]
Epoch [67/120    avg_loss:0.116, val_acc:0.955]
Epoch [68/120    avg_loss:0.099, val_acc:0.975]
Epoch [69/120    avg_loss:0.125, val_acc:0.951]
Epoch [70/120    avg_loss:0.149, val_acc:0.965]
Epoch [71/120    avg_loss:0.104, val_acc:0.975]
Epoch [72/120    avg_loss:0.108, val_acc:0.969]
Epoch [73/120    avg_loss:0.106, val_acc:0.959]
Epoch [74/120    avg_loss:0.072, val_acc:0.979]
Epoch [75/120    avg_loss:0.077, val_acc:0.963]
Epoch [76/120    avg_loss:0.094, val_acc:0.977]
Epoch [77/120    avg_loss:0.073, val_acc:0.977]
Epoch [78/120    avg_loss:0.055, val_acc:0.951]
Epoch [79/120    avg_loss:0.074, val_acc:0.986]
Epoch [80/120    avg_loss:0.049, val_acc:0.988]
Epoch [81/120    avg_loss:0.043, val_acc:0.982]
Epoch [82/120    avg_loss:0.052, val_acc:0.977]
Epoch [83/120    avg_loss:0.039, val_acc:0.984]
Epoch [84/120    avg_loss:0.071, val_acc:0.973]
Epoch [85/120    avg_loss:0.092, val_acc:0.902]
Epoch [86/120    avg_loss:0.090, val_acc:0.965]
Epoch [87/120    avg_loss:0.123, val_acc:0.926]
Epoch [88/120    avg_loss:0.101, val_acc:0.965]
Epoch [89/120    avg_loss:0.065, val_acc:0.975]
Epoch [90/120    avg_loss:0.095, val_acc:0.965]
Epoch [91/120    avg_loss:0.053, val_acc:0.980]
Epoch [92/120    avg_loss:0.064, val_acc:0.957]
Epoch [93/120    avg_loss:0.061, val_acc:0.984]
Epoch [94/120    avg_loss:0.051, val_acc:0.984]
Epoch [95/120    avg_loss:0.033, val_acc:0.984]
Epoch [96/120    avg_loss:0.036, val_acc:0.986]
Epoch [97/120    avg_loss:0.037, val_acc:0.986]
Epoch [98/120    avg_loss:0.029, val_acc:0.988]
Epoch [99/120    avg_loss:0.033, val_acc:0.988]
Epoch [100/120    avg_loss:0.029, val_acc:0.986]
Epoch [101/120    avg_loss:0.030, val_acc:0.986]
Epoch [102/120    avg_loss:0.030, val_acc:0.986]
Epoch [103/120    avg_loss:0.027, val_acc:0.986]
Epoch [104/120    avg_loss:0.025, val_acc:0.986]
Epoch [105/120    avg_loss:0.027, val_acc:0.984]
Epoch [106/120    avg_loss:0.030, val_acc:0.984]
Epoch [107/120    avg_loss:0.026, val_acc:0.984]
Epoch [108/120    avg_loss:0.025, val_acc:0.984]
Epoch [109/120    avg_loss:0.027, val_acc:0.984]
Epoch [110/120    avg_loss:0.025, val_acc:0.984]
Epoch [111/120    avg_loss:0.027, val_acc:0.984]
Epoch [112/120    avg_loss:0.026, val_acc:0.984]
Epoch [113/120    avg_loss:0.021, val_acc:0.984]
Epoch [114/120    avg_loss:0.023, val_acc:0.984]
Epoch [115/120    avg_loss:0.021, val_acc:0.984]
Epoch [116/120    avg_loss:0.026, val_acc:0.984]
Epoch [117/120    avg_loss:0.023, val_acc:0.984]
Epoch [118/120    avg_loss:0.025, val_acc:0.984]
Epoch [119/120    avg_loss:0.025, val_acc:0.984]
Epoch [120/120    avg_loss:0.022, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 221   2   0   0   0   5   2   0   0   0   0]
 [  0   0   0   0 204  21   0   0   0   0   2   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   2   0   0   2   0 202   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   3   0   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   3   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.23027718550107

F1 scores:
[       nan 0.99636364 0.95454545 0.98004435 0.89473684 0.85517241
 0.99019608 0.89247312 0.98591549 0.9978678  0.99726027 1.
 0.99667774 1.        ]

Kappa:
0.9802938682528635
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f41b438cd68>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.571, val_acc:0.418]
Epoch [2/120    avg_loss:2.428, val_acc:0.467]
Epoch [3/120    avg_loss:2.268, val_acc:0.510]
Epoch [4/120    avg_loss:2.086, val_acc:0.523]
Epoch [5/120    avg_loss:1.886, val_acc:0.584]
Epoch [6/120    avg_loss:1.688, val_acc:0.676]
Epoch [7/120    avg_loss:1.492, val_acc:0.717]
Epoch [8/120    avg_loss:1.313, val_acc:0.736]
Epoch [9/120    avg_loss:1.109, val_acc:0.822]
Epoch [10/120    avg_loss:0.961, val_acc:0.850]
Epoch [11/120    avg_loss:0.852, val_acc:0.848]
Epoch [12/120    avg_loss:0.756, val_acc:0.854]
Epoch [13/120    avg_loss:0.759, val_acc:0.887]
Epoch [14/120    avg_loss:0.603, val_acc:0.893]
Epoch [15/120    avg_loss:0.627, val_acc:0.850]
Epoch [16/120    avg_loss:0.568, val_acc:0.885]
Epoch [17/120    avg_loss:0.509, val_acc:0.920]
Epoch [18/120    avg_loss:0.481, val_acc:0.904]
Epoch [19/120    avg_loss:0.523, val_acc:0.893]
Epoch [20/120    avg_loss:0.474, val_acc:0.887]
Epoch [21/120    avg_loss:0.452, val_acc:0.908]
Epoch [22/120    avg_loss:0.437, val_acc:0.920]
Epoch [23/120    avg_loss:0.420, val_acc:0.908]
Epoch [24/120    avg_loss:0.363, val_acc:0.928]
Epoch [25/120    avg_loss:0.415, val_acc:0.914]
Epoch [26/120    avg_loss:0.381, val_acc:0.926]
Epoch [27/120    avg_loss:0.330, val_acc:0.941]
Epoch [28/120    avg_loss:0.300, val_acc:0.941]
Epoch [29/120    avg_loss:0.304, val_acc:0.941]
Epoch [30/120    avg_loss:0.288, val_acc:0.943]
Epoch [31/120    avg_loss:0.261, val_acc:0.938]
Epoch [32/120    avg_loss:0.305, val_acc:0.807]
Epoch [33/120    avg_loss:0.352, val_acc:0.928]
Epoch [34/120    avg_loss:0.269, val_acc:0.953]
Epoch [35/120    avg_loss:0.320, val_acc:0.924]
Epoch [36/120    avg_loss:0.211, val_acc:0.943]
Epoch [37/120    avg_loss:0.203, val_acc:0.961]
Epoch [38/120    avg_loss:0.220, val_acc:0.908]
Epoch [39/120    avg_loss:0.237, val_acc:0.949]
Epoch [40/120    avg_loss:0.193, val_acc:0.945]
Epoch [41/120    avg_loss:0.167, val_acc:0.949]
Epoch [42/120    avg_loss:0.243, val_acc:0.922]
Epoch [43/120    avg_loss:0.191, val_acc:0.959]
Epoch [44/120    avg_loss:0.176, val_acc:0.947]
Epoch [45/120    avg_loss:0.164, val_acc:0.957]
Epoch [46/120    avg_loss:0.149, val_acc:0.951]
Epoch [47/120    avg_loss:0.178, val_acc:0.914]
Epoch [48/120    avg_loss:0.241, val_acc:0.949]
Epoch [49/120    avg_loss:0.192, val_acc:0.967]
Epoch [50/120    avg_loss:0.163, val_acc:0.951]
Epoch [51/120    avg_loss:0.142, val_acc:0.967]
Epoch [52/120    avg_loss:0.194, val_acc:0.965]
Epoch [53/120    avg_loss:0.149, val_acc:0.967]
Epoch [54/120    avg_loss:0.112, val_acc:0.971]
Epoch [55/120    avg_loss:0.107, val_acc:0.963]
Epoch [56/120    avg_loss:0.122, val_acc:0.971]
Epoch [57/120    avg_loss:0.165, val_acc:0.961]
Epoch [58/120    avg_loss:0.092, val_acc:0.971]
Epoch [59/120    avg_loss:0.162, val_acc:0.971]
Epoch [60/120    avg_loss:0.104, val_acc:0.965]
Epoch [61/120    avg_loss:0.167, val_acc:0.982]
Epoch [62/120    avg_loss:0.123, val_acc:0.959]
Epoch [63/120    avg_loss:0.127, val_acc:0.986]
Epoch [64/120    avg_loss:0.092, val_acc:0.945]
Epoch [65/120    avg_loss:0.217, val_acc:0.949]
Epoch [66/120    avg_loss:0.149, val_acc:0.963]
Epoch [67/120    avg_loss:0.092, val_acc:0.977]
Epoch [68/120    avg_loss:0.096, val_acc:0.967]
Epoch [69/120    avg_loss:0.075, val_acc:0.971]
Epoch [70/120    avg_loss:0.076, val_acc:0.982]
Epoch [71/120    avg_loss:0.089, val_acc:0.977]
Epoch [72/120    avg_loss:0.107, val_acc:0.934]
Epoch [73/120    avg_loss:0.138, val_acc:0.965]
Epoch [74/120    avg_loss:0.073, val_acc:0.969]
Epoch [75/120    avg_loss:0.106, val_acc:0.957]
Epoch [76/120    avg_loss:0.151, val_acc:0.965]
Epoch [77/120    avg_loss:0.091, val_acc:0.986]
Epoch [78/120    avg_loss:0.051, val_acc:0.986]
Epoch [79/120    avg_loss:0.049, val_acc:0.982]
Epoch [80/120    avg_loss:0.047, val_acc:0.984]
Epoch [81/120    avg_loss:0.055, val_acc:0.984]
Epoch [82/120    avg_loss:0.044, val_acc:0.984]
Epoch [83/120    avg_loss:0.049, val_acc:0.984]
Epoch [84/120    avg_loss:0.047, val_acc:0.986]
Epoch [85/120    avg_loss:0.045, val_acc:0.984]
Epoch [86/120    avg_loss:0.049, val_acc:0.986]
Epoch [87/120    avg_loss:0.046, val_acc:0.984]
Epoch [88/120    avg_loss:0.038, val_acc:0.984]
Epoch [89/120    avg_loss:0.036, val_acc:0.984]
Epoch [90/120    avg_loss:0.030, val_acc:0.980]
Epoch [91/120    avg_loss:0.043, val_acc:0.988]
Epoch [92/120    avg_loss:0.044, val_acc:0.990]
Epoch [93/120    avg_loss:0.035, val_acc:0.990]
Epoch [94/120    avg_loss:0.032, val_acc:0.990]
Epoch [95/120    avg_loss:0.039, val_acc:0.990]
Epoch [96/120    avg_loss:0.033, val_acc:0.990]
Epoch [97/120    avg_loss:0.040, val_acc:0.988]
Epoch [98/120    avg_loss:0.033, val_acc:0.988]
Epoch [99/120    avg_loss:0.028, val_acc:0.988]
Epoch [100/120    avg_loss:0.030, val_acc:0.986]
Epoch [101/120    avg_loss:0.030, val_acc:0.990]
Epoch [102/120    avg_loss:0.037, val_acc:0.990]
Epoch [103/120    avg_loss:0.040, val_acc:0.990]
Epoch [104/120    avg_loss:0.031, val_acc:0.990]
Epoch [105/120    avg_loss:0.032, val_acc:0.990]
Epoch [106/120    avg_loss:0.029, val_acc:0.990]
Epoch [107/120    avg_loss:0.037, val_acc:0.990]
Epoch [108/120    avg_loss:0.034, val_acc:0.990]
Epoch [109/120    avg_loss:0.035, val_acc:0.990]
Epoch [110/120    avg_loss:0.038, val_acc:0.990]
Epoch [111/120    avg_loss:0.027, val_acc:0.990]
Epoch [112/120    avg_loss:0.033, val_acc:0.990]
Epoch [113/120    avg_loss:0.032, val_acc:0.990]
Epoch [114/120    avg_loss:0.030, val_acc:0.988]
Epoch [115/120    avg_loss:0.029, val_acc:0.988]
Epoch [116/120    avg_loss:0.033, val_acc:0.990]
Epoch [117/120    avg_loss:0.033, val_acc:0.990]
Epoch [118/120    avg_loss:0.037, val_acc:0.990]
Epoch [119/120    avg_loss:0.033, val_acc:0.988]
Epoch [120/120    avg_loss:0.030, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 215  11   0   0   0   0   4   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.96803653 0.96629213 0.90416667 0.87272727
 1.         0.93684211 0.99741602 0.99574468 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9843314654837938
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e30953dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.178]
Epoch [2/120    avg_loss:2.475, val_acc:0.371]
Epoch [3/120    avg_loss:2.356, val_acc:0.502]
Epoch [4/120    avg_loss:2.214, val_acc:0.535]
Epoch [5/120    avg_loss:2.036, val_acc:0.621]
Epoch [6/120    avg_loss:1.871, val_acc:0.635]
Epoch [7/120    avg_loss:1.674, val_acc:0.711]
Epoch [8/120    avg_loss:1.408, val_acc:0.762]
Epoch [9/120    avg_loss:1.250, val_acc:0.795]
Epoch [10/120    avg_loss:1.099, val_acc:0.814]
Epoch [11/120    avg_loss:0.908, val_acc:0.836]
Epoch [12/120    avg_loss:0.833, val_acc:0.859]
Epoch [13/120    avg_loss:0.746, val_acc:0.844]
Epoch [14/120    avg_loss:0.631, val_acc:0.887]
Epoch [15/120    avg_loss:0.593, val_acc:0.865]
Epoch [16/120    avg_loss:0.608, val_acc:0.842]
Epoch [17/120    avg_loss:0.631, val_acc:0.865]
Epoch [18/120    avg_loss:0.604, val_acc:0.893]
Epoch [19/120    avg_loss:0.502, val_acc:0.855]
Epoch [20/120    avg_loss:0.475, val_acc:0.879]
Epoch [21/120    avg_loss:0.452, val_acc:0.893]
Epoch [22/120    avg_loss:0.483, val_acc:0.877]
Epoch [23/120    avg_loss:0.416, val_acc:0.920]
Epoch [24/120    avg_loss:0.479, val_acc:0.887]
Epoch [25/120    avg_loss:0.392, val_acc:0.920]
Epoch [26/120    avg_loss:0.442, val_acc:0.900]
Epoch [27/120    avg_loss:0.366, val_acc:0.883]
Epoch [28/120    avg_loss:0.377, val_acc:0.896]
Epoch [29/120    avg_loss:0.371, val_acc:0.916]
Epoch [30/120    avg_loss:0.362, val_acc:0.922]
Epoch [31/120    avg_loss:0.368, val_acc:0.922]
Epoch [32/120    avg_loss:0.329, val_acc:0.918]
Epoch [33/120    avg_loss:0.337, val_acc:0.918]
Epoch [34/120    avg_loss:0.302, val_acc:0.934]
Epoch [35/120    avg_loss:0.267, val_acc:0.926]
Epoch [36/120    avg_loss:0.267, val_acc:0.920]
Epoch [37/120    avg_loss:0.286, val_acc:0.924]
Epoch [38/120    avg_loss:0.229, val_acc:0.934]
Epoch [39/120    avg_loss:0.260, val_acc:0.943]
Epoch [40/120    avg_loss:0.269, val_acc:0.928]
Epoch [41/120    avg_loss:0.286, val_acc:0.941]
Epoch [42/120    avg_loss:0.219, val_acc:0.932]
Epoch [43/120    avg_loss:0.226, val_acc:0.949]
Epoch [44/120    avg_loss:0.211, val_acc:0.953]
Epoch [45/120    avg_loss:0.162, val_acc:0.943]
Epoch [46/120    avg_loss:0.203, val_acc:0.908]
Epoch [47/120    avg_loss:0.247, val_acc:0.941]
Epoch [48/120    avg_loss:0.257, val_acc:0.893]
Epoch [49/120    avg_loss:0.259, val_acc:0.943]
Epoch [50/120    avg_loss:0.184, val_acc:0.945]
Epoch [51/120    avg_loss:0.203, val_acc:0.951]
Epoch [52/120    avg_loss:0.160, val_acc:0.953]
Epoch [53/120    avg_loss:0.194, val_acc:0.961]
Epoch [54/120    avg_loss:0.202, val_acc:0.920]
Epoch [55/120    avg_loss:0.202, val_acc:0.953]
Epoch [56/120    avg_loss:0.254, val_acc:0.914]
Epoch [57/120    avg_loss:0.233, val_acc:0.957]
Epoch [58/120    avg_loss:0.216, val_acc:0.959]
Epoch [59/120    avg_loss:0.187, val_acc:0.957]
Epoch [60/120    avg_loss:0.108, val_acc:0.969]
Epoch [61/120    avg_loss:0.105, val_acc:0.951]
Epoch [62/120    avg_loss:0.090, val_acc:0.945]
Epoch [63/120    avg_loss:0.169, val_acc:0.938]
Epoch [64/120    avg_loss:0.132, val_acc:0.961]
Epoch [65/120    avg_loss:0.128, val_acc:0.955]
Epoch [66/120    avg_loss:0.080, val_acc:0.965]
Epoch [67/120    avg_loss:0.112, val_acc:0.963]
Epoch [68/120    avg_loss:0.087, val_acc:0.973]
Epoch [69/120    avg_loss:0.080, val_acc:0.973]
Epoch [70/120    avg_loss:0.129, val_acc:0.973]
Epoch [71/120    avg_loss:0.076, val_acc:0.980]
Epoch [72/120    avg_loss:0.201, val_acc:0.941]
Epoch [73/120    avg_loss:0.121, val_acc:0.965]
Epoch [74/120    avg_loss:0.088, val_acc:0.969]
Epoch [75/120    avg_loss:0.093, val_acc:0.926]
Epoch [76/120    avg_loss:0.075, val_acc:0.977]
Epoch [77/120    avg_loss:0.069, val_acc:0.980]
Epoch [78/120    avg_loss:0.051, val_acc:0.980]
Epoch [79/120    avg_loss:0.086, val_acc:0.936]
Epoch [80/120    avg_loss:0.107, val_acc:0.971]
Epoch [81/120    avg_loss:0.074, val_acc:0.973]
Epoch [82/120    avg_loss:0.080, val_acc:0.973]
Epoch [83/120    avg_loss:0.137, val_acc:0.951]
Epoch [84/120    avg_loss:0.100, val_acc:0.969]
Epoch [85/120    avg_loss:0.088, val_acc:0.977]
Epoch [86/120    avg_loss:0.094, val_acc:0.936]
Epoch [87/120    avg_loss:0.092, val_acc:0.947]
Epoch [88/120    avg_loss:0.114, val_acc:0.961]
Epoch [89/120    avg_loss:0.101, val_acc:0.961]
Epoch [90/120    avg_loss:0.140, val_acc:0.971]
Epoch [91/120    avg_loss:0.060, val_acc:0.979]
Epoch [92/120    avg_loss:0.039, val_acc:0.980]
Epoch [93/120    avg_loss:0.035, val_acc:0.980]
Epoch [94/120    avg_loss:0.037, val_acc:0.980]
Epoch [95/120    avg_loss:0.037, val_acc:0.979]
Epoch [96/120    avg_loss:0.037, val_acc:0.980]
Epoch [97/120    avg_loss:0.031, val_acc:0.977]
Epoch [98/120    avg_loss:0.036, val_acc:0.979]
Epoch [99/120    avg_loss:0.031, val_acc:0.977]
Epoch [100/120    avg_loss:0.033, val_acc:0.979]
Epoch [101/120    avg_loss:0.028, val_acc:0.979]
Epoch [102/120    avg_loss:0.033, val_acc:0.980]
Epoch [103/120    avg_loss:0.026, val_acc:0.977]
Epoch [104/120    avg_loss:0.036, val_acc:0.980]
Epoch [105/120    avg_loss:0.023, val_acc:0.980]
Epoch [106/120    avg_loss:0.031, val_acc:0.979]
Epoch [107/120    avg_loss:0.025, val_acc:0.980]
Epoch [108/120    avg_loss:0.034, val_acc:0.982]
Epoch [109/120    avg_loss:0.026, val_acc:0.982]
Epoch [110/120    avg_loss:0.026, val_acc:0.982]
Epoch [111/120    avg_loss:0.025, val_acc:0.982]
Epoch [112/120    avg_loss:0.028, val_acc:0.980]
Epoch [113/120    avg_loss:0.019, val_acc:0.982]
Epoch [114/120    avg_loss:0.028, val_acc:0.980]
Epoch [115/120    avg_loss:0.028, val_acc:0.982]
Epoch [116/120    avg_loss:0.026, val_acc:0.982]
Epoch [117/120    avg_loss:0.035, val_acc:0.982]
Epoch [118/120    avg_loss:0.020, val_acc:0.984]
Epoch [119/120    avg_loss:0.023, val_acc:0.984]
Epoch [120/120    avg_loss:0.020, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   5   0   0   0   0   3   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   4   0   0   0   3   0   2   0 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 1.         0.96127563 0.98230088 0.91735537 0.89705882
 1.         0.92391304 0.99614891 1.         0.99726027 1.
 0.98666667 1.        ]

Kappa:
0.9852813239701672
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f43f3553da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.316]
Epoch [2/120    avg_loss:2.483, val_acc:0.363]
Epoch [3/120    avg_loss:2.374, val_acc:0.420]
Epoch [4/120    avg_loss:2.254, val_acc:0.473]
Epoch [5/120    avg_loss:2.093, val_acc:0.553]
Epoch [6/120    avg_loss:1.964, val_acc:0.566]
Epoch [7/120    avg_loss:1.752, val_acc:0.650]
Epoch [8/120    avg_loss:1.563, val_acc:0.707]
Epoch [9/120    avg_loss:1.444, val_acc:0.771]
Epoch [10/120    avg_loss:1.187, val_acc:0.785]
Epoch [11/120    avg_loss:1.037, val_acc:0.822]
Epoch [12/120    avg_loss:0.894, val_acc:0.848]
Epoch [13/120    avg_loss:0.783, val_acc:0.855]
Epoch [14/120    avg_loss:0.695, val_acc:0.846]
Epoch [15/120    avg_loss:0.636, val_acc:0.861]
Epoch [16/120    avg_loss:0.579, val_acc:0.889]
Epoch [17/120    avg_loss:0.518, val_acc:0.883]
Epoch [18/120    avg_loss:0.571, val_acc:0.883]
Epoch [19/120    avg_loss:0.514, val_acc:0.885]
Epoch [20/120    avg_loss:0.514, val_acc:0.885]
Epoch [21/120    avg_loss:0.437, val_acc:0.900]
Epoch [22/120    avg_loss:0.442, val_acc:0.904]
Epoch [23/120    avg_loss:0.472, val_acc:0.883]
Epoch [24/120    avg_loss:0.415, val_acc:0.896]
Epoch [25/120    avg_loss:0.406, val_acc:0.883]
Epoch [26/120    avg_loss:0.397, val_acc:0.908]
Epoch [27/120    avg_loss:0.334, val_acc:0.910]
Epoch [28/120    avg_loss:0.367, val_acc:0.920]
Epoch [29/120    avg_loss:0.312, val_acc:0.908]
Epoch [30/120    avg_loss:0.329, val_acc:0.914]
Epoch [31/120    avg_loss:0.306, val_acc:0.932]
Epoch [32/120    avg_loss:0.301, val_acc:0.920]
Epoch [33/120    avg_loss:0.292, val_acc:0.934]
Epoch [34/120    avg_loss:0.307, val_acc:0.943]
Epoch [35/120    avg_loss:0.274, val_acc:0.938]
Epoch [36/120    avg_loss:0.291, val_acc:0.936]
Epoch [37/120    avg_loss:0.270, val_acc:0.922]
Epoch [38/120    avg_loss:0.265, val_acc:0.918]
Epoch [39/120    avg_loss:0.271, val_acc:0.930]
Epoch [40/120    avg_loss:0.260, val_acc:0.916]
Epoch [41/120    avg_loss:0.233, val_acc:0.945]
Epoch [42/120    avg_loss:0.221, val_acc:0.932]
Epoch [43/120    avg_loss:0.203, val_acc:0.941]
Epoch [44/120    avg_loss:0.230, val_acc:0.936]
Epoch [45/120    avg_loss:0.237, val_acc:0.949]
Epoch [46/120    avg_loss:0.188, val_acc:0.947]
Epoch [47/120    avg_loss:0.193, val_acc:0.947]
Epoch [48/120    avg_loss:0.242, val_acc:0.943]
Epoch [49/120    avg_loss:0.254, val_acc:0.949]
Epoch [50/120    avg_loss:0.266, val_acc:0.934]
Epoch [51/120    avg_loss:0.201, val_acc:0.949]
Epoch [52/120    avg_loss:0.172, val_acc:0.955]
Epoch [53/120    avg_loss:0.212, val_acc:0.949]
Epoch [54/120    avg_loss:0.150, val_acc:0.963]
Epoch [55/120    avg_loss:0.171, val_acc:0.953]
Epoch [56/120    avg_loss:0.127, val_acc:0.957]
Epoch [57/120    avg_loss:0.156, val_acc:0.961]
Epoch [58/120    avg_loss:0.124, val_acc:0.943]
Epoch [59/120    avg_loss:0.183, val_acc:0.973]
Epoch [60/120    avg_loss:0.142, val_acc:0.967]
Epoch [61/120    avg_loss:0.147, val_acc:0.957]
Epoch [62/120    avg_loss:0.127, val_acc:0.953]
Epoch [63/120    avg_loss:0.151, val_acc:0.955]
Epoch [64/120    avg_loss:0.099, val_acc:0.963]
Epoch [65/120    avg_loss:0.105, val_acc:0.939]
Epoch [66/120    avg_loss:0.168, val_acc:0.959]
Epoch [67/120    avg_loss:0.121, val_acc:0.957]
Epoch [68/120    avg_loss:0.094, val_acc:0.963]
Epoch [69/120    avg_loss:0.140, val_acc:0.969]
Epoch [70/120    avg_loss:0.102, val_acc:0.965]
Epoch [71/120    avg_loss:0.163, val_acc:0.957]
Epoch [72/120    avg_loss:0.125, val_acc:0.965]
Epoch [73/120    avg_loss:0.075, val_acc:0.979]
Epoch [74/120    avg_loss:0.066, val_acc:0.982]
Epoch [75/120    avg_loss:0.058, val_acc:0.980]
Epoch [76/120    avg_loss:0.065, val_acc:0.982]
Epoch [77/120    avg_loss:0.066, val_acc:0.980]
Epoch [78/120    avg_loss:0.068, val_acc:0.980]
Epoch [79/120    avg_loss:0.067, val_acc:0.977]
Epoch [80/120    avg_loss:0.060, val_acc:0.982]
Epoch [81/120    avg_loss:0.057, val_acc:0.986]
Epoch [82/120    avg_loss:0.050, val_acc:0.986]
Epoch [83/120    avg_loss:0.057, val_acc:0.984]
Epoch [84/120    avg_loss:0.055, val_acc:0.980]
Epoch [85/120    avg_loss:0.053, val_acc:0.980]
Epoch [86/120    avg_loss:0.063, val_acc:0.984]
Epoch [87/120    avg_loss:0.053, val_acc:0.984]
Epoch [88/120    avg_loss:0.047, val_acc:0.986]
Epoch [89/120    avg_loss:0.060, val_acc:0.986]
Epoch [90/120    avg_loss:0.045, val_acc:0.980]
Epoch [91/120    avg_loss:0.054, val_acc:0.980]
Epoch [92/120    avg_loss:0.054, val_acc:0.984]
Epoch [93/120    avg_loss:0.053, val_acc:0.986]
Epoch [94/120    avg_loss:0.051, val_acc:0.982]
Epoch [95/120    avg_loss:0.047, val_acc:0.980]
Epoch [96/120    avg_loss:0.044, val_acc:0.986]
Epoch [97/120    avg_loss:0.045, val_acc:0.986]
Epoch [98/120    avg_loss:0.054, val_acc:0.986]
Epoch [99/120    avg_loss:0.065, val_acc:0.979]
Epoch [100/120    avg_loss:0.055, val_acc:0.975]
Epoch [101/120    avg_loss:0.046, val_acc:0.982]
Epoch [102/120    avg_loss:0.052, val_acc:0.982]
Epoch [103/120    avg_loss:0.049, val_acc:0.980]
Epoch [104/120    avg_loss:0.045, val_acc:0.975]
Epoch [105/120    avg_loss:0.047, val_acc:0.977]
Epoch [106/120    avg_loss:0.046, val_acc:0.982]
Epoch [107/120    avg_loss:0.058, val_acc:0.979]
Epoch [108/120    avg_loss:0.041, val_acc:0.979]
Epoch [109/120    avg_loss:0.046, val_acc:0.979]
Epoch [110/120    avg_loss:0.040, val_acc:0.975]
Epoch [111/120    avg_loss:0.049, val_acc:0.975]
Epoch [112/120    avg_loss:0.044, val_acc:0.975]
Epoch [113/120    avg_loss:0.041, val_acc:0.977]
Epoch [114/120    avg_loss:0.041, val_acc:0.979]
Epoch [115/120    avg_loss:0.041, val_acc:0.980]
Epoch [116/120    avg_loss:0.039, val_acc:0.979]
Epoch [117/120    avg_loss:0.051, val_acc:0.979]
Epoch [118/120    avg_loss:0.046, val_acc:0.977]
Epoch [119/120    avg_loss:0.038, val_acc:0.979]
Epoch [120/120    avg_loss:0.047, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 218   7   0   0   0   1   4   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 449   3]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.99573560767591

F1 scores:
[       nan 0.99636364 0.94144144 0.97321429 0.88602151 0.83916084
 0.98771499 0.85714286 0.99742931 0.99574468 1.         0.99867198
 0.9944629  0.99820467]

Kappa:
0.977679685719282
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f78fa4d3e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.561, val_acc:0.330]
Epoch [2/120    avg_loss:2.417, val_acc:0.359]
Epoch [3/120    avg_loss:2.293, val_acc:0.410]
Epoch [4/120    avg_loss:2.149, val_acc:0.498]
Epoch [5/120    avg_loss:2.000, val_acc:0.568]
Epoch [6/120    avg_loss:1.805, val_acc:0.637]
Epoch [7/120    avg_loss:1.624, val_acc:0.602]
Epoch [8/120    avg_loss:1.439, val_acc:0.752]
Epoch [9/120    avg_loss:1.274, val_acc:0.746]
Epoch [10/120    avg_loss:1.091, val_acc:0.801]
Epoch [11/120    avg_loss:0.995, val_acc:0.805]
Epoch [12/120    avg_loss:0.913, val_acc:0.791]
Epoch [13/120    avg_loss:0.889, val_acc:0.855]
Epoch [14/120    avg_loss:0.798, val_acc:0.871]
Epoch [15/120    avg_loss:0.758, val_acc:0.850]
Epoch [16/120    avg_loss:0.628, val_acc:0.850]
Epoch [17/120    avg_loss:0.609, val_acc:0.877]
Epoch [18/120    avg_loss:0.559, val_acc:0.842]
Epoch [19/120    avg_loss:0.541, val_acc:0.885]
Epoch [20/120    avg_loss:0.523, val_acc:0.900]
Epoch [21/120    avg_loss:0.427, val_acc:0.914]
Epoch [22/120    avg_loss:0.458, val_acc:0.900]
Epoch [23/120    avg_loss:0.430, val_acc:0.914]
Epoch [24/120    avg_loss:0.418, val_acc:0.906]
Epoch [25/120    avg_loss:0.391, val_acc:0.896]
Epoch [26/120    avg_loss:0.353, val_acc:0.934]
Epoch [27/120    avg_loss:0.340, val_acc:0.914]
Epoch [28/120    avg_loss:0.318, val_acc:0.941]
Epoch [29/120    avg_loss:0.426, val_acc:0.916]
Epoch [30/120    avg_loss:0.395, val_acc:0.932]
Epoch [31/120    avg_loss:0.303, val_acc:0.926]
Epoch [32/120    avg_loss:0.303, val_acc:0.945]
Epoch [33/120    avg_loss:0.339, val_acc:0.938]
Epoch [34/120    avg_loss:0.250, val_acc:0.949]
Epoch [35/120    avg_loss:0.334, val_acc:0.922]
Epoch [36/120    avg_loss:0.267, val_acc:0.951]
Epoch [37/120    avg_loss:0.241, val_acc:0.939]
Epoch [38/120    avg_loss:0.243, val_acc:0.939]
Epoch [39/120    avg_loss:0.269, val_acc:0.922]
Epoch [40/120    avg_loss:0.243, val_acc:0.953]
Epoch [41/120    avg_loss:0.209, val_acc:0.947]
Epoch [42/120    avg_loss:0.175, val_acc:0.965]
Epoch [43/120    avg_loss:0.248, val_acc:0.949]
Epoch [44/120    avg_loss:0.175, val_acc:0.971]
Epoch [45/120    avg_loss:0.217, val_acc:0.936]
Epoch [46/120    avg_loss:0.190, val_acc:0.943]
Epoch [47/120    avg_loss:0.201, val_acc:0.963]
Epoch [48/120    avg_loss:0.181, val_acc:0.947]
Epoch [49/120    avg_loss:0.182, val_acc:0.951]
Epoch [50/120    avg_loss:0.141, val_acc:0.969]
Epoch [51/120    avg_loss:0.141, val_acc:0.965]
Epoch [52/120    avg_loss:0.143, val_acc:0.959]
Epoch [53/120    avg_loss:0.142, val_acc:0.961]
Epoch [54/120    avg_loss:0.161, val_acc:0.953]
Epoch [55/120    avg_loss:0.127, val_acc:0.963]
Epoch [56/120    avg_loss:0.145, val_acc:0.953]
Epoch [57/120    avg_loss:0.211, val_acc:0.941]
Epoch [58/120    avg_loss:0.141, val_acc:0.973]
Epoch [59/120    avg_loss:0.125, val_acc:0.975]
Epoch [60/120    avg_loss:0.102, val_acc:0.975]
Epoch [61/120    avg_loss:0.098, val_acc:0.979]
Epoch [62/120    avg_loss:0.076, val_acc:0.980]
Epoch [63/120    avg_loss:0.086, val_acc:0.980]
Epoch [64/120    avg_loss:0.082, val_acc:0.979]
Epoch [65/120    avg_loss:0.079, val_acc:0.980]
Epoch [66/120    avg_loss:0.076, val_acc:0.979]
Epoch [67/120    avg_loss:0.074, val_acc:0.980]
Epoch [68/120    avg_loss:0.075, val_acc:0.979]
Epoch [69/120    avg_loss:0.067, val_acc:0.979]
Epoch [70/120    avg_loss:0.062, val_acc:0.979]
Epoch [71/120    avg_loss:0.067, val_acc:0.979]
Epoch [72/120    avg_loss:0.070, val_acc:0.980]
Epoch [73/120    avg_loss:0.066, val_acc:0.980]
Epoch [74/120    avg_loss:0.064, val_acc:0.980]
Epoch [75/120    avg_loss:0.065, val_acc:0.980]
Epoch [76/120    avg_loss:0.060, val_acc:0.982]
Epoch [77/120    avg_loss:0.066, val_acc:0.979]
Epoch [78/120    avg_loss:0.065, val_acc:0.979]
Epoch [79/120    avg_loss:0.061, val_acc:0.982]
Epoch [80/120    avg_loss:0.065, val_acc:0.980]
Epoch [81/120    avg_loss:0.056, val_acc:0.980]
Epoch [82/120    avg_loss:0.071, val_acc:0.982]
Epoch [83/120    avg_loss:0.059, val_acc:0.980]
Epoch [84/120    avg_loss:0.054, val_acc:0.980]
Epoch [85/120    avg_loss:0.055, val_acc:0.980]
Epoch [86/120    avg_loss:0.060, val_acc:0.980]
Epoch [87/120    avg_loss:0.056, val_acc:0.980]
Epoch [88/120    avg_loss:0.049, val_acc:0.980]
Epoch [89/120    avg_loss:0.063, val_acc:0.980]
Epoch [90/120    avg_loss:0.053, val_acc:0.982]
Epoch [91/120    avg_loss:0.051, val_acc:0.982]
Epoch [92/120    avg_loss:0.049, val_acc:0.982]
Epoch [93/120    avg_loss:0.062, val_acc:0.980]
Epoch [94/120    avg_loss:0.053, val_acc:0.982]
Epoch [95/120    avg_loss:0.053, val_acc:0.980]
Epoch [96/120    avg_loss:0.052, val_acc:0.980]
Epoch [97/120    avg_loss:0.051, val_acc:0.980]
Epoch [98/120    avg_loss:0.047, val_acc:0.982]
Epoch [99/120    avg_loss:0.046, val_acc:0.982]
Epoch [100/120    avg_loss:0.047, val_acc:0.980]
Epoch [101/120    avg_loss:0.043, val_acc:0.980]
Epoch [102/120    avg_loss:0.057, val_acc:0.980]
Epoch [103/120    avg_loss:0.049, val_acc:0.980]
Epoch [104/120    avg_loss:0.047, val_acc:0.980]
Epoch [105/120    avg_loss:0.042, val_acc:0.980]
Epoch [106/120    avg_loss:0.046, val_acc:0.980]
Epoch [107/120    avg_loss:0.041, val_acc:0.982]
Epoch [108/120    avg_loss:0.042, val_acc:0.982]
Epoch [109/120    avg_loss:0.047, val_acc:0.979]
Epoch [110/120    avg_loss:0.048, val_acc:0.980]
Epoch [111/120    avg_loss:0.043, val_acc:0.980]
Epoch [112/120    avg_loss:0.047, val_acc:0.980]
Epoch [113/120    avg_loss:0.045, val_acc:0.980]
Epoch [114/120    avg_loss:0.039, val_acc:0.980]
Epoch [115/120    avg_loss:0.051, val_acc:0.984]
Epoch [116/120    avg_loss:0.040, val_acc:0.980]
Epoch [117/120    avg_loss:0.042, val_acc:0.980]
Epoch [118/120    avg_loss:0.045, val_acc:0.982]
Epoch [119/120    avg_loss:0.037, val_acc:0.980]
Epoch [120/120    avg_loss:0.048, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   8   0   0   0   0   1   0]
 [  0   0   0 211  17   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 0.99927061 0.94808126 0.9569161  0.91836735 0.91512915
 0.99756691 0.89130435 0.99742931 1.         1.         0.99734043
 0.99448732 1.        ]

Kappa:
0.9838558384895398
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f76ed9dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.547, val_acc:0.391]
Epoch [2/120    avg_loss:2.418, val_acc:0.430]
Epoch [3/120    avg_loss:2.267, val_acc:0.506]
Epoch [4/120    avg_loss:2.106, val_acc:0.602]
Epoch [5/120    avg_loss:1.919, val_acc:0.621]
Epoch [6/120    avg_loss:1.711, val_acc:0.715]
Epoch [7/120    avg_loss:1.463, val_acc:0.729]
Epoch [8/120    avg_loss:1.286, val_acc:0.770]
Epoch [9/120    avg_loss:1.070, val_acc:0.797]
Epoch [10/120    avg_loss:0.985, val_acc:0.832]
Epoch [11/120    avg_loss:0.806, val_acc:0.850]
Epoch [12/120    avg_loss:0.851, val_acc:0.848]
Epoch [13/120    avg_loss:0.714, val_acc:0.834]
Epoch [14/120    avg_loss:0.623, val_acc:0.863]
Epoch [15/120    avg_loss:0.603, val_acc:0.832]
Epoch [16/120    avg_loss:0.554, val_acc:0.893]
Epoch [17/120    avg_loss:0.534, val_acc:0.877]
Epoch [18/120    avg_loss:0.494, val_acc:0.814]
Epoch [19/120    avg_loss:0.521, val_acc:0.871]
Epoch [20/120    avg_loss:0.488, val_acc:0.906]
Epoch [21/120    avg_loss:0.476, val_acc:0.914]
Epoch [22/120    avg_loss:0.455, val_acc:0.912]
Epoch [23/120    avg_loss:0.449, val_acc:0.889]
Epoch [24/120    avg_loss:0.435, val_acc:0.900]
Epoch [25/120    avg_loss:0.391, val_acc:0.920]
Epoch [26/120    avg_loss:0.354, val_acc:0.939]
Epoch [27/120    avg_loss:0.329, val_acc:0.926]
Epoch [28/120    avg_loss:0.306, val_acc:0.949]
Epoch [29/120    avg_loss:0.312, val_acc:0.941]
Epoch [30/120    avg_loss:0.287, val_acc:0.961]
Epoch [31/120    avg_loss:0.274, val_acc:0.945]
Epoch [32/120    avg_loss:0.323, val_acc:0.936]
Epoch [33/120    avg_loss:0.228, val_acc:0.975]
Epoch [34/120    avg_loss:0.204, val_acc:0.963]
Epoch [35/120    avg_loss:0.258, val_acc:0.941]
Epoch [36/120    avg_loss:0.242, val_acc:0.965]
Epoch [37/120    avg_loss:0.188, val_acc:0.959]
Epoch [38/120    avg_loss:0.163, val_acc:0.967]
Epoch [39/120    avg_loss:0.194, val_acc:0.941]
Epoch [40/120    avg_loss:0.265, val_acc:0.955]
Epoch [41/120    avg_loss:0.185, val_acc:0.975]
Epoch [42/120    avg_loss:0.221, val_acc:0.965]
Epoch [43/120    avg_loss:0.181, val_acc:0.947]
Epoch [44/120    avg_loss:0.218, val_acc:0.951]
Epoch [45/120    avg_loss:0.221, val_acc:0.963]
Epoch [46/120    avg_loss:0.203, val_acc:0.969]
Epoch [47/120    avg_loss:0.163, val_acc:0.961]
Epoch [48/120    avg_loss:0.135, val_acc:0.980]
Epoch [49/120    avg_loss:0.104, val_acc:0.979]
Epoch [50/120    avg_loss:0.148, val_acc:0.971]
Epoch [51/120    avg_loss:0.093, val_acc:0.977]
Epoch [52/120    avg_loss:0.130, val_acc:0.986]
Epoch [53/120    avg_loss:0.109, val_acc:0.982]
Epoch [54/120    avg_loss:0.145, val_acc:0.977]
Epoch [55/120    avg_loss:0.151, val_acc:0.957]
Epoch [56/120    avg_loss:0.115, val_acc:0.977]
Epoch [57/120    avg_loss:0.109, val_acc:0.961]
Epoch [58/120    avg_loss:0.107, val_acc:0.963]
Epoch [59/120    avg_loss:0.145, val_acc:0.955]
Epoch [60/120    avg_loss:0.128, val_acc:0.992]
Epoch [61/120    avg_loss:0.094, val_acc:0.988]
Epoch [62/120    avg_loss:0.059, val_acc:0.984]
Epoch [63/120    avg_loss:0.098, val_acc:0.980]
Epoch [64/120    avg_loss:0.108, val_acc:0.979]
Epoch [65/120    avg_loss:0.123, val_acc:0.961]
Epoch [66/120    avg_loss:0.105, val_acc:0.992]
Epoch [67/120    avg_loss:0.066, val_acc:0.984]
Epoch [68/120    avg_loss:0.087, val_acc:0.980]
Epoch [69/120    avg_loss:0.084, val_acc:0.965]
Epoch [70/120    avg_loss:0.096, val_acc:0.977]
Epoch [71/120    avg_loss:0.082, val_acc:0.980]
Epoch [72/120    avg_loss:0.126, val_acc:0.975]
Epoch [73/120    avg_loss:0.158, val_acc:0.977]
Epoch [74/120    avg_loss:0.077, val_acc:0.994]
Epoch [75/120    avg_loss:0.074, val_acc:0.967]
Epoch [76/120    avg_loss:0.066, val_acc:0.973]
Epoch [77/120    avg_loss:0.110, val_acc:0.988]
Epoch [78/120    avg_loss:0.064, val_acc:0.986]
Epoch [79/120    avg_loss:0.068, val_acc:0.992]
Epoch [80/120    avg_loss:0.061, val_acc:0.994]
Epoch [81/120    avg_loss:0.058, val_acc:0.980]
Epoch [82/120    avg_loss:0.075, val_acc:0.986]
Epoch [83/120    avg_loss:0.073, val_acc:0.980]
Epoch [84/120    avg_loss:0.058, val_acc:0.988]
Epoch [85/120    avg_loss:0.092, val_acc:0.982]
Epoch [86/120    avg_loss:0.054, val_acc:0.977]
Epoch [87/120    avg_loss:0.052, val_acc:0.984]
Epoch [88/120    avg_loss:0.045, val_acc:0.992]
Epoch [89/120    avg_loss:0.031, val_acc:0.992]
Epoch [90/120    avg_loss:0.029, val_acc:0.990]
Epoch [91/120    avg_loss:0.032, val_acc:0.988]
Epoch [92/120    avg_loss:0.035, val_acc:0.986]
Epoch [93/120    avg_loss:0.073, val_acc:0.939]
Epoch [94/120    avg_loss:0.064, val_acc:0.988]
Epoch [95/120    avg_loss:0.040, val_acc:0.994]
Epoch [96/120    avg_loss:0.041, val_acc:0.996]
Epoch [97/120    avg_loss:0.031, val_acc:0.994]
Epoch [98/120    avg_loss:0.032, val_acc:0.994]
Epoch [99/120    avg_loss:0.037, val_acc:0.996]
Epoch [100/120    avg_loss:0.031, val_acc:0.996]
Epoch [101/120    avg_loss:0.032, val_acc:0.996]
Epoch [102/120    avg_loss:0.023, val_acc:0.996]
Epoch [103/120    avg_loss:0.022, val_acc:0.996]
Epoch [104/120    avg_loss:0.020, val_acc:0.996]
Epoch [105/120    avg_loss:0.023, val_acc:0.996]
Epoch [106/120    avg_loss:0.023, val_acc:0.996]
Epoch [107/120    avg_loss:0.023, val_acc:0.996]
Epoch [108/120    avg_loss:0.016, val_acc:0.996]
Epoch [109/120    avg_loss:0.021, val_acc:0.996]
Epoch [110/120    avg_loss:0.017, val_acc:0.996]
Epoch [111/120    avg_loss:0.018, val_acc:0.996]
Epoch [112/120    avg_loss:0.019, val_acc:0.996]
Epoch [113/120    avg_loss:0.015, val_acc:0.996]
Epoch [114/120    avg_loss:0.018, val_acc:0.996]
Epoch [115/120    avg_loss:0.019, val_acc:0.996]
Epoch [116/120    avg_loss:0.013, val_acc:0.996]
Epoch [117/120    avg_loss:0.014, val_acc:0.996]
Epoch [118/120    avg_loss:0.012, val_acc:0.996]
Epoch [119/120    avg_loss:0.013, val_acc:0.996]
Epoch [120/120    avg_loss:0.014, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   1 218   3   0   0   0   8   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 0.99927061 0.98190045 0.97321429 0.92735043 0.88888889
 0.99756691 0.96216216 0.98979592 1.         1.         1.
 1.         1.        ]

Kappa:
0.9878917072444736
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f081f0cae10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.597, val_acc:0.258]
Epoch [2/120    avg_loss:2.486, val_acc:0.266]
Epoch [3/120    avg_loss:2.368, val_acc:0.270]
Epoch [4/120    avg_loss:2.244, val_acc:0.381]
Epoch [5/120    avg_loss:2.134, val_acc:0.551]
Epoch [6/120    avg_loss:1.959, val_acc:0.664]
Epoch [7/120    avg_loss:1.755, val_acc:0.713]
Epoch [8/120    avg_loss:1.510, val_acc:0.777]
Epoch [9/120    avg_loss:1.276, val_acc:0.795]
Epoch [10/120    avg_loss:1.080, val_acc:0.826]
Epoch [11/120    avg_loss:0.939, val_acc:0.854]
Epoch [12/120    avg_loss:0.810, val_acc:0.814]
Epoch [13/120    avg_loss:0.724, val_acc:0.861]
Epoch [14/120    avg_loss:0.720, val_acc:0.873]
Epoch [15/120    avg_loss:0.601, val_acc:0.850]
Epoch [16/120    avg_loss:0.603, val_acc:0.881]
Epoch [17/120    avg_loss:0.649, val_acc:0.852]
Epoch [18/120    avg_loss:0.548, val_acc:0.887]
Epoch [19/120    avg_loss:0.510, val_acc:0.902]
Epoch [20/120    avg_loss:0.450, val_acc:0.924]
Epoch [21/120    avg_loss:0.448, val_acc:0.900]
Epoch [22/120    avg_loss:0.421, val_acc:0.840]
Epoch [23/120    avg_loss:0.432, val_acc:0.918]
Epoch [24/120    avg_loss:0.408, val_acc:0.926]
Epoch [25/120    avg_loss:0.347, val_acc:0.934]
Epoch [26/120    avg_loss:0.316, val_acc:0.930]
Epoch [27/120    avg_loss:0.329, val_acc:0.918]
Epoch [28/120    avg_loss:0.312, val_acc:0.949]
Epoch [29/120    avg_loss:0.271, val_acc:0.920]
Epoch [30/120    avg_loss:0.239, val_acc:0.949]
Epoch [31/120    avg_loss:0.263, val_acc:0.945]
Epoch [32/120    avg_loss:0.258, val_acc:0.965]
Epoch [33/120    avg_loss:0.189, val_acc:0.943]
Epoch [34/120    avg_loss:0.193, val_acc:0.842]
Epoch [35/120    avg_loss:0.282, val_acc:0.973]
Epoch [36/120    avg_loss:0.187, val_acc:0.959]
Epoch [37/120    avg_loss:0.217, val_acc:0.977]
Epoch [38/120    avg_loss:0.176, val_acc:0.957]
Epoch [39/120    avg_loss:0.141, val_acc:0.977]
Epoch [40/120    avg_loss:0.201, val_acc:0.957]
Epoch [41/120    avg_loss:0.158, val_acc:0.971]
Epoch [42/120    avg_loss:0.133, val_acc:0.979]
Epoch [43/120    avg_loss:0.151, val_acc:0.961]
Epoch [44/120    avg_loss:0.215, val_acc:0.941]
Epoch [45/120    avg_loss:0.208, val_acc:0.945]
Epoch [46/120    avg_loss:0.163, val_acc:0.973]
Epoch [47/120    avg_loss:0.137, val_acc:0.969]
Epoch [48/120    avg_loss:0.163, val_acc:0.953]
Epoch [49/120    avg_loss:0.179, val_acc:0.967]
Epoch [50/120    avg_loss:0.106, val_acc:0.969]
Epoch [51/120    avg_loss:0.103, val_acc:0.967]
Epoch [52/120    avg_loss:0.096, val_acc:0.973]
Epoch [53/120    avg_loss:0.105, val_acc:0.951]
Epoch [54/120    avg_loss:0.154, val_acc:0.936]
Epoch [55/120    avg_loss:0.110, val_acc:0.979]
Epoch [56/120    avg_loss:0.093, val_acc:0.967]
Epoch [57/120    avg_loss:0.109, val_acc:0.953]
Epoch [58/120    avg_loss:0.110, val_acc:0.984]
Epoch [59/120    avg_loss:0.105, val_acc:0.957]
Epoch [60/120    avg_loss:0.146, val_acc:0.949]
Epoch [61/120    avg_loss:0.096, val_acc:0.979]
Epoch [62/120    avg_loss:0.081, val_acc:0.984]
Epoch [63/120    avg_loss:0.072, val_acc:0.971]
Epoch [64/120    avg_loss:0.115, val_acc:0.984]
Epoch [65/120    avg_loss:0.079, val_acc:0.988]
Epoch [66/120    avg_loss:0.061, val_acc:0.986]
Epoch [67/120    avg_loss:0.103, val_acc:0.939]
Epoch [68/120    avg_loss:0.103, val_acc:0.977]
Epoch [69/120    avg_loss:0.058, val_acc:0.990]
Epoch [70/120    avg_loss:0.042, val_acc:0.984]
Epoch [71/120    avg_loss:0.034, val_acc:0.990]
Epoch [72/120    avg_loss:0.049, val_acc:0.980]
Epoch [73/120    avg_loss:0.049, val_acc:0.979]
Epoch [74/120    avg_loss:0.095, val_acc:0.852]
Epoch [75/120    avg_loss:0.300, val_acc:0.934]
Epoch [76/120    avg_loss:0.191, val_acc:0.938]
Epoch [77/120    avg_loss:0.089, val_acc:0.984]
Epoch [78/120    avg_loss:0.056, val_acc:0.990]
Epoch [79/120    avg_loss:0.059, val_acc:0.971]
Epoch [80/120    avg_loss:0.053, val_acc:0.982]
Epoch [81/120    avg_loss:0.055, val_acc:0.984]
Epoch [82/120    avg_loss:0.047, val_acc:0.980]
Epoch [83/120    avg_loss:0.179, val_acc:0.926]
Epoch [84/120    avg_loss:0.167, val_acc:0.973]
Epoch [85/120    avg_loss:0.057, val_acc:0.957]
Epoch [86/120    avg_loss:0.048, val_acc:0.988]
Epoch [87/120    avg_loss:0.069, val_acc:0.965]
Epoch [88/120    avg_loss:0.059, val_acc:0.979]
Epoch [89/120    avg_loss:0.090, val_acc:0.977]
Epoch [90/120    avg_loss:0.038, val_acc:0.986]
Epoch [91/120    avg_loss:0.049, val_acc:0.979]
Epoch [92/120    avg_loss:0.032, val_acc:0.982]
Epoch [93/120    avg_loss:0.028, val_acc:0.988]
Epoch [94/120    avg_loss:0.026, val_acc:0.988]
Epoch [95/120    avg_loss:0.028, val_acc:0.988]
Epoch [96/120    avg_loss:0.026, val_acc:0.990]
Epoch [97/120    avg_loss:0.021, val_acc:0.988]
Epoch [98/120    avg_loss:0.019, val_acc:0.990]
Epoch [99/120    avg_loss:0.023, val_acc:0.990]
Epoch [100/120    avg_loss:0.021, val_acc:0.992]
Epoch [101/120    avg_loss:0.014, val_acc:0.992]
Epoch [102/120    avg_loss:0.018, val_acc:0.992]
Epoch [103/120    avg_loss:0.018, val_acc:0.994]
Epoch [104/120    avg_loss:0.017, val_acc:0.994]
Epoch [105/120    avg_loss:0.018, val_acc:0.994]
Epoch [106/120    avg_loss:0.015, val_acc:0.994]
Epoch [107/120    avg_loss:0.015, val_acc:0.994]
Epoch [108/120    avg_loss:0.016, val_acc:0.994]
Epoch [109/120    avg_loss:0.018, val_acc:0.994]
Epoch [110/120    avg_loss:0.021, val_acc:0.994]
Epoch [111/120    avg_loss:0.019, val_acc:0.994]
Epoch [112/120    avg_loss:0.015, val_acc:0.994]
Epoch [113/120    avg_loss:0.013, val_acc:0.994]
Epoch [114/120    avg_loss:0.012, val_acc:0.994]
Epoch [115/120    avg_loss:0.015, val_acc:0.994]
Epoch [116/120    avg_loss:0.013, val_acc:0.994]
Epoch [117/120    avg_loss:0.014, val_acc:0.994]
Epoch [118/120    avg_loss:0.016, val_acc:0.994]
Epoch [119/120    avg_loss:0.014, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   4   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  27 117   1   0   0   0   0   0   0   0]
 [  0   1   0   0   1   0 204   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.99927061 0.97333333 0.98901099 0.92975207 0.88636364
 0.99270073 0.93785311 0.99870968 0.99893276 1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9881285436234003
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50b08c8d68>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.546, val_acc:0.361]
Epoch [2/120    avg_loss:2.389, val_acc:0.422]
Epoch [3/120    avg_loss:2.244, val_acc:0.488]
Epoch [4/120    avg_loss:2.084, val_acc:0.576]
Epoch [5/120    avg_loss:1.915, val_acc:0.689]
Epoch [6/120    avg_loss:1.689, val_acc:0.740]
Epoch [7/120    avg_loss:1.508, val_acc:0.754]
Epoch [8/120    avg_loss:1.311, val_acc:0.760]
Epoch [9/120    avg_loss:1.130, val_acc:0.818]
Epoch [10/120    avg_loss:0.991, val_acc:0.791]
Epoch [11/120    avg_loss:0.896, val_acc:0.762]
Epoch [12/120    avg_loss:0.821, val_acc:0.840]
Epoch [13/120    avg_loss:0.702, val_acc:0.809]
Epoch [14/120    avg_loss:0.705, val_acc:0.848]
Epoch [15/120    avg_loss:0.617, val_acc:0.820]
Epoch [16/120    avg_loss:0.593, val_acc:0.885]
Epoch [17/120    avg_loss:0.542, val_acc:0.832]
Epoch [18/120    avg_loss:0.510, val_acc:0.873]
Epoch [19/120    avg_loss:0.485, val_acc:0.887]
Epoch [20/120    avg_loss:0.439, val_acc:0.865]
Epoch [21/120    avg_loss:0.452, val_acc:0.879]
Epoch [22/120    avg_loss:0.497, val_acc:0.871]
Epoch [23/120    avg_loss:0.429, val_acc:0.863]
Epoch [24/120    avg_loss:0.449, val_acc:0.895]
Epoch [25/120    avg_loss:0.337, val_acc:0.904]
Epoch [26/120    avg_loss:0.349, val_acc:0.896]
Epoch [27/120    avg_loss:0.332, val_acc:0.881]
Epoch [28/120    avg_loss:0.356, val_acc:0.924]
Epoch [29/120    avg_loss:0.317, val_acc:0.902]
Epoch [30/120    avg_loss:0.349, val_acc:0.906]
Epoch [31/120    avg_loss:0.347, val_acc:0.895]
Epoch [32/120    avg_loss:0.335, val_acc:0.908]
Epoch [33/120    avg_loss:0.263, val_acc:0.928]
Epoch [34/120    avg_loss:0.288, val_acc:0.928]
Epoch [35/120    avg_loss:0.272, val_acc:0.912]
Epoch [36/120    avg_loss:0.241, val_acc:0.855]
Epoch [37/120    avg_loss:0.303, val_acc:0.934]
Epoch [38/120    avg_loss:0.230, val_acc:0.945]
Epoch [39/120    avg_loss:0.207, val_acc:0.941]
Epoch [40/120    avg_loss:0.225, val_acc:0.932]
Epoch [41/120    avg_loss:0.231, val_acc:0.939]
Epoch [42/120    avg_loss:0.246, val_acc:0.877]
Epoch [43/120    avg_loss:0.284, val_acc:0.938]
Epoch [44/120    avg_loss:0.221, val_acc:0.945]
Epoch [45/120    avg_loss:0.240, val_acc:0.953]
Epoch [46/120    avg_loss:0.199, val_acc:0.955]
Epoch [47/120    avg_loss:0.172, val_acc:0.859]
Epoch [48/120    avg_loss:0.206, val_acc:0.955]
Epoch [49/120    avg_loss:0.175, val_acc:0.953]
Epoch [50/120    avg_loss:0.193, val_acc:0.955]
Epoch [51/120    avg_loss:0.143, val_acc:0.967]
Epoch [52/120    avg_loss:0.167, val_acc:0.893]
Epoch [53/120    avg_loss:0.182, val_acc:0.939]
Epoch [54/120    avg_loss:0.178, val_acc:0.953]
Epoch [55/120    avg_loss:0.132, val_acc:0.959]
Epoch [56/120    avg_loss:0.198, val_acc:0.963]
Epoch [57/120    avg_loss:0.168, val_acc:0.959]
Epoch [58/120    avg_loss:0.143, val_acc:0.961]
Epoch [59/120    avg_loss:0.110, val_acc:0.977]
Epoch [60/120    avg_loss:0.101, val_acc:0.967]
Epoch [61/120    avg_loss:0.094, val_acc:0.961]
Epoch [62/120    avg_loss:0.166, val_acc:0.953]
Epoch [63/120    avg_loss:0.118, val_acc:0.975]
Epoch [64/120    avg_loss:0.141, val_acc:0.975]
Epoch [65/120    avg_loss:0.100, val_acc:0.979]
Epoch [66/120    avg_loss:0.113, val_acc:0.959]
Epoch [67/120    avg_loss:0.099, val_acc:0.967]
Epoch [68/120    avg_loss:0.106, val_acc:0.975]
Epoch [69/120    avg_loss:0.077, val_acc:0.969]
Epoch [70/120    avg_loss:0.084, val_acc:0.941]
Epoch [71/120    avg_loss:0.089, val_acc:0.969]
Epoch [72/120    avg_loss:0.103, val_acc:0.961]
Epoch [73/120    avg_loss:0.117, val_acc:0.943]
Epoch [74/120    avg_loss:0.078, val_acc:0.973]
Epoch [75/120    avg_loss:0.076, val_acc:0.980]
Epoch [76/120    avg_loss:0.060, val_acc:0.961]
Epoch [77/120    avg_loss:0.054, val_acc:0.980]
Epoch [78/120    avg_loss:0.097, val_acc:0.965]
Epoch [79/120    avg_loss:0.044, val_acc:0.973]
Epoch [80/120    avg_loss:0.041, val_acc:0.975]
Epoch [81/120    avg_loss:0.042, val_acc:0.973]
Epoch [82/120    avg_loss:0.047, val_acc:0.971]
Epoch [83/120    avg_loss:0.058, val_acc:0.977]
Epoch [84/120    avg_loss:0.050, val_acc:0.977]
Epoch [85/120    avg_loss:0.058, val_acc:0.973]
Epoch [86/120    avg_loss:0.031, val_acc:0.980]
Epoch [87/120    avg_loss:0.072, val_acc:0.986]
Epoch [88/120    avg_loss:0.072, val_acc:0.963]
Epoch [89/120    avg_loss:0.126, val_acc:0.971]
Epoch [90/120    avg_loss:0.134, val_acc:0.965]
Epoch [91/120    avg_loss:0.103, val_acc:0.977]
Epoch [92/120    avg_loss:0.059, val_acc:0.982]
Epoch [93/120    avg_loss:0.038, val_acc:0.979]
Epoch [94/120    avg_loss:0.035, val_acc:0.971]
Epoch [95/120    avg_loss:0.043, val_acc:0.977]
Epoch [96/120    avg_loss:0.027, val_acc:0.979]
Epoch [97/120    avg_loss:0.043, val_acc:0.979]
Epoch [98/120    avg_loss:0.025, val_acc:0.980]
Epoch [99/120    avg_loss:0.072, val_acc:0.969]
Epoch [100/120    avg_loss:0.060, val_acc:0.982]
Epoch [101/120    avg_loss:0.034, val_acc:0.984]
Epoch [102/120    avg_loss:0.034, val_acc:0.984]
Epoch [103/120    avg_loss:0.042, val_acc:0.982]
Epoch [104/120    avg_loss:0.034, val_acc:0.982]
Epoch [105/120    avg_loss:0.026, val_acc:0.984]
Epoch [106/120    avg_loss:0.030, val_acc:0.984]
Epoch [107/120    avg_loss:0.023, val_acc:0.984]
Epoch [108/120    avg_loss:0.022, val_acc:0.984]
Epoch [109/120    avg_loss:0.023, val_acc:0.984]
Epoch [110/120    avg_loss:0.026, val_acc:0.984]
Epoch [111/120    avg_loss:0.024, val_acc:0.980]
Epoch [112/120    avg_loss:0.023, val_acc:0.982]
Epoch [113/120    avg_loss:0.019, val_acc:0.982]
Epoch [114/120    avg_loss:0.021, val_acc:0.982]
Epoch [115/120    avg_loss:0.021, val_acc:0.982]
Epoch [116/120    avg_loss:0.021, val_acc:0.982]
Epoch [117/120    avg_loss:0.018, val_acc:0.982]
Epoch [118/120    avg_loss:0.024, val_acc:0.982]
Epoch [119/120    avg_loss:0.024, val_acc:0.982]
Epoch [120/120    avg_loss:0.020, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 213  12   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 222   4   0   0   1   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.99543379 0.96162528 0.91170431 0.8880597
 1.         0.99470899 0.99359795 0.99893276 1.         1.
 0.99889503 1.        ]

Kappa:
0.9881294612597754
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d8831d6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.307]
Epoch [2/120    avg_loss:2.484, val_acc:0.309]
Epoch [3/120    avg_loss:2.377, val_acc:0.350]
Epoch [4/120    avg_loss:2.225, val_acc:0.479]
Epoch [5/120    avg_loss:2.069, val_acc:0.537]
Epoch [6/120    avg_loss:1.883, val_acc:0.619]
Epoch [7/120    avg_loss:1.704, val_acc:0.664]
Epoch [8/120    avg_loss:1.502, val_acc:0.768]
Epoch [9/120    avg_loss:1.334, val_acc:0.762]
Epoch [10/120    avg_loss:1.149, val_acc:0.773]
Epoch [11/120    avg_loss:0.962, val_acc:0.838]
Epoch [12/120    avg_loss:0.880, val_acc:0.797]
Epoch [13/120    avg_loss:0.794, val_acc:0.834]
Epoch [14/120    avg_loss:0.738, val_acc:0.855]
Epoch [15/120    avg_loss:0.629, val_acc:0.848]
Epoch [16/120    avg_loss:0.636, val_acc:0.861]
Epoch [17/120    avg_loss:0.598, val_acc:0.883]
Epoch [18/120    avg_loss:0.542, val_acc:0.814]
Epoch [19/120    avg_loss:0.552, val_acc:0.883]
Epoch [20/120    avg_loss:0.478, val_acc:0.850]
Epoch [21/120    avg_loss:0.530, val_acc:0.883]
Epoch [22/120    avg_loss:0.444, val_acc:0.893]
Epoch [23/120    avg_loss:0.411, val_acc:0.877]
Epoch [24/120    avg_loss:0.369, val_acc:0.885]
Epoch [25/120    avg_loss:0.421, val_acc:0.902]
Epoch [26/120    avg_loss:0.378, val_acc:0.883]
Epoch [27/120    avg_loss:0.430, val_acc:0.861]
Epoch [28/120    avg_loss:0.455, val_acc:0.895]
Epoch [29/120    avg_loss:0.387, val_acc:0.877]
Epoch [30/120    avg_loss:0.377, val_acc:0.918]
Epoch [31/120    avg_loss:0.313, val_acc:0.908]
Epoch [32/120    avg_loss:0.307, val_acc:0.910]
Epoch [33/120    avg_loss:0.329, val_acc:0.914]
Epoch [34/120    avg_loss:0.280, val_acc:0.943]
Epoch [35/120    avg_loss:0.215, val_acc:0.928]
Epoch [36/120    avg_loss:0.276, val_acc:0.936]
Epoch [37/120    avg_loss:0.263, val_acc:0.924]
Epoch [38/120    avg_loss:0.270, val_acc:0.850]
Epoch [39/120    avg_loss:0.233, val_acc:0.941]
Epoch [40/120    avg_loss:0.218, val_acc:0.922]
Epoch [41/120    avg_loss:0.195, val_acc:0.932]
Epoch [42/120    avg_loss:0.182, val_acc:0.916]
Epoch [43/120    avg_loss:0.283, val_acc:0.928]
Epoch [44/120    avg_loss:0.273, val_acc:0.902]
Epoch [45/120    avg_loss:0.195, val_acc:0.918]
Epoch [46/120    avg_loss:0.241, val_acc:0.943]
Epoch [47/120    avg_loss:0.202, val_acc:0.955]
Epoch [48/120    avg_loss:0.205, val_acc:0.939]
Epoch [49/120    avg_loss:0.303, val_acc:0.922]
Epoch [50/120    avg_loss:0.230, val_acc:0.932]
Epoch [51/120    avg_loss:0.213, val_acc:0.941]
Epoch [52/120    avg_loss:0.172, val_acc:0.955]
Epoch [53/120    avg_loss:0.191, val_acc:0.951]
Epoch [54/120    avg_loss:0.166, val_acc:0.943]
Epoch [55/120    avg_loss:0.161, val_acc:0.949]
Epoch [56/120    avg_loss:0.176, val_acc:0.963]
Epoch [57/120    avg_loss:0.154, val_acc:0.953]
Epoch [58/120    avg_loss:0.139, val_acc:0.971]
Epoch [59/120    avg_loss:0.173, val_acc:0.953]
Epoch [60/120    avg_loss:0.164, val_acc:0.945]
Epoch [61/120    avg_loss:0.104, val_acc:0.951]
Epoch [62/120    avg_loss:0.149, val_acc:0.963]
Epoch [63/120    avg_loss:0.125, val_acc:0.969]
Epoch [64/120    avg_loss:0.096, val_acc:0.955]
Epoch [65/120    avg_loss:0.153, val_acc:0.924]
Epoch [66/120    avg_loss:0.119, val_acc:0.971]
Epoch [67/120    avg_loss:0.112, val_acc:0.947]
Epoch [68/120    avg_loss:0.139, val_acc:0.973]
Epoch [69/120    avg_loss:0.102, val_acc:0.965]
Epoch [70/120    avg_loss:0.135, val_acc:0.971]
Epoch [71/120    avg_loss:0.115, val_acc:0.965]
Epoch [72/120    avg_loss:0.085, val_acc:0.973]
Epoch [73/120    avg_loss:0.081, val_acc:0.980]
Epoch [74/120    avg_loss:0.074, val_acc:0.977]
Epoch [75/120    avg_loss:0.084, val_acc:0.971]
Epoch [76/120    avg_loss:0.085, val_acc:0.955]
Epoch [77/120    avg_loss:0.078, val_acc:0.971]
Epoch [78/120    avg_loss:0.083, val_acc:0.969]
Epoch [79/120    avg_loss:0.082, val_acc:0.973]
Epoch [80/120    avg_loss:0.061, val_acc:0.984]
Epoch [81/120    avg_loss:0.065, val_acc:0.967]
Epoch [82/120    avg_loss:0.190, val_acc:0.953]
Epoch [83/120    avg_loss:0.105, val_acc:0.961]
Epoch [84/120    avg_loss:0.124, val_acc:0.941]
Epoch [85/120    avg_loss:0.169, val_acc:0.953]
Epoch [86/120    avg_loss:0.106, val_acc:0.967]
Epoch [87/120    avg_loss:0.092, val_acc:0.959]
Epoch [88/120    avg_loss:0.134, val_acc:0.955]
Epoch [89/120    avg_loss:0.139, val_acc:0.947]
Epoch [90/120    avg_loss:0.081, val_acc:0.969]
Epoch [91/120    avg_loss:0.075, val_acc:0.979]
Epoch [92/120    avg_loss:0.041, val_acc:0.973]
Epoch [93/120    avg_loss:0.044, val_acc:0.979]
Epoch [94/120    avg_loss:0.045, val_acc:0.979]
Epoch [95/120    avg_loss:0.035, val_acc:0.984]
Epoch [96/120    avg_loss:0.028, val_acc:0.984]
Epoch [97/120    avg_loss:0.036, val_acc:0.984]
Epoch [98/120    avg_loss:0.026, val_acc:0.980]
Epoch [99/120    avg_loss:0.035, val_acc:0.982]
Epoch [100/120    avg_loss:0.038, val_acc:0.982]
Epoch [101/120    avg_loss:0.033, val_acc:0.984]
Epoch [102/120    avg_loss:0.036, val_acc:0.984]
Epoch [103/120    avg_loss:0.035, val_acc:0.982]
Epoch [104/120    avg_loss:0.027, val_acc:0.982]
Epoch [105/120    avg_loss:0.026, val_acc:0.982]
Epoch [106/120    avg_loss:0.032, val_acc:0.984]
Epoch [107/120    avg_loss:0.028, val_acc:0.980]
Epoch [108/120    avg_loss:0.029, val_acc:0.980]
Epoch [109/120    avg_loss:0.025, val_acc:0.980]
Epoch [110/120    avg_loss:0.025, val_acc:0.980]
Epoch [111/120    avg_loss:0.030, val_acc:0.982]
Epoch [112/120    avg_loss:0.027, val_acc:0.980]
Epoch [113/120    avg_loss:0.021, val_acc:0.984]
Epoch [114/120    avg_loss:0.029, val_acc:0.986]
Epoch [115/120    avg_loss:0.027, val_acc:0.984]
Epoch [116/120    avg_loss:0.027, val_acc:0.984]
Epoch [117/120    avg_loss:0.027, val_acc:0.982]
Epoch [118/120    avg_loss:0.026, val_acc:0.984]
Epoch [119/120    avg_loss:0.030, val_acc:0.982]
Epoch [120/120    avg_loss:0.031, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 205   0   0   0   0  14   0   0   0   0   0   0]
 [  0   0   0 214   5   0   0   0  10   1   0   0   0   0]
 [  0   0   0   1 207  18   0   0   0   0   0   0   1   0]
 [  0   0   0   0   4 137   4   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   2   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.35820895522387

F1 scores:
[       nan 0.99927061 0.93394077 0.96179775 0.93453725 0.91333333
 0.98795181 0.84491979 0.98727735 0.99893276 0.99726027 0.99867198
 0.99558499 1.        ]

Kappa:
0.9817201198647472
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde08d9ae10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.323]
Epoch [2/120    avg_loss:2.496, val_acc:0.552]
Epoch [3/120    avg_loss:2.375, val_acc:0.615]
Epoch [4/120    avg_loss:2.244, val_acc:0.669]
Epoch [5/120    avg_loss:2.091, val_acc:0.605]
Epoch [6/120    avg_loss:1.914, val_acc:0.599]
Epoch [7/120    avg_loss:1.690, val_acc:0.639]
Epoch [8/120    avg_loss:1.473, val_acc:0.740]
Epoch [9/120    avg_loss:1.290, val_acc:0.772]
Epoch [10/120    avg_loss:1.108, val_acc:0.819]
Epoch [11/120    avg_loss:0.978, val_acc:0.827]
Epoch [12/120    avg_loss:0.917, val_acc:0.853]
Epoch [13/120    avg_loss:0.802, val_acc:0.887]
Epoch [14/120    avg_loss:0.632, val_acc:0.889]
Epoch [15/120    avg_loss:0.622, val_acc:0.873]
Epoch [16/120    avg_loss:0.647, val_acc:0.885]
Epoch [17/120    avg_loss:0.567, val_acc:0.837]
Epoch [18/120    avg_loss:0.536, val_acc:0.913]
Epoch [19/120    avg_loss:0.460, val_acc:0.911]
Epoch [20/120    avg_loss:0.426, val_acc:0.905]
Epoch [21/120    avg_loss:0.501, val_acc:0.891]
Epoch [22/120    avg_loss:0.451, val_acc:0.935]
Epoch [23/120    avg_loss:0.409, val_acc:0.875]
Epoch [24/120    avg_loss:0.447, val_acc:0.909]
Epoch [25/120    avg_loss:0.347, val_acc:0.917]
Epoch [26/120    avg_loss:0.338, val_acc:0.919]
Epoch [27/120    avg_loss:0.366, val_acc:0.911]
Epoch [28/120    avg_loss:0.355, val_acc:0.952]
Epoch [29/120    avg_loss:0.253, val_acc:0.911]
Epoch [30/120    avg_loss:0.348, val_acc:0.917]
Epoch [31/120    avg_loss:0.327, val_acc:0.938]
Epoch [32/120    avg_loss:0.349, val_acc:0.950]
Epoch [33/120    avg_loss:0.284, val_acc:0.937]
Epoch [34/120    avg_loss:0.304, val_acc:0.911]
Epoch [35/120    avg_loss:0.291, val_acc:0.948]
Epoch [36/120    avg_loss:0.220, val_acc:0.917]
Epoch [37/120    avg_loss:0.315, val_acc:0.925]
Epoch [38/120    avg_loss:0.328, val_acc:0.885]
Epoch [39/120    avg_loss:0.307, val_acc:0.935]
Epoch [40/120    avg_loss:0.228, val_acc:0.966]
Epoch [41/120    avg_loss:0.247, val_acc:0.948]
Epoch [42/120    avg_loss:0.256, val_acc:0.946]
Epoch [43/120    avg_loss:0.236, val_acc:0.950]
Epoch [44/120    avg_loss:0.244, val_acc:0.954]
Epoch [45/120    avg_loss:0.206, val_acc:0.942]
Epoch [46/120    avg_loss:0.241, val_acc:0.915]
Epoch [47/120    avg_loss:0.237, val_acc:0.942]
Epoch [48/120    avg_loss:0.174, val_acc:0.942]
Epoch [49/120    avg_loss:0.199, val_acc:0.968]
Epoch [50/120    avg_loss:0.179, val_acc:0.954]
Epoch [51/120    avg_loss:0.199, val_acc:0.940]
Epoch [52/120    avg_loss:0.214, val_acc:0.962]
Epoch [53/120    avg_loss:0.131, val_acc:0.970]
Epoch [54/120    avg_loss:0.164, val_acc:0.952]
Epoch [55/120    avg_loss:0.148, val_acc:0.968]
Epoch [56/120    avg_loss:0.132, val_acc:0.968]
Epoch [57/120    avg_loss:0.142, val_acc:0.976]
Epoch [58/120    avg_loss:0.128, val_acc:0.946]
Epoch [59/120    avg_loss:0.148, val_acc:0.958]
Epoch [60/120    avg_loss:0.126, val_acc:0.966]
Epoch [61/120    avg_loss:0.075, val_acc:0.976]
Epoch [62/120    avg_loss:0.105, val_acc:0.956]
Epoch [63/120    avg_loss:0.216, val_acc:0.958]
Epoch [64/120    avg_loss:0.120, val_acc:0.952]
Epoch [65/120    avg_loss:0.128, val_acc:0.980]
Epoch [66/120    avg_loss:0.095, val_acc:0.964]
Epoch [67/120    avg_loss:0.097, val_acc:0.940]
Epoch [68/120    avg_loss:0.142, val_acc:0.972]
Epoch [69/120    avg_loss:0.116, val_acc:0.966]
Epoch [70/120    avg_loss:0.157, val_acc:0.968]
Epoch [71/120    avg_loss:0.078, val_acc:0.966]
Epoch [72/120    avg_loss:0.131, val_acc:0.946]
Epoch [73/120    avg_loss:0.138, val_acc:0.974]
Epoch [74/120    avg_loss:0.121, val_acc:0.974]
Epoch [75/120    avg_loss:0.138, val_acc:0.978]
Epoch [76/120    avg_loss:0.083, val_acc:0.978]
Epoch [77/120    avg_loss:0.074, val_acc:0.970]
Epoch [78/120    avg_loss:0.101, val_acc:0.980]
Epoch [79/120    avg_loss:0.094, val_acc:0.980]
Epoch [80/120    avg_loss:0.061, val_acc:0.958]
Epoch [81/120    avg_loss:0.080, val_acc:0.974]
Epoch [82/120    avg_loss:0.052, val_acc:0.972]
Epoch [83/120    avg_loss:0.060, val_acc:0.978]
Epoch [84/120    avg_loss:0.076, val_acc:0.938]
Epoch [85/120    avg_loss:0.067, val_acc:0.978]
Epoch [86/120    avg_loss:0.069, val_acc:0.980]
Epoch [87/120    avg_loss:0.054, val_acc:0.988]
Epoch [88/120    avg_loss:0.055, val_acc:0.982]
Epoch [89/120    avg_loss:0.075, val_acc:0.980]
Epoch [90/120    avg_loss:0.070, val_acc:0.980]
Epoch [91/120    avg_loss:0.069, val_acc:0.978]
Epoch [92/120    avg_loss:0.050, val_acc:0.982]
Epoch [93/120    avg_loss:0.039, val_acc:0.980]
Epoch [94/120    avg_loss:0.066, val_acc:0.984]
Epoch [95/120    avg_loss:0.062, val_acc:0.986]
Epoch [96/120    avg_loss:0.044, val_acc:0.992]
Epoch [97/120    avg_loss:0.040, val_acc:0.986]
Epoch [98/120    avg_loss:0.035, val_acc:0.990]
Epoch [99/120    avg_loss:0.029, val_acc:0.990]
Epoch [100/120    avg_loss:0.018, val_acc:0.990]
Epoch [101/120    avg_loss:0.020, val_acc:0.994]
Epoch [102/120    avg_loss:0.018, val_acc:0.996]
Epoch [103/120    avg_loss:0.019, val_acc:0.996]
Epoch [104/120    avg_loss:0.030, val_acc:0.980]
Epoch [105/120    avg_loss:0.052, val_acc:0.986]
Epoch [106/120    avg_loss:0.086, val_acc:0.982]
Epoch [107/120    avg_loss:0.079, val_acc:0.964]
Epoch [108/120    avg_loss:0.044, val_acc:0.992]
Epoch [109/120    avg_loss:0.053, val_acc:0.970]
Epoch [110/120    avg_loss:0.032, val_acc:0.982]
Epoch [111/120    avg_loss:0.042, val_acc:0.986]
Epoch [112/120    avg_loss:0.018, val_acc:0.988]
Epoch [113/120    avg_loss:0.037, val_acc:0.992]
Epoch [114/120    avg_loss:0.023, val_acc:0.994]
Epoch [115/120    avg_loss:0.015, val_acc:0.994]
Epoch [116/120    avg_loss:0.015, val_acc:0.990]
Epoch [117/120    avg_loss:0.012, val_acc:0.994]
Epoch [118/120    avg_loss:0.012, val_acc:0.996]
Epoch [119/120    avg_loss:0.009, val_acc:0.996]
Epoch [120/120    avg_loss:0.010, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   2   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.99319728 0.99782135 0.95196507 0.92307692
 1.         0.98378378 0.998713   1.         0.99726027 1.
 0.99778761 1.        ]

Kappa:
0.9933530894847162
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60a0283dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.579, val_acc:0.270]
Epoch [2/120    avg_loss:2.449, val_acc:0.302]
Epoch [3/120    avg_loss:2.314, val_acc:0.323]
Epoch [4/120    avg_loss:2.194, val_acc:0.423]
Epoch [5/120    avg_loss:2.056, val_acc:0.560]
Epoch [6/120    avg_loss:1.900, val_acc:0.637]
Epoch [7/120    avg_loss:1.677, val_acc:0.665]
Epoch [8/120    avg_loss:1.459, val_acc:0.738]
Epoch [9/120    avg_loss:1.253, val_acc:0.774]
Epoch [10/120    avg_loss:1.066, val_acc:0.730]
Epoch [11/120    avg_loss:0.972, val_acc:0.821]
Epoch [12/120    avg_loss:0.839, val_acc:0.905]
Epoch [13/120    avg_loss:0.764, val_acc:0.847]
Epoch [14/120    avg_loss:0.683, val_acc:0.871]
Epoch [15/120    avg_loss:0.588, val_acc:0.907]
Epoch [16/120    avg_loss:0.524, val_acc:0.937]
Epoch [17/120    avg_loss:0.449, val_acc:0.851]
Epoch [18/120    avg_loss:0.479, val_acc:0.907]
Epoch [19/120    avg_loss:0.387, val_acc:0.948]
Epoch [20/120    avg_loss:0.378, val_acc:0.923]
Epoch [21/120    avg_loss:0.373, val_acc:0.911]
Epoch [22/120    avg_loss:0.375, val_acc:0.921]
Epoch [23/120    avg_loss:0.353, val_acc:0.808]
Epoch [24/120    avg_loss:0.309, val_acc:0.942]
Epoch [25/120    avg_loss:0.290, val_acc:0.952]
Epoch [26/120    avg_loss:0.342, val_acc:0.933]
Epoch [27/120    avg_loss:0.268, val_acc:0.944]
Epoch [28/120    avg_loss:0.206, val_acc:0.935]
Epoch [29/120    avg_loss:0.256, val_acc:0.937]
Epoch [30/120    avg_loss:0.260, val_acc:0.944]
Epoch [31/120    avg_loss:0.263, val_acc:0.960]
Epoch [32/120    avg_loss:0.280, val_acc:0.950]
Epoch [33/120    avg_loss:0.197, val_acc:0.948]
Epoch [34/120    avg_loss:0.211, val_acc:0.931]
Epoch [35/120    avg_loss:0.162, val_acc:0.940]
Epoch [36/120    avg_loss:0.156, val_acc:0.970]
Epoch [37/120    avg_loss:0.185, val_acc:0.954]
Epoch [38/120    avg_loss:0.199, val_acc:0.948]
Epoch [39/120    avg_loss:0.194, val_acc:0.962]
Epoch [40/120    avg_loss:0.212, val_acc:0.938]
Epoch [41/120    avg_loss:0.174, val_acc:0.970]
Epoch [42/120    avg_loss:0.123, val_acc:0.940]
Epoch [43/120    avg_loss:0.170, val_acc:0.964]
Epoch [44/120    avg_loss:0.139, val_acc:0.980]
Epoch [45/120    avg_loss:0.103, val_acc:0.980]
Epoch [46/120    avg_loss:0.093, val_acc:0.970]
Epoch [47/120    avg_loss:0.144, val_acc:0.962]
Epoch [48/120    avg_loss:0.161, val_acc:0.982]
Epoch [49/120    avg_loss:0.108, val_acc:0.966]
Epoch [50/120    avg_loss:0.141, val_acc:0.968]
Epoch [51/120    avg_loss:0.174, val_acc:0.964]
Epoch [52/120    avg_loss:0.099, val_acc:0.978]
Epoch [53/120    avg_loss:0.102, val_acc:0.976]
Epoch [54/120    avg_loss:0.116, val_acc:0.978]
Epoch [55/120    avg_loss:0.064, val_acc:0.978]
Epoch [56/120    avg_loss:0.065, val_acc:0.976]
Epoch [57/120    avg_loss:0.064, val_acc:0.964]
Epoch [58/120    avg_loss:0.118, val_acc:0.931]
Epoch [59/120    avg_loss:0.101, val_acc:0.982]
Epoch [60/120    avg_loss:0.061, val_acc:0.976]
Epoch [61/120    avg_loss:0.069, val_acc:0.972]
Epoch [62/120    avg_loss:0.066, val_acc:0.986]
Epoch [63/120    avg_loss:0.050, val_acc:0.980]
Epoch [64/120    avg_loss:0.053, val_acc:0.988]
Epoch [65/120    avg_loss:0.062, val_acc:0.988]
Epoch [66/120    avg_loss:0.047, val_acc:0.994]
Epoch [67/120    avg_loss:0.043, val_acc:0.984]
Epoch [68/120    avg_loss:0.054, val_acc:0.976]
Epoch [69/120    avg_loss:0.046, val_acc:0.994]
Epoch [70/120    avg_loss:0.048, val_acc:0.990]
Epoch [71/120    avg_loss:0.034, val_acc:0.988]
Epoch [72/120    avg_loss:0.028, val_acc:0.992]
Epoch [73/120    avg_loss:0.023, val_acc:0.986]
Epoch [74/120    avg_loss:0.033, val_acc:0.988]
Epoch [75/120    avg_loss:0.067, val_acc:0.990]
Epoch [76/120    avg_loss:0.046, val_acc:0.982]
Epoch [77/120    avg_loss:0.037, val_acc:0.990]
Epoch [78/120    avg_loss:0.031, val_acc:0.992]
Epoch [79/120    avg_loss:0.027, val_acc:0.990]
Epoch [80/120    avg_loss:0.024, val_acc:0.986]
Epoch [81/120    avg_loss:0.019, val_acc:0.994]
Epoch [82/120    avg_loss:0.016, val_acc:0.988]
Epoch [83/120    avg_loss:0.030, val_acc:0.986]
Epoch [84/120    avg_loss:0.036, val_acc:0.954]
Epoch [85/120    avg_loss:0.046, val_acc:0.988]
Epoch [86/120    avg_loss:0.020, val_acc:0.988]
Epoch [87/120    avg_loss:0.036, val_acc:0.990]
Epoch [88/120    avg_loss:0.041, val_acc:0.988]
Epoch [89/120    avg_loss:0.039, val_acc:0.986]
Epoch [90/120    avg_loss:0.035, val_acc:0.988]
Epoch [91/120    avg_loss:0.037, val_acc:0.982]
Epoch [92/120    avg_loss:0.017, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.986]
Epoch [94/120    avg_loss:0.025, val_acc:0.974]
Epoch [95/120    avg_loss:0.031, val_acc:0.978]
Epoch [96/120    avg_loss:0.015, val_acc:0.980]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.014, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.011, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.990]
Epoch [107/120    avg_loss:0.016, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.990]
Epoch [110/120    avg_loss:0.017, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.012, val_acc:0.990]
Epoch [114/120    avg_loss:0.011, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.012, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   2   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  32 112   0   0   1   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   1   0   0   1   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.99927061 0.97550111 0.99343545 0.91511387 0.85171103
 1.         0.93785311 0.99612903 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9869412981610188
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf465badd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.581, val_acc:0.305]
Epoch [2/120    avg_loss:2.455, val_acc:0.428]
Epoch [3/120    avg_loss:2.329, val_acc:0.447]
Epoch [4/120    avg_loss:2.185, val_acc:0.549]
Epoch [5/120    avg_loss:2.027, val_acc:0.576]
Epoch [6/120    avg_loss:1.852, val_acc:0.607]
Epoch [7/120    avg_loss:1.636, val_acc:0.643]
Epoch [8/120    avg_loss:1.460, val_acc:0.729]
Epoch [9/120    avg_loss:1.299, val_acc:0.768]
Epoch [10/120    avg_loss:1.121, val_acc:0.760]
Epoch [11/120    avg_loss:0.981, val_acc:0.863]
Epoch [12/120    avg_loss:0.885, val_acc:0.855]
Epoch [13/120    avg_loss:0.733, val_acc:0.877]
Epoch [14/120    avg_loss:0.689, val_acc:0.893]
Epoch [15/120    avg_loss:0.578, val_acc:0.865]
Epoch [16/120    avg_loss:0.581, val_acc:0.893]
Epoch [17/120    avg_loss:0.520, val_acc:0.863]
Epoch [18/120    avg_loss:0.534, val_acc:0.898]
Epoch [19/120    avg_loss:0.455, val_acc:0.896]
Epoch [20/120    avg_loss:0.450, val_acc:0.896]
Epoch [21/120    avg_loss:0.430, val_acc:0.906]
Epoch [22/120    avg_loss:0.419, val_acc:0.904]
Epoch [23/120    avg_loss:0.443, val_acc:0.914]
Epoch [24/120    avg_loss:0.364, val_acc:0.904]
Epoch [25/120    avg_loss:0.320, val_acc:0.928]
Epoch [26/120    avg_loss:0.358, val_acc:0.914]
Epoch [27/120    avg_loss:0.354, val_acc:0.939]
Epoch [28/120    avg_loss:0.321, val_acc:0.910]
Epoch [29/120    avg_loss:0.268, val_acc:0.945]
Epoch [30/120    avg_loss:0.284, val_acc:0.918]
Epoch [31/120    avg_loss:0.243, val_acc:0.961]
Epoch [32/120    avg_loss:0.240, val_acc:0.959]
Epoch [33/120    avg_loss:0.263, val_acc:0.945]
Epoch [34/120    avg_loss:0.251, val_acc:0.949]
Epoch [35/120    avg_loss:0.261, val_acc:0.951]
Epoch [36/120    avg_loss:0.224, val_acc:0.955]
Epoch [37/120    avg_loss:0.161, val_acc:0.947]
Epoch [38/120    avg_loss:0.218, val_acc:0.949]
Epoch [39/120    avg_loss:0.179, val_acc:0.959]
Epoch [40/120    avg_loss:0.240, val_acc:0.955]
Epoch [41/120    avg_loss:0.204, val_acc:0.932]
Epoch [42/120    avg_loss:0.212, val_acc:0.973]
Epoch [43/120    avg_loss:0.155, val_acc:0.969]
Epoch [44/120    avg_loss:0.264, val_acc:0.957]
Epoch [45/120    avg_loss:0.217, val_acc:0.895]
Epoch [46/120    avg_loss:0.215, val_acc:0.945]
Epoch [47/120    avg_loss:0.177, val_acc:0.971]
Epoch [48/120    avg_loss:0.149, val_acc:0.967]
Epoch [49/120    avg_loss:0.156, val_acc:0.953]
Epoch [50/120    avg_loss:0.142, val_acc:0.980]
Epoch [51/120    avg_loss:0.112, val_acc:0.945]
Epoch [52/120    avg_loss:0.199, val_acc:0.973]
Epoch [53/120    avg_loss:0.135, val_acc:0.971]
Epoch [54/120    avg_loss:0.097, val_acc:0.977]
Epoch [55/120    avg_loss:0.122, val_acc:0.945]
Epoch [56/120    avg_loss:0.132, val_acc:0.979]
Epoch [57/120    avg_loss:0.107, val_acc:0.967]
Epoch [58/120    avg_loss:0.110, val_acc:0.973]
Epoch [59/120    avg_loss:0.096, val_acc:0.977]
Epoch [60/120    avg_loss:0.093, val_acc:0.977]
Epoch [61/120    avg_loss:0.069, val_acc:0.988]
Epoch [62/120    avg_loss:0.111, val_acc:0.971]
Epoch [63/120    avg_loss:0.080, val_acc:0.971]
Epoch [64/120    avg_loss:0.067, val_acc:0.982]
Epoch [65/120    avg_loss:0.075, val_acc:0.973]
Epoch [66/120    avg_loss:0.111, val_acc:0.975]
Epoch [67/120    avg_loss:0.086, val_acc:0.977]
Epoch [68/120    avg_loss:0.071, val_acc:0.965]
Epoch [69/120    avg_loss:0.120, val_acc:0.986]
Epoch [70/120    avg_loss:0.078, val_acc:0.965]
Epoch [71/120    avg_loss:0.065, val_acc:0.982]
Epoch [72/120    avg_loss:0.056, val_acc:0.984]
Epoch [73/120    avg_loss:0.091, val_acc:0.975]
Epoch [74/120    avg_loss:0.183, val_acc:0.953]
Epoch [75/120    avg_loss:0.093, val_acc:0.975]
Epoch [76/120    avg_loss:0.087, val_acc:0.977]
Epoch [77/120    avg_loss:0.073, val_acc:0.984]
Epoch [78/120    avg_loss:0.058, val_acc:0.986]
Epoch [79/120    avg_loss:0.061, val_acc:0.986]
Epoch [80/120    avg_loss:0.057, val_acc:0.982]
Epoch [81/120    avg_loss:0.049, val_acc:0.988]
Epoch [82/120    avg_loss:0.054, val_acc:0.988]
Epoch [83/120    avg_loss:0.048, val_acc:0.990]
Epoch [84/120    avg_loss:0.045, val_acc:0.988]
Epoch [85/120    avg_loss:0.040, val_acc:0.988]
Epoch [86/120    avg_loss:0.041, val_acc:0.988]
Epoch [87/120    avg_loss:0.046, val_acc:0.988]
Epoch [88/120    avg_loss:0.035, val_acc:0.990]
Epoch [89/120    avg_loss:0.036, val_acc:0.992]
Epoch [90/120    avg_loss:0.043, val_acc:0.988]
Epoch [91/120    avg_loss:0.038, val_acc:0.992]
Epoch [92/120    avg_loss:0.035, val_acc:0.990]
Epoch [93/120    avg_loss:0.035, val_acc:0.992]
Epoch [94/120    avg_loss:0.034, val_acc:0.992]
Epoch [95/120    avg_loss:0.036, val_acc:0.990]
Epoch [96/120    avg_loss:0.033, val_acc:0.990]
Epoch [97/120    avg_loss:0.033, val_acc:0.992]
Epoch [98/120    avg_loss:0.027, val_acc:0.992]
Epoch [99/120    avg_loss:0.028, val_acc:0.992]
Epoch [100/120    avg_loss:0.028, val_acc:0.992]
Epoch [101/120    avg_loss:0.033, val_acc:0.992]
Epoch [102/120    avg_loss:0.029, val_acc:0.992]
Epoch [103/120    avg_loss:0.027, val_acc:0.992]
Epoch [104/120    avg_loss:0.024, val_acc:0.992]
Epoch [105/120    avg_loss:0.030, val_acc:0.992]
Epoch [106/120    avg_loss:0.023, val_acc:0.992]
Epoch [107/120    avg_loss:0.023, val_acc:0.992]
Epoch [108/120    avg_loss:0.024, val_acc:0.992]
Epoch [109/120    avg_loss:0.028, val_acc:0.992]
Epoch [110/120    avg_loss:0.026, val_acc:0.992]
Epoch [111/120    avg_loss:0.023, val_acc:0.992]
Epoch [112/120    avg_loss:0.023, val_acc:0.992]
Epoch [113/120    avg_loss:0.027, val_acc:0.992]
Epoch [114/120    avg_loss:0.024, val_acc:0.992]
Epoch [115/120    avg_loss:0.021, val_acc:0.992]
Epoch [116/120    avg_loss:0.024, val_acc:0.994]
Epoch [117/120    avg_loss:0.024, val_acc:0.994]
Epoch [118/120    avg_loss:0.021, val_acc:0.994]
Epoch [119/120    avg_loss:0.021, val_acc:0.992]
Epoch [120/120    avg_loss:0.024, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215  10   0   0   0   5   0   0   0   0   0]
 [  0   0   0   1 220   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.99095023 0.96412556 0.92827004 0.91756272
 1.         0.97826087 0.99359795 1.         1.         1.
 1.         1.        ]

Kappa:
0.989791478792907
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c33fa1dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.195]
Epoch [2/120    avg_loss:2.471, val_acc:0.291]
Epoch [3/120    avg_loss:2.328, val_acc:0.314]
Epoch [4/120    avg_loss:2.184, val_acc:0.438]
Epoch [5/120    avg_loss:2.048, val_acc:0.449]
Epoch [6/120    avg_loss:1.886, val_acc:0.535]
Epoch [7/120    avg_loss:1.743, val_acc:0.645]
Epoch [8/120    avg_loss:1.585, val_acc:0.758]
Epoch [9/120    avg_loss:1.395, val_acc:0.822]
Epoch [10/120    avg_loss:1.187, val_acc:0.809]
Epoch [11/120    avg_loss:1.068, val_acc:0.824]
Epoch [12/120    avg_loss:1.004, val_acc:0.838]
Epoch [13/120    avg_loss:0.881, val_acc:0.893]
Epoch [14/120    avg_loss:0.822, val_acc:0.854]
Epoch [15/120    avg_loss:0.702, val_acc:0.896]
Epoch [16/120    avg_loss:0.723, val_acc:0.855]
Epoch [17/120    avg_loss:0.611, val_acc:0.904]
Epoch [18/120    avg_loss:0.629, val_acc:0.889]
Epoch [19/120    avg_loss:0.579, val_acc:0.887]
Epoch [20/120    avg_loss:0.506, val_acc:0.887]
Epoch [21/120    avg_loss:0.494, val_acc:0.914]
Epoch [22/120    avg_loss:0.467, val_acc:0.906]
Epoch [23/120    avg_loss:0.427, val_acc:0.922]
Epoch [24/120    avg_loss:0.442, val_acc:0.883]
Epoch [25/120    avg_loss:0.429, val_acc:0.906]
Epoch [26/120    avg_loss:0.437, val_acc:0.902]
Epoch [27/120    avg_loss:0.408, val_acc:0.893]
Epoch [28/120    avg_loss:0.404, val_acc:0.912]
Epoch [29/120    avg_loss:0.355, val_acc:0.928]
Epoch [30/120    avg_loss:0.363, val_acc:0.920]
Epoch [31/120    avg_loss:0.363, val_acc:0.924]
Epoch [32/120    avg_loss:0.317, val_acc:0.873]
Epoch [33/120    avg_loss:0.370, val_acc:0.949]
Epoch [34/120    avg_loss:0.332, val_acc:0.918]
Epoch [35/120    avg_loss:0.302, val_acc:0.939]
Epoch [36/120    avg_loss:0.329, val_acc:0.945]
Epoch [37/120    avg_loss:0.248, val_acc:0.951]
Epoch [38/120    avg_loss:0.294, val_acc:0.947]
Epoch [39/120    avg_loss:0.291, val_acc:0.967]
Epoch [40/120    avg_loss:0.233, val_acc:0.900]
Epoch [41/120    avg_loss:0.234, val_acc:0.961]
Epoch [42/120    avg_loss:0.193, val_acc:0.971]
Epoch [43/120    avg_loss:0.158, val_acc:0.977]
Epoch [44/120    avg_loss:0.180, val_acc:0.947]
Epoch [45/120    avg_loss:0.231, val_acc:0.982]
Epoch [46/120    avg_loss:0.201, val_acc:0.971]
Epoch [47/120    avg_loss:0.158, val_acc:0.975]
Epoch [48/120    avg_loss:0.150, val_acc:0.969]
Epoch [49/120    avg_loss:0.147, val_acc:0.979]
Epoch [50/120    avg_loss:0.099, val_acc:0.969]
Epoch [51/120    avg_loss:0.121, val_acc:0.971]
Epoch [52/120    avg_loss:0.167, val_acc:0.969]
Epoch [53/120    avg_loss:0.167, val_acc:0.963]
Epoch [54/120    avg_loss:0.179, val_acc:0.977]
Epoch [55/120    avg_loss:0.148, val_acc:0.971]
Epoch [56/120    avg_loss:0.128, val_acc:0.963]
Epoch [57/120    avg_loss:0.101, val_acc:0.961]
Epoch [58/120    avg_loss:0.089, val_acc:0.984]
Epoch [59/120    avg_loss:0.114, val_acc:0.975]
Epoch [60/120    avg_loss:0.108, val_acc:0.967]
Epoch [61/120    avg_loss:0.111, val_acc:0.971]
Epoch [62/120    avg_loss:0.185, val_acc:0.965]
Epoch [63/120    avg_loss:0.117, val_acc:0.980]
Epoch [64/120    avg_loss:0.094, val_acc:0.965]
Epoch [65/120    avg_loss:0.121, val_acc:0.967]
Epoch [66/120    avg_loss:0.075, val_acc:0.984]
Epoch [67/120    avg_loss:0.075, val_acc:0.955]
Epoch [68/120    avg_loss:0.189, val_acc:0.951]
Epoch [69/120    avg_loss:0.160, val_acc:0.953]
Epoch [70/120    avg_loss:0.138, val_acc:0.980]
Epoch [71/120    avg_loss:0.069, val_acc:0.973]
Epoch [72/120    avg_loss:0.135, val_acc:0.979]
Epoch [73/120    avg_loss:0.061, val_acc:0.980]
Epoch [74/120    avg_loss:0.071, val_acc:0.990]
Epoch [75/120    avg_loss:0.065, val_acc:0.988]
Epoch [76/120    avg_loss:0.090, val_acc:0.965]
Epoch [77/120    avg_loss:0.090, val_acc:0.979]
Epoch [78/120    avg_loss:0.068, val_acc:0.971]
Epoch [79/120    avg_loss:0.081, val_acc:0.982]
Epoch [80/120    avg_loss:0.113, val_acc:0.965]
Epoch [81/120    avg_loss:0.068, val_acc:0.926]
Epoch [82/120    avg_loss:0.051, val_acc:0.969]
Epoch [83/120    avg_loss:0.076, val_acc:0.949]
Epoch [84/120    avg_loss:0.063, val_acc:0.965]
Epoch [85/120    avg_loss:0.079, val_acc:0.979]
Epoch [86/120    avg_loss:0.044, val_acc:0.975]
Epoch [87/120    avg_loss:0.040, val_acc:0.973]
Epoch [88/120    avg_loss:0.046, val_acc:0.980]
Epoch [89/120    avg_loss:0.029, val_acc:0.980]
Epoch [90/120    avg_loss:0.025, val_acc:0.986]
Epoch [91/120    avg_loss:0.024, val_acc:0.984]
Epoch [92/120    avg_loss:0.024, val_acc:0.980]
Epoch [93/120    avg_loss:0.022, val_acc:0.984]
Epoch [94/120    avg_loss:0.021, val_acc:0.984]
Epoch [95/120    avg_loss:0.025, val_acc:0.982]
Epoch [96/120    avg_loss:0.020, val_acc:0.984]
Epoch [97/120    avg_loss:0.023, val_acc:0.984]
Epoch [98/120    avg_loss:0.022, val_acc:0.984]
Epoch [99/120    avg_loss:0.020, val_acc:0.984]
Epoch [100/120    avg_loss:0.022, val_acc:0.984]
Epoch [101/120    avg_loss:0.021, val_acc:0.984]
Epoch [102/120    avg_loss:0.022, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.984]
Epoch [104/120    avg_loss:0.019, val_acc:0.984]
Epoch [105/120    avg_loss:0.026, val_acc:0.984]
Epoch [106/120    avg_loss:0.021, val_acc:0.984]
Epoch [107/120    avg_loss:0.019, val_acc:0.984]
Epoch [108/120    avg_loss:0.024, val_acc:0.984]
Epoch [109/120    avg_loss:0.022, val_acc:0.984]
Epoch [110/120    avg_loss:0.020, val_acc:0.984]
Epoch [111/120    avg_loss:0.021, val_acc:0.984]
Epoch [112/120    avg_loss:0.021, val_acc:0.984]
Epoch [113/120    avg_loss:0.019, val_acc:0.984]
Epoch [114/120    avg_loss:0.019, val_acc:0.984]
Epoch [115/120    avg_loss:0.018, val_acc:0.984]
Epoch [116/120    avg_loss:0.021, val_acc:0.984]
Epoch [117/120    avg_loss:0.020, val_acc:0.984]
Epoch [118/120    avg_loss:0.018, val_acc:0.984]
Epoch [119/120    avg_loss:0.020, val_acc:0.984]
Epoch [120/120    avg_loss:0.018, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215   4   0   0   0  10   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   1  12 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.98206278 0.96412556 0.92747253 0.90102389
 0.99756691 0.95555556 0.98727735 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9874171372142027
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f249b579e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.595, val_acc:0.156]
Epoch [2/120    avg_loss:2.498, val_acc:0.379]
Epoch [3/120    avg_loss:2.373, val_acc:0.486]
Epoch [4/120    avg_loss:2.215, val_acc:0.545]
Epoch [5/120    avg_loss:2.019, val_acc:0.559]
Epoch [6/120    avg_loss:1.825, val_acc:0.631]
Epoch [7/120    avg_loss:1.605, val_acc:0.648]
Epoch [8/120    avg_loss:1.384, val_acc:0.758]
Epoch [9/120    avg_loss:1.196, val_acc:0.785]
Epoch [10/120    avg_loss:1.041, val_acc:0.828]
Epoch [11/120    avg_loss:0.910, val_acc:0.758]
Epoch [12/120    avg_loss:0.825, val_acc:0.873]
Epoch [13/120    avg_loss:0.722, val_acc:0.834]
Epoch [14/120    avg_loss:0.754, val_acc:0.867]
Epoch [15/120    avg_loss:0.632, val_acc:0.857]
Epoch [16/120    avg_loss:0.572, val_acc:0.883]
Epoch [17/120    avg_loss:0.531, val_acc:0.881]
Epoch [18/120    avg_loss:0.449, val_acc:0.861]
Epoch [19/120    avg_loss:0.486, val_acc:0.885]
Epoch [20/120    avg_loss:0.457, val_acc:0.893]
Epoch [21/120    avg_loss:0.444, val_acc:0.900]
Epoch [22/120    avg_loss:0.374, val_acc:0.885]
Epoch [23/120    avg_loss:0.401, val_acc:0.908]
Epoch [24/120    avg_loss:0.355, val_acc:0.914]
Epoch [25/120    avg_loss:0.419, val_acc:0.898]
Epoch [26/120    avg_loss:0.347, val_acc:0.914]
Epoch [27/120    avg_loss:0.285, val_acc:0.932]
Epoch [28/120    avg_loss:0.322, val_acc:0.939]
Epoch [29/120    avg_loss:0.280, val_acc:0.924]
Epoch [30/120    avg_loss:0.277, val_acc:0.914]
Epoch [31/120    avg_loss:0.299, val_acc:0.932]
Epoch [32/120    avg_loss:0.227, val_acc:0.943]
Epoch [33/120    avg_loss:0.218, val_acc:0.953]
Epoch [34/120    avg_loss:0.207, val_acc:0.957]
Epoch [35/120    avg_loss:0.224, val_acc:0.951]
Epoch [36/120    avg_loss:0.242, val_acc:0.943]
Epoch [37/120    avg_loss:0.237, val_acc:0.957]
Epoch [38/120    avg_loss:0.184, val_acc:0.939]
Epoch [39/120    avg_loss:0.284, val_acc:0.926]
Epoch [40/120    avg_loss:0.262, val_acc:0.955]
Epoch [41/120    avg_loss:0.161, val_acc:0.957]
Epoch [42/120    avg_loss:0.150, val_acc:0.951]
Epoch [43/120    avg_loss:0.168, val_acc:0.959]
Epoch [44/120    avg_loss:0.116, val_acc:0.965]
Epoch [45/120    avg_loss:0.164, val_acc:0.973]
Epoch [46/120    avg_loss:0.155, val_acc:0.955]
Epoch [47/120    avg_loss:0.143, val_acc:0.963]
Epoch [48/120    avg_loss:0.153, val_acc:0.943]
Epoch [49/120    avg_loss:0.149, val_acc:0.977]
Epoch [50/120    avg_loss:0.171, val_acc:0.947]
Epoch [51/120    avg_loss:0.164, val_acc:0.969]
Epoch [52/120    avg_loss:0.137, val_acc:0.969]
Epoch [53/120    avg_loss:0.150, val_acc:0.969]
Epoch [54/120    avg_loss:0.109, val_acc:0.965]
Epoch [55/120    avg_loss:0.087, val_acc:0.973]
Epoch [56/120    avg_loss:0.081, val_acc:0.977]
Epoch [57/120    avg_loss:0.071, val_acc:0.975]
Epoch [58/120    avg_loss:0.126, val_acc:0.980]
Epoch [59/120    avg_loss:0.089, val_acc:0.979]
Epoch [60/120    avg_loss:0.100, val_acc:0.975]
Epoch [61/120    avg_loss:0.105, val_acc:0.881]
Epoch [62/120    avg_loss:0.136, val_acc:0.980]
Epoch [63/120    avg_loss:0.111, val_acc:0.984]
Epoch [64/120    avg_loss:0.122, val_acc:0.945]
Epoch [65/120    avg_loss:0.122, val_acc:0.977]
Epoch [66/120    avg_loss:0.121, val_acc:0.969]
Epoch [67/120    avg_loss:0.098, val_acc:0.984]
Epoch [68/120    avg_loss:0.058, val_acc:0.979]
Epoch [69/120    avg_loss:0.064, val_acc:0.979]
Epoch [70/120    avg_loss:0.064, val_acc:0.979]
Epoch [71/120    avg_loss:0.071, val_acc:0.980]
Epoch [72/120    avg_loss:0.066, val_acc:0.959]
Epoch [73/120    avg_loss:0.096, val_acc:0.979]
Epoch [74/120    avg_loss:0.050, val_acc:0.982]
Epoch [75/120    avg_loss:0.083, val_acc:0.953]
Epoch [76/120    avg_loss:0.060, val_acc:0.986]
Epoch [77/120    avg_loss:0.063, val_acc:0.959]
Epoch [78/120    avg_loss:0.071, val_acc:0.957]
Epoch [79/120    avg_loss:0.061, val_acc:0.984]
Epoch [80/120    avg_loss:0.073, val_acc:0.979]
Epoch [81/120    avg_loss:0.087, val_acc:0.982]
Epoch [82/120    avg_loss:0.043, val_acc:0.984]
Epoch [83/120    avg_loss:0.031, val_acc:0.977]
Epoch [84/120    avg_loss:0.043, val_acc:0.986]
Epoch [85/120    avg_loss:0.047, val_acc:0.982]
Epoch [86/120    avg_loss:0.033, val_acc:0.984]
Epoch [87/120    avg_loss:0.084, val_acc:0.975]
Epoch [88/120    avg_loss:0.048, val_acc:0.990]
Epoch [89/120    avg_loss:0.028, val_acc:0.988]
Epoch [90/120    avg_loss:0.030, val_acc:0.971]
Epoch [91/120    avg_loss:0.059, val_acc:0.977]
Epoch [92/120    avg_loss:0.074, val_acc:0.979]
Epoch [93/120    avg_loss:0.124, val_acc:0.961]
Epoch [94/120    avg_loss:0.083, val_acc:0.961]
Epoch [95/120    avg_loss:0.065, val_acc:0.971]
Epoch [96/120    avg_loss:0.050, val_acc:0.979]
Epoch [97/120    avg_loss:0.044, val_acc:0.977]
Epoch [98/120    avg_loss:0.043, val_acc:0.988]
Epoch [99/120    avg_loss:0.037, val_acc:0.980]
Epoch [100/120    avg_loss:0.040, val_acc:0.986]
Epoch [101/120    avg_loss:0.048, val_acc:0.973]
Epoch [102/120    avg_loss:0.029, val_acc:0.984]
Epoch [103/120    avg_loss:0.021, val_acc:0.986]
Epoch [104/120    avg_loss:0.029, val_acc:0.984]
Epoch [105/120    avg_loss:0.021, val_acc:0.986]
Epoch [106/120    avg_loss:0.017, val_acc:0.988]
Epoch [107/120    avg_loss:0.017, val_acc:0.988]
Epoch [108/120    avg_loss:0.015, val_acc:0.988]
Epoch [109/120    avg_loss:0.017, val_acc:0.986]
Epoch [110/120    avg_loss:0.017, val_acc:0.990]
Epoch [111/120    avg_loss:0.016, val_acc:0.988]
Epoch [112/120    avg_loss:0.021, val_acc:0.986]
Epoch [113/120    avg_loss:0.017, val_acc:0.988]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.990]
Epoch [116/120    avg_loss:0.013, val_acc:0.988]
Epoch [117/120    avg_loss:0.017, val_acc:0.992]
Epoch [118/120    avg_loss:0.017, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.990]
Epoch [120/120    avg_loss:0.016, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   3   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.97767857 0.99122807 0.93478261 0.90592334
 1.         0.94382022 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9902665105245348
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a2d298dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.113]
Epoch [2/120    avg_loss:2.473, val_acc:0.327]
Epoch [3/120    avg_loss:2.335, val_acc:0.431]
Epoch [4/120    avg_loss:2.185, val_acc:0.484]
Epoch [5/120    avg_loss:2.012, val_acc:0.532]
Epoch [6/120    avg_loss:1.802, val_acc:0.595]
Epoch [7/120    avg_loss:1.598, val_acc:0.659]
Epoch [8/120    avg_loss:1.363, val_acc:0.708]
Epoch [9/120    avg_loss:1.218, val_acc:0.726]
Epoch [10/120    avg_loss:1.100, val_acc:0.758]
Epoch [11/120    avg_loss:0.946, val_acc:0.829]
Epoch [12/120    avg_loss:0.837, val_acc:0.778]
Epoch [13/120    avg_loss:0.804, val_acc:0.841]
Epoch [14/120    avg_loss:0.683, val_acc:0.863]
Epoch [15/120    avg_loss:0.636, val_acc:0.869]
Epoch [16/120    avg_loss:0.574, val_acc:0.887]
Epoch [17/120    avg_loss:0.491, val_acc:0.861]
Epoch [18/120    avg_loss:0.516, val_acc:0.905]
Epoch [19/120    avg_loss:0.489, val_acc:0.871]
Epoch [20/120    avg_loss:0.488, val_acc:0.897]
Epoch [21/120    avg_loss:0.441, val_acc:0.871]
Epoch [22/120    avg_loss:0.416, val_acc:0.881]
Epoch [23/120    avg_loss:0.444, val_acc:0.915]
Epoch [24/120    avg_loss:0.401, val_acc:0.873]
Epoch [25/120    avg_loss:0.355, val_acc:0.913]
Epoch [26/120    avg_loss:0.367, val_acc:0.897]
Epoch [27/120    avg_loss:0.365, val_acc:0.913]
Epoch [28/120    avg_loss:0.379, val_acc:0.917]
Epoch [29/120    avg_loss:0.336, val_acc:0.917]
Epoch [30/120    avg_loss:0.261, val_acc:0.944]
Epoch [31/120    avg_loss:0.224, val_acc:0.927]
Epoch [32/120    avg_loss:0.239, val_acc:0.925]
Epoch [33/120    avg_loss:0.249, val_acc:0.942]
Epoch [34/120    avg_loss:0.241, val_acc:0.921]
Epoch [35/120    avg_loss:0.222, val_acc:0.950]
Epoch [36/120    avg_loss:0.225, val_acc:0.954]
Epoch [37/120    avg_loss:0.211, val_acc:0.950]
Epoch [38/120    avg_loss:0.191, val_acc:0.952]
Epoch [39/120    avg_loss:0.183, val_acc:0.962]
Epoch [40/120    avg_loss:0.151, val_acc:0.948]
Epoch [41/120    avg_loss:0.183, val_acc:0.940]
Epoch [42/120    avg_loss:0.218, val_acc:0.956]
Epoch [43/120    avg_loss:0.136, val_acc:0.950]
Epoch [44/120    avg_loss:0.113, val_acc:0.937]
Epoch [45/120    avg_loss:0.149, val_acc:0.962]
Epoch [46/120    avg_loss:0.141, val_acc:0.962]
Epoch [47/120    avg_loss:0.144, val_acc:0.970]
Epoch [48/120    avg_loss:0.133, val_acc:0.950]
Epoch [49/120    avg_loss:0.106, val_acc:0.960]
Epoch [50/120    avg_loss:0.094, val_acc:0.950]
Epoch [51/120    avg_loss:0.104, val_acc:0.960]
Epoch [52/120    avg_loss:0.095, val_acc:0.968]
Epoch [53/120    avg_loss:0.104, val_acc:0.974]
Epoch [54/120    avg_loss:0.095, val_acc:0.976]
Epoch [55/120    avg_loss:0.072, val_acc:0.964]
Epoch [56/120    avg_loss:0.052, val_acc:0.978]
Epoch [57/120    avg_loss:0.155, val_acc:0.942]
Epoch [58/120    avg_loss:0.183, val_acc:0.950]
Epoch [59/120    avg_loss:0.139, val_acc:0.948]
Epoch [60/120    avg_loss:0.107, val_acc:0.970]
Epoch [61/120    avg_loss:0.096, val_acc:0.976]
Epoch [62/120    avg_loss:0.072, val_acc:0.974]
Epoch [63/120    avg_loss:0.148, val_acc:0.940]
Epoch [64/120    avg_loss:0.263, val_acc:0.962]
Epoch [65/120    avg_loss:0.150, val_acc:0.974]
Epoch [66/120    avg_loss:0.075, val_acc:0.976]
Epoch [67/120    avg_loss:0.063, val_acc:0.976]
Epoch [68/120    avg_loss:0.079, val_acc:0.964]
Epoch [69/120    avg_loss:0.097, val_acc:0.962]
Epoch [70/120    avg_loss:0.062, val_acc:0.982]
Epoch [71/120    avg_loss:0.049, val_acc:0.978]
Epoch [72/120    avg_loss:0.044, val_acc:0.982]
Epoch [73/120    avg_loss:0.036, val_acc:0.980]
Epoch [74/120    avg_loss:0.036, val_acc:0.978]
Epoch [75/120    avg_loss:0.036, val_acc:0.978]
Epoch [76/120    avg_loss:0.063, val_acc:0.978]
Epoch [77/120    avg_loss:0.032, val_acc:0.980]
Epoch [78/120    avg_loss:0.028, val_acc:0.980]
Epoch [79/120    avg_loss:0.044, val_acc:0.980]
Epoch [80/120    avg_loss:0.030, val_acc:0.980]
Epoch [81/120    avg_loss:0.032, val_acc:0.980]
Epoch [82/120    avg_loss:0.024, val_acc:0.980]
Epoch [83/120    avg_loss:0.026, val_acc:0.980]
Epoch [84/120    avg_loss:0.041, val_acc:0.980]
Epoch [85/120    avg_loss:0.027, val_acc:0.980]
Epoch [86/120    avg_loss:0.033, val_acc:0.980]
Epoch [87/120    avg_loss:0.036, val_acc:0.980]
Epoch [88/120    avg_loss:0.029, val_acc:0.980]
Epoch [89/120    avg_loss:0.026, val_acc:0.980]
Epoch [90/120    avg_loss:0.030, val_acc:0.980]
Epoch [91/120    avg_loss:0.029, val_acc:0.980]
Epoch [92/120    avg_loss:0.027, val_acc:0.980]
Epoch [93/120    avg_loss:0.027, val_acc:0.980]
Epoch [94/120    avg_loss:0.025, val_acc:0.980]
Epoch [95/120    avg_loss:0.027, val_acc:0.980]
Epoch [96/120    avg_loss:0.024, val_acc:0.980]
Epoch [97/120    avg_loss:0.023, val_acc:0.980]
Epoch [98/120    avg_loss:0.024, val_acc:0.980]
Epoch [99/120    avg_loss:0.028, val_acc:0.980]
Epoch [100/120    avg_loss:0.027, val_acc:0.980]
Epoch [101/120    avg_loss:0.032, val_acc:0.980]
Epoch [102/120    avg_loss:0.030, val_acc:0.980]
Epoch [103/120    avg_loss:0.025, val_acc:0.980]
Epoch [104/120    avg_loss:0.035, val_acc:0.980]
Epoch [105/120    avg_loss:0.031, val_acc:0.980]
Epoch [106/120    avg_loss:0.028, val_acc:0.980]
Epoch [107/120    avg_loss:0.034, val_acc:0.980]
Epoch [108/120    avg_loss:0.028, val_acc:0.980]
Epoch [109/120    avg_loss:0.032, val_acc:0.980]
Epoch [110/120    avg_loss:0.023, val_acc:0.980]
Epoch [111/120    avg_loss:0.028, val_acc:0.980]
Epoch [112/120    avg_loss:0.029, val_acc:0.980]
Epoch [113/120    avg_loss:0.029, val_acc:0.980]
Epoch [114/120    avg_loss:0.026, val_acc:0.980]
Epoch [115/120    avg_loss:0.028, val_acc:0.980]
Epoch [116/120    avg_loss:0.025, val_acc:0.980]
Epoch [117/120    avg_loss:0.027, val_acc:0.980]
Epoch [118/120    avg_loss:0.029, val_acc:0.980]
Epoch [119/120    avg_loss:0.022, val_acc:0.980]
Epoch [120/120    avg_loss:0.033, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 227   1   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   1  18 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.97333333 0.99126638 0.93506494 0.89361702
 1.         0.93785311 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9895540221469309
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f564b704da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.311]
Epoch [2/120    avg_loss:2.504, val_acc:0.402]
Epoch [3/120    avg_loss:2.377, val_acc:0.400]
Epoch [4/120    avg_loss:2.237, val_acc:0.543]
Epoch [5/120    avg_loss:2.079, val_acc:0.602]
Epoch [6/120    avg_loss:1.915, val_acc:0.621]
Epoch [7/120    avg_loss:1.688, val_acc:0.613]
Epoch [8/120    avg_loss:1.468, val_acc:0.707]
Epoch [9/120    avg_loss:1.292, val_acc:0.812]
Epoch [10/120    avg_loss:1.081, val_acc:0.811]
Epoch [11/120    avg_loss:0.958, val_acc:0.805]
Epoch [12/120    avg_loss:0.885, val_acc:0.844]
Epoch [13/120    avg_loss:0.779, val_acc:0.844]
Epoch [14/120    avg_loss:0.659, val_acc:0.873]
Epoch [15/120    avg_loss:0.632, val_acc:0.912]
Epoch [16/120    avg_loss:0.628, val_acc:0.906]
Epoch [17/120    avg_loss:0.530, val_acc:0.910]
Epoch [18/120    avg_loss:0.480, val_acc:0.916]
Epoch [19/120    avg_loss:0.453, val_acc:0.895]
Epoch [20/120    avg_loss:0.489, val_acc:0.945]
Epoch [21/120    avg_loss:0.418, val_acc:0.943]
Epoch [22/120    avg_loss:0.376, val_acc:0.920]
Epoch [23/120    avg_loss:0.375, val_acc:0.934]
Epoch [24/120    avg_loss:0.394, val_acc:0.906]
Epoch [25/120    avg_loss:0.327, val_acc:0.934]
Epoch [26/120    avg_loss:0.302, val_acc:0.943]
Epoch [27/120    avg_loss:0.313, val_acc:0.922]
Epoch [28/120    avg_loss:0.327, val_acc:0.914]
Epoch [29/120    avg_loss:0.317, val_acc:0.938]
Epoch [30/120    avg_loss:0.291, val_acc:0.939]
Epoch [31/120    avg_loss:0.317, val_acc:0.928]
Epoch [32/120    avg_loss:0.275, val_acc:0.928]
Epoch [33/120    avg_loss:0.268, val_acc:0.955]
Epoch [34/120    avg_loss:0.211, val_acc:0.959]
Epoch [35/120    avg_loss:0.253, val_acc:0.961]
Epoch [36/120    avg_loss:0.203, val_acc:0.959]
Epoch [37/120    avg_loss:0.206, val_acc:0.955]
Epoch [38/120    avg_loss:0.185, val_acc:0.949]
Epoch [39/120    avg_loss:0.217, val_acc:0.943]
Epoch [40/120    avg_loss:0.203, val_acc:0.965]
Epoch [41/120    avg_loss:0.156, val_acc:0.957]
Epoch [42/120    avg_loss:0.173, val_acc:0.975]
Epoch [43/120    avg_loss:0.134, val_acc:0.979]
Epoch [44/120    avg_loss:0.151, val_acc:0.963]
Epoch [45/120    avg_loss:0.117, val_acc:0.977]
Epoch [46/120    avg_loss:0.195, val_acc:0.971]
Epoch [47/120    avg_loss:0.125, val_acc:0.977]
Epoch [48/120    avg_loss:0.116, val_acc:0.971]
Epoch [49/120    avg_loss:0.117, val_acc:0.979]
Epoch [50/120    avg_loss:0.114, val_acc:0.967]
Epoch [51/120    avg_loss:0.124, val_acc:0.977]
Epoch [52/120    avg_loss:0.181, val_acc:0.920]
Epoch [53/120    avg_loss:0.184, val_acc:0.986]
Epoch [54/120    avg_loss:0.117, val_acc:0.975]
Epoch [55/120    avg_loss:0.109, val_acc:0.980]
Epoch [56/120    avg_loss:0.107, val_acc:0.982]
Epoch [57/120    avg_loss:0.118, val_acc:0.945]
Epoch [58/120    avg_loss:0.101, val_acc:0.979]
Epoch [59/120    avg_loss:0.101, val_acc:0.979]
Epoch [60/120    avg_loss:0.099, val_acc:0.973]
Epoch [61/120    avg_loss:0.148, val_acc:0.982]
Epoch [62/120    avg_loss:0.072, val_acc:0.988]
Epoch [63/120    avg_loss:0.112, val_acc:0.977]
Epoch [64/120    avg_loss:0.091, val_acc:0.982]
Epoch [65/120    avg_loss:0.081, val_acc:0.973]
Epoch [66/120    avg_loss:0.111, val_acc:0.957]
Epoch [67/120    avg_loss:0.104, val_acc:0.984]
Epoch [68/120    avg_loss:0.073, val_acc:0.986]
Epoch [69/120    avg_loss:0.093, val_acc:0.984]
Epoch [70/120    avg_loss:0.064, val_acc:0.984]
Epoch [71/120    avg_loss:0.045, val_acc:0.979]
Epoch [72/120    avg_loss:0.059, val_acc:0.992]
Epoch [73/120    avg_loss:0.046, val_acc:0.988]
Epoch [74/120    avg_loss:0.047, val_acc:0.988]
Epoch [75/120    avg_loss:0.062, val_acc:0.990]
Epoch [76/120    avg_loss:0.068, val_acc:0.980]
Epoch [77/120    avg_loss:0.074, val_acc:0.980]
Epoch [78/120    avg_loss:0.127, val_acc:0.979]
Epoch [79/120    avg_loss:0.092, val_acc:0.955]
Epoch [80/120    avg_loss:0.047, val_acc:0.990]
Epoch [81/120    avg_loss:0.060, val_acc:0.984]
Epoch [82/120    avg_loss:0.048, val_acc:0.986]
Epoch [83/120    avg_loss:0.051, val_acc:0.980]
Epoch [84/120    avg_loss:0.044, val_acc:0.973]
Epoch [85/120    avg_loss:0.045, val_acc:0.986]
Epoch [86/120    avg_loss:0.032, val_acc:0.988]
Epoch [87/120    avg_loss:0.029, val_acc:0.990]
Epoch [88/120    avg_loss:0.025, val_acc:0.990]
Epoch [89/120    avg_loss:0.020, val_acc:0.990]
Epoch [90/120    avg_loss:0.025, val_acc:0.990]
Epoch [91/120    avg_loss:0.026, val_acc:0.990]
Epoch [92/120    avg_loss:0.024, val_acc:0.992]
Epoch [93/120    avg_loss:0.024, val_acc:0.990]
Epoch [94/120    avg_loss:0.024, val_acc:0.992]
Epoch [95/120    avg_loss:0.025, val_acc:0.992]
Epoch [96/120    avg_loss:0.023, val_acc:0.992]
Epoch [97/120    avg_loss:0.026, val_acc:0.992]
Epoch [98/120    avg_loss:0.026, val_acc:0.992]
Epoch [99/120    avg_loss:0.024, val_acc:0.992]
Epoch [100/120    avg_loss:0.017, val_acc:0.992]
Epoch [101/120    avg_loss:0.021, val_acc:0.992]
Epoch [102/120    avg_loss:0.023, val_acc:0.992]
Epoch [103/120    avg_loss:0.028, val_acc:0.992]
Epoch [104/120    avg_loss:0.022, val_acc:0.992]
Epoch [105/120    avg_loss:0.023, val_acc:0.992]
Epoch [106/120    avg_loss:0.025, val_acc:0.992]
Epoch [107/120    avg_loss:0.019, val_acc:0.992]
Epoch [108/120    avg_loss:0.024, val_acc:0.992]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.021, val_acc:0.992]
Epoch [111/120    avg_loss:0.020, val_acc:0.992]
Epoch [112/120    avg_loss:0.021, val_acc:0.992]
Epoch [113/120    avg_loss:0.022, val_acc:0.992]
Epoch [114/120    avg_loss:0.019, val_acc:0.992]
Epoch [115/120    avg_loss:0.019, val_acc:0.992]
Epoch [116/120    avg_loss:0.019, val_acc:0.992]
Epoch [117/120    avg_loss:0.020, val_acc:0.992]
Epoch [118/120    avg_loss:0.020, val_acc:0.992]
Epoch [119/120    avg_loss:0.018, val_acc:0.992]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   0 222   2   0   0   0   4   2   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.96832579 0.98230088 0.94827586 0.92198582
 1.         0.92391304 0.99487179 0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.9895540221469309
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f547b459e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.332]
Epoch [2/120    avg_loss:2.520, val_acc:0.449]
Epoch [3/120    avg_loss:2.397, val_acc:0.490]
Epoch [4/120    avg_loss:2.232, val_acc:0.566]
Epoch [5/120    avg_loss:2.063, val_acc:0.631]
Epoch [6/120    avg_loss:1.840, val_acc:0.650]
Epoch [7/120    avg_loss:1.587, val_acc:0.654]
Epoch [8/120    avg_loss:1.341, val_acc:0.672]
Epoch [9/120    avg_loss:1.170, val_acc:0.682]
Epoch [10/120    avg_loss:1.033, val_acc:0.729]
Epoch [11/120    avg_loss:0.848, val_acc:0.838]
Epoch [12/120    avg_loss:0.852, val_acc:0.865]
Epoch [13/120    avg_loss:0.691, val_acc:0.887]
Epoch [14/120    avg_loss:0.649, val_acc:0.893]
Epoch [15/120    avg_loss:0.571, val_acc:0.906]
Epoch [16/120    avg_loss:0.517, val_acc:0.896]
Epoch [17/120    avg_loss:0.515, val_acc:0.875]
Epoch [18/120    avg_loss:0.512, val_acc:0.883]
Epoch [19/120    avg_loss:0.464, val_acc:0.848]
Epoch [20/120    avg_loss:0.408, val_acc:0.916]
Epoch [21/120    avg_loss:0.417, val_acc:0.922]
Epoch [22/120    avg_loss:0.360, val_acc:0.916]
Epoch [23/120    avg_loss:0.312, val_acc:0.943]
Epoch [24/120    avg_loss:0.356, val_acc:0.906]
Epoch [25/120    avg_loss:0.311, val_acc:0.918]
Epoch [26/120    avg_loss:0.316, val_acc:0.930]
Epoch [27/120    avg_loss:0.234, val_acc:0.930]
Epoch [28/120    avg_loss:0.253, val_acc:0.936]
Epoch [29/120    avg_loss:0.311, val_acc:0.938]
Epoch [30/120    avg_loss:0.219, val_acc:0.930]
Epoch [31/120    avg_loss:0.257, val_acc:0.820]
Epoch [32/120    avg_loss:0.226, val_acc:0.949]
Epoch [33/120    avg_loss:0.212, val_acc:0.928]
Epoch [34/120    avg_loss:0.186, val_acc:0.959]
Epoch [35/120    avg_loss:0.204, val_acc:0.938]
Epoch [36/120    avg_loss:0.206, val_acc:0.924]
Epoch [37/120    avg_loss:0.204, val_acc:0.959]
Epoch [38/120    avg_loss:0.169, val_acc:0.955]
Epoch [39/120    avg_loss:0.158, val_acc:0.924]
Epoch [40/120    avg_loss:0.159, val_acc:0.969]
Epoch [41/120    avg_loss:0.137, val_acc:0.955]
Epoch [42/120    avg_loss:0.203, val_acc:0.955]
Epoch [43/120    avg_loss:0.134, val_acc:0.975]
Epoch [44/120    avg_loss:0.106, val_acc:0.971]
Epoch [45/120    avg_loss:0.139, val_acc:0.973]
Epoch [46/120    avg_loss:0.093, val_acc:0.977]
Epoch [47/120    avg_loss:0.092, val_acc:0.971]
Epoch [48/120    avg_loss:0.108, val_acc:0.971]
Epoch [49/120    avg_loss:0.120, val_acc:0.979]
Epoch [50/120    avg_loss:0.124, val_acc:0.959]
Epoch [51/120    avg_loss:0.112, val_acc:0.959]
Epoch [52/120    avg_loss:0.106, val_acc:0.953]
Epoch [53/120    avg_loss:0.163, val_acc:0.951]
Epoch [54/120    avg_loss:0.130, val_acc:0.969]
Epoch [55/120    avg_loss:0.088, val_acc:0.975]
Epoch [56/120    avg_loss:0.093, val_acc:0.980]
Epoch [57/120    avg_loss:0.069, val_acc:0.971]
Epoch [58/120    avg_loss:0.066, val_acc:0.982]
Epoch [59/120    avg_loss:0.102, val_acc:0.975]
Epoch [60/120    avg_loss:0.060, val_acc:0.980]
Epoch [61/120    avg_loss:0.058, val_acc:0.979]
Epoch [62/120    avg_loss:0.043, val_acc:0.982]
Epoch [63/120    avg_loss:0.049, val_acc:0.973]
Epoch [64/120    avg_loss:0.043, val_acc:0.977]
Epoch [65/120    avg_loss:0.029, val_acc:0.986]
Epoch [66/120    avg_loss:0.042, val_acc:0.982]
Epoch [67/120    avg_loss:0.051, val_acc:0.992]
Epoch [68/120    avg_loss:0.044, val_acc:0.961]
Epoch [69/120    avg_loss:0.035, val_acc:0.982]
Epoch [70/120    avg_loss:0.032, val_acc:0.992]
Epoch [71/120    avg_loss:0.031, val_acc:0.973]
Epoch [72/120    avg_loss:0.022, val_acc:0.990]
Epoch [73/120    avg_loss:0.027, val_acc:0.904]
Epoch [74/120    avg_loss:0.038, val_acc:0.980]
Epoch [75/120    avg_loss:0.037, val_acc:0.992]
Epoch [76/120    avg_loss:0.034, val_acc:0.996]
Epoch [77/120    avg_loss:0.018, val_acc:0.994]
Epoch [78/120    avg_loss:0.015, val_acc:1.000]
Epoch [79/120    avg_loss:0.028, val_acc:0.990]
Epoch [80/120    avg_loss:0.024, val_acc:0.982]
Epoch [81/120    avg_loss:0.018, val_acc:0.992]
Epoch [82/120    avg_loss:0.016, val_acc:0.998]
Epoch [83/120    avg_loss:0.013, val_acc:0.994]
Epoch [84/120    avg_loss:0.013, val_acc:0.996]
Epoch [85/120    avg_loss:0.013, val_acc:0.994]
Epoch [86/120    avg_loss:0.011, val_acc:0.992]
Epoch [87/120    avg_loss:0.009, val_acc:0.992]
Epoch [88/120    avg_loss:0.012, val_acc:0.998]
Epoch [89/120    avg_loss:0.012, val_acc:0.998]
Epoch [90/120    avg_loss:0.032, val_acc:0.979]
Epoch [91/120    avg_loss:0.049, val_acc:0.955]
Epoch [92/120    avg_loss:0.044, val_acc:0.988]
Epoch [93/120    avg_loss:0.021, val_acc:0.990]
Epoch [94/120    avg_loss:0.018, val_acc:0.992]
Epoch [95/120    avg_loss:0.017, val_acc:0.994]
Epoch [96/120    avg_loss:0.016, val_acc:0.992]
Epoch [97/120    avg_loss:0.013, val_acc:0.992]
Epoch [98/120    avg_loss:0.012, val_acc:0.994]
Epoch [99/120    avg_loss:0.014, val_acc:0.994]
Epoch [100/120    avg_loss:0.014, val_acc:0.994]
Epoch [101/120    avg_loss:0.012, val_acc:0.994]
Epoch [102/120    avg_loss:0.012, val_acc:0.994]
Epoch [103/120    avg_loss:0.012, val_acc:0.994]
Epoch [104/120    avg_loss:0.011, val_acc:0.994]
Epoch [105/120    avg_loss:0.012, val_acc:0.994]
Epoch [106/120    avg_loss:0.014, val_acc:0.994]
Epoch [107/120    avg_loss:0.013, val_acc:0.994]
Epoch [108/120    avg_loss:0.011, val_acc:0.994]
Epoch [109/120    avg_loss:0.010, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.994]
Epoch [111/120    avg_loss:0.011, val_acc:0.994]
Epoch [112/120    avg_loss:0.010, val_acc:0.994]
Epoch [113/120    avg_loss:0.010, val_acc:0.994]
Epoch [114/120    avg_loss:0.011, val_acc:0.994]
Epoch [115/120    avg_loss:0.012, val_acc:0.994]
Epoch [116/120    avg_loss:0.011, val_acc:0.994]
Epoch [117/120    avg_loss:0.011, val_acc:0.994]
Epoch [118/120    avg_loss:0.010, val_acc:0.994]
Epoch [119/120    avg_loss:0.012, val_acc:0.994]
Epoch [120/120    avg_loss:0.011, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   6   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.99319728 0.98230088 0.90829694 0.87671233
 1.         0.98924731 0.99742268 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9888429050356242
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a50636e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.588, val_acc:0.399]
Epoch [2/120    avg_loss:2.461, val_acc:0.506]
Epoch [3/120    avg_loss:2.326, val_acc:0.522]
Epoch [4/120    avg_loss:2.175, val_acc:0.534]
Epoch [5/120    avg_loss:2.016, val_acc:0.581]
Epoch [6/120    avg_loss:1.833, val_acc:0.653]
Epoch [7/120    avg_loss:1.611, val_acc:0.681]
Epoch [8/120    avg_loss:1.420, val_acc:0.748]
Epoch [9/120    avg_loss:1.242, val_acc:0.796]
Epoch [10/120    avg_loss:1.071, val_acc:0.778]
Epoch [11/120    avg_loss:1.037, val_acc:0.788]
Epoch [12/120    avg_loss:0.898, val_acc:0.821]
Epoch [13/120    avg_loss:0.861, val_acc:0.772]
Epoch [14/120    avg_loss:0.755, val_acc:0.851]
Epoch [15/120    avg_loss:0.673, val_acc:0.845]
Epoch [16/120    avg_loss:0.644, val_acc:0.865]
Epoch [17/120    avg_loss:0.600, val_acc:0.871]
Epoch [18/120    avg_loss:0.511, val_acc:0.893]
Epoch [19/120    avg_loss:0.510, val_acc:0.877]
Epoch [20/120    avg_loss:0.474, val_acc:0.861]
Epoch [21/120    avg_loss:0.512, val_acc:0.879]
Epoch [22/120    avg_loss:0.468, val_acc:0.917]
Epoch [23/120    avg_loss:0.380, val_acc:0.909]
Epoch [24/120    avg_loss:0.390, val_acc:0.895]
Epoch [25/120    avg_loss:0.419, val_acc:0.937]
Epoch [26/120    avg_loss:0.332, val_acc:0.946]
Epoch [27/120    avg_loss:0.365, val_acc:0.907]
Epoch [28/120    avg_loss:0.377, val_acc:0.881]
Epoch [29/120    avg_loss:0.439, val_acc:0.935]
Epoch [30/120    avg_loss:0.304, val_acc:0.913]
Epoch [31/120    avg_loss:0.274, val_acc:0.948]
Epoch [32/120    avg_loss:0.309, val_acc:0.919]
Epoch [33/120    avg_loss:0.264, val_acc:0.948]
Epoch [34/120    avg_loss:0.258, val_acc:0.935]
Epoch [35/120    avg_loss:0.303, val_acc:0.942]
Epoch [36/120    avg_loss:0.277, val_acc:0.927]
Epoch [37/120    avg_loss:0.284, val_acc:0.933]
Epoch [38/120    avg_loss:0.277, val_acc:0.948]
Epoch [39/120    avg_loss:0.166, val_acc:0.954]
Epoch [40/120    avg_loss:0.191, val_acc:0.933]
Epoch [41/120    avg_loss:0.211, val_acc:0.966]
Epoch [42/120    avg_loss:0.191, val_acc:0.935]
Epoch [43/120    avg_loss:0.237, val_acc:0.909]
Epoch [44/120    avg_loss:0.250, val_acc:0.940]
Epoch [45/120    avg_loss:0.188, val_acc:0.960]
Epoch [46/120    avg_loss:0.212, val_acc:0.964]
Epoch [47/120    avg_loss:0.186, val_acc:0.972]
Epoch [48/120    avg_loss:0.151, val_acc:0.962]
Epoch [49/120    avg_loss:0.158, val_acc:0.964]
Epoch [50/120    avg_loss:0.139, val_acc:0.956]
Epoch [51/120    avg_loss:0.123, val_acc:0.976]
Epoch [52/120    avg_loss:0.164, val_acc:0.976]
Epoch [53/120    avg_loss:0.135, val_acc:0.972]
Epoch [54/120    avg_loss:0.159, val_acc:0.972]
Epoch [55/120    avg_loss:0.129, val_acc:0.964]
Epoch [56/120    avg_loss:0.147, val_acc:0.911]
Epoch [57/120    avg_loss:0.189, val_acc:0.978]
Epoch [58/120    avg_loss:0.106, val_acc:0.976]
Epoch [59/120    avg_loss:0.088, val_acc:0.970]
Epoch [60/120    avg_loss:0.107, val_acc:0.960]
Epoch [61/120    avg_loss:0.109, val_acc:0.954]
Epoch [62/120    avg_loss:0.148, val_acc:0.976]
Epoch [63/120    avg_loss:0.097, val_acc:0.960]
Epoch [64/120    avg_loss:0.069, val_acc:0.978]
Epoch [65/120    avg_loss:0.089, val_acc:0.982]
Epoch [66/120    avg_loss:0.085, val_acc:0.972]
Epoch [67/120    avg_loss:0.088, val_acc:0.972]
Epoch [68/120    avg_loss:0.069, val_acc:0.976]
Epoch [69/120    avg_loss:0.087, val_acc:0.960]
Epoch [70/120    avg_loss:0.091, val_acc:0.970]
Epoch [71/120    avg_loss:0.145, val_acc:0.980]
Epoch [72/120    avg_loss:0.132, val_acc:0.978]
Epoch [73/120    avg_loss:0.091, val_acc:0.964]
Epoch [74/120    avg_loss:0.103, val_acc:0.958]
Epoch [75/120    avg_loss:0.075, val_acc:0.972]
Epoch [76/120    avg_loss:0.127, val_acc:0.921]
Epoch [77/120    avg_loss:0.122, val_acc:0.982]
Epoch [78/120    avg_loss:0.060, val_acc:0.982]
Epoch [79/120    avg_loss:0.059, val_acc:0.986]
Epoch [80/120    avg_loss:0.050, val_acc:0.982]
Epoch [81/120    avg_loss:0.049, val_acc:0.988]
Epoch [82/120    avg_loss:0.067, val_acc:0.982]
Epoch [83/120    avg_loss:0.052, val_acc:0.980]
Epoch [84/120    avg_loss:0.061, val_acc:0.972]
Epoch [85/120    avg_loss:0.047, val_acc:0.984]
Epoch [86/120    avg_loss:0.034, val_acc:0.984]
Epoch [87/120    avg_loss:0.052, val_acc:0.982]
Epoch [88/120    avg_loss:0.052, val_acc:0.990]
Epoch [89/120    avg_loss:0.053, val_acc:0.962]
Epoch [90/120    avg_loss:0.051, val_acc:0.980]
Epoch [91/120    avg_loss:0.040, val_acc:0.988]
Epoch [92/120    avg_loss:0.033, val_acc:0.984]
Epoch [93/120    avg_loss:0.037, val_acc:0.986]
Epoch [94/120    avg_loss:0.048, val_acc:0.986]
Epoch [95/120    avg_loss:0.031, val_acc:0.990]
Epoch [96/120    avg_loss:0.029, val_acc:0.992]
Epoch [97/120    avg_loss:0.019, val_acc:0.994]
Epoch [98/120    avg_loss:0.020, val_acc:0.990]
Epoch [99/120    avg_loss:0.019, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.994]
Epoch [101/120    avg_loss:0.031, val_acc:0.978]
Epoch [102/120    avg_loss:0.046, val_acc:0.990]
Epoch [103/120    avg_loss:0.045, val_acc:0.986]
Epoch [104/120    avg_loss:0.044, val_acc:0.980]
Epoch [105/120    avg_loss:0.034, val_acc:0.986]
Epoch [106/120    avg_loss:0.026, val_acc:0.990]
Epoch [107/120    avg_loss:0.021, val_acc:0.982]
Epoch [108/120    avg_loss:0.046, val_acc:0.988]
Epoch [109/120    avg_loss:0.044, val_acc:0.978]
Epoch [110/120    avg_loss:0.024, val_acc:0.986]
Epoch [111/120    avg_loss:0.025, val_acc:0.992]
Epoch [112/120    avg_loss:0.013, val_acc:0.994]
Epoch [113/120    avg_loss:0.013, val_acc:0.990]
Epoch [114/120    avg_loss:0.015, val_acc:0.958]
Epoch [115/120    avg_loss:0.021, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.996]
Epoch [118/120    avg_loss:0.038, val_acc:0.974]
Epoch [119/120    avg_loss:0.044, val_acc:0.980]
Epoch [120/120    avg_loss:0.045, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215   8   0   0   0   0   7   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   4   3   0   0   0   0   0 381   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 0.99708879 0.98206278 0.96629213 0.90869565 0.88356164
 1.         0.9726776  0.99089727 0.99257688 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9850427456430486
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f444c4e4e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.587, val_acc:0.403]
Epoch [2/120    avg_loss:2.466, val_acc:0.391]
Epoch [3/120    avg_loss:2.339, val_acc:0.427]
Epoch [4/120    avg_loss:2.201, val_acc:0.456]
Epoch [5/120    avg_loss:2.079, val_acc:0.583]
Epoch [6/120    avg_loss:1.896, val_acc:0.601]
Epoch [7/120    avg_loss:1.682, val_acc:0.687]
Epoch [8/120    avg_loss:1.523, val_acc:0.738]
Epoch [9/120    avg_loss:1.349, val_acc:0.764]
Epoch [10/120    avg_loss:1.139, val_acc:0.825]
Epoch [11/120    avg_loss:1.038, val_acc:0.839]
Epoch [12/120    avg_loss:0.900, val_acc:0.857]
Epoch [13/120    avg_loss:0.809, val_acc:0.863]
Epoch [14/120    avg_loss:0.725, val_acc:0.869]
Epoch [15/120    avg_loss:0.677, val_acc:0.875]
Epoch [16/120    avg_loss:0.601, val_acc:0.877]
Epoch [17/120    avg_loss:0.559, val_acc:0.899]
Epoch [18/120    avg_loss:0.514, val_acc:0.893]
Epoch [19/120    avg_loss:0.479, val_acc:0.899]
Epoch [20/120    avg_loss:0.503, val_acc:0.891]
Epoch [21/120    avg_loss:0.518, val_acc:0.889]
Epoch [22/120    avg_loss:0.419, val_acc:0.921]
Epoch [23/120    avg_loss:0.373, val_acc:0.885]
Epoch [24/120    avg_loss:0.455, val_acc:0.897]
Epoch [25/120    avg_loss:0.434, val_acc:0.841]
Epoch [26/120    avg_loss:0.337, val_acc:0.901]
Epoch [27/120    avg_loss:0.425, val_acc:0.812]
Epoch [28/120    avg_loss:0.406, val_acc:0.927]
Epoch [29/120    avg_loss:0.306, val_acc:0.879]
Epoch [30/120    avg_loss:0.369, val_acc:0.933]
Epoch [31/120    avg_loss:0.314, val_acc:0.907]
Epoch [32/120    avg_loss:0.315, val_acc:0.907]
Epoch [33/120    avg_loss:0.271, val_acc:0.940]
Epoch [34/120    avg_loss:0.265, val_acc:0.935]
Epoch [35/120    avg_loss:0.284, val_acc:0.946]
Epoch [36/120    avg_loss:0.246, val_acc:0.929]
Epoch [37/120    avg_loss:0.242, val_acc:0.929]
Epoch [38/120    avg_loss:0.266, val_acc:0.942]
Epoch [39/120    avg_loss:0.226, val_acc:0.948]
Epoch [40/120    avg_loss:0.214, val_acc:0.889]
Epoch [41/120    avg_loss:0.222, val_acc:0.933]
Epoch [42/120    avg_loss:0.251, val_acc:0.946]
Epoch [43/120    avg_loss:0.224, val_acc:0.952]
Epoch [44/120    avg_loss:0.178, val_acc:0.946]
Epoch [45/120    avg_loss:0.245, val_acc:0.948]
Epoch [46/120    avg_loss:0.177, val_acc:0.968]
Epoch [47/120    avg_loss:0.168, val_acc:0.958]
Epoch [48/120    avg_loss:0.165, val_acc:0.968]
Epoch [49/120    avg_loss:0.158, val_acc:0.966]
Epoch [50/120    avg_loss:0.124, val_acc:0.958]
Epoch [51/120    avg_loss:0.187, val_acc:0.966]
Epoch [52/120    avg_loss:0.130, val_acc:0.968]
Epoch [53/120    avg_loss:0.131, val_acc:0.958]
Epoch [54/120    avg_loss:0.101, val_acc:0.972]
Epoch [55/120    avg_loss:0.129, val_acc:0.970]
Epoch [56/120    avg_loss:0.158, val_acc:0.968]
Epoch [57/120    avg_loss:0.116, val_acc:0.982]
Epoch [58/120    avg_loss:0.109, val_acc:0.976]
Epoch [59/120    avg_loss:0.099, val_acc:0.990]
Epoch [60/120    avg_loss:0.072, val_acc:0.980]
Epoch [61/120    avg_loss:0.090, val_acc:0.988]
Epoch [62/120    avg_loss:0.073, val_acc:0.984]
Epoch [63/120    avg_loss:0.109, val_acc:0.978]
Epoch [64/120    avg_loss:0.082, val_acc:0.988]
Epoch [65/120    avg_loss:0.063, val_acc:0.982]
Epoch [66/120    avg_loss:0.054, val_acc:0.976]
Epoch [67/120    avg_loss:0.049, val_acc:0.976]
Epoch [68/120    avg_loss:0.062, val_acc:0.976]
Epoch [69/120    avg_loss:0.063, val_acc:0.976]
Epoch [70/120    avg_loss:0.068, val_acc:0.970]
Epoch [71/120    avg_loss:0.051, val_acc:0.992]
Epoch [72/120    avg_loss:0.035, val_acc:0.984]
Epoch [73/120    avg_loss:0.032, val_acc:0.990]
Epoch [74/120    avg_loss:0.034, val_acc:0.994]
Epoch [75/120    avg_loss:0.024, val_acc:0.986]
Epoch [76/120    avg_loss:0.034, val_acc:0.988]
Epoch [77/120    avg_loss:0.019, val_acc:0.990]
Epoch [78/120    avg_loss:0.048, val_acc:0.970]
Epoch [79/120    avg_loss:0.066, val_acc:0.986]
Epoch [80/120    avg_loss:0.051, val_acc:0.968]
Epoch [81/120    avg_loss:0.094, val_acc:0.984]
Epoch [82/120    avg_loss:0.065, val_acc:0.982]
Epoch [83/120    avg_loss:0.129, val_acc:0.917]
Epoch [84/120    avg_loss:0.176, val_acc:0.968]
Epoch [85/120    avg_loss:0.070, val_acc:0.982]
Epoch [86/120    avg_loss:0.031, val_acc:0.980]
Epoch [87/120    avg_loss:0.031, val_acc:0.984]
Epoch [88/120    avg_loss:0.027, val_acc:0.984]
Epoch [89/120    avg_loss:0.019, val_acc:0.988]
Epoch [90/120    avg_loss:0.025, val_acc:0.990]
Epoch [91/120    avg_loss:0.020, val_acc:0.990]
Epoch [92/120    avg_loss:0.019, val_acc:0.990]
Epoch [93/120    avg_loss:0.020, val_acc:0.990]
Epoch [94/120    avg_loss:0.016, val_acc:0.990]
Epoch [95/120    avg_loss:0.019, val_acc:0.990]
Epoch [96/120    avg_loss:0.020, val_acc:0.992]
Epoch [97/120    avg_loss:0.015, val_acc:0.992]
Epoch [98/120    avg_loss:0.015, val_acc:0.992]
Epoch [99/120    avg_loss:0.016, val_acc:0.992]
Epoch [100/120    avg_loss:0.015, val_acc:0.992]
Epoch [101/120    avg_loss:0.018, val_acc:0.992]
Epoch [102/120    avg_loss:0.014, val_acc:0.992]
Epoch [103/120    avg_loss:0.016, val_acc:0.992]
Epoch [104/120    avg_loss:0.015, val_acc:0.992]
Epoch [105/120    avg_loss:0.015, val_acc:0.992]
Epoch [106/120    avg_loss:0.015, val_acc:0.992]
Epoch [107/120    avg_loss:0.016, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.992]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.017, val_acc:0.992]
Epoch [111/120    avg_loss:0.016, val_acc:0.992]
Epoch [112/120    avg_loss:0.030, val_acc:0.992]
Epoch [113/120    avg_loss:0.015, val_acc:0.992]
Epoch [114/120    avg_loss:0.019, val_acc:0.992]
Epoch [115/120    avg_loss:0.018, val_acc:0.992]
Epoch [116/120    avg_loss:0.022, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.992]
Epoch [118/120    avg_loss:0.018, val_acc:0.992]
Epoch [119/120    avg_loss:0.015, val_acc:0.992]
Epoch [120/120    avg_loss:0.020, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   2 215   6   0   0   0   3   4   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 128   3   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.97757848 0.96629213 0.93534483 0.90459364
 0.99277108 0.96132597 0.99614891 0.99574468 1.         1.
 0.99889746 1.        ]

Kappa:
0.9881291211439133
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4686f1fe48>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.589, val_acc:0.385]
Epoch [2/120    avg_loss:2.444, val_acc:0.367]
Epoch [3/120    avg_loss:2.296, val_acc:0.425]
Epoch [4/120    avg_loss:2.162, val_acc:0.480]
Epoch [5/120    avg_loss:2.005, val_acc:0.516]
Epoch [6/120    avg_loss:1.844, val_acc:0.655]
Epoch [7/120    avg_loss:1.604, val_acc:0.665]
Epoch [8/120    avg_loss:1.386, val_acc:0.748]
Epoch [9/120    avg_loss:1.195, val_acc:0.748]
Epoch [10/120    avg_loss:1.030, val_acc:0.804]
Epoch [11/120    avg_loss:0.860, val_acc:0.829]
Epoch [12/120    avg_loss:0.745, val_acc:0.873]
Epoch [13/120    avg_loss:0.711, val_acc:0.806]
Epoch [14/120    avg_loss:0.669, val_acc:0.881]
Epoch [15/120    avg_loss:0.612, val_acc:0.903]
Epoch [16/120    avg_loss:0.529, val_acc:0.863]
Epoch [17/120    avg_loss:0.444, val_acc:0.905]
Epoch [18/120    avg_loss:0.457, val_acc:0.913]
Epoch [19/120    avg_loss:0.478, val_acc:0.907]
Epoch [20/120    avg_loss:0.453, val_acc:0.889]
Epoch [21/120    avg_loss:0.422, val_acc:0.933]
Epoch [22/120    avg_loss:0.362, val_acc:0.875]
Epoch [23/120    avg_loss:0.377, val_acc:0.933]
Epoch [24/120    avg_loss:0.328, val_acc:0.938]
Epoch [25/120    avg_loss:0.302, val_acc:0.933]
Epoch [26/120    avg_loss:0.322, val_acc:0.942]
Epoch [27/120    avg_loss:0.287, val_acc:0.933]
Epoch [28/120    avg_loss:0.318, val_acc:0.964]
Epoch [29/120    avg_loss:0.299, val_acc:0.893]
Epoch [30/120    avg_loss:0.283, val_acc:0.944]
Epoch [31/120    avg_loss:0.257, val_acc:0.925]
Epoch [32/120    avg_loss:0.250, val_acc:0.938]
Epoch [33/120    avg_loss:0.214, val_acc:0.960]
Epoch [34/120    avg_loss:0.204, val_acc:0.964]
Epoch [35/120    avg_loss:0.194, val_acc:0.909]
Epoch [36/120    avg_loss:0.232, val_acc:0.897]
Epoch [37/120    avg_loss:0.205, val_acc:0.964]
Epoch [38/120    avg_loss:0.238, val_acc:0.940]
Epoch [39/120    avg_loss:0.224, val_acc:0.946]
Epoch [40/120    avg_loss:0.182, val_acc:0.964]
Epoch [41/120    avg_loss:0.241, val_acc:0.952]
Epoch [42/120    avg_loss:0.209, val_acc:0.962]
Epoch [43/120    avg_loss:0.206, val_acc:0.964]
Epoch [44/120    avg_loss:0.139, val_acc:0.956]
Epoch [45/120    avg_loss:0.204, val_acc:0.935]
Epoch [46/120    avg_loss:0.165, val_acc:0.968]
Epoch [47/120    avg_loss:0.141, val_acc:0.968]
Epoch [48/120    avg_loss:0.125, val_acc:0.972]
Epoch [49/120    avg_loss:0.135, val_acc:0.968]
Epoch [50/120    avg_loss:0.110, val_acc:0.956]
Epoch [51/120    avg_loss:0.158, val_acc:0.968]
Epoch [52/120    avg_loss:0.111, val_acc:0.944]
Epoch [53/120    avg_loss:0.134, val_acc:0.964]
Epoch [54/120    avg_loss:0.096, val_acc:0.966]
Epoch [55/120    avg_loss:0.117, val_acc:0.976]
Epoch [56/120    avg_loss:0.084, val_acc:0.978]
Epoch [57/120    avg_loss:0.089, val_acc:0.976]
Epoch [58/120    avg_loss:0.128, val_acc:0.964]
Epoch [59/120    avg_loss:0.089, val_acc:0.982]
Epoch [60/120    avg_loss:0.081, val_acc:0.984]
Epoch [61/120    avg_loss:0.066, val_acc:0.986]
Epoch [62/120    avg_loss:0.060, val_acc:0.982]
Epoch [63/120    avg_loss:0.052, val_acc:0.984]
Epoch [64/120    avg_loss:0.077, val_acc:0.982]
Epoch [65/120    avg_loss:0.099, val_acc:0.982]
Epoch [66/120    avg_loss:0.054, val_acc:0.980]
Epoch [67/120    avg_loss:0.044, val_acc:0.988]
Epoch [68/120    avg_loss:0.066, val_acc:0.978]
Epoch [69/120    avg_loss:0.064, val_acc:0.984]
Epoch [70/120    avg_loss:0.036, val_acc:0.980]
Epoch [71/120    avg_loss:0.047, val_acc:0.986]
Epoch [72/120    avg_loss:0.036, val_acc:0.992]
Epoch [73/120    avg_loss:0.040, val_acc:0.980]
Epoch [74/120    avg_loss:0.055, val_acc:0.984]
Epoch [75/120    avg_loss:0.052, val_acc:0.986]
Epoch [76/120    avg_loss:0.047, val_acc:0.996]
Epoch [77/120    avg_loss:0.053, val_acc:0.978]
Epoch [78/120    avg_loss:0.057, val_acc:0.982]
Epoch [79/120    avg_loss:0.037, val_acc:0.990]
Epoch [80/120    avg_loss:0.027, val_acc:0.994]
Epoch [81/120    avg_loss:0.035, val_acc:0.992]
Epoch [82/120    avg_loss:0.024, val_acc:0.994]
Epoch [83/120    avg_loss:0.022, val_acc:0.994]
Epoch [84/120    avg_loss:0.065, val_acc:0.992]
Epoch [85/120    avg_loss:0.027, val_acc:0.988]
Epoch [86/120    avg_loss:0.017, val_acc:0.996]
Epoch [87/120    avg_loss:0.047, val_acc:0.978]
Epoch [88/120    avg_loss:0.057, val_acc:0.992]
Epoch [89/120    avg_loss:0.020, val_acc:0.992]
Epoch [90/120    avg_loss:0.017, val_acc:0.992]
Epoch [91/120    avg_loss:0.026, val_acc:0.988]
Epoch [92/120    avg_loss:0.021, val_acc:0.992]
Epoch [93/120    avg_loss:0.025, val_acc:0.990]
Epoch [94/120    avg_loss:0.016, val_acc:0.990]
Epoch [95/120    avg_loss:0.026, val_acc:0.996]
Epoch [96/120    avg_loss:0.016, val_acc:0.994]
Epoch [97/120    avg_loss:0.023, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.994]
Epoch [99/120    avg_loss:0.011, val_acc:0.994]
Epoch [100/120    avg_loss:0.011, val_acc:0.994]
Epoch [101/120    avg_loss:0.009, val_acc:0.994]
Epoch [102/120    avg_loss:0.014, val_acc:0.994]
Epoch [103/120    avg_loss:0.009, val_acc:0.996]
Epoch [104/120    avg_loss:0.008, val_acc:0.996]
Epoch [105/120    avg_loss:0.010, val_acc:0.994]
Epoch [106/120    avg_loss:0.009, val_acc:0.996]
Epoch [107/120    avg_loss:0.009, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.012, val_acc:0.996]
Epoch [110/120    avg_loss:0.011, val_acc:0.998]
Epoch [111/120    avg_loss:0.008, val_acc:0.994]
Epoch [112/120    avg_loss:0.007, val_acc:0.996]
Epoch [113/120    avg_loss:0.009, val_acc:0.996]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.996]
Epoch [116/120    avg_loss:0.009, val_acc:0.998]
Epoch [117/120    avg_loss:0.006, val_acc:0.996]
Epoch [118/120    avg_loss:0.007, val_acc:0.996]
Epoch [119/120    avg_loss:0.007, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   1   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 1.         0.98426966 0.99343545 0.97368421 0.96193772
 1.         0.96132597 1.         0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.995014628615394
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8015a62dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.560, val_acc:0.339]
Epoch [2/120    avg_loss:2.421, val_acc:0.381]
Epoch [3/120    avg_loss:2.286, val_acc:0.393]
Epoch [4/120    avg_loss:2.160, val_acc:0.470]
Epoch [5/120    avg_loss:1.993, val_acc:0.520]
Epoch [6/120    avg_loss:1.819, val_acc:0.585]
Epoch [7/120    avg_loss:1.614, val_acc:0.690]
Epoch [8/120    avg_loss:1.447, val_acc:0.651]
Epoch [9/120    avg_loss:1.267, val_acc:0.714]
Epoch [10/120    avg_loss:1.128, val_acc:0.714]
Epoch [11/120    avg_loss:0.982, val_acc:0.798]
Epoch [12/120    avg_loss:0.892, val_acc:0.790]
Epoch [13/120    avg_loss:0.798, val_acc:0.821]
Epoch [14/120    avg_loss:0.756, val_acc:0.861]
Epoch [15/120    avg_loss:0.705, val_acc:0.839]
Epoch [16/120    avg_loss:0.670, val_acc:0.861]
Epoch [17/120    avg_loss:0.560, val_acc:0.853]
Epoch [18/120    avg_loss:0.547, val_acc:0.847]
Epoch [19/120    avg_loss:0.573, val_acc:0.903]
Epoch [20/120    avg_loss:0.467, val_acc:0.873]
Epoch [21/120    avg_loss:0.567, val_acc:0.899]
Epoch [22/120    avg_loss:0.479, val_acc:0.869]
Epoch [23/120    avg_loss:0.383, val_acc:0.895]
Epoch [24/120    avg_loss:0.395, val_acc:0.935]
Epoch [25/120    avg_loss:0.384, val_acc:0.911]
Epoch [26/120    avg_loss:0.361, val_acc:0.944]
Epoch [27/120    avg_loss:0.342, val_acc:0.903]
Epoch [28/120    avg_loss:0.312, val_acc:0.897]
Epoch [29/120    avg_loss:0.297, val_acc:0.931]
Epoch [30/120    avg_loss:0.289, val_acc:0.935]
Epoch [31/120    avg_loss:0.299, val_acc:0.905]
Epoch [32/120    avg_loss:0.308, val_acc:0.942]
Epoch [33/120    avg_loss:0.249, val_acc:0.909]
Epoch [34/120    avg_loss:0.270, val_acc:0.931]
Epoch [35/120    avg_loss:0.243, val_acc:0.935]
Epoch [36/120    avg_loss:0.250, val_acc:0.948]
Epoch [37/120    avg_loss:0.228, val_acc:0.956]
Epoch [38/120    avg_loss:0.209, val_acc:0.942]
Epoch [39/120    avg_loss:0.199, val_acc:0.933]
Epoch [40/120    avg_loss:0.237, val_acc:0.933]
Epoch [41/120    avg_loss:0.226, val_acc:0.942]
Epoch [42/120    avg_loss:0.183, val_acc:0.950]
Epoch [43/120    avg_loss:0.214, val_acc:0.954]
Epoch [44/120    avg_loss:0.251, val_acc:0.897]
Epoch [45/120    avg_loss:0.178, val_acc:0.940]
Epoch [46/120    avg_loss:0.239, val_acc:0.956]
Epoch [47/120    avg_loss:0.218, val_acc:0.919]
Epoch [48/120    avg_loss:0.171, val_acc:0.962]
Epoch [49/120    avg_loss:0.145, val_acc:0.970]
Epoch [50/120    avg_loss:0.152, val_acc:0.964]
Epoch [51/120    avg_loss:0.138, val_acc:0.958]
Epoch [52/120    avg_loss:0.138, val_acc:0.970]
Epoch [53/120    avg_loss:0.163, val_acc:0.960]
Epoch [54/120    avg_loss:0.103, val_acc:0.974]
Epoch [55/120    avg_loss:0.108, val_acc:0.982]
Epoch [56/120    avg_loss:0.114, val_acc:0.938]
Epoch [57/120    avg_loss:0.126, val_acc:0.980]
Epoch [58/120    avg_loss:0.088, val_acc:0.972]
Epoch [59/120    avg_loss:0.125, val_acc:0.958]
Epoch [60/120    avg_loss:0.127, val_acc:0.958]
Epoch [61/120    avg_loss:0.090, val_acc:0.964]
Epoch [62/120    avg_loss:0.148, val_acc:0.944]
Epoch [63/120    avg_loss:0.110, val_acc:0.964]
Epoch [64/120    avg_loss:0.144, val_acc:0.962]
Epoch [65/120    avg_loss:0.093, val_acc:0.976]
Epoch [66/120    avg_loss:0.094, val_acc:0.968]
Epoch [67/120    avg_loss:0.089, val_acc:0.974]
Epoch [68/120    avg_loss:0.072, val_acc:0.974]
Epoch [69/120    avg_loss:0.057, val_acc:0.978]
Epoch [70/120    avg_loss:0.053, val_acc:0.980]
Epoch [71/120    avg_loss:0.059, val_acc:0.982]
Epoch [72/120    avg_loss:0.048, val_acc:0.982]
Epoch [73/120    avg_loss:0.050, val_acc:0.982]
Epoch [74/120    avg_loss:0.050, val_acc:0.982]
Epoch [75/120    avg_loss:0.053, val_acc:0.980]
Epoch [76/120    avg_loss:0.051, val_acc:0.980]
Epoch [77/120    avg_loss:0.047, val_acc:0.982]
Epoch [78/120    avg_loss:0.044, val_acc:0.982]
Epoch [79/120    avg_loss:0.051, val_acc:0.980]
Epoch [80/120    avg_loss:0.043, val_acc:0.980]
Epoch [81/120    avg_loss:0.044, val_acc:0.982]
Epoch [82/120    avg_loss:0.043, val_acc:0.982]
Epoch [83/120    avg_loss:0.045, val_acc:0.982]
Epoch [84/120    avg_loss:0.041, val_acc:0.980]
Epoch [85/120    avg_loss:0.044, val_acc:0.982]
Epoch [86/120    avg_loss:0.040, val_acc:0.980]
Epoch [87/120    avg_loss:0.046, val_acc:0.976]
Epoch [88/120    avg_loss:0.044, val_acc:0.982]
Epoch [89/120    avg_loss:0.039, val_acc:0.982]
Epoch [90/120    avg_loss:0.037, val_acc:0.980]
Epoch [91/120    avg_loss:0.040, val_acc:0.980]
Epoch [92/120    avg_loss:0.042, val_acc:0.982]
Epoch [93/120    avg_loss:0.037, val_acc:0.982]
Epoch [94/120    avg_loss:0.036, val_acc:0.982]
Epoch [95/120    avg_loss:0.036, val_acc:0.982]
Epoch [96/120    avg_loss:0.037, val_acc:0.982]
Epoch [97/120    avg_loss:0.036, val_acc:0.978]
Epoch [98/120    avg_loss:0.039, val_acc:0.980]
Epoch [99/120    avg_loss:0.042, val_acc:0.978]
Epoch [100/120    avg_loss:0.042, val_acc:0.982]
Epoch [101/120    avg_loss:0.044, val_acc:0.980]
Epoch [102/120    avg_loss:0.040, val_acc:0.980]
Epoch [103/120    avg_loss:0.032, val_acc:0.978]
Epoch [104/120    avg_loss:0.037, val_acc:0.980]
Epoch [105/120    avg_loss:0.041, val_acc:0.976]
Epoch [106/120    avg_loss:0.035, val_acc:0.978]
Epoch [107/120    avg_loss:0.036, val_acc:0.980]
Epoch [108/120    avg_loss:0.034, val_acc:0.982]
Epoch [109/120    avg_loss:0.037, val_acc:0.976]
Epoch [110/120    avg_loss:0.032, val_acc:0.980]
Epoch [111/120    avg_loss:0.036, val_acc:0.978]
Epoch [112/120    avg_loss:0.033, val_acc:0.978]
Epoch [113/120    avg_loss:0.035, val_acc:0.980]
Epoch [114/120    avg_loss:0.037, val_acc:0.980]
Epoch [115/120    avg_loss:0.036, val_acc:0.980]
Epoch [116/120    avg_loss:0.038, val_acc:0.980]
Epoch [117/120    avg_loss:0.033, val_acc:0.980]
Epoch [118/120    avg_loss:0.037, val_acc:0.980]
Epoch [119/120    avg_loss:0.032, val_acc:0.980]
Epoch [120/120    avg_loss:0.034, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 227   0   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 208  17   0   0   0   0   0   0   2   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.9753915  0.99343545 0.93693694 0.91275168
 1.         0.93854749 0.99614891 1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9900292748952398
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb52e601dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.577, val_acc:0.306]
Epoch [2/120    avg_loss:2.466, val_acc:0.347]
Epoch [3/120    avg_loss:2.343, val_acc:0.381]
Epoch [4/120    avg_loss:2.233, val_acc:0.484]
Epoch [5/120    avg_loss:2.110, val_acc:0.550]
Epoch [6/120    avg_loss:1.961, val_acc:0.615]
Epoch [7/120    avg_loss:1.789, val_acc:0.659]
Epoch [8/120    avg_loss:1.589, val_acc:0.649]
Epoch [9/120    avg_loss:1.407, val_acc:0.736]
Epoch [10/120    avg_loss:1.222, val_acc:0.762]
Epoch [11/120    avg_loss:1.110, val_acc:0.788]
Epoch [12/120    avg_loss:0.930, val_acc:0.839]
Epoch [13/120    avg_loss:0.876, val_acc:0.776]
Epoch [14/120    avg_loss:0.730, val_acc:0.857]
Epoch [15/120    avg_loss:0.702, val_acc:0.881]
Epoch [16/120    avg_loss:0.605, val_acc:0.863]
Epoch [17/120    avg_loss:0.608, val_acc:0.881]
Epoch [18/120    avg_loss:0.538, val_acc:0.899]
Epoch [19/120    avg_loss:0.495, val_acc:0.919]
Epoch [20/120    avg_loss:0.426, val_acc:0.909]
Epoch [21/120    avg_loss:0.445, val_acc:0.921]
Epoch [22/120    avg_loss:0.516, val_acc:0.897]
Epoch [23/120    avg_loss:0.434, val_acc:0.865]
Epoch [24/120    avg_loss:0.434, val_acc:0.935]
Epoch [25/120    avg_loss:0.380, val_acc:0.942]
Epoch [26/120    avg_loss:0.428, val_acc:0.923]
Epoch [27/120    avg_loss:0.341, val_acc:0.885]
Epoch [28/120    avg_loss:0.355, val_acc:0.937]
Epoch [29/120    avg_loss:0.376, val_acc:0.938]
Epoch [30/120    avg_loss:0.338, val_acc:0.933]
Epoch [31/120    avg_loss:0.321, val_acc:0.944]
Epoch [32/120    avg_loss:0.327, val_acc:0.944]
Epoch [33/120    avg_loss:0.249, val_acc:0.946]
Epoch [34/120    avg_loss:0.273, val_acc:0.942]
Epoch [35/120    avg_loss:0.232, val_acc:0.952]
Epoch [36/120    avg_loss:0.197, val_acc:0.952]
Epoch [37/120    avg_loss:0.217, val_acc:0.942]
Epoch [38/120    avg_loss:0.273, val_acc:0.937]
Epoch [39/120    avg_loss:0.204, val_acc:0.968]
Epoch [40/120    avg_loss:0.185, val_acc:0.968]
Epoch [41/120    avg_loss:0.172, val_acc:0.972]
Epoch [42/120    avg_loss:0.210, val_acc:0.962]
Epoch [43/120    avg_loss:0.197, val_acc:0.954]
Epoch [44/120    avg_loss:0.193, val_acc:0.966]
Epoch [45/120    avg_loss:0.190, val_acc:0.968]
Epoch [46/120    avg_loss:0.171, val_acc:0.962]
Epoch [47/120    avg_loss:0.160, val_acc:0.970]
Epoch [48/120    avg_loss:0.137, val_acc:0.974]
Epoch [49/120    avg_loss:0.175, val_acc:0.980]
Epoch [50/120    avg_loss:0.122, val_acc:0.990]
Epoch [51/120    avg_loss:0.175, val_acc:0.942]
Epoch [52/120    avg_loss:0.133, val_acc:0.984]
Epoch [53/120    avg_loss:0.138, val_acc:0.972]
Epoch [54/120    avg_loss:0.137, val_acc:0.966]
Epoch [55/120    avg_loss:0.113, val_acc:0.978]
Epoch [56/120    avg_loss:0.188, val_acc:0.958]
Epoch [57/120    avg_loss:0.118, val_acc:0.978]
Epoch [58/120    avg_loss:0.131, val_acc:0.958]
Epoch [59/120    avg_loss:0.131, val_acc:0.978]
Epoch [60/120    avg_loss:0.068, val_acc:0.972]
Epoch [61/120    avg_loss:0.091, val_acc:0.982]
Epoch [62/120    avg_loss:0.068, val_acc:0.992]
Epoch [63/120    avg_loss:0.066, val_acc:0.982]
Epoch [64/120    avg_loss:0.062, val_acc:0.990]
Epoch [65/120    avg_loss:0.075, val_acc:0.974]
Epoch [66/120    avg_loss:0.075, val_acc:0.988]
Epoch [67/120    avg_loss:0.068, val_acc:0.986]
Epoch [68/120    avg_loss:0.045, val_acc:0.988]
Epoch [69/120    avg_loss:0.051, val_acc:0.986]
Epoch [70/120    avg_loss:0.049, val_acc:0.992]
Epoch [71/120    avg_loss:0.037, val_acc:0.988]
Epoch [72/120    avg_loss:0.031, val_acc:0.990]
Epoch [73/120    avg_loss:0.021, val_acc:0.986]
Epoch [74/120    avg_loss:0.082, val_acc:0.968]
Epoch [75/120    avg_loss:0.074, val_acc:0.980]
Epoch [76/120    avg_loss:0.044, val_acc:0.988]
Epoch [77/120    avg_loss:0.033, val_acc:0.988]
Epoch [78/120    avg_loss:0.082, val_acc:0.988]
Epoch [79/120    avg_loss:0.053, val_acc:0.992]
Epoch [80/120    avg_loss:0.038, val_acc:0.990]
Epoch [81/120    avg_loss:0.025, val_acc:0.994]
Epoch [82/120    avg_loss:0.032, val_acc:0.980]
Epoch [83/120    avg_loss:0.063, val_acc:0.940]
Epoch [84/120    avg_loss:0.082, val_acc:0.972]
Epoch [85/120    avg_loss:0.067, val_acc:0.980]
Epoch [86/120    avg_loss:0.054, val_acc:0.988]
Epoch [87/120    avg_loss:0.045, val_acc:0.988]
Epoch [88/120    avg_loss:0.044, val_acc:0.992]
Epoch [89/120    avg_loss:0.032, val_acc:0.994]
Epoch [90/120    avg_loss:0.031, val_acc:0.996]
Epoch [91/120    avg_loss:0.016, val_acc:0.996]
Epoch [92/120    avg_loss:0.021, val_acc:0.992]
Epoch [93/120    avg_loss:0.042, val_acc:0.992]
Epoch [94/120    avg_loss:0.026, val_acc:0.986]
Epoch [95/120    avg_loss:0.020, val_acc:0.996]
Epoch [96/120    avg_loss:0.019, val_acc:0.990]
Epoch [97/120    avg_loss:0.113, val_acc:0.974]
Epoch [98/120    avg_loss:0.056, val_acc:0.980]
Epoch [99/120    avg_loss:0.039, val_acc:0.982]
Epoch [100/120    avg_loss:0.047, val_acc:0.994]
Epoch [101/120    avg_loss:0.019, val_acc:0.980]
Epoch [102/120    avg_loss:0.037, val_acc:0.974]
Epoch [103/120    avg_loss:0.025, val_acc:0.996]
Epoch [104/120    avg_loss:0.013, val_acc:0.992]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.996]
Epoch [107/120    avg_loss:0.011, val_acc:0.992]
Epoch [108/120    avg_loss:0.012, val_acc:0.994]
Epoch [109/120    avg_loss:0.010, val_acc:0.994]
Epoch [110/120    avg_loss:0.012, val_acc:0.992]
Epoch [111/120    avg_loss:0.008, val_acc:0.994]
Epoch [112/120    avg_loss:0.008, val_acc:0.994]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.996]
Epoch [116/120    avg_loss:0.007, val_acc:0.996]
Epoch [117/120    avg_loss:0.009, val_acc:0.994]
Epoch [118/120    avg_loss:0.008, val_acc:0.996]
Epoch [119/120    avg_loss:0.008, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   5   0   0   0   1   3   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.98206278 0.98004435 0.95111111 0.94314381
 1.         0.95555556 0.998713   0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9919285733917976
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d45adfdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.569, val_acc:0.306]
Epoch [2/120    avg_loss:2.439, val_acc:0.313]
Epoch [3/120    avg_loss:2.332, val_acc:0.351]
Epoch [4/120    avg_loss:2.234, val_acc:0.442]
Epoch [5/120    avg_loss:2.111, val_acc:0.526]
Epoch [6/120    avg_loss:1.960, val_acc:0.563]
Epoch [7/120    avg_loss:1.809, val_acc:0.623]
Epoch [8/120    avg_loss:1.619, val_acc:0.647]
Epoch [9/120    avg_loss:1.406, val_acc:0.720]
Epoch [10/120    avg_loss:1.227, val_acc:0.812]
Epoch [11/120    avg_loss:1.066, val_acc:0.817]
Epoch [12/120    avg_loss:0.937, val_acc:0.829]
Epoch [13/120    avg_loss:0.786, val_acc:0.823]
Epoch [14/120    avg_loss:0.687, val_acc:0.853]
Epoch [15/120    avg_loss:0.808, val_acc:0.831]
Epoch [16/120    avg_loss:0.655, val_acc:0.861]
Epoch [17/120    avg_loss:0.581, val_acc:0.879]
Epoch [18/120    avg_loss:0.520, val_acc:0.921]
Epoch [19/120    avg_loss:0.508, val_acc:0.903]
Epoch [20/120    avg_loss:0.517, val_acc:0.895]
Epoch [21/120    avg_loss:0.441, val_acc:0.919]
Epoch [22/120    avg_loss:0.486, val_acc:0.911]
Epoch [23/120    avg_loss:0.476, val_acc:0.921]
Epoch [24/120    avg_loss:0.411, val_acc:0.905]
Epoch [25/120    avg_loss:0.366, val_acc:0.921]
Epoch [26/120    avg_loss:0.353, val_acc:0.946]
Epoch [27/120    avg_loss:0.292, val_acc:0.956]
Epoch [28/120    avg_loss:0.325, val_acc:0.938]
Epoch [29/120    avg_loss:0.297, val_acc:0.937]
Epoch [30/120    avg_loss:0.241, val_acc:0.925]
Epoch [31/120    avg_loss:0.353, val_acc:0.942]
Epoch [32/120    avg_loss:0.254, val_acc:0.958]
Epoch [33/120    avg_loss:0.292, val_acc:0.931]
Epoch [34/120    avg_loss:0.240, val_acc:0.935]
Epoch [35/120    avg_loss:0.233, val_acc:0.929]
Epoch [36/120    avg_loss:0.242, val_acc:0.960]
Epoch [37/120    avg_loss:0.192, val_acc:0.843]
Epoch [38/120    avg_loss:0.240, val_acc:0.952]
Epoch [39/120    avg_loss:0.182, val_acc:0.952]
Epoch [40/120    avg_loss:0.163, val_acc:0.972]
Epoch [41/120    avg_loss:0.141, val_acc:0.958]
Epoch [42/120    avg_loss:0.177, val_acc:0.948]
Epoch [43/120    avg_loss:0.136, val_acc:0.893]
Epoch [44/120    avg_loss:0.163, val_acc:0.962]
Epoch [45/120    avg_loss:0.144, val_acc:0.976]
Epoch [46/120    avg_loss:0.118, val_acc:0.976]
Epoch [47/120    avg_loss:0.130, val_acc:0.966]
Epoch [48/120    avg_loss:0.162, val_acc:0.970]
Epoch [49/120    avg_loss:0.149, val_acc:0.962]
Epoch [50/120    avg_loss:0.108, val_acc:0.976]
Epoch [51/120    avg_loss:0.124, val_acc:0.970]
Epoch [52/120    avg_loss:0.084, val_acc:0.972]
Epoch [53/120    avg_loss:0.140, val_acc:0.976]
Epoch [54/120    avg_loss:0.095, val_acc:0.966]
Epoch [55/120    avg_loss:0.116, val_acc:0.978]
Epoch [56/120    avg_loss:0.108, val_acc:0.984]
Epoch [57/120    avg_loss:0.092, val_acc:0.978]
Epoch [58/120    avg_loss:0.117, val_acc:0.976]
Epoch [59/120    avg_loss:0.066, val_acc:0.984]
Epoch [60/120    avg_loss:0.075, val_acc:0.986]
Epoch [61/120    avg_loss:0.092, val_acc:0.972]
Epoch [62/120    avg_loss:0.080, val_acc:0.988]
Epoch [63/120    avg_loss:0.110, val_acc:0.974]
Epoch [64/120    avg_loss:0.056, val_acc:0.942]
Epoch [65/120    avg_loss:0.100, val_acc:0.966]
Epoch [66/120    avg_loss:0.082, val_acc:0.988]
Epoch [67/120    avg_loss:0.067, val_acc:0.966]
Epoch [68/120    avg_loss:0.049, val_acc:0.988]
Epoch [69/120    avg_loss:0.060, val_acc:0.992]
Epoch [70/120    avg_loss:0.046, val_acc:0.990]
Epoch [71/120    avg_loss:0.071, val_acc:0.962]
Epoch [72/120    avg_loss:0.055, val_acc:0.972]
Epoch [73/120    avg_loss:0.053, val_acc:0.984]
Epoch [74/120    avg_loss:0.035, val_acc:0.984]
Epoch [75/120    avg_loss:0.056, val_acc:0.980]
Epoch [76/120    avg_loss:0.035, val_acc:0.986]
Epoch [77/120    avg_loss:0.045, val_acc:0.990]
Epoch [78/120    avg_loss:0.031, val_acc:0.984]
Epoch [79/120    avg_loss:0.020, val_acc:0.988]
Epoch [80/120    avg_loss:0.027, val_acc:0.988]
Epoch [81/120    avg_loss:0.026, val_acc:0.990]
Epoch [82/120    avg_loss:0.019, val_acc:0.992]
Epoch [83/120    avg_loss:0.025, val_acc:0.988]
Epoch [84/120    avg_loss:0.063, val_acc:0.950]
Epoch [85/120    avg_loss:0.074, val_acc:0.978]
Epoch [86/120    avg_loss:0.046, val_acc:0.976]
Epoch [87/120    avg_loss:0.025, val_acc:0.988]
Epoch [88/120    avg_loss:0.022, val_acc:0.986]
Epoch [89/120    avg_loss:0.026, val_acc:0.986]
Epoch [90/120    avg_loss:0.025, val_acc:0.988]
Epoch [91/120    avg_loss:0.051, val_acc:0.978]
Epoch [92/120    avg_loss:0.029, val_acc:0.990]
Epoch [93/120    avg_loss:0.022, val_acc:0.988]
Epoch [94/120    avg_loss:0.024, val_acc:0.990]
Epoch [95/120    avg_loss:0.026, val_acc:0.988]
Epoch [96/120    avg_loss:0.018, val_acc:0.986]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.012, val_acc:0.988]
Epoch [99/120    avg_loss:0.014, val_acc:0.988]
Epoch [100/120    avg_loss:0.015, val_acc:0.988]
Epoch [101/120    avg_loss:0.011, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.014, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.990]
Epoch [107/120    avg_loss:0.011, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.990]
Epoch [110/120    avg_loss:0.013, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.010, val_acc:0.990]
Epoch [114/120    avg_loss:0.010, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.011, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 223   5   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 211  12   0   0   0   0   0   0   4   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.98419865 0.98454746 0.9154013  0.8943662
 1.         0.96174863 0.99742931 1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9886043508782729
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d9d684e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.581, val_acc:0.319]
Epoch [2/120    avg_loss:2.458, val_acc:0.375]
Epoch [3/120    avg_loss:2.334, val_acc:0.363]
Epoch [4/120    avg_loss:2.212, val_acc:0.431]
Epoch [5/120    avg_loss:2.107, val_acc:0.540]
Epoch [6/120    avg_loss:1.956, val_acc:0.593]
Epoch [7/120    avg_loss:1.819, val_acc:0.577]
Epoch [8/120    avg_loss:1.616, val_acc:0.611]
Epoch [9/120    avg_loss:1.427, val_acc:0.700]
Epoch [10/120    avg_loss:1.298, val_acc:0.730]
Epoch [11/120    avg_loss:1.100, val_acc:0.800]
Epoch [12/120    avg_loss:0.937, val_acc:0.806]
Epoch [13/120    avg_loss:0.850, val_acc:0.823]
Epoch [14/120    avg_loss:0.809, val_acc:0.849]
Epoch [15/120    avg_loss:0.745, val_acc:0.847]
Epoch [16/120    avg_loss:0.711, val_acc:0.823]
Epoch [17/120    avg_loss:0.687, val_acc:0.877]
Epoch [18/120    avg_loss:0.601, val_acc:0.883]
Epoch [19/120    avg_loss:0.577, val_acc:0.819]
Epoch [20/120    avg_loss:0.588, val_acc:0.853]
Epoch [21/120    avg_loss:0.509, val_acc:0.901]
Epoch [22/120    avg_loss:0.415, val_acc:0.915]
Epoch [23/120    avg_loss:0.410, val_acc:0.915]
Epoch [24/120    avg_loss:0.359, val_acc:0.909]
Epoch [25/120    avg_loss:0.462, val_acc:0.917]
Epoch [26/120    avg_loss:0.402, val_acc:0.909]
Epoch [27/120    avg_loss:0.362, val_acc:0.873]
Epoch [28/120    avg_loss:0.332, val_acc:0.935]
Epoch [29/120    avg_loss:0.299, val_acc:0.938]
Epoch [30/120    avg_loss:0.311, val_acc:0.917]
Epoch [31/120    avg_loss:0.426, val_acc:0.865]
Epoch [32/120    avg_loss:0.330, val_acc:0.935]
Epoch [33/120    avg_loss:0.279, val_acc:0.940]
Epoch [34/120    avg_loss:0.287, val_acc:0.944]
Epoch [35/120    avg_loss:0.246, val_acc:0.929]
Epoch [36/120    avg_loss:0.206, val_acc:0.946]
Epoch [37/120    avg_loss:0.204, val_acc:0.923]
Epoch [38/120    avg_loss:0.195, val_acc:0.956]
Epoch [39/120    avg_loss:0.182, val_acc:0.954]
Epoch [40/120    avg_loss:0.194, val_acc:0.968]
Epoch [41/120    avg_loss:0.183, val_acc:0.960]
Epoch [42/120    avg_loss:0.142, val_acc:0.946]
Epoch [43/120    avg_loss:0.153, val_acc:0.960]
Epoch [44/120    avg_loss:0.147, val_acc:0.974]
Epoch [45/120    avg_loss:0.132, val_acc:0.974]
Epoch [46/120    avg_loss:0.207, val_acc:0.917]
Epoch [47/120    avg_loss:0.236, val_acc:0.956]
Epoch [48/120    avg_loss:0.151, val_acc:0.944]
Epoch [49/120    avg_loss:0.132, val_acc:0.970]
Epoch [50/120    avg_loss:0.113, val_acc:0.950]
Epoch [51/120    avg_loss:0.161, val_acc:0.968]
Epoch [52/120    avg_loss:0.204, val_acc:0.966]
Epoch [53/120    avg_loss:0.200, val_acc:0.946]
Epoch [54/120    avg_loss:0.115, val_acc:0.968]
Epoch [55/120    avg_loss:0.079, val_acc:0.970]
Epoch [56/120    avg_loss:0.131, val_acc:0.952]
Epoch [57/120    avg_loss:0.171, val_acc:0.962]
Epoch [58/120    avg_loss:0.155, val_acc:0.948]
Epoch [59/120    avg_loss:0.125, val_acc:0.972]
Epoch [60/120    avg_loss:0.080, val_acc:0.988]
Epoch [61/120    avg_loss:0.072, val_acc:0.992]
Epoch [62/120    avg_loss:0.067, val_acc:0.990]
Epoch [63/120    avg_loss:0.056, val_acc:0.990]
Epoch [64/120    avg_loss:0.061, val_acc:0.994]
Epoch [65/120    avg_loss:0.061, val_acc:0.990]
Epoch [66/120    avg_loss:0.061, val_acc:0.992]
Epoch [67/120    avg_loss:0.049, val_acc:0.992]
Epoch [68/120    avg_loss:0.046, val_acc:0.994]
Epoch [69/120    avg_loss:0.050, val_acc:0.994]
Epoch [70/120    avg_loss:0.061, val_acc:0.992]
Epoch [71/120    avg_loss:0.056, val_acc:0.992]
Epoch [72/120    avg_loss:0.049, val_acc:0.994]
Epoch [73/120    avg_loss:0.049, val_acc:0.994]
Epoch [74/120    avg_loss:0.047, val_acc:0.992]
Epoch [75/120    avg_loss:0.047, val_acc:0.990]
Epoch [76/120    avg_loss:0.057, val_acc:0.992]
Epoch [77/120    avg_loss:0.043, val_acc:0.992]
Epoch [78/120    avg_loss:0.048, val_acc:0.992]
Epoch [79/120    avg_loss:0.046, val_acc:0.992]
Epoch [80/120    avg_loss:0.039, val_acc:0.994]
Epoch [81/120    avg_loss:0.054, val_acc:0.994]
Epoch [82/120    avg_loss:0.040, val_acc:0.992]
Epoch [83/120    avg_loss:0.043, val_acc:0.994]
Epoch [84/120    avg_loss:0.040, val_acc:0.994]
Epoch [85/120    avg_loss:0.042, val_acc:0.994]
Epoch [86/120    avg_loss:0.034, val_acc:0.994]
Epoch [87/120    avg_loss:0.037, val_acc:0.992]
Epoch [88/120    avg_loss:0.042, val_acc:0.992]
Epoch [89/120    avg_loss:0.038, val_acc:0.994]
Epoch [90/120    avg_loss:0.040, val_acc:0.994]
Epoch [91/120    avg_loss:0.040, val_acc:0.994]
Epoch [92/120    avg_loss:0.037, val_acc:0.992]
Epoch [93/120    avg_loss:0.039, val_acc:0.994]
Epoch [94/120    avg_loss:0.032, val_acc:0.994]
Epoch [95/120    avg_loss:0.033, val_acc:0.994]
Epoch [96/120    avg_loss:0.039, val_acc:0.992]
Epoch [97/120    avg_loss:0.035, val_acc:0.990]
Epoch [98/120    avg_loss:0.034, val_acc:0.990]
Epoch [99/120    avg_loss:0.036, val_acc:0.992]
Epoch [100/120    avg_loss:0.031, val_acc:0.994]
Epoch [101/120    avg_loss:0.036, val_acc:0.990]
Epoch [102/120    avg_loss:0.036, val_acc:0.992]
Epoch [103/120    avg_loss:0.042, val_acc:0.992]
Epoch [104/120    avg_loss:0.039, val_acc:0.994]
Epoch [105/120    avg_loss:0.032, val_acc:0.994]
Epoch [106/120    avg_loss:0.037, val_acc:0.992]
Epoch [107/120    avg_loss:0.037, val_acc:0.992]
Epoch [108/120    avg_loss:0.051, val_acc:0.990]
Epoch [109/120    avg_loss:0.035, val_acc:0.990]
Epoch [110/120    avg_loss:0.033, val_acc:0.988]
Epoch [111/120    avg_loss:0.037, val_acc:0.990]
Epoch [112/120    avg_loss:0.031, val_acc:0.990]
Epoch [113/120    avg_loss:0.034, val_acc:0.990]
Epoch [114/120    avg_loss:0.030, val_acc:0.994]
Epoch [115/120    avg_loss:0.030, val_acc:0.994]
Epoch [116/120    avg_loss:0.029, val_acc:0.992]
Epoch [117/120    avg_loss:0.029, val_acc:0.994]
Epoch [118/120    avg_loss:0.030, val_acc:0.992]
Epoch [119/120    avg_loss:0.033, val_acc:0.992]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99854227 0.97767857 1.         0.96428571 0.94594595
 0.99512195 0.94382022 1.         1.         1.         0.9843342
 0.98657718 1.        ]

Kappa:
0.9905043553930251
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdca4c8ae10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.401]
Epoch [2/120    avg_loss:2.461, val_acc:0.415]
Epoch [3/120    avg_loss:2.334, val_acc:0.431]
Epoch [4/120    avg_loss:2.221, val_acc:0.492]
Epoch [5/120    avg_loss:2.081, val_acc:0.540]
Epoch [6/120    avg_loss:1.960, val_acc:0.609]
Epoch [7/120    avg_loss:1.801, val_acc:0.659]
Epoch [8/120    avg_loss:1.595, val_acc:0.696]
Epoch [9/120    avg_loss:1.458, val_acc:0.738]
Epoch [10/120    avg_loss:1.254, val_acc:0.778]
Epoch [11/120    avg_loss:1.079, val_acc:0.778]
Epoch [12/120    avg_loss:0.943, val_acc:0.859]
Epoch [13/120    avg_loss:0.818, val_acc:0.841]
Epoch [14/120    avg_loss:0.734, val_acc:0.837]
Epoch [15/120    avg_loss:0.668, val_acc:0.841]
Epoch [16/120    avg_loss:0.598, val_acc:0.873]
Epoch [17/120    avg_loss:0.582, val_acc:0.903]
Epoch [18/120    avg_loss:0.558, val_acc:0.917]
Epoch [19/120    avg_loss:0.521, val_acc:0.891]
Epoch [20/120    avg_loss:0.521, val_acc:0.907]
Epoch [21/120    avg_loss:0.452, val_acc:0.919]
Epoch [22/120    avg_loss:0.456, val_acc:0.907]
Epoch [23/120    avg_loss:0.403, val_acc:0.881]
Epoch [24/120    avg_loss:0.444, val_acc:0.929]
Epoch [25/120    avg_loss:0.431, val_acc:0.913]
Epoch [26/120    avg_loss:0.342, val_acc:0.927]
Epoch [27/120    avg_loss:0.328, val_acc:0.931]
Epoch [28/120    avg_loss:0.355, val_acc:0.889]
Epoch [29/120    avg_loss:0.366, val_acc:0.931]
Epoch [30/120    avg_loss:0.321, val_acc:0.952]
Epoch [31/120    avg_loss:0.282, val_acc:0.927]
Epoch [32/120    avg_loss:0.326, val_acc:0.937]
Epoch [33/120    avg_loss:0.278, val_acc:0.944]
Epoch [34/120    avg_loss:0.259, val_acc:0.942]
Epoch [35/120    avg_loss:0.228, val_acc:0.962]
Epoch [36/120    avg_loss:0.214, val_acc:0.956]
Epoch [37/120    avg_loss:0.207, val_acc:0.968]
Epoch [38/120    avg_loss:0.172, val_acc:0.974]
Epoch [39/120    avg_loss:0.200, val_acc:0.893]
Epoch [40/120    avg_loss:0.218, val_acc:0.950]
Epoch [41/120    avg_loss:0.203, val_acc:0.940]
Epoch [42/120    avg_loss:0.174, val_acc:0.925]
Epoch [43/120    avg_loss:0.239, val_acc:0.962]
Epoch [44/120    avg_loss:0.286, val_acc:0.966]
Epoch [45/120    avg_loss:0.165, val_acc:0.966]
Epoch [46/120    avg_loss:0.145, val_acc:0.968]
Epoch [47/120    avg_loss:0.162, val_acc:0.972]
Epoch [48/120    avg_loss:0.139, val_acc:0.972]
Epoch [49/120    avg_loss:0.150, val_acc:0.968]
Epoch [50/120    avg_loss:0.191, val_acc:0.964]
Epoch [51/120    avg_loss:0.193, val_acc:0.950]
Epoch [52/120    avg_loss:0.196, val_acc:0.974]
Epoch [53/120    avg_loss:0.112, val_acc:0.978]
Epoch [54/120    avg_loss:0.105, val_acc:0.986]
Epoch [55/120    avg_loss:0.100, val_acc:0.986]
Epoch [56/120    avg_loss:0.082, val_acc:0.986]
Epoch [57/120    avg_loss:0.088, val_acc:0.986]
Epoch [58/120    avg_loss:0.090, val_acc:0.988]
Epoch [59/120    avg_loss:0.077, val_acc:0.986]
Epoch [60/120    avg_loss:0.077, val_acc:0.988]
Epoch [61/120    avg_loss:0.076, val_acc:0.988]
Epoch [62/120    avg_loss:0.080, val_acc:0.990]
Epoch [63/120    avg_loss:0.083, val_acc:0.988]
Epoch [64/120    avg_loss:0.071, val_acc:0.988]
Epoch [65/120    avg_loss:0.074, val_acc:0.988]
Epoch [66/120    avg_loss:0.083, val_acc:0.992]
Epoch [67/120    avg_loss:0.073, val_acc:0.990]
Epoch [68/120    avg_loss:0.073, val_acc:0.990]
Epoch [69/120    avg_loss:0.071, val_acc:0.988]
Epoch [70/120    avg_loss:0.074, val_acc:0.990]
Epoch [71/120    avg_loss:0.073, val_acc:0.988]
Epoch [72/120    avg_loss:0.068, val_acc:0.988]
Epoch [73/120    avg_loss:0.074, val_acc:0.988]
Epoch [74/120    avg_loss:0.076, val_acc:0.990]
Epoch [75/120    avg_loss:0.061, val_acc:0.990]
Epoch [76/120    avg_loss:0.067, val_acc:0.988]
Epoch [77/120    avg_loss:0.069, val_acc:0.990]
Epoch [78/120    avg_loss:0.059, val_acc:0.990]
Epoch [79/120    avg_loss:0.063, val_acc:0.988]
Epoch [80/120    avg_loss:0.063, val_acc:0.988]
Epoch [81/120    avg_loss:0.058, val_acc:0.988]
Epoch [82/120    avg_loss:0.057, val_acc:0.988]
Epoch [83/120    avg_loss:0.066, val_acc:0.988]
Epoch [84/120    avg_loss:0.063, val_acc:0.988]
Epoch [85/120    avg_loss:0.058, val_acc:0.988]
Epoch [86/120    avg_loss:0.061, val_acc:0.988]
Epoch [87/120    avg_loss:0.064, val_acc:0.988]
Epoch [88/120    avg_loss:0.055, val_acc:0.988]
Epoch [89/120    avg_loss:0.061, val_acc:0.988]
Epoch [90/120    avg_loss:0.059, val_acc:0.990]
Epoch [91/120    avg_loss:0.064, val_acc:0.990]
Epoch [92/120    avg_loss:0.055, val_acc:0.990]
Epoch [93/120    avg_loss:0.054, val_acc:0.990]
Epoch [94/120    avg_loss:0.062, val_acc:0.990]
Epoch [95/120    avg_loss:0.056, val_acc:0.990]
Epoch [96/120    avg_loss:0.064, val_acc:0.990]
Epoch [97/120    avg_loss:0.060, val_acc:0.990]
Epoch [98/120    avg_loss:0.058, val_acc:0.990]
Epoch [99/120    avg_loss:0.059, val_acc:0.990]
Epoch [100/120    avg_loss:0.059, val_acc:0.990]
Epoch [101/120    avg_loss:0.066, val_acc:0.990]
Epoch [102/120    avg_loss:0.060, val_acc:0.990]
Epoch [103/120    avg_loss:0.058, val_acc:0.990]
Epoch [104/120    avg_loss:0.060, val_acc:0.990]
Epoch [105/120    avg_loss:0.060, val_acc:0.990]
Epoch [106/120    avg_loss:0.058, val_acc:0.990]
Epoch [107/120    avg_loss:0.060, val_acc:0.990]
Epoch [108/120    avg_loss:0.060, val_acc:0.990]
Epoch [109/120    avg_loss:0.059, val_acc:0.990]
Epoch [110/120    avg_loss:0.059, val_acc:0.990]
Epoch [111/120    avg_loss:0.067, val_acc:0.990]
Epoch [112/120    avg_loss:0.059, val_acc:0.990]
Epoch [113/120    avg_loss:0.061, val_acc:0.990]
Epoch [114/120    avg_loss:0.056, val_acc:0.990]
Epoch [115/120    avg_loss:0.059, val_acc:0.990]
Epoch [116/120    avg_loss:0.053, val_acc:0.990]
Epoch [117/120    avg_loss:0.057, val_acc:0.990]
Epoch [118/120    avg_loss:0.066, val_acc:0.990]
Epoch [119/120    avg_loss:0.057, val_acc:0.990]
Epoch [120/120    avg_loss:0.061, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 198  29   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.97117517 0.98454746 0.90410959 0.88817891
 0.99756691 0.92571429 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9867066519885105
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f09cfc00e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.587, val_acc:0.335]
Epoch [2/120    avg_loss:2.470, val_acc:0.423]
Epoch [3/120    avg_loss:2.350, val_acc:0.480]
Epoch [4/120    avg_loss:2.211, val_acc:0.532]
Epoch [5/120    avg_loss:2.041, val_acc:0.558]
Epoch [6/120    avg_loss:1.874, val_acc:0.565]
Epoch [7/120    avg_loss:1.707, val_acc:0.643]
Epoch [8/120    avg_loss:1.479, val_acc:0.653]
Epoch [9/120    avg_loss:1.300, val_acc:0.728]
Epoch [10/120    avg_loss:1.098, val_acc:0.750]
Epoch [11/120    avg_loss:0.991, val_acc:0.698]
Epoch [12/120    avg_loss:0.914, val_acc:0.752]
Epoch [13/120    avg_loss:0.858, val_acc:0.819]
Epoch [14/120    avg_loss:0.714, val_acc:0.833]
Epoch [15/120    avg_loss:0.682, val_acc:0.873]
Epoch [16/120    avg_loss:0.627, val_acc:0.855]
Epoch [17/120    avg_loss:0.535, val_acc:0.835]
Epoch [18/120    avg_loss:0.554, val_acc:0.893]
Epoch [19/120    avg_loss:0.514, val_acc:0.891]
Epoch [20/120    avg_loss:0.442, val_acc:0.887]
Epoch [21/120    avg_loss:0.438, val_acc:0.925]
Epoch [22/120    avg_loss:0.434, val_acc:0.879]
Epoch [23/120    avg_loss:0.481, val_acc:0.921]
Epoch [24/120    avg_loss:0.390, val_acc:0.921]
Epoch [25/120    avg_loss:0.373, val_acc:0.927]
Epoch [26/120    avg_loss:0.344, val_acc:0.913]
Epoch [27/120    avg_loss:0.327, val_acc:0.931]
Epoch [28/120    avg_loss:0.291, val_acc:0.913]
Epoch [29/120    avg_loss:0.296, val_acc:0.921]
Epoch [30/120    avg_loss:0.367, val_acc:0.931]
Epoch [31/120    avg_loss:0.297, val_acc:0.946]
Epoch [32/120    avg_loss:0.306, val_acc:0.944]
Epoch [33/120    avg_loss:0.239, val_acc:0.940]
Epoch [34/120    avg_loss:0.216, val_acc:0.938]
Epoch [35/120    avg_loss:0.219, val_acc:0.911]
Epoch [36/120    avg_loss:0.254, val_acc:0.958]
Epoch [37/120    avg_loss:0.230, val_acc:0.933]
Epoch [38/120    avg_loss:0.199, val_acc:0.950]
Epoch [39/120    avg_loss:0.286, val_acc:0.952]
Epoch [40/120    avg_loss:0.181, val_acc:0.962]
Epoch [41/120    avg_loss:0.199, val_acc:0.954]
Epoch [42/120    avg_loss:0.168, val_acc:0.944]
Epoch [43/120    avg_loss:0.226, val_acc:0.960]
Epoch [44/120    avg_loss:0.232, val_acc:0.950]
Epoch [45/120    avg_loss:0.136, val_acc:0.964]
Epoch [46/120    avg_loss:0.149, val_acc:0.966]
Epoch [47/120    avg_loss:0.161, val_acc:0.976]
Epoch [48/120    avg_loss:0.123, val_acc:0.958]
Epoch [49/120    avg_loss:0.155, val_acc:0.942]
Epoch [50/120    avg_loss:0.154, val_acc:0.956]
Epoch [51/120    avg_loss:0.178, val_acc:0.956]
Epoch [52/120    avg_loss:0.111, val_acc:0.970]
Epoch [53/120    avg_loss:0.119, val_acc:0.986]
Epoch [54/120    avg_loss:0.100, val_acc:0.972]
Epoch [55/120    avg_loss:0.117, val_acc:0.962]
Epoch [56/120    avg_loss:0.112, val_acc:0.958]
Epoch [57/120    avg_loss:0.165, val_acc:0.958]
Epoch [58/120    avg_loss:0.096, val_acc:0.964]
Epoch [59/120    avg_loss:0.151, val_acc:0.940]
Epoch [60/120    avg_loss:0.190, val_acc:0.954]
Epoch [61/120    avg_loss:0.149, val_acc:0.960]
Epoch [62/120    avg_loss:0.120, val_acc:0.970]
Epoch [63/120    avg_loss:0.113, val_acc:0.976]
Epoch [64/120    avg_loss:0.086, val_acc:0.976]
Epoch [65/120    avg_loss:0.075, val_acc:0.978]
Epoch [66/120    avg_loss:0.112, val_acc:0.966]
Epoch [67/120    avg_loss:0.099, val_acc:0.976]
Epoch [68/120    avg_loss:0.058, val_acc:0.984]
Epoch [69/120    avg_loss:0.067, val_acc:0.990]
Epoch [70/120    avg_loss:0.062, val_acc:0.990]
Epoch [71/120    avg_loss:0.045, val_acc:0.990]
Epoch [72/120    avg_loss:0.046, val_acc:0.990]
Epoch [73/120    avg_loss:0.049, val_acc:0.990]
Epoch [74/120    avg_loss:0.055, val_acc:0.988]
Epoch [75/120    avg_loss:0.044, val_acc:0.992]
Epoch [76/120    avg_loss:0.041, val_acc:0.990]
Epoch [77/120    avg_loss:0.054, val_acc:0.990]
Epoch [78/120    avg_loss:0.045, val_acc:0.990]
Epoch [79/120    avg_loss:0.036, val_acc:0.990]
Epoch [80/120    avg_loss:0.040, val_acc:0.990]
Epoch [81/120    avg_loss:0.041, val_acc:0.990]
Epoch [82/120    avg_loss:0.045, val_acc:0.990]
Epoch [83/120    avg_loss:0.035, val_acc:0.990]
Epoch [84/120    avg_loss:0.035, val_acc:0.990]
Epoch [85/120    avg_loss:0.037, val_acc:0.990]
Epoch [86/120    avg_loss:0.034, val_acc:0.990]
Epoch [87/120    avg_loss:0.031, val_acc:0.990]
Epoch [88/120    avg_loss:0.036, val_acc:0.990]
Epoch [89/120    avg_loss:0.039, val_acc:0.990]
Epoch [90/120    avg_loss:0.028, val_acc:0.990]
Epoch [91/120    avg_loss:0.038, val_acc:0.990]
Epoch [92/120    avg_loss:0.037, val_acc:0.990]
Epoch [93/120    avg_loss:0.032, val_acc:0.990]
Epoch [94/120    avg_loss:0.037, val_acc:0.990]
Epoch [95/120    avg_loss:0.038, val_acc:0.990]
Epoch [96/120    avg_loss:0.033, val_acc:0.990]
Epoch [97/120    avg_loss:0.033, val_acc:0.990]
Epoch [98/120    avg_loss:0.034, val_acc:0.990]
Epoch [99/120    avg_loss:0.033, val_acc:0.990]
Epoch [100/120    avg_loss:0.033, val_acc:0.990]
Epoch [101/120    avg_loss:0.041, val_acc:0.990]
Epoch [102/120    avg_loss:0.033, val_acc:0.990]
Epoch [103/120    avg_loss:0.027, val_acc:0.990]
Epoch [104/120    avg_loss:0.027, val_acc:0.990]
Epoch [105/120    avg_loss:0.038, val_acc:0.990]
Epoch [106/120    avg_loss:0.028, val_acc:0.990]
Epoch [107/120    avg_loss:0.035, val_acc:0.990]
Epoch [108/120    avg_loss:0.035, val_acc:0.990]
Epoch [109/120    avg_loss:0.035, val_acc:0.990]
Epoch [110/120    avg_loss:0.040, val_acc:0.990]
Epoch [111/120    avg_loss:0.035, val_acc:0.990]
Epoch [112/120    avg_loss:0.034, val_acc:0.990]
Epoch [113/120    avg_loss:0.032, val_acc:0.990]
Epoch [114/120    avg_loss:0.037, val_acc:0.990]
Epoch [115/120    avg_loss:0.028, val_acc:0.990]
Epoch [116/120    avg_loss:0.034, val_acc:0.990]
Epoch [117/120    avg_loss:0.039, val_acc:0.990]
Epoch [118/120    avg_loss:0.035, val_acc:0.990]
Epoch [119/120    avg_loss:0.036, val_acc:0.990]
Epoch [120/120    avg_loss:0.032, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   4   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.99854227 0.97767857 0.98901099 0.9190372  0.88659794
 0.99512195 0.94382022 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9881295622091426
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5c0f92dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.561, val_acc:0.321]
Epoch [2/120    avg_loss:2.435, val_acc:0.317]
Epoch [3/120    avg_loss:2.306, val_acc:0.381]
Epoch [4/120    avg_loss:2.174, val_acc:0.482]
Epoch [5/120    avg_loss:2.041, val_acc:0.538]
Epoch [6/120    avg_loss:1.866, val_acc:0.599]
Epoch [7/120    avg_loss:1.653, val_acc:0.661]
Epoch [8/120    avg_loss:1.462, val_acc:0.720]
Epoch [9/120    avg_loss:1.236, val_acc:0.744]
Epoch [10/120    avg_loss:1.072, val_acc:0.808]
Epoch [11/120    avg_loss:1.031, val_acc:0.796]
Epoch [12/120    avg_loss:0.844, val_acc:0.855]
Epoch [13/120    avg_loss:0.785, val_acc:0.841]
Epoch [14/120    avg_loss:0.665, val_acc:0.849]
Epoch [15/120    avg_loss:0.630, val_acc:0.861]
Epoch [16/120    avg_loss:0.599, val_acc:0.887]
Epoch [17/120    avg_loss:0.566, val_acc:0.907]
Epoch [18/120    avg_loss:0.464, val_acc:0.905]
Epoch [19/120    avg_loss:0.404, val_acc:0.913]
Epoch [20/120    avg_loss:0.393, val_acc:0.885]
Epoch [21/120    avg_loss:0.447, val_acc:0.919]
Epoch [22/120    avg_loss:0.364, val_acc:0.919]
Epoch [23/120    avg_loss:0.371, val_acc:0.935]
Epoch [24/120    avg_loss:0.340, val_acc:0.923]
Epoch [25/120    avg_loss:0.367, val_acc:0.923]
Epoch [26/120    avg_loss:0.295, val_acc:0.921]
Epoch [27/120    avg_loss:0.315, val_acc:0.948]
Epoch [28/120    avg_loss:0.260, val_acc:0.937]
Epoch [29/120    avg_loss:0.308, val_acc:0.917]
Epoch [30/120    avg_loss:0.284, val_acc:0.958]
Epoch [31/120    avg_loss:0.208, val_acc:0.944]
Epoch [32/120    avg_loss:0.256, val_acc:0.952]
Epoch [33/120    avg_loss:0.214, val_acc:0.962]
Epoch [34/120    avg_loss:0.198, val_acc:0.968]
Epoch [35/120    avg_loss:0.211, val_acc:0.946]
Epoch [36/120    avg_loss:0.184, val_acc:0.964]
Epoch [37/120    avg_loss:0.180, val_acc:0.956]
Epoch [38/120    avg_loss:0.165, val_acc:0.966]
Epoch [39/120    avg_loss:0.154, val_acc:0.964]
Epoch [40/120    avg_loss:0.257, val_acc:0.921]
Epoch [41/120    avg_loss:0.175, val_acc:0.944]
Epoch [42/120    avg_loss:0.160, val_acc:0.964]
Epoch [43/120    avg_loss:0.176, val_acc:0.966]
Epoch [44/120    avg_loss:0.157, val_acc:0.881]
Epoch [45/120    avg_loss:0.225, val_acc:0.944]
Epoch [46/120    avg_loss:0.187, val_acc:0.966]
Epoch [47/120    avg_loss:0.133, val_acc:0.966]
Epoch [48/120    avg_loss:0.091, val_acc:0.982]
Epoch [49/120    avg_loss:0.080, val_acc:0.980]
Epoch [50/120    avg_loss:0.082, val_acc:0.980]
Epoch [51/120    avg_loss:0.085, val_acc:0.978]
Epoch [52/120    avg_loss:0.077, val_acc:0.982]
Epoch [53/120    avg_loss:0.081, val_acc:0.982]
Epoch [54/120    avg_loss:0.077, val_acc:0.982]
Epoch [55/120    avg_loss:0.088, val_acc:0.978]
Epoch [56/120    avg_loss:0.084, val_acc:0.980]
Epoch [57/120    avg_loss:0.075, val_acc:0.982]
Epoch [58/120    avg_loss:0.068, val_acc:0.980]
Epoch [59/120    avg_loss:0.067, val_acc:0.982]
Epoch [60/120    avg_loss:0.067, val_acc:0.980]
Epoch [61/120    avg_loss:0.064, val_acc:0.982]
Epoch [62/120    avg_loss:0.068, val_acc:0.982]
Epoch [63/120    avg_loss:0.081, val_acc:0.980]
Epoch [64/120    avg_loss:0.066, val_acc:0.982]
Epoch [65/120    avg_loss:0.066, val_acc:0.984]
Epoch [66/120    avg_loss:0.074, val_acc:0.980]
Epoch [67/120    avg_loss:0.066, val_acc:0.984]
Epoch [68/120    avg_loss:0.059, val_acc:0.982]
Epoch [69/120    avg_loss:0.054, val_acc:0.982]
Epoch [70/120    avg_loss:0.061, val_acc:0.980]
Epoch [71/120    avg_loss:0.058, val_acc:0.984]
Epoch [72/120    avg_loss:0.058, val_acc:0.984]
Epoch [73/120    avg_loss:0.053, val_acc:0.982]
Epoch [74/120    avg_loss:0.057, val_acc:0.984]
Epoch [75/120    avg_loss:0.062, val_acc:0.982]
Epoch [76/120    avg_loss:0.049, val_acc:0.982]
Epoch [77/120    avg_loss:0.058, val_acc:0.980]
Epoch [78/120    avg_loss:0.049, val_acc:0.984]
Epoch [79/120    avg_loss:0.058, val_acc:0.984]
Epoch [80/120    avg_loss:0.050, val_acc:0.984]
Epoch [81/120    avg_loss:0.056, val_acc:0.984]
Epoch [82/120    avg_loss:0.053, val_acc:0.984]
Epoch [83/120    avg_loss:0.049, val_acc:0.982]
Epoch [84/120    avg_loss:0.046, val_acc:0.982]
Epoch [85/120    avg_loss:0.059, val_acc:0.982]
Epoch [86/120    avg_loss:0.051, val_acc:0.984]
Epoch [87/120    avg_loss:0.047, val_acc:0.984]
Epoch [88/120    avg_loss:0.050, val_acc:0.984]
Epoch [89/120    avg_loss:0.058, val_acc:0.984]
Epoch [90/120    avg_loss:0.046, val_acc:0.986]
Epoch [91/120    avg_loss:0.048, val_acc:0.984]
Epoch [92/120    avg_loss:0.059, val_acc:0.986]
Epoch [93/120    avg_loss:0.053, val_acc:0.986]
Epoch [94/120    avg_loss:0.044, val_acc:0.982]
Epoch [95/120    avg_loss:0.050, val_acc:0.982]
Epoch [96/120    avg_loss:0.045, val_acc:0.986]
Epoch [97/120    avg_loss:0.054, val_acc:0.988]
Epoch [98/120    avg_loss:0.051, val_acc:0.986]
Epoch [99/120    avg_loss:0.041, val_acc:0.984]
Epoch [100/120    avg_loss:0.043, val_acc:0.986]
Epoch [101/120    avg_loss:0.045, val_acc:0.982]
Epoch [102/120    avg_loss:0.056, val_acc:0.984]
Epoch [103/120    avg_loss:0.043, val_acc:0.988]
Epoch [104/120    avg_loss:0.040, val_acc:0.988]
Epoch [105/120    avg_loss:0.045, val_acc:0.984]
Epoch [106/120    avg_loss:0.046, val_acc:0.988]
Epoch [107/120    avg_loss:0.045, val_acc:0.984]
Epoch [108/120    avg_loss:0.048, val_acc:0.982]
Epoch [109/120    avg_loss:0.038, val_acc:0.986]
Epoch [110/120    avg_loss:0.041, val_acc:0.986]
Epoch [111/120    avg_loss:0.043, val_acc:0.986]
Epoch [112/120    avg_loss:0.043, val_acc:0.990]
Epoch [113/120    avg_loss:0.033, val_acc:0.988]
Epoch [114/120    avg_loss:0.035, val_acc:0.988]
Epoch [115/120    avg_loss:0.040, val_acc:0.988]
Epoch [116/120    avg_loss:0.039, val_acc:0.984]
Epoch [117/120    avg_loss:0.040, val_acc:0.990]
Epoch [118/120    avg_loss:0.044, val_acc:0.986]
Epoch [119/120    avg_loss:0.045, val_acc:0.986]
Epoch [120/120    avg_loss:0.035, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.97550111 0.99782135 0.92       0.87755102
 1.         0.94382022 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9888425571585476
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ad2659d30>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.135]
Epoch [2/120    avg_loss:2.477, val_acc:0.323]
Epoch [3/120    avg_loss:2.341, val_acc:0.407]
Epoch [4/120    avg_loss:2.214, val_acc:0.395]
Epoch [5/120    avg_loss:2.074, val_acc:0.530]
Epoch [6/120    avg_loss:1.907, val_acc:0.649]
Epoch [7/120    avg_loss:1.731, val_acc:0.690]
Epoch [8/120    avg_loss:1.554, val_acc:0.710]
Epoch [9/120    avg_loss:1.384, val_acc:0.732]
Epoch [10/120    avg_loss:1.210, val_acc:0.819]
Epoch [11/120    avg_loss:1.020, val_acc:0.768]
Epoch [12/120    avg_loss:0.960, val_acc:0.760]
Epoch [13/120    avg_loss:0.867, val_acc:0.808]
Epoch [14/120    avg_loss:0.744, val_acc:0.847]
Epoch [15/120    avg_loss:0.691, val_acc:0.859]
Epoch [16/120    avg_loss:0.654, val_acc:0.859]
Epoch [17/120    avg_loss:0.540, val_acc:0.861]
Epoch [18/120    avg_loss:0.611, val_acc:0.869]
Epoch [19/120    avg_loss:0.529, val_acc:0.875]
Epoch [20/120    avg_loss:0.496, val_acc:0.881]
Epoch [21/120    avg_loss:0.515, val_acc:0.909]
Epoch [22/120    avg_loss:0.455, val_acc:0.897]
Epoch [23/120    avg_loss:0.531, val_acc:0.865]
Epoch [24/120    avg_loss:0.434, val_acc:0.909]
Epoch [25/120    avg_loss:0.410, val_acc:0.907]
Epoch [26/120    avg_loss:0.395, val_acc:0.901]
Epoch [27/120    avg_loss:0.342, val_acc:0.927]
Epoch [28/120    avg_loss:0.363, val_acc:0.913]
Epoch [29/120    avg_loss:0.326, val_acc:0.923]
Epoch [30/120    avg_loss:0.356, val_acc:0.915]
Epoch [31/120    avg_loss:0.348, val_acc:0.923]
Epoch [32/120    avg_loss:0.359, val_acc:0.885]
Epoch [33/120    avg_loss:0.367, val_acc:0.940]
Epoch [34/120    avg_loss:0.278, val_acc:0.958]
Epoch [35/120    avg_loss:0.244, val_acc:0.958]
Epoch [36/120    avg_loss:0.249, val_acc:0.938]
Epoch [37/120    avg_loss:0.228, val_acc:0.954]
Epoch [38/120    avg_loss:0.232, val_acc:0.938]
Epoch [39/120    avg_loss:0.276, val_acc:0.944]
Epoch [40/120    avg_loss:0.288, val_acc:0.956]
Epoch [41/120    avg_loss:0.186, val_acc:0.946]
Epoch [42/120    avg_loss:0.249, val_acc:0.940]
Epoch [43/120    avg_loss:0.195, val_acc:0.942]
Epoch [44/120    avg_loss:0.178, val_acc:0.964]
Epoch [45/120    avg_loss:0.172, val_acc:0.958]
Epoch [46/120    avg_loss:0.161, val_acc:0.952]
Epoch [47/120    avg_loss:0.180, val_acc:0.962]
Epoch [48/120    avg_loss:0.137, val_acc:0.964]
Epoch [49/120    avg_loss:0.151, val_acc:0.972]
Epoch [50/120    avg_loss:0.141, val_acc:0.964]
Epoch [51/120    avg_loss:0.148, val_acc:0.950]
Epoch [52/120    avg_loss:0.165, val_acc:0.968]
Epoch [53/120    avg_loss:0.122, val_acc:0.970]
Epoch [54/120    avg_loss:0.102, val_acc:0.972]
Epoch [55/120    avg_loss:0.136, val_acc:0.966]
Epoch [56/120    avg_loss:0.127, val_acc:0.970]
Epoch [57/120    avg_loss:0.139, val_acc:0.970]
Epoch [58/120    avg_loss:0.112, val_acc:0.966]
Epoch [59/120    avg_loss:0.107, val_acc:0.974]
Epoch [60/120    avg_loss:0.108, val_acc:0.970]
Epoch [61/120    avg_loss:0.183, val_acc:0.923]
Epoch [62/120    avg_loss:0.282, val_acc:0.954]
Epoch [63/120    avg_loss:0.188, val_acc:0.964]
Epoch [64/120    avg_loss:0.131, val_acc:0.974]
Epoch [65/120    avg_loss:0.126, val_acc:0.970]
Epoch [66/120    avg_loss:0.090, val_acc:0.978]
Epoch [67/120    avg_loss:0.115, val_acc:0.976]
Epoch [68/120    avg_loss:0.093, val_acc:0.972]
Epoch [69/120    avg_loss:0.084, val_acc:0.984]
Epoch [70/120    avg_loss:0.092, val_acc:0.982]
Epoch [71/120    avg_loss:0.100, val_acc:0.972]
Epoch [72/120    avg_loss:0.120, val_acc:0.976]
Epoch [73/120    avg_loss:0.108, val_acc:0.956]
Epoch [74/120    avg_loss:0.096, val_acc:0.980]
Epoch [75/120    avg_loss:0.119, val_acc:0.980]
Epoch [76/120    avg_loss:0.077, val_acc:0.978]
Epoch [77/120    avg_loss:0.063, val_acc:0.984]
Epoch [78/120    avg_loss:0.093, val_acc:0.978]
Epoch [79/120    avg_loss:0.077, val_acc:0.964]
Epoch [80/120    avg_loss:0.067, val_acc:0.980]
Epoch [81/120    avg_loss:0.062, val_acc:0.974]
Epoch [82/120    avg_loss:0.058, val_acc:0.990]
Epoch [83/120    avg_loss:0.064, val_acc:0.978]
Epoch [84/120    avg_loss:0.060, val_acc:0.982]
Epoch [85/120    avg_loss:0.056, val_acc:0.986]
Epoch [86/120    avg_loss:0.057, val_acc:0.966]
Epoch [87/120    avg_loss:0.048, val_acc:0.976]
Epoch [88/120    avg_loss:0.051, val_acc:0.974]
Epoch [89/120    avg_loss:0.037, val_acc:0.988]
Epoch [90/120    avg_loss:0.079, val_acc:0.952]
Epoch [91/120    avg_loss:0.116, val_acc:0.980]
Epoch [92/120    avg_loss:0.108, val_acc:0.982]
Epoch [93/120    avg_loss:0.034, val_acc:0.984]
Epoch [94/120    avg_loss:0.025, val_acc:0.988]
Epoch [95/120    avg_loss:0.042, val_acc:0.988]
Epoch [96/120    avg_loss:0.025, val_acc:0.990]
Epoch [97/120    avg_loss:0.022, val_acc:0.992]
Epoch [98/120    avg_loss:0.023, val_acc:0.992]
Epoch [99/120    avg_loss:0.024, val_acc:0.992]
Epoch [100/120    avg_loss:0.020, val_acc:0.992]
Epoch [101/120    avg_loss:0.024, val_acc:0.992]
Epoch [102/120    avg_loss:0.018, val_acc:0.992]
Epoch [103/120    avg_loss:0.023, val_acc:0.992]
Epoch [104/120    avg_loss:0.021, val_acc:0.992]
Epoch [105/120    avg_loss:0.022, val_acc:0.992]
Epoch [106/120    avg_loss:0.019, val_acc:0.992]
Epoch [107/120    avg_loss:0.020, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.992]
Epoch [109/120    avg_loss:0.017, val_acc:0.992]
Epoch [110/120    avg_loss:0.017, val_acc:0.992]
Epoch [111/120    avg_loss:0.019, val_acc:0.992]
Epoch [112/120    avg_loss:0.017, val_acc:0.992]
Epoch [113/120    avg_loss:0.016, val_acc:0.992]
Epoch [114/120    avg_loss:0.016, val_acc:0.992]
Epoch [115/120    avg_loss:0.018, val_acc:0.992]
Epoch [116/120    avg_loss:0.015, val_acc:0.992]
Epoch [117/120    avg_loss:0.021, val_acc:0.992]
Epoch [118/120    avg_loss:0.017, val_acc:0.992]
Epoch [119/120    avg_loss:0.023, val_acc:0.992]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99854015 0.97977528 0.99782135 0.95909091 0.94117647
 1.         0.95027624 0.99870968 1.         1.         1.
 1.         1.        ]

Kappa:
0.9931161472127179
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa9544dcd68>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.351]
Epoch [2/120    avg_loss:2.494, val_acc:0.284]
Epoch [3/120    avg_loss:2.368, val_acc:0.325]
Epoch [4/120    avg_loss:2.259, val_acc:0.411]
Epoch [5/120    avg_loss:2.147, val_acc:0.413]
Epoch [6/120    avg_loss:2.009, val_acc:0.468]
Epoch [7/120    avg_loss:1.880, val_acc:0.532]
Epoch [8/120    avg_loss:1.735, val_acc:0.619]
Epoch [9/120    avg_loss:1.572, val_acc:0.667]
Epoch [10/120    avg_loss:1.424, val_acc:0.718]
Epoch [11/120    avg_loss:1.236, val_acc:0.804]
Epoch [12/120    avg_loss:1.110, val_acc:0.752]
Epoch [13/120    avg_loss:0.916, val_acc:0.839]
Epoch [14/120    avg_loss:0.812, val_acc:0.812]
Epoch [15/120    avg_loss:0.765, val_acc:0.839]
Epoch [16/120    avg_loss:0.669, val_acc:0.851]
Epoch [17/120    avg_loss:0.574, val_acc:0.837]
Epoch [18/120    avg_loss:0.565, val_acc:0.893]
Epoch [19/120    avg_loss:0.564, val_acc:0.899]
Epoch [20/120    avg_loss:0.518, val_acc:0.901]
Epoch [21/120    avg_loss:0.594, val_acc:0.774]
Epoch [22/120    avg_loss:0.481, val_acc:0.835]
Epoch [23/120    avg_loss:0.421, val_acc:0.891]
Epoch [24/120    avg_loss:0.432, val_acc:0.923]
Epoch [25/120    avg_loss:0.347, val_acc:0.885]
Epoch [26/120    avg_loss:0.379, val_acc:0.935]
Epoch [27/120    avg_loss:0.354, val_acc:0.919]
Epoch [28/120    avg_loss:0.320, val_acc:0.919]
Epoch [29/120    avg_loss:0.325, val_acc:0.927]
Epoch [30/120    avg_loss:0.352, val_acc:0.931]
Epoch [31/120    avg_loss:0.348, val_acc:0.944]
Epoch [32/120    avg_loss:0.336, val_acc:0.935]
Epoch [33/120    avg_loss:0.310, val_acc:0.927]
Epoch [34/120    avg_loss:0.289, val_acc:0.911]
Epoch [35/120    avg_loss:0.283, val_acc:0.927]
Epoch [36/120    avg_loss:0.302, val_acc:0.901]
Epoch [37/120    avg_loss:0.234, val_acc:0.940]
Epoch [38/120    avg_loss:0.265, val_acc:0.954]
Epoch [39/120    avg_loss:0.242, val_acc:0.958]
Epoch [40/120    avg_loss:0.250, val_acc:0.944]
Epoch [41/120    avg_loss:0.198, val_acc:0.968]
Epoch [42/120    avg_loss:0.218, val_acc:0.962]
Epoch [43/120    avg_loss:0.198, val_acc:0.935]
Epoch [44/120    avg_loss:0.162, val_acc:0.954]
Epoch [45/120    avg_loss:0.174, val_acc:0.956]
Epoch [46/120    avg_loss:0.185, val_acc:0.974]
Epoch [47/120    avg_loss:0.147, val_acc:0.978]
Epoch [48/120    avg_loss:0.135, val_acc:0.877]
Epoch [49/120    avg_loss:0.187, val_acc:0.966]
Epoch [50/120    avg_loss:0.134, val_acc:0.952]
Epoch [51/120    avg_loss:0.212, val_acc:0.966]
Epoch [52/120    avg_loss:0.131, val_acc:0.962]
Epoch [53/120    avg_loss:0.103, val_acc:0.986]
Epoch [54/120    avg_loss:0.117, val_acc:0.972]
Epoch [55/120    avg_loss:0.109, val_acc:0.982]
Epoch [56/120    avg_loss:0.097, val_acc:0.978]
Epoch [57/120    avg_loss:0.068, val_acc:0.986]
Epoch [58/120    avg_loss:0.093, val_acc:0.974]
Epoch [59/120    avg_loss:0.106, val_acc:0.982]
Epoch [60/120    avg_loss:0.152, val_acc:0.944]
Epoch [61/120    avg_loss:0.093, val_acc:0.982]
Epoch [62/120    avg_loss:0.158, val_acc:0.980]
Epoch [63/120    avg_loss:0.141, val_acc:0.968]
Epoch [64/120    avg_loss:0.082, val_acc:0.974]
Epoch [65/120    avg_loss:0.090, val_acc:0.976]
Epoch [66/120    avg_loss:0.067, val_acc:0.984]
Epoch [67/120    avg_loss:0.045, val_acc:0.990]
Epoch [68/120    avg_loss:0.049, val_acc:0.986]
Epoch [69/120    avg_loss:0.041, val_acc:0.988]
Epoch [70/120    avg_loss:0.046, val_acc:0.986]
Epoch [71/120    avg_loss:0.034, val_acc:0.984]
Epoch [72/120    avg_loss:0.043, val_acc:0.980]
Epoch [73/120    avg_loss:0.049, val_acc:0.988]
Epoch [74/120    avg_loss:0.042, val_acc:0.990]
Epoch [75/120    avg_loss:0.039, val_acc:0.988]
Epoch [76/120    avg_loss:0.027, val_acc:0.986]
Epoch [77/120    avg_loss:0.038, val_acc:0.990]
Epoch [78/120    avg_loss:0.039, val_acc:0.984]
Epoch [79/120    avg_loss:0.022, val_acc:0.990]
Epoch [80/120    avg_loss:0.031, val_acc:0.986]
Epoch [81/120    avg_loss:0.043, val_acc:0.982]
Epoch [82/120    avg_loss:0.042, val_acc:0.982]
Epoch [83/120    avg_loss:0.041, val_acc:0.988]
Epoch [84/120    avg_loss:0.076, val_acc:0.974]
Epoch [85/120    avg_loss:0.070, val_acc:0.974]
Epoch [86/120    avg_loss:0.063, val_acc:0.970]
Epoch [87/120    avg_loss:0.116, val_acc:0.978]
Epoch [88/120    avg_loss:0.060, val_acc:0.988]
Epoch [89/120    avg_loss:0.030, val_acc:0.994]
Epoch [90/120    avg_loss:0.034, val_acc:0.988]
Epoch [91/120    avg_loss:0.023, val_acc:0.984]
Epoch [92/120    avg_loss:0.019, val_acc:0.986]
Epoch [93/120    avg_loss:0.043, val_acc:0.978]
Epoch [94/120    avg_loss:0.134, val_acc:0.976]
Epoch [95/120    avg_loss:0.098, val_acc:0.986]
Epoch [96/120    avg_loss:0.045, val_acc:0.986]
Epoch [97/120    avg_loss:0.041, val_acc:0.988]
Epoch [98/120    avg_loss:0.024, val_acc:0.990]
Epoch [99/120    avg_loss:0.029, val_acc:0.988]
Epoch [100/120    avg_loss:0.026, val_acc:0.988]
Epoch [101/120    avg_loss:0.019, val_acc:0.986]
Epoch [102/120    avg_loss:0.021, val_acc:0.988]
Epoch [103/120    avg_loss:0.024, val_acc:0.990]
Epoch [104/120    avg_loss:0.013, val_acc:0.990]
Epoch [105/120    avg_loss:0.015, val_acc:0.990]
Epoch [106/120    avg_loss:0.025, val_acc:0.992]
Epoch [107/120    avg_loss:0.012, val_acc:0.990]
Epoch [108/120    avg_loss:0.015, val_acc:0.992]
Epoch [109/120    avg_loss:0.012, val_acc:0.992]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.011, val_acc:0.988]
Epoch [112/120    avg_loss:0.015, val_acc:0.990]
Epoch [113/120    avg_loss:0.010, val_acc:0.990]
Epoch [114/120    avg_loss:0.012, val_acc:0.990]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.011, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   6   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 220   5   0   0   0   0   0   0   2   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.97986577 0.98004435 0.94017094 0.92857143
 1.         0.94972067 0.99742931 0.99893276 1.         1.
 0.99779736 1.        ]

Kappa:
0.9905033320092332
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0861addd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.587, val_acc:0.111]
Epoch [2/120    avg_loss:2.482, val_acc:0.290]
Epoch [3/120    avg_loss:2.364, val_acc:0.308]
Epoch [4/120    avg_loss:2.233, val_acc:0.409]
Epoch [5/120    avg_loss:2.115, val_acc:0.532]
Epoch [6/120    avg_loss:1.974, val_acc:0.595]
Epoch [7/120    avg_loss:1.809, val_acc:0.677]
Epoch [8/120    avg_loss:1.614, val_acc:0.645]
Epoch [9/120    avg_loss:1.439, val_acc:0.710]
Epoch [10/120    avg_loss:1.265, val_acc:0.730]
Epoch [11/120    avg_loss:1.110, val_acc:0.768]
Epoch [12/120    avg_loss:0.952, val_acc:0.790]
Epoch [13/120    avg_loss:0.868, val_acc:0.746]
Epoch [14/120    avg_loss:0.802, val_acc:0.788]
Epoch [15/120    avg_loss:0.772, val_acc:0.827]
Epoch [16/120    avg_loss:0.718, val_acc:0.782]
Epoch [17/120    avg_loss:0.671, val_acc:0.815]
Epoch [18/120    avg_loss:0.680, val_acc:0.833]
Epoch [19/120    avg_loss:0.538, val_acc:0.881]
Epoch [20/120    avg_loss:0.524, val_acc:0.903]
Epoch [21/120    avg_loss:0.499, val_acc:0.871]
Epoch [22/120    avg_loss:0.458, val_acc:0.891]
Epoch [23/120    avg_loss:0.495, val_acc:0.909]
Epoch [24/120    avg_loss:0.457, val_acc:0.792]
Epoch [25/120    avg_loss:0.435, val_acc:0.903]
Epoch [26/120    avg_loss:0.399, val_acc:0.899]
Epoch [27/120    avg_loss:0.368, val_acc:0.937]
Epoch [28/120    avg_loss:0.305, val_acc:0.905]
Epoch [29/120    avg_loss:0.414, val_acc:0.925]
Epoch [30/120    avg_loss:0.333, val_acc:0.940]
Epoch [31/120    avg_loss:0.286, val_acc:0.917]
Epoch [32/120    avg_loss:0.293, val_acc:0.915]
Epoch [33/120    avg_loss:0.286, val_acc:0.837]
Epoch [34/120    avg_loss:0.257, val_acc:0.954]
Epoch [35/120    avg_loss:0.238, val_acc:0.849]
Epoch [36/120    avg_loss:0.284, val_acc:0.946]
Epoch [37/120    avg_loss:0.245, val_acc:0.956]
Epoch [38/120    avg_loss:0.209, val_acc:0.942]
Epoch [39/120    avg_loss:0.165, val_acc:0.960]
Epoch [40/120    avg_loss:0.204, val_acc:0.954]
Epoch [41/120    avg_loss:0.183, val_acc:0.938]
Epoch [42/120    avg_loss:0.239, val_acc:0.948]
Epoch [43/120    avg_loss:0.185, val_acc:0.946]
Epoch [44/120    avg_loss:0.169, val_acc:0.964]
Epoch [45/120    avg_loss:0.158, val_acc:0.984]
Epoch [46/120    avg_loss:0.124, val_acc:0.984]
Epoch [47/120    avg_loss:0.178, val_acc:0.921]
Epoch [48/120    avg_loss:0.223, val_acc:0.938]
Epoch [49/120    avg_loss:0.151, val_acc:0.962]
Epoch [50/120    avg_loss:0.120, val_acc:0.976]
Epoch [51/120    avg_loss:0.099, val_acc:0.976]
Epoch [52/120    avg_loss:0.093, val_acc:0.976]
Epoch [53/120    avg_loss:0.104, val_acc:0.976]
Epoch [54/120    avg_loss:0.081, val_acc:0.990]
Epoch [55/120    avg_loss:0.073, val_acc:0.992]
Epoch [56/120    avg_loss:0.067, val_acc:0.984]
Epoch [57/120    avg_loss:0.074, val_acc:0.976]
Epoch [58/120    avg_loss:0.065, val_acc:0.980]
Epoch [59/120    avg_loss:0.093, val_acc:0.982]
Epoch [60/120    avg_loss:0.056, val_acc:0.984]
Epoch [61/120    avg_loss:0.092, val_acc:0.976]
Epoch [62/120    avg_loss:0.134, val_acc:0.976]
Epoch [63/120    avg_loss:0.090, val_acc:0.980]
Epoch [64/120    avg_loss:0.057, val_acc:0.966]
Epoch [65/120    avg_loss:0.122, val_acc:0.964]
Epoch [66/120    avg_loss:0.096, val_acc:0.972]
Epoch [67/120    avg_loss:0.086, val_acc:0.972]
Epoch [68/120    avg_loss:0.062, val_acc:0.972]
Epoch [69/120    avg_loss:0.042, val_acc:0.978]
Epoch [70/120    avg_loss:0.033, val_acc:0.978]
Epoch [71/120    avg_loss:0.043, val_acc:0.986]
Epoch [72/120    avg_loss:0.041, val_acc:0.982]
Epoch [73/120    avg_loss:0.035, val_acc:0.982]
Epoch [74/120    avg_loss:0.033, val_acc:0.986]
Epoch [75/120    avg_loss:0.033, val_acc:0.988]
Epoch [76/120    avg_loss:0.033, val_acc:0.990]
Epoch [77/120    avg_loss:0.034, val_acc:0.986]
Epoch [78/120    avg_loss:0.029, val_acc:0.990]
Epoch [79/120    avg_loss:0.031, val_acc:0.990]
Epoch [80/120    avg_loss:0.031, val_acc:0.990]
Epoch [81/120    avg_loss:0.030, val_acc:0.992]
Epoch [82/120    avg_loss:0.028, val_acc:0.992]
Epoch [83/120    avg_loss:0.028, val_acc:0.992]
Epoch [84/120    avg_loss:0.025, val_acc:0.992]
Epoch [85/120    avg_loss:0.026, val_acc:0.992]
Epoch [86/120    avg_loss:0.029, val_acc:0.992]
Epoch [87/120    avg_loss:0.027, val_acc:0.992]
Epoch [88/120    avg_loss:0.037, val_acc:0.992]
Epoch [89/120    avg_loss:0.021, val_acc:0.992]
Epoch [90/120    avg_loss:0.030, val_acc:0.992]
Epoch [91/120    avg_loss:0.026, val_acc:0.994]
Epoch [92/120    avg_loss:0.028, val_acc:0.994]
Epoch [93/120    avg_loss:0.027, val_acc:0.994]
Epoch [94/120    avg_loss:0.024, val_acc:0.994]
Epoch [95/120    avg_loss:0.022, val_acc:0.994]
Epoch [96/120    avg_loss:0.029, val_acc:0.994]
Epoch [97/120    avg_loss:0.027, val_acc:0.994]
Epoch [98/120    avg_loss:0.028, val_acc:0.994]
Epoch [99/120    avg_loss:0.027, val_acc:0.994]
Epoch [100/120    avg_loss:0.026, val_acc:0.994]
Epoch [101/120    avg_loss:0.025, val_acc:0.994]
Epoch [102/120    avg_loss:0.021, val_acc:0.994]
Epoch [103/120    avg_loss:0.023, val_acc:0.994]
Epoch [104/120    avg_loss:0.025, val_acc:0.994]
Epoch [105/120    avg_loss:0.026, val_acc:0.994]
Epoch [106/120    avg_loss:0.022, val_acc:0.994]
Epoch [107/120    avg_loss:0.022, val_acc:0.994]
Epoch [108/120    avg_loss:0.026, val_acc:0.994]
Epoch [109/120    avg_loss:0.022, val_acc:0.996]
Epoch [110/120    avg_loss:0.023, val_acc:0.994]
Epoch [111/120    avg_loss:0.026, val_acc:0.994]
Epoch [112/120    avg_loss:0.026, val_acc:0.998]
Epoch [113/120    avg_loss:0.018, val_acc:0.998]
Epoch [114/120    avg_loss:0.022, val_acc:0.996]
Epoch [115/120    avg_loss:0.026, val_acc:0.998]
Epoch [116/120    avg_loss:0.023, val_acc:0.998]
Epoch [117/120    avg_loss:0.022, val_acc:0.998]
Epoch [118/120    avg_loss:0.022, val_acc:0.998]
Epoch [119/120    avg_loss:0.021, val_acc:0.996]
Epoch [120/120    avg_loss:0.022, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.99095023 0.98230088 0.95132743 0.95333333
 1.         0.97826087 1.         1.         1.         0.98817346
 0.98998888 1.        ]

Kappa:
0.9916920174433723
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbbce499dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.611, val_acc:0.447]
Epoch [2/120    avg_loss:2.517, val_acc:0.445]
Epoch [3/120    avg_loss:2.402, val_acc:0.477]
Epoch [4/120    avg_loss:2.260, val_acc:0.506]
Epoch [5/120    avg_loss:2.093, val_acc:0.551]
Epoch [6/120    avg_loss:1.903, val_acc:0.578]
Epoch [7/120    avg_loss:1.722, val_acc:0.654]
Epoch [8/120    avg_loss:1.502, val_acc:0.744]
Epoch [9/120    avg_loss:1.296, val_acc:0.779]
Epoch [10/120    avg_loss:1.171, val_acc:0.799]
Epoch [11/120    avg_loss:1.007, val_acc:0.842]
Epoch [12/120    avg_loss:0.831, val_acc:0.770]
Epoch [13/120    avg_loss:0.706, val_acc:0.881]
Epoch [14/120    avg_loss:0.660, val_acc:0.877]
Epoch [15/120    avg_loss:0.619, val_acc:0.922]
Epoch [16/120    avg_loss:0.533, val_acc:0.922]
Epoch [17/120    avg_loss:0.521, val_acc:0.914]
Epoch [18/120    avg_loss:0.451, val_acc:0.891]
Epoch [19/120    avg_loss:0.465, val_acc:0.928]
Epoch [20/120    avg_loss:0.407, val_acc:0.932]
Epoch [21/120    avg_loss:0.389, val_acc:0.889]
Epoch [22/120    avg_loss:0.357, val_acc:0.922]
Epoch [23/120    avg_loss:0.339, val_acc:0.914]
Epoch [24/120    avg_loss:0.308, val_acc:0.951]
Epoch [25/120    avg_loss:0.266, val_acc:0.961]
Epoch [26/120    avg_loss:0.256, val_acc:0.949]
Epoch [27/120    avg_loss:0.259, val_acc:0.965]
Epoch [28/120    avg_loss:0.267, val_acc:0.941]
Epoch [29/120    avg_loss:0.283, val_acc:0.928]
Epoch [30/120    avg_loss:0.256, val_acc:0.953]
Epoch [31/120    avg_loss:0.232, val_acc:0.961]
Epoch [32/120    avg_loss:0.169, val_acc:0.957]
Epoch [33/120    avg_loss:0.199, val_acc:0.955]
Epoch [34/120    avg_loss:0.198, val_acc:0.936]
Epoch [35/120    avg_loss:0.178, val_acc:0.967]
Epoch [36/120    avg_loss:0.187, val_acc:0.971]
Epoch [37/120    avg_loss:0.148, val_acc:0.949]
Epoch [38/120    avg_loss:0.171, val_acc:0.961]
Epoch [39/120    avg_loss:0.157, val_acc:0.961]
Epoch [40/120    avg_loss:0.119, val_acc:0.949]
Epoch [41/120    avg_loss:0.128, val_acc:0.971]
Epoch [42/120    avg_loss:0.102, val_acc:0.982]
Epoch [43/120    avg_loss:0.110, val_acc:0.932]
Epoch [44/120    avg_loss:0.191, val_acc:0.973]
Epoch [45/120    avg_loss:0.119, val_acc:0.980]
Epoch [46/120    avg_loss:0.104, val_acc:0.977]
Epoch [47/120    avg_loss:0.147, val_acc:0.955]
Epoch [48/120    avg_loss:0.155, val_acc:0.969]
Epoch [49/120    avg_loss:0.117, val_acc:0.984]
Epoch [50/120    avg_loss:0.099, val_acc:0.980]
Epoch [51/120    avg_loss:0.078, val_acc:0.988]
Epoch [52/120    avg_loss:0.105, val_acc:0.980]
Epoch [53/120    avg_loss:0.094, val_acc:0.971]
Epoch [54/120    avg_loss:0.139, val_acc:0.967]
Epoch [55/120    avg_loss:0.104, val_acc:0.990]
Epoch [56/120    avg_loss:0.068, val_acc:0.990]
Epoch [57/120    avg_loss:0.079, val_acc:0.967]
Epoch [58/120    avg_loss:0.056, val_acc:0.988]
Epoch [59/120    avg_loss:0.036, val_acc:0.992]
Epoch [60/120    avg_loss:0.052, val_acc:0.988]
Epoch [61/120    avg_loss:0.052, val_acc:0.986]
Epoch [62/120    avg_loss:0.120, val_acc:0.982]
Epoch [63/120    avg_loss:0.068, val_acc:0.990]
Epoch [64/120    avg_loss:0.070, val_acc:0.973]
Epoch [65/120    avg_loss:0.047, val_acc:0.988]
Epoch [66/120    avg_loss:0.039, val_acc:0.992]
Epoch [67/120    avg_loss:0.032, val_acc:0.982]
Epoch [68/120    avg_loss:0.052, val_acc:0.986]
Epoch [69/120    avg_loss:0.029, val_acc:0.990]
Epoch [70/120    avg_loss:0.031, val_acc:0.990]
Epoch [71/120    avg_loss:0.034, val_acc:0.986]
Epoch [72/120    avg_loss:0.038, val_acc:0.992]
Epoch [73/120    avg_loss:0.021, val_acc:0.990]
Epoch [74/120    avg_loss:0.022, val_acc:0.992]
Epoch [75/120    avg_loss:0.017, val_acc:0.988]
Epoch [76/120    avg_loss:0.030, val_acc:0.986]
Epoch [77/120    avg_loss:0.021, val_acc:0.992]
Epoch [78/120    avg_loss:0.019, val_acc:0.994]
Epoch [79/120    avg_loss:0.017, val_acc:0.992]
Epoch [80/120    avg_loss:0.015, val_acc:0.992]
Epoch [81/120    avg_loss:0.020, val_acc:0.990]
Epoch [82/120    avg_loss:0.012, val_acc:0.990]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.013, val_acc:0.992]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.992]
Epoch [87/120    avg_loss:0.010, val_acc:0.994]
Epoch [88/120    avg_loss:0.067, val_acc:0.990]
Epoch [89/120    avg_loss:0.044, val_acc:0.949]
Epoch [90/120    avg_loss:0.044, val_acc:0.986]
Epoch [91/120    avg_loss:0.142, val_acc:0.988]
Epoch [92/120    avg_loss:0.031, val_acc:0.994]
Epoch [93/120    avg_loss:0.037, val_acc:0.994]
Epoch [94/120    avg_loss:0.019, val_acc:0.994]
Epoch [95/120    avg_loss:0.015, val_acc:0.992]
Epoch [96/120    avg_loss:0.016, val_acc:0.994]
Epoch [97/120    avg_loss:0.011, val_acc:0.994]
Epoch [98/120    avg_loss:0.012, val_acc:0.994]
Epoch [99/120    avg_loss:0.014, val_acc:0.994]
Epoch [100/120    avg_loss:0.011, val_acc:0.994]
Epoch [101/120    avg_loss:0.012, val_acc:0.994]
Epoch [102/120    avg_loss:0.009, val_acc:0.994]
Epoch [103/120    avg_loss:0.009, val_acc:0.994]
Epoch [104/120    avg_loss:0.007, val_acc:0.994]
Epoch [105/120    avg_loss:0.013, val_acc:0.994]
Epoch [106/120    avg_loss:0.013, val_acc:0.992]
Epoch [107/120    avg_loss:0.009, val_acc:0.994]
Epoch [108/120    avg_loss:0.007, val_acc:0.994]
Epoch [109/120    avg_loss:0.007, val_acc:0.994]
Epoch [110/120    avg_loss:0.007, val_acc:0.994]
Epoch [111/120    avg_loss:0.006, val_acc:0.994]
Epoch [112/120    avg_loss:0.007, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.006, val_acc:0.994]
Epoch [115/120    avg_loss:0.008, val_acc:0.994]
Epoch [116/120    avg_loss:0.006, val_acc:0.994]
Epoch [117/120    avg_loss:0.006, val_acc:0.994]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.011, val_acc:0.994]
Epoch [120/120    avg_loss:0.010, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   5   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.9977221  0.98678414 0.93186813 0.91156463
 1.         0.99465241 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9921664357814255
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe2bb80be10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.276]
Epoch [2/120    avg_loss:2.499, val_acc:0.308]
Epoch [3/120    avg_loss:2.387, val_acc:0.329]
Epoch [4/120    avg_loss:2.267, val_acc:0.355]
Epoch [5/120    avg_loss:2.164, val_acc:0.417]
Epoch [6/120    avg_loss:2.041, val_acc:0.546]
Epoch [7/120    avg_loss:1.892, val_acc:0.619]
Epoch [8/120    avg_loss:1.743, val_acc:0.659]
Epoch [9/120    avg_loss:1.551, val_acc:0.696]
Epoch [10/120    avg_loss:1.404, val_acc:0.706]
Epoch [11/120    avg_loss:1.246, val_acc:0.732]
Epoch [12/120    avg_loss:1.040, val_acc:0.762]
Epoch [13/120    avg_loss:0.944, val_acc:0.806]
Epoch [14/120    avg_loss:0.838, val_acc:0.786]
Epoch [15/120    avg_loss:0.809, val_acc:0.812]
Epoch [16/120    avg_loss:0.689, val_acc:0.845]
Epoch [17/120    avg_loss:0.650, val_acc:0.855]
Epoch [18/120    avg_loss:0.632, val_acc:0.845]
Epoch [19/120    avg_loss:0.566, val_acc:0.899]
Epoch [20/120    avg_loss:0.531, val_acc:0.895]
Epoch [21/120    avg_loss:0.503, val_acc:0.901]
Epoch [22/120    avg_loss:0.433, val_acc:0.889]
Epoch [23/120    avg_loss:0.410, val_acc:0.931]
Epoch [24/120    avg_loss:0.424, val_acc:0.917]
Epoch [25/120    avg_loss:0.374, val_acc:0.917]
Epoch [26/120    avg_loss:0.383, val_acc:0.929]
Epoch [27/120    avg_loss:0.387, val_acc:0.925]
Epoch [28/120    avg_loss:0.322, val_acc:0.940]
Epoch [29/120    avg_loss:0.345, val_acc:0.946]
Epoch [30/120    avg_loss:0.276, val_acc:0.935]
Epoch [31/120    avg_loss:0.313, val_acc:0.907]
Epoch [32/120    avg_loss:0.224, val_acc:0.929]
Epoch [33/120    avg_loss:0.251, val_acc:0.913]
Epoch [34/120    avg_loss:0.211, val_acc:0.958]
Epoch [35/120    avg_loss:0.259, val_acc:0.970]
Epoch [36/120    avg_loss:0.284, val_acc:0.952]
Epoch [37/120    avg_loss:0.235, val_acc:0.958]
Epoch [38/120    avg_loss:0.209, val_acc:0.968]
Epoch [39/120    avg_loss:0.202, val_acc:0.958]
Epoch [40/120    avg_loss:0.211, val_acc:0.964]
Epoch [41/120    avg_loss:0.186, val_acc:0.972]
Epoch [42/120    avg_loss:0.172, val_acc:0.974]
Epoch [43/120    avg_loss:0.204, val_acc:0.966]
Epoch [44/120    avg_loss:0.154, val_acc:0.962]
Epoch [45/120    avg_loss:0.191, val_acc:0.948]
Epoch [46/120    avg_loss:0.142, val_acc:0.901]
Epoch [47/120    avg_loss:0.132, val_acc:0.976]
Epoch [48/120    avg_loss:0.138, val_acc:0.972]
Epoch [49/120    avg_loss:0.154, val_acc:0.980]
Epoch [50/120    avg_loss:0.129, val_acc:0.974]
Epoch [51/120    avg_loss:0.087, val_acc:0.970]
Epoch [52/120    avg_loss:0.114, val_acc:0.968]
Epoch [53/120    avg_loss:0.117, val_acc:0.972]
Epoch [54/120    avg_loss:0.095, val_acc:0.960]
Epoch [55/120    avg_loss:0.088, val_acc:0.978]
Epoch [56/120    avg_loss:0.143, val_acc:0.984]
Epoch [57/120    avg_loss:0.112, val_acc:0.944]
Epoch [58/120    avg_loss:0.126, val_acc:0.978]
Epoch [59/120    avg_loss:0.090, val_acc:0.960]
Epoch [60/120    avg_loss:0.074, val_acc:0.988]
Epoch [61/120    avg_loss:0.060, val_acc:0.978]
Epoch [62/120    avg_loss:0.059, val_acc:0.986]
Epoch [63/120    avg_loss:0.113, val_acc:0.988]
Epoch [64/120    avg_loss:0.058, val_acc:0.968]
Epoch [65/120    avg_loss:0.129, val_acc:0.964]
Epoch [66/120    avg_loss:0.074, val_acc:0.988]
Epoch [67/120    avg_loss:0.046, val_acc:0.970]
Epoch [68/120    avg_loss:0.055, val_acc:0.984]
Epoch [69/120    avg_loss:0.068, val_acc:0.980]
Epoch [70/120    avg_loss:0.099, val_acc:0.976]
Epoch [71/120    avg_loss:0.078, val_acc:0.980]
Epoch [72/120    avg_loss:0.045, val_acc:0.984]
Epoch [73/120    avg_loss:0.066, val_acc:0.992]
Epoch [74/120    avg_loss:0.048, val_acc:0.996]
Epoch [75/120    avg_loss:0.029, val_acc:0.994]
Epoch [76/120    avg_loss:0.023, val_acc:0.994]
Epoch [77/120    avg_loss:0.024, val_acc:0.996]
Epoch [78/120    avg_loss:0.037, val_acc:0.988]
Epoch [79/120    avg_loss:0.044, val_acc:0.990]
Epoch [80/120    avg_loss:0.035, val_acc:0.992]
Epoch [81/120    avg_loss:0.022, val_acc:0.990]
Epoch [82/120    avg_loss:0.013, val_acc:0.994]
Epoch [83/120    avg_loss:0.021, val_acc:0.994]
Epoch [84/120    avg_loss:0.021, val_acc:0.990]
Epoch [85/120    avg_loss:0.030, val_acc:0.982]
Epoch [86/120    avg_loss:0.025, val_acc:0.992]
Epoch [87/120    avg_loss:0.019, val_acc:0.992]
Epoch [88/120    avg_loss:0.031, val_acc:0.982]
Epoch [89/120    avg_loss:0.060, val_acc:0.992]
Epoch [90/120    avg_loss:0.050, val_acc:0.988]
Epoch [91/120    avg_loss:0.051, val_acc:0.990]
Epoch [92/120    avg_loss:0.020, val_acc:0.992]
Epoch [93/120    avg_loss:0.016, val_acc:0.994]
Epoch [94/120    avg_loss:0.022, val_acc:0.994]
Epoch [95/120    avg_loss:0.019, val_acc:0.994]
Epoch [96/120    avg_loss:0.023, val_acc:0.994]
Epoch [97/120    avg_loss:0.013, val_acc:0.994]
Epoch [98/120    avg_loss:0.011, val_acc:0.994]
Epoch [99/120    avg_loss:0.014, val_acc:0.994]
Epoch [100/120    avg_loss:0.016, val_acc:0.996]
Epoch [101/120    avg_loss:0.016, val_acc:0.996]
Epoch [102/120    avg_loss:0.014, val_acc:0.996]
Epoch [103/120    avg_loss:0.012, val_acc:0.996]
Epoch [104/120    avg_loss:0.013, val_acc:0.996]
Epoch [105/120    avg_loss:0.013, val_acc:0.996]
Epoch [106/120    avg_loss:0.014, val_acc:0.996]
Epoch [107/120    avg_loss:0.011, val_acc:0.996]
Epoch [108/120    avg_loss:0.010, val_acc:0.996]
Epoch [109/120    avg_loss:0.011, val_acc:0.996]
Epoch [110/120    avg_loss:0.011, val_acc:0.996]
Epoch [111/120    avg_loss:0.011, val_acc:0.996]
Epoch [112/120    avg_loss:0.011, val_acc:0.996]
Epoch [113/120    avg_loss:0.010, val_acc:0.996]
Epoch [114/120    avg_loss:0.010, val_acc:0.996]
Epoch [115/120    avg_loss:0.010, val_acc:0.996]
Epoch [116/120    avg_loss:0.012, val_acc:0.996]
Epoch [117/120    avg_loss:0.012, val_acc:0.996]
Epoch [118/120    avg_loss:0.010, val_acc:0.996]
Epoch [119/120    avg_loss:0.010, val_acc:0.996]
Epoch [120/120    avg_loss:0.009, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 0.99780541 0.99319728 0.99122807 0.93162393 0.9
 0.99277108 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9909796206844961
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a617b5da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.579, val_acc:0.351]
Epoch [2/120    avg_loss:2.445, val_acc:0.425]
Epoch [3/120    avg_loss:2.314, val_acc:0.415]
Epoch [4/120    avg_loss:2.167, val_acc:0.486]
Epoch [5/120    avg_loss:2.018, val_acc:0.530]
Epoch [6/120    avg_loss:1.829, val_acc:0.619]
Epoch [7/120    avg_loss:1.665, val_acc:0.609]
Epoch [8/120    avg_loss:1.452, val_acc:0.647]
Epoch [9/120    avg_loss:1.269, val_acc:0.700]
Epoch [10/120    avg_loss:1.097, val_acc:0.776]
Epoch [11/120    avg_loss:0.993, val_acc:0.734]
Epoch [12/120    avg_loss:0.876, val_acc:0.792]
Epoch [13/120    avg_loss:0.821, val_acc:0.847]
Epoch [14/120    avg_loss:0.699, val_acc:0.845]
Epoch [15/120    avg_loss:0.649, val_acc:0.863]
Epoch [16/120    avg_loss:0.632, val_acc:0.883]
Epoch [17/120    avg_loss:0.537, val_acc:0.915]
Epoch [18/120    avg_loss:0.485, val_acc:0.853]
Epoch [19/120    avg_loss:0.449, val_acc:0.883]
Epoch [20/120    avg_loss:0.472, val_acc:0.915]
Epoch [21/120    avg_loss:0.439, val_acc:0.923]
Epoch [22/120    avg_loss:0.448, val_acc:0.855]
Epoch [23/120    avg_loss:0.416, val_acc:0.905]
Epoch [24/120    avg_loss:0.317, val_acc:0.907]
Epoch [25/120    avg_loss:0.344, val_acc:0.901]
Epoch [26/120    avg_loss:0.366, val_acc:0.925]
Epoch [27/120    avg_loss:0.327, val_acc:0.940]
Epoch [28/120    avg_loss:0.283, val_acc:0.915]
Epoch [29/120    avg_loss:0.282, val_acc:0.929]
Epoch [30/120    avg_loss:0.292, val_acc:0.931]
Epoch [31/120    avg_loss:0.244, val_acc:0.954]
Epoch [32/120    avg_loss:0.226, val_acc:0.946]
Epoch [33/120    avg_loss:0.254, val_acc:0.960]
Epoch [34/120    avg_loss:0.186, val_acc:0.958]
Epoch [35/120    avg_loss:0.276, val_acc:0.909]
Epoch [36/120    avg_loss:0.217, val_acc:0.948]
Epoch [37/120    avg_loss:0.187, val_acc:0.940]
Epoch [38/120    avg_loss:0.185, val_acc:0.960]
Epoch [39/120    avg_loss:0.245, val_acc:0.942]
Epoch [40/120    avg_loss:0.212, val_acc:0.960]
Epoch [41/120    avg_loss:0.142, val_acc:0.960]
Epoch [42/120    avg_loss:0.144, val_acc:0.968]
Epoch [43/120    avg_loss:0.155, val_acc:0.948]
Epoch [44/120    avg_loss:0.174, val_acc:0.970]
Epoch [45/120    avg_loss:0.149, val_acc:0.958]
Epoch [46/120    avg_loss:0.171, val_acc:0.976]
Epoch [47/120    avg_loss:0.148, val_acc:0.960]
Epoch [48/120    avg_loss:0.143, val_acc:0.935]
Epoch [49/120    avg_loss:0.127, val_acc:0.970]
Epoch [50/120    avg_loss:0.108, val_acc:0.970]
Epoch [51/120    avg_loss:0.146, val_acc:0.974]
Epoch [52/120    avg_loss:0.126, val_acc:0.952]
Epoch [53/120    avg_loss:0.135, val_acc:0.976]
Epoch [54/120    avg_loss:0.098, val_acc:0.984]
Epoch [55/120    avg_loss:0.095, val_acc:0.984]
Epoch [56/120    avg_loss:0.107, val_acc:0.968]
Epoch [57/120    avg_loss:0.097, val_acc:0.970]
Epoch [58/120    avg_loss:0.079, val_acc:0.976]
Epoch [59/120    avg_loss:0.068, val_acc:0.978]
Epoch [60/120    avg_loss:0.060, val_acc:0.980]
Epoch [61/120    avg_loss:0.144, val_acc:0.952]
Epoch [62/120    avg_loss:0.084, val_acc:0.982]
Epoch [63/120    avg_loss:0.048, val_acc:0.982]
Epoch [64/120    avg_loss:0.052, val_acc:0.982]
Epoch [65/120    avg_loss:0.044, val_acc:0.990]
Epoch [66/120    avg_loss:0.055, val_acc:0.968]
Epoch [67/120    avg_loss:0.053, val_acc:0.992]
Epoch [68/120    avg_loss:0.055, val_acc:0.970]
Epoch [69/120    avg_loss:0.042, val_acc:0.990]
Epoch [70/120    avg_loss:0.102, val_acc:0.976]
Epoch [71/120    avg_loss:0.067, val_acc:0.990]
Epoch [72/120    avg_loss:0.053, val_acc:0.990]
Epoch [73/120    avg_loss:0.034, val_acc:0.990]
Epoch [74/120    avg_loss:0.035, val_acc:0.984]
Epoch [75/120    avg_loss:0.045, val_acc:0.990]
Epoch [76/120    avg_loss:0.049, val_acc:0.986]
Epoch [77/120    avg_loss:0.116, val_acc:0.970]
Epoch [78/120    avg_loss:0.231, val_acc:0.962]
Epoch [79/120    avg_loss:0.092, val_acc:0.986]
Epoch [80/120    avg_loss:0.054, val_acc:0.982]
Epoch [81/120    avg_loss:0.043, val_acc:0.990]
Epoch [82/120    avg_loss:0.030, val_acc:0.990]
Epoch [83/120    avg_loss:0.063, val_acc:0.990]
Epoch [84/120    avg_loss:0.042, val_acc:0.990]
Epoch [85/120    avg_loss:0.036, val_acc:0.990]
Epoch [86/120    avg_loss:0.034, val_acc:0.990]
Epoch [87/120    avg_loss:0.032, val_acc:0.990]
Epoch [88/120    avg_loss:0.030, val_acc:0.990]
Epoch [89/120    avg_loss:0.029, val_acc:0.990]
Epoch [90/120    avg_loss:0.034, val_acc:0.992]
Epoch [91/120    avg_loss:0.026, val_acc:0.992]
Epoch [92/120    avg_loss:0.024, val_acc:0.992]
Epoch [93/120    avg_loss:0.031, val_acc:0.992]
Epoch [94/120    avg_loss:0.026, val_acc:0.992]
Epoch [95/120    avg_loss:0.025, val_acc:0.992]
Epoch [96/120    avg_loss:0.026, val_acc:0.992]
Epoch [97/120    avg_loss:0.023, val_acc:0.992]
Epoch [98/120    avg_loss:0.022, val_acc:0.992]
Epoch [99/120    avg_loss:0.022, val_acc:0.992]
Epoch [100/120    avg_loss:0.025, val_acc:0.992]
Epoch [101/120    avg_loss:0.026, val_acc:0.992]
Epoch [102/120    avg_loss:0.022, val_acc:0.992]
Epoch [103/120    avg_loss:0.025, val_acc:0.994]
Epoch [104/120    avg_loss:0.023, val_acc:0.992]
Epoch [105/120    avg_loss:0.023, val_acc:0.992]
Epoch [106/120    avg_loss:0.022, val_acc:0.992]
Epoch [107/120    avg_loss:0.021, val_acc:0.992]
Epoch [108/120    avg_loss:0.026, val_acc:0.992]
Epoch [109/120    avg_loss:0.023, val_acc:0.992]
Epoch [110/120    avg_loss:0.020, val_acc:0.992]
Epoch [111/120    avg_loss:0.020, val_acc:0.992]
Epoch [112/120    avg_loss:0.029, val_acc:0.992]
Epoch [113/120    avg_loss:0.018, val_acc:0.992]
Epoch [114/120    avg_loss:0.019, val_acc:0.992]
Epoch [115/120    avg_loss:0.019, val_acc:0.992]
Epoch [116/120    avg_loss:0.017, val_acc:0.992]
Epoch [117/120    avg_loss:0.018, val_acc:0.992]
Epoch [118/120    avg_loss:0.017, val_acc:0.992]
Epoch [119/120    avg_loss:0.018, val_acc:0.992]
Epoch [120/120    avg_loss:0.019, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   1 228   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  19   0   0   0   0   0   0   4   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.97321429 0.99563319 0.93577982 0.92459016
 1.         0.94382022 1.         1.         1.         1.
 0.99451153 1.        ]

Kappa:
0.990504191974058
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2cf22ece10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.576, val_acc:0.359]
Epoch [2/120    avg_loss:2.455, val_acc:0.334]
Epoch [3/120    avg_loss:2.343, val_acc:0.416]
Epoch [4/120    avg_loss:2.212, val_acc:0.500]
Epoch [5/120    avg_loss:2.071, val_acc:0.627]
Epoch [6/120    avg_loss:1.918, val_acc:0.660]
Epoch [7/120    avg_loss:1.743, val_acc:0.680]
Epoch [8/120    avg_loss:1.572, val_acc:0.676]
Epoch [9/120    avg_loss:1.403, val_acc:0.672]
Epoch [10/120    avg_loss:1.210, val_acc:0.748]
Epoch [11/120    avg_loss:1.044, val_acc:0.844]
Epoch [12/120    avg_loss:0.909, val_acc:0.842]
Epoch [13/120    avg_loss:0.861, val_acc:0.869]
Epoch [14/120    avg_loss:0.720, val_acc:0.871]
Epoch [15/120    avg_loss:0.694, val_acc:0.881]
Epoch [16/120    avg_loss:0.632, val_acc:0.904]
Epoch [17/120    avg_loss:0.614, val_acc:0.861]
Epoch [18/120    avg_loss:0.548, val_acc:0.906]
Epoch [19/120    avg_loss:0.522, val_acc:0.887]
Epoch [20/120    avg_loss:0.473, val_acc:0.912]
Epoch [21/120    avg_loss:0.432, val_acc:0.922]
Epoch [22/120    avg_loss:0.380, val_acc:0.926]
Epoch [23/120    avg_loss:0.403, val_acc:0.922]
Epoch [24/120    avg_loss:0.340, val_acc:0.955]
Epoch [25/120    avg_loss:0.343, val_acc:0.955]
Epoch [26/120    avg_loss:0.306, val_acc:0.910]
Epoch [27/120    avg_loss:0.330, val_acc:0.936]
Epoch [28/120    avg_loss:0.277, val_acc:0.973]
Epoch [29/120    avg_loss:0.217, val_acc:0.967]
Epoch [30/120    avg_loss:0.296, val_acc:0.947]
Epoch [31/120    avg_loss:0.219, val_acc:0.920]
Epoch [32/120    avg_loss:0.273, val_acc:0.965]
Epoch [33/120    avg_loss:0.241, val_acc:0.934]
Epoch [34/120    avg_loss:0.238, val_acc:0.963]
Epoch [35/120    avg_loss:0.211, val_acc:0.963]
Epoch [36/120    avg_loss:0.162, val_acc:0.947]
Epoch [37/120    avg_loss:0.252, val_acc:0.947]
Epoch [38/120    avg_loss:0.207, val_acc:0.980]
Epoch [39/120    avg_loss:0.168, val_acc:0.977]
Epoch [40/120    avg_loss:0.164, val_acc:0.982]
Epoch [41/120    avg_loss:0.134, val_acc:0.973]
Epoch [42/120    avg_loss:0.149, val_acc:0.965]
Epoch [43/120    avg_loss:0.172, val_acc:0.986]
Epoch [44/120    avg_loss:0.142, val_acc:0.980]
Epoch [45/120    avg_loss:0.157, val_acc:0.984]
Epoch [46/120    avg_loss:0.182, val_acc:0.947]
Epoch [47/120    avg_loss:0.086, val_acc:0.990]
Epoch [48/120    avg_loss:0.102, val_acc:0.947]
Epoch [49/120    avg_loss:0.118, val_acc:0.945]
Epoch [50/120    avg_loss:0.135, val_acc:0.971]
Epoch [51/120    avg_loss:0.079, val_acc:0.982]
Epoch [52/120    avg_loss:0.095, val_acc:0.980]
Epoch [53/120    avg_loss:0.101, val_acc:0.994]
Epoch [54/120    avg_loss:0.072, val_acc:0.988]
Epoch [55/120    avg_loss:0.087, val_acc:0.994]
Epoch [56/120    avg_loss:0.066, val_acc:0.984]
Epoch [57/120    avg_loss:0.108, val_acc:0.980]
Epoch [58/120    avg_loss:0.083, val_acc:0.994]
Epoch [59/120    avg_loss:0.085, val_acc:0.957]
Epoch [60/120    avg_loss:0.065, val_acc:0.992]
Epoch [61/120    avg_loss:0.068, val_acc:0.992]
Epoch [62/120    avg_loss:0.061, val_acc:0.996]
Epoch [63/120    avg_loss:0.062, val_acc:0.980]
Epoch [64/120    avg_loss:0.076, val_acc:0.994]
Epoch [65/120    avg_loss:0.067, val_acc:0.984]
Epoch [66/120    avg_loss:0.044, val_acc:0.996]
Epoch [67/120    avg_loss:0.068, val_acc:0.988]
Epoch [68/120    avg_loss:0.061, val_acc:0.982]
Epoch [69/120    avg_loss:0.096, val_acc:0.969]
Epoch [70/120    avg_loss:0.123, val_acc:0.953]
Epoch [71/120    avg_loss:0.065, val_acc:0.994]
Epoch [72/120    avg_loss:0.045, val_acc:0.998]
Epoch [73/120    avg_loss:0.028, val_acc:0.994]
Epoch [74/120    avg_loss:0.040, val_acc:0.990]
Epoch [75/120    avg_loss:0.039, val_acc:0.990]
Epoch [76/120    avg_loss:0.036, val_acc:0.994]
Epoch [77/120    avg_loss:0.040, val_acc:0.990]
Epoch [78/120    avg_loss:0.037, val_acc:0.996]
Epoch [79/120    avg_loss:0.028, val_acc:1.000]
Epoch [80/120    avg_loss:0.063, val_acc:0.984]
Epoch [81/120    avg_loss:0.038, val_acc:0.994]
Epoch [82/120    avg_loss:0.038, val_acc:0.990]
Epoch [83/120    avg_loss:0.052, val_acc:0.996]
Epoch [84/120    avg_loss:0.028, val_acc:0.998]
Epoch [85/120    avg_loss:0.020, val_acc:0.996]
Epoch [86/120    avg_loss:0.014, val_acc:0.996]
Epoch [87/120    avg_loss:0.019, val_acc:0.996]
Epoch [88/120    avg_loss:0.027, val_acc:0.984]
Epoch [89/120    avg_loss:0.023, val_acc:1.000]
Epoch [90/120    avg_loss:0.020, val_acc:1.000]
Epoch [91/120    avg_loss:0.019, val_acc:0.994]
Epoch [92/120    avg_loss:0.027, val_acc:0.994]
Epoch [93/120    avg_loss:0.026, val_acc:0.994]
Epoch [94/120    avg_loss:0.020, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.996]
Epoch [96/120    avg_loss:0.010, val_acc:0.996]
Epoch [97/120    avg_loss:0.011, val_acc:0.998]
Epoch [98/120    avg_loss:0.012, val_acc:0.996]
Epoch [99/120    avg_loss:0.023, val_acc:1.000]
Epoch [100/120    avg_loss:0.014, val_acc:1.000]
Epoch [101/120    avg_loss:0.016, val_acc:0.994]
Epoch [102/120    avg_loss:0.022, val_acc:0.996]
Epoch [103/120    avg_loss:0.014, val_acc:0.994]
Epoch [104/120    avg_loss:0.013, val_acc:0.996]
Epoch [105/120    avg_loss:0.014, val_acc:0.998]
Epoch [106/120    avg_loss:0.018, val_acc:0.994]
Epoch [107/120    avg_loss:0.037, val_acc:0.990]
Epoch [108/120    avg_loss:0.132, val_acc:0.992]
Epoch [109/120    avg_loss:0.125, val_acc:0.965]
Epoch [110/120    avg_loss:0.157, val_acc:0.959]
Epoch [111/120    avg_loss:0.087, val_acc:0.984]
Epoch [112/120    avg_loss:0.041, val_acc:1.000]
Epoch [113/120    avg_loss:0.025, val_acc:0.996]
Epoch [114/120    avg_loss:0.022, val_acc:0.994]
Epoch [115/120    avg_loss:0.017, val_acc:0.998]
Epoch [116/120    avg_loss:0.017, val_acc:1.000]
Epoch [117/120    avg_loss:0.026, val_acc:0.980]
Epoch [118/120    avg_loss:0.025, val_acc:0.994]
Epoch [119/120    avg_loss:0.036, val_acc:0.992]
Epoch [120/120    avg_loss:0.043, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  47 406   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.31556503198294

F1 scores:
[       nan 0.99927061 0.98426966 1.         0.94827586 0.91428571
 0.99756691 0.96132597 1.         1.         1.         0.94132335
 0.94528522 1.        ]

Kappa:
0.9812481919291157
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c5077cd68>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.359]
Epoch [2/120    avg_loss:2.488, val_acc:0.317]
Epoch [3/120    avg_loss:2.370, val_acc:0.385]
Epoch [4/120    avg_loss:2.248, val_acc:0.464]
Epoch [5/120    avg_loss:2.127, val_acc:0.468]
Epoch [6/120    avg_loss:2.005, val_acc:0.492]
Epoch [7/120    avg_loss:1.843, val_acc:0.542]
Epoch [8/120    avg_loss:1.678, val_acc:0.607]
Epoch [9/120    avg_loss:1.539, val_acc:0.637]
Epoch [10/120    avg_loss:1.331, val_acc:0.655]
Epoch [11/120    avg_loss:1.224, val_acc:0.720]
Epoch [12/120    avg_loss:1.131, val_acc:0.694]
Epoch [13/120    avg_loss:1.066, val_acc:0.730]
Epoch [14/120    avg_loss:0.955, val_acc:0.764]
Epoch [15/120    avg_loss:0.833, val_acc:0.823]
Epoch [16/120    avg_loss:0.791, val_acc:0.849]
Epoch [17/120    avg_loss:0.747, val_acc:0.806]
Epoch [18/120    avg_loss:0.758, val_acc:0.762]
Epoch [19/120    avg_loss:0.681, val_acc:0.839]
Epoch [20/120    avg_loss:0.593, val_acc:0.867]
Epoch [21/120    avg_loss:0.550, val_acc:0.825]
Epoch [22/120    avg_loss:0.539, val_acc:0.879]
Epoch [23/120    avg_loss:0.501, val_acc:0.891]
Epoch [24/120    avg_loss:0.412, val_acc:0.897]
Epoch [25/120    avg_loss:0.472, val_acc:0.921]
Epoch [26/120    avg_loss:0.458, val_acc:0.875]
Epoch [27/120    avg_loss:0.413, val_acc:0.897]
Epoch [28/120    avg_loss:0.366, val_acc:0.927]
Epoch [29/120    avg_loss:0.314, val_acc:0.938]
Epoch [30/120    avg_loss:0.306, val_acc:0.942]
Epoch [31/120    avg_loss:0.323, val_acc:0.946]
Epoch [32/120    avg_loss:0.240, val_acc:0.952]
Epoch [33/120    avg_loss:0.262, val_acc:0.937]
Epoch [34/120    avg_loss:0.259, val_acc:0.942]
Epoch [35/120    avg_loss:0.283, val_acc:0.927]
Epoch [36/120    avg_loss:0.247, val_acc:0.966]
Epoch [37/120    avg_loss:0.226, val_acc:0.962]
Epoch [38/120    avg_loss:0.212, val_acc:0.962]
Epoch [39/120    avg_loss:0.193, val_acc:0.958]
Epoch [40/120    avg_loss:0.186, val_acc:0.964]
Epoch [41/120    avg_loss:0.186, val_acc:0.966]
Epoch [42/120    avg_loss:0.354, val_acc:0.921]
Epoch [43/120    avg_loss:0.154, val_acc:0.978]
Epoch [44/120    avg_loss:0.158, val_acc:0.974]
Epoch [45/120    avg_loss:0.186, val_acc:0.982]
Epoch [46/120    avg_loss:0.159, val_acc:0.970]
Epoch [47/120    avg_loss:0.129, val_acc:0.980]
Epoch [48/120    avg_loss:0.164, val_acc:0.960]
Epoch [49/120    avg_loss:0.103, val_acc:0.980]
Epoch [50/120    avg_loss:0.151, val_acc:0.972]
Epoch [51/120    avg_loss:0.108, val_acc:0.982]
Epoch [52/120    avg_loss:0.093, val_acc:0.986]
Epoch [53/120    avg_loss:0.094, val_acc:0.986]
Epoch [54/120    avg_loss:0.109, val_acc:0.938]
Epoch [55/120    avg_loss:0.160, val_acc:0.980]
Epoch [56/120    avg_loss:0.121, val_acc:0.992]
Epoch [57/120    avg_loss:0.098, val_acc:0.923]
Epoch [58/120    avg_loss:0.107, val_acc:0.984]
Epoch [59/120    avg_loss:0.101, val_acc:0.978]
Epoch [60/120    avg_loss:0.067, val_acc:0.984]
Epoch [61/120    avg_loss:0.092, val_acc:0.968]
Epoch [62/120    avg_loss:0.164, val_acc:0.974]
Epoch [63/120    avg_loss:0.086, val_acc:0.988]
Epoch [64/120    avg_loss:0.121, val_acc:0.980]
Epoch [65/120    avg_loss:0.110, val_acc:0.972]
Epoch [66/120    avg_loss:0.117, val_acc:0.986]
Epoch [67/120    avg_loss:0.126, val_acc:0.976]
Epoch [68/120    avg_loss:0.081, val_acc:0.990]
Epoch [69/120    avg_loss:0.079, val_acc:0.986]
Epoch [70/120    avg_loss:0.056, val_acc:0.986]
Epoch [71/120    avg_loss:0.039, val_acc:0.986]
Epoch [72/120    avg_loss:0.050, val_acc:0.990]
Epoch [73/120    avg_loss:0.047, val_acc:0.990]
Epoch [74/120    avg_loss:0.041, val_acc:0.990]
Epoch [75/120    avg_loss:0.046, val_acc:0.992]
Epoch [76/120    avg_loss:0.037, val_acc:0.992]
Epoch [77/120    avg_loss:0.043, val_acc:0.992]
Epoch [78/120    avg_loss:0.034, val_acc:0.992]
Epoch [79/120    avg_loss:0.042, val_acc:0.992]
Epoch [80/120    avg_loss:0.037, val_acc:0.992]
Epoch [81/120    avg_loss:0.039, val_acc:0.992]
Epoch [82/120    avg_loss:0.033, val_acc:0.992]
Epoch [83/120    avg_loss:0.041, val_acc:0.994]
Epoch [84/120    avg_loss:0.042, val_acc:0.994]
Epoch [85/120    avg_loss:0.031, val_acc:0.994]
Epoch [86/120    avg_loss:0.039, val_acc:0.994]
Epoch [87/120    avg_loss:0.040, val_acc:0.994]
Epoch [88/120    avg_loss:0.042, val_acc:0.994]
Epoch [89/120    avg_loss:0.035, val_acc:0.994]
Epoch [90/120    avg_loss:0.034, val_acc:0.992]
Epoch [91/120    avg_loss:0.033, val_acc:0.994]
Epoch [92/120    avg_loss:0.034, val_acc:0.992]
Epoch [93/120    avg_loss:0.033, val_acc:0.994]
Epoch [94/120    avg_loss:0.031, val_acc:0.994]
Epoch [95/120    avg_loss:0.034, val_acc:0.994]
Epoch [96/120    avg_loss:0.029, val_acc:0.994]
Epoch [97/120    avg_loss:0.029, val_acc:0.996]
Epoch [98/120    avg_loss:0.036, val_acc:0.996]
Epoch [99/120    avg_loss:0.028, val_acc:0.994]
Epoch [100/120    avg_loss:0.025, val_acc:0.994]
Epoch [101/120    avg_loss:0.030, val_acc:0.992]
Epoch [102/120    avg_loss:0.031, val_acc:0.992]
Epoch [103/120    avg_loss:0.034, val_acc:0.994]
Epoch [104/120    avg_loss:0.032, val_acc:0.994]
Epoch [105/120    avg_loss:0.031, val_acc:0.994]
Epoch [106/120    avg_loss:0.029, val_acc:0.992]
Epoch [107/120    avg_loss:0.031, val_acc:0.994]
Epoch [108/120    avg_loss:0.030, val_acc:0.992]
Epoch [109/120    avg_loss:0.029, val_acc:0.992]
Epoch [110/120    avg_loss:0.030, val_acc:0.992]
Epoch [111/120    avg_loss:0.027, val_acc:0.992]
Epoch [112/120    avg_loss:0.026, val_acc:0.992]
Epoch [113/120    avg_loss:0.028, val_acc:0.992]
Epoch [114/120    avg_loss:0.029, val_acc:0.992]
Epoch [115/120    avg_loss:0.033, val_acc:0.992]
Epoch [116/120    avg_loss:0.030, val_acc:0.992]
Epoch [117/120    avg_loss:0.033, val_acc:0.992]
Epoch [118/120    avg_loss:0.035, val_acc:0.992]
Epoch [119/120    avg_loss:0.030, val_acc:0.992]
Epoch [120/120    avg_loss:0.025, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   1   0   0   0   0   0   0   5   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 0.99926954 0.98648649 0.99782135 0.93446089 0.90636704
 0.99757869 0.96703297 1.         1.         1.         1.
 0.99451153 1.        ]

Kappa:
0.9909780051033854
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb711ad4dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.596, val_acc:0.329]
Epoch [2/120    avg_loss:2.462, val_acc:0.325]
Epoch [3/120    avg_loss:2.339, val_acc:0.343]
Epoch [4/120    avg_loss:2.235, val_acc:0.401]
Epoch [5/120    avg_loss:2.133, val_acc:0.452]
Epoch [6/120    avg_loss:1.994, val_acc:0.460]
Epoch [7/120    avg_loss:1.868, val_acc:0.508]
Epoch [8/120    avg_loss:1.730, val_acc:0.573]
Epoch [9/120    avg_loss:1.580, val_acc:0.613]
Epoch [10/120    avg_loss:1.417, val_acc:0.702]
Epoch [11/120    avg_loss:1.249, val_acc:0.750]
Epoch [12/120    avg_loss:1.087, val_acc:0.833]
Epoch [13/120    avg_loss:1.026, val_acc:0.829]
Epoch [14/120    avg_loss:0.867, val_acc:0.804]
Epoch [15/120    avg_loss:0.745, val_acc:0.839]
Epoch [16/120    avg_loss:0.746, val_acc:0.841]
Epoch [17/120    avg_loss:0.699, val_acc:0.817]
Epoch [18/120    avg_loss:0.601, val_acc:0.859]
Epoch [19/120    avg_loss:0.566, val_acc:0.875]
Epoch [20/120    avg_loss:0.507, val_acc:0.883]
Epoch [21/120    avg_loss:0.449, val_acc:0.897]
Epoch [22/120    avg_loss:0.432, val_acc:0.808]
Epoch [23/120    avg_loss:0.550, val_acc:0.915]
Epoch [24/120    avg_loss:0.368, val_acc:0.907]
Epoch [25/120    avg_loss:0.363, val_acc:0.913]
Epoch [26/120    avg_loss:0.343, val_acc:0.877]
Epoch [27/120    avg_loss:0.378, val_acc:0.901]
Epoch [28/120    avg_loss:0.456, val_acc:0.911]
Epoch [29/120    avg_loss:0.304, val_acc:0.891]
Epoch [30/120    avg_loss:0.372, val_acc:0.909]
Epoch [31/120    avg_loss:0.343, val_acc:0.938]
Epoch [32/120    avg_loss:0.268, val_acc:0.907]
Epoch [33/120    avg_loss:0.261, val_acc:0.940]
Epoch [34/120    avg_loss:0.265, val_acc:0.958]
Epoch [35/120    avg_loss:0.266, val_acc:0.935]
Epoch [36/120    avg_loss:0.297, val_acc:0.927]
Epoch [37/120    avg_loss:0.255, val_acc:0.889]
Epoch [38/120    avg_loss:0.234, val_acc:0.948]
Epoch [39/120    avg_loss:0.179, val_acc:0.958]
Epoch [40/120    avg_loss:0.180, val_acc:0.958]
Epoch [41/120    avg_loss:0.203, val_acc:0.964]
Epoch [42/120    avg_loss:0.138, val_acc:0.970]
Epoch [43/120    avg_loss:0.188, val_acc:0.974]
Epoch [44/120    avg_loss:0.132, val_acc:0.968]
Epoch [45/120    avg_loss:0.193, val_acc:0.966]
Epoch [46/120    avg_loss:0.132, val_acc:0.968]
Epoch [47/120    avg_loss:0.159, val_acc:0.948]
Epoch [48/120    avg_loss:0.151, val_acc:0.958]
Epoch [49/120    avg_loss:0.137, val_acc:0.972]
Epoch [50/120    avg_loss:0.134, val_acc:0.948]
Epoch [51/120    avg_loss:0.167, val_acc:0.921]
Epoch [52/120    avg_loss:0.247, val_acc:0.958]
Epoch [53/120    avg_loss:0.108, val_acc:0.964]
Epoch [54/120    avg_loss:0.114, val_acc:0.972]
Epoch [55/120    avg_loss:0.083, val_acc:0.980]
Epoch [56/120    avg_loss:0.089, val_acc:0.982]
Epoch [57/120    avg_loss:0.084, val_acc:0.978]
Epoch [58/120    avg_loss:0.061, val_acc:0.976]
Epoch [59/120    avg_loss:0.057, val_acc:0.984]
Epoch [60/120    avg_loss:0.052, val_acc:0.978]
Epoch [61/120    avg_loss:0.098, val_acc:0.964]
Epoch [62/120    avg_loss:0.120, val_acc:0.972]
Epoch [63/120    avg_loss:0.113, val_acc:0.978]
Epoch [64/120    avg_loss:0.116, val_acc:0.976]
Epoch [65/120    avg_loss:0.076, val_acc:0.972]
Epoch [66/120    avg_loss:0.065, val_acc:0.984]
Epoch [67/120    avg_loss:0.065, val_acc:0.984]
Epoch [68/120    avg_loss:0.068, val_acc:0.984]
Epoch [69/120    avg_loss:0.040, val_acc:0.986]
Epoch [70/120    avg_loss:0.047, val_acc:0.980]
Epoch [71/120    avg_loss:0.042, val_acc:0.982]
Epoch [72/120    avg_loss:0.037, val_acc:0.976]
Epoch [73/120    avg_loss:0.077, val_acc:0.968]
Epoch [74/120    avg_loss:0.175, val_acc:0.968]
Epoch [75/120    avg_loss:0.102, val_acc:0.952]
Epoch [76/120    avg_loss:0.118, val_acc:0.980]
Epoch [77/120    avg_loss:0.066, val_acc:0.978]
Epoch [78/120    avg_loss:0.054, val_acc:0.976]
Epoch [79/120    avg_loss:0.089, val_acc:0.980]
Epoch [80/120    avg_loss:0.034, val_acc:0.982]
Epoch [81/120    avg_loss:0.038, val_acc:0.984]
Epoch [82/120    avg_loss:0.027, val_acc:0.986]
Epoch [83/120    avg_loss:0.026, val_acc:0.986]
Epoch [84/120    avg_loss:0.055, val_acc:0.986]
Epoch [85/120    avg_loss:0.049, val_acc:0.984]
Epoch [86/120    avg_loss:0.029, val_acc:0.976]
Epoch [87/120    avg_loss:0.031, val_acc:0.986]
Epoch [88/120    avg_loss:0.024, val_acc:0.984]
Epoch [89/120    avg_loss:0.029, val_acc:0.988]
Epoch [90/120    avg_loss:0.022, val_acc:0.986]
Epoch [91/120    avg_loss:0.021, val_acc:0.986]
Epoch [92/120    avg_loss:0.016, val_acc:0.986]
Epoch [93/120    avg_loss:0.051, val_acc:0.968]
Epoch [94/120    avg_loss:0.047, val_acc:0.984]
Epoch [95/120    avg_loss:0.026, val_acc:0.984]
Epoch [96/120    avg_loss:0.019, val_acc:0.974]
Epoch [97/120    avg_loss:0.043, val_acc:0.974]
Epoch [98/120    avg_loss:0.046, val_acc:0.984]
Epoch [99/120    avg_loss:0.034, val_acc:0.978]
Epoch [100/120    avg_loss:0.029, val_acc:0.986]
Epoch [101/120    avg_loss:0.024, val_acc:0.984]
Epoch [102/120    avg_loss:0.018, val_acc:0.988]
Epoch [103/120    avg_loss:0.043, val_acc:0.986]
Epoch [104/120    avg_loss:0.014, val_acc:0.986]
Epoch [105/120    avg_loss:0.013, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.986]
Epoch [107/120    avg_loss:0.014, val_acc:0.988]
Epoch [108/120    avg_loss:0.016, val_acc:0.988]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.986]
Epoch [115/120    avg_loss:0.019, val_acc:0.988]
Epoch [116/120    avg_loss:0.020, val_acc:0.990]
Epoch [117/120    avg_loss:0.068, val_acc:0.925]
Epoch [118/120    avg_loss:0.230, val_acc:0.964]
Epoch [119/120    avg_loss:0.111, val_acc:0.980]
Epoch [120/120    avg_loss:0.059, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 218  12   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  12   0   0   0   0   0   0   1   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.98871332 0.97321429 0.9106383  0.89824561
 1.         0.9726776  1.         1.         1.         0.9973545
 0.99668508 1.        ]

Kappa:
0.9883676769728039
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa11bd3ed68>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.622, val_acc:0.161]
Epoch [2/120    avg_loss:2.520, val_acc:0.333]
Epoch [3/120    avg_loss:2.412, val_acc:0.353]
Epoch [4/120    avg_loss:2.288, val_acc:0.403]
Epoch [5/120    avg_loss:2.175, val_acc:0.492]
Epoch [6/120    avg_loss:2.053, val_acc:0.526]
Epoch [7/120    avg_loss:1.897, val_acc:0.577]
Epoch [8/120    avg_loss:1.756, val_acc:0.633]
Epoch [9/120    avg_loss:1.565, val_acc:0.710]
Epoch [10/120    avg_loss:1.374, val_acc:0.732]
Epoch [11/120    avg_loss:1.192, val_acc:0.774]
Epoch [12/120    avg_loss:1.052, val_acc:0.758]
Epoch [13/120    avg_loss:0.895, val_acc:0.780]
Epoch [14/120    avg_loss:0.872, val_acc:0.782]
Epoch [15/120    avg_loss:0.757, val_acc:0.821]
Epoch [16/120    avg_loss:0.734, val_acc:0.833]
Epoch [17/120    avg_loss:0.663, val_acc:0.873]
Epoch [18/120    avg_loss:0.678, val_acc:0.883]
Epoch [19/120    avg_loss:0.557, val_acc:0.873]
Epoch [20/120    avg_loss:0.516, val_acc:0.879]
Epoch [21/120    avg_loss:0.506, val_acc:0.867]
Epoch [22/120    avg_loss:0.462, val_acc:0.895]
Epoch [23/120    avg_loss:0.449, val_acc:0.869]
Epoch [24/120    avg_loss:0.486, val_acc:0.869]
Epoch [25/120    avg_loss:0.427, val_acc:0.889]
Epoch [26/120    avg_loss:0.449, val_acc:0.885]
Epoch [27/120    avg_loss:0.431, val_acc:0.887]
Epoch [28/120    avg_loss:0.385, val_acc:0.942]
Epoch [29/120    avg_loss:0.332, val_acc:0.921]
Epoch [30/120    avg_loss:0.339, val_acc:0.923]
Epoch [31/120    avg_loss:0.376, val_acc:0.923]
Epoch [32/120    avg_loss:0.309, val_acc:0.919]
Epoch [33/120    avg_loss:0.340, val_acc:0.893]
Epoch [34/120    avg_loss:0.302, val_acc:0.958]
Epoch [35/120    avg_loss:0.271, val_acc:0.913]
Epoch [36/120    avg_loss:0.282, val_acc:0.899]
Epoch [37/120    avg_loss:0.245, val_acc:0.919]
Epoch [38/120    avg_loss:0.201, val_acc:0.940]
Epoch [39/120    avg_loss:0.232, val_acc:0.933]
Epoch [40/120    avg_loss:0.195, val_acc:0.935]
Epoch [41/120    avg_loss:0.294, val_acc:0.948]
Epoch [42/120    avg_loss:0.198, val_acc:0.948]
Epoch [43/120    avg_loss:0.183, val_acc:0.954]
Epoch [44/120    avg_loss:0.176, val_acc:0.972]
Epoch [45/120    avg_loss:0.151, val_acc:0.919]
Epoch [46/120    avg_loss:0.170, val_acc:0.952]
Epoch [47/120    avg_loss:0.139, val_acc:0.950]
Epoch [48/120    avg_loss:0.166, val_acc:0.974]
Epoch [49/120    avg_loss:0.189, val_acc:0.968]
Epoch [50/120    avg_loss:0.149, val_acc:0.964]
Epoch [51/120    avg_loss:0.173, val_acc:0.964]
Epoch [52/120    avg_loss:0.228, val_acc:0.948]
Epoch [53/120    avg_loss:0.182, val_acc:0.954]
Epoch [54/120    avg_loss:0.110, val_acc:0.968]
Epoch [55/120    avg_loss:0.106, val_acc:0.954]
Epoch [56/120    avg_loss:0.108, val_acc:0.982]
Epoch [57/120    avg_loss:0.094, val_acc:0.974]
Epoch [58/120    avg_loss:0.108, val_acc:0.958]
Epoch [59/120    avg_loss:0.069, val_acc:0.972]
Epoch [60/120    avg_loss:0.084, val_acc:0.984]
Epoch [61/120    avg_loss:0.083, val_acc:0.970]
Epoch [62/120    avg_loss:0.092, val_acc:0.954]
Epoch [63/120    avg_loss:0.149, val_acc:0.962]
Epoch [64/120    avg_loss:0.068, val_acc:0.972]
Epoch [65/120    avg_loss:0.060, val_acc:0.972]
Epoch [66/120    avg_loss:0.063, val_acc:0.978]
Epoch [67/120    avg_loss:0.101, val_acc:0.972]
Epoch [68/120    avg_loss:0.042, val_acc:0.978]
Epoch [69/120    avg_loss:0.074, val_acc:0.956]
Epoch [70/120    avg_loss:0.109, val_acc:0.982]
Epoch [71/120    avg_loss:0.067, val_acc:0.966]
Epoch [72/120    avg_loss:0.042, val_acc:0.974]
Epoch [73/120    avg_loss:0.034, val_acc:0.972]
Epoch [74/120    avg_loss:0.036, val_acc:0.976]
Epoch [75/120    avg_loss:0.028, val_acc:0.980]
Epoch [76/120    avg_loss:0.027, val_acc:0.980]
Epoch [77/120    avg_loss:0.024, val_acc:0.980]
Epoch [78/120    avg_loss:0.022, val_acc:0.980]
Epoch [79/120    avg_loss:0.022, val_acc:0.980]
Epoch [80/120    avg_loss:0.037, val_acc:0.984]
Epoch [81/120    avg_loss:0.024, val_acc:0.984]
Epoch [82/120    avg_loss:0.026, val_acc:0.984]
Epoch [83/120    avg_loss:0.021, val_acc:0.980]
Epoch [84/120    avg_loss:0.023, val_acc:0.978]
Epoch [85/120    avg_loss:0.021, val_acc:0.982]
Epoch [86/120    avg_loss:0.022, val_acc:0.980]
Epoch [87/120    avg_loss:0.021, val_acc:0.982]
Epoch [88/120    avg_loss:0.022, val_acc:0.982]
Epoch [89/120    avg_loss:0.019, val_acc:0.982]
Epoch [90/120    avg_loss:0.028, val_acc:0.982]
Epoch [91/120    avg_loss:0.025, val_acc:0.984]
Epoch [92/120    avg_loss:0.020, val_acc:0.984]
Epoch [93/120    avg_loss:0.019, val_acc:0.984]
Epoch [94/120    avg_loss:0.018, val_acc:0.984]
Epoch [95/120    avg_loss:0.019, val_acc:0.984]
Epoch [96/120    avg_loss:0.019, val_acc:0.984]
Epoch [97/120    avg_loss:0.019, val_acc:0.984]
Epoch [98/120    avg_loss:0.018, val_acc:0.982]
Epoch [99/120    avg_loss:0.018, val_acc:0.984]
Epoch [100/120    avg_loss:0.020, val_acc:0.982]
Epoch [101/120    avg_loss:0.018, val_acc:0.984]
Epoch [102/120    avg_loss:0.022, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.984]
Epoch [104/120    avg_loss:0.023, val_acc:0.984]
Epoch [105/120    avg_loss:0.016, val_acc:0.982]
Epoch [106/120    avg_loss:0.019, val_acc:0.982]
Epoch [107/120    avg_loss:0.019, val_acc:0.982]
Epoch [108/120    avg_loss:0.020, val_acc:0.984]
Epoch [109/120    avg_loss:0.018, val_acc:0.984]
Epoch [110/120    avg_loss:0.019, val_acc:0.984]
Epoch [111/120    avg_loss:0.021, val_acc:0.984]
Epoch [112/120    avg_loss:0.018, val_acc:0.984]
Epoch [113/120    avg_loss:0.020, val_acc:0.984]
Epoch [114/120    avg_loss:0.018, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.984]
Epoch [116/120    avg_loss:0.017, val_acc:0.984]
Epoch [117/120    avg_loss:0.018, val_acc:0.984]
Epoch [118/120    avg_loss:0.016, val_acc:0.984]
Epoch [119/120    avg_loss:0.018, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99853801 0.99319728 1.         0.94666667 0.92567568
 0.99029126 0.98378378 1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9926416162430033
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f914b5aee10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.208]
Epoch [2/120    avg_loss:2.436, val_acc:0.343]
Epoch [3/120    avg_loss:2.314, val_acc:0.335]
Epoch [4/120    avg_loss:2.177, val_acc:0.415]
Epoch [5/120    avg_loss:2.045, val_acc:0.534]
Epoch [6/120    avg_loss:1.879, val_acc:0.603]
Epoch [7/120    avg_loss:1.687, val_acc:0.641]
Epoch [8/120    avg_loss:1.493, val_acc:0.706]
Epoch [9/120    avg_loss:1.289, val_acc:0.770]
Epoch [10/120    avg_loss:1.064, val_acc:0.839]
Epoch [11/120    avg_loss:0.938, val_acc:0.853]
Epoch [12/120    avg_loss:0.849, val_acc:0.847]
Epoch [13/120    avg_loss:0.729, val_acc:0.825]
Epoch [14/120    avg_loss:0.727, val_acc:0.891]
Epoch [15/120    avg_loss:0.642, val_acc:0.903]
Epoch [16/120    avg_loss:0.523, val_acc:0.911]
Epoch [17/120    avg_loss:0.536, val_acc:0.871]
Epoch [18/120    avg_loss:0.493, val_acc:0.907]
Epoch [19/120    avg_loss:0.470, val_acc:0.927]
Epoch [20/120    avg_loss:0.381, val_acc:0.938]
Epoch [21/120    avg_loss:0.360, val_acc:0.917]
Epoch [22/120    avg_loss:0.330, val_acc:0.931]
Epoch [23/120    avg_loss:0.328, val_acc:0.933]
Epoch [24/120    avg_loss:0.314, val_acc:0.919]
Epoch [25/120    avg_loss:0.322, val_acc:0.946]
Epoch [26/120    avg_loss:0.278, val_acc:0.940]
Epoch [27/120    avg_loss:0.263, val_acc:0.960]
Epoch [28/120    avg_loss:0.219, val_acc:0.940]
Epoch [29/120    avg_loss:0.280, val_acc:0.931]
Epoch [30/120    avg_loss:0.330, val_acc:0.950]
Epoch [31/120    avg_loss:0.276, val_acc:0.931]
Epoch [32/120    avg_loss:0.238, val_acc:0.952]
Epoch [33/120    avg_loss:0.192, val_acc:0.972]
Epoch [34/120    avg_loss:0.203, val_acc:0.968]
Epoch [35/120    avg_loss:0.144, val_acc:0.976]
Epoch [36/120    avg_loss:0.170, val_acc:0.954]
Epoch [37/120    avg_loss:0.168, val_acc:0.954]
Epoch [38/120    avg_loss:0.158, val_acc:0.950]
Epoch [39/120    avg_loss:0.142, val_acc:0.964]
Epoch [40/120    avg_loss:0.130, val_acc:0.980]
Epoch [41/120    avg_loss:0.111, val_acc:0.970]
Epoch [42/120    avg_loss:0.136, val_acc:0.946]
Epoch [43/120    avg_loss:0.121, val_acc:0.982]
Epoch [44/120    avg_loss:0.098, val_acc:0.974]
Epoch [45/120    avg_loss:0.101, val_acc:0.954]
Epoch [46/120    avg_loss:0.108, val_acc:0.972]
Epoch [47/120    avg_loss:0.115, val_acc:0.970]
Epoch [48/120    avg_loss:0.136, val_acc:0.974]
Epoch [49/120    avg_loss:0.136, val_acc:0.956]
Epoch [50/120    avg_loss:0.211, val_acc:0.944]
Epoch [51/120    avg_loss:0.131, val_acc:0.950]
Epoch [52/120    avg_loss:0.100, val_acc:0.980]
Epoch [53/120    avg_loss:0.080, val_acc:0.988]
Epoch [54/120    avg_loss:0.062, val_acc:0.982]
Epoch [55/120    avg_loss:0.050, val_acc:0.986]
Epoch [56/120    avg_loss:0.044, val_acc:0.992]
Epoch [57/120    avg_loss:0.173, val_acc:0.970]
Epoch [58/120    avg_loss:0.095, val_acc:0.984]
Epoch [59/120    avg_loss:0.070, val_acc:0.984]
Epoch [60/120    avg_loss:0.052, val_acc:0.992]
Epoch [61/120    avg_loss:0.086, val_acc:0.970]
Epoch [62/120    avg_loss:0.080, val_acc:0.978]
Epoch [63/120    avg_loss:0.084, val_acc:0.980]
Epoch [64/120    avg_loss:0.067, val_acc:0.982]
Epoch [65/120    avg_loss:0.086, val_acc:0.986]
Epoch [66/120    avg_loss:0.042, val_acc:0.990]
Epoch [67/120    avg_loss:0.040, val_acc:0.976]
Epoch [68/120    avg_loss:0.078, val_acc:0.974]
Epoch [69/120    avg_loss:0.056, val_acc:0.988]
Epoch [70/120    avg_loss:0.039, val_acc:0.990]
Epoch [71/120    avg_loss:0.034, val_acc:0.990]
Epoch [72/120    avg_loss:0.028, val_acc:0.978]
Epoch [73/120    avg_loss:0.033, val_acc:0.984]
Epoch [74/120    avg_loss:0.027, val_acc:0.988]
Epoch [75/120    avg_loss:0.024, val_acc:0.990]
Epoch [76/120    avg_loss:0.022, val_acc:0.990]
Epoch [77/120    avg_loss:0.020, val_acc:0.990]
Epoch [78/120    avg_loss:0.024, val_acc:0.990]
Epoch [79/120    avg_loss:0.026, val_acc:0.992]
Epoch [80/120    avg_loss:0.022, val_acc:0.990]
Epoch [81/120    avg_loss:0.018, val_acc:0.990]
Epoch [82/120    avg_loss:0.017, val_acc:0.990]
Epoch [83/120    avg_loss:0.018, val_acc:0.990]
Epoch [84/120    avg_loss:0.021, val_acc:0.986]
Epoch [85/120    avg_loss:0.027, val_acc:0.990]
Epoch [86/120    avg_loss:0.020, val_acc:0.986]
Epoch [87/120    avg_loss:0.022, val_acc:0.990]
Epoch [88/120    avg_loss:0.020, val_acc:0.990]
Epoch [89/120    avg_loss:0.018, val_acc:0.990]
Epoch [90/120    avg_loss:0.018, val_acc:0.988]
Epoch [91/120    avg_loss:0.020, val_acc:0.990]
Epoch [92/120    avg_loss:0.020, val_acc:0.988]
Epoch [93/120    avg_loss:0.019, val_acc:0.990]
Epoch [94/120    avg_loss:0.018, val_acc:0.990]
Epoch [95/120    avg_loss:0.018, val_acc:0.990]
Epoch [96/120    avg_loss:0.017, val_acc:0.988]
Epoch [97/120    avg_loss:0.017, val_acc:0.988]
Epoch [98/120    avg_loss:0.020, val_acc:0.988]
Epoch [99/120    avg_loss:0.021, val_acc:0.990]
Epoch [100/120    avg_loss:0.017, val_acc:0.988]
Epoch [101/120    avg_loss:0.020, val_acc:0.988]
Epoch [102/120    avg_loss:0.021, val_acc:0.988]
Epoch [103/120    avg_loss:0.016, val_acc:0.988]
Epoch [104/120    avg_loss:0.020, val_acc:0.988]
Epoch [105/120    avg_loss:0.018, val_acc:0.988]
Epoch [106/120    avg_loss:0.018, val_acc:0.988]
Epoch [107/120    avg_loss:0.020, val_acc:0.988]
Epoch [108/120    avg_loss:0.018, val_acc:0.988]
Epoch [109/120    avg_loss:0.017, val_acc:0.988]
Epoch [110/120    avg_loss:0.018, val_acc:0.988]
Epoch [111/120    avg_loss:0.016, val_acc:0.988]
Epoch [112/120    avg_loss:0.019, val_acc:0.988]
Epoch [113/120    avg_loss:0.019, val_acc:0.988]
Epoch [114/120    avg_loss:0.017, val_acc:0.988]
Epoch [115/120    avg_loss:0.017, val_acc:0.988]
Epoch [116/120    avg_loss:0.020, val_acc:0.988]
Epoch [117/120    avg_loss:0.020, val_acc:0.988]
Epoch [118/120    avg_loss:0.021, val_acc:0.988]
Epoch [119/120    avg_loss:0.018, val_acc:0.988]
Epoch [120/120    avg_loss:0.020, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.98206278 1.         0.93126386 0.89419795
 1.         0.95555556 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9907417699391329
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcbe6d65d68>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.570, val_acc:0.434]
Epoch [2/120    avg_loss:2.429, val_acc:0.441]
Epoch [3/120    avg_loss:2.302, val_acc:0.463]
Epoch [4/120    avg_loss:2.156, val_acc:0.488]
Epoch [5/120    avg_loss:2.008, val_acc:0.641]
Epoch [6/120    avg_loss:1.841, val_acc:0.633]
Epoch [7/120    avg_loss:1.663, val_acc:0.668]
Epoch [8/120    avg_loss:1.428, val_acc:0.697]
Epoch [9/120    avg_loss:1.241, val_acc:0.727]
Epoch [10/120    avg_loss:1.109, val_acc:0.738]
Epoch [11/120    avg_loss:0.966, val_acc:0.785]
Epoch [12/120    avg_loss:0.859, val_acc:0.760]
Epoch [13/120    avg_loss:0.800, val_acc:0.811]
Epoch [14/120    avg_loss:0.705, val_acc:0.828]
Epoch [15/120    avg_loss:0.649, val_acc:0.832]
Epoch [16/120    avg_loss:0.605, val_acc:0.828]
Epoch [17/120    avg_loss:0.547, val_acc:0.842]
Epoch [18/120    avg_loss:0.555, val_acc:0.873]
Epoch [19/120    avg_loss:0.527, val_acc:0.846]
Epoch [20/120    avg_loss:0.481, val_acc:0.908]
Epoch [21/120    avg_loss:0.433, val_acc:0.904]
Epoch [22/120    avg_loss:0.449, val_acc:0.846]
Epoch [23/120    avg_loss:0.456, val_acc:0.889]
Epoch [24/120    avg_loss:0.362, val_acc:0.906]
Epoch [25/120    avg_loss:0.363, val_acc:0.920]
Epoch [26/120    avg_loss:0.339, val_acc:0.900]
Epoch [27/120    avg_loss:0.280, val_acc:0.936]
Epoch [28/120    avg_loss:0.271, val_acc:0.885]
Epoch [29/120    avg_loss:0.310, val_acc:0.918]
Epoch [30/120    avg_loss:0.311, val_acc:0.926]
Epoch [31/120    avg_loss:0.279, val_acc:0.943]
Epoch [32/120    avg_loss:0.250, val_acc:0.932]
Epoch [33/120    avg_loss:0.254, val_acc:0.930]
Epoch [34/120    avg_loss:0.312, val_acc:0.932]
Epoch [35/120    avg_loss:0.229, val_acc:0.951]
Epoch [36/120    avg_loss:0.187, val_acc:0.943]
Epoch [37/120    avg_loss:0.212, val_acc:0.959]
Epoch [38/120    avg_loss:0.257, val_acc:0.924]
Epoch [39/120    avg_loss:0.228, val_acc:0.936]
Epoch [40/120    avg_loss:0.170, val_acc:0.939]
Epoch [41/120    avg_loss:0.184, val_acc:0.934]
Epoch [42/120    avg_loss:0.158, val_acc:0.963]
Epoch [43/120    avg_loss:0.200, val_acc:0.955]
Epoch [44/120    avg_loss:0.158, val_acc:0.934]
Epoch [45/120    avg_loss:0.122, val_acc:0.955]
Epoch [46/120    avg_loss:0.161, val_acc:0.943]
Epoch [47/120    avg_loss:0.150, val_acc:0.961]
Epoch [48/120    avg_loss:0.123, val_acc:0.961]
Epoch [49/120    avg_loss:0.127, val_acc:0.951]
Epoch [50/120    avg_loss:0.093, val_acc:0.973]
Epoch [51/120    avg_loss:0.098, val_acc:0.977]
Epoch [52/120    avg_loss:0.117, val_acc:0.963]
Epoch [53/120    avg_loss:0.309, val_acc:0.900]
Epoch [54/120    avg_loss:0.190, val_acc:0.969]
Epoch [55/120    avg_loss:0.114, val_acc:0.982]
Epoch [56/120    avg_loss:0.084, val_acc:0.977]
Epoch [57/120    avg_loss:0.083, val_acc:0.969]
Epoch [58/120    avg_loss:0.075, val_acc:0.975]
Epoch [59/120    avg_loss:0.048, val_acc:0.969]
Epoch [60/120    avg_loss:0.056, val_acc:0.965]
Epoch [61/120    avg_loss:0.046, val_acc:0.975]
Epoch [62/120    avg_loss:0.048, val_acc:0.980]
Epoch [63/120    avg_loss:0.038, val_acc:0.973]
Epoch [64/120    avg_loss:0.131, val_acc:0.969]
Epoch [65/120    avg_loss:0.134, val_acc:0.975]
Epoch [66/120    avg_loss:0.056, val_acc:0.984]
Epoch [67/120    avg_loss:0.044, val_acc:0.984]
Epoch [68/120    avg_loss:0.043, val_acc:0.984]
Epoch [69/120    avg_loss:0.059, val_acc:0.975]
Epoch [70/120    avg_loss:0.058, val_acc:0.984]
Epoch [71/120    avg_loss:0.052, val_acc:0.975]
Epoch [72/120    avg_loss:0.103, val_acc:0.969]
Epoch [73/120    avg_loss:0.074, val_acc:0.980]
Epoch [74/120    avg_loss:0.055, val_acc:0.980]
Epoch [75/120    avg_loss:0.083, val_acc:0.986]
Epoch [76/120    avg_loss:0.054, val_acc:0.986]
Epoch [77/120    avg_loss:0.044, val_acc:0.982]
Epoch [78/120    avg_loss:0.036, val_acc:0.980]
Epoch [79/120    avg_loss:0.028, val_acc:0.982]
Epoch [80/120    avg_loss:0.051, val_acc:0.986]
Epoch [81/120    avg_loss:0.053, val_acc:0.988]
Epoch [82/120    avg_loss:0.016, val_acc:0.984]
Epoch [83/120    avg_loss:0.024, val_acc:0.986]
Epoch [84/120    avg_loss:0.025, val_acc:0.986]
Epoch [85/120    avg_loss:0.016, val_acc:0.988]
Epoch [86/120    avg_loss:0.016, val_acc:0.988]
Epoch [87/120    avg_loss:0.064, val_acc:0.982]
Epoch [88/120    avg_loss:0.028, val_acc:0.986]
Epoch [89/120    avg_loss:0.018, val_acc:0.984]
Epoch [90/120    avg_loss:0.032, val_acc:0.977]
Epoch [91/120    avg_loss:0.033, val_acc:0.980]
Epoch [92/120    avg_loss:0.028, val_acc:0.986]
Epoch [93/120    avg_loss:0.020, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.990]
Epoch [95/120    avg_loss:0.010, val_acc:0.988]
Epoch [96/120    avg_loss:0.021, val_acc:0.967]
Epoch [97/120    avg_loss:0.024, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.990]
Epoch [103/120    avg_loss:0.014, val_acc:0.988]
Epoch [104/120    avg_loss:0.043, val_acc:0.971]
Epoch [105/120    avg_loss:0.023, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.986]
Epoch [107/120    avg_loss:0.044, val_acc:0.984]
Epoch [108/120    avg_loss:0.031, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.051, val_acc:0.980]
Epoch [112/120    avg_loss:0.016, val_acc:0.984]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.018, val_acc:0.984]
Epoch [115/120    avg_loss:0.024, val_acc:0.984]
Epoch [116/120    avg_loss:0.015, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   1   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   5   0   0   0   0   0   0   2   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99926954 0.99319728 1.         0.94017094 0.90909091
 1.         0.98378378 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9926405792859767
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a5b4e8e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.599, val_acc:0.287]
Epoch [2/120    avg_loss:2.498, val_acc:0.465]
Epoch [3/120    avg_loss:2.392, val_acc:0.500]
Epoch [4/120    avg_loss:2.260, val_acc:0.484]
Epoch [5/120    avg_loss:2.123, val_acc:0.559]
Epoch [6/120    avg_loss:1.985, val_acc:0.568]
Epoch [7/120    avg_loss:1.827, val_acc:0.648]
Epoch [8/120    avg_loss:1.647, val_acc:0.672]
Epoch [9/120    avg_loss:1.476, val_acc:0.660]
Epoch [10/120    avg_loss:1.341, val_acc:0.705]
Epoch [11/120    avg_loss:1.171, val_acc:0.715]
Epoch [12/120    avg_loss:1.052, val_acc:0.705]
Epoch [13/120    avg_loss:0.937, val_acc:0.750]
Epoch [14/120    avg_loss:0.871, val_acc:0.768]
Epoch [15/120    avg_loss:0.774, val_acc:0.805]
Epoch [16/120    avg_loss:0.667, val_acc:0.846]
Epoch [17/120    avg_loss:0.683, val_acc:0.832]
Epoch [18/120    avg_loss:0.626, val_acc:0.875]
Epoch [19/120    avg_loss:0.550, val_acc:0.889]
Epoch [20/120    avg_loss:0.540, val_acc:0.885]
Epoch [21/120    avg_loss:0.454, val_acc:0.895]
Epoch [22/120    avg_loss:0.434, val_acc:0.914]
Epoch [23/120    avg_loss:0.382, val_acc:0.924]
Epoch [24/120    avg_loss:0.392, val_acc:0.910]
Epoch [25/120    avg_loss:0.355, val_acc:0.914]
Epoch [26/120    avg_loss:0.344, val_acc:0.928]
Epoch [27/120    avg_loss:0.350, val_acc:0.912]
Epoch [28/120    avg_loss:0.324, val_acc:0.965]
Epoch [29/120    avg_loss:0.292, val_acc:0.891]
Epoch [30/120    avg_loss:0.283, val_acc:0.955]
Epoch [31/120    avg_loss:0.284, val_acc:0.945]
Epoch [32/120    avg_loss:0.233, val_acc:0.924]
Epoch [33/120    avg_loss:0.209, val_acc:0.934]
Epoch [34/120    avg_loss:0.319, val_acc:0.881]
Epoch [35/120    avg_loss:0.278, val_acc:0.922]
Epoch [36/120    avg_loss:0.254, val_acc:0.953]
Epoch [37/120    avg_loss:0.191, val_acc:0.959]
Epoch [38/120    avg_loss:0.174, val_acc:0.965]
Epoch [39/120    avg_loss:0.132, val_acc:0.973]
Epoch [40/120    avg_loss:0.138, val_acc:0.969]
Epoch [41/120    avg_loss:0.146, val_acc:0.953]
Epoch [42/120    avg_loss:0.132, val_acc:0.928]
Epoch [43/120    avg_loss:0.141, val_acc:0.957]
Epoch [44/120    avg_loss:0.151, val_acc:0.967]
Epoch [45/120    avg_loss:0.168, val_acc:0.973]
Epoch [46/120    avg_loss:0.142, val_acc:0.967]
Epoch [47/120    avg_loss:0.130, val_acc:0.980]
Epoch [48/120    avg_loss:0.121, val_acc:0.965]
Epoch [49/120    avg_loss:0.148, val_acc:0.971]
Epoch [50/120    avg_loss:0.110, val_acc:0.965]
Epoch [51/120    avg_loss:0.119, val_acc:0.975]
Epoch [52/120    avg_loss:0.115, val_acc:0.955]
Epoch [53/120    avg_loss:0.123, val_acc:0.973]
Epoch [54/120    avg_loss:0.223, val_acc:0.943]
Epoch [55/120    avg_loss:0.113, val_acc:0.982]
Epoch [56/120    avg_loss:0.111, val_acc:0.961]
Epoch [57/120    avg_loss:0.121, val_acc:0.967]
Epoch [58/120    avg_loss:0.079, val_acc:0.992]
Epoch [59/120    avg_loss:0.071, val_acc:0.973]
Epoch [60/120    avg_loss:0.077, val_acc:0.967]
Epoch [61/120    avg_loss:0.093, val_acc:0.949]
Epoch [62/120    avg_loss:0.065, val_acc:0.988]
Epoch [63/120    avg_loss:0.064, val_acc:0.988]
Epoch [64/120    avg_loss:0.081, val_acc:0.984]
Epoch [65/120    avg_loss:0.085, val_acc:0.961]
Epoch [66/120    avg_loss:0.048, val_acc:0.990]
Epoch [67/120    avg_loss:0.112, val_acc:0.975]
Epoch [68/120    avg_loss:0.085, val_acc:0.986]
Epoch [69/120    avg_loss:0.042, val_acc:0.984]
Epoch [70/120    avg_loss:0.044, val_acc:0.990]
Epoch [71/120    avg_loss:0.040, val_acc:0.986]
Epoch [72/120    avg_loss:0.035, val_acc:0.990]
Epoch [73/120    avg_loss:0.035, val_acc:0.994]
Epoch [74/120    avg_loss:0.024, val_acc:0.994]
Epoch [75/120    avg_loss:0.029, val_acc:0.994]
Epoch [76/120    avg_loss:0.028, val_acc:0.994]
Epoch [77/120    avg_loss:0.034, val_acc:0.994]
Epoch [78/120    avg_loss:0.024, val_acc:0.994]
Epoch [79/120    avg_loss:0.030, val_acc:0.994]
Epoch [80/120    avg_loss:0.030, val_acc:0.994]
Epoch [81/120    avg_loss:0.028, val_acc:0.994]
Epoch [82/120    avg_loss:0.030, val_acc:0.994]
Epoch [83/120    avg_loss:0.029, val_acc:0.994]
Epoch [84/120    avg_loss:0.029, val_acc:0.994]
Epoch [85/120    avg_loss:0.026, val_acc:0.994]
Epoch [86/120    avg_loss:0.025, val_acc:0.994]
Epoch [87/120    avg_loss:0.022, val_acc:0.994]
Epoch [88/120    avg_loss:0.026, val_acc:0.994]
Epoch [89/120    avg_loss:0.023, val_acc:0.994]
Epoch [90/120    avg_loss:0.022, val_acc:0.994]
Epoch [91/120    avg_loss:0.022, val_acc:0.994]
Epoch [92/120    avg_loss:0.027, val_acc:0.994]
Epoch [93/120    avg_loss:0.025, val_acc:0.994]
Epoch [94/120    avg_loss:0.021, val_acc:0.994]
Epoch [95/120    avg_loss:0.021, val_acc:0.994]
Epoch [96/120    avg_loss:0.022, val_acc:0.994]
Epoch [97/120    avg_loss:0.022, val_acc:0.994]
Epoch [98/120    avg_loss:0.020, val_acc:0.994]
Epoch [99/120    avg_loss:0.022, val_acc:0.994]
Epoch [100/120    avg_loss:0.024, val_acc:0.994]
Epoch [101/120    avg_loss:0.019, val_acc:0.994]
Epoch [102/120    avg_loss:0.022, val_acc:0.994]
Epoch [103/120    avg_loss:0.021, val_acc:0.994]
Epoch [104/120    avg_loss:0.021, val_acc:0.994]
Epoch [105/120    avg_loss:0.019, val_acc:0.994]
Epoch [106/120    avg_loss:0.020, val_acc:0.994]
Epoch [107/120    avg_loss:0.023, val_acc:0.994]
Epoch [108/120    avg_loss:0.020, val_acc:0.994]
Epoch [109/120    avg_loss:0.022, val_acc:0.994]
Epoch [110/120    avg_loss:0.023, val_acc:0.994]
Epoch [111/120    avg_loss:0.019, val_acc:0.994]
Epoch [112/120    avg_loss:0.020, val_acc:0.994]
Epoch [113/120    avg_loss:0.026, val_acc:0.994]
Epoch [114/120    avg_loss:0.022, val_acc:0.994]
Epoch [115/120    avg_loss:0.022, val_acc:0.994]
Epoch [116/120    avg_loss:0.024, val_acc:0.992]
Epoch [117/120    avg_loss:0.020, val_acc:0.994]
Epoch [118/120    avg_loss:0.019, val_acc:0.992]
Epoch [119/120    avg_loss:0.020, val_acc:0.994]
Epoch [120/120    avg_loss:0.020, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  15   0   0   0   0   0   0   1   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99266862 0.9977221  1.         0.9569161  0.94039735
 0.98095238 0.99465241 0.99741602 1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9928799513751789
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f42ce181dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.302]
Epoch [2/120    avg_loss:2.489, val_acc:0.323]
Epoch [3/120    avg_loss:2.367, val_acc:0.371]
Epoch [4/120    avg_loss:2.238, val_acc:0.492]
Epoch [5/120    avg_loss:2.093, val_acc:0.558]
Epoch [6/120    avg_loss:1.934, val_acc:0.608]
Epoch [7/120    avg_loss:1.759, val_acc:0.660]
Epoch [8/120    avg_loss:1.578, val_acc:0.708]
Epoch [9/120    avg_loss:1.370, val_acc:0.717]
Epoch [10/120    avg_loss:1.228, val_acc:0.760]
Epoch [11/120    avg_loss:1.107, val_acc:0.781]
Epoch [12/120    avg_loss:1.008, val_acc:0.798]
Epoch [13/120    avg_loss:0.925, val_acc:0.804]
Epoch [14/120    avg_loss:0.788, val_acc:0.860]
Epoch [15/120    avg_loss:0.729, val_acc:0.867]
Epoch [16/120    avg_loss:0.647, val_acc:0.912]
Epoch [17/120    avg_loss:0.550, val_acc:0.852]
Epoch [18/120    avg_loss:0.604, val_acc:0.906]
Epoch [19/120    avg_loss:0.578, val_acc:0.908]
Epoch [20/120    avg_loss:0.505, val_acc:0.902]
Epoch [21/120    avg_loss:0.483, val_acc:0.858]
Epoch [22/120    avg_loss:0.477, val_acc:0.933]
Epoch [23/120    avg_loss:0.493, val_acc:0.892]
Epoch [24/120    avg_loss:0.540, val_acc:0.898]
Epoch [25/120    avg_loss:0.404, val_acc:0.960]
Epoch [26/120    avg_loss:0.335, val_acc:0.956]
Epoch [27/120    avg_loss:0.346, val_acc:0.933]
Epoch [28/120    avg_loss:0.305, val_acc:0.942]
Epoch [29/120    avg_loss:0.364, val_acc:0.960]
Epoch [30/120    avg_loss:0.281, val_acc:0.958]
Epoch [31/120    avg_loss:0.321, val_acc:0.956]
Epoch [32/120    avg_loss:0.300, val_acc:0.969]
Epoch [33/120    avg_loss:0.282, val_acc:0.965]
Epoch [34/120    avg_loss:0.248, val_acc:0.967]
Epoch [35/120    avg_loss:0.275, val_acc:0.969]
Epoch [36/120    avg_loss:0.206, val_acc:0.979]
Epoch [37/120    avg_loss:0.204, val_acc:0.971]
Epoch [38/120    avg_loss:0.256, val_acc:0.965]
Epoch [39/120    avg_loss:0.227, val_acc:0.967]
Epoch [40/120    avg_loss:0.192, val_acc:0.979]
Epoch [41/120    avg_loss:0.156, val_acc:0.981]
Epoch [42/120    avg_loss:0.158, val_acc:0.981]
Epoch [43/120    avg_loss:0.159, val_acc:0.983]
Epoch [44/120    avg_loss:0.195, val_acc:0.910]
Epoch [45/120    avg_loss:0.188, val_acc:0.975]
Epoch [46/120    avg_loss:0.147, val_acc:0.927]
Epoch [47/120    avg_loss:0.182, val_acc:0.952]
Epoch [48/120    avg_loss:0.134, val_acc:0.965]
Epoch [49/120    avg_loss:0.179, val_acc:0.988]
Epoch [50/120    avg_loss:0.112, val_acc:0.983]
Epoch [51/120    avg_loss:0.154, val_acc:0.985]
Epoch [52/120    avg_loss:0.115, val_acc:0.988]
Epoch [53/120    avg_loss:0.144, val_acc:0.979]
Epoch [54/120    avg_loss:0.112, val_acc:0.983]
Epoch [55/120    avg_loss:0.102, val_acc:0.985]
Epoch [56/120    avg_loss:0.092, val_acc:0.975]
Epoch [57/120    avg_loss:0.095, val_acc:0.967]
Epoch [58/120    avg_loss:0.109, val_acc:0.988]
Epoch [59/120    avg_loss:0.096, val_acc:0.983]
Epoch [60/120    avg_loss:0.111, val_acc:0.983]
Epoch [61/120    avg_loss:0.090, val_acc:0.992]
Epoch [62/120    avg_loss:0.070, val_acc:0.983]
Epoch [63/120    avg_loss:0.122, val_acc:0.963]
Epoch [64/120    avg_loss:0.130, val_acc:0.988]
Epoch [65/120    avg_loss:0.101, val_acc:0.992]
Epoch [66/120    avg_loss:0.072, val_acc:0.990]
Epoch [67/120    avg_loss:0.049, val_acc:0.990]
Epoch [68/120    avg_loss:0.064, val_acc:0.981]
Epoch [69/120    avg_loss:0.052, val_acc:0.992]
Epoch [70/120    avg_loss:0.102, val_acc:0.981]
Epoch [71/120    avg_loss:0.084, val_acc:0.996]
Epoch [72/120    avg_loss:0.089, val_acc:0.988]
Epoch [73/120    avg_loss:0.050, val_acc:0.992]
Epoch [74/120    avg_loss:0.052, val_acc:0.985]
Epoch [75/120    avg_loss:0.074, val_acc:0.992]
Epoch [76/120    avg_loss:0.068, val_acc:0.992]
Epoch [77/120    avg_loss:0.053, val_acc:0.992]
Epoch [78/120    avg_loss:0.058, val_acc:0.992]
Epoch [79/120    avg_loss:0.040, val_acc:0.992]
Epoch [80/120    avg_loss:0.039, val_acc:0.992]
Epoch [81/120    avg_loss:0.059, val_acc:0.973]
Epoch [82/120    avg_loss:0.089, val_acc:0.990]
Epoch [83/120    avg_loss:0.053, val_acc:0.992]
Epoch [84/120    avg_loss:0.062, val_acc:0.994]
Epoch [85/120    avg_loss:0.037, val_acc:0.994]
Epoch [86/120    avg_loss:0.028, val_acc:0.994]
Epoch [87/120    avg_loss:0.037, val_acc:0.994]
Epoch [88/120    avg_loss:0.028, val_acc:0.994]
Epoch [89/120    avg_loss:0.023, val_acc:0.994]
Epoch [90/120    avg_loss:0.023, val_acc:0.994]
Epoch [91/120    avg_loss:0.025, val_acc:0.994]
Epoch [92/120    avg_loss:0.022, val_acc:0.994]
Epoch [93/120    avg_loss:0.021, val_acc:0.994]
Epoch [94/120    avg_loss:0.025, val_acc:0.994]
Epoch [95/120    avg_loss:0.019, val_acc:0.994]
Epoch [96/120    avg_loss:0.027, val_acc:0.994]
Epoch [97/120    avg_loss:0.023, val_acc:0.994]
Epoch [98/120    avg_loss:0.021, val_acc:0.994]
Epoch [99/120    avg_loss:0.021, val_acc:0.994]
Epoch [100/120    avg_loss:0.024, val_acc:0.994]
Epoch [101/120    avg_loss:0.021, val_acc:0.994]
Epoch [102/120    avg_loss:0.020, val_acc:0.994]
Epoch [103/120    avg_loss:0.017, val_acc:0.994]
Epoch [104/120    avg_loss:0.021, val_acc:0.994]
Epoch [105/120    avg_loss:0.023, val_acc:0.994]
Epoch [106/120    avg_loss:0.019, val_acc:0.994]
Epoch [107/120    avg_loss:0.020, val_acc:0.994]
Epoch [108/120    avg_loss:0.022, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.022, val_acc:0.994]
Epoch [111/120    avg_loss:0.024, val_acc:0.994]
Epoch [112/120    avg_loss:0.025, val_acc:0.994]
Epoch [113/120    avg_loss:0.017, val_acc:0.994]
Epoch [114/120    avg_loss:0.022, val_acc:0.994]
Epoch [115/120    avg_loss:0.018, val_acc:0.994]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.020, val_acc:0.994]
Epoch [118/120    avg_loss:0.023, val_acc:0.994]
Epoch [119/120    avg_loss:0.021, val_acc:0.994]
Epoch [120/120    avg_loss:0.019, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.996337   0.99095023 0.99343545 0.9380531  0.91525424
 0.98800959 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.991217914745401
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f07a9b34e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.581, val_acc:0.293]
Epoch [2/120    avg_loss:2.454, val_acc:0.324]
Epoch [3/120    avg_loss:2.348, val_acc:0.332]
Epoch [4/120    avg_loss:2.239, val_acc:0.361]
Epoch [5/120    avg_loss:2.127, val_acc:0.492]
Epoch [6/120    avg_loss:2.001, val_acc:0.594]
Epoch [7/120    avg_loss:1.845, val_acc:0.613]
Epoch [8/120    avg_loss:1.662, val_acc:0.672]
Epoch [9/120    avg_loss:1.480, val_acc:0.699]
Epoch [10/120    avg_loss:1.303, val_acc:0.734]
Epoch [11/120    avg_loss:1.169, val_acc:0.725]
Epoch [12/120    avg_loss:1.082, val_acc:0.734]
Epoch [13/120    avg_loss:0.948, val_acc:0.732]
Epoch [14/120    avg_loss:0.870, val_acc:0.797]
Epoch [15/120    avg_loss:0.778, val_acc:0.818]
Epoch [16/120    avg_loss:0.721, val_acc:0.789]
Epoch [17/120    avg_loss:0.715, val_acc:0.863]
Epoch [18/120    avg_loss:0.646, val_acc:0.852]
Epoch [19/120    avg_loss:0.597, val_acc:0.869]
Epoch [20/120    avg_loss:0.582, val_acc:0.859]
Epoch [21/120    avg_loss:0.539, val_acc:0.875]
Epoch [22/120    avg_loss:0.582, val_acc:0.900]
Epoch [23/120    avg_loss:0.462, val_acc:0.902]
Epoch [24/120    avg_loss:0.445, val_acc:0.920]
Epoch [25/120    avg_loss:0.417, val_acc:0.920]
Epoch [26/120    avg_loss:0.435, val_acc:0.914]
Epoch [27/120    avg_loss:0.405, val_acc:0.932]
Epoch [28/120    avg_loss:0.347, val_acc:0.930]
Epoch [29/120    avg_loss:0.361, val_acc:0.924]
Epoch [30/120    avg_loss:0.345, val_acc:0.941]
Epoch [31/120    avg_loss:0.295, val_acc:0.943]
Epoch [32/120    avg_loss:0.281, val_acc:0.953]
Epoch [33/120    avg_loss:0.274, val_acc:0.936]
Epoch [34/120    avg_loss:0.292, val_acc:0.951]
Epoch [35/120    avg_loss:0.261, val_acc:0.947]
Epoch [36/120    avg_loss:0.235, val_acc:0.955]
Epoch [37/120    avg_loss:0.222, val_acc:0.945]
Epoch [38/120    avg_loss:0.211, val_acc:0.949]
Epoch [39/120    avg_loss:0.182, val_acc:0.934]
Epoch [40/120    avg_loss:0.167, val_acc:0.959]
Epoch [41/120    avg_loss:0.149, val_acc:0.945]
Epoch [42/120    avg_loss:0.210, val_acc:0.971]
Epoch [43/120    avg_loss:0.181, val_acc:0.955]
Epoch [44/120    avg_loss:0.193, val_acc:0.980]
Epoch [45/120    avg_loss:0.213, val_acc:0.934]
Epoch [46/120    avg_loss:0.251, val_acc:0.961]
Epoch [47/120    avg_loss:0.152, val_acc:0.969]
Epoch [48/120    avg_loss:0.141, val_acc:0.975]
Epoch [49/120    avg_loss:0.146, val_acc:0.980]
Epoch [50/120    avg_loss:0.184, val_acc:0.971]
Epoch [51/120    avg_loss:0.143, val_acc:0.980]
Epoch [52/120    avg_loss:0.133, val_acc:0.984]
Epoch [53/120    avg_loss:0.102, val_acc:0.965]
Epoch [54/120    avg_loss:0.109, val_acc:0.982]
Epoch [55/120    avg_loss:0.072, val_acc:0.984]
Epoch [56/120    avg_loss:0.078, val_acc:0.982]
Epoch [57/120    avg_loss:0.083, val_acc:0.990]
Epoch [58/120    avg_loss:0.082, val_acc:0.984]
Epoch [59/120    avg_loss:0.060, val_acc:0.986]
Epoch [60/120    avg_loss:0.064, val_acc:0.975]
Epoch [61/120    avg_loss:0.095, val_acc:0.984]
Epoch [62/120    avg_loss:0.082, val_acc:0.967]
Epoch [63/120    avg_loss:0.067, val_acc:0.986]
Epoch [64/120    avg_loss:0.050, val_acc:0.984]
Epoch [65/120    avg_loss:0.051, val_acc:0.988]
Epoch [66/120    avg_loss:0.046, val_acc:0.994]
Epoch [67/120    avg_loss:0.050, val_acc:0.990]
Epoch [68/120    avg_loss:0.048, val_acc:0.982]
Epoch [69/120    avg_loss:0.039, val_acc:0.977]
Epoch [70/120    avg_loss:0.045, val_acc:0.992]
Epoch [71/120    avg_loss:0.101, val_acc:0.980]
Epoch [72/120    avg_loss:0.087, val_acc:0.982]
Epoch [73/120    avg_loss:0.047, val_acc:0.988]
Epoch [74/120    avg_loss:0.032, val_acc:0.990]
Epoch [75/120    avg_loss:0.030, val_acc:0.988]
Epoch [76/120    avg_loss:0.042, val_acc:0.986]
Epoch [77/120    avg_loss:0.066, val_acc:0.984]
Epoch [78/120    avg_loss:0.088, val_acc:0.988]
Epoch [79/120    avg_loss:0.050, val_acc:0.992]
Epoch [80/120    avg_loss:0.033, val_acc:0.996]
Epoch [81/120    avg_loss:0.024, val_acc:0.994]
Epoch [82/120    avg_loss:0.023, val_acc:0.994]
Epoch [83/120    avg_loss:0.027, val_acc:0.994]
Epoch [84/120    avg_loss:0.026, val_acc:0.996]
Epoch [85/120    avg_loss:0.026, val_acc:0.996]
Epoch [86/120    avg_loss:0.022, val_acc:0.998]
Epoch [87/120    avg_loss:0.021, val_acc:0.998]
Epoch [88/120    avg_loss:0.022, val_acc:0.998]
Epoch [89/120    avg_loss:0.022, val_acc:0.998]
Epoch [90/120    avg_loss:0.023, val_acc:0.998]
Epoch [91/120    avg_loss:0.021, val_acc:0.996]
Epoch [92/120    avg_loss:0.023, val_acc:0.996]
Epoch [93/120    avg_loss:0.021, val_acc:0.996]
Epoch [94/120    avg_loss:0.021, val_acc:0.996]
Epoch [95/120    avg_loss:0.020, val_acc:0.996]
Epoch [96/120    avg_loss:0.019, val_acc:0.998]
Epoch [97/120    avg_loss:0.018, val_acc:0.996]
Epoch [98/120    avg_loss:0.020, val_acc:0.998]
Epoch [99/120    avg_loss:0.018, val_acc:0.998]
Epoch [100/120    avg_loss:0.020, val_acc:0.998]
Epoch [101/120    avg_loss:0.022, val_acc:0.998]
Epoch [102/120    avg_loss:0.023, val_acc:1.000]
Epoch [103/120    avg_loss:0.018, val_acc:1.000]
Epoch [104/120    avg_loss:0.017, val_acc:1.000]
Epoch [105/120    avg_loss:0.017, val_acc:1.000]
Epoch [106/120    avg_loss:0.018, val_acc:1.000]
Epoch [107/120    avg_loss:0.020, val_acc:0.998]
Epoch [108/120    avg_loss:0.020, val_acc:0.998]
Epoch [109/120    avg_loss:0.017, val_acc:0.998]
Epoch [110/120    avg_loss:0.019, val_acc:1.000]
Epoch [111/120    avg_loss:0.017, val_acc:1.000]
Epoch [112/120    avg_loss:0.017, val_acc:1.000]
Epoch [113/120    avg_loss:0.018, val_acc:1.000]
Epoch [114/120    avg_loss:0.021, val_acc:1.000]
Epoch [115/120    avg_loss:0.017, val_acc:1.000]
Epoch [116/120    avg_loss:0.016, val_acc:1.000]
Epoch [117/120    avg_loss:0.017, val_acc:1.000]
Epoch [118/120    avg_loss:0.019, val_acc:1.000]
Epoch [119/120    avg_loss:0.018, val_acc:1.000]
Epoch [120/120    avg_loss:0.015, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99560117 0.99095023 1.         0.95726496 0.92753623
 0.98564593 0.97826087 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.99264160320836
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fafa7708dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.613, val_acc:0.326]
Epoch [2/120    avg_loss:2.521, val_acc:0.293]
Epoch [3/120    avg_loss:2.415, val_acc:0.309]
Epoch [4/120    avg_loss:2.301, val_acc:0.424]
Epoch [5/120    avg_loss:2.193, val_acc:0.559]
Epoch [6/120    avg_loss:2.057, val_acc:0.637]
Epoch [7/120    avg_loss:1.923, val_acc:0.635]
Epoch [8/120    avg_loss:1.749, val_acc:0.611]
Epoch [9/120    avg_loss:1.589, val_acc:0.680]
Epoch [10/120    avg_loss:1.366, val_acc:0.693]
Epoch [11/120    avg_loss:1.181, val_acc:0.773]
Epoch [12/120    avg_loss:1.040, val_acc:0.764]
Epoch [13/120    avg_loss:0.939, val_acc:0.842]
Epoch [14/120    avg_loss:0.796, val_acc:0.861]
Epoch [15/120    avg_loss:0.755, val_acc:0.859]
Epoch [16/120    avg_loss:0.670, val_acc:0.881]
Epoch [17/120    avg_loss:0.633, val_acc:0.863]
Epoch [18/120    avg_loss:0.658, val_acc:0.836]
Epoch [19/120    avg_loss:0.527, val_acc:0.910]
Epoch [20/120    avg_loss:0.487, val_acc:0.910]
Epoch [21/120    avg_loss:0.393, val_acc:0.920]
Epoch [22/120    avg_loss:0.378, val_acc:0.912]
Epoch [23/120    avg_loss:0.445, val_acc:0.930]
Epoch [24/120    avg_loss:0.345, val_acc:0.951]
Epoch [25/120    avg_loss:0.323, val_acc:0.955]
Epoch [26/120    avg_loss:0.304, val_acc:0.939]
Epoch [27/120    avg_loss:0.258, val_acc:0.945]
Epoch [28/120    avg_loss:0.268, val_acc:0.965]
Epoch [29/120    avg_loss:0.267, val_acc:0.932]
Epoch [30/120    avg_loss:0.206, val_acc:0.955]
Epoch [31/120    avg_loss:0.260, val_acc:0.916]
Epoch [32/120    avg_loss:0.238, val_acc:0.967]
Epoch [33/120    avg_loss:0.249, val_acc:0.963]
Epoch [34/120    avg_loss:0.214, val_acc:0.971]
Epoch [35/120    avg_loss:0.196, val_acc:0.965]
Epoch [36/120    avg_loss:0.270, val_acc:0.953]
Epoch [37/120    avg_loss:0.206, val_acc:0.939]
Epoch [38/120    avg_loss:0.175, val_acc:0.949]
Epoch [39/120    avg_loss:0.203, val_acc:0.959]
Epoch [40/120    avg_loss:0.320, val_acc:0.910]
Epoch [41/120    avg_loss:0.213, val_acc:0.959]
Epoch [42/120    avg_loss:0.174, val_acc:0.961]
Epoch [43/120    avg_loss:0.165, val_acc:0.971]
Epoch [44/120    avg_loss:0.143, val_acc:0.971]
Epoch [45/120    avg_loss:0.136, val_acc:0.982]
Epoch [46/120    avg_loss:0.115, val_acc:0.980]
Epoch [47/120    avg_loss:0.141, val_acc:0.973]
Epoch [48/120    avg_loss:0.104, val_acc:0.977]
Epoch [49/120    avg_loss:0.129, val_acc:0.959]
Epoch [50/120    avg_loss:0.134, val_acc:0.973]
Epoch [51/120    avg_loss:0.097, val_acc:0.980]
Epoch [52/120    avg_loss:0.077, val_acc:0.982]
Epoch [53/120    avg_loss:0.109, val_acc:0.973]
Epoch [54/120    avg_loss:0.095, val_acc:0.980]
Epoch [55/120    avg_loss:0.083, val_acc:0.984]
Epoch [56/120    avg_loss:0.061, val_acc:0.982]
Epoch [57/120    avg_loss:0.149, val_acc:0.959]
Epoch [58/120    avg_loss:0.106, val_acc:0.965]
Epoch [59/120    avg_loss:0.133, val_acc:0.963]
Epoch [60/120    avg_loss:0.155, val_acc:0.967]
Epoch [61/120    avg_loss:0.141, val_acc:0.984]
Epoch [62/120    avg_loss:0.088, val_acc:0.986]
Epoch [63/120    avg_loss:0.076, val_acc:0.908]
Epoch [64/120    avg_loss:0.086, val_acc:0.973]
Epoch [65/120    avg_loss:0.115, val_acc:0.984]
Epoch [66/120    avg_loss:0.058, val_acc:0.980]
Epoch [67/120    avg_loss:0.046, val_acc:0.988]
Epoch [68/120    avg_loss:0.053, val_acc:0.986]
Epoch [69/120    avg_loss:0.042, val_acc:0.988]
Epoch [70/120    avg_loss:0.037, val_acc:0.986]
Epoch [71/120    avg_loss:0.036, val_acc:0.980]
Epoch [72/120    avg_loss:0.050, val_acc:0.994]
Epoch [73/120    avg_loss:0.082, val_acc:0.984]
Epoch [74/120    avg_loss:0.102, val_acc:0.984]
Epoch [75/120    avg_loss:0.044, val_acc:0.977]
Epoch [76/120    avg_loss:0.064, val_acc:0.984]
Epoch [77/120    avg_loss:0.071, val_acc:0.988]
Epoch [78/120    avg_loss:0.040, val_acc:0.984]
Epoch [79/120    avg_loss:0.048, val_acc:0.986]
Epoch [80/120    avg_loss:0.026, val_acc:0.984]
Epoch [81/120    avg_loss:0.031, val_acc:0.986]
Epoch [82/120    avg_loss:0.021, val_acc:0.988]
Epoch [83/120    avg_loss:0.031, val_acc:0.998]
Epoch [84/120    avg_loss:0.018, val_acc:0.990]
Epoch [85/120    avg_loss:0.024, val_acc:0.992]
Epoch [86/120    avg_loss:0.044, val_acc:0.988]
Epoch [87/120    avg_loss:0.024, val_acc:0.992]
Epoch [88/120    avg_loss:0.047, val_acc:0.971]
Epoch [89/120    avg_loss:0.030, val_acc:0.988]
Epoch [90/120    avg_loss:0.036, val_acc:0.992]
Epoch [91/120    avg_loss:0.048, val_acc:0.988]
Epoch [92/120    avg_loss:0.043, val_acc:0.990]
Epoch [93/120    avg_loss:0.054, val_acc:0.947]
Epoch [94/120    avg_loss:0.076, val_acc:0.980]
Epoch [95/120    avg_loss:0.095, val_acc:0.977]
Epoch [96/120    avg_loss:0.038, val_acc:0.984]
Epoch [97/120    avg_loss:0.026, val_acc:0.988]
Epoch [98/120    avg_loss:0.020, val_acc:0.990]
Epoch [99/120    avg_loss:0.025, val_acc:0.984]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.027, val_acc:0.984]
Epoch [102/120    avg_loss:0.019, val_acc:0.986]
Epoch [103/120    avg_loss:0.022, val_acc:0.986]
Epoch [104/120    avg_loss:0.023, val_acc:0.988]
Epoch [105/120    avg_loss:0.017, val_acc:0.988]
Epoch [106/120    avg_loss:0.017, val_acc:0.988]
Epoch [107/120    avg_loss:0.018, val_acc:0.986]
Epoch [108/120    avg_loss:0.017, val_acc:0.986]
Epoch [109/120    avg_loss:0.018, val_acc:0.986]
Epoch [110/120    avg_loss:0.023, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.986]
Epoch [112/120    avg_loss:0.017, val_acc:0.986]
Epoch [113/120    avg_loss:0.018, val_acc:0.986]
Epoch [114/120    avg_loss:0.016, val_acc:0.986]
Epoch [115/120    avg_loss:0.013, val_acc:0.986]
Epoch [116/120    avg_loss:0.014, val_acc:0.986]
Epoch [117/120    avg_loss:0.018, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.015, val_acc:0.986]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214   7   0   0   0   0   0   0   6   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.98206278 1.         0.94690265 0.93706294
 1.         0.95555556 1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9924027613694589
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9c17ad7d68>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.258]
Epoch [2/120    avg_loss:2.488, val_acc:0.413]
Epoch [3/120    avg_loss:2.349, val_acc:0.468]
Epoch [4/120    avg_loss:2.208, val_acc:0.474]
Epoch [5/120    avg_loss:2.065, val_acc:0.563]
Epoch [6/120    avg_loss:1.891, val_acc:0.619]
Epoch [7/120    avg_loss:1.722, val_acc:0.639]
Epoch [8/120    avg_loss:1.556, val_acc:0.639]
Epoch [9/120    avg_loss:1.340, val_acc:0.694]
Epoch [10/120    avg_loss:1.180, val_acc:0.700]
Epoch [11/120    avg_loss:1.033, val_acc:0.784]
Epoch [12/120    avg_loss:0.862, val_acc:0.813]
Epoch [13/120    avg_loss:0.750, val_acc:0.817]
Epoch [14/120    avg_loss:0.705, val_acc:0.869]
Epoch [15/120    avg_loss:0.649, val_acc:0.883]
Epoch [16/120    avg_loss:0.553, val_acc:0.883]
Epoch [17/120    avg_loss:0.577, val_acc:0.841]
Epoch [18/120    avg_loss:0.486, val_acc:0.923]
Epoch [19/120    avg_loss:0.410, val_acc:0.909]
Epoch [20/120    avg_loss:0.390, val_acc:0.937]
Epoch [21/120    avg_loss:0.403, val_acc:0.869]
Epoch [22/120    avg_loss:0.333, val_acc:0.915]
Epoch [23/120    avg_loss:0.338, val_acc:0.944]
Epoch [24/120    avg_loss:0.304, val_acc:0.946]
Epoch [25/120    avg_loss:0.292, val_acc:0.958]
Epoch [26/120    avg_loss:0.270, val_acc:0.948]
Epoch [27/120    avg_loss:0.253, val_acc:0.942]
Epoch [28/120    avg_loss:0.312, val_acc:0.909]
Epoch [29/120    avg_loss:0.287, val_acc:0.964]
Epoch [30/120    avg_loss:0.186, val_acc:0.942]
Epoch [31/120    avg_loss:0.189, val_acc:0.952]
Epoch [32/120    avg_loss:0.177, val_acc:0.966]
Epoch [33/120    avg_loss:0.176, val_acc:0.958]
Epoch [34/120    avg_loss:0.172, val_acc:0.970]
Epoch [35/120    avg_loss:0.143, val_acc:0.974]
Epoch [36/120    avg_loss:0.127, val_acc:0.980]
Epoch [37/120    avg_loss:0.174, val_acc:0.958]
Epoch [38/120    avg_loss:0.172, val_acc:0.968]
Epoch [39/120    avg_loss:0.158, val_acc:0.970]
Epoch [40/120    avg_loss:0.113, val_acc:0.976]
Epoch [41/120    avg_loss:0.084, val_acc:0.986]
Epoch [42/120    avg_loss:0.175, val_acc:0.978]
Epoch [43/120    avg_loss:0.143, val_acc:0.976]
Epoch [44/120    avg_loss:0.087, val_acc:0.980]
Epoch [45/120    avg_loss:0.091, val_acc:0.980]
Epoch [46/120    avg_loss:0.102, val_acc:0.976]
Epoch [47/120    avg_loss:0.079, val_acc:0.966]
Epoch [48/120    avg_loss:0.098, val_acc:0.972]
Epoch [49/120    avg_loss:0.100, val_acc:0.964]
Epoch [50/120    avg_loss:0.086, val_acc:0.974]
Epoch [51/120    avg_loss:0.124, val_acc:0.855]
Epoch [52/120    avg_loss:0.147, val_acc:0.972]
Epoch [53/120    avg_loss:0.094, val_acc:0.984]
Epoch [54/120    avg_loss:0.089, val_acc:0.984]
Epoch [55/120    avg_loss:0.053, val_acc:0.988]
Epoch [56/120    avg_loss:0.062, val_acc:0.990]
Epoch [57/120    avg_loss:0.050, val_acc:0.990]
Epoch [58/120    avg_loss:0.056, val_acc:0.990]
Epoch [59/120    avg_loss:0.048, val_acc:0.990]
Epoch [60/120    avg_loss:0.039, val_acc:0.992]
Epoch [61/120    avg_loss:0.045, val_acc:0.992]
Epoch [62/120    avg_loss:0.039, val_acc:0.990]
Epoch [63/120    avg_loss:0.038, val_acc:0.992]
Epoch [64/120    avg_loss:0.039, val_acc:0.992]
Epoch [65/120    avg_loss:0.035, val_acc:0.992]
Epoch [66/120    avg_loss:0.042, val_acc:0.992]
Epoch [67/120    avg_loss:0.038, val_acc:0.992]
Epoch [68/120    avg_loss:0.034, val_acc:0.990]
Epoch [69/120    avg_loss:0.038, val_acc:0.990]
Epoch [70/120    avg_loss:0.035, val_acc:0.992]
Epoch [71/120    avg_loss:0.037, val_acc:0.992]
Epoch [72/120    avg_loss:0.037, val_acc:0.990]
Epoch [73/120    avg_loss:0.035, val_acc:0.990]
Epoch [74/120    avg_loss:0.032, val_acc:0.990]
Epoch [75/120    avg_loss:0.036, val_acc:0.988]
Epoch [76/120    avg_loss:0.033, val_acc:0.990]
Epoch [77/120    avg_loss:0.034, val_acc:0.990]
Epoch [78/120    avg_loss:0.035, val_acc:0.992]
Epoch [79/120    avg_loss:0.034, val_acc:0.992]
Epoch [80/120    avg_loss:0.031, val_acc:0.992]
Epoch [81/120    avg_loss:0.040, val_acc:0.990]
Epoch [82/120    avg_loss:0.036, val_acc:0.990]
Epoch [83/120    avg_loss:0.034, val_acc:0.988]
Epoch [84/120    avg_loss:0.032, val_acc:0.992]
Epoch [85/120    avg_loss:0.028, val_acc:0.988]
Epoch [86/120    avg_loss:0.032, val_acc:0.988]
Epoch [87/120    avg_loss:0.030, val_acc:0.988]
Epoch [88/120    avg_loss:0.031, val_acc:0.988]
Epoch [89/120    avg_loss:0.030, val_acc:0.988]
Epoch [90/120    avg_loss:0.030, val_acc:0.988]
Epoch [91/120    avg_loss:0.028, val_acc:0.988]
Epoch [92/120    avg_loss:0.028, val_acc:0.990]
Epoch [93/120    avg_loss:0.029, val_acc:0.990]
Epoch [94/120    avg_loss:0.028, val_acc:0.990]
Epoch [95/120    avg_loss:0.026, val_acc:0.988]
Epoch [96/120    avg_loss:0.030, val_acc:0.988]
Epoch [97/120    avg_loss:0.029, val_acc:0.988]
Epoch [98/120    avg_loss:0.028, val_acc:0.990]
Epoch [99/120    avg_loss:0.028, val_acc:0.990]
Epoch [100/120    avg_loss:0.026, val_acc:0.990]
Epoch [101/120    avg_loss:0.030, val_acc:0.988]
Epoch [102/120    avg_loss:0.029, val_acc:0.988]
Epoch [103/120    avg_loss:0.026, val_acc:0.988]
Epoch [104/120    avg_loss:0.029, val_acc:0.988]
Epoch [105/120    avg_loss:0.036, val_acc:0.988]
Epoch [106/120    avg_loss:0.025, val_acc:0.988]
Epoch [107/120    avg_loss:0.026, val_acc:0.988]
Epoch [108/120    avg_loss:0.030, val_acc:0.988]
Epoch [109/120    avg_loss:0.030, val_acc:0.988]
Epoch [110/120    avg_loss:0.028, val_acc:0.988]
Epoch [111/120    avg_loss:0.026, val_acc:0.988]
Epoch [112/120    avg_loss:0.028, val_acc:0.988]
Epoch [113/120    avg_loss:0.027, val_acc:0.988]
Epoch [114/120    avg_loss:0.028, val_acc:0.988]
Epoch [115/120    avg_loss:0.027, val_acc:0.988]
Epoch [116/120    avg_loss:0.043, val_acc:0.988]
Epoch [117/120    avg_loss:0.031, val_acc:0.988]
Epoch [118/120    avg_loss:0.031, val_acc:0.988]
Epoch [119/120    avg_loss:0.027, val_acc:0.988]
Epoch [120/120    avg_loss:0.024, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 674   0   0   0   0  11   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99190581 0.98871332 1.         0.93859649 0.90277778
 0.97399527 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895575838325732
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47802cee48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.628, val_acc:0.199]
Epoch [2/120    avg_loss:2.562, val_acc:0.242]
Epoch [3/120    avg_loss:2.475, val_acc:0.340]
Epoch [4/120    avg_loss:2.361, val_acc:0.424]
Epoch [5/120    avg_loss:2.237, val_acc:0.543]
Epoch [6/120    avg_loss:2.072, val_acc:0.607]
Epoch [7/120    avg_loss:1.911, val_acc:0.645]
Epoch [8/120    avg_loss:1.685, val_acc:0.689]
Epoch [9/120    avg_loss:1.468, val_acc:0.721]
Epoch [10/120    avg_loss:1.290, val_acc:0.738]
Epoch [11/120    avg_loss:1.129, val_acc:0.793]
Epoch [12/120    avg_loss:1.005, val_acc:0.818]
Epoch [13/120    avg_loss:0.879, val_acc:0.748]
Epoch [14/120    avg_loss:0.863, val_acc:0.775]
Epoch [15/120    avg_loss:0.759, val_acc:0.885]
Epoch [16/120    avg_loss:0.686, val_acc:0.834]
Epoch [17/120    avg_loss:0.613, val_acc:0.850]
Epoch [18/120    avg_loss:0.529, val_acc:0.859]
Epoch [19/120    avg_loss:0.550, val_acc:0.830]
Epoch [20/120    avg_loss:0.517, val_acc:0.846]
Epoch [21/120    avg_loss:0.487, val_acc:0.932]
Epoch [22/120    avg_loss:0.457, val_acc:0.908]
Epoch [23/120    avg_loss:0.394, val_acc:0.916]
Epoch [24/120    avg_loss:0.340, val_acc:0.941]
Epoch [25/120    avg_loss:0.353, val_acc:0.900]
Epoch [26/120    avg_loss:0.327, val_acc:0.926]
Epoch [27/120    avg_loss:0.353, val_acc:0.934]
Epoch [28/120    avg_loss:0.321, val_acc:0.936]
Epoch [29/120    avg_loss:0.252, val_acc:0.953]
Epoch [30/120    avg_loss:0.240, val_acc:0.895]
Epoch [31/120    avg_loss:0.273, val_acc:0.904]
Epoch [32/120    avg_loss:0.287, val_acc:0.932]
Epoch [33/120    avg_loss:0.235, val_acc:0.945]
Epoch [34/120    avg_loss:0.229, val_acc:0.936]
Epoch [35/120    avg_loss:0.243, val_acc:0.963]
Epoch [36/120    avg_loss:0.168, val_acc:0.967]
Epoch [37/120    avg_loss:0.181, val_acc:0.957]
Epoch [38/120    avg_loss:0.170, val_acc:0.928]
Epoch [39/120    avg_loss:0.155, val_acc:0.959]
Epoch [40/120    avg_loss:0.152, val_acc:0.961]
Epoch [41/120    avg_loss:0.138, val_acc:0.967]
Epoch [42/120    avg_loss:0.126, val_acc:0.959]
Epoch [43/120    avg_loss:0.162, val_acc:0.973]
Epoch [44/120    avg_loss:0.158, val_acc:0.957]
Epoch [45/120    avg_loss:0.132, val_acc:0.969]
Epoch [46/120    avg_loss:0.189, val_acc:0.955]
Epoch [47/120    avg_loss:0.190, val_acc:0.941]
Epoch [48/120    avg_loss:0.154, val_acc:0.973]
Epoch [49/120    avg_loss:0.104, val_acc:0.971]
Epoch [50/120    avg_loss:0.070, val_acc:0.982]
Epoch [51/120    avg_loss:0.074, val_acc:0.971]
Epoch [52/120    avg_loss:0.089, val_acc:0.961]
Epoch [53/120    avg_loss:0.155, val_acc:0.951]
Epoch [54/120    avg_loss:0.129, val_acc:0.955]
Epoch [55/120    avg_loss:0.082, val_acc:0.967]
Epoch [56/120    avg_loss:0.100, val_acc:0.967]
Epoch [57/120    avg_loss:0.130, val_acc:0.973]
Epoch [58/120    avg_loss:0.064, val_acc:0.980]
Epoch [59/120    avg_loss:0.076, val_acc:0.975]
Epoch [60/120    avg_loss:0.052, val_acc:0.982]
Epoch [61/120    avg_loss:0.045, val_acc:0.984]
Epoch [62/120    avg_loss:0.042, val_acc:0.980]
Epoch [63/120    avg_loss:0.032, val_acc:0.980]
Epoch [64/120    avg_loss:0.042, val_acc:0.984]
Epoch [65/120    avg_loss:0.116, val_acc:0.971]
Epoch [66/120    avg_loss:0.068, val_acc:0.982]
Epoch [67/120    avg_loss:0.034, val_acc:0.984]
Epoch [68/120    avg_loss:0.037, val_acc:0.973]
Epoch [69/120    avg_loss:0.037, val_acc:0.977]
Epoch [70/120    avg_loss:0.045, val_acc:0.977]
Epoch [71/120    avg_loss:0.044, val_acc:0.977]
Epoch [72/120    avg_loss:0.025, val_acc:0.980]
Epoch [73/120    avg_loss:0.051, val_acc:0.984]
Epoch [74/120    avg_loss:0.027, val_acc:0.982]
Epoch [75/120    avg_loss:0.038, val_acc:0.982]
Epoch [76/120    avg_loss:0.024, val_acc:0.988]
Epoch [77/120    avg_loss:0.021, val_acc:0.984]
Epoch [78/120    avg_loss:0.026, val_acc:0.986]
Epoch [79/120    avg_loss:0.025, val_acc:0.975]
Epoch [80/120    avg_loss:0.025, val_acc:0.986]
Epoch [81/120    avg_loss:0.018, val_acc:0.986]
Epoch [82/120    avg_loss:0.022, val_acc:0.975]
Epoch [83/120    avg_loss:0.020, val_acc:0.984]
Epoch [84/120    avg_loss:0.018, val_acc:0.986]
Epoch [85/120    avg_loss:0.018, val_acc:0.986]
Epoch [86/120    avg_loss:0.021, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.986]
Epoch [88/120    avg_loss:0.013, val_acc:0.986]
Epoch [89/120    avg_loss:0.034, val_acc:0.980]
Epoch [90/120    avg_loss:0.017, val_acc:0.982]
Epoch [91/120    avg_loss:0.016, val_acc:0.984]
Epoch [92/120    avg_loss:0.017, val_acc:0.984]
Epoch [93/120    avg_loss:0.013, val_acc:0.984]
Epoch [94/120    avg_loss:0.015, val_acc:0.984]
Epoch [95/120    avg_loss:0.013, val_acc:0.984]
Epoch [96/120    avg_loss:0.012, val_acc:0.984]
Epoch [97/120    avg_loss:0.012, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.013, val_acc:0.988]
Epoch [100/120    avg_loss:0.012, val_acc:0.986]
Epoch [101/120    avg_loss:0.013, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.011, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.986]
Epoch [105/120    avg_loss:0.011, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.011, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.016, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.017, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   8   0   0   0   0   0   0   1   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99264706 1.         1.         0.94372294 0.91103203
 0.97630332 1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9914559221936318
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f405adda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.584, val_acc:0.348]
Epoch [2/120    avg_loss:2.451, val_acc:0.371]
Epoch [3/120    avg_loss:2.332, val_acc:0.395]
Epoch [4/120    avg_loss:2.213, val_acc:0.463]
Epoch [5/120    avg_loss:2.085, val_acc:0.500]
Epoch [6/120    avg_loss:1.971, val_acc:0.547]
Epoch [7/120    avg_loss:1.815, val_acc:0.615]
Epoch [8/120    avg_loss:1.658, val_acc:0.641]
Epoch [9/120    avg_loss:1.485, val_acc:0.693]
Epoch [10/120    avg_loss:1.234, val_acc:0.723]
Epoch [11/120    avg_loss:1.074, val_acc:0.748]
Epoch [12/120    avg_loss:0.912, val_acc:0.715]
Epoch [13/120    avg_loss:0.832, val_acc:0.803]
Epoch [14/120    avg_loss:0.719, val_acc:0.805]
Epoch [15/120    avg_loss:0.704, val_acc:0.871]
Epoch [16/120    avg_loss:0.609, val_acc:0.848]
Epoch [17/120    avg_loss:0.637, val_acc:0.885]
Epoch [18/120    avg_loss:0.553, val_acc:0.900]
Epoch [19/120    avg_loss:0.455, val_acc:0.877]
Epoch [20/120    avg_loss:0.443, val_acc:0.902]
Epoch [21/120    avg_loss:0.414, val_acc:0.889]
Epoch [22/120    avg_loss:0.438, val_acc:0.830]
Epoch [23/120    avg_loss:0.437, val_acc:0.877]
Epoch [24/120    avg_loss:0.409, val_acc:0.924]
Epoch [25/120    avg_loss:0.308, val_acc:0.934]
Epoch [26/120    avg_loss:0.345, val_acc:0.902]
Epoch [27/120    avg_loss:0.305, val_acc:0.932]
Epoch [28/120    avg_loss:0.337, val_acc:0.912]
Epoch [29/120    avg_loss:0.252, val_acc:0.928]
Epoch [30/120    avg_loss:0.229, val_acc:0.916]
Epoch [31/120    avg_loss:0.226, val_acc:0.930]
Epoch [32/120    avg_loss:0.228, val_acc:0.947]
Epoch [33/120    avg_loss:0.197, val_acc:0.953]
Epoch [34/120    avg_loss:0.279, val_acc:0.934]
Epoch [35/120    avg_loss:0.204, val_acc:0.955]
Epoch [36/120    avg_loss:0.131, val_acc:0.961]
Epoch [37/120    avg_loss:0.221, val_acc:0.928]
Epoch [38/120    avg_loss:0.198, val_acc:0.936]
Epoch [39/120    avg_loss:0.147, val_acc:0.963]
Epoch [40/120    avg_loss:0.135, val_acc:0.953]
Epoch [41/120    avg_loss:0.188, val_acc:0.949]
Epoch [42/120    avg_loss:0.148, val_acc:0.943]
Epoch [43/120    avg_loss:0.139, val_acc:0.963]
Epoch [44/120    avg_loss:0.137, val_acc:0.969]
Epoch [45/120    avg_loss:0.117, val_acc:0.967]
Epoch [46/120    avg_loss:0.117, val_acc:0.971]
Epoch [47/120    avg_loss:0.107, val_acc:0.975]
Epoch [48/120    avg_loss:0.113, val_acc:0.961]
Epoch [49/120    avg_loss:0.128, val_acc:0.969]
Epoch [50/120    avg_loss:0.093, val_acc:0.965]
Epoch [51/120    avg_loss:0.131, val_acc:0.959]
Epoch [52/120    avg_loss:0.082, val_acc:0.971]
Epoch [53/120    avg_loss:0.081, val_acc:0.965]
Epoch [54/120    avg_loss:0.079, val_acc:0.973]
Epoch [55/120    avg_loss:0.068, val_acc:0.975]
Epoch [56/120    avg_loss:0.071, val_acc:0.982]
Epoch [57/120    avg_loss:0.105, val_acc:0.975]
Epoch [58/120    avg_loss:0.076, val_acc:0.975]
Epoch [59/120    avg_loss:0.053, val_acc:0.973]
Epoch [60/120    avg_loss:0.049, val_acc:0.973]
Epoch [61/120    avg_loss:0.041, val_acc:0.982]
Epoch [62/120    avg_loss:0.042, val_acc:0.984]
Epoch [63/120    avg_loss:0.033, val_acc:0.982]
Epoch [64/120    avg_loss:0.034, val_acc:0.984]
Epoch [65/120    avg_loss:0.052, val_acc:0.984]
Epoch [66/120    avg_loss:0.041, val_acc:0.986]
Epoch [67/120    avg_loss:0.042, val_acc:0.984]
Epoch [68/120    avg_loss:0.033, val_acc:0.988]
Epoch [69/120    avg_loss:0.036, val_acc:0.973]
Epoch [70/120    avg_loss:0.061, val_acc:0.977]
Epoch [71/120    avg_loss:0.048, val_acc:0.973]
Epoch [72/120    avg_loss:0.050, val_acc:0.984]
Epoch [73/120    avg_loss:0.034, val_acc:0.980]
Epoch [74/120    avg_loss:0.065, val_acc:0.980]
Epoch [75/120    avg_loss:0.103, val_acc:0.984]
Epoch [76/120    avg_loss:0.066, val_acc:0.984]
Epoch [77/120    avg_loss:0.045, val_acc:0.990]
Epoch [78/120    avg_loss:0.038, val_acc:0.984]
Epoch [79/120    avg_loss:0.051, val_acc:0.969]
Epoch [80/120    avg_loss:0.069, val_acc:0.939]
Epoch [81/120    avg_loss:0.078, val_acc:0.973]
Epoch [82/120    avg_loss:0.034, val_acc:0.984]
Epoch [83/120    avg_loss:0.028, val_acc:0.988]
Epoch [84/120    avg_loss:0.024, val_acc:0.986]
Epoch [85/120    avg_loss:0.021, val_acc:0.990]
Epoch [86/120    avg_loss:0.018, val_acc:0.990]
Epoch [87/120    avg_loss:0.023, val_acc:0.986]
Epoch [88/120    avg_loss:0.018, val_acc:0.984]
Epoch [89/120    avg_loss:0.015, val_acc:0.988]
Epoch [90/120    avg_loss:0.011, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.990]
Epoch [93/120    avg_loss:0.011, val_acc:0.990]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.011, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.022, val_acc:0.984]
Epoch [98/120    avg_loss:0.015, val_acc:0.988]
Epoch [99/120    avg_loss:0.026, val_acc:0.982]
Epoch [100/120    avg_loss:0.025, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.990]
Epoch [102/120    avg_loss:0.017, val_acc:0.992]
Epoch [103/120    avg_loss:0.020, val_acc:0.990]
Epoch [104/120    avg_loss:0.013, val_acc:0.990]
Epoch [105/120    avg_loss:0.015, val_acc:0.990]
Epoch [106/120    avg_loss:0.015, val_acc:0.990]
Epoch [107/120    avg_loss:0.014, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.992]
Epoch [110/120    avg_loss:0.009, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.007, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 0.99926954 0.99319728 1.         0.96629213 0.95302013
 0.99757869 0.98378378 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9954898742590508
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba665dcda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.601, val_acc:0.305]
Epoch [2/120    avg_loss:2.496, val_acc:0.410]
Epoch [3/120    avg_loss:2.377, val_acc:0.418]
Epoch [4/120    avg_loss:2.246, val_acc:0.494]
Epoch [5/120    avg_loss:2.122, val_acc:0.572]
Epoch [6/120    avg_loss:1.949, val_acc:0.611]
Epoch [7/120    avg_loss:1.751, val_acc:0.670]
Epoch [8/120    avg_loss:1.534, val_acc:0.695]
Epoch [9/120    avg_loss:1.342, val_acc:0.734]
Epoch [10/120    avg_loss:1.129, val_acc:0.752]
Epoch [11/120    avg_loss:1.011, val_acc:0.764]
Epoch [12/120    avg_loss:0.922, val_acc:0.773]
Epoch [13/120    avg_loss:0.806, val_acc:0.836]
Epoch [14/120    avg_loss:0.776, val_acc:0.820]
Epoch [15/120    avg_loss:0.693, val_acc:0.848]
Epoch [16/120    avg_loss:0.582, val_acc:0.904]
Epoch [17/120    avg_loss:0.577, val_acc:0.891]
Epoch [18/120    avg_loss:0.504, val_acc:0.873]
Epoch [19/120    avg_loss:0.580, val_acc:0.922]
Epoch [20/120    avg_loss:0.433, val_acc:0.904]
Epoch [21/120    avg_loss:0.485, val_acc:0.928]
Epoch [22/120    avg_loss:0.432, val_acc:0.846]
Epoch [23/120    avg_loss:0.432, val_acc:0.908]
Epoch [24/120    avg_loss:0.335, val_acc:0.945]
Epoch [25/120    avg_loss:0.305, val_acc:0.930]
Epoch [26/120    avg_loss:0.292, val_acc:0.963]
Epoch [27/120    avg_loss:0.260, val_acc:0.936]
Epoch [28/120    avg_loss:0.253, val_acc:0.936]
Epoch [29/120    avg_loss:0.239, val_acc:0.959]
Epoch [30/120    avg_loss:0.226, val_acc:0.955]
Epoch [31/120    avg_loss:0.258, val_acc:0.932]
Epoch [32/120    avg_loss:0.293, val_acc:0.910]
Epoch [33/120    avg_loss:0.210, val_acc:0.951]
Epoch [34/120    avg_loss:0.191, val_acc:0.951]
Epoch [35/120    avg_loss:0.190, val_acc:0.965]
Epoch [36/120    avg_loss:0.145, val_acc:0.967]
Epoch [37/120    avg_loss:0.140, val_acc:0.971]
Epoch [38/120    avg_loss:0.199, val_acc:0.959]
Epoch [39/120    avg_loss:0.199, val_acc:0.980]
Epoch [40/120    avg_loss:0.123, val_acc:0.967]
Epoch [41/120    avg_loss:0.174, val_acc:0.977]
Epoch [42/120    avg_loss:0.126, val_acc:0.977]
Epoch [43/120    avg_loss:0.108, val_acc:0.975]
Epoch [44/120    avg_loss:0.107, val_acc:0.971]
Epoch [45/120    avg_loss:0.092, val_acc:0.980]
Epoch [46/120    avg_loss:0.092, val_acc:0.975]
Epoch [47/120    avg_loss:0.105, val_acc:0.977]
Epoch [48/120    avg_loss:0.091, val_acc:0.969]
Epoch [49/120    avg_loss:0.096, val_acc:0.977]
Epoch [50/120    avg_loss:0.072, val_acc:0.984]
Epoch [51/120    avg_loss:0.081, val_acc:0.969]
Epoch [52/120    avg_loss:0.090, val_acc:0.980]
Epoch [53/120    avg_loss:0.134, val_acc:0.973]
Epoch [54/120    avg_loss:0.095, val_acc:0.984]
Epoch [55/120    avg_loss:0.056, val_acc:0.990]
Epoch [56/120    avg_loss:0.050, val_acc:0.986]
Epoch [57/120    avg_loss:0.122, val_acc:0.965]
Epoch [58/120    avg_loss:0.081, val_acc:0.984]
Epoch [59/120    avg_loss:0.076, val_acc:0.980]
Epoch [60/120    avg_loss:0.062, val_acc:0.986]
Epoch [61/120    avg_loss:0.071, val_acc:0.986]
Epoch [62/120    avg_loss:0.051, val_acc:0.977]
Epoch [63/120    avg_loss:0.048, val_acc:0.988]
Epoch [64/120    avg_loss:0.049, val_acc:0.988]
Epoch [65/120    avg_loss:0.044, val_acc:0.986]
Epoch [66/120    avg_loss:0.058, val_acc:0.975]
Epoch [67/120    avg_loss:0.049, val_acc:0.986]
Epoch [68/120    avg_loss:0.039, val_acc:0.990]
Epoch [69/120    avg_loss:0.036, val_acc:0.988]
Epoch [70/120    avg_loss:0.044, val_acc:0.986]
Epoch [71/120    avg_loss:0.025, val_acc:0.992]
Epoch [72/120    avg_loss:0.023, val_acc:0.990]
Epoch [73/120    avg_loss:0.018, val_acc:0.990]
Epoch [74/120    avg_loss:0.024, val_acc:0.986]
Epoch [75/120    avg_loss:0.028, val_acc:0.990]
Epoch [76/120    avg_loss:0.017, val_acc:0.992]
Epoch [77/120    avg_loss:0.024, val_acc:0.990]
Epoch [78/120    avg_loss:0.051, val_acc:0.990]
Epoch [79/120    avg_loss:0.069, val_acc:0.986]
Epoch [80/120    avg_loss:0.038, val_acc:0.996]
Epoch [81/120    avg_loss:0.035, val_acc:0.990]
Epoch [82/120    avg_loss:0.025, val_acc:0.992]
Epoch [83/120    avg_loss:0.039, val_acc:0.990]
Epoch [84/120    avg_loss:0.023, val_acc:0.988]
Epoch [85/120    avg_loss:0.028, val_acc:0.990]
Epoch [86/120    avg_loss:0.023, val_acc:0.955]
Epoch [87/120    avg_loss:0.034, val_acc:0.990]
Epoch [88/120    avg_loss:0.022, val_acc:0.992]
Epoch [89/120    avg_loss:0.026, val_acc:0.988]
Epoch [90/120    avg_loss:0.041, val_acc:0.990]
Epoch [91/120    avg_loss:0.057, val_acc:0.980]
Epoch [92/120    avg_loss:0.073, val_acc:0.988]
Epoch [93/120    avg_loss:0.025, val_acc:0.990]
Epoch [94/120    avg_loss:0.025, val_acc:0.990]
Epoch [95/120    avg_loss:0.020, val_acc:0.990]
Epoch [96/120    avg_loss:0.019, val_acc:0.990]
Epoch [97/120    avg_loss:0.016, val_acc:0.990]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.020, val_acc:0.990]
Epoch [100/120    avg_loss:0.015, val_acc:0.990]
Epoch [101/120    avg_loss:0.013, val_acc:0.990]
Epoch [102/120    avg_loss:0.012, val_acc:0.990]
Epoch [103/120    avg_loss:0.015, val_acc:0.990]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.020, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.990]
Epoch [107/120    avg_loss:0.014, val_acc:0.990]
Epoch [108/120    avg_loss:0.016, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.990]
Epoch [110/120    avg_loss:0.019, val_acc:0.990]
Epoch [111/120    avg_loss:0.012, val_acc:0.990]
Epoch [112/120    avg_loss:0.015, val_acc:0.990]
Epoch [113/120    avg_loss:0.013, val_acc:0.990]
Epoch [114/120    avg_loss:0.013, val_acc:0.990]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.015, val_acc:0.990]
Epoch [117/120    avg_loss:0.012, val_acc:0.990]
Epoch [118/120    avg_loss:0.013, val_acc:0.990]
Epoch [119/120    avg_loss:0.015, val_acc:0.990]
Epoch [120/120    avg_loss:0.011, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99853801 0.98648649 1.         0.93913043 0.90140845
 0.99516908 0.96703297 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9914541447853327
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f539f6d8e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.299]
Epoch [2/120    avg_loss:2.509, val_acc:0.309]
Epoch [3/120    avg_loss:2.406, val_acc:0.324]
Epoch [4/120    avg_loss:2.299, val_acc:0.371]
Epoch [5/120    avg_loss:2.184, val_acc:0.426]
Epoch [6/120    avg_loss:2.055, val_acc:0.463]
Epoch [7/120    avg_loss:1.921, val_acc:0.537]
Epoch [8/120    avg_loss:1.773, val_acc:0.594]
Epoch [9/120    avg_loss:1.552, val_acc:0.633]
Epoch [10/120    avg_loss:1.369, val_acc:0.689]
Epoch [11/120    avg_loss:1.247, val_acc:0.695]
Epoch [12/120    avg_loss:1.144, val_acc:0.768]
Epoch [13/120    avg_loss:1.005, val_acc:0.820]
Epoch [14/120    avg_loss:0.860, val_acc:0.807]
Epoch [15/120    avg_loss:0.777, val_acc:0.879]
Epoch [16/120    avg_loss:0.766, val_acc:0.842]
Epoch [17/120    avg_loss:0.613, val_acc:0.848]
Epoch [18/120    avg_loss:0.604, val_acc:0.828]
Epoch [19/120    avg_loss:0.471, val_acc:0.910]
Epoch [20/120    avg_loss:0.517, val_acc:0.877]
Epoch [21/120    avg_loss:0.407, val_acc:0.934]
Epoch [22/120    avg_loss:0.463, val_acc:0.930]
Epoch [23/120    avg_loss:0.384, val_acc:0.918]
Epoch [24/120    avg_loss:0.394, val_acc:0.916]
Epoch [25/120    avg_loss:0.338, val_acc:0.934]
Epoch [26/120    avg_loss:0.321, val_acc:0.951]
Epoch [27/120    avg_loss:0.277, val_acc:0.943]
Epoch [28/120    avg_loss:0.342, val_acc:0.912]
Epoch [29/120    avg_loss:0.254, val_acc:0.943]
Epoch [30/120    avg_loss:0.235, val_acc:0.955]
Epoch [31/120    avg_loss:0.257, val_acc:0.953]
Epoch [32/120    avg_loss:0.240, val_acc:0.977]
Epoch [33/120    avg_loss:0.220, val_acc:0.953]
Epoch [34/120    avg_loss:0.180, val_acc:0.939]
Epoch [35/120    avg_loss:0.216, val_acc:0.951]
Epoch [36/120    avg_loss:0.163, val_acc:0.963]
Epoch [37/120    avg_loss:0.152, val_acc:0.973]
Epoch [38/120    avg_loss:0.151, val_acc:0.951]
Epoch [39/120    avg_loss:0.199, val_acc:0.945]
Epoch [40/120    avg_loss:0.148, val_acc:0.967]
Epoch [41/120    avg_loss:0.172, val_acc:0.961]
Epoch [42/120    avg_loss:0.144, val_acc:0.916]
Epoch [43/120    avg_loss:0.176, val_acc:0.959]
Epoch [44/120    avg_loss:0.182, val_acc:0.975]
Epoch [45/120    avg_loss:0.083, val_acc:0.975]
Epoch [46/120    avg_loss:0.076, val_acc:0.986]
Epoch [47/120    avg_loss:0.071, val_acc:0.986]
Epoch [48/120    avg_loss:0.077, val_acc:0.980]
Epoch [49/120    avg_loss:0.068, val_acc:0.980]
Epoch [50/120    avg_loss:0.071, val_acc:0.980]
Epoch [51/120    avg_loss:0.069, val_acc:0.980]
Epoch [52/120    avg_loss:0.065, val_acc:0.986]
Epoch [53/120    avg_loss:0.072, val_acc:0.977]
Epoch [54/120    avg_loss:0.065, val_acc:0.980]
Epoch [55/120    avg_loss:0.059, val_acc:0.977]
Epoch [56/120    avg_loss:0.063, val_acc:0.982]
Epoch [57/120    avg_loss:0.056, val_acc:0.982]
Epoch [58/120    avg_loss:0.055, val_acc:0.982]
Epoch [59/120    avg_loss:0.053, val_acc:0.980]
Epoch [60/120    avg_loss:0.053, val_acc:0.980]
Epoch [61/120    avg_loss:0.057, val_acc:0.982]
Epoch [62/120    avg_loss:0.062, val_acc:0.980]
Epoch [63/120    avg_loss:0.062, val_acc:0.984]
Epoch [64/120    avg_loss:0.060, val_acc:0.982]
Epoch [65/120    avg_loss:0.059, val_acc:0.980]
Epoch [66/120    avg_loss:0.056, val_acc:0.980]
Epoch [67/120    avg_loss:0.057, val_acc:0.980]
Epoch [68/120    avg_loss:0.054, val_acc:0.980]
Epoch [69/120    avg_loss:0.055, val_acc:0.980]
Epoch [70/120    avg_loss:0.060, val_acc:0.980]
Epoch [71/120    avg_loss:0.065, val_acc:0.980]
Epoch [72/120    avg_loss:0.058, val_acc:0.980]
Epoch [73/120    avg_loss:0.057, val_acc:0.980]
Epoch [74/120    avg_loss:0.051, val_acc:0.980]
Epoch [75/120    avg_loss:0.049, val_acc:0.980]
Epoch [76/120    avg_loss:0.054, val_acc:0.980]
Epoch [77/120    avg_loss:0.052, val_acc:0.980]
Epoch [78/120    avg_loss:0.050, val_acc:0.980]
Epoch [79/120    avg_loss:0.052, val_acc:0.980]
Epoch [80/120    avg_loss:0.060, val_acc:0.980]
Epoch [81/120    avg_loss:0.057, val_acc:0.980]
Epoch [82/120    avg_loss:0.052, val_acc:0.980]
Epoch [83/120    avg_loss:0.056, val_acc:0.980]
Epoch [84/120    avg_loss:0.059, val_acc:0.980]
Epoch [85/120    avg_loss:0.062, val_acc:0.980]
Epoch [86/120    avg_loss:0.051, val_acc:0.980]
Epoch [87/120    avg_loss:0.052, val_acc:0.980]
Epoch [88/120    avg_loss:0.057, val_acc:0.980]
Epoch [89/120    avg_loss:0.053, val_acc:0.980]
Epoch [90/120    avg_loss:0.054, val_acc:0.980]
Epoch [91/120    avg_loss:0.058, val_acc:0.980]
Epoch [92/120    avg_loss:0.047, val_acc:0.980]
Epoch [93/120    avg_loss:0.074, val_acc:0.980]
Epoch [94/120    avg_loss:0.053, val_acc:0.980]
Epoch [95/120    avg_loss:0.061, val_acc:0.980]
Epoch [96/120    avg_loss:0.059, val_acc:0.980]
Epoch [97/120    avg_loss:0.054, val_acc:0.980]
Epoch [98/120    avg_loss:0.045, val_acc:0.980]
Epoch [99/120    avg_loss:0.057, val_acc:0.980]
Epoch [100/120    avg_loss:0.054, val_acc:0.980]
Epoch [101/120    avg_loss:0.052, val_acc:0.980]
Epoch [102/120    avg_loss:0.055, val_acc:0.980]
Epoch [103/120    avg_loss:0.053, val_acc:0.980]
Epoch [104/120    avg_loss:0.050, val_acc:0.980]
Epoch [105/120    avg_loss:0.050, val_acc:0.980]
Epoch [106/120    avg_loss:0.063, val_acc:0.980]
Epoch [107/120    avg_loss:0.054, val_acc:0.980]
Epoch [108/120    avg_loss:0.060, val_acc:0.980]
Epoch [109/120    avg_loss:0.049, val_acc:0.980]
Epoch [110/120    avg_loss:0.065, val_acc:0.980]
Epoch [111/120    avg_loss:0.052, val_acc:0.980]
Epoch [112/120    avg_loss:0.057, val_acc:0.980]
Epoch [113/120    avg_loss:0.057, val_acc:0.980]
Epoch [114/120    avg_loss:0.050, val_acc:0.980]
Epoch [115/120    avg_loss:0.058, val_acc:0.980]
Epoch [116/120    avg_loss:0.060, val_acc:0.980]
Epoch [117/120    avg_loss:0.056, val_acc:0.980]
Epoch [118/120    avg_loss:0.064, val_acc:0.980]
Epoch [119/120    avg_loss:0.055, val_acc:0.980]
Epoch [120/120    avg_loss:0.054, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99854227 0.99319728 1.         0.94545455 0.92105263
 0.99512195 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9931158733504936
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f52263c2dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.309]
Epoch [2/120    avg_loss:2.471, val_acc:0.369]
Epoch [3/120    avg_loss:2.330, val_acc:0.428]
Epoch [4/120    avg_loss:2.205, val_acc:0.484]
Epoch [5/120    avg_loss:2.054, val_acc:0.533]
Epoch [6/120    avg_loss:1.904, val_acc:0.551]
Epoch [7/120    avg_loss:1.751, val_acc:0.594]
Epoch [8/120    avg_loss:1.595, val_acc:0.660]
Epoch [9/120    avg_loss:1.445, val_acc:0.703]
Epoch [10/120    avg_loss:1.312, val_acc:0.719]
Epoch [11/120    avg_loss:1.173, val_acc:0.748]
Epoch [12/120    avg_loss:1.020, val_acc:0.734]
Epoch [13/120    avg_loss:0.859, val_acc:0.834]
Epoch [14/120    avg_loss:0.741, val_acc:0.799]
Epoch [15/120    avg_loss:0.650, val_acc:0.883]
Epoch [16/120    avg_loss:0.628, val_acc:0.842]
Epoch [17/120    avg_loss:0.583, val_acc:0.873]
Epoch [18/120    avg_loss:0.524, val_acc:0.898]
Epoch [19/120    avg_loss:0.467, val_acc:0.852]
Epoch [20/120    avg_loss:0.472, val_acc:0.924]
Epoch [21/120    avg_loss:0.389, val_acc:0.912]
Epoch [22/120    avg_loss:0.350, val_acc:0.934]
Epoch [23/120    avg_loss:0.377, val_acc:0.922]
Epoch [24/120    avg_loss:0.298, val_acc:0.906]
Epoch [25/120    avg_loss:0.250, val_acc:0.945]
Epoch [26/120    avg_loss:0.271, val_acc:0.955]
Epoch [27/120    avg_loss:0.257, val_acc:0.924]
Epoch [28/120    avg_loss:0.228, val_acc:0.963]
Epoch [29/120    avg_loss:0.203, val_acc:0.943]
Epoch [30/120    avg_loss:0.185, val_acc:0.947]
Epoch [31/120    avg_loss:0.204, val_acc:0.955]
Epoch [32/120    avg_loss:0.218, val_acc:0.963]
Epoch [33/120    avg_loss:0.184, val_acc:0.953]
Epoch [34/120    avg_loss:0.194, val_acc:0.779]
Epoch [35/120    avg_loss:0.568, val_acc:0.924]
Epoch [36/120    avg_loss:0.231, val_acc:0.959]
Epoch [37/120    avg_loss:0.167, val_acc:0.967]
Epoch [38/120    avg_loss:0.152, val_acc:0.957]
Epoch [39/120    avg_loss:0.206, val_acc:0.953]
Epoch [40/120    avg_loss:0.272, val_acc:0.939]
Epoch [41/120    avg_loss:0.188, val_acc:0.969]
Epoch [42/120    avg_loss:0.160, val_acc:0.980]
Epoch [43/120    avg_loss:0.126, val_acc:0.973]
Epoch [44/120    avg_loss:0.104, val_acc:0.967]
Epoch [45/120    avg_loss:0.117, val_acc:0.986]
Epoch [46/120    avg_loss:0.104, val_acc:0.973]
Epoch [47/120    avg_loss:0.106, val_acc:0.977]
Epoch [48/120    avg_loss:0.095, val_acc:0.975]
Epoch [49/120    avg_loss:0.111, val_acc:0.973]
Epoch [50/120    avg_loss:0.115, val_acc:0.965]
Epoch [51/120    avg_loss:0.120, val_acc:0.977]
Epoch [52/120    avg_loss:0.102, val_acc:0.980]
Epoch [53/120    avg_loss:0.103, val_acc:0.973]
Epoch [54/120    avg_loss:0.068, val_acc:0.977]
Epoch [55/120    avg_loss:0.117, val_acc:0.945]
Epoch [56/120    avg_loss:0.084, val_acc:0.967]
Epoch [57/120    avg_loss:0.097, val_acc:0.975]
Epoch [58/120    avg_loss:0.069, val_acc:0.980]
Epoch [59/120    avg_loss:0.079, val_acc:0.986]
Epoch [60/120    avg_loss:0.049, val_acc:0.988]
Epoch [61/120    avg_loss:0.052, val_acc:0.988]
Epoch [62/120    avg_loss:0.055, val_acc:0.988]
Epoch [63/120    avg_loss:0.048, val_acc:0.988]
Epoch [64/120    avg_loss:0.047, val_acc:0.988]
Epoch [65/120    avg_loss:0.049, val_acc:0.990]
Epoch [66/120    avg_loss:0.047, val_acc:0.990]
Epoch [67/120    avg_loss:0.048, val_acc:0.992]
Epoch [68/120    avg_loss:0.041, val_acc:0.994]
Epoch [69/120    avg_loss:0.039, val_acc:0.992]
Epoch [70/120    avg_loss:0.038, val_acc:0.994]
Epoch [71/120    avg_loss:0.044, val_acc:0.990]
Epoch [72/120    avg_loss:0.047, val_acc:0.990]
Epoch [73/120    avg_loss:0.042, val_acc:0.990]
Epoch [74/120    avg_loss:0.050, val_acc:0.988]
Epoch [75/120    avg_loss:0.035, val_acc:0.990]
Epoch [76/120    avg_loss:0.045, val_acc:0.994]
Epoch [77/120    avg_loss:0.038, val_acc:0.994]
Epoch [78/120    avg_loss:0.035, val_acc:0.994]
Epoch [79/120    avg_loss:0.036, val_acc:0.994]
Epoch [80/120    avg_loss:0.034, val_acc:0.992]
Epoch [81/120    avg_loss:0.042, val_acc:0.990]
Epoch [82/120    avg_loss:0.032, val_acc:0.990]
Epoch [83/120    avg_loss:0.034, val_acc:0.992]
Epoch [84/120    avg_loss:0.032, val_acc:0.992]
Epoch [85/120    avg_loss:0.040, val_acc:0.990]
Epoch [86/120    avg_loss:0.040, val_acc:0.992]
Epoch [87/120    avg_loss:0.036, val_acc:0.990]
Epoch [88/120    avg_loss:0.031, val_acc:0.992]
Epoch [89/120    avg_loss:0.034, val_acc:0.992]
Epoch [90/120    avg_loss:0.029, val_acc:0.990]
Epoch [91/120    avg_loss:0.032, val_acc:0.992]
Epoch [92/120    avg_loss:0.034, val_acc:0.992]
Epoch [93/120    avg_loss:0.034, val_acc:0.992]
Epoch [94/120    avg_loss:0.034, val_acc:0.992]
Epoch [95/120    avg_loss:0.029, val_acc:0.992]
Epoch [96/120    avg_loss:0.033, val_acc:0.992]
Epoch [97/120    avg_loss:0.030, val_acc:0.992]
Epoch [98/120    avg_loss:0.029, val_acc:0.992]
Epoch [99/120    avg_loss:0.031, val_acc:0.992]
Epoch [100/120    avg_loss:0.031, val_acc:0.992]
Epoch [101/120    avg_loss:0.026, val_acc:0.992]
Epoch [102/120    avg_loss:0.029, val_acc:0.992]
Epoch [103/120    avg_loss:0.031, val_acc:0.992]
Epoch [104/120    avg_loss:0.030, val_acc:0.990]
Epoch [105/120    avg_loss:0.036, val_acc:0.990]
Epoch [106/120    avg_loss:0.030, val_acc:0.990]
Epoch [107/120    avg_loss:0.028, val_acc:0.990]
Epoch [108/120    avg_loss:0.031, val_acc:0.990]
Epoch [109/120    avg_loss:0.034, val_acc:0.990]
Epoch [110/120    avg_loss:0.034, val_acc:0.990]
Epoch [111/120    avg_loss:0.025, val_acc:0.990]
Epoch [112/120    avg_loss:0.035, val_acc:0.992]
Epoch [113/120    avg_loss:0.031, val_acc:0.990]
Epoch [114/120    avg_loss:0.028, val_acc:0.990]
Epoch [115/120    avg_loss:0.041, val_acc:0.990]
Epoch [116/120    avg_loss:0.030, val_acc:0.990]
Epoch [117/120    avg_loss:0.033, val_acc:0.990]
Epoch [118/120    avg_loss:0.032, val_acc:0.990]
Epoch [119/120    avg_loss:0.034, val_acc:0.990]
Epoch [120/120    avg_loss:0.033, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   2 210  11   0   0   0   0   0   0   4   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   4   0   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99635834 0.97986577 0.995671   0.94808126 0.94237288
 0.99757869 0.94972067 0.99481865 1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.991215848466122
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f782fddd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.311]
Epoch [2/120    avg_loss:2.540, val_acc:0.416]
Epoch [3/120    avg_loss:2.474, val_acc:0.471]
Epoch [4/120    avg_loss:2.401, val_acc:0.488]
Epoch [5/120    avg_loss:2.341, val_acc:0.479]
Epoch [6/120    avg_loss:2.266, val_acc:0.443]
Epoch [7/120    avg_loss:2.182, val_acc:0.465]
Epoch [8/120    avg_loss:2.086, val_acc:0.463]
Epoch [9/120    avg_loss:1.983, val_acc:0.584]
Epoch [10/120    avg_loss:1.924, val_acc:0.678]
Epoch [11/120    avg_loss:1.790, val_acc:0.689]
Epoch [12/120    avg_loss:1.651, val_acc:0.701]
Epoch [13/120    avg_loss:1.489, val_acc:0.785]
Epoch [14/120    avg_loss:1.397, val_acc:0.787]
Epoch [15/120    avg_loss:1.307, val_acc:0.812]
Epoch [16/120    avg_loss:1.126, val_acc:0.840]
Epoch [17/120    avg_loss:0.994, val_acc:0.846]
Epoch [18/120    avg_loss:0.908, val_acc:0.904]
Epoch [19/120    avg_loss:0.867, val_acc:0.879]
Epoch [20/120    avg_loss:0.765, val_acc:0.871]
Epoch [21/120    avg_loss:0.694, val_acc:0.869]
Epoch [22/120    avg_loss:0.683, val_acc:0.881]
Epoch [23/120    avg_loss:0.644, val_acc:0.910]
Epoch [24/120    avg_loss:0.557, val_acc:0.898]
Epoch [25/120    avg_loss:0.561, val_acc:0.875]
Epoch [26/120    avg_loss:0.498, val_acc:0.885]
Epoch [27/120    avg_loss:0.452, val_acc:0.891]
Epoch [28/120    avg_loss:0.477, val_acc:0.912]
Epoch [29/120    avg_loss:0.421, val_acc:0.898]
Epoch [30/120    avg_loss:0.404, val_acc:0.924]
Epoch [31/120    avg_loss:0.367, val_acc:0.926]
Epoch [32/120    avg_loss:0.398, val_acc:0.922]
Epoch [33/120    avg_loss:0.437, val_acc:0.926]
Epoch [34/120    avg_loss:0.392, val_acc:0.938]
Epoch [35/120    avg_loss:0.354, val_acc:0.938]
Epoch [36/120    avg_loss:0.354, val_acc:0.934]
Epoch [37/120    avg_loss:0.333, val_acc:0.939]
Epoch [38/120    avg_loss:0.427, val_acc:0.928]
Epoch [39/120    avg_loss:0.332, val_acc:0.902]
Epoch [40/120    avg_loss:0.299, val_acc:0.953]
Epoch [41/120    avg_loss:0.352, val_acc:0.904]
Epoch [42/120    avg_loss:0.336, val_acc:0.941]
Epoch [43/120    avg_loss:0.284, val_acc:0.934]
Epoch [44/120    avg_loss:0.272, val_acc:0.949]
Epoch [45/120    avg_loss:0.268, val_acc:0.936]
Epoch [46/120    avg_loss:0.296, val_acc:0.951]
Epoch [47/120    avg_loss:0.273, val_acc:0.904]
Epoch [48/120    avg_loss:0.326, val_acc:0.943]
Epoch [49/120    avg_loss:0.311, val_acc:0.936]
Epoch [50/120    avg_loss:0.189, val_acc:0.959]
Epoch [51/120    avg_loss:0.249, val_acc:0.965]
Epoch [52/120    avg_loss:0.193, val_acc:0.967]
Epoch [53/120    avg_loss:0.257, val_acc:0.965]
Epoch [54/120    avg_loss:0.174, val_acc:0.955]
Epoch [55/120    avg_loss:0.245, val_acc:0.945]
Epoch [56/120    avg_loss:0.242, val_acc:0.967]
Epoch [57/120    avg_loss:0.190, val_acc:0.965]
Epoch [58/120    avg_loss:0.179, val_acc:0.953]
Epoch [59/120    avg_loss:0.170, val_acc:0.961]
Epoch [60/120    avg_loss:0.146, val_acc:0.977]
Epoch [61/120    avg_loss:0.158, val_acc:0.959]
Epoch [62/120    avg_loss:0.129, val_acc:0.980]
Epoch [63/120    avg_loss:0.117, val_acc:0.977]
Epoch [64/120    avg_loss:0.121, val_acc:0.980]
Epoch [65/120    avg_loss:0.147, val_acc:0.977]
Epoch [66/120    avg_loss:0.166, val_acc:0.945]
Epoch [67/120    avg_loss:0.200, val_acc:0.967]
Epoch [68/120    avg_loss:0.155, val_acc:0.984]
Epoch [69/120    avg_loss:0.107, val_acc:0.979]
Epoch [70/120    avg_loss:0.193, val_acc:0.967]
Epoch [71/120    avg_loss:0.220, val_acc:0.961]
Epoch [72/120    avg_loss:0.180, val_acc:0.975]
Epoch [73/120    avg_loss:0.106, val_acc:0.977]
Epoch [74/120    avg_loss:0.105, val_acc:0.969]
Epoch [75/120    avg_loss:0.102, val_acc:0.967]
Epoch [76/120    avg_loss:0.172, val_acc:0.961]
Epoch [77/120    avg_loss:0.143, val_acc:0.984]
Epoch [78/120    avg_loss:0.092, val_acc:0.971]
Epoch [79/120    avg_loss:0.128, val_acc:0.949]
Epoch [80/120    avg_loss:0.105, val_acc:0.986]
Epoch [81/120    avg_loss:0.076, val_acc:0.986]
Epoch [82/120    avg_loss:0.068, val_acc:0.975]
Epoch [83/120    avg_loss:0.087, val_acc:0.984]
Epoch [84/120    avg_loss:0.079, val_acc:0.984]
Epoch [85/120    avg_loss:0.053, val_acc:0.984]
Epoch [86/120    avg_loss:0.067, val_acc:0.984]
Epoch [87/120    avg_loss:0.081, val_acc:0.980]
Epoch [88/120    avg_loss:0.106, val_acc:0.971]
Epoch [89/120    avg_loss:0.110, val_acc:0.979]
Epoch [90/120    avg_loss:0.078, val_acc:0.986]
Epoch [91/120    avg_loss:0.120, val_acc:0.941]
Epoch [92/120    avg_loss:0.142, val_acc:0.969]
Epoch [93/120    avg_loss:0.119, val_acc:0.973]
Epoch [94/120    avg_loss:0.131, val_acc:0.957]
Epoch [95/120    avg_loss:0.229, val_acc:0.934]
Epoch [96/120    avg_loss:0.179, val_acc:0.957]
Epoch [97/120    avg_loss:0.101, val_acc:0.982]
Epoch [98/120    avg_loss:0.061, val_acc:0.977]
Epoch [99/120    avg_loss:0.066, val_acc:0.984]
Epoch [100/120    avg_loss:0.080, val_acc:0.977]
Epoch [101/120    avg_loss:0.079, val_acc:0.977]
Epoch [102/120    avg_loss:0.075, val_acc:0.977]
Epoch [103/120    avg_loss:0.051, val_acc:0.982]
Epoch [104/120    avg_loss:0.049, val_acc:0.984]
Epoch [105/120    avg_loss:0.041, val_acc:0.986]
Epoch [106/120    avg_loss:0.044, val_acc:0.986]
Epoch [107/120    avg_loss:0.041, val_acc:0.986]
Epoch [108/120    avg_loss:0.039, val_acc:0.986]
Epoch [109/120    avg_loss:0.030, val_acc:0.988]
Epoch [110/120    avg_loss:0.037, val_acc:0.988]
Epoch [111/120    avg_loss:0.037, val_acc:0.988]
Epoch [112/120    avg_loss:0.033, val_acc:0.988]
Epoch [113/120    avg_loss:0.040, val_acc:0.988]
Epoch [114/120    avg_loss:0.035, val_acc:0.988]
Epoch [115/120    avg_loss:0.031, val_acc:0.990]
Epoch [116/120    avg_loss:0.037, val_acc:0.990]
Epoch [117/120    avg_loss:0.032, val_acc:0.988]
Epoch [118/120    avg_loss:0.029, val_acc:0.986]
Epoch [119/120    avg_loss:0.030, val_acc:0.984]
Epoch [120/120    avg_loss:0.029, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   0 222   4   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 0.997815   0.94090909 0.98230088 0.94017094 0.91428571
 0.99266504 0.86021505 0.99487179 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9852799634221417
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff84aed0da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.094]
Epoch [2/120    avg_loss:2.568, val_acc:0.254]
Epoch [3/120    avg_loss:2.523, val_acc:0.309]
Epoch [4/120    avg_loss:2.472, val_acc:0.311]
Epoch [5/120    avg_loss:2.411, val_acc:0.324]
Epoch [6/120    avg_loss:2.339, val_acc:0.486]
Epoch [7/120    avg_loss:2.259, val_acc:0.594]
Epoch [8/120    avg_loss:2.175, val_acc:0.615]
Epoch [9/120    avg_loss:2.082, val_acc:0.660]
Epoch [10/120    avg_loss:1.979, val_acc:0.652]
Epoch [11/120    avg_loss:1.897, val_acc:0.738]
Epoch [12/120    avg_loss:1.745, val_acc:0.766]
Epoch [13/120    avg_loss:1.620, val_acc:0.732]
Epoch [14/120    avg_loss:1.472, val_acc:0.795]
Epoch [15/120    avg_loss:1.347, val_acc:0.834]
Epoch [16/120    avg_loss:1.192, val_acc:0.828]
Epoch [17/120    avg_loss:1.074, val_acc:0.879]
Epoch [18/120    avg_loss:0.958, val_acc:0.854]
Epoch [19/120    avg_loss:0.904, val_acc:0.840]
Epoch [20/120    avg_loss:0.805, val_acc:0.896]
Epoch [21/120    avg_loss:0.737, val_acc:0.861]
Epoch [22/120    avg_loss:0.676, val_acc:0.898]
Epoch [23/120    avg_loss:0.613, val_acc:0.924]
Epoch [24/120    avg_loss:0.590, val_acc:0.912]
Epoch [25/120    avg_loss:0.523, val_acc:0.896]
Epoch [26/120    avg_loss:0.498, val_acc:0.900]
Epoch [27/120    avg_loss:0.476, val_acc:0.879]
Epoch [28/120    avg_loss:0.509, val_acc:0.924]
Epoch [29/120    avg_loss:0.451, val_acc:0.900]
Epoch [30/120    avg_loss:0.437, val_acc:0.920]
Epoch [31/120    avg_loss:0.452, val_acc:0.904]
Epoch [32/120    avg_loss:0.443, val_acc:0.941]
Epoch [33/120    avg_loss:0.387, val_acc:0.920]
Epoch [34/120    avg_loss:0.353, val_acc:0.943]
Epoch [35/120    avg_loss:0.373, val_acc:0.932]
Epoch [36/120    avg_loss:0.347, val_acc:0.930]
Epoch [37/120    avg_loss:0.348, val_acc:0.941]
Epoch [38/120    avg_loss:0.334, val_acc:0.951]
Epoch [39/120    avg_loss:0.283, val_acc:0.961]
Epoch [40/120    avg_loss:0.292, val_acc:0.951]
Epoch [41/120    avg_loss:0.266, val_acc:0.947]
Epoch [42/120    avg_loss:0.255, val_acc:0.914]
Epoch [43/120    avg_loss:0.311, val_acc:0.943]
Epoch [44/120    avg_loss:0.260, val_acc:0.947]
Epoch [45/120    avg_loss:0.266, val_acc:0.953]
Epoch [46/120    avg_loss:0.218, val_acc:0.965]
Epoch [47/120    avg_loss:0.240, val_acc:0.906]
Epoch [48/120    avg_loss:0.258, val_acc:0.969]
Epoch [49/120    avg_loss:0.183, val_acc:0.961]
Epoch [50/120    avg_loss:0.229, val_acc:0.918]
Epoch [51/120    avg_loss:0.243, val_acc:0.918]
Epoch [52/120    avg_loss:0.206, val_acc:0.953]
Epoch [53/120    avg_loss:0.206, val_acc:0.955]
Epoch [54/120    avg_loss:0.212, val_acc:0.947]
Epoch [55/120    avg_loss:0.181, val_acc:0.926]
Epoch [56/120    avg_loss:0.208, val_acc:0.961]
Epoch [57/120    avg_loss:0.197, val_acc:0.963]
Epoch [58/120    avg_loss:0.167, val_acc:0.938]
Epoch [59/120    avg_loss:0.148, val_acc:0.969]
Epoch [60/120    avg_loss:0.148, val_acc:0.953]
Epoch [61/120    avg_loss:0.163, val_acc:0.965]
Epoch [62/120    avg_loss:0.128, val_acc:0.975]
Epoch [63/120    avg_loss:0.105, val_acc:0.984]
Epoch [64/120    avg_loss:0.118, val_acc:0.969]
Epoch [65/120    avg_loss:0.166, val_acc:0.945]
Epoch [66/120    avg_loss:0.161, val_acc:0.975]
Epoch [67/120    avg_loss:0.125, val_acc:0.969]
Epoch [68/120    avg_loss:0.128, val_acc:0.961]
Epoch [69/120    avg_loss:0.170, val_acc:0.957]
Epoch [70/120    avg_loss:0.114, val_acc:0.979]
Epoch [71/120    avg_loss:0.134, val_acc:0.975]
Epoch [72/120    avg_loss:0.160, val_acc:0.982]
Epoch [73/120    avg_loss:0.094, val_acc:0.980]
Epoch [74/120    avg_loss:0.093, val_acc:0.977]
Epoch [75/120    avg_loss:0.097, val_acc:0.975]
Epoch [76/120    avg_loss:0.111, val_acc:0.975]
Epoch [77/120    avg_loss:0.083, val_acc:0.986]
Epoch [78/120    avg_loss:0.059, val_acc:0.990]
Epoch [79/120    avg_loss:0.058, val_acc:0.994]
Epoch [80/120    avg_loss:0.056, val_acc:0.992]
Epoch [81/120    avg_loss:0.044, val_acc:0.988]
Epoch [82/120    avg_loss:0.053, val_acc:0.990]
Epoch [83/120    avg_loss:0.060, val_acc:0.986]
Epoch [84/120    avg_loss:0.051, val_acc:0.992]
Epoch [85/120    avg_loss:0.049, val_acc:0.992]
Epoch [86/120    avg_loss:0.060, val_acc:0.992]
Epoch [87/120    avg_loss:0.046, val_acc:0.992]
Epoch [88/120    avg_loss:0.045, val_acc:0.992]
Epoch [89/120    avg_loss:0.053, val_acc:0.990]
Epoch [90/120    avg_loss:0.039, val_acc:0.992]
Epoch [91/120    avg_loss:0.044, val_acc:0.988]
Epoch [92/120    avg_loss:0.050, val_acc:0.992]
Epoch [93/120    avg_loss:0.040, val_acc:0.992]
Epoch [94/120    avg_loss:0.040, val_acc:0.992]
Epoch [95/120    avg_loss:0.043, val_acc:0.992]
Epoch [96/120    avg_loss:0.048, val_acc:0.992]
Epoch [97/120    avg_loss:0.040, val_acc:0.992]
Epoch [98/120    avg_loss:0.040, val_acc:0.992]
Epoch [99/120    avg_loss:0.048, val_acc:0.992]
Epoch [100/120    avg_loss:0.042, val_acc:0.992]
Epoch [101/120    avg_loss:0.043, val_acc:0.992]
Epoch [102/120    avg_loss:0.046, val_acc:0.992]
Epoch [103/120    avg_loss:0.047, val_acc:0.992]
Epoch [104/120    avg_loss:0.042, val_acc:0.992]
Epoch [105/120    avg_loss:0.044, val_acc:0.992]
Epoch [106/120    avg_loss:0.043, val_acc:0.992]
Epoch [107/120    avg_loss:0.048, val_acc:0.992]
Epoch [108/120    avg_loss:0.045, val_acc:0.992]
Epoch [109/120    avg_loss:0.050, val_acc:0.992]
Epoch [110/120    avg_loss:0.041, val_acc:0.992]
Epoch [111/120    avg_loss:0.038, val_acc:0.992]
Epoch [112/120    avg_loss:0.037, val_acc:0.992]
Epoch [113/120    avg_loss:0.045, val_acc:0.992]
Epoch [114/120    avg_loss:0.041, val_acc:0.992]
Epoch [115/120    avg_loss:0.037, val_acc:0.992]
Epoch [116/120    avg_loss:0.043, val_acc:0.992]
Epoch [117/120    avg_loss:0.046, val_acc:0.992]
Epoch [118/120    avg_loss:0.043, val_acc:0.992]
Epoch [119/120    avg_loss:0.038, val_acc:0.992]
Epoch [120/120    avg_loss:0.040, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 222   5   0   0   0   1   2   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  28 115   2   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 1.         0.94222222 0.98230088 0.90985325 0.85185185
 0.99516908 0.89617486 0.99483204 0.9978678  1.         1.
 0.99556541 1.        ]

Kappa:
0.9824325284013615
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fce1a3fee10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.354]
Epoch [2/120    avg_loss:2.545, val_acc:0.369]
Epoch [3/120    avg_loss:2.476, val_acc:0.369]
Epoch [4/120    avg_loss:2.399, val_acc:0.398]
Epoch [5/120    avg_loss:2.322, val_acc:0.479]
Epoch [6/120    avg_loss:2.246, val_acc:0.605]
Epoch [7/120    avg_loss:2.147, val_acc:0.633]
Epoch [8/120    avg_loss:2.065, val_acc:0.666]
Epoch [9/120    avg_loss:1.944, val_acc:0.674]
Epoch [10/120    avg_loss:1.817, val_acc:0.699]
Epoch [11/120    avg_loss:1.719, val_acc:0.682]
Epoch [12/120    avg_loss:1.580, val_acc:0.723]
Epoch [13/120    avg_loss:1.452, val_acc:0.756]
Epoch [14/120    avg_loss:1.324, val_acc:0.787]
Epoch [15/120    avg_loss:1.194, val_acc:0.791]
Epoch [16/120    avg_loss:1.079, val_acc:0.807]
Epoch [17/120    avg_loss:1.073, val_acc:0.799]
Epoch [18/120    avg_loss:0.941, val_acc:0.855]
Epoch [19/120    avg_loss:0.818, val_acc:0.844]
Epoch [20/120    avg_loss:0.755, val_acc:0.891]
Epoch [21/120    avg_loss:0.634, val_acc:0.865]
Epoch [22/120    avg_loss:0.628, val_acc:0.863]
Epoch [23/120    avg_loss:0.647, val_acc:0.887]
Epoch [24/120    avg_loss:0.560, val_acc:0.904]
Epoch [25/120    avg_loss:0.516, val_acc:0.900]
Epoch [26/120    avg_loss:0.504, val_acc:0.904]
Epoch [27/120    avg_loss:0.463, val_acc:0.906]
Epoch [28/120    avg_loss:0.449, val_acc:0.906]
Epoch [29/120    avg_loss:0.481, val_acc:0.889]
Epoch [30/120    avg_loss:0.472, val_acc:0.920]
Epoch [31/120    avg_loss:0.412, val_acc:0.922]
Epoch [32/120    avg_loss:0.386, val_acc:0.922]
Epoch [33/120    avg_loss:0.354, val_acc:0.932]
Epoch [34/120    avg_loss:0.454, val_acc:0.797]
Epoch [35/120    avg_loss:0.381, val_acc:0.906]
Epoch [36/120    avg_loss:0.332, val_acc:0.918]
Epoch [37/120    avg_loss:0.301, val_acc:0.910]
Epoch [38/120    avg_loss:0.295, val_acc:0.951]
Epoch [39/120    avg_loss:0.278, val_acc:0.947]
Epoch [40/120    avg_loss:0.328, val_acc:0.924]
Epoch [41/120    avg_loss:0.275, val_acc:0.949]
Epoch [42/120    avg_loss:0.251, val_acc:0.943]
Epoch [43/120    avg_loss:0.298, val_acc:0.945]
Epoch [44/120    avg_loss:0.264, val_acc:0.947]
Epoch [45/120    avg_loss:0.237, val_acc:0.936]
Epoch [46/120    avg_loss:0.212, val_acc:0.934]
Epoch [47/120    avg_loss:0.235, val_acc:0.955]
Epoch [48/120    avg_loss:0.212, val_acc:0.947]
Epoch [49/120    avg_loss:0.213, val_acc:0.955]
Epoch [50/120    avg_loss:0.218, val_acc:0.947]
Epoch [51/120    avg_loss:0.219, val_acc:0.947]
Epoch [52/120    avg_loss:0.251, val_acc:0.914]
Epoch [53/120    avg_loss:0.205, val_acc:0.959]
Epoch [54/120    avg_loss:0.171, val_acc:0.959]
Epoch [55/120    avg_loss:0.203, val_acc:0.963]
Epoch [56/120    avg_loss:0.143, val_acc:0.961]
Epoch [57/120    avg_loss:0.155, val_acc:0.957]
Epoch [58/120    avg_loss:0.138, val_acc:0.967]
Epoch [59/120    avg_loss:0.230, val_acc:0.938]
Epoch [60/120    avg_loss:0.185, val_acc:0.961]
Epoch [61/120    avg_loss:0.168, val_acc:0.955]
Epoch [62/120    avg_loss:0.168, val_acc:0.934]
Epoch [63/120    avg_loss:0.192, val_acc:0.953]
Epoch [64/120    avg_loss:0.143, val_acc:0.977]
Epoch [65/120    avg_loss:0.132, val_acc:0.971]
Epoch [66/120    avg_loss:0.137, val_acc:0.969]
Epoch [67/120    avg_loss:0.119, val_acc:0.977]
Epoch [68/120    avg_loss:0.141, val_acc:0.971]
Epoch [69/120    avg_loss:0.105, val_acc:0.969]
Epoch [70/120    avg_loss:0.092, val_acc:0.975]
Epoch [71/120    avg_loss:0.068, val_acc:0.982]
Epoch [72/120    avg_loss:0.085, val_acc:0.982]
Epoch [73/120    avg_loss:0.089, val_acc:0.980]
Epoch [74/120    avg_loss:0.082, val_acc:0.982]
Epoch [75/120    avg_loss:0.140, val_acc:0.955]
Epoch [76/120    avg_loss:0.106, val_acc:0.975]
Epoch [77/120    avg_loss:0.128, val_acc:0.979]
Epoch [78/120    avg_loss:0.076, val_acc:0.979]
Epoch [79/120    avg_loss:0.068, val_acc:0.975]
Epoch [80/120    avg_loss:0.103, val_acc:0.977]
Epoch [81/120    avg_loss:0.143, val_acc:0.961]
Epoch [82/120    avg_loss:0.108, val_acc:0.955]
Epoch [83/120    avg_loss:0.085, val_acc:0.969]
Epoch [84/120    avg_loss:0.155, val_acc:0.977]
Epoch [85/120    avg_loss:0.114, val_acc:0.971]
Epoch [86/120    avg_loss:0.100, val_acc:0.977]
Epoch [87/120    avg_loss:0.064, val_acc:0.975]
Epoch [88/120    avg_loss:0.054, val_acc:0.977]
Epoch [89/120    avg_loss:0.051, val_acc:0.979]
Epoch [90/120    avg_loss:0.040, val_acc:0.982]
Epoch [91/120    avg_loss:0.042, val_acc:0.982]
Epoch [92/120    avg_loss:0.037, val_acc:0.980]
Epoch [93/120    avg_loss:0.042, val_acc:0.980]
Epoch [94/120    avg_loss:0.047, val_acc:0.984]
Epoch [95/120    avg_loss:0.035, val_acc:0.980]
Epoch [96/120    avg_loss:0.046, val_acc:0.982]
Epoch [97/120    avg_loss:0.040, val_acc:0.980]
Epoch [98/120    avg_loss:0.045, val_acc:0.984]
Epoch [99/120    avg_loss:0.041, val_acc:0.980]
Epoch [100/120    avg_loss:0.038, val_acc:0.980]
Epoch [101/120    avg_loss:0.036, val_acc:0.980]
Epoch [102/120    avg_loss:0.041, val_acc:0.984]
Epoch [103/120    avg_loss:0.042, val_acc:0.984]
Epoch [104/120    avg_loss:0.038, val_acc:0.982]
Epoch [105/120    avg_loss:0.036, val_acc:0.984]
Epoch [106/120    avg_loss:0.036, val_acc:0.984]
Epoch [107/120    avg_loss:0.034, val_acc:0.984]
Epoch [108/120    avg_loss:0.040, val_acc:0.984]
Epoch [109/120    avg_loss:0.035, val_acc:0.982]
Epoch [110/120    avg_loss:0.038, val_acc:0.982]
Epoch [111/120    avg_loss:0.038, val_acc:0.984]
Epoch [112/120    avg_loss:0.035, val_acc:0.984]
Epoch [113/120    avg_loss:0.033, val_acc:0.982]
Epoch [114/120    avg_loss:0.044, val_acc:0.980]
Epoch [115/120    avg_loss:0.041, val_acc:0.984]
Epoch [116/120    avg_loss:0.036, val_acc:0.982]
Epoch [117/120    avg_loss:0.037, val_acc:0.982]
Epoch [118/120    avg_loss:0.027, val_acc:0.982]
Epoch [119/120    avg_loss:0.031, val_acc:0.982]
Epoch [120/120    avg_loss:0.035, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 215  11   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 1.         0.9580574  0.96629213 0.91170431 0.88059701
 1.         0.89017341 0.99487179 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9840919367893567
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15ab3c5e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.246]
Epoch [2/120    avg_loss:2.550, val_acc:0.492]
Epoch [3/120    avg_loss:2.480, val_acc:0.461]
Epoch [4/120    avg_loss:2.420, val_acc:0.473]
Epoch [5/120    avg_loss:2.350, val_acc:0.525]
Epoch [6/120    avg_loss:2.298, val_acc:0.574]
Epoch [7/120    avg_loss:2.221, val_acc:0.592]
Epoch [8/120    avg_loss:2.131, val_acc:0.613]
Epoch [9/120    avg_loss:2.033, val_acc:0.631]
Epoch [10/120    avg_loss:1.901, val_acc:0.639]
Epoch [11/120    avg_loss:1.807, val_acc:0.699]
Epoch [12/120    avg_loss:1.683, val_acc:0.646]
Epoch [13/120    avg_loss:1.564, val_acc:0.713]
Epoch [14/120    avg_loss:1.482, val_acc:0.758]
Epoch [15/120    avg_loss:1.350, val_acc:0.709]
Epoch [16/120    avg_loss:1.226, val_acc:0.846]
Epoch [17/120    avg_loss:1.130, val_acc:0.836]
Epoch [18/120    avg_loss:1.030, val_acc:0.871]
Epoch [19/120    avg_loss:0.926, val_acc:0.863]
Epoch [20/120    avg_loss:0.865, val_acc:0.869]
Epoch [21/120    avg_loss:0.769, val_acc:0.865]
Epoch [22/120    avg_loss:0.718, val_acc:0.857]
Epoch [23/120    avg_loss:0.632, val_acc:0.889]
Epoch [24/120    avg_loss:0.636, val_acc:0.883]
Epoch [25/120    avg_loss:0.622, val_acc:0.889]
Epoch [26/120    avg_loss:0.604, val_acc:0.889]
Epoch [27/120    avg_loss:0.566, val_acc:0.902]
Epoch [28/120    avg_loss:0.491, val_acc:0.916]
Epoch [29/120    avg_loss:0.454, val_acc:0.922]
Epoch [30/120    avg_loss:0.461, val_acc:0.934]
Epoch [31/120    avg_loss:0.448, val_acc:0.904]
Epoch [32/120    avg_loss:0.441, val_acc:0.920]
Epoch [33/120    avg_loss:0.497, val_acc:0.908]
Epoch [34/120    avg_loss:0.410, val_acc:0.924]
Epoch [35/120    avg_loss:0.402, val_acc:0.916]
Epoch [36/120    avg_loss:0.338, val_acc:0.939]
Epoch [37/120    avg_loss:0.323, val_acc:0.941]
Epoch [38/120    avg_loss:0.376, val_acc:0.916]
Epoch [39/120    avg_loss:0.412, val_acc:0.908]
Epoch [40/120    avg_loss:0.321, val_acc:0.938]
Epoch [41/120    avg_loss:0.319, val_acc:0.941]
Epoch [42/120    avg_loss:0.355, val_acc:0.916]
Epoch [43/120    avg_loss:0.341, val_acc:0.943]
Epoch [44/120    avg_loss:0.305, val_acc:0.943]
Epoch [45/120    avg_loss:0.256, val_acc:0.938]
Epoch [46/120    avg_loss:0.287, val_acc:0.920]
Epoch [47/120    avg_loss:0.340, val_acc:0.908]
Epoch [48/120    avg_loss:0.302, val_acc:0.930]
Epoch [49/120    avg_loss:0.249, val_acc:0.945]
Epoch [50/120    avg_loss:0.223, val_acc:0.965]
Epoch [51/120    avg_loss:0.243, val_acc:0.947]
Epoch [52/120    avg_loss:0.208, val_acc:0.943]
Epoch [53/120    avg_loss:0.228, val_acc:0.971]
Epoch [54/120    avg_loss:0.216, val_acc:0.957]
Epoch [55/120    avg_loss:0.222, val_acc:0.957]
Epoch [56/120    avg_loss:0.190, val_acc:0.963]
Epoch [57/120    avg_loss:0.199, val_acc:0.969]
Epoch [58/120    avg_loss:0.204, val_acc:0.947]
Epoch [59/120    avg_loss:0.171, val_acc:0.953]
Epoch [60/120    avg_loss:0.172, val_acc:0.973]
Epoch [61/120    avg_loss:0.165, val_acc:0.961]
Epoch [62/120    avg_loss:0.148, val_acc:0.971]
Epoch [63/120    avg_loss:0.163, val_acc:0.965]
Epoch [64/120    avg_loss:0.135, val_acc:0.975]
Epoch [65/120    avg_loss:0.142, val_acc:0.943]
Epoch [66/120    avg_loss:0.145, val_acc:0.961]
Epoch [67/120    avg_loss:0.217, val_acc:0.963]
Epoch [68/120    avg_loss:0.152, val_acc:0.984]
Epoch [69/120    avg_loss:0.107, val_acc:0.965]
Epoch [70/120    avg_loss:0.138, val_acc:0.959]
Epoch [71/120    avg_loss:0.137, val_acc:0.971]
Epoch [72/120    avg_loss:0.125, val_acc:0.967]
Epoch [73/120    avg_loss:0.152, val_acc:0.980]
Epoch [74/120    avg_loss:0.109, val_acc:0.979]
Epoch [75/120    avg_loss:0.128, val_acc:0.971]
Epoch [76/120    avg_loss:0.089, val_acc:0.973]
Epoch [77/120    avg_loss:0.093, val_acc:0.975]
Epoch [78/120    avg_loss:0.103, val_acc:0.977]
Epoch [79/120    avg_loss:0.079, val_acc:0.961]
Epoch [80/120    avg_loss:0.119, val_acc:0.975]
Epoch [81/120    avg_loss:0.096, val_acc:0.969]
Epoch [82/120    avg_loss:0.072, val_acc:0.980]
Epoch [83/120    avg_loss:0.062, val_acc:0.988]
Epoch [84/120    avg_loss:0.058, val_acc:0.986]
Epoch [85/120    avg_loss:0.055, val_acc:0.986]
Epoch [86/120    avg_loss:0.056, val_acc:0.986]
Epoch [87/120    avg_loss:0.053, val_acc:0.980]
Epoch [88/120    avg_loss:0.054, val_acc:0.988]
Epoch [89/120    avg_loss:0.050, val_acc:0.986]
Epoch [90/120    avg_loss:0.049, val_acc:0.986]
Epoch [91/120    avg_loss:0.048, val_acc:0.988]
Epoch [92/120    avg_loss:0.042, val_acc:0.988]
Epoch [93/120    avg_loss:0.045, val_acc:0.988]
Epoch [94/120    avg_loss:0.059, val_acc:0.986]
Epoch [95/120    avg_loss:0.057, val_acc:0.986]
Epoch [96/120    avg_loss:0.047, val_acc:0.986]
Epoch [97/120    avg_loss:0.039, val_acc:0.986]
Epoch [98/120    avg_loss:0.043, val_acc:0.986]
Epoch [99/120    avg_loss:0.045, val_acc:0.986]
Epoch [100/120    avg_loss:0.051, val_acc:0.988]
Epoch [101/120    avg_loss:0.045, val_acc:0.986]
Epoch [102/120    avg_loss:0.039, val_acc:0.986]
Epoch [103/120    avg_loss:0.038, val_acc:0.984]
Epoch [104/120    avg_loss:0.043, val_acc:0.986]
Epoch [105/120    avg_loss:0.036, val_acc:0.986]
Epoch [106/120    avg_loss:0.046, val_acc:0.986]
Epoch [107/120    avg_loss:0.044, val_acc:0.986]
Epoch [108/120    avg_loss:0.039, val_acc:0.986]
Epoch [109/120    avg_loss:0.039, val_acc:0.986]
Epoch [110/120    avg_loss:0.040, val_acc:0.984]
Epoch [111/120    avg_loss:0.038, val_acc:0.986]
Epoch [112/120    avg_loss:0.038, val_acc:0.986]
Epoch [113/120    avg_loss:0.033, val_acc:0.986]
Epoch [114/120    avg_loss:0.037, val_acc:0.986]
Epoch [115/120    avg_loss:0.037, val_acc:0.986]
Epoch [116/120    avg_loss:0.039, val_acc:0.986]
Epoch [117/120    avg_loss:0.033, val_acc:0.986]
Epoch [118/120    avg_loss:0.035, val_acc:0.986]
Epoch [119/120    avg_loss:0.039, val_acc:0.986]
Epoch [120/120    avg_loss:0.034, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 219   6   0   0   0   0   5   0   0   0   0]
 [  0   0   0   0 224   2   0   0   0   0   1   0   0   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.97977528 0.97550111 0.92561983 0.89056604
 1.         0.95027624 1.         0.9946865  0.99862826 0.99867198
 0.99889746 1.        ]

Kappa:
0.9878909042308681
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa3b024e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.143]
Epoch [2/120    avg_loss:2.546, val_acc:0.336]
Epoch [3/120    avg_loss:2.476, val_acc:0.348]
Epoch [4/120    avg_loss:2.405, val_acc:0.348]
Epoch [5/120    avg_loss:2.350, val_acc:0.344]
Epoch [6/120    avg_loss:2.276, val_acc:0.381]
Epoch [7/120    avg_loss:2.205, val_acc:0.461]
Epoch [8/120    avg_loss:2.115, val_acc:0.535]
Epoch [9/120    avg_loss:1.997, val_acc:0.553]
Epoch [10/120    avg_loss:1.883, val_acc:0.609]
Epoch [11/120    avg_loss:1.766, val_acc:0.629]
Epoch [12/120    avg_loss:1.648, val_acc:0.650]
Epoch [13/120    avg_loss:1.498, val_acc:0.686]
Epoch [14/120    avg_loss:1.404, val_acc:0.668]
Epoch [15/120    avg_loss:1.277, val_acc:0.738]
Epoch [16/120    avg_loss:1.126, val_acc:0.729]
Epoch [17/120    avg_loss:1.039, val_acc:0.730]
Epoch [18/120    avg_loss:0.985, val_acc:0.758]
Epoch [19/120    avg_loss:0.865, val_acc:0.861]
Epoch [20/120    avg_loss:0.784, val_acc:0.826]
Epoch [21/120    avg_loss:0.755, val_acc:0.885]
Epoch [22/120    avg_loss:0.731, val_acc:0.869]
Epoch [23/120    avg_loss:0.637, val_acc:0.891]
Epoch [24/120    avg_loss:0.634, val_acc:0.887]
Epoch [25/120    avg_loss:0.607, val_acc:0.818]
Epoch [26/120    avg_loss:0.528, val_acc:0.902]
Epoch [27/120    avg_loss:0.499, val_acc:0.932]
Epoch [28/120    avg_loss:0.487, val_acc:0.924]
Epoch [29/120    avg_loss:0.481, val_acc:0.914]
Epoch [30/120    avg_loss:0.409, val_acc:0.938]
Epoch [31/120    avg_loss:0.471, val_acc:0.889]
Epoch [32/120    avg_loss:0.435, val_acc:0.891]
Epoch [33/120    avg_loss:0.395, val_acc:0.945]
Epoch [34/120    avg_loss:0.401, val_acc:0.910]
Epoch [35/120    avg_loss:0.368, val_acc:0.936]
Epoch [36/120    avg_loss:0.395, val_acc:0.938]
Epoch [37/120    avg_loss:0.339, val_acc:0.934]
Epoch [38/120    avg_loss:0.337, val_acc:0.941]
Epoch [39/120    avg_loss:0.330, val_acc:0.904]
Epoch [40/120    avg_loss:0.378, val_acc:0.930]
Epoch [41/120    avg_loss:0.266, val_acc:0.922]
Epoch [42/120    avg_loss:0.296, val_acc:0.939]
Epoch [43/120    avg_loss:0.323, val_acc:0.941]
Epoch [44/120    avg_loss:0.243, val_acc:0.953]
Epoch [45/120    avg_loss:0.206, val_acc:0.945]
Epoch [46/120    avg_loss:0.276, val_acc:0.963]
Epoch [47/120    avg_loss:0.227, val_acc:0.939]
Epoch [48/120    avg_loss:0.256, val_acc:0.943]
Epoch [49/120    avg_loss:0.264, val_acc:0.953]
Epoch [50/120    avg_loss:0.209, val_acc:0.955]
Epoch [51/120    avg_loss:0.203, val_acc:0.965]
Epoch [52/120    avg_loss:0.194, val_acc:0.963]
Epoch [53/120    avg_loss:0.185, val_acc:0.947]
Epoch [54/120    avg_loss:0.221, val_acc:0.953]
Epoch [55/120    avg_loss:0.269, val_acc:0.967]
Epoch [56/120    avg_loss:0.194, val_acc:0.961]
Epoch [57/120    avg_loss:0.154, val_acc:0.969]
Epoch [58/120    avg_loss:0.193, val_acc:0.973]
Epoch [59/120    avg_loss:0.220, val_acc:0.953]
Epoch [60/120    avg_loss:0.203, val_acc:0.965]
Epoch [61/120    avg_loss:0.159, val_acc:0.955]
Epoch [62/120    avg_loss:0.154, val_acc:0.969]
Epoch [63/120    avg_loss:0.142, val_acc:0.971]
Epoch [64/120    avg_loss:0.123, val_acc:0.949]
Epoch [65/120    avg_loss:0.197, val_acc:0.943]
Epoch [66/120    avg_loss:0.133, val_acc:0.971]
Epoch [67/120    avg_loss:0.123, val_acc:0.959]
Epoch [68/120    avg_loss:0.124, val_acc:0.971]
Epoch [69/120    avg_loss:0.106, val_acc:0.979]
Epoch [70/120    avg_loss:0.132, val_acc:0.955]
Epoch [71/120    avg_loss:0.130, val_acc:0.938]
Epoch [72/120    avg_loss:0.115, val_acc:0.963]
Epoch [73/120    avg_loss:0.092, val_acc:0.979]
Epoch [74/120    avg_loss:0.111, val_acc:0.975]
Epoch [75/120    avg_loss:0.099, val_acc:0.977]
Epoch [76/120    avg_loss:0.125, val_acc:0.953]
Epoch [77/120    avg_loss:0.106, val_acc:0.977]
Epoch [78/120    avg_loss:0.162, val_acc:0.977]
Epoch [79/120    avg_loss:0.093, val_acc:0.982]
Epoch [80/120    avg_loss:0.102, val_acc:0.959]
Epoch [81/120    avg_loss:0.123, val_acc:0.984]
Epoch [82/120    avg_loss:0.086, val_acc:0.957]
Epoch [83/120    avg_loss:0.075, val_acc:0.982]
Epoch [84/120    avg_loss:0.068, val_acc:0.975]
Epoch [85/120    avg_loss:0.092, val_acc:0.977]
Epoch [86/120    avg_loss:0.096, val_acc:0.980]
Epoch [87/120    avg_loss:0.094, val_acc:0.965]
Epoch [88/120    avg_loss:0.106, val_acc:0.988]
Epoch [89/120    avg_loss:0.090, val_acc:0.963]
Epoch [90/120    avg_loss:0.122, val_acc:0.973]
Epoch [91/120    avg_loss:0.131, val_acc:0.965]
Epoch [92/120    avg_loss:0.147, val_acc:0.973]
Epoch [93/120    avg_loss:0.089, val_acc:0.984]
Epoch [94/120    avg_loss:0.062, val_acc:0.980]
Epoch [95/120    avg_loss:0.074, val_acc:0.959]
Epoch [96/120    avg_loss:0.067, val_acc:0.967]
Epoch [97/120    avg_loss:0.052, val_acc:0.979]
Epoch [98/120    avg_loss:0.078, val_acc:0.980]
Epoch [99/120    avg_loss:0.095, val_acc:0.980]
Epoch [100/120    avg_loss:0.052, val_acc:0.975]
Epoch [101/120    avg_loss:0.072, val_acc:0.977]
Epoch [102/120    avg_loss:0.051, val_acc:0.986]
Epoch [103/120    avg_loss:0.041, val_acc:0.986]
Epoch [104/120    avg_loss:0.041, val_acc:0.988]
Epoch [105/120    avg_loss:0.040, val_acc:0.986]
Epoch [106/120    avg_loss:0.033, val_acc:0.986]
Epoch [107/120    avg_loss:0.032, val_acc:0.986]
Epoch [108/120    avg_loss:0.028, val_acc:0.986]
Epoch [109/120    avg_loss:0.031, val_acc:0.986]
Epoch [110/120    avg_loss:0.034, val_acc:0.986]
Epoch [111/120    avg_loss:0.027, val_acc:0.986]
Epoch [112/120    avg_loss:0.032, val_acc:0.986]
Epoch [113/120    avg_loss:0.031, val_acc:0.986]
Epoch [114/120    avg_loss:0.028, val_acc:0.986]
Epoch [115/120    avg_loss:0.029, val_acc:0.986]
Epoch [116/120    avg_loss:0.031, val_acc:0.986]
Epoch [117/120    avg_loss:0.028, val_acc:0.988]
Epoch [118/120    avg_loss:0.028, val_acc:0.986]
Epoch [119/120    avg_loss:0.027, val_acc:0.986]
Epoch [120/120    avg_loss:0.026, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 218   8   0   0   0   2   2   0   0   0   0]
 [  0   0   0   1 223   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   3   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 1.         0.97737557 0.97104677 0.91769547 0.89552239
 1.         0.95135135 0.996139   0.9978678  1.         1.
 0.99667774 1.        ]

Kappa:
0.9871800368582798
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb68d193e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.635, val_acc:0.213]
Epoch [2/120    avg_loss:2.553, val_acc:0.291]
Epoch [3/120    avg_loss:2.475, val_acc:0.420]
Epoch [4/120    avg_loss:2.405, val_acc:0.420]
Epoch [5/120    avg_loss:2.328, val_acc:0.422]
Epoch [6/120    avg_loss:2.255, val_acc:0.422]
Epoch [7/120    avg_loss:2.179, val_acc:0.430]
Epoch [8/120    avg_loss:2.115, val_acc:0.484]
Epoch [9/120    avg_loss:2.028, val_acc:0.537]
Epoch [10/120    avg_loss:1.949, val_acc:0.549]
Epoch [11/120    avg_loss:1.860, val_acc:0.576]
Epoch [12/120    avg_loss:1.780, val_acc:0.645]
Epoch [13/120    avg_loss:1.699, val_acc:0.684]
Epoch [14/120    avg_loss:1.602, val_acc:0.676]
Epoch [15/120    avg_loss:1.487, val_acc:0.740]
Epoch [16/120    avg_loss:1.373, val_acc:0.809]
Epoch [17/120    avg_loss:1.256, val_acc:0.803]
Epoch [18/120    avg_loss:1.149, val_acc:0.799]
Epoch [19/120    avg_loss:1.057, val_acc:0.787]
Epoch [20/120    avg_loss:0.975, val_acc:0.852]
Epoch [21/120    avg_loss:0.908, val_acc:0.885]
Epoch [22/120    avg_loss:0.812, val_acc:0.771]
Epoch [23/120    avg_loss:0.779, val_acc:0.846]
Epoch [24/120    avg_loss:0.719, val_acc:0.857]
Epoch [25/120    avg_loss:0.678, val_acc:0.877]
Epoch [26/120    avg_loss:0.625, val_acc:0.904]
Epoch [27/120    avg_loss:0.578, val_acc:0.826]
Epoch [28/120    avg_loss:0.569, val_acc:0.887]
Epoch [29/120    avg_loss:0.553, val_acc:0.889]
Epoch [30/120    avg_loss:0.558, val_acc:0.895]
Epoch [31/120    avg_loss:0.498, val_acc:0.877]
Epoch [32/120    avg_loss:0.515, val_acc:0.910]
Epoch [33/120    avg_loss:0.470, val_acc:0.873]
Epoch [34/120    avg_loss:0.407, val_acc:0.873]
Epoch [35/120    avg_loss:0.464, val_acc:0.895]
Epoch [36/120    avg_loss:0.409, val_acc:0.912]
Epoch [37/120    avg_loss:0.411, val_acc:0.887]
Epoch [38/120    avg_loss:0.483, val_acc:0.904]
Epoch [39/120    avg_loss:0.374, val_acc:0.906]
Epoch [40/120    avg_loss:0.380, val_acc:0.891]
Epoch [41/120    avg_loss:0.348, val_acc:0.920]
Epoch [42/120    avg_loss:0.370, val_acc:0.936]
Epoch [43/120    avg_loss:0.353, val_acc:0.902]
Epoch [44/120    avg_loss:0.318, val_acc:0.914]
Epoch [45/120    avg_loss:0.337, val_acc:0.918]
Epoch [46/120    avg_loss:0.295, val_acc:0.932]
Epoch [47/120    avg_loss:0.320, val_acc:0.930]
Epoch [48/120    avg_loss:0.328, val_acc:0.928]
Epoch [49/120    avg_loss:0.284, val_acc:0.920]
Epoch [50/120    avg_loss:0.296, val_acc:0.924]
Epoch [51/120    avg_loss:0.236, val_acc:0.947]
Epoch [52/120    avg_loss:0.295, val_acc:0.924]
Epoch [53/120    avg_loss:0.284, val_acc:0.928]
Epoch [54/120    avg_loss:0.229, val_acc:0.945]
Epoch [55/120    avg_loss:0.235, val_acc:0.930]
Epoch [56/120    avg_loss:0.262, val_acc:0.959]
Epoch [57/120    avg_loss:0.201, val_acc:0.941]
Epoch [58/120    avg_loss:0.209, val_acc:0.930]
Epoch [59/120    avg_loss:0.246, val_acc:0.936]
Epoch [60/120    avg_loss:0.232, val_acc:0.914]
Epoch [61/120    avg_loss:0.250, val_acc:0.951]
Epoch [62/120    avg_loss:0.236, val_acc:0.920]
Epoch [63/120    avg_loss:0.252, val_acc:0.947]
Epoch [64/120    avg_loss:0.214, val_acc:0.943]
Epoch [65/120    avg_loss:0.156, val_acc:0.955]
Epoch [66/120    avg_loss:0.137, val_acc:0.975]
Epoch [67/120    avg_loss:0.144, val_acc:0.930]
Epoch [68/120    avg_loss:0.177, val_acc:0.967]
Epoch [69/120    avg_loss:0.137, val_acc:0.926]
Epoch [70/120    avg_loss:0.207, val_acc:0.967]
Epoch [71/120    avg_loss:0.187, val_acc:0.957]
Epoch [72/120    avg_loss:0.186, val_acc:0.951]
Epoch [73/120    avg_loss:0.167, val_acc:0.951]
Epoch [74/120    avg_loss:0.174, val_acc:0.959]
Epoch [75/120    avg_loss:0.149, val_acc:0.947]
Epoch [76/120    avg_loss:0.107, val_acc:0.951]
Epoch [77/120    avg_loss:0.117, val_acc:0.957]
Epoch [78/120    avg_loss:0.146, val_acc:0.896]
Epoch [79/120    avg_loss:0.188, val_acc:0.955]
Epoch [80/120    avg_loss:0.118, val_acc:0.969]
Epoch [81/120    avg_loss:0.084, val_acc:0.977]
Epoch [82/120    avg_loss:0.084, val_acc:0.977]
Epoch [83/120    avg_loss:0.078, val_acc:0.979]
Epoch [84/120    avg_loss:0.071, val_acc:0.973]
Epoch [85/120    avg_loss:0.068, val_acc:0.979]
Epoch [86/120    avg_loss:0.091, val_acc:0.965]
Epoch [87/120    avg_loss:0.072, val_acc:0.975]
Epoch [88/120    avg_loss:0.078, val_acc:0.975]
Epoch [89/120    avg_loss:0.071, val_acc:0.977]
Epoch [90/120    avg_loss:0.064, val_acc:0.980]
Epoch [91/120    avg_loss:0.068, val_acc:0.977]
Epoch [92/120    avg_loss:0.063, val_acc:0.977]
Epoch [93/120    avg_loss:0.062, val_acc:0.980]
Epoch [94/120    avg_loss:0.071, val_acc:0.980]
Epoch [95/120    avg_loss:0.062, val_acc:0.979]
Epoch [96/120    avg_loss:0.068, val_acc:0.979]
Epoch [97/120    avg_loss:0.062, val_acc:0.979]
Epoch [98/120    avg_loss:0.058, val_acc:0.977]
Epoch [99/120    avg_loss:0.055, val_acc:0.979]
Epoch [100/120    avg_loss:0.071, val_acc:0.977]
Epoch [101/120    avg_loss:0.067, val_acc:0.979]
Epoch [102/120    avg_loss:0.059, val_acc:0.977]
Epoch [103/120    avg_loss:0.049, val_acc:0.979]
Epoch [104/120    avg_loss:0.052, val_acc:0.980]
Epoch [105/120    avg_loss:0.051, val_acc:0.979]
Epoch [106/120    avg_loss:0.053, val_acc:0.980]
Epoch [107/120    avg_loss:0.059, val_acc:0.977]
Epoch [108/120    avg_loss:0.062, val_acc:0.979]
Epoch [109/120    avg_loss:0.054, val_acc:0.980]
Epoch [110/120    avg_loss:0.047, val_acc:0.977]
Epoch [111/120    avg_loss:0.053, val_acc:0.977]
Epoch [112/120    avg_loss:0.050, val_acc:0.979]
Epoch [113/120    avg_loss:0.050, val_acc:0.977]
Epoch [114/120    avg_loss:0.044, val_acc:0.979]
Epoch [115/120    avg_loss:0.057, val_acc:0.979]
Epoch [116/120    avg_loss:0.055, val_acc:0.982]
Epoch [117/120    avg_loss:0.048, val_acc:0.973]
Epoch [118/120    avg_loss:0.051, val_acc:0.980]
Epoch [119/120    avg_loss:0.060, val_acc:0.980]
Epoch [120/120    avg_loss:0.060, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 216   8   0   0   0   5   1   0   0   0   0]
 [  0   0   0   0 200  27   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.4861407249467

F1 scores:
[       nan 1.         0.97065463 0.96860987 0.88888889 0.86092715
 1.         0.92896175 0.99359795 0.99893276 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9831452367945165
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f602a4cbda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.622, val_acc:0.200]
Epoch [2/120    avg_loss:2.554, val_acc:0.275]
Epoch [3/120    avg_loss:2.482, val_acc:0.297]
Epoch [4/120    avg_loss:2.421, val_acc:0.334]
Epoch [5/120    avg_loss:2.346, val_acc:0.418]
Epoch [6/120    avg_loss:2.283, val_acc:0.428]
Epoch [7/120    avg_loss:2.209, val_acc:0.480]
Epoch [8/120    avg_loss:2.116, val_acc:0.553]
Epoch [9/120    avg_loss:2.013, val_acc:0.545]
Epoch [10/120    avg_loss:1.907, val_acc:0.678]
Epoch [11/120    avg_loss:1.816, val_acc:0.689]
Epoch [12/120    avg_loss:1.665, val_acc:0.740]
Epoch [13/120    avg_loss:1.550, val_acc:0.770]
Epoch [14/120    avg_loss:1.405, val_acc:0.721]
Epoch [15/120    avg_loss:1.286, val_acc:0.824]
Epoch [16/120    avg_loss:1.155, val_acc:0.803]
Epoch [17/120    avg_loss:1.053, val_acc:0.852]
Epoch [18/120    avg_loss:0.956, val_acc:0.877]
Epoch [19/120    avg_loss:0.844, val_acc:0.873]
Epoch [20/120    avg_loss:0.774, val_acc:0.885]
Epoch [21/120    avg_loss:0.703, val_acc:0.852]
Epoch [22/120    avg_loss:0.681, val_acc:0.879]
Epoch [23/120    avg_loss:0.649, val_acc:0.877]
Epoch [24/120    avg_loss:0.629, val_acc:0.881]
Epoch [25/120    avg_loss:0.596, val_acc:0.877]
Epoch [26/120    avg_loss:0.593, val_acc:0.898]
Epoch [27/120    avg_loss:0.553, val_acc:0.893]
Epoch [28/120    avg_loss:0.540, val_acc:0.918]
Epoch [29/120    avg_loss:0.504, val_acc:0.910]
Epoch [30/120    avg_loss:0.444, val_acc:0.922]
Epoch [31/120    avg_loss:0.451, val_acc:0.916]
Epoch [32/120    avg_loss:0.457, val_acc:0.918]
Epoch [33/120    avg_loss:0.471, val_acc:0.852]
Epoch [34/120    avg_loss:0.461, val_acc:0.934]
Epoch [35/120    avg_loss:0.423, val_acc:0.885]
Epoch [36/120    avg_loss:0.395, val_acc:0.938]
Epoch [37/120    avg_loss:0.369, val_acc:0.930]
Epoch [38/120    avg_loss:0.367, val_acc:0.902]
Epoch [39/120    avg_loss:0.354, val_acc:0.934]
Epoch [40/120    avg_loss:0.341, val_acc:0.928]
Epoch [41/120    avg_loss:0.408, val_acc:0.945]
Epoch [42/120    avg_loss:0.275, val_acc:0.928]
Epoch [43/120    avg_loss:0.350, val_acc:0.924]
Epoch [44/120    avg_loss:0.300, val_acc:0.947]
Epoch [45/120    avg_loss:0.275, val_acc:0.943]
Epoch [46/120    avg_loss:0.275, val_acc:0.920]
Epoch [47/120    avg_loss:0.270, val_acc:0.943]
Epoch [48/120    avg_loss:0.230, val_acc:0.936]
Epoch [49/120    avg_loss:0.244, val_acc:0.928]
Epoch [50/120    avg_loss:0.301, val_acc:0.951]
Epoch [51/120    avg_loss:0.201, val_acc:0.951]
Epoch [52/120    avg_loss:0.217, val_acc:0.955]
Epoch [53/120    avg_loss:0.203, val_acc:0.945]
Epoch [54/120    avg_loss:0.196, val_acc:0.947]
Epoch [55/120    avg_loss:0.213, val_acc:0.955]
Epoch [56/120    avg_loss:0.164, val_acc:0.934]
Epoch [57/120    avg_loss:0.250, val_acc:0.951]
Epoch [58/120    avg_loss:0.209, val_acc:0.949]
Epoch [59/120    avg_loss:0.199, val_acc:0.963]
Epoch [60/120    avg_loss:0.134, val_acc:0.949]
Epoch [61/120    avg_loss:0.167, val_acc:0.947]
Epoch [62/120    avg_loss:0.154, val_acc:0.945]
Epoch [63/120    avg_loss:0.185, val_acc:0.949]
Epoch [64/120    avg_loss:0.126, val_acc:0.971]
Epoch [65/120    avg_loss:0.192, val_acc:0.924]
Epoch [66/120    avg_loss:0.229, val_acc:0.953]
Epoch [67/120    avg_loss:0.150, val_acc:0.949]
Epoch [68/120    avg_loss:0.196, val_acc:0.949]
Epoch [69/120    avg_loss:0.154, val_acc:0.957]
Epoch [70/120    avg_loss:0.139, val_acc:0.953]
Epoch [71/120    avg_loss:0.135, val_acc:0.957]
Epoch [72/120    avg_loss:0.139, val_acc:0.953]
Epoch [73/120    avg_loss:0.137, val_acc:0.963]
Epoch [74/120    avg_loss:0.151, val_acc:0.965]
Epoch [75/120    avg_loss:0.135, val_acc:0.959]
Epoch [76/120    avg_loss:0.100, val_acc:0.975]
Epoch [77/120    avg_loss:0.128, val_acc:0.967]
Epoch [78/120    avg_loss:0.118, val_acc:0.961]
Epoch [79/120    avg_loss:0.074, val_acc:0.973]
Epoch [80/120    avg_loss:0.107, val_acc:0.963]
Epoch [81/120    avg_loss:0.098, val_acc:0.967]
Epoch [82/120    avg_loss:0.140, val_acc:0.938]
Epoch [83/120    avg_loss:0.103, val_acc:0.965]
Epoch [84/120    avg_loss:0.089, val_acc:0.953]
Epoch [85/120    avg_loss:0.094, val_acc:0.969]
Epoch [86/120    avg_loss:0.098, val_acc:0.928]
Epoch [87/120    avg_loss:0.122, val_acc:0.900]
Epoch [88/120    avg_loss:0.128, val_acc:0.959]
Epoch [89/120    avg_loss:0.092, val_acc:0.979]
Epoch [90/120    avg_loss:0.093, val_acc:0.982]
Epoch [91/120    avg_loss:0.069, val_acc:0.965]
Epoch [92/120    avg_loss:0.070, val_acc:0.980]
Epoch [93/120    avg_loss:0.080, val_acc:0.969]
Epoch [94/120    avg_loss:0.098, val_acc:0.979]
Epoch [95/120    avg_loss:0.077, val_acc:0.967]
Epoch [96/120    avg_loss:0.062, val_acc:0.980]
Epoch [97/120    avg_loss:0.069, val_acc:0.969]
Epoch [98/120    avg_loss:0.063, val_acc:0.971]
Epoch [99/120    avg_loss:0.098, val_acc:0.971]
Epoch [100/120    avg_loss:0.071, val_acc:0.979]
Epoch [101/120    avg_loss:0.041, val_acc:0.977]
Epoch [102/120    avg_loss:0.047, val_acc:0.973]
Epoch [103/120    avg_loss:0.042, val_acc:0.975]
Epoch [104/120    avg_loss:0.050, val_acc:0.980]
Epoch [105/120    avg_loss:0.032, val_acc:0.980]
Epoch [106/120    avg_loss:0.041, val_acc:0.980]
Epoch [107/120    avg_loss:0.029, val_acc:0.982]
Epoch [108/120    avg_loss:0.044, val_acc:0.982]
Epoch [109/120    avg_loss:0.034, val_acc:0.982]
Epoch [110/120    avg_loss:0.039, val_acc:0.982]
Epoch [111/120    avg_loss:0.036, val_acc:0.982]
Epoch [112/120    avg_loss:0.030, val_acc:0.984]
Epoch [113/120    avg_loss:0.026, val_acc:0.984]
Epoch [114/120    avg_loss:0.030, val_acc:0.980]
Epoch [115/120    avg_loss:0.035, val_acc:0.980]
Epoch [116/120    avg_loss:0.032, val_acc:0.982]
Epoch [117/120    avg_loss:0.027, val_acc:0.982]
Epoch [118/120    avg_loss:0.034, val_acc:0.984]
Epoch [119/120    avg_loss:0.020, val_acc:0.984]
Epoch [120/120    avg_loss:0.027, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 223   1   0   0   0   2   4   0   0   0   0]
 [  0   0   0   1 217   7   0   0   0   0   0   0   2   0]
 [  0   0   0   2  22 121   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 0.99927061 0.96230599 0.97807018 0.92933619 0.88644689
 0.99756691 0.90285714 0.99487179 0.99574468 1.         0.99867198
 0.99448732 1.        ]

Kappa:
0.9852784975339282
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6523ff3d68>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.632, val_acc:0.215]
Epoch [2/120    avg_loss:2.557, val_acc:0.395]
Epoch [3/120    avg_loss:2.500, val_acc:0.410]
Epoch [4/120    avg_loss:2.442, val_acc:0.434]
Epoch [5/120    avg_loss:2.377, val_acc:0.439]
Epoch [6/120    avg_loss:2.321, val_acc:0.439]
Epoch [7/120    avg_loss:2.249, val_acc:0.467]
Epoch [8/120    avg_loss:2.172, val_acc:0.506]
Epoch [9/120    avg_loss:2.101, val_acc:0.549]
Epoch [10/120    avg_loss:2.007, val_acc:0.588]
Epoch [11/120    avg_loss:1.898, val_acc:0.689]
Epoch [12/120    avg_loss:1.794, val_acc:0.736]
Epoch [13/120    avg_loss:1.623, val_acc:0.758]
Epoch [14/120    avg_loss:1.485, val_acc:0.799]
Epoch [15/120    avg_loss:1.365, val_acc:0.812]
Epoch [16/120    avg_loss:1.248, val_acc:0.836]
Epoch [17/120    avg_loss:1.118, val_acc:0.887]
Epoch [18/120    avg_loss:1.006, val_acc:0.867]
Epoch [19/120    avg_loss:0.908, val_acc:0.889]
Epoch [20/120    avg_loss:0.824, val_acc:0.910]
Epoch [21/120    avg_loss:0.714, val_acc:0.906]
Epoch [22/120    avg_loss:0.699, val_acc:0.914]
Epoch [23/120    avg_loss:0.701, val_acc:0.871]
Epoch [24/120    avg_loss:0.636, val_acc:0.916]
Epoch [25/120    avg_loss:0.557, val_acc:0.916]
Epoch [26/120    avg_loss:0.538, val_acc:0.920]
Epoch [27/120    avg_loss:0.516, val_acc:0.908]
Epoch [28/120    avg_loss:0.570, val_acc:0.896]
Epoch [29/120    avg_loss:0.545, val_acc:0.916]
Epoch [30/120    avg_loss:0.503, val_acc:0.920]
Epoch [31/120    avg_loss:0.484, val_acc:0.871]
Epoch [32/120    avg_loss:0.491, val_acc:0.904]
Epoch [33/120    avg_loss:0.525, val_acc:0.883]
Epoch [34/120    avg_loss:0.456, val_acc:0.926]
Epoch [35/120    avg_loss:0.445, val_acc:0.939]
Epoch [36/120    avg_loss:0.375, val_acc:0.930]
Epoch [37/120    avg_loss:0.335, val_acc:0.943]
Epoch [38/120    avg_loss:0.343, val_acc:0.941]
Epoch [39/120    avg_loss:0.327, val_acc:0.941]
Epoch [40/120    avg_loss:0.336, val_acc:0.934]
Epoch [41/120    avg_loss:0.365, val_acc:0.939]
Epoch [42/120    avg_loss:0.337, val_acc:0.945]
Epoch [43/120    avg_loss:0.287, val_acc:0.947]
Epoch [44/120    avg_loss:0.278, val_acc:0.934]
Epoch [45/120    avg_loss:0.324, val_acc:0.938]
Epoch [46/120    avg_loss:0.312, val_acc:0.941]
Epoch [47/120    avg_loss:0.268, val_acc:0.951]
Epoch [48/120    avg_loss:0.233, val_acc:0.941]
Epoch [49/120    avg_loss:0.254, val_acc:0.936]
Epoch [50/120    avg_loss:0.336, val_acc:0.926]
Epoch [51/120    avg_loss:0.246, val_acc:0.951]
Epoch [52/120    avg_loss:0.243, val_acc:0.936]
Epoch [53/120    avg_loss:0.353, val_acc:0.949]
Epoch [54/120    avg_loss:0.235, val_acc:0.951]
Epoch [55/120    avg_loss:0.244, val_acc:0.936]
Epoch [56/120    avg_loss:0.235, val_acc:0.959]
Epoch [57/120    avg_loss:0.228, val_acc:0.949]
Epoch [58/120    avg_loss:0.205, val_acc:0.941]
Epoch [59/120    avg_loss:0.199, val_acc:0.951]
Epoch [60/120    avg_loss:0.235, val_acc:0.951]
Epoch [61/120    avg_loss:0.178, val_acc:0.938]
Epoch [62/120    avg_loss:0.189, val_acc:0.969]
Epoch [63/120    avg_loss:0.205, val_acc:0.963]
Epoch [64/120    avg_loss:0.191, val_acc:0.965]
Epoch [65/120    avg_loss:0.239, val_acc:0.939]
Epoch [66/120    avg_loss:0.188, val_acc:0.947]
Epoch [67/120    avg_loss:0.159, val_acc:0.945]
Epoch [68/120    avg_loss:0.200, val_acc:0.951]
Epoch [69/120    avg_loss:0.296, val_acc:0.961]
Epoch [70/120    avg_loss:0.216, val_acc:0.938]
Epoch [71/120    avg_loss:0.215, val_acc:0.957]
Epoch [72/120    avg_loss:0.142, val_acc:0.959]
Epoch [73/120    avg_loss:0.162, val_acc:0.971]
Epoch [74/120    avg_loss:0.162, val_acc:0.961]
Epoch [75/120    avg_loss:0.135, val_acc:0.973]
Epoch [76/120    avg_loss:0.158, val_acc:0.887]
Epoch [77/120    avg_loss:0.217, val_acc:0.951]
Epoch [78/120    avg_loss:0.143, val_acc:0.971]
Epoch [79/120    avg_loss:0.159, val_acc:0.957]
Epoch [80/120    avg_loss:0.217, val_acc:0.967]
Epoch [81/120    avg_loss:0.150, val_acc:0.951]
Epoch [82/120    avg_loss:0.123, val_acc:0.973]
Epoch [83/120    avg_loss:0.118, val_acc:0.971]
Epoch [84/120    avg_loss:0.134, val_acc:0.957]
Epoch [85/120    avg_loss:0.138, val_acc:0.971]
Epoch [86/120    avg_loss:0.094, val_acc:0.973]
Epoch [87/120    avg_loss:0.107, val_acc:0.965]
Epoch [88/120    avg_loss:0.146, val_acc:0.957]
Epoch [89/120    avg_loss:0.169, val_acc:0.953]
Epoch [90/120    avg_loss:0.181, val_acc:0.959]
Epoch [91/120    avg_loss:0.126, val_acc:0.959]
Epoch [92/120    avg_loss:0.140, val_acc:0.971]
Epoch [93/120    avg_loss:0.090, val_acc:0.955]
Epoch [94/120    avg_loss:0.134, val_acc:0.951]
Epoch [95/120    avg_loss:0.138, val_acc:0.957]
Epoch [96/120    avg_loss:0.134, val_acc:0.957]
Epoch [97/120    avg_loss:0.095, val_acc:0.971]
Epoch [98/120    avg_loss:0.071, val_acc:0.979]
Epoch [99/120    avg_loss:0.060, val_acc:0.975]
Epoch [100/120    avg_loss:0.055, val_acc:0.969]
Epoch [101/120    avg_loss:0.082, val_acc:0.965]
Epoch [102/120    avg_loss:0.091, val_acc:0.971]
Epoch [103/120    avg_loss:0.080, val_acc:0.975]
Epoch [104/120    avg_loss:0.069, val_acc:0.971]
Epoch [105/120    avg_loss:0.060, val_acc:0.971]
Epoch [106/120    avg_loss:0.079, val_acc:0.969]
Epoch [107/120    avg_loss:0.065, val_acc:0.977]
Epoch [108/120    avg_loss:0.060, val_acc:0.975]
Epoch [109/120    avg_loss:0.063, val_acc:0.975]
Epoch [110/120    avg_loss:0.066, val_acc:0.984]
Epoch [111/120    avg_loss:0.054, val_acc:0.967]
Epoch [112/120    avg_loss:0.054, val_acc:0.977]
Epoch [113/120    avg_loss:0.067, val_acc:0.979]
Epoch [114/120    avg_loss:0.090, val_acc:0.973]
Epoch [115/120    avg_loss:0.055, val_acc:0.980]
Epoch [116/120    avg_loss:0.061, val_acc:0.963]
Epoch [117/120    avg_loss:0.098, val_acc:0.955]
Epoch [118/120    avg_loss:0.125, val_acc:0.965]
Epoch [119/120    avg_loss:0.074, val_acc:0.955]
Epoch [120/120    avg_loss:0.055, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 673   0   0   0   0  12   0   0   0   0   0   0   0]
 [  0   0 204   0   0   0   0  15   0   0   0   0   0   0]
 [  0   0   0 214   8   0   0   2   6   0   0   0   0   0]
 [  0   0   0   0 179  48   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   1 201   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0   0 382   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   1   0   0   0   2   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.46268656716418

F1 scores:
[       nan 0.99116348 0.92727273 0.96396396 0.84834123 0.8452381
 0.95942721 0.86734694 0.98200514 1.         1.         1.
 0.99445061 1.        ]

Kappa:
0.9717678269370978
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa4b10d1c50>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.207]
Epoch [2/120    avg_loss:2.543, val_acc:0.377]
Epoch [3/120    avg_loss:2.474, val_acc:0.375]
Epoch [4/120    avg_loss:2.408, val_acc:0.383]
Epoch [5/120    avg_loss:2.345, val_acc:0.389]
Epoch [6/120    avg_loss:2.280, val_acc:0.406]
Epoch [7/120    avg_loss:2.204, val_acc:0.461]
Epoch [8/120    avg_loss:2.114, val_acc:0.514]
Epoch [9/120    avg_loss:2.012, val_acc:0.574]
Epoch [10/120    avg_loss:1.923, val_acc:0.559]
Epoch [11/120    avg_loss:1.817, val_acc:0.639]
Epoch [12/120    avg_loss:1.705, val_acc:0.627]
Epoch [13/120    avg_loss:1.620, val_acc:0.711]
Epoch [14/120    avg_loss:1.480, val_acc:0.713]
Epoch [15/120    avg_loss:1.362, val_acc:0.807]
Epoch [16/120    avg_loss:1.247, val_acc:0.822]
Epoch [17/120    avg_loss:1.179, val_acc:0.867]
Epoch [18/120    avg_loss:1.022, val_acc:0.805]
Epoch [19/120    avg_loss:0.972, val_acc:0.842]
Epoch [20/120    avg_loss:0.904, val_acc:0.852]
Epoch [21/120    avg_loss:0.876, val_acc:0.869]
Epoch [22/120    avg_loss:0.835, val_acc:0.902]
Epoch [23/120    avg_loss:0.753, val_acc:0.891]
Epoch [24/120    avg_loss:0.663, val_acc:0.879]
Epoch [25/120    avg_loss:0.643, val_acc:0.867]
Epoch [26/120    avg_loss:0.616, val_acc:0.900]
Epoch [27/120    avg_loss:0.608, val_acc:0.910]
Epoch [28/120    avg_loss:0.581, val_acc:0.920]
Epoch [29/120    avg_loss:0.579, val_acc:0.926]
Epoch [30/120    avg_loss:0.532, val_acc:0.906]
Epoch [31/120    avg_loss:0.522, val_acc:0.920]
Epoch [32/120    avg_loss:0.490, val_acc:0.912]
Epoch [33/120    avg_loss:0.444, val_acc:0.941]
Epoch [34/120    avg_loss:0.429, val_acc:0.920]
Epoch [35/120    avg_loss:0.427, val_acc:0.945]
Epoch [36/120    avg_loss:0.401, val_acc:0.932]
Epoch [37/120    avg_loss:0.471, val_acc:0.922]
Epoch [38/120    avg_loss:0.408, val_acc:0.924]
Epoch [39/120    avg_loss:0.386, val_acc:0.961]
Epoch [40/120    avg_loss:0.368, val_acc:0.949]
Epoch [41/120    avg_loss:0.401, val_acc:0.934]
Epoch [42/120    avg_loss:0.359, val_acc:0.949]
Epoch [43/120    avg_loss:0.311, val_acc:0.955]
Epoch [44/120    avg_loss:0.323, val_acc:0.959]
Epoch [45/120    avg_loss:0.281, val_acc:0.957]
Epoch [46/120    avg_loss:0.296, val_acc:0.951]
Epoch [47/120    avg_loss:0.316, val_acc:0.877]
Epoch [48/120    avg_loss:0.295, val_acc:0.895]
Epoch [49/120    avg_loss:0.287, val_acc:0.959]
Epoch [50/120    avg_loss:0.233, val_acc:0.957]
Epoch [51/120    avg_loss:0.254, val_acc:0.957]
Epoch [52/120    avg_loss:0.253, val_acc:0.961]
Epoch [53/120    avg_loss:0.196, val_acc:0.959]
Epoch [54/120    avg_loss:0.252, val_acc:0.955]
Epoch [55/120    avg_loss:0.258, val_acc:0.939]
Epoch [56/120    avg_loss:0.226, val_acc:0.963]
Epoch [57/120    avg_loss:0.260, val_acc:0.961]
Epoch [58/120    avg_loss:0.224, val_acc:0.963]
Epoch [59/120    avg_loss:0.222, val_acc:0.949]
Epoch [60/120    avg_loss:0.202, val_acc:0.941]
Epoch [61/120    avg_loss:0.211, val_acc:0.947]
Epoch [62/120    avg_loss:0.260, val_acc:0.965]
Epoch [63/120    avg_loss:0.220, val_acc:0.953]
Epoch [64/120    avg_loss:0.219, val_acc:0.965]
Epoch [65/120    avg_loss:0.227, val_acc:0.934]
Epoch [66/120    avg_loss:0.222, val_acc:0.967]
Epoch [67/120    avg_loss:0.184, val_acc:0.959]
Epoch [68/120    avg_loss:0.195, val_acc:0.961]
Epoch [69/120    avg_loss:0.138, val_acc:0.973]
Epoch [70/120    avg_loss:0.134, val_acc:0.971]
Epoch [71/120    avg_loss:0.191, val_acc:0.936]
Epoch [72/120    avg_loss:0.253, val_acc:0.936]
Epoch [73/120    avg_loss:0.264, val_acc:0.941]
Epoch [74/120    avg_loss:0.207, val_acc:0.943]
Epoch [75/120    avg_loss:0.142, val_acc:0.955]
Epoch [76/120    avg_loss:0.128, val_acc:0.967]
Epoch [77/120    avg_loss:0.100, val_acc:0.980]
Epoch [78/120    avg_loss:0.154, val_acc:0.982]
Epoch [79/120    avg_loss:0.109, val_acc:0.963]
Epoch [80/120    avg_loss:0.138, val_acc:0.955]
Epoch [81/120    avg_loss:0.121, val_acc:0.961]
Epoch [82/120    avg_loss:0.112, val_acc:0.971]
Epoch [83/120    avg_loss:0.101, val_acc:0.969]
Epoch [84/120    avg_loss:0.095, val_acc:0.975]
Epoch [85/120    avg_loss:0.095, val_acc:0.973]
Epoch [86/120    avg_loss:0.085, val_acc:0.979]
Epoch [87/120    avg_loss:0.094, val_acc:0.973]
Epoch [88/120    avg_loss:0.105, val_acc:0.977]
Epoch [89/120    avg_loss:0.181, val_acc:0.906]
Epoch [90/120    avg_loss:0.161, val_acc:0.961]
Epoch [91/120    avg_loss:0.093, val_acc:0.975]
Epoch [92/120    avg_loss:0.057, val_acc:0.969]
Epoch [93/120    avg_loss:0.060, val_acc:0.980]
Epoch [94/120    avg_loss:0.074, val_acc:0.977]
Epoch [95/120    avg_loss:0.060, val_acc:0.975]
Epoch [96/120    avg_loss:0.076, val_acc:0.982]
Epoch [97/120    avg_loss:0.063, val_acc:0.979]
Epoch [98/120    avg_loss:0.049, val_acc:0.979]
Epoch [99/120    avg_loss:0.056, val_acc:0.980]
Epoch [100/120    avg_loss:0.056, val_acc:0.984]
Epoch [101/120    avg_loss:0.051, val_acc:0.986]
Epoch [102/120    avg_loss:0.059, val_acc:0.984]
Epoch [103/120    avg_loss:0.058, val_acc:0.986]
Epoch [104/120    avg_loss:0.044, val_acc:0.986]
Epoch [105/120    avg_loss:0.052, val_acc:0.980]
Epoch [106/120    avg_loss:0.065, val_acc:0.982]
Epoch [107/120    avg_loss:0.045, val_acc:0.992]
Epoch [108/120    avg_loss:0.041, val_acc:0.988]
Epoch [109/120    avg_loss:0.057, val_acc:0.979]
Epoch [110/120    avg_loss:0.043, val_acc:0.982]
Epoch [111/120    avg_loss:0.071, val_acc:0.986]
Epoch [112/120    avg_loss:0.049, val_acc:0.992]
Epoch [113/120    avg_loss:0.041, val_acc:0.992]
Epoch [114/120    avg_loss:0.046, val_acc:0.988]
Epoch [115/120    avg_loss:0.057, val_acc:0.992]
Epoch [116/120    avg_loss:0.054, val_acc:0.988]
Epoch [117/120    avg_loss:0.039, val_acc:0.990]
Epoch [118/120    avg_loss:0.041, val_acc:0.986]
Epoch [119/120    avg_loss:0.040, val_acc:0.992]
Epoch [120/120    avg_loss:0.060, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 217  13   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   7   0   0   1   0   0   0   0   0]
 [  0   0   0   0  32 113   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   3   0   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 0.99636364 0.97737557 0.97091723 0.89205703 0.85283019
 0.99512195 0.95135135 0.99483204 1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9838546599444703
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f7723fda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.123]
Epoch [2/120    avg_loss:2.544, val_acc:0.301]
Epoch [3/120    avg_loss:2.494, val_acc:0.355]
Epoch [4/120    avg_loss:2.435, val_acc:0.398]
Epoch [5/120    avg_loss:2.368, val_acc:0.443]
Epoch [6/120    avg_loss:2.286, val_acc:0.508]
Epoch [7/120    avg_loss:2.209, val_acc:0.500]
Epoch [8/120    avg_loss:2.116, val_acc:0.521]
Epoch [9/120    avg_loss:2.006, val_acc:0.627]
Epoch [10/120    avg_loss:1.914, val_acc:0.621]
Epoch [11/120    avg_loss:1.783, val_acc:0.689]
Epoch [12/120    avg_loss:1.659, val_acc:0.613]
Epoch [13/120    avg_loss:1.545, val_acc:0.715]
Epoch [14/120    avg_loss:1.440, val_acc:0.711]
Epoch [15/120    avg_loss:1.330, val_acc:0.709]
Epoch [16/120    avg_loss:1.180, val_acc:0.818]
Epoch [17/120    avg_loss:1.107, val_acc:0.807]
Epoch [18/120    avg_loss:1.016, val_acc:0.857]
Epoch [19/120    avg_loss:0.902, val_acc:0.850]
Epoch [20/120    avg_loss:0.818, val_acc:0.855]
Epoch [21/120    avg_loss:0.765, val_acc:0.879]
Epoch [22/120    avg_loss:0.735, val_acc:0.863]
Epoch [23/120    avg_loss:0.751, val_acc:0.883]
Epoch [24/120    avg_loss:0.663, val_acc:0.883]
Epoch [25/120    avg_loss:0.649, val_acc:0.871]
Epoch [26/120    avg_loss:0.565, val_acc:0.891]
Epoch [27/120    avg_loss:0.579, val_acc:0.873]
Epoch [28/120    avg_loss:0.533, val_acc:0.910]
Epoch [29/120    avg_loss:0.547, val_acc:0.900]
Epoch [30/120    avg_loss:0.454, val_acc:0.910]
Epoch [31/120    avg_loss:0.467, val_acc:0.908]
Epoch [32/120    avg_loss:0.433, val_acc:0.930]
Epoch [33/120    avg_loss:0.389, val_acc:0.932]
Epoch [34/120    avg_loss:0.472, val_acc:0.900]
Epoch [35/120    avg_loss:0.434, val_acc:0.930]
Epoch [36/120    avg_loss:0.368, val_acc:0.930]
Epoch [37/120    avg_loss:0.385, val_acc:0.928]
Epoch [38/120    avg_loss:0.395, val_acc:0.904]
Epoch [39/120    avg_loss:0.407, val_acc:0.922]
Epoch [40/120    avg_loss:0.362, val_acc:0.934]
Epoch [41/120    avg_loss:0.334, val_acc:0.898]
Epoch [42/120    avg_loss:0.340, val_acc:0.928]
Epoch [43/120    avg_loss:0.297, val_acc:0.947]
Epoch [44/120    avg_loss:0.260, val_acc:0.959]
Epoch [45/120    avg_loss:0.310, val_acc:0.934]
Epoch [46/120    avg_loss:0.292, val_acc:0.928]
Epoch [47/120    avg_loss:0.304, val_acc:0.896]
Epoch [48/120    avg_loss:0.249, val_acc:0.943]
Epoch [49/120    avg_loss:0.276, val_acc:0.938]
Epoch [50/120    avg_loss:0.247, val_acc:0.928]
Epoch [51/120    avg_loss:0.227, val_acc:0.959]
Epoch [52/120    avg_loss:0.236, val_acc:0.957]
Epoch [53/120    avg_loss:0.199, val_acc:0.951]
Epoch [54/120    avg_loss:0.168, val_acc:0.939]
Epoch [55/120    avg_loss:0.260, val_acc:0.955]
Epoch [56/120    avg_loss:0.223, val_acc:0.910]
Epoch [57/120    avg_loss:0.272, val_acc:0.914]
Epoch [58/120    avg_loss:0.194, val_acc:0.947]
Epoch [59/120    avg_loss:0.221, val_acc:0.902]
Epoch [60/120    avg_loss:0.372, val_acc:0.922]
Epoch [61/120    avg_loss:0.293, val_acc:0.945]
Epoch [62/120    avg_loss:0.217, val_acc:0.943]
Epoch [63/120    avg_loss:0.201, val_acc:0.957]
Epoch [64/120    avg_loss:0.236, val_acc:0.924]
Epoch [65/120    avg_loss:0.230, val_acc:0.965]
Epoch [66/120    avg_loss:0.142, val_acc:0.965]
Epoch [67/120    avg_loss:0.136, val_acc:0.967]
Epoch [68/120    avg_loss:0.108, val_acc:0.969]
Epoch [69/120    avg_loss:0.117, val_acc:0.969]
Epoch [70/120    avg_loss:0.119, val_acc:0.967]
Epoch [71/120    avg_loss:0.115, val_acc:0.967]
Epoch [72/120    avg_loss:0.109, val_acc:0.973]
Epoch [73/120    avg_loss:0.100, val_acc:0.977]
Epoch [74/120    avg_loss:0.110, val_acc:0.973]
Epoch [75/120    avg_loss:0.105, val_acc:0.973]
Epoch [76/120    avg_loss:0.104, val_acc:0.973]
Epoch [77/120    avg_loss:0.097, val_acc:0.971]
Epoch [78/120    avg_loss:0.109, val_acc:0.977]
Epoch [79/120    avg_loss:0.094, val_acc:0.979]
Epoch [80/120    avg_loss:0.098, val_acc:0.977]
Epoch [81/120    avg_loss:0.094, val_acc:0.975]
Epoch [82/120    avg_loss:0.097, val_acc:0.973]
Epoch [83/120    avg_loss:0.092, val_acc:0.973]
Epoch [84/120    avg_loss:0.091, val_acc:0.975]
Epoch [85/120    avg_loss:0.114, val_acc:0.979]
Epoch [86/120    avg_loss:0.088, val_acc:0.975]
Epoch [87/120    avg_loss:0.095, val_acc:0.975]
Epoch [88/120    avg_loss:0.098, val_acc:0.977]
Epoch [89/120    avg_loss:0.093, val_acc:0.977]
Epoch [90/120    avg_loss:0.087, val_acc:0.977]
Epoch [91/120    avg_loss:0.085, val_acc:0.977]
Epoch [92/120    avg_loss:0.094, val_acc:0.975]
Epoch [93/120    avg_loss:0.073, val_acc:0.977]
Epoch [94/120    avg_loss:0.096, val_acc:0.975]
Epoch [95/120    avg_loss:0.082, val_acc:0.973]
Epoch [96/120    avg_loss:0.078, val_acc:0.975]
Epoch [97/120    avg_loss:0.077, val_acc:0.971]
Epoch [98/120    avg_loss:0.082, val_acc:0.979]
Epoch [99/120    avg_loss:0.067, val_acc:0.973]
Epoch [100/120    avg_loss:0.085, val_acc:0.979]
Epoch [101/120    avg_loss:0.098, val_acc:0.971]
Epoch [102/120    avg_loss:0.083, val_acc:0.975]
Epoch [103/120    avg_loss:0.090, val_acc:0.975]
Epoch [104/120    avg_loss:0.081, val_acc:0.977]
Epoch [105/120    avg_loss:0.084, val_acc:0.977]
Epoch [106/120    avg_loss:0.078, val_acc:0.977]
Epoch [107/120    avg_loss:0.077, val_acc:0.971]
Epoch [108/120    avg_loss:0.070, val_acc:0.977]
Epoch [109/120    avg_loss:0.073, val_acc:0.979]
Epoch [110/120    avg_loss:0.081, val_acc:0.977]
Epoch [111/120    avg_loss:0.063, val_acc:0.979]
Epoch [112/120    avg_loss:0.071, val_acc:0.979]
Epoch [113/120    avg_loss:0.076, val_acc:0.975]
Epoch [114/120    avg_loss:0.074, val_acc:0.973]
Epoch [115/120    avg_loss:0.080, val_acc:0.973]
Epoch [116/120    avg_loss:0.085, val_acc:0.975]
Epoch [117/120    avg_loss:0.073, val_acc:0.977]
Epoch [118/120    avg_loss:0.079, val_acc:0.975]
Epoch [119/120    avg_loss:0.078, val_acc:0.973]
Epoch [120/120    avg_loss:0.076, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 204   0   0   0   0  15   0   0   0   0   0   0]
 [  0   0   0 220   4   0   0   0   5   1   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.93363844 0.97777778 0.94315789 0.91575092
 1.         0.84656085 0.99359795 0.99893276 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.984805712719946
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6573306e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.115]
Epoch [2/120    avg_loss:2.541, val_acc:0.317]
Epoch [3/120    avg_loss:2.467, val_acc:0.381]
Epoch [4/120    avg_loss:2.404, val_acc:0.438]
Epoch [5/120    avg_loss:2.333, val_acc:0.446]
Epoch [6/120    avg_loss:2.266, val_acc:0.456]
Epoch [7/120    avg_loss:2.200, val_acc:0.508]
Epoch [8/120    avg_loss:2.119, val_acc:0.579]
Epoch [9/120    avg_loss:2.043, val_acc:0.663]
Epoch [10/120    avg_loss:1.943, val_acc:0.643]
Epoch [11/120    avg_loss:1.829, val_acc:0.710]
Epoch [12/120    avg_loss:1.784, val_acc:0.724]
Epoch [13/120    avg_loss:1.640, val_acc:0.728]
Epoch [14/120    avg_loss:1.522, val_acc:0.738]
Epoch [15/120    avg_loss:1.415, val_acc:0.760]
Epoch [16/120    avg_loss:1.270, val_acc:0.738]
Epoch [17/120    avg_loss:1.191, val_acc:0.857]
Epoch [18/120    avg_loss:1.072, val_acc:0.893]
Epoch [19/120    avg_loss:0.970, val_acc:0.877]
Epoch [20/120    avg_loss:0.915, val_acc:0.909]
Epoch [21/120    avg_loss:0.808, val_acc:0.919]
Epoch [22/120    avg_loss:0.796, val_acc:0.857]
Epoch [23/120    avg_loss:0.743, val_acc:0.897]
Epoch [24/120    avg_loss:0.685, val_acc:0.897]
Epoch [25/120    avg_loss:0.678, val_acc:0.919]
Epoch [26/120    avg_loss:0.594, val_acc:0.921]
Epoch [27/120    avg_loss:0.583, val_acc:0.915]
Epoch [28/120    avg_loss:0.587, val_acc:0.919]
Epoch [29/120    avg_loss:0.528, val_acc:0.883]
Epoch [30/120    avg_loss:0.546, val_acc:0.891]
Epoch [31/120    avg_loss:0.479, val_acc:0.931]
Epoch [32/120    avg_loss:0.460, val_acc:0.887]
Epoch [33/120    avg_loss:0.498, val_acc:0.935]
Epoch [34/120    avg_loss:0.415, val_acc:0.925]
Epoch [35/120    avg_loss:0.389, val_acc:0.921]
Epoch [36/120    avg_loss:0.393, val_acc:0.913]
Epoch [37/120    avg_loss:0.393, val_acc:0.927]
Epoch [38/120    avg_loss:0.374, val_acc:0.887]
Epoch [39/120    avg_loss:0.502, val_acc:0.911]
Epoch [40/120    avg_loss:0.397, val_acc:0.942]
Epoch [41/120    avg_loss:0.372, val_acc:0.938]
Epoch [42/120    avg_loss:0.362, val_acc:0.931]
Epoch [43/120    avg_loss:0.346, val_acc:0.946]
Epoch [44/120    avg_loss:0.345, val_acc:0.923]
Epoch [45/120    avg_loss:0.311, val_acc:0.933]
Epoch [46/120    avg_loss:0.320, val_acc:0.940]
Epoch [47/120    avg_loss:0.274, val_acc:0.946]
Epoch [48/120    avg_loss:0.350, val_acc:0.933]
Epoch [49/120    avg_loss:0.297, val_acc:0.935]
Epoch [50/120    avg_loss:0.304, val_acc:0.942]
Epoch [51/120    avg_loss:0.234, val_acc:0.944]
Epoch [52/120    avg_loss:0.246, val_acc:0.944]
Epoch [53/120    avg_loss:0.237, val_acc:0.954]
Epoch [54/120    avg_loss:0.220, val_acc:0.940]
Epoch [55/120    avg_loss:0.180, val_acc:0.935]
Epoch [56/120    avg_loss:0.214, val_acc:0.944]
Epoch [57/120    avg_loss:0.228, val_acc:0.956]
Epoch [58/120    avg_loss:0.224, val_acc:0.954]
Epoch [59/120    avg_loss:0.191, val_acc:0.958]
Epoch [60/120    avg_loss:0.191, val_acc:0.964]
Epoch [61/120    avg_loss:0.202, val_acc:0.968]
Epoch [62/120    avg_loss:0.161, val_acc:0.970]
Epoch [63/120    avg_loss:0.199, val_acc:0.942]
Epoch [64/120    avg_loss:0.189, val_acc:0.972]
Epoch [65/120    avg_loss:0.156, val_acc:0.954]
Epoch [66/120    avg_loss:0.209, val_acc:0.960]
Epoch [67/120    avg_loss:0.186, val_acc:0.964]
Epoch [68/120    avg_loss:0.144, val_acc:0.958]
Epoch [69/120    avg_loss:0.174, val_acc:0.948]
Epoch [70/120    avg_loss:0.208, val_acc:0.938]
Epoch [71/120    avg_loss:0.146, val_acc:0.972]
Epoch [72/120    avg_loss:0.112, val_acc:0.954]
Epoch [73/120    avg_loss:0.121, val_acc:0.972]
Epoch [74/120    avg_loss:0.101, val_acc:0.972]
Epoch [75/120    avg_loss:0.115, val_acc:0.944]
Epoch [76/120    avg_loss:0.136, val_acc:0.976]
Epoch [77/120    avg_loss:0.134, val_acc:0.970]
Epoch [78/120    avg_loss:0.116, val_acc:0.972]
Epoch [79/120    avg_loss:0.108, val_acc:0.976]
Epoch [80/120    avg_loss:0.075, val_acc:0.974]
Epoch [81/120    avg_loss:0.085, val_acc:0.976]
Epoch [82/120    avg_loss:0.148, val_acc:0.964]
Epoch [83/120    avg_loss:0.113, val_acc:0.970]
Epoch [84/120    avg_loss:0.123, val_acc:0.978]
Epoch [85/120    avg_loss:0.083, val_acc:0.974]
Epoch [86/120    avg_loss:0.073, val_acc:0.976]
Epoch [87/120    avg_loss:0.166, val_acc:0.942]
Epoch [88/120    avg_loss:0.233, val_acc:0.950]
Epoch [89/120    avg_loss:0.137, val_acc:0.964]
Epoch [90/120    avg_loss:0.080, val_acc:0.976]
Epoch [91/120    avg_loss:0.124, val_acc:0.974]
Epoch [92/120    avg_loss:0.117, val_acc:0.972]
Epoch [93/120    avg_loss:0.084, val_acc:0.980]
Epoch [94/120    avg_loss:0.069, val_acc:0.980]
Epoch [95/120    avg_loss:0.043, val_acc:0.978]
Epoch [96/120    avg_loss:0.060, val_acc:0.980]
Epoch [97/120    avg_loss:0.055, val_acc:0.984]
Epoch [98/120    avg_loss:0.061, val_acc:0.976]
Epoch [99/120    avg_loss:0.088, val_acc:0.976]
Epoch [100/120    avg_loss:0.067, val_acc:0.974]
Epoch [101/120    avg_loss:0.077, val_acc:0.974]
Epoch [102/120    avg_loss:0.079, val_acc:0.978]
Epoch [103/120    avg_loss:0.080, val_acc:0.980]
Epoch [104/120    avg_loss:0.043, val_acc:0.982]
Epoch [105/120    avg_loss:0.035, val_acc:0.978]
Epoch [106/120    avg_loss:0.034, val_acc:0.978]
Epoch [107/120    avg_loss:0.057, val_acc:0.970]
Epoch [108/120    avg_loss:0.047, val_acc:0.976]
Epoch [109/120    avg_loss:0.042, val_acc:0.980]
Epoch [110/120    avg_loss:0.087, val_acc:0.972]
Epoch [111/120    avg_loss:0.062, val_acc:0.978]
Epoch [112/120    avg_loss:0.035, val_acc:0.980]
Epoch [113/120    avg_loss:0.035, val_acc:0.984]
Epoch [114/120    avg_loss:0.032, val_acc:0.984]
Epoch [115/120    avg_loss:0.029, val_acc:0.986]
Epoch [116/120    avg_loss:0.027, val_acc:0.986]
Epoch [117/120    avg_loss:0.026, val_acc:0.984]
Epoch [118/120    avg_loss:0.027, val_acc:0.984]
Epoch [119/120    avg_loss:0.025, val_acc:0.986]
Epoch [120/120    avg_loss:0.023, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 216   7   0   0   0   7   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.98648649 0.96860987 0.92105263 0.90540541
 0.99756691 0.96703297 0.99106003 1.         1.         1.
 1.         1.        ]

Kappa:
0.9883675056309857
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f044ea75dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.216]
Epoch [2/120    avg_loss:2.552, val_acc:0.286]
Epoch [3/120    avg_loss:2.486, val_acc:0.286]
Epoch [4/120    avg_loss:2.428, val_acc:0.298]
Epoch [5/120    avg_loss:2.377, val_acc:0.395]
Epoch [6/120    avg_loss:2.302, val_acc:0.407]
Epoch [7/120    avg_loss:2.246, val_acc:0.470]
Epoch [8/120    avg_loss:2.171, val_acc:0.484]
Epoch [9/120    avg_loss:2.061, val_acc:0.522]
Epoch [10/120    avg_loss:1.982, val_acc:0.520]
Epoch [11/120    avg_loss:1.922, val_acc:0.587]
Epoch [12/120    avg_loss:1.846, val_acc:0.639]
Epoch [13/120    avg_loss:1.736, val_acc:0.683]
Epoch [14/120    avg_loss:1.610, val_acc:0.669]
Epoch [15/120    avg_loss:1.484, val_acc:0.696]
Epoch [16/120    avg_loss:1.382, val_acc:0.714]
Epoch [17/120    avg_loss:1.299, val_acc:0.736]
Epoch [18/120    avg_loss:1.171, val_acc:0.829]
Epoch [19/120    avg_loss:1.144, val_acc:0.835]
Epoch [20/120    avg_loss:1.010, val_acc:0.833]
Epoch [21/120    avg_loss:1.006, val_acc:0.825]
Epoch [22/120    avg_loss:0.904, val_acc:0.871]
Epoch [23/120    avg_loss:0.871, val_acc:0.744]
Epoch [24/120    avg_loss:0.772, val_acc:0.875]
Epoch [25/120    avg_loss:0.771, val_acc:0.857]
Epoch [26/120    avg_loss:0.666, val_acc:0.921]
Epoch [27/120    avg_loss:0.626, val_acc:0.917]
Epoch [28/120    avg_loss:0.621, val_acc:0.913]
Epoch [29/120    avg_loss:0.595, val_acc:0.915]
Epoch [30/120    avg_loss:0.512, val_acc:0.935]
Epoch [31/120    avg_loss:0.494, val_acc:0.933]
Epoch [32/120    avg_loss:0.433, val_acc:0.944]
Epoch [33/120    avg_loss:0.459, val_acc:0.948]
Epoch [34/120    avg_loss:0.423, val_acc:0.921]
Epoch [35/120    avg_loss:0.437, val_acc:0.954]
Epoch [36/120    avg_loss:0.400, val_acc:0.944]
Epoch [37/120    avg_loss:0.425, val_acc:0.935]
Epoch [38/120    avg_loss:0.340, val_acc:0.946]
Epoch [39/120    avg_loss:0.355, val_acc:0.948]
Epoch [40/120    avg_loss:0.360, val_acc:0.954]
Epoch [41/120    avg_loss:0.303, val_acc:0.960]
Epoch [42/120    avg_loss:0.287, val_acc:0.960]
Epoch [43/120    avg_loss:0.247, val_acc:0.956]
Epoch [44/120    avg_loss:0.262, val_acc:0.974]
Epoch [45/120    avg_loss:0.331, val_acc:0.915]
Epoch [46/120    avg_loss:0.278, val_acc:0.956]
Epoch [47/120    avg_loss:0.274, val_acc:0.976]
Epoch [48/120    avg_loss:0.253, val_acc:0.980]
Epoch [49/120    avg_loss:0.278, val_acc:0.935]
Epoch [50/120    avg_loss:0.206, val_acc:0.944]
Epoch [51/120    avg_loss:0.222, val_acc:0.978]
Epoch [52/120    avg_loss:0.259, val_acc:0.976]
Epoch [53/120    avg_loss:0.192, val_acc:0.984]
Epoch [54/120    avg_loss:0.201, val_acc:0.970]
Epoch [55/120    avg_loss:0.191, val_acc:0.978]
Epoch [56/120    avg_loss:0.218, val_acc:0.988]
Epoch [57/120    avg_loss:0.175, val_acc:0.986]
Epoch [58/120    avg_loss:0.152, val_acc:0.974]
Epoch [59/120    avg_loss:0.185, val_acc:0.988]
Epoch [60/120    avg_loss:0.160, val_acc:0.984]
Epoch [61/120    avg_loss:0.156, val_acc:0.986]
Epoch [62/120    avg_loss:0.128, val_acc:0.972]
Epoch [63/120    avg_loss:0.144, val_acc:0.948]
Epoch [64/120    avg_loss:0.157, val_acc:0.990]
Epoch [65/120    avg_loss:0.146, val_acc:0.972]
Epoch [66/120    avg_loss:0.172, val_acc:0.956]
Epoch [67/120    avg_loss:0.177, val_acc:0.978]
Epoch [68/120    avg_loss:0.157, val_acc:0.984]
Epoch [69/120    avg_loss:0.122, val_acc:0.966]
Epoch [70/120    avg_loss:0.126, val_acc:0.984]
Epoch [71/120    avg_loss:0.109, val_acc:0.988]
Epoch [72/120    avg_loss:0.096, val_acc:0.988]
Epoch [73/120    avg_loss:0.092, val_acc:0.970]
Epoch [74/120    avg_loss:0.078, val_acc:0.988]
Epoch [75/120    avg_loss:0.083, val_acc:0.988]
Epoch [76/120    avg_loss:0.112, val_acc:0.976]
Epoch [77/120    avg_loss:0.086, val_acc:0.982]
Epoch [78/120    avg_loss:0.068, val_acc:0.988]
Epoch [79/120    avg_loss:0.061, val_acc:0.990]
Epoch [80/120    avg_loss:0.048, val_acc:0.992]
Epoch [81/120    avg_loss:0.059, val_acc:0.992]
Epoch [82/120    avg_loss:0.055, val_acc:0.992]
Epoch [83/120    avg_loss:0.049, val_acc:0.994]
Epoch [84/120    avg_loss:0.045, val_acc:0.990]
Epoch [85/120    avg_loss:0.053, val_acc:0.990]
Epoch [86/120    avg_loss:0.060, val_acc:0.988]
Epoch [87/120    avg_loss:0.052, val_acc:0.990]
Epoch [88/120    avg_loss:0.050, val_acc:0.990]
Epoch [89/120    avg_loss:0.048, val_acc:0.988]
Epoch [90/120    avg_loss:0.044, val_acc:0.990]
Epoch [91/120    avg_loss:0.050, val_acc:0.990]
Epoch [92/120    avg_loss:0.047, val_acc:0.990]
Epoch [93/120    avg_loss:0.042, val_acc:0.992]
Epoch [94/120    avg_loss:0.048, val_acc:0.990]
Epoch [95/120    avg_loss:0.056, val_acc:0.992]
Epoch [96/120    avg_loss:0.045, val_acc:0.994]
Epoch [97/120    avg_loss:0.053, val_acc:0.988]
Epoch [98/120    avg_loss:0.048, val_acc:0.992]
Epoch [99/120    avg_loss:0.052, val_acc:0.992]
Epoch [100/120    avg_loss:0.046, val_acc:0.988]
Epoch [101/120    avg_loss:0.043, val_acc:0.990]
Epoch [102/120    avg_loss:0.047, val_acc:0.990]
Epoch [103/120    avg_loss:0.047, val_acc:0.994]
Epoch [104/120    avg_loss:0.045, val_acc:0.992]
Epoch [105/120    avg_loss:0.044, val_acc:0.990]
Epoch [106/120    avg_loss:0.049, val_acc:0.994]
Epoch [107/120    avg_loss:0.042, val_acc:0.990]
Epoch [108/120    avg_loss:0.046, val_acc:0.992]
Epoch [109/120    avg_loss:0.056, val_acc:0.996]
Epoch [110/120    avg_loss:0.046, val_acc:0.990]
Epoch [111/120    avg_loss:0.043, val_acc:0.994]
Epoch [112/120    avg_loss:0.044, val_acc:0.992]
Epoch [113/120    avg_loss:0.039, val_acc:0.996]
Epoch [114/120    avg_loss:0.038, val_acc:0.990]
Epoch [115/120    avg_loss:0.037, val_acc:0.992]
Epoch [116/120    avg_loss:0.039, val_acc:0.994]
Epoch [117/120    avg_loss:0.037, val_acc:0.996]
Epoch [118/120    avg_loss:0.043, val_acc:0.996]
Epoch [119/120    avg_loss:0.036, val_acc:0.994]
Epoch [120/120    avg_loss:0.040, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.98426966 0.99343545 0.92340426 0.88086643
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9897915991953147
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f42bc414e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.639, val_acc:0.146]
Epoch [2/120    avg_loss:2.572, val_acc:0.192]
Epoch [3/120    avg_loss:2.505, val_acc:0.345]
Epoch [4/120    avg_loss:2.438, val_acc:0.359]
Epoch [5/120    avg_loss:2.380, val_acc:0.363]
Epoch [6/120    avg_loss:2.314, val_acc:0.381]
Epoch [7/120    avg_loss:2.251, val_acc:0.429]
Epoch [8/120    avg_loss:2.188, val_acc:0.508]
Epoch [9/120    avg_loss:2.108, val_acc:0.528]
Epoch [10/120    avg_loss:2.020, val_acc:0.595]
Epoch [11/120    avg_loss:1.948, val_acc:0.627]
Epoch [12/120    avg_loss:1.850, val_acc:0.627]
Epoch [13/120    avg_loss:1.743, val_acc:0.663]
Epoch [14/120    avg_loss:1.665, val_acc:0.673]
Epoch [15/120    avg_loss:1.502, val_acc:0.740]
Epoch [16/120    avg_loss:1.358, val_acc:0.669]
Epoch [17/120    avg_loss:1.244, val_acc:0.734]
Epoch [18/120    avg_loss:1.124, val_acc:0.740]
Epoch [19/120    avg_loss:1.086, val_acc:0.778]
Epoch [20/120    avg_loss:0.957, val_acc:0.851]
Epoch [21/120    avg_loss:0.864, val_acc:0.867]
Epoch [22/120    avg_loss:0.856, val_acc:0.776]
Epoch [23/120    avg_loss:0.738, val_acc:0.895]
Epoch [24/120    avg_loss:0.658, val_acc:0.905]
Epoch [25/120    avg_loss:0.676, val_acc:0.911]
Epoch [26/120    avg_loss:0.623, val_acc:0.917]
Epoch [27/120    avg_loss:0.551, val_acc:0.909]
Epoch [28/120    avg_loss:0.503, val_acc:0.923]
Epoch [29/120    avg_loss:0.458, val_acc:0.933]
Epoch [30/120    avg_loss:0.430, val_acc:0.925]
Epoch [31/120    avg_loss:0.443, val_acc:0.909]
Epoch [32/120    avg_loss:0.488, val_acc:0.919]
Epoch [33/120    avg_loss:0.440, val_acc:0.907]
Epoch [34/120    avg_loss:0.369, val_acc:0.897]
Epoch [35/120    avg_loss:0.481, val_acc:0.935]
Epoch [36/120    avg_loss:0.409, val_acc:0.944]
Epoch [37/120    avg_loss:0.401, val_acc:0.938]
Epoch [38/120    avg_loss:0.356, val_acc:0.946]
Epoch [39/120    avg_loss:0.324, val_acc:0.940]
Epoch [40/120    avg_loss:0.280, val_acc:0.950]
Epoch [41/120    avg_loss:0.338, val_acc:0.935]
Epoch [42/120    avg_loss:0.305, val_acc:0.841]
Epoch [43/120    avg_loss:0.366, val_acc:0.938]
Epoch [44/120    avg_loss:0.247, val_acc:0.940]
Epoch [45/120    avg_loss:0.269, val_acc:0.935]
Epoch [46/120    avg_loss:0.248, val_acc:0.958]
Epoch [47/120    avg_loss:0.244, val_acc:0.925]
Epoch [48/120    avg_loss:0.247, val_acc:0.958]
Epoch [49/120    avg_loss:0.193, val_acc:0.972]
Epoch [50/120    avg_loss:0.192, val_acc:0.942]
Epoch [51/120    avg_loss:0.225, val_acc:0.958]
Epoch [52/120    avg_loss:0.222, val_acc:0.944]
Epoch [53/120    avg_loss:0.217, val_acc:0.946]
Epoch [54/120    avg_loss:0.205, val_acc:0.966]
Epoch [55/120    avg_loss:0.188, val_acc:0.982]
Epoch [56/120    avg_loss:0.182, val_acc:0.960]
Epoch [57/120    avg_loss:0.160, val_acc:0.970]
Epoch [58/120    avg_loss:0.174, val_acc:0.974]
Epoch [59/120    avg_loss:0.158, val_acc:0.962]
Epoch [60/120    avg_loss:0.183, val_acc:0.984]
Epoch [61/120    avg_loss:0.161, val_acc:0.988]
Epoch [62/120    avg_loss:0.148, val_acc:0.935]
Epoch [63/120    avg_loss:0.167, val_acc:0.972]
Epoch [64/120    avg_loss:0.121, val_acc:0.982]
Epoch [65/120    avg_loss:0.128, val_acc:0.970]
Epoch [66/120    avg_loss:0.145, val_acc:0.960]
Epoch [67/120    avg_loss:0.147, val_acc:0.976]
Epoch [68/120    avg_loss:0.144, val_acc:0.974]
Epoch [69/120    avg_loss:0.152, val_acc:0.982]
Epoch [70/120    avg_loss:0.137, val_acc:0.966]
Epoch [71/120    avg_loss:0.106, val_acc:0.966]
Epoch [72/120    avg_loss:0.117, val_acc:0.988]
Epoch [73/120    avg_loss:0.100, val_acc:0.974]
Epoch [74/120    avg_loss:0.107, val_acc:0.976]
Epoch [75/120    avg_loss:0.133, val_acc:0.964]
Epoch [76/120    avg_loss:0.136, val_acc:0.982]
Epoch [77/120    avg_loss:0.109, val_acc:0.980]
Epoch [78/120    avg_loss:0.096, val_acc:0.970]
Epoch [79/120    avg_loss:0.102, val_acc:0.978]
Epoch [80/120    avg_loss:0.102, val_acc:0.972]
Epoch [81/120    avg_loss:0.098, val_acc:0.968]
Epoch [82/120    avg_loss:0.129, val_acc:0.974]
Epoch [83/120    avg_loss:0.117, val_acc:0.990]
Epoch [84/120    avg_loss:0.088, val_acc:0.990]
Epoch [85/120    avg_loss:0.074, val_acc:0.990]
Epoch [86/120    avg_loss:0.061, val_acc:0.986]
Epoch [87/120    avg_loss:0.084, val_acc:0.980]
Epoch [88/120    avg_loss:0.101, val_acc:0.972]
Epoch [89/120    avg_loss:0.120, val_acc:0.970]
Epoch [90/120    avg_loss:0.102, val_acc:0.988]
Epoch [91/120    avg_loss:0.083, val_acc:0.986]
Epoch [92/120    avg_loss:0.082, val_acc:0.986]
Epoch [93/120    avg_loss:0.092, val_acc:0.982]
Epoch [94/120    avg_loss:0.104, val_acc:0.984]
Epoch [95/120    avg_loss:0.066, val_acc:0.990]
Epoch [96/120    avg_loss:0.032, val_acc:0.992]
Epoch [97/120    avg_loss:0.044, val_acc:0.992]
Epoch [98/120    avg_loss:0.060, val_acc:0.984]
Epoch [99/120    avg_loss:0.050, val_acc:0.990]
Epoch [100/120    avg_loss:0.032, val_acc:0.998]
Epoch [101/120    avg_loss:0.041, val_acc:0.996]
Epoch [102/120    avg_loss:0.034, val_acc:0.992]
Epoch [103/120    avg_loss:0.041, val_acc:0.986]
Epoch [104/120    avg_loss:0.033, val_acc:0.998]
Epoch [105/120    avg_loss:0.027, val_acc:0.994]
Epoch [106/120    avg_loss:0.032, val_acc:0.986]
Epoch [107/120    avg_loss:0.031, val_acc:0.988]
Epoch [108/120    avg_loss:0.042, val_acc:0.994]
Epoch [109/120    avg_loss:0.086, val_acc:0.994]
Epoch [110/120    avg_loss:0.093, val_acc:0.917]
Epoch [111/120    avg_loss:0.197, val_acc:0.960]
Epoch [112/120    avg_loss:0.085, val_acc:0.984]
Epoch [113/120    avg_loss:0.074, val_acc:0.984]
Epoch [114/120    avg_loss:0.062, val_acc:0.980]
Epoch [115/120    avg_loss:0.097, val_acc:0.974]
Epoch [116/120    avg_loss:0.113, val_acc:0.976]
Epoch [117/120    avg_loss:0.079, val_acc:0.980]
Epoch [118/120    avg_loss:0.061, val_acc:0.992]
Epoch [119/120    avg_loss:0.047, val_acc:0.994]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   1   0   0   5   0 200   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 0.99927061 0.96902655 0.98678414 0.93390192 0.93006993
 0.98522167 0.91954023 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9890789915820054
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8ee330e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.613, val_acc:0.220]
Epoch [2/120    avg_loss:2.553, val_acc:0.262]
Epoch [3/120    avg_loss:2.486, val_acc:0.270]
Epoch [4/120    avg_loss:2.425, val_acc:0.292]
Epoch [5/120    avg_loss:2.366, val_acc:0.331]
Epoch [6/120    avg_loss:2.301, val_acc:0.355]
Epoch [7/120    avg_loss:2.238, val_acc:0.425]
Epoch [8/120    avg_loss:2.175, val_acc:0.494]
Epoch [9/120    avg_loss:2.077, val_acc:0.542]
Epoch [10/120    avg_loss:1.974, val_acc:0.605]
Epoch [11/120    avg_loss:1.869, val_acc:0.591]
Epoch [12/120    avg_loss:1.735, val_acc:0.659]
Epoch [13/120    avg_loss:1.599, val_acc:0.675]
Epoch [14/120    avg_loss:1.484, val_acc:0.714]
Epoch [15/120    avg_loss:1.338, val_acc:0.788]
Epoch [16/120    avg_loss:1.246, val_acc:0.782]
Epoch [17/120    avg_loss:1.126, val_acc:0.800]
Epoch [18/120    avg_loss:1.010, val_acc:0.855]
Epoch [19/120    avg_loss:0.932, val_acc:0.843]
Epoch [20/120    avg_loss:0.846, val_acc:0.889]
Epoch [21/120    avg_loss:0.746, val_acc:0.899]
Epoch [22/120    avg_loss:0.727, val_acc:0.901]
Epoch [23/120    avg_loss:0.624, val_acc:0.861]
Epoch [24/120    avg_loss:0.550, val_acc:0.905]
Epoch [25/120    avg_loss:0.615, val_acc:0.839]
Epoch [26/120    avg_loss:0.539, val_acc:0.903]
Epoch [27/120    avg_loss:0.501, val_acc:0.903]
Epoch [28/120    avg_loss:0.606, val_acc:0.907]
Epoch [29/120    avg_loss:0.476, val_acc:0.909]
Epoch [30/120    avg_loss:0.483, val_acc:0.917]
Epoch [31/120    avg_loss:0.419, val_acc:0.933]
Epoch [32/120    avg_loss:0.358, val_acc:0.875]
Epoch [33/120    avg_loss:0.461, val_acc:0.917]
Epoch [34/120    avg_loss:0.341, val_acc:0.933]
Epoch [35/120    avg_loss:0.319, val_acc:0.942]
Epoch [36/120    avg_loss:0.346, val_acc:0.923]
Epoch [37/120    avg_loss:0.326, val_acc:0.935]
Epoch [38/120    avg_loss:0.392, val_acc:0.933]
Epoch [39/120    avg_loss:0.336, val_acc:0.935]
Epoch [40/120    avg_loss:0.334, val_acc:0.893]
Epoch [41/120    avg_loss:0.253, val_acc:0.935]
Epoch [42/120    avg_loss:0.221, val_acc:0.948]
Epoch [43/120    avg_loss:0.213, val_acc:0.946]
Epoch [44/120    avg_loss:0.241, val_acc:0.938]
Epoch [45/120    avg_loss:0.343, val_acc:0.954]
Epoch [46/120    avg_loss:0.289, val_acc:0.940]
Epoch [47/120    avg_loss:0.240, val_acc:0.925]
Epoch [48/120    avg_loss:0.235, val_acc:0.946]
Epoch [49/120    avg_loss:0.231, val_acc:0.958]
Epoch [50/120    avg_loss:0.225, val_acc:0.942]
Epoch [51/120    avg_loss:0.232, val_acc:0.919]
Epoch [52/120    avg_loss:0.308, val_acc:0.946]
Epoch [53/120    avg_loss:0.200, val_acc:0.964]
Epoch [54/120    avg_loss:0.226, val_acc:0.948]
Epoch [55/120    avg_loss:0.234, val_acc:0.962]
Epoch [56/120    avg_loss:0.268, val_acc:0.952]
Epoch [57/120    avg_loss:0.239, val_acc:0.966]
Epoch [58/120    avg_loss:0.184, val_acc:0.956]
Epoch [59/120    avg_loss:0.184, val_acc:0.911]
Epoch [60/120    avg_loss:0.214, val_acc:0.960]
Epoch [61/120    avg_loss:0.234, val_acc:0.964]
Epoch [62/120    avg_loss:0.248, val_acc:0.958]
Epoch [63/120    avg_loss:0.179, val_acc:0.964]
Epoch [64/120    avg_loss:0.166, val_acc:0.974]
Epoch [65/120    avg_loss:0.135, val_acc:0.966]
Epoch [66/120    avg_loss:0.146, val_acc:0.966]
Epoch [67/120    avg_loss:0.112, val_acc:0.978]
Epoch [68/120    avg_loss:0.128, val_acc:0.960]
Epoch [69/120    avg_loss:0.205, val_acc:0.933]
Epoch [70/120    avg_loss:0.238, val_acc:0.972]
Epoch [71/120    avg_loss:0.141, val_acc:0.960]
Epoch [72/120    avg_loss:0.134, val_acc:0.960]
Epoch [73/120    avg_loss:0.145, val_acc:0.954]
Epoch [74/120    avg_loss:0.144, val_acc:0.964]
Epoch [75/120    avg_loss:0.120, val_acc:0.976]
Epoch [76/120    avg_loss:0.128, val_acc:0.976]
Epoch [77/120    avg_loss:0.104, val_acc:0.964]
Epoch [78/120    avg_loss:0.111, val_acc:0.962]
Epoch [79/120    avg_loss:0.111, val_acc:0.978]
Epoch [80/120    avg_loss:0.112, val_acc:0.972]
Epoch [81/120    avg_loss:0.084, val_acc:0.976]
Epoch [82/120    avg_loss:0.084, val_acc:0.974]
Epoch [83/120    avg_loss:0.106, val_acc:0.966]
Epoch [84/120    avg_loss:0.126, val_acc:0.984]
Epoch [85/120    avg_loss:0.086, val_acc:0.984]
Epoch [86/120    avg_loss:0.075, val_acc:0.978]
Epoch [87/120    avg_loss:0.091, val_acc:0.974]
Epoch [88/120    avg_loss:0.084, val_acc:0.980]
Epoch [89/120    avg_loss:0.077, val_acc:0.974]
Epoch [90/120    avg_loss:0.086, val_acc:0.970]
Epoch [91/120    avg_loss:0.181, val_acc:0.968]
Epoch [92/120    avg_loss:0.116, val_acc:0.980]
Epoch [93/120    avg_loss:0.092, val_acc:0.976]
Epoch [94/120    avg_loss:0.085, val_acc:0.964]
Epoch [95/120    avg_loss:0.098, val_acc:0.982]
Epoch [96/120    avg_loss:0.114, val_acc:0.966]
Epoch [97/120    avg_loss:0.089, val_acc:0.986]
Epoch [98/120    avg_loss:0.069, val_acc:0.982]
Epoch [99/120    avg_loss:0.116, val_acc:0.974]
Epoch [100/120    avg_loss:0.053, val_acc:0.982]
Epoch [101/120    avg_loss:0.060, val_acc:0.988]
Epoch [102/120    avg_loss:0.119, val_acc:0.950]
Epoch [103/120    avg_loss:0.150, val_acc:0.974]
Epoch [104/120    avg_loss:0.083, val_acc:0.982]
Epoch [105/120    avg_loss:0.074, val_acc:0.976]
Epoch [106/120    avg_loss:0.044, val_acc:0.990]
Epoch [107/120    avg_loss:0.059, val_acc:0.988]
Epoch [108/120    avg_loss:0.053, val_acc:0.982]
Epoch [109/120    avg_loss:0.041, val_acc:0.988]
Epoch [110/120    avg_loss:0.092, val_acc:0.988]
Epoch [111/120    avg_loss:0.069, val_acc:0.986]
Epoch [112/120    avg_loss:0.047, val_acc:0.982]
Epoch [113/120    avg_loss:0.063, val_acc:0.984]
Epoch [114/120    avg_loss:0.056, val_acc:0.982]
Epoch [115/120    avg_loss:0.054, val_acc:0.990]
Epoch [116/120    avg_loss:0.048, val_acc:0.986]
Epoch [117/120    avg_loss:0.041, val_acc:0.986]
Epoch [118/120    avg_loss:0.037, val_acc:0.988]
Epoch [119/120    avg_loss:0.032, val_acc:0.972]
Epoch [120/120    avg_loss:0.036, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   1   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 227   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.98206278 0.99563319 0.94190871 0.90977444
 0.99266504 0.95555556 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9912154340263047
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b131b5e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.362]
Epoch [2/120    avg_loss:2.535, val_acc:0.464]
Epoch [3/120    avg_loss:2.464, val_acc:0.458]
Epoch [4/120    avg_loss:2.409, val_acc:0.462]
Epoch [5/120    avg_loss:2.337, val_acc:0.482]
Epoch [6/120    avg_loss:2.287, val_acc:0.536]
Epoch [7/120    avg_loss:2.216, val_acc:0.573]
Epoch [8/120    avg_loss:2.148, val_acc:0.603]
Epoch [9/120    avg_loss:2.054, val_acc:0.694]
Epoch [10/120    avg_loss:1.941, val_acc:0.690]
Epoch [11/120    avg_loss:1.860, val_acc:0.681]
Epoch [12/120    avg_loss:1.718, val_acc:0.706]
Epoch [13/120    avg_loss:1.594, val_acc:0.770]
Epoch [14/120    avg_loss:1.460, val_acc:0.788]
Epoch [15/120    avg_loss:1.327, val_acc:0.843]
Epoch [16/120    avg_loss:1.198, val_acc:0.772]
Epoch [17/120    avg_loss:1.056, val_acc:0.845]
Epoch [18/120    avg_loss:1.020, val_acc:0.857]
Epoch [19/120    avg_loss:0.923, val_acc:0.853]
Epoch [20/120    avg_loss:0.839, val_acc:0.887]
Epoch [21/120    avg_loss:0.725, val_acc:0.859]
Epoch [22/120    avg_loss:0.731, val_acc:0.895]
Epoch [23/120    avg_loss:0.615, val_acc:0.855]
Epoch [24/120    avg_loss:0.623, val_acc:0.859]
Epoch [25/120    avg_loss:0.565, val_acc:0.903]
Epoch [26/120    avg_loss:0.522, val_acc:0.897]
Epoch [27/120    avg_loss:0.524, val_acc:0.897]
Epoch [28/120    avg_loss:0.512, val_acc:0.871]
Epoch [29/120    avg_loss:0.434, val_acc:0.865]
Epoch [30/120    avg_loss:0.493, val_acc:0.907]
Epoch [31/120    avg_loss:0.506, val_acc:0.917]
Epoch [32/120    avg_loss:0.433, val_acc:0.927]
Epoch [33/120    avg_loss:0.396, val_acc:0.881]
Epoch [34/120    avg_loss:0.457, val_acc:0.899]
Epoch [35/120    avg_loss:0.380, val_acc:0.891]
Epoch [36/120    avg_loss:0.360, val_acc:0.923]
Epoch [37/120    avg_loss:0.334, val_acc:0.913]
Epoch [38/120    avg_loss:0.308, val_acc:0.919]
Epoch [39/120    avg_loss:0.430, val_acc:0.909]
Epoch [40/120    avg_loss:0.341, val_acc:0.921]
Epoch [41/120    avg_loss:0.313, val_acc:0.933]
Epoch [42/120    avg_loss:0.309, val_acc:0.956]
Epoch [43/120    avg_loss:0.285, val_acc:0.923]
Epoch [44/120    avg_loss:0.298, val_acc:0.913]
Epoch [45/120    avg_loss:0.291, val_acc:0.962]
Epoch [46/120    avg_loss:0.228, val_acc:0.956]
Epoch [47/120    avg_loss:0.306, val_acc:0.946]
Epoch [48/120    avg_loss:0.227, val_acc:0.938]
Epoch [49/120    avg_loss:0.252, val_acc:0.954]
Epoch [50/120    avg_loss:0.216, val_acc:0.962]
Epoch [51/120    avg_loss:0.242, val_acc:0.915]
Epoch [52/120    avg_loss:0.227, val_acc:0.970]
Epoch [53/120    avg_loss:0.207, val_acc:0.956]
Epoch [54/120    avg_loss:0.178, val_acc:0.970]
Epoch [55/120    avg_loss:0.198, val_acc:0.962]
Epoch [56/120    avg_loss:0.228, val_acc:0.950]
Epoch [57/120    avg_loss:0.273, val_acc:0.919]
Epoch [58/120    avg_loss:0.205, val_acc:0.946]
Epoch [59/120    avg_loss:0.147, val_acc:0.962]
Epoch [60/120    avg_loss:0.148, val_acc:0.968]
Epoch [61/120    avg_loss:0.165, val_acc:0.897]
Epoch [62/120    avg_loss:0.140, val_acc:0.966]
Epoch [63/120    avg_loss:0.262, val_acc:0.960]
Epoch [64/120    avg_loss:0.168, val_acc:0.976]
Epoch [65/120    avg_loss:0.136, val_acc:0.954]
Epoch [66/120    avg_loss:0.145, val_acc:0.976]
Epoch [67/120    avg_loss:0.118, val_acc:0.940]
Epoch [68/120    avg_loss:0.093, val_acc:0.976]
Epoch [69/120    avg_loss:0.148, val_acc:0.972]
Epoch [70/120    avg_loss:0.093, val_acc:0.976]
Epoch [71/120    avg_loss:0.104, val_acc:0.968]
Epoch [72/120    avg_loss:0.128, val_acc:0.970]
Epoch [73/120    avg_loss:0.116, val_acc:0.966]
Epoch [74/120    avg_loss:0.115, val_acc:0.978]
Epoch [75/120    avg_loss:0.090, val_acc:0.970]
Epoch [76/120    avg_loss:0.159, val_acc:0.970]
Epoch [77/120    avg_loss:0.132, val_acc:0.974]
Epoch [78/120    avg_loss:0.093, val_acc:0.980]
Epoch [79/120    avg_loss:0.073, val_acc:0.982]
Epoch [80/120    avg_loss:0.064, val_acc:0.986]
Epoch [81/120    avg_loss:0.081, val_acc:0.984]
Epoch [82/120    avg_loss:0.066, val_acc:0.980]
Epoch [83/120    avg_loss:0.105, val_acc:0.974]
Epoch [84/120    avg_loss:0.088, val_acc:0.986]
Epoch [85/120    avg_loss:0.071, val_acc:0.972]
Epoch [86/120    avg_loss:0.105, val_acc:0.980]
Epoch [87/120    avg_loss:0.099, val_acc:0.940]
Epoch [88/120    avg_loss:0.184, val_acc:0.980]
Epoch [89/120    avg_loss:0.073, val_acc:0.968]
Epoch [90/120    avg_loss:0.077, val_acc:0.980]
Epoch [91/120    avg_loss:0.084, val_acc:0.974]
Epoch [92/120    avg_loss:0.103, val_acc:0.984]
Epoch [93/120    avg_loss:0.067, val_acc:0.982]
Epoch [94/120    avg_loss:0.053, val_acc:0.986]
Epoch [95/120    avg_loss:0.036, val_acc:0.990]
Epoch [96/120    avg_loss:0.032, val_acc:0.988]
Epoch [97/120    avg_loss:0.045, val_acc:0.990]
Epoch [98/120    avg_loss:0.047, val_acc:0.986]
Epoch [99/120    avg_loss:0.044, val_acc:0.986]
Epoch [100/120    avg_loss:0.039, val_acc:0.988]
Epoch [101/120    avg_loss:0.033, val_acc:0.990]
Epoch [102/120    avg_loss:0.065, val_acc:0.982]
Epoch [103/120    avg_loss:0.071, val_acc:0.982]
Epoch [104/120    avg_loss:0.042, val_acc:0.988]
Epoch [105/120    avg_loss:0.048, val_acc:0.984]
Epoch [106/120    avg_loss:0.055, val_acc:0.990]
Epoch [107/120    avg_loss:0.037, val_acc:0.988]
Epoch [108/120    avg_loss:0.047, val_acc:0.990]
Epoch [109/120    avg_loss:0.071, val_acc:0.992]
Epoch [110/120    avg_loss:0.030, val_acc:0.986]
Epoch [111/120    avg_loss:0.037, val_acc:0.996]
Epoch [112/120    avg_loss:0.029, val_acc:0.990]
Epoch [113/120    avg_loss:0.050, val_acc:0.992]
Epoch [114/120    avg_loss:0.050, val_acc:0.990]
Epoch [115/120    avg_loss:0.031, val_acc:0.986]
Epoch [116/120    avg_loss:0.035, val_acc:0.988]
Epoch [117/120    avg_loss:0.050, val_acc:0.956]
Epoch [118/120    avg_loss:0.038, val_acc:0.994]
Epoch [119/120    avg_loss:0.026, val_acc:0.986]
Epoch [120/120    avg_loss:0.021, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 213  16   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  31 114   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 451   2]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 1.         0.97986577 0.96162528 0.88843813 0.85393258
 1.         0.94972067 1.         0.99893276 1.         1.
 0.99778761 0.9988024 ]

Kappa:
0.9840922516766467
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd099b08e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.622, val_acc:0.220]
Epoch [2/120    avg_loss:2.543, val_acc:0.488]
Epoch [3/120    avg_loss:2.461, val_acc:0.556]
Epoch [4/120    avg_loss:2.394, val_acc:0.552]
Epoch [5/120    avg_loss:2.324, val_acc:0.536]
Epoch [6/120    avg_loss:2.253, val_acc:0.538]
Epoch [7/120    avg_loss:2.182, val_acc:0.544]
Epoch [8/120    avg_loss:2.087, val_acc:0.526]
Epoch [9/120    avg_loss:2.006, val_acc:0.554]
Epoch [10/120    avg_loss:1.900, val_acc:0.609]
Epoch [11/120    avg_loss:1.794, val_acc:0.607]
Epoch [12/120    avg_loss:1.698, val_acc:0.623]
Epoch [13/120    avg_loss:1.588, val_acc:0.673]
Epoch [14/120    avg_loss:1.502, val_acc:0.667]
Epoch [15/120    avg_loss:1.368, val_acc:0.764]
Epoch [16/120    avg_loss:1.281, val_acc:0.782]
Epoch [17/120    avg_loss:1.165, val_acc:0.855]
Epoch [18/120    avg_loss:1.085, val_acc:0.802]
Epoch [19/120    avg_loss:0.988, val_acc:0.841]
Epoch [20/120    avg_loss:0.905, val_acc:0.827]
Epoch [21/120    avg_loss:0.863, val_acc:0.897]
Epoch [22/120    avg_loss:0.793, val_acc:0.881]
Epoch [23/120    avg_loss:0.732, val_acc:0.901]
Epoch [24/120    avg_loss:0.630, val_acc:0.871]
Epoch [25/120    avg_loss:0.579, val_acc:0.831]
Epoch [26/120    avg_loss:0.622, val_acc:0.861]
Epoch [27/120    avg_loss:0.617, val_acc:0.867]
Epoch [28/120    avg_loss:0.538, val_acc:0.891]
Epoch [29/120    avg_loss:0.493, val_acc:0.897]
Epoch [30/120    avg_loss:0.519, val_acc:0.905]
Epoch [31/120    avg_loss:0.418, val_acc:0.887]
Epoch [32/120    avg_loss:0.399, val_acc:0.901]
Epoch [33/120    avg_loss:0.398, val_acc:0.911]
Epoch [34/120    avg_loss:0.475, val_acc:0.919]
Epoch [35/120    avg_loss:0.430, val_acc:0.913]
Epoch [36/120    avg_loss:0.360, val_acc:0.905]
Epoch [37/120    avg_loss:0.351, val_acc:0.893]
Epoch [38/120    avg_loss:0.317, val_acc:0.931]
Epoch [39/120    avg_loss:0.288, val_acc:0.901]
Epoch [40/120    avg_loss:0.339, val_acc:0.927]
Epoch [41/120    avg_loss:0.309, val_acc:0.911]
Epoch [42/120    avg_loss:0.306, val_acc:0.948]
Epoch [43/120    avg_loss:0.244, val_acc:0.927]
Epoch [44/120    avg_loss:0.321, val_acc:0.915]
Epoch [45/120    avg_loss:0.262, val_acc:0.938]
Epoch [46/120    avg_loss:0.281, val_acc:0.931]
Epoch [47/120    avg_loss:0.342, val_acc:0.950]
Epoch [48/120    avg_loss:0.272, val_acc:0.925]
Epoch [49/120    avg_loss:0.222, val_acc:0.946]
Epoch [50/120    avg_loss:0.205, val_acc:0.929]
Epoch [51/120    avg_loss:0.215, val_acc:0.919]
Epoch [52/120    avg_loss:0.226, val_acc:0.931]
Epoch [53/120    avg_loss:0.217, val_acc:0.925]
Epoch [54/120    avg_loss:0.194, val_acc:0.938]
Epoch [55/120    avg_loss:0.210, val_acc:0.960]
Epoch [56/120    avg_loss:0.195, val_acc:0.956]
Epoch [57/120    avg_loss:0.161, val_acc:0.935]
Epoch [58/120    avg_loss:0.136, val_acc:0.956]
Epoch [59/120    avg_loss:0.198, val_acc:0.948]
Epoch [60/120    avg_loss:0.139, val_acc:0.956]
Epoch [61/120    avg_loss:0.157, val_acc:0.952]
Epoch [62/120    avg_loss:0.178, val_acc:0.954]
Epoch [63/120    avg_loss:0.152, val_acc:0.946]
Epoch [64/120    avg_loss:0.208, val_acc:0.938]
Epoch [65/120    avg_loss:0.187, val_acc:0.964]
Epoch [66/120    avg_loss:0.112, val_acc:0.960]
Epoch [67/120    avg_loss:0.115, val_acc:0.966]
Epoch [68/120    avg_loss:0.141, val_acc:0.972]
Epoch [69/120    avg_loss:0.279, val_acc:0.907]
Epoch [70/120    avg_loss:0.244, val_acc:0.938]
Epoch [71/120    avg_loss:0.183, val_acc:0.956]
Epoch [72/120    avg_loss:0.146, val_acc:0.952]
Epoch [73/120    avg_loss:0.139, val_acc:0.966]
Epoch [74/120    avg_loss:0.145, val_acc:0.958]
Epoch [75/120    avg_loss:0.131, val_acc:0.962]
Epoch [76/120    avg_loss:0.144, val_acc:0.956]
Epoch [77/120    avg_loss:0.174, val_acc:0.972]
Epoch [78/120    avg_loss:0.081, val_acc:0.966]
Epoch [79/120    avg_loss:0.110, val_acc:0.966]
Epoch [80/120    avg_loss:0.130, val_acc:0.958]
Epoch [81/120    avg_loss:0.108, val_acc:0.970]
Epoch [82/120    avg_loss:0.089, val_acc:0.972]
Epoch [83/120    avg_loss:0.092, val_acc:0.980]
Epoch [84/120    avg_loss:0.079, val_acc:0.980]
Epoch [85/120    avg_loss:0.072, val_acc:0.976]
Epoch [86/120    avg_loss:0.069, val_acc:0.982]
Epoch [87/120    avg_loss:0.064, val_acc:0.976]
Epoch [88/120    avg_loss:0.071, val_acc:0.976]
Epoch [89/120    avg_loss:0.065, val_acc:0.972]
Epoch [90/120    avg_loss:0.045, val_acc:0.974]
Epoch [91/120    avg_loss:0.075, val_acc:0.970]
Epoch [92/120    avg_loss:0.061, val_acc:0.976]
Epoch [93/120    avg_loss:0.048, val_acc:0.972]
Epoch [94/120    avg_loss:0.052, val_acc:0.980]
Epoch [95/120    avg_loss:0.047, val_acc:0.972]
Epoch [96/120    avg_loss:0.109, val_acc:0.970]
Epoch [97/120    avg_loss:0.061, val_acc:0.974]
Epoch [98/120    avg_loss:0.054, val_acc:0.968]
Epoch [99/120    avg_loss:0.058, val_acc:0.976]
Epoch [100/120    avg_loss:0.052, val_acc:0.982]
Epoch [101/120    avg_loss:0.026, val_acc:0.978]
Epoch [102/120    avg_loss:0.038, val_acc:0.982]
Epoch [103/120    avg_loss:0.037, val_acc:0.980]
Epoch [104/120    avg_loss:0.027, val_acc:0.978]
Epoch [105/120    avg_loss:0.034, val_acc:0.982]
Epoch [106/120    avg_loss:0.027, val_acc:0.982]
Epoch [107/120    avg_loss:0.025, val_acc:0.984]
Epoch [108/120    avg_loss:0.037, val_acc:0.982]
Epoch [109/120    avg_loss:0.031, val_acc:0.982]
Epoch [110/120    avg_loss:0.025, val_acc:0.978]
Epoch [111/120    avg_loss:0.023, val_acc:0.982]
Epoch [112/120    avg_loss:0.026, val_acc:0.982]
Epoch [113/120    avg_loss:0.022, val_acc:0.980]
Epoch [114/120    avg_loss:0.022, val_acc:0.980]
Epoch [115/120    avg_loss:0.024, val_acc:0.978]
Epoch [116/120    avg_loss:0.022, val_acc:0.980]
Epoch [117/120    avg_loss:0.018, val_acc:0.980]
Epoch [118/120    avg_loss:0.031, val_acc:0.978]
Epoch [119/120    avg_loss:0.026, val_acc:0.980]
Epoch [120/120    avg_loss:0.021, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 223   2   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.98426966 0.98454746 0.93418259 0.89454545
 1.         0.9726776  1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9902661084667038
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc691d2e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.394]
Epoch [2/120    avg_loss:2.567, val_acc:0.339]
Epoch [3/120    avg_loss:2.512, val_acc:0.365]
Epoch [4/120    avg_loss:2.441, val_acc:0.395]
Epoch [5/120    avg_loss:2.381, val_acc:0.403]
Epoch [6/120    avg_loss:2.323, val_acc:0.427]
Epoch [7/120    avg_loss:2.260, val_acc:0.468]
Epoch [8/120    avg_loss:2.186, val_acc:0.540]
Epoch [9/120    avg_loss:2.089, val_acc:0.581]
Epoch [10/120    avg_loss:1.983, val_acc:0.605]
Epoch [11/120    avg_loss:1.903, val_acc:0.619]
Epoch [12/120    avg_loss:1.773, val_acc:0.615]
Epoch [13/120    avg_loss:1.667, val_acc:0.655]
Epoch [14/120    avg_loss:1.548, val_acc:0.683]
Epoch [15/120    avg_loss:1.431, val_acc:0.679]
Epoch [16/120    avg_loss:1.312, val_acc:0.738]
Epoch [17/120    avg_loss:1.185, val_acc:0.812]
Epoch [18/120    avg_loss:1.091, val_acc:0.794]
Epoch [19/120    avg_loss:0.972, val_acc:0.812]
Epoch [20/120    avg_loss:0.879, val_acc:0.887]
Epoch [21/120    avg_loss:0.810, val_acc:0.869]
Epoch [22/120    avg_loss:0.733, val_acc:0.857]
Epoch [23/120    avg_loss:0.707, val_acc:0.897]
Epoch [24/120    avg_loss:0.602, val_acc:0.891]
Epoch [25/120    avg_loss:0.574, val_acc:0.917]
Epoch [26/120    avg_loss:0.530, val_acc:0.921]
Epoch [27/120    avg_loss:0.509, val_acc:0.927]
Epoch [28/120    avg_loss:0.446, val_acc:0.889]
Epoch [29/120    avg_loss:0.561, val_acc:0.913]
Epoch [30/120    avg_loss:0.451, val_acc:0.881]
Epoch [31/120    avg_loss:0.427, val_acc:0.911]
Epoch [32/120    avg_loss:0.393, val_acc:0.903]
Epoch [33/120    avg_loss:0.435, val_acc:0.909]
Epoch [34/120    avg_loss:0.410, val_acc:0.915]
Epoch [35/120    avg_loss:0.364, val_acc:0.923]
Epoch [36/120    avg_loss:0.349, val_acc:0.940]
Epoch [37/120    avg_loss:0.311, val_acc:0.919]
Epoch [38/120    avg_loss:0.311, val_acc:0.905]
Epoch [39/120    avg_loss:0.315, val_acc:0.883]
Epoch [40/120    avg_loss:0.347, val_acc:0.940]
Epoch [41/120    avg_loss:0.320, val_acc:0.907]
Epoch [42/120    avg_loss:0.348, val_acc:0.929]
Epoch [43/120    avg_loss:0.340, val_acc:0.956]
Epoch [44/120    avg_loss:0.265, val_acc:0.946]
Epoch [45/120    avg_loss:0.267, val_acc:0.942]
Epoch [46/120    avg_loss:0.259, val_acc:0.962]
Epoch [47/120    avg_loss:0.257, val_acc:0.952]
Epoch [48/120    avg_loss:0.232, val_acc:0.942]
Epoch [49/120    avg_loss:0.241, val_acc:0.964]
Epoch [50/120    avg_loss:0.217, val_acc:0.968]
Epoch [51/120    avg_loss:0.196, val_acc:0.972]
Epoch [52/120    avg_loss:0.182, val_acc:0.954]
Epoch [53/120    avg_loss:0.179, val_acc:0.946]
Epoch [54/120    avg_loss:0.207, val_acc:0.940]
Epoch [55/120    avg_loss:0.176, val_acc:0.948]
Epoch [56/120    avg_loss:0.201, val_acc:0.966]
Epoch [57/120    avg_loss:0.148, val_acc:0.962]
Epoch [58/120    avg_loss:0.153, val_acc:0.976]
Epoch [59/120    avg_loss:0.194, val_acc:0.974]
Epoch [60/120    avg_loss:0.197, val_acc:0.964]
Epoch [61/120    avg_loss:0.175, val_acc:0.966]
Epoch [62/120    avg_loss:0.135, val_acc:0.982]
Epoch [63/120    avg_loss:0.174, val_acc:0.958]
Epoch [64/120    avg_loss:0.163, val_acc:0.970]
Epoch [65/120    avg_loss:0.107, val_acc:0.966]
Epoch [66/120    avg_loss:0.130, val_acc:0.970]
Epoch [67/120    avg_loss:0.190, val_acc:0.970]
Epoch [68/120    avg_loss:0.128, val_acc:0.966]
Epoch [69/120    avg_loss:0.144, val_acc:0.988]
Epoch [70/120    avg_loss:0.111, val_acc:0.974]
Epoch [71/120    avg_loss:0.124, val_acc:0.972]
Epoch [72/120    avg_loss:0.103, val_acc:0.984]
Epoch [73/120    avg_loss:0.088, val_acc:0.954]
Epoch [74/120    avg_loss:0.111, val_acc:0.966]
Epoch [75/120    avg_loss:0.119, val_acc:0.974]
Epoch [76/120    avg_loss:0.095, val_acc:0.976]
Epoch [77/120    avg_loss:0.081, val_acc:0.982]
Epoch [78/120    avg_loss:0.107, val_acc:0.990]
Epoch [79/120    avg_loss:0.084, val_acc:0.976]
Epoch [80/120    avg_loss:0.066, val_acc:0.990]
Epoch [81/120    avg_loss:0.073, val_acc:0.982]
Epoch [82/120    avg_loss:0.085, val_acc:0.966]
Epoch [83/120    avg_loss:0.078, val_acc:0.974]
Epoch [84/120    avg_loss:0.072, val_acc:0.990]
Epoch [85/120    avg_loss:0.138, val_acc:0.952]
Epoch [86/120    avg_loss:0.175, val_acc:0.984]
Epoch [87/120    avg_loss:0.155, val_acc:0.978]
Epoch [88/120    avg_loss:0.084, val_acc:0.976]
Epoch [89/120    avg_loss:0.102, val_acc:0.970]
Epoch [90/120    avg_loss:0.188, val_acc:0.948]
Epoch [91/120    avg_loss:0.124, val_acc:0.982]
Epoch [92/120    avg_loss:0.072, val_acc:0.976]
Epoch [93/120    avg_loss:0.079, val_acc:0.982]
Epoch [94/120    avg_loss:0.090, val_acc:0.958]
Epoch [95/120    avg_loss:0.079, val_acc:0.986]
Epoch [96/120    avg_loss:0.060, val_acc:0.986]
Epoch [97/120    avg_loss:0.057, val_acc:0.984]
Epoch [98/120    avg_loss:0.042, val_acc:0.986]
Epoch [99/120    avg_loss:0.036, val_acc:0.988]
Epoch [100/120    avg_loss:0.043, val_acc:0.990]
Epoch [101/120    avg_loss:0.034, val_acc:0.988]
Epoch [102/120    avg_loss:0.042, val_acc:0.990]
Epoch [103/120    avg_loss:0.034, val_acc:0.990]
Epoch [104/120    avg_loss:0.029, val_acc:0.990]
Epoch [105/120    avg_loss:0.038, val_acc:0.994]
Epoch [106/120    avg_loss:0.035, val_acc:0.992]
Epoch [107/120    avg_loss:0.035, val_acc:0.994]
Epoch [108/120    avg_loss:0.031, val_acc:0.994]
Epoch [109/120    avg_loss:0.028, val_acc:0.992]
Epoch [110/120    avg_loss:0.040, val_acc:0.994]
Epoch [111/120    avg_loss:0.054, val_acc:0.990]
Epoch [112/120    avg_loss:0.026, val_acc:0.992]
Epoch [113/120    avg_loss:0.031, val_acc:0.990]
Epoch [114/120    avg_loss:0.029, val_acc:0.994]
Epoch [115/120    avg_loss:0.041, val_acc:0.994]
Epoch [116/120    avg_loss:0.034, val_acc:0.994]
Epoch [117/120    avg_loss:0.037, val_acc:0.994]
Epoch [118/120    avg_loss:0.033, val_acc:0.994]
Epoch [119/120    avg_loss:0.033, val_acc:0.994]
Epoch [120/120    avg_loss:0.032, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 223   1   0   0   0   3   3   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.96674058 0.98454746 0.94168467 0.90780142
 1.         0.91428571 0.99614891 0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9886036909218923
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f70ed2f0dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.078]
Epoch [2/120    avg_loss:2.597, val_acc:0.290]
Epoch [3/120    avg_loss:2.530, val_acc:0.302]
Epoch [4/120    avg_loss:2.464, val_acc:0.343]
Epoch [5/120    avg_loss:2.406, val_acc:0.365]
Epoch [6/120    avg_loss:2.345, val_acc:0.472]
Epoch [7/120    avg_loss:2.287, val_acc:0.510]
Epoch [8/120    avg_loss:2.206, val_acc:0.498]
Epoch [9/120    avg_loss:2.108, val_acc:0.562]
Epoch [10/120    avg_loss:2.008, val_acc:0.649]
Epoch [11/120    avg_loss:1.881, val_acc:0.685]
Epoch [12/120    avg_loss:1.755, val_acc:0.716]
Epoch [13/120    avg_loss:1.646, val_acc:0.720]
Epoch [14/120    avg_loss:1.505, val_acc:0.744]
Epoch [15/120    avg_loss:1.381, val_acc:0.736]
Epoch [16/120    avg_loss:1.245, val_acc:0.760]
Epoch [17/120    avg_loss:1.157, val_acc:0.744]
Epoch [18/120    avg_loss:1.040, val_acc:0.760]
Epoch [19/120    avg_loss:0.950, val_acc:0.770]
Epoch [20/120    avg_loss:0.881, val_acc:0.821]
Epoch [21/120    avg_loss:0.753, val_acc:0.849]
Epoch [22/120    avg_loss:0.759, val_acc:0.873]
Epoch [23/120    avg_loss:0.684, val_acc:0.792]
Epoch [24/120    avg_loss:0.658, val_acc:0.865]
Epoch [25/120    avg_loss:0.615, val_acc:0.915]
Epoch [26/120    avg_loss:0.569, val_acc:0.835]
Epoch [27/120    avg_loss:0.569, val_acc:0.911]
Epoch [28/120    avg_loss:0.547, val_acc:0.905]
Epoch [29/120    avg_loss:0.542, val_acc:0.893]
Epoch [30/120    avg_loss:0.462, val_acc:0.915]
Epoch [31/120    avg_loss:0.429, val_acc:0.921]
Epoch [32/120    avg_loss:0.402, val_acc:0.940]
Epoch [33/120    avg_loss:0.440, val_acc:0.887]
Epoch [34/120    avg_loss:0.505, val_acc:0.925]
Epoch [35/120    avg_loss:0.433, val_acc:0.849]
Epoch [36/120    avg_loss:0.460, val_acc:0.901]
Epoch [37/120    avg_loss:0.393, val_acc:0.935]
Epoch [38/120    avg_loss:0.349, val_acc:0.933]
Epoch [39/120    avg_loss:0.395, val_acc:0.954]
Epoch [40/120    avg_loss:0.390, val_acc:0.946]
Epoch [41/120    avg_loss:0.363, val_acc:0.927]
Epoch [42/120    avg_loss:0.337, val_acc:0.946]
Epoch [43/120    avg_loss:0.334, val_acc:0.938]
Epoch [44/120    avg_loss:0.315, val_acc:0.946]
Epoch [45/120    avg_loss:0.330, val_acc:0.952]
Epoch [46/120    avg_loss:0.236, val_acc:0.964]
Epoch [47/120    avg_loss:0.219, val_acc:0.948]
Epoch [48/120    avg_loss:0.258, val_acc:0.944]
Epoch [49/120    avg_loss:0.252, val_acc:0.948]
Epoch [50/120    avg_loss:0.216, val_acc:0.958]
Epoch [51/120    avg_loss:0.219, val_acc:0.958]
Epoch [52/120    avg_loss:0.206, val_acc:0.970]
Epoch [53/120    avg_loss:0.178, val_acc:0.942]
Epoch [54/120    avg_loss:0.189, val_acc:0.966]
Epoch [55/120    avg_loss:0.220, val_acc:0.915]
Epoch [56/120    avg_loss:0.197, val_acc:0.958]
Epoch [57/120    avg_loss:0.192, val_acc:0.968]
Epoch [58/120    avg_loss:0.180, val_acc:0.952]
Epoch [59/120    avg_loss:0.177, val_acc:0.948]
Epoch [60/120    avg_loss:0.168, val_acc:0.962]
Epoch [61/120    avg_loss:0.180, val_acc:0.944]
Epoch [62/120    avg_loss:0.190, val_acc:0.942]
Epoch [63/120    avg_loss:0.211, val_acc:0.952]
Epoch [64/120    avg_loss:0.186, val_acc:0.968]
Epoch [65/120    avg_loss:0.157, val_acc:0.962]
Epoch [66/120    avg_loss:0.120, val_acc:0.974]
Epoch [67/120    avg_loss:0.095, val_acc:0.978]
Epoch [68/120    avg_loss:0.084, val_acc:0.980]
Epoch [69/120    avg_loss:0.093, val_acc:0.982]
Epoch [70/120    avg_loss:0.108, val_acc:0.982]
Epoch [71/120    avg_loss:0.097, val_acc:0.982]
Epoch [72/120    avg_loss:0.102, val_acc:0.976]
Epoch [73/120    avg_loss:0.100, val_acc:0.982]
Epoch [74/120    avg_loss:0.087, val_acc:0.980]
Epoch [75/120    avg_loss:0.079, val_acc:0.980]
Epoch [76/120    avg_loss:0.092, val_acc:0.982]
Epoch [77/120    avg_loss:0.076, val_acc:0.976]
Epoch [78/120    avg_loss:0.075, val_acc:0.980]
Epoch [79/120    avg_loss:0.085, val_acc:0.980]
Epoch [80/120    avg_loss:0.076, val_acc:0.982]
Epoch [81/120    avg_loss:0.085, val_acc:0.980]
Epoch [82/120    avg_loss:0.082, val_acc:0.980]
Epoch [83/120    avg_loss:0.081, val_acc:0.980]
Epoch [84/120    avg_loss:0.066, val_acc:0.982]
Epoch [85/120    avg_loss:0.068, val_acc:0.982]
Epoch [86/120    avg_loss:0.079, val_acc:0.982]
Epoch [87/120    avg_loss:0.077, val_acc:0.980]
Epoch [88/120    avg_loss:0.076, val_acc:0.980]
Epoch [89/120    avg_loss:0.073, val_acc:0.980]
Epoch [90/120    avg_loss:0.067, val_acc:0.980]
Epoch [91/120    avg_loss:0.086, val_acc:0.982]
Epoch [92/120    avg_loss:0.075, val_acc:0.982]
Epoch [93/120    avg_loss:0.069, val_acc:0.982]
Epoch [94/120    avg_loss:0.073, val_acc:0.982]
Epoch [95/120    avg_loss:0.067, val_acc:0.978]
Epoch [96/120    avg_loss:0.067, val_acc:0.982]
Epoch [97/120    avg_loss:0.069, val_acc:0.982]
Epoch [98/120    avg_loss:0.070, val_acc:0.982]
Epoch [99/120    avg_loss:0.065, val_acc:0.982]
Epoch [100/120    avg_loss:0.073, val_acc:0.980]
Epoch [101/120    avg_loss:0.062, val_acc:0.980]
Epoch [102/120    avg_loss:0.056, val_acc:0.980]
Epoch [103/120    avg_loss:0.058, val_acc:0.984]
Epoch [104/120    avg_loss:0.062, val_acc:0.980]
Epoch [105/120    avg_loss:0.075, val_acc:0.982]
Epoch [106/120    avg_loss:0.066, val_acc:0.982]
Epoch [107/120    avg_loss:0.057, val_acc:0.980]
Epoch [108/120    avg_loss:0.060, val_acc:0.980]
Epoch [109/120    avg_loss:0.059, val_acc:0.984]
Epoch [110/120    avg_loss:0.066, val_acc:0.984]
Epoch [111/120    avg_loss:0.064, val_acc:0.984]
Epoch [112/120    avg_loss:0.066, val_acc:0.978]
Epoch [113/120    avg_loss:0.055, val_acc:0.984]
Epoch [114/120    avg_loss:0.060, val_acc:0.984]
Epoch [115/120    avg_loss:0.060, val_acc:0.984]
Epoch [116/120    avg_loss:0.060, val_acc:0.984]
Epoch [117/120    avg_loss:0.052, val_acc:0.982]
Epoch [118/120    avg_loss:0.049, val_acc:0.982]
Epoch [119/120    avg_loss:0.056, val_acc:0.982]
Epoch [120/120    avg_loss:0.051, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 224   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.96688742 0.98678414 0.92505353 0.89361702
 1.         0.91954023 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9881294973133179
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f551b306e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.113]
Epoch [2/120    avg_loss:2.554, val_acc:0.302]
Epoch [3/120    avg_loss:2.489, val_acc:0.331]
Epoch [4/120    avg_loss:2.428, val_acc:0.462]
Epoch [5/120    avg_loss:2.357, val_acc:0.514]
Epoch [6/120    avg_loss:2.291, val_acc:0.538]
Epoch [7/120    avg_loss:2.231, val_acc:0.560]
Epoch [8/120    avg_loss:2.135, val_acc:0.560]
Epoch [9/120    avg_loss:2.029, val_acc:0.569]
Epoch [10/120    avg_loss:1.934, val_acc:0.583]
Epoch [11/120    avg_loss:1.825, val_acc:0.599]
Epoch [12/120    avg_loss:1.697, val_acc:0.667]
Epoch [13/120    avg_loss:1.575, val_acc:0.718]
Epoch [14/120    avg_loss:1.433, val_acc:0.720]
Epoch [15/120    avg_loss:1.312, val_acc:0.790]
Epoch [16/120    avg_loss:1.178, val_acc:0.780]
Epoch [17/120    avg_loss:1.040, val_acc:0.873]
Epoch [18/120    avg_loss:0.988, val_acc:0.849]
Epoch [19/120    avg_loss:0.902, val_acc:0.897]
Epoch [20/120    avg_loss:0.872, val_acc:0.879]
Epoch [21/120    avg_loss:0.769, val_acc:0.897]
Epoch [22/120    avg_loss:0.696, val_acc:0.899]
Epoch [23/120    avg_loss:0.671, val_acc:0.901]
Epoch [24/120    avg_loss:0.630, val_acc:0.897]
Epoch [25/120    avg_loss:0.569, val_acc:0.879]
Epoch [26/120    avg_loss:0.600, val_acc:0.883]
Epoch [27/120    avg_loss:0.557, val_acc:0.909]
Epoch [28/120    avg_loss:0.562, val_acc:0.927]
Epoch [29/120    avg_loss:0.448, val_acc:0.897]
Epoch [30/120    avg_loss:0.477, val_acc:0.905]
Epoch [31/120    avg_loss:0.453, val_acc:0.913]
Epoch [32/120    avg_loss:0.409, val_acc:0.917]
Epoch [33/120    avg_loss:0.431, val_acc:0.919]
Epoch [34/120    avg_loss:0.448, val_acc:0.929]
Epoch [35/120    avg_loss:0.436, val_acc:0.927]
Epoch [36/120    avg_loss:0.396, val_acc:0.897]
Epoch [37/120    avg_loss:0.382, val_acc:0.905]
Epoch [38/120    avg_loss:0.363, val_acc:0.954]
Epoch [39/120    avg_loss:0.330, val_acc:0.946]
Epoch [40/120    avg_loss:0.314, val_acc:0.946]
Epoch [41/120    avg_loss:0.330, val_acc:0.938]
Epoch [42/120    avg_loss:0.311, val_acc:0.948]
Epoch [43/120    avg_loss:0.281, val_acc:0.925]
Epoch [44/120    avg_loss:0.305, val_acc:0.929]
Epoch [45/120    avg_loss:0.289, val_acc:0.940]
Epoch [46/120    avg_loss:0.263, val_acc:0.942]
Epoch [47/120    avg_loss:0.283, val_acc:0.944]
Epoch [48/120    avg_loss:0.229, val_acc:0.942]
Epoch [49/120    avg_loss:0.298, val_acc:0.958]
Epoch [50/120    avg_loss:0.254, val_acc:0.964]
Epoch [51/120    avg_loss:0.239, val_acc:0.958]
Epoch [52/120    avg_loss:0.214, val_acc:0.962]
Epoch [53/120    avg_loss:0.259, val_acc:0.954]
Epoch [54/120    avg_loss:0.241, val_acc:0.958]
Epoch [55/120    avg_loss:0.227, val_acc:0.946]
Epoch [56/120    avg_loss:0.177, val_acc:0.956]
Epoch [57/120    avg_loss:0.142, val_acc:0.966]
Epoch [58/120    avg_loss:0.210, val_acc:0.933]
Epoch [59/120    avg_loss:0.229, val_acc:0.968]
Epoch [60/120    avg_loss:0.186, val_acc:0.948]
Epoch [61/120    avg_loss:0.181, val_acc:0.956]
Epoch [62/120    avg_loss:0.154, val_acc:0.956]
Epoch [63/120    avg_loss:0.167, val_acc:0.958]
Epoch [64/120    avg_loss:0.122, val_acc:0.980]
Epoch [65/120    avg_loss:0.180, val_acc:0.972]
Epoch [66/120    avg_loss:0.163, val_acc:0.958]
Epoch [67/120    avg_loss:0.167, val_acc:0.968]
Epoch [68/120    avg_loss:0.118, val_acc:0.946]
Epoch [69/120    avg_loss:0.156, val_acc:0.974]
Epoch [70/120    avg_loss:0.124, val_acc:0.972]
Epoch [71/120    avg_loss:0.140, val_acc:0.976]
Epoch [72/120    avg_loss:0.135, val_acc:0.980]
Epoch [73/120    avg_loss:0.128, val_acc:0.972]
Epoch [74/120    avg_loss:0.098, val_acc:0.966]
Epoch [75/120    avg_loss:0.115, val_acc:0.982]
Epoch [76/120    avg_loss:0.089, val_acc:0.984]
Epoch [77/120    avg_loss:0.094, val_acc:0.976]
Epoch [78/120    avg_loss:0.104, val_acc:0.978]
Epoch [79/120    avg_loss:0.105, val_acc:0.962]
Epoch [80/120    avg_loss:0.110, val_acc:0.972]
Epoch [81/120    avg_loss:0.135, val_acc:0.962]
Epoch [82/120    avg_loss:0.101, val_acc:0.976]
Epoch [83/120    avg_loss:0.092, val_acc:0.976]
Epoch [84/120    avg_loss:0.141, val_acc:0.952]
Epoch [85/120    avg_loss:0.107, val_acc:0.954]
Epoch [86/120    avg_loss:0.199, val_acc:0.972]
Epoch [87/120    avg_loss:0.130, val_acc:0.976]
Epoch [88/120    avg_loss:0.088, val_acc:0.974]
Epoch [89/120    avg_loss:0.120, val_acc:0.968]
Epoch [90/120    avg_loss:0.094, val_acc:0.978]
Epoch [91/120    avg_loss:0.056, val_acc:0.978]
Epoch [92/120    avg_loss:0.067, val_acc:0.974]
Epoch [93/120    avg_loss:0.056, val_acc:0.976]
Epoch [94/120    avg_loss:0.066, val_acc:0.976]
Epoch [95/120    avg_loss:0.055, val_acc:0.976]
Epoch [96/120    avg_loss:0.052, val_acc:0.980]
Epoch [97/120    avg_loss:0.053, val_acc:0.980]
Epoch [98/120    avg_loss:0.060, val_acc:0.982]
Epoch [99/120    avg_loss:0.047, val_acc:0.980]
Epoch [100/120    avg_loss:0.046, val_acc:0.980]
Epoch [101/120    avg_loss:0.044, val_acc:0.982]
Epoch [102/120    avg_loss:0.040, val_acc:0.982]
Epoch [103/120    avg_loss:0.038, val_acc:0.982]
Epoch [104/120    avg_loss:0.046, val_acc:0.982]
Epoch [105/120    avg_loss:0.035, val_acc:0.982]
Epoch [106/120    avg_loss:0.039, val_acc:0.982]
Epoch [107/120    avg_loss:0.042, val_acc:0.982]
Epoch [108/120    avg_loss:0.046, val_acc:0.982]
Epoch [109/120    avg_loss:0.037, val_acc:0.982]
Epoch [110/120    avg_loss:0.045, val_acc:0.982]
Epoch [111/120    avg_loss:0.042, val_acc:0.982]
Epoch [112/120    avg_loss:0.044, val_acc:0.982]
Epoch [113/120    avg_loss:0.040, val_acc:0.982]
Epoch [114/120    avg_loss:0.037, val_acc:0.982]
Epoch [115/120    avg_loss:0.042, val_acc:0.982]
Epoch [116/120    avg_loss:0.045, val_acc:0.982]
Epoch [117/120    avg_loss:0.041, val_acc:0.982]
Epoch [118/120    avg_loss:0.043, val_acc:0.982]
Epoch [119/120    avg_loss:0.050, val_acc:0.982]
Epoch [120/120    avg_loss:0.039, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 217   6   0   0   0   4   3   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 1.         0.96888889 0.97091723 0.92517007 0.91262136
 1.         0.92045455 0.99487179 0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9871805774055019
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3ecd71e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.640, val_acc:0.065]
Epoch [2/120    avg_loss:2.578, val_acc:0.212]
Epoch [3/120    avg_loss:2.521, val_acc:0.375]
Epoch [4/120    avg_loss:2.463, val_acc:0.409]
Epoch [5/120    avg_loss:2.405, val_acc:0.403]
Epoch [6/120    avg_loss:2.347, val_acc:0.417]
Epoch [7/120    avg_loss:2.266, val_acc:0.446]
Epoch [8/120    avg_loss:2.177, val_acc:0.464]
Epoch [9/120    avg_loss:2.100, val_acc:0.518]
Epoch [10/120    avg_loss:1.996, val_acc:0.542]
Epoch [11/120    avg_loss:1.907, val_acc:0.560]
Epoch [12/120    avg_loss:1.807, val_acc:0.595]
Epoch [13/120    avg_loss:1.704, val_acc:0.599]
Epoch [14/120    avg_loss:1.593, val_acc:0.629]
Epoch [15/120    avg_loss:1.525, val_acc:0.639]
Epoch [16/120    avg_loss:1.398, val_acc:0.655]
Epoch [17/120    avg_loss:1.280, val_acc:0.677]
Epoch [18/120    avg_loss:1.195, val_acc:0.736]
Epoch [19/120    avg_loss:1.081, val_acc:0.804]
Epoch [20/120    avg_loss:0.959, val_acc:0.776]
Epoch [21/120    avg_loss:0.903, val_acc:0.863]
Epoch [22/120    avg_loss:0.865, val_acc:0.863]
Epoch [23/120    avg_loss:0.729, val_acc:0.883]
Epoch [24/120    avg_loss:0.681, val_acc:0.887]
Epoch [25/120    avg_loss:0.644, val_acc:0.899]
Epoch [26/120    avg_loss:0.646, val_acc:0.827]
Epoch [27/120    avg_loss:0.591, val_acc:0.889]
Epoch [28/120    avg_loss:0.534, val_acc:0.903]
Epoch [29/120    avg_loss:0.476, val_acc:0.917]
Epoch [30/120    avg_loss:0.523, val_acc:0.899]
Epoch [31/120    avg_loss:0.443, val_acc:0.925]
Epoch [32/120    avg_loss:0.404, val_acc:0.917]
Epoch [33/120    avg_loss:0.360, val_acc:0.929]
Epoch [34/120    avg_loss:0.351, val_acc:0.923]
Epoch [35/120    avg_loss:0.381, val_acc:0.905]
Epoch [36/120    avg_loss:0.326, val_acc:0.946]
Epoch [37/120    avg_loss:0.311, val_acc:0.933]
Epoch [38/120    avg_loss:0.358, val_acc:0.919]
Epoch [39/120    avg_loss:0.327, val_acc:0.938]
Epoch [40/120    avg_loss:0.303, val_acc:0.919]
Epoch [41/120    avg_loss:0.326, val_acc:0.901]
Epoch [42/120    avg_loss:0.345, val_acc:0.950]
Epoch [43/120    avg_loss:0.274, val_acc:0.931]
Epoch [44/120    avg_loss:0.210, val_acc:0.933]
Epoch [45/120    avg_loss:0.221, val_acc:0.956]
Epoch [46/120    avg_loss:0.231, val_acc:0.954]
Epoch [47/120    avg_loss:0.247, val_acc:0.935]
Epoch [48/120    avg_loss:0.216, val_acc:0.942]
Epoch [49/120    avg_loss:0.235, val_acc:0.940]
Epoch [50/120    avg_loss:0.185, val_acc:0.954]
Epoch [51/120    avg_loss:0.216, val_acc:0.960]
Epoch [52/120    avg_loss:0.175, val_acc:0.946]
Epoch [53/120    avg_loss:0.171, val_acc:0.952]
Epoch [54/120    avg_loss:0.182, val_acc:0.966]
Epoch [55/120    avg_loss:0.172, val_acc:0.952]
Epoch [56/120    avg_loss:0.196, val_acc:0.954]
Epoch [57/120    avg_loss:0.218, val_acc:0.952]
Epoch [58/120    avg_loss:0.171, val_acc:0.952]
Epoch [59/120    avg_loss:0.161, val_acc:0.944]
Epoch [60/120    avg_loss:0.145, val_acc:0.962]
Epoch [61/120    avg_loss:0.155, val_acc:0.968]
Epoch [62/120    avg_loss:0.183, val_acc:0.960]
Epoch [63/120    avg_loss:0.139, val_acc:0.974]
Epoch [64/120    avg_loss:0.103, val_acc:0.980]
Epoch [65/120    avg_loss:0.115, val_acc:0.982]
Epoch [66/120    avg_loss:0.143, val_acc:0.964]
Epoch [67/120    avg_loss:0.114, val_acc:0.978]
Epoch [68/120    avg_loss:0.117, val_acc:0.966]
Epoch [69/120    avg_loss:0.132, val_acc:0.968]
Epoch [70/120    avg_loss:0.104, val_acc:0.974]
Epoch [71/120    avg_loss:0.094, val_acc:0.976]
Epoch [72/120    avg_loss:0.147, val_acc:0.970]
Epoch [73/120    avg_loss:0.206, val_acc:0.958]
Epoch [74/120    avg_loss:0.140, val_acc:0.956]
Epoch [75/120    avg_loss:0.112, val_acc:0.976]
Epoch [76/120    avg_loss:0.078, val_acc:0.974]
Epoch [77/120    avg_loss:0.120, val_acc:0.976]
Epoch [78/120    avg_loss:0.164, val_acc:0.940]
Epoch [79/120    avg_loss:0.160, val_acc:0.960]
Epoch [80/120    avg_loss:0.097, val_acc:0.968]
Epoch [81/120    avg_loss:0.074, val_acc:0.972]
Epoch [82/120    avg_loss:0.081, val_acc:0.974]
Epoch [83/120    avg_loss:0.066, val_acc:0.980]
Epoch [84/120    avg_loss:0.075, val_acc:0.982]
Epoch [85/120    avg_loss:0.074, val_acc:0.984]
Epoch [86/120    avg_loss:0.069, val_acc:0.980]
Epoch [87/120    avg_loss:0.063, val_acc:0.980]
Epoch [88/120    avg_loss:0.053, val_acc:0.984]
Epoch [89/120    avg_loss:0.055, val_acc:0.980]
Epoch [90/120    avg_loss:0.062, val_acc:0.980]
Epoch [91/120    avg_loss:0.056, val_acc:0.982]
Epoch [92/120    avg_loss:0.061, val_acc:0.982]
Epoch [93/120    avg_loss:0.047, val_acc:0.982]
Epoch [94/120    avg_loss:0.061, val_acc:0.980]
Epoch [95/120    avg_loss:0.051, val_acc:0.982]
Epoch [96/120    avg_loss:0.052, val_acc:0.982]
Epoch [97/120    avg_loss:0.055, val_acc:0.982]
Epoch [98/120    avg_loss:0.052, val_acc:0.982]
Epoch [99/120    avg_loss:0.058, val_acc:0.982]
Epoch [100/120    avg_loss:0.053, val_acc:0.984]
Epoch [101/120    avg_loss:0.055, val_acc:0.982]
Epoch [102/120    avg_loss:0.047, val_acc:0.980]
Epoch [103/120    avg_loss:0.049, val_acc:0.982]
Epoch [104/120    avg_loss:0.050, val_acc:0.982]
Epoch [105/120    avg_loss:0.052, val_acc:0.982]
Epoch [106/120    avg_loss:0.056, val_acc:0.982]
Epoch [107/120    avg_loss:0.046, val_acc:0.984]
Epoch [108/120    avg_loss:0.056, val_acc:0.978]
Epoch [109/120    avg_loss:0.047, val_acc:0.980]
Epoch [110/120    avg_loss:0.046, val_acc:0.982]
Epoch [111/120    avg_loss:0.048, val_acc:0.982]
Epoch [112/120    avg_loss:0.050, val_acc:0.982]
Epoch [113/120    avg_loss:0.046, val_acc:0.982]
Epoch [114/120    avg_loss:0.052, val_acc:0.982]
Epoch [115/120    avg_loss:0.051, val_acc:0.982]
Epoch [116/120    avg_loss:0.048, val_acc:0.984]
Epoch [117/120    avg_loss:0.044, val_acc:0.984]
Epoch [118/120    avg_loss:0.045, val_acc:0.984]
Epoch [119/120    avg_loss:0.047, val_acc:0.980]
Epoch [120/120    avg_loss:0.044, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   1 224   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   3   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 1.         0.94382022 0.98678414 0.93043478 0.90657439
 1.         0.86813187 0.99614891 1.         1.         1.
 0.99667774 1.        ]

Kappa:
0.9857566722322276
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe37516edd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.149]
Epoch [2/120    avg_loss:2.588, val_acc:0.244]
Epoch [3/120    avg_loss:2.555, val_acc:0.391]
Epoch [4/120    avg_loss:2.524, val_acc:0.440]
Epoch [5/120    avg_loss:2.482, val_acc:0.482]
Epoch [6/120    avg_loss:2.442, val_acc:0.522]
Epoch [7/120    avg_loss:2.379, val_acc:0.514]
Epoch [8/120    avg_loss:2.317, val_acc:0.536]
Epoch [9/120    avg_loss:2.249, val_acc:0.571]
Epoch [10/120    avg_loss:2.149, val_acc:0.567]
Epoch [11/120    avg_loss:2.054, val_acc:0.591]
Epoch [12/120    avg_loss:1.952, val_acc:0.595]
Epoch [13/120    avg_loss:1.858, val_acc:0.601]
Epoch [14/120    avg_loss:1.737, val_acc:0.653]
Epoch [15/120    avg_loss:1.595, val_acc:0.659]
Epoch [16/120    avg_loss:1.493, val_acc:0.681]
Epoch [17/120    avg_loss:1.376, val_acc:0.742]
Epoch [18/120    avg_loss:1.258, val_acc:0.708]
Epoch [19/120    avg_loss:1.141, val_acc:0.837]
Epoch [20/120    avg_loss:1.066, val_acc:0.847]
Epoch [21/120    avg_loss:0.928, val_acc:0.845]
Epoch [22/120    avg_loss:0.878, val_acc:0.875]
Epoch [23/120    avg_loss:0.835, val_acc:0.841]
Epoch [24/120    avg_loss:0.773, val_acc:0.885]
Epoch [25/120    avg_loss:0.665, val_acc:0.871]
Epoch [26/120    avg_loss:0.594, val_acc:0.845]
Epoch [27/120    avg_loss:0.651, val_acc:0.863]
Epoch [28/120    avg_loss:0.556, val_acc:0.863]
Epoch [29/120    avg_loss:0.655, val_acc:0.849]
Epoch [30/120    avg_loss:0.545, val_acc:0.913]
Epoch [31/120    avg_loss:0.494, val_acc:0.903]
Epoch [32/120    avg_loss:0.478, val_acc:0.911]
Epoch [33/120    avg_loss:0.435, val_acc:0.865]
Epoch [34/120    avg_loss:0.450, val_acc:0.923]
Epoch [35/120    avg_loss:0.421, val_acc:0.877]
Epoch [36/120    avg_loss:0.429, val_acc:0.919]
Epoch [37/120    avg_loss:0.385, val_acc:0.929]
Epoch [38/120    avg_loss:0.314, val_acc:0.938]
Epoch [39/120    avg_loss:0.339, val_acc:0.933]
Epoch [40/120    avg_loss:0.309, val_acc:0.925]
Epoch [41/120    avg_loss:0.329, val_acc:0.948]
Epoch [42/120    avg_loss:0.281, val_acc:0.935]
Epoch [43/120    avg_loss:0.275, val_acc:0.958]
Epoch [44/120    avg_loss:0.287, val_acc:0.944]
Epoch [45/120    avg_loss:0.309, val_acc:0.889]
Epoch [46/120    avg_loss:0.302, val_acc:0.954]
Epoch [47/120    avg_loss:0.309, val_acc:0.929]
Epoch [48/120    avg_loss:0.278, val_acc:0.966]
Epoch [49/120    avg_loss:0.226, val_acc:0.964]
Epoch [50/120    avg_loss:0.214, val_acc:0.960]
Epoch [51/120    avg_loss:0.226, val_acc:0.954]
Epoch [52/120    avg_loss:0.285, val_acc:0.968]
Epoch [53/120    avg_loss:0.224, val_acc:0.968]
Epoch [54/120    avg_loss:0.265, val_acc:0.925]
Epoch [55/120    avg_loss:0.193, val_acc:0.956]
Epoch [56/120    avg_loss:0.234, val_acc:0.960]
Epoch [57/120    avg_loss:0.222, val_acc:0.962]
Epoch [58/120    avg_loss:0.183, val_acc:0.966]
Epoch [59/120    avg_loss:0.163, val_acc:0.950]
Epoch [60/120    avg_loss:0.196, val_acc:0.962]
Epoch [61/120    avg_loss:0.191, val_acc:0.986]
Epoch [62/120    avg_loss:0.219, val_acc:0.982]
Epoch [63/120    avg_loss:0.246, val_acc:0.968]
Epoch [64/120    avg_loss:0.185, val_acc:0.970]
Epoch [65/120    avg_loss:0.155, val_acc:0.980]
Epoch [66/120    avg_loss:0.124, val_acc:0.990]
Epoch [67/120    avg_loss:0.097, val_acc:0.988]
Epoch [68/120    avg_loss:0.123, val_acc:0.976]
Epoch [69/120    avg_loss:0.197, val_acc:0.984]
Epoch [70/120    avg_loss:0.129, val_acc:0.994]
Epoch [71/120    avg_loss:0.117, val_acc:0.978]
Epoch [72/120    avg_loss:0.094, val_acc:0.988]
Epoch [73/120    avg_loss:0.089, val_acc:0.990]
Epoch [74/120    avg_loss:0.122, val_acc:0.982]
Epoch [75/120    avg_loss:0.105, val_acc:0.992]
Epoch [76/120    avg_loss:0.111, val_acc:0.978]
Epoch [77/120    avg_loss:0.114, val_acc:0.978]
Epoch [78/120    avg_loss:0.111, val_acc:0.984]
Epoch [79/120    avg_loss:0.102, val_acc:0.992]
Epoch [80/120    avg_loss:0.088, val_acc:0.988]
Epoch [81/120    avg_loss:0.066, val_acc:0.982]
Epoch [82/120    avg_loss:0.086, val_acc:0.976]
Epoch [83/120    avg_loss:0.068, val_acc:0.988]
Epoch [84/120    avg_loss:0.063, val_acc:0.996]
Epoch [85/120    avg_loss:0.049, val_acc:0.996]
Epoch [86/120    avg_loss:0.036, val_acc:0.996]
Epoch [87/120    avg_loss:0.040, val_acc:0.996]
Epoch [88/120    avg_loss:0.043, val_acc:0.994]
Epoch [89/120    avg_loss:0.039, val_acc:0.996]
Epoch [90/120    avg_loss:0.036, val_acc:0.996]
Epoch [91/120    avg_loss:0.036, val_acc:0.996]
Epoch [92/120    avg_loss:0.040, val_acc:0.996]
Epoch [93/120    avg_loss:0.031, val_acc:0.996]
Epoch [94/120    avg_loss:0.038, val_acc:0.996]
Epoch [95/120    avg_loss:0.038, val_acc:0.996]
Epoch [96/120    avg_loss:0.038, val_acc:0.996]
Epoch [97/120    avg_loss:0.043, val_acc:0.996]
Epoch [98/120    avg_loss:0.037, val_acc:0.996]
Epoch [99/120    avg_loss:0.032, val_acc:0.996]
Epoch [100/120    avg_loss:0.035, val_acc:0.994]
Epoch [101/120    avg_loss:0.032, val_acc:0.996]
Epoch [102/120    avg_loss:0.029, val_acc:0.994]
Epoch [103/120    avg_loss:0.044, val_acc:0.994]
Epoch [104/120    avg_loss:0.035, val_acc:0.996]
Epoch [105/120    avg_loss:0.037, val_acc:0.994]
Epoch [106/120    avg_loss:0.027, val_acc:0.994]
Epoch [107/120    avg_loss:0.032, val_acc:0.994]
Epoch [108/120    avg_loss:0.031, val_acc:0.996]
Epoch [109/120    avg_loss:0.030, val_acc:0.996]
Epoch [110/120    avg_loss:0.027, val_acc:0.996]
Epoch [111/120    avg_loss:0.034, val_acc:0.994]
Epoch [112/120    avg_loss:0.031, val_acc:0.996]
Epoch [113/120    avg_loss:0.032, val_acc:0.996]
Epoch [114/120    avg_loss:0.030, val_acc:0.996]
Epoch [115/120    avg_loss:0.025, val_acc:0.996]
Epoch [116/120    avg_loss:0.027, val_acc:0.996]
Epoch [117/120    avg_loss:0.033, val_acc:0.996]
Epoch [118/120    avg_loss:0.032, val_acc:0.996]
Epoch [119/120    avg_loss:0.031, val_acc:0.996]
Epoch [120/120    avg_loss:0.027, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 218  12   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.98648649 0.97321429 0.92410714 0.92857143
 1.         0.96703297 1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9900304401061153
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4786d2e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.630, val_acc:0.097]
Epoch [2/120    avg_loss:2.571, val_acc:0.216]
Epoch [3/120    avg_loss:2.509, val_acc:0.294]
Epoch [4/120    avg_loss:2.449, val_acc:0.296]
Epoch [5/120    avg_loss:2.388, val_acc:0.339]
Epoch [6/120    avg_loss:2.307, val_acc:0.369]
Epoch [7/120    avg_loss:2.243, val_acc:0.452]
Epoch [8/120    avg_loss:2.172, val_acc:0.514]
Epoch [9/120    avg_loss:2.069, val_acc:0.536]
Epoch [10/120    avg_loss:1.981, val_acc:0.587]
Epoch [11/120    avg_loss:1.884, val_acc:0.601]
Epoch [12/120    avg_loss:1.770, val_acc:0.667]
Epoch [13/120    avg_loss:1.637, val_acc:0.706]
Epoch [14/120    avg_loss:1.546, val_acc:0.780]
Epoch [15/120    avg_loss:1.389, val_acc:0.792]
Epoch [16/120    avg_loss:1.281, val_acc:0.849]
Epoch [17/120    avg_loss:1.205, val_acc:0.851]
Epoch [18/120    avg_loss:1.048, val_acc:0.873]
Epoch [19/120    avg_loss:0.963, val_acc:0.845]
Epoch [20/120    avg_loss:0.836, val_acc:0.863]
Epoch [21/120    avg_loss:0.772, val_acc:0.883]
Epoch [22/120    avg_loss:0.757, val_acc:0.903]
Epoch [23/120    avg_loss:0.678, val_acc:0.889]
Epoch [24/120    avg_loss:0.624, val_acc:0.885]
Epoch [25/120    avg_loss:0.608, val_acc:0.869]
Epoch [26/120    avg_loss:0.560, val_acc:0.883]
Epoch [27/120    avg_loss:0.557, val_acc:0.893]
Epoch [28/120    avg_loss:0.505, val_acc:0.879]
Epoch [29/120    avg_loss:0.464, val_acc:0.919]
Epoch [30/120    avg_loss:0.459, val_acc:0.913]
Epoch [31/120    avg_loss:0.418, val_acc:0.935]
Epoch [32/120    avg_loss:0.375, val_acc:0.944]
Epoch [33/120    avg_loss:0.414, val_acc:0.927]
Epoch [34/120    avg_loss:0.363, val_acc:0.929]
Epoch [35/120    avg_loss:0.339, val_acc:0.925]
Epoch [36/120    avg_loss:0.364, val_acc:0.927]
Epoch [37/120    avg_loss:0.318, val_acc:0.935]
Epoch [38/120    avg_loss:0.354, val_acc:0.905]
Epoch [39/120    avg_loss:0.295, val_acc:0.946]
Epoch [40/120    avg_loss:0.270, val_acc:0.944]
Epoch [41/120    avg_loss:0.262, val_acc:0.958]
Epoch [42/120    avg_loss:0.262, val_acc:0.946]
Epoch [43/120    avg_loss:0.262, val_acc:0.950]
Epoch [44/120    avg_loss:0.274, val_acc:0.927]
Epoch [45/120    avg_loss:0.220, val_acc:0.970]
Epoch [46/120    avg_loss:0.206, val_acc:0.966]
Epoch [47/120    avg_loss:0.219, val_acc:0.980]
Epoch [48/120    avg_loss:0.203, val_acc:0.962]
Epoch [49/120    avg_loss:0.178, val_acc:0.968]
Epoch [50/120    avg_loss:0.188, val_acc:0.972]
Epoch [51/120    avg_loss:0.154, val_acc:0.935]
Epoch [52/120    avg_loss:0.154, val_acc:0.974]
Epoch [53/120    avg_loss:0.149, val_acc:0.972]
Epoch [54/120    avg_loss:0.193, val_acc:0.921]
Epoch [55/120    avg_loss:0.229, val_acc:0.964]
Epoch [56/120    avg_loss:0.194, val_acc:0.962]
Epoch [57/120    avg_loss:0.163, val_acc:0.970]
Epoch [58/120    avg_loss:0.148, val_acc:0.964]
Epoch [59/120    avg_loss:0.189, val_acc:0.952]
Epoch [60/120    avg_loss:0.181, val_acc:0.964]
Epoch [61/120    avg_loss:0.117, val_acc:0.974]
Epoch [62/120    avg_loss:0.097, val_acc:0.982]
Epoch [63/120    avg_loss:0.092, val_acc:0.986]
Epoch [64/120    avg_loss:0.088, val_acc:0.984]
Epoch [65/120    avg_loss:0.085, val_acc:0.984]
Epoch [66/120    avg_loss:0.093, val_acc:0.984]
Epoch [67/120    avg_loss:0.069, val_acc:0.984]
Epoch [68/120    avg_loss:0.081, val_acc:0.986]
Epoch [69/120    avg_loss:0.076, val_acc:0.986]
Epoch [70/120    avg_loss:0.065, val_acc:0.988]
Epoch [71/120    avg_loss:0.080, val_acc:0.986]
Epoch [72/120    avg_loss:0.081, val_acc:0.986]
Epoch [73/120    avg_loss:0.063, val_acc:0.990]
Epoch [74/120    avg_loss:0.074, val_acc:0.988]
Epoch [75/120    avg_loss:0.067, val_acc:0.986]
Epoch [76/120    avg_loss:0.066, val_acc:0.988]
Epoch [77/120    avg_loss:0.068, val_acc:0.990]
Epoch [78/120    avg_loss:0.061, val_acc:0.986]
Epoch [79/120    avg_loss:0.069, val_acc:0.988]
Epoch [80/120    avg_loss:0.068, val_acc:0.988]
Epoch [81/120    avg_loss:0.063, val_acc:0.992]
Epoch [82/120    avg_loss:0.055, val_acc:0.986]
Epoch [83/120    avg_loss:0.063, val_acc:0.984]
Epoch [84/120    avg_loss:0.055, val_acc:0.988]
Epoch [85/120    avg_loss:0.056, val_acc:0.986]
Epoch [86/120    avg_loss:0.058, val_acc:0.988]
Epoch [87/120    avg_loss:0.060, val_acc:0.992]
Epoch [88/120    avg_loss:0.058, val_acc:0.990]
Epoch [89/120    avg_loss:0.060, val_acc:0.992]
Epoch [90/120    avg_loss:0.058, val_acc:0.992]
Epoch [91/120    avg_loss:0.064, val_acc:0.990]
Epoch [92/120    avg_loss:0.062, val_acc:0.988]
Epoch [93/120    avg_loss:0.060, val_acc:0.992]
Epoch [94/120    avg_loss:0.057, val_acc:0.990]
Epoch [95/120    avg_loss:0.058, val_acc:0.994]
Epoch [96/120    avg_loss:0.065, val_acc:0.988]
Epoch [97/120    avg_loss:0.066, val_acc:0.988]
Epoch [98/120    avg_loss:0.062, val_acc:0.990]
Epoch [99/120    avg_loss:0.051, val_acc:0.990]
Epoch [100/120    avg_loss:0.054, val_acc:0.996]
Epoch [101/120    avg_loss:0.058, val_acc:0.992]
Epoch [102/120    avg_loss:0.055, val_acc:0.990]
Epoch [103/120    avg_loss:0.055, val_acc:0.986]
Epoch [104/120    avg_loss:0.052, val_acc:0.988]
Epoch [105/120    avg_loss:0.051, val_acc:0.986]
Epoch [106/120    avg_loss:0.055, val_acc:0.992]
Epoch [107/120    avg_loss:0.055, val_acc:0.992]
Epoch [108/120    avg_loss:0.051, val_acc:0.990]
Epoch [109/120    avg_loss:0.053, val_acc:0.988]
Epoch [110/120    avg_loss:0.048, val_acc:0.992]
Epoch [111/120    avg_loss:0.045, val_acc:0.988]
Epoch [112/120    avg_loss:0.053, val_acc:0.990]
Epoch [113/120    avg_loss:0.046, val_acc:0.992]
Epoch [114/120    avg_loss:0.046, val_acc:0.992]
Epoch [115/120    avg_loss:0.053, val_acc:0.992]
Epoch [116/120    avg_loss:0.056, val_acc:0.992]
Epoch [117/120    avg_loss:0.051, val_acc:0.992]
Epoch [118/120    avg_loss:0.052, val_acc:0.992]
Epoch [119/120    avg_loss:0.050, val_acc:0.992]
Epoch [120/120    avg_loss:0.048, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215  13   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.98206278 0.96629213 0.94323144 0.96
 0.99756691 0.95555556 0.99742931 1.         1.         1.
 1.         1.        ]

Kappa:
0.9914540608699244
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14e3db5dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.282]
Epoch [2/120    avg_loss:2.556, val_acc:0.296]
Epoch [3/120    avg_loss:2.495, val_acc:0.317]
Epoch [4/120    avg_loss:2.438, val_acc:0.329]
Epoch [5/120    avg_loss:2.381, val_acc:0.329]
Epoch [6/120    avg_loss:2.303, val_acc:0.347]
Epoch [7/120    avg_loss:2.233, val_acc:0.381]
Epoch [8/120    avg_loss:2.159, val_acc:0.413]
Epoch [9/120    avg_loss:2.090, val_acc:0.528]
Epoch [10/120    avg_loss:2.003, val_acc:0.583]
Epoch [11/120    avg_loss:1.897, val_acc:0.617]
Epoch [12/120    avg_loss:1.782, val_acc:0.627]
Epoch [13/120    avg_loss:1.673, val_acc:0.647]
Epoch [14/120    avg_loss:1.549, val_acc:0.655]
Epoch [15/120    avg_loss:1.442, val_acc:0.673]
Epoch [16/120    avg_loss:1.333, val_acc:0.706]
Epoch [17/120    avg_loss:1.232, val_acc:0.704]
Epoch [18/120    avg_loss:1.148, val_acc:0.716]
Epoch [19/120    avg_loss:1.076, val_acc:0.714]
Epoch [20/120    avg_loss:0.919, val_acc:0.774]
Epoch [21/120    avg_loss:0.867, val_acc:0.847]
Epoch [22/120    avg_loss:0.799, val_acc:0.792]
Epoch [23/120    avg_loss:0.823, val_acc:0.879]
Epoch [24/120    avg_loss:0.722, val_acc:0.881]
Epoch [25/120    avg_loss:0.656, val_acc:0.865]
Epoch [26/120    avg_loss:0.663, val_acc:0.891]
Epoch [27/120    avg_loss:0.604, val_acc:0.905]
Epoch [28/120    avg_loss:0.566, val_acc:0.905]
Epoch [29/120    avg_loss:0.486, val_acc:0.911]
Epoch [30/120    avg_loss:0.475, val_acc:0.907]
Epoch [31/120    avg_loss:0.509, val_acc:0.845]
Epoch [32/120    avg_loss:0.481, val_acc:0.893]
Epoch [33/120    avg_loss:0.439, val_acc:0.925]
Epoch [34/120    avg_loss:0.473, val_acc:0.938]
Epoch [35/120    avg_loss:0.347, val_acc:0.933]
Epoch [36/120    avg_loss:0.356, val_acc:0.913]
Epoch [37/120    avg_loss:0.333, val_acc:0.931]
Epoch [38/120    avg_loss:0.310, val_acc:0.952]
Epoch [39/120    avg_loss:0.305, val_acc:0.958]
Epoch [40/120    avg_loss:0.321, val_acc:0.915]
Epoch [41/120    avg_loss:0.301, val_acc:0.956]
Epoch [42/120    avg_loss:0.271, val_acc:0.948]
Epoch [43/120    avg_loss:0.301, val_acc:0.917]
Epoch [44/120    avg_loss:0.283, val_acc:0.935]
Epoch [45/120    avg_loss:0.278, val_acc:0.964]
Epoch [46/120    avg_loss:0.206, val_acc:0.946]
Epoch [47/120    avg_loss:0.204, val_acc:0.954]
Epoch [48/120    avg_loss:0.345, val_acc:0.950]
Epoch [49/120    avg_loss:0.319, val_acc:0.938]
Epoch [50/120    avg_loss:0.260, val_acc:0.940]
Epoch [51/120    avg_loss:0.243, val_acc:0.958]
Epoch [52/120    avg_loss:0.319, val_acc:0.954]
Epoch [53/120    avg_loss:0.211, val_acc:0.958]
Epoch [54/120    avg_loss:0.263, val_acc:0.964]
Epoch [55/120    avg_loss:0.158, val_acc:0.984]
Epoch [56/120    avg_loss:0.134, val_acc:0.958]
Epoch [57/120    avg_loss:0.172, val_acc:0.978]
Epoch [58/120    avg_loss:0.133, val_acc:0.978]
Epoch [59/120    avg_loss:0.163, val_acc:0.962]
Epoch [60/120    avg_loss:0.221, val_acc:0.968]
Epoch [61/120    avg_loss:0.175, val_acc:0.960]
Epoch [62/120    avg_loss:0.138, val_acc:0.976]
Epoch [63/120    avg_loss:0.185, val_acc:0.964]
Epoch [64/120    avg_loss:0.158, val_acc:0.982]
Epoch [65/120    avg_loss:0.116, val_acc:0.976]
Epoch [66/120    avg_loss:0.123, val_acc:0.956]
Epoch [67/120    avg_loss:0.155, val_acc:0.970]
Epoch [68/120    avg_loss:0.139, val_acc:0.968]
Epoch [69/120    avg_loss:0.092, val_acc:0.976]
Epoch [70/120    avg_loss:0.087, val_acc:0.986]
Epoch [71/120    avg_loss:0.074, val_acc:0.984]
Epoch [72/120    avg_loss:0.081, val_acc:0.984]
Epoch [73/120    avg_loss:0.079, val_acc:0.984]
Epoch [74/120    avg_loss:0.077, val_acc:0.984]
Epoch [75/120    avg_loss:0.070, val_acc:0.988]
Epoch [76/120    avg_loss:0.069, val_acc:0.990]
Epoch [77/120    avg_loss:0.061, val_acc:0.988]
Epoch [78/120    avg_loss:0.064, val_acc:0.988]
Epoch [79/120    avg_loss:0.059, val_acc:0.988]
Epoch [80/120    avg_loss:0.056, val_acc:0.990]
Epoch [81/120    avg_loss:0.065, val_acc:0.988]
Epoch [82/120    avg_loss:0.058, val_acc:0.988]
Epoch [83/120    avg_loss:0.066, val_acc:0.990]
Epoch [84/120    avg_loss:0.065, val_acc:0.990]
Epoch [85/120    avg_loss:0.059, val_acc:0.990]
Epoch [86/120    avg_loss:0.058, val_acc:0.988]
Epoch [87/120    avg_loss:0.063, val_acc:0.986]
Epoch [88/120    avg_loss:0.062, val_acc:0.988]
Epoch [89/120    avg_loss:0.048, val_acc:0.988]
Epoch [90/120    avg_loss:0.050, val_acc:0.988]
Epoch [91/120    avg_loss:0.058, val_acc:0.988]
Epoch [92/120    avg_loss:0.062, val_acc:0.986]
Epoch [93/120    avg_loss:0.054, val_acc:0.988]
Epoch [94/120    avg_loss:0.063, val_acc:0.990]
Epoch [95/120    avg_loss:0.050, val_acc:0.988]
Epoch [96/120    avg_loss:0.049, val_acc:0.988]
Epoch [97/120    avg_loss:0.052, val_acc:0.988]
Epoch [98/120    avg_loss:0.051, val_acc:0.990]
Epoch [99/120    avg_loss:0.047, val_acc:0.988]
Epoch [100/120    avg_loss:0.050, val_acc:0.988]
Epoch [101/120    avg_loss:0.049, val_acc:0.988]
Epoch [102/120    avg_loss:0.054, val_acc:0.988]
Epoch [103/120    avg_loss:0.050, val_acc:0.990]
Epoch [104/120    avg_loss:0.046, val_acc:0.988]
Epoch [105/120    avg_loss:0.046, val_acc:0.992]
Epoch [106/120    avg_loss:0.051, val_acc:0.988]
Epoch [107/120    avg_loss:0.047, val_acc:0.992]
Epoch [108/120    avg_loss:0.046, val_acc:0.988]
Epoch [109/120    avg_loss:0.045, val_acc:0.990]
Epoch [110/120    avg_loss:0.047, val_acc:0.990]
Epoch [111/120    avg_loss:0.057, val_acc:0.988]
Epoch [112/120    avg_loss:0.047, val_acc:0.990]
Epoch [113/120    avg_loss:0.044, val_acc:0.992]
Epoch [114/120    avg_loss:0.043, val_acc:0.988]
Epoch [115/120    avg_loss:0.049, val_acc:0.990]
Epoch [116/120    avg_loss:0.049, val_acc:0.988]
Epoch [117/120    avg_loss:0.053, val_acc:0.992]
Epoch [118/120    avg_loss:0.049, val_acc:0.990]
Epoch [119/120    avg_loss:0.051, val_acc:0.990]
Epoch [120/120    avg_loss:0.043, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 224   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.96902655 0.98678414 0.94854586 0.94039735
 1.         0.92571429 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9912166127648891
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb561004e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.589, val_acc:0.232]
Epoch [2/120    avg_loss:2.522, val_acc:0.294]
Epoch [3/120    avg_loss:2.464, val_acc:0.312]
Epoch [4/120    avg_loss:2.403, val_acc:0.341]
Epoch [5/120    avg_loss:2.336, val_acc:0.359]
Epoch [6/120    avg_loss:2.279, val_acc:0.375]
Epoch [7/120    avg_loss:2.197, val_acc:0.425]
Epoch [8/120    avg_loss:2.134, val_acc:0.492]
Epoch [9/120    avg_loss:2.058, val_acc:0.524]
Epoch [10/120    avg_loss:1.968, val_acc:0.562]
Epoch [11/120    avg_loss:1.866, val_acc:0.635]
Epoch [12/120    avg_loss:1.736, val_acc:0.694]
Epoch [13/120    avg_loss:1.602, val_acc:0.778]
Epoch [14/120    avg_loss:1.483, val_acc:0.736]
Epoch [15/120    avg_loss:1.359, val_acc:0.710]
Epoch [16/120    avg_loss:1.180, val_acc:0.821]
Epoch [17/120    avg_loss:1.047, val_acc:0.815]
Epoch [18/120    avg_loss:0.949, val_acc:0.841]
Epoch [19/120    avg_loss:0.849, val_acc:0.873]
Epoch [20/120    avg_loss:0.858, val_acc:0.827]
Epoch [21/120    avg_loss:0.766, val_acc:0.903]
Epoch [22/120    avg_loss:0.697, val_acc:0.871]
Epoch [23/120    avg_loss:0.620, val_acc:0.915]
Epoch [24/120    avg_loss:0.591, val_acc:0.895]
Epoch [25/120    avg_loss:0.551, val_acc:0.833]
Epoch [26/120    avg_loss:0.598, val_acc:0.887]
Epoch [27/120    avg_loss:0.483, val_acc:0.911]
Epoch [28/120    avg_loss:0.524, val_acc:0.827]
Epoch [29/120    avg_loss:0.552, val_acc:0.901]
Epoch [30/120    avg_loss:0.510, val_acc:0.915]
Epoch [31/120    avg_loss:0.403, val_acc:0.935]
Epoch [32/120    avg_loss:0.350, val_acc:0.901]
Epoch [33/120    avg_loss:0.365, val_acc:0.881]
Epoch [34/120    avg_loss:0.340, val_acc:0.925]
Epoch [35/120    avg_loss:0.343, val_acc:0.921]
Epoch [36/120    avg_loss:0.364, val_acc:0.927]
Epoch [37/120    avg_loss:0.297, val_acc:0.931]
Epoch [38/120    avg_loss:0.309, val_acc:0.935]
Epoch [39/120    avg_loss:0.320, val_acc:0.938]
Epoch [40/120    avg_loss:0.289, val_acc:0.923]
Epoch [41/120    avg_loss:0.302, val_acc:0.970]
Epoch [42/120    avg_loss:0.247, val_acc:0.954]
Epoch [43/120    avg_loss:0.328, val_acc:0.948]
Epoch [44/120    avg_loss:0.259, val_acc:0.948]
Epoch [45/120    avg_loss:0.224, val_acc:0.964]
Epoch [46/120    avg_loss:0.219, val_acc:0.948]
Epoch [47/120    avg_loss:0.244, val_acc:0.940]
Epoch [48/120    avg_loss:0.217, val_acc:0.944]
Epoch [49/120    avg_loss:0.296, val_acc:0.970]
Epoch [50/120    avg_loss:0.243, val_acc:0.956]
Epoch [51/120    avg_loss:0.184, val_acc:0.956]
Epoch [52/120    avg_loss:0.208, val_acc:0.927]
Epoch [53/120    avg_loss:0.186, val_acc:0.964]
Epoch [54/120    avg_loss:0.174, val_acc:0.944]
Epoch [55/120    avg_loss:0.203, val_acc:0.942]
Epoch [56/120    avg_loss:0.250, val_acc:0.962]
Epoch [57/120    avg_loss:0.144, val_acc:0.980]
Epoch [58/120    avg_loss:0.130, val_acc:0.974]
Epoch [59/120    avg_loss:0.168, val_acc:0.974]
Epoch [60/120    avg_loss:0.129, val_acc:0.980]
Epoch [61/120    avg_loss:0.127, val_acc:0.964]
Epoch [62/120    avg_loss:0.112, val_acc:0.982]
Epoch [63/120    avg_loss:0.118, val_acc:0.974]
Epoch [64/120    avg_loss:0.182, val_acc:0.974]
Epoch [65/120    avg_loss:0.142, val_acc:0.956]
Epoch [66/120    avg_loss:0.116, val_acc:0.980]
Epoch [67/120    avg_loss:0.138, val_acc:0.980]
Epoch [68/120    avg_loss:0.133, val_acc:0.974]
Epoch [69/120    avg_loss:0.103, val_acc:0.976]
Epoch [70/120    avg_loss:0.134, val_acc:0.966]
Epoch [71/120    avg_loss:0.173, val_acc:0.966]
Epoch [72/120    avg_loss:0.109, val_acc:0.976]
Epoch [73/120    avg_loss:0.116, val_acc:0.970]
Epoch [74/120    avg_loss:0.124, val_acc:0.976]
Epoch [75/120    avg_loss:0.108, val_acc:0.948]
Epoch [76/120    avg_loss:0.144, val_acc:0.984]
Epoch [77/120    avg_loss:0.073, val_acc:0.988]
Epoch [78/120    avg_loss:0.065, val_acc:0.990]
Epoch [79/120    avg_loss:0.067, val_acc:0.988]
Epoch [80/120    avg_loss:0.068, val_acc:0.990]
Epoch [81/120    avg_loss:0.061, val_acc:0.990]
Epoch [82/120    avg_loss:0.061, val_acc:0.988]
Epoch [83/120    avg_loss:0.052, val_acc:0.988]
Epoch [84/120    avg_loss:0.064, val_acc:0.988]
Epoch [85/120    avg_loss:0.056, val_acc:0.986]
Epoch [86/120    avg_loss:0.062, val_acc:0.988]
Epoch [87/120    avg_loss:0.056, val_acc:0.988]
Epoch [88/120    avg_loss:0.055, val_acc:0.988]
Epoch [89/120    avg_loss:0.049, val_acc:0.988]
Epoch [90/120    avg_loss:0.059, val_acc:0.986]
Epoch [91/120    avg_loss:0.054, val_acc:0.988]
Epoch [92/120    avg_loss:0.052, val_acc:0.988]
Epoch [93/120    avg_loss:0.054, val_acc:0.988]
Epoch [94/120    avg_loss:0.051, val_acc:0.988]
Epoch [95/120    avg_loss:0.052, val_acc:0.988]
Epoch [96/120    avg_loss:0.050, val_acc:0.988]
Epoch [97/120    avg_loss:0.051, val_acc:0.988]
Epoch [98/120    avg_loss:0.054, val_acc:0.988]
Epoch [99/120    avg_loss:0.053, val_acc:0.988]
Epoch [100/120    avg_loss:0.055, val_acc:0.988]
Epoch [101/120    avg_loss:0.049, val_acc:0.988]
Epoch [102/120    avg_loss:0.055, val_acc:0.988]
Epoch [103/120    avg_loss:0.045, val_acc:0.990]
Epoch [104/120    avg_loss:0.054, val_acc:0.990]
Epoch [105/120    avg_loss:0.057, val_acc:0.990]
Epoch [106/120    avg_loss:0.047, val_acc:0.988]
Epoch [107/120    avg_loss:0.051, val_acc:0.990]
Epoch [108/120    avg_loss:0.050, val_acc:0.988]
Epoch [109/120    avg_loss:0.053, val_acc:0.988]
Epoch [110/120    avg_loss:0.053, val_acc:0.988]
Epoch [111/120    avg_loss:0.050, val_acc:0.988]
Epoch [112/120    avg_loss:0.051, val_acc:0.988]
Epoch [113/120    avg_loss:0.051, val_acc:0.988]
Epoch [114/120    avg_loss:0.049, val_acc:0.988]
Epoch [115/120    avg_loss:0.048, val_acc:0.988]
Epoch [116/120    avg_loss:0.050, val_acc:0.988]
Epoch [117/120    avg_loss:0.044, val_acc:0.988]
Epoch [118/120    avg_loss:0.049, val_acc:0.988]
Epoch [119/120    avg_loss:0.047, val_acc:0.988]
Epoch [120/120    avg_loss:0.048, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  21   0   0   0   0   0   0   3   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.97767857 0.99563319 0.90423163 0.8707483
 1.         0.94382022 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9874177677701694
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c96302dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.266]
Epoch [2/120    avg_loss:2.567, val_acc:0.276]
Epoch [3/120    avg_loss:2.502, val_acc:0.280]
Epoch [4/120    avg_loss:2.439, val_acc:0.343]
Epoch [5/120    avg_loss:2.380, val_acc:0.419]
Epoch [6/120    avg_loss:2.311, val_acc:0.468]
Epoch [7/120    avg_loss:2.221, val_acc:0.462]
Epoch [8/120    avg_loss:2.138, val_acc:0.550]
Epoch [9/120    avg_loss:2.051, val_acc:0.577]
Epoch [10/120    avg_loss:1.929, val_acc:0.579]
Epoch [11/120    avg_loss:1.852, val_acc:0.591]
Epoch [12/120    avg_loss:1.762, val_acc:0.603]
Epoch [13/120    avg_loss:1.602, val_acc:0.637]
Epoch [14/120    avg_loss:1.486, val_acc:0.645]
Epoch [15/120    avg_loss:1.332, val_acc:0.677]
Epoch [16/120    avg_loss:1.250, val_acc:0.724]
Epoch [17/120    avg_loss:1.154, val_acc:0.812]
Epoch [18/120    avg_loss:1.090, val_acc:0.732]
Epoch [19/120    avg_loss:0.957, val_acc:0.845]
Epoch [20/120    avg_loss:0.871, val_acc:0.825]
Epoch [21/120    avg_loss:0.828, val_acc:0.847]
Epoch [22/120    avg_loss:0.792, val_acc:0.859]
Epoch [23/120    avg_loss:0.763, val_acc:0.877]
Epoch [24/120    avg_loss:0.687, val_acc:0.897]
Epoch [25/120    avg_loss:0.638, val_acc:0.893]
Epoch [26/120    avg_loss:0.576, val_acc:0.869]
Epoch [27/120    avg_loss:0.691, val_acc:0.885]
Epoch [28/120    avg_loss:0.524, val_acc:0.891]
Epoch [29/120    avg_loss:0.485, val_acc:0.887]
Epoch [30/120    avg_loss:0.519, val_acc:0.883]
Epoch [31/120    avg_loss:0.463, val_acc:0.929]
Epoch [32/120    avg_loss:0.446, val_acc:0.907]
Epoch [33/120    avg_loss:0.453, val_acc:0.875]
Epoch [34/120    avg_loss:0.430, val_acc:0.921]
Epoch [35/120    avg_loss:0.389, val_acc:0.942]
Epoch [36/120    avg_loss:0.366, val_acc:0.901]
Epoch [37/120    avg_loss:0.451, val_acc:0.897]
Epoch [38/120    avg_loss:0.391, val_acc:0.925]
Epoch [39/120    avg_loss:0.344, val_acc:0.909]
Epoch [40/120    avg_loss:0.350, val_acc:0.911]
Epoch [41/120    avg_loss:0.365, val_acc:0.921]
Epoch [42/120    avg_loss:0.322, val_acc:0.966]
Epoch [43/120    avg_loss:0.281, val_acc:0.931]
Epoch [44/120    avg_loss:0.298, val_acc:0.938]
Epoch [45/120    avg_loss:0.458, val_acc:0.821]
Epoch [46/120    avg_loss:0.412, val_acc:0.921]
Epoch [47/120    avg_loss:0.290, val_acc:0.960]
Epoch [48/120    avg_loss:0.227, val_acc:0.964]
Epoch [49/120    avg_loss:0.211, val_acc:0.970]
Epoch [50/120    avg_loss:0.185, val_acc:0.958]
Epoch [51/120    avg_loss:0.272, val_acc:0.927]
Epoch [52/120    avg_loss:0.251, val_acc:0.962]
Epoch [53/120    avg_loss:0.219, val_acc:0.970]
Epoch [54/120    avg_loss:0.207, val_acc:0.952]
Epoch [55/120    avg_loss:0.186, val_acc:0.974]
Epoch [56/120    avg_loss:0.180, val_acc:0.954]
Epoch [57/120    avg_loss:0.162, val_acc:0.948]
Epoch [58/120    avg_loss:0.206, val_acc:0.970]
Epoch [59/120    avg_loss:0.187, val_acc:0.923]
Epoch [60/120    avg_loss:0.194, val_acc:0.974]
Epoch [61/120    avg_loss:0.165, val_acc:0.974]
Epoch [62/120    avg_loss:0.172, val_acc:0.954]
Epoch [63/120    avg_loss:0.227, val_acc:0.954]
Epoch [64/120    avg_loss:0.176, val_acc:0.966]
Epoch [65/120    avg_loss:0.172, val_acc:0.960]
Epoch [66/120    avg_loss:0.143, val_acc:0.966]
Epoch [67/120    avg_loss:0.131, val_acc:0.962]
Epoch [68/120    avg_loss:0.208, val_acc:0.942]
Epoch [69/120    avg_loss:0.180, val_acc:0.954]
Epoch [70/120    avg_loss:0.135, val_acc:0.966]
Epoch [71/120    avg_loss:0.105, val_acc:0.980]
Epoch [72/120    avg_loss:0.116, val_acc:0.984]
Epoch [73/120    avg_loss:0.123, val_acc:0.978]
Epoch [74/120    avg_loss:0.168, val_acc:0.968]
Epoch [75/120    avg_loss:0.105, val_acc:0.980]
Epoch [76/120    avg_loss:0.175, val_acc:0.970]
Epoch [77/120    avg_loss:0.104, val_acc:0.964]
Epoch [78/120    avg_loss:0.131, val_acc:0.986]
Epoch [79/120    avg_loss:0.106, val_acc:0.976]
Epoch [80/120    avg_loss:0.078, val_acc:0.966]
Epoch [81/120    avg_loss:0.108, val_acc:0.964]
Epoch [82/120    avg_loss:0.124, val_acc:0.966]
Epoch [83/120    avg_loss:0.106, val_acc:0.986]
Epoch [84/120    avg_loss:0.058, val_acc:0.984]
Epoch [85/120    avg_loss:0.165, val_acc:0.968]
Epoch [86/120    avg_loss:0.091, val_acc:0.974]
Epoch [87/120    avg_loss:0.064, val_acc:0.978]
Epoch [88/120    avg_loss:0.066, val_acc:0.984]
Epoch [89/120    avg_loss:0.066, val_acc:0.980]
Epoch [90/120    avg_loss:0.065, val_acc:0.970]
Epoch [91/120    avg_loss:0.071, val_acc:0.988]
Epoch [92/120    avg_loss:0.071, val_acc:0.978]
Epoch [93/120    avg_loss:0.057, val_acc:0.976]
Epoch [94/120    avg_loss:0.054, val_acc:0.978]
Epoch [95/120    avg_loss:0.065, val_acc:0.980]
Epoch [96/120    avg_loss:0.051, val_acc:0.980]
Epoch [97/120    avg_loss:0.053, val_acc:0.986]
Epoch [98/120    avg_loss:0.054, val_acc:0.988]
Epoch [99/120    avg_loss:0.039, val_acc:0.986]
Epoch [100/120    avg_loss:0.059, val_acc:0.982]
Epoch [101/120    avg_loss:0.107, val_acc:0.984]
Epoch [102/120    avg_loss:0.073, val_acc:0.970]
Epoch [103/120    avg_loss:0.084, val_acc:0.980]
Epoch [104/120    avg_loss:0.062, val_acc:0.986]
Epoch [105/120    avg_loss:0.075, val_acc:0.960]
Epoch [106/120    avg_loss:0.060, val_acc:0.974]
Epoch [107/120    avg_loss:0.071, val_acc:0.970]
Epoch [108/120    avg_loss:0.056, val_acc:0.974]
Epoch [109/120    avg_loss:0.037, val_acc:0.980]
Epoch [110/120    avg_loss:0.038, val_acc:0.974]
Epoch [111/120    avg_loss:0.032, val_acc:0.988]
Epoch [112/120    avg_loss:0.021, val_acc:0.976]
Epoch [113/120    avg_loss:0.048, val_acc:0.980]
Epoch [114/120    avg_loss:0.064, val_acc:0.974]
Epoch [115/120    avg_loss:0.063, val_acc:0.946]
Epoch [116/120    avg_loss:0.101, val_acc:0.962]
Epoch [117/120    avg_loss:0.105, val_acc:0.968]
Epoch [118/120    avg_loss:0.055, val_acc:0.972]
Epoch [119/120    avg_loss:0.041, val_acc:0.980]
Epoch [120/120    avg_loss:0.041, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0  13 212   1   0   0   0   0   0   0   1   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 1.         0.97333333 0.97251586 0.92374728 0.92250923
 1.         0.93181818 1.         1.         1.         0.98691099
 0.9877369  1.        ]

Kappa:
0.9864675015782346
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4328e08e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.256]
Epoch [2/120    avg_loss:2.560, val_acc:0.308]
Epoch [3/120    avg_loss:2.498, val_acc:0.361]
Epoch [4/120    avg_loss:2.441, val_acc:0.405]
Epoch [5/120    avg_loss:2.374, val_acc:0.417]
Epoch [6/120    avg_loss:2.300, val_acc:0.464]
Epoch [7/120    avg_loss:2.220, val_acc:0.470]
Epoch [8/120    avg_loss:2.153, val_acc:0.498]
Epoch [9/120    avg_loss:2.041, val_acc:0.512]
Epoch [10/120    avg_loss:1.931, val_acc:0.560]
Epoch [11/120    avg_loss:1.815, val_acc:0.617]
Epoch [12/120    avg_loss:1.711, val_acc:0.665]
Epoch [13/120    avg_loss:1.590, val_acc:0.677]
Epoch [14/120    avg_loss:1.470, val_acc:0.710]
Epoch [15/120    avg_loss:1.364, val_acc:0.712]
Epoch [16/120    avg_loss:1.250, val_acc:0.722]
Epoch [17/120    avg_loss:1.111, val_acc:0.768]
Epoch [18/120    avg_loss:0.997, val_acc:0.802]
Epoch [19/120    avg_loss:0.960, val_acc:0.810]
Epoch [20/120    avg_loss:0.890, val_acc:0.849]
Epoch [21/120    avg_loss:0.780, val_acc:0.913]
Epoch [22/120    avg_loss:0.732, val_acc:0.877]
Epoch [23/120    avg_loss:0.715, val_acc:0.835]
Epoch [24/120    avg_loss:0.636, val_acc:0.903]
Epoch [25/120    avg_loss:0.623, val_acc:0.897]
Epoch [26/120    avg_loss:0.633, val_acc:0.917]
Epoch [27/120    avg_loss:0.513, val_acc:0.893]
Epoch [28/120    avg_loss:0.516, val_acc:0.905]
Epoch [29/120    avg_loss:0.485, val_acc:0.889]
Epoch [30/120    avg_loss:0.437, val_acc:0.938]
Epoch [31/120    avg_loss:0.417, val_acc:0.923]
Epoch [32/120    avg_loss:0.497, val_acc:0.927]
Epoch [33/120    avg_loss:0.388, val_acc:0.927]
Epoch [34/120    avg_loss:0.392, val_acc:0.927]
Epoch [35/120    avg_loss:0.404, val_acc:0.913]
Epoch [36/120    avg_loss:0.442, val_acc:0.915]
Epoch [37/120    avg_loss:0.415, val_acc:0.901]
Epoch [38/120    avg_loss:0.340, val_acc:0.919]
Epoch [39/120    avg_loss:0.351, val_acc:0.956]
Epoch [40/120    avg_loss:0.313, val_acc:0.905]
Epoch [41/120    avg_loss:0.267, val_acc:0.946]
Epoch [42/120    avg_loss:0.306, val_acc:0.958]
Epoch [43/120    avg_loss:0.275, val_acc:0.954]
Epoch [44/120    avg_loss:0.217, val_acc:0.944]
Epoch [45/120    avg_loss:0.225, val_acc:0.948]
Epoch [46/120    avg_loss:0.233, val_acc:0.954]
Epoch [47/120    avg_loss:0.266, val_acc:0.968]
Epoch [48/120    avg_loss:0.234, val_acc:0.970]
Epoch [49/120    avg_loss:0.233, val_acc:0.927]
Epoch [50/120    avg_loss:0.267, val_acc:0.946]
Epoch [51/120    avg_loss:0.241, val_acc:0.954]
Epoch [52/120    avg_loss:0.176, val_acc:0.960]
Epoch [53/120    avg_loss:0.199, val_acc:0.946]
Epoch [54/120    avg_loss:0.195, val_acc:0.970]
Epoch [55/120    avg_loss:0.139, val_acc:0.976]
Epoch [56/120    avg_loss:0.207, val_acc:0.972]
Epoch [57/120    avg_loss:0.149, val_acc:0.966]
Epoch [58/120    avg_loss:0.147, val_acc:0.966]
Epoch [59/120    avg_loss:0.169, val_acc:0.956]
Epoch [60/120    avg_loss:0.149, val_acc:0.942]
Epoch [61/120    avg_loss:0.193, val_acc:0.960]
Epoch [62/120    avg_loss:0.118, val_acc:0.970]
Epoch [63/120    avg_loss:0.092, val_acc:0.970]
Epoch [64/120    avg_loss:0.100, val_acc:0.976]
Epoch [65/120    avg_loss:0.120, val_acc:0.956]
Epoch [66/120    avg_loss:0.120, val_acc:0.958]
Epoch [67/120    avg_loss:0.100, val_acc:0.974]
Epoch [68/120    avg_loss:0.112, val_acc:0.956]
Epoch [69/120    avg_loss:0.151, val_acc:0.974]
Epoch [70/120    avg_loss:0.084, val_acc:0.978]
Epoch [71/120    avg_loss:0.104, val_acc:0.974]
Epoch [72/120    avg_loss:0.116, val_acc:0.978]
Epoch [73/120    avg_loss:0.087, val_acc:0.982]
Epoch [74/120    avg_loss:0.061, val_acc:0.976]
Epoch [75/120    avg_loss:0.064, val_acc:0.986]
Epoch [76/120    avg_loss:0.080, val_acc:0.990]
Epoch [77/120    avg_loss:0.078, val_acc:0.990]
Epoch [78/120    avg_loss:0.063, val_acc:0.988]
Epoch [79/120    avg_loss:0.064, val_acc:0.976]
Epoch [80/120    avg_loss:0.212, val_acc:0.948]
Epoch [81/120    avg_loss:0.121, val_acc:0.970]
Epoch [82/120    avg_loss:0.096, val_acc:0.950]
Epoch [83/120    avg_loss:0.073, val_acc:0.984]
Epoch [84/120    avg_loss:0.065, val_acc:0.984]
Epoch [85/120    avg_loss:0.060, val_acc:0.982]
Epoch [86/120    avg_loss:0.064, val_acc:0.984]
Epoch [87/120    avg_loss:0.060, val_acc:0.992]
Epoch [88/120    avg_loss:0.039, val_acc:0.990]
Epoch [89/120    avg_loss:0.035, val_acc:0.990]
Epoch [90/120    avg_loss:0.034, val_acc:0.984]
Epoch [91/120    avg_loss:0.045, val_acc:0.986]
Epoch [92/120    avg_loss:0.050, val_acc:0.966]
Epoch [93/120    avg_loss:0.048, val_acc:0.990]
Epoch [94/120    avg_loss:0.033, val_acc:0.994]
Epoch [95/120    avg_loss:0.028, val_acc:0.982]
Epoch [96/120    avg_loss:0.026, val_acc:0.994]
Epoch [97/120    avg_loss:0.030, val_acc:0.996]
Epoch [98/120    avg_loss:0.048, val_acc:0.986]
Epoch [99/120    avg_loss:0.064, val_acc:0.982]
Epoch [100/120    avg_loss:0.084, val_acc:0.982]
Epoch [101/120    avg_loss:0.059, val_acc:0.986]
Epoch [102/120    avg_loss:0.034, val_acc:0.992]
Epoch [103/120    avg_loss:0.030, val_acc:0.988]
Epoch [104/120    avg_loss:0.027, val_acc:0.976]
Epoch [105/120    avg_loss:0.023, val_acc:0.992]
Epoch [106/120    avg_loss:0.028, val_acc:0.992]
Epoch [107/120    avg_loss:0.023, val_acc:0.984]
Epoch [108/120    avg_loss:0.031, val_acc:0.994]
Epoch [109/120    avg_loss:0.028, val_acc:0.990]
Epoch [110/120    avg_loss:0.038, val_acc:0.996]
Epoch [111/120    avg_loss:0.031, val_acc:0.984]
Epoch [112/120    avg_loss:0.026, val_acc:0.988]
Epoch [113/120    avg_loss:0.018, val_acc:0.992]
Epoch [114/120    avg_loss:0.028, val_acc:0.976]
Epoch [115/120    avg_loss:0.038, val_acc:0.986]
Epoch [116/120    avg_loss:0.031, val_acc:0.984]
Epoch [117/120    avg_loss:0.018, val_acc:0.994]
Epoch [118/120    avg_loss:0.016, val_acc:0.984]
Epoch [119/120    avg_loss:0.032, val_acc:0.990]
Epoch [120/120    avg_loss:0.030, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.97986577 1.         0.9475891  0.90636704
 1.         0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919278003347032
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2af8ade10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.371]
Epoch [2/120    avg_loss:2.551, val_acc:0.431]
Epoch [3/120    avg_loss:2.493, val_acc:0.442]
Epoch [4/120    avg_loss:2.446, val_acc:0.498]
Epoch [5/120    avg_loss:2.396, val_acc:0.516]
Epoch [6/120    avg_loss:2.342, val_acc:0.526]
Epoch [7/120    avg_loss:2.266, val_acc:0.613]
Epoch [8/120    avg_loss:2.176, val_acc:0.595]
Epoch [9/120    avg_loss:2.107, val_acc:0.661]
Epoch [10/120    avg_loss:1.994, val_acc:0.597]
Epoch [11/120    avg_loss:1.888, val_acc:0.663]
Epoch [12/120    avg_loss:1.758, val_acc:0.589]
Epoch [13/120    avg_loss:1.616, val_acc:0.722]
Epoch [14/120    avg_loss:1.489, val_acc:0.661]
Epoch [15/120    avg_loss:1.367, val_acc:0.718]
Epoch [16/120    avg_loss:1.267, val_acc:0.808]
Epoch [17/120    avg_loss:1.207, val_acc:0.859]
Epoch [18/120    avg_loss:1.114, val_acc:0.859]
Epoch [19/120    avg_loss:0.975, val_acc:0.861]
Epoch [20/120    avg_loss:0.910, val_acc:0.881]
Epoch [21/120    avg_loss:0.919, val_acc:0.855]
Epoch [22/120    avg_loss:0.801, val_acc:0.895]
Epoch [23/120    avg_loss:0.754, val_acc:0.903]
Epoch [24/120    avg_loss:0.690, val_acc:0.893]
Epoch [25/120    avg_loss:0.585, val_acc:0.913]
Epoch [26/120    avg_loss:0.597, val_acc:0.899]
Epoch [27/120    avg_loss:0.555, val_acc:0.923]
Epoch [28/120    avg_loss:0.509, val_acc:0.859]
Epoch [29/120    avg_loss:0.515, val_acc:0.903]
Epoch [30/120    avg_loss:0.464, val_acc:0.944]
Epoch [31/120    avg_loss:0.495, val_acc:0.935]
Epoch [32/120    avg_loss:0.399, val_acc:0.905]
Epoch [33/120    avg_loss:0.422, val_acc:0.881]
Epoch [34/120    avg_loss:0.399, val_acc:0.923]
Epoch [35/120    avg_loss:0.382, val_acc:0.883]
Epoch [36/120    avg_loss:0.469, val_acc:0.938]
Epoch [37/120    avg_loss:0.353, val_acc:0.915]
Epoch [38/120    avg_loss:0.388, val_acc:0.929]
Epoch [39/120    avg_loss:0.348, val_acc:0.919]
Epoch [40/120    avg_loss:0.329, val_acc:0.950]
Epoch [41/120    avg_loss:0.326, val_acc:0.950]
Epoch [42/120    avg_loss:0.299, val_acc:0.913]
Epoch [43/120    avg_loss:0.253, val_acc:0.970]
Epoch [44/120    avg_loss:0.295, val_acc:0.960]
Epoch [45/120    avg_loss:0.262, val_acc:0.962]
Epoch [46/120    avg_loss:0.237, val_acc:0.960]
Epoch [47/120    avg_loss:0.282, val_acc:0.921]
Epoch [48/120    avg_loss:0.243, val_acc:0.964]
Epoch [49/120    avg_loss:0.238, val_acc:0.970]
Epoch [50/120    avg_loss:0.232, val_acc:0.978]
Epoch [51/120    avg_loss:0.231, val_acc:0.940]
Epoch [52/120    avg_loss:0.224, val_acc:0.966]
Epoch [53/120    avg_loss:0.204, val_acc:0.966]
Epoch [54/120    avg_loss:0.223, val_acc:0.964]
Epoch [55/120    avg_loss:0.198, val_acc:0.966]
Epoch [56/120    avg_loss:0.209, val_acc:0.982]
Epoch [57/120    avg_loss:0.184, val_acc:0.974]
Epoch [58/120    avg_loss:0.174, val_acc:0.974]
Epoch [59/120    avg_loss:0.147, val_acc:0.976]
Epoch [60/120    avg_loss:0.160, val_acc:0.984]
Epoch [61/120    avg_loss:0.116, val_acc:0.988]
Epoch [62/120    avg_loss:0.139, val_acc:0.935]
Epoch [63/120    avg_loss:0.206, val_acc:0.891]
Epoch [64/120    avg_loss:0.224, val_acc:0.964]
Epoch [65/120    avg_loss:0.163, val_acc:0.982]
Epoch [66/120    avg_loss:0.120, val_acc:0.984]
Epoch [67/120    avg_loss:0.112, val_acc:0.982]
Epoch [68/120    avg_loss:0.120, val_acc:0.982]
Epoch [69/120    avg_loss:0.107, val_acc:0.980]
Epoch [70/120    avg_loss:0.098, val_acc:0.984]
Epoch [71/120    avg_loss:0.086, val_acc:0.974]
Epoch [72/120    avg_loss:0.127, val_acc:0.982]
Epoch [73/120    avg_loss:0.084, val_acc:0.984]
Epoch [74/120    avg_loss:0.074, val_acc:0.982]
Epoch [75/120    avg_loss:0.061, val_acc:0.986]
Epoch [76/120    avg_loss:0.056, val_acc:0.986]
Epoch [77/120    avg_loss:0.054, val_acc:0.986]
Epoch [78/120    avg_loss:0.050, val_acc:0.988]
Epoch [79/120    avg_loss:0.048, val_acc:0.986]
Epoch [80/120    avg_loss:0.044, val_acc:0.986]
Epoch [81/120    avg_loss:0.046, val_acc:0.986]
Epoch [82/120    avg_loss:0.056, val_acc:0.986]
Epoch [83/120    avg_loss:0.046, val_acc:0.986]
Epoch [84/120    avg_loss:0.044, val_acc:0.986]
Epoch [85/120    avg_loss:0.045, val_acc:0.988]
Epoch [86/120    avg_loss:0.046, val_acc:0.984]
Epoch [87/120    avg_loss:0.046, val_acc:0.984]
Epoch [88/120    avg_loss:0.046, val_acc:0.984]
Epoch [89/120    avg_loss:0.046, val_acc:0.986]
Epoch [90/120    avg_loss:0.046, val_acc:0.988]
Epoch [91/120    avg_loss:0.046, val_acc:0.986]
Epoch [92/120    avg_loss:0.046, val_acc:0.988]
Epoch [93/120    avg_loss:0.039, val_acc:0.988]
Epoch [94/120    avg_loss:0.043, val_acc:0.988]
Epoch [95/120    avg_loss:0.040, val_acc:0.988]
Epoch [96/120    avg_loss:0.047, val_acc:0.986]
Epoch [97/120    avg_loss:0.047, val_acc:0.988]
Epoch [98/120    avg_loss:0.043, val_acc:0.988]
Epoch [99/120    avg_loss:0.036, val_acc:0.984]
Epoch [100/120    avg_loss:0.042, val_acc:0.988]
Epoch [101/120    avg_loss:0.040, val_acc:0.986]
Epoch [102/120    avg_loss:0.048, val_acc:0.986]
Epoch [103/120    avg_loss:0.038, val_acc:0.988]
Epoch [104/120    avg_loss:0.040, val_acc:0.988]
Epoch [105/120    avg_loss:0.040, val_acc:0.988]
Epoch [106/120    avg_loss:0.038, val_acc:0.988]
Epoch [107/120    avg_loss:0.043, val_acc:0.986]
Epoch [108/120    avg_loss:0.043, val_acc:0.986]
Epoch [109/120    avg_loss:0.040, val_acc:0.986]
Epoch [110/120    avg_loss:0.039, val_acc:0.988]
Epoch [111/120    avg_loss:0.040, val_acc:0.988]
Epoch [112/120    avg_loss:0.040, val_acc:0.988]
Epoch [113/120    avg_loss:0.042, val_acc:0.988]
Epoch [114/120    avg_loss:0.046, val_acc:0.988]
Epoch [115/120    avg_loss:0.038, val_acc:0.988]
Epoch [116/120    avg_loss:0.035, val_acc:0.988]
Epoch [117/120    avg_loss:0.038, val_acc:0.986]
Epoch [118/120    avg_loss:0.038, val_acc:0.988]
Epoch [119/120    avg_loss:0.038, val_acc:0.988]
Epoch [120/120    avg_loss:0.038, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.97986577 1.         0.96127563 0.9442623
 1.         0.94972067 1.         1.         1.         0.98950131
 0.99109131 1.        ]

Kappa:
0.9919293217922533
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb096392eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.204]
Epoch [2/120    avg_loss:2.577, val_acc:0.286]
Epoch [3/120    avg_loss:2.515, val_acc:0.355]
Epoch [4/120    avg_loss:2.453, val_acc:0.391]
Epoch [5/120    avg_loss:2.388, val_acc:0.399]
Epoch [6/120    avg_loss:2.322, val_acc:0.397]
Epoch [7/120    avg_loss:2.258, val_acc:0.454]
Epoch [8/120    avg_loss:2.182, val_acc:0.466]
Epoch [9/120    avg_loss:2.114, val_acc:0.492]
Epoch [10/120    avg_loss:2.008, val_acc:0.500]
Epoch [11/120    avg_loss:1.896, val_acc:0.562]
Epoch [12/120    avg_loss:1.778, val_acc:0.649]
Epoch [13/120    avg_loss:1.643, val_acc:0.706]
Epoch [14/120    avg_loss:1.541, val_acc:0.780]
Epoch [15/120    avg_loss:1.369, val_acc:0.796]
Epoch [16/120    avg_loss:1.296, val_acc:0.784]
Epoch [17/120    avg_loss:1.152, val_acc:0.871]
Epoch [18/120    avg_loss:1.058, val_acc:0.817]
Epoch [19/120    avg_loss:0.951, val_acc:0.851]
Epoch [20/120    avg_loss:0.844, val_acc:0.885]
Epoch [21/120    avg_loss:0.771, val_acc:0.873]
Epoch [22/120    avg_loss:0.700, val_acc:0.857]
Epoch [23/120    avg_loss:0.634, val_acc:0.891]
Epoch [24/120    avg_loss:0.573, val_acc:0.899]
Epoch [25/120    avg_loss:0.623, val_acc:0.883]
Epoch [26/120    avg_loss:0.547, val_acc:0.879]
Epoch [27/120    avg_loss:0.558, val_acc:0.897]
Epoch [28/120    avg_loss:0.470, val_acc:0.917]
Epoch [29/120    avg_loss:0.421, val_acc:0.919]
Epoch [30/120    avg_loss:0.408, val_acc:0.903]
Epoch [31/120    avg_loss:0.416, val_acc:0.917]
Epoch [32/120    avg_loss:0.499, val_acc:0.911]
Epoch [33/120    avg_loss:0.416, val_acc:0.921]
Epoch [34/120    avg_loss:0.364, val_acc:0.925]
Epoch [35/120    avg_loss:0.358, val_acc:0.921]
Epoch [36/120    avg_loss:0.315, val_acc:0.925]
Epoch [37/120    avg_loss:0.344, val_acc:0.931]
Epoch [38/120    avg_loss:0.345, val_acc:0.950]
Epoch [39/120    avg_loss:0.301, val_acc:0.942]
Epoch [40/120    avg_loss:0.272, val_acc:0.950]
Epoch [41/120    avg_loss:0.268, val_acc:0.964]
Epoch [42/120    avg_loss:0.281, val_acc:0.927]
Epoch [43/120    avg_loss:0.299, val_acc:0.929]
Epoch [44/120    avg_loss:0.226, val_acc:0.968]
Epoch [45/120    avg_loss:0.224, val_acc:0.972]
Epoch [46/120    avg_loss:0.199, val_acc:0.984]
Epoch [47/120    avg_loss:0.298, val_acc:0.946]
Epoch [48/120    avg_loss:0.250, val_acc:0.952]
Epoch [49/120    avg_loss:0.220, val_acc:0.952]
Epoch [50/120    avg_loss:0.206, val_acc:0.942]
Epoch [51/120    avg_loss:0.221, val_acc:0.946]
Epoch [52/120    avg_loss:0.212, val_acc:0.968]
Epoch [53/120    avg_loss:0.187, val_acc:0.984]
Epoch [54/120    avg_loss:0.173, val_acc:0.968]
Epoch [55/120    avg_loss:0.157, val_acc:0.976]
Epoch [56/120    avg_loss:0.135, val_acc:0.970]
Epoch [57/120    avg_loss:0.145, val_acc:0.948]
Epoch [58/120    avg_loss:0.163, val_acc:0.972]
Epoch [59/120    avg_loss:0.145, val_acc:0.968]
Epoch [60/120    avg_loss:0.141, val_acc:0.974]
Epoch [61/120    avg_loss:0.132, val_acc:0.962]
Epoch [62/120    avg_loss:0.109, val_acc:0.976]
Epoch [63/120    avg_loss:0.109, val_acc:0.980]
Epoch [64/120    avg_loss:0.116, val_acc:0.984]
Epoch [65/120    avg_loss:0.098, val_acc:0.978]
Epoch [66/120    avg_loss:0.116, val_acc:0.978]
Epoch [67/120    avg_loss:0.089, val_acc:0.996]
Epoch [68/120    avg_loss:0.091, val_acc:0.980]
Epoch [69/120    avg_loss:0.089, val_acc:0.998]
Epoch [70/120    avg_loss:0.058, val_acc:0.992]
Epoch [71/120    avg_loss:0.068, val_acc:0.990]
Epoch [72/120    avg_loss:0.116, val_acc:0.990]
Epoch [73/120    avg_loss:0.122, val_acc:0.976]
Epoch [74/120    avg_loss:0.123, val_acc:0.970]
Epoch [75/120    avg_loss:0.096, val_acc:0.980]
Epoch [76/120    avg_loss:0.071, val_acc:0.996]
Epoch [77/120    avg_loss:0.068, val_acc:0.994]
Epoch [78/120    avg_loss:0.142, val_acc:0.992]
Epoch [79/120    avg_loss:0.092, val_acc:0.998]
Epoch [80/120    avg_loss:0.051, val_acc:0.994]
Epoch [81/120    avg_loss:0.073, val_acc:0.992]
Epoch [82/120    avg_loss:0.058, val_acc:0.994]
Epoch [83/120    avg_loss:0.053, val_acc:0.988]
Epoch [84/120    avg_loss:0.053, val_acc:0.990]
Epoch [85/120    avg_loss:0.049, val_acc:0.988]
Epoch [86/120    avg_loss:0.047, val_acc:0.986]
Epoch [87/120    avg_loss:0.036, val_acc:0.996]
Epoch [88/120    avg_loss:0.030, val_acc:0.996]
Epoch [89/120    avg_loss:0.052, val_acc:0.996]
Epoch [90/120    avg_loss:0.038, val_acc:0.996]
Epoch [91/120    avg_loss:0.033, val_acc:0.988]
Epoch [92/120    avg_loss:0.040, val_acc:0.992]
Epoch [93/120    avg_loss:0.030, val_acc:0.996]
Epoch [94/120    avg_loss:0.020, val_acc:0.998]
Epoch [95/120    avg_loss:0.021, val_acc:0.998]
Epoch [96/120    avg_loss:0.023, val_acc:0.998]
Epoch [97/120    avg_loss:0.027, val_acc:0.998]
Epoch [98/120    avg_loss:0.025, val_acc:0.998]
Epoch [99/120    avg_loss:0.022, val_acc:0.998]
Epoch [100/120    avg_loss:0.020, val_acc:0.998]
Epoch [101/120    avg_loss:0.033, val_acc:0.998]
Epoch [102/120    avg_loss:0.024, val_acc:0.998]
Epoch [103/120    avg_loss:0.022, val_acc:0.998]
Epoch [104/120    avg_loss:0.020, val_acc:0.998]
Epoch [105/120    avg_loss:0.021, val_acc:0.998]
Epoch [106/120    avg_loss:0.021, val_acc:0.998]
Epoch [107/120    avg_loss:0.018, val_acc:0.998]
Epoch [108/120    avg_loss:0.021, val_acc:0.998]
Epoch [109/120    avg_loss:0.023, val_acc:0.998]
Epoch [110/120    avg_loss:0.022, val_acc:0.998]
Epoch [111/120    avg_loss:0.018, val_acc:0.998]
Epoch [112/120    avg_loss:0.021, val_acc:0.998]
Epoch [113/120    avg_loss:0.016, val_acc:0.998]
Epoch [114/120    avg_loss:0.020, val_acc:0.998]
Epoch [115/120    avg_loss:0.022, val_acc:0.998]
Epoch [116/120    avg_loss:0.016, val_acc:0.998]
Epoch [117/120    avg_loss:0.019, val_acc:0.998]
Epoch [118/120    avg_loss:0.016, val_acc:0.998]
Epoch [119/120    avg_loss:0.019, val_acc:0.998]
Epoch [120/120    avg_loss:0.020, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 216  14   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.99095023 0.96860987 0.92982456 0.94039735
 1.         0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.991454495135986
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc34b6bdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.614, val_acc:0.234]
Epoch [2/120    avg_loss:2.567, val_acc:0.365]
Epoch [3/120    avg_loss:2.516, val_acc:0.427]
Epoch [4/120    avg_loss:2.467, val_acc:0.411]
Epoch [5/120    avg_loss:2.415, val_acc:0.399]
Epoch [6/120    avg_loss:2.355, val_acc:0.415]
Epoch [7/120    avg_loss:2.318, val_acc:0.438]
Epoch [8/120    avg_loss:2.242, val_acc:0.474]
Epoch [9/120    avg_loss:2.161, val_acc:0.518]
Epoch [10/120    avg_loss:2.069, val_acc:0.554]
Epoch [11/120    avg_loss:1.982, val_acc:0.641]
Epoch [12/120    avg_loss:1.859, val_acc:0.643]
Epoch [13/120    avg_loss:1.766, val_acc:0.633]
Epoch [14/120    avg_loss:1.668, val_acc:0.704]
Epoch [15/120    avg_loss:1.538, val_acc:0.736]
Epoch [16/120    avg_loss:1.444, val_acc:0.760]
Epoch [17/120    avg_loss:1.341, val_acc:0.764]
Epoch [18/120    avg_loss:1.270, val_acc:0.780]
Epoch [19/120    avg_loss:1.127, val_acc:0.768]
Epoch [20/120    avg_loss:1.010, val_acc:0.796]
Epoch [21/120    avg_loss:0.873, val_acc:0.889]
Epoch [22/120    avg_loss:0.800, val_acc:0.883]
Epoch [23/120    avg_loss:0.735, val_acc:0.907]
Epoch [24/120    avg_loss:0.688, val_acc:0.889]
Epoch [25/120    avg_loss:0.738, val_acc:0.827]
Epoch [26/120    avg_loss:0.647, val_acc:0.885]
Epoch [27/120    avg_loss:0.570, val_acc:0.897]
Epoch [28/120    avg_loss:0.575, val_acc:0.891]
Epoch [29/120    avg_loss:0.542, val_acc:0.907]
Epoch [30/120    avg_loss:0.457, val_acc:0.917]
Epoch [31/120    avg_loss:0.475, val_acc:0.798]
Epoch [32/120    avg_loss:0.465, val_acc:0.915]
Epoch [33/120    avg_loss:0.379, val_acc:0.913]
Epoch [34/120    avg_loss:0.360, val_acc:0.923]
Epoch [35/120    avg_loss:0.341, val_acc:0.938]
Epoch [36/120    avg_loss:0.304, val_acc:0.929]
Epoch [37/120    avg_loss:0.344, val_acc:0.933]
Epoch [38/120    avg_loss:0.294, val_acc:0.940]
Epoch [39/120    avg_loss:0.283, val_acc:0.925]
Epoch [40/120    avg_loss:0.321, val_acc:0.938]
Epoch [41/120    avg_loss:0.308, val_acc:0.942]
Epoch [42/120    avg_loss:0.269, val_acc:0.921]
Epoch [43/120    avg_loss:0.291, val_acc:0.952]
Epoch [44/120    avg_loss:0.275, val_acc:0.933]
Epoch [45/120    avg_loss:0.258, val_acc:0.954]
Epoch [46/120    avg_loss:0.239, val_acc:0.952]
Epoch [47/120    avg_loss:0.225, val_acc:0.944]
Epoch [48/120    avg_loss:0.227, val_acc:0.960]
Epoch [49/120    avg_loss:0.204, val_acc:0.938]
Epoch [50/120    avg_loss:0.220, val_acc:0.960]
Epoch [51/120    avg_loss:0.205, val_acc:0.964]
Epoch [52/120    avg_loss:0.170, val_acc:0.958]
Epoch [53/120    avg_loss:0.206, val_acc:0.966]
Epoch [54/120    avg_loss:0.185, val_acc:0.974]
Epoch [55/120    avg_loss:0.215, val_acc:0.935]
Epoch [56/120    avg_loss:0.272, val_acc:0.958]
Epoch [57/120    avg_loss:0.176, val_acc:0.950]
Epoch [58/120    avg_loss:0.191, val_acc:0.948]
Epoch [59/120    avg_loss:0.159, val_acc:0.964]
Epoch [60/120    avg_loss:0.179, val_acc:0.942]
Epoch [61/120    avg_loss:0.210, val_acc:0.960]
Epoch [62/120    avg_loss:0.156, val_acc:0.960]
Epoch [63/120    avg_loss:0.116, val_acc:0.972]
Epoch [64/120    avg_loss:0.117, val_acc:0.974]
Epoch [65/120    avg_loss:0.129, val_acc:0.956]
Epoch [66/120    avg_loss:0.177, val_acc:0.968]
Epoch [67/120    avg_loss:0.130, val_acc:0.972]
Epoch [68/120    avg_loss:0.095, val_acc:0.958]
Epoch [69/120    avg_loss:0.102, val_acc:0.970]
Epoch [70/120    avg_loss:0.124, val_acc:0.962]
Epoch [71/120    avg_loss:0.145, val_acc:0.966]
Epoch [72/120    avg_loss:0.135, val_acc:0.960]
Epoch [73/120    avg_loss:0.115, val_acc:0.978]
Epoch [74/120    avg_loss:0.091, val_acc:0.976]
Epoch [75/120    avg_loss:0.097, val_acc:0.974]
Epoch [76/120    avg_loss:0.079, val_acc:0.970]
Epoch [77/120    avg_loss:0.072, val_acc:0.972]
Epoch [78/120    avg_loss:0.081, val_acc:0.970]
Epoch [79/120    avg_loss:0.060, val_acc:0.972]
Epoch [80/120    avg_loss:0.110, val_acc:0.976]
Epoch [81/120    avg_loss:0.085, val_acc:0.954]
Epoch [82/120    avg_loss:0.087, val_acc:0.976]
Epoch [83/120    avg_loss:0.088, val_acc:0.968]
Epoch [84/120    avg_loss:0.058, val_acc:0.984]
Epoch [85/120    avg_loss:0.054, val_acc:0.980]
Epoch [86/120    avg_loss:0.119, val_acc:0.972]
Epoch [87/120    avg_loss:0.082, val_acc:0.976]
Epoch [88/120    avg_loss:0.075, val_acc:0.974]
Epoch [89/120    avg_loss:0.058, val_acc:0.978]
Epoch [90/120    avg_loss:0.064, val_acc:0.982]
Epoch [91/120    avg_loss:0.045, val_acc:0.984]
Epoch [92/120    avg_loss:0.038, val_acc:0.984]
Epoch [93/120    avg_loss:0.052, val_acc:0.986]
Epoch [94/120    avg_loss:0.048, val_acc:0.976]
Epoch [95/120    avg_loss:0.030, val_acc:0.984]
Epoch [96/120    avg_loss:0.030, val_acc:0.988]
Epoch [97/120    avg_loss:0.043, val_acc:0.992]
Epoch [98/120    avg_loss:0.040, val_acc:0.986]
Epoch [99/120    avg_loss:0.051, val_acc:0.988]
Epoch [100/120    avg_loss:0.061, val_acc:0.982]
Epoch [101/120    avg_loss:0.058, val_acc:0.986]
Epoch [102/120    avg_loss:0.035, val_acc:0.986]
Epoch [103/120    avg_loss:0.041, val_acc:0.988]
Epoch [104/120    avg_loss:0.026, val_acc:0.990]
Epoch [105/120    avg_loss:0.030, val_acc:0.990]
Epoch [106/120    avg_loss:0.024, val_acc:0.990]
Epoch [107/120    avg_loss:0.025, val_acc:0.996]
Epoch [108/120    avg_loss:0.020, val_acc:0.990]
Epoch [109/120    avg_loss:0.025, val_acc:0.992]
Epoch [110/120    avg_loss:0.035, val_acc:0.988]
Epoch [111/120    avg_loss:0.036, val_acc:0.984]
Epoch [112/120    avg_loss:0.020, val_acc:0.992]
Epoch [113/120    avg_loss:0.021, val_acc:0.990]
Epoch [114/120    avg_loss:0.021, val_acc:0.994]
Epoch [115/120    avg_loss:0.019, val_acc:0.992]
Epoch [116/120    avg_loss:0.037, val_acc:0.992]
Epoch [117/120    avg_loss:0.078, val_acc:0.978]
Epoch [118/120    avg_loss:0.101, val_acc:0.974]
Epoch [119/120    avg_loss:0.056, val_acc:0.992]
Epoch [120/120    avg_loss:0.028, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 204  26   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 1.         0.9977221  0.94009217 0.88391039 0.88888889
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9862312142654784
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe261e50e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.190]
Epoch [2/120    avg_loss:2.546, val_acc:0.329]
Epoch [3/120    avg_loss:2.480, val_acc:0.317]
Epoch [4/120    avg_loss:2.417, val_acc:0.310]
Epoch [5/120    avg_loss:2.342, val_acc:0.321]
Epoch [6/120    avg_loss:2.288, val_acc:0.347]
Epoch [7/120    avg_loss:2.197, val_acc:0.411]
Epoch [8/120    avg_loss:2.105, val_acc:0.498]
Epoch [9/120    avg_loss:2.052, val_acc:0.538]
Epoch [10/120    avg_loss:1.958, val_acc:0.625]
Epoch [11/120    avg_loss:1.867, val_acc:0.690]
Epoch [12/120    avg_loss:1.742, val_acc:0.742]
Epoch [13/120    avg_loss:1.637, val_acc:0.768]
Epoch [14/120    avg_loss:1.461, val_acc:0.784]
Epoch [15/120    avg_loss:1.361, val_acc:0.851]
Epoch [16/120    avg_loss:1.230, val_acc:0.833]
Epoch [17/120    avg_loss:1.116, val_acc:0.859]
Epoch [18/120    avg_loss:1.015, val_acc:0.857]
Epoch [19/120    avg_loss:0.959, val_acc:0.851]
Epoch [20/120    avg_loss:0.831, val_acc:0.883]
Epoch [21/120    avg_loss:0.748, val_acc:0.891]
Epoch [22/120    avg_loss:0.688, val_acc:0.819]
Epoch [23/120    avg_loss:0.655, val_acc:0.887]
Epoch [24/120    avg_loss:0.587, val_acc:0.815]
Epoch [25/120    avg_loss:0.636, val_acc:0.903]
Epoch [26/120    avg_loss:0.600, val_acc:0.883]
Epoch [27/120    avg_loss:0.545, val_acc:0.841]
Epoch [28/120    avg_loss:0.531, val_acc:0.891]
Epoch [29/120    avg_loss:0.468, val_acc:0.873]
Epoch [30/120    avg_loss:0.437, val_acc:0.899]
Epoch [31/120    avg_loss:0.429, val_acc:0.905]
Epoch [32/120    avg_loss:0.390, val_acc:0.913]
Epoch [33/120    avg_loss:0.447, val_acc:0.899]
Epoch [34/120    avg_loss:0.430, val_acc:0.923]
Epoch [35/120    avg_loss:0.394, val_acc:0.927]
Epoch [36/120    avg_loss:0.383, val_acc:0.919]
Epoch [37/120    avg_loss:0.383, val_acc:0.919]
Epoch [38/120    avg_loss:0.331, val_acc:0.933]
Epoch [39/120    avg_loss:0.347, val_acc:0.927]
Epoch [40/120    avg_loss:0.329, val_acc:0.927]
Epoch [41/120    avg_loss:0.298, val_acc:0.931]
Epoch [42/120    avg_loss:0.330, val_acc:0.887]
Epoch [43/120    avg_loss:0.304, val_acc:0.946]
Epoch [44/120    avg_loss:0.342, val_acc:0.931]
Epoch [45/120    avg_loss:0.286, val_acc:0.954]
Epoch [46/120    avg_loss:0.241, val_acc:0.950]
Epoch [47/120    avg_loss:0.240, val_acc:0.950]
Epoch [48/120    avg_loss:0.273, val_acc:0.954]
Epoch [49/120    avg_loss:0.295, val_acc:0.933]
Epoch [50/120    avg_loss:0.290, val_acc:0.950]
Epoch [51/120    avg_loss:0.233, val_acc:0.960]
Epoch [52/120    avg_loss:0.254, val_acc:0.958]
Epoch [53/120    avg_loss:0.226, val_acc:0.956]
Epoch [54/120    avg_loss:0.201, val_acc:0.948]
Epoch [55/120    avg_loss:0.191, val_acc:0.956]
Epoch [56/120    avg_loss:0.151, val_acc:0.966]
Epoch [57/120    avg_loss:0.195, val_acc:0.952]
Epoch [58/120    avg_loss:0.228, val_acc:0.927]
Epoch [59/120    avg_loss:0.200, val_acc:0.946]
Epoch [60/120    avg_loss:0.141, val_acc:0.950]
Epoch [61/120    avg_loss:0.178, val_acc:0.938]
Epoch [62/120    avg_loss:0.179, val_acc:0.966]
Epoch [63/120    avg_loss:0.190, val_acc:0.974]
Epoch [64/120    avg_loss:0.178, val_acc:0.962]
Epoch [65/120    avg_loss:0.202, val_acc:0.966]
Epoch [66/120    avg_loss:0.166, val_acc:0.976]
Epoch [67/120    avg_loss:0.128, val_acc:0.976]
Epoch [68/120    avg_loss:0.114, val_acc:0.966]
Epoch [69/120    avg_loss:0.142, val_acc:0.974]
Epoch [70/120    avg_loss:0.177, val_acc:0.974]
Epoch [71/120    avg_loss:0.126, val_acc:0.972]
Epoch [72/120    avg_loss:0.154, val_acc:0.972]
Epoch [73/120    avg_loss:0.115, val_acc:0.972]
Epoch [74/120    avg_loss:0.117, val_acc:0.982]
Epoch [75/120    avg_loss:0.095, val_acc:0.978]
Epoch [76/120    avg_loss:0.091, val_acc:0.984]
Epoch [77/120    avg_loss:0.132, val_acc:0.982]
Epoch [78/120    avg_loss:0.134, val_acc:0.976]
Epoch [79/120    avg_loss:0.161, val_acc:0.970]
Epoch [80/120    avg_loss:0.137, val_acc:0.978]
Epoch [81/120    avg_loss:0.088, val_acc:0.984]
Epoch [82/120    avg_loss:0.074, val_acc:0.974]
Epoch [83/120    avg_loss:0.078, val_acc:0.980]
Epoch [84/120    avg_loss:0.091, val_acc:0.984]
Epoch [85/120    avg_loss:0.095, val_acc:0.980]
Epoch [86/120    avg_loss:0.071, val_acc:0.984]
Epoch [87/120    avg_loss:0.079, val_acc:0.972]
Epoch [88/120    avg_loss:0.095, val_acc:0.980]
Epoch [89/120    avg_loss:0.110, val_acc:0.984]
Epoch [90/120    avg_loss:0.075, val_acc:0.980]
Epoch [91/120    avg_loss:0.051, val_acc:0.986]
Epoch [92/120    avg_loss:0.047, val_acc:0.980]
Epoch [93/120    avg_loss:0.073, val_acc:0.984]
Epoch [94/120    avg_loss:0.088, val_acc:0.988]
Epoch [95/120    avg_loss:0.094, val_acc:0.988]
Epoch [96/120    avg_loss:0.057, val_acc:0.980]
Epoch [97/120    avg_loss:0.047, val_acc:0.988]
Epoch [98/120    avg_loss:0.064, val_acc:0.986]
Epoch [99/120    avg_loss:0.055, val_acc:0.988]
Epoch [100/120    avg_loss:0.051, val_acc:0.980]
Epoch [101/120    avg_loss:0.104, val_acc:0.984]
Epoch [102/120    avg_loss:0.072, val_acc:0.986]
Epoch [103/120    avg_loss:0.047, val_acc:0.990]
Epoch [104/120    avg_loss:0.043, val_acc:0.988]
Epoch [105/120    avg_loss:0.032, val_acc:0.984]
Epoch [106/120    avg_loss:0.048, val_acc:0.994]
Epoch [107/120    avg_loss:0.035, val_acc:0.992]
Epoch [108/120    avg_loss:0.030, val_acc:0.990]
Epoch [109/120    avg_loss:0.021, val_acc:0.990]
Epoch [110/120    avg_loss:0.036, val_acc:0.994]
Epoch [111/120    avg_loss:0.034, val_acc:0.992]
Epoch [112/120    avg_loss:0.031, val_acc:0.990]
Epoch [113/120    avg_loss:0.021, val_acc:0.990]
Epoch [114/120    avg_loss:0.029, val_acc:0.986]
Epoch [115/120    avg_loss:0.022, val_acc:0.990]
Epoch [116/120    avg_loss:0.028, val_acc:0.988]
Epoch [117/120    avg_loss:0.023, val_acc:0.994]
Epoch [118/120    avg_loss:0.018, val_acc:0.992]
Epoch [119/120    avg_loss:0.029, val_acc:0.976]
Epoch [120/120    avg_loss:0.080, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   3 218   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  11   2   0   0   0   1   0   3   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   3   0   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.69936034115139

F1 scores:
[       nan 0.997815   0.98871332 0.97321429 0.88794926 0.86131387
 0.99516908 0.98924731 0.99611902 1.         0.99862826 1.
 0.99669967 1.        ]

Kappa:
0.9855174104933547
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f448a105e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.131]
Epoch [2/120    avg_loss:2.550, val_acc:0.224]
Epoch [3/120    avg_loss:2.494, val_acc:0.317]
Epoch [4/120    avg_loss:2.438, val_acc:0.345]
Epoch [5/120    avg_loss:2.382, val_acc:0.341]
Epoch [6/120    avg_loss:2.313, val_acc:0.357]
Epoch [7/120    avg_loss:2.259, val_acc:0.383]
Epoch [8/120    avg_loss:2.194, val_acc:0.431]
Epoch [9/120    avg_loss:2.120, val_acc:0.482]
Epoch [10/120    avg_loss:2.044, val_acc:0.528]
Epoch [11/120    avg_loss:1.967, val_acc:0.593]
Epoch [12/120    avg_loss:1.852, val_acc:0.567]
Epoch [13/120    avg_loss:1.759, val_acc:0.611]
Epoch [14/120    avg_loss:1.668, val_acc:0.655]
Epoch [15/120    avg_loss:1.504, val_acc:0.694]
Epoch [16/120    avg_loss:1.399, val_acc:0.734]
Epoch [17/120    avg_loss:1.238, val_acc:0.736]
Epoch [18/120    avg_loss:1.155, val_acc:0.802]
Epoch [19/120    avg_loss:1.010, val_acc:0.796]
Epoch [20/120    avg_loss:0.909, val_acc:0.853]
Epoch [21/120    avg_loss:0.833, val_acc:0.867]
Epoch [22/120    avg_loss:0.737, val_acc:0.859]
Epoch [23/120    avg_loss:0.689, val_acc:0.917]
Epoch [24/120    avg_loss:0.618, val_acc:0.835]
Epoch [25/120    avg_loss:0.595, val_acc:0.804]
Epoch [26/120    avg_loss:0.540, val_acc:0.889]
Epoch [27/120    avg_loss:0.460, val_acc:0.935]
Epoch [28/120    avg_loss:0.443, val_acc:0.933]
Epoch [29/120    avg_loss:0.425, val_acc:0.935]
Epoch [30/120    avg_loss:0.456, val_acc:0.929]
Epoch [31/120    avg_loss:0.447, val_acc:0.909]
Epoch [32/120    avg_loss:0.386, val_acc:0.851]
Epoch [33/120    avg_loss:0.342, val_acc:0.944]
Epoch [34/120    avg_loss:0.357, val_acc:0.944]
Epoch [35/120    avg_loss:0.297, val_acc:0.958]
Epoch [36/120    avg_loss:0.288, val_acc:0.942]
Epoch [37/120    avg_loss:0.356, val_acc:0.891]
Epoch [38/120    avg_loss:0.340, val_acc:0.946]
Epoch [39/120    avg_loss:0.283, val_acc:0.913]
Epoch [40/120    avg_loss:0.249, val_acc:0.952]
Epoch [41/120    avg_loss:0.207, val_acc:0.958]
Epoch [42/120    avg_loss:0.293, val_acc:0.933]
Epoch [43/120    avg_loss:0.262, val_acc:0.960]
Epoch [44/120    avg_loss:0.198, val_acc:0.952]
Epoch [45/120    avg_loss:0.221, val_acc:0.942]
Epoch [46/120    avg_loss:0.197, val_acc:0.942]
Epoch [47/120    avg_loss:0.208, val_acc:0.944]
Epoch [48/120    avg_loss:0.178, val_acc:0.974]
Epoch [49/120    avg_loss:0.183, val_acc:0.972]
Epoch [50/120    avg_loss:0.151, val_acc:0.972]
Epoch [51/120    avg_loss:0.136, val_acc:0.968]
Epoch [52/120    avg_loss:0.156, val_acc:0.978]
Epoch [53/120    avg_loss:0.141, val_acc:0.992]
Epoch [54/120    avg_loss:0.126, val_acc:0.986]
Epoch [55/120    avg_loss:0.100, val_acc:0.968]
Epoch [56/120    avg_loss:0.189, val_acc:0.929]
Epoch [57/120    avg_loss:0.188, val_acc:0.950]
Epoch [58/120    avg_loss:0.126, val_acc:0.980]
Epoch [59/120    avg_loss:0.076, val_acc:0.980]
Epoch [60/120    avg_loss:0.139, val_acc:0.986]
Epoch [61/120    avg_loss:0.144, val_acc:0.988]
Epoch [62/120    avg_loss:0.089, val_acc:0.992]
Epoch [63/120    avg_loss:0.131, val_acc:0.966]
Epoch [64/120    avg_loss:0.128, val_acc:0.972]
Epoch [65/120    avg_loss:0.138, val_acc:0.978]
Epoch [66/120    avg_loss:0.101, val_acc:0.970]
Epoch [67/120    avg_loss:0.098, val_acc:0.986]
Epoch [68/120    avg_loss:0.073, val_acc:0.988]
Epoch [69/120    avg_loss:0.066, val_acc:0.982]
Epoch [70/120    avg_loss:0.079, val_acc:0.990]
Epoch [71/120    avg_loss:0.061, val_acc:0.980]
Epoch [72/120    avg_loss:0.066, val_acc:0.990]
Epoch [73/120    avg_loss:0.051, val_acc:0.990]
Epoch [74/120    avg_loss:0.089, val_acc:0.968]
Epoch [75/120    avg_loss:0.081, val_acc:0.972]
Epoch [76/120    avg_loss:0.075, val_acc:0.990]
Epoch [77/120    avg_loss:0.054, val_acc:0.992]
Epoch [78/120    avg_loss:0.040, val_acc:0.994]
Epoch [79/120    avg_loss:0.050, val_acc:0.992]
Epoch [80/120    avg_loss:0.041, val_acc:0.990]
Epoch [81/120    avg_loss:0.038, val_acc:0.992]
Epoch [82/120    avg_loss:0.042, val_acc:0.992]
Epoch [83/120    avg_loss:0.043, val_acc:0.992]
Epoch [84/120    avg_loss:0.029, val_acc:0.992]
Epoch [85/120    avg_loss:0.036, val_acc:0.992]
Epoch [86/120    avg_loss:0.031, val_acc:0.992]
Epoch [87/120    avg_loss:0.035, val_acc:0.994]
Epoch [88/120    avg_loss:0.031, val_acc:0.994]
Epoch [89/120    avg_loss:0.036, val_acc:0.992]
Epoch [90/120    avg_loss:0.030, val_acc:0.994]
Epoch [91/120    avg_loss:0.027, val_acc:0.994]
Epoch [92/120    avg_loss:0.032, val_acc:0.994]
Epoch [93/120    avg_loss:0.025, val_acc:0.996]
Epoch [94/120    avg_loss:0.028, val_acc:0.996]
Epoch [95/120    avg_loss:0.026, val_acc:0.994]
Epoch [96/120    avg_loss:0.028, val_acc:0.994]
Epoch [97/120    avg_loss:0.027, val_acc:0.996]
Epoch [98/120    avg_loss:0.024, val_acc:0.996]
Epoch [99/120    avg_loss:0.034, val_acc:0.994]
Epoch [100/120    avg_loss:0.023, val_acc:0.994]
Epoch [101/120    avg_loss:0.025, val_acc:0.994]
Epoch [102/120    avg_loss:0.025, val_acc:0.994]
Epoch [103/120    avg_loss:0.026, val_acc:0.994]
Epoch [104/120    avg_loss:0.030, val_acc:0.998]
Epoch [105/120    avg_loss:0.026, val_acc:0.996]
Epoch [106/120    avg_loss:0.023, val_acc:0.998]
Epoch [107/120    avg_loss:0.022, val_acc:0.996]
Epoch [108/120    avg_loss:0.027, val_acc:0.996]
Epoch [109/120    avg_loss:0.024, val_acc:0.994]
Epoch [110/120    avg_loss:0.023, val_acc:0.994]
Epoch [111/120    avg_loss:0.024, val_acc:0.994]
Epoch [112/120    avg_loss:0.023, val_acc:0.996]
Epoch [113/120    avg_loss:0.025, val_acc:0.996]
Epoch [114/120    avg_loss:0.028, val_acc:0.996]
Epoch [115/120    avg_loss:0.022, val_acc:0.994]
Epoch [116/120    avg_loss:0.022, val_acc:0.994]
Epoch [117/120    avg_loss:0.025, val_acc:0.996]
Epoch [118/120    avg_loss:0.027, val_acc:0.994]
Epoch [119/120    avg_loss:0.027, val_acc:0.996]
Epoch [120/120    avg_loss:0.023, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   3   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99926954 1.         0.99122807 0.93991416 0.91103203
 0.99757869 0.99470899 1.         1.         1.         0.99472296
 0.99556541 1.        ]

Kappa:
0.9919291081527851
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb2a6de7da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.659, val_acc:0.113]
Epoch [2/120    avg_loss:2.590, val_acc:0.315]
Epoch [3/120    avg_loss:2.519, val_acc:0.440]
Epoch [4/120    avg_loss:2.453, val_acc:0.462]
Epoch [5/120    avg_loss:2.380, val_acc:0.466]
Epoch [6/120    avg_loss:2.310, val_acc:0.462]
Epoch [7/120    avg_loss:2.234, val_acc:0.446]
Epoch [8/120    avg_loss:2.142, val_acc:0.500]
Epoch [9/120    avg_loss:2.059, val_acc:0.528]
Epoch [10/120    avg_loss:1.958, val_acc:0.554]
Epoch [11/120    avg_loss:1.835, val_acc:0.579]
Epoch [12/120    avg_loss:1.700, val_acc:0.667]
Epoch [13/120    avg_loss:1.563, val_acc:0.675]
Epoch [14/120    avg_loss:1.452, val_acc:0.663]
Epoch [15/120    avg_loss:1.305, val_acc:0.744]
Epoch [16/120    avg_loss:1.166, val_acc:0.790]
Epoch [17/120    avg_loss:1.057, val_acc:0.823]
Epoch [18/120    avg_loss:1.009, val_acc:0.804]
Epoch [19/120    avg_loss:0.860, val_acc:0.905]
Epoch [20/120    avg_loss:0.825, val_acc:0.903]
Epoch [21/120    avg_loss:0.717, val_acc:0.843]
Epoch [22/120    avg_loss:0.698, val_acc:0.905]
Epoch [23/120    avg_loss:0.662, val_acc:0.905]
Epoch [24/120    avg_loss:0.581, val_acc:0.921]
Epoch [25/120    avg_loss:0.553, val_acc:0.927]
Epoch [26/120    avg_loss:0.497, val_acc:0.881]
Epoch [27/120    avg_loss:0.480, val_acc:0.919]
Epoch [28/120    avg_loss:0.451, val_acc:0.935]
Epoch [29/120    avg_loss:0.409, val_acc:0.942]
Epoch [30/120    avg_loss:0.415, val_acc:0.935]
Epoch [31/120    avg_loss:0.381, val_acc:0.927]
Epoch [32/120    avg_loss:0.329, val_acc:0.942]
Epoch [33/120    avg_loss:0.294, val_acc:0.923]
Epoch [34/120    avg_loss:0.353, val_acc:0.931]
Epoch [35/120    avg_loss:0.298, val_acc:0.956]
Epoch [36/120    avg_loss:0.291, val_acc:0.958]
Epoch [37/120    avg_loss:0.316, val_acc:0.931]
Epoch [38/120    avg_loss:0.331, val_acc:0.940]
Epoch [39/120    avg_loss:0.276, val_acc:0.931]
Epoch [40/120    avg_loss:0.235, val_acc:0.946]
Epoch [41/120    avg_loss:0.221, val_acc:0.962]
Epoch [42/120    avg_loss:0.236, val_acc:0.946]
Epoch [43/120    avg_loss:0.197, val_acc:0.968]
Epoch [44/120    avg_loss:0.169, val_acc:0.974]
Epoch [45/120    avg_loss:0.301, val_acc:0.960]
Epoch [46/120    avg_loss:0.192, val_acc:0.952]
Epoch [47/120    avg_loss:0.163, val_acc:0.970]
Epoch [48/120    avg_loss:0.179, val_acc:0.962]
Epoch [49/120    avg_loss:0.200, val_acc:0.970]
Epoch [50/120    avg_loss:0.143, val_acc:0.974]
Epoch [51/120    avg_loss:0.128, val_acc:0.980]
Epoch [52/120    avg_loss:0.152, val_acc:0.938]
Epoch [53/120    avg_loss:0.171, val_acc:0.984]
Epoch [54/120    avg_loss:0.188, val_acc:0.958]
Epoch [55/120    avg_loss:0.146, val_acc:0.980]
Epoch [56/120    avg_loss:0.116, val_acc:0.984]
Epoch [57/120    avg_loss:0.117, val_acc:0.988]
Epoch [58/120    avg_loss:0.083, val_acc:0.988]
Epoch [59/120    avg_loss:0.088, val_acc:0.988]
Epoch [60/120    avg_loss:0.089, val_acc:0.968]
Epoch [61/120    avg_loss:0.089, val_acc:0.994]
Epoch [62/120    avg_loss:0.089, val_acc:0.956]
Epoch [63/120    avg_loss:0.091, val_acc:0.972]
Epoch [64/120    avg_loss:0.093, val_acc:0.972]
Epoch [65/120    avg_loss:0.082, val_acc:0.988]
Epoch [66/120    avg_loss:0.089, val_acc:0.990]
Epoch [67/120    avg_loss:0.075, val_acc:0.986]
Epoch [68/120    avg_loss:0.054, val_acc:0.994]
Epoch [69/120    avg_loss:0.050, val_acc:0.984]
Epoch [70/120    avg_loss:0.060, val_acc:0.986]
Epoch [71/120    avg_loss:0.071, val_acc:0.992]
Epoch [72/120    avg_loss:0.065, val_acc:0.992]
Epoch [73/120    avg_loss:0.060, val_acc:0.990]
Epoch [74/120    avg_loss:0.053, val_acc:0.996]
Epoch [75/120    avg_loss:0.094, val_acc:0.964]
Epoch [76/120    avg_loss:0.149, val_acc:0.972]
Epoch [77/120    avg_loss:0.141, val_acc:0.986]
Epoch [78/120    avg_loss:0.077, val_acc:0.982]
Epoch [79/120    avg_loss:0.074, val_acc:1.000]
Epoch [80/120    avg_loss:0.045, val_acc:0.984]
Epoch [81/120    avg_loss:0.067, val_acc:0.984]
Epoch [82/120    avg_loss:0.098, val_acc:0.990]
Epoch [83/120    avg_loss:0.048, val_acc:0.996]
Epoch [84/120    avg_loss:0.047, val_acc:0.992]
Epoch [85/120    avg_loss:0.048, val_acc:0.990]
Epoch [86/120    avg_loss:0.073, val_acc:0.998]
Epoch [87/120    avg_loss:0.041, val_acc:0.990]
Epoch [88/120    avg_loss:0.035, val_acc:0.992]
Epoch [89/120    avg_loss:0.031, val_acc:0.992]
Epoch [90/120    avg_loss:0.033, val_acc:0.992]
Epoch [91/120    avg_loss:0.031, val_acc:0.948]
Epoch [92/120    avg_loss:0.072, val_acc:0.988]
Epoch [93/120    avg_loss:0.045, val_acc:1.000]
Epoch [94/120    avg_loss:0.030, val_acc:0.998]
Epoch [95/120    avg_loss:0.024, val_acc:0.998]
Epoch [96/120    avg_loss:0.025, val_acc:0.998]
Epoch [97/120    avg_loss:0.022, val_acc:0.996]
Epoch [98/120    avg_loss:0.019, val_acc:0.996]
Epoch [99/120    avg_loss:0.018, val_acc:0.996]
Epoch [100/120    avg_loss:0.017, val_acc:0.996]
Epoch [101/120    avg_loss:0.022, val_acc:0.996]
Epoch [102/120    avg_loss:0.020, val_acc:0.994]
Epoch [103/120    avg_loss:0.017, val_acc:0.996]
Epoch [104/120    avg_loss:0.018, val_acc:0.996]
Epoch [105/120    avg_loss:0.015, val_acc:0.996]
Epoch [106/120    avg_loss:0.017, val_acc:0.996]
Epoch [107/120    avg_loss:0.018, val_acc:0.996]
Epoch [108/120    avg_loss:0.020, val_acc:0.996]
Epoch [109/120    avg_loss:0.015, val_acc:0.996]
Epoch [110/120    avg_loss:0.016, val_acc:0.996]
Epoch [111/120    avg_loss:0.017, val_acc:0.996]
Epoch [112/120    avg_loss:0.016, val_acc:0.996]
Epoch [113/120    avg_loss:0.017, val_acc:0.996]
Epoch [114/120    avg_loss:0.018, val_acc:0.996]
Epoch [115/120    avg_loss:0.018, val_acc:0.996]
Epoch [116/120    avg_loss:0.018, val_acc:0.996]
Epoch [117/120    avg_loss:0.020, val_acc:0.996]
Epoch [118/120    avg_loss:0.014, val_acc:0.996]
Epoch [119/120    avg_loss:0.022, val_acc:0.996]
Epoch [120/120    avg_loss:0.018, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99926954 0.99545455 0.98230088 0.94922737 0.94983278
 0.99757869 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9938283917362803
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0830891e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.637, val_acc:0.263]
Epoch [2/120    avg_loss:2.573, val_acc:0.201]
Epoch [3/120    avg_loss:2.511, val_acc:0.390]
Epoch [4/120    avg_loss:2.459, val_acc:0.419]
Epoch [5/120    avg_loss:2.404, val_acc:0.450]
Epoch [6/120    avg_loss:2.347, val_acc:0.548]
Epoch [7/120    avg_loss:2.285, val_acc:0.619]
Epoch [8/120    avg_loss:2.213, val_acc:0.631]
Epoch [9/120    avg_loss:2.120, val_acc:0.710]
Epoch [10/120    avg_loss:2.021, val_acc:0.704]
Epoch [11/120    avg_loss:1.912, val_acc:0.760]
Epoch [12/120    avg_loss:1.807, val_acc:0.746]
Epoch [13/120    avg_loss:1.661, val_acc:0.787]
Epoch [14/120    avg_loss:1.569, val_acc:0.825]
Epoch [15/120    avg_loss:1.430, val_acc:0.815]
Epoch [16/120    avg_loss:1.299, val_acc:0.846]
Epoch [17/120    avg_loss:1.195, val_acc:0.865]
Epoch [18/120    avg_loss:1.070, val_acc:0.900]
Epoch [19/120    avg_loss:0.955, val_acc:0.856]
Epoch [20/120    avg_loss:0.899, val_acc:0.894]
Epoch [21/120    avg_loss:0.824, val_acc:0.890]
Epoch [22/120    avg_loss:0.776, val_acc:0.887]
Epoch [23/120    avg_loss:0.720, val_acc:0.902]
Epoch [24/120    avg_loss:0.673, val_acc:0.915]
Epoch [25/120    avg_loss:0.668, val_acc:0.919]
Epoch [26/120    avg_loss:0.596, val_acc:0.935]
Epoch [27/120    avg_loss:0.537, val_acc:0.948]
Epoch [28/120    avg_loss:0.499, val_acc:0.933]
Epoch [29/120    avg_loss:0.461, val_acc:0.942]
Epoch [30/120    avg_loss:0.482, val_acc:0.935]
Epoch [31/120    avg_loss:0.428, val_acc:0.948]
Epoch [32/120    avg_loss:0.398, val_acc:0.952]
Epoch [33/120    avg_loss:0.399, val_acc:0.948]
Epoch [34/120    avg_loss:0.360, val_acc:0.952]
Epoch [35/120    avg_loss:0.343, val_acc:0.952]
Epoch [36/120    avg_loss:0.310, val_acc:0.938]
Epoch [37/120    avg_loss:0.318, val_acc:0.942]
Epoch [38/120    avg_loss:0.326, val_acc:0.952]
Epoch [39/120    avg_loss:0.400, val_acc:0.915]
Epoch [40/120    avg_loss:0.388, val_acc:0.958]
Epoch [41/120    avg_loss:0.356, val_acc:0.958]
Epoch [42/120    avg_loss:0.360, val_acc:0.912]
Epoch [43/120    avg_loss:0.313, val_acc:0.896]
Epoch [44/120    avg_loss:0.286, val_acc:0.960]
Epoch [45/120    avg_loss:0.216, val_acc:0.960]
Epoch [46/120    avg_loss:0.278, val_acc:0.938]
Epoch [47/120    avg_loss:0.313, val_acc:0.933]
Epoch [48/120    avg_loss:0.230, val_acc:0.967]
Epoch [49/120    avg_loss:0.208, val_acc:0.977]
Epoch [50/120    avg_loss:0.227, val_acc:0.965]
Epoch [51/120    avg_loss:0.187, val_acc:0.967]
Epoch [52/120    avg_loss:0.194, val_acc:0.973]
Epoch [53/120    avg_loss:0.186, val_acc:0.965]
Epoch [54/120    avg_loss:0.173, val_acc:0.973]
Epoch [55/120    avg_loss:0.138, val_acc:0.971]
Epoch [56/120    avg_loss:0.122, val_acc:0.988]
Epoch [57/120    avg_loss:0.128, val_acc:0.952]
Epoch [58/120    avg_loss:0.173, val_acc:0.958]
Epoch [59/120    avg_loss:0.199, val_acc:0.960]
Epoch [60/120    avg_loss:0.210, val_acc:0.960]
Epoch [61/120    avg_loss:0.157, val_acc:0.983]
Epoch [62/120    avg_loss:0.106, val_acc:0.988]
Epoch [63/120    avg_loss:0.133, val_acc:0.975]
Epoch [64/120    avg_loss:0.132, val_acc:0.988]
Epoch [65/120    avg_loss:0.111, val_acc:0.988]
Epoch [66/120    avg_loss:0.107, val_acc:0.981]
Epoch [67/120    avg_loss:0.092, val_acc:0.985]
Epoch [68/120    avg_loss:0.180, val_acc:0.967]
Epoch [69/120    avg_loss:0.165, val_acc:0.958]
Epoch [70/120    avg_loss:0.117, val_acc:0.967]
Epoch [71/120    avg_loss:0.129, val_acc:0.979]
Epoch [72/120    avg_loss:0.087, val_acc:0.977]
Epoch [73/120    avg_loss:0.106, val_acc:0.977]
Epoch [74/120    avg_loss:0.159, val_acc:0.946]
Epoch [75/120    avg_loss:0.156, val_acc:0.975]
Epoch [76/120    avg_loss:0.163, val_acc:0.950]
Epoch [77/120    avg_loss:0.093, val_acc:0.994]
Epoch [78/120    avg_loss:0.131, val_acc:0.971]
Epoch [79/120    avg_loss:0.094, val_acc:0.985]
Epoch [80/120    avg_loss:0.076, val_acc:0.981]
Epoch [81/120    avg_loss:0.066, val_acc:0.992]
Epoch [82/120    avg_loss:0.056, val_acc:0.994]
Epoch [83/120    avg_loss:0.062, val_acc:0.994]
Epoch [84/120    avg_loss:0.041, val_acc:0.985]
Epoch [85/120    avg_loss:0.049, val_acc:0.990]
Epoch [86/120    avg_loss:0.050, val_acc:0.988]
Epoch [87/120    avg_loss:0.047, val_acc:0.994]
Epoch [88/120    avg_loss:0.044, val_acc:0.990]
Epoch [89/120    avg_loss:0.091, val_acc:0.963]
Epoch [90/120    avg_loss:0.065, val_acc:0.994]
Epoch [91/120    avg_loss:0.060, val_acc:0.990]
Epoch [92/120    avg_loss:0.034, val_acc:0.990]
Epoch [93/120    avg_loss:0.038, val_acc:0.994]
Epoch [94/120    avg_loss:0.041, val_acc:0.988]
Epoch [95/120    avg_loss:0.037, val_acc:0.996]
Epoch [96/120    avg_loss:0.028, val_acc:0.994]
Epoch [97/120    avg_loss:0.047, val_acc:0.994]
Epoch [98/120    avg_loss:0.050, val_acc:0.979]
Epoch [99/120    avg_loss:0.058, val_acc:0.992]
Epoch [100/120    avg_loss:0.049, val_acc:0.992]
Epoch [101/120    avg_loss:0.042, val_acc:0.985]
Epoch [102/120    avg_loss:0.036, val_acc:0.994]
Epoch [103/120    avg_loss:0.036, val_acc:0.998]
Epoch [104/120    avg_loss:0.068, val_acc:0.971]
Epoch [105/120    avg_loss:0.055, val_acc:0.990]
Epoch [106/120    avg_loss:0.057, val_acc:0.990]
Epoch [107/120    avg_loss:0.065, val_acc:0.990]
Epoch [108/120    avg_loss:0.036, val_acc:0.996]
Epoch [109/120    avg_loss:0.031, val_acc:0.994]
Epoch [110/120    avg_loss:0.021, val_acc:0.990]
Epoch [111/120    avg_loss:0.018, val_acc:0.994]
Epoch [112/120    avg_loss:0.020, val_acc:0.996]
Epoch [113/120    avg_loss:0.018, val_acc:0.994]
Epoch [114/120    avg_loss:0.022, val_acc:0.990]
Epoch [115/120    avg_loss:0.021, val_acc:0.994]
Epoch [116/120    avg_loss:0.029, val_acc:0.990]
Epoch [117/120    avg_loss:0.017, val_acc:0.992]
Epoch [118/120    avg_loss:0.015, val_acc:0.992]
Epoch [119/120    avg_loss:0.012, val_acc:0.996]
Epoch [120/120    avg_loss:0.014, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 1.         0.99319728 0.99563319 0.96875    0.95973154
 1.         0.98378378 1.         1.         1.         0.98950131
 0.99109131 1.        ]

Kappa:
0.9940657324518395
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b88825e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.125]
Epoch [2/120    avg_loss:2.552, val_acc:0.369]
Epoch [3/120    avg_loss:2.468, val_acc:0.442]
Epoch [4/120    avg_loss:2.381, val_acc:0.517]
Epoch [5/120    avg_loss:2.294, val_acc:0.581]
Epoch [6/120    avg_loss:2.190, val_acc:0.571]
Epoch [7/120    avg_loss:2.083, val_acc:0.615]
Epoch [8/120    avg_loss:1.976, val_acc:0.621]
Epoch [9/120    avg_loss:1.884, val_acc:0.667]
Epoch [10/120    avg_loss:1.760, val_acc:0.692]
Epoch [11/120    avg_loss:1.639, val_acc:0.752]
Epoch [12/120    avg_loss:1.516, val_acc:0.744]
Epoch [13/120    avg_loss:1.391, val_acc:0.760]
Epoch [14/120    avg_loss:1.268, val_acc:0.787]
Epoch [15/120    avg_loss:1.124, val_acc:0.802]
Epoch [16/120    avg_loss:1.004, val_acc:0.773]
Epoch [17/120    avg_loss:0.949, val_acc:0.800]
Epoch [18/120    avg_loss:0.873, val_acc:0.896]
Epoch [19/120    avg_loss:0.812, val_acc:0.892]
Epoch [20/120    avg_loss:0.694, val_acc:0.873]
Epoch [21/120    avg_loss:0.665, val_acc:0.833]
Epoch [22/120    avg_loss:0.704, val_acc:0.896]
Epoch [23/120    avg_loss:0.619, val_acc:0.833]
Epoch [24/120    avg_loss:0.589, val_acc:0.919]
Epoch [25/120    avg_loss:0.532, val_acc:0.896]
Epoch [26/120    avg_loss:0.551, val_acc:0.910]
Epoch [27/120    avg_loss:0.485, val_acc:0.898]
Epoch [28/120    avg_loss:0.429, val_acc:0.927]
Epoch [29/120    avg_loss:0.403, val_acc:0.904]
Epoch [30/120    avg_loss:0.449, val_acc:0.873]
Epoch [31/120    avg_loss:0.417, val_acc:0.923]
Epoch [32/120    avg_loss:0.465, val_acc:0.915]
Epoch [33/120    avg_loss:0.425, val_acc:0.883]
Epoch [34/120    avg_loss:0.391, val_acc:0.923]
Epoch [35/120    avg_loss:0.329, val_acc:0.946]
Epoch [36/120    avg_loss:0.316, val_acc:0.948]
Epoch [37/120    avg_loss:0.285, val_acc:0.944]
Epoch [38/120    avg_loss:0.291, val_acc:0.944]
Epoch [39/120    avg_loss:0.288, val_acc:0.954]
Epoch [40/120    avg_loss:0.249, val_acc:0.963]
Epoch [41/120    avg_loss:0.230, val_acc:0.965]
Epoch [42/120    avg_loss:0.288, val_acc:0.967]
Epoch [43/120    avg_loss:0.276, val_acc:0.969]
Epoch [44/120    avg_loss:0.189, val_acc:0.956]
Epoch [45/120    avg_loss:0.229, val_acc:0.954]
Epoch [46/120    avg_loss:0.223, val_acc:0.973]
Epoch [47/120    avg_loss:0.200, val_acc:0.973]
Epoch [48/120    avg_loss:0.219, val_acc:0.963]
Epoch [49/120    avg_loss:0.191, val_acc:0.971]
Epoch [50/120    avg_loss:0.212, val_acc:0.979]
Epoch [51/120    avg_loss:0.163, val_acc:0.965]
Epoch [52/120    avg_loss:0.189, val_acc:0.958]
Epoch [53/120    avg_loss:0.211, val_acc:0.969]
Epoch [54/120    avg_loss:0.145, val_acc:0.971]
Epoch [55/120    avg_loss:0.152, val_acc:0.981]
Epoch [56/120    avg_loss:0.144, val_acc:0.973]
Epoch [57/120    avg_loss:0.141, val_acc:0.985]
Epoch [58/120    avg_loss:0.107, val_acc:0.969]
Epoch [59/120    avg_loss:0.187, val_acc:0.975]
Epoch [60/120    avg_loss:0.138, val_acc:0.965]
Epoch [61/120    avg_loss:0.115, val_acc:0.977]
Epoch [62/120    avg_loss:0.231, val_acc:0.956]
Epoch [63/120    avg_loss:0.130, val_acc:0.985]
Epoch [64/120    avg_loss:0.092, val_acc:0.992]
Epoch [65/120    avg_loss:0.108, val_acc:0.988]
Epoch [66/120    avg_loss:0.080, val_acc:0.927]
Epoch [67/120    avg_loss:0.159, val_acc:0.990]
Epoch [68/120    avg_loss:0.081, val_acc:0.988]
Epoch [69/120    avg_loss:0.060, val_acc:0.994]
Epoch [70/120    avg_loss:0.086, val_acc:0.988]
Epoch [71/120    avg_loss:0.082, val_acc:0.990]
Epoch [72/120    avg_loss:0.071, val_acc:0.998]
Epoch [73/120    avg_loss:0.086, val_acc:0.994]
Epoch [74/120    avg_loss:0.074, val_acc:0.992]
Epoch [75/120    avg_loss:0.070, val_acc:0.990]
Epoch [76/120    avg_loss:0.087, val_acc:0.994]
Epoch [77/120    avg_loss:0.056, val_acc:0.994]
Epoch [78/120    avg_loss:0.042, val_acc:0.994]
Epoch [79/120    avg_loss:0.071, val_acc:0.977]
Epoch [80/120    avg_loss:0.086, val_acc:0.998]
Epoch [81/120    avg_loss:0.052, val_acc:0.996]
Epoch [82/120    avg_loss:0.045, val_acc:0.992]
Epoch [83/120    avg_loss:0.046, val_acc:0.998]
Epoch [84/120    avg_loss:0.038, val_acc:0.994]
Epoch [85/120    avg_loss:0.031, val_acc:0.996]
Epoch [86/120    avg_loss:0.046, val_acc:0.996]
Epoch [87/120    avg_loss:0.042, val_acc:0.994]
Epoch [88/120    avg_loss:0.051, val_acc:0.992]
Epoch [89/120    avg_loss:0.039, val_acc:0.994]
Epoch [90/120    avg_loss:0.021, val_acc:0.996]
Epoch [91/120    avg_loss:0.024, val_acc:0.992]
Epoch [92/120    avg_loss:0.021, val_acc:0.998]
Epoch [93/120    avg_loss:0.021, val_acc:0.996]
Epoch [94/120    avg_loss:0.021, val_acc:0.990]
Epoch [95/120    avg_loss:0.113, val_acc:0.973]
Epoch [96/120    avg_loss:0.141, val_acc:0.983]
Epoch [97/120    avg_loss:0.158, val_acc:0.998]
Epoch [98/120    avg_loss:0.071, val_acc:0.958]
Epoch [99/120    avg_loss:0.079, val_acc:0.994]
Epoch [100/120    avg_loss:0.035, val_acc:0.992]
Epoch [101/120    avg_loss:0.025, val_acc:0.998]
Epoch [102/120    avg_loss:0.041, val_acc:0.985]
Epoch [103/120    avg_loss:0.042, val_acc:0.996]
Epoch [104/120    avg_loss:0.030, val_acc:0.988]
Epoch [105/120    avg_loss:0.069, val_acc:0.994]
Epoch [106/120    avg_loss:0.040, val_acc:0.992]
Epoch [107/120    avg_loss:0.027, val_acc:0.994]
Epoch [108/120    avg_loss:0.041, val_acc:0.996]
Epoch [109/120    avg_loss:0.029, val_acc:0.996]
Epoch [110/120    avg_loss:0.040, val_acc:0.992]
Epoch [111/120    avg_loss:0.026, val_acc:0.996]
Epoch [112/120    avg_loss:0.021, val_acc:0.996]
Epoch [113/120    avg_loss:0.020, val_acc:0.998]
Epoch [114/120    avg_loss:0.013, val_acc:0.998]
Epoch [115/120    avg_loss:0.017, val_acc:0.998]
Epoch [116/120    avg_loss:0.020, val_acc:0.998]
Epoch [117/120    avg_loss:0.022, val_acc:0.998]
Epoch [118/120    avg_loss:0.015, val_acc:0.998]
Epoch [119/120    avg_loss:0.010, val_acc:0.998]
Epoch [120/120    avg_loss:0.011, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 1.         1.         1.         0.96375267 0.93818182
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9959642640210569
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f58af7c0dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.615, val_acc:0.202]
Epoch [2/120    avg_loss:2.562, val_acc:0.349]
Epoch [3/120    avg_loss:2.508, val_acc:0.363]
Epoch [4/120    avg_loss:2.454, val_acc:0.367]
Epoch [5/120    avg_loss:2.390, val_acc:0.389]
Epoch [6/120    avg_loss:2.340, val_acc:0.468]
Epoch [7/120    avg_loss:2.275, val_acc:0.488]
Epoch [8/120    avg_loss:2.217, val_acc:0.536]
Epoch [9/120    avg_loss:2.135, val_acc:0.544]
Epoch [10/120    avg_loss:2.057, val_acc:0.532]
Epoch [11/120    avg_loss:1.967, val_acc:0.554]
Epoch [12/120    avg_loss:1.865, val_acc:0.603]
Epoch [13/120    avg_loss:1.747, val_acc:0.633]
Epoch [14/120    avg_loss:1.621, val_acc:0.700]
Epoch [15/120    avg_loss:1.492, val_acc:0.746]
Epoch [16/120    avg_loss:1.378, val_acc:0.736]
Epoch [17/120    avg_loss:1.283, val_acc:0.806]
Epoch [18/120    avg_loss:1.160, val_acc:0.853]
Epoch [19/120    avg_loss:1.076, val_acc:0.869]
Epoch [20/120    avg_loss:0.954, val_acc:0.849]
Epoch [21/120    avg_loss:0.917, val_acc:0.841]
Epoch [22/120    avg_loss:0.798, val_acc:0.875]
Epoch [23/120    avg_loss:0.723, val_acc:0.887]
Epoch [24/120    avg_loss:0.652, val_acc:0.907]
Epoch [25/120    avg_loss:0.593, val_acc:0.921]
Epoch [26/120    avg_loss:0.577, val_acc:0.897]
Epoch [27/120    avg_loss:0.547, val_acc:0.901]
Epoch [28/120    avg_loss:0.478, val_acc:0.903]
Epoch [29/120    avg_loss:0.470, val_acc:0.871]
Epoch [30/120    avg_loss:0.419, val_acc:0.923]
Epoch [31/120    avg_loss:0.387, val_acc:0.942]
Epoch [32/120    avg_loss:0.352, val_acc:0.933]
Epoch [33/120    avg_loss:0.393, val_acc:0.935]
Epoch [34/120    avg_loss:0.340, val_acc:0.925]
Epoch [35/120    avg_loss:0.312, val_acc:0.927]
Epoch [36/120    avg_loss:0.319, val_acc:0.942]
Epoch [37/120    avg_loss:0.295, val_acc:0.881]
Epoch [38/120    avg_loss:0.274, val_acc:0.946]
Epoch [39/120    avg_loss:0.241, val_acc:0.927]
Epoch [40/120    avg_loss:0.214, val_acc:0.956]
Epoch [41/120    avg_loss:0.232, val_acc:0.940]
Epoch [42/120    avg_loss:0.223, val_acc:0.942]
Epoch [43/120    avg_loss:0.216, val_acc:0.964]
Epoch [44/120    avg_loss:0.154, val_acc:0.968]
Epoch [45/120    avg_loss:0.157, val_acc:0.958]
Epoch [46/120    avg_loss:0.166, val_acc:0.913]
Epoch [47/120    avg_loss:0.198, val_acc:0.925]
Epoch [48/120    avg_loss:0.147, val_acc:0.954]
Epoch [49/120    avg_loss:0.184, val_acc:0.948]
Epoch [50/120    avg_loss:0.203, val_acc:0.958]
Epoch [51/120    avg_loss:0.132, val_acc:0.962]
Epoch [52/120    avg_loss:0.206, val_acc:0.929]
Epoch [53/120    avg_loss:0.166, val_acc:0.915]
Epoch [54/120    avg_loss:0.157, val_acc:0.958]
Epoch [55/120    avg_loss:0.122, val_acc:0.970]
Epoch [56/120    avg_loss:0.106, val_acc:0.946]
Epoch [57/120    avg_loss:0.097, val_acc:0.958]
Epoch [58/120    avg_loss:0.089, val_acc:0.974]
Epoch [59/120    avg_loss:0.081, val_acc:0.974]
Epoch [60/120    avg_loss:0.085, val_acc:0.978]
Epoch [61/120    avg_loss:0.095, val_acc:0.972]
Epoch [62/120    avg_loss:0.159, val_acc:0.935]
Epoch [63/120    avg_loss:0.096, val_acc:0.976]
Epoch [64/120    avg_loss:0.076, val_acc:0.978]
Epoch [65/120    avg_loss:0.061, val_acc:0.982]
Epoch [66/120    avg_loss:0.053, val_acc:0.980]
Epoch [67/120    avg_loss:0.057, val_acc:0.972]
Epoch [68/120    avg_loss:0.053, val_acc:0.984]
Epoch [69/120    avg_loss:0.074, val_acc:0.968]
Epoch [70/120    avg_loss:0.055, val_acc:0.986]
Epoch [71/120    avg_loss:0.065, val_acc:0.984]
Epoch [72/120    avg_loss:0.076, val_acc:0.970]
Epoch [73/120    avg_loss:0.056, val_acc:0.974]
Epoch [74/120    avg_loss:0.084, val_acc:0.972]
Epoch [75/120    avg_loss:0.055, val_acc:0.984]
Epoch [76/120    avg_loss:0.041, val_acc:0.976]
Epoch [77/120    avg_loss:0.036, val_acc:0.980]
Epoch [78/120    avg_loss:0.043, val_acc:0.984]
Epoch [79/120    avg_loss:0.043, val_acc:0.958]
Epoch [80/120    avg_loss:0.118, val_acc:0.982]
Epoch [81/120    avg_loss:0.053, val_acc:0.966]
Epoch [82/120    avg_loss:0.037, val_acc:0.984]
Epoch [83/120    avg_loss:0.040, val_acc:0.982]
Epoch [84/120    avg_loss:0.029, val_acc:0.982]
Epoch [85/120    avg_loss:0.025, val_acc:0.984]
Epoch [86/120    avg_loss:0.029, val_acc:0.986]
Epoch [87/120    avg_loss:0.028, val_acc:0.984]
Epoch [88/120    avg_loss:0.024, val_acc:0.986]
Epoch [89/120    avg_loss:0.025, val_acc:0.990]
Epoch [90/120    avg_loss:0.026, val_acc:0.984]
Epoch [91/120    avg_loss:0.025, val_acc:0.986]
Epoch [92/120    avg_loss:0.022, val_acc:0.988]
Epoch [93/120    avg_loss:0.024, val_acc:0.990]
Epoch [94/120    avg_loss:0.023, val_acc:0.988]
Epoch [95/120    avg_loss:0.026, val_acc:0.990]
Epoch [96/120    avg_loss:0.022, val_acc:0.988]
Epoch [97/120    avg_loss:0.023, val_acc:0.986]
Epoch [98/120    avg_loss:0.023, val_acc:0.990]
Epoch [99/120    avg_loss:0.021, val_acc:0.990]
Epoch [100/120    avg_loss:0.020, val_acc:0.990]
Epoch [101/120    avg_loss:0.023, val_acc:0.990]
Epoch [102/120    avg_loss:0.022, val_acc:0.988]
Epoch [103/120    avg_loss:0.023, val_acc:0.988]
Epoch [104/120    avg_loss:0.028, val_acc:0.992]
Epoch [105/120    avg_loss:0.020, val_acc:0.992]
Epoch [106/120    avg_loss:0.018, val_acc:0.990]
Epoch [107/120    avg_loss:0.023, val_acc:0.986]
Epoch [108/120    avg_loss:0.023, val_acc:0.986]
Epoch [109/120    avg_loss:0.019, val_acc:0.992]
Epoch [110/120    avg_loss:0.022, val_acc:0.992]
Epoch [111/120    avg_loss:0.022, val_acc:0.992]
Epoch [112/120    avg_loss:0.021, val_acc:0.992]
Epoch [113/120    avg_loss:0.021, val_acc:0.992]
Epoch [114/120    avg_loss:0.019, val_acc:0.992]
Epoch [115/120    avg_loss:0.019, val_acc:0.992]
Epoch [116/120    avg_loss:0.016, val_acc:0.992]
Epoch [117/120    avg_loss:0.024, val_acc:0.990]
Epoch [118/120    avg_loss:0.019, val_acc:0.990]
Epoch [119/120    avg_loss:0.019, val_acc:0.990]
Epoch [120/120    avg_loss:0.017, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.70149253731343

F1 scores:
[       nan 0.99926954 0.99545455 1.         0.97603486 0.95804196
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9966766394477734
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91f1e71e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.644, val_acc:0.087]
Epoch [2/120    avg_loss:2.589, val_acc:0.192]
Epoch [3/120    avg_loss:2.532, val_acc:0.410]
Epoch [4/120    avg_loss:2.479, val_acc:0.412]
Epoch [5/120    avg_loss:2.420, val_acc:0.423]
Epoch [6/120    avg_loss:2.367, val_acc:0.435]
Epoch [7/120    avg_loss:2.305, val_acc:0.454]
Epoch [8/120    avg_loss:2.252, val_acc:0.508]
Epoch [9/120    avg_loss:2.176, val_acc:0.523]
Epoch [10/120    avg_loss:2.090, val_acc:0.506]
Epoch [11/120    avg_loss:1.994, val_acc:0.531]
Epoch [12/120    avg_loss:1.900, val_acc:0.533]
Epoch [13/120    avg_loss:1.779, val_acc:0.592]
Epoch [14/120    avg_loss:1.700, val_acc:0.658]
Epoch [15/120    avg_loss:1.593, val_acc:0.654]
Epoch [16/120    avg_loss:1.464, val_acc:0.710]
Epoch [17/120    avg_loss:1.369, val_acc:0.713]
Epoch [18/120    avg_loss:1.285, val_acc:0.775]
Epoch [19/120    avg_loss:1.131, val_acc:0.754]
Epoch [20/120    avg_loss:1.052, val_acc:0.715]
Epoch [21/120    avg_loss:1.054, val_acc:0.767]
Epoch [22/120    avg_loss:0.926, val_acc:0.867]
Epoch [23/120    avg_loss:0.847, val_acc:0.821]
Epoch [24/120    avg_loss:0.855, val_acc:0.867]
Epoch [25/120    avg_loss:0.755, val_acc:0.850]
Epoch [26/120    avg_loss:0.684, val_acc:0.904]
Epoch [27/120    avg_loss:0.650, val_acc:0.906]
Epoch [28/120    avg_loss:0.593, val_acc:0.896]
Epoch [29/120    avg_loss:0.597, val_acc:0.900]
Epoch [30/120    avg_loss:0.564, val_acc:0.915]
Epoch [31/120    avg_loss:0.519, val_acc:0.900]
Epoch [32/120    avg_loss:0.541, val_acc:0.885]
Epoch [33/120    avg_loss:0.464, val_acc:0.904]
Epoch [34/120    avg_loss:0.442, val_acc:0.912]
Epoch [35/120    avg_loss:0.422, val_acc:0.902]
Epoch [36/120    avg_loss:0.409, val_acc:0.931]
Epoch [37/120    avg_loss:0.390, val_acc:0.938]
Epoch [38/120    avg_loss:0.378, val_acc:0.917]
Epoch [39/120    avg_loss:0.333, val_acc:0.935]
Epoch [40/120    avg_loss:0.298, val_acc:0.929]
Epoch [41/120    avg_loss:0.344, val_acc:0.944]
Epoch [42/120    avg_loss:0.318, val_acc:0.940]
Epoch [43/120    avg_loss:0.231, val_acc:0.952]
Epoch [44/120    avg_loss:0.225, val_acc:0.944]
Epoch [45/120    avg_loss:0.205, val_acc:0.956]
Epoch [46/120    avg_loss:0.203, val_acc:0.967]
Epoch [47/120    avg_loss:0.237, val_acc:0.942]
Epoch [48/120    avg_loss:0.212, val_acc:0.960]
Epoch [49/120    avg_loss:0.229, val_acc:0.946]
Epoch [50/120    avg_loss:0.223, val_acc:0.927]
Epoch [51/120    avg_loss:0.277, val_acc:0.963]
Epoch [52/120    avg_loss:0.210, val_acc:0.977]
Epoch [53/120    avg_loss:0.134, val_acc:0.973]
Epoch [54/120    avg_loss:0.168, val_acc:0.977]
Epoch [55/120    avg_loss:0.122, val_acc:0.994]
Epoch [56/120    avg_loss:0.132, val_acc:0.977]
Epoch [57/120    avg_loss:0.119, val_acc:0.985]
Epoch [58/120    avg_loss:0.155, val_acc:0.969]
Epoch [59/120    avg_loss:0.216, val_acc:0.988]
Epoch [60/120    avg_loss:0.129, val_acc:0.977]
Epoch [61/120    avg_loss:0.121, val_acc:0.992]
Epoch [62/120    avg_loss:0.094, val_acc:0.992]
Epoch [63/120    avg_loss:0.154, val_acc:0.988]
Epoch [64/120    avg_loss:0.105, val_acc:0.981]
Epoch [65/120    avg_loss:0.093, val_acc:0.973]
Epoch [66/120    avg_loss:0.070, val_acc:0.994]
Epoch [67/120    avg_loss:0.080, val_acc:0.973]
Epoch [68/120    avg_loss:0.084, val_acc:0.992]
Epoch [69/120    avg_loss:0.075, val_acc:0.985]
Epoch [70/120    avg_loss:0.072, val_acc:0.985]
Epoch [71/120    avg_loss:0.070, val_acc:0.944]
Epoch [72/120    avg_loss:0.092, val_acc:0.996]
Epoch [73/120    avg_loss:0.059, val_acc:0.983]
Epoch [74/120    avg_loss:0.058, val_acc:0.994]
Epoch [75/120    avg_loss:0.080, val_acc:0.983]
Epoch [76/120    avg_loss:0.064, val_acc:0.988]
Epoch [77/120    avg_loss:0.057, val_acc:0.992]
Epoch [78/120    avg_loss:0.053, val_acc:0.992]
Epoch [79/120    avg_loss:0.076, val_acc:0.994]
Epoch [80/120    avg_loss:0.050, val_acc:0.985]
Epoch [81/120    avg_loss:0.075, val_acc:0.965]
Epoch [82/120    avg_loss:0.128, val_acc:0.979]
Epoch [83/120    avg_loss:0.085, val_acc:0.983]
Epoch [84/120    avg_loss:0.077, val_acc:0.981]
Epoch [85/120    avg_loss:0.047, val_acc:0.998]
Epoch [86/120    avg_loss:0.044, val_acc:0.996]
Epoch [87/120    avg_loss:0.042, val_acc:0.990]
Epoch [88/120    avg_loss:0.030, val_acc:0.998]
Epoch [89/120    avg_loss:0.029, val_acc:0.998]
Epoch [90/120    avg_loss:0.040, val_acc:0.981]
Epoch [91/120    avg_loss:0.075, val_acc:0.988]
Epoch [92/120    avg_loss:0.073, val_acc:0.973]
Epoch [93/120    avg_loss:0.079, val_acc:0.985]
Epoch [94/120    avg_loss:0.096, val_acc:0.956]
Epoch [95/120    avg_loss:0.092, val_acc:0.992]
Epoch [96/120    avg_loss:0.065, val_acc:0.990]
Epoch [97/120    avg_loss:0.075, val_acc:0.990]
Epoch [98/120    avg_loss:0.053, val_acc:0.992]
Epoch [99/120    avg_loss:0.040, val_acc:0.992]
Epoch [100/120    avg_loss:0.039, val_acc:0.998]
Epoch [101/120    avg_loss:0.031, val_acc:0.998]
Epoch [102/120    avg_loss:0.022, val_acc:0.998]
Epoch [103/120    avg_loss:0.026, val_acc:0.996]
Epoch [104/120    avg_loss:0.023, val_acc:0.994]
Epoch [105/120    avg_loss:0.018, val_acc:0.994]
Epoch [106/120    avg_loss:0.020, val_acc:0.992]
Epoch [107/120    avg_loss:0.021, val_acc:0.994]
Epoch [108/120    avg_loss:0.015, val_acc:0.996]
Epoch [109/120    avg_loss:0.022, val_acc:0.996]
Epoch [110/120    avg_loss:0.016, val_acc:1.000]
Epoch [111/120    avg_loss:0.013, val_acc:0.998]
Epoch [112/120    avg_loss:0.014, val_acc:0.996]
Epoch [113/120    avg_loss:0.014, val_acc:0.998]
Epoch [114/120    avg_loss:0.013, val_acc:0.998]
Epoch [115/120    avg_loss:0.010, val_acc:1.000]
Epoch [116/120    avg_loss:0.010, val_acc:0.998]
Epoch [117/120    avg_loss:0.013, val_acc:0.994]
Epoch [118/120    avg_loss:0.013, val_acc:0.998]
Epoch [119/120    avg_loss:0.011, val_acc:0.998]
Epoch [120/120    avg_loss:0.017, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  11   0   0   0   0   0   0   3   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.98206278 1.         0.94877506 0.93150685
 1.         0.95555556 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9926406109502687
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff23c951e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.131]
Epoch [2/120    avg_loss:2.542, val_acc:0.333]
Epoch [3/120    avg_loss:2.477, val_acc:0.413]
Epoch [4/120    avg_loss:2.410, val_acc:0.433]
Epoch [5/120    avg_loss:2.349, val_acc:0.460]
Epoch [6/120    avg_loss:2.245, val_acc:0.462]
Epoch [7/120    avg_loss:2.148, val_acc:0.502]
Epoch [8/120    avg_loss:2.085, val_acc:0.540]
Epoch [9/120    avg_loss:1.979, val_acc:0.605]
Epoch [10/120    avg_loss:1.872, val_acc:0.633]
Epoch [11/120    avg_loss:1.734, val_acc:0.649]
Epoch [12/120    avg_loss:1.591, val_acc:0.692]
Epoch [13/120    avg_loss:1.464, val_acc:0.784]
Epoch [14/120    avg_loss:1.342, val_acc:0.786]
Epoch [15/120    avg_loss:1.199, val_acc:0.831]
Epoch [16/120    avg_loss:1.056, val_acc:0.812]
Epoch [17/120    avg_loss:0.970, val_acc:0.875]
Epoch [18/120    avg_loss:0.837, val_acc:0.895]
Epoch [19/120    avg_loss:0.799, val_acc:0.857]
Epoch [20/120    avg_loss:0.709, val_acc:0.869]
Epoch [21/120    avg_loss:0.707, val_acc:0.772]
Epoch [22/120    avg_loss:0.627, val_acc:0.899]
Epoch [23/120    avg_loss:0.613, val_acc:0.881]
Epoch [24/120    avg_loss:0.546, val_acc:0.877]
Epoch [25/120    avg_loss:0.502, val_acc:0.905]
Epoch [26/120    avg_loss:0.461, val_acc:0.913]
Epoch [27/120    avg_loss:0.386, val_acc:0.935]
Epoch [28/120    avg_loss:0.374, val_acc:0.929]
Epoch [29/120    avg_loss:0.362, val_acc:0.940]
Epoch [30/120    avg_loss:0.326, val_acc:0.913]
Epoch [31/120    avg_loss:0.325, val_acc:0.933]
Epoch [32/120    avg_loss:0.417, val_acc:0.907]
Epoch [33/120    avg_loss:0.412, val_acc:0.933]
Epoch [34/120    avg_loss:0.329, val_acc:0.950]
Epoch [35/120    avg_loss:0.332, val_acc:0.933]
Epoch [36/120    avg_loss:0.276, val_acc:0.938]
Epoch [37/120    avg_loss:0.274, val_acc:0.925]
Epoch [38/120    avg_loss:0.290, val_acc:0.929]
Epoch [39/120    avg_loss:0.226, val_acc:0.960]
Epoch [40/120    avg_loss:0.205, val_acc:0.921]
Epoch [41/120    avg_loss:0.215, val_acc:0.968]
Epoch [42/120    avg_loss:0.213, val_acc:0.970]
Epoch [43/120    avg_loss:0.179, val_acc:0.950]
Epoch [44/120    avg_loss:0.179, val_acc:0.956]
Epoch [45/120    avg_loss:0.180, val_acc:0.946]
Epoch [46/120    avg_loss:0.162, val_acc:0.970]
Epoch [47/120    avg_loss:0.144, val_acc:0.950]
Epoch [48/120    avg_loss:0.156, val_acc:0.966]
Epoch [49/120    avg_loss:0.188, val_acc:0.970]
Epoch [50/120    avg_loss:0.145, val_acc:0.895]
Epoch [51/120    avg_loss:0.188, val_acc:0.970]
Epoch [52/120    avg_loss:0.159, val_acc:0.966]
Epoch [53/120    avg_loss:0.128, val_acc:0.960]
Epoch [54/120    avg_loss:0.139, val_acc:0.970]
Epoch [55/120    avg_loss:0.111, val_acc:0.974]
Epoch [56/120    avg_loss:0.090, val_acc:0.986]
Epoch [57/120    avg_loss:0.102, val_acc:0.935]
Epoch [58/120    avg_loss:0.117, val_acc:0.968]
Epoch [59/120    avg_loss:0.118, val_acc:0.964]
Epoch [60/120    avg_loss:0.132, val_acc:0.966]
Epoch [61/120    avg_loss:0.115, val_acc:0.978]
Epoch [62/120    avg_loss:0.087, val_acc:0.982]
Epoch [63/120    avg_loss:0.107, val_acc:0.968]
Epoch [64/120    avg_loss:0.089, val_acc:0.980]
Epoch [65/120    avg_loss:0.093, val_acc:0.974]
Epoch [66/120    avg_loss:0.109, val_acc:0.964]
Epoch [67/120    avg_loss:0.109, val_acc:0.960]
Epoch [68/120    avg_loss:0.117, val_acc:0.984]
Epoch [69/120    avg_loss:0.124, val_acc:0.952]
Epoch [70/120    avg_loss:0.099, val_acc:0.988]
Epoch [71/120    avg_loss:0.066, val_acc:0.990]
Epoch [72/120    avg_loss:0.060, val_acc:0.990]
Epoch [73/120    avg_loss:0.061, val_acc:0.990]
Epoch [74/120    avg_loss:0.054, val_acc:0.990]
Epoch [75/120    avg_loss:0.058, val_acc:0.990]
Epoch [76/120    avg_loss:0.058, val_acc:0.990]
Epoch [77/120    avg_loss:0.054, val_acc:0.990]
Epoch [78/120    avg_loss:0.051, val_acc:0.990]
Epoch [79/120    avg_loss:0.054, val_acc:0.990]
Epoch [80/120    avg_loss:0.050, val_acc:0.990]
Epoch [81/120    avg_loss:0.049, val_acc:0.992]
Epoch [82/120    avg_loss:0.050, val_acc:0.990]
Epoch [83/120    avg_loss:0.047, val_acc:0.990]
Epoch [84/120    avg_loss:0.044, val_acc:0.992]
Epoch [85/120    avg_loss:0.043, val_acc:0.992]
Epoch [86/120    avg_loss:0.051, val_acc:0.992]
Epoch [87/120    avg_loss:0.041, val_acc:0.990]
Epoch [88/120    avg_loss:0.047, val_acc:0.990]
Epoch [89/120    avg_loss:0.048, val_acc:0.994]
Epoch [90/120    avg_loss:0.039, val_acc:0.994]
Epoch [91/120    avg_loss:0.041, val_acc:0.992]
Epoch [92/120    avg_loss:0.043, val_acc:0.992]
Epoch [93/120    avg_loss:0.045, val_acc:0.990]
Epoch [94/120    avg_loss:0.043, val_acc:0.990]
Epoch [95/120    avg_loss:0.045, val_acc:0.990]
Epoch [96/120    avg_loss:0.040, val_acc:0.990]
Epoch [97/120    avg_loss:0.041, val_acc:0.990]
Epoch [98/120    avg_loss:0.041, val_acc:0.992]
Epoch [99/120    avg_loss:0.051, val_acc:0.996]
Epoch [100/120    avg_loss:0.038, val_acc:0.996]
Epoch [101/120    avg_loss:0.038, val_acc:0.994]
Epoch [102/120    avg_loss:0.038, val_acc:0.992]
Epoch [103/120    avg_loss:0.043, val_acc:0.996]
Epoch [104/120    avg_loss:0.041, val_acc:0.996]
Epoch [105/120    avg_loss:0.038, val_acc:0.996]
Epoch [106/120    avg_loss:0.042, val_acc:0.988]
Epoch [107/120    avg_loss:0.036, val_acc:0.990]
Epoch [108/120    avg_loss:0.035, val_acc:0.994]
Epoch [109/120    avg_loss:0.036, val_acc:0.992]
Epoch [110/120    avg_loss:0.039, val_acc:0.992]
Epoch [111/120    avg_loss:0.037, val_acc:0.994]
Epoch [112/120    avg_loss:0.034, val_acc:0.994]
Epoch [113/120    avg_loss:0.045, val_acc:0.992]
Epoch [114/120    avg_loss:0.036, val_acc:0.990]
Epoch [115/120    avg_loss:0.033, val_acc:0.992]
Epoch [116/120    avg_loss:0.033, val_acc:0.992]
Epoch [117/120    avg_loss:0.035, val_acc:0.990]
Epoch [118/120    avg_loss:0.038, val_acc:0.992]
Epoch [119/120    avg_loss:0.033, val_acc:0.992]
Epoch [120/120    avg_loss:0.037, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  18   0   0   0   0   0   0   3   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.98871332 1.         0.91150442 0.87197232
 1.         0.9726776  1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9893170856523728
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ee5eb5da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.597, val_acc:0.111]
Epoch [2/120    avg_loss:2.529, val_acc:0.240]
Epoch [3/120    avg_loss:2.464, val_acc:0.333]
Epoch [4/120    avg_loss:2.408, val_acc:0.347]
Epoch [5/120    avg_loss:2.355, val_acc:0.347]
Epoch [6/120    avg_loss:2.308, val_acc:0.409]
Epoch [7/120    avg_loss:2.241, val_acc:0.458]
Epoch [8/120    avg_loss:2.191, val_acc:0.514]
Epoch [9/120    avg_loss:2.114, val_acc:0.552]
Epoch [10/120    avg_loss:2.051, val_acc:0.565]
Epoch [11/120    avg_loss:1.967, val_acc:0.542]
Epoch [12/120    avg_loss:1.863, val_acc:0.579]
Epoch [13/120    avg_loss:1.761, val_acc:0.643]
Epoch [14/120    avg_loss:1.669, val_acc:0.667]
Epoch [15/120    avg_loss:1.547, val_acc:0.667]
Epoch [16/120    avg_loss:1.431, val_acc:0.704]
Epoch [17/120    avg_loss:1.325, val_acc:0.708]
Epoch [18/120    avg_loss:1.191, val_acc:0.740]
Epoch [19/120    avg_loss:1.058, val_acc:0.780]
Epoch [20/120    avg_loss:0.971, val_acc:0.821]
Epoch [21/120    avg_loss:0.872, val_acc:0.808]
Epoch [22/120    avg_loss:0.880, val_acc:0.877]
Epoch [23/120    avg_loss:0.833, val_acc:0.865]
Epoch [24/120    avg_loss:0.704, val_acc:0.889]
Epoch [25/120    avg_loss:0.729, val_acc:0.877]
Epoch [26/120    avg_loss:0.675, val_acc:0.879]
Epoch [27/120    avg_loss:0.558, val_acc:0.907]
Epoch [28/120    avg_loss:0.540, val_acc:0.897]
Epoch [29/120    avg_loss:0.509, val_acc:0.889]
Epoch [30/120    avg_loss:0.526, val_acc:0.909]
Epoch [31/120    avg_loss:0.495, val_acc:0.905]
Epoch [32/120    avg_loss:0.448, val_acc:0.915]
Epoch [33/120    avg_loss:0.398, val_acc:0.925]
Epoch [34/120    avg_loss:0.396, val_acc:0.925]
Epoch [35/120    avg_loss:0.368, val_acc:0.925]
Epoch [36/120    avg_loss:0.387, val_acc:0.913]
Epoch [37/120    avg_loss:0.338, val_acc:0.925]
Epoch [38/120    avg_loss:0.301, val_acc:0.893]
Epoch [39/120    avg_loss:0.357, val_acc:0.925]
Epoch [40/120    avg_loss:0.340, val_acc:0.942]
Epoch [41/120    avg_loss:0.260, val_acc:0.944]
Epoch [42/120    avg_loss:0.264, val_acc:0.940]
Epoch [43/120    avg_loss:0.284, val_acc:0.931]
Epoch [44/120    avg_loss:0.277, val_acc:0.940]
Epoch [45/120    avg_loss:0.306, val_acc:0.927]
Epoch [46/120    avg_loss:0.270, val_acc:0.923]
Epoch [47/120    avg_loss:0.248, val_acc:0.974]
Epoch [48/120    avg_loss:0.219, val_acc:0.960]
Epoch [49/120    avg_loss:0.236, val_acc:0.950]
Epoch [50/120    avg_loss:0.225, val_acc:0.944]
Epoch [51/120    avg_loss:0.227, val_acc:0.954]
Epoch [52/120    avg_loss:0.206, val_acc:0.958]
Epoch [53/120    avg_loss:0.192, val_acc:0.970]
Epoch [54/120    avg_loss:0.179, val_acc:0.954]
Epoch [55/120    avg_loss:0.187, val_acc:0.960]
Epoch [56/120    avg_loss:0.169, val_acc:0.956]
Epoch [57/120    avg_loss:0.176, val_acc:0.968]
Epoch [58/120    avg_loss:0.161, val_acc:0.968]
Epoch [59/120    avg_loss:0.141, val_acc:0.964]
Epoch [60/120    avg_loss:0.170, val_acc:0.974]
Epoch [61/120    avg_loss:0.161, val_acc:0.968]
Epoch [62/120    avg_loss:0.138, val_acc:0.901]
Epoch [63/120    avg_loss:0.171, val_acc:0.976]
Epoch [64/120    avg_loss:0.156, val_acc:0.938]
Epoch [65/120    avg_loss:0.199, val_acc:0.972]
Epoch [66/120    avg_loss:0.139, val_acc:0.970]
Epoch [67/120    avg_loss:0.132, val_acc:0.974]
Epoch [68/120    avg_loss:0.108, val_acc:0.972]
Epoch [69/120    avg_loss:0.134, val_acc:0.976]
Epoch [70/120    avg_loss:0.103, val_acc:0.976]
Epoch [71/120    avg_loss:0.086, val_acc:0.976]
Epoch [72/120    avg_loss:0.080, val_acc:0.970]
Epoch [73/120    avg_loss:0.100, val_acc:0.976]
Epoch [74/120    avg_loss:0.062, val_acc:0.976]
Epoch [75/120    avg_loss:0.103, val_acc:0.984]
Epoch [76/120    avg_loss:0.105, val_acc:0.978]
Epoch [77/120    avg_loss:0.134, val_acc:0.972]
Epoch [78/120    avg_loss:0.105, val_acc:0.968]
Epoch [79/120    avg_loss:0.115, val_acc:0.986]
Epoch [80/120    avg_loss:0.107, val_acc:0.974]
Epoch [81/120    avg_loss:0.092, val_acc:0.978]
Epoch [82/120    avg_loss:0.076, val_acc:0.962]
Epoch [83/120    avg_loss:0.088, val_acc:0.986]
Epoch [84/120    avg_loss:0.068, val_acc:0.982]
Epoch [85/120    avg_loss:0.065, val_acc:0.988]
Epoch [86/120    avg_loss:0.061, val_acc:0.988]
Epoch [87/120    avg_loss:0.065, val_acc:0.990]
Epoch [88/120    avg_loss:0.058, val_acc:0.992]
Epoch [89/120    avg_loss:0.040, val_acc:0.988]
Epoch [90/120    avg_loss:0.051, val_acc:0.986]
Epoch [91/120    avg_loss:0.038, val_acc:0.988]
Epoch [92/120    avg_loss:0.031, val_acc:0.988]
Epoch [93/120    avg_loss:0.039, val_acc:0.992]
Epoch [94/120    avg_loss:0.032, val_acc:0.990]
Epoch [95/120    avg_loss:0.047, val_acc:0.988]
Epoch [96/120    avg_loss:0.062, val_acc:0.976]
Epoch [97/120    avg_loss:0.040, val_acc:0.986]
Epoch [98/120    avg_loss:0.048, val_acc:0.992]
Epoch [99/120    avg_loss:0.092, val_acc:0.984]
Epoch [100/120    avg_loss:0.092, val_acc:0.988]
Epoch [101/120    avg_loss:0.046, val_acc:0.988]
Epoch [102/120    avg_loss:0.030, val_acc:0.988]
Epoch [103/120    avg_loss:0.044, val_acc:0.994]
Epoch [104/120    avg_loss:0.023, val_acc:0.988]
Epoch [105/120    avg_loss:0.019, val_acc:0.992]
Epoch [106/120    avg_loss:0.020, val_acc:0.988]
Epoch [107/120    avg_loss:0.018, val_acc:0.990]
Epoch [108/120    avg_loss:0.014, val_acc:0.994]
Epoch [109/120    avg_loss:0.014, val_acc:0.992]
Epoch [110/120    avg_loss:0.022, val_acc:0.992]
Epoch [111/120    avg_loss:0.028, val_acc:0.998]
Epoch [112/120    avg_loss:0.020, val_acc:0.992]
Epoch [113/120    avg_loss:0.022, val_acc:0.986]
Epoch [114/120    avg_loss:0.028, val_acc:0.988]
Epoch [115/120    avg_loss:0.028, val_acc:0.992]
Epoch [116/120    avg_loss:0.034, val_acc:0.994]
Epoch [117/120    avg_loss:0.019, val_acc:0.988]
Epoch [118/120    avg_loss:0.012, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.996]
Epoch [120/120    avg_loss:0.021, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   4  19 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.99319728 0.99137931 0.94646681 0.89377289
 1.         0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924031086309402
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe77842def0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.637, val_acc:0.085]
Epoch [2/120    avg_loss:2.578, val_acc:0.290]
Epoch [3/120    avg_loss:2.519, val_acc:0.312]
Epoch [4/120    avg_loss:2.451, val_acc:0.302]
Epoch [5/120    avg_loss:2.404, val_acc:0.310]
Epoch [6/120    avg_loss:2.336, val_acc:0.343]
Epoch [7/120    avg_loss:2.268, val_acc:0.363]
Epoch [8/120    avg_loss:2.200, val_acc:0.371]
Epoch [9/120    avg_loss:2.112, val_acc:0.490]
Epoch [10/120    avg_loss:2.018, val_acc:0.526]
Epoch [11/120    avg_loss:1.906, val_acc:0.583]
Epoch [12/120    avg_loss:1.789, val_acc:0.577]
Epoch [13/120    avg_loss:1.679, val_acc:0.621]
Epoch [14/120    avg_loss:1.558, val_acc:0.637]
Epoch [15/120    avg_loss:1.447, val_acc:0.663]
Epoch [16/120    avg_loss:1.328, val_acc:0.716]
Epoch [17/120    avg_loss:1.169, val_acc:0.716]
Epoch [18/120    avg_loss:1.086, val_acc:0.756]
Epoch [19/120    avg_loss:1.015, val_acc:0.671]
Epoch [20/120    avg_loss:0.988, val_acc:0.766]
Epoch [21/120    avg_loss:0.844, val_acc:0.810]
Epoch [22/120    avg_loss:0.764, val_acc:0.887]
Epoch [23/120    avg_loss:0.682, val_acc:0.897]
Epoch [24/120    avg_loss:0.697, val_acc:0.911]
Epoch [25/120    avg_loss:0.673, val_acc:0.901]
Epoch [26/120    avg_loss:0.535, val_acc:0.923]
Epoch [27/120    avg_loss:0.490, val_acc:0.931]
Epoch [28/120    avg_loss:0.460, val_acc:0.935]
Epoch [29/120    avg_loss:0.471, val_acc:0.931]
Epoch [30/120    avg_loss:0.393, val_acc:0.929]
Epoch [31/120    avg_loss:0.358, val_acc:0.948]
Epoch [32/120    avg_loss:0.375, val_acc:0.960]
Epoch [33/120    avg_loss:0.337, val_acc:0.925]
Epoch [34/120    avg_loss:0.355, val_acc:0.931]
Epoch [35/120    avg_loss:0.333, val_acc:0.946]
Epoch [36/120    avg_loss:0.304, val_acc:0.950]
Epoch [37/120    avg_loss:0.350, val_acc:0.948]
Epoch [38/120    avg_loss:0.252, val_acc:0.966]
Epoch [39/120    avg_loss:0.265, val_acc:0.966]
Epoch [40/120    avg_loss:0.322, val_acc:0.944]
Epoch [41/120    avg_loss:0.317, val_acc:0.948]
Epoch [42/120    avg_loss:0.324, val_acc:0.964]
Epoch [43/120    avg_loss:0.250, val_acc:0.966]
Epoch [44/120    avg_loss:0.223, val_acc:0.935]
Epoch [45/120    avg_loss:0.198, val_acc:0.954]
Epoch [46/120    avg_loss:0.200, val_acc:0.980]
Epoch [47/120    avg_loss:0.163, val_acc:0.978]
Epoch [48/120    avg_loss:0.174, val_acc:0.954]
Epoch [49/120    avg_loss:0.211, val_acc:0.972]
Epoch [50/120    avg_loss:0.162, val_acc:0.962]
Epoch [51/120    avg_loss:0.187, val_acc:0.972]
Epoch [52/120    avg_loss:0.171, val_acc:0.978]
Epoch [53/120    avg_loss:0.161, val_acc:0.964]
Epoch [54/120    avg_loss:0.147, val_acc:0.956]
Epoch [55/120    avg_loss:0.189, val_acc:0.966]
Epoch [56/120    avg_loss:0.171, val_acc:0.968]
Epoch [57/120    avg_loss:0.141, val_acc:0.984]
Epoch [58/120    avg_loss:0.134, val_acc:0.986]
Epoch [59/120    avg_loss:0.120, val_acc:0.982]
Epoch [60/120    avg_loss:0.191, val_acc:0.976]
Epoch [61/120    avg_loss:0.144, val_acc:0.966]
Epoch [62/120    avg_loss:0.150, val_acc:0.962]
Epoch [63/120    avg_loss:0.125, val_acc:0.976]
Epoch [64/120    avg_loss:0.092, val_acc:0.976]
Epoch [65/120    avg_loss:0.106, val_acc:0.988]
Epoch [66/120    avg_loss:0.086, val_acc:0.986]
Epoch [67/120    avg_loss:0.086, val_acc:0.988]
Epoch [68/120    avg_loss:0.067, val_acc:0.992]
Epoch [69/120    avg_loss:0.068, val_acc:0.994]
Epoch [70/120    avg_loss:0.069, val_acc:0.992]
Epoch [71/120    avg_loss:0.061, val_acc:0.984]
Epoch [72/120    avg_loss:0.058, val_acc:0.952]
Epoch [73/120    avg_loss:0.068, val_acc:0.982]
Epoch [74/120    avg_loss:0.067, val_acc:0.990]
Epoch [75/120    avg_loss:0.075, val_acc:0.988]
Epoch [76/120    avg_loss:0.076, val_acc:0.992]
Epoch [77/120    avg_loss:0.072, val_acc:0.986]
Epoch [78/120    avg_loss:0.051, val_acc:0.980]
Epoch [79/120    avg_loss:0.077, val_acc:0.980]
Epoch [80/120    avg_loss:0.064, val_acc:0.970]
Epoch [81/120    avg_loss:0.074, val_acc:0.988]
Epoch [82/120    avg_loss:0.070, val_acc:0.980]
Epoch [83/120    avg_loss:0.061, val_acc:0.994]
Epoch [84/120    avg_loss:0.044, val_acc:0.994]
Epoch [85/120    avg_loss:0.036, val_acc:0.994]
Epoch [86/120    avg_loss:0.040, val_acc:0.994]
Epoch [87/120    avg_loss:0.032, val_acc:0.994]
Epoch [88/120    avg_loss:0.033, val_acc:0.994]
Epoch [89/120    avg_loss:0.032, val_acc:0.994]
Epoch [90/120    avg_loss:0.031, val_acc:0.994]
Epoch [91/120    avg_loss:0.029, val_acc:0.994]
Epoch [92/120    avg_loss:0.034, val_acc:0.994]
Epoch [93/120    avg_loss:0.035, val_acc:0.994]
Epoch [94/120    avg_loss:0.032, val_acc:0.994]
Epoch [95/120    avg_loss:0.031, val_acc:0.994]
Epoch [96/120    avg_loss:0.032, val_acc:0.994]
Epoch [97/120    avg_loss:0.027, val_acc:0.994]
Epoch [98/120    avg_loss:0.031, val_acc:0.994]
Epoch [99/120    avg_loss:0.030, val_acc:0.994]
Epoch [100/120    avg_loss:0.029, val_acc:0.994]
Epoch [101/120    avg_loss:0.034, val_acc:0.994]
Epoch [102/120    avg_loss:0.033, val_acc:0.994]
Epoch [103/120    avg_loss:0.030, val_acc:0.994]
Epoch [104/120    avg_loss:0.027, val_acc:0.994]
Epoch [105/120    avg_loss:0.026, val_acc:0.994]
Epoch [106/120    avg_loss:0.026, val_acc:0.994]
Epoch [107/120    avg_loss:0.033, val_acc:0.994]
Epoch [108/120    avg_loss:0.028, val_acc:0.994]
Epoch [109/120    avg_loss:0.024, val_acc:0.994]
Epoch [110/120    avg_loss:0.025, val_acc:0.992]
Epoch [111/120    avg_loss:0.026, val_acc:0.994]
Epoch [112/120    avg_loss:0.024, val_acc:0.994]
Epoch [113/120    avg_loss:0.032, val_acc:0.994]
Epoch [114/120    avg_loss:0.027, val_acc:0.994]
Epoch [115/120    avg_loss:0.026, val_acc:0.994]
Epoch [116/120    avg_loss:0.024, val_acc:0.994]
Epoch [117/120    avg_loss:0.024, val_acc:0.994]
Epoch [118/120    avg_loss:0.028, val_acc:0.992]
Epoch [119/120    avg_loss:0.025, val_acc:0.994]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 0.99853801 0.97986577 1.         0.97117517 0.9556314
 0.99516908 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9943028678652543
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0b09631eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.151]
Epoch [2/120    avg_loss:2.560, val_acc:0.315]
Epoch [3/120    avg_loss:2.507, val_acc:0.351]
Epoch [4/120    avg_loss:2.439, val_acc:0.331]
Epoch [5/120    avg_loss:2.374, val_acc:0.329]
Epoch [6/120    avg_loss:2.299, val_acc:0.377]
Epoch [7/120    avg_loss:2.219, val_acc:0.427]
Epoch [8/120    avg_loss:2.118, val_acc:0.514]
Epoch [9/120    avg_loss:2.039, val_acc:0.556]
Epoch [10/120    avg_loss:1.913, val_acc:0.677]
Epoch [11/120    avg_loss:1.791, val_acc:0.659]
Epoch [12/120    avg_loss:1.700, val_acc:0.688]
Epoch [13/120    avg_loss:1.560, val_acc:0.708]
Epoch [14/120    avg_loss:1.424, val_acc:0.722]
Epoch [15/120    avg_loss:1.323, val_acc:0.756]
Epoch [16/120    avg_loss:1.161, val_acc:0.825]
Epoch [17/120    avg_loss:1.081, val_acc:0.796]
Epoch [18/120    avg_loss:0.985, val_acc:0.835]
Epoch [19/120    avg_loss:0.854, val_acc:0.855]
Epoch [20/120    avg_loss:0.788, val_acc:0.875]
Epoch [21/120    avg_loss:0.728, val_acc:0.919]
Epoch [22/120    avg_loss:0.660, val_acc:0.907]
Epoch [23/120    avg_loss:0.641, val_acc:0.881]
Epoch [24/120    avg_loss:0.624, val_acc:0.917]
Epoch [25/120    avg_loss:0.584, val_acc:0.891]
Epoch [26/120    avg_loss:0.511, val_acc:0.915]
Epoch [27/120    avg_loss:0.451, val_acc:0.929]
Epoch [28/120    avg_loss:0.445, val_acc:0.917]
Epoch [29/120    avg_loss:0.428, val_acc:0.927]
Epoch [30/120    avg_loss:0.417, val_acc:0.851]
Epoch [31/120    avg_loss:0.418, val_acc:0.861]
Epoch [32/120    avg_loss:0.373, val_acc:0.948]
Epoch [33/120    avg_loss:0.363, val_acc:0.946]
Epoch [34/120    avg_loss:0.326, val_acc:0.940]
Epoch [35/120    avg_loss:0.329, val_acc:0.942]
Epoch [36/120    avg_loss:0.317, val_acc:0.923]
Epoch [37/120    avg_loss:0.341, val_acc:0.948]
Epoch [38/120    avg_loss:0.320, val_acc:0.940]
Epoch [39/120    avg_loss:0.273, val_acc:0.956]
Epoch [40/120    avg_loss:0.232, val_acc:0.976]
Epoch [41/120    avg_loss:0.230, val_acc:0.976]
Epoch [42/120    avg_loss:0.230, val_acc:0.950]
Epoch [43/120    avg_loss:0.196, val_acc:0.970]
Epoch [44/120    avg_loss:0.193, val_acc:0.946]
Epoch [45/120    avg_loss:0.226, val_acc:0.966]
Epoch [46/120    avg_loss:0.184, val_acc:0.952]
Epoch [47/120    avg_loss:0.208, val_acc:0.931]
Epoch [48/120    avg_loss:0.265, val_acc:0.954]
Epoch [49/120    avg_loss:0.201, val_acc:0.958]
Epoch [50/120    avg_loss:0.200, val_acc:0.968]
Epoch [51/120    avg_loss:0.204, val_acc:0.964]
Epoch [52/120    avg_loss:0.140, val_acc:0.986]
Epoch [53/120    avg_loss:0.172, val_acc:0.972]
Epoch [54/120    avg_loss:0.155, val_acc:0.970]
Epoch [55/120    avg_loss:0.199, val_acc:0.954]
Epoch [56/120    avg_loss:0.255, val_acc:0.962]
Epoch [57/120    avg_loss:0.257, val_acc:0.948]
Epoch [58/120    avg_loss:0.206, val_acc:0.974]
Epoch [59/120    avg_loss:0.137, val_acc:0.972]
Epoch [60/120    avg_loss:0.142, val_acc:0.984]
Epoch [61/120    avg_loss:0.148, val_acc:0.972]
Epoch [62/120    avg_loss:0.129, val_acc:0.972]
Epoch [63/120    avg_loss:0.109, val_acc:0.974]
Epoch [64/120    avg_loss:0.091, val_acc:0.976]
Epoch [65/120    avg_loss:0.098, val_acc:0.984]
Epoch [66/120    avg_loss:0.101, val_acc:0.990]
Epoch [67/120    avg_loss:0.080, val_acc:0.990]
Epoch [68/120    avg_loss:0.075, val_acc:0.992]
Epoch [69/120    avg_loss:0.084, val_acc:0.988]
Epoch [70/120    avg_loss:0.064, val_acc:0.992]
Epoch [71/120    avg_loss:0.064, val_acc:0.988]
Epoch [72/120    avg_loss:0.072, val_acc:0.992]
Epoch [73/120    avg_loss:0.074, val_acc:0.990]
Epoch [74/120    avg_loss:0.073, val_acc:0.992]
Epoch [75/120    avg_loss:0.071, val_acc:0.992]
Epoch [76/120    avg_loss:0.060, val_acc:0.994]
Epoch [77/120    avg_loss:0.070, val_acc:0.990]
Epoch [78/120    avg_loss:0.065, val_acc:0.992]
Epoch [79/120    avg_loss:0.067, val_acc:0.992]
Epoch [80/120    avg_loss:0.064, val_acc:0.990]
Epoch [81/120    avg_loss:0.065, val_acc:0.990]
Epoch [82/120    avg_loss:0.057, val_acc:0.994]
Epoch [83/120    avg_loss:0.068, val_acc:0.988]
Epoch [84/120    avg_loss:0.062, val_acc:0.990]
Epoch [85/120    avg_loss:0.065, val_acc:0.996]
Epoch [86/120    avg_loss:0.057, val_acc:0.994]
Epoch [87/120    avg_loss:0.066, val_acc:0.988]
Epoch [88/120    avg_loss:0.061, val_acc:0.990]
Epoch [89/120    avg_loss:0.064, val_acc:0.992]
Epoch [90/120    avg_loss:0.061, val_acc:0.996]
Epoch [91/120    avg_loss:0.064, val_acc:0.988]
Epoch [92/120    avg_loss:0.055, val_acc:0.992]
Epoch [93/120    avg_loss:0.051, val_acc:0.992]
Epoch [94/120    avg_loss:0.055, val_acc:0.992]
Epoch [95/120    avg_loss:0.063, val_acc:0.994]
Epoch [96/120    avg_loss:0.064, val_acc:0.988]
Epoch [97/120    avg_loss:0.055, val_acc:0.994]
Epoch [98/120    avg_loss:0.051, val_acc:0.994]
Epoch [99/120    avg_loss:0.062, val_acc:0.994]
Epoch [100/120    avg_loss:0.055, val_acc:0.990]
Epoch [101/120    avg_loss:0.062, val_acc:0.990]
Epoch [102/120    avg_loss:0.059, val_acc:0.990]
Epoch [103/120    avg_loss:0.055, val_acc:0.992]
Epoch [104/120    avg_loss:0.055, val_acc:0.994]
Epoch [105/120    avg_loss:0.054, val_acc:0.994]
Epoch [106/120    avg_loss:0.048, val_acc:0.994]
Epoch [107/120    avg_loss:0.054, val_acc:0.994]
Epoch [108/120    avg_loss:0.047, val_acc:0.994]
Epoch [109/120    avg_loss:0.052, val_acc:0.992]
Epoch [110/120    avg_loss:0.055, val_acc:0.992]
Epoch [111/120    avg_loss:0.058, val_acc:0.992]
Epoch [112/120    avg_loss:0.051, val_acc:0.992]
Epoch [113/120    avg_loss:0.054, val_acc:0.992]
Epoch [114/120    avg_loss:0.051, val_acc:0.992]
Epoch [115/120    avg_loss:0.048, val_acc:0.992]
Epoch [116/120    avg_loss:0.064, val_acc:0.992]
Epoch [117/120    avg_loss:0.053, val_acc:0.992]
Epoch [118/120    avg_loss:0.046, val_acc:0.992]
Epoch [119/120    avg_loss:0.048, val_acc:0.992]
Epoch [120/120    avg_loss:0.055, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   1   0   0   0   0   4   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.99095023 0.98901099 0.94520548 0.92857143
 0.99756691 0.97826087 1.         0.99574468 1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.991929095080787
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c2415ae48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.054]
Epoch [2/120    avg_loss:2.580, val_acc:0.273]
Epoch [3/120    avg_loss:2.526, val_acc:0.415]
Epoch [4/120    avg_loss:2.456, val_acc:0.444]
Epoch [5/120    avg_loss:2.384, val_acc:0.463]
Epoch [6/120    avg_loss:2.292, val_acc:0.442]
Epoch [7/120    avg_loss:2.203, val_acc:0.440]
Epoch [8/120    avg_loss:2.119, val_acc:0.487]
Epoch [9/120    avg_loss:2.020, val_acc:0.546]
Epoch [10/120    avg_loss:1.913, val_acc:0.540]
Epoch [11/120    avg_loss:1.808, val_acc:0.594]
Epoch [12/120    avg_loss:1.703, val_acc:0.635]
Epoch [13/120    avg_loss:1.593, val_acc:0.669]
Epoch [14/120    avg_loss:1.454, val_acc:0.713]
Epoch [15/120    avg_loss:1.309, val_acc:0.733]
Epoch [16/120    avg_loss:1.235, val_acc:0.767]
Epoch [17/120    avg_loss:1.134, val_acc:0.765]
Epoch [18/120    avg_loss:1.061, val_acc:0.794]
Epoch [19/120    avg_loss:0.978, val_acc:0.794]
Epoch [20/120    avg_loss:0.846, val_acc:0.804]
Epoch [21/120    avg_loss:0.807, val_acc:0.844]
Epoch [22/120    avg_loss:0.694, val_acc:0.854]
Epoch [23/120    avg_loss:0.717, val_acc:0.863]
Epoch [24/120    avg_loss:0.653, val_acc:0.908]
Epoch [25/120    avg_loss:0.601, val_acc:0.887]
Epoch [26/120    avg_loss:0.573, val_acc:0.896]
Epoch [27/120    avg_loss:0.509, val_acc:0.933]
Epoch [28/120    avg_loss:0.426, val_acc:0.933]
Epoch [29/120    avg_loss:0.419, val_acc:0.954]
Epoch [30/120    avg_loss:0.379, val_acc:0.935]
Epoch [31/120    avg_loss:0.382, val_acc:0.921]
Epoch [32/120    avg_loss:0.414, val_acc:0.906]
Epoch [33/120    avg_loss:0.342, val_acc:0.950]
Epoch [34/120    avg_loss:0.346, val_acc:0.948]
Epoch [35/120    avg_loss:0.349, val_acc:0.954]
Epoch [36/120    avg_loss:0.277, val_acc:0.963]
Epoch [37/120    avg_loss:0.257, val_acc:0.946]
Epoch [38/120    avg_loss:0.245, val_acc:0.946]
Epoch [39/120    avg_loss:0.288, val_acc:0.940]
Epoch [40/120    avg_loss:0.246, val_acc:0.948]
Epoch [41/120    avg_loss:0.208, val_acc:0.940]
Epoch [42/120    avg_loss:0.269, val_acc:0.954]
Epoch [43/120    avg_loss:0.191, val_acc:0.950]
Epoch [44/120    avg_loss:0.236, val_acc:0.950]
Epoch [45/120    avg_loss:0.180, val_acc:0.969]
Epoch [46/120    avg_loss:0.211, val_acc:0.954]
Epoch [47/120    avg_loss:0.289, val_acc:0.950]
Epoch [48/120    avg_loss:0.201, val_acc:0.969]
Epoch [49/120    avg_loss:0.172, val_acc:0.954]
Epoch [50/120    avg_loss:0.217, val_acc:0.975]
Epoch [51/120    avg_loss:0.164, val_acc:0.973]
Epoch [52/120    avg_loss:0.160, val_acc:0.977]
Epoch [53/120    avg_loss:0.153, val_acc:0.977]
Epoch [54/120    avg_loss:0.156, val_acc:0.975]
Epoch [55/120    avg_loss:0.170, val_acc:0.979]
Epoch [56/120    avg_loss:0.119, val_acc:0.979]
Epoch [57/120    avg_loss:0.146, val_acc:0.979]
Epoch [58/120    avg_loss:0.132, val_acc:0.990]
Epoch [59/120    avg_loss:0.106, val_acc:0.979]
Epoch [60/120    avg_loss:0.116, val_acc:0.979]
Epoch [61/120    avg_loss:0.084, val_acc:0.990]
Epoch [62/120    avg_loss:0.090, val_acc:0.988]
Epoch [63/120    avg_loss:0.093, val_acc:0.990]
Epoch [64/120    avg_loss:0.089, val_acc:0.990]
Epoch [65/120    avg_loss:0.088, val_acc:0.979]
Epoch [66/120    avg_loss:0.099, val_acc:0.973]
Epoch [67/120    avg_loss:0.072, val_acc:0.981]
Epoch [68/120    avg_loss:0.079, val_acc:0.992]
Epoch [69/120    avg_loss:0.078, val_acc:0.988]
Epoch [70/120    avg_loss:0.091, val_acc:0.979]
Epoch [71/120    avg_loss:0.099, val_acc:0.983]
Epoch [72/120    avg_loss:0.085, val_acc:0.990]
Epoch [73/120    avg_loss:0.085, val_acc:0.977]
Epoch [74/120    avg_loss:0.078, val_acc:0.985]
Epoch [75/120    avg_loss:0.083, val_acc:0.981]
Epoch [76/120    avg_loss:0.069, val_acc:0.979]
Epoch [77/120    avg_loss:0.067, val_acc:0.990]
Epoch [78/120    avg_loss:0.047, val_acc:0.975]
Epoch [79/120    avg_loss:0.090, val_acc:0.985]
Epoch [80/120    avg_loss:0.047, val_acc:0.988]
Epoch [81/120    avg_loss:0.047, val_acc:0.992]
Epoch [82/120    avg_loss:0.099, val_acc:0.988]
Epoch [83/120    avg_loss:0.085, val_acc:0.956]
Epoch [84/120    avg_loss:0.080, val_acc:0.988]
Epoch [85/120    avg_loss:0.050, val_acc:0.992]
Epoch [86/120    avg_loss:0.063, val_acc:0.990]
Epoch [87/120    avg_loss:0.048, val_acc:0.988]
Epoch [88/120    avg_loss:0.042, val_acc:0.992]
Epoch [89/120    avg_loss:0.039, val_acc:0.992]
Epoch [90/120    avg_loss:0.059, val_acc:0.994]
Epoch [91/120    avg_loss:0.049, val_acc:0.992]
Epoch [92/120    avg_loss:0.039, val_acc:0.992]
Epoch [93/120    avg_loss:0.043, val_acc:0.990]
Epoch [94/120    avg_loss:0.040, val_acc:0.992]
Epoch [95/120    avg_loss:0.036, val_acc:0.988]
Epoch [96/120    avg_loss:0.035, val_acc:0.992]
Epoch [97/120    avg_loss:0.027, val_acc:0.994]
Epoch [98/120    avg_loss:0.059, val_acc:0.988]
Epoch [99/120    avg_loss:0.079, val_acc:0.988]
Epoch [100/120    avg_loss:0.034, val_acc:0.992]
Epoch [101/120    avg_loss:0.027, val_acc:0.992]
Epoch [102/120    avg_loss:0.023, val_acc:0.994]
Epoch [103/120    avg_loss:0.022, val_acc:0.994]
Epoch [104/120    avg_loss:0.036, val_acc:0.979]
Epoch [105/120    avg_loss:0.034, val_acc:0.996]
Epoch [106/120    avg_loss:0.034, val_acc:0.994]
Epoch [107/120    avg_loss:0.023, val_acc:0.994]
Epoch [108/120    avg_loss:0.023, val_acc:0.979]
Epoch [109/120    avg_loss:0.026, val_acc:0.996]
Epoch [110/120    avg_loss:0.021, val_acc:0.996]
Epoch [111/120    avg_loss:0.014, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.994]
Epoch [113/120    avg_loss:0.019, val_acc:0.992]
Epoch [114/120    avg_loss:0.040, val_acc:0.990]
Epoch [115/120    avg_loss:0.040, val_acc:0.988]
Epoch [116/120    avg_loss:0.025, val_acc:0.990]
Epoch [117/120    avg_loss:0.028, val_acc:0.994]
Epoch [118/120    avg_loss:0.019, val_acc:0.996]
Epoch [119/120    avg_loss:0.016, val_acc:0.994]
Epoch [120/120    avg_loss:0.019, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   0   0   0   0   0   0   0   1   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99707174 1.         1.         0.9596603  0.93382353
 0.99038462 1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.994540285676629
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6548ef2e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.395]
Epoch [2/120    avg_loss:2.537, val_acc:0.387]
Epoch [3/120    avg_loss:2.466, val_acc:0.367]
Epoch [4/120    avg_loss:2.387, val_acc:0.397]
Epoch [5/120    avg_loss:2.330, val_acc:0.417]
Epoch [6/120    avg_loss:2.267, val_acc:0.458]
Epoch [7/120    avg_loss:2.199, val_acc:0.486]
Epoch [8/120    avg_loss:2.135, val_acc:0.506]
Epoch [9/120    avg_loss:2.065, val_acc:0.554]
Epoch [10/120    avg_loss:1.972, val_acc:0.558]
Epoch [11/120    avg_loss:1.873, val_acc:0.546]
Epoch [12/120    avg_loss:1.778, val_acc:0.585]
Epoch [13/120    avg_loss:1.708, val_acc:0.546]
Epoch [14/120    avg_loss:1.590, val_acc:0.613]
Epoch [15/120    avg_loss:1.505, val_acc:0.607]
Epoch [16/120    avg_loss:1.411, val_acc:0.601]
Epoch [17/120    avg_loss:1.327, val_acc:0.633]
Epoch [18/120    avg_loss:1.260, val_acc:0.677]
Epoch [19/120    avg_loss:1.146, val_acc:0.665]
Epoch [20/120    avg_loss:1.070, val_acc:0.724]
Epoch [21/120    avg_loss:0.983, val_acc:0.726]
Epoch [22/120    avg_loss:0.922, val_acc:0.746]
Epoch [23/120    avg_loss:0.844, val_acc:0.764]
Epoch [24/120    avg_loss:0.795, val_acc:0.833]
Epoch [25/120    avg_loss:0.712, val_acc:0.891]
Epoch [26/120    avg_loss:0.633, val_acc:0.907]
Epoch [27/120    avg_loss:0.587, val_acc:0.935]
Epoch [28/120    avg_loss:0.577, val_acc:0.944]
Epoch [29/120    avg_loss:0.632, val_acc:0.857]
Epoch [30/120    avg_loss:0.565, val_acc:0.917]
Epoch [31/120    avg_loss:0.512, val_acc:0.913]
Epoch [32/120    avg_loss:0.527, val_acc:0.940]
Epoch [33/120    avg_loss:0.407, val_acc:0.927]
Epoch [34/120    avg_loss:0.371, val_acc:0.925]
Epoch [35/120    avg_loss:0.434, val_acc:0.931]
Epoch [36/120    avg_loss:0.464, val_acc:0.913]
Epoch [37/120    avg_loss:0.402, val_acc:0.921]
Epoch [38/120    avg_loss:0.389, val_acc:0.958]
Epoch [39/120    avg_loss:0.314, val_acc:0.958]
Epoch [40/120    avg_loss:0.334, val_acc:0.946]
Epoch [41/120    avg_loss:0.299, val_acc:0.966]
Epoch [42/120    avg_loss:0.280, val_acc:0.960]
Epoch [43/120    avg_loss:0.329, val_acc:0.950]
Epoch [44/120    avg_loss:0.281, val_acc:0.970]
Epoch [45/120    avg_loss:0.248, val_acc:0.962]
Epoch [46/120    avg_loss:0.255, val_acc:0.964]
Epoch [47/120    avg_loss:0.228, val_acc:0.968]
Epoch [48/120    avg_loss:0.195, val_acc:0.972]
Epoch [49/120    avg_loss:0.209, val_acc:0.976]
Epoch [50/120    avg_loss:0.185, val_acc:0.964]
Epoch [51/120    avg_loss:0.220, val_acc:0.978]
Epoch [52/120    avg_loss:0.199, val_acc:0.970]
Epoch [53/120    avg_loss:0.199, val_acc:0.980]
Epoch [54/120    avg_loss:0.185, val_acc:0.974]
Epoch [55/120    avg_loss:0.157, val_acc:0.980]
Epoch [56/120    avg_loss:0.152, val_acc:0.972]
Epoch [57/120    avg_loss:0.133, val_acc:0.980]
Epoch [58/120    avg_loss:0.120, val_acc:0.986]
Epoch [59/120    avg_loss:0.135, val_acc:0.980]
Epoch [60/120    avg_loss:0.168, val_acc:0.978]
Epoch [61/120    avg_loss:0.188, val_acc:0.964]
Epoch [62/120    avg_loss:0.144, val_acc:0.964]
Epoch [63/120    avg_loss:0.160, val_acc:0.974]
Epoch [64/120    avg_loss:0.155, val_acc:0.982]
Epoch [65/120    avg_loss:0.164, val_acc:0.980]
Epoch [66/120    avg_loss:0.148, val_acc:0.966]
Epoch [67/120    avg_loss:0.138, val_acc:0.976]
Epoch [68/120    avg_loss:0.113, val_acc:0.990]
Epoch [69/120    avg_loss:0.124, val_acc:0.972]
Epoch [70/120    avg_loss:0.122, val_acc:0.976]
Epoch [71/120    avg_loss:0.120, val_acc:0.988]
Epoch [72/120    avg_loss:0.142, val_acc:0.984]
Epoch [73/120    avg_loss:0.111, val_acc:0.982]
Epoch [74/120    avg_loss:0.103, val_acc:0.974]
Epoch [75/120    avg_loss:0.107, val_acc:0.986]
Epoch [76/120    avg_loss:0.096, val_acc:0.988]
Epoch [77/120    avg_loss:0.084, val_acc:0.978]
Epoch [78/120    avg_loss:0.089, val_acc:0.986]
Epoch [79/120    avg_loss:0.074, val_acc:0.990]
Epoch [80/120    avg_loss:0.061, val_acc:0.990]
Epoch [81/120    avg_loss:0.066, val_acc:0.992]
Epoch [82/120    avg_loss:0.095, val_acc:0.982]
Epoch [83/120    avg_loss:0.067, val_acc:0.978]
Epoch [84/120    avg_loss:0.064, val_acc:0.988]
Epoch [85/120    avg_loss:0.094, val_acc:0.986]
Epoch [86/120    avg_loss:0.069, val_acc:0.984]
Epoch [87/120    avg_loss:0.063, val_acc:0.984]
Epoch [88/120    avg_loss:0.051, val_acc:0.990]
Epoch [89/120    avg_loss:0.064, val_acc:0.986]
Epoch [90/120    avg_loss:0.072, val_acc:0.986]
Epoch [91/120    avg_loss:0.057, val_acc:0.994]
Epoch [92/120    avg_loss:0.062, val_acc:0.986]
Epoch [93/120    avg_loss:0.056, val_acc:0.986]
Epoch [94/120    avg_loss:0.043, val_acc:0.990]
Epoch [95/120    avg_loss:0.039, val_acc:0.986]
Epoch [96/120    avg_loss:0.038, val_acc:0.992]
Epoch [97/120    avg_loss:0.036, val_acc:0.990]
Epoch [98/120    avg_loss:0.039, val_acc:0.982]
Epoch [99/120    avg_loss:0.047, val_acc:0.994]
Epoch [100/120    avg_loss:0.031, val_acc:0.988]
Epoch [101/120    avg_loss:0.033, val_acc:0.992]
Epoch [102/120    avg_loss:0.033, val_acc:0.992]
Epoch [103/120    avg_loss:0.052, val_acc:0.988]
Epoch [104/120    avg_loss:0.043, val_acc:0.994]
Epoch [105/120    avg_loss:0.025, val_acc:0.992]
Epoch [106/120    avg_loss:0.038, val_acc:0.996]
Epoch [107/120    avg_loss:0.028, val_acc:0.992]
Epoch [108/120    avg_loss:0.025, val_acc:0.992]
Epoch [109/120    avg_loss:0.029, val_acc:0.994]
Epoch [110/120    avg_loss:0.021, val_acc:0.998]
Epoch [111/120    avg_loss:0.016, val_acc:0.996]
Epoch [112/120    avg_loss:0.021, val_acc:0.994]
Epoch [113/120    avg_loss:0.023, val_acc:0.994]
Epoch [114/120    avg_loss:0.017, val_acc:0.996]
Epoch [115/120    avg_loss:0.021, val_acc:0.996]
Epoch [116/120    avg_loss:0.043, val_acc:0.988]
Epoch [117/120    avg_loss:0.021, val_acc:0.996]
Epoch [118/120    avg_loss:0.023, val_acc:0.992]
Epoch [119/120    avg_loss:0.015, val_acc:0.990]
Epoch [120/120    avg_loss:0.018, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99780541 0.98648649 1.         0.96536797 0.94326241
 0.99277108 0.96703297 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9940654729418212
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fecb630cdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.296]
Epoch [2/120    avg_loss:2.557, val_acc:0.352]
Epoch [3/120    avg_loss:2.496, val_acc:0.388]
Epoch [4/120    avg_loss:2.438, val_acc:0.454]
Epoch [5/120    avg_loss:2.373, val_acc:0.525]
Epoch [6/120    avg_loss:2.299, val_acc:0.573]
Epoch [7/120    avg_loss:2.234, val_acc:0.629]
Epoch [8/120    avg_loss:2.165, val_acc:0.658]
Epoch [9/120    avg_loss:2.075, val_acc:0.694]
Epoch [10/120    avg_loss:1.975, val_acc:0.694]
Epoch [11/120    avg_loss:1.870, val_acc:0.698]
Epoch [12/120    avg_loss:1.727, val_acc:0.708]
Epoch [13/120    avg_loss:1.596, val_acc:0.731]
Epoch [14/120    avg_loss:1.485, val_acc:0.744]
Epoch [15/120    avg_loss:1.376, val_acc:0.756]
Epoch [16/120    avg_loss:1.240, val_acc:0.769]
Epoch [17/120    avg_loss:1.079, val_acc:0.783]
Epoch [18/120    avg_loss:0.983, val_acc:0.904]
Epoch [19/120    avg_loss:0.896, val_acc:0.812]
Epoch [20/120    avg_loss:0.809, val_acc:0.902]
Epoch [21/120    avg_loss:0.722, val_acc:0.825]
Epoch [22/120    avg_loss:0.702, val_acc:0.906]
Epoch [23/120    avg_loss:0.620, val_acc:0.823]
Epoch [24/120    avg_loss:0.651, val_acc:0.950]
Epoch [25/120    avg_loss:0.521, val_acc:0.881]
Epoch [26/120    avg_loss:0.545, val_acc:0.935]
Epoch [27/120    avg_loss:0.474, val_acc:0.927]
Epoch [28/120    avg_loss:0.535, val_acc:0.921]
Epoch [29/120    avg_loss:0.449, val_acc:0.940]
Epoch [30/120    avg_loss:0.435, val_acc:0.906]
Epoch [31/120    avg_loss:0.453, val_acc:0.929]
Epoch [32/120    avg_loss:0.347, val_acc:0.921]
Epoch [33/120    avg_loss:0.333, val_acc:0.950]
Epoch [34/120    avg_loss:0.297, val_acc:0.942]
Epoch [35/120    avg_loss:0.286, val_acc:0.952]
Epoch [36/120    avg_loss:0.259, val_acc:0.954]
Epoch [37/120    avg_loss:0.284, val_acc:0.896]
Epoch [38/120    avg_loss:0.302, val_acc:0.944]
Epoch [39/120    avg_loss:0.257, val_acc:0.952]
Epoch [40/120    avg_loss:0.260, val_acc:0.952]
Epoch [41/120    avg_loss:0.224, val_acc:0.938]
Epoch [42/120    avg_loss:0.203, val_acc:0.969]
Epoch [43/120    avg_loss:0.191, val_acc:0.958]
Epoch [44/120    avg_loss:0.213, val_acc:0.975]
Epoch [45/120    avg_loss:0.179, val_acc:0.975]
Epoch [46/120    avg_loss:0.182, val_acc:0.965]
Epoch [47/120    avg_loss:0.154, val_acc:0.958]
Epoch [48/120    avg_loss:0.194, val_acc:0.990]
Epoch [49/120    avg_loss:0.174, val_acc:0.963]
Epoch [50/120    avg_loss:0.149, val_acc:0.973]
Epoch [51/120    avg_loss:0.123, val_acc:0.981]
Epoch [52/120    avg_loss:0.124, val_acc:0.973]
Epoch [53/120    avg_loss:0.127, val_acc:0.977]
Epoch [54/120    avg_loss:0.129, val_acc:0.981]
Epoch [55/120    avg_loss:0.129, val_acc:0.981]
Epoch [56/120    avg_loss:0.111, val_acc:0.981]
Epoch [57/120    avg_loss:0.128, val_acc:0.977]
Epoch [58/120    avg_loss:0.130, val_acc:0.973]
Epoch [59/120    avg_loss:0.117, val_acc:0.990]
Epoch [60/120    avg_loss:0.108, val_acc:0.990]
Epoch [61/120    avg_loss:0.214, val_acc:0.983]
Epoch [62/120    avg_loss:0.130, val_acc:0.988]
Epoch [63/120    avg_loss:0.116, val_acc:0.983]
Epoch [64/120    avg_loss:0.093, val_acc:0.998]
Epoch [65/120    avg_loss:0.070, val_acc:0.994]
Epoch [66/120    avg_loss:0.068, val_acc:0.996]
Epoch [67/120    avg_loss:0.080, val_acc:0.992]
Epoch [68/120    avg_loss:0.106, val_acc:0.990]
Epoch [69/120    avg_loss:0.090, val_acc:0.992]
Epoch [70/120    avg_loss:0.084, val_acc:0.983]
Epoch [71/120    avg_loss:0.068, val_acc:0.979]
Epoch [72/120    avg_loss:0.057, val_acc:0.998]
Epoch [73/120    avg_loss:0.059, val_acc:0.996]
Epoch [74/120    avg_loss:0.057, val_acc:0.990]
Epoch [75/120    avg_loss:0.122, val_acc:0.977]
Epoch [76/120    avg_loss:0.123, val_acc:0.965]
Epoch [77/120    avg_loss:0.089, val_acc:0.996]
Epoch [78/120    avg_loss:0.059, val_acc:0.992]
Epoch [79/120    avg_loss:0.068, val_acc:0.996]
Epoch [80/120    avg_loss:0.045, val_acc:0.994]
Epoch [81/120    avg_loss:0.046, val_acc:0.990]
Epoch [82/120    avg_loss:0.043, val_acc:0.996]
Epoch [83/120    avg_loss:0.046, val_acc:0.994]
Epoch [84/120    avg_loss:0.032, val_acc:0.992]
Epoch [85/120    avg_loss:0.053, val_acc:0.985]
Epoch [86/120    avg_loss:0.045, val_acc:0.994]
Epoch [87/120    avg_loss:0.023, val_acc:0.998]
Epoch [88/120    avg_loss:0.023, val_acc:0.996]
Epoch [89/120    avg_loss:0.027, val_acc:0.998]
Epoch [90/120    avg_loss:0.025, val_acc:1.000]
Epoch [91/120    avg_loss:0.022, val_acc:1.000]
Epoch [92/120    avg_loss:0.021, val_acc:1.000]
Epoch [93/120    avg_loss:0.020, val_acc:0.998]
Epoch [94/120    avg_loss:0.020, val_acc:0.998]
Epoch [95/120    avg_loss:0.027, val_acc:0.998]
Epoch [96/120    avg_loss:0.020, val_acc:0.998]
Epoch [97/120    avg_loss:0.020, val_acc:0.998]
Epoch [98/120    avg_loss:0.023, val_acc:0.998]
Epoch [99/120    avg_loss:0.019, val_acc:0.998]
Epoch [100/120    avg_loss:0.023, val_acc:0.998]
Epoch [101/120    avg_loss:0.021, val_acc:0.998]
Epoch [102/120    avg_loss:0.018, val_acc:0.998]
Epoch [103/120    avg_loss:0.024, val_acc:0.998]
Epoch [104/120    avg_loss:0.019, val_acc:1.000]
Epoch [105/120    avg_loss:0.021, val_acc:1.000]
Epoch [106/120    avg_loss:0.017, val_acc:0.998]
Epoch [107/120    avg_loss:0.022, val_acc:0.998]
Epoch [108/120    avg_loss:0.021, val_acc:0.998]
Epoch [109/120    avg_loss:0.017, val_acc:0.998]
Epoch [110/120    avg_loss:0.018, val_acc:0.998]
Epoch [111/120    avg_loss:0.018, val_acc:0.998]
Epoch [112/120    avg_loss:0.022, val_acc:0.998]
Epoch [113/120    avg_loss:0.019, val_acc:0.998]
Epoch [114/120    avg_loss:0.015, val_acc:0.998]
Epoch [115/120    avg_loss:0.020, val_acc:0.998]
Epoch [116/120    avg_loss:0.022, val_acc:0.998]
Epoch [117/120    avg_loss:0.021, val_acc:0.998]
Epoch [118/120    avg_loss:0.019, val_acc:0.998]
Epoch [119/120    avg_loss:0.019, val_acc:0.998]
Epoch [120/120    avg_loss:0.018, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.996337   1.         0.99782135 0.95464363 0.92907801
 0.98800959 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9938286263070771
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcdc5f5ee48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.177]
Epoch [2/120    avg_loss:2.556, val_acc:0.381]
Epoch [3/120    avg_loss:2.483, val_acc:0.460]
Epoch [4/120    avg_loss:2.419, val_acc:0.456]
Epoch [5/120    avg_loss:2.349, val_acc:0.452]
Epoch [6/120    avg_loss:2.274, val_acc:0.452]
Epoch [7/120    avg_loss:2.189, val_acc:0.448]
Epoch [8/120    avg_loss:2.125, val_acc:0.452]
Epoch [9/120    avg_loss:2.034, val_acc:0.479]
Epoch [10/120    avg_loss:1.932, val_acc:0.460]
Epoch [11/120    avg_loss:1.861, val_acc:0.531]
Epoch [12/120    avg_loss:1.774, val_acc:0.544]
Epoch [13/120    avg_loss:1.691, val_acc:0.571]
Epoch [14/120    avg_loss:1.623, val_acc:0.658]
Epoch [15/120    avg_loss:1.534, val_acc:0.706]
Epoch [16/120    avg_loss:1.446, val_acc:0.733]
Epoch [17/120    avg_loss:1.352, val_acc:0.775]
Epoch [18/120    avg_loss:1.193, val_acc:0.815]
Epoch [19/120    avg_loss:1.079, val_acc:0.833]
Epoch [20/120    avg_loss:0.974, val_acc:0.844]
Epoch [21/120    avg_loss:0.889, val_acc:0.835]
Epoch [22/120    avg_loss:0.878, val_acc:0.835]
Epoch [23/120    avg_loss:0.742, val_acc:0.906]
Epoch [24/120    avg_loss:0.642, val_acc:0.910]
Epoch [25/120    avg_loss:0.679, val_acc:0.815]
Epoch [26/120    avg_loss:0.734, val_acc:0.919]
Epoch [27/120    avg_loss:0.564, val_acc:0.917]
Epoch [28/120    avg_loss:0.503, val_acc:0.946]
Epoch [29/120    avg_loss:0.469, val_acc:0.896]
Epoch [30/120    avg_loss:0.472, val_acc:0.929]
Epoch [31/120    avg_loss:0.417, val_acc:0.842]
Epoch [32/120    avg_loss:0.553, val_acc:0.917]
Epoch [33/120    avg_loss:0.390, val_acc:0.952]
Epoch [34/120    avg_loss:0.423, val_acc:0.942]
Epoch [35/120    avg_loss:0.365, val_acc:0.944]
Epoch [36/120    avg_loss:0.310, val_acc:0.967]
Epoch [37/120    avg_loss:0.315, val_acc:0.958]
Epoch [38/120    avg_loss:0.317, val_acc:0.948]
Epoch [39/120    avg_loss:0.325, val_acc:0.931]
Epoch [40/120    avg_loss:0.294, val_acc:0.971]
Epoch [41/120    avg_loss:0.273, val_acc:0.963]
Epoch [42/120    avg_loss:0.292, val_acc:0.958]
Epoch [43/120    avg_loss:0.205, val_acc:0.935]
Epoch [44/120    avg_loss:0.224, val_acc:0.975]
Epoch [45/120    avg_loss:0.200, val_acc:0.977]
Epoch [46/120    avg_loss:0.186, val_acc:0.983]
Epoch [47/120    avg_loss:0.186, val_acc:0.981]
Epoch [48/120    avg_loss:0.166, val_acc:0.942]
Epoch [49/120    avg_loss:0.157, val_acc:0.960]
Epoch [50/120    avg_loss:0.155, val_acc:0.969]
Epoch [51/120    avg_loss:0.141, val_acc:0.992]
Epoch [52/120    avg_loss:0.134, val_acc:0.981]
Epoch [53/120    avg_loss:0.122, val_acc:0.983]
Epoch [54/120    avg_loss:0.121, val_acc:0.983]
Epoch [55/120    avg_loss:0.145, val_acc:0.958]
Epoch [56/120    avg_loss:0.162, val_acc:0.988]
Epoch [57/120    avg_loss:0.149, val_acc:0.977]
Epoch [58/120    avg_loss:0.143, val_acc:0.969]
Epoch [59/120    avg_loss:0.119, val_acc:0.988]
Epoch [60/120    avg_loss:0.096, val_acc:0.983]
Epoch [61/120    avg_loss:0.115, val_acc:0.971]
Epoch [62/120    avg_loss:0.109, val_acc:0.985]
Epoch [63/120    avg_loss:0.137, val_acc:0.983]
Epoch [64/120    avg_loss:0.093, val_acc:0.983]
Epoch [65/120    avg_loss:0.089, val_acc:0.990]
Epoch [66/120    avg_loss:0.076, val_acc:0.988]
Epoch [67/120    avg_loss:0.075, val_acc:0.988]
Epoch [68/120    avg_loss:0.069, val_acc:0.990]
Epoch [69/120    avg_loss:0.068, val_acc:0.990]
Epoch [70/120    avg_loss:0.074, val_acc:0.992]
Epoch [71/120    avg_loss:0.060, val_acc:0.992]
Epoch [72/120    avg_loss:0.070, val_acc:0.992]
Epoch [73/120    avg_loss:0.066, val_acc:0.992]
Epoch [74/120    avg_loss:0.060, val_acc:0.992]
Epoch [75/120    avg_loss:0.062, val_acc:0.992]
Epoch [76/120    avg_loss:0.077, val_acc:0.992]
Epoch [77/120    avg_loss:0.067, val_acc:0.992]
Epoch [78/120    avg_loss:0.064, val_acc:0.992]
Epoch [79/120    avg_loss:0.065, val_acc:0.992]
Epoch [80/120    avg_loss:0.058, val_acc:0.992]
Epoch [81/120    avg_loss:0.063, val_acc:0.992]
Epoch [82/120    avg_loss:0.055, val_acc:0.992]
Epoch [83/120    avg_loss:0.058, val_acc:0.992]
Epoch [84/120    avg_loss:0.064, val_acc:0.992]
Epoch [85/120    avg_loss:0.073, val_acc:0.992]
Epoch [86/120    avg_loss:0.056, val_acc:0.992]
Epoch [87/120    avg_loss:0.070, val_acc:0.992]
Epoch [88/120    avg_loss:0.055, val_acc:0.992]
Epoch [89/120    avg_loss:0.057, val_acc:0.992]
Epoch [90/120    avg_loss:0.056, val_acc:0.992]
Epoch [91/120    avg_loss:0.059, val_acc:0.992]
Epoch [92/120    avg_loss:0.055, val_acc:0.992]
Epoch [93/120    avg_loss:0.051, val_acc:0.994]
Epoch [94/120    avg_loss:0.051, val_acc:0.994]
Epoch [95/120    avg_loss:0.056, val_acc:0.994]
Epoch [96/120    avg_loss:0.055, val_acc:0.994]
Epoch [97/120    avg_loss:0.056, val_acc:0.994]
Epoch [98/120    avg_loss:0.054, val_acc:0.992]
Epoch [99/120    avg_loss:0.051, val_acc:0.994]
Epoch [100/120    avg_loss:0.056, val_acc:0.994]
Epoch [101/120    avg_loss:0.048, val_acc:0.994]
Epoch [102/120    avg_loss:0.052, val_acc:0.994]
Epoch [103/120    avg_loss:0.058, val_acc:0.994]
Epoch [104/120    avg_loss:0.055, val_acc:0.994]
Epoch [105/120    avg_loss:0.045, val_acc:0.994]
Epoch [106/120    avg_loss:0.052, val_acc:0.994]
Epoch [107/120    avg_loss:0.054, val_acc:0.994]
Epoch [108/120    avg_loss:0.060, val_acc:0.994]
Epoch [109/120    avg_loss:0.048, val_acc:0.994]
Epoch [110/120    avg_loss:0.047, val_acc:0.994]
Epoch [111/120    avg_loss:0.049, val_acc:0.994]
Epoch [112/120    avg_loss:0.052, val_acc:0.994]
Epoch [113/120    avg_loss:0.046, val_acc:0.994]
Epoch [114/120    avg_loss:0.049, val_acc:0.994]
Epoch [115/120    avg_loss:0.047, val_acc:0.994]
Epoch [116/120    avg_loss:0.049, val_acc:0.994]
Epoch [117/120    avg_loss:0.051, val_acc:0.994]
Epoch [118/120    avg_loss:0.045, val_acc:0.994]
Epoch [119/120    avg_loss:0.046, val_acc:0.994]
Epoch [120/120    avg_loss:0.051, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99264706 0.98871332 1.         0.95594714 0.93103448
 0.97630332 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9916934001875881
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b7f6f6e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.270]
Epoch [2/120    avg_loss:2.545, val_acc:0.300]
Epoch [3/120    avg_loss:2.478, val_acc:0.339]
Epoch [4/120    avg_loss:2.414, val_acc:0.361]
Epoch [5/120    avg_loss:2.345, val_acc:0.371]
Epoch [6/120    avg_loss:2.275, val_acc:0.395]
Epoch [7/120    avg_loss:2.182, val_acc:0.431]
Epoch [8/120    avg_loss:2.088, val_acc:0.474]
Epoch [9/120    avg_loss:1.985, val_acc:0.546]
Epoch [10/120    avg_loss:1.867, val_acc:0.593]
Epoch [11/120    avg_loss:1.738, val_acc:0.591]
Epoch [12/120    avg_loss:1.651, val_acc:0.615]
Epoch [13/120    avg_loss:1.511, val_acc:0.635]
Epoch [14/120    avg_loss:1.404, val_acc:0.657]
Epoch [15/120    avg_loss:1.305, val_acc:0.716]
Epoch [16/120    avg_loss:1.204, val_acc:0.714]
Epoch [17/120    avg_loss:1.092, val_acc:0.728]
Epoch [18/120    avg_loss:1.007, val_acc:0.760]
Epoch [19/120    avg_loss:0.904, val_acc:0.788]
Epoch [20/120    avg_loss:0.818, val_acc:0.808]
Epoch [21/120    avg_loss:0.749, val_acc:0.796]
Epoch [22/120    avg_loss:0.714, val_acc:0.855]
Epoch [23/120    avg_loss:0.654, val_acc:0.853]
Epoch [24/120    avg_loss:0.608, val_acc:0.901]
Epoch [25/120    avg_loss:0.579, val_acc:0.853]
Epoch [26/120    avg_loss:0.542, val_acc:0.851]
Epoch [27/120    avg_loss:0.503, val_acc:0.893]
Epoch [28/120    avg_loss:0.469, val_acc:0.915]
Epoch [29/120    avg_loss:0.484, val_acc:0.923]
Epoch [30/120    avg_loss:0.444, val_acc:0.921]
Epoch [31/120    avg_loss:0.409, val_acc:0.927]
Epoch [32/120    avg_loss:0.407, val_acc:0.938]
Epoch [33/120    avg_loss:0.357, val_acc:0.929]
Epoch [34/120    avg_loss:0.422, val_acc:0.911]
Epoch [35/120    avg_loss:0.393, val_acc:0.933]
Epoch [36/120    avg_loss:0.356, val_acc:0.919]
Epoch [37/120    avg_loss:0.372, val_acc:0.948]
Epoch [38/120    avg_loss:0.305, val_acc:0.950]
Epoch [39/120    avg_loss:0.275, val_acc:0.944]
Epoch [40/120    avg_loss:0.265, val_acc:0.958]
Epoch [41/120    avg_loss:0.216, val_acc:0.960]
Epoch [42/120    avg_loss:0.252, val_acc:0.948]
Epoch [43/120    avg_loss:0.250, val_acc:0.956]
Epoch [44/120    avg_loss:0.234, val_acc:0.968]
Epoch [45/120    avg_loss:0.225, val_acc:0.968]
Epoch [46/120    avg_loss:0.232, val_acc:0.952]
Epoch [47/120    avg_loss:0.267, val_acc:0.933]
Epoch [48/120    avg_loss:0.388, val_acc:0.935]
Epoch [49/120    avg_loss:0.249, val_acc:0.960]
Epoch [50/120    avg_loss:0.213, val_acc:0.960]
Epoch [51/120    avg_loss:0.214, val_acc:0.960]
Epoch [52/120    avg_loss:0.209, val_acc:0.962]
Epoch [53/120    avg_loss:0.183, val_acc:0.960]
Epoch [54/120    avg_loss:0.173, val_acc:0.968]
Epoch [55/120    avg_loss:0.154, val_acc:0.978]
Epoch [56/120    avg_loss:0.186, val_acc:0.964]
Epoch [57/120    avg_loss:0.177, val_acc:0.948]
Epoch [58/120    avg_loss:0.172, val_acc:0.966]
Epoch [59/120    avg_loss:0.129, val_acc:0.978]
Epoch [60/120    avg_loss:0.126, val_acc:0.966]
Epoch [61/120    avg_loss:0.159, val_acc:0.966]
Epoch [62/120    avg_loss:0.120, val_acc:0.984]
Epoch [63/120    avg_loss:0.114, val_acc:0.972]
Epoch [64/120    avg_loss:0.139, val_acc:0.980]
Epoch [65/120    avg_loss:0.130, val_acc:0.915]
Epoch [66/120    avg_loss:0.132, val_acc:0.966]
Epoch [67/120    avg_loss:0.113, val_acc:0.966]
Epoch [68/120    avg_loss:0.169, val_acc:0.968]
Epoch [69/120    avg_loss:0.128, val_acc:0.980]
Epoch [70/120    avg_loss:0.097, val_acc:0.958]
Epoch [71/120    avg_loss:0.097, val_acc:0.966]
Epoch [72/120    avg_loss:0.097, val_acc:0.978]
Epoch [73/120    avg_loss:0.136, val_acc:0.986]
Epoch [74/120    avg_loss:0.079, val_acc:0.984]
Epoch [75/120    avg_loss:0.091, val_acc:0.976]
Epoch [76/120    avg_loss:0.096, val_acc:0.982]
Epoch [77/120    avg_loss:0.090, val_acc:0.980]
Epoch [78/120    avg_loss:0.082, val_acc:0.980]
Epoch [79/120    avg_loss:0.076, val_acc:0.986]
Epoch [80/120    avg_loss:0.068, val_acc:0.986]
Epoch [81/120    avg_loss:0.057, val_acc:0.992]
Epoch [82/120    avg_loss:0.089, val_acc:0.984]
Epoch [83/120    avg_loss:0.056, val_acc:0.976]
Epoch [84/120    avg_loss:0.065, val_acc:0.982]
Epoch [85/120    avg_loss:0.056, val_acc:0.972]
Epoch [86/120    avg_loss:0.048, val_acc:0.988]
Epoch [87/120    avg_loss:0.047, val_acc:0.994]
Epoch [88/120    avg_loss:0.036, val_acc:0.974]
Epoch [89/120    avg_loss:0.045, val_acc:0.986]
Epoch [90/120    avg_loss:0.032, val_acc:0.990]
Epoch [91/120    avg_loss:0.032, val_acc:0.992]
Epoch [92/120    avg_loss:0.038, val_acc:0.976]
Epoch [93/120    avg_loss:0.060, val_acc:0.986]
Epoch [94/120    avg_loss:0.042, val_acc:0.994]
Epoch [95/120    avg_loss:0.030, val_acc:0.994]
Epoch [96/120    avg_loss:0.027, val_acc:0.990]
Epoch [97/120    avg_loss:0.023, val_acc:0.996]
Epoch [98/120    avg_loss:0.025, val_acc:0.994]
Epoch [99/120    avg_loss:0.029, val_acc:0.996]
Epoch [100/120    avg_loss:0.028, val_acc:0.990]
Epoch [101/120    avg_loss:0.086, val_acc:0.970]
Epoch [102/120    avg_loss:0.099, val_acc:0.982]
Epoch [103/120    avg_loss:0.058, val_acc:0.990]
Epoch [104/120    avg_loss:0.033, val_acc:0.988]
Epoch [105/120    avg_loss:0.040, val_acc:0.996]
Epoch [106/120    avg_loss:0.076, val_acc:0.970]
Epoch [107/120    avg_loss:0.053, val_acc:0.984]
Epoch [108/120    avg_loss:0.066, val_acc:0.976]
Epoch [109/120    avg_loss:0.074, val_acc:0.978]
Epoch [110/120    avg_loss:0.046, val_acc:0.988]
Epoch [111/120    avg_loss:0.027, val_acc:0.994]
Epoch [112/120    avg_loss:0.039, val_acc:0.990]
Epoch [113/120    avg_loss:0.051, val_acc:0.992]
Epoch [114/120    avg_loss:0.023, val_acc:0.988]
Epoch [115/120    avg_loss:0.018, val_acc:0.992]
Epoch [116/120    avg_loss:0.024, val_acc:0.996]
Epoch [117/120    avg_loss:0.014, val_acc:0.994]
Epoch [118/120    avg_loss:0.015, val_acc:0.994]
Epoch [119/120    avg_loss:0.015, val_acc:0.994]
Epoch [120/120    avg_loss:0.014, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99560117 0.98648649 1.         0.9466951  0.90909091
 0.98564593 0.9726776  1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9912174431609957
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0de5199ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.598, val_acc:0.312]
Epoch [2/120    avg_loss:2.537, val_acc:0.362]
Epoch [3/120    avg_loss:2.474, val_acc:0.415]
Epoch [4/120    avg_loss:2.405, val_acc:0.450]
Epoch [5/120    avg_loss:2.332, val_acc:0.483]
Epoch [6/120    avg_loss:2.251, val_acc:0.487]
Epoch [7/120    avg_loss:2.183, val_acc:0.533]
Epoch [8/120    avg_loss:2.122, val_acc:0.548]
Epoch [9/120    avg_loss:2.026, val_acc:0.550]
Epoch [10/120    avg_loss:1.934, val_acc:0.542]
Epoch [11/120    avg_loss:1.832, val_acc:0.625]
Epoch [12/120    avg_loss:1.723, val_acc:0.631]
Epoch [13/120    avg_loss:1.597, val_acc:0.642]
Epoch [14/120    avg_loss:1.498, val_acc:0.696]
Epoch [15/120    avg_loss:1.384, val_acc:0.735]
Epoch [16/120    avg_loss:1.302, val_acc:0.767]
Epoch [17/120    avg_loss:1.174, val_acc:0.769]
Epoch [18/120    avg_loss:1.092, val_acc:0.798]
Epoch [19/120    avg_loss:0.938, val_acc:0.865]
Epoch [20/120    avg_loss:0.885, val_acc:0.808]
Epoch [21/120    avg_loss:0.796, val_acc:0.885]
Epoch [22/120    avg_loss:0.715, val_acc:0.881]
Epoch [23/120    avg_loss:0.684, val_acc:0.906]
Epoch [24/120    avg_loss:0.643, val_acc:0.904]
Epoch [25/120    avg_loss:0.581, val_acc:0.892]
Epoch [26/120    avg_loss:0.558, val_acc:0.912]
Epoch [27/120    avg_loss:0.553, val_acc:0.896]
Epoch [28/120    avg_loss:0.516, val_acc:0.881]
Epoch [29/120    avg_loss:0.503, val_acc:0.915]
Epoch [30/120    avg_loss:0.483, val_acc:0.898]
Epoch [31/120    avg_loss:0.446, val_acc:0.871]
Epoch [32/120    avg_loss:0.391, val_acc:0.956]
Epoch [33/120    avg_loss:0.334, val_acc:0.950]
Epoch [34/120    avg_loss:0.434, val_acc:0.938]
Epoch [35/120    avg_loss:0.324, val_acc:0.915]
Epoch [36/120    avg_loss:0.323, val_acc:0.942]
Epoch [37/120    avg_loss:0.282, val_acc:0.963]
Epoch [38/120    avg_loss:0.310, val_acc:0.944]
Epoch [39/120    avg_loss:0.310, val_acc:0.948]
Epoch [40/120    avg_loss:0.283, val_acc:0.940]
Epoch [41/120    avg_loss:0.353, val_acc:0.931]
Epoch [42/120    avg_loss:0.313, val_acc:0.929]
Epoch [43/120    avg_loss:0.275, val_acc:0.954]
Epoch [44/120    avg_loss:0.223, val_acc:0.956]
Epoch [45/120    avg_loss:0.203, val_acc:0.948]
Epoch [46/120    avg_loss:0.218, val_acc:0.950]
Epoch [47/120    avg_loss:0.293, val_acc:0.898]
Epoch [48/120    avg_loss:0.277, val_acc:0.956]
Epoch [49/120    avg_loss:0.199, val_acc:0.958]
Epoch [50/120    avg_loss:0.195, val_acc:0.960]
Epoch [51/120    avg_loss:0.154, val_acc:0.963]
Epoch [52/120    avg_loss:0.161, val_acc:0.969]
Epoch [53/120    avg_loss:0.133, val_acc:0.975]
Epoch [54/120    avg_loss:0.133, val_acc:0.975]
Epoch [55/120    avg_loss:0.132, val_acc:0.971]
Epoch [56/120    avg_loss:0.132, val_acc:0.975]
Epoch [57/120    avg_loss:0.129, val_acc:0.973]
Epoch [58/120    avg_loss:0.129, val_acc:0.975]
Epoch [59/120    avg_loss:0.122, val_acc:0.979]
Epoch [60/120    avg_loss:0.134, val_acc:0.977]
Epoch [61/120    avg_loss:0.117, val_acc:0.981]
Epoch [62/120    avg_loss:0.120, val_acc:0.981]
Epoch [63/120    avg_loss:0.123, val_acc:0.979]
Epoch [64/120    avg_loss:0.120, val_acc:0.979]
Epoch [65/120    avg_loss:0.113, val_acc:0.981]
Epoch [66/120    avg_loss:0.112, val_acc:0.981]
Epoch [67/120    avg_loss:0.118, val_acc:0.983]
Epoch [68/120    avg_loss:0.120, val_acc:0.983]
Epoch [69/120    avg_loss:0.120, val_acc:0.979]
Epoch [70/120    avg_loss:0.112, val_acc:0.983]
Epoch [71/120    avg_loss:0.102, val_acc:0.985]
Epoch [72/120    avg_loss:0.117, val_acc:0.981]
Epoch [73/120    avg_loss:0.116, val_acc:0.983]
Epoch [74/120    avg_loss:0.107, val_acc:0.981]
Epoch [75/120    avg_loss:0.101, val_acc:0.985]
Epoch [76/120    avg_loss:0.100, val_acc:0.981]
Epoch [77/120    avg_loss:0.105, val_acc:0.983]
Epoch [78/120    avg_loss:0.103, val_acc:0.983]
Epoch [79/120    avg_loss:0.120, val_acc:0.979]
Epoch [80/120    avg_loss:0.098, val_acc:0.983]
Epoch [81/120    avg_loss:0.105, val_acc:0.983]
Epoch [82/120    avg_loss:0.092, val_acc:0.981]
Epoch [83/120    avg_loss:0.109, val_acc:0.983]
Epoch [84/120    avg_loss:0.105, val_acc:0.981]
Epoch [85/120    avg_loss:0.104, val_acc:0.985]
Epoch [86/120    avg_loss:0.098, val_acc:0.985]
Epoch [87/120    avg_loss:0.098, val_acc:0.988]
Epoch [88/120    avg_loss:0.110, val_acc:0.983]
Epoch [89/120    avg_loss:0.108, val_acc:0.985]
Epoch [90/120    avg_loss:0.108, val_acc:0.985]
Epoch [91/120    avg_loss:0.093, val_acc:0.988]
Epoch [92/120    avg_loss:0.094, val_acc:0.983]
Epoch [93/120    avg_loss:0.087, val_acc:0.985]
Epoch [94/120    avg_loss:0.092, val_acc:0.990]
Epoch [95/120    avg_loss:0.093, val_acc:0.988]
Epoch [96/120    avg_loss:0.090, val_acc:0.981]
Epoch [97/120    avg_loss:0.092, val_acc:0.983]
Epoch [98/120    avg_loss:0.090, val_acc:0.985]
Epoch [99/120    avg_loss:0.088, val_acc:0.992]
Epoch [100/120    avg_loss:0.078, val_acc:0.983]
Epoch [101/120    avg_loss:0.080, val_acc:0.985]
Epoch [102/120    avg_loss:0.090, val_acc:0.990]
Epoch [103/120    avg_loss:0.087, val_acc:0.983]
Epoch [104/120    avg_loss:0.079, val_acc:0.985]
Epoch [105/120    avg_loss:0.082, val_acc:0.983]
Epoch [106/120    avg_loss:0.076, val_acc:0.983]
Epoch [107/120    avg_loss:0.080, val_acc:0.985]
Epoch [108/120    avg_loss:0.080, val_acc:0.985]
Epoch [109/120    avg_loss:0.081, val_acc:0.985]
Epoch [110/120    avg_loss:0.086, val_acc:0.983]
Epoch [111/120    avg_loss:0.085, val_acc:0.985]
Epoch [112/120    avg_loss:0.084, val_acc:0.983]
Epoch [113/120    avg_loss:0.077, val_acc:0.983]
Epoch [114/120    avg_loss:0.082, val_acc:0.983]
Epoch [115/120    avg_loss:0.073, val_acc:0.983]
Epoch [116/120    avg_loss:0.084, val_acc:0.983]
Epoch [117/120    avg_loss:0.076, val_acc:0.983]
Epoch [118/120    avg_loss:0.084, val_acc:0.983]
Epoch [119/120    avg_loss:0.086, val_acc:0.983]
Epoch [120/120    avg_loss:0.073, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 196  31   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.99707174 0.99319728 1.         0.89090909 0.84210526
 0.99038462 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9869457966907658
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f833266cda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.653, val_acc:0.094]
Epoch [2/120    avg_loss:2.589, val_acc:0.383]
Epoch [3/120    avg_loss:2.520, val_acc:0.410]
Epoch [4/120    avg_loss:2.459, val_acc:0.425]
Epoch [5/120    avg_loss:2.405, val_acc:0.425]
Epoch [6/120    avg_loss:2.354, val_acc:0.446]
Epoch [7/120    avg_loss:2.291, val_acc:0.535]
Epoch [8/120    avg_loss:2.238, val_acc:0.565]
Epoch [9/120    avg_loss:2.163, val_acc:0.615]
Epoch [10/120    avg_loss:2.087, val_acc:0.646]
Epoch [11/120    avg_loss:2.005, val_acc:0.677]
Epoch [12/120    avg_loss:1.907, val_acc:0.662]
Epoch [13/120    avg_loss:1.799, val_acc:0.679]
Epoch [14/120    avg_loss:1.706, val_acc:0.694]
Epoch [15/120    avg_loss:1.578, val_acc:0.717]
Epoch [16/120    avg_loss:1.480, val_acc:0.723]
Epoch [17/120    avg_loss:1.392, val_acc:0.723]
Epoch [18/120    avg_loss:1.256, val_acc:0.727]
Epoch [19/120    avg_loss:1.170, val_acc:0.735]
Epoch [20/120    avg_loss:1.077, val_acc:0.779]
Epoch [21/120    avg_loss:0.972, val_acc:0.781]
Epoch [22/120    avg_loss:0.921, val_acc:0.810]
Epoch [23/120    avg_loss:0.837, val_acc:0.800]
Epoch [24/120    avg_loss:0.773, val_acc:0.846]
Epoch [25/120    avg_loss:0.747, val_acc:0.858]
Epoch [26/120    avg_loss:0.658, val_acc:0.904]
Epoch [27/120    avg_loss:0.592, val_acc:0.881]
Epoch [28/120    avg_loss:0.603, val_acc:0.923]
Epoch [29/120    avg_loss:0.539, val_acc:0.929]
Epoch [30/120    avg_loss:0.556, val_acc:0.933]
Epoch [31/120    avg_loss:0.480, val_acc:0.935]
Epoch [32/120    avg_loss:0.508, val_acc:0.942]
Epoch [33/120    avg_loss:0.433, val_acc:0.923]
Epoch [34/120    avg_loss:0.404, val_acc:0.963]
Epoch [35/120    avg_loss:0.391, val_acc:0.952]
Epoch [36/120    avg_loss:0.368, val_acc:0.935]
Epoch [37/120    avg_loss:0.397, val_acc:0.940]
Epoch [38/120    avg_loss:0.337, val_acc:0.929]
Epoch [39/120    avg_loss:0.364, val_acc:0.931]
Epoch [40/120    avg_loss:0.328, val_acc:0.948]
Epoch [41/120    avg_loss:0.340, val_acc:0.935]
Epoch [42/120    avg_loss:0.313, val_acc:0.956]
Epoch [43/120    avg_loss:0.331, val_acc:0.946]
Epoch [44/120    avg_loss:0.310, val_acc:0.944]
Epoch [45/120    avg_loss:0.250, val_acc:0.969]
Epoch [46/120    avg_loss:0.257, val_acc:0.931]
Epoch [47/120    avg_loss:0.274, val_acc:0.973]
Epoch [48/120    avg_loss:0.202, val_acc:0.979]
Epoch [49/120    avg_loss:0.209, val_acc:0.952]
Epoch [50/120    avg_loss:0.205, val_acc:0.933]
Epoch [51/120    avg_loss:0.232, val_acc:0.948]
Epoch [52/120    avg_loss:0.231, val_acc:0.969]
Epoch [53/120    avg_loss:0.198, val_acc:0.973]
Epoch [54/120    avg_loss:0.183, val_acc:0.963]
Epoch [55/120    avg_loss:0.175, val_acc:0.969]
Epoch [56/120    avg_loss:0.159, val_acc:0.960]
Epoch [57/120    avg_loss:0.146, val_acc:0.979]
Epoch [58/120    avg_loss:0.129, val_acc:0.971]
Epoch [59/120    avg_loss:0.158, val_acc:0.929]
Epoch [60/120    avg_loss:0.203, val_acc:0.902]
Epoch [61/120    avg_loss:0.215, val_acc:0.973]
Epoch [62/120    avg_loss:0.169, val_acc:0.963]
Epoch [63/120    avg_loss:0.198, val_acc:0.977]
Epoch [64/120    avg_loss:0.174, val_acc:0.981]
Epoch [65/120    avg_loss:0.173, val_acc:0.960]
Epoch [66/120    avg_loss:0.133, val_acc:0.981]
Epoch [67/120    avg_loss:0.157, val_acc:0.935]
Epoch [68/120    avg_loss:0.141, val_acc:0.960]
Epoch [69/120    avg_loss:0.123, val_acc:0.983]
Epoch [70/120    avg_loss:0.101, val_acc:0.977]
Epoch [71/120    avg_loss:0.114, val_acc:0.985]
Epoch [72/120    avg_loss:0.099, val_acc:0.979]
Epoch [73/120    avg_loss:0.115, val_acc:0.981]
Epoch [74/120    avg_loss:0.095, val_acc:0.988]
Epoch [75/120    avg_loss:0.079, val_acc:0.988]
Epoch [76/120    avg_loss:0.072, val_acc:0.985]
Epoch [77/120    avg_loss:0.064, val_acc:0.992]
Epoch [78/120    avg_loss:0.064, val_acc:0.994]
Epoch [79/120    avg_loss:0.072, val_acc:0.988]
Epoch [80/120    avg_loss:0.057, val_acc:0.988]
Epoch [81/120    avg_loss:0.067, val_acc:0.990]
Epoch [82/120    avg_loss:0.125, val_acc:0.967]
Epoch [83/120    avg_loss:0.099, val_acc:0.981]
Epoch [84/120    avg_loss:0.077, val_acc:0.979]
Epoch [85/120    avg_loss:0.096, val_acc:0.979]
Epoch [86/120    avg_loss:0.133, val_acc:0.985]
Epoch [87/120    avg_loss:0.102, val_acc:0.985]
Epoch [88/120    avg_loss:0.077, val_acc:0.965]
Epoch [89/120    avg_loss:0.083, val_acc:0.985]
Epoch [90/120    avg_loss:0.081, val_acc:0.981]
Epoch [91/120    avg_loss:0.061, val_acc:0.992]
Epoch [92/120    avg_loss:0.047, val_acc:0.992]
Epoch [93/120    avg_loss:0.039, val_acc:0.994]
Epoch [94/120    avg_loss:0.040, val_acc:0.990]
Epoch [95/120    avg_loss:0.033, val_acc:0.992]
Epoch [96/120    avg_loss:0.031, val_acc:0.992]
Epoch [97/120    avg_loss:0.027, val_acc:0.994]
Epoch [98/120    avg_loss:0.035, val_acc:0.994]
Epoch [99/120    avg_loss:0.032, val_acc:0.994]
Epoch [100/120    avg_loss:0.036, val_acc:0.994]
Epoch [101/120    avg_loss:0.031, val_acc:0.994]
Epoch [102/120    avg_loss:0.033, val_acc:0.994]
Epoch [103/120    avg_loss:0.034, val_acc:0.994]
Epoch [104/120    avg_loss:0.028, val_acc:0.992]
Epoch [105/120    avg_loss:0.032, val_acc:0.994]
Epoch [106/120    avg_loss:0.034, val_acc:0.994]
Epoch [107/120    avg_loss:0.033, val_acc:0.992]
Epoch [108/120    avg_loss:0.030, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.992]
Epoch [110/120    avg_loss:0.033, val_acc:0.990]
Epoch [111/120    avg_loss:0.032, val_acc:0.992]
Epoch [112/120    avg_loss:0.031, val_acc:0.992]
Epoch [113/120    avg_loss:0.030, val_acc:0.990]
Epoch [114/120    avg_loss:0.026, val_acc:0.990]
Epoch [115/120    avg_loss:0.030, val_acc:0.990]
Epoch [116/120    avg_loss:0.027, val_acc:0.992]
Epoch [117/120    avg_loss:0.031, val_acc:0.992]
Epoch [118/120    avg_loss:0.026, val_acc:0.994]
Epoch [119/120    avg_loss:0.028, val_acc:0.992]
Epoch [120/120    avg_loss:0.026, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99707174 0.99095023 1.         0.93959732 0.90909091
 0.99038462 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9916924858369915
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff8aeac7eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.183]
Epoch [2/120    avg_loss:2.560, val_acc:0.273]
Epoch [3/120    avg_loss:2.498, val_acc:0.273]
Epoch [4/120    avg_loss:2.442, val_acc:0.285]
Epoch [5/120    avg_loss:2.386, val_acc:0.306]
Epoch [6/120    avg_loss:2.330, val_acc:0.335]
Epoch [7/120    avg_loss:2.280, val_acc:0.367]
Epoch [8/120    avg_loss:2.235, val_acc:0.448]
Epoch [9/120    avg_loss:2.166, val_acc:0.567]
Epoch [10/120    avg_loss:2.090, val_acc:0.604]
Epoch [11/120    avg_loss:2.011, val_acc:0.596]
Epoch [12/120    avg_loss:1.895, val_acc:0.621]
Epoch [13/120    avg_loss:1.796, val_acc:0.619]
Epoch [14/120    avg_loss:1.673, val_acc:0.640]
Epoch [15/120    avg_loss:1.546, val_acc:0.658]
Epoch [16/120    avg_loss:1.468, val_acc:0.675]
Epoch [17/120    avg_loss:1.343, val_acc:0.669]
Epoch [18/120    avg_loss:1.215, val_acc:0.715]
Epoch [19/120    avg_loss:1.168, val_acc:0.738]
Epoch [20/120    avg_loss:1.081, val_acc:0.850]
Epoch [21/120    avg_loss:0.952, val_acc:0.752]
Epoch [22/120    avg_loss:0.868, val_acc:0.840]
Epoch [23/120    avg_loss:0.794, val_acc:0.856]
Epoch [24/120    avg_loss:0.726, val_acc:0.879]
Epoch [25/120    avg_loss:0.677, val_acc:0.867]
Epoch [26/120    avg_loss:0.647, val_acc:0.875]
Epoch [27/120    avg_loss:0.552, val_acc:0.879]
Epoch [28/120    avg_loss:0.529, val_acc:0.910]
Epoch [29/120    avg_loss:0.611, val_acc:0.915]
Epoch [30/120    avg_loss:0.587, val_acc:0.919]
Epoch [31/120    avg_loss:0.506, val_acc:0.931]
Epoch [32/120    avg_loss:0.446, val_acc:0.931]
Epoch [33/120    avg_loss:0.400, val_acc:0.940]
Epoch [34/120    avg_loss:0.355, val_acc:0.948]
Epoch [35/120    avg_loss:0.312, val_acc:0.940]
Epoch [36/120    avg_loss:0.405, val_acc:0.929]
Epoch [37/120    avg_loss:0.409, val_acc:0.881]
Epoch [38/120    avg_loss:0.351, val_acc:0.952]
Epoch [39/120    avg_loss:0.291, val_acc:0.960]
Epoch [40/120    avg_loss:0.268, val_acc:0.958]
Epoch [41/120    avg_loss:0.225, val_acc:0.967]
Epoch [42/120    avg_loss:0.248, val_acc:0.952]
Epoch [43/120    avg_loss:0.261, val_acc:0.965]
Epoch [44/120    avg_loss:0.243, val_acc:0.981]
Epoch [45/120    avg_loss:0.202, val_acc:0.960]
Epoch [46/120    avg_loss:0.254, val_acc:0.940]
Epoch [47/120    avg_loss:0.254, val_acc:0.965]
Epoch [48/120    avg_loss:0.208, val_acc:0.973]
Epoch [49/120    avg_loss:0.164, val_acc:0.965]
Epoch [50/120    avg_loss:0.143, val_acc:0.983]
Epoch [51/120    avg_loss:0.149, val_acc:0.967]
Epoch [52/120    avg_loss:0.136, val_acc:0.979]
Epoch [53/120    avg_loss:0.184, val_acc:0.981]
Epoch [54/120    avg_loss:0.110, val_acc:0.983]
Epoch [55/120    avg_loss:0.160, val_acc:0.983]
Epoch [56/120    avg_loss:0.135, val_acc:0.958]
Epoch [57/120    avg_loss:0.154, val_acc:0.981]
Epoch [58/120    avg_loss:0.099, val_acc:0.981]
Epoch [59/120    avg_loss:0.111, val_acc:0.977]
Epoch [60/120    avg_loss:0.114, val_acc:0.969]
Epoch [61/120    avg_loss:0.099, val_acc:0.981]
Epoch [62/120    avg_loss:0.060, val_acc:0.979]
Epoch [63/120    avg_loss:0.081, val_acc:0.988]
Epoch [64/120    avg_loss:0.082, val_acc:0.981]
Epoch [65/120    avg_loss:0.095, val_acc:0.973]
Epoch [66/120    avg_loss:0.106, val_acc:0.917]
Epoch [67/120    avg_loss:0.151, val_acc:0.965]
Epoch [68/120    avg_loss:0.125, val_acc:0.983]
Epoch [69/120    avg_loss:0.080, val_acc:0.985]
Epoch [70/120    avg_loss:0.070, val_acc:0.975]
Epoch [71/120    avg_loss:0.118, val_acc:0.975]
Epoch [72/120    avg_loss:0.061, val_acc:0.983]
Epoch [73/120    avg_loss:0.065, val_acc:0.988]
Epoch [74/120    avg_loss:0.078, val_acc:0.988]
Epoch [75/120    avg_loss:0.044, val_acc:0.985]
Epoch [76/120    avg_loss:0.044, val_acc:0.990]
Epoch [77/120    avg_loss:0.040, val_acc:0.990]
Epoch [78/120    avg_loss:0.038, val_acc:0.990]
Epoch [79/120    avg_loss:0.046, val_acc:0.985]
Epoch [80/120    avg_loss:0.064, val_acc:0.990]
Epoch [81/120    avg_loss:0.060, val_acc:0.992]
Epoch [82/120    avg_loss:0.036, val_acc:0.990]
Epoch [83/120    avg_loss:0.032, val_acc:0.988]
Epoch [84/120    avg_loss:0.025, val_acc:0.988]
Epoch [85/120    avg_loss:0.024, val_acc:0.994]
Epoch [86/120    avg_loss:0.022, val_acc:0.994]
Epoch [87/120    avg_loss:0.024, val_acc:0.992]
Epoch [88/120    avg_loss:0.019, val_acc:0.994]
Epoch [89/120    avg_loss:0.021, val_acc:0.983]
Epoch [90/120    avg_loss:0.041, val_acc:0.994]
Epoch [91/120    avg_loss:0.028, val_acc:0.990]
Epoch [92/120    avg_loss:0.022, val_acc:0.985]
Epoch [93/120    avg_loss:0.020, val_acc:0.990]
Epoch [94/120    avg_loss:0.017, val_acc:0.994]
Epoch [95/120    avg_loss:0.016, val_acc:0.990]
Epoch [96/120    avg_loss:0.014, val_acc:0.994]
Epoch [97/120    avg_loss:0.014, val_acc:0.992]
Epoch [98/120    avg_loss:0.014, val_acc:0.988]
Epoch [99/120    avg_loss:0.012, val_acc:0.992]
Epoch [100/120    avg_loss:0.014, val_acc:0.994]
Epoch [101/120    avg_loss:0.014, val_acc:0.992]
Epoch [102/120    avg_loss:0.014, val_acc:0.992]
Epoch [103/120    avg_loss:0.015, val_acc:0.992]
Epoch [104/120    avg_loss:0.018, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.994]
Epoch [106/120    avg_loss:0.012, val_acc:0.994]
Epoch [107/120    avg_loss:0.011, val_acc:0.994]
Epoch [108/120    avg_loss:0.010, val_acc:0.994]
Epoch [109/120    avg_loss:0.011, val_acc:0.996]
Epoch [110/120    avg_loss:0.012, val_acc:0.994]
Epoch [111/120    avg_loss:0.013, val_acc:0.992]
Epoch [112/120    avg_loss:0.011, val_acc:0.994]
Epoch [113/120    avg_loss:0.011, val_acc:0.992]
Epoch [114/120    avg_loss:0.009, val_acc:0.994]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.994]
Epoch [117/120    avg_loss:0.011, val_acc:0.996]
Epoch [118/120    avg_loss:0.010, val_acc:0.994]
Epoch [119/120    avg_loss:0.012, val_acc:0.994]
Epoch [120/120    avg_loss:0.008, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   0   9   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99338722 0.9977221  1.         0.95633188 0.93006993
 0.97862233 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9928799466911943
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f503fd75dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.194]
Epoch [2/120    avg_loss:2.555, val_acc:0.315]
Epoch [3/120    avg_loss:2.491, val_acc:0.352]
Epoch [4/120    avg_loss:2.428, val_acc:0.377]
Epoch [5/120    avg_loss:2.362, val_acc:0.402]
Epoch [6/120    avg_loss:2.303, val_acc:0.402]
Epoch [7/120    avg_loss:2.248, val_acc:0.423]
Epoch [8/120    avg_loss:2.179, val_acc:0.429]
Epoch [9/120    avg_loss:2.087, val_acc:0.427]
Epoch [10/120    avg_loss:2.024, val_acc:0.433]
Epoch [11/120    avg_loss:1.923, val_acc:0.458]
Epoch [12/120    avg_loss:1.851, val_acc:0.487]
Epoch [13/120    avg_loss:1.766, val_acc:0.573]
Epoch [14/120    avg_loss:1.696, val_acc:0.615]
Epoch [15/120    avg_loss:1.562, val_acc:0.654]
Epoch [16/120    avg_loss:1.478, val_acc:0.677]
Epoch [17/120    avg_loss:1.357, val_acc:0.713]
Epoch [18/120    avg_loss:1.275, val_acc:0.742]
Epoch [19/120    avg_loss:1.179, val_acc:0.723]
Epoch [20/120    avg_loss:1.047, val_acc:0.742]
Epoch [21/120    avg_loss:0.982, val_acc:0.783]
Epoch [22/120    avg_loss:0.919, val_acc:0.823]
Epoch [23/120    avg_loss:0.800, val_acc:0.808]
Epoch [24/120    avg_loss:0.755, val_acc:0.760]
Epoch [25/120    avg_loss:0.697, val_acc:0.800]
Epoch [26/120    avg_loss:0.657, val_acc:0.892]
Epoch [27/120    avg_loss:0.604, val_acc:0.881]
Epoch [28/120    avg_loss:0.576, val_acc:0.885]
Epoch [29/120    avg_loss:0.561, val_acc:0.906]
Epoch [30/120    avg_loss:0.533, val_acc:0.898]
Epoch [31/120    avg_loss:0.490, val_acc:0.890]
Epoch [32/120    avg_loss:0.552, val_acc:0.915]
Epoch [33/120    avg_loss:0.452, val_acc:0.881]
Epoch [34/120    avg_loss:0.439, val_acc:0.900]
Epoch [35/120    avg_loss:0.466, val_acc:0.908]
Epoch [36/120    avg_loss:0.407, val_acc:0.931]
Epoch [37/120    avg_loss:0.406, val_acc:0.875]
Epoch [38/120    avg_loss:0.428, val_acc:0.904]
Epoch [39/120    avg_loss:0.421, val_acc:0.908]
Epoch [40/120    avg_loss:0.419, val_acc:0.929]
Epoch [41/120    avg_loss:0.396, val_acc:0.908]
Epoch [42/120    avg_loss:0.407, val_acc:0.921]
Epoch [43/120    avg_loss:0.300, val_acc:0.929]
Epoch [44/120    avg_loss:0.285, val_acc:0.956]
Epoch [45/120    avg_loss:0.316, val_acc:0.952]
Epoch [46/120    avg_loss:0.282, val_acc:0.927]
Epoch [47/120    avg_loss:0.308, val_acc:0.956]
Epoch [48/120    avg_loss:0.283, val_acc:0.946]
Epoch [49/120    avg_loss:0.244, val_acc:0.952]
Epoch [50/120    avg_loss:0.227, val_acc:0.954]
Epoch [51/120    avg_loss:0.217, val_acc:0.956]
Epoch [52/120    avg_loss:0.265, val_acc:0.965]
Epoch [53/120    avg_loss:0.225, val_acc:0.954]
Epoch [54/120    avg_loss:0.195, val_acc:0.950]
Epoch [55/120    avg_loss:0.212, val_acc:0.965]
Epoch [56/120    avg_loss:0.251, val_acc:0.969]
Epoch [57/120    avg_loss:0.207, val_acc:0.956]
Epoch [58/120    avg_loss:0.154, val_acc:0.981]
Epoch [59/120    avg_loss:0.160, val_acc:0.977]
Epoch [60/120    avg_loss:0.169, val_acc:0.960]
Epoch [61/120    avg_loss:0.159, val_acc:0.973]
Epoch [62/120    avg_loss:0.163, val_acc:0.967]
Epoch [63/120    avg_loss:0.143, val_acc:0.973]
Epoch [64/120    avg_loss:0.186, val_acc:0.971]
Epoch [65/120    avg_loss:0.137, val_acc:0.975]
Epoch [66/120    avg_loss:0.134, val_acc:0.969]
Epoch [67/120    avg_loss:0.190, val_acc:0.973]
Epoch [68/120    avg_loss:0.182, val_acc:0.975]
Epoch [69/120    avg_loss:0.147, val_acc:0.979]
Epoch [70/120    avg_loss:0.144, val_acc:0.979]
Epoch [71/120    avg_loss:0.126, val_acc:0.981]
Epoch [72/120    avg_loss:0.112, val_acc:0.977]
Epoch [73/120    avg_loss:0.142, val_acc:0.973]
Epoch [74/120    avg_loss:0.148, val_acc:0.975]
Epoch [75/120    avg_loss:0.105, val_acc:0.969]
Epoch [76/120    avg_loss:0.112, val_acc:0.956]
Epoch [77/120    avg_loss:0.150, val_acc:0.985]
Epoch [78/120    avg_loss:0.116, val_acc:0.977]
Epoch [79/120    avg_loss:0.094, val_acc:0.979]
Epoch [80/120    avg_loss:0.084, val_acc:0.977]
Epoch [81/120    avg_loss:0.083, val_acc:0.985]
Epoch [82/120    avg_loss:0.102, val_acc:0.973]
Epoch [83/120    avg_loss:0.100, val_acc:0.965]
Epoch [84/120    avg_loss:0.113, val_acc:0.983]
Epoch [85/120    avg_loss:0.077, val_acc:0.981]
Epoch [86/120    avg_loss:0.073, val_acc:0.988]
Epoch [87/120    avg_loss:0.077, val_acc:0.975]
Epoch [88/120    avg_loss:0.081, val_acc:0.992]
Epoch [89/120    avg_loss:0.071, val_acc:0.988]
Epoch [90/120    avg_loss:0.055, val_acc:0.985]
Epoch [91/120    avg_loss:0.065, val_acc:0.992]
Epoch [92/120    avg_loss:0.056, val_acc:0.992]
Epoch [93/120    avg_loss:0.046, val_acc:0.988]
Epoch [94/120    avg_loss:0.048, val_acc:0.990]
Epoch [95/120    avg_loss:0.050, val_acc:0.979]
Epoch [96/120    avg_loss:0.091, val_acc:0.981]
Epoch [97/120    avg_loss:0.080, val_acc:0.969]
Epoch [98/120    avg_loss:0.061, val_acc:0.990]
Epoch [99/120    avg_loss:0.042, val_acc:0.996]
Epoch [100/120    avg_loss:0.062, val_acc:0.983]
Epoch [101/120    avg_loss:0.059, val_acc:0.985]
Epoch [102/120    avg_loss:0.065, val_acc:0.994]
Epoch [103/120    avg_loss:0.039, val_acc:0.992]
Epoch [104/120    avg_loss:0.060, val_acc:0.994]
Epoch [105/120    avg_loss:0.036, val_acc:0.996]
Epoch [106/120    avg_loss:0.039, val_acc:0.994]
Epoch [107/120    avg_loss:0.053, val_acc:0.990]
Epoch [108/120    avg_loss:0.046, val_acc:0.983]
Epoch [109/120    avg_loss:0.046, val_acc:0.985]
Epoch [110/120    avg_loss:0.035, val_acc:0.990]
Epoch [111/120    avg_loss:0.061, val_acc:0.983]
Epoch [112/120    avg_loss:0.061, val_acc:0.985]
Epoch [113/120    avg_loss:0.060, val_acc:0.983]
Epoch [114/120    avg_loss:0.104, val_acc:0.981]
Epoch [115/120    avg_loss:0.058, val_acc:0.990]
Epoch [116/120    avg_loss:0.043, val_acc:0.988]
Epoch [117/120    avg_loss:0.045, val_acc:0.988]
Epoch [118/120    avg_loss:0.058, val_acc:0.988]
Epoch [119/120    avg_loss:0.035, val_acc:0.992]
Epoch [120/120    avg_loss:0.021, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   3   0   0   0   0   0   0   1   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 0.99926954 0.9977221  1.         0.96328294 0.94285714
 0.99757869 0.99465241 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9954895943813638
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff125ef6e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.599, val_acc:0.273]
Epoch [2/120    avg_loss:2.529, val_acc:0.298]
Epoch [3/120    avg_loss:2.461, val_acc:0.298]
Epoch [4/120    avg_loss:2.387, val_acc:0.317]
Epoch [5/120    avg_loss:2.333, val_acc:0.373]
Epoch [6/120    avg_loss:2.269, val_acc:0.396]
Epoch [7/120    avg_loss:2.213, val_acc:0.404]
Epoch [8/120    avg_loss:2.139, val_acc:0.417]
Epoch [9/120    avg_loss:2.086, val_acc:0.421]
Epoch [10/120    avg_loss:2.009, val_acc:0.446]
Epoch [11/120    avg_loss:1.913, val_acc:0.479]
Epoch [12/120    avg_loss:1.835, val_acc:0.525]
Epoch [13/120    avg_loss:1.727, val_acc:0.519]
Epoch [14/120    avg_loss:1.609, val_acc:0.552]
Epoch [15/120    avg_loss:1.551, val_acc:0.546]
Epoch [16/120    avg_loss:1.461, val_acc:0.552]
Epoch [17/120    avg_loss:1.334, val_acc:0.575]
Epoch [18/120    avg_loss:1.257, val_acc:0.671]
Epoch [19/120    avg_loss:1.165, val_acc:0.704]
Epoch [20/120    avg_loss:1.043, val_acc:0.740]
Epoch [21/120    avg_loss:0.940, val_acc:0.765]
Epoch [22/120    avg_loss:0.840, val_acc:0.785]
Epoch [23/120    avg_loss:0.781, val_acc:0.792]
Epoch [24/120    avg_loss:0.732, val_acc:0.835]
Epoch [25/120    avg_loss:0.694, val_acc:0.829]
Epoch [26/120    avg_loss:0.642, val_acc:0.812]
Epoch [27/120    avg_loss:0.613, val_acc:0.865]
Epoch [28/120    avg_loss:0.521, val_acc:0.885]
Epoch [29/120    avg_loss:0.558, val_acc:0.833]
Epoch [30/120    avg_loss:0.507, val_acc:0.883]
Epoch [31/120    avg_loss:0.444, val_acc:0.887]
Epoch [32/120    avg_loss:0.418, val_acc:0.863]
Epoch [33/120    avg_loss:0.470, val_acc:0.935]
Epoch [34/120    avg_loss:0.400, val_acc:0.842]
Epoch [35/120    avg_loss:0.421, val_acc:0.912]
Epoch [36/120    avg_loss:0.361, val_acc:0.915]
Epoch [37/120    avg_loss:0.340, val_acc:0.931]
Epoch [38/120    avg_loss:0.304, val_acc:0.946]
Epoch [39/120    avg_loss:0.302, val_acc:0.946]
Epoch [40/120    avg_loss:0.308, val_acc:0.935]
Epoch [41/120    avg_loss:0.242, val_acc:0.948]
Epoch [42/120    avg_loss:0.218, val_acc:0.954]
Epoch [43/120    avg_loss:0.254, val_acc:0.954]
Epoch [44/120    avg_loss:0.238, val_acc:0.929]
Epoch [45/120    avg_loss:0.222, val_acc:0.948]
Epoch [46/120    avg_loss:0.251, val_acc:0.960]
Epoch [47/120    avg_loss:0.253, val_acc:0.952]
Epoch [48/120    avg_loss:0.207, val_acc:0.963]
Epoch [49/120    avg_loss:0.219, val_acc:0.906]
Epoch [50/120    avg_loss:0.190, val_acc:0.963]
Epoch [51/120    avg_loss:0.215, val_acc:0.944]
Epoch [52/120    avg_loss:0.172, val_acc:0.965]
Epoch [53/120    avg_loss:0.224, val_acc:0.958]
Epoch [54/120    avg_loss:0.173, val_acc:0.971]
Epoch [55/120    avg_loss:0.183, val_acc:0.977]
Epoch [56/120    avg_loss:0.151, val_acc:0.977]
Epoch [57/120    avg_loss:0.164, val_acc:0.963]
Epoch [58/120    avg_loss:0.138, val_acc:0.979]
Epoch [59/120    avg_loss:0.135, val_acc:0.969]
Epoch [60/120    avg_loss:0.148, val_acc:0.981]
Epoch [61/120    avg_loss:0.109, val_acc:0.979]
Epoch [62/120    avg_loss:0.119, val_acc:0.967]
Epoch [63/120    avg_loss:0.163, val_acc:0.973]
Epoch [64/120    avg_loss:0.128, val_acc:0.977]
Epoch [65/120    avg_loss:0.122, val_acc:0.977]
Epoch [66/120    avg_loss:0.105, val_acc:0.981]
Epoch [67/120    avg_loss:0.097, val_acc:0.990]
Epoch [68/120    avg_loss:0.109, val_acc:0.973]
Epoch [69/120    avg_loss:0.081, val_acc:0.975]
Epoch [70/120    avg_loss:0.075, val_acc:0.975]
Epoch [71/120    avg_loss:0.057, val_acc:0.963]
Epoch [72/120    avg_loss:0.079, val_acc:0.954]
Epoch [73/120    avg_loss:0.198, val_acc:0.950]
Epoch [74/120    avg_loss:0.162, val_acc:0.973]
Epoch [75/120    avg_loss:0.106, val_acc:0.973]
Epoch [76/120    avg_loss:0.127, val_acc:0.950]
Epoch [77/120    avg_loss:0.119, val_acc:0.988]
Epoch [78/120    avg_loss:0.081, val_acc:0.971]
Epoch [79/120    avg_loss:0.116, val_acc:0.975]
Epoch [80/120    avg_loss:0.105, val_acc:0.965]
Epoch [81/120    avg_loss:0.095, val_acc:0.985]
Epoch [82/120    avg_loss:0.058, val_acc:0.983]
Epoch [83/120    avg_loss:0.059, val_acc:0.988]
Epoch [84/120    avg_loss:0.075, val_acc:0.990]
Epoch [85/120    avg_loss:0.052, val_acc:0.990]
Epoch [86/120    avg_loss:0.054, val_acc:0.985]
Epoch [87/120    avg_loss:0.049, val_acc:0.985]
Epoch [88/120    avg_loss:0.042, val_acc:0.990]
Epoch [89/120    avg_loss:0.050, val_acc:0.992]
Epoch [90/120    avg_loss:0.040, val_acc:0.988]
Epoch [91/120    avg_loss:0.040, val_acc:0.988]
Epoch [92/120    avg_loss:0.043, val_acc:0.988]
Epoch [93/120    avg_loss:0.043, val_acc:0.994]
Epoch [94/120    avg_loss:0.039, val_acc:0.996]
Epoch [95/120    avg_loss:0.041, val_acc:0.994]
Epoch [96/120    avg_loss:0.039, val_acc:0.992]
Epoch [97/120    avg_loss:0.045, val_acc:0.990]
Epoch [98/120    avg_loss:0.039, val_acc:0.988]
Epoch [99/120    avg_loss:0.044, val_acc:0.994]
Epoch [100/120    avg_loss:0.041, val_acc:0.990]
Epoch [101/120    avg_loss:0.047, val_acc:0.992]
Epoch [102/120    avg_loss:0.035, val_acc:0.992]
Epoch [103/120    avg_loss:0.038, val_acc:0.988]
Epoch [104/120    avg_loss:0.035, val_acc:0.992]
Epoch [105/120    avg_loss:0.041, val_acc:0.992]
Epoch [106/120    avg_loss:0.035, val_acc:0.990]
Epoch [107/120    avg_loss:0.040, val_acc:0.992]
Epoch [108/120    avg_loss:0.040, val_acc:0.992]
Epoch [109/120    avg_loss:0.035, val_acc:0.992]
Epoch [110/120    avg_loss:0.039, val_acc:0.992]
Epoch [111/120    avg_loss:0.040, val_acc:0.992]
Epoch [112/120    avg_loss:0.038, val_acc:0.992]
Epoch [113/120    avg_loss:0.033, val_acc:0.992]
Epoch [114/120    avg_loss:0.040, val_acc:0.992]
Epoch [115/120    avg_loss:0.035, val_acc:0.992]
Epoch [116/120    avg_loss:0.034, val_acc:0.992]
Epoch [117/120    avg_loss:0.042, val_acc:0.992]
Epoch [118/120    avg_loss:0.041, val_acc:0.992]
Epoch [119/120    avg_loss:0.031, val_acc:0.992]
Epoch [120/120    avg_loss:0.040, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99708029 0.99319728 0.98454746 0.91067538 0.88356164
 0.99516908 0.98378378 0.99741602 1.         1.         1.
 1.         1.        ]

Kappa:
0.9886057905178232
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac44d42e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.617, val_acc:0.143]
Epoch [2/120    avg_loss:2.586, val_acc:0.288]
Epoch [3/120    avg_loss:2.543, val_acc:0.296]
Epoch [4/120    avg_loss:2.499, val_acc:0.298]
Epoch [5/120    avg_loss:2.459, val_acc:0.298]
Epoch [6/120    avg_loss:2.419, val_acc:0.315]
Epoch [7/120    avg_loss:2.373, val_acc:0.389]
Epoch [8/120    avg_loss:2.324, val_acc:0.460]
Epoch [9/120    avg_loss:2.272, val_acc:0.452]
Epoch [10/120    avg_loss:2.215, val_acc:0.548]
Epoch [11/120    avg_loss:2.143, val_acc:0.653]
Epoch [12/120    avg_loss:2.066, val_acc:0.591]
Epoch [13/120    avg_loss:1.982, val_acc:0.657]
Epoch [14/120    avg_loss:1.898, val_acc:0.677]
Epoch [15/120    avg_loss:1.810, val_acc:0.702]
Epoch [16/120    avg_loss:1.705, val_acc:0.714]
Epoch [17/120    avg_loss:1.613, val_acc:0.736]
Epoch [18/120    avg_loss:1.495, val_acc:0.746]
Epoch [19/120    avg_loss:1.434, val_acc:0.829]
Epoch [20/120    avg_loss:1.336, val_acc:0.812]
Epoch [21/120    avg_loss:1.241, val_acc:0.802]
Epoch [22/120    avg_loss:1.166, val_acc:0.835]
Epoch [23/120    avg_loss:1.060, val_acc:0.857]
Epoch [24/120    avg_loss:0.991, val_acc:0.881]
Epoch [25/120    avg_loss:0.925, val_acc:0.887]
Epoch [26/120    avg_loss:0.896, val_acc:0.887]
Epoch [27/120    avg_loss:0.839, val_acc:0.865]
Epoch [28/120    avg_loss:0.776, val_acc:0.887]
Epoch [29/120    avg_loss:0.745, val_acc:0.891]
Epoch [30/120    avg_loss:0.729, val_acc:0.863]
Epoch [31/120    avg_loss:0.636, val_acc:0.889]
Epoch [32/120    avg_loss:0.620, val_acc:0.893]
Epoch [33/120    avg_loss:0.563, val_acc:0.901]
Epoch [34/120    avg_loss:0.529, val_acc:0.883]
Epoch [35/120    avg_loss:0.527, val_acc:0.901]
Epoch [36/120    avg_loss:0.519, val_acc:0.897]
Epoch [37/120    avg_loss:0.519, val_acc:0.911]
Epoch [38/120    avg_loss:0.482, val_acc:0.917]
Epoch [39/120    avg_loss:0.477, val_acc:0.911]
Epoch [40/120    avg_loss:0.444, val_acc:0.909]
Epoch [41/120    avg_loss:0.400, val_acc:0.931]
Epoch [42/120    avg_loss:0.430, val_acc:0.919]
Epoch [43/120    avg_loss:0.406, val_acc:0.935]
Epoch [44/120    avg_loss:0.474, val_acc:0.889]
Epoch [45/120    avg_loss:0.417, val_acc:0.901]
Epoch [46/120    avg_loss:0.460, val_acc:0.917]
Epoch [47/120    avg_loss:0.361, val_acc:0.935]
Epoch [48/120    avg_loss:0.349, val_acc:0.919]
Epoch [49/120    avg_loss:0.312, val_acc:0.929]
Epoch [50/120    avg_loss:0.320, val_acc:0.913]
Epoch [51/120    avg_loss:0.344, val_acc:0.923]
Epoch [52/120    avg_loss:0.346, val_acc:0.897]
Epoch [53/120    avg_loss:0.439, val_acc:0.931]
Epoch [54/120    avg_loss:0.361, val_acc:0.944]
Epoch [55/120    avg_loss:0.281, val_acc:0.940]
Epoch [56/120    avg_loss:0.283, val_acc:0.946]
Epoch [57/120    avg_loss:0.302, val_acc:0.927]
Epoch [58/120    avg_loss:0.279, val_acc:0.956]
Epoch [59/120    avg_loss:0.271, val_acc:0.942]
Epoch [60/120    avg_loss:0.233, val_acc:0.933]
Epoch [61/120    avg_loss:0.291, val_acc:0.950]
Epoch [62/120    avg_loss:0.268, val_acc:0.923]
Epoch [63/120    avg_loss:0.235, val_acc:0.964]
Epoch [64/120    avg_loss:0.211, val_acc:0.968]
Epoch [65/120    avg_loss:0.303, val_acc:0.923]
Epoch [66/120    avg_loss:0.315, val_acc:0.944]
Epoch [67/120    avg_loss:0.234, val_acc:0.956]
Epoch [68/120    avg_loss:0.199, val_acc:0.952]
Epoch [69/120    avg_loss:0.193, val_acc:0.962]
Epoch [70/120    avg_loss:0.202, val_acc:0.962]
Epoch [71/120    avg_loss:0.193, val_acc:0.938]
Epoch [72/120    avg_loss:0.237, val_acc:0.940]
Epoch [73/120    avg_loss:0.220, val_acc:0.954]
Epoch [74/120    avg_loss:0.204, val_acc:0.954]
Epoch [75/120    avg_loss:0.236, val_acc:0.944]
Epoch [76/120    avg_loss:0.263, val_acc:0.948]
Epoch [77/120    avg_loss:0.213, val_acc:0.938]
Epoch [78/120    avg_loss:0.211, val_acc:0.970]
Epoch [79/120    avg_loss:0.137, val_acc:0.972]
Epoch [80/120    avg_loss:0.159, val_acc:0.972]
Epoch [81/120    avg_loss:0.156, val_acc:0.972]
Epoch [82/120    avg_loss:0.134, val_acc:0.976]
Epoch [83/120    avg_loss:0.138, val_acc:0.978]
Epoch [84/120    avg_loss:0.127, val_acc:0.974]
Epoch [85/120    avg_loss:0.162, val_acc:0.974]
Epoch [86/120    avg_loss:0.131, val_acc:0.978]
Epoch [87/120    avg_loss:0.128, val_acc:0.978]
Epoch [88/120    avg_loss:0.139, val_acc:0.972]
Epoch [89/120    avg_loss:0.128, val_acc:0.976]
Epoch [90/120    avg_loss:0.134, val_acc:0.974]
Epoch [91/120    avg_loss:0.148, val_acc:0.978]
Epoch [92/120    avg_loss:0.113, val_acc:0.976]
Epoch [93/120    avg_loss:0.115, val_acc:0.978]
Epoch [94/120    avg_loss:0.119, val_acc:0.978]
Epoch [95/120    avg_loss:0.128, val_acc:0.980]
Epoch [96/120    avg_loss:0.142, val_acc:0.972]
Epoch [97/120    avg_loss:0.112, val_acc:0.974]
Epoch [98/120    avg_loss:0.125, val_acc:0.980]
Epoch [99/120    avg_loss:0.119, val_acc:0.978]
Epoch [100/120    avg_loss:0.149, val_acc:0.978]
Epoch [101/120    avg_loss:0.123, val_acc:0.980]
Epoch [102/120    avg_loss:0.111, val_acc:0.978]
Epoch [103/120    avg_loss:0.120, val_acc:0.972]
Epoch [104/120    avg_loss:0.114, val_acc:0.980]
Epoch [105/120    avg_loss:0.120, val_acc:0.980]
Epoch [106/120    avg_loss:0.111, val_acc:0.980]
Epoch [107/120    avg_loss:0.123, val_acc:0.978]
Epoch [108/120    avg_loss:0.124, val_acc:0.980]
Epoch [109/120    avg_loss:0.115, val_acc:0.980]
Epoch [110/120    avg_loss:0.120, val_acc:0.982]
Epoch [111/120    avg_loss:0.112, val_acc:0.982]
Epoch [112/120    avg_loss:0.130, val_acc:0.982]
Epoch [113/120    avg_loss:0.114, val_acc:0.980]
Epoch [114/120    avg_loss:0.112, val_acc:0.980]
Epoch [115/120    avg_loss:0.112, val_acc:0.982]
Epoch [116/120    avg_loss:0.103, val_acc:0.982]
Epoch [117/120    avg_loss:0.098, val_acc:0.982]
Epoch [118/120    avg_loss:0.117, val_acc:0.982]
Epoch [119/120    avg_loss:0.097, val_acc:0.982]
Epoch [120/120    avg_loss:0.114, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   1   0   0   5   0   0   0   0   2   0]
 [  0   0   0 211  14   0   0   0   1   4   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  40 105   0   0   0   0   0   0   0   0]
 [  0   3   0   0   7   0 196   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.97441364605544

F1 scores:
[       nan 0.997815   0.94407159 0.9569161  0.8776699  0.83665339
 0.97512438 0.88135593 0.99742268 0.99574468 1.         1.
 0.99779736 1.        ]

Kappa:
0.9774401613634801
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1819bace80>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.231]
Epoch [2/120    avg_loss:2.586, val_acc:0.313]
Epoch [3/120    avg_loss:2.535, val_acc:0.306]
Epoch [4/120    avg_loss:2.496, val_acc:0.308]
Epoch [5/120    avg_loss:2.461, val_acc:0.300]
Epoch [6/120    avg_loss:2.418, val_acc:0.302]
Epoch [7/120    avg_loss:2.380, val_acc:0.306]
Epoch [8/120    avg_loss:2.341, val_acc:0.349]
Epoch [9/120    avg_loss:2.315, val_acc:0.361]
Epoch [10/120    avg_loss:2.262, val_acc:0.397]
Epoch [11/120    avg_loss:2.221, val_acc:0.482]
Epoch [12/120    avg_loss:2.163, val_acc:0.550]
Epoch [13/120    avg_loss:2.107, val_acc:0.621]
Epoch [14/120    avg_loss:2.035, val_acc:0.631]
Epoch [15/120    avg_loss:1.954, val_acc:0.657]
Epoch [16/120    avg_loss:1.877, val_acc:0.694]
Epoch [17/120    avg_loss:1.790, val_acc:0.746]
Epoch [18/120    avg_loss:1.720, val_acc:0.712]
Epoch [19/120    avg_loss:1.626, val_acc:0.738]
Epoch [20/120    avg_loss:1.480, val_acc:0.819]
Epoch [21/120    avg_loss:1.384, val_acc:0.835]
Epoch [22/120    avg_loss:1.301, val_acc:0.855]
Epoch [23/120    avg_loss:1.205, val_acc:0.839]
Epoch [24/120    avg_loss:1.132, val_acc:0.889]
Epoch [25/120    avg_loss:1.016, val_acc:0.881]
Epoch [26/120    avg_loss:0.940, val_acc:0.907]
Epoch [27/120    avg_loss:0.930, val_acc:0.897]
Epoch [28/120    avg_loss:0.855, val_acc:0.899]
Epoch [29/120    avg_loss:0.811, val_acc:0.901]
Epoch [30/120    avg_loss:0.708, val_acc:0.899]
Epoch [31/120    avg_loss:0.697, val_acc:0.917]
Epoch [32/120    avg_loss:0.662, val_acc:0.907]
Epoch [33/120    avg_loss:0.592, val_acc:0.927]
Epoch [34/120    avg_loss:0.582, val_acc:0.917]
Epoch [35/120    avg_loss:0.546, val_acc:0.895]
Epoch [36/120    avg_loss:0.518, val_acc:0.915]
Epoch [37/120    avg_loss:0.479, val_acc:0.881]
Epoch [38/120    avg_loss:0.551, val_acc:0.915]
Epoch [39/120    avg_loss:0.492, val_acc:0.889]
Epoch [40/120    avg_loss:0.437, val_acc:0.927]
Epoch [41/120    avg_loss:0.420, val_acc:0.915]
Epoch [42/120    avg_loss:0.460, val_acc:0.925]
Epoch [43/120    avg_loss:0.415, val_acc:0.938]
Epoch [44/120    avg_loss:0.402, val_acc:0.927]
Epoch [45/120    avg_loss:0.420, val_acc:0.919]
Epoch [46/120    avg_loss:0.395, val_acc:0.915]
Epoch [47/120    avg_loss:0.372, val_acc:0.907]
Epoch [48/120    avg_loss:0.326, val_acc:0.929]
Epoch [49/120    avg_loss:0.311, val_acc:0.929]
Epoch [50/120    avg_loss:0.372, val_acc:0.913]
Epoch [51/120    avg_loss:0.271, val_acc:0.944]
Epoch [52/120    avg_loss:0.268, val_acc:0.931]
Epoch [53/120    avg_loss:0.281, val_acc:0.933]
Epoch [54/120    avg_loss:0.321, val_acc:0.859]
Epoch [55/120    avg_loss:0.298, val_acc:0.883]
Epoch [56/120    avg_loss:0.326, val_acc:0.905]
Epoch [57/120    avg_loss:0.258, val_acc:0.937]
Epoch [58/120    avg_loss:0.238, val_acc:0.944]
Epoch [59/120    avg_loss:0.269, val_acc:0.956]
Epoch [60/120    avg_loss:0.279, val_acc:0.938]
Epoch [61/120    avg_loss:0.229, val_acc:0.940]
Epoch [62/120    avg_loss:0.216, val_acc:0.948]
Epoch [63/120    avg_loss:0.240, val_acc:0.917]
Epoch [64/120    avg_loss:0.221, val_acc:0.925]
Epoch [65/120    avg_loss:0.207, val_acc:0.948]
Epoch [66/120    avg_loss:0.209, val_acc:0.938]
Epoch [67/120    avg_loss:0.231, val_acc:0.950]
Epoch [68/120    avg_loss:0.198, val_acc:0.935]
Epoch [69/120    avg_loss:0.248, val_acc:0.946]
Epoch [70/120    avg_loss:0.201, val_acc:0.952]
Epoch [71/120    avg_loss:0.168, val_acc:0.962]
Epoch [72/120    avg_loss:0.181, val_acc:0.946]
Epoch [73/120    avg_loss:0.216, val_acc:0.950]
Epoch [74/120    avg_loss:0.224, val_acc:0.956]
Epoch [75/120    avg_loss:0.173, val_acc:0.970]
Epoch [76/120    avg_loss:0.153, val_acc:0.976]
Epoch [77/120    avg_loss:0.155, val_acc:0.968]
Epoch [78/120    avg_loss:0.129, val_acc:0.976]
Epoch [79/120    avg_loss:0.172, val_acc:0.974]
Epoch [80/120    avg_loss:0.157, val_acc:0.954]
Epoch [81/120    avg_loss:0.238, val_acc:0.905]
Epoch [82/120    avg_loss:0.224, val_acc:0.950]
Epoch [83/120    avg_loss:0.172, val_acc:0.954]
Epoch [84/120    avg_loss:0.125, val_acc:0.964]
Epoch [85/120    avg_loss:0.109, val_acc:0.962]
Epoch [86/120    avg_loss:0.107, val_acc:0.978]
Epoch [87/120    avg_loss:0.090, val_acc:0.972]
Epoch [88/120    avg_loss:0.103, val_acc:0.976]
Epoch [89/120    avg_loss:0.103, val_acc:0.960]
Epoch [90/120    avg_loss:0.161, val_acc:0.972]
Epoch [91/120    avg_loss:0.145, val_acc:0.952]
Epoch [92/120    avg_loss:0.140, val_acc:0.948]
Epoch [93/120    avg_loss:0.139, val_acc:0.972]
Epoch [94/120    avg_loss:0.096, val_acc:0.976]
Epoch [95/120    avg_loss:0.083, val_acc:0.982]
Epoch [96/120    avg_loss:0.089, val_acc:0.982]
Epoch [97/120    avg_loss:0.080, val_acc:0.972]
Epoch [98/120    avg_loss:0.091, val_acc:0.986]
Epoch [99/120    avg_loss:0.105, val_acc:0.980]
Epoch [100/120    avg_loss:0.101, val_acc:0.970]
Epoch [101/120    avg_loss:0.083, val_acc:0.966]
Epoch [102/120    avg_loss:0.077, val_acc:0.986]
Epoch [103/120    avg_loss:0.088, val_acc:0.972]
Epoch [104/120    avg_loss:0.137, val_acc:0.962]
Epoch [105/120    avg_loss:0.087, val_acc:0.978]
Epoch [106/120    avg_loss:0.073, val_acc:0.974]
Epoch [107/120    avg_loss:0.097, val_acc:0.970]
Epoch [108/120    avg_loss:0.101, val_acc:0.952]
Epoch [109/120    avg_loss:0.093, val_acc:0.978]
Epoch [110/120    avg_loss:0.074, val_acc:0.968]
Epoch [111/120    avg_loss:0.087, val_acc:0.972]
Epoch [112/120    avg_loss:0.061, val_acc:0.984]
Epoch [113/120    avg_loss:0.059, val_acc:0.984]
Epoch [114/120    avg_loss:0.058, val_acc:0.986]
Epoch [115/120    avg_loss:0.046, val_acc:0.980]
Epoch [116/120    avg_loss:0.076, val_acc:0.978]
Epoch [117/120    avg_loss:0.054, val_acc:0.978]
Epoch [118/120    avg_loss:0.059, val_acc:0.978]
Epoch [119/120    avg_loss:0.055, val_acc:0.974]
Epoch [120/120    avg_loss:0.072, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 215   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 224   3   0   0   0   3   0   0   0   0   0]
 [  0   0   0   2 210  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   1   0   0   1   0 204   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   9   0   0   0 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 0.99854227 0.93886463 0.98245614 0.95022624 0.94736842
 0.99512195 0.87209302 0.97959184 1.         1.         1.
 0.98883929 1.        ]

Kappa:
0.9850443673140237
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f624db9fe10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.063]
Epoch [2/120    avg_loss:2.591, val_acc:0.304]
Epoch [3/120    avg_loss:2.538, val_acc:0.331]
Epoch [4/120    avg_loss:2.494, val_acc:0.337]
Epoch [5/120    avg_loss:2.445, val_acc:0.325]
Epoch [6/120    avg_loss:2.399, val_acc:0.310]
Epoch [7/120    avg_loss:2.349, val_acc:0.313]
Epoch [8/120    avg_loss:2.303, val_acc:0.321]
Epoch [9/120    avg_loss:2.267, val_acc:0.351]
Epoch [10/120    avg_loss:2.203, val_acc:0.415]
Epoch [11/120    avg_loss:2.131, val_acc:0.458]
Epoch [12/120    avg_loss:2.072, val_acc:0.562]
Epoch [13/120    avg_loss:2.006, val_acc:0.651]
Epoch [14/120    avg_loss:1.951, val_acc:0.718]
Epoch [15/120    avg_loss:1.866, val_acc:0.688]
Epoch [16/120    avg_loss:1.767, val_acc:0.633]
Epoch [17/120    avg_loss:1.713, val_acc:0.744]
Epoch [18/120    avg_loss:1.643, val_acc:0.780]
Epoch [19/120    avg_loss:1.581, val_acc:0.712]
Epoch [20/120    avg_loss:1.476, val_acc:0.796]
Epoch [21/120    avg_loss:1.363, val_acc:0.792]
Epoch [22/120    avg_loss:1.259, val_acc:0.855]
Epoch [23/120    avg_loss:1.176, val_acc:0.859]
Epoch [24/120    avg_loss:1.099, val_acc:0.871]
Epoch [25/120    avg_loss:0.983, val_acc:0.881]
Epoch [26/120    avg_loss:0.936, val_acc:0.893]
Epoch [27/120    avg_loss:0.859, val_acc:0.875]
Epoch [28/120    avg_loss:0.808, val_acc:0.895]
Epoch [29/120    avg_loss:0.729, val_acc:0.903]
Epoch [30/120    avg_loss:0.636, val_acc:0.911]
Epoch [31/120    avg_loss:0.619, val_acc:0.911]
Epoch [32/120    avg_loss:0.610, val_acc:0.905]
Epoch [33/120    avg_loss:0.569, val_acc:0.893]
Epoch [34/120    avg_loss:0.564, val_acc:0.917]
Epoch [35/120    avg_loss:0.567, val_acc:0.913]
Epoch [36/120    avg_loss:0.508, val_acc:0.901]
Epoch [37/120    avg_loss:0.554, val_acc:0.915]
Epoch [38/120    avg_loss:0.485, val_acc:0.909]
Epoch [39/120    avg_loss:0.437, val_acc:0.931]
Epoch [40/120    avg_loss:0.382, val_acc:0.899]
Epoch [41/120    avg_loss:0.440, val_acc:0.923]
Epoch [42/120    avg_loss:0.393, val_acc:0.937]
Epoch [43/120    avg_loss:0.372, val_acc:0.907]
Epoch [44/120    avg_loss:0.363, val_acc:0.935]
Epoch [45/120    avg_loss:0.339, val_acc:0.940]
Epoch [46/120    avg_loss:0.328, val_acc:0.929]
Epoch [47/120    avg_loss:0.336, val_acc:0.944]
Epoch [48/120    avg_loss:0.342, val_acc:0.944]
Epoch [49/120    avg_loss:0.292, val_acc:0.956]
Epoch [50/120    avg_loss:0.307, val_acc:0.931]
Epoch [51/120    avg_loss:0.281, val_acc:0.952]
Epoch [52/120    avg_loss:0.327, val_acc:0.938]
Epoch [53/120    avg_loss:0.283, val_acc:0.879]
Epoch [54/120    avg_loss:0.259, val_acc:0.948]
Epoch [55/120    avg_loss:0.233, val_acc:0.952]
Epoch [56/120    avg_loss:0.283, val_acc:0.950]
Epoch [57/120    avg_loss:0.240, val_acc:0.950]
Epoch [58/120    avg_loss:0.254, val_acc:0.933]
Epoch [59/120    avg_loss:0.286, val_acc:0.944]
Epoch [60/120    avg_loss:0.234, val_acc:0.956]
Epoch [61/120    avg_loss:0.207, val_acc:0.958]
Epoch [62/120    avg_loss:0.207, val_acc:0.950]
Epoch [63/120    avg_loss:0.251, val_acc:0.944]
Epoch [64/120    avg_loss:0.201, val_acc:0.956]
Epoch [65/120    avg_loss:0.237, val_acc:0.960]
Epoch [66/120    avg_loss:0.210, val_acc:0.968]
Epoch [67/120    avg_loss:0.193, val_acc:0.956]
Epoch [68/120    avg_loss:0.192, val_acc:0.966]
Epoch [69/120    avg_loss:0.170, val_acc:0.952]
Epoch [70/120    avg_loss:0.217, val_acc:0.966]
Epoch [71/120    avg_loss:0.167, val_acc:0.972]
Epoch [72/120    avg_loss:0.150, val_acc:0.966]
Epoch [73/120    avg_loss:0.153, val_acc:0.968]
Epoch [74/120    avg_loss:0.169, val_acc:0.956]
Epoch [75/120    avg_loss:0.151, val_acc:0.966]
Epoch [76/120    avg_loss:0.145, val_acc:0.964]
Epoch [77/120    avg_loss:0.121, val_acc:0.976]
Epoch [78/120    avg_loss:0.139, val_acc:0.974]
Epoch [79/120    avg_loss:0.110, val_acc:0.964]
Epoch [80/120    avg_loss:0.116, val_acc:0.950]
Epoch [81/120    avg_loss:0.167, val_acc:0.944]
Epoch [82/120    avg_loss:0.125, val_acc:0.946]
Epoch [83/120    avg_loss:0.110, val_acc:0.970]
Epoch [84/120    avg_loss:0.121, val_acc:0.948]
Epoch [85/120    avg_loss:0.283, val_acc:0.944]
Epoch [86/120    avg_loss:0.186, val_acc:0.960]
Epoch [87/120    avg_loss:0.192, val_acc:0.952]
Epoch [88/120    avg_loss:0.158, val_acc:0.962]
Epoch [89/120    avg_loss:0.106, val_acc:0.966]
Epoch [90/120    avg_loss:0.120, val_acc:0.972]
Epoch [91/120    avg_loss:0.103, val_acc:0.974]
Epoch [92/120    avg_loss:0.101, val_acc:0.978]
Epoch [93/120    avg_loss:0.077, val_acc:0.976]
Epoch [94/120    avg_loss:0.080, val_acc:0.980]
Epoch [95/120    avg_loss:0.082, val_acc:0.972]
Epoch [96/120    avg_loss:0.070, val_acc:0.980]
Epoch [97/120    avg_loss:0.064, val_acc:0.982]
Epoch [98/120    avg_loss:0.064, val_acc:0.980]
Epoch [99/120    avg_loss:0.073, val_acc:0.982]
Epoch [100/120    avg_loss:0.067, val_acc:0.976]
Epoch [101/120    avg_loss:0.057, val_acc:0.976]
Epoch [102/120    avg_loss:0.061, val_acc:0.976]
Epoch [103/120    avg_loss:0.055, val_acc:0.976]
Epoch [104/120    avg_loss:0.055, val_acc:0.976]
Epoch [105/120    avg_loss:0.064, val_acc:0.978]
Epoch [106/120    avg_loss:0.060, val_acc:0.978]
Epoch [107/120    avg_loss:0.056, val_acc:0.980]
Epoch [108/120    avg_loss:0.056, val_acc:0.978]
Epoch [109/120    avg_loss:0.058, val_acc:0.980]
Epoch [110/120    avg_loss:0.060, val_acc:0.978]
Epoch [111/120    avg_loss:0.067, val_acc:0.978]
Epoch [112/120    avg_loss:0.061, val_acc:0.978]
Epoch [113/120    avg_loss:0.068, val_acc:0.978]
Epoch [114/120    avg_loss:0.055, val_acc:0.978]
Epoch [115/120    avg_loss:0.062, val_acc:0.978]
Epoch [116/120    avg_loss:0.052, val_acc:0.978]
Epoch [117/120    avg_loss:0.067, val_acc:0.978]
Epoch [118/120    avg_loss:0.054, val_acc:0.978]
Epoch [119/120    avg_loss:0.054, val_acc:0.978]
Epoch [120/120    avg_loss:0.053, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   2 216   5   0   0   0   3   4   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 1.         0.93033708 0.96860987 0.92723493 0.8880597
 1.         0.84782609 0.99485861 0.99574468 1.         1.
 1.         1.        ]

Kappa:
0.9826680623878881
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f70d9303e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.645, val_acc:0.277]
Epoch [2/120    avg_loss:2.586, val_acc:0.264]
Epoch [3/120    avg_loss:2.541, val_acc:0.296]
Epoch [4/120    avg_loss:2.502, val_acc:0.288]
Epoch [5/120    avg_loss:2.450, val_acc:0.288]
Epoch [6/120    avg_loss:2.407, val_acc:0.286]
Epoch [7/120    avg_loss:2.364, val_acc:0.286]
Epoch [8/120    avg_loss:2.333, val_acc:0.290]
Epoch [9/120    avg_loss:2.279, val_acc:0.292]
Epoch [10/120    avg_loss:2.219, val_acc:0.321]
Epoch [11/120    avg_loss:2.172, val_acc:0.379]
Epoch [12/120    avg_loss:2.097, val_acc:0.440]
Epoch [13/120    avg_loss:2.047, val_acc:0.498]
Epoch [14/120    avg_loss:1.974, val_acc:0.526]
Epoch [15/120    avg_loss:1.931, val_acc:0.562]
Epoch [16/120    avg_loss:1.826, val_acc:0.581]
Epoch [17/120    avg_loss:1.772, val_acc:0.673]
Epoch [18/120    avg_loss:1.697, val_acc:0.722]
Epoch [19/120    avg_loss:1.593, val_acc:0.800]
Epoch [20/120    avg_loss:1.507, val_acc:0.790]
Epoch [21/120    avg_loss:1.407, val_acc:0.841]
Epoch [22/120    avg_loss:1.266, val_acc:0.849]
Epoch [23/120    avg_loss:1.172, val_acc:0.867]
Epoch [24/120    avg_loss:1.068, val_acc:0.869]
Epoch [25/120    avg_loss:0.984, val_acc:0.871]
Epoch [26/120    avg_loss:0.981, val_acc:0.845]
Epoch [27/120    avg_loss:0.898, val_acc:0.877]
Epoch [28/120    avg_loss:0.813, val_acc:0.887]
Epoch [29/120    avg_loss:0.727, val_acc:0.905]
Epoch [30/120    avg_loss:0.669, val_acc:0.887]
Epoch [31/120    avg_loss:0.690, val_acc:0.903]
Epoch [32/120    avg_loss:0.637, val_acc:0.815]
Epoch [33/120    avg_loss:0.596, val_acc:0.907]
Epoch [34/120    avg_loss:0.559, val_acc:0.915]
Epoch [35/120    avg_loss:0.563, val_acc:0.909]
Epoch [36/120    avg_loss:0.528, val_acc:0.915]
Epoch [37/120    avg_loss:0.516, val_acc:0.919]
Epoch [38/120    avg_loss:0.533, val_acc:0.796]
Epoch [39/120    avg_loss:0.565, val_acc:0.893]
Epoch [40/120    avg_loss:0.505, val_acc:0.917]
Epoch [41/120    avg_loss:0.486, val_acc:0.929]
Epoch [42/120    avg_loss:0.464, val_acc:0.933]
Epoch [43/120    avg_loss:0.399, val_acc:0.937]
Epoch [44/120    avg_loss:0.390, val_acc:0.925]
Epoch [45/120    avg_loss:0.361, val_acc:0.942]
Epoch [46/120    avg_loss:0.362, val_acc:0.938]
Epoch [47/120    avg_loss:0.340, val_acc:0.921]
Epoch [48/120    avg_loss:0.379, val_acc:0.935]
Epoch [49/120    avg_loss:0.339, val_acc:0.933]
Epoch [50/120    avg_loss:0.338, val_acc:0.937]
Epoch [51/120    avg_loss:0.346, val_acc:0.946]
Epoch [52/120    avg_loss:0.324, val_acc:0.950]
Epoch [53/120    avg_loss:0.287, val_acc:0.933]
Epoch [54/120    avg_loss:0.301, val_acc:0.929]
Epoch [55/120    avg_loss:0.282, val_acc:0.956]
Epoch [56/120    avg_loss:0.230, val_acc:0.944]
Epoch [57/120    avg_loss:0.260, val_acc:0.948]
Epoch [58/120    avg_loss:0.269, val_acc:0.954]
Epoch [59/120    avg_loss:0.275, val_acc:0.952]
Epoch [60/120    avg_loss:0.239, val_acc:0.948]
Epoch [61/120    avg_loss:0.196, val_acc:0.970]
Epoch [62/120    avg_loss:0.182, val_acc:0.962]
Epoch [63/120    avg_loss:0.197, val_acc:0.940]
Epoch [64/120    avg_loss:0.241, val_acc:0.960]
Epoch [65/120    avg_loss:0.206, val_acc:0.970]
Epoch [66/120    avg_loss:0.203, val_acc:0.954]
Epoch [67/120    avg_loss:0.228, val_acc:0.944]
Epoch [68/120    avg_loss:0.219, val_acc:0.954]
Epoch [69/120    avg_loss:0.227, val_acc:0.942]
Epoch [70/120    avg_loss:0.253, val_acc:0.948]
Epoch [71/120    avg_loss:0.238, val_acc:0.970]
Epoch [72/120    avg_loss:0.211, val_acc:0.968]
Epoch [73/120    avg_loss:0.190, val_acc:0.968]
Epoch [74/120    avg_loss:0.183, val_acc:0.968]
Epoch [75/120    avg_loss:0.129, val_acc:0.962]
Epoch [76/120    avg_loss:0.145, val_acc:0.958]
Epoch [77/120    avg_loss:0.164, val_acc:0.970]
Epoch [78/120    avg_loss:0.120, val_acc:0.968]
Epoch [79/120    avg_loss:0.123, val_acc:0.964]
Epoch [80/120    avg_loss:0.187, val_acc:0.964]
Epoch [81/120    avg_loss:0.141, val_acc:0.966]
Epoch [82/120    avg_loss:0.142, val_acc:0.964]
Epoch [83/120    avg_loss:0.154, val_acc:0.976]
Epoch [84/120    avg_loss:0.125, val_acc:0.968]
Epoch [85/120    avg_loss:0.129, val_acc:0.966]
Epoch [86/120    avg_loss:0.113, val_acc:0.970]
Epoch [87/120    avg_loss:0.127, val_acc:0.966]
Epoch [88/120    avg_loss:0.163, val_acc:0.962]
Epoch [89/120    avg_loss:0.156, val_acc:0.968]
Epoch [90/120    avg_loss:0.105, val_acc:0.964]
Epoch [91/120    avg_loss:0.156, val_acc:0.958]
Epoch [92/120    avg_loss:0.111, val_acc:0.974]
Epoch [93/120    avg_loss:0.121, val_acc:0.962]
Epoch [94/120    avg_loss:0.128, val_acc:0.958]
Epoch [95/120    avg_loss:0.114, val_acc:0.964]
Epoch [96/120    avg_loss:0.106, val_acc:0.976]
Epoch [97/120    avg_loss:0.099, val_acc:0.974]
Epoch [98/120    avg_loss:0.109, val_acc:0.938]
Epoch [99/120    avg_loss:0.171, val_acc:0.978]
Epoch [100/120    avg_loss:0.096, val_acc:0.966]
Epoch [101/120    avg_loss:0.108, val_acc:0.970]
Epoch [102/120    avg_loss:0.123, val_acc:0.964]
Epoch [103/120    avg_loss:0.129, val_acc:0.960]
Epoch [104/120    avg_loss:0.106, val_acc:0.974]
Epoch [105/120    avg_loss:0.105, val_acc:0.972]
Epoch [106/120    avg_loss:0.096, val_acc:0.970]
Epoch [107/120    avg_loss:0.078, val_acc:0.984]
Epoch [108/120    avg_loss:0.080, val_acc:0.968]
Epoch [109/120    avg_loss:0.092, val_acc:0.982]
Epoch [110/120    avg_loss:0.050, val_acc:0.984]
Epoch [111/120    avg_loss:0.062, val_acc:0.974]
Epoch [112/120    avg_loss:0.123, val_acc:0.960]
Epoch [113/120    avg_loss:0.118, val_acc:0.972]
Epoch [114/120    avg_loss:0.096, val_acc:0.984]
Epoch [115/120    avg_loss:0.075, val_acc:0.978]
Epoch [116/120    avg_loss:0.067, val_acc:0.980]
Epoch [117/120    avg_loss:0.066, val_acc:0.970]
Epoch [118/120    avg_loss:0.079, val_acc:0.982]
Epoch [119/120    avg_loss:0.060, val_acc:0.980]
Epoch [120/120    avg_loss:0.053, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   1 214   6   0   0   0   6   3   0   0   0   0]
 [  0   0   0   0 198  28   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   7   0   0   0   0 199   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.37953091684435

F1 scores:
[       nan 0.99491649 0.94305239 0.96396396 0.9187935  0.91194969
 0.98271605 0.87830688 0.99103713 0.99680511 1.         1.
 0.99889746 1.        ]

Kappa:
0.9819566148819556
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe9a275ee80>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.175]
Epoch [2/120    avg_loss:2.591, val_acc:0.181]
Epoch [3/120    avg_loss:2.558, val_acc:0.282]
Epoch [4/120    avg_loss:2.519, val_acc:0.339]
Epoch [5/120    avg_loss:2.489, val_acc:0.369]
Epoch [6/120    avg_loss:2.447, val_acc:0.379]
Epoch [7/120    avg_loss:2.413, val_acc:0.395]
Epoch [8/120    avg_loss:2.378, val_acc:0.399]
Epoch [9/120    avg_loss:2.340, val_acc:0.423]
Epoch [10/120    avg_loss:2.290, val_acc:0.470]
Epoch [11/120    avg_loss:2.245, val_acc:0.490]
Epoch [12/120    avg_loss:2.189, val_acc:0.502]
Epoch [13/120    avg_loss:2.122, val_acc:0.512]
Epoch [14/120    avg_loss:2.061, val_acc:0.524]
Epoch [15/120    avg_loss:1.992, val_acc:0.563]
Epoch [16/120    avg_loss:1.914, val_acc:0.575]
Epoch [17/120    avg_loss:1.830, val_acc:0.567]
Epoch [18/120    avg_loss:1.754, val_acc:0.635]
Epoch [19/120    avg_loss:1.679, val_acc:0.714]
Epoch [20/120    avg_loss:1.560, val_acc:0.742]
Epoch [21/120    avg_loss:1.429, val_acc:0.778]
Epoch [22/120    avg_loss:1.347, val_acc:0.800]
Epoch [23/120    avg_loss:1.274, val_acc:0.804]
Epoch [24/120    avg_loss:1.161, val_acc:0.815]
Epoch [25/120    avg_loss:1.073, val_acc:0.810]
Epoch [26/120    avg_loss:1.037, val_acc:0.825]
Epoch [27/120    avg_loss:0.933, val_acc:0.871]
Epoch [28/120    avg_loss:0.869, val_acc:0.863]
Epoch [29/120    avg_loss:0.806, val_acc:0.885]
Epoch [30/120    avg_loss:0.749, val_acc:0.879]
Epoch [31/120    avg_loss:0.680, val_acc:0.871]
Epoch [32/120    avg_loss:0.649, val_acc:0.883]
Epoch [33/120    avg_loss:0.572, val_acc:0.901]
Epoch [34/120    avg_loss:0.610, val_acc:0.881]
Epoch [35/120    avg_loss:0.596, val_acc:0.883]
Epoch [36/120    avg_loss:0.614, val_acc:0.845]
Epoch [37/120    avg_loss:0.623, val_acc:0.905]
Epoch [38/120    avg_loss:0.543, val_acc:0.905]
Epoch [39/120    avg_loss:0.585, val_acc:0.901]
Epoch [40/120    avg_loss:0.516, val_acc:0.919]
Epoch [41/120    avg_loss:0.444, val_acc:0.903]
Epoch [42/120    avg_loss:0.495, val_acc:0.891]
Epoch [43/120    avg_loss:0.452, val_acc:0.905]
Epoch [44/120    avg_loss:0.424, val_acc:0.905]
Epoch [45/120    avg_loss:0.426, val_acc:0.911]
Epoch [46/120    avg_loss:0.414, val_acc:0.919]
Epoch [47/120    avg_loss:0.405, val_acc:0.903]
Epoch [48/120    avg_loss:0.425, val_acc:0.911]
Epoch [49/120    avg_loss:0.364, val_acc:0.927]
Epoch [50/120    avg_loss:0.373, val_acc:0.893]
Epoch [51/120    avg_loss:0.385, val_acc:0.927]
Epoch [52/120    avg_loss:0.363, val_acc:0.913]
Epoch [53/120    avg_loss:0.380, val_acc:0.897]
Epoch [54/120    avg_loss:0.343, val_acc:0.931]
Epoch [55/120    avg_loss:0.387, val_acc:0.931]
Epoch [56/120    avg_loss:0.402, val_acc:0.869]
Epoch [57/120    avg_loss:0.383, val_acc:0.913]
Epoch [58/120    avg_loss:0.288, val_acc:0.923]
Epoch [59/120    avg_loss:0.288, val_acc:0.925]
Epoch [60/120    avg_loss:0.287, val_acc:0.937]
Epoch [61/120    avg_loss:0.252, val_acc:0.950]
Epoch [62/120    avg_loss:0.284, val_acc:0.925]
Epoch [63/120    avg_loss:0.282, val_acc:0.923]
Epoch [64/120    avg_loss:0.331, val_acc:0.883]
Epoch [65/120    avg_loss:0.289, val_acc:0.929]
Epoch [66/120    avg_loss:0.245, val_acc:0.948]
Epoch [67/120    avg_loss:0.252, val_acc:0.942]
Epoch [68/120    avg_loss:0.294, val_acc:0.940]
Epoch [69/120    avg_loss:0.305, val_acc:0.931]
Epoch [70/120    avg_loss:0.278, val_acc:0.940]
Epoch [71/120    avg_loss:0.265, val_acc:0.935]
Epoch [72/120    avg_loss:0.208, val_acc:0.935]
Epoch [73/120    avg_loss:0.336, val_acc:0.933]
Epoch [74/120    avg_loss:0.301, val_acc:0.919]
Epoch [75/120    avg_loss:0.255, val_acc:0.935]
Epoch [76/120    avg_loss:0.189, val_acc:0.940]
Epoch [77/120    avg_loss:0.191, val_acc:0.940]
Epoch [78/120    avg_loss:0.187, val_acc:0.946]
Epoch [79/120    avg_loss:0.184, val_acc:0.948]
Epoch [80/120    avg_loss:0.165, val_acc:0.948]
Epoch [81/120    avg_loss:0.169, val_acc:0.950]
Epoch [82/120    avg_loss:0.163, val_acc:0.954]
Epoch [83/120    avg_loss:0.166, val_acc:0.954]
Epoch [84/120    avg_loss:0.157, val_acc:0.950]
Epoch [85/120    avg_loss:0.152, val_acc:0.952]
Epoch [86/120    avg_loss:0.149, val_acc:0.952]
Epoch [87/120    avg_loss:0.168, val_acc:0.952]
Epoch [88/120    avg_loss:0.142, val_acc:0.958]
Epoch [89/120    avg_loss:0.147, val_acc:0.960]
Epoch [90/120    avg_loss:0.144, val_acc:0.956]
Epoch [91/120    avg_loss:0.144, val_acc:0.958]
Epoch [92/120    avg_loss:0.141, val_acc:0.956]
Epoch [93/120    avg_loss:0.153, val_acc:0.962]
Epoch [94/120    avg_loss:0.129, val_acc:0.964]
Epoch [95/120    avg_loss:0.126, val_acc:0.962]
Epoch [96/120    avg_loss:0.125, val_acc:0.962]
Epoch [97/120    avg_loss:0.132, val_acc:0.962]
Epoch [98/120    avg_loss:0.127, val_acc:0.958]
Epoch [99/120    avg_loss:0.124, val_acc:0.960]
Epoch [100/120    avg_loss:0.130, val_acc:0.964]
Epoch [101/120    avg_loss:0.137, val_acc:0.964]
Epoch [102/120    avg_loss:0.117, val_acc:0.964]
Epoch [103/120    avg_loss:0.141, val_acc:0.958]
Epoch [104/120    avg_loss:0.137, val_acc:0.964]
Epoch [105/120    avg_loss:0.125, val_acc:0.962]
Epoch [106/120    avg_loss:0.124, val_acc:0.960]
Epoch [107/120    avg_loss:0.110, val_acc:0.964]
Epoch [108/120    avg_loss:0.121, val_acc:0.960]
Epoch [109/120    avg_loss:0.116, val_acc:0.958]
Epoch [110/120    avg_loss:0.125, val_acc:0.964]
Epoch [111/120    avg_loss:0.113, val_acc:0.960]
Epoch [112/120    avg_loss:0.125, val_acc:0.958]
Epoch [113/120    avg_loss:0.118, val_acc:0.958]
Epoch [114/120    avg_loss:0.119, val_acc:0.960]
Epoch [115/120    avg_loss:0.108, val_acc:0.960]
Epoch [116/120    avg_loss:0.116, val_acc:0.958]
Epoch [117/120    avg_loss:0.108, val_acc:0.962]
Epoch [118/120    avg_loss:0.113, val_acc:0.958]
Epoch [119/120    avg_loss:0.100, val_acc:0.962]
Epoch [120/120    avg_loss:0.112, val_acc:0.958]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 205   0   0   0   0  14   0   0   0   0   0   0]
 [  0   0   1 220   5   0   0   0   2   2   0   0   0   0]
 [  0   0   0   5 204  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   5   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.01705756929637

F1 scores:
[       nan 0.99927061 0.9255079  0.96703297 0.89082969 0.86013986
 1.         0.82608696 0.98976982 0.9978678  1.         1.
 0.99445061 1.        ]

Kappa:
0.9779217579741369
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa647dabe10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.599, val_acc:0.349]
Epoch [2/120    avg_loss:2.535, val_acc:0.405]
Epoch [3/120    avg_loss:2.481, val_acc:0.405]
Epoch [4/120    avg_loss:2.433, val_acc:0.405]
Epoch [5/120    avg_loss:2.390, val_acc:0.409]
Epoch [6/120    avg_loss:2.341, val_acc:0.417]
Epoch [7/120    avg_loss:2.293, val_acc:0.435]
Epoch [8/120    avg_loss:2.243, val_acc:0.448]
Epoch [9/120    avg_loss:2.169, val_acc:0.528]
Epoch [10/120    avg_loss:2.103, val_acc:0.560]
Epoch [11/120    avg_loss:2.030, val_acc:0.633]
Epoch [12/120    avg_loss:1.961, val_acc:0.651]
Epoch [13/120    avg_loss:1.873, val_acc:0.690]
Epoch [14/120    avg_loss:1.743, val_acc:0.720]
Epoch [15/120    avg_loss:1.636, val_acc:0.756]
Epoch [16/120    avg_loss:1.539, val_acc:0.772]
Epoch [17/120    avg_loss:1.436, val_acc:0.774]
Epoch [18/120    avg_loss:1.377, val_acc:0.776]
Epoch [19/120    avg_loss:1.250, val_acc:0.758]
Epoch [20/120    avg_loss:1.194, val_acc:0.780]
Epoch [21/120    avg_loss:1.103, val_acc:0.810]
Epoch [22/120    avg_loss:1.009, val_acc:0.728]
Epoch [23/120    avg_loss:0.997, val_acc:0.835]
Epoch [24/120    avg_loss:0.968, val_acc:0.829]
Epoch [25/120    avg_loss:0.864, val_acc:0.875]
Epoch [26/120    avg_loss:0.787, val_acc:0.865]
Epoch [27/120    avg_loss:0.727, val_acc:0.885]
Epoch [28/120    avg_loss:0.647, val_acc:0.895]
Epoch [29/120    avg_loss:0.625, val_acc:0.883]
Epoch [30/120    avg_loss:0.577, val_acc:0.901]
Epoch [31/120    avg_loss:0.625, val_acc:0.873]
Epoch [32/120    avg_loss:0.609, val_acc:0.879]
Epoch [33/120    avg_loss:0.582, val_acc:0.875]
Epoch [34/120    avg_loss:0.537, val_acc:0.829]
Epoch [35/120    avg_loss:0.535, val_acc:0.849]
Epoch [36/120    avg_loss:0.535, val_acc:0.901]
Epoch [37/120    avg_loss:0.553, val_acc:0.887]
Epoch [38/120    avg_loss:0.457, val_acc:0.885]
Epoch [39/120    avg_loss:0.451, val_acc:0.927]
Epoch [40/120    avg_loss:0.443, val_acc:0.933]
Epoch [41/120    avg_loss:0.422, val_acc:0.921]
Epoch [42/120    avg_loss:0.388, val_acc:0.919]
Epoch [43/120    avg_loss:0.361, val_acc:0.917]
Epoch [44/120    avg_loss:0.367, val_acc:0.921]
Epoch [45/120    avg_loss:0.350, val_acc:0.931]
Epoch [46/120    avg_loss:0.348, val_acc:0.919]
Epoch [47/120    avg_loss:0.365, val_acc:0.919]
Epoch [48/120    avg_loss:0.332, val_acc:0.935]
Epoch [49/120    avg_loss:0.314, val_acc:0.940]
Epoch [50/120    avg_loss:0.302, val_acc:0.935]
Epoch [51/120    avg_loss:0.324, val_acc:0.911]
Epoch [52/120    avg_loss:0.293, val_acc:0.942]
Epoch [53/120    avg_loss:0.259, val_acc:0.950]
Epoch [54/120    avg_loss:0.297, val_acc:0.944]
Epoch [55/120    avg_loss:0.401, val_acc:0.937]
Epoch [56/120    avg_loss:0.263, val_acc:0.948]
Epoch [57/120    avg_loss:0.275, val_acc:0.942]
Epoch [58/120    avg_loss:0.303, val_acc:0.946]
Epoch [59/120    avg_loss:0.296, val_acc:0.929]
Epoch [60/120    avg_loss:0.258, val_acc:0.954]
Epoch [61/120    avg_loss:0.218, val_acc:0.948]
Epoch [62/120    avg_loss:0.247, val_acc:0.935]
Epoch [63/120    avg_loss:0.241, val_acc:0.937]
Epoch [64/120    avg_loss:0.205, val_acc:0.956]
Epoch [65/120    avg_loss:0.198, val_acc:0.942]
Epoch [66/120    avg_loss:0.163, val_acc:0.970]
Epoch [67/120    avg_loss:0.144, val_acc:0.954]
Epoch [68/120    avg_loss:0.152, val_acc:0.958]
Epoch [69/120    avg_loss:0.189, val_acc:0.954]
Epoch [70/120    avg_loss:0.240, val_acc:0.940]
Epoch [71/120    avg_loss:0.251, val_acc:0.952]
Epoch [72/120    avg_loss:0.204, val_acc:0.948]
Epoch [73/120    avg_loss:0.177, val_acc:0.958]
Epoch [74/120    avg_loss:0.173, val_acc:0.944]
Epoch [75/120    avg_loss:0.156, val_acc:0.962]
Epoch [76/120    avg_loss:0.152, val_acc:0.956]
Epoch [77/120    avg_loss:0.197, val_acc:0.937]
Epoch [78/120    avg_loss:0.209, val_acc:0.942]
Epoch [79/120    avg_loss:0.146, val_acc:0.968]
Epoch [80/120    avg_loss:0.117, val_acc:0.970]
Epoch [81/120    avg_loss:0.113, val_acc:0.970]
Epoch [82/120    avg_loss:0.102, val_acc:0.972]
Epoch [83/120    avg_loss:0.109, val_acc:0.974]
Epoch [84/120    avg_loss:0.103, val_acc:0.974]
Epoch [85/120    avg_loss:0.097, val_acc:0.976]
Epoch [86/120    avg_loss:0.088, val_acc:0.976]
Epoch [87/120    avg_loss:0.096, val_acc:0.974]
Epoch [88/120    avg_loss:0.078, val_acc:0.976]
Epoch [89/120    avg_loss:0.080, val_acc:0.976]
Epoch [90/120    avg_loss:0.087, val_acc:0.978]
Epoch [91/120    avg_loss:0.083, val_acc:0.974]
Epoch [92/120    avg_loss:0.082, val_acc:0.976]
Epoch [93/120    avg_loss:0.081, val_acc:0.978]
Epoch [94/120    avg_loss:0.080, val_acc:0.978]
Epoch [95/120    avg_loss:0.085, val_acc:0.976]
Epoch [96/120    avg_loss:0.079, val_acc:0.976]
Epoch [97/120    avg_loss:0.073, val_acc:0.974]
Epoch [98/120    avg_loss:0.084, val_acc:0.978]
Epoch [99/120    avg_loss:0.078, val_acc:0.974]
Epoch [100/120    avg_loss:0.075, val_acc:0.976]
Epoch [101/120    avg_loss:0.074, val_acc:0.976]
Epoch [102/120    avg_loss:0.075, val_acc:0.974]
Epoch [103/120    avg_loss:0.085, val_acc:0.978]
Epoch [104/120    avg_loss:0.072, val_acc:0.978]
Epoch [105/120    avg_loss:0.077, val_acc:0.974]
Epoch [106/120    avg_loss:0.077, val_acc:0.974]
Epoch [107/120    avg_loss:0.081, val_acc:0.972]
Epoch [108/120    avg_loss:0.080, val_acc:0.976]
Epoch [109/120    avg_loss:0.069, val_acc:0.976]
Epoch [110/120    avg_loss:0.075, val_acc:0.980]
Epoch [111/120    avg_loss:0.079, val_acc:0.976]
Epoch [112/120    avg_loss:0.065, val_acc:0.976]
Epoch [113/120    avg_loss:0.068, val_acc:0.978]
Epoch [114/120    avg_loss:0.074, val_acc:0.980]
Epoch [115/120    avg_loss:0.068, val_acc:0.982]
Epoch [116/120    avg_loss:0.067, val_acc:0.980]
Epoch [117/120    avg_loss:0.072, val_acc:0.974]
Epoch [118/120    avg_loss:0.067, val_acc:0.980]
Epoch [119/120    avg_loss:0.078, val_acc:0.976]
Epoch [120/120    avg_loss:0.074, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   1 215   7   0   0   0   3   4   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   5  27 113   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 372   5   0]
 [  0   0   0   0   0   0   0   0   0   0   7   0 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.14498933901919

F1 scores:
[       nan 1.         0.94382022 0.95555556 0.92213115 0.86923077
 0.99512195 0.88648649 0.99226804 0.99574468 0.99047619 0.99332443
 0.98672566 1.        ]

Kappa:
0.9793441165991537
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff85d570da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.230]
Epoch [2/120    avg_loss:2.585, val_acc:0.230]
Epoch [3/120    avg_loss:2.549, val_acc:0.262]
Epoch [4/120    avg_loss:2.517, val_acc:0.300]
Epoch [5/120    avg_loss:2.484, val_acc:0.355]
Epoch [6/120    avg_loss:2.444, val_acc:0.373]
Epoch [7/120    avg_loss:2.401, val_acc:0.405]
Epoch [8/120    avg_loss:2.353, val_acc:0.407]
Epoch [9/120    avg_loss:2.304, val_acc:0.407]
Epoch [10/120    avg_loss:2.258, val_acc:0.478]
Epoch [11/120    avg_loss:2.204, val_acc:0.528]
Epoch [12/120    avg_loss:2.132, val_acc:0.562]
Epoch [13/120    avg_loss:2.068, val_acc:0.587]
Epoch [14/120    avg_loss:2.019, val_acc:0.585]
Epoch [15/120    avg_loss:1.948, val_acc:0.621]
Epoch [16/120    avg_loss:1.858, val_acc:0.665]
Epoch [17/120    avg_loss:1.817, val_acc:0.714]
Epoch [18/120    avg_loss:1.737, val_acc:0.708]
Epoch [19/120    avg_loss:1.647, val_acc:0.710]
Epoch [20/120    avg_loss:1.558, val_acc:0.738]
Epoch [21/120    avg_loss:1.470, val_acc:0.752]
Epoch [22/120    avg_loss:1.377, val_acc:0.770]
Epoch [23/120    avg_loss:1.289, val_acc:0.790]
Epoch [24/120    avg_loss:1.190, val_acc:0.813]
Epoch [25/120    avg_loss:1.107, val_acc:0.776]
Epoch [26/120    avg_loss:1.079, val_acc:0.843]
Epoch [27/120    avg_loss:0.973, val_acc:0.877]
Epoch [28/120    avg_loss:0.897, val_acc:0.895]
Epoch [29/120    avg_loss:0.813, val_acc:0.859]
Epoch [30/120    avg_loss:0.791, val_acc:0.873]
Epoch [31/120    avg_loss:0.752, val_acc:0.897]
Epoch [32/120    avg_loss:0.740, val_acc:0.875]
Epoch [33/120    avg_loss:0.694, val_acc:0.915]
Epoch [34/120    avg_loss:0.596, val_acc:0.913]
Epoch [35/120    avg_loss:0.601, val_acc:0.913]
Epoch [36/120    avg_loss:0.561, val_acc:0.899]
Epoch [37/120    avg_loss:0.560, val_acc:0.923]
Epoch [38/120    avg_loss:0.561, val_acc:0.923]
Epoch [39/120    avg_loss:0.455, val_acc:0.933]
Epoch [40/120    avg_loss:0.520, val_acc:0.901]
Epoch [41/120    avg_loss:0.473, val_acc:0.938]
Epoch [42/120    avg_loss:0.431, val_acc:0.931]
Epoch [43/120    avg_loss:0.414, val_acc:0.944]
Epoch [44/120    avg_loss:0.400, val_acc:0.946]
Epoch [45/120    avg_loss:0.359, val_acc:0.940]
Epoch [46/120    avg_loss:0.377, val_acc:0.935]
Epoch [47/120    avg_loss:0.408, val_acc:0.923]
Epoch [48/120    avg_loss:0.367, val_acc:0.950]
Epoch [49/120    avg_loss:0.307, val_acc:0.940]
Epoch [50/120    avg_loss:0.350, val_acc:0.938]
Epoch [51/120    avg_loss:0.333, val_acc:0.946]
Epoch [52/120    avg_loss:0.322, val_acc:0.929]
Epoch [53/120    avg_loss:0.327, val_acc:0.940]
Epoch [54/120    avg_loss:0.302, val_acc:0.950]
Epoch [55/120    avg_loss:0.260, val_acc:0.935]
Epoch [56/120    avg_loss:0.277, val_acc:0.952]
Epoch [57/120    avg_loss:0.281, val_acc:0.950]
Epoch [58/120    avg_loss:0.238, val_acc:0.950]
Epoch [59/120    avg_loss:0.261, val_acc:0.962]
Epoch [60/120    avg_loss:0.239, val_acc:0.954]
Epoch [61/120    avg_loss:0.242, val_acc:0.966]
Epoch [62/120    avg_loss:0.232, val_acc:0.956]
Epoch [63/120    avg_loss:0.227, val_acc:0.958]
Epoch [64/120    avg_loss:0.295, val_acc:0.958]
Epoch [65/120    avg_loss:0.245, val_acc:0.948]
Epoch [66/120    avg_loss:0.283, val_acc:0.944]
Epoch [67/120    avg_loss:0.225, val_acc:0.948]
Epoch [68/120    avg_loss:0.173, val_acc:0.950]
Epoch [69/120    avg_loss:0.177, val_acc:0.964]
Epoch [70/120    avg_loss:0.186, val_acc:0.966]
Epoch [71/120    avg_loss:0.183, val_acc:0.956]
Epoch [72/120    avg_loss:0.214, val_acc:0.960]
Epoch [73/120    avg_loss:0.255, val_acc:0.950]
Epoch [74/120    avg_loss:0.214, val_acc:0.962]
Epoch [75/120    avg_loss:0.198, val_acc:0.940]
Epoch [76/120    avg_loss:0.209, val_acc:0.956]
Epoch [77/120    avg_loss:0.235, val_acc:0.948]
Epoch [78/120    avg_loss:0.196, val_acc:0.964]
Epoch [79/120    avg_loss:0.147, val_acc:0.954]
Epoch [80/120    avg_loss:0.175, val_acc:0.962]
Epoch [81/120    avg_loss:0.136, val_acc:0.966]
Epoch [82/120    avg_loss:0.141, val_acc:0.978]
Epoch [83/120    avg_loss:0.125, val_acc:0.954]
Epoch [84/120    avg_loss:0.100, val_acc:0.960]
Epoch [85/120    avg_loss:0.113, val_acc:0.968]
Epoch [86/120    avg_loss:0.130, val_acc:0.960]
Epoch [87/120    avg_loss:0.119, val_acc:0.966]
Epoch [88/120    avg_loss:0.110, val_acc:0.972]
Epoch [89/120    avg_loss:0.082, val_acc:0.962]
Epoch [90/120    avg_loss:0.104, val_acc:0.960]
Epoch [91/120    avg_loss:0.109, val_acc:0.974]
Epoch [92/120    avg_loss:0.133, val_acc:0.974]
Epoch [93/120    avg_loss:0.101, val_acc:0.964]
Epoch [94/120    avg_loss:0.089, val_acc:0.954]
Epoch [95/120    avg_loss:0.118, val_acc:0.974]
Epoch [96/120    avg_loss:0.072, val_acc:0.974]
Epoch [97/120    avg_loss:0.067, val_acc:0.972]
Epoch [98/120    avg_loss:0.071, val_acc:0.978]
Epoch [99/120    avg_loss:0.061, val_acc:0.978]
Epoch [100/120    avg_loss:0.071, val_acc:0.978]
Epoch [101/120    avg_loss:0.066, val_acc:0.976]
Epoch [102/120    avg_loss:0.052, val_acc:0.978]
Epoch [103/120    avg_loss:0.055, val_acc:0.978]
Epoch [104/120    avg_loss:0.062, val_acc:0.978]
Epoch [105/120    avg_loss:0.054, val_acc:0.978]
Epoch [106/120    avg_loss:0.053, val_acc:0.976]
Epoch [107/120    avg_loss:0.053, val_acc:0.976]
Epoch [108/120    avg_loss:0.058, val_acc:0.976]
Epoch [109/120    avg_loss:0.058, val_acc:0.976]
Epoch [110/120    avg_loss:0.060, val_acc:0.976]
Epoch [111/120    avg_loss:0.059, val_acc:0.978]
Epoch [112/120    avg_loss:0.059, val_acc:0.978]
Epoch [113/120    avg_loss:0.048, val_acc:0.978]
Epoch [114/120    avg_loss:0.054, val_acc:0.978]
Epoch [115/120    avg_loss:0.058, val_acc:0.978]
Epoch [116/120    avg_loss:0.065, val_acc:0.978]
Epoch [117/120    avg_loss:0.060, val_acc:0.976]
Epoch [118/120    avg_loss:0.062, val_acc:0.978]
Epoch [119/120    avg_loss:0.058, val_acc:0.980]
Epoch [120/120    avg_loss:0.057, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 226   2   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  22   0   0   0   0  72   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 0.99854227 0.94759825 0.99122807 0.94481236 0.92150171
 0.99512195 0.85714286 0.99614891 0.99893276 1.         0.99734043
 0.99558499 1.        ]

Kappa:
0.9864667747245743
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fccfea0fe10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.347]
Epoch [2/120    avg_loss:2.566, val_acc:0.339]
Epoch [3/120    avg_loss:2.517, val_acc:0.341]
Epoch [4/120    avg_loss:2.462, val_acc:0.329]
Epoch [5/120    avg_loss:2.403, val_acc:0.357]
Epoch [6/120    avg_loss:2.348, val_acc:0.385]
Epoch [7/120    avg_loss:2.295, val_acc:0.437]
Epoch [8/120    avg_loss:2.232, val_acc:0.512]
Epoch [9/120    avg_loss:2.175, val_acc:0.589]
Epoch [10/120    avg_loss:2.107, val_acc:0.595]
Epoch [11/120    avg_loss:2.020, val_acc:0.665]
Epoch [12/120    avg_loss:1.967, val_acc:0.698]
Epoch [13/120    avg_loss:1.866, val_acc:0.724]
Epoch [14/120    avg_loss:1.766, val_acc:0.726]
Epoch [15/120    avg_loss:1.679, val_acc:0.738]
Epoch [16/120    avg_loss:1.575, val_acc:0.776]
Epoch [17/120    avg_loss:1.493, val_acc:0.812]
Epoch [18/120    avg_loss:1.396, val_acc:0.786]
Epoch [19/120    avg_loss:1.291, val_acc:0.831]
Epoch [20/120    avg_loss:1.222, val_acc:0.837]
Epoch [21/120    avg_loss:1.093, val_acc:0.871]
Epoch [22/120    avg_loss:1.022, val_acc:0.849]
Epoch [23/120    avg_loss:0.963, val_acc:0.883]
Epoch [24/120    avg_loss:0.877, val_acc:0.885]
Epoch [25/120    avg_loss:0.809, val_acc:0.873]
Epoch [26/120    avg_loss:0.761, val_acc:0.903]
Epoch [27/120    avg_loss:0.697, val_acc:0.893]
Epoch [28/120    avg_loss:0.682, val_acc:0.899]
Epoch [29/120    avg_loss:0.621, val_acc:0.921]
Epoch [30/120    avg_loss:0.604, val_acc:0.909]
Epoch [31/120    avg_loss:0.575, val_acc:0.899]
Epoch [32/120    avg_loss:0.509, val_acc:0.933]
Epoch [33/120    avg_loss:0.471, val_acc:0.915]
Epoch [34/120    avg_loss:0.507, val_acc:0.927]
Epoch [35/120    avg_loss:0.465, val_acc:0.925]
Epoch [36/120    avg_loss:0.514, val_acc:0.907]
Epoch [37/120    avg_loss:0.501, val_acc:0.913]
Epoch [38/120    avg_loss:0.434, val_acc:0.919]
Epoch [39/120    avg_loss:0.453, val_acc:0.938]
Epoch [40/120    avg_loss:0.388, val_acc:0.923]
Epoch [41/120    avg_loss:0.362, val_acc:0.927]
Epoch [42/120    avg_loss:0.365, val_acc:0.942]
Epoch [43/120    avg_loss:0.356, val_acc:0.915]
Epoch [44/120    avg_loss:0.356, val_acc:0.893]
Epoch [45/120    avg_loss:0.372, val_acc:0.942]
Epoch [46/120    avg_loss:0.365, val_acc:0.893]
Epoch [47/120    avg_loss:0.372, val_acc:0.946]
Epoch [48/120    avg_loss:0.324, val_acc:0.927]
Epoch [49/120    avg_loss:0.300, val_acc:0.937]
Epoch [50/120    avg_loss:0.297, val_acc:0.946]
Epoch [51/120    avg_loss:0.253, val_acc:0.948]
Epoch [52/120    avg_loss:0.228, val_acc:0.937]
Epoch [53/120    avg_loss:0.287, val_acc:0.944]
Epoch [54/120    avg_loss:0.286, val_acc:0.946]
Epoch [55/120    avg_loss:0.278, val_acc:0.925]
Epoch [56/120    avg_loss:0.240, val_acc:0.935]
Epoch [57/120    avg_loss:0.207, val_acc:0.948]
Epoch [58/120    avg_loss:0.213, val_acc:0.938]
Epoch [59/120    avg_loss:0.214, val_acc:0.946]
Epoch [60/120    avg_loss:0.242, val_acc:0.940]
Epoch [61/120    avg_loss:0.234, val_acc:0.946]
Epoch [62/120    avg_loss:0.202, val_acc:0.944]
Epoch [63/120    avg_loss:0.206, val_acc:0.938]
Epoch [64/120    avg_loss:0.215, val_acc:0.954]
Epoch [65/120    avg_loss:0.170, val_acc:0.958]
Epoch [66/120    avg_loss:0.167, val_acc:0.950]
Epoch [67/120    avg_loss:0.250, val_acc:0.954]
Epoch [68/120    avg_loss:0.238, val_acc:0.895]
Epoch [69/120    avg_loss:0.236, val_acc:0.948]
Epoch [70/120    avg_loss:0.171, val_acc:0.954]
Epoch [71/120    avg_loss:0.243, val_acc:0.944]
Epoch [72/120    avg_loss:0.253, val_acc:0.933]
Epoch [73/120    avg_loss:0.212, val_acc:0.954]
Epoch [74/120    avg_loss:0.175, val_acc:0.968]
Epoch [75/120    avg_loss:0.163, val_acc:0.954]
Epoch [76/120    avg_loss:0.146, val_acc:0.942]
Epoch [77/120    avg_loss:0.187, val_acc:0.927]
Epoch [78/120    avg_loss:0.163, val_acc:0.956]
Epoch [79/120    avg_loss:0.174, val_acc:0.956]
Epoch [80/120    avg_loss:0.139, val_acc:0.966]
Epoch [81/120    avg_loss:0.149, val_acc:0.942]
Epoch [82/120    avg_loss:0.131, val_acc:0.954]
Epoch [83/120    avg_loss:0.135, val_acc:0.966]
Epoch [84/120    avg_loss:0.176, val_acc:0.954]
Epoch [85/120    avg_loss:0.151, val_acc:0.958]
Epoch [86/120    avg_loss:0.140, val_acc:0.915]
Epoch [87/120    avg_loss:0.161, val_acc:0.956]
Epoch [88/120    avg_loss:0.113, val_acc:0.960]
Epoch [89/120    avg_loss:0.113, val_acc:0.966]
Epoch [90/120    avg_loss:0.105, val_acc:0.966]
Epoch [91/120    avg_loss:0.113, val_acc:0.970]
Epoch [92/120    avg_loss:0.098, val_acc:0.970]
Epoch [93/120    avg_loss:0.102, val_acc:0.970]
Epoch [94/120    avg_loss:0.106, val_acc:0.976]
Epoch [95/120    avg_loss:0.087, val_acc:0.974]
Epoch [96/120    avg_loss:0.097, val_acc:0.972]
Epoch [97/120    avg_loss:0.089, val_acc:0.972]
Epoch [98/120    avg_loss:0.086, val_acc:0.972]
Epoch [99/120    avg_loss:0.092, val_acc:0.974]
Epoch [100/120    avg_loss:0.077, val_acc:0.972]
Epoch [101/120    avg_loss:0.081, val_acc:0.974]
Epoch [102/120    avg_loss:0.080, val_acc:0.976]
Epoch [103/120    avg_loss:0.071, val_acc:0.972]
Epoch [104/120    avg_loss:0.074, val_acc:0.974]
Epoch [105/120    avg_loss:0.075, val_acc:0.972]
Epoch [106/120    avg_loss:0.080, val_acc:0.974]
Epoch [107/120    avg_loss:0.063, val_acc:0.974]
Epoch [108/120    avg_loss:0.079, val_acc:0.972]
Epoch [109/120    avg_loss:0.070, val_acc:0.974]
Epoch [110/120    avg_loss:0.068, val_acc:0.972]
Epoch [111/120    avg_loss:0.069, val_acc:0.974]
Epoch [112/120    avg_loss:0.076, val_acc:0.976]
Epoch [113/120    avg_loss:0.074, val_acc:0.976]
Epoch [114/120    avg_loss:0.069, val_acc:0.974]
Epoch [115/120    avg_loss:0.082, val_acc:0.976]
Epoch [116/120    avg_loss:0.081, val_acc:0.976]
Epoch [117/120    avg_loss:0.066, val_acc:0.976]
Epoch [118/120    avg_loss:0.075, val_acc:0.976]
Epoch [119/120    avg_loss:0.067, val_acc:0.974]
Epoch [120/120    avg_loss:0.067, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 224   4   0   0   0   1   1   0   0   0   0]
 [  0   0   0   1 226   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1  39 105   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   1   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 0.99927061 0.95927602 0.98030635 0.91129032 0.84
 1.         0.90217391 0.99742268 0.99893276 1.         1.
 0.99889503 1.        ]

Kappa:
0.984092025377834
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03bd154d68>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.615, val_acc:0.177]
Epoch [2/120    avg_loss:2.576, val_acc:0.173]
Epoch [3/120    avg_loss:2.536, val_acc:0.317]
Epoch [4/120    avg_loss:2.501, val_acc:0.355]
Epoch [5/120    avg_loss:2.461, val_acc:0.357]
Epoch [6/120    avg_loss:2.427, val_acc:0.373]
Epoch [7/120    avg_loss:2.398, val_acc:0.383]
Epoch [8/120    avg_loss:2.362, val_acc:0.401]
Epoch [9/120    avg_loss:2.318, val_acc:0.435]
Epoch [10/120    avg_loss:2.276, val_acc:0.448]
Epoch [11/120    avg_loss:2.219, val_acc:0.474]
Epoch [12/120    avg_loss:2.149, val_acc:0.494]
Epoch [13/120    avg_loss:2.070, val_acc:0.488]
Epoch [14/120    avg_loss:1.984, val_acc:0.500]
Epoch [15/120    avg_loss:1.919, val_acc:0.518]
Epoch [16/120    avg_loss:1.824, val_acc:0.534]
Epoch [17/120    avg_loss:1.753, val_acc:0.554]
Epoch [18/120    avg_loss:1.664, val_acc:0.583]
Epoch [19/120    avg_loss:1.563, val_acc:0.609]
Epoch [20/120    avg_loss:1.481, val_acc:0.631]
Epoch [21/120    avg_loss:1.404, val_acc:0.641]
Epoch [22/120    avg_loss:1.308, val_acc:0.690]
Epoch [23/120    avg_loss:1.210, val_acc:0.700]
Epoch [24/120    avg_loss:1.110, val_acc:0.766]
Epoch [25/120    avg_loss:1.049, val_acc:0.784]
Epoch [26/120    avg_loss:0.968, val_acc:0.847]
Epoch [27/120    avg_loss:0.891, val_acc:0.794]
Epoch [28/120    avg_loss:0.867, val_acc:0.770]
Epoch [29/120    avg_loss:0.769, val_acc:0.903]
Epoch [30/120    avg_loss:0.746, val_acc:0.911]
Epoch [31/120    avg_loss:0.677, val_acc:0.929]
Epoch [32/120    avg_loss:0.647, val_acc:0.909]
Epoch [33/120    avg_loss:0.631, val_acc:0.915]
Epoch [34/120    avg_loss:0.650, val_acc:0.931]
Epoch [35/120    avg_loss:0.565, val_acc:0.907]
Epoch [36/120    avg_loss:0.497, val_acc:0.903]
Epoch [37/120    avg_loss:0.472, val_acc:0.913]
Epoch [38/120    avg_loss:0.465, val_acc:0.915]
Epoch [39/120    avg_loss:0.505, val_acc:0.915]
Epoch [40/120    avg_loss:0.452, val_acc:0.923]
Epoch [41/120    avg_loss:0.406, val_acc:0.913]
Epoch [42/120    avg_loss:0.396, val_acc:0.903]
Epoch [43/120    avg_loss:0.444, val_acc:0.905]
Epoch [44/120    avg_loss:0.457, val_acc:0.901]
Epoch [45/120    avg_loss:0.364, val_acc:0.923]
Epoch [46/120    avg_loss:0.334, val_acc:0.921]
Epoch [47/120    avg_loss:0.322, val_acc:0.917]
Epoch [48/120    avg_loss:0.309, val_acc:0.938]
Epoch [49/120    avg_loss:0.285, val_acc:0.946]
Epoch [50/120    avg_loss:0.252, val_acc:0.948]
Epoch [51/120    avg_loss:0.241, val_acc:0.954]
Epoch [52/120    avg_loss:0.238, val_acc:0.952]
Epoch [53/120    avg_loss:0.237, val_acc:0.954]
Epoch [54/120    avg_loss:0.228, val_acc:0.960]
Epoch [55/120    avg_loss:0.233, val_acc:0.956]
Epoch [56/120    avg_loss:0.243, val_acc:0.954]
Epoch [57/120    avg_loss:0.234, val_acc:0.960]
Epoch [58/120    avg_loss:0.241, val_acc:0.958]
Epoch [59/120    avg_loss:0.231, val_acc:0.958]
Epoch [60/120    avg_loss:0.231, val_acc:0.962]
Epoch [61/120    avg_loss:0.216, val_acc:0.962]
Epoch [62/120    avg_loss:0.243, val_acc:0.960]
Epoch [63/120    avg_loss:0.228, val_acc:0.960]
Epoch [64/120    avg_loss:0.212, val_acc:0.962]
Epoch [65/120    avg_loss:0.229, val_acc:0.956]
Epoch [66/120    avg_loss:0.215, val_acc:0.964]
Epoch [67/120    avg_loss:0.213, val_acc:0.960]
Epoch [68/120    avg_loss:0.218, val_acc:0.962]
Epoch [69/120    avg_loss:0.199, val_acc:0.962]
Epoch [70/120    avg_loss:0.214, val_acc:0.960]
Epoch [71/120    avg_loss:0.209, val_acc:0.960]
Epoch [72/120    avg_loss:0.226, val_acc:0.964]
Epoch [73/120    avg_loss:0.199, val_acc:0.962]
Epoch [74/120    avg_loss:0.206, val_acc:0.964]
Epoch [75/120    avg_loss:0.181, val_acc:0.962]
Epoch [76/120    avg_loss:0.194, val_acc:0.964]
Epoch [77/120    avg_loss:0.198, val_acc:0.962]
Epoch [78/120    avg_loss:0.210, val_acc:0.968]
Epoch [79/120    avg_loss:0.202, val_acc:0.970]
Epoch [80/120    avg_loss:0.207, val_acc:0.962]
Epoch [81/120    avg_loss:0.218, val_acc:0.970]
Epoch [82/120    avg_loss:0.191, val_acc:0.962]
Epoch [83/120    avg_loss:0.193, val_acc:0.960]
Epoch [84/120    avg_loss:0.212, val_acc:0.966]
Epoch [85/120    avg_loss:0.200, val_acc:0.970]
Epoch [86/120    avg_loss:0.181, val_acc:0.964]
Epoch [87/120    avg_loss:0.202, val_acc:0.966]
Epoch [88/120    avg_loss:0.181, val_acc:0.962]
Epoch [89/120    avg_loss:0.180, val_acc:0.968]
Epoch [90/120    avg_loss:0.160, val_acc:0.968]
Epoch [91/120    avg_loss:0.176, val_acc:0.968]
Epoch [92/120    avg_loss:0.176, val_acc:0.958]
Epoch [93/120    avg_loss:0.182, val_acc:0.970]
Epoch [94/120    avg_loss:0.162, val_acc:0.972]
Epoch [95/120    avg_loss:0.168, val_acc:0.964]
Epoch [96/120    avg_loss:0.173, val_acc:0.970]
Epoch [97/120    avg_loss:0.173, val_acc:0.966]
Epoch [98/120    avg_loss:0.165, val_acc:0.968]
Epoch [99/120    avg_loss:0.174, val_acc:0.966]
Epoch [100/120    avg_loss:0.171, val_acc:0.972]
Epoch [101/120    avg_loss:0.164, val_acc:0.972]
Epoch [102/120    avg_loss:0.168, val_acc:0.970]
Epoch [103/120    avg_loss:0.171, val_acc:0.970]
Epoch [104/120    avg_loss:0.177, val_acc:0.970]
Epoch [105/120    avg_loss:0.159, val_acc:0.970]
Epoch [106/120    avg_loss:0.169, val_acc:0.970]
Epoch [107/120    avg_loss:0.164, val_acc:0.968]
Epoch [108/120    avg_loss:0.168, val_acc:0.972]
Epoch [109/120    avg_loss:0.157, val_acc:0.972]
Epoch [110/120    avg_loss:0.180, val_acc:0.972]
Epoch [111/120    avg_loss:0.151, val_acc:0.974]
Epoch [112/120    avg_loss:0.161, val_acc:0.970]
Epoch [113/120    avg_loss:0.154, val_acc:0.972]
Epoch [114/120    avg_loss:0.160, val_acc:0.972]
Epoch [115/120    avg_loss:0.148, val_acc:0.974]
Epoch [116/120    avg_loss:0.158, val_acc:0.974]
Epoch [117/120    avg_loss:0.153, val_acc:0.974]
Epoch [118/120    avg_loss:0.143, val_acc:0.976]
Epoch [119/120    avg_loss:0.141, val_acc:0.972]
Epoch [120/120    avg_loss:0.144, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 205   0   0   0   0  14   0   0   0   0   0   0]
 [  0   0   0 217   6   3   0   0   0   4   0   0   0   0]
 [  0   0   0   0 200  27   0   0   0   0   0   0   0   0]
 [  0   0   0   0  35 110   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.67590618336887

F1 scores:
[       nan 1.         0.93394077 0.97091723 0.8492569  0.77192982
 0.99266504 0.84491979 1.         0.99574468 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9741232276187634
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f745cb2ce10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.626, val_acc:0.063]
Epoch [2/120    avg_loss:2.572, val_acc:0.230]
Epoch [3/120    avg_loss:2.533, val_acc:0.286]
Epoch [4/120    avg_loss:2.492, val_acc:0.292]
Epoch [5/120    avg_loss:2.449, val_acc:0.296]
Epoch [6/120    avg_loss:2.407, val_acc:0.359]
Epoch [7/120    avg_loss:2.361, val_acc:0.421]
Epoch [8/120    avg_loss:2.316, val_acc:0.494]
Epoch [9/120    avg_loss:2.256, val_acc:0.464]
Epoch [10/120    avg_loss:2.194, val_acc:0.492]
Epoch [11/120    avg_loss:2.113, val_acc:0.524]
Epoch [12/120    avg_loss:2.052, val_acc:0.601]
Epoch [13/120    avg_loss:2.008, val_acc:0.605]
Epoch [14/120    avg_loss:1.900, val_acc:0.708]
Epoch [15/120    avg_loss:1.797, val_acc:0.714]
Epoch [16/120    avg_loss:1.749, val_acc:0.752]
Epoch [17/120    avg_loss:1.678, val_acc:0.688]
Epoch [18/120    avg_loss:1.595, val_acc:0.770]
Epoch [19/120    avg_loss:1.499, val_acc:0.835]
Epoch [20/120    avg_loss:1.379, val_acc:0.837]
Epoch [21/120    avg_loss:1.279, val_acc:0.841]
Epoch [22/120    avg_loss:1.167, val_acc:0.853]
Epoch [23/120    avg_loss:1.108, val_acc:0.881]
Epoch [24/120    avg_loss:0.999, val_acc:0.853]
Epoch [25/120    avg_loss:0.925, val_acc:0.875]
Epoch [26/120    avg_loss:0.884, val_acc:0.873]
Epoch [27/120    avg_loss:0.832, val_acc:0.889]
Epoch [28/120    avg_loss:0.783, val_acc:0.871]
Epoch [29/120    avg_loss:0.760, val_acc:0.875]
Epoch [30/120    avg_loss:0.669, val_acc:0.883]
Epoch [31/120    avg_loss:0.641, val_acc:0.895]
Epoch [32/120    avg_loss:0.608, val_acc:0.893]
Epoch [33/120    avg_loss:0.575, val_acc:0.933]
Epoch [34/120    avg_loss:0.524, val_acc:0.899]
Epoch [35/120    avg_loss:0.507, val_acc:0.931]
Epoch [36/120    avg_loss:0.491, val_acc:0.925]
Epoch [37/120    avg_loss:0.493, val_acc:0.863]
Epoch [38/120    avg_loss:0.455, val_acc:0.901]
Epoch [39/120    avg_loss:0.491, val_acc:0.907]
Epoch [40/120    avg_loss:0.440, val_acc:0.881]
Epoch [41/120    avg_loss:0.451, val_acc:0.806]
Epoch [42/120    avg_loss:0.473, val_acc:0.921]
Epoch [43/120    avg_loss:0.409, val_acc:0.903]
Epoch [44/120    avg_loss:0.450, val_acc:0.942]
Epoch [45/120    avg_loss:0.459, val_acc:0.907]
Epoch [46/120    avg_loss:0.380, val_acc:0.897]
Epoch [47/120    avg_loss:0.388, val_acc:0.919]
Epoch [48/120    avg_loss:0.371, val_acc:0.907]
Epoch [49/120    avg_loss:0.364, val_acc:0.935]
Epoch [50/120    avg_loss:0.326, val_acc:0.911]
Epoch [51/120    avg_loss:0.310, val_acc:0.935]
Epoch [52/120    avg_loss:0.324, val_acc:0.921]
Epoch [53/120    avg_loss:0.300, val_acc:0.929]
Epoch [54/120    avg_loss:0.314, val_acc:0.931]
Epoch [55/120    avg_loss:0.310, val_acc:0.903]
Epoch [56/120    avg_loss:0.281, val_acc:0.937]
Epoch [57/120    avg_loss:0.279, val_acc:0.931]
Epoch [58/120    avg_loss:0.249, val_acc:0.938]
Epoch [59/120    avg_loss:0.205, val_acc:0.944]
Epoch [60/120    avg_loss:0.206, val_acc:0.950]
Epoch [61/120    avg_loss:0.216, val_acc:0.950]
Epoch [62/120    avg_loss:0.193, val_acc:0.954]
Epoch [63/120    avg_loss:0.188, val_acc:0.952]
Epoch [64/120    avg_loss:0.185, val_acc:0.954]
Epoch [65/120    avg_loss:0.211, val_acc:0.948]
Epoch [66/120    avg_loss:0.185, val_acc:0.954]
Epoch [67/120    avg_loss:0.206, val_acc:0.956]
Epoch [68/120    avg_loss:0.174, val_acc:0.954]
Epoch [69/120    avg_loss:0.192, val_acc:0.962]
Epoch [70/120    avg_loss:0.199, val_acc:0.954]
Epoch [71/120    avg_loss:0.190, val_acc:0.962]
Epoch [72/120    avg_loss:0.192, val_acc:0.958]
Epoch [73/120    avg_loss:0.188, val_acc:0.960]
Epoch [74/120    avg_loss:0.174, val_acc:0.966]
Epoch [75/120    avg_loss:0.187, val_acc:0.952]
Epoch [76/120    avg_loss:0.175, val_acc:0.962]
Epoch [77/120    avg_loss:0.176, val_acc:0.960]
Epoch [78/120    avg_loss:0.185, val_acc:0.964]
Epoch [79/120    avg_loss:0.177, val_acc:0.964]
Epoch [80/120    avg_loss:0.189, val_acc:0.964]
Epoch [81/120    avg_loss:0.166, val_acc:0.960]
Epoch [82/120    avg_loss:0.174, val_acc:0.966]
Epoch [83/120    avg_loss:0.173, val_acc:0.968]
Epoch [84/120    avg_loss:0.172, val_acc:0.970]
Epoch [85/120    avg_loss:0.160, val_acc:0.970]
Epoch [86/120    avg_loss:0.174, val_acc:0.968]
Epoch [87/120    avg_loss:0.174, val_acc:0.966]
Epoch [88/120    avg_loss:0.163, val_acc:0.962]
Epoch [89/120    avg_loss:0.163, val_acc:0.968]
Epoch [90/120    avg_loss:0.166, val_acc:0.970]
Epoch [91/120    avg_loss:0.153, val_acc:0.968]
Epoch [92/120    avg_loss:0.169, val_acc:0.970]
Epoch [93/120    avg_loss:0.151, val_acc:0.962]
Epoch [94/120    avg_loss:0.155, val_acc:0.972]
Epoch [95/120    avg_loss:0.147, val_acc:0.960]
Epoch [96/120    avg_loss:0.154, val_acc:0.964]
Epoch [97/120    avg_loss:0.148, val_acc:0.966]
Epoch [98/120    avg_loss:0.153, val_acc:0.964]
Epoch [99/120    avg_loss:0.179, val_acc:0.964]
Epoch [100/120    avg_loss:0.151, val_acc:0.974]
Epoch [101/120    avg_loss:0.181, val_acc:0.968]
Epoch [102/120    avg_loss:0.172, val_acc:0.966]
Epoch [103/120    avg_loss:0.137, val_acc:0.972]
Epoch [104/120    avg_loss:0.144, val_acc:0.974]
Epoch [105/120    avg_loss:0.169, val_acc:0.968]
Epoch [106/120    avg_loss:0.142, val_acc:0.966]
Epoch [107/120    avg_loss:0.159, val_acc:0.970]
Epoch [108/120    avg_loss:0.150, val_acc:0.968]
Epoch [109/120    avg_loss:0.142, val_acc:0.972]
Epoch [110/120    avg_loss:0.147, val_acc:0.974]
Epoch [111/120    avg_loss:0.160, val_acc:0.970]
Epoch [112/120    avg_loss:0.142, val_acc:0.972]
Epoch [113/120    avg_loss:0.156, val_acc:0.968]
Epoch [114/120    avg_loss:0.149, val_acc:0.970]
Epoch [115/120    avg_loss:0.137, val_acc:0.970]
Epoch [116/120    avg_loss:0.138, val_acc:0.970]
Epoch [117/120    avg_loss:0.147, val_acc:0.962]
Epoch [118/120    avg_loss:0.139, val_acc:0.972]
Epoch [119/120    avg_loss:0.141, val_acc:0.972]
Epoch [120/120    avg_loss:0.171, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   4   0   0   0   0   4   0]
 [  0   0   0 212   7   0   0   0   8   3   0   0   0   0]
 [  0   0   0   3 186  38   0   0   0   0   0   0   0   0]
 [  0   0   0   0  43 102   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20   0 186   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.92963752665246

F1 scores:
[       nan 1.         0.95475113 0.95280899 0.77018634 0.71578947
 0.94897959 0.91111111 0.98979592 0.99680511 1.         0.99734043
 0.99342105 1.        ]

Kappa:
0.9658086183787992
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe56fd17e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.191]
Epoch [2/120    avg_loss:2.583, val_acc:0.286]
Epoch [3/120    avg_loss:2.548, val_acc:0.286]
Epoch [4/120    avg_loss:2.508, val_acc:0.288]
Epoch [5/120    avg_loss:2.459, val_acc:0.290]
Epoch [6/120    avg_loss:2.424, val_acc:0.294]
Epoch [7/120    avg_loss:2.387, val_acc:0.298]
Epoch [8/120    avg_loss:2.344, val_acc:0.308]
Epoch [9/120    avg_loss:2.300, val_acc:0.343]
Epoch [10/120    avg_loss:2.254, val_acc:0.379]
Epoch [11/120    avg_loss:2.203, val_acc:0.401]
Epoch [12/120    avg_loss:2.149, val_acc:0.452]
Epoch [13/120    avg_loss:2.083, val_acc:0.488]
Epoch [14/120    avg_loss:2.037, val_acc:0.538]
Epoch [15/120    avg_loss:1.963, val_acc:0.565]
Epoch [16/120    avg_loss:1.884, val_acc:0.575]
Epoch [17/120    avg_loss:1.819, val_acc:0.585]
Epoch [18/120    avg_loss:1.749, val_acc:0.595]
Epoch [19/120    avg_loss:1.679, val_acc:0.633]
Epoch [20/120    avg_loss:1.578, val_acc:0.685]
Epoch [21/120    avg_loss:1.490, val_acc:0.683]
Epoch [22/120    avg_loss:1.435, val_acc:0.736]
Epoch [23/120    avg_loss:1.316, val_acc:0.770]
Epoch [24/120    avg_loss:1.250, val_acc:0.788]
Epoch [25/120    avg_loss:1.149, val_acc:0.821]
Epoch [26/120    avg_loss:1.050, val_acc:0.786]
Epoch [27/120    avg_loss:0.962, val_acc:0.859]
Epoch [28/120    avg_loss:0.916, val_acc:0.871]
Epoch [29/120    avg_loss:0.844, val_acc:0.877]
Epoch [30/120    avg_loss:0.759, val_acc:0.833]
Epoch [31/120    avg_loss:0.707, val_acc:0.909]
Epoch [32/120    avg_loss:0.650, val_acc:0.923]
Epoch [33/120    avg_loss:0.590, val_acc:0.889]
Epoch [34/120    avg_loss:0.598, val_acc:0.917]
Epoch [35/120    avg_loss:0.599, val_acc:0.907]
Epoch [36/120    avg_loss:0.577, val_acc:0.907]
Epoch [37/120    avg_loss:0.536, val_acc:0.911]
Epoch [38/120    avg_loss:0.578, val_acc:0.835]
Epoch [39/120    avg_loss:0.542, val_acc:0.897]
Epoch [40/120    avg_loss:0.462, val_acc:0.855]
Epoch [41/120    avg_loss:0.462, val_acc:0.911]
Epoch [42/120    avg_loss:0.447, val_acc:0.913]
Epoch [43/120    avg_loss:0.424, val_acc:0.899]
Epoch [44/120    avg_loss:0.374, val_acc:0.938]
Epoch [45/120    avg_loss:0.406, val_acc:0.919]
Epoch [46/120    avg_loss:0.359, val_acc:0.938]
Epoch [47/120    avg_loss:0.340, val_acc:0.929]
Epoch [48/120    avg_loss:0.310, val_acc:0.937]
Epoch [49/120    avg_loss:0.279, val_acc:0.948]
Epoch [50/120    avg_loss:0.267, val_acc:0.940]
Epoch [51/120    avg_loss:0.341, val_acc:0.915]
Epoch [52/120    avg_loss:0.298, val_acc:0.931]
Epoch [53/120    avg_loss:0.256, val_acc:0.927]
Epoch [54/120    avg_loss:0.325, val_acc:0.950]
Epoch [55/120    avg_loss:0.285, val_acc:0.931]
Epoch [56/120    avg_loss:0.268, val_acc:0.925]
Epoch [57/120    avg_loss:0.291, val_acc:0.927]
Epoch [58/120    avg_loss:0.246, val_acc:0.923]
Epoch [59/120    avg_loss:0.268, val_acc:0.946]
Epoch [60/120    avg_loss:0.249, val_acc:0.952]
Epoch [61/120    avg_loss:0.256, val_acc:0.954]
Epoch [62/120    avg_loss:0.312, val_acc:0.929]
Epoch [63/120    avg_loss:0.221, val_acc:0.958]
Epoch [64/120    avg_loss:0.158, val_acc:0.964]
Epoch [65/120    avg_loss:0.143, val_acc:0.956]
Epoch [66/120    avg_loss:0.156, val_acc:0.952]
Epoch [67/120    avg_loss:0.184, val_acc:0.950]
Epoch [68/120    avg_loss:0.188, val_acc:0.942]
Epoch [69/120    avg_loss:0.159, val_acc:0.960]
Epoch [70/120    avg_loss:0.125, val_acc:0.972]
Epoch [71/120    avg_loss:0.149, val_acc:0.958]
Epoch [72/120    avg_loss:0.147, val_acc:0.958]
Epoch [73/120    avg_loss:0.282, val_acc:0.929]
Epoch [74/120    avg_loss:0.168, val_acc:0.948]
Epoch [75/120    avg_loss:0.180, val_acc:0.929]
Epoch [76/120    avg_loss:0.192, val_acc:0.942]
Epoch [77/120    avg_loss:0.176, val_acc:0.958]
Epoch [78/120    avg_loss:0.158, val_acc:0.962]
Epoch [79/120    avg_loss:0.144, val_acc:0.962]
Epoch [80/120    avg_loss:0.133, val_acc:0.958]
Epoch [81/120    avg_loss:0.198, val_acc:0.937]
Epoch [82/120    avg_loss:0.145, val_acc:0.966]
Epoch [83/120    avg_loss:0.110, val_acc:0.972]
Epoch [84/120    avg_loss:0.099, val_acc:0.970]
Epoch [85/120    avg_loss:0.094, val_acc:0.974]
Epoch [86/120    avg_loss:0.085, val_acc:0.978]
Epoch [87/120    avg_loss:0.117, val_acc:0.968]
Epoch [88/120    avg_loss:0.097, val_acc:0.919]
Epoch [89/120    avg_loss:0.126, val_acc:0.964]
Epoch [90/120    avg_loss:0.113, val_acc:0.962]
Epoch [91/120    avg_loss:0.104, val_acc:0.964]
Epoch [92/120    avg_loss:0.082, val_acc:0.962]
Epoch [93/120    avg_loss:0.066, val_acc:0.974]
Epoch [94/120    avg_loss:0.066, val_acc:0.986]
Epoch [95/120    avg_loss:0.064, val_acc:0.986]
Epoch [96/120    avg_loss:0.097, val_acc:0.923]
Epoch [97/120    avg_loss:0.184, val_acc:0.972]
Epoch [98/120    avg_loss:0.106, val_acc:0.968]
Epoch [99/120    avg_loss:0.085, val_acc:0.976]
Epoch [100/120    avg_loss:0.075, val_acc:0.956]
Epoch [101/120    avg_loss:0.059, val_acc:0.976]
Epoch [102/120    avg_loss:0.107, val_acc:0.952]
Epoch [103/120    avg_loss:0.090, val_acc:0.972]
Epoch [104/120    avg_loss:0.086, val_acc:0.972]
Epoch [105/120    avg_loss:0.060, val_acc:0.982]
Epoch [106/120    avg_loss:0.060, val_acc:0.982]
Epoch [107/120    avg_loss:0.058, val_acc:0.978]
Epoch [108/120    avg_loss:0.059, val_acc:0.978]
Epoch [109/120    avg_loss:0.059, val_acc:0.984]
Epoch [110/120    avg_loss:0.037, val_acc:0.986]
Epoch [111/120    avg_loss:0.029, val_acc:0.986]
Epoch [112/120    avg_loss:0.033, val_acc:0.984]
Epoch [113/120    avg_loss:0.039, val_acc:0.984]
Epoch [114/120    avg_loss:0.033, val_acc:0.986]
Epoch [115/120    avg_loss:0.033, val_acc:0.986]
Epoch [116/120    avg_loss:0.029, val_acc:0.986]
Epoch [117/120    avg_loss:0.034, val_acc:0.984]
Epoch [118/120    avg_loss:0.034, val_acc:0.988]
Epoch [119/120    avg_loss:0.026, val_acc:0.988]
Epoch [120/120    avg_loss:0.037, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.97968397 0.98901099 0.93478261 0.91349481
 1.         0.95081967 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9907417989933257
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f122fa27dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.640, val_acc:0.101]
Epoch [2/120    avg_loss:2.584, val_acc:0.179]
Epoch [3/120    avg_loss:2.532, val_acc:0.276]
Epoch [4/120    avg_loss:2.480, val_acc:0.393]
Epoch [5/120    avg_loss:2.427, val_acc:0.389]
Epoch [6/120    avg_loss:2.383, val_acc:0.383]
Epoch [7/120    avg_loss:2.327, val_acc:0.387]
Epoch [8/120    avg_loss:2.271, val_acc:0.446]
Epoch [9/120    avg_loss:2.221, val_acc:0.484]
Epoch [10/120    avg_loss:2.179, val_acc:0.472]
Epoch [11/120    avg_loss:2.110, val_acc:0.571]
Epoch [12/120    avg_loss:2.047, val_acc:0.603]
Epoch [13/120    avg_loss:1.979, val_acc:0.613]
Epoch [14/120    avg_loss:1.924, val_acc:0.627]
Epoch [15/120    avg_loss:1.848, val_acc:0.639]
Epoch [16/120    avg_loss:1.786, val_acc:0.653]
Epoch [17/120    avg_loss:1.720, val_acc:0.685]
Epoch [18/120    avg_loss:1.670, val_acc:0.673]
Epoch [19/120    avg_loss:1.577, val_acc:0.740]
Epoch [20/120    avg_loss:1.483, val_acc:0.714]
Epoch [21/120    avg_loss:1.445, val_acc:0.728]
Epoch [22/120    avg_loss:1.367, val_acc:0.786]
Epoch [23/120    avg_loss:1.287, val_acc:0.833]
Epoch [24/120    avg_loss:1.212, val_acc:0.851]
Epoch [25/120    avg_loss:1.135, val_acc:0.853]
Epoch [26/120    avg_loss:1.073, val_acc:0.867]
Epoch [27/120    avg_loss:0.958, val_acc:0.907]
Epoch [28/120    avg_loss:0.902, val_acc:0.889]
Epoch [29/120    avg_loss:0.815, val_acc:0.889]
Epoch [30/120    avg_loss:0.755, val_acc:0.911]
Epoch [31/120    avg_loss:0.676, val_acc:0.921]
Epoch [32/120    avg_loss:0.641, val_acc:0.893]
Epoch [33/120    avg_loss:0.618, val_acc:0.905]
Epoch [34/120    avg_loss:0.668, val_acc:0.881]
Epoch [35/120    avg_loss:0.625, val_acc:0.901]
Epoch [36/120    avg_loss:0.593, val_acc:0.903]
Epoch [37/120    avg_loss:0.512, val_acc:0.875]
Epoch [38/120    avg_loss:0.525, val_acc:0.915]
Epoch [39/120    avg_loss:0.517, val_acc:0.895]
Epoch [40/120    avg_loss:0.500, val_acc:0.935]
Epoch [41/120    avg_loss:0.474, val_acc:0.891]
Epoch [42/120    avg_loss:0.475, val_acc:0.929]
Epoch [43/120    avg_loss:0.424, val_acc:0.913]
Epoch [44/120    avg_loss:0.445, val_acc:0.933]
Epoch [45/120    avg_loss:0.399, val_acc:0.927]
Epoch [46/120    avg_loss:0.381, val_acc:0.925]
Epoch [47/120    avg_loss:0.379, val_acc:0.950]
Epoch [48/120    avg_loss:0.362, val_acc:0.942]
Epoch [49/120    avg_loss:0.345, val_acc:0.937]
Epoch [50/120    avg_loss:0.337, val_acc:0.937]
Epoch [51/120    avg_loss:0.309, val_acc:0.944]
Epoch [52/120    avg_loss:0.310, val_acc:0.946]
Epoch [53/120    avg_loss:0.332, val_acc:0.931]
Epoch [54/120    avg_loss:0.302, val_acc:0.940]
Epoch [55/120    avg_loss:0.300, val_acc:0.958]
Epoch [56/120    avg_loss:0.347, val_acc:0.925]
Epoch [57/120    avg_loss:0.261, val_acc:0.960]
Epoch [58/120    avg_loss:0.259, val_acc:0.952]
Epoch [59/120    avg_loss:0.262, val_acc:0.937]
Epoch [60/120    avg_loss:0.262, val_acc:0.954]
Epoch [61/120    avg_loss:0.228, val_acc:0.940]
Epoch [62/120    avg_loss:0.241, val_acc:0.954]
Epoch [63/120    avg_loss:0.242, val_acc:0.946]
Epoch [64/120    avg_loss:0.198, val_acc:0.944]
Epoch [65/120    avg_loss:0.234, val_acc:0.960]
Epoch [66/120    avg_loss:0.257, val_acc:0.942]
Epoch [67/120    avg_loss:0.291, val_acc:0.942]
Epoch [68/120    avg_loss:0.233, val_acc:0.942]
Epoch [69/120    avg_loss:0.296, val_acc:0.950]
Epoch [70/120    avg_loss:0.246, val_acc:0.956]
Epoch [71/120    avg_loss:0.217, val_acc:0.956]
Epoch [72/120    avg_loss:0.209, val_acc:0.954]
Epoch [73/120    avg_loss:0.187, val_acc:0.954]
Epoch [74/120    avg_loss:0.172, val_acc:0.962]
Epoch [75/120    avg_loss:0.151, val_acc:0.972]
Epoch [76/120    avg_loss:0.180, val_acc:0.974]
Epoch [77/120    avg_loss:0.162, val_acc:0.974]
Epoch [78/120    avg_loss:0.187, val_acc:0.954]
Epoch [79/120    avg_loss:0.141, val_acc:0.976]
Epoch [80/120    avg_loss:0.124, val_acc:0.982]
Epoch [81/120    avg_loss:0.116, val_acc:0.958]
Epoch [82/120    avg_loss:0.175, val_acc:0.935]
Epoch [83/120    avg_loss:0.199, val_acc:0.954]
Epoch [84/120    avg_loss:0.168, val_acc:0.976]
Epoch [85/120    avg_loss:0.140, val_acc:0.984]
Epoch [86/120    avg_loss:0.144, val_acc:0.946]
Epoch [87/120    avg_loss:0.153, val_acc:0.972]
Epoch [88/120    avg_loss:0.141, val_acc:0.976]
Epoch [89/120    avg_loss:0.128, val_acc:0.950]
Epoch [90/120    avg_loss:0.147, val_acc:0.964]
Epoch [91/120    avg_loss:0.130, val_acc:0.970]
Epoch [92/120    avg_loss:0.090, val_acc:0.972]
Epoch [93/120    avg_loss:0.114, val_acc:0.976]
Epoch [94/120    avg_loss:0.129, val_acc:0.974]
Epoch [95/120    avg_loss:0.113, val_acc:0.974]
Epoch [96/120    avg_loss:0.098, val_acc:0.970]
Epoch [97/120    avg_loss:0.084, val_acc:0.982]
Epoch [98/120    avg_loss:0.119, val_acc:0.962]
Epoch [99/120    avg_loss:0.090, val_acc:0.974]
Epoch [100/120    avg_loss:0.088, val_acc:0.982]
Epoch [101/120    avg_loss:0.066, val_acc:0.978]
Epoch [102/120    avg_loss:0.064, val_acc:0.978]
Epoch [103/120    avg_loss:0.060, val_acc:0.982]
Epoch [104/120    avg_loss:0.058, val_acc:0.982]
Epoch [105/120    avg_loss:0.074, val_acc:0.986]
Epoch [106/120    avg_loss:0.062, val_acc:0.986]
Epoch [107/120    avg_loss:0.057, val_acc:0.982]
Epoch [108/120    avg_loss:0.055, val_acc:0.984]
Epoch [109/120    avg_loss:0.058, val_acc:0.984]
Epoch [110/120    avg_loss:0.070, val_acc:0.984]
Epoch [111/120    avg_loss:0.056, val_acc:0.982]
Epoch [112/120    avg_loss:0.057, val_acc:0.988]
Epoch [113/120    avg_loss:0.062, val_acc:0.986]
Epoch [114/120    avg_loss:0.061, val_acc:0.986]
Epoch [115/120    avg_loss:0.049, val_acc:0.988]
Epoch [116/120    avg_loss:0.046, val_acc:0.984]
Epoch [117/120    avg_loss:0.050, val_acc:0.984]
Epoch [118/120    avg_loss:0.050, val_acc:0.984]
Epoch [119/120    avg_loss:0.051, val_acc:0.984]
Epoch [120/120    avg_loss:0.046, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 215   7   0   0   0   2   6   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.37953091684435

F1 scores:
[       nan 0.997815   0.9752809  0.96629213 0.8967033  0.86486486
 0.99266504 0.93922652 0.99742931 0.99363057 1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9819562595716894
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f63836adef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.611, val_acc:0.129]
Epoch [2/120    avg_loss:2.578, val_acc:0.266]
Epoch [3/120    avg_loss:2.546, val_acc:0.298]
Epoch [4/120    avg_loss:2.512, val_acc:0.319]
Epoch [5/120    avg_loss:2.483, val_acc:0.363]
Epoch [6/120    avg_loss:2.442, val_acc:0.409]
Epoch [7/120    avg_loss:2.416, val_acc:0.438]
Epoch [8/120    avg_loss:2.376, val_acc:0.427]
Epoch [9/120    avg_loss:2.336, val_acc:0.429]
Epoch [10/120    avg_loss:2.295, val_acc:0.409]
Epoch [11/120    avg_loss:2.244, val_acc:0.427]
Epoch [12/120    avg_loss:2.187, val_acc:0.437]
Epoch [13/120    avg_loss:2.125, val_acc:0.470]
Epoch [14/120    avg_loss:2.060, val_acc:0.502]
Epoch [15/120    avg_loss:2.013, val_acc:0.532]
Epoch [16/120    avg_loss:1.952, val_acc:0.558]
Epoch [17/120    avg_loss:1.899, val_acc:0.607]
Epoch [18/120    avg_loss:1.819, val_acc:0.617]
Epoch [19/120    avg_loss:1.740, val_acc:0.651]
Epoch [20/120    avg_loss:1.663, val_acc:0.677]
Epoch [21/120    avg_loss:1.560, val_acc:0.712]
Epoch [22/120    avg_loss:1.481, val_acc:0.736]
Epoch [23/120    avg_loss:1.397, val_acc:0.752]
Epoch [24/120    avg_loss:1.315, val_acc:0.794]
Epoch [25/120    avg_loss:1.221, val_acc:0.778]
Epoch [26/120    avg_loss:1.182, val_acc:0.861]
Epoch [27/120    avg_loss:1.078, val_acc:0.853]
Epoch [28/120    avg_loss:1.019, val_acc:0.889]
Epoch [29/120    avg_loss:0.966, val_acc:0.833]
Epoch [30/120    avg_loss:0.863, val_acc:0.883]
Epoch [31/120    avg_loss:0.813, val_acc:0.911]
Epoch [32/120    avg_loss:0.774, val_acc:0.913]
Epoch [33/120    avg_loss:0.753, val_acc:0.921]
Epoch [34/120    avg_loss:0.673, val_acc:0.917]
Epoch [35/120    avg_loss:0.628, val_acc:0.915]
Epoch [36/120    avg_loss:0.596, val_acc:0.907]
Epoch [37/120    avg_loss:0.636, val_acc:0.933]
Epoch [38/120    avg_loss:0.589, val_acc:0.911]
Epoch [39/120    avg_loss:0.605, val_acc:0.891]
Epoch [40/120    avg_loss:0.577, val_acc:0.913]
Epoch [41/120    avg_loss:0.519, val_acc:0.923]
Epoch [42/120    avg_loss:0.555, val_acc:0.931]
Epoch [43/120    avg_loss:0.471, val_acc:0.925]
Epoch [44/120    avg_loss:0.422, val_acc:0.919]
Epoch [45/120    avg_loss:0.495, val_acc:0.948]
Epoch [46/120    avg_loss:0.454, val_acc:0.929]
Epoch [47/120    avg_loss:0.371, val_acc:0.950]
Epoch [48/120    avg_loss:0.406, val_acc:0.915]
Epoch [49/120    avg_loss:0.397, val_acc:0.940]
Epoch [50/120    avg_loss:0.358, val_acc:0.937]
Epoch [51/120    avg_loss:0.337, val_acc:0.942]
Epoch [52/120    avg_loss:0.336, val_acc:0.925]
Epoch [53/120    avg_loss:0.328, val_acc:0.954]
Epoch [54/120    avg_loss:0.313, val_acc:0.944]
Epoch [55/120    avg_loss:0.293, val_acc:0.956]
Epoch [56/120    avg_loss:0.295, val_acc:0.944]
Epoch [57/120    avg_loss:0.305, val_acc:0.962]
Epoch [58/120    avg_loss:0.283, val_acc:0.954]
Epoch [59/120    avg_loss:0.291, val_acc:0.946]
Epoch [60/120    avg_loss:0.266, val_acc:0.946]
Epoch [61/120    avg_loss:0.262, val_acc:0.935]
Epoch [62/120    avg_loss:0.256, val_acc:0.956]
Epoch [63/120    avg_loss:0.245, val_acc:0.970]
Epoch [64/120    avg_loss:0.241, val_acc:0.972]
Epoch [65/120    avg_loss:0.200, val_acc:0.970]
Epoch [66/120    avg_loss:0.243, val_acc:0.944]
Epoch [67/120    avg_loss:0.264, val_acc:0.968]
Epoch [68/120    avg_loss:0.237, val_acc:0.980]
Epoch [69/120    avg_loss:0.196, val_acc:0.982]
Epoch [70/120    avg_loss:0.181, val_acc:0.968]
Epoch [71/120    avg_loss:0.181, val_acc:0.960]
Epoch [72/120    avg_loss:0.248, val_acc:0.980]
Epoch [73/120    avg_loss:0.180, val_acc:0.974]
Epoch [74/120    avg_loss:0.294, val_acc:0.954]
Epoch [75/120    avg_loss:0.199, val_acc:0.966]
Epoch [76/120    avg_loss:0.174, val_acc:0.972]
Epoch [77/120    avg_loss:0.156, val_acc:0.970]
Epoch [78/120    avg_loss:0.196, val_acc:0.958]
Epoch [79/120    avg_loss:0.167, val_acc:0.972]
Epoch [80/120    avg_loss:0.175, val_acc:0.974]
Epoch [81/120    avg_loss:0.154, val_acc:0.974]
Epoch [82/120    avg_loss:0.175, val_acc:0.980]
Epoch [83/120    avg_loss:0.131, val_acc:0.980]
Epoch [84/120    avg_loss:0.125, val_acc:0.978]
Epoch [85/120    avg_loss:0.109, val_acc:0.980]
Epoch [86/120    avg_loss:0.109, val_acc:0.978]
Epoch [87/120    avg_loss:0.107, val_acc:0.982]
Epoch [88/120    avg_loss:0.101, val_acc:0.982]
Epoch [89/120    avg_loss:0.106, val_acc:0.982]
Epoch [90/120    avg_loss:0.102, val_acc:0.984]
Epoch [91/120    avg_loss:0.105, val_acc:0.982]
Epoch [92/120    avg_loss:0.094, val_acc:0.982]
Epoch [93/120    avg_loss:0.101, val_acc:0.980]
Epoch [94/120    avg_loss:0.089, val_acc:0.980]
Epoch [95/120    avg_loss:0.106, val_acc:0.980]
Epoch [96/120    avg_loss:0.105, val_acc:0.984]
Epoch [97/120    avg_loss:0.098, val_acc:0.984]
Epoch [98/120    avg_loss:0.091, val_acc:0.988]
Epoch [99/120    avg_loss:0.086, val_acc:0.982]
Epoch [100/120    avg_loss:0.086, val_acc:0.988]
Epoch [101/120    avg_loss:0.091, val_acc:0.988]
Epoch [102/120    avg_loss:0.099, val_acc:0.982]
Epoch [103/120    avg_loss:0.078, val_acc:0.982]
Epoch [104/120    avg_loss:0.102, val_acc:0.982]
Epoch [105/120    avg_loss:0.094, val_acc:0.984]
Epoch [106/120    avg_loss:0.096, val_acc:0.986]
Epoch [107/120    avg_loss:0.083, val_acc:0.982]
Epoch [108/120    avg_loss:0.090, val_acc:0.982]
Epoch [109/120    avg_loss:0.081, val_acc:0.988]
Epoch [110/120    avg_loss:0.082, val_acc:0.982]
Epoch [111/120    avg_loss:0.096, val_acc:0.982]
Epoch [112/120    avg_loss:0.082, val_acc:0.982]
Epoch [113/120    avg_loss:0.084, val_acc:0.988]
Epoch [114/120    avg_loss:0.087, val_acc:0.982]
Epoch [115/120    avg_loss:0.083, val_acc:0.982]
Epoch [116/120    avg_loss:0.091, val_acc:0.984]
Epoch [117/120    avg_loss:0.084, val_acc:0.984]
Epoch [118/120    avg_loss:0.084, val_acc:0.988]
Epoch [119/120    avg_loss:0.087, val_acc:0.984]
Epoch [120/120    avg_loss:0.104, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   2 217   9   0   0   1   0   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  34 111   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.31556503198294

F1 scores:
[       nan 1.         0.95768374 0.97091723 0.87966805 0.81918819
 1.         0.9        1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9812442677600348
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15b601fe80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.083]
Epoch [2/120    avg_loss:2.579, val_acc:0.083]
Epoch [3/120    avg_loss:2.547, val_acc:0.083]
Epoch [4/120    avg_loss:2.504, val_acc:0.117]
Epoch [5/120    avg_loss:2.473, val_acc:0.234]
Epoch [6/120    avg_loss:2.443, val_acc:0.282]
Epoch [7/120    avg_loss:2.398, val_acc:0.304]
Epoch [8/120    avg_loss:2.387, val_acc:0.315]
Epoch [9/120    avg_loss:2.342, val_acc:0.323]
Epoch [10/120    avg_loss:2.295, val_acc:0.385]
Epoch [11/120    avg_loss:2.250, val_acc:0.429]
Epoch [12/120    avg_loss:2.201, val_acc:0.454]
Epoch [13/120    avg_loss:2.149, val_acc:0.502]
Epoch [14/120    avg_loss:2.081, val_acc:0.591]
Epoch [15/120    avg_loss:2.017, val_acc:0.645]
Epoch [16/120    avg_loss:1.943, val_acc:0.623]
Epoch [17/120    avg_loss:1.866, val_acc:0.651]
Epoch [18/120    avg_loss:1.781, val_acc:0.734]
Epoch [19/120    avg_loss:1.686, val_acc:0.708]
Epoch [20/120    avg_loss:1.610, val_acc:0.724]
Epoch [21/120    avg_loss:1.536, val_acc:0.677]
Epoch [22/120    avg_loss:1.438, val_acc:0.817]
Epoch [23/120    avg_loss:1.348, val_acc:0.812]
Epoch [24/120    avg_loss:1.232, val_acc:0.815]
Epoch [25/120    avg_loss:1.175, val_acc:0.867]
Epoch [26/120    avg_loss:1.092, val_acc:0.823]
Epoch [27/120    avg_loss:1.021, val_acc:0.881]
Epoch [28/120    avg_loss:0.958, val_acc:0.883]
Epoch [29/120    avg_loss:0.860, val_acc:0.899]
Epoch [30/120    avg_loss:0.802, val_acc:0.907]
Epoch [31/120    avg_loss:0.781, val_acc:0.891]
Epoch [32/120    avg_loss:0.715, val_acc:0.903]
Epoch [33/120    avg_loss:0.696, val_acc:0.907]
Epoch [34/120    avg_loss:0.607, val_acc:0.921]
Epoch [35/120    avg_loss:0.587, val_acc:0.927]
Epoch [36/120    avg_loss:0.573, val_acc:0.923]
Epoch [37/120    avg_loss:0.555, val_acc:0.905]
Epoch [38/120    avg_loss:0.479, val_acc:0.942]
Epoch [39/120    avg_loss:0.431, val_acc:0.954]
Epoch [40/120    avg_loss:0.439, val_acc:0.931]
Epoch [41/120    avg_loss:0.454, val_acc:0.929]
Epoch [42/120    avg_loss:0.396, val_acc:0.942]
Epoch [43/120    avg_loss:0.426, val_acc:0.944]
Epoch [44/120    avg_loss:0.384, val_acc:0.938]
Epoch [45/120    avg_loss:0.378, val_acc:0.940]
Epoch [46/120    avg_loss:0.347, val_acc:0.935]
Epoch [47/120    avg_loss:0.314, val_acc:0.944]
Epoch [48/120    avg_loss:0.317, val_acc:0.927]
Epoch [49/120    avg_loss:0.316, val_acc:0.946]
Epoch [50/120    avg_loss:0.267, val_acc:0.952]
Epoch [51/120    avg_loss:0.275, val_acc:0.958]
Epoch [52/120    avg_loss:0.282, val_acc:0.962]
Epoch [53/120    avg_loss:0.267, val_acc:0.950]
Epoch [54/120    avg_loss:0.231, val_acc:0.946]
Epoch [55/120    avg_loss:0.284, val_acc:0.942]
Epoch [56/120    avg_loss:0.231, val_acc:0.950]
Epoch [57/120    avg_loss:0.229, val_acc:0.958]
Epoch [58/120    avg_loss:0.214, val_acc:0.968]
Epoch [59/120    avg_loss:0.187, val_acc:0.962]
Epoch [60/120    avg_loss:0.221, val_acc:0.960]
Epoch [61/120    avg_loss:0.196, val_acc:0.968]
Epoch [62/120    avg_loss:0.237, val_acc:0.976]
Epoch [63/120    avg_loss:0.200, val_acc:0.966]
Epoch [64/120    avg_loss:0.189, val_acc:0.974]
Epoch [65/120    avg_loss:0.167, val_acc:0.968]
Epoch [66/120    avg_loss:0.151, val_acc:0.954]
Epoch [67/120    avg_loss:0.202, val_acc:0.978]
Epoch [68/120    avg_loss:0.197, val_acc:0.966]
Epoch [69/120    avg_loss:0.144, val_acc:0.972]
Epoch [70/120    avg_loss:0.127, val_acc:0.950]
Epoch [71/120    avg_loss:0.194, val_acc:0.950]
Epoch [72/120    avg_loss:0.156, val_acc:0.968]
Epoch [73/120    avg_loss:0.135, val_acc:0.972]
Epoch [74/120    avg_loss:0.160, val_acc:0.974]
Epoch [75/120    avg_loss:0.125, val_acc:0.968]
Epoch [76/120    avg_loss:0.158, val_acc:0.958]
Epoch [77/120    avg_loss:0.167, val_acc:0.960]
Epoch [78/120    avg_loss:0.160, val_acc:0.966]
Epoch [79/120    avg_loss:0.150, val_acc:0.964]
Epoch [80/120    avg_loss:0.159, val_acc:0.950]
Epoch [81/120    avg_loss:0.143, val_acc:0.966]
Epoch [82/120    avg_loss:0.104, val_acc:0.982]
Epoch [83/120    avg_loss:0.089, val_acc:0.982]
Epoch [84/120    avg_loss:0.093, val_acc:0.986]
Epoch [85/120    avg_loss:0.092, val_acc:0.988]
Epoch [86/120    avg_loss:0.084, val_acc:0.988]
Epoch [87/120    avg_loss:0.078, val_acc:0.990]
Epoch [88/120    avg_loss:0.085, val_acc:0.986]
Epoch [89/120    avg_loss:0.082, val_acc:0.984]
Epoch [90/120    avg_loss:0.083, val_acc:0.984]
Epoch [91/120    avg_loss:0.081, val_acc:0.986]
Epoch [92/120    avg_loss:0.075, val_acc:0.988]
Epoch [93/120    avg_loss:0.070, val_acc:0.990]
Epoch [94/120    avg_loss:0.080, val_acc:0.986]
Epoch [95/120    avg_loss:0.075, val_acc:0.986]
Epoch [96/120    avg_loss:0.069, val_acc:0.988]
Epoch [97/120    avg_loss:0.068, val_acc:0.990]
Epoch [98/120    avg_loss:0.066, val_acc:0.990]
Epoch [99/120    avg_loss:0.066, val_acc:0.988]
Epoch [100/120    avg_loss:0.079, val_acc:0.990]
Epoch [101/120    avg_loss:0.069, val_acc:0.986]
Epoch [102/120    avg_loss:0.071, val_acc:0.986]
Epoch [103/120    avg_loss:0.065, val_acc:0.990]
Epoch [104/120    avg_loss:0.069, val_acc:0.990]
Epoch [105/120    avg_loss:0.070, val_acc:0.988]
Epoch [106/120    avg_loss:0.065, val_acc:0.990]
Epoch [107/120    avg_loss:0.063, val_acc:0.990]
Epoch [108/120    avg_loss:0.061, val_acc:0.990]
Epoch [109/120    avg_loss:0.061, val_acc:0.990]
Epoch [110/120    avg_loss:0.064, val_acc:0.992]
Epoch [111/120    avg_loss:0.067, val_acc:0.990]
Epoch [112/120    avg_loss:0.064, val_acc:0.986]
Epoch [113/120    avg_loss:0.058, val_acc:0.988]
Epoch [114/120    avg_loss:0.060, val_acc:0.990]
Epoch [115/120    avg_loss:0.056, val_acc:0.990]
Epoch [116/120    avg_loss:0.058, val_acc:0.988]
Epoch [117/120    avg_loss:0.057, val_acc:0.990]
Epoch [118/120    avg_loss:0.057, val_acc:0.990]
Epoch [119/120    avg_loss:0.055, val_acc:0.988]
Epoch [120/120    avg_loss:0.058, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   2   0   0   0   5   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.97333333 0.98230088 0.93362832 0.90847458
 0.99756691 0.93181818 0.99359795 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9886044599021712
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47a3f8cdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.183]
Epoch [2/120    avg_loss:2.552, val_acc:0.345]
Epoch [3/120    avg_loss:2.493, val_acc:0.373]
Epoch [4/120    avg_loss:2.455, val_acc:0.371]
Epoch [5/120    avg_loss:2.392, val_acc:0.365]
Epoch [6/120    avg_loss:2.338, val_acc:0.403]
Epoch [7/120    avg_loss:2.284, val_acc:0.407]
Epoch [8/120    avg_loss:2.225, val_acc:0.403]
Epoch [9/120    avg_loss:2.180, val_acc:0.411]
Epoch [10/120    avg_loss:2.101, val_acc:0.413]
Epoch [11/120    avg_loss:2.058, val_acc:0.466]
Epoch [12/120    avg_loss:1.995, val_acc:0.522]
Epoch [13/120    avg_loss:1.931, val_acc:0.621]
Epoch [14/120    avg_loss:1.861, val_acc:0.667]
Epoch [15/120    avg_loss:1.781, val_acc:0.681]
Epoch [16/120    avg_loss:1.696, val_acc:0.708]
Epoch [17/120    avg_loss:1.614, val_acc:0.718]
Epoch [18/120    avg_loss:1.513, val_acc:0.746]
Epoch [19/120    avg_loss:1.449, val_acc:0.764]
Epoch [20/120    avg_loss:1.316, val_acc:0.786]
Epoch [21/120    avg_loss:1.243, val_acc:0.812]
Epoch [22/120    avg_loss:1.166, val_acc:0.819]
Epoch [23/120    avg_loss:1.070, val_acc:0.853]
Epoch [24/120    avg_loss:0.980, val_acc:0.891]
Epoch [25/120    avg_loss:0.900, val_acc:0.889]
Epoch [26/120    avg_loss:0.820, val_acc:0.873]
Epoch [27/120    avg_loss:0.782, val_acc:0.887]
Epoch [28/120    avg_loss:0.722, val_acc:0.891]
Epoch [29/120    avg_loss:0.693, val_acc:0.915]
Epoch [30/120    avg_loss:0.623, val_acc:0.899]
Epoch [31/120    avg_loss:0.566, val_acc:0.911]
Epoch [32/120    avg_loss:0.538, val_acc:0.917]
Epoch [33/120    avg_loss:0.559, val_acc:0.911]
Epoch [34/120    avg_loss:0.500, val_acc:0.915]
Epoch [35/120    avg_loss:0.484, val_acc:0.911]
Epoch [36/120    avg_loss:0.463, val_acc:0.927]
Epoch [37/120    avg_loss:0.461, val_acc:0.925]
Epoch [38/120    avg_loss:0.466, val_acc:0.901]
Epoch [39/120    avg_loss:0.461, val_acc:0.935]
Epoch [40/120    avg_loss:0.427, val_acc:0.909]
Epoch [41/120    avg_loss:0.401, val_acc:0.925]
Epoch [42/120    avg_loss:0.387, val_acc:0.927]
Epoch [43/120    avg_loss:0.324, val_acc:0.929]
Epoch [44/120    avg_loss:0.352, val_acc:0.938]
Epoch [45/120    avg_loss:0.324, val_acc:0.909]
Epoch [46/120    avg_loss:0.377, val_acc:0.925]
Epoch [47/120    avg_loss:0.341, val_acc:0.925]
Epoch [48/120    avg_loss:0.341, val_acc:0.940]
Epoch [49/120    avg_loss:0.285, val_acc:0.938]
Epoch [50/120    avg_loss:0.298, val_acc:0.929]
Epoch [51/120    avg_loss:0.302, val_acc:0.938]
Epoch [52/120    avg_loss:0.253, val_acc:0.931]
Epoch [53/120    avg_loss:0.353, val_acc:0.857]
Epoch [54/120    avg_loss:0.366, val_acc:0.933]
Epoch [55/120    avg_loss:0.282, val_acc:0.948]
Epoch [56/120    avg_loss:0.316, val_acc:0.923]
Epoch [57/120    avg_loss:0.296, val_acc:0.931]
Epoch [58/120    avg_loss:0.315, val_acc:0.913]
Epoch [59/120    avg_loss:0.284, val_acc:0.931]
Epoch [60/120    avg_loss:0.294, val_acc:0.944]
Epoch [61/120    avg_loss:0.243, val_acc:0.950]
Epoch [62/120    avg_loss:0.215, val_acc:0.944]
Epoch [63/120    avg_loss:0.235, val_acc:0.925]
Epoch [64/120    avg_loss:0.274, val_acc:0.935]
Epoch [65/120    avg_loss:0.236, val_acc:0.938]
Epoch [66/120    avg_loss:0.189, val_acc:0.950]
Epoch [67/120    avg_loss:0.180, val_acc:0.952]
Epoch [68/120    avg_loss:0.174, val_acc:0.927]
Epoch [69/120    avg_loss:0.218, val_acc:0.935]
Epoch [70/120    avg_loss:0.221, val_acc:0.956]
Epoch [71/120    avg_loss:0.177, val_acc:0.962]
Epoch [72/120    avg_loss:0.192, val_acc:0.903]
Epoch [73/120    avg_loss:0.197, val_acc:0.958]
Epoch [74/120    avg_loss:0.166, val_acc:0.956]
Epoch [75/120    avg_loss:0.150, val_acc:0.966]
Epoch [76/120    avg_loss:0.195, val_acc:0.938]
Epoch [77/120    avg_loss:0.174, val_acc:0.952]
Epoch [78/120    avg_loss:0.197, val_acc:0.950]
Epoch [79/120    avg_loss:0.179, val_acc:0.968]
Epoch [80/120    avg_loss:0.148, val_acc:0.956]
Epoch [81/120    avg_loss:0.133, val_acc:0.956]
Epoch [82/120    avg_loss:0.138, val_acc:0.956]
Epoch [83/120    avg_loss:0.132, val_acc:0.972]
Epoch [84/120    avg_loss:0.121, val_acc:0.966]
Epoch [85/120    avg_loss:0.162, val_acc:0.966]
Epoch [86/120    avg_loss:0.133, val_acc:0.962]
Epoch [87/120    avg_loss:0.138, val_acc:0.966]
Epoch [88/120    avg_loss:0.137, val_acc:0.972]
Epoch [89/120    avg_loss:0.116, val_acc:0.962]
Epoch [90/120    avg_loss:0.147, val_acc:0.954]
Epoch [91/120    avg_loss:0.097, val_acc:0.968]
Epoch [92/120    avg_loss:0.112, val_acc:0.956]
Epoch [93/120    avg_loss:0.103, val_acc:0.974]
Epoch [94/120    avg_loss:0.096, val_acc:0.956]
Epoch [95/120    avg_loss:0.121, val_acc:0.970]
Epoch [96/120    avg_loss:0.102, val_acc:0.964]
Epoch [97/120    avg_loss:0.081, val_acc:0.972]
Epoch [98/120    avg_loss:0.080, val_acc:0.970]
Epoch [99/120    avg_loss:0.074, val_acc:0.962]
Epoch [100/120    avg_loss:0.103, val_acc:0.962]
Epoch [101/120    avg_loss:0.130, val_acc:0.958]
Epoch [102/120    avg_loss:0.118, val_acc:0.935]
Epoch [103/120    avg_loss:0.151, val_acc:0.960]
Epoch [104/120    avg_loss:0.083, val_acc:0.964]
Epoch [105/120    avg_loss:0.099, val_acc:0.976]
Epoch [106/120    avg_loss:0.113, val_acc:0.974]
Epoch [107/120    avg_loss:0.070, val_acc:0.974]
Epoch [108/120    avg_loss:0.068, val_acc:0.970]
Epoch [109/120    avg_loss:0.064, val_acc:0.974]
Epoch [110/120    avg_loss:0.058, val_acc:0.968]
Epoch [111/120    avg_loss:0.049, val_acc:0.972]
Epoch [112/120    avg_loss:0.066, val_acc:0.956]
Epoch [113/120    avg_loss:0.073, val_acc:0.978]
Epoch [114/120    avg_loss:0.060, val_acc:0.966]
Epoch [115/120    avg_loss:0.049, val_acc:0.976]
Epoch [116/120    avg_loss:0.047, val_acc:0.962]
Epoch [117/120    avg_loss:0.062, val_acc:0.966]
Epoch [118/120    avg_loss:0.083, val_acc:0.970]
Epoch [119/120    avg_loss:0.080, val_acc:0.958]
Epoch [120/120    avg_loss:0.089, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 179   0   0   0   0  40   0   0   0   0   0   0]
 [  0   0   0 210  14   0   0   2   1   3   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  64  81   0   0   0   0   0   0   0   0]
 [  0   8   0   0   1   0 197   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   2   5   0   0   0   0   0 381   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.71641791044776

F1 scores:
[       nan 0.99275362 0.88833747 0.95454545 0.84962406 0.71365639
 0.97766749 0.8173913  0.98961039 0.99680511 1.         0.98305085
 0.98544233 1.        ]

Kappa:
0.9634370786273408
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f113b8cfef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.107]
Epoch [2/120    avg_loss:2.585, val_acc:0.294]
Epoch [3/120    avg_loss:2.555, val_acc:0.319]
Epoch [4/120    avg_loss:2.516, val_acc:0.331]
Epoch [5/120    avg_loss:2.479, val_acc:0.343]
Epoch [6/120    avg_loss:2.434, val_acc:0.361]
Epoch [7/120    avg_loss:2.400, val_acc:0.393]
Epoch [8/120    avg_loss:2.361, val_acc:0.411]
Epoch [9/120    avg_loss:2.326, val_acc:0.409]
Epoch [10/120    avg_loss:2.261, val_acc:0.427]
Epoch [11/120    avg_loss:2.225, val_acc:0.438]
Epoch [12/120    avg_loss:2.169, val_acc:0.435]
Epoch [13/120    avg_loss:2.103, val_acc:0.458]
Epoch [14/120    avg_loss:2.054, val_acc:0.498]
Epoch [15/120    avg_loss:1.983, val_acc:0.538]
Epoch [16/120    avg_loss:1.910, val_acc:0.558]
Epoch [17/120    avg_loss:1.822, val_acc:0.627]
Epoch [18/120    avg_loss:1.728, val_acc:0.688]
Epoch [19/120    avg_loss:1.623, val_acc:0.710]
Epoch [20/120    avg_loss:1.521, val_acc:0.808]
Epoch [21/120    avg_loss:1.426, val_acc:0.831]
Epoch [22/120    avg_loss:1.302, val_acc:0.821]
Epoch [23/120    avg_loss:1.241, val_acc:0.819]
Epoch [24/120    avg_loss:1.131, val_acc:0.875]
Epoch [25/120    avg_loss:1.067, val_acc:0.907]
Epoch [26/120    avg_loss:0.991, val_acc:0.889]
Epoch [27/120    avg_loss:0.901, val_acc:0.899]
Epoch [28/120    avg_loss:0.846, val_acc:0.875]
Epoch [29/120    avg_loss:0.736, val_acc:0.893]
Epoch [30/120    avg_loss:0.664, val_acc:0.919]
Epoch [31/120    avg_loss:0.634, val_acc:0.893]
Epoch [32/120    avg_loss:0.588, val_acc:0.929]
Epoch [33/120    avg_loss:0.556, val_acc:0.919]
Epoch [34/120    avg_loss:0.538, val_acc:0.919]
Epoch [35/120    avg_loss:0.532, val_acc:0.935]
Epoch [36/120    avg_loss:0.467, val_acc:0.917]
Epoch [37/120    avg_loss:0.455, val_acc:0.935]
Epoch [38/120    avg_loss:0.414, val_acc:0.946]
Epoch [39/120    avg_loss:0.474, val_acc:0.938]
Epoch [40/120    avg_loss:0.388, val_acc:0.958]
Epoch [41/120    avg_loss:0.407, val_acc:0.929]
Epoch [42/120    avg_loss:0.400, val_acc:0.940]
Epoch [43/120    avg_loss:0.365, val_acc:0.925]
Epoch [44/120    avg_loss:0.388, val_acc:0.954]
Epoch [45/120    avg_loss:0.359, val_acc:0.956]
Epoch [46/120    avg_loss:0.346, val_acc:0.938]
Epoch [47/120    avg_loss:0.319, val_acc:0.927]
Epoch [48/120    avg_loss:0.319, val_acc:0.946]
Epoch [49/120    avg_loss:0.299, val_acc:0.964]
Epoch [50/120    avg_loss:0.300, val_acc:0.935]
Epoch [51/120    avg_loss:0.275, val_acc:0.940]
Epoch [52/120    avg_loss:0.270, val_acc:0.933]
Epoch [53/120    avg_loss:0.231, val_acc:0.954]
Epoch [54/120    avg_loss:0.259, val_acc:0.966]
Epoch [55/120    avg_loss:0.259, val_acc:0.964]
Epoch [56/120    avg_loss:0.270, val_acc:0.952]
Epoch [57/120    avg_loss:0.224, val_acc:0.958]
Epoch [58/120    avg_loss:0.289, val_acc:0.954]
Epoch [59/120    avg_loss:0.222, val_acc:0.954]
Epoch [60/120    avg_loss:0.209, val_acc:0.954]
Epoch [61/120    avg_loss:0.213, val_acc:0.970]
Epoch [62/120    avg_loss:0.244, val_acc:0.948]
Epoch [63/120    avg_loss:0.230, val_acc:0.964]
Epoch [64/120    avg_loss:0.213, val_acc:0.966]
Epoch [65/120    avg_loss:0.260, val_acc:0.940]
Epoch [66/120    avg_loss:0.321, val_acc:0.935]
Epoch [67/120    avg_loss:0.284, val_acc:0.956]
Epoch [68/120    avg_loss:0.209, val_acc:0.960]
Epoch [69/120    avg_loss:0.167, val_acc:0.964]
Epoch [70/120    avg_loss:0.240, val_acc:0.907]
Epoch [71/120    avg_loss:0.193, val_acc:0.915]
Epoch [72/120    avg_loss:0.162, val_acc:0.968]
Epoch [73/120    avg_loss:0.144, val_acc:0.972]
Epoch [74/120    avg_loss:0.175, val_acc:0.948]
Epoch [75/120    avg_loss:0.193, val_acc:0.974]
Epoch [76/120    avg_loss:0.211, val_acc:0.931]
Epoch [77/120    avg_loss:0.151, val_acc:0.952]
Epoch [78/120    avg_loss:0.171, val_acc:0.966]
Epoch [79/120    avg_loss:0.171, val_acc:0.962]
Epoch [80/120    avg_loss:0.161, val_acc:0.978]
Epoch [81/120    avg_loss:0.114, val_acc:0.974]
Epoch [82/120    avg_loss:0.119, val_acc:0.964]
Epoch [83/120    avg_loss:0.100, val_acc:0.974]
Epoch [84/120    avg_loss:0.101, val_acc:0.972]
Epoch [85/120    avg_loss:0.095, val_acc:0.980]
Epoch [86/120    avg_loss:0.138, val_acc:0.954]
Epoch [87/120    avg_loss:0.122, val_acc:0.954]
Epoch [88/120    avg_loss:0.179, val_acc:0.923]
Epoch [89/120    avg_loss:0.132, val_acc:0.968]
Epoch [90/120    avg_loss:0.082, val_acc:0.972]
Epoch [91/120    avg_loss:0.084, val_acc:0.978]
Epoch [92/120    avg_loss:0.091, val_acc:0.984]
Epoch [93/120    avg_loss:0.100, val_acc:0.962]
Epoch [94/120    avg_loss:0.092, val_acc:0.954]
Epoch [95/120    avg_loss:0.087, val_acc:0.982]
Epoch [96/120    avg_loss:0.090, val_acc:0.970]
Epoch [97/120    avg_loss:0.123, val_acc:0.970]
Epoch [98/120    avg_loss:0.081, val_acc:0.984]
Epoch [99/120    avg_loss:0.086, val_acc:0.968]
Epoch [100/120    avg_loss:0.102, val_acc:0.976]
Epoch [101/120    avg_loss:0.089, val_acc:0.982]
Epoch [102/120    avg_loss:0.083, val_acc:0.976]
Epoch [103/120    avg_loss:0.057, val_acc:0.980]
Epoch [104/120    avg_loss:0.073, val_acc:0.980]
Epoch [105/120    avg_loss:0.070, val_acc:0.984]
Epoch [106/120    avg_loss:0.049, val_acc:0.978]
Epoch [107/120    avg_loss:0.077, val_acc:0.972]
Epoch [108/120    avg_loss:0.113, val_acc:0.982]
Epoch [109/120    avg_loss:0.064, val_acc:0.970]
Epoch [110/120    avg_loss:0.079, val_acc:0.968]
Epoch [111/120    avg_loss:0.211, val_acc:0.966]
Epoch [112/120    avg_loss:0.158, val_acc:0.964]
Epoch [113/120    avg_loss:0.108, val_acc:0.970]
Epoch [114/120    avg_loss:0.083, val_acc:0.974]
Epoch [115/120    avg_loss:0.075, val_acc:0.982]
Epoch [116/120    avg_loss:0.062, val_acc:0.984]
Epoch [117/120    avg_loss:0.070, val_acc:0.952]
Epoch [118/120    avg_loss:0.067, val_acc:0.978]
Epoch [119/120    avg_loss:0.062, val_acc:0.984]
Epoch [120/120    avg_loss:0.049, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 219   8   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 200  26   1   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 1.         0.95216401 0.97550111 0.89285714 0.87128713
 0.99757869 0.88770053 1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9829087705520667
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99feef0e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.647, val_acc:0.069]
Epoch [2/120    avg_loss:2.590, val_acc:0.246]
Epoch [3/120    avg_loss:2.533, val_acc:0.284]
Epoch [4/120    avg_loss:2.491, val_acc:0.343]
Epoch [5/120    avg_loss:2.437, val_acc:0.347]
Epoch [6/120    avg_loss:2.400, val_acc:0.379]
Epoch [7/120    avg_loss:2.342, val_acc:0.413]
Epoch [8/120    avg_loss:2.290, val_acc:0.480]
Epoch [9/120    avg_loss:2.230, val_acc:0.565]
Epoch [10/120    avg_loss:2.156, val_acc:0.615]
Epoch [11/120    avg_loss:2.085, val_acc:0.708]
Epoch [12/120    avg_loss:2.015, val_acc:0.714]
Epoch [13/120    avg_loss:1.921, val_acc:0.726]
Epoch [14/120    avg_loss:1.837, val_acc:0.742]
Epoch [15/120    avg_loss:1.743, val_acc:0.706]
Epoch [16/120    avg_loss:1.655, val_acc:0.768]
Epoch [17/120    avg_loss:1.566, val_acc:0.823]
Epoch [18/120    avg_loss:1.439, val_acc:0.825]
Epoch [19/120    avg_loss:1.338, val_acc:0.837]
Epoch [20/120    avg_loss:1.283, val_acc:0.812]
Epoch [21/120    avg_loss:1.172, val_acc:0.813]
Epoch [22/120    avg_loss:1.086, val_acc:0.817]
Epoch [23/120    avg_loss:1.026, val_acc:0.800]
Epoch [24/120    avg_loss:0.946, val_acc:0.742]
Epoch [25/120    avg_loss:0.893, val_acc:0.800]
Epoch [26/120    avg_loss:0.805, val_acc:0.800]
Epoch [27/120    avg_loss:0.757, val_acc:0.827]
Epoch [28/120    avg_loss:0.701, val_acc:0.825]
Epoch [29/120    avg_loss:0.690, val_acc:0.802]
Epoch [30/120    avg_loss:0.672, val_acc:0.841]
Epoch [31/120    avg_loss:0.633, val_acc:0.901]
Epoch [32/120    avg_loss:0.574, val_acc:0.903]
Epoch [33/120    avg_loss:0.551, val_acc:0.913]
Epoch [34/120    avg_loss:0.491, val_acc:0.915]
Epoch [35/120    avg_loss:0.499, val_acc:0.913]
Epoch [36/120    avg_loss:0.452, val_acc:0.909]
Epoch [37/120    avg_loss:0.435, val_acc:0.911]
Epoch [38/120    avg_loss:0.451, val_acc:0.905]
Epoch [39/120    avg_loss:0.388, val_acc:0.935]
Epoch [40/120    avg_loss:0.386, val_acc:0.927]
Epoch [41/120    avg_loss:0.398, val_acc:0.935]
Epoch [42/120    avg_loss:0.357, val_acc:0.931]
Epoch [43/120    avg_loss:0.340, val_acc:0.937]
Epoch [44/120    avg_loss:0.357, val_acc:0.933]
Epoch [45/120    avg_loss:0.365, val_acc:0.952]
Epoch [46/120    avg_loss:0.319, val_acc:0.958]
Epoch [47/120    avg_loss:0.322, val_acc:0.954]
Epoch [48/120    avg_loss:0.336, val_acc:0.937]
Epoch [49/120    avg_loss:0.275, val_acc:0.940]
Epoch [50/120    avg_loss:0.308, val_acc:0.946]
Epoch [51/120    avg_loss:0.274, val_acc:0.919]
Epoch [52/120    avg_loss:0.306, val_acc:0.944]
Epoch [53/120    avg_loss:0.270, val_acc:0.954]
Epoch [54/120    avg_loss:0.248, val_acc:0.956]
Epoch [55/120    avg_loss:0.268, val_acc:0.946]
Epoch [56/120    avg_loss:0.250, val_acc:0.938]
Epoch [57/120    avg_loss:0.283, val_acc:0.935]
Epoch [58/120    avg_loss:0.263, val_acc:0.950]
Epoch [59/120    avg_loss:0.203, val_acc:0.950]
Epoch [60/120    avg_loss:0.196, val_acc:0.960]
Epoch [61/120    avg_loss:0.162, val_acc:0.964]
Epoch [62/120    avg_loss:0.164, val_acc:0.966]
Epoch [63/120    avg_loss:0.168, val_acc:0.964]
Epoch [64/120    avg_loss:0.159, val_acc:0.968]
Epoch [65/120    avg_loss:0.156, val_acc:0.968]
Epoch [66/120    avg_loss:0.154, val_acc:0.968]
Epoch [67/120    avg_loss:0.166, val_acc:0.968]
Epoch [68/120    avg_loss:0.155, val_acc:0.970]
Epoch [69/120    avg_loss:0.149, val_acc:0.968]
Epoch [70/120    avg_loss:0.140, val_acc:0.968]
Epoch [71/120    avg_loss:0.160, val_acc:0.976]
Epoch [72/120    avg_loss:0.150, val_acc:0.972]
Epoch [73/120    avg_loss:0.136, val_acc:0.974]
Epoch [74/120    avg_loss:0.147, val_acc:0.972]
Epoch [75/120    avg_loss:0.135, val_acc:0.974]
Epoch [76/120    avg_loss:0.149, val_acc:0.976]
Epoch [77/120    avg_loss:0.144, val_acc:0.982]
Epoch [78/120    avg_loss:0.142, val_acc:0.984]
Epoch [79/120    avg_loss:0.130, val_acc:0.980]
Epoch [80/120    avg_loss:0.137, val_acc:0.978]
Epoch [81/120    avg_loss:0.140, val_acc:0.980]
Epoch [82/120    avg_loss:0.129, val_acc:0.982]
Epoch [83/120    avg_loss:0.138, val_acc:0.984]
Epoch [84/120    avg_loss:0.132, val_acc:0.982]
Epoch [85/120    avg_loss:0.132, val_acc:0.982]
Epoch [86/120    avg_loss:0.142, val_acc:0.986]
Epoch [87/120    avg_loss:0.127, val_acc:0.988]
Epoch [88/120    avg_loss:0.130, val_acc:0.980]
Epoch [89/120    avg_loss:0.128, val_acc:0.986]
Epoch [90/120    avg_loss:0.132, val_acc:0.986]
Epoch [91/120    avg_loss:0.122, val_acc:0.990]
Epoch [92/120    avg_loss:0.126, val_acc:0.986]
Epoch [93/120    avg_loss:0.129, val_acc:0.980]
Epoch [94/120    avg_loss:0.123, val_acc:0.980]
Epoch [95/120    avg_loss:0.123, val_acc:0.988]
Epoch [96/120    avg_loss:0.137, val_acc:0.988]
Epoch [97/120    avg_loss:0.121, val_acc:0.980]
Epoch [98/120    avg_loss:0.134, val_acc:0.988]
Epoch [99/120    avg_loss:0.120, val_acc:0.992]
Epoch [100/120    avg_loss:0.130, val_acc:0.984]
Epoch [101/120    avg_loss:0.133, val_acc:0.992]
Epoch [102/120    avg_loss:0.123, val_acc:0.996]
Epoch [103/120    avg_loss:0.134, val_acc:0.994]
Epoch [104/120    avg_loss:0.125, val_acc:0.992]
Epoch [105/120    avg_loss:0.133, val_acc:0.994]
Epoch [106/120    avg_loss:0.119, val_acc:0.990]
Epoch [107/120    avg_loss:0.124, val_acc:0.992]
Epoch [108/120    avg_loss:0.112, val_acc:0.990]
Epoch [109/120    avg_loss:0.128, val_acc:0.992]
Epoch [110/120    avg_loss:0.113, val_acc:0.992]
Epoch [111/120    avg_loss:0.114, val_acc:0.988]
Epoch [112/120    avg_loss:0.111, val_acc:0.990]
Epoch [113/120    avg_loss:0.113, val_acc:0.992]
Epoch [114/120    avg_loss:0.116, val_acc:0.990]
Epoch [115/120    avg_loss:0.122, val_acc:0.990]
Epoch [116/120    avg_loss:0.117, val_acc:0.990]
Epoch [117/120    avg_loss:0.111, val_acc:0.994]
Epoch [118/120    avg_loss:0.102, val_acc:0.994]
Epoch [119/120    avg_loss:0.113, val_acc:0.992]
Epoch [120/120    avg_loss:0.105, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 227   0   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 200  27   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.95945946 0.99343545 0.92807425 0.90095847
 1.         0.9010989  1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9876564280006551
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa26b028f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.613, val_acc:0.137]
Epoch [2/120    avg_loss:2.564, val_acc:0.175]
Epoch [3/120    avg_loss:2.513, val_acc:0.349]
Epoch [4/120    avg_loss:2.466, val_acc:0.448]
Epoch [5/120    avg_loss:2.422, val_acc:0.468]
Epoch [6/120    avg_loss:2.376, val_acc:0.464]
Epoch [7/120    avg_loss:2.320, val_acc:0.488]
Epoch [8/120    avg_loss:2.263, val_acc:0.500]
Epoch [9/120    avg_loss:2.188, val_acc:0.536]
Epoch [10/120    avg_loss:2.132, val_acc:0.565]
Epoch [11/120    avg_loss:2.053, val_acc:0.677]
Epoch [12/120    avg_loss:1.987, val_acc:0.688]
Epoch [13/120    avg_loss:1.913, val_acc:0.704]
Epoch [14/120    avg_loss:1.832, val_acc:0.704]
Epoch [15/120    avg_loss:1.767, val_acc:0.768]
Epoch [16/120    avg_loss:1.671, val_acc:0.829]
Epoch [17/120    avg_loss:1.572, val_acc:0.847]
Epoch [18/120    avg_loss:1.469, val_acc:0.812]
Epoch [19/120    avg_loss:1.367, val_acc:0.879]
Epoch [20/120    avg_loss:1.269, val_acc:0.877]
Epoch [21/120    avg_loss:1.141, val_acc:0.885]
Epoch [22/120    avg_loss:1.049, val_acc:0.855]
Epoch [23/120    avg_loss:1.049, val_acc:0.853]
Epoch [24/120    avg_loss:0.902, val_acc:0.903]
Epoch [25/120    avg_loss:0.819, val_acc:0.913]
Epoch [26/120    avg_loss:0.774, val_acc:0.901]
Epoch [27/120    avg_loss:0.712, val_acc:0.919]
Epoch [28/120    avg_loss:0.646, val_acc:0.913]
Epoch [29/120    avg_loss:0.591, val_acc:0.923]
Epoch [30/120    avg_loss:0.643, val_acc:0.875]
Epoch [31/120    avg_loss:0.654, val_acc:0.909]
Epoch [32/120    avg_loss:0.583, val_acc:0.893]
Epoch [33/120    avg_loss:0.521, val_acc:0.927]
Epoch [34/120    avg_loss:0.514, val_acc:0.907]
Epoch [35/120    avg_loss:0.510, val_acc:0.921]
Epoch [36/120    avg_loss:0.547, val_acc:0.909]
Epoch [37/120    avg_loss:0.465, val_acc:0.915]
Epoch [38/120    avg_loss:0.419, val_acc:0.933]
Epoch [39/120    avg_loss:0.409, val_acc:0.935]
Epoch [40/120    avg_loss:0.386, val_acc:0.948]
Epoch [41/120    avg_loss:0.363, val_acc:0.935]
Epoch [42/120    avg_loss:0.382, val_acc:0.929]
Epoch [43/120    avg_loss:0.356, val_acc:0.929]
Epoch [44/120    avg_loss:0.356, val_acc:0.938]
Epoch [45/120    avg_loss:0.383, val_acc:0.933]
Epoch [46/120    avg_loss:0.334, val_acc:0.952]
Epoch [47/120    avg_loss:0.305, val_acc:0.935]
Epoch [48/120    avg_loss:0.277, val_acc:0.946]
Epoch [49/120    avg_loss:0.266, val_acc:0.931]
Epoch [50/120    avg_loss:0.298, val_acc:0.935]
Epoch [51/120    avg_loss:0.320, val_acc:0.933]
Epoch [52/120    avg_loss:0.271, val_acc:0.948]
Epoch [53/120    avg_loss:0.315, val_acc:0.946]
Epoch [54/120    avg_loss:0.244, val_acc:0.952]
Epoch [55/120    avg_loss:0.272, val_acc:0.946]
Epoch [56/120    avg_loss:0.222, val_acc:0.940]
Epoch [57/120    avg_loss:0.224, val_acc:0.931]
Epoch [58/120    avg_loss:0.219, val_acc:0.933]
Epoch [59/120    avg_loss:0.232, val_acc:0.942]
Epoch [60/120    avg_loss:0.203, val_acc:0.944]
Epoch [61/120    avg_loss:0.214, val_acc:0.960]
Epoch [62/120    avg_loss:0.185, val_acc:0.954]
Epoch [63/120    avg_loss:0.229, val_acc:0.952]
Epoch [64/120    avg_loss:0.223, val_acc:0.913]
Epoch [65/120    avg_loss:0.236, val_acc:0.946]
Epoch [66/120    avg_loss:0.229, val_acc:0.950]
Epoch [67/120    avg_loss:0.208, val_acc:0.946]
Epoch [68/120    avg_loss:0.194, val_acc:0.964]
Epoch [69/120    avg_loss:0.204, val_acc:0.960]
Epoch [70/120    avg_loss:0.284, val_acc:0.958]
Epoch [71/120    avg_loss:0.162, val_acc:0.950]
Epoch [72/120    avg_loss:0.135, val_acc:0.972]
Epoch [73/120    avg_loss:0.113, val_acc:0.968]
Epoch [74/120    avg_loss:0.124, val_acc:0.976]
Epoch [75/120    avg_loss:0.188, val_acc:0.950]
Epoch [76/120    avg_loss:0.150, val_acc:0.966]
Epoch [77/120    avg_loss:0.129, val_acc:0.968]
Epoch [78/120    avg_loss:0.129, val_acc:0.970]
Epoch [79/120    avg_loss:0.114, val_acc:0.970]
Epoch [80/120    avg_loss:0.112, val_acc:0.964]
Epoch [81/120    avg_loss:0.096, val_acc:0.982]
Epoch [82/120    avg_loss:0.118, val_acc:0.964]
Epoch [83/120    avg_loss:0.106, val_acc:0.974]
Epoch [84/120    avg_loss:0.102, val_acc:0.974]
Epoch [85/120    avg_loss:0.098, val_acc:0.970]
Epoch [86/120    avg_loss:0.087, val_acc:0.954]
Epoch [87/120    avg_loss:0.114, val_acc:0.972]
Epoch [88/120    avg_loss:0.138, val_acc:0.958]
Epoch [89/120    avg_loss:0.093, val_acc:0.980]
Epoch [90/120    avg_loss:0.106, val_acc:0.984]
Epoch [91/120    avg_loss:0.092, val_acc:0.972]
Epoch [92/120    avg_loss:0.080, val_acc:0.962]
Epoch [93/120    avg_loss:0.075, val_acc:0.982]
Epoch [94/120    avg_loss:0.074, val_acc:0.980]
Epoch [95/120    avg_loss:0.164, val_acc:0.950]
Epoch [96/120    avg_loss:0.208, val_acc:0.972]
Epoch [97/120    avg_loss:0.143, val_acc:0.966]
Epoch [98/120    avg_loss:0.079, val_acc:0.976]
Epoch [99/120    avg_loss:0.065, val_acc:0.980]
Epoch [100/120    avg_loss:0.064, val_acc:0.986]
Epoch [101/120    avg_loss:0.057, val_acc:0.982]
Epoch [102/120    avg_loss:0.066, val_acc:0.976]
Epoch [103/120    avg_loss:0.064, val_acc:0.980]
Epoch [104/120    avg_loss:0.063, val_acc:0.978]
Epoch [105/120    avg_loss:0.084, val_acc:0.962]
Epoch [106/120    avg_loss:0.186, val_acc:0.968]
Epoch [107/120    avg_loss:0.189, val_acc:0.952]
Epoch [108/120    avg_loss:0.121, val_acc:0.970]
Epoch [109/120    avg_loss:0.089, val_acc:0.988]
Epoch [110/120    avg_loss:0.050, val_acc:0.982]
Epoch [111/120    avg_loss:0.048, val_acc:0.978]
Epoch [112/120    avg_loss:0.060, val_acc:0.978]
Epoch [113/120    avg_loss:0.035, val_acc:0.984]
Epoch [114/120    avg_loss:0.061, val_acc:0.974]
Epoch [115/120    avg_loss:0.042, val_acc:0.986]
Epoch [116/120    avg_loss:0.048, val_acc:0.976]
Epoch [117/120    avg_loss:0.031, val_acc:0.984]
Epoch [118/120    avg_loss:0.039, val_acc:0.984]
Epoch [119/120    avg_loss:0.042, val_acc:0.984]
Epoch [120/120    avg_loss:0.043, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   5   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   1   0   0   1   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.99780862 0.96688742 0.98678414 0.92783505 0.89473684
 0.99273608 0.9132948  0.99741602 0.99893276 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9869418118123474
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68b5ed6e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.292]
Epoch [2/120    avg_loss:2.554, val_acc:0.331]
Epoch [3/120    avg_loss:2.509, val_acc:0.333]
Epoch [4/120    avg_loss:2.467, val_acc:0.343]
Epoch [5/120    avg_loss:2.425, val_acc:0.353]
Epoch [6/120    avg_loss:2.379, val_acc:0.353]
Epoch [7/120    avg_loss:2.338, val_acc:0.351]
Epoch [8/120    avg_loss:2.285, val_acc:0.345]
Epoch [9/120    avg_loss:2.234, val_acc:0.331]
Epoch [10/120    avg_loss:2.169, val_acc:0.361]
Epoch [11/120    avg_loss:2.116, val_acc:0.411]
Epoch [12/120    avg_loss:2.055, val_acc:0.458]
Epoch [13/120    avg_loss:1.991, val_acc:0.490]
Epoch [14/120    avg_loss:1.917, val_acc:0.554]
Epoch [15/120    avg_loss:1.858, val_acc:0.552]
Epoch [16/120    avg_loss:1.787, val_acc:0.609]
Epoch [17/120    avg_loss:1.685, val_acc:0.655]
Epoch [18/120    avg_loss:1.653, val_acc:0.663]
Epoch [19/120    avg_loss:1.545, val_acc:0.679]
Epoch [20/120    avg_loss:1.440, val_acc:0.728]
Epoch [21/120    avg_loss:1.355, val_acc:0.726]
Epoch [22/120    avg_loss:1.252, val_acc:0.754]
Epoch [23/120    avg_loss:1.167, val_acc:0.752]
Epoch [24/120    avg_loss:1.065, val_acc:0.885]
Epoch [25/120    avg_loss:0.994, val_acc:0.907]
Epoch [26/120    avg_loss:0.976, val_acc:0.841]
Epoch [27/120    avg_loss:0.872, val_acc:0.893]
Epoch [28/120    avg_loss:0.906, val_acc:0.917]
Epoch [29/120    avg_loss:0.796, val_acc:0.883]
Epoch [30/120    avg_loss:0.741, val_acc:0.887]
Epoch [31/120    avg_loss:0.679, val_acc:0.905]
Epoch [32/120    avg_loss:0.628, val_acc:0.931]
Epoch [33/120    avg_loss:0.623, val_acc:0.885]
Epoch [34/120    avg_loss:0.621, val_acc:0.911]
Epoch [35/120    avg_loss:0.583, val_acc:0.893]
Epoch [36/120    avg_loss:0.575, val_acc:0.921]
Epoch [37/120    avg_loss:0.511, val_acc:0.946]
Epoch [38/120    avg_loss:0.452, val_acc:0.966]
Epoch [39/120    avg_loss:0.440, val_acc:0.935]
Epoch [40/120    avg_loss:0.523, val_acc:0.927]
Epoch [41/120    avg_loss:0.479, val_acc:0.935]
Epoch [42/120    avg_loss:0.405, val_acc:0.935]
Epoch [43/120    avg_loss:0.407, val_acc:0.954]
Epoch [44/120    avg_loss:0.396, val_acc:0.937]
Epoch [45/120    avg_loss:0.362, val_acc:0.958]
Epoch [46/120    avg_loss:0.343, val_acc:0.960]
Epoch [47/120    avg_loss:0.373, val_acc:0.972]
Epoch [48/120    avg_loss:0.357, val_acc:0.968]
Epoch [49/120    avg_loss:0.316, val_acc:0.966]
Epoch [50/120    avg_loss:0.299, val_acc:0.972]
Epoch [51/120    avg_loss:0.275, val_acc:0.942]
Epoch [52/120    avg_loss:0.312, val_acc:0.972]
Epoch [53/120    avg_loss:0.269, val_acc:0.970]
Epoch [54/120    avg_loss:0.244, val_acc:0.950]
Epoch [55/120    avg_loss:0.290, val_acc:0.948]
Epoch [56/120    avg_loss:0.298, val_acc:0.937]
Epoch [57/120    avg_loss:0.276, val_acc:0.954]
Epoch [58/120    avg_loss:0.246, val_acc:0.972]
Epoch [59/120    avg_loss:0.270, val_acc:0.940]
Epoch [60/120    avg_loss:0.315, val_acc:0.966]
Epoch [61/120    avg_loss:0.254, val_acc:0.962]
Epoch [62/120    avg_loss:0.217, val_acc:0.964]
Epoch [63/120    avg_loss:0.188, val_acc:0.960]
Epoch [64/120    avg_loss:0.226, val_acc:0.974]
Epoch [65/120    avg_loss:0.224, val_acc:0.980]
Epoch [66/120    avg_loss:0.173, val_acc:0.970]
Epoch [67/120    avg_loss:0.253, val_acc:0.948]
Epoch [68/120    avg_loss:0.193, val_acc:0.970]
Epoch [69/120    avg_loss:0.172, val_acc:0.968]
Epoch [70/120    avg_loss:0.160, val_acc:0.976]
Epoch [71/120    avg_loss:0.179, val_acc:0.933]
Epoch [72/120    avg_loss:0.169, val_acc:0.970]
Epoch [73/120    avg_loss:0.137, val_acc:0.980]
Epoch [74/120    avg_loss:0.137, val_acc:0.976]
Epoch [75/120    avg_loss:0.136, val_acc:0.972]
Epoch [76/120    avg_loss:0.148, val_acc:0.964]
Epoch [77/120    avg_loss:0.154, val_acc:0.962]
Epoch [78/120    avg_loss:0.174, val_acc:0.980]
Epoch [79/120    avg_loss:0.131, val_acc:0.960]
Epoch [80/120    avg_loss:0.123, val_acc:0.982]
Epoch [81/120    avg_loss:0.120, val_acc:0.982]
Epoch [82/120    avg_loss:0.121, val_acc:0.982]
Epoch [83/120    avg_loss:0.105, val_acc:0.982]
Epoch [84/120    avg_loss:0.112, val_acc:0.986]
Epoch [85/120    avg_loss:0.125, val_acc:0.964]
Epoch [86/120    avg_loss:0.128, val_acc:0.982]
Epoch [87/120    avg_loss:0.106, val_acc:0.978]
Epoch [88/120    avg_loss:0.103, val_acc:0.992]
Epoch [89/120    avg_loss:0.098, val_acc:0.984]
Epoch [90/120    avg_loss:0.102, val_acc:0.984]
Epoch [91/120    avg_loss:0.098, val_acc:0.946]
Epoch [92/120    avg_loss:0.146, val_acc:0.966]
Epoch [93/120    avg_loss:0.084, val_acc:0.986]
Epoch [94/120    avg_loss:0.072, val_acc:0.986]
Epoch [95/120    avg_loss:0.089, val_acc:0.958]
Epoch [96/120    avg_loss:0.085, val_acc:0.978]
Epoch [97/120    avg_loss:0.112, val_acc:0.970]
Epoch [98/120    avg_loss:0.159, val_acc:0.962]
Epoch [99/120    avg_loss:0.120, val_acc:0.982]
Epoch [100/120    avg_loss:0.092, val_acc:0.984]
Epoch [101/120    avg_loss:0.091, val_acc:0.988]
Epoch [102/120    avg_loss:0.071, val_acc:0.990]
Epoch [103/120    avg_loss:0.061, val_acc:0.990]
Epoch [104/120    avg_loss:0.057, val_acc:0.990]
Epoch [105/120    avg_loss:0.057, val_acc:0.988]
Epoch [106/120    avg_loss:0.060, val_acc:0.988]
Epoch [107/120    avg_loss:0.056, val_acc:0.988]
Epoch [108/120    avg_loss:0.049, val_acc:0.988]
Epoch [109/120    avg_loss:0.051, val_acc:0.988]
Epoch [110/120    avg_loss:0.050, val_acc:0.990]
Epoch [111/120    avg_loss:0.057, val_acc:0.988]
Epoch [112/120    avg_loss:0.048, val_acc:0.990]
Epoch [113/120    avg_loss:0.044, val_acc:0.988]
Epoch [114/120    avg_loss:0.051, val_acc:0.986]
Epoch [115/120    avg_loss:0.048, val_acc:0.986]
Epoch [116/120    avg_loss:0.044, val_acc:0.986]
Epoch [117/120    avg_loss:0.043, val_acc:0.986]
Epoch [118/120    avg_loss:0.042, val_acc:0.986]
Epoch [119/120    avg_loss:0.042, val_acc:0.986]
Epoch [120/120    avg_loss:0.052, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 225   3   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.97550111 0.98901099 0.91796009 0.89261745
 0.99512195 0.94382022 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9883678059174414
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd271f98e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.637, val_acc:0.050]
Epoch [2/120    avg_loss:2.578, val_acc:0.143]
Epoch [3/120    avg_loss:2.530, val_acc:0.349]
Epoch [4/120    avg_loss:2.475, val_acc:0.369]
Epoch [5/120    avg_loss:2.424, val_acc:0.371]
Epoch [6/120    avg_loss:2.380, val_acc:0.355]
Epoch [7/120    avg_loss:2.339, val_acc:0.347]
Epoch [8/120    avg_loss:2.281, val_acc:0.365]
Epoch [9/120    avg_loss:2.242, val_acc:0.357]
Epoch [10/120    avg_loss:2.179, val_acc:0.377]
Epoch [11/120    avg_loss:2.121, val_acc:0.379]
Epoch [12/120    avg_loss:2.051, val_acc:0.381]
Epoch [13/120    avg_loss:2.010, val_acc:0.425]
Epoch [14/120    avg_loss:1.949, val_acc:0.472]
Epoch [15/120    avg_loss:1.884, val_acc:0.538]
Epoch [16/120    avg_loss:1.827, val_acc:0.615]
Epoch [17/120    avg_loss:1.757, val_acc:0.637]
Epoch [18/120    avg_loss:1.670, val_acc:0.663]
Epoch [19/120    avg_loss:1.581, val_acc:0.663]
Epoch [20/120    avg_loss:1.503, val_acc:0.681]
Epoch [21/120    avg_loss:1.414, val_acc:0.706]
Epoch [22/120    avg_loss:1.337, val_acc:0.687]
Epoch [23/120    avg_loss:1.253, val_acc:0.726]
Epoch [24/120    avg_loss:1.161, val_acc:0.710]
Epoch [25/120    avg_loss:1.082, val_acc:0.829]
Epoch [26/120    avg_loss:1.010, val_acc:0.841]
Epoch [27/120    avg_loss:0.931, val_acc:0.879]
Epoch [28/120    avg_loss:0.851, val_acc:0.881]
Epoch [29/120    avg_loss:0.780, val_acc:0.899]
Epoch [30/120    avg_loss:0.767, val_acc:0.893]
Epoch [31/120    avg_loss:0.716, val_acc:0.909]
Epoch [32/120    avg_loss:0.666, val_acc:0.895]
Epoch [33/120    avg_loss:0.634, val_acc:0.889]
Epoch [34/120    avg_loss:0.596, val_acc:0.895]
Epoch [35/120    avg_loss:0.486, val_acc:0.917]
Epoch [36/120    avg_loss:0.469, val_acc:0.919]
Epoch [37/120    avg_loss:0.470, val_acc:0.889]
Epoch [38/120    avg_loss:0.441, val_acc:0.923]
Epoch [39/120    avg_loss:0.410, val_acc:0.937]
Epoch [40/120    avg_loss:0.398, val_acc:0.925]
Epoch [41/120    avg_loss:0.392, val_acc:0.937]
Epoch [42/120    avg_loss:0.395, val_acc:0.925]
Epoch [43/120    avg_loss:0.341, val_acc:0.935]
Epoch [44/120    avg_loss:0.330, val_acc:0.944]
Epoch [45/120    avg_loss:0.328, val_acc:0.931]
Epoch [46/120    avg_loss:0.337, val_acc:0.923]
Epoch [47/120    avg_loss:0.307, val_acc:0.948]
Epoch [48/120    avg_loss:0.281, val_acc:0.940]
Epoch [49/120    avg_loss:0.272, val_acc:0.942]
Epoch [50/120    avg_loss:0.273, val_acc:0.952]
Epoch [51/120    avg_loss:0.226, val_acc:0.960]
Epoch [52/120    avg_loss:0.277, val_acc:0.921]
Epoch [53/120    avg_loss:0.341, val_acc:0.948]
Epoch [54/120    avg_loss:0.226, val_acc:0.954]
Epoch [55/120    avg_loss:0.235, val_acc:0.966]
Epoch [56/120    avg_loss:0.218, val_acc:0.964]
Epoch [57/120    avg_loss:0.214, val_acc:0.958]
Epoch [58/120    avg_loss:0.206, val_acc:0.962]
Epoch [59/120    avg_loss:0.189, val_acc:0.984]
Epoch [60/120    avg_loss:0.189, val_acc:0.976]
Epoch [61/120    avg_loss:0.169, val_acc:0.968]
Epoch [62/120    avg_loss:0.175, val_acc:0.962]
Epoch [63/120    avg_loss:0.162, val_acc:0.966]
Epoch [64/120    avg_loss:0.209, val_acc:0.970]
Epoch [65/120    avg_loss:0.160, val_acc:0.986]
Epoch [66/120    avg_loss:0.148, val_acc:0.988]
Epoch [67/120    avg_loss:0.167, val_acc:0.984]
Epoch [68/120    avg_loss:0.145, val_acc:0.992]
Epoch [69/120    avg_loss:0.120, val_acc:0.982]
Epoch [70/120    avg_loss:0.120, val_acc:0.990]
Epoch [71/120    avg_loss:0.162, val_acc:0.972]
Epoch [72/120    avg_loss:0.151, val_acc:0.970]
Epoch [73/120    avg_loss:0.140, val_acc:0.984]
Epoch [74/120    avg_loss:0.151, val_acc:0.978]
Epoch [75/120    avg_loss:0.145, val_acc:0.976]
Epoch [76/120    avg_loss:0.175, val_acc:0.972]
Epoch [77/120    avg_loss:0.139, val_acc:0.988]
Epoch [78/120    avg_loss:0.119, val_acc:0.984]
Epoch [79/120    avg_loss:0.099, val_acc:0.986]
Epoch [80/120    avg_loss:0.175, val_acc:0.966]
Epoch [81/120    avg_loss:0.169, val_acc:0.984]
Epoch [82/120    avg_loss:0.106, val_acc:0.988]
Epoch [83/120    avg_loss:0.087, val_acc:0.986]
Epoch [84/120    avg_loss:0.096, val_acc:0.986]
Epoch [85/120    avg_loss:0.093, val_acc:0.988]
Epoch [86/120    avg_loss:0.080, val_acc:0.988]
Epoch [87/120    avg_loss:0.077, val_acc:0.990]
Epoch [88/120    avg_loss:0.078, val_acc:0.988]
Epoch [89/120    avg_loss:0.077, val_acc:0.990]
Epoch [90/120    avg_loss:0.074, val_acc:0.990]
Epoch [91/120    avg_loss:0.069, val_acc:0.990]
Epoch [92/120    avg_loss:0.082, val_acc:0.990]
Epoch [93/120    avg_loss:0.073, val_acc:0.992]
Epoch [94/120    avg_loss:0.071, val_acc:0.990]
Epoch [95/120    avg_loss:0.070, val_acc:0.992]
Epoch [96/120    avg_loss:0.060, val_acc:0.990]
Epoch [97/120    avg_loss:0.067, val_acc:0.992]
Epoch [98/120    avg_loss:0.064, val_acc:0.990]
Epoch [99/120    avg_loss:0.079, val_acc:0.992]
Epoch [100/120    avg_loss:0.056, val_acc:0.992]
Epoch [101/120    avg_loss:0.070, val_acc:0.992]
Epoch [102/120    avg_loss:0.060, val_acc:0.992]
Epoch [103/120    avg_loss:0.064, val_acc:0.992]
Epoch [104/120    avg_loss:0.074, val_acc:0.990]
Epoch [105/120    avg_loss:0.058, val_acc:0.990]
Epoch [106/120    avg_loss:0.057, val_acc:0.990]
Epoch [107/120    avg_loss:0.056, val_acc:0.992]
Epoch [108/120    avg_loss:0.070, val_acc:0.992]
Epoch [109/120    avg_loss:0.074, val_acc:0.992]
Epoch [110/120    avg_loss:0.060, val_acc:0.992]
Epoch [111/120    avg_loss:0.060, val_acc:0.992]
Epoch [112/120    avg_loss:0.058, val_acc:0.992]
Epoch [113/120    avg_loss:0.059, val_acc:0.992]
Epoch [114/120    avg_loss:0.057, val_acc:0.992]
Epoch [115/120    avg_loss:0.061, val_acc:0.992]
Epoch [116/120    avg_loss:0.052, val_acc:0.992]
Epoch [117/120    avg_loss:0.050, val_acc:0.992]
Epoch [118/120    avg_loss:0.049, val_acc:0.992]
Epoch [119/120    avg_loss:0.063, val_acc:0.992]
Epoch [120/120    avg_loss:0.052, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.97550111 0.98678414 0.92477876 0.90604027
 1.         0.93785311 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9893174555105304
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83c2c5bda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.237]
Epoch [2/120    avg_loss:2.559, val_acc:0.371]
Epoch [3/120    avg_loss:2.508, val_acc:0.365]
Epoch [4/120    avg_loss:2.458, val_acc:0.346]
Epoch [5/120    avg_loss:2.411, val_acc:0.358]
Epoch [6/120    avg_loss:2.379, val_acc:0.379]
Epoch [7/120    avg_loss:2.338, val_acc:0.394]
Epoch [8/120    avg_loss:2.292, val_acc:0.400]
Epoch [9/120    avg_loss:2.258, val_acc:0.402]
Epoch [10/120    avg_loss:2.217, val_acc:0.400]
Epoch [11/120    avg_loss:2.181, val_acc:0.404]
Epoch [12/120    avg_loss:2.141, val_acc:0.404]
Epoch [13/120    avg_loss:2.083, val_acc:0.429]
Epoch [14/120    avg_loss:2.040, val_acc:0.450]
Epoch [15/120    avg_loss:1.964, val_acc:0.487]
Epoch [16/120    avg_loss:1.915, val_acc:0.481]
Epoch [17/120    avg_loss:1.846, val_acc:0.554]
Epoch [18/120    avg_loss:1.793, val_acc:0.560]
Epoch [19/120    avg_loss:1.707, val_acc:0.648]
Epoch [20/120    avg_loss:1.641, val_acc:0.652]
Epoch [21/120    avg_loss:1.560, val_acc:0.696]
Epoch [22/120    avg_loss:1.445, val_acc:0.744]
Epoch [23/120    avg_loss:1.381, val_acc:0.733]
Epoch [24/120    avg_loss:1.273, val_acc:0.821]
Epoch [25/120    avg_loss:1.167, val_acc:0.835]
Epoch [26/120    avg_loss:1.065, val_acc:0.833]
Epoch [27/120    avg_loss:0.982, val_acc:0.890]
Epoch [28/120    avg_loss:0.951, val_acc:0.883]
Epoch [29/120    avg_loss:0.876, val_acc:0.879]
Epoch [30/120    avg_loss:0.810, val_acc:0.879]
Epoch [31/120    avg_loss:0.757, val_acc:0.881]
Epoch [32/120    avg_loss:0.719, val_acc:0.894]
Epoch [33/120    avg_loss:0.667, val_acc:0.906]
Epoch [34/120    avg_loss:0.613, val_acc:0.896]
Epoch [35/120    avg_loss:0.602, val_acc:0.896]
Epoch [36/120    avg_loss:0.591, val_acc:0.898]
Epoch [37/120    avg_loss:0.572, val_acc:0.904]
Epoch [38/120    avg_loss:0.518, val_acc:0.871]
Epoch [39/120    avg_loss:0.547, val_acc:0.898]
Epoch [40/120    avg_loss:0.497, val_acc:0.915]
Epoch [41/120    avg_loss:0.481, val_acc:0.875]
Epoch [42/120    avg_loss:0.462, val_acc:0.915]
Epoch [43/120    avg_loss:0.404, val_acc:0.925]
Epoch [44/120    avg_loss:0.384, val_acc:0.923]
Epoch [45/120    avg_loss:0.366, val_acc:0.912]
Epoch [46/120    avg_loss:0.371, val_acc:0.927]
Epoch [47/120    avg_loss:0.361, val_acc:0.923]
Epoch [48/120    avg_loss:0.352, val_acc:0.933]
Epoch [49/120    avg_loss:0.332, val_acc:0.948]
Epoch [50/120    avg_loss:0.330, val_acc:0.942]
Epoch [51/120    avg_loss:0.302, val_acc:0.946]
Epoch [52/120    avg_loss:0.309, val_acc:0.912]
Epoch [53/120    avg_loss:0.292, val_acc:0.965]
Epoch [54/120    avg_loss:0.258, val_acc:0.910]
Epoch [55/120    avg_loss:0.268, val_acc:0.956]
Epoch [56/120    avg_loss:0.247, val_acc:0.952]
Epoch [57/120    avg_loss:0.230, val_acc:0.912]
Epoch [58/120    avg_loss:0.257, val_acc:0.942]
Epoch [59/120    avg_loss:0.257, val_acc:0.931]
Epoch [60/120    avg_loss:0.233, val_acc:0.967]
Epoch [61/120    avg_loss:0.211, val_acc:0.940]
Epoch [62/120    avg_loss:0.204, val_acc:0.969]
Epoch [63/120    avg_loss:0.212, val_acc:0.965]
Epoch [64/120    avg_loss:0.207, val_acc:0.971]
Epoch [65/120    avg_loss:0.195, val_acc:0.965]
Epoch [66/120    avg_loss:0.207, val_acc:0.960]
Epoch [67/120    avg_loss:0.163, val_acc:0.956]
Epoch [68/120    avg_loss:0.200, val_acc:0.963]
Epoch [69/120    avg_loss:0.192, val_acc:0.956]
Epoch [70/120    avg_loss:0.153, val_acc:0.967]
Epoch [71/120    avg_loss:0.151, val_acc:0.944]
Epoch [72/120    avg_loss:0.252, val_acc:0.958]
Epoch [73/120    avg_loss:0.187, val_acc:0.915]
Epoch [74/120    avg_loss:0.139, val_acc:0.967]
Epoch [75/120    avg_loss:0.121, val_acc:0.975]
Epoch [76/120    avg_loss:0.122, val_acc:0.967]
Epoch [77/120    avg_loss:0.126, val_acc:0.979]
Epoch [78/120    avg_loss:0.103, val_acc:0.958]
Epoch [79/120    avg_loss:0.118, val_acc:0.979]
Epoch [80/120    avg_loss:0.111, val_acc:0.988]
Epoch [81/120    avg_loss:0.110, val_acc:0.985]
Epoch [82/120    avg_loss:0.116, val_acc:0.971]
Epoch [83/120    avg_loss:0.100, val_acc:0.965]
Epoch [84/120    avg_loss:0.142, val_acc:0.981]
Epoch [85/120    avg_loss:0.170, val_acc:0.950]
Epoch [86/120    avg_loss:0.181, val_acc:0.960]
Epoch [87/120    avg_loss:0.139, val_acc:0.983]
Epoch [88/120    avg_loss:0.092, val_acc:0.977]
Epoch [89/120    avg_loss:0.089, val_acc:0.975]
Epoch [90/120    avg_loss:0.102, val_acc:0.965]
Epoch [91/120    avg_loss:0.087, val_acc:0.979]
Epoch [92/120    avg_loss:0.080, val_acc:0.981]
Epoch [93/120    avg_loss:0.076, val_acc:0.973]
Epoch [94/120    avg_loss:0.055, val_acc:0.988]
Epoch [95/120    avg_loss:0.050, val_acc:0.992]
Epoch [96/120    avg_loss:0.043, val_acc:0.994]
Epoch [97/120    avg_loss:0.041, val_acc:0.990]
Epoch [98/120    avg_loss:0.042, val_acc:0.994]
Epoch [99/120    avg_loss:0.044, val_acc:0.992]
Epoch [100/120    avg_loss:0.039, val_acc:0.990]
Epoch [101/120    avg_loss:0.037, val_acc:0.994]
Epoch [102/120    avg_loss:0.055, val_acc:0.990]
Epoch [103/120    avg_loss:0.040, val_acc:0.988]
Epoch [104/120    avg_loss:0.040, val_acc:0.992]
Epoch [105/120    avg_loss:0.043, val_acc:0.992]
Epoch [106/120    avg_loss:0.040, val_acc:0.992]
Epoch [107/120    avg_loss:0.038, val_acc:0.990]
Epoch [108/120    avg_loss:0.036, val_acc:0.992]
Epoch [109/120    avg_loss:0.043, val_acc:0.994]
Epoch [110/120    avg_loss:0.033, val_acc:0.992]
Epoch [111/120    avg_loss:0.038, val_acc:0.992]
Epoch [112/120    avg_loss:0.032, val_acc:0.992]
Epoch [113/120    avg_loss:0.037, val_acc:0.992]
Epoch [114/120    avg_loss:0.033, val_acc:0.992]
Epoch [115/120    avg_loss:0.038, val_acc:0.988]
Epoch [116/120    avg_loss:0.035, val_acc:0.992]
Epoch [117/120    avg_loss:0.034, val_acc:0.992]
Epoch [118/120    avg_loss:0.035, val_acc:0.992]
Epoch [119/120    avg_loss:0.034, val_acc:0.990]
Epoch [120/120    avg_loss:0.034, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 218  12   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.98648649 0.97321429 0.91428571 0.92105263
 0.99266504 0.96703297 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9893180351197497
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fadd92bce48>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.062]
Epoch [2/120    avg_loss:2.593, val_acc:0.242]
Epoch [3/120    avg_loss:2.537, val_acc:0.323]
Epoch [4/120    avg_loss:2.492, val_acc:0.319]
Epoch [5/120    avg_loss:2.449, val_acc:0.325]
Epoch [6/120    avg_loss:2.415, val_acc:0.335]
Epoch [7/120    avg_loss:2.364, val_acc:0.335]
Epoch [8/120    avg_loss:2.318, val_acc:0.369]
Epoch [9/120    avg_loss:2.268, val_acc:0.477]
Epoch [10/120    avg_loss:2.213, val_acc:0.548]
Epoch [11/120    avg_loss:2.153, val_acc:0.604]
Epoch [12/120    avg_loss:2.070, val_acc:0.658]
Epoch [13/120    avg_loss:2.001, val_acc:0.652]
Epoch [14/120    avg_loss:1.919, val_acc:0.742]
Epoch [15/120    avg_loss:1.845, val_acc:0.746]
Epoch [16/120    avg_loss:1.774, val_acc:0.738]
Epoch [17/120    avg_loss:1.696, val_acc:0.754]
Epoch [18/120    avg_loss:1.615, val_acc:0.762]
Epoch [19/120    avg_loss:1.524, val_acc:0.806]
Epoch [20/120    avg_loss:1.418, val_acc:0.838]
Epoch [21/120    avg_loss:1.327, val_acc:0.829]
Epoch [22/120    avg_loss:1.252, val_acc:0.806]
Epoch [23/120    avg_loss:1.155, val_acc:0.860]
Epoch [24/120    avg_loss:1.099, val_acc:0.871]
Epoch [25/120    avg_loss:0.986, val_acc:0.860]
Epoch [26/120    avg_loss:0.884, val_acc:0.883]
Epoch [27/120    avg_loss:0.868, val_acc:0.863]
Epoch [28/120    avg_loss:0.819, val_acc:0.900]
Epoch [29/120    avg_loss:0.723, val_acc:0.883]
Epoch [30/120    avg_loss:0.719, val_acc:0.910]
Epoch [31/120    avg_loss:0.630, val_acc:0.910]
Epoch [32/120    avg_loss:0.592, val_acc:0.898]
Epoch [33/120    avg_loss:0.617, val_acc:0.865]
Epoch [34/120    avg_loss:0.620, val_acc:0.887]
Epoch [35/120    avg_loss:0.602, val_acc:0.885]
Epoch [36/120    avg_loss:0.563, val_acc:0.923]
Epoch [37/120    avg_loss:0.488, val_acc:0.923]
Epoch [38/120    avg_loss:0.438, val_acc:0.944]
Epoch [39/120    avg_loss:0.426, val_acc:0.950]
Epoch [40/120    avg_loss:0.424, val_acc:0.906]
Epoch [41/120    avg_loss:0.460, val_acc:0.923]
Epoch [42/120    avg_loss:0.468, val_acc:0.852]
Epoch [43/120    avg_loss:0.473, val_acc:0.954]
Epoch [44/120    avg_loss:0.428, val_acc:0.948]
Epoch [45/120    avg_loss:0.398, val_acc:0.946]
Epoch [46/120    avg_loss:0.308, val_acc:0.969]
Epoch [47/120    avg_loss:0.293, val_acc:0.969]
Epoch [48/120    avg_loss:0.289, val_acc:0.967]
Epoch [49/120    avg_loss:0.325, val_acc:0.942]
Epoch [50/120    avg_loss:0.312, val_acc:0.954]
Epoch [51/120    avg_loss:0.259, val_acc:0.963]
Epoch [52/120    avg_loss:0.254, val_acc:0.950]
Epoch [53/120    avg_loss:0.291, val_acc:0.954]
Epoch [54/120    avg_loss:0.256, val_acc:0.965]
Epoch [55/120    avg_loss:0.244, val_acc:0.979]
Epoch [56/120    avg_loss:0.217, val_acc:0.983]
Epoch [57/120    avg_loss:0.231, val_acc:0.973]
Epoch [58/120    avg_loss:0.197, val_acc:0.967]
Epoch [59/120    avg_loss:0.213, val_acc:0.960]
Epoch [60/120    avg_loss:0.214, val_acc:0.981]
Epoch [61/120    avg_loss:0.200, val_acc:0.977]
Epoch [62/120    avg_loss:0.191, val_acc:0.985]
Epoch [63/120    avg_loss:0.177, val_acc:0.975]
Epoch [64/120    avg_loss:0.179, val_acc:0.942]
Epoch [65/120    avg_loss:0.196, val_acc:0.971]
Epoch [66/120    avg_loss:0.155, val_acc:0.983]
Epoch [67/120    avg_loss:0.157, val_acc:0.967]
Epoch [68/120    avg_loss:0.230, val_acc:0.971]
Epoch [69/120    avg_loss:0.139, val_acc:0.983]
Epoch [70/120    avg_loss:0.142, val_acc:0.977]
Epoch [71/120    avg_loss:0.127, val_acc:0.985]
Epoch [72/120    avg_loss:0.117, val_acc:0.990]
Epoch [73/120    avg_loss:0.099, val_acc:0.983]
Epoch [74/120    avg_loss:0.101, val_acc:0.988]
Epoch [75/120    avg_loss:0.113, val_acc:0.983]
Epoch [76/120    avg_loss:0.177, val_acc:0.977]
Epoch [77/120    avg_loss:0.117, val_acc:0.990]
Epoch [78/120    avg_loss:0.116, val_acc:0.965]
Epoch [79/120    avg_loss:0.121, val_acc:0.996]
Epoch [80/120    avg_loss:0.120, val_acc:0.985]
Epoch [81/120    avg_loss:0.119, val_acc:0.994]
Epoch [82/120    avg_loss:0.086, val_acc:0.992]
Epoch [83/120    avg_loss:0.075, val_acc:0.992]
Epoch [84/120    avg_loss:0.082, val_acc:0.994]
Epoch [85/120    avg_loss:0.131, val_acc:0.973]
Epoch [86/120    avg_loss:0.184, val_acc:0.975]
Epoch [87/120    avg_loss:0.129, val_acc:0.979]
Epoch [88/120    avg_loss:0.157, val_acc:0.979]
Epoch [89/120    avg_loss:0.121, val_acc:0.975]
Epoch [90/120    avg_loss:0.086, val_acc:0.990]
Epoch [91/120    avg_loss:0.061, val_acc:0.996]
Epoch [92/120    avg_loss:0.091, val_acc:0.988]
Epoch [93/120    avg_loss:0.108, val_acc:0.992]
Epoch [94/120    avg_loss:0.105, val_acc:0.973]
Epoch [95/120    avg_loss:0.103, val_acc:0.992]
Epoch [96/120    avg_loss:0.070, val_acc:0.990]
Epoch [97/120    avg_loss:0.085, val_acc:0.971]
Epoch [98/120    avg_loss:0.063, val_acc:0.994]
Epoch [99/120    avg_loss:0.069, val_acc:0.994]
Epoch [100/120    avg_loss:0.082, val_acc:0.992]
Epoch [101/120    avg_loss:0.057, val_acc:0.992]
Epoch [102/120    avg_loss:0.053, val_acc:0.994]
Epoch [103/120    avg_loss:0.063, val_acc:0.992]
Epoch [104/120    avg_loss:0.059, val_acc:0.996]
Epoch [105/120    avg_loss:0.054, val_acc:0.992]
Epoch [106/120    avg_loss:0.048, val_acc:0.998]
Epoch [107/120    avg_loss:0.056, val_acc:0.992]
Epoch [108/120    avg_loss:0.067, val_acc:0.996]
Epoch [109/120    avg_loss:0.086, val_acc:0.977]
Epoch [110/120    avg_loss:0.063, val_acc:0.994]
Epoch [111/120    avg_loss:0.068, val_acc:0.992]
Epoch [112/120    avg_loss:0.036, val_acc:0.996]
Epoch [113/120    avg_loss:0.037, val_acc:0.994]
Epoch [114/120    avg_loss:0.037, val_acc:0.994]
Epoch [115/120    avg_loss:0.035, val_acc:0.994]
Epoch [116/120    avg_loss:0.048, val_acc:0.996]
Epoch [117/120    avg_loss:0.090, val_acc:0.973]
Epoch [118/120    avg_loss:0.064, val_acc:0.988]
Epoch [119/120    avg_loss:0.049, val_acc:0.994]
Epoch [120/120    avg_loss:0.032, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.98426966 0.98004435 0.92810458 0.91836735
 1.         0.96132597 1.         1.         1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9888430784013276
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4bd24e8e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.597, val_acc:0.315]
Epoch [2/120    avg_loss:2.552, val_acc:0.383]
Epoch [3/120    avg_loss:2.511, val_acc:0.352]
Epoch [4/120    avg_loss:2.472, val_acc:0.354]
Epoch [5/120    avg_loss:2.430, val_acc:0.358]
Epoch [6/120    avg_loss:2.376, val_acc:0.360]
Epoch [7/120    avg_loss:2.325, val_acc:0.362]
Epoch [8/120    avg_loss:2.269, val_acc:0.390]
Epoch [9/120    avg_loss:2.218, val_acc:0.377]
Epoch [10/120    avg_loss:2.156, val_acc:0.406]
Epoch [11/120    avg_loss:2.093, val_acc:0.456]
Epoch [12/120    avg_loss:2.019, val_acc:0.492]
Epoch [13/120    avg_loss:1.958, val_acc:0.540]
Epoch [14/120    avg_loss:1.883, val_acc:0.558]
Epoch [15/120    avg_loss:1.799, val_acc:0.640]
Epoch [16/120    avg_loss:1.721, val_acc:0.662]
Epoch [17/120    avg_loss:1.662, val_acc:0.652]
Epoch [18/120    avg_loss:1.616, val_acc:0.700]
Epoch [19/120    avg_loss:1.518, val_acc:0.702]
Epoch [20/120    avg_loss:1.418, val_acc:0.688]
Epoch [21/120    avg_loss:1.346, val_acc:0.748]
Epoch [22/120    avg_loss:1.237, val_acc:0.838]
Epoch [23/120    avg_loss:1.174, val_acc:0.821]
Epoch [24/120    avg_loss:1.131, val_acc:0.846]
Epoch [25/120    avg_loss:1.027, val_acc:0.769]
Epoch [26/120    avg_loss:0.939, val_acc:0.858]
Epoch [27/120    avg_loss:0.836, val_acc:0.854]
Epoch [28/120    avg_loss:0.825, val_acc:0.883]
Epoch [29/120    avg_loss:0.775, val_acc:0.865]
Epoch [30/120    avg_loss:0.733, val_acc:0.881]
Epoch [31/120    avg_loss:0.678, val_acc:0.879]
Epoch [32/120    avg_loss:0.659, val_acc:0.867]
Epoch [33/120    avg_loss:0.630, val_acc:0.912]
Epoch [34/120    avg_loss:0.544, val_acc:0.912]
Epoch [35/120    avg_loss:0.537, val_acc:0.912]
Epoch [36/120    avg_loss:0.537, val_acc:0.910]
Epoch [37/120    avg_loss:0.548, val_acc:0.929]
Epoch [38/120    avg_loss:0.541, val_acc:0.892]
Epoch [39/120    avg_loss:0.522, val_acc:0.908]
Epoch [40/120    avg_loss:0.492, val_acc:0.904]
Epoch [41/120    avg_loss:0.459, val_acc:0.894]
Epoch [42/120    avg_loss:0.438, val_acc:0.927]
Epoch [43/120    avg_loss:0.404, val_acc:0.925]
Epoch [44/120    avg_loss:0.411, val_acc:0.921]
Epoch [45/120    avg_loss:0.369, val_acc:0.933]
Epoch [46/120    avg_loss:0.350, val_acc:0.908]
Epoch [47/120    avg_loss:0.371, val_acc:0.879]
Epoch [48/120    avg_loss:0.434, val_acc:0.921]
Epoch [49/120    avg_loss:0.401, val_acc:0.929]
Epoch [50/120    avg_loss:0.361, val_acc:0.948]
Epoch [51/120    avg_loss:0.346, val_acc:0.938]
Epoch [52/120    avg_loss:0.403, val_acc:0.904]
Epoch [53/120    avg_loss:0.425, val_acc:0.863]
Epoch [54/120    avg_loss:0.433, val_acc:0.938]
Epoch [55/120    avg_loss:0.319, val_acc:0.948]
Epoch [56/120    avg_loss:0.305, val_acc:0.948]
Epoch [57/120    avg_loss:0.306, val_acc:0.938]
Epoch [58/120    avg_loss:0.338, val_acc:0.952]
Epoch [59/120    avg_loss:0.318, val_acc:0.960]
Epoch [60/120    avg_loss:0.266, val_acc:0.942]
Epoch [61/120    avg_loss:0.262, val_acc:0.950]
Epoch [62/120    avg_loss:0.326, val_acc:0.933]
Epoch [63/120    avg_loss:0.304, val_acc:0.952]
Epoch [64/120    avg_loss:0.246, val_acc:0.940]
Epoch [65/120    avg_loss:0.248, val_acc:0.952]
Epoch [66/120    avg_loss:0.297, val_acc:0.946]
Epoch [67/120    avg_loss:0.251, val_acc:0.954]
Epoch [68/120    avg_loss:0.219, val_acc:0.973]
Epoch [69/120    avg_loss:0.189, val_acc:0.967]
Epoch [70/120    avg_loss:0.226, val_acc:0.971]
Epoch [71/120    avg_loss:0.216, val_acc:0.933]
Epoch [72/120    avg_loss:0.220, val_acc:0.944]
Epoch [73/120    avg_loss:0.183, val_acc:0.971]
Epoch [74/120    avg_loss:0.211, val_acc:0.971]
Epoch [75/120    avg_loss:0.237, val_acc:0.973]
Epoch [76/120    avg_loss:0.170, val_acc:0.960]
Epoch [77/120    avg_loss:0.180, val_acc:0.931]
Epoch [78/120    avg_loss:0.176, val_acc:0.971]
Epoch [79/120    avg_loss:0.175, val_acc:0.965]
Epoch [80/120    avg_loss:0.185, val_acc:0.973]
Epoch [81/120    avg_loss:0.176, val_acc:0.979]
Epoch [82/120    avg_loss:0.143, val_acc:0.985]
Epoch [83/120    avg_loss:0.166, val_acc:0.969]
Epoch [84/120    avg_loss:0.148, val_acc:0.971]
Epoch [85/120    avg_loss:0.163, val_acc:0.975]
Epoch [86/120    avg_loss:0.162, val_acc:0.958]
Epoch [87/120    avg_loss:0.162, val_acc:0.979]
Epoch [88/120    avg_loss:0.120, val_acc:0.977]
Epoch [89/120    avg_loss:0.112, val_acc:0.981]
Epoch [90/120    avg_loss:0.127, val_acc:0.981]
Epoch [91/120    avg_loss:0.111, val_acc:0.971]
Epoch [92/120    avg_loss:0.125, val_acc:0.956]
Epoch [93/120    avg_loss:0.141, val_acc:0.975]
Epoch [94/120    avg_loss:0.090, val_acc:0.983]
Epoch [95/120    avg_loss:0.099, val_acc:0.988]
Epoch [96/120    avg_loss:0.111, val_acc:0.981]
Epoch [97/120    avg_loss:0.110, val_acc:0.979]
Epoch [98/120    avg_loss:0.121, val_acc:0.979]
Epoch [99/120    avg_loss:0.106, val_acc:0.965]
Epoch [100/120    avg_loss:0.117, val_acc:0.975]
Epoch [101/120    avg_loss:0.147, val_acc:0.963]
Epoch [102/120    avg_loss:0.138, val_acc:0.969]
Epoch [103/120    avg_loss:0.091, val_acc:0.979]
Epoch [104/120    avg_loss:0.213, val_acc:0.969]
Epoch [105/120    avg_loss:0.145, val_acc:0.971]
Epoch [106/120    avg_loss:0.171, val_acc:0.983]
Epoch [107/120    avg_loss:0.126, val_acc:0.977]
Epoch [108/120    avg_loss:0.098, val_acc:0.983]
Epoch [109/120    avg_loss:0.080, val_acc:0.983]
Epoch [110/120    avg_loss:0.074, val_acc:0.985]
Epoch [111/120    avg_loss:0.064, val_acc:0.983]
Epoch [112/120    avg_loss:0.066, val_acc:0.983]
Epoch [113/120    avg_loss:0.068, val_acc:0.983]
Epoch [114/120    avg_loss:0.056, val_acc:0.983]
Epoch [115/120    avg_loss:0.054, val_acc:0.983]
Epoch [116/120    avg_loss:0.057, val_acc:0.985]
Epoch [117/120    avg_loss:0.058, val_acc:0.988]
Epoch [118/120    avg_loss:0.049, val_acc:0.988]
Epoch [119/120    avg_loss:0.054, val_acc:0.988]
Epoch [120/120    avg_loss:0.049, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.97333333 0.99563319 0.922049   0.88888889
 1.         0.93181818 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9888425520757672
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f628be98e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.056]
Epoch [2/120    avg_loss:2.583, val_acc:0.319]
Epoch [3/120    avg_loss:2.536, val_acc:0.371]
Epoch [4/120    avg_loss:2.497, val_acc:0.417]
Epoch [5/120    avg_loss:2.459, val_acc:0.440]
Epoch [6/120    avg_loss:2.418, val_acc:0.452]
Epoch [7/120    avg_loss:2.370, val_acc:0.463]
Epoch [8/120    avg_loss:2.324, val_acc:0.471]
Epoch [9/120    avg_loss:2.281, val_acc:0.483]
Epoch [10/120    avg_loss:2.234, val_acc:0.542]
Epoch [11/120    avg_loss:2.165, val_acc:0.562]
Epoch [12/120    avg_loss:2.096, val_acc:0.565]
Epoch [13/120    avg_loss:2.043, val_acc:0.588]
Epoch [14/120    avg_loss:1.969, val_acc:0.585]
Epoch [15/120    avg_loss:1.885, val_acc:0.571]
Epoch [16/120    avg_loss:1.827, val_acc:0.594]
Epoch [17/120    avg_loss:1.741, val_acc:0.637]
Epoch [18/120    avg_loss:1.657, val_acc:0.652]
Epoch [19/120    avg_loss:1.568, val_acc:0.679]
Epoch [20/120    avg_loss:1.480, val_acc:0.719]
Epoch [21/120    avg_loss:1.376, val_acc:0.733]
Epoch [22/120    avg_loss:1.312, val_acc:0.750]
Epoch [23/120    avg_loss:1.192, val_acc:0.858]
Epoch [24/120    avg_loss:1.126, val_acc:0.827]
Epoch [25/120    avg_loss:1.063, val_acc:0.883]
Epoch [26/120    avg_loss:1.004, val_acc:0.904]
Epoch [27/120    avg_loss:0.936, val_acc:0.894]
Epoch [28/120    avg_loss:0.838, val_acc:0.881]
Epoch [29/120    avg_loss:0.783, val_acc:0.917]
Epoch [30/120    avg_loss:0.693, val_acc:0.902]
Epoch [31/120    avg_loss:0.640, val_acc:0.931]
Epoch [32/120    avg_loss:0.620, val_acc:0.883]
Epoch [33/120    avg_loss:0.647, val_acc:0.927]
Epoch [34/120    avg_loss:0.585, val_acc:0.908]
Epoch [35/120    avg_loss:0.536, val_acc:0.908]
Epoch [36/120    avg_loss:0.518, val_acc:0.919]
Epoch [37/120    avg_loss:0.467, val_acc:0.944]
Epoch [38/120    avg_loss:0.434, val_acc:0.927]
Epoch [39/120    avg_loss:0.389, val_acc:0.923]
Epoch [40/120    avg_loss:0.445, val_acc:0.929]
Epoch [41/120    avg_loss:0.379, val_acc:0.940]
Epoch [42/120    avg_loss:0.410, val_acc:0.894]
Epoch [43/120    avg_loss:0.389, val_acc:0.948]
Epoch [44/120    avg_loss:0.352, val_acc:0.919]
Epoch [45/120    avg_loss:0.334, val_acc:0.946]
Epoch [46/120    avg_loss:0.321, val_acc:0.958]
Epoch [47/120    avg_loss:0.315, val_acc:0.952]
Epoch [48/120    avg_loss:0.365, val_acc:0.925]
Epoch [49/120    avg_loss:0.316, val_acc:0.956]
Epoch [50/120    avg_loss:0.241, val_acc:0.967]
Epoch [51/120    avg_loss:0.241, val_acc:0.956]
Epoch [52/120    avg_loss:0.228, val_acc:0.963]
Epoch [53/120    avg_loss:0.220, val_acc:0.948]
Epoch [54/120    avg_loss:0.284, val_acc:0.958]
Epoch [55/120    avg_loss:0.242, val_acc:0.969]
Epoch [56/120    avg_loss:0.208, val_acc:0.960]
Epoch [57/120    avg_loss:0.251, val_acc:0.925]
Epoch [58/120    avg_loss:0.253, val_acc:0.975]
Epoch [59/120    avg_loss:0.208, val_acc:0.954]
Epoch [60/120    avg_loss:0.177, val_acc:0.977]
Epoch [61/120    avg_loss:0.187, val_acc:0.981]
Epoch [62/120    avg_loss:0.188, val_acc:0.969]
Epoch [63/120    avg_loss:0.176, val_acc:0.958]
Epoch [64/120    avg_loss:0.163, val_acc:0.983]
Epoch [65/120    avg_loss:0.121, val_acc:0.979]
Epoch [66/120    avg_loss:0.141, val_acc:0.988]
Epoch [67/120    avg_loss:0.147, val_acc:0.979]
Epoch [68/120    avg_loss:0.169, val_acc:0.988]
Epoch [69/120    avg_loss:0.137, val_acc:0.973]
Epoch [70/120    avg_loss:0.211, val_acc:0.965]
Epoch [71/120    avg_loss:0.150, val_acc:0.988]
Epoch [72/120    avg_loss:0.161, val_acc:0.973]
Epoch [73/120    avg_loss:0.161, val_acc:0.979]
Epoch [74/120    avg_loss:0.125, val_acc:0.988]
Epoch [75/120    avg_loss:0.131, val_acc:0.983]
Epoch [76/120    avg_loss:0.144, val_acc:0.983]
Epoch [77/120    avg_loss:0.091, val_acc:0.981]
Epoch [78/120    avg_loss:0.118, val_acc:0.975]
Epoch [79/120    avg_loss:0.148, val_acc:0.931]
Epoch [80/120    avg_loss:0.218, val_acc:0.979]
Epoch [81/120    avg_loss:0.172, val_acc:0.977]
Epoch [82/120    avg_loss:0.129, val_acc:0.971]
Epoch [83/120    avg_loss:0.094, val_acc:0.992]
Epoch [84/120    avg_loss:0.109, val_acc:0.990]
Epoch [85/120    avg_loss:0.078, val_acc:0.994]
Epoch [86/120    avg_loss:0.109, val_acc:0.981]
Epoch [87/120    avg_loss:0.125, val_acc:0.988]
Epoch [88/120    avg_loss:0.075, val_acc:0.994]
Epoch [89/120    avg_loss:0.068, val_acc:0.988]
Epoch [90/120    avg_loss:0.072, val_acc:0.985]
Epoch [91/120    avg_loss:0.070, val_acc:0.990]
Epoch [92/120    avg_loss:0.065, val_acc:0.990]
Epoch [93/120    avg_loss:0.093, val_acc:0.963]
Epoch [94/120    avg_loss:0.063, val_acc:0.985]
Epoch [95/120    avg_loss:0.073, val_acc:0.988]
Epoch [96/120    avg_loss:0.063, val_acc:0.992]
Epoch [97/120    avg_loss:0.055, val_acc:0.990]
Epoch [98/120    avg_loss:0.051, val_acc:0.990]
Epoch [99/120    avg_loss:0.073, val_acc:0.983]
Epoch [100/120    avg_loss:0.071, val_acc:0.992]
Epoch [101/120    avg_loss:0.060, val_acc:0.988]
Epoch [102/120    avg_loss:0.044, val_acc:0.992]
Epoch [103/120    avg_loss:0.038, val_acc:0.990]
Epoch [104/120    avg_loss:0.033, val_acc:0.992]
Epoch [105/120    avg_loss:0.034, val_acc:0.992]
Epoch [106/120    avg_loss:0.035, val_acc:0.994]
Epoch [107/120    avg_loss:0.035, val_acc:0.992]
Epoch [108/120    avg_loss:0.029, val_acc:0.992]
Epoch [109/120    avg_loss:0.029, val_acc:0.992]
Epoch [110/120    avg_loss:0.033, val_acc:0.992]
Epoch [111/120    avg_loss:0.027, val_acc:0.992]
Epoch [112/120    avg_loss:0.034, val_acc:0.992]
Epoch [113/120    avg_loss:0.036, val_acc:0.992]
Epoch [114/120    avg_loss:0.029, val_acc:0.994]
Epoch [115/120    avg_loss:0.026, val_acc:0.994]
Epoch [116/120    avg_loss:0.029, val_acc:0.994]
Epoch [117/120    avg_loss:0.036, val_acc:0.996]
Epoch [118/120    avg_loss:0.028, val_acc:0.996]
Epoch [119/120    avg_loss:0.035, val_acc:0.996]
Epoch [120/120    avg_loss:0.032, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   6   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.98206278 0.98230088 0.92715232 0.90909091
 1.         0.95555556 1.         0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.9897921407126602
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa874fa4e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.302]
Epoch [2/120    avg_loss:2.550, val_acc:0.313]
Epoch [3/120    avg_loss:2.513, val_acc:0.312]
Epoch [4/120    avg_loss:2.477, val_acc:0.310]
Epoch [5/120    avg_loss:2.433, val_acc:0.325]
Epoch [6/120    avg_loss:2.404, val_acc:0.347]
Epoch [7/120    avg_loss:2.369, val_acc:0.385]
Epoch [8/120    avg_loss:2.326, val_acc:0.399]
Epoch [9/120    avg_loss:2.286, val_acc:0.403]
Epoch [10/120    avg_loss:2.235, val_acc:0.411]
Epoch [11/120    avg_loss:2.186, val_acc:0.456]
Epoch [12/120    avg_loss:2.129, val_acc:0.496]
Epoch [13/120    avg_loss:2.079, val_acc:0.520]
Epoch [14/120    avg_loss:2.030, val_acc:0.526]
Epoch [15/120    avg_loss:1.968, val_acc:0.540]
Epoch [16/120    avg_loss:1.901, val_acc:0.554]
Epoch [17/120    avg_loss:1.838, val_acc:0.593]
Epoch [18/120    avg_loss:1.776, val_acc:0.625]
Epoch [19/120    avg_loss:1.704, val_acc:0.613]
Epoch [20/120    avg_loss:1.630, val_acc:0.651]
Epoch [21/120    avg_loss:1.520, val_acc:0.706]
Epoch [22/120    avg_loss:1.452, val_acc:0.746]
Epoch [23/120    avg_loss:1.360, val_acc:0.756]
Epoch [24/120    avg_loss:1.316, val_acc:0.712]
Epoch [25/120    avg_loss:1.244, val_acc:0.823]
Epoch [26/120    avg_loss:1.161, val_acc:0.855]
Epoch [27/120    avg_loss:1.105, val_acc:0.855]
Epoch [28/120    avg_loss:1.053, val_acc:0.812]
Epoch [29/120    avg_loss:0.983, val_acc:0.871]
Epoch [30/120    avg_loss:0.893, val_acc:0.895]
Epoch [31/120    avg_loss:0.839, val_acc:0.917]
Epoch [32/120    avg_loss:0.788, val_acc:0.903]
Epoch [33/120    avg_loss:0.738, val_acc:0.903]
Epoch [34/120    avg_loss:0.692, val_acc:0.923]
Epoch [35/120    avg_loss:0.628, val_acc:0.899]
Epoch [36/120    avg_loss:0.604, val_acc:0.913]
Epoch [37/120    avg_loss:0.644, val_acc:0.921]
Epoch [38/120    avg_loss:0.524, val_acc:0.913]
Epoch [39/120    avg_loss:0.510, val_acc:0.921]
Epoch [40/120    avg_loss:0.550, val_acc:0.913]
Epoch [41/120    avg_loss:0.470, val_acc:0.913]
Epoch [42/120    avg_loss:0.458, val_acc:0.927]
Epoch [43/120    avg_loss:0.404, val_acc:0.915]
Epoch [44/120    avg_loss:0.422, val_acc:0.921]
Epoch [45/120    avg_loss:0.389, val_acc:0.925]
Epoch [46/120    avg_loss:0.397, val_acc:0.911]
Epoch [47/120    avg_loss:0.384, val_acc:0.929]
Epoch [48/120    avg_loss:0.353, val_acc:0.937]
Epoch [49/120    avg_loss:0.332, val_acc:0.909]
Epoch [50/120    avg_loss:0.352, val_acc:0.948]
Epoch [51/120    avg_loss:0.340, val_acc:0.944]
Epoch [52/120    avg_loss:0.335, val_acc:0.944]
Epoch [53/120    avg_loss:0.318, val_acc:0.940]
Epoch [54/120    avg_loss:0.305, val_acc:0.946]
Epoch [55/120    avg_loss:0.320, val_acc:0.937]
Epoch [56/120    avg_loss:0.291, val_acc:0.946]
Epoch [57/120    avg_loss:0.292, val_acc:0.940]
Epoch [58/120    avg_loss:0.279, val_acc:0.952]
Epoch [59/120    avg_loss:0.283, val_acc:0.946]
Epoch [60/120    avg_loss:0.303, val_acc:0.940]
Epoch [61/120    avg_loss:0.348, val_acc:0.940]
Epoch [62/120    avg_loss:0.263, val_acc:0.933]
Epoch [63/120    avg_loss:0.259, val_acc:0.962]
Epoch [64/120    avg_loss:0.215, val_acc:0.944]
Epoch [65/120    avg_loss:0.235, val_acc:0.944]
Epoch [66/120    avg_loss:0.255, val_acc:0.948]
Epoch [67/120    avg_loss:0.213, val_acc:0.952]
Epoch [68/120    avg_loss:0.216, val_acc:0.948]
Epoch [69/120    avg_loss:0.171, val_acc:0.940]
Epoch [70/120    avg_loss:0.212, val_acc:0.950]
Epoch [71/120    avg_loss:0.249, val_acc:0.950]
Epoch [72/120    avg_loss:0.267, val_acc:0.931]
Epoch [73/120    avg_loss:0.231, val_acc:0.950]
Epoch [74/120    avg_loss:0.176, val_acc:0.942]
Epoch [75/120    avg_loss:0.179, val_acc:0.960]
Epoch [76/120    avg_loss:0.175, val_acc:0.966]
Epoch [77/120    avg_loss:0.173, val_acc:0.958]
Epoch [78/120    avg_loss:0.138, val_acc:0.966]
Epoch [79/120    avg_loss:0.192, val_acc:0.938]
Epoch [80/120    avg_loss:0.235, val_acc:0.954]
Epoch [81/120    avg_loss:0.180, val_acc:0.956]
Epoch [82/120    avg_loss:0.137, val_acc:0.974]
Epoch [83/120    avg_loss:0.150, val_acc:0.968]
Epoch [84/120    avg_loss:0.159, val_acc:0.946]
Epoch [85/120    avg_loss:0.150, val_acc:0.970]
Epoch [86/120    avg_loss:0.159, val_acc:0.972]
Epoch [87/120    avg_loss:0.125, val_acc:0.970]
Epoch [88/120    avg_loss:0.152, val_acc:0.960]
Epoch [89/120    avg_loss:0.150, val_acc:0.964]
Epoch [90/120    avg_loss:0.121, val_acc:0.972]
Epoch [91/120    avg_loss:0.129, val_acc:0.964]
Epoch [92/120    avg_loss:0.115, val_acc:0.952]
Epoch [93/120    avg_loss:0.133, val_acc:0.968]
Epoch [94/120    avg_loss:0.104, val_acc:0.982]
Epoch [95/120    avg_loss:0.092, val_acc:0.976]
Epoch [96/120    avg_loss:0.076, val_acc:0.984]
Epoch [97/120    avg_loss:0.069, val_acc:0.978]
Epoch [98/120    avg_loss:0.061, val_acc:0.980]
Epoch [99/120    avg_loss:0.077, val_acc:0.982]
Epoch [100/120    avg_loss:0.100, val_acc:0.974]
Epoch [101/120    avg_loss:0.094, val_acc:0.970]
Epoch [102/120    avg_loss:0.090, val_acc:0.978]
Epoch [103/120    avg_loss:0.082, val_acc:0.978]
Epoch [104/120    avg_loss:0.085, val_acc:0.966]
Epoch [105/120    avg_loss:0.112, val_acc:0.964]
Epoch [106/120    avg_loss:0.139, val_acc:0.976]
Epoch [107/120    avg_loss:0.096, val_acc:0.976]
Epoch [108/120    avg_loss:0.084, val_acc:0.968]
Epoch [109/120    avg_loss:0.064, val_acc:0.984]
Epoch [110/120    avg_loss:0.085, val_acc:0.980]
Epoch [111/120    avg_loss:0.094, val_acc:0.942]
Epoch [112/120    avg_loss:0.119, val_acc:0.954]
Epoch [113/120    avg_loss:0.091, val_acc:0.970]
Epoch [114/120    avg_loss:0.067, val_acc:0.976]
Epoch [115/120    avg_loss:0.076, val_acc:0.956]
Epoch [116/120    avg_loss:0.060, val_acc:0.974]
Epoch [117/120    avg_loss:0.074, val_acc:0.974]
Epoch [118/120    avg_loss:0.055, val_acc:0.980]
Epoch [119/120    avg_loss:0.060, val_acc:0.978]
Epoch [120/120    avg_loss:0.063, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   0   0   1   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99926954 0.98206278 0.99782135 0.92765957 0.87591241
 1.         0.96132597 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9897916265825902
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c4ad0ddd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.102]
Epoch [2/120    avg_loss:2.577, val_acc:0.361]
Epoch [3/120    avg_loss:2.529, val_acc:0.369]
Epoch [4/120    avg_loss:2.478, val_acc:0.363]
Epoch [5/120    avg_loss:2.443, val_acc:0.365]
Epoch [6/120    avg_loss:2.397, val_acc:0.391]
Epoch [7/120    avg_loss:2.357, val_acc:0.419]
Epoch [8/120    avg_loss:2.324, val_acc:0.442]
Epoch [9/120    avg_loss:2.283, val_acc:0.427]
Epoch [10/120    avg_loss:2.233, val_acc:0.429]
Epoch [11/120    avg_loss:2.195, val_acc:0.401]
Epoch [12/120    avg_loss:2.129, val_acc:0.423]
Epoch [13/120    avg_loss:2.067, val_acc:0.454]
Epoch [14/120    avg_loss:2.014, val_acc:0.431]
Epoch [15/120    avg_loss:1.948, val_acc:0.504]
Epoch [16/120    avg_loss:1.891, val_acc:0.532]
Epoch [17/120    avg_loss:1.800, val_acc:0.556]
Epoch [18/120    avg_loss:1.725, val_acc:0.573]
Epoch [19/120    avg_loss:1.677, val_acc:0.649]
Epoch [20/120    avg_loss:1.587, val_acc:0.651]
Epoch [21/120    avg_loss:1.513, val_acc:0.623]
Epoch [22/120    avg_loss:1.422, val_acc:0.675]
Epoch [23/120    avg_loss:1.318, val_acc:0.742]
Epoch [24/120    avg_loss:1.282, val_acc:0.808]
Epoch [25/120    avg_loss:1.189, val_acc:0.706]
Epoch [26/120    avg_loss:1.133, val_acc:0.804]
Epoch [27/120    avg_loss:1.018, val_acc:0.829]
Epoch [28/120    avg_loss:0.957, val_acc:0.857]
Epoch [29/120    avg_loss:0.929, val_acc:0.839]
Epoch [30/120    avg_loss:0.854, val_acc:0.833]
Epoch [31/120    avg_loss:0.813, val_acc:0.861]
Epoch [32/120    avg_loss:0.732, val_acc:0.913]
Epoch [33/120    avg_loss:0.701, val_acc:0.905]
Epoch [34/120    avg_loss:0.622, val_acc:0.921]
Epoch [35/120    avg_loss:0.647, val_acc:0.901]
Epoch [36/120    avg_loss:0.571, val_acc:0.913]
Epoch [37/120    avg_loss:0.557, val_acc:0.923]
Epoch [38/120    avg_loss:0.535, val_acc:0.911]
Epoch [39/120    avg_loss:0.493, val_acc:0.935]
Epoch [40/120    avg_loss:0.445, val_acc:0.927]
Epoch [41/120    avg_loss:0.447, val_acc:0.927]
Epoch [42/120    avg_loss:0.437, val_acc:0.913]
Epoch [43/120    avg_loss:0.428, val_acc:0.917]
Epoch [44/120    avg_loss:0.452, val_acc:0.952]
Epoch [45/120    avg_loss:0.372, val_acc:0.944]
Epoch [46/120    avg_loss:0.339, val_acc:0.942]
Epoch [47/120    avg_loss:0.340, val_acc:0.958]
Epoch [48/120    avg_loss:0.353, val_acc:0.946]
Epoch [49/120    avg_loss:0.304, val_acc:0.938]
Epoch [50/120    avg_loss:0.335, val_acc:0.944]
Epoch [51/120    avg_loss:0.331, val_acc:0.935]
Epoch [52/120    avg_loss:0.313, val_acc:0.948]
Epoch [53/120    avg_loss:0.336, val_acc:0.935]
Epoch [54/120    avg_loss:0.356, val_acc:0.937]
Epoch [55/120    avg_loss:0.278, val_acc:0.960]
Epoch [56/120    avg_loss:0.258, val_acc:0.952]
Epoch [57/120    avg_loss:0.305, val_acc:0.927]
Epoch [58/120    avg_loss:0.388, val_acc:0.923]
Epoch [59/120    avg_loss:0.319, val_acc:0.944]
Epoch [60/120    avg_loss:0.231, val_acc:0.956]
Epoch [61/120    avg_loss:0.207, val_acc:0.935]
Epoch [62/120    avg_loss:0.229, val_acc:0.958]
Epoch [63/120    avg_loss:0.204, val_acc:0.960]
Epoch [64/120    avg_loss:0.203, val_acc:0.970]
Epoch [65/120    avg_loss:0.197, val_acc:0.974]
Epoch [66/120    avg_loss:0.165, val_acc:0.948]
Epoch [67/120    avg_loss:0.178, val_acc:0.942]
Epoch [68/120    avg_loss:0.193, val_acc:0.970]
Epoch [69/120    avg_loss:0.223, val_acc:0.968]
Epoch [70/120    avg_loss:0.182, val_acc:0.960]
Epoch [71/120    avg_loss:0.149, val_acc:0.970]
Epoch [72/120    avg_loss:0.151, val_acc:0.962]
Epoch [73/120    avg_loss:0.228, val_acc:0.958]
Epoch [74/120    avg_loss:0.143, val_acc:0.968]
Epoch [75/120    avg_loss:0.146, val_acc:0.976]
Epoch [76/120    avg_loss:0.129, val_acc:0.978]
Epoch [77/120    avg_loss:0.108, val_acc:0.964]
Epoch [78/120    avg_loss:0.106, val_acc:0.970]
Epoch [79/120    avg_loss:0.110, val_acc:0.964]
Epoch [80/120    avg_loss:0.102, val_acc:0.976]
Epoch [81/120    avg_loss:0.142, val_acc:0.974]
Epoch [82/120    avg_loss:0.102, val_acc:0.968]
Epoch [83/120    avg_loss:0.100, val_acc:0.954]
Epoch [84/120    avg_loss:0.118, val_acc:0.964]
Epoch [85/120    avg_loss:0.133, val_acc:0.956]
Epoch [86/120    avg_loss:0.209, val_acc:0.962]
Epoch [87/120    avg_loss:0.156, val_acc:0.972]
Epoch [88/120    avg_loss:0.092, val_acc:0.982]
Epoch [89/120    avg_loss:0.074, val_acc:0.978]
Epoch [90/120    avg_loss:0.086, val_acc:0.978]
Epoch [91/120    avg_loss:0.148, val_acc:0.984]
Epoch [92/120    avg_loss:0.089, val_acc:0.980]
Epoch [93/120    avg_loss:0.107, val_acc:0.974]
Epoch [94/120    avg_loss:0.075, val_acc:0.974]
Epoch [95/120    avg_loss:0.064, val_acc:0.976]
Epoch [96/120    avg_loss:0.068, val_acc:0.980]
Epoch [97/120    avg_loss:0.079, val_acc:0.970]
Epoch [98/120    avg_loss:0.084, val_acc:0.984]
Epoch [99/120    avg_loss:0.071, val_acc:0.976]
Epoch [100/120    avg_loss:0.060, val_acc:0.986]
Epoch [101/120    avg_loss:0.055, val_acc:0.986]
Epoch [102/120    avg_loss:0.043, val_acc:0.982]
Epoch [103/120    avg_loss:0.036, val_acc:0.980]
Epoch [104/120    avg_loss:0.052, val_acc:0.982]
Epoch [105/120    avg_loss:0.045, val_acc:0.982]
Epoch [106/120    avg_loss:0.046, val_acc:0.986]
Epoch [107/120    avg_loss:0.038, val_acc:0.982]
Epoch [108/120    avg_loss:0.036, val_acc:0.984]
Epoch [109/120    avg_loss:0.041, val_acc:0.978]
Epoch [110/120    avg_loss:0.036, val_acc:0.992]
Epoch [111/120    avg_loss:0.049, val_acc:0.986]
Epoch [112/120    avg_loss:0.048, val_acc:0.988]
Epoch [113/120    avg_loss:0.045, val_acc:0.984]
Epoch [114/120    avg_loss:0.034, val_acc:0.974]
Epoch [115/120    avg_loss:0.037, val_acc:0.988]
Epoch [116/120    avg_loss:0.029, val_acc:0.990]
Epoch [117/120    avg_loss:0.033, val_acc:0.986]
Epoch [118/120    avg_loss:0.031, val_acc:0.990]
Epoch [119/120    avg_loss:0.030, val_acc:0.988]
Epoch [120/120    avg_loss:0.039, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0  90 140   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.35394456289978

F1 scores:
[       nan 0.99854227 0.99319728 0.5625     0.71768707 0.91216216
 1.         0.98378378 0.99741602 1.         1.         1.
 1.         1.        ]

Kappa:
0.9594076541213664
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4105fdee10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.626, val_acc:0.050]
Epoch [2/120    avg_loss:2.591, val_acc:0.212]
Epoch [3/120    avg_loss:2.561, val_acc:0.321]
Epoch [4/120    avg_loss:2.526, val_acc:0.355]
Epoch [5/120    avg_loss:2.490, val_acc:0.375]
Epoch [6/120    avg_loss:2.452, val_acc:0.391]
Epoch [7/120    avg_loss:2.410, val_acc:0.395]
Epoch [8/120    avg_loss:2.372, val_acc:0.413]
Epoch [9/120    avg_loss:2.329, val_acc:0.417]
Epoch [10/120    avg_loss:2.285, val_acc:0.462]
Epoch [11/120    avg_loss:2.238, val_acc:0.472]
Epoch [12/120    avg_loss:2.188, val_acc:0.496]
Epoch [13/120    avg_loss:2.131, val_acc:0.524]
Epoch [14/120    avg_loss:2.060, val_acc:0.504]
Epoch [15/120    avg_loss:1.991, val_acc:0.520]
Epoch [16/120    avg_loss:1.929, val_acc:0.524]
Epoch [17/120    avg_loss:1.842, val_acc:0.663]
Epoch [18/120    avg_loss:1.760, val_acc:0.565]
Epoch [19/120    avg_loss:1.663, val_acc:0.615]
Epoch [20/120    avg_loss:1.578, val_acc:0.667]
Epoch [21/120    avg_loss:1.514, val_acc:0.702]
Epoch [22/120    avg_loss:1.436, val_acc:0.623]
Epoch [23/120    avg_loss:1.358, val_acc:0.704]
Epoch [24/120    avg_loss:1.254, val_acc:0.766]
Epoch [25/120    avg_loss:1.204, val_acc:0.740]
Epoch [26/120    avg_loss:1.154, val_acc:0.798]
Epoch [27/120    avg_loss:1.118, val_acc:0.806]
Epoch [28/120    avg_loss:0.987, val_acc:0.825]
Epoch [29/120    avg_loss:0.950, val_acc:0.871]
Epoch [30/120    avg_loss:0.840, val_acc:0.883]
Epoch [31/120    avg_loss:0.844, val_acc:0.835]
Epoch [32/120    avg_loss:0.741, val_acc:0.879]
Epoch [33/120    avg_loss:0.709, val_acc:0.893]
Epoch [34/120    avg_loss:0.657, val_acc:0.895]
Epoch [35/120    avg_loss:0.629, val_acc:0.897]
Epoch [36/120    avg_loss:0.613, val_acc:0.917]
Epoch [37/120    avg_loss:0.581, val_acc:0.907]
Epoch [38/120    avg_loss:0.602, val_acc:0.891]
Epoch [39/120    avg_loss:0.516, val_acc:0.895]
Epoch [40/120    avg_loss:0.464, val_acc:0.891]
Epoch [41/120    avg_loss:0.519, val_acc:0.919]
Epoch [42/120    avg_loss:0.472, val_acc:0.923]
Epoch [43/120    avg_loss:0.474, val_acc:0.837]
Epoch [44/120    avg_loss:0.488, val_acc:0.909]
Epoch [45/120    avg_loss:0.405, val_acc:0.925]
Epoch [46/120    avg_loss:0.423, val_acc:0.940]
Epoch [47/120    avg_loss:0.420, val_acc:0.935]
Epoch [48/120    avg_loss:0.390, val_acc:0.921]
Epoch [49/120    avg_loss:0.408, val_acc:0.933]
Epoch [50/120    avg_loss:0.334, val_acc:0.929]
Epoch [51/120    avg_loss:0.294, val_acc:0.948]
Epoch [52/120    avg_loss:0.274, val_acc:0.919]
Epoch [53/120    avg_loss:0.286, val_acc:0.917]
Epoch [54/120    avg_loss:0.263, val_acc:0.968]
Epoch [55/120    avg_loss:0.243, val_acc:0.968]
Epoch [56/120    avg_loss:0.262, val_acc:0.956]
Epoch [57/120    avg_loss:0.264, val_acc:0.952]
Epoch [58/120    avg_loss:0.243, val_acc:0.968]
Epoch [59/120    avg_loss:0.193, val_acc:0.929]
Epoch [60/120    avg_loss:0.250, val_acc:0.946]
Epoch [61/120    avg_loss:0.247, val_acc:0.956]
Epoch [62/120    avg_loss:0.206, val_acc:0.976]
Epoch [63/120    avg_loss:0.212, val_acc:0.952]
Epoch [64/120    avg_loss:0.205, val_acc:0.960]
Epoch [65/120    avg_loss:0.196, val_acc:0.966]
Epoch [66/120    avg_loss:0.186, val_acc:0.976]
Epoch [67/120    avg_loss:0.171, val_acc:0.966]
Epoch [68/120    avg_loss:0.150, val_acc:0.944]
Epoch [69/120    avg_loss:0.168, val_acc:0.954]
Epoch [70/120    avg_loss:0.177, val_acc:0.974]
Epoch [71/120    avg_loss:0.230, val_acc:0.927]
Epoch [72/120    avg_loss:0.254, val_acc:0.976]
Epoch [73/120    avg_loss:0.181, val_acc:0.974]
Epoch [74/120    avg_loss:0.176, val_acc:0.970]
Epoch [75/120    avg_loss:0.175, val_acc:0.964]
Epoch [76/120    avg_loss:0.184, val_acc:0.966]
Epoch [77/120    avg_loss:0.136, val_acc:0.966]
Epoch [78/120    avg_loss:0.119, val_acc:0.980]
Epoch [79/120    avg_loss:0.111, val_acc:0.984]
Epoch [80/120    avg_loss:0.121, val_acc:0.968]
Epoch [81/120    avg_loss:0.145, val_acc:0.988]
Epoch [82/120    avg_loss:0.112, val_acc:0.982]
Epoch [83/120    avg_loss:0.111, val_acc:0.986]
Epoch [84/120    avg_loss:0.097, val_acc:0.970]
Epoch [85/120    avg_loss:0.110, val_acc:0.976]
Epoch [86/120    avg_loss:0.082, val_acc:0.980]
Epoch [87/120    avg_loss:0.084, val_acc:0.986]
Epoch [88/120    avg_loss:0.093, val_acc:0.990]
Epoch [89/120    avg_loss:0.086, val_acc:0.984]
Epoch [90/120    avg_loss:0.081, val_acc:0.992]
Epoch [91/120    avg_loss:0.074, val_acc:0.966]
Epoch [92/120    avg_loss:0.097, val_acc:0.968]
Epoch [93/120    avg_loss:0.087, val_acc:0.986]
Epoch [94/120    avg_loss:0.069, val_acc:0.986]
Epoch [95/120    avg_loss:0.093, val_acc:0.988]
Epoch [96/120    avg_loss:0.082, val_acc:0.992]
Epoch [97/120    avg_loss:0.080, val_acc:0.984]
Epoch [98/120    avg_loss:0.070, val_acc:0.986]
Epoch [99/120    avg_loss:0.072, val_acc:0.980]
Epoch [100/120    avg_loss:0.060, val_acc:0.990]
Epoch [101/120    avg_loss:0.050, val_acc:0.992]
Epoch [102/120    avg_loss:0.045, val_acc:0.996]
Epoch [103/120    avg_loss:0.046, val_acc:0.988]
Epoch [104/120    avg_loss:0.065, val_acc:0.966]
Epoch [105/120    avg_loss:0.098, val_acc:0.988]
Epoch [106/120    avg_loss:0.065, val_acc:0.986]
Epoch [107/120    avg_loss:0.061, val_acc:0.986]
Epoch [108/120    avg_loss:0.043, val_acc:0.984]
Epoch [109/120    avg_loss:0.068, val_acc:0.986]
Epoch [110/120    avg_loss:0.069, val_acc:0.984]
Epoch [111/120    avg_loss:0.064, val_acc:0.988]
Epoch [112/120    avg_loss:0.058, val_acc:0.986]
Epoch [113/120    avg_loss:0.040, val_acc:0.992]
Epoch [114/120    avg_loss:0.041, val_acc:0.994]
Epoch [115/120    avg_loss:0.030, val_acc:0.996]
Epoch [116/120    avg_loss:0.047, val_acc:0.990]
Epoch [117/120    avg_loss:0.048, val_acc:0.994]
Epoch [118/120    avg_loss:0.040, val_acc:0.994]
Epoch [119/120    avg_loss:0.036, val_acc:0.988]
Epoch [120/120    avg_loss:0.039, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 214  15   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 0.99636364 0.98198198 0.96396396 0.88284519 0.85409253
 0.98771499 0.96132597 1.         0.99893276 1.         0.9973545
 0.99668508 1.        ]

Kappa:
0.9829048293401486
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9cb8d0ce80>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.617, val_acc:0.131]
Epoch [2/120    avg_loss:2.581, val_acc:0.302]
Epoch [3/120    avg_loss:2.550, val_acc:0.313]
Epoch [4/120    avg_loss:2.511, val_acc:0.317]
Epoch [5/120    avg_loss:2.473, val_acc:0.343]
Epoch [6/120    avg_loss:2.435, val_acc:0.369]
Epoch [7/120    avg_loss:2.389, val_acc:0.403]
Epoch [8/120    avg_loss:2.351, val_acc:0.438]
Epoch [9/120    avg_loss:2.310, val_acc:0.468]
Epoch [10/120    avg_loss:2.268, val_acc:0.512]
Epoch [11/120    avg_loss:2.217, val_acc:0.514]
Epoch [12/120    avg_loss:2.168, val_acc:0.532]
Epoch [13/120    avg_loss:2.120, val_acc:0.534]
Epoch [14/120    avg_loss:2.051, val_acc:0.536]
Epoch [15/120    avg_loss:1.988, val_acc:0.534]
Epoch [16/120    avg_loss:1.916, val_acc:0.536]
Epoch [17/120    avg_loss:1.838, val_acc:0.548]
Epoch [18/120    avg_loss:1.766, val_acc:0.577]
Epoch [19/120    avg_loss:1.672, val_acc:0.571]
Epoch [20/120    avg_loss:1.562, val_acc:0.645]
Epoch [21/120    avg_loss:1.510, val_acc:0.687]
Epoch [22/120    avg_loss:1.421, val_acc:0.778]
Epoch [23/120    avg_loss:1.354, val_acc:0.843]
Epoch [24/120    avg_loss:1.270, val_acc:0.802]
Epoch [25/120    avg_loss:1.175, val_acc:0.865]
Epoch [26/120    avg_loss:1.088, val_acc:0.845]
Epoch [27/120    avg_loss:1.057, val_acc:0.885]
Epoch [28/120    avg_loss:0.956, val_acc:0.905]
Epoch [29/120    avg_loss:0.867, val_acc:0.921]
Epoch [30/120    avg_loss:0.869, val_acc:0.825]
Epoch [31/120    avg_loss:0.830, val_acc:0.905]
Epoch [32/120    avg_loss:0.741, val_acc:0.885]
Epoch [33/120    avg_loss:0.676, val_acc:0.935]
Epoch [34/120    avg_loss:0.670, val_acc:0.905]
Epoch [35/120    avg_loss:0.597, val_acc:0.935]
Epoch [36/120    avg_loss:0.539, val_acc:0.937]
Epoch [37/120    avg_loss:0.549, val_acc:0.917]
Epoch [38/120    avg_loss:0.487, val_acc:0.933]
Epoch [39/120    avg_loss:0.496, val_acc:0.944]
Epoch [40/120    avg_loss:0.466, val_acc:0.937]
Epoch [41/120    avg_loss:0.421, val_acc:0.933]
Epoch [42/120    avg_loss:0.359, val_acc:0.911]
Epoch [43/120    avg_loss:0.374, val_acc:0.940]
Epoch [44/120    avg_loss:0.384, val_acc:0.935]
Epoch [45/120    avg_loss:0.346, val_acc:0.938]
Epoch [46/120    avg_loss:0.336, val_acc:0.954]
Epoch [47/120    avg_loss:0.348, val_acc:0.938]
Epoch [48/120    avg_loss:0.329, val_acc:0.958]
Epoch [49/120    avg_loss:0.272, val_acc:0.966]
Epoch [50/120    avg_loss:0.279, val_acc:0.933]
Epoch [51/120    avg_loss:0.265, val_acc:0.954]
Epoch [52/120    avg_loss:0.273, val_acc:0.938]
Epoch [53/120    avg_loss:0.254, val_acc:0.958]
Epoch [54/120    avg_loss:0.282, val_acc:0.938]
Epoch [55/120    avg_loss:0.247, val_acc:0.962]
Epoch [56/120    avg_loss:0.240, val_acc:0.964]
Epoch [57/120    avg_loss:0.291, val_acc:0.946]
Epoch [58/120    avg_loss:0.257, val_acc:0.964]
Epoch [59/120    avg_loss:0.195, val_acc:0.984]
Epoch [60/120    avg_loss:0.189, val_acc:0.980]
Epoch [61/120    avg_loss:0.209, val_acc:0.956]
Epoch [62/120    avg_loss:0.214, val_acc:0.952]
Epoch [63/120    avg_loss:0.211, val_acc:0.966]
Epoch [64/120    avg_loss:0.218, val_acc:0.935]
Epoch [65/120    avg_loss:0.203, val_acc:0.968]
Epoch [66/120    avg_loss:0.183, val_acc:0.952]
Epoch [67/120    avg_loss:0.194, val_acc:0.952]
Epoch [68/120    avg_loss:0.159, val_acc:0.976]
Epoch [69/120    avg_loss:0.154, val_acc:0.980]
Epoch [70/120    avg_loss:0.304, val_acc:0.952]
Epoch [71/120    avg_loss:0.207, val_acc:0.992]
Epoch [72/120    avg_loss:0.175, val_acc:0.980]
Epoch [73/120    avg_loss:0.133, val_acc:0.974]
Epoch [74/120    avg_loss:0.169, val_acc:0.950]
Epoch [75/120    avg_loss:0.225, val_acc:0.958]
Epoch [76/120    avg_loss:0.197, val_acc:0.964]
Epoch [77/120    avg_loss:0.126, val_acc:0.992]
Epoch [78/120    avg_loss:0.129, val_acc:0.970]
Epoch [79/120    avg_loss:0.143, val_acc:0.988]
Epoch [80/120    avg_loss:0.106, val_acc:0.986]
Epoch [81/120    avg_loss:0.111, val_acc:0.986]
Epoch [82/120    avg_loss:0.109, val_acc:0.960]
Epoch [83/120    avg_loss:0.155, val_acc:0.984]
Epoch [84/120    avg_loss:0.133, val_acc:0.984]
Epoch [85/120    avg_loss:0.107, val_acc:0.992]
Epoch [86/120    avg_loss:0.116, val_acc:0.990]
Epoch [87/120    avg_loss:0.117, val_acc:0.986]
Epoch [88/120    avg_loss:0.136, val_acc:0.978]
Epoch [89/120    avg_loss:0.082, val_acc:0.986]
Epoch [90/120    avg_loss:0.083, val_acc:0.982]
Epoch [91/120    avg_loss:0.113, val_acc:0.968]
Epoch [92/120    avg_loss:0.079, val_acc:0.988]
Epoch [93/120    avg_loss:0.080, val_acc:0.990]
Epoch [94/120    avg_loss:0.071, val_acc:0.990]
Epoch [95/120    avg_loss:0.063, val_acc:0.990]
Epoch [96/120    avg_loss:0.076, val_acc:0.982]
Epoch [97/120    avg_loss:0.070, val_acc:0.990]
Epoch [98/120    avg_loss:0.068, val_acc:0.990]
Epoch [99/120    avg_loss:0.062, val_acc:0.992]
Epoch [100/120    avg_loss:0.042, val_acc:0.990]
Epoch [101/120    avg_loss:0.046, val_acc:0.990]
Epoch [102/120    avg_loss:0.049, val_acc:0.992]
Epoch [103/120    avg_loss:0.046, val_acc:0.992]
Epoch [104/120    avg_loss:0.043, val_acc:0.992]
Epoch [105/120    avg_loss:0.043, val_acc:0.992]
Epoch [106/120    avg_loss:0.039, val_acc:0.992]
Epoch [107/120    avg_loss:0.047, val_acc:0.992]
Epoch [108/120    avg_loss:0.041, val_acc:0.994]
Epoch [109/120    avg_loss:0.039, val_acc:0.992]
Epoch [110/120    avg_loss:0.043, val_acc:0.994]
Epoch [111/120    avg_loss:0.037, val_acc:0.994]
Epoch [112/120    avg_loss:0.044, val_acc:0.994]
Epoch [113/120    avg_loss:0.046, val_acc:0.992]
Epoch [114/120    avg_loss:0.040, val_acc:0.990]
Epoch [115/120    avg_loss:0.047, val_acc:0.990]
Epoch [116/120    avg_loss:0.046, val_acc:0.992]
Epoch [117/120    avg_loss:0.050, val_acc:0.992]
Epoch [118/120    avg_loss:0.040, val_acc:0.992]
Epoch [119/120    avg_loss:0.043, val_acc:0.990]
Epoch [120/120    avg_loss:0.035, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.98206278 0.99563319 0.95412844 0.93851133
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9933535537407341
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe8b31fcef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.645, val_acc:0.050]
Epoch [2/120    avg_loss:2.604, val_acc:0.238]
Epoch [3/120    avg_loss:2.564, val_acc:0.310]
Epoch [4/120    avg_loss:2.525, val_acc:0.310]
Epoch [5/120    avg_loss:2.484, val_acc:0.312]
Epoch [6/120    avg_loss:2.446, val_acc:0.312]
Epoch [7/120    avg_loss:2.401, val_acc:0.312]
Epoch [8/120    avg_loss:2.362, val_acc:0.335]
Epoch [9/120    avg_loss:2.315, val_acc:0.433]
Epoch [10/120    avg_loss:2.271, val_acc:0.478]
Epoch [11/120    avg_loss:2.213, val_acc:0.502]
Epoch [12/120    avg_loss:2.152, val_acc:0.532]
Epoch [13/120    avg_loss:2.082, val_acc:0.550]
Epoch [14/120    avg_loss:2.011, val_acc:0.571]
Epoch [15/120    avg_loss:1.921, val_acc:0.583]
Epoch [16/120    avg_loss:1.850, val_acc:0.591]
Epoch [17/120    avg_loss:1.780, val_acc:0.599]
Epoch [18/120    avg_loss:1.704, val_acc:0.611]
Epoch [19/120    avg_loss:1.637, val_acc:0.593]
Epoch [20/120    avg_loss:1.520, val_acc:0.669]
Epoch [21/120    avg_loss:1.460, val_acc:0.655]
Epoch [22/120    avg_loss:1.355, val_acc:0.766]
Epoch [23/120    avg_loss:1.312, val_acc:0.688]
Epoch [24/120    avg_loss:1.242, val_acc:0.804]
Epoch [25/120    avg_loss:1.150, val_acc:0.720]
Epoch [26/120    avg_loss:1.087, val_acc:0.865]
Epoch [27/120    avg_loss:1.022, val_acc:0.859]
Epoch [28/120    avg_loss:0.906, val_acc:0.877]
Epoch [29/120    avg_loss:0.856, val_acc:0.889]
Epoch [30/120    avg_loss:0.870, val_acc:0.889]
Epoch [31/120    avg_loss:0.800, val_acc:0.883]
Epoch [32/120    avg_loss:0.727, val_acc:0.889]
Epoch [33/120    avg_loss:0.680, val_acc:0.905]
Epoch [34/120    avg_loss:0.630, val_acc:0.869]
Epoch [35/120    avg_loss:0.618, val_acc:0.935]
Epoch [36/120    avg_loss:0.613, val_acc:0.917]
Epoch [37/120    avg_loss:0.551, val_acc:0.915]
Epoch [38/120    avg_loss:0.498, val_acc:0.915]
Epoch [39/120    avg_loss:0.523, val_acc:0.929]
Epoch [40/120    avg_loss:0.533, val_acc:0.948]
Epoch [41/120    avg_loss:0.451, val_acc:0.925]
Epoch [42/120    avg_loss:0.450, val_acc:0.948]
Epoch [43/120    avg_loss:0.410, val_acc:0.935]
Epoch [44/120    avg_loss:0.378, val_acc:0.942]
Epoch [45/120    avg_loss:0.396, val_acc:0.948]
Epoch [46/120    avg_loss:0.345, val_acc:0.938]
Epoch [47/120    avg_loss:0.379, val_acc:0.938]
Epoch [48/120    avg_loss:0.341, val_acc:0.917]
Epoch [49/120    avg_loss:0.340, val_acc:0.946]
Epoch [50/120    avg_loss:0.291, val_acc:0.940]
Epoch [51/120    avg_loss:0.309, val_acc:0.927]
Epoch [52/120    avg_loss:0.354, val_acc:0.942]
Epoch [53/120    avg_loss:0.314, val_acc:0.887]
Epoch [54/120    avg_loss:0.402, val_acc:0.889]
Epoch [55/120    avg_loss:0.384, val_acc:0.919]
Epoch [56/120    avg_loss:0.329, val_acc:0.962]
Epoch [57/120    avg_loss:0.275, val_acc:0.954]
Epoch [58/120    avg_loss:0.245, val_acc:0.950]
Epoch [59/120    avg_loss:0.263, val_acc:0.942]
Epoch [60/120    avg_loss:0.269, val_acc:0.956]
Epoch [61/120    avg_loss:0.204, val_acc:0.968]
Epoch [62/120    avg_loss:0.200, val_acc:0.968]
Epoch [63/120    avg_loss:0.205, val_acc:0.968]
Epoch [64/120    avg_loss:0.174, val_acc:0.970]
Epoch [65/120    avg_loss:0.193, val_acc:0.964]
Epoch [66/120    avg_loss:0.204, val_acc:0.960]
Epoch [67/120    avg_loss:0.195, val_acc:0.970]
Epoch [68/120    avg_loss:0.176, val_acc:0.966]
Epoch [69/120    avg_loss:0.189, val_acc:0.925]
Epoch [70/120    avg_loss:0.191, val_acc:0.974]
Epoch [71/120    avg_loss:0.200, val_acc:0.960]
Epoch [72/120    avg_loss:0.159, val_acc:0.982]
Epoch [73/120    avg_loss:0.137, val_acc:0.978]
Epoch [74/120    avg_loss:0.143, val_acc:0.970]
Epoch [75/120    avg_loss:0.125, val_acc:0.978]
Epoch [76/120    avg_loss:0.102, val_acc:0.978]
Epoch [77/120    avg_loss:0.103, val_acc:0.980]
Epoch [78/120    avg_loss:0.114, val_acc:0.978]
Epoch [79/120    avg_loss:0.143, val_acc:0.976]
Epoch [80/120    avg_loss:0.121, val_acc:0.978]
Epoch [81/120    avg_loss:0.121, val_acc:0.974]
Epoch [82/120    avg_loss:0.099, val_acc:0.982]
Epoch [83/120    avg_loss:0.109, val_acc:0.970]
Epoch [84/120    avg_loss:0.108, val_acc:0.984]
Epoch [85/120    avg_loss:0.117, val_acc:0.946]
Epoch [86/120    avg_loss:0.168, val_acc:0.937]
Epoch [87/120    avg_loss:0.133, val_acc:0.968]
Epoch [88/120    avg_loss:0.102, val_acc:0.982]
Epoch [89/120    avg_loss:0.078, val_acc:0.982]
Epoch [90/120    avg_loss:0.166, val_acc:0.940]
Epoch [91/120    avg_loss:0.168, val_acc:0.962]
Epoch [92/120    avg_loss:0.119, val_acc:0.980]
Epoch [93/120    avg_loss:0.081, val_acc:0.984]
Epoch [94/120    avg_loss:0.101, val_acc:0.980]
Epoch [95/120    avg_loss:0.075, val_acc:0.972]
Epoch [96/120    avg_loss:0.065, val_acc:0.970]
Epoch [97/120    avg_loss:0.093, val_acc:0.978]
Epoch [98/120    avg_loss:0.083, val_acc:0.972]
Epoch [99/120    avg_loss:0.164, val_acc:0.966]
Epoch [100/120    avg_loss:0.146, val_acc:0.974]
Epoch [101/120    avg_loss:0.127, val_acc:0.978]
Epoch [102/120    avg_loss:0.100, val_acc:0.980]
Epoch [103/120    avg_loss:0.087, val_acc:0.978]
Epoch [104/120    avg_loss:0.067, val_acc:0.978]
Epoch [105/120    avg_loss:0.089, val_acc:0.966]
Epoch [106/120    avg_loss:0.064, val_acc:0.982]
Epoch [107/120    avg_loss:0.063, val_acc:0.982]
Epoch [108/120    avg_loss:0.064, val_acc:0.982]
Epoch [109/120    avg_loss:0.047, val_acc:0.978]
Epoch [110/120    avg_loss:0.043, val_acc:0.980]
Epoch [111/120    avg_loss:0.058, val_acc:0.978]
Epoch [112/120    avg_loss:0.056, val_acc:0.978]
Epoch [113/120    avg_loss:0.043, val_acc:0.980]
Epoch [114/120    avg_loss:0.042, val_acc:0.978]
Epoch [115/120    avg_loss:0.046, val_acc:0.980]
Epoch [116/120    avg_loss:0.046, val_acc:0.980]
Epoch [117/120    avg_loss:0.051, val_acc:0.978]
Epoch [118/120    avg_loss:0.039, val_acc:0.978]
Epoch [119/120    avg_loss:0.051, val_acc:0.980]
Epoch [120/120    avg_loss:0.041, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.97986577 0.99122807 0.93569845 0.91582492
 1.         0.95555556 0.99870968 1.         1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9893179897048805
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89128ffdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.647, val_acc:0.110]
Epoch [2/120    avg_loss:2.595, val_acc:0.210]
Epoch [3/120    avg_loss:2.543, val_acc:0.360]
Epoch [4/120    avg_loss:2.492, val_acc:0.352]
Epoch [5/120    avg_loss:2.449, val_acc:0.340]
Epoch [6/120    avg_loss:2.414, val_acc:0.331]
Epoch [7/120    avg_loss:2.375, val_acc:0.333]
Epoch [8/120    avg_loss:2.344, val_acc:0.371]
Epoch [9/120    avg_loss:2.302, val_acc:0.402]
Epoch [10/120    avg_loss:2.265, val_acc:0.448]
Epoch [11/120    avg_loss:2.228, val_acc:0.444]
Epoch [12/120    avg_loss:2.181, val_acc:0.440]
Epoch [13/120    avg_loss:2.142, val_acc:0.440]
Epoch [14/120    avg_loss:2.083, val_acc:0.454]
Epoch [15/120    avg_loss:2.034, val_acc:0.496]
Epoch [16/120    avg_loss:1.991, val_acc:0.508]
Epoch [17/120    avg_loss:1.907, val_acc:0.506]
Epoch [18/120    avg_loss:1.856, val_acc:0.500]
Epoch [19/120    avg_loss:1.778, val_acc:0.558]
Epoch [20/120    avg_loss:1.684, val_acc:0.629]
Epoch [21/120    avg_loss:1.628, val_acc:0.650]
Epoch [22/120    avg_loss:1.567, val_acc:0.698]
Epoch [23/120    avg_loss:1.490, val_acc:0.781]
Epoch [24/120    avg_loss:1.436, val_acc:0.792]
Epoch [25/120    avg_loss:1.319, val_acc:0.840]
Epoch [26/120    avg_loss:1.255, val_acc:0.817]
Epoch [27/120    avg_loss:1.193, val_acc:0.838]
Epoch [28/120    avg_loss:1.089, val_acc:0.865]
Epoch [29/120    avg_loss:1.044, val_acc:0.852]
Epoch [30/120    avg_loss:0.979, val_acc:0.871]
Epoch [31/120    avg_loss:0.927, val_acc:0.863]
Epoch [32/120    avg_loss:0.867, val_acc:0.838]
Epoch [33/120    avg_loss:0.835, val_acc:0.902]
Epoch [34/120    avg_loss:0.719, val_acc:0.885]
Epoch [35/120    avg_loss:0.707, val_acc:0.881]
Epoch [36/120    avg_loss:0.635, val_acc:0.900]
Epoch [37/120    avg_loss:0.637, val_acc:0.892]
Epoch [38/120    avg_loss:0.657, val_acc:0.904]
Epoch [39/120    avg_loss:0.647, val_acc:0.896]
Epoch [40/120    avg_loss:0.591, val_acc:0.923]
Epoch [41/120    avg_loss:0.475, val_acc:0.887]
Epoch [42/120    avg_loss:0.512, val_acc:0.927]
Epoch [43/120    avg_loss:0.442, val_acc:0.912]
Epoch [44/120    avg_loss:0.414, val_acc:0.917]
Epoch [45/120    avg_loss:0.408, val_acc:0.896]
Epoch [46/120    avg_loss:0.420, val_acc:0.919]
Epoch [47/120    avg_loss:0.406, val_acc:0.931]
Epoch [48/120    avg_loss:0.408, val_acc:0.929]
Epoch [49/120    avg_loss:0.374, val_acc:0.921]
Epoch [50/120    avg_loss:0.371, val_acc:0.946]
Epoch [51/120    avg_loss:0.350, val_acc:0.917]
Epoch [52/120    avg_loss:0.347, val_acc:0.921]
Epoch [53/120    avg_loss:0.382, val_acc:0.925]
Epoch [54/120    avg_loss:0.362, val_acc:0.921]
Epoch [55/120    avg_loss:0.340, val_acc:0.923]
Epoch [56/120    avg_loss:0.299, val_acc:0.923]
Epoch [57/120    avg_loss:0.313, val_acc:0.921]
Epoch [58/120    avg_loss:0.314, val_acc:0.938]
Epoch [59/120    avg_loss:0.269, val_acc:0.927]
Epoch [60/120    avg_loss:0.275, val_acc:0.940]
Epoch [61/120    avg_loss:0.331, val_acc:0.942]
Epoch [62/120    avg_loss:0.266, val_acc:0.950]
Epoch [63/120    avg_loss:0.223, val_acc:0.967]
Epoch [64/120    avg_loss:0.209, val_acc:0.965]
Epoch [65/120    avg_loss:0.219, val_acc:0.975]
Epoch [66/120    avg_loss:0.177, val_acc:0.956]
Epoch [67/120    avg_loss:0.241, val_acc:0.956]
Epoch [68/120    avg_loss:0.201, val_acc:0.960]
Epoch [69/120    avg_loss:0.190, val_acc:0.931]
Epoch [70/120    avg_loss:0.252, val_acc:0.948]
Epoch [71/120    avg_loss:0.217, val_acc:0.973]
Epoch [72/120    avg_loss:0.168, val_acc:0.965]
Epoch [73/120    avg_loss:0.200, val_acc:0.977]
Epoch [74/120    avg_loss:0.210, val_acc:0.954]
Epoch [75/120    avg_loss:0.233, val_acc:0.967]
Epoch [76/120    avg_loss:0.164, val_acc:0.969]
Epoch [77/120    avg_loss:0.194, val_acc:0.960]
Epoch [78/120    avg_loss:0.179, val_acc:0.954]
Epoch [79/120    avg_loss:0.192, val_acc:0.950]
Epoch [80/120    avg_loss:0.177, val_acc:0.981]
Epoch [81/120    avg_loss:0.183, val_acc:0.944]
Epoch [82/120    avg_loss:0.200, val_acc:0.963]
Epoch [83/120    avg_loss:0.220, val_acc:0.960]
Epoch [84/120    avg_loss:0.204, val_acc:0.954]
Epoch [85/120    avg_loss:0.204, val_acc:0.956]
Epoch [86/120    avg_loss:0.161, val_acc:0.979]
Epoch [87/120    avg_loss:0.126, val_acc:0.975]
Epoch [88/120    avg_loss:0.113, val_acc:0.975]
Epoch [89/120    avg_loss:0.119, val_acc:0.981]
Epoch [90/120    avg_loss:0.126, val_acc:0.979]
Epoch [91/120    avg_loss:0.126, val_acc:0.990]
Epoch [92/120    avg_loss:0.130, val_acc:0.985]
Epoch [93/120    avg_loss:0.101, val_acc:0.979]
Epoch [94/120    avg_loss:0.100, val_acc:0.977]
Epoch [95/120    avg_loss:0.096, val_acc:0.971]
Epoch [96/120    avg_loss:0.138, val_acc:0.975]
Epoch [97/120    avg_loss:0.132, val_acc:0.965]
Epoch [98/120    avg_loss:0.126, val_acc:0.969]
Epoch [99/120    avg_loss:0.114, val_acc:0.990]
Epoch [100/120    avg_loss:0.093, val_acc:0.979]
Epoch [101/120    avg_loss:0.083, val_acc:0.983]
Epoch [102/120    avg_loss:0.085, val_acc:0.990]
Epoch [103/120    avg_loss:0.065, val_acc:0.985]
Epoch [104/120    avg_loss:0.087, val_acc:0.992]
Epoch [105/120    avg_loss:0.065, val_acc:0.988]
Epoch [106/120    avg_loss:0.066, val_acc:0.990]
Epoch [107/120    avg_loss:0.058, val_acc:0.990]
Epoch [108/120    avg_loss:0.057, val_acc:0.994]
Epoch [109/120    avg_loss:0.058, val_acc:0.971]
Epoch [110/120    avg_loss:0.078, val_acc:0.971]
Epoch [111/120    avg_loss:0.108, val_acc:0.985]
Epoch [112/120    avg_loss:0.071, val_acc:0.990]
Epoch [113/120    avg_loss:0.057, val_acc:0.994]
Epoch [114/120    avg_loss:0.057, val_acc:0.988]
Epoch [115/120    avg_loss:0.041, val_acc:0.994]
Epoch [116/120    avg_loss:0.048, val_acc:0.990]
Epoch [117/120    avg_loss:0.038, val_acc:0.996]
Epoch [118/120    avg_loss:0.031, val_acc:0.994]
Epoch [119/120    avg_loss:0.039, val_acc:0.996]
Epoch [120/120    avg_loss:0.041, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 213  16   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  30 115   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         1.         0.96162528 0.8968254  0.88122605
 0.98771499 1.         1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.987417080524762
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb40ba11da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.651, val_acc:0.052]
Epoch [2/120    avg_loss:2.616, val_acc:0.113]
Epoch [3/120    avg_loss:2.587, val_acc:0.133]
Epoch [4/120    avg_loss:2.550, val_acc:0.235]
Epoch [5/120    avg_loss:2.514, val_acc:0.308]
Epoch [6/120    avg_loss:2.483, val_acc:0.325]
Epoch [7/120    avg_loss:2.441, val_acc:0.327]
Epoch [8/120    avg_loss:2.397, val_acc:0.356]
Epoch [9/120    avg_loss:2.350, val_acc:0.415]
Epoch [10/120    avg_loss:2.301, val_acc:0.535]
Epoch [11/120    avg_loss:2.242, val_acc:0.552]
Epoch [12/120    avg_loss:2.194, val_acc:0.583]
Epoch [13/120    avg_loss:2.131, val_acc:0.594]
Epoch [14/120    avg_loss:2.061, val_acc:0.629]
Epoch [15/120    avg_loss:1.994, val_acc:0.656]
Epoch [16/120    avg_loss:1.881, val_acc:0.692]
Epoch [17/120    avg_loss:1.788, val_acc:0.729]
Epoch [18/120    avg_loss:1.688, val_acc:0.715]
Epoch [19/120    avg_loss:1.594, val_acc:0.731]
Epoch [20/120    avg_loss:1.495, val_acc:0.704]
Epoch [21/120    avg_loss:1.424, val_acc:0.740]
Epoch [22/120    avg_loss:1.349, val_acc:0.769]
Epoch [23/120    avg_loss:1.252, val_acc:0.744]
Epoch [24/120    avg_loss:1.112, val_acc:0.827]
Epoch [25/120    avg_loss:1.024, val_acc:0.815]
Epoch [26/120    avg_loss:0.970, val_acc:0.790]
Epoch [27/120    avg_loss:0.922, val_acc:0.756]
Epoch [28/120    avg_loss:0.838, val_acc:0.850]
Epoch [29/120    avg_loss:0.795, val_acc:0.854]
Epoch [30/120    avg_loss:0.735, val_acc:0.906]
Epoch [31/120    avg_loss:0.687, val_acc:0.846]
Epoch [32/120    avg_loss:0.686, val_acc:0.875]
Epoch [33/120    avg_loss:0.617, val_acc:0.883]
Epoch [34/120    avg_loss:0.576, val_acc:0.912]
Epoch [35/120    avg_loss:0.527, val_acc:0.915]
Epoch [36/120    avg_loss:0.544, val_acc:0.883]
Epoch [37/120    avg_loss:0.521, val_acc:0.906]
Epoch [38/120    avg_loss:0.539, val_acc:0.940]
Epoch [39/120    avg_loss:0.436, val_acc:0.944]
Epoch [40/120    avg_loss:0.399, val_acc:0.933]
Epoch [41/120    avg_loss:0.391, val_acc:0.935]
Epoch [42/120    avg_loss:0.358, val_acc:0.935]
Epoch [43/120    avg_loss:0.340, val_acc:0.956]
Epoch [44/120    avg_loss:0.325, val_acc:0.944]
Epoch [45/120    avg_loss:0.313, val_acc:0.948]
Epoch [46/120    avg_loss:0.378, val_acc:0.948]
Epoch [47/120    avg_loss:0.304, val_acc:0.948]
Epoch [48/120    avg_loss:0.353, val_acc:0.898]
Epoch [49/120    avg_loss:0.269, val_acc:0.956]
Epoch [50/120    avg_loss:0.270, val_acc:0.940]
Epoch [51/120    avg_loss:0.261, val_acc:0.946]
Epoch [52/120    avg_loss:0.247, val_acc:0.950]
Epoch [53/120    avg_loss:0.281, val_acc:0.948]
Epoch [54/120    avg_loss:0.259, val_acc:0.950]
Epoch [55/120    avg_loss:0.224, val_acc:0.958]
Epoch [56/120    avg_loss:0.226, val_acc:0.956]
Epoch [57/120    avg_loss:0.214, val_acc:0.956]
Epoch [58/120    avg_loss:0.239, val_acc:0.944]
Epoch [59/120    avg_loss:0.208, val_acc:0.960]
Epoch [60/120    avg_loss:0.196, val_acc:0.925]
Epoch [61/120    avg_loss:0.300, val_acc:0.952]
Epoch [62/120    avg_loss:0.294, val_acc:0.960]
Epoch [63/120    avg_loss:0.271, val_acc:0.946]
Epoch [64/120    avg_loss:0.223, val_acc:0.950]
Epoch [65/120    avg_loss:0.200, val_acc:0.960]
Epoch [66/120    avg_loss:0.187, val_acc:0.956]
Epoch [67/120    avg_loss:0.240, val_acc:0.940]
Epoch [68/120    avg_loss:0.278, val_acc:0.915]
Epoch [69/120    avg_loss:0.263, val_acc:0.960]
Epoch [70/120    avg_loss:0.223, val_acc:0.963]
Epoch [71/120    avg_loss:0.155, val_acc:0.965]
Epoch [72/120    avg_loss:0.138, val_acc:0.954]
Epoch [73/120    avg_loss:0.112, val_acc:0.950]
Epoch [74/120    avg_loss:0.128, val_acc:0.967]
Epoch [75/120    avg_loss:0.103, val_acc:0.954]
Epoch [76/120    avg_loss:0.127, val_acc:0.965]
Epoch [77/120    avg_loss:0.130, val_acc:0.967]
Epoch [78/120    avg_loss:0.098, val_acc:0.960]
Epoch [79/120    avg_loss:0.128, val_acc:0.958]
Epoch [80/120    avg_loss:0.173, val_acc:0.975]
Epoch [81/120    avg_loss:0.109, val_acc:0.960]
Epoch [82/120    avg_loss:0.126, val_acc:0.969]
Epoch [83/120    avg_loss:0.155, val_acc:0.954]
Epoch [84/120    avg_loss:0.131, val_acc:0.985]
Epoch [85/120    avg_loss:0.123, val_acc:0.963]
Epoch [86/120    avg_loss:0.108, val_acc:0.971]
Epoch [87/120    avg_loss:0.120, val_acc:0.973]
Epoch [88/120    avg_loss:0.090, val_acc:0.977]
Epoch [89/120    avg_loss:0.106, val_acc:0.956]
Epoch [90/120    avg_loss:0.095, val_acc:0.977]
Epoch [91/120    avg_loss:0.067, val_acc:0.973]
Epoch [92/120    avg_loss:0.100, val_acc:0.975]
Epoch [93/120    avg_loss:0.094, val_acc:0.969]
Epoch [94/120    avg_loss:0.070, val_acc:0.973]
Epoch [95/120    avg_loss:0.048, val_acc:0.973]
Epoch [96/120    avg_loss:0.061, val_acc:0.979]
Epoch [97/120    avg_loss:0.067, val_acc:0.965]
Epoch [98/120    avg_loss:0.052, val_acc:0.967]
Epoch [99/120    avg_loss:0.046, val_acc:0.975]
Epoch [100/120    avg_loss:0.051, val_acc:0.977]
Epoch [101/120    avg_loss:0.039, val_acc:0.975]
Epoch [102/120    avg_loss:0.040, val_acc:0.975]
Epoch [103/120    avg_loss:0.044, val_acc:0.977]
Epoch [104/120    avg_loss:0.039, val_acc:0.977]
Epoch [105/120    avg_loss:0.045, val_acc:0.977]
Epoch [106/120    avg_loss:0.044, val_acc:0.979]
Epoch [107/120    avg_loss:0.038, val_acc:0.979]
Epoch [108/120    avg_loss:0.043, val_acc:0.977]
Epoch [109/120    avg_loss:0.034, val_acc:0.977]
Epoch [110/120    avg_loss:0.034, val_acc:0.977]
Epoch [111/120    avg_loss:0.036, val_acc:0.977]
Epoch [112/120    avg_loss:0.037, val_acc:0.977]
Epoch [113/120    avg_loss:0.037, val_acc:0.977]
Epoch [114/120    avg_loss:0.039, val_acc:0.977]
Epoch [115/120    avg_loss:0.041, val_acc:0.977]
Epoch [116/120    avg_loss:0.032, val_acc:0.977]
Epoch [117/120    avg_loss:0.039, val_acc:0.977]
Epoch [118/120    avg_loss:0.035, val_acc:0.977]
Epoch [119/120    avg_loss:0.031, val_acc:0.977]
Epoch [120/120    avg_loss:0.036, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 214   5   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99926954 0.98165138 0.98488121 0.93186813 0.90034364
 0.99757869 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9905048076497238
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff171cf3e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.148]
Epoch [2/120    avg_loss:2.591, val_acc:0.152]
Epoch [3/120    avg_loss:2.566, val_acc:0.171]
Epoch [4/120    avg_loss:2.536, val_acc:0.256]
Epoch [5/120    avg_loss:2.512, val_acc:0.346]
Epoch [6/120    avg_loss:2.478, val_acc:0.362]
Epoch [7/120    avg_loss:2.450, val_acc:0.394]
Epoch [8/120    avg_loss:2.411, val_acc:0.431]
Epoch [9/120    avg_loss:2.377, val_acc:0.465]
Epoch [10/120    avg_loss:2.319, val_acc:0.537]
Epoch [11/120    avg_loss:2.280, val_acc:0.515]
Epoch [12/120    avg_loss:2.239, val_acc:0.567]
Epoch [13/120    avg_loss:2.191, val_acc:0.569]
Epoch [14/120    avg_loss:2.136, val_acc:0.610]
Epoch [15/120    avg_loss:2.081, val_acc:0.658]
Epoch [16/120    avg_loss:2.025, val_acc:0.633]
Epoch [17/120    avg_loss:1.921, val_acc:0.681]
Epoch [18/120    avg_loss:1.874, val_acc:0.696]
Epoch [19/120    avg_loss:1.785, val_acc:0.706]
Epoch [20/120    avg_loss:1.713, val_acc:0.704]
Epoch [21/120    avg_loss:1.647, val_acc:0.692]
Epoch [22/120    avg_loss:1.565, val_acc:0.692]
Epoch [23/120    avg_loss:1.477, val_acc:0.702]
Epoch [24/120    avg_loss:1.399, val_acc:0.740]
Epoch [25/120    avg_loss:1.276, val_acc:0.723]
Epoch [26/120    avg_loss:1.243, val_acc:0.760]
Epoch [27/120    avg_loss:1.173, val_acc:0.744]
Epoch [28/120    avg_loss:1.098, val_acc:0.750]
Epoch [29/120    avg_loss:1.015, val_acc:0.746]
Epoch [30/120    avg_loss:1.001, val_acc:0.777]
Epoch [31/120    avg_loss:0.915, val_acc:0.787]
Epoch [32/120    avg_loss:0.833, val_acc:0.806]
Epoch [33/120    avg_loss:0.800, val_acc:0.810]
Epoch [34/120    avg_loss:0.754, val_acc:0.787]
Epoch [35/120    avg_loss:0.760, val_acc:0.840]
Epoch [36/120    avg_loss:0.668, val_acc:0.890]
Epoch [37/120    avg_loss:0.655, val_acc:0.860]
Epoch [38/120    avg_loss:0.637, val_acc:0.894]
Epoch [39/120    avg_loss:0.551, val_acc:0.912]
Epoch [40/120    avg_loss:0.645, val_acc:0.808]
Epoch [41/120    avg_loss:0.656, val_acc:0.848]
Epoch [42/120    avg_loss:0.568, val_acc:0.923]
Epoch [43/120    avg_loss:0.552, val_acc:0.904]
Epoch [44/120    avg_loss:0.497, val_acc:0.917]
Epoch [45/120    avg_loss:0.420, val_acc:0.929]
Epoch [46/120    avg_loss:0.427, val_acc:0.904]
Epoch [47/120    avg_loss:0.456, val_acc:0.942]
Epoch [48/120    avg_loss:0.426, val_acc:0.919]
Epoch [49/120    avg_loss:0.426, val_acc:0.858]
Epoch [50/120    avg_loss:0.364, val_acc:0.948]
Epoch [51/120    avg_loss:0.374, val_acc:0.883]
Epoch [52/120    avg_loss:0.378, val_acc:0.935]
Epoch [53/120    avg_loss:0.337, val_acc:0.948]
Epoch [54/120    avg_loss:0.326, val_acc:0.956]
Epoch [55/120    avg_loss:0.408, val_acc:0.881]
Epoch [56/120    avg_loss:0.348, val_acc:0.950]
Epoch [57/120    avg_loss:0.420, val_acc:0.875]
Epoch [58/120    avg_loss:0.332, val_acc:0.948]
Epoch [59/120    avg_loss:0.294, val_acc:0.946]
Epoch [60/120    avg_loss:0.261, val_acc:0.950]
Epoch [61/120    avg_loss:0.319, val_acc:0.960]
Epoch [62/120    avg_loss:0.289, val_acc:0.935]
Epoch [63/120    avg_loss:0.258, val_acc:0.963]
Epoch [64/120    avg_loss:0.229, val_acc:0.946]
Epoch [65/120    avg_loss:0.290, val_acc:0.969]
Epoch [66/120    avg_loss:0.209, val_acc:0.969]
Epoch [67/120    avg_loss:0.215, val_acc:0.969]
Epoch [68/120    avg_loss:0.197, val_acc:0.969]
Epoch [69/120    avg_loss:0.229, val_acc:0.940]
Epoch [70/120    avg_loss:0.199, val_acc:0.950]
Epoch [71/120    avg_loss:0.186, val_acc:0.965]
Epoch [72/120    avg_loss:0.175, val_acc:0.979]
Epoch [73/120    avg_loss:0.154, val_acc:0.960]
Epoch [74/120    avg_loss:0.146, val_acc:0.952]
Epoch [75/120    avg_loss:0.192, val_acc:0.967]
Epoch [76/120    avg_loss:0.176, val_acc:0.990]
Epoch [77/120    avg_loss:0.168, val_acc:0.973]
Epoch [78/120    avg_loss:0.199, val_acc:0.965]
Epoch [79/120    avg_loss:0.173, val_acc:0.981]
Epoch [80/120    avg_loss:0.144, val_acc:0.988]
Epoch [81/120    avg_loss:0.157, val_acc:0.981]
Epoch [82/120    avg_loss:0.163, val_acc:0.977]
Epoch [83/120    avg_loss:0.162, val_acc:0.956]
Epoch [84/120    avg_loss:0.191, val_acc:0.958]
Epoch [85/120    avg_loss:0.183, val_acc:0.973]
Epoch [86/120    avg_loss:0.228, val_acc:0.967]
Epoch [87/120    avg_loss:0.173, val_acc:0.971]
Epoch [88/120    avg_loss:0.124, val_acc:0.963]
Epoch [89/120    avg_loss:0.132, val_acc:0.935]
Epoch [90/120    avg_loss:0.134, val_acc:0.975]
Epoch [91/120    avg_loss:0.107, val_acc:0.988]
Epoch [92/120    avg_loss:0.082, val_acc:0.988]
Epoch [93/120    avg_loss:0.081, val_acc:0.988]
Epoch [94/120    avg_loss:0.077, val_acc:0.990]
Epoch [95/120    avg_loss:0.079, val_acc:0.988]
Epoch [96/120    avg_loss:0.079, val_acc:0.988]
Epoch [97/120    avg_loss:0.090, val_acc:0.988]
Epoch [98/120    avg_loss:0.080, val_acc:0.988]
Epoch [99/120    avg_loss:0.076, val_acc:0.988]
Epoch [100/120    avg_loss:0.072, val_acc:0.988]
Epoch [101/120    avg_loss:0.072, val_acc:0.990]
Epoch [102/120    avg_loss:0.075, val_acc:0.990]
Epoch [103/120    avg_loss:0.078, val_acc:0.990]
Epoch [104/120    avg_loss:0.070, val_acc:0.990]
Epoch [105/120    avg_loss:0.108, val_acc:0.990]
Epoch [106/120    avg_loss:0.074, val_acc:0.990]
Epoch [107/120    avg_loss:0.077, val_acc:0.990]
Epoch [108/120    avg_loss:0.099, val_acc:0.990]
Epoch [109/120    avg_loss:0.076, val_acc:0.990]
Epoch [110/120    avg_loss:0.065, val_acc:0.992]
Epoch [111/120    avg_loss:0.062, val_acc:0.990]
Epoch [112/120    avg_loss:0.068, val_acc:0.990]
Epoch [113/120    avg_loss:0.060, val_acc:0.990]
Epoch [114/120    avg_loss:0.074, val_acc:0.988]
Epoch [115/120    avg_loss:0.058, val_acc:0.992]
Epoch [116/120    avg_loss:0.078, val_acc:0.992]
Epoch [117/120    avg_loss:0.067, val_acc:0.990]
Epoch [118/120    avg_loss:0.069, val_acc:0.990]
Epoch [119/120    avg_loss:0.057, val_acc:0.990]
Epoch [120/120    avg_loss:0.061, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  19   0   0   0   0   0   0   1   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.98871332 1.         0.93665158 0.910299
 1.         0.9726776  1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9921664322130155
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc57cde8e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.033]
Epoch [2/120    avg_loss:2.565, val_acc:0.098]
Epoch [3/120    avg_loss:2.522, val_acc:0.127]
Epoch [4/120    avg_loss:2.482, val_acc:0.290]
Epoch [5/120    avg_loss:2.442, val_acc:0.302]
Epoch [6/120    avg_loss:2.407, val_acc:0.302]
Epoch [7/120    avg_loss:2.364, val_acc:0.323]
Epoch [8/120    avg_loss:2.320, val_acc:0.325]
Epoch [9/120    avg_loss:2.280, val_acc:0.350]
Epoch [10/120    avg_loss:2.240, val_acc:0.398]
Epoch [11/120    avg_loss:2.189, val_acc:0.456]
Epoch [12/120    avg_loss:2.119, val_acc:0.504]
Epoch [13/120    avg_loss:2.050, val_acc:0.544]
Epoch [14/120    avg_loss:1.983, val_acc:0.571]
Epoch [15/120    avg_loss:1.903, val_acc:0.606]
Epoch [16/120    avg_loss:1.819, val_acc:0.610]
Epoch [17/120    avg_loss:1.773, val_acc:0.596]
Epoch [18/120    avg_loss:1.673, val_acc:0.646]
Epoch [19/120    avg_loss:1.581, val_acc:0.660]
Epoch [20/120    avg_loss:1.527, val_acc:0.644]
Epoch [21/120    avg_loss:1.428, val_acc:0.656]
Epoch [22/120    avg_loss:1.355, val_acc:0.675]
Epoch [23/120    avg_loss:1.300, val_acc:0.669]
Epoch [24/120    avg_loss:1.219, val_acc:0.733]
Epoch [25/120    avg_loss:1.113, val_acc:0.727]
Epoch [26/120    avg_loss:1.071, val_acc:0.802]
Epoch [27/120    avg_loss:1.009, val_acc:0.825]
Epoch [28/120    avg_loss:0.941, val_acc:0.879]
Epoch [29/120    avg_loss:0.819, val_acc:0.823]
Epoch [30/120    avg_loss:0.768, val_acc:0.904]
Epoch [31/120    avg_loss:0.766, val_acc:0.885]
Epoch [32/120    avg_loss:0.693, val_acc:0.929]
Epoch [33/120    avg_loss:0.641, val_acc:0.929]
Epoch [34/120    avg_loss:0.611, val_acc:0.927]
Epoch [35/120    avg_loss:0.539, val_acc:0.829]
Epoch [36/120    avg_loss:0.611, val_acc:0.919]
Epoch [37/120    avg_loss:0.517, val_acc:0.950]
Epoch [38/120    avg_loss:0.526, val_acc:0.929]
Epoch [39/120    avg_loss:0.453, val_acc:0.944]
Epoch [40/120    avg_loss:0.432, val_acc:0.944]
Epoch [41/120    avg_loss:0.479, val_acc:0.921]
Epoch [42/120    avg_loss:0.392, val_acc:0.919]
Epoch [43/120    avg_loss:0.334, val_acc:0.956]
Epoch [44/120    avg_loss:0.312, val_acc:0.960]
Epoch [45/120    avg_loss:0.359, val_acc:0.952]
Epoch [46/120    avg_loss:0.414, val_acc:0.894]
Epoch [47/120    avg_loss:0.359, val_acc:0.965]
Epoch [48/120    avg_loss:0.257, val_acc:0.969]
Epoch [49/120    avg_loss:0.274, val_acc:0.948]
Epoch [50/120    avg_loss:0.316, val_acc:0.956]
Epoch [51/120    avg_loss:0.250, val_acc:0.946]
Epoch [52/120    avg_loss:0.291, val_acc:0.973]
Epoch [53/120    avg_loss:0.256, val_acc:0.952]
Epoch [54/120    avg_loss:0.237, val_acc:0.940]
Epoch [55/120    avg_loss:0.243, val_acc:0.954]
Epoch [56/120    avg_loss:0.268, val_acc:0.942]
Epoch [57/120    avg_loss:0.211, val_acc:0.965]
Epoch [58/120    avg_loss:0.175, val_acc:0.973]
Epoch [59/120    avg_loss:0.178, val_acc:0.981]
Epoch [60/120    avg_loss:0.169, val_acc:0.960]
Epoch [61/120    avg_loss:0.222, val_acc:0.973]
Epoch [62/120    avg_loss:0.182, val_acc:0.956]
Epoch [63/120    avg_loss:0.162, val_acc:0.981]
Epoch [64/120    avg_loss:0.171, val_acc:0.981]
Epoch [65/120    avg_loss:0.114, val_acc:0.983]
Epoch [66/120    avg_loss:0.160, val_acc:0.942]
Epoch [67/120    avg_loss:0.188, val_acc:0.971]
Epoch [68/120    avg_loss:0.207, val_acc:0.942]
Epoch [69/120    avg_loss:0.196, val_acc:0.975]
Epoch [70/120    avg_loss:0.132, val_acc:0.973]
Epoch [71/120    avg_loss:0.155, val_acc:0.952]
Epoch [72/120    avg_loss:0.132, val_acc:0.985]
Epoch [73/120    avg_loss:0.101, val_acc:0.985]
Epoch [74/120    avg_loss:0.110, val_acc:0.988]
Epoch [75/120    avg_loss:0.079, val_acc:0.994]
Epoch [76/120    avg_loss:0.090, val_acc:0.990]
Epoch [77/120    avg_loss:0.088, val_acc:0.979]
Epoch [78/120    avg_loss:0.152, val_acc:0.969]
Epoch [79/120    avg_loss:0.093, val_acc:0.983]
Epoch [80/120    avg_loss:0.090, val_acc:0.983]
Epoch [81/120    avg_loss:0.074, val_acc:0.990]
Epoch [82/120    avg_loss:0.063, val_acc:0.994]
Epoch [83/120    avg_loss:0.058, val_acc:0.988]
Epoch [84/120    avg_loss:0.070, val_acc:0.988]
Epoch [85/120    avg_loss:0.102, val_acc:0.994]
Epoch [86/120    avg_loss:0.060, val_acc:0.994]
Epoch [87/120    avg_loss:0.052, val_acc:0.992]
Epoch [88/120    avg_loss:0.063, val_acc:0.973]
Epoch [89/120    avg_loss:0.129, val_acc:0.960]
Epoch [90/120    avg_loss:0.081, val_acc:0.996]
Epoch [91/120    avg_loss:0.064, val_acc:0.994]
Epoch [92/120    avg_loss:0.050, val_acc:0.994]
Epoch [93/120    avg_loss:0.045, val_acc:0.996]
Epoch [94/120    avg_loss:0.050, val_acc:0.981]
Epoch [95/120    avg_loss:0.053, val_acc:0.990]
Epoch [96/120    avg_loss:0.051, val_acc:0.983]
Epoch [97/120    avg_loss:0.079, val_acc:0.965]
Epoch [98/120    avg_loss:0.114, val_acc:0.994]
Epoch [99/120    avg_loss:0.058, val_acc:0.946]
Epoch [100/120    avg_loss:0.140, val_acc:0.977]
Epoch [101/120    avg_loss:0.069, val_acc:0.996]
Epoch [102/120    avg_loss:0.071, val_acc:0.979]
Epoch [103/120    avg_loss:0.094, val_acc:0.985]
Epoch [104/120    avg_loss:0.063, val_acc:0.996]
Epoch [105/120    avg_loss:0.046, val_acc:0.990]
Epoch [106/120    avg_loss:0.043, val_acc:0.985]
Epoch [107/120    avg_loss:0.039, val_acc:0.996]
Epoch [108/120    avg_loss:0.027, val_acc:0.990]
Epoch [109/120    avg_loss:0.041, val_acc:0.992]
Epoch [110/120    avg_loss:0.037, val_acc:0.998]
Epoch [111/120    avg_loss:0.030, val_acc:0.992]
Epoch [112/120    avg_loss:0.024, val_acc:0.996]
Epoch [113/120    avg_loss:0.030, val_acc:0.996]
Epoch [114/120    avg_loss:0.032, val_acc:0.994]
Epoch [115/120    avg_loss:0.026, val_acc:0.996]
Epoch [116/120    avg_loss:0.026, val_acc:0.979]
Epoch [117/120    avg_loss:0.060, val_acc:0.983]
Epoch [118/120    avg_loss:0.045, val_acc:0.992]
Epoch [119/120    avg_loss:0.037, val_acc:0.983]
Epoch [120/120    avg_loss:0.033, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  12   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99707174 0.99095023 1.         0.97052154 0.9602649
 0.99038462 0.97826087 1.         1.         1.         0.98820446
 0.98886414 1.        ]

Kappa:
0.9928795863662145
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa5463b0e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.383]
Epoch [2/120    avg_loss:2.567, val_acc:0.373]
Epoch [3/120    avg_loss:2.537, val_acc:0.390]
Epoch [4/120    avg_loss:2.505, val_acc:0.388]
Epoch [5/120    avg_loss:2.483, val_acc:0.381]
Epoch [6/120    avg_loss:2.455, val_acc:0.369]
Epoch [7/120    avg_loss:2.418, val_acc:0.377]
Epoch [8/120    avg_loss:2.403, val_acc:0.348]
Epoch [9/120    avg_loss:2.381, val_acc:0.335]
Epoch [10/120    avg_loss:2.348, val_acc:0.340]
Epoch [11/120    avg_loss:2.319, val_acc:0.362]
Epoch [12/120    avg_loss:2.291, val_acc:0.402]
Epoch [13/120    avg_loss:2.259, val_acc:0.419]
Epoch [14/120    avg_loss:2.235, val_acc:0.463]
Epoch [15/120    avg_loss:2.191, val_acc:0.485]
Epoch [16/120    avg_loss:2.151, val_acc:0.508]
Epoch [17/120    avg_loss:2.130, val_acc:0.533]
Epoch [18/120    avg_loss:2.098, val_acc:0.556]
Epoch [19/120    avg_loss:2.072, val_acc:0.558]
Epoch [20/120    avg_loss:2.028, val_acc:0.594]
Epoch [21/120    avg_loss:1.959, val_acc:0.594]
Epoch [22/120    avg_loss:1.902, val_acc:0.592]
Epoch [23/120    avg_loss:1.833, val_acc:0.621]
Epoch [24/120    avg_loss:1.755, val_acc:0.629]
Epoch [25/120    avg_loss:1.706, val_acc:0.656]
Epoch [26/120    avg_loss:1.595, val_acc:0.665]
Epoch [27/120    avg_loss:1.550, val_acc:0.688]
Epoch [28/120    avg_loss:1.502, val_acc:0.706]
Epoch [29/120    avg_loss:1.372, val_acc:0.738]
Epoch [30/120    avg_loss:1.306, val_acc:0.725]
Epoch [31/120    avg_loss:1.255, val_acc:0.742]
Epoch [32/120    avg_loss:1.203, val_acc:0.729]
Epoch [33/120    avg_loss:1.087, val_acc:0.758]
Epoch [34/120    avg_loss:1.030, val_acc:0.819]
Epoch [35/120    avg_loss:0.940, val_acc:0.896]
Epoch [36/120    avg_loss:0.901, val_acc:0.883]
Epoch [37/120    avg_loss:0.830, val_acc:0.892]
Epoch [38/120    avg_loss:0.761, val_acc:0.915]
Epoch [39/120    avg_loss:0.705, val_acc:0.912]
Epoch [40/120    avg_loss:0.639, val_acc:0.927]
Epoch [41/120    avg_loss:0.626, val_acc:0.919]
Epoch [42/120    avg_loss:0.572, val_acc:0.915]
Epoch [43/120    avg_loss:0.551, val_acc:0.950]
Epoch [44/120    avg_loss:0.544, val_acc:0.933]
Epoch [45/120    avg_loss:0.529, val_acc:0.940]
Epoch [46/120    avg_loss:0.514, val_acc:0.931]
Epoch [47/120    avg_loss:0.513, val_acc:0.917]
Epoch [48/120    avg_loss:0.439, val_acc:0.927]
Epoch [49/120    avg_loss:0.431, val_acc:0.921]
Epoch [50/120    avg_loss:0.399, val_acc:0.938]
Epoch [51/120    avg_loss:0.431, val_acc:0.910]
Epoch [52/120    avg_loss:0.432, val_acc:0.938]
Epoch [53/120    avg_loss:0.369, val_acc:0.912]
Epoch [54/120    avg_loss:0.327, val_acc:0.929]
Epoch [55/120    avg_loss:0.308, val_acc:0.929]
Epoch [56/120    avg_loss:0.420, val_acc:0.950]
Epoch [57/120    avg_loss:0.347, val_acc:0.952]
Epoch [58/120    avg_loss:0.275, val_acc:0.948]
Epoch [59/120    avg_loss:0.282, val_acc:0.956]
Epoch [60/120    avg_loss:0.241, val_acc:0.967]
Epoch [61/120    avg_loss:0.287, val_acc:0.954]
Epoch [62/120    avg_loss:0.295, val_acc:0.971]
Epoch [63/120    avg_loss:0.302, val_acc:0.958]
Epoch [64/120    avg_loss:0.311, val_acc:0.931]
Epoch [65/120    avg_loss:0.228, val_acc:0.944]
Epoch [66/120    avg_loss:0.334, val_acc:0.944]
Epoch [67/120    avg_loss:0.225, val_acc:0.956]
Epoch [68/120    avg_loss:0.234, val_acc:0.973]
Epoch [69/120    avg_loss:0.203, val_acc:0.954]
Epoch [70/120    avg_loss:0.214, val_acc:0.969]
Epoch [71/120    avg_loss:0.211, val_acc:0.965]
Epoch [72/120    avg_loss:0.178, val_acc:0.965]
Epoch [73/120    avg_loss:0.197, val_acc:0.938]
Epoch [74/120    avg_loss:0.213, val_acc:0.971]
Epoch [75/120    avg_loss:0.176, val_acc:0.979]
Epoch [76/120    avg_loss:0.190, val_acc:0.983]
Epoch [77/120    avg_loss:0.167, val_acc:0.973]
Epoch [78/120    avg_loss:0.129, val_acc:0.975]
Epoch [79/120    avg_loss:0.137, val_acc:0.983]
Epoch [80/120    avg_loss:0.124, val_acc:0.977]
Epoch [81/120    avg_loss:0.170, val_acc:0.981]
Epoch [82/120    avg_loss:0.177, val_acc:0.969]
Epoch [83/120    avg_loss:0.125, val_acc:0.988]
Epoch [84/120    avg_loss:0.126, val_acc:0.988]
Epoch [85/120    avg_loss:0.115, val_acc:0.992]
Epoch [86/120    avg_loss:0.120, val_acc:0.981]
Epoch [87/120    avg_loss:0.095, val_acc:0.981]
Epoch [88/120    avg_loss:0.102, val_acc:0.988]
Epoch [89/120    avg_loss:0.118, val_acc:0.983]
Epoch [90/120    avg_loss:0.102, val_acc:0.977]
Epoch [91/120    avg_loss:0.102, val_acc:0.979]
Epoch [92/120    avg_loss:0.144, val_acc:0.979]
Epoch [93/120    avg_loss:0.087, val_acc:0.992]
Epoch [94/120    avg_loss:0.103, val_acc:0.988]
Epoch [95/120    avg_loss:0.095, val_acc:0.979]
Epoch [96/120    avg_loss:0.091, val_acc:0.992]
Epoch [97/120    avg_loss:0.087, val_acc:0.988]
Epoch [98/120    avg_loss:0.095, val_acc:0.965]
Epoch [99/120    avg_loss:0.095, val_acc:0.983]
Epoch [100/120    avg_loss:0.074, val_acc:0.994]
Epoch [101/120    avg_loss:0.072, val_acc:0.988]
Epoch [102/120    avg_loss:0.055, val_acc:0.992]
Epoch [103/120    avg_loss:0.088, val_acc:0.992]
Epoch [104/120    avg_loss:0.071, val_acc:0.983]
Epoch [105/120    avg_loss:0.056, val_acc:0.990]
Epoch [106/120    avg_loss:0.071, val_acc:0.988]
Epoch [107/120    avg_loss:0.069, val_acc:0.985]
Epoch [108/120    avg_loss:0.052, val_acc:0.992]
Epoch [109/120    avg_loss:0.040, val_acc:0.994]
Epoch [110/120    avg_loss:0.064, val_acc:0.996]
Epoch [111/120    avg_loss:0.054, val_acc:0.994]
Epoch [112/120    avg_loss:0.085, val_acc:0.990]
Epoch [113/120    avg_loss:0.090, val_acc:0.988]
Epoch [114/120    avg_loss:0.098, val_acc:0.965]
Epoch [115/120    avg_loss:0.082, val_acc:0.985]
Epoch [116/120    avg_loss:0.070, val_acc:0.988]
Epoch [117/120    avg_loss:0.061, val_acc:0.981]
Epoch [118/120    avg_loss:0.064, val_acc:0.990]
Epoch [119/120    avg_loss:0.043, val_acc:0.990]
Epoch [120/120    avg_loss:0.031, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   6   0   0   0   0   0   0   2   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99780541 0.98648649 0.99122807 0.96263736 0.96219931
 0.99277108 0.96703297 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9938281849493655
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb383a17da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.667, val_acc:0.004]
Epoch [2/120    avg_loss:2.627, val_acc:0.090]
Epoch [3/120    avg_loss:2.590, val_acc:0.165]
Epoch [4/120    avg_loss:2.551, val_acc:0.298]
Epoch [5/120    avg_loss:2.519, val_acc:0.300]
Epoch [6/120    avg_loss:2.469, val_acc:0.300]
Epoch [7/120    avg_loss:2.434, val_acc:0.300]
Epoch [8/120    avg_loss:2.391, val_acc:0.302]
Epoch [9/120    avg_loss:2.333, val_acc:0.327]
Epoch [10/120    avg_loss:2.270, val_acc:0.315]
Epoch [11/120    avg_loss:2.225, val_acc:0.306]
Epoch [12/120    avg_loss:2.164, val_acc:0.321]
Epoch [13/120    avg_loss:2.108, val_acc:0.350]
Epoch [14/120    avg_loss:2.058, val_acc:0.423]
Epoch [15/120    avg_loss:1.977, val_acc:0.508]
Epoch [16/120    avg_loss:1.911, val_acc:0.579]
Epoch [17/120    avg_loss:1.813, val_acc:0.627]
Epoch [18/120    avg_loss:1.733, val_acc:0.617]
Epoch [19/120    avg_loss:1.640, val_acc:0.662]
Epoch [20/120    avg_loss:1.539, val_acc:0.660]
Epoch [21/120    avg_loss:1.444, val_acc:0.688]
Epoch [22/120    avg_loss:1.375, val_acc:0.723]
Epoch [23/120    avg_loss:1.276, val_acc:0.694]
Epoch [24/120    avg_loss:1.185, val_acc:0.752]
Epoch [25/120    avg_loss:1.121, val_acc:0.721]
Epoch [26/120    avg_loss:1.081, val_acc:0.715]
Epoch [27/120    avg_loss:0.960, val_acc:0.760]
Epoch [28/120    avg_loss:0.921, val_acc:0.812]
Epoch [29/120    avg_loss:0.887, val_acc:0.835]
Epoch [30/120    avg_loss:0.849, val_acc:0.877]
Epoch [31/120    avg_loss:0.771, val_acc:0.896]
Epoch [32/120    avg_loss:0.703, val_acc:0.887]
Epoch [33/120    avg_loss:0.676, val_acc:0.890]
Epoch [34/120    avg_loss:0.672, val_acc:0.902]
Epoch [35/120    avg_loss:0.591, val_acc:0.906]
Epoch [36/120    avg_loss:0.538, val_acc:0.902]
Epoch [37/120    avg_loss:0.515, val_acc:0.919]
Epoch [38/120    avg_loss:0.521, val_acc:0.919]
Epoch [39/120    avg_loss:0.489, val_acc:0.923]
Epoch [40/120    avg_loss:0.450, val_acc:0.921]
Epoch [41/120    avg_loss:0.437, val_acc:0.919]
Epoch [42/120    avg_loss:0.406, val_acc:0.917]
Epoch [43/120    avg_loss:0.490, val_acc:0.919]
Epoch [44/120    avg_loss:0.453, val_acc:0.908]
Epoch [45/120    avg_loss:0.365, val_acc:0.931]
Epoch [46/120    avg_loss:0.382, val_acc:0.942]
Epoch [47/120    avg_loss:0.400, val_acc:0.894]
Epoch [48/120    avg_loss:0.413, val_acc:0.940]
Epoch [49/120    avg_loss:0.392, val_acc:0.927]
Epoch [50/120    avg_loss:0.328, val_acc:0.950]
Epoch [51/120    avg_loss:0.286, val_acc:0.938]
Epoch [52/120    avg_loss:0.289, val_acc:0.938]
Epoch [53/120    avg_loss:0.277, val_acc:0.938]
Epoch [54/120    avg_loss:0.283, val_acc:0.925]
Epoch [55/120    avg_loss:0.250, val_acc:0.950]
Epoch [56/120    avg_loss:0.242, val_acc:0.963]
Epoch [57/120    avg_loss:0.243, val_acc:0.952]
Epoch [58/120    avg_loss:0.222, val_acc:0.950]
Epoch [59/120    avg_loss:0.208, val_acc:0.958]
Epoch [60/120    avg_loss:0.187, val_acc:0.967]
Epoch [61/120    avg_loss:0.183, val_acc:0.981]
Epoch [62/120    avg_loss:0.224, val_acc:0.967]
Epoch [63/120    avg_loss:0.192, val_acc:0.958]
Epoch [64/120    avg_loss:0.182, val_acc:0.960]
Epoch [65/120    avg_loss:0.177, val_acc:0.967]
Epoch [66/120    avg_loss:0.199, val_acc:0.944]
Epoch [67/120    avg_loss:0.222, val_acc:0.960]
Epoch [68/120    avg_loss:0.228, val_acc:0.965]
Epoch [69/120    avg_loss:0.184, val_acc:0.942]
Epoch [70/120    avg_loss:0.172, val_acc:0.973]
Epoch [71/120    avg_loss:0.165, val_acc:0.979]
Epoch [72/120    avg_loss:0.162, val_acc:0.969]
Epoch [73/120    avg_loss:0.136, val_acc:0.973]
Epoch [74/120    avg_loss:0.132, val_acc:0.981]
Epoch [75/120    avg_loss:0.166, val_acc:0.954]
Epoch [76/120    avg_loss:0.132, val_acc:0.965]
Epoch [77/120    avg_loss:0.130, val_acc:0.977]
Epoch [78/120    avg_loss:0.130, val_acc:0.965]
Epoch [79/120    avg_loss:0.135, val_acc:0.979]
Epoch [80/120    avg_loss:0.115, val_acc:0.967]
Epoch [81/120    avg_loss:0.104, val_acc:0.975]
Epoch [82/120    avg_loss:0.083, val_acc:0.973]
Epoch [83/120    avg_loss:0.096, val_acc:0.977]
Epoch [84/120    avg_loss:0.093, val_acc:0.975]
Epoch [85/120    avg_loss:0.090, val_acc:0.973]
Epoch [86/120    avg_loss:0.077, val_acc:0.952]
Epoch [87/120    avg_loss:0.086, val_acc:0.985]
Epoch [88/120    avg_loss:0.082, val_acc:0.973]
Epoch [89/120    avg_loss:0.084, val_acc:0.979]
Epoch [90/120    avg_loss:0.081, val_acc:0.977]
Epoch [91/120    avg_loss:0.075, val_acc:0.977]
Epoch [92/120    avg_loss:0.074, val_acc:0.977]
Epoch [93/120    avg_loss:0.087, val_acc:0.979]
Epoch [94/120    avg_loss:0.139, val_acc:0.971]
Epoch [95/120    avg_loss:0.130, val_acc:0.973]
Epoch [96/120    avg_loss:0.106, val_acc:0.979]
Epoch [97/120    avg_loss:0.069, val_acc:0.985]
Epoch [98/120    avg_loss:0.077, val_acc:0.979]
Epoch [99/120    avg_loss:0.065, val_acc:0.988]
Epoch [100/120    avg_loss:0.055, val_acc:0.973]
Epoch [101/120    avg_loss:0.066, val_acc:0.985]
Epoch [102/120    avg_loss:0.058, val_acc:0.977]
Epoch [103/120    avg_loss:0.090, val_acc:0.983]
Epoch [104/120    avg_loss:0.087, val_acc:0.975]
Epoch [105/120    avg_loss:0.061, val_acc:0.981]
Epoch [106/120    avg_loss:0.045, val_acc:0.981]
Epoch [107/120    avg_loss:0.046, val_acc:0.990]
Epoch [108/120    avg_loss:0.052, val_acc:0.985]
Epoch [109/120    avg_loss:0.041, val_acc:0.988]
Epoch [110/120    avg_loss:0.037, val_acc:0.990]
Epoch [111/120    avg_loss:0.166, val_acc:0.977]
Epoch [112/120    avg_loss:0.190, val_acc:0.979]
Epoch [113/120    avg_loss:0.130, val_acc:0.981]
Epoch [114/120    avg_loss:0.084, val_acc:0.973]
Epoch [115/120    avg_loss:0.076, val_acc:0.981]
Epoch [116/120    avg_loss:0.078, val_acc:0.981]
Epoch [117/120    avg_loss:0.053, val_acc:0.979]
Epoch [118/120    avg_loss:0.050, val_acc:0.985]
Epoch [119/120    avg_loss:0.049, val_acc:0.983]
Epoch [120/120    avg_loss:0.045, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99854227 0.98426966 0.98901099 0.93275488 0.90972222
 0.99512195 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9905037891703511
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efbe4d75e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.646, val_acc:0.079]
Epoch [2/120    avg_loss:2.608, val_acc:0.108]
Epoch [3/120    avg_loss:2.569, val_acc:0.306]
Epoch [4/120    avg_loss:2.541, val_acc:0.377]
Epoch [5/120    avg_loss:2.500, val_acc:0.408]
Epoch [6/120    avg_loss:2.465, val_acc:0.419]
Epoch [7/120    avg_loss:2.422, val_acc:0.421]
Epoch [8/120    avg_loss:2.385, val_acc:0.427]
Epoch [9/120    avg_loss:2.335, val_acc:0.415]
Epoch [10/120    avg_loss:2.297, val_acc:0.427]
Epoch [11/120    avg_loss:2.235, val_acc:0.473]
Epoch [12/120    avg_loss:2.177, val_acc:0.533]
Epoch [13/120    avg_loss:2.132, val_acc:0.610]
Epoch [14/120    avg_loss:2.064, val_acc:0.625]
Epoch [15/120    avg_loss:1.978, val_acc:0.627]
Epoch [16/120    avg_loss:1.930, val_acc:0.637]
Epoch [17/120    avg_loss:1.842, val_acc:0.694]
Epoch [18/120    avg_loss:1.809, val_acc:0.675]
Epoch [19/120    avg_loss:1.717, val_acc:0.690]
Epoch [20/120    avg_loss:1.610, val_acc:0.715]
Epoch [21/120    avg_loss:1.520, val_acc:0.729]
Epoch [22/120    avg_loss:1.416, val_acc:0.702]
Epoch [23/120    avg_loss:1.333, val_acc:0.719]
Epoch [24/120    avg_loss:1.283, val_acc:0.738]
Epoch [25/120    avg_loss:1.134, val_acc:0.779]
Epoch [26/120    avg_loss:1.084, val_acc:0.760]
Epoch [27/120    avg_loss:0.997, val_acc:0.777]
Epoch [28/120    avg_loss:0.920, val_acc:0.815]
Epoch [29/120    avg_loss:0.913, val_acc:0.819]
Epoch [30/120    avg_loss:0.833, val_acc:0.819]
Epoch [31/120    avg_loss:0.808, val_acc:0.904]
Epoch [32/120    avg_loss:0.760, val_acc:0.910]
Epoch [33/120    avg_loss:0.667, val_acc:0.906]
Epoch [34/120    avg_loss:0.617, val_acc:0.933]
Epoch [35/120    avg_loss:0.636, val_acc:0.931]
Epoch [36/120    avg_loss:0.668, val_acc:0.927]
Epoch [37/120    avg_loss:0.657, val_acc:0.917]
Epoch [38/120    avg_loss:0.550, val_acc:0.923]
Epoch [39/120    avg_loss:0.496, val_acc:0.919]
Epoch [40/120    avg_loss:0.462, val_acc:0.938]
Epoch [41/120    avg_loss:0.455, val_acc:0.933]
Epoch [42/120    avg_loss:0.398, val_acc:0.933]
Epoch [43/120    avg_loss:0.424, val_acc:0.927]
Epoch [44/120    avg_loss:0.415, val_acc:0.960]
Epoch [45/120    avg_loss:0.388, val_acc:0.946]
Epoch [46/120    avg_loss:0.317, val_acc:0.950]
Epoch [47/120    avg_loss:0.349, val_acc:0.940]
Epoch [48/120    avg_loss:0.363, val_acc:0.954]
Epoch [49/120    avg_loss:0.355, val_acc:0.942]
Epoch [50/120    avg_loss:0.353, val_acc:0.896]
Epoch [51/120    avg_loss:0.309, val_acc:0.940]
Epoch [52/120    avg_loss:0.366, val_acc:0.906]
Epoch [53/120    avg_loss:0.316, val_acc:0.963]
Epoch [54/120    avg_loss:0.262, val_acc:0.954]
Epoch [55/120    avg_loss:0.283, val_acc:0.958]
Epoch [56/120    avg_loss:0.230, val_acc:0.963]
Epoch [57/120    avg_loss:0.284, val_acc:0.969]
Epoch [58/120    avg_loss:0.236, val_acc:0.948]
Epoch [59/120    avg_loss:0.283, val_acc:0.935]
Epoch [60/120    avg_loss:0.251, val_acc:0.960]
Epoch [61/120    avg_loss:0.229, val_acc:0.967]
Epoch [62/120    avg_loss:0.263, val_acc:0.954]
Epoch [63/120    avg_loss:0.260, val_acc:0.960]
Epoch [64/120    avg_loss:0.249, val_acc:0.946]
Epoch [65/120    avg_loss:0.280, val_acc:0.967]
Epoch [66/120    avg_loss:0.209, val_acc:0.971]
Epoch [67/120    avg_loss:0.172, val_acc:0.975]
Epoch [68/120    avg_loss:0.173, val_acc:0.973]
Epoch [69/120    avg_loss:0.210, val_acc:0.981]
Epoch [70/120    avg_loss:0.200, val_acc:0.975]
Epoch [71/120    avg_loss:0.192, val_acc:0.963]
Epoch [72/120    avg_loss:0.163, val_acc:0.963]
Epoch [73/120    avg_loss:0.160, val_acc:0.979]
Epoch [74/120    avg_loss:0.154, val_acc:0.950]
Epoch [75/120    avg_loss:0.176, val_acc:0.971]
Epoch [76/120    avg_loss:0.177, val_acc:0.983]
Epoch [77/120    avg_loss:0.130, val_acc:0.967]
Epoch [78/120    avg_loss:0.163, val_acc:0.975]
Epoch [79/120    avg_loss:0.145, val_acc:0.967]
Epoch [80/120    avg_loss:0.155, val_acc:0.965]
Epoch [81/120    avg_loss:0.166, val_acc:0.981]
Epoch [82/120    avg_loss:0.196, val_acc:0.969]
Epoch [83/120    avg_loss:0.138, val_acc:0.977]
Epoch [84/120    avg_loss:0.136, val_acc:0.979]
Epoch [85/120    avg_loss:0.128, val_acc:0.983]
Epoch [86/120    avg_loss:0.131, val_acc:0.979]
Epoch [87/120    avg_loss:0.099, val_acc:0.988]
Epoch [88/120    avg_loss:0.097, val_acc:0.979]
Epoch [89/120    avg_loss:0.099, val_acc:0.985]
Epoch [90/120    avg_loss:0.084, val_acc:0.992]
Epoch [91/120    avg_loss:0.084, val_acc:0.973]
Epoch [92/120    avg_loss:0.074, val_acc:0.990]
Epoch [93/120    avg_loss:0.067, val_acc:0.994]
Epoch [94/120    avg_loss:0.096, val_acc:0.983]
Epoch [95/120    avg_loss:0.098, val_acc:0.985]
Epoch [96/120    avg_loss:0.076, val_acc:0.977]
Epoch [97/120    avg_loss:0.098, val_acc:0.981]
Epoch [98/120    avg_loss:0.074, val_acc:0.992]
Epoch [99/120    avg_loss:0.065, val_acc:0.977]
Epoch [100/120    avg_loss:0.083, val_acc:0.979]
Epoch [101/120    avg_loss:0.087, val_acc:0.985]
Epoch [102/120    avg_loss:0.085, val_acc:0.988]
Epoch [103/120    avg_loss:0.155, val_acc:0.973]
Epoch [104/120    avg_loss:0.114, val_acc:0.981]
Epoch [105/120    avg_loss:0.084, val_acc:0.981]
Epoch [106/120    avg_loss:0.086, val_acc:0.992]
Epoch [107/120    avg_loss:0.066, val_acc:0.988]
Epoch [108/120    avg_loss:0.058, val_acc:0.988]
Epoch [109/120    avg_loss:0.053, val_acc:0.988]
Epoch [110/120    avg_loss:0.046, val_acc:0.988]
Epoch [111/120    avg_loss:0.058, val_acc:0.988]
Epoch [112/120    avg_loss:0.054, val_acc:0.988]
Epoch [113/120    avg_loss:0.043, val_acc:0.988]
Epoch [114/120    avg_loss:0.050, val_acc:0.988]
Epoch [115/120    avg_loss:0.047, val_acc:0.988]
Epoch [116/120    avg_loss:0.046, val_acc:0.988]
Epoch [117/120    avg_loss:0.046, val_acc:0.988]
Epoch [118/120    avg_loss:0.041, val_acc:0.988]
Epoch [119/120    avg_loss:0.036, val_acc:0.988]
Epoch [120/120    avg_loss:0.037, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 0.99926954 0.97986577 1.         0.94444444 0.92307692
 0.99757869 0.94972067 1.         1.         1.         0.99472296
 0.99556541 1.        ]

Kappa:
0.9909801721692914
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f916752cdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.085]
Epoch [2/120    avg_loss:2.571, val_acc:0.177]
Epoch [3/120    avg_loss:2.531, val_acc:0.278]
Epoch [4/120    avg_loss:2.493, val_acc:0.331]
Epoch [5/120    avg_loss:2.439, val_acc:0.361]
Epoch [6/120    avg_loss:2.390, val_acc:0.393]
Epoch [7/120    avg_loss:2.339, val_acc:0.411]
Epoch [8/120    avg_loss:2.289, val_acc:0.423]
Epoch [9/120    avg_loss:2.231, val_acc:0.452]
Epoch [10/120    avg_loss:2.171, val_acc:0.464]
Epoch [11/120    avg_loss:2.114, val_acc:0.506]
Epoch [12/120    avg_loss:2.055, val_acc:0.542]
Epoch [13/120    avg_loss:1.979, val_acc:0.587]
Epoch [14/120    avg_loss:1.895, val_acc:0.607]
Epoch [15/120    avg_loss:1.823, val_acc:0.613]
Epoch [16/120    avg_loss:1.739, val_acc:0.639]
Epoch [17/120    avg_loss:1.647, val_acc:0.669]
Epoch [18/120    avg_loss:1.547, val_acc:0.677]
Epoch [19/120    avg_loss:1.470, val_acc:0.677]
Epoch [20/120    avg_loss:1.398, val_acc:0.687]
Epoch [21/120    avg_loss:1.292, val_acc:0.714]
Epoch [22/120    avg_loss:1.176, val_acc:0.762]
Epoch [23/120    avg_loss:1.097, val_acc:0.786]
Epoch [24/120    avg_loss:1.017, val_acc:0.825]
Epoch [25/120    avg_loss:0.960, val_acc:0.871]
Epoch [26/120    avg_loss:0.935, val_acc:0.806]
Epoch [27/120    avg_loss:0.878, val_acc:0.867]
Epoch [28/120    avg_loss:0.840, val_acc:0.849]
Epoch [29/120    avg_loss:0.788, val_acc:0.851]
Epoch [30/120    avg_loss:0.687, val_acc:0.887]
Epoch [31/120    avg_loss:0.660, val_acc:0.903]
Epoch [32/120    avg_loss:0.564, val_acc:0.911]
Epoch [33/120    avg_loss:0.556, val_acc:0.917]
Epoch [34/120    avg_loss:0.583, val_acc:0.899]
Epoch [35/120    avg_loss:0.532, val_acc:0.909]
Epoch [36/120    avg_loss:0.525, val_acc:0.889]
Epoch [37/120    avg_loss:0.586, val_acc:0.909]
Epoch [38/120    avg_loss:0.512, val_acc:0.929]
Epoch [39/120    avg_loss:0.444, val_acc:0.925]
Epoch [40/120    avg_loss:0.448, val_acc:0.933]
Epoch [41/120    avg_loss:0.472, val_acc:0.899]
Epoch [42/120    avg_loss:0.456, val_acc:0.946]
Epoch [43/120    avg_loss:0.402, val_acc:0.935]
Epoch [44/120    avg_loss:0.437, val_acc:0.877]
Epoch [45/120    avg_loss:0.379, val_acc:0.933]
Epoch [46/120    avg_loss:0.363, val_acc:0.925]
Epoch [47/120    avg_loss:0.321, val_acc:0.942]
Epoch [48/120    avg_loss:0.371, val_acc:0.925]
Epoch [49/120    avg_loss:0.294, val_acc:0.925]
Epoch [50/120    avg_loss:0.281, val_acc:0.931]
Epoch [51/120    avg_loss:0.380, val_acc:0.913]
Epoch [52/120    avg_loss:0.302, val_acc:0.950]
Epoch [53/120    avg_loss:0.283, val_acc:0.946]
Epoch [54/120    avg_loss:0.224, val_acc:0.966]
Epoch [55/120    avg_loss:0.213, val_acc:0.958]
Epoch [56/120    avg_loss:0.222, val_acc:0.966]
Epoch [57/120    avg_loss:0.229, val_acc:0.950]
Epoch [58/120    avg_loss:0.259, val_acc:0.942]
Epoch [59/120    avg_loss:0.233, val_acc:0.952]
Epoch [60/120    avg_loss:0.262, val_acc:0.948]
Epoch [61/120    avg_loss:0.232, val_acc:0.964]
Epoch [62/120    avg_loss:0.184, val_acc:0.952]
Epoch [63/120    avg_loss:0.234, val_acc:0.962]
Epoch [64/120    avg_loss:0.238, val_acc:0.954]
Epoch [65/120    avg_loss:0.191, val_acc:0.960]
Epoch [66/120    avg_loss:0.229, val_acc:0.972]
Epoch [67/120    avg_loss:0.223, val_acc:0.968]
Epoch [68/120    avg_loss:0.177, val_acc:0.946]
Epoch [69/120    avg_loss:0.204, val_acc:0.946]
Epoch [70/120    avg_loss:0.212, val_acc:0.948]
Epoch [71/120    avg_loss:0.216, val_acc:0.980]
Epoch [72/120    avg_loss:0.188, val_acc:0.966]
Epoch [73/120    avg_loss:0.158, val_acc:0.960]
Epoch [74/120    avg_loss:0.166, val_acc:0.976]
Epoch [75/120    avg_loss:0.188, val_acc:0.966]
Epoch [76/120    avg_loss:0.154, val_acc:0.980]
Epoch [77/120    avg_loss:0.158, val_acc:0.980]
Epoch [78/120    avg_loss:0.152, val_acc:0.970]
Epoch [79/120    avg_loss:0.160, val_acc:0.974]
Epoch [80/120    avg_loss:0.201, val_acc:0.984]
Epoch [81/120    avg_loss:0.130, val_acc:0.976]
Epoch [82/120    avg_loss:0.112, val_acc:0.988]
Epoch [83/120    avg_loss:0.102, val_acc:0.976]
Epoch [84/120    avg_loss:0.091, val_acc:0.990]
Epoch [85/120    avg_loss:0.106, val_acc:0.964]
Epoch [86/120    avg_loss:0.142, val_acc:0.974]
Epoch [87/120    avg_loss:0.098, val_acc:0.974]
Epoch [88/120    avg_loss:0.101, val_acc:0.988]
Epoch [89/120    avg_loss:0.106, val_acc:0.986]
Epoch [90/120    avg_loss:0.105, val_acc:0.990]
Epoch [91/120    avg_loss:0.080, val_acc:0.988]
Epoch [92/120    avg_loss:0.070, val_acc:0.982]
Epoch [93/120    avg_loss:0.090, val_acc:0.976]
Epoch [94/120    avg_loss:0.079, val_acc:0.982]
Epoch [95/120    avg_loss:0.075, val_acc:0.982]
Epoch [96/120    avg_loss:0.072, val_acc:0.992]
Epoch [97/120    avg_loss:0.063, val_acc:0.990]
Epoch [98/120    avg_loss:0.082, val_acc:0.972]
Epoch [99/120    avg_loss:0.084, val_acc:0.994]
Epoch [100/120    avg_loss:0.061, val_acc:0.984]
Epoch [101/120    avg_loss:0.053, val_acc:0.988]
Epoch [102/120    avg_loss:0.037, val_acc:0.990]
Epoch [103/120    avg_loss:0.050, val_acc:0.988]
Epoch [104/120    avg_loss:0.086, val_acc:0.984]
Epoch [105/120    avg_loss:0.066, val_acc:0.988]
Epoch [106/120    avg_loss:0.061, val_acc:0.984]
Epoch [107/120    avg_loss:0.090, val_acc:0.952]
Epoch [108/120    avg_loss:0.090, val_acc:0.986]
Epoch [109/120    avg_loss:0.070, val_acc:0.994]
Epoch [110/120    avg_loss:0.048, val_acc:0.990]
Epoch [111/120    avg_loss:0.045, val_acc:0.994]
Epoch [112/120    avg_loss:0.047, val_acc:0.986]
Epoch [113/120    avg_loss:0.051, val_acc:0.980]
Epoch [114/120    avg_loss:0.050, val_acc:0.988]
Epoch [115/120    avg_loss:0.073, val_acc:0.974]
Epoch [116/120    avg_loss:0.051, val_acc:0.992]
Epoch [117/120    avg_loss:0.054, val_acc:0.996]
Epoch [118/120    avg_loss:0.052, val_acc:0.984]
Epoch [119/120    avg_loss:0.053, val_acc:0.990]
Epoch [120/120    avg_loss:0.034, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 1.         0.99545455 1.         0.96444444 0.94557823
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9957271506826739
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51d74b7898>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.350]
Epoch [2/120    avg_loss:2.546, val_acc:0.442]
Epoch [3/120    avg_loss:2.498, val_acc:0.456]
Epoch [4/120    avg_loss:2.447, val_acc:0.444]
Epoch [5/120    avg_loss:2.416, val_acc:0.417]
Epoch [6/120    avg_loss:2.384, val_acc:0.481]
Epoch [7/120    avg_loss:2.352, val_acc:0.521]
Epoch [8/120    avg_loss:2.299, val_acc:0.500]
Epoch [9/120    avg_loss:2.258, val_acc:0.535]
Epoch [10/120    avg_loss:2.212, val_acc:0.569]
Epoch [11/120    avg_loss:2.145, val_acc:0.569]
Epoch [12/120    avg_loss:2.083, val_acc:0.602]
Epoch [13/120    avg_loss:2.017, val_acc:0.602]
Epoch [14/120    avg_loss:1.941, val_acc:0.619]
Epoch [15/120    avg_loss:1.861, val_acc:0.635]
Epoch [16/120    avg_loss:1.805, val_acc:0.694]
Epoch [17/120    avg_loss:1.722, val_acc:0.704]
Epoch [18/120    avg_loss:1.612, val_acc:0.704]
Epoch [19/120    avg_loss:1.504, val_acc:0.715]
Epoch [20/120    avg_loss:1.483, val_acc:0.696]
Epoch [21/120    avg_loss:1.393, val_acc:0.715]
Epoch [22/120    avg_loss:1.289, val_acc:0.708]
Epoch [23/120    avg_loss:1.194, val_acc:0.694]
Epoch [24/120    avg_loss:1.122, val_acc:0.762]
Epoch [25/120    avg_loss:1.037, val_acc:0.773]
Epoch [26/120    avg_loss:0.983, val_acc:0.785]
Epoch [27/120    avg_loss:0.911, val_acc:0.783]
Epoch [28/120    avg_loss:0.849, val_acc:0.779]
Epoch [29/120    avg_loss:0.875, val_acc:0.796]
Epoch [30/120    avg_loss:0.818, val_acc:0.806]
Epoch [31/120    avg_loss:0.727, val_acc:0.833]
Epoch [32/120    avg_loss:0.651, val_acc:0.894]
Epoch [33/120    avg_loss:0.599, val_acc:0.887]
Epoch [34/120    avg_loss:0.636, val_acc:0.885]
Epoch [35/120    avg_loss:0.578, val_acc:0.915]
Epoch [36/120    avg_loss:0.608, val_acc:0.883]
Epoch [37/120    avg_loss:0.539, val_acc:0.923]
Epoch [38/120    avg_loss:0.517, val_acc:0.898]
Epoch [39/120    avg_loss:0.490, val_acc:0.950]
Epoch [40/120    avg_loss:0.421, val_acc:0.915]
Epoch [41/120    avg_loss:0.394, val_acc:0.952]
Epoch [42/120    avg_loss:0.398, val_acc:0.929]
Epoch [43/120    avg_loss:0.368, val_acc:0.946]
Epoch [44/120    avg_loss:0.328, val_acc:0.935]
Epoch [45/120    avg_loss:0.379, val_acc:0.942]
Epoch [46/120    avg_loss:0.356, val_acc:0.944]
Epoch [47/120    avg_loss:0.315, val_acc:0.948]
Epoch [48/120    avg_loss:0.317, val_acc:0.940]
Epoch [49/120    avg_loss:0.275, val_acc:0.965]
Epoch [50/120    avg_loss:0.262, val_acc:0.950]
Epoch [51/120    avg_loss:0.286, val_acc:0.963]
Epoch [52/120    avg_loss:0.291, val_acc:0.940]
Epoch [53/120    avg_loss:0.264, val_acc:0.956]
Epoch [54/120    avg_loss:0.242, val_acc:0.940]
Epoch [55/120    avg_loss:0.303, val_acc:0.975]
Epoch [56/120    avg_loss:0.259, val_acc:0.940]
Epoch [57/120    avg_loss:0.219, val_acc:0.965]
Epoch [58/120    avg_loss:0.233, val_acc:0.956]
Epoch [59/120    avg_loss:0.216, val_acc:0.969]
Epoch [60/120    avg_loss:0.210, val_acc:0.973]
Epoch [61/120    avg_loss:0.196, val_acc:0.960]
Epoch [62/120    avg_loss:0.236, val_acc:0.958]
Epoch [63/120    avg_loss:0.219, val_acc:0.954]
Epoch [64/120    avg_loss:0.156, val_acc:0.956]
Epoch [65/120    avg_loss:0.177, val_acc:0.971]
Epoch [66/120    avg_loss:0.203, val_acc:0.960]
Epoch [67/120    avg_loss:0.156, val_acc:0.979]
Epoch [68/120    avg_loss:0.167, val_acc:0.954]
Epoch [69/120    avg_loss:0.172, val_acc:0.969]
Epoch [70/120    avg_loss:0.121, val_acc:0.965]
Epoch [71/120    avg_loss:0.156, val_acc:0.965]
Epoch [72/120    avg_loss:0.195, val_acc:0.967]
Epoch [73/120    avg_loss:0.147, val_acc:0.983]
Epoch [74/120    avg_loss:0.176, val_acc:0.981]
Epoch [75/120    avg_loss:0.137, val_acc:0.967]
Epoch [76/120    avg_loss:0.137, val_acc:0.983]
Epoch [77/120    avg_loss:0.134, val_acc:0.967]
Epoch [78/120    avg_loss:0.147, val_acc:0.967]
Epoch [79/120    avg_loss:0.138, val_acc:0.975]
Epoch [80/120    avg_loss:0.117, val_acc:0.979]
Epoch [81/120    avg_loss:0.098, val_acc:0.969]
Epoch [82/120    avg_loss:0.098, val_acc:0.983]
Epoch [83/120    avg_loss:0.116, val_acc:0.973]
Epoch [84/120    avg_loss:0.081, val_acc:0.977]
Epoch [85/120    avg_loss:0.090, val_acc:0.942]
Epoch [86/120    avg_loss:0.149, val_acc:0.975]
Epoch [87/120    avg_loss:0.151, val_acc:0.960]
Epoch [88/120    avg_loss:0.103, val_acc:0.979]
Epoch [89/120    avg_loss:0.085, val_acc:0.969]
Epoch [90/120    avg_loss:0.110, val_acc:0.969]
Epoch [91/120    avg_loss:0.090, val_acc:0.977]
Epoch [92/120    avg_loss:0.091, val_acc:0.979]
Epoch [93/120    avg_loss:0.084, val_acc:0.965]
Epoch [94/120    avg_loss:0.091, val_acc:0.981]
Epoch [95/120    avg_loss:0.091, val_acc:0.938]
Epoch [96/120    avg_loss:0.104, val_acc:0.981]
Epoch [97/120    avg_loss:0.057, val_acc:0.985]
Epoch [98/120    avg_loss:0.060, val_acc:0.983]
Epoch [99/120    avg_loss:0.045, val_acc:0.985]
Epoch [100/120    avg_loss:0.051, val_acc:0.985]
Epoch [101/120    avg_loss:0.048, val_acc:0.985]
Epoch [102/120    avg_loss:0.050, val_acc:0.988]
Epoch [103/120    avg_loss:0.058, val_acc:0.988]
Epoch [104/120    avg_loss:0.046, val_acc:0.988]
Epoch [105/120    avg_loss:0.048, val_acc:0.985]
Epoch [106/120    avg_loss:0.051, val_acc:0.981]
Epoch [107/120    avg_loss:0.051, val_acc:0.988]
Epoch [108/120    avg_loss:0.050, val_acc:0.983]
Epoch [109/120    avg_loss:0.043, val_acc:0.983]
Epoch [110/120    avg_loss:0.041, val_acc:0.988]
Epoch [111/120    avg_loss:0.051, val_acc:0.985]
Epoch [112/120    avg_loss:0.047, val_acc:0.985]
Epoch [113/120    avg_loss:0.043, val_acc:0.983]
Epoch [114/120    avg_loss:0.042, val_acc:0.983]
Epoch [115/120    avg_loss:0.046, val_acc:0.983]
Epoch [116/120    avg_loss:0.043, val_acc:0.983]
Epoch [117/120    avg_loss:0.043, val_acc:0.983]
Epoch [118/120    avg_loss:0.038, val_acc:0.985]
Epoch [119/120    avg_loss:0.042, val_acc:0.985]
Epoch [120/120    avg_loss:0.043, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.98206278 0.99782135 0.9255079  0.89036545
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9902673790728557
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e98bffe10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.655, val_acc:0.063]
Epoch [2/120    avg_loss:2.617, val_acc:0.110]
Epoch [3/120    avg_loss:2.589, val_acc:0.131]
Epoch [4/120    avg_loss:2.558, val_acc:0.131]
Epoch [5/120    avg_loss:2.527, val_acc:0.133]
Epoch [6/120    avg_loss:2.483, val_acc:0.275]
Epoch [7/120    avg_loss:2.438, val_acc:0.344]
Epoch [8/120    avg_loss:2.395, val_acc:0.362]
Epoch [9/120    avg_loss:2.359, val_acc:0.383]
Epoch [10/120    avg_loss:2.303, val_acc:0.419]
Epoch [11/120    avg_loss:2.264, val_acc:0.473]
Epoch [12/120    avg_loss:2.202, val_acc:0.523]
Epoch [13/120    avg_loss:2.153, val_acc:0.515]
Epoch [14/120    avg_loss:2.084, val_acc:0.583]
Epoch [15/120    avg_loss:2.028, val_acc:0.590]
Epoch [16/120    avg_loss:1.962, val_acc:0.625]
Epoch [17/120    avg_loss:1.890, val_acc:0.640]
Epoch [18/120    avg_loss:1.803, val_acc:0.642]
Epoch [19/120    avg_loss:1.730, val_acc:0.675]
Epoch [20/120    avg_loss:1.673, val_acc:0.688]
Epoch [21/120    avg_loss:1.586, val_acc:0.715]
Epoch [22/120    avg_loss:1.494, val_acc:0.721]
Epoch [23/120    avg_loss:1.394, val_acc:0.725]
Epoch [24/120    avg_loss:1.306, val_acc:0.698]
Epoch [25/120    avg_loss:1.219, val_acc:0.721]
Epoch [26/120    avg_loss:1.135, val_acc:0.783]
Epoch [27/120    avg_loss:1.111, val_acc:0.756]
Epoch [28/120    avg_loss:1.072, val_acc:0.756]
Epoch [29/120    avg_loss:0.968, val_acc:0.790]
Epoch [30/120    avg_loss:0.855, val_acc:0.842]
Epoch [31/120    avg_loss:0.841, val_acc:0.829]
Epoch [32/120    avg_loss:0.723, val_acc:0.890]
Epoch [33/120    avg_loss:0.670, val_acc:0.848]
Epoch [34/120    avg_loss:0.627, val_acc:0.933]
Epoch [35/120    avg_loss:0.607, val_acc:0.896]
Epoch [36/120    avg_loss:0.594, val_acc:0.896]
Epoch [37/120    avg_loss:0.495, val_acc:0.919]
Epoch [38/120    avg_loss:0.544, val_acc:0.898]
Epoch [39/120    avg_loss:0.478, val_acc:0.919]
Epoch [40/120    avg_loss:0.463, val_acc:0.946]
Epoch [41/120    avg_loss:0.438, val_acc:0.948]
Epoch [42/120    avg_loss:0.450, val_acc:0.921]
Epoch [43/120    avg_loss:0.421, val_acc:0.940]
Epoch [44/120    avg_loss:0.423, val_acc:0.940]
Epoch [45/120    avg_loss:0.338, val_acc:0.956]
Epoch [46/120    avg_loss:0.318, val_acc:0.967]
Epoch [47/120    avg_loss:0.307, val_acc:0.938]
Epoch [48/120    avg_loss:0.253, val_acc:0.952]
Epoch [49/120    avg_loss:0.243, val_acc:0.967]
Epoch [50/120    avg_loss:0.298, val_acc:0.935]
Epoch [51/120    avg_loss:0.285, val_acc:0.944]
Epoch [52/120    avg_loss:0.259, val_acc:0.958]
Epoch [53/120    avg_loss:0.259, val_acc:0.969]
Epoch [54/120    avg_loss:0.227, val_acc:0.969]
Epoch [55/120    avg_loss:0.218, val_acc:0.969]
Epoch [56/120    avg_loss:0.199, val_acc:0.975]
Epoch [57/120    avg_loss:0.186, val_acc:0.965]
Epoch [58/120    avg_loss:0.294, val_acc:0.956]
Epoch [59/120    avg_loss:0.214, val_acc:0.952]
Epoch [60/120    avg_loss:0.191, val_acc:0.973]
Epoch [61/120    avg_loss:0.154, val_acc:0.979]
Epoch [62/120    avg_loss:0.181, val_acc:0.967]
Epoch [63/120    avg_loss:0.182, val_acc:0.942]
Epoch [64/120    avg_loss:0.150, val_acc:0.969]
Epoch [65/120    avg_loss:0.165, val_acc:0.983]
Epoch [66/120    avg_loss:0.135, val_acc:0.973]
Epoch [67/120    avg_loss:0.121, val_acc:0.952]
Epoch [68/120    avg_loss:0.134, val_acc:0.979]
Epoch [69/120    avg_loss:0.138, val_acc:0.971]
Epoch [70/120    avg_loss:0.120, val_acc:0.985]
Epoch [71/120    avg_loss:0.101, val_acc:0.971]
Epoch [72/120    avg_loss:0.090, val_acc:0.983]
Epoch [73/120    avg_loss:0.088, val_acc:0.983]
Epoch [74/120    avg_loss:0.092, val_acc:0.977]
Epoch [75/120    avg_loss:0.132, val_acc:0.975]
Epoch [76/120    avg_loss:0.113, val_acc:0.948]
Epoch [77/120    avg_loss:0.080, val_acc:0.990]
Epoch [78/120    avg_loss:0.077, val_acc:0.988]
Epoch [79/120    avg_loss:0.076, val_acc:0.977]
Epoch [80/120    avg_loss:0.104, val_acc:0.981]
Epoch [81/120    avg_loss:0.131, val_acc:0.979]
Epoch [82/120    avg_loss:0.159, val_acc:0.973]
Epoch [83/120    avg_loss:0.151, val_acc:0.975]
Epoch [84/120    avg_loss:0.118, val_acc:0.983]
Epoch [85/120    avg_loss:0.091, val_acc:0.990]
Epoch [86/120    avg_loss:0.078, val_acc:0.985]
Epoch [87/120    avg_loss:0.059, val_acc:0.983]
Epoch [88/120    avg_loss:0.068, val_acc:0.979]
Epoch [89/120    avg_loss:0.073, val_acc:0.967]
Epoch [90/120    avg_loss:0.063, val_acc:0.983]
Epoch [91/120    avg_loss:0.071, val_acc:0.988]
Epoch [92/120    avg_loss:0.076, val_acc:0.983]
Epoch [93/120    avg_loss:0.044, val_acc:0.981]
Epoch [94/120    avg_loss:0.056, val_acc:0.985]
Epoch [95/120    avg_loss:0.045, val_acc:0.988]
Epoch [96/120    avg_loss:0.064, val_acc:0.988]
Epoch [97/120    avg_loss:0.047, val_acc:0.988]
Epoch [98/120    avg_loss:0.069, val_acc:0.992]
Epoch [99/120    avg_loss:0.055, val_acc:0.979]
Epoch [100/120    avg_loss:0.051, val_acc:0.988]
Epoch [101/120    avg_loss:0.047, val_acc:0.981]
Epoch [102/120    avg_loss:0.050, val_acc:0.988]
Epoch [103/120    avg_loss:0.043, val_acc:0.985]
Epoch [104/120    avg_loss:0.033, val_acc:0.990]
Epoch [105/120    avg_loss:0.030, val_acc:0.985]
Epoch [106/120    avg_loss:0.024, val_acc:0.990]
Epoch [107/120    avg_loss:0.026, val_acc:0.988]
Epoch [108/120    avg_loss:0.029, val_acc:0.985]
Epoch [109/120    avg_loss:0.027, val_acc:0.988]
Epoch [110/120    avg_loss:0.024, val_acc:0.985]
Epoch [111/120    avg_loss:0.028, val_acc:0.988]
Epoch [112/120    avg_loss:0.041, val_acc:0.988]
Epoch [113/120    avg_loss:0.026, val_acc:0.988]
Epoch [114/120    avg_loss:0.021, val_acc:0.988]
Epoch [115/120    avg_loss:0.023, val_acc:0.988]
Epoch [116/120    avg_loss:0.019, val_acc:0.988]
Epoch [117/120    avg_loss:0.023, val_acc:0.985]
Epoch [118/120    avg_loss:0.021, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.018, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 0.99926954 0.99095023 1.         0.97345133 0.95890411
 0.99757869 0.97826087 1.         1.         1.         0.98691099
 0.98883929 1.        ]

Kappa:
0.9935909936429723
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e7518ae48>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.612, val_acc:0.125]
Epoch [2/120    avg_loss:2.548, val_acc:0.292]
Epoch [3/120    avg_loss:2.501, val_acc:0.310]
Epoch [4/120    avg_loss:2.452, val_acc:0.325]
Epoch [5/120    avg_loss:2.407, val_acc:0.352]
Epoch [6/120    avg_loss:2.361, val_acc:0.390]
Epoch [7/120    avg_loss:2.319, val_acc:0.410]
Epoch [8/120    avg_loss:2.260, val_acc:0.438]
Epoch [9/120    avg_loss:2.208, val_acc:0.477]
Epoch [10/120    avg_loss:2.158, val_acc:0.500]
Epoch [11/120    avg_loss:2.098, val_acc:0.523]
Epoch [12/120    avg_loss:2.026, val_acc:0.546]
Epoch [13/120    avg_loss:1.972, val_acc:0.562]
Epoch [14/120    avg_loss:1.914, val_acc:0.588]
Epoch [15/120    avg_loss:1.811, val_acc:0.577]
Epoch [16/120    avg_loss:1.757, val_acc:0.604]
Epoch [17/120    avg_loss:1.657, val_acc:0.640]
Epoch [18/120    avg_loss:1.599, val_acc:0.625]
Epoch [19/120    avg_loss:1.506, val_acc:0.654]
Epoch [20/120    avg_loss:1.433, val_acc:0.665]
Epoch [21/120    avg_loss:1.348, val_acc:0.692]
Epoch [22/120    avg_loss:1.272, val_acc:0.696]
Epoch [23/120    avg_loss:1.192, val_acc:0.700]
Epoch [24/120    avg_loss:1.097, val_acc:0.721]
Epoch [25/120    avg_loss:1.026, val_acc:0.727]
Epoch [26/120    avg_loss:1.043, val_acc:0.704]
Epoch [27/120    avg_loss:0.966, val_acc:0.738]
Epoch [28/120    avg_loss:0.903, val_acc:0.787]
Epoch [29/120    avg_loss:0.810, val_acc:0.850]
Epoch [30/120    avg_loss:0.764, val_acc:0.771]
Epoch [31/120    avg_loss:0.731, val_acc:0.910]
Epoch [32/120    avg_loss:0.673, val_acc:0.900]
Epoch [33/120    avg_loss:0.718, val_acc:0.858]
Epoch [34/120    avg_loss:0.668, val_acc:0.900]
Epoch [35/120    avg_loss:0.672, val_acc:0.863]
Epoch [36/120    avg_loss:0.636, val_acc:0.887]
Epoch [37/120    avg_loss:0.631, val_acc:0.900]
Epoch [38/120    avg_loss:0.531, val_acc:0.902]
Epoch [39/120    avg_loss:0.522, val_acc:0.927]
Epoch [40/120    avg_loss:0.562, val_acc:0.835]
Epoch [41/120    avg_loss:0.462, val_acc:0.938]
Epoch [42/120    avg_loss:0.484, val_acc:0.942]
Epoch [43/120    avg_loss:0.459, val_acc:0.933]
Epoch [44/120    avg_loss:0.389, val_acc:0.944]
Epoch [45/120    avg_loss:0.401, val_acc:0.933]
Epoch [46/120    avg_loss:0.412, val_acc:0.940]
Epoch [47/120    avg_loss:0.377, val_acc:0.944]
Epoch [48/120    avg_loss:0.363, val_acc:0.912]
Epoch [49/120    avg_loss:0.361, val_acc:0.948]
Epoch [50/120    avg_loss:0.318, val_acc:0.942]
Epoch [51/120    avg_loss:0.307, val_acc:0.948]
Epoch [52/120    avg_loss:0.306, val_acc:0.940]
Epoch [53/120    avg_loss:0.318, val_acc:0.940]
Epoch [54/120    avg_loss:0.321, val_acc:0.965]
Epoch [55/120    avg_loss:0.279, val_acc:0.952]
Epoch [56/120    avg_loss:0.280, val_acc:0.958]
Epoch [57/120    avg_loss:0.274, val_acc:0.960]
Epoch [58/120    avg_loss:0.239, val_acc:0.954]
Epoch [59/120    avg_loss:0.264, val_acc:0.944]
Epoch [60/120    avg_loss:0.316, val_acc:0.965]
Epoch [61/120    avg_loss:0.271, val_acc:0.950]
Epoch [62/120    avg_loss:0.300, val_acc:0.946]
Epoch [63/120    avg_loss:0.184, val_acc:0.977]
Epoch [64/120    avg_loss:0.186, val_acc:0.958]
Epoch [65/120    avg_loss:0.178, val_acc:0.975]
Epoch [66/120    avg_loss:0.194, val_acc:0.967]
Epoch [67/120    avg_loss:0.225, val_acc:0.950]
Epoch [68/120    avg_loss:0.217, val_acc:0.973]
Epoch [69/120    avg_loss:0.202, val_acc:0.973]
Epoch [70/120    avg_loss:0.193, val_acc:0.967]
Epoch [71/120    avg_loss:0.189, val_acc:0.960]
Epoch [72/120    avg_loss:0.146, val_acc:0.969]
Epoch [73/120    avg_loss:0.217, val_acc:0.935]
Epoch [74/120    avg_loss:0.277, val_acc:0.983]
Epoch [75/120    avg_loss:0.142, val_acc:0.977]
Epoch [76/120    avg_loss:0.198, val_acc:0.956]
Epoch [77/120    avg_loss:0.170, val_acc:0.954]
Epoch [78/120    avg_loss:0.165, val_acc:0.967]
Epoch [79/120    avg_loss:0.160, val_acc:0.969]
Epoch [80/120    avg_loss:0.143, val_acc:0.967]
Epoch [81/120    avg_loss:0.191, val_acc:0.977]
Epoch [82/120    avg_loss:0.172, val_acc:0.981]
Epoch [83/120    avg_loss:0.160, val_acc:0.985]
Epoch [84/120    avg_loss:0.142, val_acc:0.963]
Epoch [85/120    avg_loss:0.104, val_acc:0.981]
Epoch [86/120    avg_loss:0.089, val_acc:0.981]
Epoch [87/120    avg_loss:0.113, val_acc:0.975]
Epoch [88/120    avg_loss:0.106, val_acc:0.988]
Epoch [89/120    avg_loss:0.125, val_acc:0.979]
Epoch [90/120    avg_loss:0.105, val_acc:0.981]
Epoch [91/120    avg_loss:0.089, val_acc:0.985]
Epoch [92/120    avg_loss:0.137, val_acc:0.979]
Epoch [93/120    avg_loss:0.104, val_acc:0.979]
Epoch [94/120    avg_loss:0.148, val_acc:0.985]
Epoch [95/120    avg_loss:0.105, val_acc:0.985]
Epoch [96/120    avg_loss:0.086, val_acc:0.990]
Epoch [97/120    avg_loss:0.095, val_acc:0.992]
Epoch [98/120    avg_loss:0.108, val_acc:0.979]
Epoch [99/120    avg_loss:0.127, val_acc:0.967]
Epoch [100/120    avg_loss:0.197, val_acc:0.988]
Epoch [101/120    avg_loss:0.163, val_acc:0.981]
Epoch [102/120    avg_loss:0.091, val_acc:0.967]
Epoch [103/120    avg_loss:0.070, val_acc:0.988]
Epoch [104/120    avg_loss:0.062, val_acc:0.981]
Epoch [105/120    avg_loss:0.063, val_acc:0.979]
Epoch [106/120    avg_loss:0.078, val_acc:0.988]
Epoch [107/120    avg_loss:0.082, val_acc:0.994]
Epoch [108/120    avg_loss:0.053, val_acc:0.985]
Epoch [109/120    avg_loss:0.059, val_acc:0.990]
Epoch [110/120    avg_loss:0.060, val_acc:0.994]
Epoch [111/120    avg_loss:0.071, val_acc:0.990]
Epoch [112/120    avg_loss:0.057, val_acc:0.992]
Epoch [113/120    avg_loss:0.038, val_acc:0.994]
Epoch [114/120    avg_loss:0.044, val_acc:0.988]
Epoch [115/120    avg_loss:0.041, val_acc:0.996]
Epoch [116/120    avg_loss:0.029, val_acc:0.994]
Epoch [117/120    avg_loss:0.040, val_acc:0.996]
Epoch [118/120    avg_loss:0.038, val_acc:0.988]
Epoch [119/120    avg_loss:0.052, val_acc:0.990]
Epoch [120/120    avg_loss:0.073, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   5 197  28   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0 200   0   0   0   0   0   0   0]
 [  0   0  68   0   0   0   0  26   0   0   0   0   0   0]
 [  0   3   4   0   0   0   0   0 381   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.20682302771854

F1 scores:
[       nan 0.997815   0.84069098 0.92271663 0.90322581 0.94462541
 0.98522167 0.43333333 0.99089727 1.         1.         1.
 1.         1.        ]

Kappa:
0.9688915983163519
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3cc4b32e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.637, val_acc:0.071]
Epoch [2/120    avg_loss:2.593, val_acc:0.302]
Epoch [3/120    avg_loss:2.552, val_acc:0.458]
Epoch [4/120    avg_loss:2.506, val_acc:0.498]
Epoch [5/120    avg_loss:2.451, val_acc:0.556]
Epoch [6/120    avg_loss:2.394, val_acc:0.500]
Epoch [7/120    avg_loss:2.335, val_acc:0.517]
Epoch [8/120    avg_loss:2.269, val_acc:0.506]
Epoch [9/120    avg_loss:2.211, val_acc:0.521]
Epoch [10/120    avg_loss:2.141, val_acc:0.546]
Epoch [11/120    avg_loss:2.077, val_acc:0.623]
Epoch [12/120    avg_loss:2.022, val_acc:0.637]
Epoch [13/120    avg_loss:1.937, val_acc:0.665]
Epoch [14/120    avg_loss:1.853, val_acc:0.683]
Epoch [15/120    avg_loss:1.763, val_acc:0.688]
Epoch [16/120    avg_loss:1.682, val_acc:0.688]
Epoch [17/120    avg_loss:1.599, val_acc:0.708]
Epoch [18/120    avg_loss:1.509, val_acc:0.721]
Epoch [19/120    avg_loss:1.424, val_acc:0.723]
Epoch [20/120    avg_loss:1.317, val_acc:0.756]
Epoch [21/120    avg_loss:1.235, val_acc:0.760]
Epoch [22/120    avg_loss:1.158, val_acc:0.742]
Epoch [23/120    avg_loss:1.089, val_acc:0.800]
Epoch [24/120    avg_loss:0.990, val_acc:0.785]
Epoch [25/120    avg_loss:0.921, val_acc:0.854]
Epoch [26/120    avg_loss:0.863, val_acc:0.867]
Epoch [27/120    avg_loss:0.833, val_acc:0.806]
Epoch [28/120    avg_loss:0.790, val_acc:0.794]
Epoch [29/120    avg_loss:0.709, val_acc:0.858]
Epoch [30/120    avg_loss:0.664, val_acc:0.902]
Epoch [31/120    avg_loss:0.619, val_acc:0.785]
Epoch [32/120    avg_loss:0.642, val_acc:0.840]
Epoch [33/120    avg_loss:0.581, val_acc:0.921]
Epoch [34/120    avg_loss:0.567, val_acc:0.921]
Epoch [35/120    avg_loss:0.475, val_acc:0.906]
Epoch [36/120    avg_loss:0.462, val_acc:0.921]
Epoch [37/120    avg_loss:0.432, val_acc:0.921]
Epoch [38/120    avg_loss:0.410, val_acc:0.904]
Epoch [39/120    avg_loss:0.434, val_acc:0.908]
Epoch [40/120    avg_loss:0.475, val_acc:0.906]
Epoch [41/120    avg_loss:0.426, val_acc:0.927]
Epoch [42/120    avg_loss:0.399, val_acc:0.923]
Epoch [43/120    avg_loss:0.371, val_acc:0.908]
Epoch [44/120    avg_loss:0.362, val_acc:0.938]
Epoch [45/120    avg_loss:0.293, val_acc:0.948]
Epoch [46/120    avg_loss:0.324, val_acc:0.942]
Epoch [47/120    avg_loss:0.280, val_acc:0.929]
Epoch [48/120    avg_loss:0.270, val_acc:0.952]
Epoch [49/120    avg_loss:0.316, val_acc:0.935]
Epoch [50/120    avg_loss:0.357, val_acc:0.940]
Epoch [51/120    avg_loss:0.281, val_acc:0.960]
Epoch [52/120    avg_loss:0.228, val_acc:0.975]
Epoch [53/120    avg_loss:0.247, val_acc:0.948]
Epoch [54/120    avg_loss:0.325, val_acc:0.952]
Epoch [55/120    avg_loss:0.208, val_acc:0.956]
Epoch [56/120    avg_loss:0.228, val_acc:0.975]
Epoch [57/120    avg_loss:0.187, val_acc:0.935]
Epoch [58/120    avg_loss:0.196, val_acc:0.952]
Epoch [59/120    avg_loss:0.219, val_acc:0.960]
Epoch [60/120    avg_loss:0.193, val_acc:0.958]
Epoch [61/120    avg_loss:0.141, val_acc:0.967]
Epoch [62/120    avg_loss:0.181, val_acc:0.977]
Epoch [63/120    avg_loss:0.165, val_acc:0.960]
Epoch [64/120    avg_loss:0.155, val_acc:0.985]
Epoch [65/120    avg_loss:0.127, val_acc:0.967]
Epoch [66/120    avg_loss:0.180, val_acc:0.977]
Epoch [67/120    avg_loss:0.158, val_acc:0.965]
Epoch [68/120    avg_loss:0.159, val_acc:0.975]
Epoch [69/120    avg_loss:0.144, val_acc:0.963]
Epoch [70/120    avg_loss:0.144, val_acc:0.975]
Epoch [71/120    avg_loss:0.142, val_acc:0.956]
Epoch [72/120    avg_loss:0.132, val_acc:0.975]
Epoch [73/120    avg_loss:0.167, val_acc:0.956]
Epoch [74/120    avg_loss:0.139, val_acc:0.956]
Epoch [75/120    avg_loss:0.122, val_acc:0.958]
Epoch [76/120    avg_loss:0.111, val_acc:0.969]
Epoch [77/120    avg_loss:0.118, val_acc:0.967]
Epoch [78/120    avg_loss:0.102, val_acc:0.985]
Epoch [79/120    avg_loss:0.079, val_acc:0.985]
Epoch [80/120    avg_loss:0.084, val_acc:0.988]
Epoch [81/120    avg_loss:0.086, val_acc:0.985]
Epoch [82/120    avg_loss:0.077, val_acc:0.985]
Epoch [83/120    avg_loss:0.081, val_acc:0.985]
Epoch [84/120    avg_loss:0.072, val_acc:0.985]
Epoch [85/120    avg_loss:0.080, val_acc:0.985]
Epoch [86/120    avg_loss:0.079, val_acc:0.985]
Epoch [87/120    avg_loss:0.071, val_acc:0.988]
Epoch [88/120    avg_loss:0.069, val_acc:0.985]
Epoch [89/120    avg_loss:0.085, val_acc:0.985]
Epoch [90/120    avg_loss:0.073, val_acc:0.988]
Epoch [91/120    avg_loss:0.072, val_acc:0.988]
Epoch [92/120    avg_loss:0.058, val_acc:0.988]
Epoch [93/120    avg_loss:0.071, val_acc:0.988]
Epoch [94/120    avg_loss:0.060, val_acc:0.990]
Epoch [95/120    avg_loss:0.068, val_acc:0.990]
Epoch [96/120    avg_loss:0.072, val_acc:0.988]
Epoch [97/120    avg_loss:0.079, val_acc:0.988]
Epoch [98/120    avg_loss:0.061, val_acc:0.990]
Epoch [99/120    avg_loss:0.060, val_acc:0.985]
Epoch [100/120    avg_loss:0.068, val_acc:0.988]
Epoch [101/120    avg_loss:0.062, val_acc:0.992]
Epoch [102/120    avg_loss:0.074, val_acc:0.990]
Epoch [103/120    avg_loss:0.059, val_acc:0.990]
Epoch [104/120    avg_loss:0.058, val_acc:0.992]
Epoch [105/120    avg_loss:0.062, val_acc:0.990]
Epoch [106/120    avg_loss:0.065, val_acc:0.992]
Epoch [107/120    avg_loss:0.071, val_acc:0.992]
Epoch [108/120    avg_loss:0.057, val_acc:0.992]
Epoch [109/120    avg_loss:0.061, val_acc:0.990]
Epoch [110/120    avg_loss:0.062, val_acc:0.990]
Epoch [111/120    avg_loss:0.060, val_acc:0.992]
Epoch [112/120    avg_loss:0.060, val_acc:0.990]
Epoch [113/120    avg_loss:0.061, val_acc:0.990]
Epoch [114/120    avg_loss:0.052, val_acc:0.990]
Epoch [115/120    avg_loss:0.049, val_acc:0.990]
Epoch [116/120    avg_loss:0.052, val_acc:0.990]
Epoch [117/120    avg_loss:0.048, val_acc:0.990]
Epoch [118/120    avg_loss:0.058, val_acc:0.990]
Epoch [119/120    avg_loss:0.055, val_acc:0.992]
Epoch [120/120    avg_loss:0.058, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 194  33   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 0.99264706 1.         1.         0.91509434 0.8875
 0.97630332 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9890844585842886
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4dea046e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.193]
Epoch [2/120    avg_loss:2.571, val_acc:0.215]
Epoch [3/120    avg_loss:2.523, val_acc:0.197]
Epoch [4/120    avg_loss:2.481, val_acc:0.174]
Epoch [5/120    avg_loss:2.443, val_acc:0.308]
Epoch [6/120    avg_loss:2.393, val_acc:0.317]
Epoch [7/120    avg_loss:2.345, val_acc:0.365]
Epoch [8/120    avg_loss:2.293, val_acc:0.431]
Epoch [9/120    avg_loss:2.244, val_acc:0.471]
Epoch [10/120    avg_loss:2.185, val_acc:0.479]
Epoch [11/120    avg_loss:2.125, val_acc:0.498]
Epoch [12/120    avg_loss:2.071, val_acc:0.487]
Epoch [13/120    avg_loss:1.988, val_acc:0.525]
Epoch [14/120    avg_loss:1.915, val_acc:0.567]
Epoch [15/120    avg_loss:1.834, val_acc:0.565]
Epoch [16/120    avg_loss:1.774, val_acc:0.590]
Epoch [17/120    avg_loss:1.693, val_acc:0.606]
Epoch [18/120    avg_loss:1.629, val_acc:0.627]
Epoch [19/120    avg_loss:1.562, val_acc:0.669]
Epoch [20/120    avg_loss:1.499, val_acc:0.660]
Epoch [21/120    avg_loss:1.413, val_acc:0.658]
Epoch [22/120    avg_loss:1.371, val_acc:0.692]
Epoch [23/120    avg_loss:1.270, val_acc:0.704]
Epoch [24/120    avg_loss:1.199, val_acc:0.738]
Epoch [25/120    avg_loss:1.125, val_acc:0.746]
Epoch [26/120    avg_loss:1.057, val_acc:0.735]
Epoch [27/120    avg_loss:0.978, val_acc:0.746]
Epoch [28/120    avg_loss:0.896, val_acc:0.781]
Epoch [29/120    avg_loss:0.847, val_acc:0.752]
Epoch [30/120    avg_loss:0.788, val_acc:0.792]
Epoch [31/120    avg_loss:0.730, val_acc:0.785]
Epoch [32/120    avg_loss:0.658, val_acc:0.810]
Epoch [33/120    avg_loss:0.636, val_acc:0.787]
Epoch [34/120    avg_loss:0.592, val_acc:0.846]
Epoch [35/120    avg_loss:0.584, val_acc:0.856]
Epoch [36/120    avg_loss:0.611, val_acc:0.829]
Epoch [37/120    avg_loss:0.601, val_acc:0.810]
Epoch [38/120    avg_loss:0.518, val_acc:0.894]
Epoch [39/120    avg_loss:0.464, val_acc:0.865]
Epoch [40/120    avg_loss:0.471, val_acc:0.919]
Epoch [41/120    avg_loss:0.484, val_acc:0.935]
Epoch [42/120    avg_loss:0.476, val_acc:0.906]
Epoch [43/120    avg_loss:0.418, val_acc:0.931]
Epoch [44/120    avg_loss:0.336, val_acc:0.921]
Epoch [45/120    avg_loss:0.356, val_acc:0.942]
Epoch [46/120    avg_loss:0.320, val_acc:0.958]
Epoch [47/120    avg_loss:0.316, val_acc:0.935]
Epoch [48/120    avg_loss:0.345, val_acc:0.952]
Epoch [49/120    avg_loss:0.294, val_acc:0.946]
Epoch [50/120    avg_loss:0.273, val_acc:0.954]
Epoch [51/120    avg_loss:0.265, val_acc:0.952]
Epoch [52/120    avg_loss:0.279, val_acc:0.969]
Epoch [53/120    avg_loss:0.263, val_acc:0.956]
Epoch [54/120    avg_loss:0.297, val_acc:0.925]
Epoch [55/120    avg_loss:0.265, val_acc:0.931]
Epoch [56/120    avg_loss:0.372, val_acc:0.923]
Epoch [57/120    avg_loss:0.268, val_acc:0.963]
Epoch [58/120    avg_loss:0.232, val_acc:0.977]
Epoch [59/120    avg_loss:0.229, val_acc:0.967]
Epoch [60/120    avg_loss:0.239, val_acc:0.946]
Epoch [61/120    avg_loss:0.180, val_acc:0.973]
Epoch [62/120    avg_loss:0.177, val_acc:0.969]
Epoch [63/120    avg_loss:0.152, val_acc:0.963]
Epoch [64/120    avg_loss:0.170, val_acc:0.965]
Epoch [65/120    avg_loss:0.175, val_acc:0.910]
Epoch [66/120    avg_loss:0.310, val_acc:0.954]
Epoch [67/120    avg_loss:0.179, val_acc:0.973]
Epoch [68/120    avg_loss:0.139, val_acc:0.973]
Epoch [69/120    avg_loss:0.155, val_acc:0.979]
Epoch [70/120    avg_loss:0.118, val_acc:0.981]
Epoch [71/120    avg_loss:0.116, val_acc:0.977]
Epoch [72/120    avg_loss:0.158, val_acc:0.992]
Epoch [73/120    avg_loss:0.119, val_acc:0.988]
Epoch [74/120    avg_loss:0.119, val_acc:0.981]
Epoch [75/120    avg_loss:0.111, val_acc:0.988]
Epoch [76/120    avg_loss:0.098, val_acc:0.979]
Epoch [77/120    avg_loss:0.091, val_acc:0.979]
Epoch [78/120    avg_loss:0.103, val_acc:0.973]
Epoch [79/120    avg_loss:0.096, val_acc:0.971]
Epoch [80/120    avg_loss:0.108, val_acc:0.994]
Epoch [81/120    avg_loss:0.095, val_acc:0.979]
Epoch [82/120    avg_loss:0.139, val_acc:0.988]
Epoch [83/120    avg_loss:0.110, val_acc:0.985]
Epoch [84/120    avg_loss:0.133, val_acc:0.983]
Epoch [85/120    avg_loss:0.090, val_acc:0.988]
Epoch [86/120    avg_loss:0.081, val_acc:0.985]
Epoch [87/120    avg_loss:0.080, val_acc:0.990]
Epoch [88/120    avg_loss:0.078, val_acc:0.992]
Epoch [89/120    avg_loss:0.110, val_acc:0.992]
Epoch [90/120    avg_loss:0.077, val_acc:0.990]
Epoch [91/120    avg_loss:0.076, val_acc:0.981]
Epoch [92/120    avg_loss:0.073, val_acc:0.992]
Epoch [93/120    avg_loss:0.067, val_acc:0.992]
Epoch [94/120    avg_loss:0.050, val_acc:0.994]
Epoch [95/120    avg_loss:0.062, val_acc:0.990]
Epoch [96/120    avg_loss:0.053, val_acc:0.990]
Epoch [97/120    avg_loss:0.045, val_acc:0.994]
Epoch [98/120    avg_loss:0.052, val_acc:0.990]
Epoch [99/120    avg_loss:0.054, val_acc:0.994]
Epoch [100/120    avg_loss:0.045, val_acc:0.994]
Epoch [101/120    avg_loss:0.047, val_acc:0.992]
Epoch [102/120    avg_loss:0.045, val_acc:0.992]
Epoch [103/120    avg_loss:0.042, val_acc:0.992]
Epoch [104/120    avg_loss:0.044, val_acc:0.992]
Epoch [105/120    avg_loss:0.039, val_acc:0.992]
Epoch [106/120    avg_loss:0.049, val_acc:0.992]
Epoch [107/120    avg_loss:0.047, val_acc:0.994]
Epoch [108/120    avg_loss:0.043, val_acc:0.994]
Epoch [109/120    avg_loss:0.052, val_acc:0.994]
Epoch [110/120    avg_loss:0.038, val_acc:0.994]
Epoch [111/120    avg_loss:0.042, val_acc:0.994]
Epoch [112/120    avg_loss:0.043, val_acc:0.994]
Epoch [113/120    avg_loss:0.042, val_acc:0.994]
Epoch [114/120    avg_loss:0.043, val_acc:0.994]
Epoch [115/120    avg_loss:0.036, val_acc:0.994]
Epoch [116/120    avg_loss:0.047, val_acc:0.990]
Epoch [117/120    avg_loss:0.039, val_acc:0.990]
Epoch [118/120    avg_loss:0.044, val_acc:0.994]
Epoch [119/120    avg_loss:0.049, val_acc:0.994]
Epoch [120/120    avg_loss:0.042, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99486427 0.98871332 1.         0.94977169 0.92810458
 0.98329356 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919306805686071
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f37a1c3ae80>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.632, val_acc:0.115]
Epoch [2/120    avg_loss:2.581, val_acc:0.281]
Epoch [3/120    avg_loss:2.533, val_acc:0.374]
Epoch [4/120    avg_loss:2.488, val_acc:0.440]
Epoch [5/120    avg_loss:2.431, val_acc:0.442]
Epoch [6/120    avg_loss:2.370, val_acc:0.427]
Epoch [7/120    avg_loss:2.318, val_acc:0.427]
Epoch [8/120    avg_loss:2.257, val_acc:0.433]
Epoch [9/120    avg_loss:2.204, val_acc:0.473]
Epoch [10/120    avg_loss:2.141, val_acc:0.537]
Epoch [11/120    avg_loss:2.073, val_acc:0.573]
Epoch [12/120    avg_loss:2.002, val_acc:0.579]
Epoch [13/120    avg_loss:1.922, val_acc:0.596]
Epoch [14/120    avg_loss:1.854, val_acc:0.608]
Epoch [15/120    avg_loss:1.788, val_acc:0.608]
Epoch [16/120    avg_loss:1.703, val_acc:0.629]
Epoch [17/120    avg_loss:1.618, val_acc:0.650]
Epoch [18/120    avg_loss:1.531, val_acc:0.667]
Epoch [19/120    avg_loss:1.448, val_acc:0.698]
Epoch [20/120    avg_loss:1.356, val_acc:0.700]
Epoch [21/120    avg_loss:1.276, val_acc:0.727]
Epoch [22/120    avg_loss:1.201, val_acc:0.752]
Epoch [23/120    avg_loss:1.182, val_acc:0.767]
Epoch [24/120    avg_loss:1.078, val_acc:0.769]
Epoch [25/120    avg_loss:1.006, val_acc:0.798]
Epoch [26/120    avg_loss:0.928, val_acc:0.796]
Epoch [27/120    avg_loss:0.857, val_acc:0.821]
Epoch [28/120    avg_loss:0.792, val_acc:0.821]
Epoch [29/120    avg_loss:0.681, val_acc:0.887]
Epoch [30/120    avg_loss:0.662, val_acc:0.925]
Epoch [31/120    avg_loss:0.706, val_acc:0.881]
Epoch [32/120    avg_loss:0.591, val_acc:0.931]
Epoch [33/120    avg_loss:0.564, val_acc:0.908]
Epoch [34/120    avg_loss:0.480, val_acc:0.931]
Epoch [35/120    avg_loss:0.478, val_acc:0.931]
Epoch [36/120    avg_loss:0.425, val_acc:0.933]
Epoch [37/120    avg_loss:0.389, val_acc:0.931]
Epoch [38/120    avg_loss:0.401, val_acc:0.933]
Epoch [39/120    avg_loss:0.406, val_acc:0.927]
Epoch [40/120    avg_loss:0.340, val_acc:0.942]
Epoch [41/120    avg_loss:0.330, val_acc:0.938]
Epoch [42/120    avg_loss:0.380, val_acc:0.967]
Epoch [43/120    avg_loss:0.293, val_acc:0.935]
Epoch [44/120    avg_loss:0.291, val_acc:0.958]
Epoch [45/120    avg_loss:0.249, val_acc:0.956]
Epoch [46/120    avg_loss:0.245, val_acc:0.950]
Epoch [47/120    avg_loss:0.245, val_acc:0.965]
Epoch [48/120    avg_loss:0.233, val_acc:0.969]
Epoch [49/120    avg_loss:0.283, val_acc:0.950]
Epoch [50/120    avg_loss:0.237, val_acc:0.956]
Epoch [51/120    avg_loss:0.234, val_acc:0.965]
Epoch [52/120    avg_loss:0.217, val_acc:0.967]
Epoch [53/120    avg_loss:0.190, val_acc:0.971]
Epoch [54/120    avg_loss:0.253, val_acc:0.956]
Epoch [55/120    avg_loss:0.280, val_acc:0.967]
Epoch [56/120    avg_loss:0.208, val_acc:0.977]
Epoch [57/120    avg_loss:0.168, val_acc:0.975]
Epoch [58/120    avg_loss:0.165, val_acc:0.979]
Epoch [59/120    avg_loss:0.150, val_acc:0.954]
Epoch [60/120    avg_loss:0.219, val_acc:0.944]
Epoch [61/120    avg_loss:0.201, val_acc:0.946]
Epoch [62/120    avg_loss:0.158, val_acc:0.942]
Epoch [63/120    avg_loss:0.169, val_acc:0.935]
Epoch [64/120    avg_loss:0.206, val_acc:0.952]
Epoch [65/120    avg_loss:0.146, val_acc:0.960]
Epoch [66/120    avg_loss:0.137, val_acc:0.958]
Epoch [67/120    avg_loss:0.154, val_acc:0.977]
Epoch [68/120    avg_loss:0.206, val_acc:0.956]
Epoch [69/120    avg_loss:0.139, val_acc:0.971]
Epoch [70/120    avg_loss:0.150, val_acc:0.969]
Epoch [71/120    avg_loss:0.114, val_acc:0.975]
Epoch [72/120    avg_loss:0.092, val_acc:0.983]
Epoch [73/120    avg_loss:0.091, val_acc:0.988]
Epoch [74/120    avg_loss:0.078, val_acc:0.983]
Epoch [75/120    avg_loss:0.067, val_acc:0.988]
Epoch [76/120    avg_loss:0.070, val_acc:0.988]
Epoch [77/120    avg_loss:0.073, val_acc:0.985]
Epoch [78/120    avg_loss:0.080, val_acc:0.988]
Epoch [79/120    avg_loss:0.069, val_acc:0.988]
Epoch [80/120    avg_loss:0.079, val_acc:0.988]
Epoch [81/120    avg_loss:0.065, val_acc:0.990]
Epoch [82/120    avg_loss:0.067, val_acc:0.988]
Epoch [83/120    avg_loss:0.063, val_acc:0.988]
Epoch [84/120    avg_loss:0.061, val_acc:0.988]
Epoch [85/120    avg_loss:0.062, val_acc:0.985]
Epoch [86/120    avg_loss:0.067, val_acc:0.988]
Epoch [87/120    avg_loss:0.067, val_acc:0.988]
Epoch [88/120    avg_loss:0.059, val_acc:0.988]
Epoch [89/120    avg_loss:0.066, val_acc:0.990]
Epoch [90/120    avg_loss:0.068, val_acc:0.992]
Epoch [91/120    avg_loss:0.064, val_acc:0.988]
Epoch [92/120    avg_loss:0.066, val_acc:0.990]
Epoch [93/120    avg_loss:0.072, val_acc:0.988]
Epoch [94/120    avg_loss:0.061, val_acc:0.985]
Epoch [95/120    avg_loss:0.060, val_acc:0.988]
Epoch [96/120    avg_loss:0.050, val_acc:0.990]
Epoch [97/120    avg_loss:0.060, val_acc:0.988]
Epoch [98/120    avg_loss:0.069, val_acc:0.990]
Epoch [99/120    avg_loss:0.063, val_acc:0.988]
Epoch [100/120    avg_loss:0.055, val_acc:0.988]
Epoch [101/120    avg_loss:0.052, val_acc:0.992]
Epoch [102/120    avg_loss:0.055, val_acc:0.990]
Epoch [103/120    avg_loss:0.055, val_acc:0.992]
Epoch [104/120    avg_loss:0.047, val_acc:0.994]
Epoch [105/120    avg_loss:0.056, val_acc:0.992]
Epoch [106/120    avg_loss:0.051, val_acc:0.992]
Epoch [107/120    avg_loss:0.055, val_acc:0.992]
Epoch [108/120    avg_loss:0.047, val_acc:0.990]
Epoch [109/120    avg_loss:0.057, val_acc:0.992]
Epoch [110/120    avg_loss:0.052, val_acc:0.994]
Epoch [111/120    avg_loss:0.050, val_acc:0.992]
Epoch [112/120    avg_loss:0.052, val_acc:0.990]
Epoch [113/120    avg_loss:0.053, val_acc:0.992]
Epoch [114/120    avg_loss:0.046, val_acc:0.992]
Epoch [115/120    avg_loss:0.059, val_acc:0.992]
Epoch [116/120    avg_loss:0.052, val_acc:0.990]
Epoch [117/120    avg_loss:0.049, val_acc:0.992]
Epoch [118/120    avg_loss:0.051, val_acc:0.992]
Epoch [119/120    avg_loss:0.044, val_acc:0.994]
Epoch [120/120    avg_loss:0.044, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.996337   0.99545455 0.98454746 0.93709328 0.92413793
 0.98800959 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9914552043999725
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f099dbe10>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.127]
Epoch [2/120    avg_loss:2.583, val_acc:0.269]
Epoch [3/120    avg_loss:2.543, val_acc:0.329]
Epoch [4/120    avg_loss:2.501, val_acc:0.415]
Epoch [5/120    avg_loss:2.461, val_acc:0.423]
Epoch [6/120    avg_loss:2.420, val_acc:0.460]
Epoch [7/120    avg_loss:2.377, val_acc:0.460]
Epoch [8/120    avg_loss:2.324, val_acc:0.438]
Epoch [9/120    avg_loss:2.282, val_acc:0.433]
Epoch [10/120    avg_loss:2.240, val_acc:0.471]
Epoch [11/120    avg_loss:2.188, val_acc:0.463]
Epoch [12/120    avg_loss:2.126, val_acc:0.492]
Epoch [13/120    avg_loss:2.090, val_acc:0.454]
Epoch [14/120    avg_loss:2.049, val_acc:0.533]
Epoch [15/120    avg_loss:1.975, val_acc:0.554]
Epoch [16/120    avg_loss:1.897, val_acc:0.544]
Epoch [17/120    avg_loss:1.880, val_acc:0.608]
Epoch [18/120    avg_loss:1.796, val_acc:0.621]
Epoch [19/120    avg_loss:1.734, val_acc:0.660]
Epoch [20/120    avg_loss:1.666, val_acc:0.673]
Epoch [21/120    avg_loss:1.579, val_acc:0.679]
Epoch [22/120    avg_loss:1.510, val_acc:0.721]
Epoch [23/120    avg_loss:1.444, val_acc:0.731]
Epoch [24/120    avg_loss:1.336, val_acc:0.750]
Epoch [25/120    avg_loss:1.260, val_acc:0.746]
Epoch [26/120    avg_loss:1.151, val_acc:0.775]
Epoch [27/120    avg_loss:1.049, val_acc:0.785]
Epoch [28/120    avg_loss:0.981, val_acc:0.819]
Epoch [29/120    avg_loss:0.894, val_acc:0.842]
Epoch [30/120    avg_loss:0.830, val_acc:0.887]
Epoch [31/120    avg_loss:0.795, val_acc:0.917]
Epoch [32/120    avg_loss:0.708, val_acc:0.902]
Epoch [33/120    avg_loss:0.637, val_acc:0.879]
Epoch [34/120    avg_loss:0.616, val_acc:0.927]
Epoch [35/120    avg_loss:0.575, val_acc:0.921]
Epoch [36/120    avg_loss:0.534, val_acc:0.921]
Epoch [37/120    avg_loss:0.537, val_acc:0.915]
Epoch [38/120    avg_loss:0.469, val_acc:0.944]
Epoch [39/120    avg_loss:0.484, val_acc:0.915]
Epoch [40/120    avg_loss:0.457, val_acc:0.931]
Epoch [41/120    avg_loss:0.470, val_acc:0.935]
Epoch [42/120    avg_loss:0.360, val_acc:0.925]
Epoch [43/120    avg_loss:0.355, val_acc:0.952]
Epoch [44/120    avg_loss:0.357, val_acc:0.946]
Epoch [45/120    avg_loss:0.352, val_acc:0.944]
Epoch [46/120    avg_loss:0.310, val_acc:0.952]
Epoch [47/120    avg_loss:0.307, val_acc:0.944]
Epoch [48/120    avg_loss:0.341, val_acc:0.946]
Epoch [49/120    avg_loss:0.302, val_acc:0.948]
Epoch [50/120    avg_loss:0.275, val_acc:0.958]
Epoch [51/120    avg_loss:0.264, val_acc:0.969]
Epoch [52/120    avg_loss:0.237, val_acc:0.950]
Epoch [53/120    avg_loss:0.213, val_acc:0.956]
Epoch [54/120    avg_loss:0.229, val_acc:0.967]
Epoch [55/120    avg_loss:0.215, val_acc:0.960]
Epoch [56/120    avg_loss:0.231, val_acc:0.958]
Epoch [57/120    avg_loss:0.247, val_acc:0.938]
Epoch [58/120    avg_loss:0.185, val_acc:0.969]
Epoch [59/120    avg_loss:0.196, val_acc:0.967]
Epoch [60/120    avg_loss:0.225, val_acc:0.963]
Epoch [61/120    avg_loss:0.193, val_acc:0.973]
Epoch [62/120    avg_loss:0.186, val_acc:0.963]
Epoch [63/120    avg_loss:0.176, val_acc:0.971]
Epoch [64/120    avg_loss:0.244, val_acc:0.950]
Epoch [65/120    avg_loss:0.270, val_acc:0.973]
Epoch [66/120    avg_loss:0.189, val_acc:0.971]
Epoch [67/120    avg_loss:0.172, val_acc:0.977]
Epoch [68/120    avg_loss:0.143, val_acc:0.971]
Epoch [69/120    avg_loss:0.131, val_acc:0.979]
Epoch [70/120    avg_loss:0.101, val_acc:0.977]
Epoch [71/120    avg_loss:0.122, val_acc:0.977]
Epoch [72/120    avg_loss:0.106, val_acc:0.975]
Epoch [73/120    avg_loss:0.107, val_acc:0.977]
Epoch [74/120    avg_loss:0.081, val_acc:0.975]
Epoch [75/120    avg_loss:0.090, val_acc:0.975]
Epoch [76/120    avg_loss:0.124, val_acc:0.967]
Epoch [77/120    avg_loss:0.108, val_acc:0.975]
Epoch [78/120    avg_loss:0.114, val_acc:0.979]
Epoch [79/120    avg_loss:0.104, val_acc:0.981]
Epoch [80/120    avg_loss:0.082, val_acc:0.973]
Epoch [81/120    avg_loss:0.085, val_acc:0.981]
Epoch [82/120    avg_loss:0.089, val_acc:0.981]
Epoch [83/120    avg_loss:0.098, val_acc:0.975]
Epoch [84/120    avg_loss:0.090, val_acc:0.981]
Epoch [85/120    avg_loss:0.067, val_acc:0.983]
Epoch [86/120    avg_loss:0.058, val_acc:0.988]
Epoch [87/120    avg_loss:0.061, val_acc:0.985]
Epoch [88/120    avg_loss:0.055, val_acc:0.981]
Epoch [89/120    avg_loss:0.055, val_acc:0.981]
Epoch [90/120    avg_loss:0.071, val_acc:0.979]
Epoch [91/120    avg_loss:0.091, val_acc:0.985]
Epoch [92/120    avg_loss:0.067, val_acc:0.988]
Epoch [93/120    avg_loss:0.049, val_acc:0.983]
Epoch [94/120    avg_loss:0.039, val_acc:0.981]
Epoch [95/120    avg_loss:0.060, val_acc:0.979]
Epoch [96/120    avg_loss:0.072, val_acc:0.983]
Epoch [97/120    avg_loss:0.049, val_acc:0.990]
Epoch [98/120    avg_loss:0.037, val_acc:0.988]
Epoch [99/120    avg_loss:0.046, val_acc:0.990]
Epoch [100/120    avg_loss:0.075, val_acc:0.985]
Epoch [101/120    avg_loss:0.087, val_acc:0.981]
Epoch [102/120    avg_loss:0.089, val_acc:0.977]
Epoch [103/120    avg_loss:0.054, val_acc:0.983]
Epoch [104/120    avg_loss:0.050, val_acc:0.981]
Epoch [105/120    avg_loss:0.061, val_acc:0.977]
Epoch [106/120    avg_loss:0.047, val_acc:0.979]
Epoch [107/120    avg_loss:0.058, val_acc:0.956]
Epoch [108/120    avg_loss:0.060, val_acc:0.992]
Epoch [109/120    avg_loss:0.056, val_acc:0.990]
Epoch [110/120    avg_loss:0.045, val_acc:0.988]
Epoch [111/120    avg_loss:0.033, val_acc:0.994]
Epoch [112/120    avg_loss:0.036, val_acc:0.983]
Epoch [113/120    avg_loss:0.046, val_acc:0.990]
Epoch [114/120    avg_loss:0.028, val_acc:0.992]
Epoch [115/120    avg_loss:0.024, val_acc:0.992]
Epoch [116/120    avg_loss:0.021, val_acc:0.994]
Epoch [117/120    avg_loss:0.026, val_acc:0.994]
Epoch [118/120    avg_loss:0.022, val_acc:0.992]
Epoch [119/120    avg_loss:0.021, val_acc:0.994]
Epoch [120/120    avg_loss:0.022, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.95522388 0.92363636
 0.99038462 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.993828257420073
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e45876e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.612, val_acc:0.148]
Epoch [2/120    avg_loss:2.563, val_acc:0.383]
Epoch [3/120    avg_loss:2.517, val_acc:0.415]
Epoch [4/120    avg_loss:2.470, val_acc:0.419]
Epoch [5/120    avg_loss:2.426, val_acc:0.369]
Epoch [6/120    avg_loss:2.386, val_acc:0.375]
Epoch [7/120    avg_loss:2.345, val_acc:0.375]
Epoch [8/120    avg_loss:2.297, val_acc:0.398]
Epoch [9/120    avg_loss:2.245, val_acc:0.412]
Epoch [10/120    avg_loss:2.182, val_acc:0.442]
Epoch [11/120    avg_loss:2.134, val_acc:0.452]
Epoch [12/120    avg_loss:2.075, val_acc:0.504]
Epoch [13/120    avg_loss:2.019, val_acc:0.510]
Epoch [14/120    avg_loss:1.967, val_acc:0.523]
Epoch [15/120    avg_loss:1.899, val_acc:0.585]
Epoch [16/120    avg_loss:1.819, val_acc:0.579]
Epoch [17/120    avg_loss:1.744, val_acc:0.613]
Epoch [18/120    avg_loss:1.685, val_acc:0.652]
Epoch [19/120    avg_loss:1.623, val_acc:0.671]
Epoch [20/120    avg_loss:1.520, val_acc:0.677]
Epoch [21/120    avg_loss:1.466, val_acc:0.733]
Epoch [22/120    avg_loss:1.353, val_acc:0.746]
Epoch [23/120    avg_loss:1.250, val_acc:0.783]
Epoch [24/120    avg_loss:1.158, val_acc:0.790]
Epoch [25/120    avg_loss:1.049, val_acc:0.777]
Epoch [26/120    avg_loss:0.974, val_acc:0.829]
Epoch [27/120    avg_loss:0.978, val_acc:0.773]
Epoch [28/120    avg_loss:0.860, val_acc:0.825]
Epoch [29/120    avg_loss:0.842, val_acc:0.800]
Epoch [30/120    avg_loss:0.743, val_acc:0.817]
Epoch [31/120    avg_loss:0.745, val_acc:0.792]
Epoch [32/120    avg_loss:0.713, val_acc:0.808]
Epoch [33/120    avg_loss:0.656, val_acc:0.850]
Epoch [34/120    avg_loss:0.596, val_acc:0.912]
Epoch [35/120    avg_loss:0.538, val_acc:0.904]
Epoch [36/120    avg_loss:0.511, val_acc:0.890]
Epoch [37/120    avg_loss:0.489, val_acc:0.935]
Epoch [38/120    avg_loss:0.487, val_acc:0.929]
Epoch [39/120    avg_loss:0.473, val_acc:0.940]
Epoch [40/120    avg_loss:0.401, val_acc:0.950]
Epoch [41/120    avg_loss:0.402, val_acc:0.938]
Epoch [42/120    avg_loss:0.383, val_acc:0.906]
Epoch [43/120    avg_loss:0.359, val_acc:0.948]
Epoch [44/120    avg_loss:0.343, val_acc:0.963]
Epoch [45/120    avg_loss:0.382, val_acc:0.944]
Epoch [46/120    avg_loss:0.321, val_acc:0.971]
Epoch [47/120    avg_loss:0.328, val_acc:0.946]
Epoch [48/120    avg_loss:0.316, val_acc:0.969]
Epoch [49/120    avg_loss:0.305, val_acc:0.946]
Epoch [50/120    avg_loss:0.279, val_acc:0.871]
Epoch [51/120    avg_loss:0.277, val_acc:0.927]
Epoch [52/120    avg_loss:0.251, val_acc:0.963]
Epoch [53/120    avg_loss:0.296, val_acc:0.948]
Epoch [54/120    avg_loss:0.277, val_acc:0.952]
Epoch [55/120    avg_loss:0.244, val_acc:0.950]
Epoch [56/120    avg_loss:0.203, val_acc:0.963]
Epoch [57/120    avg_loss:0.201, val_acc:0.979]
Epoch [58/120    avg_loss:0.209, val_acc:0.946]
Epoch [59/120    avg_loss:0.205, val_acc:0.973]
Epoch [60/120    avg_loss:0.187, val_acc:0.944]
Epoch [61/120    avg_loss:0.204, val_acc:0.960]
Epoch [62/120    avg_loss:0.206, val_acc:0.973]
Epoch [63/120    avg_loss:0.281, val_acc:0.933]
Epoch [64/120    avg_loss:0.288, val_acc:0.969]
Epoch [65/120    avg_loss:0.191, val_acc:0.977]
Epoch [66/120    avg_loss:0.182, val_acc:0.977]
Epoch [67/120    avg_loss:0.168, val_acc:0.960]
Epoch [68/120    avg_loss:0.243, val_acc:0.979]
Epoch [69/120    avg_loss:0.180, val_acc:0.975]
Epoch [70/120    avg_loss:0.141, val_acc:0.983]
Epoch [71/120    avg_loss:0.176, val_acc:0.985]
Epoch [72/120    avg_loss:0.166, val_acc:0.969]
Epoch [73/120    avg_loss:0.147, val_acc:0.973]
Epoch [74/120    avg_loss:0.121, val_acc:0.975]
Epoch [75/120    avg_loss:0.124, val_acc:0.988]
Epoch [76/120    avg_loss:0.114, val_acc:0.979]
Epoch [77/120    avg_loss:0.108, val_acc:0.979]
Epoch [78/120    avg_loss:0.096, val_acc:0.988]
Epoch [79/120    avg_loss:0.137, val_acc:0.988]
Epoch [80/120    avg_loss:0.151, val_acc:0.992]
Epoch [81/120    avg_loss:0.128, val_acc:0.971]
Epoch [82/120    avg_loss:0.124, val_acc:0.990]
Epoch [83/120    avg_loss:0.075, val_acc:0.983]
Epoch [84/120    avg_loss:0.092, val_acc:0.985]
Epoch [85/120    avg_loss:0.078, val_acc:0.992]
Epoch [86/120    avg_loss:0.066, val_acc:0.994]
Epoch [87/120    avg_loss:0.069, val_acc:0.994]
Epoch [88/120    avg_loss:0.071, val_acc:0.990]
Epoch [89/120    avg_loss:0.096, val_acc:0.990]
Epoch [90/120    avg_loss:0.085, val_acc:0.969]
Epoch [91/120    avg_loss:0.108, val_acc:0.973]
Epoch [92/120    avg_loss:0.068, val_acc:0.996]
Epoch [93/120    avg_loss:0.073, val_acc:0.992]
Epoch [94/120    avg_loss:0.071, val_acc:0.985]
Epoch [95/120    avg_loss:0.060, val_acc:0.996]
Epoch [96/120    avg_loss:0.070, val_acc:0.992]
Epoch [97/120    avg_loss:0.045, val_acc:0.992]
Epoch [98/120    avg_loss:0.046, val_acc:0.996]
Epoch [99/120    avg_loss:0.049, val_acc:0.992]
Epoch [100/120    avg_loss:0.035, val_acc:0.998]
Epoch [101/120    avg_loss:0.047, val_acc:0.996]
Epoch [102/120    avg_loss:0.054, val_acc:0.985]
Epoch [103/120    avg_loss:0.053, val_acc:0.992]
Epoch [104/120    avg_loss:0.073, val_acc:0.990]
Epoch [105/120    avg_loss:0.067, val_acc:0.994]
Epoch [106/120    avg_loss:0.034, val_acc:0.998]
Epoch [107/120    avg_loss:0.041, val_acc:0.994]
Epoch [108/120    avg_loss:0.029, val_acc:0.996]
Epoch [109/120    avg_loss:0.030, val_acc:0.994]
Epoch [110/120    avg_loss:0.036, val_acc:0.994]
Epoch [111/120    avg_loss:0.052, val_acc:0.990]
Epoch [112/120    avg_loss:0.066, val_acc:0.983]
Epoch [113/120    avg_loss:0.068, val_acc:0.971]
Epoch [114/120    avg_loss:0.098, val_acc:0.994]
Epoch [115/120    avg_loss:0.044, val_acc:0.996]
Epoch [116/120    avg_loss:0.044, val_acc:0.996]
Epoch [117/120    avg_loss:0.033, val_acc:0.992]
Epoch [118/120    avg_loss:0.052, val_acc:0.988]
Epoch [119/120    avg_loss:0.076, val_acc:0.992]
Epoch [120/120    avg_loss:0.058, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  15   0   0   0   0   0   0   7   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 0.99560117 1.         1.         0.92760181 0.91525424
 0.98564593 1.         1.         1.         1.         1.
 0.99233297 1.        ]

Kappa:
0.9909802817277776
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d1b43eda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.632, val_acc:0.100]
Epoch [2/120    avg_loss:2.583, val_acc:0.113]
Epoch [3/120    avg_loss:2.535, val_acc:0.200]
Epoch [4/120    avg_loss:2.494, val_acc:0.221]
Epoch [5/120    avg_loss:2.460, val_acc:0.310]
Epoch [6/120    avg_loss:2.413, val_acc:0.431]
Epoch [7/120    avg_loss:2.371, val_acc:0.473]
Epoch [8/120    avg_loss:2.333, val_acc:0.475]
Epoch [9/120    avg_loss:2.280, val_acc:0.471]
Epoch [10/120    avg_loss:2.235, val_acc:0.498]
Epoch [11/120    avg_loss:2.178, val_acc:0.585]
Epoch [12/120    avg_loss:2.121, val_acc:0.629]
Epoch [13/120    avg_loss:2.047, val_acc:0.646]
Epoch [14/120    avg_loss:1.981, val_acc:0.656]
Epoch [15/120    avg_loss:1.903, val_acc:0.698]
Epoch [16/120    avg_loss:1.828, val_acc:0.681]
Epoch [17/120    avg_loss:1.760, val_acc:0.683]
Epoch [18/120    avg_loss:1.639, val_acc:0.735]
Epoch [19/120    avg_loss:1.550, val_acc:0.725]
Epoch [20/120    avg_loss:1.447, val_acc:0.742]
Epoch [21/120    avg_loss:1.357, val_acc:0.746]
Epoch [22/120    avg_loss:1.289, val_acc:0.760]
Epoch [23/120    avg_loss:1.211, val_acc:0.792]
Epoch [24/120    avg_loss:1.133, val_acc:0.827]
Epoch [25/120    avg_loss:1.028, val_acc:0.779]
Epoch [26/120    avg_loss:0.958, val_acc:0.902]
Epoch [27/120    avg_loss:0.949, val_acc:0.856]
Epoch [28/120    avg_loss:0.870, val_acc:0.898]
Epoch [29/120    avg_loss:0.772, val_acc:0.925]
Epoch [30/120    avg_loss:0.723, val_acc:0.942]
Epoch [31/120    avg_loss:0.686, val_acc:0.944]
Epoch [32/120    avg_loss:0.658, val_acc:0.917]
Epoch [33/120    avg_loss:0.540, val_acc:0.944]
Epoch [34/120    avg_loss:0.505, val_acc:0.933]
Epoch [35/120    avg_loss:0.528, val_acc:0.938]
Epoch [36/120    avg_loss:0.477, val_acc:0.942]
Epoch [37/120    avg_loss:0.436, val_acc:0.931]
Epoch [38/120    avg_loss:0.486, val_acc:0.925]
Epoch [39/120    avg_loss:0.446, val_acc:0.948]
Epoch [40/120    avg_loss:0.384, val_acc:0.942]
Epoch [41/120    avg_loss:0.343, val_acc:0.960]
Epoch [42/120    avg_loss:0.397, val_acc:0.912]
Epoch [43/120    avg_loss:0.412, val_acc:0.946]
Epoch [44/120    avg_loss:0.355, val_acc:0.948]
Epoch [45/120    avg_loss:0.346, val_acc:0.954]
Epoch [46/120    avg_loss:0.307, val_acc:0.965]
Epoch [47/120    avg_loss:0.262, val_acc:0.975]
Epoch [48/120    avg_loss:0.279, val_acc:0.977]
Epoch [49/120    avg_loss:0.233, val_acc:0.958]
Epoch [50/120    avg_loss:0.228, val_acc:0.963]
Epoch [51/120    avg_loss:0.216, val_acc:0.971]
Epoch [52/120    avg_loss:0.190, val_acc:0.981]
Epoch [53/120    avg_loss:0.237, val_acc:0.950]
Epoch [54/120    avg_loss:0.283, val_acc:0.958]
Epoch [55/120    avg_loss:0.239, val_acc:0.965]
Epoch [56/120    avg_loss:0.189, val_acc:0.956]
Epoch [57/120    avg_loss:0.197, val_acc:0.958]
Epoch [58/120    avg_loss:0.173, val_acc:0.973]
Epoch [59/120    avg_loss:0.162, val_acc:0.983]
Epoch [60/120    avg_loss:0.140, val_acc:0.979]
Epoch [61/120    avg_loss:0.172, val_acc:0.971]
Epoch [62/120    avg_loss:0.153, val_acc:0.973]
Epoch [63/120    avg_loss:0.178, val_acc:0.931]
Epoch [64/120    avg_loss:0.314, val_acc:0.971]
Epoch [65/120    avg_loss:0.181, val_acc:0.985]
Epoch [66/120    avg_loss:0.128, val_acc:0.979]
Epoch [67/120    avg_loss:0.119, val_acc:0.983]
Epoch [68/120    avg_loss:0.116, val_acc:0.983]
Epoch [69/120    avg_loss:0.135, val_acc:0.990]
Epoch [70/120    avg_loss:0.150, val_acc:0.975]
Epoch [71/120    avg_loss:0.125, val_acc:0.975]
Epoch [72/120    avg_loss:0.123, val_acc:0.977]
Epoch [73/120    avg_loss:0.132, val_acc:0.988]
Epoch [74/120    avg_loss:0.114, val_acc:0.979]
Epoch [75/120    avg_loss:0.094, val_acc:0.981]
Epoch [76/120    avg_loss:0.094, val_acc:0.990]
Epoch [77/120    avg_loss:0.083, val_acc:0.985]
Epoch [78/120    avg_loss:0.060, val_acc:0.983]
Epoch [79/120    avg_loss:0.078, val_acc:0.985]
Epoch [80/120    avg_loss:0.069, val_acc:0.990]
Epoch [81/120    avg_loss:0.183, val_acc:0.977]
Epoch [82/120    avg_loss:0.101, val_acc:0.988]
Epoch [83/120    avg_loss:0.116, val_acc:0.969]
Epoch [84/120    avg_loss:0.131, val_acc:0.983]
Epoch [85/120    avg_loss:0.079, val_acc:0.965]
Epoch [86/120    avg_loss:0.160, val_acc:0.935]
Epoch [87/120    avg_loss:0.126, val_acc:0.977]
Epoch [88/120    avg_loss:0.123, val_acc:0.969]
Epoch [89/120    avg_loss:0.083, val_acc:0.985]
Epoch [90/120    avg_loss:0.116, val_acc:0.983]
Epoch [91/120    avg_loss:0.081, val_acc:0.981]
Epoch [92/120    avg_loss:0.069, val_acc:0.988]
Epoch [93/120    avg_loss:0.077, val_acc:0.990]
Epoch [94/120    avg_loss:0.061, val_acc:0.985]
Epoch [95/120    avg_loss:0.054, val_acc:0.990]
Epoch [96/120    avg_loss:0.063, val_acc:0.994]
Epoch [97/120    avg_loss:0.040, val_acc:0.992]
Epoch [98/120    avg_loss:0.045, val_acc:0.990]
Epoch [99/120    avg_loss:0.047, val_acc:0.994]
Epoch [100/120    avg_loss:0.040, val_acc:0.992]
Epoch [101/120    avg_loss:0.036, val_acc:0.992]
Epoch [102/120    avg_loss:0.031, val_acc:0.992]
Epoch [103/120    avg_loss:0.028, val_acc:0.988]
Epoch [104/120    avg_loss:0.032, val_acc:0.990]
Epoch [105/120    avg_loss:0.029, val_acc:0.992]
Epoch [106/120    avg_loss:0.039, val_acc:0.988]
Epoch [107/120    avg_loss:0.042, val_acc:0.998]
Epoch [108/120    avg_loss:0.039, val_acc:0.985]
Epoch [109/120    avg_loss:0.036, val_acc:0.985]
Epoch [110/120    avg_loss:0.027, val_acc:0.996]
Epoch [111/120    avg_loss:0.028, val_acc:0.998]
Epoch [112/120    avg_loss:0.037, val_acc:0.994]
Epoch [113/120    avg_loss:0.030, val_acc:0.994]
Epoch [114/120    avg_loss:0.025, val_acc:0.996]
Epoch [115/120    avg_loss:0.027, val_acc:0.996]
Epoch [116/120    avg_loss:0.024, val_acc:0.996]
Epoch [117/120    avg_loss:0.026, val_acc:0.994]
Epoch [118/120    avg_loss:0.032, val_acc:0.988]
Epoch [119/120    avg_loss:0.028, val_acc:0.996]
Epoch [120/120    avg_loss:0.029, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 0.99412628 0.99545455 1.         0.97995546 0.96949153
 0.98095238 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9954906634822482
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91a2825f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.048]
Epoch [2/120    avg_loss:2.593, val_acc:0.190]
Epoch [3/120    avg_loss:2.544, val_acc:0.304]
Epoch [4/120    avg_loss:2.505, val_acc:0.317]
Epoch [5/120    avg_loss:2.462, val_acc:0.319]
Epoch [6/120    avg_loss:2.419, val_acc:0.331]
Epoch [7/120    avg_loss:2.383, val_acc:0.396]
Epoch [8/120    avg_loss:2.331, val_acc:0.492]
Epoch [9/120    avg_loss:2.272, val_acc:0.546]
Epoch [10/120    avg_loss:2.213, val_acc:0.600]
Epoch [11/120    avg_loss:2.142, val_acc:0.617]
Epoch [12/120    avg_loss:2.068, val_acc:0.600]
Epoch [13/120    avg_loss:2.009, val_acc:0.602]
Epoch [14/120    avg_loss:1.913, val_acc:0.617]
Epoch [15/120    avg_loss:1.840, val_acc:0.579]
Epoch [16/120    avg_loss:1.760, val_acc:0.629]
Epoch [17/120    avg_loss:1.656, val_acc:0.650]
Epoch [18/120    avg_loss:1.595, val_acc:0.654]
Epoch [19/120    avg_loss:1.532, val_acc:0.665]
Epoch [20/120    avg_loss:1.451, val_acc:0.669]
Epoch [21/120    avg_loss:1.374, val_acc:0.692]
Epoch [22/120    avg_loss:1.302, val_acc:0.688]
Epoch [23/120    avg_loss:1.250, val_acc:0.706]
Epoch [24/120    avg_loss:1.179, val_acc:0.702]
Epoch [25/120    avg_loss:1.115, val_acc:0.758]
Epoch [26/120    avg_loss:1.036, val_acc:0.825]
Epoch [27/120    avg_loss:0.985, val_acc:0.758]
Epoch [28/120    avg_loss:0.916, val_acc:0.810]
Epoch [29/120    avg_loss:0.840, val_acc:0.875]
Epoch [30/120    avg_loss:0.786, val_acc:0.879]
Epoch [31/120    avg_loss:0.749, val_acc:0.906]
Epoch [32/120    avg_loss:0.736, val_acc:0.858]
Epoch [33/120    avg_loss:0.648, val_acc:0.898]
Epoch [34/120    avg_loss:0.637, val_acc:0.906]
Epoch [35/120    avg_loss:0.628, val_acc:0.921]
Epoch [36/120    avg_loss:0.576, val_acc:0.942]
Epoch [37/120    avg_loss:0.502, val_acc:0.898]
Epoch [38/120    avg_loss:0.529, val_acc:0.912]
Epoch [39/120    avg_loss:0.457, val_acc:0.940]
Epoch [40/120    avg_loss:0.453, val_acc:0.948]
Epoch [41/120    avg_loss:0.415, val_acc:0.948]
Epoch [42/120    avg_loss:0.387, val_acc:0.954]
Epoch [43/120    avg_loss:0.385, val_acc:0.933]
Epoch [44/120    avg_loss:0.382, val_acc:0.967]
Epoch [45/120    avg_loss:0.328, val_acc:0.967]
Epoch [46/120    avg_loss:0.316, val_acc:0.975]
Epoch [47/120    avg_loss:0.329, val_acc:0.967]
Epoch [48/120    avg_loss:0.311, val_acc:0.967]
Epoch [49/120    avg_loss:0.300, val_acc:0.948]
Epoch [50/120    avg_loss:0.378, val_acc:0.944]
Epoch [51/120    avg_loss:0.316, val_acc:0.935]
Epoch [52/120    avg_loss:0.365, val_acc:0.958]
Epoch [53/120    avg_loss:0.305, val_acc:0.973]
Epoch [54/120    avg_loss:0.243, val_acc:0.952]
Epoch [55/120    avg_loss:0.278, val_acc:0.975]
Epoch [56/120    avg_loss:0.250, val_acc:0.946]
Epoch [57/120    avg_loss:0.293, val_acc:0.963]
Epoch [58/120    avg_loss:0.230, val_acc:0.967]
Epoch [59/120    avg_loss:0.280, val_acc:0.935]
Epoch [60/120    avg_loss:0.295, val_acc:0.963]
Epoch [61/120    avg_loss:0.217, val_acc:0.981]
Epoch [62/120    avg_loss:0.205, val_acc:0.965]
Epoch [63/120    avg_loss:0.164, val_acc:0.979]
Epoch [64/120    avg_loss:0.175, val_acc:0.977]
Epoch [65/120    avg_loss:0.186, val_acc:0.977]
Epoch [66/120    avg_loss:0.186, val_acc:0.981]
Epoch [67/120    avg_loss:0.158, val_acc:0.988]
Epoch [68/120    avg_loss:0.165, val_acc:0.992]
Epoch [69/120    avg_loss:0.198, val_acc:0.975]
Epoch [70/120    avg_loss:0.170, val_acc:0.969]
Epoch [71/120    avg_loss:0.149, val_acc:0.981]
Epoch [72/120    avg_loss:0.111, val_acc:0.985]
Epoch [73/120    avg_loss:0.107, val_acc:0.988]
Epoch [74/120    avg_loss:0.101, val_acc:0.977]
Epoch [75/120    avg_loss:0.115, val_acc:0.979]
Epoch [76/120    avg_loss:0.118, val_acc:0.977]
Epoch [77/120    avg_loss:0.118, val_acc:0.969]
Epoch [78/120    avg_loss:0.104, val_acc:0.967]
Epoch [79/120    avg_loss:0.090, val_acc:0.990]
Epoch [80/120    avg_loss:0.102, val_acc:0.994]
Epoch [81/120    avg_loss:0.093, val_acc:0.983]
Epoch [82/120    avg_loss:0.085, val_acc:0.998]
Epoch [83/120    avg_loss:0.080, val_acc:0.988]
Epoch [84/120    avg_loss:0.125, val_acc:0.975]
Epoch [85/120    avg_loss:0.087, val_acc:0.979]
Epoch [86/120    avg_loss:0.123, val_acc:0.977]
Epoch [87/120    avg_loss:0.102, val_acc:0.988]
Epoch [88/120    avg_loss:0.069, val_acc:0.983]
Epoch [89/120    avg_loss:0.068, val_acc:0.988]
Epoch [90/120    avg_loss:0.096, val_acc:0.981]
Epoch [91/120    avg_loss:0.121, val_acc:0.990]
Epoch [92/120    avg_loss:0.090, val_acc:0.979]
Epoch [93/120    avg_loss:0.070, val_acc:0.990]
Epoch [94/120    avg_loss:0.063, val_acc:0.994]
Epoch [95/120    avg_loss:0.054, val_acc:0.994]
Epoch [96/120    avg_loss:0.062, val_acc:0.992]
Epoch [97/120    avg_loss:0.034, val_acc:0.992]
Epoch [98/120    avg_loss:0.035, val_acc:0.996]
Epoch [99/120    avg_loss:0.035, val_acc:0.992]
Epoch [100/120    avg_loss:0.039, val_acc:0.990]
Epoch [101/120    avg_loss:0.035, val_acc:0.992]
Epoch [102/120    avg_loss:0.033, val_acc:0.992]
Epoch [103/120    avg_loss:0.037, val_acc:0.994]
Epoch [104/120    avg_loss:0.033, val_acc:0.994]
Epoch [105/120    avg_loss:0.035, val_acc:0.994]
Epoch [106/120    avg_loss:0.038, val_acc:0.996]
Epoch [107/120    avg_loss:0.038, val_acc:0.996]
Epoch [108/120    avg_loss:0.029, val_acc:0.996]
Epoch [109/120    avg_loss:0.034, val_acc:0.996]
Epoch [110/120    avg_loss:0.030, val_acc:0.996]
Epoch [111/120    avg_loss:0.036, val_acc:0.994]
Epoch [112/120    avg_loss:0.038, val_acc:0.994]
Epoch [113/120    avg_loss:0.033, val_acc:0.994]
Epoch [114/120    avg_loss:0.033, val_acc:0.994]
Epoch [115/120    avg_loss:0.030, val_acc:0.994]
Epoch [116/120    avg_loss:0.034, val_acc:0.994]
Epoch [117/120    avg_loss:0.031, val_acc:0.994]
Epoch [118/120    avg_loss:0.030, val_acc:0.994]
Epoch [119/120    avg_loss:0.028, val_acc:0.994]
Epoch [120/120    avg_loss:0.037, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.996337   1.         1.         0.95652174 0.92957746
 0.98800959 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9940660351848434
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff42b359e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.104]
Epoch [2/120    avg_loss:2.603, val_acc:0.129]
Epoch [3/120    avg_loss:2.570, val_acc:0.223]
Epoch [4/120    avg_loss:2.535, val_acc:0.277]
Epoch [5/120    avg_loss:2.493, val_acc:0.408]
Epoch [6/120    avg_loss:2.464, val_acc:0.440]
Epoch [7/120    avg_loss:2.419, val_acc:0.444]
Epoch [8/120    avg_loss:2.379, val_acc:0.471]
Epoch [9/120    avg_loss:2.340, val_acc:0.483]
Epoch [10/120    avg_loss:2.295, val_acc:0.492]
Epoch [11/120    avg_loss:2.249, val_acc:0.521]
Epoch [12/120    avg_loss:2.188, val_acc:0.546]
Epoch [13/120    avg_loss:2.128, val_acc:0.550]
Epoch [14/120    avg_loss:2.070, val_acc:0.592]
Epoch [15/120    avg_loss:1.989, val_acc:0.579]
Epoch [16/120    avg_loss:1.920, val_acc:0.602]
Epoch [17/120    avg_loss:1.855, val_acc:0.617]
Epoch [18/120    avg_loss:1.786, val_acc:0.629]
Epoch [19/120    avg_loss:1.734, val_acc:0.629]
Epoch [20/120    avg_loss:1.644, val_acc:0.635]
Epoch [21/120    avg_loss:1.607, val_acc:0.673]
Epoch [22/120    avg_loss:1.520, val_acc:0.694]
Epoch [23/120    avg_loss:1.451, val_acc:0.690]
Epoch [24/120    avg_loss:1.417, val_acc:0.694]
Epoch [25/120    avg_loss:1.360, val_acc:0.727]
Epoch [26/120    avg_loss:1.282, val_acc:0.762]
Epoch [27/120    avg_loss:1.225, val_acc:0.744]
Epoch [28/120    avg_loss:1.179, val_acc:0.771]
Epoch [29/120    avg_loss:1.109, val_acc:0.769]
Epoch [30/120    avg_loss:1.056, val_acc:0.783]
Epoch [31/120    avg_loss:0.960, val_acc:0.794]
Epoch [32/120    avg_loss:0.858, val_acc:0.831]
Epoch [33/120    avg_loss:0.831, val_acc:0.794]
Epoch [34/120    avg_loss:0.790, val_acc:0.875]
Epoch [35/120    avg_loss:0.693, val_acc:0.869]
Epoch [36/120    avg_loss:0.679, val_acc:0.821]
Epoch [37/120    avg_loss:0.689, val_acc:0.794]
Epoch [38/120    avg_loss:0.689, val_acc:0.865]
Epoch [39/120    avg_loss:0.594, val_acc:0.908]
Epoch [40/120    avg_loss:0.576, val_acc:0.885]
Epoch [41/120    avg_loss:0.623, val_acc:0.921]
Epoch [42/120    avg_loss:0.519, val_acc:0.869]
Epoch [43/120    avg_loss:0.464, val_acc:0.852]
Epoch [44/120    avg_loss:0.502, val_acc:0.910]
Epoch [45/120    avg_loss:0.430, val_acc:0.960]
Epoch [46/120    avg_loss:0.403, val_acc:0.910]
Epoch [47/120    avg_loss:0.443, val_acc:0.935]
Epoch [48/120    avg_loss:0.389, val_acc:0.921]
Epoch [49/120    avg_loss:0.414, val_acc:0.919]
Epoch [50/120    avg_loss:0.397, val_acc:0.931]
Epoch [51/120    avg_loss:0.379, val_acc:0.940]
Epoch [52/120    avg_loss:0.386, val_acc:0.938]
Epoch [53/120    avg_loss:0.378, val_acc:0.938]
Epoch [54/120    avg_loss:0.417, val_acc:0.850]
Epoch [55/120    avg_loss:0.350, val_acc:0.942]
Epoch [56/120    avg_loss:0.347, val_acc:0.933]
Epoch [57/120    avg_loss:0.352, val_acc:0.969]
Epoch [58/120    avg_loss:0.268, val_acc:0.960]
Epoch [59/120    avg_loss:0.269, val_acc:0.960]
Epoch [60/120    avg_loss:0.307, val_acc:0.923]
Epoch [61/120    avg_loss:0.273, val_acc:0.960]
Epoch [62/120    avg_loss:0.262, val_acc:0.969]
Epoch [63/120    avg_loss:0.238, val_acc:0.975]
Epoch [64/120    avg_loss:0.271, val_acc:0.971]
Epoch [65/120    avg_loss:0.244, val_acc:0.942]
Epoch [66/120    avg_loss:0.239, val_acc:0.979]
Epoch [67/120    avg_loss:0.214, val_acc:0.958]
Epoch [68/120    avg_loss:0.275, val_acc:0.954]
Epoch [69/120    avg_loss:0.184, val_acc:0.969]
Epoch [70/120    avg_loss:0.214, val_acc:0.969]
Epoch [71/120    avg_loss:0.225, val_acc:0.960]
Epoch [72/120    avg_loss:0.198, val_acc:0.935]
Epoch [73/120    avg_loss:0.190, val_acc:0.967]
Epoch [74/120    avg_loss:0.166, val_acc:0.977]
Epoch [75/120    avg_loss:0.162, val_acc:0.975]
Epoch [76/120    avg_loss:0.208, val_acc:0.952]
Epoch [77/120    avg_loss:0.167, val_acc:0.985]
Epoch [78/120    avg_loss:0.191, val_acc:0.963]
Epoch [79/120    avg_loss:0.199, val_acc:0.969]
Epoch [80/120    avg_loss:0.217, val_acc:0.971]
Epoch [81/120    avg_loss:0.342, val_acc:0.931]
Epoch [82/120    avg_loss:0.226, val_acc:0.929]
Epoch [83/120    avg_loss:0.193, val_acc:0.975]
Epoch [84/120    avg_loss:0.169, val_acc:0.973]
Epoch [85/120    avg_loss:0.164, val_acc:0.975]
Epoch [86/120    avg_loss:0.135, val_acc:0.969]
Epoch [87/120    avg_loss:0.134, val_acc:0.990]
Epoch [88/120    avg_loss:0.145, val_acc:0.981]
Epoch [89/120    avg_loss:0.161, val_acc:0.973]
Epoch [90/120    avg_loss:0.154, val_acc:0.977]
Epoch [91/120    avg_loss:0.106, val_acc:0.994]
Epoch [92/120    avg_loss:0.092, val_acc:0.981]
Epoch [93/120    avg_loss:0.101, val_acc:0.975]
Epoch [94/120    avg_loss:0.120, val_acc:0.971]
Epoch [95/120    avg_loss:0.097, val_acc:0.975]
Epoch [96/120    avg_loss:0.120, val_acc:0.985]
Epoch [97/120    avg_loss:0.099, val_acc:0.985]
Epoch [98/120    avg_loss:0.128, val_acc:0.979]
Epoch [99/120    avg_loss:0.113, val_acc:0.994]
Epoch [100/120    avg_loss:0.079, val_acc:0.988]
Epoch [101/120    avg_loss:0.126, val_acc:0.935]
Epoch [102/120    avg_loss:0.102, val_acc:0.992]
Epoch [103/120    avg_loss:0.085, val_acc:0.994]
Epoch [104/120    avg_loss:0.100, val_acc:0.992]
Epoch [105/120    avg_loss:0.093, val_acc:0.990]
Epoch [106/120    avg_loss:0.081, val_acc:0.985]
Epoch [107/120    avg_loss:0.076, val_acc:0.983]
Epoch [108/120    avg_loss:0.055, val_acc:0.985]
Epoch [109/120    avg_loss:0.063, val_acc:0.985]
Epoch [110/120    avg_loss:0.073, val_acc:0.992]
Epoch [111/120    avg_loss:0.060, val_acc:0.994]
Epoch [112/120    avg_loss:0.086, val_acc:0.992]
Epoch [113/120    avg_loss:0.063, val_acc:0.979]
Epoch [114/120    avg_loss:0.082, val_acc:0.981]
Epoch [115/120    avg_loss:0.097, val_acc:0.994]
Epoch [116/120    avg_loss:0.075, val_acc:0.990]
Epoch [117/120    avg_loss:0.057, val_acc:0.994]
Epoch [118/120    avg_loss:0.057, val_acc:0.992]
Epoch [119/120    avg_loss:0.071, val_acc:0.992]
Epoch [120/120    avg_loss:0.047, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   2   0 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99412628 1.         0.995671   0.95632184 0.93159609
 0.98095238 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9931177561486557
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f49c7f51e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.663, val_acc:0.079]
Epoch [2/120    avg_loss:2.622, val_acc:0.127]
Epoch [3/120    avg_loss:2.585, val_acc:0.294]
Epoch [4/120    avg_loss:2.555, val_acc:0.362]
Epoch [5/120    avg_loss:2.520, val_acc:0.404]
Epoch [6/120    avg_loss:2.480, val_acc:0.435]
Epoch [7/120    avg_loss:2.438, val_acc:0.458]
Epoch [8/120    avg_loss:2.395, val_acc:0.471]
Epoch [9/120    avg_loss:2.352, val_acc:0.492]
Epoch [10/120    avg_loss:2.309, val_acc:0.508]
Epoch [11/120    avg_loss:2.263, val_acc:0.548]
Epoch [12/120    avg_loss:2.204, val_acc:0.583]
Epoch [13/120    avg_loss:2.152, val_acc:0.623]
Epoch [14/120    avg_loss:2.078, val_acc:0.625]
Epoch [15/120    avg_loss:2.011, val_acc:0.633]
Epoch [16/120    avg_loss:1.933, val_acc:0.665]
Epoch [17/120    avg_loss:1.837, val_acc:0.731]
Epoch [18/120    avg_loss:1.755, val_acc:0.719]
Epoch [19/120    avg_loss:1.673, val_acc:0.725]
Epoch [20/120    avg_loss:1.580, val_acc:0.760]
Epoch [21/120    avg_loss:1.492, val_acc:0.746]
Epoch [22/120    avg_loss:1.404, val_acc:0.790]
Epoch [23/120    avg_loss:1.295, val_acc:0.773]
Epoch [24/120    avg_loss:1.212, val_acc:0.760]
Epoch [25/120    avg_loss:1.124, val_acc:0.817]
Epoch [26/120    avg_loss:1.044, val_acc:0.825]
Epoch [27/120    avg_loss:0.970, val_acc:0.796]
Epoch [28/120    avg_loss:0.910, val_acc:0.815]
Epoch [29/120    avg_loss:0.812, val_acc:0.806]
Epoch [30/120    avg_loss:0.769, val_acc:0.827]
Epoch [31/120    avg_loss:0.712, val_acc:0.881]
Epoch [32/120    avg_loss:0.651, val_acc:0.892]
Epoch [33/120    avg_loss:0.614, val_acc:0.863]
Epoch [34/120    avg_loss:0.617, val_acc:0.875]
Epoch [35/120    avg_loss:0.540, val_acc:0.892]
Epoch [36/120    avg_loss:0.536, val_acc:0.940]
Epoch [37/120    avg_loss:0.473, val_acc:0.921]
Epoch [38/120    avg_loss:0.458, val_acc:0.908]
Epoch [39/120    avg_loss:0.439, val_acc:0.938]
Epoch [40/120    avg_loss:0.390, val_acc:0.946]
Epoch [41/120    avg_loss:0.465, val_acc:0.963]
Epoch [42/120    avg_loss:0.451, val_acc:0.954]
Epoch [43/120    avg_loss:0.416, val_acc:0.948]
Epoch [44/120    avg_loss:0.365, val_acc:0.956]
Epoch [45/120    avg_loss:0.340, val_acc:0.944]
Epoch [46/120    avg_loss:0.296, val_acc:0.973]
Epoch [47/120    avg_loss:0.342, val_acc:0.927]
Epoch [48/120    avg_loss:0.341, val_acc:0.929]
Epoch [49/120    avg_loss:0.270, val_acc:0.952]
Epoch [50/120    avg_loss:0.292, val_acc:0.960]
Epoch [51/120    avg_loss:0.293, val_acc:0.933]
Epoch [52/120    avg_loss:0.286, val_acc:0.942]
Epoch [53/120    avg_loss:0.267, val_acc:0.952]
Epoch [54/120    avg_loss:0.251, val_acc:0.977]
Epoch [55/120    avg_loss:0.197, val_acc:0.977]
Epoch [56/120    avg_loss:0.240, val_acc:0.948]
Epoch [57/120    avg_loss:0.251, val_acc:0.960]
Epoch [58/120    avg_loss:0.247, val_acc:0.975]
Epoch [59/120    avg_loss:0.232, val_acc:0.965]
Epoch [60/120    avg_loss:0.231, val_acc:0.965]
Epoch [61/120    avg_loss:0.228, val_acc:0.967]
Epoch [62/120    avg_loss:0.197, val_acc:0.973]
Epoch [63/120    avg_loss:0.242, val_acc:0.973]
Epoch [64/120    avg_loss:0.197, val_acc:0.973]
Epoch [65/120    avg_loss:0.206, val_acc:0.971]
Epoch [66/120    avg_loss:0.171, val_acc:0.973]
Epoch [67/120    avg_loss:0.204, val_acc:0.965]
Epoch [68/120    avg_loss:0.242, val_acc:0.965]
Epoch [69/120    avg_loss:0.165, val_acc:0.983]
Epoch [70/120    avg_loss:0.148, val_acc:0.983]
Epoch [71/120    avg_loss:0.137, val_acc:0.988]
Epoch [72/120    avg_loss:0.131, val_acc:0.983]
Epoch [73/120    avg_loss:0.111, val_acc:0.983]
Epoch [74/120    avg_loss:0.118, val_acc:0.985]
Epoch [75/120    avg_loss:0.120, val_acc:0.988]
Epoch [76/120    avg_loss:0.108, val_acc:0.985]
Epoch [77/120    avg_loss:0.112, val_acc:0.988]
Epoch [78/120    avg_loss:0.112, val_acc:0.990]
Epoch [79/120    avg_loss:0.120, val_acc:0.990]
Epoch [80/120    avg_loss:0.106, val_acc:0.990]
Epoch [81/120    avg_loss:0.097, val_acc:0.990]
Epoch [82/120    avg_loss:0.106, val_acc:0.990]
Epoch [83/120    avg_loss:0.131, val_acc:0.985]
Epoch [84/120    avg_loss:0.110, val_acc:0.988]
Epoch [85/120    avg_loss:0.112, val_acc:0.983]
Epoch [86/120    avg_loss:0.099, val_acc:0.988]
Epoch [87/120    avg_loss:0.122, val_acc:0.988]
Epoch [88/120    avg_loss:0.106, val_acc:0.988]
Epoch [89/120    avg_loss:0.095, val_acc:0.990]
Epoch [90/120    avg_loss:0.094, val_acc:0.990]
Epoch [91/120    avg_loss:0.101, val_acc:0.988]
Epoch [92/120    avg_loss:0.115, val_acc:0.992]
Epoch [93/120    avg_loss:0.097, val_acc:0.992]
Epoch [94/120    avg_loss:0.103, val_acc:0.990]
Epoch [95/120    avg_loss:0.090, val_acc:0.990]
Epoch [96/120    avg_loss:0.097, val_acc:0.990]
Epoch [97/120    avg_loss:0.098, val_acc:0.990]
Epoch [98/120    avg_loss:0.111, val_acc:0.994]
Epoch [99/120    avg_loss:0.093, val_acc:0.994]
Epoch [100/120    avg_loss:0.099, val_acc:0.992]
Epoch [101/120    avg_loss:0.092, val_acc:0.988]
Epoch [102/120    avg_loss:0.099, val_acc:0.988]
Epoch [103/120    avg_loss:0.097, val_acc:0.992]
Epoch [104/120    avg_loss:0.092, val_acc:0.994]
Epoch [105/120    avg_loss:0.088, val_acc:0.992]
Epoch [106/120    avg_loss:0.086, val_acc:0.992]
Epoch [107/120    avg_loss:0.078, val_acc:0.994]
Epoch [108/120    avg_loss:0.096, val_acc:0.994]
Epoch [109/120    avg_loss:0.090, val_acc:0.994]
Epoch [110/120    avg_loss:0.104, val_acc:0.990]
Epoch [111/120    avg_loss:0.097, val_acc:0.988]
Epoch [112/120    avg_loss:0.090, val_acc:0.988]
Epoch [113/120    avg_loss:0.082, val_acc:0.994]
Epoch [114/120    avg_loss:0.084, val_acc:0.994]
Epoch [115/120    avg_loss:0.101, val_acc:0.988]
Epoch [116/120    avg_loss:0.083, val_acc:0.992]
Epoch [117/120    avg_loss:0.089, val_acc:0.992]
Epoch [118/120    avg_loss:0.096, val_acc:0.990]
Epoch [119/120    avg_loss:0.086, val_acc:0.996]
Epoch [120/120    avg_loss:0.089, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   2 204  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99560117 0.99545455 0.995671   0.91891892 0.88590604
 0.98564593 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895569465232263
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a667c1e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.235]
Epoch [2/120    avg_loss:2.601, val_acc:0.244]
Epoch [3/120    avg_loss:2.572, val_acc:0.308]
Epoch [4/120    avg_loss:2.545, val_acc:0.358]
Epoch [5/120    avg_loss:2.514, val_acc:0.333]
Epoch [6/120    avg_loss:2.484, val_acc:0.323]
Epoch [7/120    avg_loss:2.447, val_acc:0.304]
Epoch [8/120    avg_loss:2.404, val_acc:0.310]
Epoch [9/120    avg_loss:2.363, val_acc:0.317]
Epoch [10/120    avg_loss:2.319, val_acc:0.310]
Epoch [11/120    avg_loss:2.279, val_acc:0.308]
Epoch [12/120    avg_loss:2.223, val_acc:0.319]
Epoch [13/120    avg_loss:2.162, val_acc:0.390]
Epoch [14/120    avg_loss:2.098, val_acc:0.463]
Epoch [15/120    avg_loss:2.049, val_acc:0.500]
Epoch [16/120    avg_loss:1.985, val_acc:0.517]
Epoch [17/120    avg_loss:1.959, val_acc:0.585]
Epoch [18/120    avg_loss:1.891, val_acc:0.637]
Epoch [19/120    avg_loss:1.782, val_acc:0.650]
Epoch [20/120    avg_loss:1.712, val_acc:0.648]
Epoch [21/120    avg_loss:1.635, val_acc:0.660]
Epoch [22/120    avg_loss:1.577, val_acc:0.669]
Epoch [23/120    avg_loss:1.488, val_acc:0.725]
Epoch [24/120    avg_loss:1.403, val_acc:0.723]
Epoch [25/120    avg_loss:1.307, val_acc:0.735]
Epoch [26/120    avg_loss:1.208, val_acc:0.731]
Epoch [27/120    avg_loss:1.126, val_acc:0.758]
Epoch [28/120    avg_loss:1.060, val_acc:0.725]
Epoch [29/120    avg_loss:0.974, val_acc:0.762]
Epoch [30/120    avg_loss:0.945, val_acc:0.787]
Epoch [31/120    avg_loss:0.860, val_acc:0.808]
Epoch [32/120    avg_loss:0.844, val_acc:0.854]
Epoch [33/120    avg_loss:0.775, val_acc:0.821]
Epoch [34/120    avg_loss:0.695, val_acc:0.842]
Epoch [35/120    avg_loss:0.649, val_acc:0.877]
Epoch [36/120    avg_loss:0.635, val_acc:0.902]
Epoch [37/120    avg_loss:0.600, val_acc:0.902]
Epoch [38/120    avg_loss:0.525, val_acc:0.894]
Epoch [39/120    avg_loss:0.565, val_acc:0.898]
Epoch [40/120    avg_loss:0.527, val_acc:0.898]
Epoch [41/120    avg_loss:0.566, val_acc:0.935]
Epoch [42/120    avg_loss:0.460, val_acc:0.921]
Epoch [43/120    avg_loss:0.414, val_acc:0.935]
Epoch [44/120    avg_loss:0.424, val_acc:0.954]
Epoch [45/120    avg_loss:0.400, val_acc:0.929]
Epoch [46/120    avg_loss:0.413, val_acc:0.935]
Epoch [47/120    avg_loss:0.383, val_acc:0.921]
Epoch [48/120    avg_loss:0.366, val_acc:0.948]
Epoch [49/120    avg_loss:0.306, val_acc:0.942]
Epoch [50/120    avg_loss:0.368, val_acc:0.879]
Epoch [51/120    avg_loss:0.401, val_acc:0.875]
Epoch [52/120    avg_loss:0.384, val_acc:0.940]
Epoch [53/120    avg_loss:0.314, val_acc:0.958]
Epoch [54/120    avg_loss:0.257, val_acc:0.904]
Epoch [55/120    avg_loss:0.296, val_acc:0.950]
Epoch [56/120    avg_loss:0.248, val_acc:0.950]
Epoch [57/120    avg_loss:0.238, val_acc:0.963]
Epoch [58/120    avg_loss:0.271, val_acc:0.931]
Epoch [59/120    avg_loss:0.272, val_acc:0.946]
Epoch [60/120    avg_loss:0.260, val_acc:0.956]
Epoch [61/120    avg_loss:0.205, val_acc:0.956]
Epoch [62/120    avg_loss:0.239, val_acc:0.963]
Epoch [63/120    avg_loss:0.194, val_acc:0.963]
Epoch [64/120    avg_loss:0.187, val_acc:0.965]
Epoch [65/120    avg_loss:0.179, val_acc:0.965]
Epoch [66/120    avg_loss:0.160, val_acc:0.967]
Epoch [67/120    avg_loss:0.187, val_acc:0.933]
Epoch [68/120    avg_loss:0.205, val_acc:0.954]
Epoch [69/120    avg_loss:0.176, val_acc:0.967]
Epoch [70/120    avg_loss:0.161, val_acc:0.950]
Epoch [71/120    avg_loss:0.156, val_acc:0.977]
Epoch [72/120    avg_loss:0.193, val_acc:0.969]
Epoch [73/120    avg_loss:0.145, val_acc:0.977]
Epoch [74/120    avg_loss:0.129, val_acc:0.973]
Epoch [75/120    avg_loss:0.153, val_acc:0.950]
Epoch [76/120    avg_loss:0.172, val_acc:0.919]
Epoch [77/120    avg_loss:0.172, val_acc:0.981]
Epoch [78/120    avg_loss:0.157, val_acc:0.967]
Epoch [79/120    avg_loss:0.155, val_acc:0.967]
Epoch [80/120    avg_loss:0.114, val_acc:0.960]
Epoch [81/120    avg_loss:0.111, val_acc:0.973]
Epoch [82/120    avg_loss:0.127, val_acc:0.973]
Epoch [83/120    avg_loss:0.107, val_acc:0.977]
Epoch [84/120    avg_loss:0.095, val_acc:0.969]
Epoch [85/120    avg_loss:0.125, val_acc:0.956]
Epoch [86/120    avg_loss:0.146, val_acc:0.965]
Epoch [87/120    avg_loss:0.110, val_acc:0.985]
Epoch [88/120    avg_loss:0.113, val_acc:0.981]
Epoch [89/120    avg_loss:0.084, val_acc:0.973]
Epoch [90/120    avg_loss:0.095, val_acc:0.965]
Epoch [91/120    avg_loss:0.089, val_acc:0.969]
Epoch [92/120    avg_loss:0.108, val_acc:0.985]
Epoch [93/120    avg_loss:0.077, val_acc:0.975]
Epoch [94/120    avg_loss:0.065, val_acc:0.977]
Epoch [95/120    avg_loss:0.065, val_acc:0.985]
Epoch [96/120    avg_loss:0.070, val_acc:0.967]
Epoch [97/120    avg_loss:0.088, val_acc:0.973]
Epoch [98/120    avg_loss:0.085, val_acc:0.979]
Epoch [99/120    avg_loss:0.073, val_acc:0.977]
Epoch [100/120    avg_loss:0.082, val_acc:0.967]
Epoch [101/120    avg_loss:0.061, val_acc:0.983]
Epoch [102/120    avg_loss:0.061, val_acc:0.981]
Epoch [103/120    avg_loss:0.063, val_acc:0.975]
Epoch [104/120    avg_loss:0.044, val_acc:0.985]
Epoch [105/120    avg_loss:0.040, val_acc:0.990]
Epoch [106/120    avg_loss:0.042, val_acc:0.973]
Epoch [107/120    avg_loss:0.067, val_acc:0.983]
Epoch [108/120    avg_loss:0.086, val_acc:0.973]
Epoch [109/120    avg_loss:0.188, val_acc:0.971]
Epoch [110/120    avg_loss:0.137, val_acc:0.981]
Epoch [111/120    avg_loss:0.075, val_acc:0.979]
Epoch [112/120    avg_loss:0.112, val_acc:0.973]
Epoch [113/120    avg_loss:0.106, val_acc:0.975]
Epoch [114/120    avg_loss:0.082, val_acc:0.985]
Epoch [115/120    avg_loss:0.054, val_acc:0.983]
Epoch [116/120    avg_loss:0.048, val_acc:0.981]
Epoch [117/120    avg_loss:0.078, val_acc:0.979]
Epoch [118/120    avg_loss:0.075, val_acc:0.973]
Epoch [119/120    avg_loss:0.067, val_acc:0.981]
Epoch [120/120    avg_loss:0.052, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  22   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.94663573 0.92948718
 0.99038462 0.99465241 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9933544524637133
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa668a73d30>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.092]
Epoch [2/120    avg_loss:2.590, val_acc:0.283]
Epoch [3/120    avg_loss:2.551, val_acc:0.367]
Epoch [4/120    avg_loss:2.513, val_acc:0.365]
Epoch [5/120    avg_loss:2.477, val_acc:0.410]
Epoch [6/120    avg_loss:2.441, val_acc:0.430]
Epoch [7/120    avg_loss:2.415, val_acc:0.473]
Epoch [8/120    avg_loss:2.381, val_acc:0.477]
Epoch [9/120    avg_loss:2.352, val_acc:0.498]
Epoch [10/120    avg_loss:2.311, val_acc:0.467]
Epoch [11/120    avg_loss:2.287, val_acc:0.525]
Epoch [12/120    avg_loss:2.249, val_acc:0.439]
Epoch [13/120    avg_loss:2.202, val_acc:0.555]
Epoch [14/120    avg_loss:2.154, val_acc:0.539]
Epoch [15/120    avg_loss:2.099, val_acc:0.570]
Epoch [16/120    avg_loss:2.028, val_acc:0.576]
Epoch [17/120    avg_loss:1.974, val_acc:0.674]
Epoch [18/120    avg_loss:1.897, val_acc:0.721]
Epoch [19/120    avg_loss:1.852, val_acc:0.660]
Epoch [20/120    avg_loss:1.781, val_acc:0.734]
Epoch [21/120    avg_loss:1.722, val_acc:0.789]
Epoch [22/120    avg_loss:1.641, val_acc:0.764]
Epoch [23/120    avg_loss:1.568, val_acc:0.736]
Epoch [24/120    avg_loss:1.511, val_acc:0.748]
Epoch [25/120    avg_loss:1.435, val_acc:0.824]
Epoch [26/120    avg_loss:1.336, val_acc:0.758]
Epoch [27/120    avg_loss:1.278, val_acc:0.777]
Epoch [28/120    avg_loss:1.213, val_acc:0.822]
Epoch [29/120    avg_loss:1.141, val_acc:0.816]
Epoch [30/120    avg_loss:1.062, val_acc:0.879]
Epoch [31/120    avg_loss:0.980, val_acc:0.879]
Epoch [32/120    avg_loss:0.994, val_acc:0.881]
Epoch [33/120    avg_loss:0.892, val_acc:0.855]
Epoch [34/120    avg_loss:0.848, val_acc:0.926]
Epoch [35/120    avg_loss:0.797, val_acc:0.912]
Epoch [36/120    avg_loss:0.774, val_acc:0.896]
Epoch [37/120    avg_loss:0.721, val_acc:0.898]
Epoch [38/120    avg_loss:0.678, val_acc:0.918]
Epoch [39/120    avg_loss:0.638, val_acc:0.908]
Epoch [40/120    avg_loss:0.644, val_acc:0.924]
Epoch [41/120    avg_loss:0.595, val_acc:0.920]
Epoch [42/120    avg_loss:0.537, val_acc:0.934]
Epoch [43/120    avg_loss:0.541, val_acc:0.920]
Epoch [44/120    avg_loss:0.510, val_acc:0.932]
Epoch [45/120    avg_loss:0.470, val_acc:0.932]
Epoch [46/120    avg_loss:0.481, val_acc:0.938]
Epoch [47/120    avg_loss:0.453, val_acc:0.896]
Epoch [48/120    avg_loss:0.465, val_acc:0.928]
Epoch [49/120    avg_loss:0.448, val_acc:0.842]
Epoch [50/120    avg_loss:0.413, val_acc:0.939]
Epoch [51/120    avg_loss:0.399, val_acc:0.930]
Epoch [52/120    avg_loss:0.443, val_acc:0.936]
Epoch [53/120    avg_loss:0.392, val_acc:0.928]
Epoch [54/120    avg_loss:0.374, val_acc:0.934]
Epoch [55/120    avg_loss:0.344, val_acc:0.914]
Epoch [56/120    avg_loss:0.354, val_acc:0.949]
Epoch [57/120    avg_loss:0.375, val_acc:0.938]
Epoch [58/120    avg_loss:0.367, val_acc:0.895]
Epoch [59/120    avg_loss:0.338, val_acc:0.941]
Epoch [60/120    avg_loss:0.296, val_acc:0.924]
Epoch [61/120    avg_loss:0.289, val_acc:0.947]
Epoch [62/120    avg_loss:0.302, val_acc:0.949]
Epoch [63/120    avg_loss:0.301, val_acc:0.930]
Epoch [64/120    avg_loss:0.293, val_acc:0.959]
Epoch [65/120    avg_loss:0.330, val_acc:0.955]
Epoch [66/120    avg_loss:0.260, val_acc:0.959]
Epoch [67/120    avg_loss:0.229, val_acc:0.941]
Epoch [68/120    avg_loss:0.214, val_acc:0.967]
Epoch [69/120    avg_loss:0.213, val_acc:0.965]
Epoch [70/120    avg_loss:0.207, val_acc:0.971]
Epoch [71/120    avg_loss:0.191, val_acc:0.965]
Epoch [72/120    avg_loss:0.234, val_acc:0.965]
Epoch [73/120    avg_loss:0.213, val_acc:0.955]
Epoch [74/120    avg_loss:0.241, val_acc:0.900]
Epoch [75/120    avg_loss:0.233, val_acc:0.955]
Epoch [76/120    avg_loss:0.194, val_acc:0.963]
Epoch [77/120    avg_loss:0.200, val_acc:0.959]
Epoch [78/120    avg_loss:0.192, val_acc:0.957]
Epoch [79/120    avg_loss:0.201, val_acc:0.975]
Epoch [80/120    avg_loss:0.164, val_acc:0.963]
Epoch [81/120    avg_loss:0.141, val_acc:0.975]
Epoch [82/120    avg_loss:0.154, val_acc:0.971]
Epoch [83/120    avg_loss:0.196, val_acc:0.961]
Epoch [84/120    avg_loss:0.192, val_acc:0.959]
Epoch [85/120    avg_loss:0.182, val_acc:0.965]
Epoch [86/120    avg_loss:0.163, val_acc:0.971]
Epoch [87/120    avg_loss:0.190, val_acc:0.961]
Epoch [88/120    avg_loss:0.194, val_acc:0.967]
Epoch [89/120    avg_loss:0.156, val_acc:0.969]
Epoch [90/120    avg_loss:0.173, val_acc:0.963]
Epoch [91/120    avg_loss:0.126, val_acc:0.977]
Epoch [92/120    avg_loss:0.106, val_acc:0.977]
Epoch [93/120    avg_loss:0.125, val_acc:0.975]
Epoch [94/120    avg_loss:0.137, val_acc:0.973]
Epoch [95/120    avg_loss:0.136, val_acc:0.982]
Epoch [96/120    avg_loss:0.151, val_acc:0.967]
Epoch [97/120    avg_loss:0.133, val_acc:0.969]
Epoch [98/120    avg_loss:0.149, val_acc:0.980]
Epoch [99/120    avg_loss:0.104, val_acc:0.977]
Epoch [100/120    avg_loss:0.136, val_acc:0.979]
Epoch [101/120    avg_loss:0.115, val_acc:0.971]
Epoch [102/120    avg_loss:0.105, val_acc:0.961]
Epoch [103/120    avg_loss:0.128, val_acc:0.977]
Epoch [104/120    avg_loss:0.109, val_acc:0.963]
Epoch [105/120    avg_loss:0.137, val_acc:0.959]
Epoch [106/120    avg_loss:0.163, val_acc:0.965]
Epoch [107/120    avg_loss:0.122, val_acc:0.973]
Epoch [108/120    avg_loss:0.136, val_acc:0.959]
Epoch [109/120    avg_loss:0.126, val_acc:0.982]
Epoch [110/120    avg_loss:0.081, val_acc:0.980]
Epoch [111/120    avg_loss:0.078, val_acc:0.982]
Epoch [112/120    avg_loss:0.071, val_acc:0.980]
Epoch [113/120    avg_loss:0.085, val_acc:0.980]
Epoch [114/120    avg_loss:0.081, val_acc:0.980]
Epoch [115/120    avg_loss:0.077, val_acc:0.980]
Epoch [116/120    avg_loss:0.070, val_acc:0.980]
Epoch [117/120    avg_loss:0.066, val_acc:0.986]
Epoch [118/120    avg_loss:0.073, val_acc:0.984]
Epoch [119/120    avg_loss:0.067, val_acc:0.986]
Epoch [120/120    avg_loss:0.066, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 219   4   0   0   0   4   3   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   4   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.2089552238806

F1 scores:
[       nan 0.99854227 0.93512304 0.97550111 0.91257996 0.86738351
 0.99512195 0.83798883 0.99487179 0.99680511 0.99453552 0.99867198
 0.9944629  1.        ]

Kappa:
0.9800558805916473
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd02c4e6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.186]
Epoch [2/120    avg_loss:2.583, val_acc:0.252]
Epoch [3/120    avg_loss:2.545, val_acc:0.264]
Epoch [4/120    avg_loss:2.508, val_acc:0.270]
Epoch [5/120    avg_loss:2.477, val_acc:0.299]
Epoch [6/120    avg_loss:2.444, val_acc:0.320]
Epoch [7/120    avg_loss:2.415, val_acc:0.330]
Epoch [8/120    avg_loss:2.386, val_acc:0.348]
Epoch [9/120    avg_loss:2.358, val_acc:0.381]
Epoch [10/120    avg_loss:2.328, val_acc:0.398]
Epoch [11/120    avg_loss:2.294, val_acc:0.408]
Epoch [12/120    avg_loss:2.258, val_acc:0.424]
Epoch [13/120    avg_loss:2.221, val_acc:0.463]
Epoch [14/120    avg_loss:2.189, val_acc:0.484]
Epoch [15/120    avg_loss:2.135, val_acc:0.510]
Epoch [16/120    avg_loss:2.097, val_acc:0.504]
Epoch [17/120    avg_loss:2.045, val_acc:0.521]
Epoch [18/120    avg_loss:1.992, val_acc:0.535]
Epoch [19/120    avg_loss:1.928, val_acc:0.543]
Epoch [20/120    avg_loss:1.910, val_acc:0.541]
Epoch [21/120    avg_loss:1.829, val_acc:0.576]
Epoch [22/120    avg_loss:1.751, val_acc:0.637]
Epoch [23/120    avg_loss:1.686, val_acc:0.617]
Epoch [24/120    avg_loss:1.612, val_acc:0.693]
Epoch [25/120    avg_loss:1.554, val_acc:0.709]
Epoch [26/120    avg_loss:1.472, val_acc:0.773]
Epoch [27/120    avg_loss:1.403, val_acc:0.756]
Epoch [28/120    avg_loss:1.351, val_acc:0.801]
Epoch [29/120    avg_loss:1.289, val_acc:0.805]
Epoch [30/120    avg_loss:1.203, val_acc:0.824]
Epoch [31/120    avg_loss:1.163, val_acc:0.824]
Epoch [32/120    avg_loss:1.087, val_acc:0.852]
Epoch [33/120    avg_loss:1.023, val_acc:0.840]
Epoch [34/120    avg_loss:0.962, val_acc:0.863]
Epoch [35/120    avg_loss:0.918, val_acc:0.898]
Epoch [36/120    avg_loss:0.861, val_acc:0.902]
Epoch [37/120    avg_loss:0.849, val_acc:0.857]
Epoch [38/120    avg_loss:0.806, val_acc:0.889]
Epoch [39/120    avg_loss:0.736, val_acc:0.920]
Epoch [40/120    avg_loss:0.694, val_acc:0.887]
Epoch [41/120    avg_loss:0.667, val_acc:0.928]
Epoch [42/120    avg_loss:0.644, val_acc:0.887]
Epoch [43/120    avg_loss:0.609, val_acc:0.939]
Epoch [44/120    avg_loss:0.570, val_acc:0.934]
Epoch [45/120    avg_loss:0.523, val_acc:0.928]
Epoch [46/120    avg_loss:0.493, val_acc:0.934]
Epoch [47/120    avg_loss:0.526, val_acc:0.914]
Epoch [48/120    avg_loss:0.517, val_acc:0.932]
Epoch [49/120    avg_loss:0.473, val_acc:0.934]
Epoch [50/120    avg_loss:0.455, val_acc:0.941]
Epoch [51/120    avg_loss:0.418, val_acc:0.936]
Epoch [52/120    avg_loss:0.403, val_acc:0.936]
Epoch [53/120    avg_loss:0.397, val_acc:0.951]
Epoch [54/120    avg_loss:0.348, val_acc:0.961]
Epoch [55/120    avg_loss:0.365, val_acc:0.945]
Epoch [56/120    avg_loss:0.343, val_acc:0.941]
Epoch [57/120    avg_loss:0.366, val_acc:0.961]
Epoch [58/120    avg_loss:0.344, val_acc:0.963]
Epoch [59/120    avg_loss:0.331, val_acc:0.934]
Epoch [60/120    avg_loss:0.317, val_acc:0.941]
Epoch [61/120    avg_loss:0.327, val_acc:0.951]
Epoch [62/120    avg_loss:0.283, val_acc:0.955]
Epoch [63/120    avg_loss:0.255, val_acc:0.949]
Epoch [64/120    avg_loss:0.433, val_acc:0.939]
Epoch [65/120    avg_loss:0.316, val_acc:0.951]
Epoch [66/120    avg_loss:0.268, val_acc:0.941]
Epoch [67/120    avg_loss:0.342, val_acc:0.949]
Epoch [68/120    avg_loss:0.316, val_acc:0.961]
Epoch [69/120    avg_loss:0.268, val_acc:0.967]
Epoch [70/120    avg_loss:0.246, val_acc:0.959]
Epoch [71/120    avg_loss:0.245, val_acc:0.975]
Epoch [72/120    avg_loss:0.216, val_acc:0.963]
Epoch [73/120    avg_loss:0.222, val_acc:0.965]
Epoch [74/120    avg_loss:0.242, val_acc:0.955]
Epoch [75/120    avg_loss:0.216, val_acc:0.969]
Epoch [76/120    avg_loss:0.209, val_acc:0.957]
Epoch [77/120    avg_loss:0.237, val_acc:0.971]
Epoch [78/120    avg_loss:0.220, val_acc:0.967]
Epoch [79/120    avg_loss:0.200, val_acc:0.961]
Epoch [80/120    avg_loss:0.204, val_acc:0.963]
Epoch [81/120    avg_loss:0.175, val_acc:0.969]
Epoch [82/120    avg_loss:0.165, val_acc:0.979]
Epoch [83/120    avg_loss:0.169, val_acc:0.973]
Epoch [84/120    avg_loss:0.178, val_acc:0.973]
Epoch [85/120    avg_loss:0.168, val_acc:0.975]
Epoch [86/120    avg_loss:0.207, val_acc:0.963]
Epoch [87/120    avg_loss:0.158, val_acc:0.969]
Epoch [88/120    avg_loss:0.172, val_acc:0.959]
Epoch [89/120    avg_loss:0.208, val_acc:0.977]
Epoch [90/120    avg_loss:0.181, val_acc:0.969]
Epoch [91/120    avg_loss:0.180, val_acc:0.971]
Epoch [92/120    avg_loss:0.203, val_acc:0.980]
Epoch [93/120    avg_loss:0.176, val_acc:0.979]
Epoch [94/120    avg_loss:0.133, val_acc:0.986]
Epoch [95/120    avg_loss:0.138, val_acc:0.992]
Epoch [96/120    avg_loss:0.119, val_acc:0.984]
Epoch [97/120    avg_loss:0.141, val_acc:0.988]
Epoch [98/120    avg_loss:0.121, val_acc:0.959]
Epoch [99/120    avg_loss:0.147, val_acc:0.979]
Epoch [100/120    avg_loss:0.130, val_acc:0.984]
Epoch [101/120    avg_loss:0.130, val_acc:0.986]
Epoch [102/120    avg_loss:0.100, val_acc:0.982]
Epoch [103/120    avg_loss:0.117, val_acc:0.986]
Epoch [104/120    avg_loss:0.123, val_acc:0.965]
Epoch [105/120    avg_loss:0.136, val_acc:0.973]
Epoch [106/120    avg_loss:0.111, val_acc:0.986]
Epoch [107/120    avg_loss:0.099, val_acc:0.990]
Epoch [108/120    avg_loss:0.160, val_acc:0.963]
Epoch [109/120    avg_loss:0.121, val_acc:0.984]
Epoch [110/120    avg_loss:0.088, val_acc:0.990]
Epoch [111/120    avg_loss:0.068, val_acc:0.992]
Epoch [112/120    avg_loss:0.063, val_acc:0.990]
Epoch [113/120    avg_loss:0.062, val_acc:0.990]
Epoch [114/120    avg_loss:0.078, val_acc:0.994]
Epoch [115/120    avg_loss:0.068, val_acc:0.992]
Epoch [116/120    avg_loss:0.068, val_acc:0.990]
Epoch [117/120    avg_loss:0.066, val_acc:0.994]
Epoch [118/120    avg_loss:0.061, val_acc:0.996]
Epoch [119/120    avg_loss:0.071, val_acc:0.996]
Epoch [120/120    avg_loss:0.071, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 224   2   0   0   0   2   0   2   0   0   0]
 [  0   0   1   0 206  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   8   0   0   4   0 194   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.50746268656717

F1 scores:
[       nan 0.99419448 0.94877506 0.98678414 0.92170022 0.90728477
 0.97       0.87640449 0.99742931 1.         0.99726027 0.99867198
 0.99889746 1.        ]

Kappa:
0.9833792366113245
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a8e413e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.662, val_acc:0.047]
Epoch [2/120    avg_loss:2.632, val_acc:0.043]
Epoch [3/120    avg_loss:2.600, val_acc:0.125]
Epoch [4/120    avg_loss:2.567, val_acc:0.139]
Epoch [5/120    avg_loss:2.542, val_acc:0.199]
Epoch [6/120    avg_loss:2.509, val_acc:0.361]
Epoch [7/120    avg_loss:2.483, val_acc:0.369]
Epoch [8/120    avg_loss:2.459, val_acc:0.373]
Epoch [9/120    avg_loss:2.427, val_acc:0.383]
Epoch [10/120    avg_loss:2.407, val_acc:0.406]
Epoch [11/120    avg_loss:2.371, val_acc:0.422]
Epoch [12/120    avg_loss:2.343, val_acc:0.463]
Epoch [13/120    avg_loss:2.306, val_acc:0.473]
Epoch [14/120    avg_loss:2.267, val_acc:0.514]
Epoch [15/120    avg_loss:2.225, val_acc:0.518]
Epoch [16/120    avg_loss:2.171, val_acc:0.535]
Epoch [17/120    avg_loss:2.111, val_acc:0.527]
Epoch [18/120    avg_loss:2.078, val_acc:0.531]
Epoch [19/120    avg_loss:1.995, val_acc:0.547]
Epoch [20/120    avg_loss:1.936, val_acc:0.619]
Epoch [21/120    avg_loss:1.879, val_acc:0.615]
Epoch [22/120    avg_loss:1.809, val_acc:0.664]
Epoch [23/120    avg_loss:1.765, val_acc:0.666]
Epoch [24/120    avg_loss:1.701, val_acc:0.697]
Epoch [25/120    avg_loss:1.609, val_acc:0.691]
Epoch [26/120    avg_loss:1.540, val_acc:0.787]
Epoch [27/120    avg_loss:1.459, val_acc:0.787]
Epoch [28/120    avg_loss:1.410, val_acc:0.832]
Epoch [29/120    avg_loss:1.334, val_acc:0.828]
Epoch [30/120    avg_loss:1.248, val_acc:0.881]
Epoch [31/120    avg_loss:1.147, val_acc:0.885]
Epoch [32/120    avg_loss:1.069, val_acc:0.898]
Epoch [33/120    avg_loss:1.021, val_acc:0.867]
Epoch [34/120    avg_loss:0.986, val_acc:0.863]
Epoch [35/120    avg_loss:0.930, val_acc:0.883]
Epoch [36/120    avg_loss:0.846, val_acc:0.906]
Epoch [37/120    avg_loss:0.770, val_acc:0.895]
Epoch [38/120    avg_loss:0.717, val_acc:0.906]
Epoch [39/120    avg_loss:0.703, val_acc:0.896]
Epoch [40/120    avg_loss:0.666, val_acc:0.857]
Epoch [41/120    avg_loss:0.624, val_acc:0.908]
Epoch [42/120    avg_loss:0.622, val_acc:0.904]
Epoch [43/120    avg_loss:0.600, val_acc:0.861]
Epoch [44/120    avg_loss:0.548, val_acc:0.893]
Epoch [45/120    avg_loss:0.539, val_acc:0.914]
Epoch [46/120    avg_loss:0.513, val_acc:0.926]
Epoch [47/120    avg_loss:0.543, val_acc:0.891]
Epoch [48/120    avg_loss:0.535, val_acc:0.916]
Epoch [49/120    avg_loss:0.472, val_acc:0.928]
Epoch [50/120    avg_loss:0.458, val_acc:0.910]
Epoch [51/120    avg_loss:0.466, val_acc:0.926]
Epoch [52/120    avg_loss:0.476, val_acc:0.932]
Epoch [53/120    avg_loss:0.377, val_acc:0.936]
Epoch [54/120    avg_loss:0.355, val_acc:0.928]
Epoch [55/120    avg_loss:0.375, val_acc:0.943]
Epoch [56/120    avg_loss:0.325, val_acc:0.953]
Epoch [57/120    avg_loss:0.366, val_acc:0.926]
Epoch [58/120    avg_loss:0.344, val_acc:0.953]
Epoch [59/120    avg_loss:0.374, val_acc:0.941]
Epoch [60/120    avg_loss:0.325, val_acc:0.951]
Epoch [61/120    avg_loss:0.339, val_acc:0.924]
Epoch [62/120    avg_loss:0.365, val_acc:0.898]
Epoch [63/120    avg_loss:0.311, val_acc:0.945]
Epoch [64/120    avg_loss:0.305, val_acc:0.959]
Epoch [65/120    avg_loss:0.278, val_acc:0.936]
Epoch [66/120    avg_loss:0.289, val_acc:0.938]
Epoch [67/120    avg_loss:0.291, val_acc:0.957]
Epoch [68/120    avg_loss:0.297, val_acc:0.953]
Epoch [69/120    avg_loss:0.221, val_acc:0.963]
Epoch [70/120    avg_loss:0.251, val_acc:0.938]
Epoch [71/120    avg_loss:0.245, val_acc:0.951]
Epoch [72/120    avg_loss:0.218, val_acc:0.967]
Epoch [73/120    avg_loss:0.243, val_acc:0.939]
Epoch [74/120    avg_loss:0.256, val_acc:0.938]
Epoch [75/120    avg_loss:0.280, val_acc:0.941]
Epoch [76/120    avg_loss:0.278, val_acc:0.959]
Epoch [77/120    avg_loss:0.193, val_acc:0.963]
Epoch [78/120    avg_loss:0.198, val_acc:0.971]
Epoch [79/120    avg_loss:0.168, val_acc:0.949]
Epoch [80/120    avg_loss:0.196, val_acc:0.963]
Epoch [81/120    avg_loss:0.164, val_acc:0.961]
Epoch [82/120    avg_loss:0.144, val_acc:0.961]
Epoch [83/120    avg_loss:0.178, val_acc:0.924]
Epoch [84/120    avg_loss:0.255, val_acc:0.951]
Epoch [85/120    avg_loss:0.219, val_acc:0.967]
Epoch [86/120    avg_loss:0.181, val_acc:0.965]
Epoch [87/120    avg_loss:0.201, val_acc:0.949]
Epoch [88/120    avg_loss:0.211, val_acc:0.957]
Epoch [89/120    avg_loss:0.181, val_acc:0.957]
Epoch [90/120    avg_loss:0.158, val_acc:0.953]
Epoch [91/120    avg_loss:0.178, val_acc:0.953]
Epoch [92/120    avg_loss:0.152, val_acc:0.979]
Epoch [93/120    avg_loss:0.123, val_acc:0.977]
Epoch [94/120    avg_loss:0.110, val_acc:0.979]
Epoch [95/120    avg_loss:0.106, val_acc:0.980]
Epoch [96/120    avg_loss:0.116, val_acc:0.980]
Epoch [97/120    avg_loss:0.094, val_acc:0.980]
Epoch [98/120    avg_loss:0.104, val_acc:0.982]
Epoch [99/120    avg_loss:0.120, val_acc:0.982]
Epoch [100/120    avg_loss:0.095, val_acc:0.982]
Epoch [101/120    avg_loss:0.090, val_acc:0.984]
Epoch [102/120    avg_loss:0.099, val_acc:0.982]
Epoch [103/120    avg_loss:0.097, val_acc:0.980]
Epoch [104/120    avg_loss:0.103, val_acc:0.982]
Epoch [105/120    avg_loss:0.089, val_acc:0.982]
Epoch [106/120    avg_loss:0.099, val_acc:0.982]
Epoch [107/120    avg_loss:0.094, val_acc:0.980]
Epoch [108/120    avg_loss:0.095, val_acc:0.980]
Epoch [109/120    avg_loss:0.087, val_acc:0.982]
Epoch [110/120    avg_loss:0.092, val_acc:0.982]
Epoch [111/120    avg_loss:0.099, val_acc:0.980]
Epoch [112/120    avg_loss:0.100, val_acc:0.984]
Epoch [113/120    avg_loss:0.090, val_acc:0.984]
Epoch [114/120    avg_loss:0.086, val_acc:0.982]
Epoch [115/120    avg_loss:0.086, val_acc:0.984]
Epoch [116/120    avg_loss:0.088, val_acc:0.984]
Epoch [117/120    avg_loss:0.086, val_acc:0.984]
Epoch [118/120    avg_loss:0.096, val_acc:0.982]
Epoch [119/120    avg_loss:0.083, val_acc:0.984]
Epoch [120/120    avg_loss:0.093, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   8   0   0   0   1   0   0   0   0   0]
 [  0   0   0   2 207  17   0   0   0   0   1   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 1.         0.96475771 0.97571744 0.87898089 0.85106383
 0.99019608 0.90697674 0.998713   1.         0.99862826 1.
 1.         1.        ]

Kappa:
0.9824310895098681
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fefb40ddda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.653, val_acc:0.022]
Epoch [2/120    avg_loss:2.609, val_acc:0.111]
Epoch [3/120    avg_loss:2.573, val_acc:0.232]
Epoch [4/120    avg_loss:2.542, val_acc:0.271]
Epoch [5/120    avg_loss:2.511, val_acc:0.281]
Epoch [6/120    avg_loss:2.483, val_acc:0.287]
Epoch [7/120    avg_loss:2.453, val_acc:0.299]
Epoch [8/120    avg_loss:2.420, val_acc:0.305]
Epoch [9/120    avg_loss:2.390, val_acc:0.338]
Epoch [10/120    avg_loss:2.358, val_acc:0.375]
Epoch [11/120    avg_loss:2.328, val_acc:0.400]
Epoch [12/120    avg_loss:2.298, val_acc:0.418]
Epoch [13/120    avg_loss:2.273, val_acc:0.443]
Epoch [14/120    avg_loss:2.233, val_acc:0.457]
Epoch [15/120    avg_loss:2.205, val_acc:0.480]
Epoch [16/120    avg_loss:2.170, val_acc:0.498]
Epoch [17/120    avg_loss:2.135, val_acc:0.516]
Epoch [18/120    avg_loss:2.104, val_acc:0.537]
Epoch [19/120    avg_loss:2.054, val_acc:0.543]
Epoch [20/120    avg_loss:2.006, val_acc:0.566]
Epoch [21/120    avg_loss:1.958, val_acc:0.576]
Epoch [22/120    avg_loss:1.903, val_acc:0.639]
Epoch [23/120    avg_loss:1.848, val_acc:0.621]
Epoch [24/120    avg_loss:1.792, val_acc:0.754]
Epoch [25/120    avg_loss:1.718, val_acc:0.795]
Epoch [26/120    avg_loss:1.653, val_acc:0.809]
Epoch [27/120    avg_loss:1.588, val_acc:0.758]
Epoch [28/120    avg_loss:1.530, val_acc:0.848]
Epoch [29/120    avg_loss:1.438, val_acc:0.809]
Epoch [30/120    avg_loss:1.384, val_acc:0.789]
Epoch [31/120    avg_loss:1.335, val_acc:0.789]
Epoch [32/120    avg_loss:1.238, val_acc:0.869]
Epoch [33/120    avg_loss:1.160, val_acc:0.871]
Epoch [34/120    avg_loss:1.084, val_acc:0.898]
Epoch [35/120    avg_loss:1.003, val_acc:0.887]
Epoch [36/120    avg_loss:0.966, val_acc:0.904]
Epoch [37/120    avg_loss:0.911, val_acc:0.797]
Epoch [38/120    avg_loss:0.874, val_acc:0.898]
Epoch [39/120    avg_loss:0.842, val_acc:0.906]
Epoch [40/120    avg_loss:0.770, val_acc:0.887]
Epoch [41/120    avg_loss:0.755, val_acc:0.902]
Epoch [42/120    avg_loss:0.691, val_acc:0.914]
Epoch [43/120    avg_loss:0.635, val_acc:0.914]
Epoch [44/120    avg_loss:0.582, val_acc:0.912]
Epoch [45/120    avg_loss:0.570, val_acc:0.920]
Epoch [46/120    avg_loss:0.598, val_acc:0.906]
Epoch [47/120    avg_loss:0.538, val_acc:0.865]
Epoch [48/120    avg_loss:0.564, val_acc:0.922]
Epoch [49/120    avg_loss:0.534, val_acc:0.928]
Epoch [50/120    avg_loss:0.474, val_acc:0.895]
Epoch [51/120    avg_loss:0.454, val_acc:0.924]
Epoch [52/120    avg_loss:0.459, val_acc:0.920]
Epoch [53/120    avg_loss:0.432, val_acc:0.939]
Epoch [54/120    avg_loss:0.396, val_acc:0.928]
Epoch [55/120    avg_loss:0.375, val_acc:0.951]
Epoch [56/120    avg_loss:0.368, val_acc:0.936]
Epoch [57/120    avg_loss:0.349, val_acc:0.943]
Epoch [58/120    avg_loss:0.380, val_acc:0.873]
Epoch [59/120    avg_loss:0.345, val_acc:0.951]
Epoch [60/120    avg_loss:0.338, val_acc:0.953]
Epoch [61/120    avg_loss:0.367, val_acc:0.883]
Epoch [62/120    avg_loss:0.345, val_acc:0.963]
Epoch [63/120    avg_loss:0.319, val_acc:0.957]
Epoch [64/120    avg_loss:0.256, val_acc:0.963]
Epoch [65/120    avg_loss:0.242, val_acc:0.957]
Epoch [66/120    avg_loss:0.292, val_acc:0.959]
Epoch [67/120    avg_loss:0.245, val_acc:0.938]
Epoch [68/120    avg_loss:0.253, val_acc:0.965]
Epoch [69/120    avg_loss:0.201, val_acc:0.947]
Epoch [70/120    avg_loss:0.248, val_acc:0.959]
Epoch [71/120    avg_loss:0.202, val_acc:0.965]
Epoch [72/120    avg_loss:0.189, val_acc:0.957]
Epoch [73/120    avg_loss:0.324, val_acc:0.947]
Epoch [74/120    avg_loss:0.293, val_acc:0.930]
Epoch [75/120    avg_loss:0.230, val_acc:0.959]
Epoch [76/120    avg_loss:0.207, val_acc:0.957]
Epoch [77/120    avg_loss:0.198, val_acc:0.932]
Epoch [78/120    avg_loss:0.230, val_acc:0.951]
Epoch [79/120    avg_loss:0.202, val_acc:0.963]
Epoch [80/120    avg_loss:0.208, val_acc:0.973]
Epoch [81/120    avg_loss:0.178, val_acc:0.963]
Epoch [82/120    avg_loss:0.202, val_acc:0.957]
Epoch [83/120    avg_loss:0.187, val_acc:0.965]
Epoch [84/120    avg_loss:0.185, val_acc:0.951]
Epoch [85/120    avg_loss:0.191, val_acc:0.977]
Epoch [86/120    avg_loss:0.172, val_acc:0.959]
Epoch [87/120    avg_loss:0.137, val_acc:0.971]
Epoch [88/120    avg_loss:0.131, val_acc:0.977]
Epoch [89/120    avg_loss:0.120, val_acc:0.967]
Epoch [90/120    avg_loss:0.109, val_acc:0.967]
Epoch [91/120    avg_loss:0.126, val_acc:0.969]
Epoch [92/120    avg_loss:0.130, val_acc:0.965]
Epoch [93/120    avg_loss:0.118, val_acc:0.969]
Epoch [94/120    avg_loss:0.119, val_acc:0.967]
Epoch [95/120    avg_loss:0.117, val_acc:0.967]
Epoch [96/120    avg_loss:0.131, val_acc:0.963]
Epoch [97/120    avg_loss:0.152, val_acc:0.967]
Epoch [98/120    avg_loss:0.156, val_acc:0.934]
Epoch [99/120    avg_loss:0.186, val_acc:0.963]
Epoch [100/120    avg_loss:0.126, val_acc:0.965]
Epoch [101/120    avg_loss:0.103, val_acc:0.969]
Epoch [102/120    avg_loss:0.096, val_acc:0.973]
Epoch [103/120    avg_loss:0.090, val_acc:0.982]
Epoch [104/120    avg_loss:0.077, val_acc:0.977]
Epoch [105/120    avg_loss:0.078, val_acc:0.977]
Epoch [106/120    avg_loss:0.068, val_acc:0.979]
Epoch [107/120    avg_loss:0.072, val_acc:0.979]
Epoch [108/120    avg_loss:0.072, val_acc:0.977]
Epoch [109/120    avg_loss:0.075, val_acc:0.979]
Epoch [110/120    avg_loss:0.075, val_acc:0.977]
Epoch [111/120    avg_loss:0.063, val_acc:0.975]
Epoch [112/120    avg_loss:0.073, val_acc:0.973]
Epoch [113/120    avg_loss:0.067, val_acc:0.977]
Epoch [114/120    avg_loss:0.062, val_acc:0.982]
Epoch [115/120    avg_loss:0.066, val_acc:0.979]
Epoch [116/120    avg_loss:0.086, val_acc:0.980]
Epoch [117/120    avg_loss:0.065, val_acc:0.980]
Epoch [118/120    avg_loss:0.056, val_acc:0.979]
Epoch [119/120    avg_loss:0.062, val_acc:0.980]
Epoch [120/120    avg_loss:0.060, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 218   5   0   0   0   4   3   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 0.99708879 0.94666667 0.97321429 0.91561181 0.87272727
 0.99019608 0.86363636 0.99487179 0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9821910032745771
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f615148add8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.653, val_acc:0.016]
Epoch [2/120    avg_loss:2.603, val_acc:0.293]
Epoch [3/120    avg_loss:2.559, val_acc:0.284]
Epoch [4/120    avg_loss:2.523, val_acc:0.326]
Epoch [5/120    avg_loss:2.486, val_acc:0.295]
Epoch [6/120    avg_loss:2.459, val_acc:0.311]
Epoch [7/120    avg_loss:2.433, val_acc:0.295]
Epoch [8/120    avg_loss:2.405, val_acc:0.291]
Epoch [9/120    avg_loss:2.385, val_acc:0.311]
Epoch [10/120    avg_loss:2.357, val_acc:0.326]
Epoch [11/120    avg_loss:2.330, val_acc:0.336]
Epoch [12/120    avg_loss:2.309, val_acc:0.369]
Epoch [13/120    avg_loss:2.277, val_acc:0.387]
Epoch [14/120    avg_loss:2.249, val_acc:0.400]
Epoch [15/120    avg_loss:2.210, val_acc:0.416]
Epoch [16/120    avg_loss:2.181, val_acc:0.418]
Epoch [17/120    avg_loss:2.159, val_acc:0.430]
Epoch [18/120    avg_loss:2.103, val_acc:0.406]
Epoch [19/120    avg_loss:2.056, val_acc:0.459]
Epoch [20/120    avg_loss:2.022, val_acc:0.459]
Epoch [21/120    avg_loss:1.959, val_acc:0.525]
Epoch [22/120    avg_loss:1.921, val_acc:0.541]
Epoch [23/120    avg_loss:1.861, val_acc:0.648]
Epoch [24/120    avg_loss:1.804, val_acc:0.633]
Epoch [25/120    avg_loss:1.747, val_acc:0.639]
Epoch [26/120    avg_loss:1.694, val_acc:0.650]
Epoch [27/120    avg_loss:1.640, val_acc:0.648]
Epoch [28/120    avg_loss:1.580, val_acc:0.699]
Epoch [29/120    avg_loss:1.523, val_acc:0.779]
Epoch [30/120    avg_loss:1.479, val_acc:0.811]
Epoch [31/120    avg_loss:1.409, val_acc:0.783]
Epoch [32/120    avg_loss:1.316, val_acc:0.801]
Epoch [33/120    avg_loss:1.255, val_acc:0.787]
Epoch [34/120    avg_loss:1.193, val_acc:0.846]
Epoch [35/120    avg_loss:1.117, val_acc:0.869]
Epoch [36/120    avg_loss:1.041, val_acc:0.877]
Epoch [37/120    avg_loss:0.972, val_acc:0.902]
Epoch [38/120    avg_loss:0.962, val_acc:0.910]
Epoch [39/120    avg_loss:0.907, val_acc:0.898]
Epoch [40/120    avg_loss:0.888, val_acc:0.904]
Epoch [41/120    avg_loss:0.777, val_acc:0.896]
Epoch [42/120    avg_loss:0.746, val_acc:0.887]
Epoch [43/120    avg_loss:0.751, val_acc:0.904]
Epoch [44/120    avg_loss:0.681, val_acc:0.918]
Epoch [45/120    avg_loss:0.599, val_acc:0.928]
Epoch [46/120    avg_loss:0.576, val_acc:0.932]
Epoch [47/120    avg_loss:0.616, val_acc:0.918]
Epoch [48/120    avg_loss:0.561, val_acc:0.930]
Epoch [49/120    avg_loss:0.510, val_acc:0.918]
Epoch [50/120    avg_loss:0.558, val_acc:0.896]
Epoch [51/120    avg_loss:0.530, val_acc:0.920]
Epoch [52/120    avg_loss:0.565, val_acc:0.922]
Epoch [53/120    avg_loss:0.531, val_acc:0.916]
Epoch [54/120    avg_loss:0.506, val_acc:0.902]
Epoch [55/120    avg_loss:0.512, val_acc:0.926]
Epoch [56/120    avg_loss:0.411, val_acc:0.926]
Epoch [57/120    avg_loss:0.431, val_acc:0.938]
Epoch [58/120    avg_loss:0.412, val_acc:0.922]
Epoch [59/120    avg_loss:0.383, val_acc:0.939]
Epoch [60/120    avg_loss:0.377, val_acc:0.920]
Epoch [61/120    avg_loss:0.342, val_acc:0.945]
Epoch [62/120    avg_loss:0.329, val_acc:0.953]
Epoch [63/120    avg_loss:0.279, val_acc:0.949]
Epoch [64/120    avg_loss:0.264, val_acc:0.953]
Epoch [65/120    avg_loss:0.262, val_acc:0.939]
Epoch [66/120    avg_loss:0.241, val_acc:0.943]
Epoch [67/120    avg_loss:0.290, val_acc:0.951]
Epoch [68/120    avg_loss:0.267, val_acc:0.945]
Epoch [69/120    avg_loss:0.256, val_acc:0.938]
Epoch [70/120    avg_loss:0.237, val_acc:0.943]
Epoch [71/120    avg_loss:0.222, val_acc:0.953]
Epoch [72/120    avg_loss:0.199, val_acc:0.957]
Epoch [73/120    avg_loss:0.234, val_acc:0.932]
Epoch [74/120    avg_loss:0.215, val_acc:0.943]
Epoch [75/120    avg_loss:0.239, val_acc:0.938]
Epoch [76/120    avg_loss:0.172, val_acc:0.943]
Epoch [77/120    avg_loss:0.279, val_acc:0.939]
Epoch [78/120    avg_loss:0.200, val_acc:0.941]
Epoch [79/120    avg_loss:0.240, val_acc:0.959]
Epoch [80/120    avg_loss:0.199, val_acc:0.945]
Epoch [81/120    avg_loss:0.201, val_acc:0.961]
Epoch [82/120    avg_loss:0.188, val_acc:0.959]
Epoch [83/120    avg_loss:0.190, val_acc:0.953]
Epoch [84/120    avg_loss:0.201, val_acc:0.959]
Epoch [85/120    avg_loss:0.196, val_acc:0.957]
Epoch [86/120    avg_loss:0.235, val_acc:0.957]
Epoch [87/120    avg_loss:0.220, val_acc:0.943]
Epoch [88/120    avg_loss:0.216, val_acc:0.949]
Epoch [89/120    avg_loss:0.188, val_acc:0.955]
Epoch [90/120    avg_loss:0.209, val_acc:0.967]
Epoch [91/120    avg_loss:0.241, val_acc:0.963]
Epoch [92/120    avg_loss:0.156, val_acc:0.938]
Epoch [93/120    avg_loss:0.155, val_acc:0.955]
Epoch [94/120    avg_loss:0.136, val_acc:0.963]
Epoch [95/120    avg_loss:0.108, val_acc:0.959]
Epoch [96/120    avg_loss:0.132, val_acc:0.951]
Epoch [97/120    avg_loss:0.108, val_acc:0.969]
Epoch [98/120    avg_loss:0.122, val_acc:0.955]
Epoch [99/120    avg_loss:0.100, val_acc:0.969]
Epoch [100/120    avg_loss:0.131, val_acc:0.953]
Epoch [101/120    avg_loss:0.133, val_acc:0.957]
Epoch [102/120    avg_loss:0.090, val_acc:0.963]
Epoch [103/120    avg_loss:0.144, val_acc:0.965]
Epoch [104/120    avg_loss:0.136, val_acc:0.965]
Epoch [105/120    avg_loss:0.148, val_acc:0.965]
Epoch [106/120    avg_loss:0.146, val_acc:0.963]
Epoch [107/120    avg_loss:0.098, val_acc:0.961]
Epoch [108/120    avg_loss:0.111, val_acc:0.977]
Epoch [109/120    avg_loss:0.103, val_acc:0.955]
Epoch [110/120    avg_loss:0.163, val_acc:0.938]
Epoch [111/120    avg_loss:0.140, val_acc:0.959]
Epoch [112/120    avg_loss:0.101, val_acc:0.965]
Epoch [113/120    avg_loss:0.104, val_acc:0.963]
Epoch [114/120    avg_loss:0.090, val_acc:0.965]
Epoch [115/120    avg_loss:0.130, val_acc:0.967]
Epoch [116/120    avg_loss:0.149, val_acc:0.965]
Epoch [117/120    avg_loss:0.106, val_acc:0.947]
Epoch [118/120    avg_loss:0.087, val_acc:0.953]
Epoch [119/120    avg_loss:0.068, val_acc:0.979]
Epoch [120/120    avg_loss:0.080, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 500   0   0   0   0 185   0   0   0   0   0   0   0]
 [  0   0 184   0   0   3   0  32   0   0   0   0   0   0]
 [  0   0   0 217   9   0   0   0   0   4   0   0   0   0]
 [  0   0   0   3 217   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  36 109   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   4 201   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   1   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 451   2]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
93.64605543710022

F1 scores:
[       nan 0.84388186 0.89104116 0.96230599 0.88571429 0.81343284
 0.67905405 0.8        0.99870968 0.99574468 1.         0.99867198
 0.99668508 0.9988024 ]

Kappa:
0.9295733625839555
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5cdf218e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.096]
Epoch [2/120    avg_loss:2.573, val_acc:0.096]
Epoch [3/120    avg_loss:2.533, val_acc:0.098]
Epoch [4/120    avg_loss:2.502, val_acc:0.115]
Epoch [5/120    avg_loss:2.468, val_acc:0.227]
Epoch [6/120    avg_loss:2.437, val_acc:0.307]
Epoch [7/120    avg_loss:2.414, val_acc:0.322]
Epoch [8/120    avg_loss:2.372, val_acc:0.311]
Epoch [9/120    avg_loss:2.340, val_acc:0.303]
Epoch [10/120    avg_loss:2.300, val_acc:0.299]
Epoch [11/120    avg_loss:2.250, val_acc:0.297]
Epoch [12/120    avg_loss:2.205, val_acc:0.299]
Epoch [13/120    avg_loss:2.170, val_acc:0.320]
Epoch [14/120    avg_loss:2.110, val_acc:0.445]
Epoch [15/120    avg_loss:2.058, val_acc:0.441]
Epoch [16/120    avg_loss:2.011, val_acc:0.549]
Epoch [17/120    avg_loss:1.982, val_acc:0.564]
Epoch [18/120    avg_loss:1.911, val_acc:0.520]
Epoch [19/120    avg_loss:1.846, val_acc:0.619]
Epoch [20/120    avg_loss:1.797, val_acc:0.648]
Epoch [21/120    avg_loss:1.714, val_acc:0.689]
Epoch [22/120    avg_loss:1.640, val_acc:0.721]
Epoch [23/120    avg_loss:1.575, val_acc:0.785]
Epoch [24/120    avg_loss:1.539, val_acc:0.775]
Epoch [25/120    avg_loss:1.456, val_acc:0.834]
Epoch [26/120    avg_loss:1.411, val_acc:0.824]
Epoch [27/120    avg_loss:1.356, val_acc:0.805]
Epoch [28/120    avg_loss:1.300, val_acc:0.863]
Epoch [29/120    avg_loss:1.213, val_acc:0.854]
Epoch [30/120    avg_loss:1.187, val_acc:0.865]
Epoch [31/120    avg_loss:1.112, val_acc:0.877]
Epoch [32/120    avg_loss:1.029, val_acc:0.891]
Epoch [33/120    avg_loss:0.985, val_acc:0.893]
Epoch [34/120    avg_loss:0.918, val_acc:0.908]
Epoch [35/120    avg_loss:0.907, val_acc:0.902]
Epoch [36/120    avg_loss:0.826, val_acc:0.885]
Epoch [37/120    avg_loss:0.798, val_acc:0.912]
Epoch [38/120    avg_loss:0.746, val_acc:0.916]
Epoch [39/120    avg_loss:0.721, val_acc:0.928]
Epoch [40/120    avg_loss:0.665, val_acc:0.912]
Epoch [41/120    avg_loss:0.654, val_acc:0.930]
Epoch [42/120    avg_loss:0.647, val_acc:0.914]
Epoch [43/120    avg_loss:0.619, val_acc:0.924]
Epoch [44/120    avg_loss:0.598, val_acc:0.930]
Epoch [45/120    avg_loss:0.590, val_acc:0.920]
Epoch [46/120    avg_loss:0.545, val_acc:0.932]
Epoch [47/120    avg_loss:0.528, val_acc:0.930]
Epoch [48/120    avg_loss:0.508, val_acc:0.932]
Epoch [49/120    avg_loss:0.474, val_acc:0.938]
Epoch [50/120    avg_loss:0.446, val_acc:0.941]
Epoch [51/120    avg_loss:0.482, val_acc:0.932]
Epoch [52/120    avg_loss:0.461, val_acc:0.898]
Epoch [53/120    avg_loss:0.438, val_acc:0.943]
Epoch [54/120    avg_loss:0.433, val_acc:0.939]
Epoch [55/120    avg_loss:0.440, val_acc:0.930]
Epoch [56/120    avg_loss:0.436, val_acc:0.947]
Epoch [57/120    avg_loss:0.420, val_acc:0.951]
Epoch [58/120    avg_loss:0.465, val_acc:0.938]
Epoch [59/120    avg_loss:0.394, val_acc:0.936]
Epoch [60/120    avg_loss:0.350, val_acc:0.947]
Epoch [61/120    avg_loss:0.385, val_acc:0.926]
Epoch [62/120    avg_loss:0.457, val_acc:0.930]
Epoch [63/120    avg_loss:0.415, val_acc:0.941]
Epoch [64/120    avg_loss:0.394, val_acc:0.941]
Epoch [65/120    avg_loss:0.330, val_acc:0.951]
Epoch [66/120    avg_loss:0.346, val_acc:0.953]
Epoch [67/120    avg_loss:0.330, val_acc:0.949]
Epoch [68/120    avg_loss:0.340, val_acc:0.932]
Epoch [69/120    avg_loss:0.391, val_acc:0.928]
Epoch [70/120    avg_loss:0.359, val_acc:0.916]
Epoch [71/120    avg_loss:0.384, val_acc:0.938]
Epoch [72/120    avg_loss:0.304, val_acc:0.943]
Epoch [73/120    avg_loss:0.302, val_acc:0.930]
Epoch [74/120    avg_loss:0.343, val_acc:0.949]
Epoch [75/120    avg_loss:0.306, val_acc:0.941]
Epoch [76/120    avg_loss:0.247, val_acc:0.965]
Epoch [77/120    avg_loss:0.259, val_acc:0.951]
Epoch [78/120    avg_loss:0.262, val_acc:0.957]
Epoch [79/120    avg_loss:0.332, val_acc:0.945]
Epoch [80/120    avg_loss:0.294, val_acc:0.963]
Epoch [81/120    avg_loss:0.309, val_acc:0.955]
Epoch [82/120    avg_loss:0.307, val_acc:0.957]
Epoch [83/120    avg_loss:0.265, val_acc:0.965]
Epoch [84/120    avg_loss:0.265, val_acc:0.965]
Epoch [85/120    avg_loss:0.221, val_acc:0.963]
Epoch [86/120    avg_loss:0.209, val_acc:0.965]
Epoch [87/120    avg_loss:0.218, val_acc:0.963]
Epoch [88/120    avg_loss:0.208, val_acc:0.973]
Epoch [89/120    avg_loss:0.231, val_acc:0.932]
Epoch [90/120    avg_loss:0.273, val_acc:0.967]
Epoch [91/120    avg_loss:0.247, val_acc:0.951]
Epoch [92/120    avg_loss:0.205, val_acc:0.963]
Epoch [93/120    avg_loss:0.214, val_acc:0.969]
Epoch [94/120    avg_loss:0.203, val_acc:0.965]
Epoch [95/120    avg_loss:0.170, val_acc:0.959]
Epoch [96/120    avg_loss:0.204, val_acc:0.953]
Epoch [97/120    avg_loss:0.178, val_acc:0.949]
Epoch [98/120    avg_loss:0.240, val_acc:0.945]
Epoch [99/120    avg_loss:0.220, val_acc:0.951]
Epoch [100/120    avg_loss:0.187, val_acc:0.963]
Epoch [101/120    avg_loss:0.174, val_acc:0.965]
Epoch [102/120    avg_loss:0.160, val_acc:0.967]
Epoch [103/120    avg_loss:0.128, val_acc:0.969]
Epoch [104/120    avg_loss:0.126, val_acc:0.965]
Epoch [105/120    avg_loss:0.138, val_acc:0.971]
Epoch [106/120    avg_loss:0.117, val_acc:0.963]
Epoch [107/120    avg_loss:0.126, val_acc:0.971]
Epoch [108/120    avg_loss:0.115, val_acc:0.967]
Epoch [109/120    avg_loss:0.128, val_acc:0.967]
Epoch [110/120    avg_loss:0.112, val_acc:0.969]
Epoch [111/120    avg_loss:0.110, val_acc:0.969]
Epoch [112/120    avg_loss:0.111, val_acc:0.965]
Epoch [113/120    avg_loss:0.132, val_acc:0.975]
Epoch [114/120    avg_loss:0.111, val_acc:0.973]
Epoch [115/120    avg_loss:0.122, val_acc:0.973]
Epoch [116/120    avg_loss:0.133, val_acc:0.967]
Epoch [117/120    avg_loss:0.113, val_acc:0.969]
Epoch [118/120    avg_loss:0.111, val_acc:0.969]
Epoch [119/120    avg_loss:0.098, val_acc:0.967]
Epoch [120/120    avg_loss:0.108, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 220   6   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 219   7   0   0   0   0   1   0   0   0]
 [  0   0   0   0  39 106   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  21   0   0   0   0  73   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.12366737739872

F1 scores:
[       nan 0.99927061 0.93832599 0.97777778 0.89205703 0.82170543
 0.99756691 0.84393064 0.99230769 0.99893276 0.99862826 1.
 0.99778761 1.        ]

Kappa:
0.9791046045610204
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f24b1e8dda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.327]
Epoch [2/120    avg_loss:2.592, val_acc:0.387]
Epoch [3/120    avg_loss:2.554, val_acc:0.404]
Epoch [4/120    avg_loss:2.520, val_acc:0.357]
Epoch [5/120    avg_loss:2.484, val_acc:0.309]
Epoch [6/120    avg_loss:2.448, val_acc:0.314]
Epoch [7/120    avg_loss:2.416, val_acc:0.314]
Epoch [8/120    avg_loss:2.377, val_acc:0.314]
Epoch [9/120    avg_loss:2.342, val_acc:0.318]
Epoch [10/120    avg_loss:2.319, val_acc:0.330]
Epoch [11/120    avg_loss:2.249, val_acc:0.359]
Epoch [12/120    avg_loss:2.205, val_acc:0.408]
Epoch [13/120    avg_loss:2.156, val_acc:0.436]
Epoch [14/120    avg_loss:2.103, val_acc:0.469]
Epoch [15/120    avg_loss:2.059, val_acc:0.436]
Epoch [16/120    avg_loss:2.000, val_acc:0.506]
Epoch [17/120    avg_loss:1.946, val_acc:0.535]
Epoch [18/120    avg_loss:1.896, val_acc:0.549]
Epoch [19/120    avg_loss:1.847, val_acc:0.566]
Epoch [20/120    avg_loss:1.795, val_acc:0.574]
Epoch [21/120    avg_loss:1.743, val_acc:0.604]
Epoch [22/120    avg_loss:1.725, val_acc:0.629]
Epoch [23/120    avg_loss:1.678, val_acc:0.654]
Epoch [24/120    avg_loss:1.566, val_acc:0.697]
Epoch [25/120    avg_loss:1.504, val_acc:0.705]
Epoch [26/120    avg_loss:1.436, val_acc:0.760]
Epoch [27/120    avg_loss:1.389, val_acc:0.752]
Epoch [28/120    avg_loss:1.306, val_acc:0.863]
Epoch [29/120    avg_loss:1.237, val_acc:0.771]
Epoch [30/120    avg_loss:1.165, val_acc:0.770]
Epoch [31/120    avg_loss:1.123, val_acc:0.783]
Epoch [32/120    avg_loss:1.060, val_acc:0.783]
Epoch [33/120    avg_loss:0.950, val_acc:0.893]
Epoch [34/120    avg_loss:0.899, val_acc:0.924]
Epoch [35/120    avg_loss:0.839, val_acc:0.904]
Epoch [36/120    avg_loss:0.798, val_acc:0.928]
Epoch [37/120    avg_loss:0.742, val_acc:0.891]
Epoch [38/120    avg_loss:0.730, val_acc:0.887]
Epoch [39/120    avg_loss:0.673, val_acc:0.941]
Epoch [40/120    avg_loss:0.640, val_acc:0.928]
Epoch [41/120    avg_loss:0.619, val_acc:0.920]
Epoch [42/120    avg_loss:0.573, val_acc:0.938]
Epoch [43/120    avg_loss:0.558, val_acc:0.930]
Epoch [44/120    avg_loss:0.587, val_acc:0.926]
Epoch [45/120    avg_loss:0.525, val_acc:0.943]
Epoch [46/120    avg_loss:0.581, val_acc:0.891]
Epoch [47/120    avg_loss:0.525, val_acc:0.941]
Epoch [48/120    avg_loss:0.512, val_acc:0.934]
Epoch [49/120    avg_loss:0.458, val_acc:0.928]
Epoch [50/120    avg_loss:0.481, val_acc:0.938]
Epoch [51/120    avg_loss:0.430, val_acc:0.953]
Epoch [52/120    avg_loss:0.398, val_acc:0.941]
Epoch [53/120    avg_loss:0.367, val_acc:0.947]
Epoch [54/120    avg_loss:0.356, val_acc:0.957]
Epoch [55/120    avg_loss:0.346, val_acc:0.957]
Epoch [56/120    avg_loss:0.350, val_acc:0.951]
Epoch [57/120    avg_loss:0.361, val_acc:0.910]
Epoch [58/120    avg_loss:0.309, val_acc:0.965]
Epoch [59/120    avg_loss:0.296, val_acc:0.980]
Epoch [60/120    avg_loss:0.262, val_acc:0.953]
Epoch [61/120    avg_loss:0.306, val_acc:0.969]
Epoch [62/120    avg_loss:0.283, val_acc:0.963]
Epoch [63/120    avg_loss:0.254, val_acc:0.965]
Epoch [64/120    avg_loss:0.232, val_acc:0.955]
Epoch [65/120    avg_loss:0.244, val_acc:0.973]
Epoch [66/120    avg_loss:0.209, val_acc:0.986]
Epoch [67/120    avg_loss:0.232, val_acc:0.965]
Epoch [68/120    avg_loss:0.251, val_acc:0.979]
Epoch [69/120    avg_loss:0.212, val_acc:0.984]
Epoch [70/120    avg_loss:0.229, val_acc:0.969]
Epoch [71/120    avg_loss:0.193, val_acc:0.977]
Epoch [72/120    avg_loss:0.214, val_acc:0.973]
Epoch [73/120    avg_loss:0.313, val_acc:0.924]
Epoch [74/120    avg_loss:0.310, val_acc:0.932]
Epoch [75/120    avg_loss:0.240, val_acc:0.967]
Epoch [76/120    avg_loss:0.203, val_acc:0.965]
Epoch [77/120    avg_loss:0.204, val_acc:0.939]
Epoch [78/120    avg_loss:0.225, val_acc:0.959]
Epoch [79/120    avg_loss:0.213, val_acc:0.973]
Epoch [80/120    avg_loss:0.158, val_acc:0.984]
Epoch [81/120    avg_loss:0.146, val_acc:0.975]
Epoch [82/120    avg_loss:0.135, val_acc:0.979]
Epoch [83/120    avg_loss:0.133, val_acc:0.979]
Epoch [84/120    avg_loss:0.131, val_acc:0.982]
Epoch [85/120    avg_loss:0.119, val_acc:0.982]
Epoch [86/120    avg_loss:0.142, val_acc:0.984]
Epoch [87/120    avg_loss:0.126, val_acc:0.984]
Epoch [88/120    avg_loss:0.129, val_acc:0.986]
Epoch [89/120    avg_loss:0.130, val_acc:0.986]
Epoch [90/120    avg_loss:0.113, val_acc:0.984]
Epoch [91/120    avg_loss:0.123, val_acc:0.982]
Epoch [92/120    avg_loss:0.117, val_acc:0.988]
Epoch [93/120    avg_loss:0.123, val_acc:0.986]
Epoch [94/120    avg_loss:0.113, val_acc:0.984]
Epoch [95/120    avg_loss:0.123, val_acc:0.982]
Epoch [96/120    avg_loss:0.123, val_acc:0.982]
Epoch [97/120    avg_loss:0.111, val_acc:0.984]
Epoch [98/120    avg_loss:0.113, val_acc:0.984]
Epoch [99/120    avg_loss:0.108, val_acc:0.982]
Epoch [100/120    avg_loss:0.110, val_acc:0.986]
Epoch [101/120    avg_loss:0.110, val_acc:0.984]
Epoch [102/120    avg_loss:0.113, val_acc:0.982]
Epoch [103/120    avg_loss:0.124, val_acc:0.988]
Epoch [104/120    avg_loss:0.126, val_acc:0.986]
Epoch [105/120    avg_loss:0.114, val_acc:0.986]
Epoch [106/120    avg_loss:0.103, val_acc:0.984]
Epoch [107/120    avg_loss:0.117, val_acc:0.988]
Epoch [108/120    avg_loss:0.108, val_acc:0.988]
Epoch [109/120    avg_loss:0.098, val_acc:0.980]
Epoch [110/120    avg_loss:0.101, val_acc:0.984]
Epoch [111/120    avg_loss:0.106, val_acc:0.980]
Epoch [112/120    avg_loss:0.103, val_acc:0.988]
Epoch [113/120    avg_loss:0.115, val_acc:0.988]
Epoch [114/120    avg_loss:0.101, val_acc:0.982]
Epoch [115/120    avg_loss:0.110, val_acc:0.984]
Epoch [116/120    avg_loss:0.104, val_acc:0.986]
Epoch [117/120    avg_loss:0.115, val_acc:0.982]
Epoch [118/120    avg_loss:0.095, val_acc:0.988]
Epoch [119/120    avg_loss:0.099, val_acc:0.988]
Epoch [120/120    avg_loss:0.101, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 217   7   0   0   0   2   4   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  21   0   0   0   0  73   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.2089552238806

F1 scores:
[       nan 1.         0.95424837 0.97091723 0.88222698 0.83098592
 1.         0.8742515  0.99742931 0.99574468 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9800553989694784
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc43aa77dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.626, val_acc:0.082]
Epoch [2/120    avg_loss:2.602, val_acc:0.082]
Epoch [3/120    avg_loss:2.574, val_acc:0.145]
Epoch [4/120    avg_loss:2.545, val_acc:0.188]
Epoch [5/120    avg_loss:2.517, val_acc:0.268]
Epoch [6/120    avg_loss:2.482, val_acc:0.273]
Epoch [7/120    avg_loss:2.459, val_acc:0.285]
Epoch [8/120    avg_loss:2.432, val_acc:0.326]
Epoch [9/120    avg_loss:2.405, val_acc:0.363]
Epoch [10/120    avg_loss:2.369, val_acc:0.369]
Epoch [11/120    avg_loss:2.336, val_acc:0.391]
Epoch [12/120    avg_loss:2.296, val_acc:0.402]
Epoch [13/120    avg_loss:2.248, val_acc:0.439]
Epoch [14/120    avg_loss:2.193, val_acc:0.473]
Epoch [15/120    avg_loss:2.132, val_acc:0.506]
Epoch [16/120    avg_loss:2.079, val_acc:0.525]
Epoch [17/120    avg_loss:2.017, val_acc:0.537]
Epoch [18/120    avg_loss:1.959, val_acc:0.580]
Epoch [19/120    avg_loss:1.906, val_acc:0.605]
Epoch [20/120    avg_loss:1.833, val_acc:0.625]
Epoch [21/120    avg_loss:1.780, val_acc:0.643]
Epoch [22/120    avg_loss:1.715, val_acc:0.682]
Epoch [23/120    avg_loss:1.639, val_acc:0.684]
Epoch [24/120    avg_loss:1.565, val_acc:0.697]
Epoch [25/120    avg_loss:1.498, val_acc:0.717]
Epoch [26/120    avg_loss:1.446, val_acc:0.713]
Epoch [27/120    avg_loss:1.403, val_acc:0.742]
Epoch [28/120    avg_loss:1.304, val_acc:0.756]
Epoch [29/120    avg_loss:1.220, val_acc:0.781]
Epoch [30/120    avg_loss:1.114, val_acc:0.838]
Epoch [31/120    avg_loss:1.079, val_acc:0.889]
Epoch [32/120    avg_loss:0.985, val_acc:0.895]
Epoch [33/120    avg_loss:0.928, val_acc:0.904]
Epoch [34/120    avg_loss:0.870, val_acc:0.928]
Epoch [35/120    avg_loss:0.785, val_acc:0.904]
Epoch [36/120    avg_loss:0.741, val_acc:0.924]
Epoch [37/120    avg_loss:0.669, val_acc:0.936]
Epoch [38/120    avg_loss:0.617, val_acc:0.924]
Epoch [39/120    avg_loss:0.582, val_acc:0.904]
Epoch [40/120    avg_loss:0.577, val_acc:0.906]
Epoch [41/120    avg_loss:0.551, val_acc:0.930]
Epoch [42/120    avg_loss:0.510, val_acc:0.916]
Epoch [43/120    avg_loss:0.508, val_acc:0.932]
Epoch [44/120    avg_loss:0.504, val_acc:0.914]
Epoch [45/120    avg_loss:0.492, val_acc:0.914]
Epoch [46/120    avg_loss:0.484, val_acc:0.912]
Epoch [47/120    avg_loss:0.442, val_acc:0.885]
Epoch [48/120    avg_loss:0.417, val_acc:0.906]
Epoch [49/120    avg_loss:0.416, val_acc:0.947]
Epoch [50/120    avg_loss:0.369, val_acc:0.939]
Epoch [51/120    avg_loss:0.361, val_acc:0.916]
Epoch [52/120    avg_loss:0.387, val_acc:0.947]
Epoch [53/120    avg_loss:0.326, val_acc:0.963]
Epoch [54/120    avg_loss:0.329, val_acc:0.947]
Epoch [55/120    avg_loss:0.315, val_acc:0.961]
Epoch [56/120    avg_loss:0.357, val_acc:0.936]
Epoch [57/120    avg_loss:0.301, val_acc:0.941]
Epoch [58/120    avg_loss:0.340, val_acc:0.963]
Epoch [59/120    avg_loss:0.346, val_acc:0.912]
Epoch [60/120    avg_loss:0.396, val_acc:0.887]
Epoch [61/120    avg_loss:0.368, val_acc:0.945]
Epoch [62/120    avg_loss:0.305, val_acc:0.943]
Epoch [63/120    avg_loss:0.280, val_acc:0.961]
Epoch [64/120    avg_loss:0.225, val_acc:0.959]
Epoch [65/120    avg_loss:0.258, val_acc:0.967]
Epoch [66/120    avg_loss:0.223, val_acc:0.967]
Epoch [67/120    avg_loss:0.218, val_acc:0.980]
Epoch [68/120    avg_loss:0.267, val_acc:0.934]
Epoch [69/120    avg_loss:0.238, val_acc:0.955]
Epoch [70/120    avg_loss:0.201, val_acc:0.973]
Epoch [71/120    avg_loss:0.229, val_acc:0.961]
Epoch [72/120    avg_loss:0.232, val_acc:0.959]
Epoch [73/120    avg_loss:0.185, val_acc:0.957]
Epoch [74/120    avg_loss:0.209, val_acc:0.967]
Epoch [75/120    avg_loss:0.219, val_acc:0.963]
Epoch [76/120    avg_loss:0.206, val_acc:0.971]
Epoch [77/120    avg_loss:0.168, val_acc:0.979]
Epoch [78/120    avg_loss:0.165, val_acc:0.979]
Epoch [79/120    avg_loss:0.198, val_acc:0.971]
Epoch [80/120    avg_loss:0.172, val_acc:0.969]
Epoch [81/120    avg_loss:0.151, val_acc:0.973]
Epoch [82/120    avg_loss:0.129, val_acc:0.975]
Epoch [83/120    avg_loss:0.128, val_acc:0.975]
Epoch [84/120    avg_loss:0.118, val_acc:0.975]
Epoch [85/120    avg_loss:0.137, val_acc:0.975]
Epoch [86/120    avg_loss:0.128, val_acc:0.979]
Epoch [87/120    avg_loss:0.123, val_acc:0.979]
Epoch [88/120    avg_loss:0.136, val_acc:0.979]
Epoch [89/120    avg_loss:0.115, val_acc:0.979]
Epoch [90/120    avg_loss:0.120, val_acc:0.979]
Epoch [91/120    avg_loss:0.127, val_acc:0.980]
Epoch [92/120    avg_loss:0.121, val_acc:0.980]
Epoch [93/120    avg_loss:0.125, val_acc:0.979]
Epoch [94/120    avg_loss:0.122, val_acc:0.977]
Epoch [95/120    avg_loss:0.126, val_acc:0.980]
Epoch [96/120    avg_loss:0.108, val_acc:0.977]
Epoch [97/120    avg_loss:0.115, val_acc:0.980]
Epoch [98/120    avg_loss:0.113, val_acc:0.979]
Epoch [99/120    avg_loss:0.117, val_acc:0.980]
Epoch [100/120    avg_loss:0.112, val_acc:0.982]
Epoch [101/120    avg_loss:0.116, val_acc:0.982]
Epoch [102/120    avg_loss:0.117, val_acc:0.982]
Epoch [103/120    avg_loss:0.120, val_acc:0.982]
Epoch [104/120    avg_loss:0.127, val_acc:0.984]
Epoch [105/120    avg_loss:0.116, val_acc:0.980]
Epoch [106/120    avg_loss:0.111, val_acc:0.984]
Epoch [107/120    avg_loss:0.116, val_acc:0.982]
Epoch [108/120    avg_loss:0.105, val_acc:0.982]
Epoch [109/120    avg_loss:0.107, val_acc:0.980]
Epoch [110/120    avg_loss:0.115, val_acc:0.980]
Epoch [111/120    avg_loss:0.119, val_acc:0.980]
Epoch [112/120    avg_loss:0.115, val_acc:0.980]
Epoch [113/120    avg_loss:0.127, val_acc:0.982]
Epoch [114/120    avg_loss:0.113, val_acc:0.982]
Epoch [115/120    avg_loss:0.124, val_acc:0.982]
Epoch [116/120    avg_loss:0.108, val_acc:0.980]
Epoch [117/120    avg_loss:0.113, val_acc:0.982]
Epoch [118/120    avg_loss:0.103, val_acc:0.980]
Epoch [119/120    avg_loss:0.107, val_acc:0.982]
Epoch [120/120    avg_loss:0.099, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 218   5   1   0   0   5   1   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  30 115   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  22   0   0   0   0  72   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   3   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.01705756929637

F1 scores:
[       nan 1.         0.93126386 0.97321429 0.88983051 0.82733813
 1.         0.82285714 0.99359795 0.99893276 0.99589603 1.
 0.99667774 1.        ]

Kappa:
0.9779197785872729
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f271cd52e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.031]
Epoch [2/120    avg_loss:2.589, val_acc:0.178]
Epoch [3/120    avg_loss:2.557, val_acc:0.201]
Epoch [4/120    avg_loss:2.524, val_acc:0.377]
Epoch [5/120    avg_loss:2.491, val_acc:0.504]
Epoch [6/120    avg_loss:2.456, val_acc:0.504]
Epoch [7/120    avg_loss:2.418, val_acc:0.508]
Epoch [8/120    avg_loss:2.386, val_acc:0.559]
Epoch [9/120    avg_loss:2.342, val_acc:0.576]
Epoch [10/120    avg_loss:2.297, val_acc:0.617]
Epoch [11/120    avg_loss:2.263, val_acc:0.631]
Epoch [12/120    avg_loss:2.214, val_acc:0.617]
Epoch [13/120    avg_loss:2.166, val_acc:0.639]
Epoch [14/120    avg_loss:2.110, val_acc:0.680]
Epoch [15/120    avg_loss:2.051, val_acc:0.709]
Epoch [16/120    avg_loss:1.981, val_acc:0.697]
Epoch [17/120    avg_loss:1.913, val_acc:0.717]
Epoch [18/120    avg_loss:1.856, val_acc:0.734]
Epoch [19/120    avg_loss:1.774, val_acc:0.709]
Epoch [20/120    avg_loss:1.696, val_acc:0.768]
Epoch [21/120    avg_loss:1.620, val_acc:0.750]
Epoch [22/120    avg_loss:1.520, val_acc:0.770]
Epoch [23/120    avg_loss:1.455, val_acc:0.756]
Epoch [24/120    avg_loss:1.377, val_acc:0.809]
Epoch [25/120    avg_loss:1.299, val_acc:0.852]
Epoch [26/120    avg_loss:1.232, val_acc:0.836]
Epoch [27/120    avg_loss:1.177, val_acc:0.820]
Epoch [28/120    avg_loss:1.115, val_acc:0.830]
Epoch [29/120    avg_loss:1.049, val_acc:0.832]
Epoch [30/120    avg_loss:0.988, val_acc:0.867]
Epoch [31/120    avg_loss:0.935, val_acc:0.865]
Epoch [32/120    avg_loss:0.870, val_acc:0.863]
Epoch [33/120    avg_loss:0.804, val_acc:0.865]
Epoch [34/120    avg_loss:0.765, val_acc:0.900]
Epoch [35/120    avg_loss:0.752, val_acc:0.908]
Epoch [36/120    avg_loss:0.724, val_acc:0.891]
Epoch [37/120    avg_loss:0.675, val_acc:0.896]
Epoch [38/120    avg_loss:0.642, val_acc:0.914]
Epoch [39/120    avg_loss:0.623, val_acc:0.912]
Epoch [40/120    avg_loss:0.605, val_acc:0.916]
Epoch [41/120    avg_loss:0.571, val_acc:0.918]
Epoch [42/120    avg_loss:0.535, val_acc:0.924]
Epoch [43/120    avg_loss:0.504, val_acc:0.914]
Epoch [44/120    avg_loss:0.496, val_acc:0.920]
Epoch [45/120    avg_loss:0.485, val_acc:0.898]
Epoch [46/120    avg_loss:0.464, val_acc:0.902]
Epoch [47/120    avg_loss:0.484, val_acc:0.916]
Epoch [48/120    avg_loss:0.447, val_acc:0.928]
Epoch [49/120    avg_loss:0.426, val_acc:0.936]
Epoch [50/120    avg_loss:0.415, val_acc:0.924]
Epoch [51/120    avg_loss:0.419, val_acc:0.926]
Epoch [52/120    avg_loss:0.369, val_acc:0.930]
Epoch [53/120    avg_loss:0.354, val_acc:0.938]
Epoch [54/120    avg_loss:0.366, val_acc:0.934]
Epoch [55/120    avg_loss:0.384, val_acc:0.926]
Epoch [56/120    avg_loss:0.364, val_acc:0.943]
Epoch [57/120    avg_loss:0.384, val_acc:0.934]
Epoch [58/120    avg_loss:0.316, val_acc:0.953]
Epoch [59/120    avg_loss:0.289, val_acc:0.953]
Epoch [60/120    avg_loss:0.288, val_acc:0.930]
Epoch [61/120    avg_loss:0.297, val_acc:0.953]
Epoch [62/120    avg_loss:0.320, val_acc:0.922]
Epoch [63/120    avg_loss:0.298, val_acc:0.965]
Epoch [64/120    avg_loss:0.301, val_acc:0.955]
Epoch [65/120    avg_loss:0.283, val_acc:0.949]
Epoch [66/120    avg_loss:0.305, val_acc:0.932]
Epoch [67/120    avg_loss:0.270, val_acc:0.947]
Epoch [68/120    avg_loss:0.264, val_acc:0.969]
Epoch [69/120    avg_loss:0.208, val_acc:0.963]
Epoch [70/120    avg_loss:0.202, val_acc:0.953]
Epoch [71/120    avg_loss:0.211, val_acc:0.963]
Epoch [72/120    avg_loss:0.184, val_acc:0.967]
Epoch [73/120    avg_loss:0.185, val_acc:0.971]
Epoch [74/120    avg_loss:0.209, val_acc:0.959]
Epoch [75/120    avg_loss:0.216, val_acc:0.969]
Epoch [76/120    avg_loss:0.239, val_acc:0.959]
Epoch [77/120    avg_loss:0.258, val_acc:0.951]
Epoch [78/120    avg_loss:0.302, val_acc:0.904]
Epoch [79/120    avg_loss:0.273, val_acc:0.941]
Epoch [80/120    avg_loss:0.240, val_acc:0.971]
Epoch [81/120    avg_loss:0.194, val_acc:0.980]
Epoch [82/120    avg_loss:0.185, val_acc:0.969]
Epoch [83/120    avg_loss:0.142, val_acc:0.969]
Epoch [84/120    avg_loss:0.137, val_acc:0.979]
Epoch [85/120    avg_loss:0.133, val_acc:0.979]
Epoch [86/120    avg_loss:0.167, val_acc:0.973]
Epoch [87/120    avg_loss:0.198, val_acc:0.963]
Epoch [88/120    avg_loss:0.255, val_acc:0.951]
Epoch [89/120    avg_loss:0.212, val_acc:0.977]
Epoch [90/120    avg_loss:0.186, val_acc:0.971]
Epoch [91/120    avg_loss:0.133, val_acc:0.963]
Epoch [92/120    avg_loss:0.162, val_acc:0.975]
Epoch [93/120    avg_loss:0.174, val_acc:0.977]
Epoch [94/120    avg_loss:0.183, val_acc:0.971]
Epoch [95/120    avg_loss:0.141, val_acc:0.979]
Epoch [96/120    avg_loss:0.122, val_acc:0.980]
Epoch [97/120    avg_loss:0.104, val_acc:0.984]
Epoch [98/120    avg_loss:0.104, val_acc:0.988]
Epoch [99/120    avg_loss:0.110, val_acc:0.986]
Epoch [100/120    avg_loss:0.121, val_acc:0.986]
Epoch [101/120    avg_loss:0.105, val_acc:0.984]
Epoch [102/120    avg_loss:0.094, val_acc:0.982]
Epoch [103/120    avg_loss:0.106, val_acc:0.988]
Epoch [104/120    avg_loss:0.098, val_acc:0.984]
Epoch [105/120    avg_loss:0.096, val_acc:0.986]
Epoch [106/120    avg_loss:0.094, val_acc:0.986]
Epoch [107/120    avg_loss:0.090, val_acc:0.986]
Epoch [108/120    avg_loss:0.095, val_acc:0.984]
Epoch [109/120    avg_loss:0.094, val_acc:0.982]
Epoch [110/120    avg_loss:0.093, val_acc:0.986]
Epoch [111/120    avg_loss:0.100, val_acc:0.986]
Epoch [112/120    avg_loss:0.091, val_acc:0.986]
Epoch [113/120    avg_loss:0.090, val_acc:0.984]
Epoch [114/120    avg_loss:0.091, val_acc:0.986]
Epoch [115/120    avg_loss:0.088, val_acc:0.986]
Epoch [116/120    avg_loss:0.088, val_acc:0.986]
Epoch [117/120    avg_loss:0.083, val_acc:0.986]
Epoch [118/120    avg_loss:0.082, val_acc:0.986]
Epoch [119/120    avg_loss:0.092, val_acc:0.986]
Epoch [120/120    avg_loss:0.080, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0   6   0   0   0   0   5   0]
 [  0   0   0 218   9   0   0   0   1   2   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  32 113   0   0   0   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.97441364605544

F1 scores:
[       nan 0.99636364 0.93273543 0.97321429 0.89026915 0.83703704
 0.98771499 0.85714286 0.99614891 0.9978678  1.         0.99734043
 0.99012075 1.        ]

Kappa:
0.9774402447501797
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38078bddd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.607, val_acc:0.203]
Epoch [2/120    avg_loss:2.572, val_acc:0.215]
Epoch [3/120    avg_loss:2.534, val_acc:0.340]
Epoch [4/120    avg_loss:2.503, val_acc:0.398]
Epoch [5/120    avg_loss:2.470, val_acc:0.402]
Epoch [6/120    avg_loss:2.441, val_acc:0.408]
Epoch [7/120    avg_loss:2.417, val_acc:0.428]
Epoch [8/120    avg_loss:2.387, val_acc:0.469]
Epoch [9/120    avg_loss:2.360, val_acc:0.484]
Epoch [10/120    avg_loss:2.337, val_acc:0.514]
Epoch [11/120    avg_loss:2.290, val_acc:0.549]
Epoch [12/120    avg_loss:2.253, val_acc:0.594]
Epoch [13/120    avg_loss:2.213, val_acc:0.643]
Epoch [14/120    avg_loss:2.158, val_acc:0.662]
Epoch [15/120    avg_loss:2.098, val_acc:0.664]
Epoch [16/120    avg_loss:2.041, val_acc:0.676]
Epoch [17/120    avg_loss:1.981, val_acc:0.686]
Epoch [18/120    avg_loss:1.913, val_acc:0.689]
Epoch [19/120    avg_loss:1.869, val_acc:0.695]
Epoch [20/120    avg_loss:1.801, val_acc:0.707]
Epoch [21/120    avg_loss:1.730, val_acc:0.713]
Epoch [22/120    avg_loss:1.658, val_acc:0.715]
Epoch [23/120    avg_loss:1.595, val_acc:0.725]
Epoch [24/120    avg_loss:1.558, val_acc:0.715]
Epoch [25/120    avg_loss:1.494, val_acc:0.725]
Epoch [26/120    avg_loss:1.413, val_acc:0.740]
Epoch [27/120    avg_loss:1.372, val_acc:0.732]
Epoch [28/120    avg_loss:1.292, val_acc:0.777]
Epoch [29/120    avg_loss:1.230, val_acc:0.789]
Epoch [30/120    avg_loss:1.180, val_acc:0.791]
Epoch [31/120    avg_loss:1.145, val_acc:0.846]
Epoch [32/120    avg_loss:1.100, val_acc:0.824]
Epoch [33/120    avg_loss:1.030, val_acc:0.852]
Epoch [34/120    avg_loss:0.982, val_acc:0.842]
Epoch [35/120    avg_loss:0.884, val_acc:0.891]
Epoch [36/120    avg_loss:0.828, val_acc:0.885]
Epoch [37/120    avg_loss:0.877, val_acc:0.873]
Epoch [38/120    avg_loss:0.786, val_acc:0.895]
Epoch [39/120    avg_loss:0.768, val_acc:0.885]
Epoch [40/120    avg_loss:0.686, val_acc:0.914]
Epoch [41/120    avg_loss:0.671, val_acc:0.898]
Epoch [42/120    avg_loss:0.612, val_acc:0.926]
Epoch [43/120    avg_loss:0.613, val_acc:0.916]
Epoch [44/120    avg_loss:0.569, val_acc:0.898]
Epoch [45/120    avg_loss:0.580, val_acc:0.922]
Epoch [46/120    avg_loss:0.568, val_acc:0.912]
Epoch [47/120    avg_loss:0.538, val_acc:0.906]
Epoch [48/120    avg_loss:0.463, val_acc:0.912]
Epoch [49/120    avg_loss:0.473, val_acc:0.936]
Epoch [50/120    avg_loss:0.483, val_acc:0.936]
Epoch [51/120    avg_loss:0.446, val_acc:0.918]
Epoch [52/120    avg_loss:0.493, val_acc:0.918]
Epoch [53/120    avg_loss:0.415, val_acc:0.938]
Epoch [54/120    avg_loss:0.427, val_acc:0.893]
Epoch [55/120    avg_loss:0.411, val_acc:0.932]
Epoch [56/120    avg_loss:0.429, val_acc:0.934]
Epoch [57/120    avg_loss:0.373, val_acc:0.945]
Epoch [58/120    avg_loss:0.383, val_acc:0.939]
Epoch [59/120    avg_loss:0.377, val_acc:0.938]
Epoch [60/120    avg_loss:0.357, val_acc:0.936]
Epoch [61/120    avg_loss:0.305, val_acc:0.941]
Epoch [62/120    avg_loss:0.299, val_acc:0.943]
Epoch [63/120    avg_loss:0.289, val_acc:0.941]
Epoch [64/120    avg_loss:0.319, val_acc:0.928]
Epoch [65/120    avg_loss:0.330, val_acc:0.941]
Epoch [66/120    avg_loss:0.270, val_acc:0.945]
Epoch [67/120    avg_loss:0.272, val_acc:0.926]
Epoch [68/120    avg_loss:0.281, val_acc:0.943]
Epoch [69/120    avg_loss:0.297, val_acc:0.957]
Epoch [70/120    avg_loss:0.290, val_acc:0.953]
Epoch [71/120    avg_loss:0.294, val_acc:0.957]
Epoch [72/120    avg_loss:0.281, val_acc:0.959]
Epoch [73/120    avg_loss:0.212, val_acc:0.908]
Epoch [74/120    avg_loss:0.281, val_acc:0.957]
Epoch [75/120    avg_loss:0.213, val_acc:0.963]
Epoch [76/120    avg_loss:0.235, val_acc:0.957]
Epoch [77/120    avg_loss:0.222, val_acc:0.959]
Epoch [78/120    avg_loss:0.195, val_acc:0.947]
Epoch [79/120    avg_loss:0.230, val_acc:0.957]
Epoch [80/120    avg_loss:0.224, val_acc:0.961]
Epoch [81/120    avg_loss:0.247, val_acc:0.945]
Epoch [82/120    avg_loss:0.233, val_acc:0.973]
Epoch [83/120    avg_loss:0.212, val_acc:0.947]
Epoch [84/120    avg_loss:0.173, val_acc:0.951]
Epoch [85/120    avg_loss:0.186, val_acc:0.973]
Epoch [86/120    avg_loss:0.153, val_acc:0.887]
Epoch [87/120    avg_loss:0.238, val_acc:0.939]
Epoch [88/120    avg_loss:0.202, val_acc:0.955]
Epoch [89/120    avg_loss:0.164, val_acc:0.955]
Epoch [90/120    avg_loss:0.156, val_acc:0.967]
Epoch [91/120    avg_loss:0.170, val_acc:0.971]
Epoch [92/120    avg_loss:0.169, val_acc:0.959]
Epoch [93/120    avg_loss:0.298, val_acc:0.953]
Epoch [94/120    avg_loss:0.173, val_acc:0.947]
Epoch [95/120    avg_loss:0.160, val_acc:0.971]
Epoch [96/120    avg_loss:0.161, val_acc:0.971]
Epoch [97/120    avg_loss:0.126, val_acc:0.971]
Epoch [98/120    avg_loss:0.149, val_acc:0.977]
Epoch [99/120    avg_loss:0.138, val_acc:0.963]
Epoch [100/120    avg_loss:0.125, val_acc:0.963]
Epoch [101/120    avg_loss:0.123, val_acc:0.961]
Epoch [102/120    avg_loss:0.123, val_acc:0.975]
Epoch [103/120    avg_loss:0.129, val_acc:0.971]
Epoch [104/120    avg_loss:0.097, val_acc:0.971]
Epoch [105/120    avg_loss:0.114, val_acc:0.971]
Epoch [106/120    avg_loss:0.087, val_acc:0.971]
Epoch [107/120    avg_loss:0.083, val_acc:0.973]
Epoch [108/120    avg_loss:0.093, val_acc:0.973]
Epoch [109/120    avg_loss:0.100, val_acc:0.973]
Epoch [110/120    avg_loss:0.100, val_acc:0.973]
Epoch [111/120    avg_loss:0.091, val_acc:0.977]
Epoch [112/120    avg_loss:0.081, val_acc:0.977]
Epoch [113/120    avg_loss:0.100, val_acc:0.975]
Epoch [114/120    avg_loss:0.081, val_acc:0.963]
Epoch [115/120    avg_loss:0.173, val_acc:0.963]
Epoch [116/120    avg_loss:0.157, val_acc:0.959]
Epoch [117/120    avg_loss:0.129, val_acc:0.930]
Epoch [118/120    avg_loss:0.108, val_acc:0.975]
Epoch [119/120    avg_loss:0.092, val_acc:0.965]
Epoch [120/120    avg_loss:0.076, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 208   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   4 210  12   0   0   0   1   3   0   0   0   0]
 [  0   0   0   0 227   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  61  84   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.63326226012794

F1 scores:
[       nan 0.99927061 0.92650334 0.95454545 0.86148008 0.73362445
 1.         0.85082873 0.99742268 0.99680511 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9736412635667062
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7effe7198f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.611, val_acc:0.211]
Epoch [2/120    avg_loss:2.569, val_acc:0.319]
Epoch [3/120    avg_loss:2.527, val_acc:0.323]
Epoch [4/120    avg_loss:2.482, val_acc:0.365]
Epoch [5/120    avg_loss:2.447, val_acc:0.421]
Epoch [6/120    avg_loss:2.419, val_acc:0.440]
Epoch [7/120    avg_loss:2.385, val_acc:0.419]
Epoch [8/120    avg_loss:2.351, val_acc:0.412]
Epoch [9/120    avg_loss:2.336, val_acc:0.333]
Epoch [10/120    avg_loss:2.304, val_acc:0.331]
Epoch [11/120    avg_loss:2.283, val_acc:0.362]
Epoch [12/120    avg_loss:2.244, val_acc:0.362]
Epoch [13/120    avg_loss:2.207, val_acc:0.379]
Epoch [14/120    avg_loss:2.199, val_acc:0.381]
Epoch [15/120    avg_loss:2.141, val_acc:0.394]
Epoch [16/120    avg_loss:2.107, val_acc:0.415]
Epoch [17/120    avg_loss:2.062, val_acc:0.440]
Epoch [18/120    avg_loss:2.033, val_acc:0.479]
Epoch [19/120    avg_loss:1.985, val_acc:0.537]
Epoch [20/120    avg_loss:1.971, val_acc:0.525]
Epoch [21/120    avg_loss:1.924, val_acc:0.542]
Epoch [22/120    avg_loss:1.852, val_acc:0.577]
Epoch [23/120    avg_loss:1.792, val_acc:0.621]
Epoch [24/120    avg_loss:1.759, val_acc:0.627]
Epoch [25/120    avg_loss:1.697, val_acc:0.625]
Epoch [26/120    avg_loss:1.675, val_acc:0.610]
Epoch [27/120    avg_loss:1.637, val_acc:0.683]
Epoch [28/120    avg_loss:1.592, val_acc:0.690]
Epoch [29/120    avg_loss:1.533, val_acc:0.694]
Epoch [30/120    avg_loss:1.431, val_acc:0.798]
Epoch [31/120    avg_loss:1.384, val_acc:0.792]
Epoch [32/120    avg_loss:1.346, val_acc:0.800]
Epoch [33/120    avg_loss:1.259, val_acc:0.783]
Epoch [34/120    avg_loss:1.256, val_acc:0.804]
Epoch [35/120    avg_loss:1.165, val_acc:0.815]
Epoch [36/120    avg_loss:1.140, val_acc:0.860]
Epoch [37/120    avg_loss:1.097, val_acc:0.863]
Epoch [38/120    avg_loss:0.994, val_acc:0.863]
Epoch [39/120    avg_loss:0.995, val_acc:0.867]
Epoch [40/120    avg_loss:0.916, val_acc:0.873]
Epoch [41/120    avg_loss:0.863, val_acc:0.892]
Epoch [42/120    avg_loss:0.812, val_acc:0.869]
Epoch [43/120    avg_loss:0.782, val_acc:0.894]
Epoch [44/120    avg_loss:0.725, val_acc:0.879]
Epoch [45/120    avg_loss:0.745, val_acc:0.867]
Epoch [46/120    avg_loss:0.698, val_acc:0.887]
Epoch [47/120    avg_loss:0.639, val_acc:0.904]
Epoch [48/120    avg_loss:0.614, val_acc:0.917]
Epoch [49/120    avg_loss:0.545, val_acc:0.890]
Epoch [50/120    avg_loss:0.530, val_acc:0.912]
Epoch [51/120    avg_loss:0.512, val_acc:0.877]
Epoch [52/120    avg_loss:0.529, val_acc:0.912]
Epoch [53/120    avg_loss:0.500, val_acc:0.883]
Epoch [54/120    avg_loss:0.504, val_acc:0.925]
Epoch [55/120    avg_loss:0.503, val_acc:0.912]
Epoch [56/120    avg_loss:0.473, val_acc:0.919]
Epoch [57/120    avg_loss:0.427, val_acc:0.915]
Epoch [58/120    avg_loss:0.403, val_acc:0.912]
Epoch [59/120    avg_loss:0.402, val_acc:0.917]
Epoch [60/120    avg_loss:0.403, val_acc:0.912]
Epoch [61/120    avg_loss:0.383, val_acc:0.929]
Epoch [62/120    avg_loss:0.363, val_acc:0.933]
Epoch [63/120    avg_loss:0.360, val_acc:0.948]
Epoch [64/120    avg_loss:0.362, val_acc:0.938]
Epoch [65/120    avg_loss:0.410, val_acc:0.910]
Epoch [66/120    avg_loss:0.439, val_acc:0.925]
Epoch [67/120    avg_loss:0.337, val_acc:0.935]
Epoch [68/120    avg_loss:0.311, val_acc:0.944]
Epoch [69/120    avg_loss:0.322, val_acc:0.910]
Epoch [70/120    avg_loss:0.310, val_acc:0.925]
Epoch [71/120    avg_loss:0.306, val_acc:0.938]
Epoch [72/120    avg_loss:0.303, val_acc:0.954]
Epoch [73/120    avg_loss:0.320, val_acc:0.938]
Epoch [74/120    avg_loss:0.302, val_acc:0.938]
Epoch [75/120    avg_loss:0.264, val_acc:0.940]
Epoch [76/120    avg_loss:0.283, val_acc:0.933]
Epoch [77/120    avg_loss:0.242, val_acc:0.952]
Epoch [78/120    avg_loss:0.239, val_acc:0.963]
Epoch [79/120    avg_loss:0.315, val_acc:0.938]
Epoch [80/120    avg_loss:0.313, val_acc:0.927]
Epoch [81/120    avg_loss:0.308, val_acc:0.963]
Epoch [82/120    avg_loss:0.239, val_acc:0.944]
Epoch [83/120    avg_loss:0.228, val_acc:0.956]
Epoch [84/120    avg_loss:0.235, val_acc:0.952]
Epoch [85/120    avg_loss:0.219, val_acc:0.969]
Epoch [86/120    avg_loss:0.201, val_acc:0.942]
Epoch [87/120    avg_loss:0.245, val_acc:0.942]
Epoch [88/120    avg_loss:0.189, val_acc:0.967]
Epoch [89/120    avg_loss:0.181, val_acc:0.954]
Epoch [90/120    avg_loss:0.155, val_acc:0.965]
Epoch [91/120    avg_loss:0.150, val_acc:0.954]
Epoch [92/120    avg_loss:0.167, val_acc:0.944]
Epoch [93/120    avg_loss:0.215, val_acc:0.954]
Epoch [94/120    avg_loss:0.177, val_acc:0.971]
Epoch [95/120    avg_loss:0.144, val_acc:0.954]
Epoch [96/120    avg_loss:0.166, val_acc:0.942]
Epoch [97/120    avg_loss:0.160, val_acc:0.948]
Epoch [98/120    avg_loss:0.204, val_acc:0.973]
Epoch [99/120    avg_loss:0.165, val_acc:0.954]
Epoch [100/120    avg_loss:0.192, val_acc:0.954]
Epoch [101/120    avg_loss:0.117, val_acc:0.979]
Epoch [102/120    avg_loss:0.127, val_acc:0.977]
Epoch [103/120    avg_loss:0.162, val_acc:0.958]
Epoch [104/120    avg_loss:0.164, val_acc:0.946]
Epoch [105/120    avg_loss:0.172, val_acc:0.979]
Epoch [106/120    avg_loss:0.131, val_acc:0.983]
Epoch [107/120    avg_loss:0.118, val_acc:0.971]
Epoch [108/120    avg_loss:0.129, val_acc:0.965]
Epoch [109/120    avg_loss:0.159, val_acc:0.981]
Epoch [110/120    avg_loss:0.163, val_acc:0.983]
Epoch [111/120    avg_loss:0.120, val_acc:0.958]
Epoch [112/120    avg_loss:0.112, val_acc:0.967]
Epoch [113/120    avg_loss:0.089, val_acc:0.979]
Epoch [114/120    avg_loss:0.123, val_acc:0.956]
Epoch [115/120    avg_loss:0.133, val_acc:0.973]
Epoch [116/120    avg_loss:0.126, val_acc:0.981]
Epoch [117/120    avg_loss:0.097, val_acc:0.973]
Epoch [118/120    avg_loss:0.110, val_acc:0.960]
Epoch [119/120    avg_loss:0.129, val_acc:0.975]
Epoch [120/120    avg_loss:0.093, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 211   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 224   4   0   0   2   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   1  12 132   0   0   0   0   0   0   0   0]
 [  0  45   0   0   0   0 161   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.86780383795309

F1 scores:
[       nan 0.96683133 0.96788991 0.98461538 0.91517857 0.88294314
 0.8773842  0.92631579 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9762360328423093
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc2d418e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.131]
Epoch [2/120    avg_loss:2.594, val_acc:0.317]
Epoch [3/120    avg_loss:2.558, val_acc:0.325]
Epoch [4/120    avg_loss:2.527, val_acc:0.327]
Epoch [5/120    avg_loss:2.491, val_acc:0.358]
Epoch [6/120    avg_loss:2.457, val_acc:0.365]
Epoch [7/120    avg_loss:2.424, val_acc:0.375]
Epoch [8/120    avg_loss:2.383, val_acc:0.377]
Epoch [9/120    avg_loss:2.354, val_acc:0.383]
Epoch [10/120    avg_loss:2.317, val_acc:0.360]
Epoch [11/120    avg_loss:2.278, val_acc:0.352]
Epoch [12/120    avg_loss:2.230, val_acc:0.369]
Epoch [13/120    avg_loss:2.188, val_acc:0.358]
Epoch [14/120    avg_loss:2.137, val_acc:0.400]
Epoch [15/120    avg_loss:2.088, val_acc:0.465]
Epoch [16/120    avg_loss:2.046, val_acc:0.502]
Epoch [17/120    avg_loss:2.000, val_acc:0.537]
Epoch [18/120    avg_loss:1.948, val_acc:0.583]
Epoch [19/120    avg_loss:1.883, val_acc:0.606]
Epoch [20/120    avg_loss:1.868, val_acc:0.604]
Epoch [21/120    avg_loss:1.773, val_acc:0.629]
Epoch [22/120    avg_loss:1.712, val_acc:0.623]
Epoch [23/120    avg_loss:1.667, val_acc:0.629]
Epoch [24/120    avg_loss:1.580, val_acc:0.646]
Epoch [25/120    avg_loss:1.555, val_acc:0.650]
Epoch [26/120    avg_loss:1.495, val_acc:0.671]
Epoch [27/120    avg_loss:1.450, val_acc:0.646]
Epoch [28/120    avg_loss:1.376, val_acc:0.675]
Epoch [29/120    avg_loss:1.286, val_acc:0.694]
Epoch [30/120    avg_loss:1.225, val_acc:0.729]
Epoch [31/120    avg_loss:1.163, val_acc:0.719]
Epoch [32/120    avg_loss:1.126, val_acc:0.696]
Epoch [33/120    avg_loss:1.068, val_acc:0.760]
Epoch [34/120    avg_loss:1.006, val_acc:0.781]
Epoch [35/120    avg_loss:0.950, val_acc:0.777]
Epoch [36/120    avg_loss:0.924, val_acc:0.887]
Epoch [37/120    avg_loss:0.849, val_acc:0.846]
Epoch [38/120    avg_loss:0.809, val_acc:0.904]
Epoch [39/120    avg_loss:0.786, val_acc:0.877]
Epoch [40/120    avg_loss:0.770, val_acc:0.844]
Epoch [41/120    avg_loss:0.736, val_acc:0.796]
Epoch [42/120    avg_loss:0.748, val_acc:0.781]
Epoch [43/120    avg_loss:0.692, val_acc:0.898]
Epoch [44/120    avg_loss:0.606, val_acc:0.908]
Epoch [45/120    avg_loss:0.611, val_acc:0.885]
Epoch [46/120    avg_loss:0.604, val_acc:0.877]
Epoch [47/120    avg_loss:0.542, val_acc:0.910]
Epoch [48/120    avg_loss:0.525, val_acc:0.935]
Epoch [49/120    avg_loss:0.492, val_acc:0.896]
Epoch [50/120    avg_loss:0.532, val_acc:0.904]
Epoch [51/120    avg_loss:0.469, val_acc:0.929]
Epoch [52/120    avg_loss:0.447, val_acc:0.887]
Epoch [53/120    avg_loss:0.439, val_acc:0.919]
Epoch [54/120    avg_loss:0.430, val_acc:0.935]
Epoch [55/120    avg_loss:0.368, val_acc:0.940]
Epoch [56/120    avg_loss:0.367, val_acc:0.925]
Epoch [57/120    avg_loss:0.340, val_acc:0.946]
Epoch [58/120    avg_loss:0.305, val_acc:0.954]
Epoch [59/120    avg_loss:0.322, val_acc:0.910]
Epoch [60/120    avg_loss:0.387, val_acc:0.929]
Epoch [61/120    avg_loss:0.356, val_acc:0.956]
Epoch [62/120    avg_loss:0.297, val_acc:0.960]
Epoch [63/120    avg_loss:0.312, val_acc:0.935]
Epoch [64/120    avg_loss:0.317, val_acc:0.935]
Epoch [65/120    avg_loss:0.272, val_acc:0.960]
Epoch [66/120    avg_loss:0.252, val_acc:0.950]
Epoch [67/120    avg_loss:0.270, val_acc:0.942]
Epoch [68/120    avg_loss:0.309, val_acc:0.929]
Epoch [69/120    avg_loss:0.350, val_acc:0.944]
Epoch [70/120    avg_loss:0.281, val_acc:0.960]
Epoch [71/120    avg_loss:0.262, val_acc:0.940]
Epoch [72/120    avg_loss:0.256, val_acc:0.958]
Epoch [73/120    avg_loss:0.234, val_acc:0.956]
Epoch [74/120    avg_loss:0.199, val_acc:0.967]
Epoch [75/120    avg_loss:0.240, val_acc:0.935]
Epoch [76/120    avg_loss:0.218, val_acc:0.958]
Epoch [77/120    avg_loss:0.243, val_acc:0.967]
Epoch [78/120    avg_loss:0.213, val_acc:0.967]
Epoch [79/120    avg_loss:0.177, val_acc:0.960]
Epoch [80/120    avg_loss:0.258, val_acc:0.971]
Epoch [81/120    avg_loss:0.221, val_acc:0.954]
Epoch [82/120    avg_loss:0.384, val_acc:0.921]
Epoch [83/120    avg_loss:0.288, val_acc:0.948]
Epoch [84/120    avg_loss:0.224, val_acc:0.963]
Epoch [85/120    avg_loss:0.202, val_acc:0.960]
Epoch [86/120    avg_loss:0.167, val_acc:0.969]
Epoch [87/120    avg_loss:0.139, val_acc:0.977]
Epoch [88/120    avg_loss:0.166, val_acc:0.983]
Epoch [89/120    avg_loss:0.139, val_acc:0.967]
Epoch [90/120    avg_loss:0.174, val_acc:0.954]
Epoch [91/120    avg_loss:0.187, val_acc:0.967]
Epoch [92/120    avg_loss:0.171, val_acc:0.969]
Epoch [93/120    avg_loss:0.155, val_acc:0.963]
Epoch [94/120    avg_loss:0.135, val_acc:0.977]
Epoch [95/120    avg_loss:0.131, val_acc:0.975]
Epoch [96/120    avg_loss:0.133, val_acc:0.973]
Epoch [97/120    avg_loss:0.132, val_acc:0.967]
Epoch [98/120    avg_loss:0.117, val_acc:0.973]
Epoch [99/120    avg_loss:0.140, val_acc:0.975]
Epoch [100/120    avg_loss:0.155, val_acc:0.981]
Epoch [101/120    avg_loss:0.160, val_acc:0.933]
Epoch [102/120    avg_loss:0.174, val_acc:0.967]
Epoch [103/120    avg_loss:0.097, val_acc:0.981]
Epoch [104/120    avg_loss:0.112, val_acc:0.981]
Epoch [105/120    avg_loss:0.094, val_acc:0.977]
Epoch [106/120    avg_loss:0.082, val_acc:0.979]
Epoch [107/120    avg_loss:0.082, val_acc:0.977]
Epoch [108/120    avg_loss:0.084, val_acc:0.979]
Epoch [109/120    avg_loss:0.084, val_acc:0.979]
Epoch [110/120    avg_loss:0.080, val_acc:0.981]
Epoch [111/120    avg_loss:0.090, val_acc:0.983]
Epoch [112/120    avg_loss:0.078, val_acc:0.983]
Epoch [113/120    avg_loss:0.088, val_acc:0.979]
Epoch [114/120    avg_loss:0.084, val_acc:0.981]
Epoch [115/120    avg_loss:0.073, val_acc:0.985]
Epoch [116/120    avg_loss:0.080, val_acc:0.983]
Epoch [117/120    avg_loss:0.069, val_acc:0.983]
Epoch [118/120    avg_loss:0.074, val_acc:0.983]
Epoch [119/120    avg_loss:0.085, val_acc:0.981]
Epoch [120/120    avg_loss:0.075, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 225   0   0   0   1   0   1   2   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 1.         0.96263736 0.98901099 0.91479821 0.87625418
 0.99756691 0.9017341  1.         0.99893276 0.99726027 1.
 1.         1.        ]

Kappa:
0.9859933300896941
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f437739bef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.204]
Epoch [2/120    avg_loss:2.589, val_acc:0.258]
Epoch [3/120    avg_loss:2.564, val_acc:0.275]
Epoch [4/120    avg_loss:2.541, val_acc:0.285]
Epoch [5/120    avg_loss:2.517, val_acc:0.287]
Epoch [6/120    avg_loss:2.484, val_acc:0.287]
Epoch [7/120    avg_loss:2.457, val_acc:0.287]
Epoch [8/120    avg_loss:2.427, val_acc:0.285]
Epoch [9/120    avg_loss:2.396, val_acc:0.285]
Epoch [10/120    avg_loss:2.362, val_acc:0.287]
Epoch [11/120    avg_loss:2.340, val_acc:0.302]
Epoch [12/120    avg_loss:2.307, val_acc:0.321]
Epoch [13/120    avg_loss:2.279, val_acc:0.333]
Epoch [14/120    avg_loss:2.251, val_acc:0.377]
Epoch [15/120    avg_loss:2.226, val_acc:0.396]
Epoch [16/120    avg_loss:2.177, val_acc:0.406]
Epoch [17/120    avg_loss:2.132, val_acc:0.425]
Epoch [18/120    avg_loss:2.089, val_acc:0.412]
Epoch [19/120    avg_loss:2.059, val_acc:0.450]
Epoch [20/120    avg_loss:2.017, val_acc:0.492]
Epoch [21/120    avg_loss:1.953, val_acc:0.533]
Epoch [22/120    avg_loss:1.891, val_acc:0.562]
Epoch [23/120    avg_loss:1.825, val_acc:0.592]
Epoch [24/120    avg_loss:1.774, val_acc:0.625]
Epoch [25/120    avg_loss:1.738, val_acc:0.631]
Epoch [26/120    avg_loss:1.681, val_acc:0.642]
Epoch [27/120    avg_loss:1.627, val_acc:0.654]
Epoch [28/120    avg_loss:1.577, val_acc:0.642]
Epoch [29/120    avg_loss:1.507, val_acc:0.654]
Epoch [30/120    avg_loss:1.429, val_acc:0.669]
Epoch [31/120    avg_loss:1.356, val_acc:0.696]
Epoch [32/120    avg_loss:1.293, val_acc:0.706]
Epoch [33/120    avg_loss:1.266, val_acc:0.723]
Epoch [34/120    avg_loss:1.183, val_acc:0.752]
Epoch [35/120    avg_loss:1.112, val_acc:0.781]
Epoch [36/120    avg_loss:1.029, val_acc:0.854]
Epoch [37/120    avg_loss:0.983, val_acc:0.867]
Epoch [38/120    avg_loss:0.943, val_acc:0.887]
Epoch [39/120    avg_loss:0.865, val_acc:0.910]
Epoch [40/120    avg_loss:0.784, val_acc:0.921]
Epoch [41/120    avg_loss:0.771, val_acc:0.887]
Epoch [42/120    avg_loss:0.791, val_acc:0.904]
Epoch [43/120    avg_loss:0.727, val_acc:0.919]
Epoch [44/120    avg_loss:0.686, val_acc:0.917]
Epoch [45/120    avg_loss:0.639, val_acc:0.915]
Epoch [46/120    avg_loss:0.583, val_acc:0.912]
Epoch [47/120    avg_loss:0.594, val_acc:0.915]
Epoch [48/120    avg_loss:0.579, val_acc:0.923]
Epoch [49/120    avg_loss:0.517, val_acc:0.925]
Epoch [50/120    avg_loss:0.489, val_acc:0.942]
Epoch [51/120    avg_loss:0.526, val_acc:0.875]
Epoch [52/120    avg_loss:0.527, val_acc:0.923]
Epoch [53/120    avg_loss:0.481, val_acc:0.908]
Epoch [54/120    avg_loss:0.439, val_acc:0.938]
Epoch [55/120    avg_loss:0.399, val_acc:0.929]
Epoch [56/120    avg_loss:0.383, val_acc:0.908]
Epoch [57/120    avg_loss:0.467, val_acc:0.921]
Epoch [58/120    avg_loss:0.402, val_acc:0.933]
Epoch [59/120    avg_loss:0.381, val_acc:0.933]
Epoch [60/120    avg_loss:0.354, val_acc:0.946]
Epoch [61/120    avg_loss:0.357, val_acc:0.923]
Epoch [62/120    avg_loss:0.359, val_acc:0.950]
Epoch [63/120    avg_loss:0.292, val_acc:0.944]
Epoch [64/120    avg_loss:0.292, val_acc:0.967]
Epoch [65/120    avg_loss:0.305, val_acc:0.944]
Epoch [66/120    avg_loss:0.306, val_acc:0.971]
Epoch [67/120    avg_loss:0.252, val_acc:0.969]
Epoch [68/120    avg_loss:0.270, val_acc:0.954]
Epoch [69/120    avg_loss:0.252, val_acc:0.963]
Epoch [70/120    avg_loss:0.240, val_acc:0.977]
Epoch [71/120    avg_loss:0.217, val_acc:0.965]
Epoch [72/120    avg_loss:0.219, val_acc:0.956]
Epoch [73/120    avg_loss:0.227, val_acc:0.952]
Epoch [74/120    avg_loss:0.200, val_acc:0.960]
Epoch [75/120    avg_loss:0.242, val_acc:0.971]
Epoch [76/120    avg_loss:0.242, val_acc:0.952]
Epoch [77/120    avg_loss:0.253, val_acc:0.958]
Epoch [78/120    avg_loss:0.233, val_acc:0.954]
Epoch [79/120    avg_loss:0.264, val_acc:0.950]
Epoch [80/120    avg_loss:0.252, val_acc:0.952]
Epoch [81/120    avg_loss:0.223, val_acc:0.954]
Epoch [82/120    avg_loss:0.223, val_acc:0.963]
Epoch [83/120    avg_loss:0.183, val_acc:0.946]
Epoch [84/120    avg_loss:0.161, val_acc:0.967]
Epoch [85/120    avg_loss:0.132, val_acc:0.981]
Epoch [86/120    avg_loss:0.137, val_acc:0.981]
Epoch [87/120    avg_loss:0.128, val_acc:0.983]
Epoch [88/120    avg_loss:0.140, val_acc:0.979]
Epoch [89/120    avg_loss:0.130, val_acc:0.977]
Epoch [90/120    avg_loss:0.122, val_acc:0.977]
Epoch [91/120    avg_loss:0.141, val_acc:0.977]
Epoch [92/120    avg_loss:0.124, val_acc:0.983]
Epoch [93/120    avg_loss:0.125, val_acc:0.979]
Epoch [94/120    avg_loss:0.131, val_acc:0.979]
Epoch [95/120    avg_loss:0.116, val_acc:0.979]
Epoch [96/120    avg_loss:0.114, val_acc:0.981]
Epoch [97/120    avg_loss:0.131, val_acc:0.977]
Epoch [98/120    avg_loss:0.102, val_acc:0.979]
Epoch [99/120    avg_loss:0.124, val_acc:0.981]
Epoch [100/120    avg_loss:0.128, val_acc:0.981]
Epoch [101/120    avg_loss:0.107, val_acc:0.979]
Epoch [102/120    avg_loss:0.113, val_acc:0.975]
Epoch [103/120    avg_loss:0.116, val_acc:0.988]
Epoch [104/120    avg_loss:0.103, val_acc:0.985]
Epoch [105/120    avg_loss:0.098, val_acc:0.988]
Epoch [106/120    avg_loss:0.125, val_acc:0.990]
Epoch [107/120    avg_loss:0.107, val_acc:0.990]
Epoch [108/120    avg_loss:0.114, val_acc:0.985]
Epoch [109/120    avg_loss:0.102, val_acc:0.988]
Epoch [110/120    avg_loss:0.108, val_acc:0.988]
Epoch [111/120    avg_loss:0.112, val_acc:0.988]
Epoch [112/120    avg_loss:0.102, val_acc:0.990]
Epoch [113/120    avg_loss:0.116, val_acc:0.992]
Epoch [114/120    avg_loss:0.105, val_acc:0.990]
Epoch [115/120    avg_loss:0.112, val_acc:0.985]
Epoch [116/120    avg_loss:0.106, val_acc:0.990]
Epoch [117/120    avg_loss:0.106, val_acc:0.988]
Epoch [118/120    avg_loss:0.103, val_acc:0.990]
Epoch [119/120    avg_loss:0.111, val_acc:0.988]
Epoch [120/120    avg_loss:0.093, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 223   5   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 1.         0.95842451 0.98454746 0.92170022 0.90066225
 1.         0.89411765 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9869430960943584
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1734e30e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.031]
Epoch [2/120    avg_loss:2.606, val_acc:0.083]
Epoch [3/120    avg_loss:2.575, val_acc:0.083]
Epoch [4/120    avg_loss:2.543, val_acc:0.140]
Epoch [5/120    avg_loss:2.508, val_acc:0.242]
Epoch [6/120    avg_loss:2.477, val_acc:0.263]
Epoch [7/120    avg_loss:2.435, val_acc:0.275]
Epoch [8/120    avg_loss:2.404, val_acc:0.292]
Epoch [9/120    avg_loss:2.372, val_acc:0.300]
Epoch [10/120    avg_loss:2.334, val_acc:0.440]
Epoch [11/120    avg_loss:2.308, val_acc:0.450]
Epoch [12/120    avg_loss:2.263, val_acc:0.421]
Epoch [13/120    avg_loss:2.225, val_acc:0.490]
Epoch [14/120    avg_loss:2.182, val_acc:0.517]
Epoch [15/120    avg_loss:2.139, val_acc:0.562]
Epoch [16/120    avg_loss:2.090, val_acc:0.548]
Epoch [17/120    avg_loss:2.037, val_acc:0.590]
Epoch [18/120    avg_loss:1.987, val_acc:0.700]
Epoch [19/120    avg_loss:1.916, val_acc:0.750]
Epoch [20/120    avg_loss:1.872, val_acc:0.765]
Epoch [21/120    avg_loss:1.778, val_acc:0.765]
Epoch [22/120    avg_loss:1.725, val_acc:0.790]
Epoch [23/120    avg_loss:1.671, val_acc:0.787]
Epoch [24/120    avg_loss:1.616, val_acc:0.848]
Epoch [25/120    avg_loss:1.545, val_acc:0.825]
Epoch [26/120    avg_loss:1.456, val_acc:0.846]
Epoch [27/120    avg_loss:1.365, val_acc:0.842]
Epoch [28/120    avg_loss:1.335, val_acc:0.856]
Epoch [29/120    avg_loss:1.259, val_acc:0.860]
Epoch [30/120    avg_loss:1.179, val_acc:0.852]
Epoch [31/120    avg_loss:1.093, val_acc:0.871]
Epoch [32/120    avg_loss:1.050, val_acc:0.860]
Epoch [33/120    avg_loss:0.954, val_acc:0.885]
Epoch [34/120    avg_loss:0.886, val_acc:0.917]
Epoch [35/120    avg_loss:0.868, val_acc:0.887]
Epoch [36/120    avg_loss:0.780, val_acc:0.906]
Epoch [37/120    avg_loss:0.782, val_acc:0.910]
Epoch [38/120    avg_loss:0.724, val_acc:0.912]
Epoch [39/120    avg_loss:0.703, val_acc:0.908]
Epoch [40/120    avg_loss:0.679, val_acc:0.896]
Epoch [41/120    avg_loss:0.610, val_acc:0.919]
Epoch [42/120    avg_loss:0.584, val_acc:0.910]
Epoch [43/120    avg_loss:0.508, val_acc:0.919]
Epoch [44/120    avg_loss:0.566, val_acc:0.923]
Epoch [45/120    avg_loss:0.473, val_acc:0.923]
Epoch [46/120    avg_loss:0.482, val_acc:0.919]
Epoch [47/120    avg_loss:0.481, val_acc:0.910]
Epoch [48/120    avg_loss:0.447, val_acc:0.908]
Epoch [49/120    avg_loss:0.470, val_acc:0.912]
Epoch [50/120    avg_loss:0.462, val_acc:0.923]
Epoch [51/120    avg_loss:0.473, val_acc:0.877]
Epoch [52/120    avg_loss:0.420, val_acc:0.929]
Epoch [53/120    avg_loss:0.432, val_acc:0.929]
Epoch [54/120    avg_loss:0.389, val_acc:0.927]
Epoch [55/120    avg_loss:0.387, val_acc:0.948]
Epoch [56/120    avg_loss:0.332, val_acc:0.921]
Epoch [57/120    avg_loss:0.338, val_acc:0.935]
Epoch [58/120    avg_loss:0.317, val_acc:0.956]
Epoch [59/120    avg_loss:0.313, val_acc:0.946]
Epoch [60/120    avg_loss:0.293, val_acc:0.942]
Epoch [61/120    avg_loss:0.369, val_acc:0.940]
Epoch [62/120    avg_loss:0.349, val_acc:0.912]
Epoch [63/120    avg_loss:0.371, val_acc:0.927]
Epoch [64/120    avg_loss:0.279, val_acc:0.952]
Epoch [65/120    avg_loss:0.276, val_acc:0.938]
Epoch [66/120    avg_loss:0.348, val_acc:0.948]
Epoch [67/120    avg_loss:0.279, val_acc:0.956]
Epoch [68/120    avg_loss:0.238, val_acc:0.946]
Epoch [69/120    avg_loss:0.267, val_acc:0.950]
Epoch [70/120    avg_loss:0.192, val_acc:0.960]
Epoch [71/120    avg_loss:0.215, val_acc:0.942]
Epoch [72/120    avg_loss:0.226, val_acc:0.933]
Epoch [73/120    avg_loss:0.188, val_acc:0.933]
Epoch [74/120    avg_loss:0.208, val_acc:0.942]
Epoch [75/120    avg_loss:0.261, val_acc:0.942]
Epoch [76/120    avg_loss:0.270, val_acc:0.938]
Epoch [77/120    avg_loss:0.251, val_acc:0.938]
Epoch [78/120    avg_loss:0.219, val_acc:0.946]
Epoch [79/120    avg_loss:0.206, val_acc:0.944]
Epoch [80/120    avg_loss:0.198, val_acc:0.954]
Epoch [81/120    avg_loss:0.178, val_acc:0.967]
Epoch [82/120    avg_loss:0.140, val_acc:0.946]
Epoch [83/120    avg_loss:0.154, val_acc:0.938]
Epoch [84/120    avg_loss:0.133, val_acc:0.952]
Epoch [85/120    avg_loss:0.158, val_acc:0.963]
Epoch [86/120    avg_loss:0.207, val_acc:0.965]
Epoch [87/120    avg_loss:0.229, val_acc:0.946]
Epoch [88/120    avg_loss:0.206, val_acc:0.956]
Epoch [89/120    avg_loss:0.297, val_acc:0.927]
Epoch [90/120    avg_loss:0.227, val_acc:0.946]
Epoch [91/120    avg_loss:0.328, val_acc:0.942]
Epoch [92/120    avg_loss:0.245, val_acc:0.956]
Epoch [93/120    avg_loss:0.173, val_acc:0.952]
Epoch [94/120    avg_loss:0.144, val_acc:0.954]
Epoch [95/120    avg_loss:0.155, val_acc:0.969]
Epoch [96/120    avg_loss:0.127, val_acc:0.973]
Epoch [97/120    avg_loss:0.106, val_acc:0.971]
Epoch [98/120    avg_loss:0.109, val_acc:0.973]
Epoch [99/120    avg_loss:0.113, val_acc:0.973]
Epoch [100/120    avg_loss:0.108, val_acc:0.967]
Epoch [101/120    avg_loss:0.095, val_acc:0.971]
Epoch [102/120    avg_loss:0.102, val_acc:0.971]
Epoch [103/120    avg_loss:0.095, val_acc:0.969]
Epoch [104/120    avg_loss:0.098, val_acc:0.973]
Epoch [105/120    avg_loss:0.084, val_acc:0.973]
Epoch [106/120    avg_loss:0.105, val_acc:0.977]
Epoch [107/120    avg_loss:0.092, val_acc:0.971]
Epoch [108/120    avg_loss:0.091, val_acc:0.967]
Epoch [109/120    avg_loss:0.087, val_acc:0.973]
Epoch [110/120    avg_loss:0.086, val_acc:0.975]
Epoch [111/120    avg_loss:0.085, val_acc:0.975]
Epoch [112/120    avg_loss:0.083, val_acc:0.973]
Epoch [113/120    avg_loss:0.099, val_acc:0.975]
Epoch [114/120    avg_loss:0.069, val_acc:0.971]
Epoch [115/120    avg_loss:0.083, val_acc:0.975]
Epoch [116/120    avg_loss:0.087, val_acc:0.973]
Epoch [117/120    avg_loss:0.084, val_acc:0.971]
Epoch [118/120    avg_loss:0.081, val_acc:0.971]
Epoch [119/120    avg_loss:0.092, val_acc:0.975]
Epoch [120/120    avg_loss:0.082, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   1 218   5   0   0   0   6   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 1.         0.96460177 0.97321429 0.9082774  0.89542484
 0.99019608 0.91428571 0.99232737 1.         1.         1.
 1.         1.        ]

Kappa:
0.9850439698781798
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6fe1469dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.645, val_acc:0.100]
Epoch [2/120    avg_loss:2.629, val_acc:0.127]
Epoch [3/120    avg_loss:2.611, val_acc:0.315]
Epoch [4/120    avg_loss:2.590, val_acc:0.369]
Epoch [5/120    avg_loss:2.569, val_acc:0.396]
Epoch [6/120    avg_loss:2.551, val_acc:0.408]
Epoch [7/120    avg_loss:2.525, val_acc:0.415]
Epoch [8/120    avg_loss:2.500, val_acc:0.429]
Epoch [9/120    avg_loss:2.476, val_acc:0.456]
Epoch [10/120    avg_loss:2.452, val_acc:0.487]
Epoch [11/120    avg_loss:2.428, val_acc:0.477]
Epoch [12/120    avg_loss:2.394, val_acc:0.504]
Epoch [13/120    avg_loss:2.367, val_acc:0.527]
Epoch [14/120    avg_loss:2.339, val_acc:0.531]
Epoch [15/120    avg_loss:2.314, val_acc:0.544]
Epoch [16/120    avg_loss:2.278, val_acc:0.546]
Epoch [17/120    avg_loss:2.249, val_acc:0.579]
Epoch [18/120    avg_loss:2.218, val_acc:0.588]
Epoch [19/120    avg_loss:2.174, val_acc:0.592]
Epoch [20/120    avg_loss:2.127, val_acc:0.613]
Epoch [21/120    avg_loss:2.087, val_acc:0.613]
Epoch [22/120    avg_loss:2.053, val_acc:0.617]
Epoch [23/120    avg_loss:1.995, val_acc:0.621]
Epoch [24/120    avg_loss:1.924, val_acc:0.621]
Epoch [25/120    avg_loss:1.886, val_acc:0.662]
Epoch [26/120    avg_loss:1.847, val_acc:0.700]
Epoch [27/120    avg_loss:1.762, val_acc:0.694]
Epoch [28/120    avg_loss:1.716, val_acc:0.719]
Epoch [29/120    avg_loss:1.660, val_acc:0.738]
Epoch [30/120    avg_loss:1.582, val_acc:0.750]
Epoch [31/120    avg_loss:1.520, val_acc:0.731]
Epoch [32/120    avg_loss:1.475, val_acc:0.750]
Epoch [33/120    avg_loss:1.412, val_acc:0.735]
Epoch [34/120    avg_loss:1.319, val_acc:0.800]
Epoch [35/120    avg_loss:1.277, val_acc:0.854]
Epoch [36/120    avg_loss:1.208, val_acc:0.875]
Epoch [37/120    avg_loss:1.111, val_acc:0.873]
Epoch [38/120    avg_loss:1.073, val_acc:0.885]
Epoch [39/120    avg_loss:1.043, val_acc:0.848]
Epoch [40/120    avg_loss:0.988, val_acc:0.846]
Epoch [41/120    avg_loss:0.958, val_acc:0.877]
Epoch [42/120    avg_loss:0.885, val_acc:0.879]
Epoch [43/120    avg_loss:0.885, val_acc:0.865]
Epoch [44/120    avg_loss:0.833, val_acc:0.881]
Epoch [45/120    avg_loss:0.745, val_acc:0.881]
Epoch [46/120    avg_loss:0.680, val_acc:0.890]
Epoch [47/120    avg_loss:0.708, val_acc:0.898]
Epoch [48/120    avg_loss:0.684, val_acc:0.894]
Epoch [49/120    avg_loss:0.638, val_acc:0.885]
Epoch [50/120    avg_loss:0.646, val_acc:0.898]
Epoch [51/120    avg_loss:0.624, val_acc:0.887]
Epoch [52/120    avg_loss:0.610, val_acc:0.858]
Epoch [53/120    avg_loss:0.603, val_acc:0.887]
Epoch [54/120    avg_loss:0.568, val_acc:0.875]
Epoch [55/120    avg_loss:0.568, val_acc:0.883]
Epoch [56/120    avg_loss:0.523, val_acc:0.887]
Epoch [57/120    avg_loss:0.498, val_acc:0.904]
Epoch [58/120    avg_loss:0.461, val_acc:0.883]
Epoch [59/120    avg_loss:0.476, val_acc:0.892]
Epoch [60/120    avg_loss:0.404, val_acc:0.910]
Epoch [61/120    avg_loss:0.388, val_acc:0.938]
Epoch [62/120    avg_loss:0.389, val_acc:0.938]
Epoch [63/120    avg_loss:0.370, val_acc:0.912]
Epoch [64/120    avg_loss:0.364, val_acc:0.898]
Epoch [65/120    avg_loss:0.440, val_acc:0.935]
Epoch [66/120    avg_loss:0.344, val_acc:0.940]
Epoch [67/120    avg_loss:0.366, val_acc:0.917]
Epoch [68/120    avg_loss:0.340, val_acc:0.925]
Epoch [69/120    avg_loss:0.329, val_acc:0.908]
Epoch [70/120    avg_loss:0.353, val_acc:0.933]
Epoch [71/120    avg_loss:0.302, val_acc:0.904]
Epoch [72/120    avg_loss:0.307, val_acc:0.950]
Epoch [73/120    avg_loss:0.262, val_acc:0.956]
Epoch [74/120    avg_loss:0.276, val_acc:0.944]
Epoch [75/120    avg_loss:0.213, val_acc:0.942]
Epoch [76/120    avg_loss:0.229, val_acc:0.935]
Epoch [77/120    avg_loss:0.257, val_acc:0.946]
Epoch [78/120    avg_loss:0.264, val_acc:0.938]
Epoch [79/120    avg_loss:0.243, val_acc:0.940]
Epoch [80/120    avg_loss:0.234, val_acc:0.950]
Epoch [81/120    avg_loss:0.254, val_acc:0.929]
Epoch [82/120    avg_loss:0.282, val_acc:0.952]
Epoch [83/120    avg_loss:0.222, val_acc:0.956]
Epoch [84/120    avg_loss:0.208, val_acc:0.954]
Epoch [85/120    avg_loss:0.202, val_acc:0.969]
Epoch [86/120    avg_loss:0.183, val_acc:0.950]
Epoch [87/120    avg_loss:0.174, val_acc:0.956]
Epoch [88/120    avg_loss:0.146, val_acc:0.956]
Epoch [89/120    avg_loss:0.162, val_acc:0.952]
Epoch [90/120    avg_loss:0.174, val_acc:0.969]
Epoch [91/120    avg_loss:0.148, val_acc:0.956]
Epoch [92/120    avg_loss:0.161, val_acc:0.954]
Epoch [93/120    avg_loss:0.147, val_acc:0.967]
Epoch [94/120    avg_loss:0.145, val_acc:0.963]
Epoch [95/120    avg_loss:0.114, val_acc:0.969]
Epoch [96/120    avg_loss:0.116, val_acc:0.975]
Epoch [97/120    avg_loss:0.096, val_acc:0.973]
Epoch [98/120    avg_loss:0.158, val_acc:0.965]
Epoch [99/120    avg_loss:0.162, val_acc:0.956]
Epoch [100/120    avg_loss:0.151, val_acc:0.958]
Epoch [101/120    avg_loss:0.128, val_acc:0.971]
Epoch [102/120    avg_loss:0.142, val_acc:0.960]
Epoch [103/120    avg_loss:0.174, val_acc:0.931]
Epoch [104/120    avg_loss:0.179, val_acc:0.935]
Epoch [105/120    avg_loss:0.187, val_acc:0.958]
Epoch [106/120    avg_loss:0.122, val_acc:0.967]
Epoch [107/120    avg_loss:0.137, val_acc:0.960]
Epoch [108/120    avg_loss:0.130, val_acc:0.954]
Epoch [109/120    avg_loss:0.145, val_acc:0.956]
Epoch [110/120    avg_loss:0.108, val_acc:0.975]
Epoch [111/120    avg_loss:0.076, val_acc:0.981]
Epoch [112/120    avg_loss:0.067, val_acc:0.981]
Epoch [113/120    avg_loss:0.084, val_acc:0.979]
Epoch [114/120    avg_loss:0.074, val_acc:0.983]
Epoch [115/120    avg_loss:0.081, val_acc:0.981]
Epoch [116/120    avg_loss:0.077, val_acc:0.979]
Epoch [117/120    avg_loss:0.069, val_acc:0.981]
Epoch [118/120    avg_loss:0.071, val_acc:0.985]
Epoch [119/120    avg_loss:0.078, val_acc:0.983]
Epoch [120/120    avg_loss:0.076, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.97550111 0.98454746 0.88559322 0.84805654
 0.98771499 0.93785311 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9843310205297344
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f782cf92e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.079]
Epoch [2/120    avg_loss:2.616, val_acc:0.177]
Epoch [3/120    avg_loss:2.597, val_acc:0.227]
Epoch [4/120    avg_loss:2.574, val_acc:0.233]
Epoch [5/120    avg_loss:2.544, val_acc:0.271]
Epoch [6/120    avg_loss:2.514, val_acc:0.292]
Epoch [7/120    avg_loss:2.478, val_acc:0.315]
Epoch [8/120    avg_loss:2.456, val_acc:0.331]
Epoch [9/120    avg_loss:2.416, val_acc:0.350]
Epoch [10/120    avg_loss:2.376, val_acc:0.358]
Epoch [11/120    avg_loss:2.329, val_acc:0.369]
Epoch [12/120    avg_loss:2.287, val_acc:0.419]
Epoch [13/120    avg_loss:2.272, val_acc:0.425]
Epoch [14/120    avg_loss:2.208, val_acc:0.412]
Epoch [15/120    avg_loss:2.148, val_acc:0.417]
Epoch [16/120    avg_loss:2.115, val_acc:0.469]
Epoch [17/120    avg_loss:2.080, val_acc:0.496]
Epoch [18/120    avg_loss:2.020, val_acc:0.554]
Epoch [19/120    avg_loss:1.950, val_acc:0.592]
Epoch [20/120    avg_loss:1.927, val_acc:0.573]
Epoch [21/120    avg_loss:1.837, val_acc:0.613]
Epoch [22/120    avg_loss:1.783, val_acc:0.656]
Epoch [23/120    avg_loss:1.714, val_acc:0.673]
Epoch [24/120    avg_loss:1.620, val_acc:0.677]
Epoch [25/120    avg_loss:1.558, val_acc:0.677]
Epoch [26/120    avg_loss:1.484, val_acc:0.694]
Epoch [27/120    avg_loss:1.429, val_acc:0.708]
Epoch [28/120    avg_loss:1.314, val_acc:0.694]
Epoch [29/120    avg_loss:1.268, val_acc:0.713]
Epoch [30/120    avg_loss:1.186, val_acc:0.715]
Epoch [31/120    avg_loss:1.154, val_acc:0.752]
Epoch [32/120    avg_loss:1.076, val_acc:0.831]
Epoch [33/120    avg_loss:0.998, val_acc:0.731]
Epoch [34/120    avg_loss:0.979, val_acc:0.854]
Epoch [35/120    avg_loss:0.986, val_acc:0.883]
Epoch [36/120    avg_loss:0.897, val_acc:0.815]
Epoch [37/120    avg_loss:0.847, val_acc:0.777]
Epoch [38/120    avg_loss:0.785, val_acc:0.919]
Epoch [39/120    avg_loss:0.752, val_acc:0.827]
Epoch [40/120    avg_loss:0.724, val_acc:0.890]
Epoch [41/120    avg_loss:0.700, val_acc:0.883]
Epoch [42/120    avg_loss:0.651, val_acc:0.912]
Epoch [43/120    avg_loss:0.636, val_acc:0.902]
Epoch [44/120    avg_loss:0.583, val_acc:0.906]
Epoch [45/120    avg_loss:0.584, val_acc:0.917]
Epoch [46/120    avg_loss:0.593, val_acc:0.929]
Epoch [47/120    avg_loss:0.550, val_acc:0.927]
Epoch [48/120    avg_loss:0.542, val_acc:0.904]
Epoch [49/120    avg_loss:0.522, val_acc:0.927]
Epoch [50/120    avg_loss:0.493, val_acc:0.927]
Epoch [51/120    avg_loss:0.455, val_acc:0.931]
Epoch [52/120    avg_loss:0.463, val_acc:0.923]
Epoch [53/120    avg_loss:0.444, val_acc:0.944]
Epoch [54/120    avg_loss:0.424, val_acc:0.933]
Epoch [55/120    avg_loss:0.369, val_acc:0.925]
Epoch [56/120    avg_loss:0.393, val_acc:0.942]
Epoch [57/120    avg_loss:0.368, val_acc:0.933]
Epoch [58/120    avg_loss:0.340, val_acc:0.929]
Epoch [59/120    avg_loss:0.361, val_acc:0.950]
Epoch [60/120    avg_loss:0.338, val_acc:0.940]
Epoch [61/120    avg_loss:0.356, val_acc:0.956]
Epoch [62/120    avg_loss:0.311, val_acc:0.948]
Epoch [63/120    avg_loss:0.294, val_acc:0.954]
Epoch [64/120    avg_loss:0.273, val_acc:0.871]
Epoch [65/120    avg_loss:0.294, val_acc:0.942]
Epoch [66/120    avg_loss:0.314, val_acc:0.933]
Epoch [67/120    avg_loss:0.276, val_acc:0.946]
Epoch [68/120    avg_loss:0.273, val_acc:0.950]
Epoch [69/120    avg_loss:0.254, val_acc:0.948]
Epoch [70/120    avg_loss:0.241, val_acc:0.938]
Epoch [71/120    avg_loss:0.245, val_acc:0.956]
Epoch [72/120    avg_loss:0.202, val_acc:0.967]
Epoch [73/120    avg_loss:0.204, val_acc:0.965]
Epoch [74/120    avg_loss:0.251, val_acc:0.942]
Epoch [75/120    avg_loss:0.283, val_acc:0.935]
Epoch [76/120    avg_loss:0.303, val_acc:0.954]
Epoch [77/120    avg_loss:0.231, val_acc:0.960]
Epoch [78/120    avg_loss:0.214, val_acc:0.956]
Epoch [79/120    avg_loss:0.214, val_acc:0.969]
Epoch [80/120    avg_loss:0.232, val_acc:0.948]
Epoch [81/120    avg_loss:0.233, val_acc:0.938]
Epoch [82/120    avg_loss:0.224, val_acc:0.965]
Epoch [83/120    avg_loss:0.197, val_acc:0.969]
Epoch [84/120    avg_loss:0.206, val_acc:0.967]
Epoch [85/120    avg_loss:0.170, val_acc:0.963]
Epoch [86/120    avg_loss:0.165, val_acc:0.958]
Epoch [87/120    avg_loss:0.144, val_acc:0.969]
Epoch [88/120    avg_loss:0.163, val_acc:0.977]
Epoch [89/120    avg_loss:0.178, val_acc:0.965]
Epoch [90/120    avg_loss:0.147, val_acc:0.973]
Epoch [91/120    avg_loss:0.148, val_acc:0.969]
Epoch [92/120    avg_loss:0.171, val_acc:0.933]
Epoch [93/120    avg_loss:0.178, val_acc:0.965]
Epoch [94/120    avg_loss:0.153, val_acc:0.979]
Epoch [95/120    avg_loss:0.127, val_acc:0.983]
Epoch [96/120    avg_loss:0.120, val_acc:0.977]
Epoch [97/120    avg_loss:0.191, val_acc:0.967]
Epoch [98/120    avg_loss:0.174, val_acc:0.979]
Epoch [99/120    avg_loss:0.138, val_acc:0.979]
Epoch [100/120    avg_loss:0.145, val_acc:0.969]
Epoch [101/120    avg_loss:0.135, val_acc:0.919]
Epoch [102/120    avg_loss:0.124, val_acc:0.969]
Epoch [103/120    avg_loss:0.126, val_acc:0.973]
Epoch [104/120    avg_loss:0.115, val_acc:0.979]
Epoch [105/120    avg_loss:0.114, val_acc:0.979]
Epoch [106/120    avg_loss:0.115, val_acc:0.973]
Epoch [107/120    avg_loss:0.113, val_acc:0.979]
Epoch [108/120    avg_loss:0.098, val_acc:0.975]
Epoch [109/120    avg_loss:0.093, val_acc:0.975]
Epoch [110/120    avg_loss:0.083, val_acc:0.975]
Epoch [111/120    avg_loss:0.074, val_acc:0.977]
Epoch [112/120    avg_loss:0.073, val_acc:0.975]
Epoch [113/120    avg_loss:0.099, val_acc:0.979]
Epoch [114/120    avg_loss:0.074, val_acc:0.979]
Epoch [115/120    avg_loss:0.070, val_acc:0.979]
Epoch [116/120    avg_loss:0.067, val_acc:0.983]
Epoch [117/120    avg_loss:0.080, val_acc:0.979]
Epoch [118/120    avg_loss:0.080, val_acc:0.983]
Epoch [119/120    avg_loss:0.060, val_acc:0.985]
Epoch [120/120    avg_loss:0.062, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.96196868 0.98454746 0.89715536 0.86006826
 1.         0.90502793 1.         0.99893276 0.99862826 1.
 0.99889503 1.        ]

Kappa:
0.9843320230286968
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0eac44de10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.129]
Epoch [2/120    avg_loss:2.602, val_acc:0.242]
Epoch [3/120    avg_loss:2.568, val_acc:0.275]
Epoch [4/120    avg_loss:2.535, val_acc:0.279]
Epoch [5/120    avg_loss:2.494, val_acc:0.279]
Epoch [6/120    avg_loss:2.458, val_acc:0.279]
Epoch [7/120    avg_loss:2.421, val_acc:0.279]
Epoch [8/120    avg_loss:2.384, val_acc:0.279]
Epoch [9/120    avg_loss:2.349, val_acc:0.285]
Epoch [10/120    avg_loss:2.313, val_acc:0.310]
Epoch [11/120    avg_loss:2.276, val_acc:0.331]
Epoch [12/120    avg_loss:2.239, val_acc:0.333]
Epoch [13/120    avg_loss:2.191, val_acc:0.365]
Epoch [14/120    avg_loss:2.157, val_acc:0.381]
Epoch [15/120    avg_loss:2.111, val_acc:0.415]
Epoch [16/120    avg_loss:2.102, val_acc:0.444]
Epoch [17/120    avg_loss:2.053, val_acc:0.485]
Epoch [18/120    avg_loss:1.988, val_acc:0.473]
Epoch [19/120    avg_loss:1.975, val_acc:0.479]
Epoch [20/120    avg_loss:1.911, val_acc:0.504]
Epoch [21/120    avg_loss:1.867, val_acc:0.548]
Epoch [22/120    avg_loss:1.806, val_acc:0.556]
Epoch [23/120    avg_loss:1.790, val_acc:0.567]
Epoch [24/120    avg_loss:1.748, val_acc:0.592]
Epoch [25/120    avg_loss:1.691, val_acc:0.596]
Epoch [26/120    avg_loss:1.623, val_acc:0.615]
Epoch [27/120    avg_loss:1.568, val_acc:0.621]
Epoch [28/120    avg_loss:1.493, val_acc:0.696]
Epoch [29/120    avg_loss:1.459, val_acc:0.685]
Epoch [30/120    avg_loss:1.401, val_acc:0.706]
Epoch [31/120    avg_loss:1.339, val_acc:0.683]
Epoch [32/120    avg_loss:1.302, val_acc:0.702]
Epoch [33/120    avg_loss:1.292, val_acc:0.708]
Epoch [34/120    avg_loss:1.212, val_acc:0.752]
Epoch [35/120    avg_loss:1.148, val_acc:0.740]
Epoch [36/120    avg_loss:1.044, val_acc:0.783]
Epoch [37/120    avg_loss:0.977, val_acc:0.860]
Epoch [38/120    avg_loss:0.922, val_acc:0.867]
Epoch [39/120    avg_loss:0.862, val_acc:0.867]
Epoch [40/120    avg_loss:0.834, val_acc:0.890]
Epoch [41/120    avg_loss:0.798, val_acc:0.896]
Epoch [42/120    avg_loss:0.711, val_acc:0.917]
Epoch [43/120    avg_loss:0.676, val_acc:0.898]
Epoch [44/120    avg_loss:0.668, val_acc:0.896]
Epoch [45/120    avg_loss:0.613, val_acc:0.906]
Epoch [46/120    avg_loss:0.622, val_acc:0.883]
Epoch [47/120    avg_loss:0.615, val_acc:0.879]
Epoch [48/120    avg_loss:0.620, val_acc:0.873]
Epoch [49/120    avg_loss:0.602, val_acc:0.896]
Epoch [50/120    avg_loss:0.505, val_acc:0.912]
Epoch [51/120    avg_loss:0.532, val_acc:0.902]
Epoch [52/120    avg_loss:0.506, val_acc:0.915]
Epoch [53/120    avg_loss:0.432, val_acc:0.933]
Epoch [54/120    avg_loss:0.458, val_acc:0.917]
Epoch [55/120    avg_loss:0.444, val_acc:0.910]
Epoch [56/120    avg_loss:0.444, val_acc:0.921]
Epoch [57/120    avg_loss:0.415, val_acc:0.925]
Epoch [58/120    avg_loss:0.413, val_acc:0.894]
Epoch [59/120    avg_loss:0.439, val_acc:0.917]
Epoch [60/120    avg_loss:0.398, val_acc:0.919]
Epoch [61/120    avg_loss:0.422, val_acc:0.917]
Epoch [62/120    avg_loss:0.377, val_acc:0.940]
Epoch [63/120    avg_loss:0.374, val_acc:0.935]
Epoch [64/120    avg_loss:0.369, val_acc:0.948]
Epoch [65/120    avg_loss:0.315, val_acc:0.942]
Epoch [66/120    avg_loss:0.307, val_acc:0.942]
Epoch [67/120    avg_loss:0.349, val_acc:0.917]
Epoch [68/120    avg_loss:0.345, val_acc:0.944]
Epoch [69/120    avg_loss:0.279, val_acc:0.958]
Epoch [70/120    avg_loss:0.262, val_acc:0.950]
Epoch [71/120    avg_loss:0.311, val_acc:0.946]
Epoch [72/120    avg_loss:0.246, val_acc:0.944]
Epoch [73/120    avg_loss:0.261, val_acc:0.946]
Epoch [74/120    avg_loss:0.272, val_acc:0.950]
Epoch [75/120    avg_loss:0.236, val_acc:0.965]
Epoch [76/120    avg_loss:0.241, val_acc:0.965]
Epoch [77/120    avg_loss:0.243, val_acc:0.958]
Epoch [78/120    avg_loss:0.258, val_acc:0.958]
Epoch [79/120    avg_loss:0.240, val_acc:0.956]
Epoch [80/120    avg_loss:0.247, val_acc:0.960]
Epoch [81/120    avg_loss:0.200, val_acc:0.958]
Epoch [82/120    avg_loss:0.196, val_acc:0.977]
Epoch [83/120    avg_loss:0.179, val_acc:0.958]
Epoch [84/120    avg_loss:0.229, val_acc:0.971]
Epoch [85/120    avg_loss:0.190, val_acc:0.967]
Epoch [86/120    avg_loss:0.220, val_acc:0.969]
Epoch [87/120    avg_loss:0.212, val_acc:0.971]
Epoch [88/120    avg_loss:0.181, val_acc:0.973]
Epoch [89/120    avg_loss:0.163, val_acc:0.975]
Epoch [90/120    avg_loss:0.150, val_acc:0.979]
Epoch [91/120    avg_loss:0.149, val_acc:0.981]
Epoch [92/120    avg_loss:0.155, val_acc:0.971]
Epoch [93/120    avg_loss:0.167, val_acc:0.973]
Epoch [94/120    avg_loss:0.229, val_acc:0.940]
Epoch [95/120    avg_loss:0.176, val_acc:0.969]
Epoch [96/120    avg_loss:0.163, val_acc:0.973]
Epoch [97/120    avg_loss:0.153, val_acc:0.981]
Epoch [98/120    avg_loss:0.159, val_acc:0.971]
Epoch [99/120    avg_loss:0.164, val_acc:0.979]
Epoch [100/120    avg_loss:0.172, val_acc:0.971]
Epoch [101/120    avg_loss:0.167, val_acc:0.985]
Epoch [102/120    avg_loss:0.125, val_acc:0.985]
Epoch [103/120    avg_loss:0.144, val_acc:0.967]
Epoch [104/120    avg_loss:0.134, val_acc:0.971]
Epoch [105/120    avg_loss:0.123, val_acc:0.977]
Epoch [106/120    avg_loss:0.116, val_acc:0.971]
Epoch [107/120    avg_loss:0.146, val_acc:0.967]
Epoch [108/120    avg_loss:0.127, val_acc:0.981]
Epoch [109/120    avg_loss:0.134, val_acc:0.981]
Epoch [110/120    avg_loss:0.120, val_acc:0.985]
Epoch [111/120    avg_loss:0.093, val_acc:0.979]
Epoch [112/120    avg_loss:0.114, val_acc:0.969]
Epoch [113/120    avg_loss:0.093, val_acc:0.979]
Epoch [114/120    avg_loss:0.107, val_acc:0.983]
Epoch [115/120    avg_loss:0.105, val_acc:0.967]
Epoch [116/120    avg_loss:0.120, val_acc:0.969]
Epoch [117/120    avg_loss:0.139, val_acc:0.977]
Epoch [118/120    avg_loss:0.107, val_acc:0.983]
Epoch [119/120    avg_loss:0.087, val_acc:0.981]
Epoch [120/120    avg_loss:0.103, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 220   7   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.69936034115139

F1 scores:
[       nan 0.99708879 0.9580574  0.97777778 0.92027335 0.91025641
 0.99019608 0.89017341 1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9855177667929667
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4001b81e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.655, val_acc:0.171]
Epoch [2/120    avg_loss:2.621, val_acc:0.229]
Epoch [3/120    avg_loss:2.578, val_acc:0.296]
Epoch [4/120    avg_loss:2.543, val_acc:0.304]
Epoch [5/120    avg_loss:2.501, val_acc:0.310]
Epoch [6/120    avg_loss:2.470, val_acc:0.312]
Epoch [7/120    avg_loss:2.432, val_acc:0.312]
Epoch [8/120    avg_loss:2.407, val_acc:0.312]
Epoch [9/120    avg_loss:2.374, val_acc:0.312]
Epoch [10/120    avg_loss:2.341, val_acc:0.327]
Epoch [11/120    avg_loss:2.309, val_acc:0.346]
Epoch [12/120    avg_loss:2.272, val_acc:0.396]
Epoch [13/120    avg_loss:2.233, val_acc:0.450]
Epoch [14/120    avg_loss:2.178, val_acc:0.467]
Epoch [15/120    avg_loss:2.139, val_acc:0.508]
Epoch [16/120    avg_loss:2.076, val_acc:0.504]
Epoch [17/120    avg_loss:2.008, val_acc:0.529]
Epoch [18/120    avg_loss:1.950, val_acc:0.512]
Epoch [19/120    avg_loss:1.881, val_acc:0.554]
Epoch [20/120    avg_loss:1.844, val_acc:0.571]
Epoch [21/120    avg_loss:1.784, val_acc:0.585]
Epoch [22/120    avg_loss:1.701, val_acc:0.579]
Epoch [23/120    avg_loss:1.643, val_acc:0.669]
Epoch [24/120    avg_loss:1.561, val_acc:0.677]
Epoch [25/120    avg_loss:1.510, val_acc:0.740]
Epoch [26/120    avg_loss:1.431, val_acc:0.729]
Epoch [27/120    avg_loss:1.395, val_acc:0.790]
Epoch [28/120    avg_loss:1.283, val_acc:0.796]
Epoch [29/120    avg_loss:1.213, val_acc:0.775]
Epoch [30/120    avg_loss:1.166, val_acc:0.765]
Epoch [31/120    avg_loss:1.078, val_acc:0.823]
Epoch [32/120    avg_loss:1.047, val_acc:0.829]
Epoch [33/120    avg_loss:0.971, val_acc:0.835]
Epoch [34/120    avg_loss:0.932, val_acc:0.856]
Epoch [35/120    avg_loss:0.852, val_acc:0.827]
Epoch [36/120    avg_loss:0.785, val_acc:0.877]
Epoch [37/120    avg_loss:0.769, val_acc:0.875]
Epoch [38/120    avg_loss:0.760, val_acc:0.892]
Epoch [39/120    avg_loss:0.680, val_acc:0.881]
Epoch [40/120    avg_loss:0.682, val_acc:0.875]
Epoch [41/120    avg_loss:0.629, val_acc:0.883]
Epoch [42/120    avg_loss:0.639, val_acc:0.904]
Epoch [43/120    avg_loss:0.616, val_acc:0.869]
Epoch [44/120    avg_loss:0.593, val_acc:0.883]
Epoch [45/120    avg_loss:0.522, val_acc:0.902]
Epoch [46/120    avg_loss:0.572, val_acc:0.896]
Epoch [47/120    avg_loss:0.507, val_acc:0.908]
Epoch [48/120    avg_loss:0.538, val_acc:0.908]
Epoch [49/120    avg_loss:0.477, val_acc:0.919]
Epoch [50/120    avg_loss:0.479, val_acc:0.902]
Epoch [51/120    avg_loss:0.452, val_acc:0.929]
Epoch [52/120    avg_loss:0.432, val_acc:0.906]
Epoch [53/120    avg_loss:0.429, val_acc:0.904]
Epoch [54/120    avg_loss:0.430, val_acc:0.858]
Epoch [55/120    avg_loss:0.447, val_acc:0.925]
Epoch [56/120    avg_loss:0.398, val_acc:0.919]
Epoch [57/120    avg_loss:0.355, val_acc:0.933]
Epoch [58/120    avg_loss:0.346, val_acc:0.929]
Epoch [59/120    avg_loss:0.303, val_acc:0.942]
Epoch [60/120    avg_loss:0.347, val_acc:0.867]
Epoch [61/120    avg_loss:0.344, val_acc:0.931]
Epoch [62/120    avg_loss:0.329, val_acc:0.929]
Epoch [63/120    avg_loss:0.288, val_acc:0.935]
Epoch [64/120    avg_loss:0.263, val_acc:0.950]
Epoch [65/120    avg_loss:0.318, val_acc:0.921]
Epoch [66/120    avg_loss:0.349, val_acc:0.919]
Epoch [67/120    avg_loss:0.321, val_acc:0.944]
Epoch [68/120    avg_loss:0.285, val_acc:0.944]
Epoch [69/120    avg_loss:0.286, val_acc:0.940]
Epoch [70/120    avg_loss:0.359, val_acc:0.935]
Epoch [71/120    avg_loss:0.280, val_acc:0.956]
Epoch [72/120    avg_loss:0.239, val_acc:0.956]
Epoch [73/120    avg_loss:0.246, val_acc:0.923]
Epoch [74/120    avg_loss:0.227, val_acc:0.956]
Epoch [75/120    avg_loss:0.226, val_acc:0.944]
Epoch [76/120    avg_loss:0.221, val_acc:0.958]
Epoch [77/120    avg_loss:0.219, val_acc:0.954]
Epoch [78/120    avg_loss:0.211, val_acc:0.950]
Epoch [79/120    avg_loss:0.196, val_acc:0.954]
Epoch [80/120    avg_loss:0.198, val_acc:0.942]
Epoch [81/120    avg_loss:0.204, val_acc:0.963]
Epoch [82/120    avg_loss:0.255, val_acc:0.942]
Epoch [83/120    avg_loss:0.202, val_acc:0.940]
Epoch [84/120    avg_loss:0.177, val_acc:0.944]
Epoch [85/120    avg_loss:0.138, val_acc:0.963]
Epoch [86/120    avg_loss:0.138, val_acc:0.965]
Epoch [87/120    avg_loss:0.125, val_acc:0.958]
Epoch [88/120    avg_loss:0.151, val_acc:0.954]
Epoch [89/120    avg_loss:0.205, val_acc:0.946]
Epoch [90/120    avg_loss:0.176, val_acc:0.954]
Epoch [91/120    avg_loss:0.172, val_acc:0.965]
Epoch [92/120    avg_loss:0.139, val_acc:0.965]
Epoch [93/120    avg_loss:0.120, val_acc:0.952]
Epoch [94/120    avg_loss:0.128, val_acc:0.910]
Epoch [95/120    avg_loss:0.143, val_acc:0.975]
Epoch [96/120    avg_loss:0.162, val_acc:0.956]
Epoch [97/120    avg_loss:0.161, val_acc:0.944]
Epoch [98/120    avg_loss:0.242, val_acc:0.946]
Epoch [99/120    avg_loss:0.203, val_acc:0.971]
Epoch [100/120    avg_loss:0.144, val_acc:0.942]
Epoch [101/120    avg_loss:0.149, val_acc:0.965]
Epoch [102/120    avg_loss:0.148, val_acc:0.946]
Epoch [103/120    avg_loss:0.190, val_acc:0.952]
Epoch [104/120    avg_loss:0.193, val_acc:0.967]
Epoch [105/120    avg_loss:0.137, val_acc:0.969]
Epoch [106/120    avg_loss:0.117, val_acc:0.958]
Epoch [107/120    avg_loss:0.105, val_acc:0.967]
Epoch [108/120    avg_loss:0.153, val_acc:0.960]
Epoch [109/120    avg_loss:0.103, val_acc:0.981]
Epoch [110/120    avg_loss:0.083, val_acc:0.971]
Epoch [111/120    avg_loss:0.080, val_acc:0.971]
Epoch [112/120    avg_loss:0.062, val_acc:0.971]
Epoch [113/120    avg_loss:0.071, val_acc:0.973]
Epoch [114/120    avg_loss:0.073, val_acc:0.975]
Epoch [115/120    avg_loss:0.064, val_acc:0.979]
Epoch [116/120    avg_loss:0.059, val_acc:0.979]
Epoch [117/120    avg_loss:0.062, val_acc:0.977]
Epoch [118/120    avg_loss:0.063, val_acc:0.977]
Epoch [119/120    avg_loss:0.067, val_acc:0.975]
Epoch [120/120    avg_loss:0.058, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215  15   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   0 199   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 1.         0.97986577 0.96629213 0.87631027 0.87197232
 0.98271605 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9838569915318045
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffac4fb3e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.646, val_acc:0.050]
Epoch [2/120    avg_loss:2.616, val_acc:0.217]
Epoch [3/120    avg_loss:2.586, val_acc:0.325]
Epoch [4/120    avg_loss:2.548, val_acc:0.352]
Epoch [5/120    avg_loss:2.511, val_acc:0.367]
Epoch [6/120    avg_loss:2.485, val_acc:0.398]
Epoch [7/120    avg_loss:2.449, val_acc:0.398]
Epoch [8/120    avg_loss:2.410, val_acc:0.400]
Epoch [9/120    avg_loss:2.377, val_acc:0.419]
Epoch [10/120    avg_loss:2.348, val_acc:0.425]
Epoch [11/120    avg_loss:2.312, val_acc:0.415]
Epoch [12/120    avg_loss:2.268, val_acc:0.431]
Epoch [13/120    avg_loss:2.229, val_acc:0.433]
Epoch [14/120    avg_loss:2.184, val_acc:0.438]
Epoch [15/120    avg_loss:2.144, val_acc:0.446]
Epoch [16/120    avg_loss:2.079, val_acc:0.458]
Epoch [17/120    avg_loss:2.011, val_acc:0.467]
Epoch [18/120    avg_loss:1.976, val_acc:0.490]
Epoch [19/120    avg_loss:1.904, val_acc:0.544]
Epoch [20/120    avg_loss:1.836, val_acc:0.588]
Epoch [21/120    avg_loss:1.797, val_acc:0.596]
Epoch [22/120    avg_loss:1.737, val_acc:0.650]
Epoch [23/120    avg_loss:1.696, val_acc:0.660]
Epoch [24/120    avg_loss:1.599, val_acc:0.694]
Epoch [25/120    avg_loss:1.527, val_acc:0.700]
Epoch [26/120    avg_loss:1.459, val_acc:0.719]
Epoch [27/120    avg_loss:1.416, val_acc:0.717]
Epoch [28/120    avg_loss:1.359, val_acc:0.735]
Epoch [29/120    avg_loss:1.287, val_acc:0.729]
Epoch [30/120    avg_loss:1.209, val_acc:0.765]
Epoch [31/120    avg_loss:1.201, val_acc:0.738]
Epoch [32/120    avg_loss:1.095, val_acc:0.762]
Epoch [33/120    avg_loss:1.054, val_acc:0.771]
Epoch [34/120    avg_loss:0.991, val_acc:0.783]
Epoch [35/120    avg_loss:0.909, val_acc:0.783]
Epoch [36/120    avg_loss:0.854, val_acc:0.781]
Epoch [37/120    avg_loss:0.829, val_acc:0.906]
Epoch [38/120    avg_loss:0.765, val_acc:0.787]
Epoch [39/120    avg_loss:0.777, val_acc:0.881]
Epoch [40/120    avg_loss:0.756, val_acc:0.869]
Epoch [41/120    avg_loss:0.704, val_acc:0.896]
Epoch [42/120    avg_loss:0.672, val_acc:0.931]
Epoch [43/120    avg_loss:0.613, val_acc:0.915]
Epoch [44/120    avg_loss:0.638, val_acc:0.873]
Epoch [45/120    avg_loss:0.584, val_acc:0.929]
Epoch [46/120    avg_loss:0.536, val_acc:0.923]
Epoch [47/120    avg_loss:0.564, val_acc:0.910]
Epoch [48/120    avg_loss:0.514, val_acc:0.927]
Epoch [49/120    avg_loss:0.511, val_acc:0.940]
Epoch [50/120    avg_loss:0.472, val_acc:0.921]
Epoch [51/120    avg_loss:0.449, val_acc:0.935]
Epoch [52/120    avg_loss:0.467, val_acc:0.923]
Epoch [53/120    avg_loss:0.389, val_acc:0.935]
Epoch [54/120    avg_loss:0.409, val_acc:0.935]
Epoch [55/120    avg_loss:0.433, val_acc:0.956]
Epoch [56/120    avg_loss:0.364, val_acc:0.910]
Epoch [57/120    avg_loss:0.344, val_acc:0.900]
Epoch [58/120    avg_loss:0.394, val_acc:0.948]
Epoch [59/120    avg_loss:0.347, val_acc:0.931]
Epoch [60/120    avg_loss:0.356, val_acc:0.946]
Epoch [61/120    avg_loss:0.388, val_acc:0.942]
Epoch [62/120    avg_loss:0.416, val_acc:0.931]
Epoch [63/120    avg_loss:0.418, val_acc:0.935]
Epoch [64/120    avg_loss:0.351, val_acc:0.956]
Epoch [65/120    avg_loss:0.356, val_acc:0.960]
Epoch [66/120    avg_loss:0.318, val_acc:0.965]
Epoch [67/120    avg_loss:0.301, val_acc:0.956]
Epoch [68/120    avg_loss:0.259, val_acc:0.952]
Epoch [69/120    avg_loss:0.327, val_acc:0.960]
Epoch [70/120    avg_loss:0.288, val_acc:0.960]
Epoch [71/120    avg_loss:0.279, val_acc:0.942]
Epoch [72/120    avg_loss:0.251, val_acc:0.954]
Epoch [73/120    avg_loss:0.251, val_acc:0.933]
Epoch [74/120    avg_loss:0.279, val_acc:0.944]
Epoch [75/120    avg_loss:0.271, val_acc:0.965]
Epoch [76/120    avg_loss:0.236, val_acc:0.967]
Epoch [77/120    avg_loss:0.226, val_acc:0.960]
Epoch [78/120    avg_loss:0.218, val_acc:0.981]
Epoch [79/120    avg_loss:0.256, val_acc:0.950]
Epoch [80/120    avg_loss:0.207, val_acc:0.973]
Epoch [81/120    avg_loss:0.232, val_acc:0.956]
Epoch [82/120    avg_loss:0.208, val_acc:0.981]
Epoch [83/120    avg_loss:0.193, val_acc:0.942]
Epoch [84/120    avg_loss:0.214, val_acc:0.977]
Epoch [85/120    avg_loss:0.160, val_acc:0.975]
Epoch [86/120    avg_loss:0.152, val_acc:0.981]
Epoch [87/120    avg_loss:0.176, val_acc:0.981]
Epoch [88/120    avg_loss:0.150, val_acc:0.985]
Epoch [89/120    avg_loss:0.140, val_acc:0.985]
Epoch [90/120    avg_loss:0.190, val_acc:0.969]
Epoch [91/120    avg_loss:0.133, val_acc:0.965]
Epoch [92/120    avg_loss:0.135, val_acc:0.969]
Epoch [93/120    avg_loss:0.154, val_acc:0.960]
Epoch [94/120    avg_loss:0.129, val_acc:0.977]
Epoch [95/120    avg_loss:0.148, val_acc:0.977]
Epoch [96/120    avg_loss:0.141, val_acc:0.981]
Epoch [97/120    avg_loss:0.146, val_acc:0.981]
Epoch [98/120    avg_loss:0.126, val_acc:0.983]
Epoch [99/120    avg_loss:0.119, val_acc:0.973]
Epoch [100/120    avg_loss:0.126, val_acc:0.975]
Epoch [101/120    avg_loss:0.188, val_acc:0.965]
Epoch [102/120    avg_loss:0.187, val_acc:0.977]
Epoch [103/120    avg_loss:0.110, val_acc:0.971]
Epoch [104/120    avg_loss:0.109, val_acc:0.983]
Epoch [105/120    avg_loss:0.079, val_acc:0.979]
Epoch [106/120    avg_loss:0.087, val_acc:0.981]
Epoch [107/120    avg_loss:0.091, val_acc:0.981]
Epoch [108/120    avg_loss:0.090, val_acc:0.983]
Epoch [109/120    avg_loss:0.083, val_acc:0.985]
Epoch [110/120    avg_loss:0.098, val_acc:0.981]
Epoch [111/120    avg_loss:0.081, val_acc:0.983]
Epoch [112/120    avg_loss:0.075, val_acc:0.981]
Epoch [113/120    avg_loss:0.084, val_acc:0.985]
Epoch [114/120    avg_loss:0.069, val_acc:0.981]
Epoch [115/120    avg_loss:0.064, val_acc:0.983]
Epoch [116/120    avg_loss:0.073, val_acc:0.988]
Epoch [117/120    avg_loss:0.079, val_acc:0.985]
Epoch [118/120    avg_loss:0.074, val_acc:0.988]
Epoch [119/120    avg_loss:0.070, val_acc:0.988]
Epoch [120/120    avg_loss:0.069, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   2 224   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 1.         0.94305239 0.98678414 0.92307692 0.92307692
 0.98522167 0.87830688 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9859957962590036
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb46d084e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.158]
Epoch [2/120    avg_loss:2.558, val_acc:0.273]
Epoch [3/120    avg_loss:2.530, val_acc:0.340]
Epoch [4/120    avg_loss:2.502, val_acc:0.342]
Epoch [5/120    avg_loss:2.471, val_acc:0.342]
Epoch [6/120    avg_loss:2.443, val_acc:0.342]
Epoch [7/120    avg_loss:2.431, val_acc:0.344]
Epoch [8/120    avg_loss:2.411, val_acc:0.357]
Epoch [9/120    avg_loss:2.382, val_acc:0.373]
Epoch [10/120    avg_loss:2.360, val_acc:0.393]
Epoch [11/120    avg_loss:2.341, val_acc:0.418]
Epoch [12/120    avg_loss:2.306, val_acc:0.443]
Epoch [13/120    avg_loss:2.277, val_acc:0.479]
Epoch [14/120    avg_loss:2.233, val_acc:0.504]
Epoch [15/120    avg_loss:2.198, val_acc:0.504]
Epoch [16/120    avg_loss:2.163, val_acc:0.504]
Epoch [17/120    avg_loss:2.117, val_acc:0.535]
Epoch [18/120    avg_loss:2.057, val_acc:0.518]
Epoch [19/120    avg_loss:2.016, val_acc:0.541]
Epoch [20/120    avg_loss:1.951, val_acc:0.537]
Epoch [21/120    avg_loss:1.895, val_acc:0.545]
Epoch [22/120    avg_loss:1.813, val_acc:0.598]
Epoch [23/120    avg_loss:1.754, val_acc:0.582]
Epoch [24/120    avg_loss:1.673, val_acc:0.654]
Epoch [25/120    avg_loss:1.620, val_acc:0.678]
Epoch [26/120    avg_loss:1.536, val_acc:0.662]
Epoch [27/120    avg_loss:1.527, val_acc:0.641]
Epoch [28/120    avg_loss:1.459, val_acc:0.709]
Epoch [29/120    avg_loss:1.346, val_acc:0.738]
Epoch [30/120    avg_loss:1.290, val_acc:0.744]
Epoch [31/120    avg_loss:1.265, val_acc:0.727]
Epoch [32/120    avg_loss:1.211, val_acc:0.709]
Epoch [33/120    avg_loss:1.117, val_acc:0.752]
Epoch [34/120    avg_loss:1.034, val_acc:0.793]
Epoch [35/120    avg_loss:0.993, val_acc:0.793]
Epoch [36/120    avg_loss:0.943, val_acc:0.783]
Epoch [37/120    avg_loss:0.899, val_acc:0.836]
Epoch [38/120    avg_loss:0.823, val_acc:0.863]
Epoch [39/120    avg_loss:0.767, val_acc:0.889]
Epoch [40/120    avg_loss:0.747, val_acc:0.896]
Epoch [41/120    avg_loss:0.723, val_acc:0.861]
Epoch [42/120    avg_loss:0.695, val_acc:0.930]
Epoch [43/120    avg_loss:0.659, val_acc:0.895]
Epoch [44/120    avg_loss:0.648, val_acc:0.908]
Epoch [45/120    avg_loss:0.602, val_acc:0.930]
Epoch [46/120    avg_loss:0.616, val_acc:0.930]
Epoch [47/120    avg_loss:0.510, val_acc:0.943]
Epoch [48/120    avg_loss:0.472, val_acc:0.943]
Epoch [49/120    avg_loss:0.470, val_acc:0.938]
Epoch [50/120    avg_loss:0.460, val_acc:0.883]
Epoch [51/120    avg_loss:0.440, val_acc:0.902]
Epoch [52/120    avg_loss:0.429, val_acc:0.926]
Epoch [53/120    avg_loss:0.430, val_acc:0.938]
Epoch [54/120    avg_loss:0.391, val_acc:0.930]
Epoch [55/120    avg_loss:0.419, val_acc:0.887]
Epoch [56/120    avg_loss:0.394, val_acc:0.941]
Epoch [57/120    avg_loss:0.306, val_acc:0.955]
Epoch [58/120    avg_loss:0.351, val_acc:0.938]
Epoch [59/120    avg_loss:0.358, val_acc:0.961]
Epoch [60/120    avg_loss:0.387, val_acc:0.939]
Epoch [61/120    avg_loss:0.367, val_acc:0.955]
Epoch [62/120    avg_loss:0.295, val_acc:0.973]
Epoch [63/120    avg_loss:0.258, val_acc:0.951]
Epoch [64/120    avg_loss:0.282, val_acc:0.938]
Epoch [65/120    avg_loss:0.286, val_acc:0.961]
Epoch [66/120    avg_loss:0.235, val_acc:0.971]
Epoch [67/120    avg_loss:0.252, val_acc:0.969]
Epoch [68/120    avg_loss:0.240, val_acc:0.955]
Epoch [69/120    avg_loss:0.257, val_acc:0.979]
Epoch [70/120    avg_loss:0.210, val_acc:0.924]
Epoch [71/120    avg_loss:0.318, val_acc:0.922]
Epoch [72/120    avg_loss:0.254, val_acc:0.943]
Epoch [73/120    avg_loss:0.208, val_acc:0.951]
Epoch [74/120    avg_loss:0.194, val_acc:0.967]
Epoch [75/120    avg_loss:0.205, val_acc:0.961]
Epoch [76/120    avg_loss:0.206, val_acc:0.965]
Epoch [77/120    avg_loss:0.255, val_acc:0.963]
Epoch [78/120    avg_loss:0.212, val_acc:0.938]
Epoch [79/120    avg_loss:0.296, val_acc:0.934]
Epoch [80/120    avg_loss:0.261, val_acc:0.973]
Epoch [81/120    avg_loss:0.196, val_acc:0.971]
Epoch [82/120    avg_loss:0.227, val_acc:0.955]
Epoch [83/120    avg_loss:0.174, val_acc:0.973]
Epoch [84/120    avg_loss:0.142, val_acc:0.977]
Epoch [85/120    avg_loss:0.149, val_acc:0.980]
Epoch [86/120    avg_loss:0.127, val_acc:0.984]
Epoch [87/120    avg_loss:0.117, val_acc:0.986]
Epoch [88/120    avg_loss:0.121, val_acc:0.986]
Epoch [89/120    avg_loss:0.117, val_acc:0.984]
Epoch [90/120    avg_loss:0.113, val_acc:0.980]
Epoch [91/120    avg_loss:0.119, val_acc:0.984]
Epoch [92/120    avg_loss:0.115, val_acc:0.979]
Epoch [93/120    avg_loss:0.115, val_acc:0.982]
Epoch [94/120    avg_loss:0.125, val_acc:0.982]
Epoch [95/120    avg_loss:0.101, val_acc:0.979]
Epoch [96/120    avg_loss:0.118, val_acc:0.980]
Epoch [97/120    avg_loss:0.100, val_acc:0.982]
Epoch [98/120    avg_loss:0.103, val_acc:0.986]
Epoch [99/120    avg_loss:0.094, val_acc:0.980]
Epoch [100/120    avg_loss:0.128, val_acc:0.979]
Epoch [101/120    avg_loss:0.119, val_acc:0.979]
Epoch [102/120    avg_loss:0.104, val_acc:0.988]
Epoch [103/120    avg_loss:0.109, val_acc:0.986]
Epoch [104/120    avg_loss:0.093, val_acc:0.984]
Epoch [105/120    avg_loss:0.106, val_acc:0.986]
Epoch [106/120    avg_loss:0.097, val_acc:0.986]
Epoch [107/120    avg_loss:0.107, val_acc:0.990]
Epoch [108/120    avg_loss:0.087, val_acc:0.990]
Epoch [109/120    avg_loss:0.091, val_acc:0.988]
Epoch [110/120    avg_loss:0.096, val_acc:0.988]
Epoch [111/120    avg_loss:0.086, val_acc:0.990]
Epoch [112/120    avg_loss:0.086, val_acc:0.986]
Epoch [113/120    avg_loss:0.085, val_acc:0.988]
Epoch [114/120    avg_loss:0.089, val_acc:0.988]
Epoch [115/120    avg_loss:0.091, val_acc:0.986]
Epoch [116/120    avg_loss:0.097, val_acc:0.988]
Epoch [117/120    avg_loss:0.095, val_acc:0.990]
Epoch [118/120    avg_loss:0.089, val_acc:0.988]
Epoch [119/120    avg_loss:0.091, val_acc:0.986]
Epoch [120/120    avg_loss:0.092, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 219   6   0   0   0   4   1   0   0   0   0]
 [  0   0   0   3 202  22   0   0   0   0   0   0   0   0]
 [  0   0   0   5   9 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   1   0   0   1   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.29424307036247

F1 scores:
[       nan 1.         0.95       0.95633188 0.89777778 0.87919463
 0.98280098 0.88172043 0.99487179 0.99893276 1.         1.
 0.99778761 1.        ]

Kappa:
0.98100936250576
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff922fefe80>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.081]
Epoch [2/120    avg_loss:2.578, val_acc:0.190]
Epoch [3/120    avg_loss:2.545, val_acc:0.254]
Epoch [4/120    avg_loss:2.513, val_acc:0.306]
Epoch [5/120    avg_loss:2.485, val_acc:0.344]
Epoch [6/120    avg_loss:2.458, val_acc:0.367]
Epoch [7/120    avg_loss:2.427, val_acc:0.377]
Epoch [8/120    avg_loss:2.390, val_acc:0.379]
Epoch [9/120    avg_loss:2.372, val_acc:0.371]
Epoch [10/120    avg_loss:2.344, val_acc:0.369]
Epoch [11/120    avg_loss:2.302, val_acc:0.352]
Epoch [12/120    avg_loss:2.275, val_acc:0.362]
Epoch [13/120    avg_loss:2.223, val_acc:0.354]
Epoch [14/120    avg_loss:2.190, val_acc:0.346]
Epoch [15/120    avg_loss:2.156, val_acc:0.358]
Epoch [16/120    avg_loss:2.098, val_acc:0.390]
Epoch [17/120    avg_loss:2.074, val_acc:0.412]
Epoch [18/120    avg_loss:2.014, val_acc:0.504]
Epoch [19/120    avg_loss:1.969, val_acc:0.598]
Epoch [20/120    avg_loss:1.933, val_acc:0.573]
Epoch [21/120    avg_loss:1.895, val_acc:0.600]
Epoch [22/120    avg_loss:1.849, val_acc:0.696]
Epoch [23/120    avg_loss:1.774, val_acc:0.738]
Epoch [24/120    avg_loss:1.728, val_acc:0.779]
Epoch [25/120    avg_loss:1.673, val_acc:0.779]
Epoch [26/120    avg_loss:1.637, val_acc:0.775]
Epoch [27/120    avg_loss:1.570, val_acc:0.792]
Epoch [28/120    avg_loss:1.468, val_acc:0.804]
Epoch [29/120    avg_loss:1.428, val_acc:0.842]
Epoch [30/120    avg_loss:1.362, val_acc:0.838]
Epoch [31/120    avg_loss:1.292, val_acc:0.871]
Epoch [32/120    avg_loss:1.247, val_acc:0.852]
Epoch [33/120    avg_loss:1.200, val_acc:0.867]
Epoch [34/120    avg_loss:1.160, val_acc:0.894]
Epoch [35/120    avg_loss:1.074, val_acc:0.917]
Epoch [36/120    avg_loss:1.040, val_acc:0.900]
Epoch [37/120    avg_loss:0.976, val_acc:0.894]
Epoch [38/120    avg_loss:0.899, val_acc:0.902]
Epoch [39/120    avg_loss:0.876, val_acc:0.910]
Epoch [40/120    avg_loss:0.803, val_acc:0.885]
Epoch [41/120    avg_loss:0.761, val_acc:0.910]
Epoch [42/120    avg_loss:0.758, val_acc:0.904]
Epoch [43/120    avg_loss:0.694, val_acc:0.869]
Epoch [44/120    avg_loss:0.683, val_acc:0.904]
Epoch [45/120    avg_loss:0.604, val_acc:0.925]
Epoch [46/120    avg_loss:0.584, val_acc:0.906]
Epoch [47/120    avg_loss:0.533, val_acc:0.929]
Epoch [48/120    avg_loss:0.553, val_acc:0.940]
Epoch [49/120    avg_loss:0.580, val_acc:0.906]
Epoch [50/120    avg_loss:0.534, val_acc:0.892]
Epoch [51/120    avg_loss:0.500, val_acc:0.908]
Epoch [52/120    avg_loss:0.461, val_acc:0.927]
Epoch [53/120    avg_loss:0.444, val_acc:0.935]
Epoch [54/120    avg_loss:0.398, val_acc:0.917]
Epoch [55/120    avg_loss:0.427, val_acc:0.910]
Epoch [56/120    avg_loss:0.406, val_acc:0.925]
Epoch [57/120    avg_loss:0.386, val_acc:0.940]
Epoch [58/120    avg_loss:0.386, val_acc:0.942]
Epoch [59/120    avg_loss:0.365, val_acc:0.963]
Epoch [60/120    avg_loss:0.346, val_acc:0.950]
Epoch [61/120    avg_loss:0.395, val_acc:0.952]
Epoch [62/120    avg_loss:0.390, val_acc:0.940]
Epoch [63/120    avg_loss:0.394, val_acc:0.942]
Epoch [64/120    avg_loss:0.333, val_acc:0.952]
Epoch [65/120    avg_loss:0.366, val_acc:0.960]
Epoch [66/120    avg_loss:0.334, val_acc:0.960]
Epoch [67/120    avg_loss:0.280, val_acc:0.950]
Epoch [68/120    avg_loss:0.319, val_acc:0.940]
Epoch [69/120    avg_loss:0.268, val_acc:0.965]
Epoch [70/120    avg_loss:0.267, val_acc:0.963]
Epoch [71/120    avg_loss:0.252, val_acc:0.940]
Epoch [72/120    avg_loss:0.287, val_acc:0.940]
Epoch [73/120    avg_loss:0.327, val_acc:0.923]
Epoch [74/120    avg_loss:0.306, val_acc:0.954]
Epoch [75/120    avg_loss:0.252, val_acc:0.956]
Epoch [76/120    avg_loss:0.244, val_acc:0.975]
Epoch [77/120    avg_loss:0.216, val_acc:0.950]
Epoch [78/120    avg_loss:0.280, val_acc:0.969]
Epoch [79/120    avg_loss:0.216, val_acc:0.960]
Epoch [80/120    avg_loss:0.190, val_acc:0.960]
Epoch [81/120    avg_loss:0.206, val_acc:0.971]
Epoch [82/120    avg_loss:0.210, val_acc:0.958]
Epoch [83/120    avg_loss:0.221, val_acc:0.965]
Epoch [84/120    avg_loss:0.195, val_acc:0.979]
Epoch [85/120    avg_loss:0.193, val_acc:0.979]
Epoch [86/120    avg_loss:0.169, val_acc:0.977]
Epoch [87/120    avg_loss:0.176, val_acc:0.963]
Epoch [88/120    avg_loss:0.175, val_acc:0.979]
Epoch [89/120    avg_loss:0.154, val_acc:0.975]
Epoch [90/120    avg_loss:0.143, val_acc:0.977]
Epoch [91/120    avg_loss:0.164, val_acc:0.952]
Epoch [92/120    avg_loss:0.235, val_acc:0.948]
Epoch [93/120    avg_loss:0.171, val_acc:0.981]
Epoch [94/120    avg_loss:0.153, val_acc:0.977]
Epoch [95/120    avg_loss:0.167, val_acc:0.971]
Epoch [96/120    avg_loss:0.183, val_acc:0.977]
Epoch [97/120    avg_loss:0.162, val_acc:0.975]
Epoch [98/120    avg_loss:0.146, val_acc:0.963]
Epoch [99/120    avg_loss:0.174, val_acc:0.965]
Epoch [100/120    avg_loss:0.221, val_acc:0.933]
Epoch [101/120    avg_loss:0.206, val_acc:0.977]
Epoch [102/120    avg_loss:0.169, val_acc:0.956]
Epoch [103/120    avg_loss:0.165, val_acc:0.975]
Epoch [104/120    avg_loss:0.150, val_acc:0.977]
Epoch [105/120    avg_loss:0.120, val_acc:0.965]
Epoch [106/120    avg_loss:0.137, val_acc:0.983]
Epoch [107/120    avg_loss:0.142, val_acc:0.975]
Epoch [108/120    avg_loss:0.139, val_acc:0.946]
Epoch [109/120    avg_loss:0.111, val_acc:0.979]
Epoch [110/120    avg_loss:0.127, val_acc:0.971]
Epoch [111/120    avg_loss:0.138, val_acc:0.977]
Epoch [112/120    avg_loss:0.166, val_acc:0.971]
Epoch [113/120    avg_loss:0.134, val_acc:0.967]
Epoch [114/120    avg_loss:0.160, val_acc:0.973]
Epoch [115/120    avg_loss:0.134, val_acc:0.983]
Epoch [116/120    avg_loss:0.139, val_acc:0.971]
Epoch [117/120    avg_loss:0.192, val_acc:0.973]
Epoch [118/120    avg_loss:0.153, val_acc:0.963]
Epoch [119/120    avg_loss:0.133, val_acc:0.960]
Epoch [120/120    avg_loss:0.099, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   3 194  27   0   0   0   6   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  44 101   0   0   0   0   0   0   0   0]
 [  0  89   0   0   0   0 117   0   0   0   0   0   0   0]
 [  0   0  23   0   0   0   0  71   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
95.84221748400853

F1 scores:
[       nan 0.93899931 0.94396552 0.91509434 0.85823755 0.81124498
 0.7244582  0.86060606 0.99232737 1.         1.         1.
 1.         1.        ]

Kappa:
0.9535938753962445
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f968b2c6e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.040]
Epoch [2/120    avg_loss:2.599, val_acc:0.098]
Epoch [3/120    avg_loss:2.568, val_acc:0.131]
Epoch [4/120    avg_loss:2.544, val_acc:0.308]
Epoch [5/120    avg_loss:2.513, val_acc:0.312]
Epoch [6/120    avg_loss:2.491, val_acc:0.304]
Epoch [7/120    avg_loss:2.458, val_acc:0.302]
Epoch [8/120    avg_loss:2.428, val_acc:0.306]
Epoch [9/120    avg_loss:2.405, val_acc:0.312]
Epoch [10/120    avg_loss:2.378, val_acc:0.319]
Epoch [11/120    avg_loss:2.352, val_acc:0.321]
Epoch [12/120    avg_loss:2.321, val_acc:0.329]
Epoch [13/120    avg_loss:2.290, val_acc:0.329]
Epoch [14/120    avg_loss:2.257, val_acc:0.331]
Epoch [15/120    avg_loss:2.234, val_acc:0.342]
Epoch [16/120    avg_loss:2.182, val_acc:0.367]
Epoch [17/120    avg_loss:2.147, val_acc:0.412]
Epoch [18/120    avg_loss:2.102, val_acc:0.454]
Epoch [19/120    avg_loss:2.041, val_acc:0.481]
Epoch [20/120    avg_loss:1.998, val_acc:0.492]
Epoch [21/120    avg_loss:1.938, val_acc:0.529]
Epoch [22/120    avg_loss:1.885, val_acc:0.546]
Epoch [23/120    avg_loss:1.842, val_acc:0.579]
Epoch [24/120    avg_loss:1.788, val_acc:0.610]
Epoch [25/120    avg_loss:1.744, val_acc:0.635]
Epoch [26/120    avg_loss:1.647, val_acc:0.650]
Epoch [27/120    avg_loss:1.568, val_acc:0.721]
Epoch [28/120    avg_loss:1.496, val_acc:0.700]
Epoch [29/120    avg_loss:1.431, val_acc:0.729]
Epoch [30/120    avg_loss:1.411, val_acc:0.727]
Epoch [31/120    avg_loss:1.324, val_acc:0.765]
Epoch [32/120    avg_loss:1.273, val_acc:0.802]
Epoch [33/120    avg_loss:1.175, val_acc:0.829]
Epoch [34/120    avg_loss:1.106, val_acc:0.863]
Epoch [35/120    avg_loss:1.033, val_acc:0.867]
Epoch [36/120    avg_loss:0.974, val_acc:0.852]
Epoch [37/120    avg_loss:0.932, val_acc:0.827]
Epoch [38/120    avg_loss:0.886, val_acc:0.887]
Epoch [39/120    avg_loss:0.836, val_acc:0.894]
Epoch [40/120    avg_loss:0.811, val_acc:0.879]
Epoch [41/120    avg_loss:0.777, val_acc:0.873]
Epoch [42/120    avg_loss:0.712, val_acc:0.904]
Epoch [43/120    avg_loss:0.707, val_acc:0.915]
Epoch [44/120    avg_loss:0.657, val_acc:0.910]
Epoch [45/120    avg_loss:0.573, val_acc:0.929]
Epoch [46/120    avg_loss:0.530, val_acc:0.938]
Epoch [47/120    avg_loss:0.509, val_acc:0.919]
Epoch [48/120    avg_loss:0.522, val_acc:0.900]
Epoch [49/120    avg_loss:0.478, val_acc:0.921]
Epoch [50/120    avg_loss:0.515, val_acc:0.908]
Epoch [51/120    avg_loss:0.601, val_acc:0.925]
Epoch [52/120    avg_loss:0.531, val_acc:0.915]
Epoch [53/120    avg_loss:0.555, val_acc:0.908]
Epoch [54/120    avg_loss:0.458, val_acc:0.935]
Epoch [55/120    avg_loss:0.464, val_acc:0.931]
Epoch [56/120    avg_loss:0.385, val_acc:0.944]
Epoch [57/120    avg_loss:0.365, val_acc:0.935]
Epoch [58/120    avg_loss:0.385, val_acc:0.956]
Epoch [59/120    avg_loss:0.335, val_acc:0.929]
Epoch [60/120    avg_loss:0.376, val_acc:0.944]
Epoch [61/120    avg_loss:0.332, val_acc:0.944]
Epoch [62/120    avg_loss:0.351, val_acc:0.925]
Epoch [63/120    avg_loss:0.330, val_acc:0.938]
Epoch [64/120    avg_loss:0.324, val_acc:0.960]
Epoch [65/120    avg_loss:0.294, val_acc:0.944]
Epoch [66/120    avg_loss:0.289, val_acc:0.885]
Epoch [67/120    avg_loss:0.324, val_acc:0.929]
Epoch [68/120    avg_loss:0.304, val_acc:0.946]
Epoch [69/120    avg_loss:0.292, val_acc:0.938]
Epoch [70/120    avg_loss:0.342, val_acc:0.927]
Epoch [71/120    avg_loss:0.290, val_acc:0.948]
Epoch [72/120    avg_loss:0.292, val_acc:0.952]
Epoch [73/120    avg_loss:0.236, val_acc:0.965]
Epoch [74/120    avg_loss:0.222, val_acc:0.954]
Epoch [75/120    avg_loss:0.215, val_acc:0.971]
Epoch [76/120    avg_loss:0.221, val_acc:0.931]
Epoch [77/120    avg_loss:0.213, val_acc:0.973]
Epoch [78/120    avg_loss:0.196, val_acc:0.969]
Epoch [79/120    avg_loss:0.158, val_acc:0.971]
Epoch [80/120    avg_loss:0.164, val_acc:0.971]
Epoch [81/120    avg_loss:0.156, val_acc:0.971]
Epoch [82/120    avg_loss:0.144, val_acc:0.967]
Epoch [83/120    avg_loss:0.199, val_acc:0.952]
Epoch [84/120    avg_loss:0.257, val_acc:0.954]
Epoch [85/120    avg_loss:0.210, val_acc:0.956]
Epoch [86/120    avg_loss:0.190, val_acc:0.983]
Epoch [87/120    avg_loss:0.148, val_acc:0.967]
Epoch [88/120    avg_loss:0.152, val_acc:0.971]
Epoch [89/120    avg_loss:0.149, val_acc:0.975]
Epoch [90/120    avg_loss:0.153, val_acc:0.977]
Epoch [91/120    avg_loss:0.161, val_acc:0.946]
Epoch [92/120    avg_loss:0.228, val_acc:0.958]
Epoch [93/120    avg_loss:0.160, val_acc:0.948]
Epoch [94/120    avg_loss:0.183, val_acc:0.971]
Epoch [95/120    avg_loss:0.158, val_acc:0.958]
Epoch [96/120    avg_loss:0.132, val_acc:0.971]
Epoch [97/120    avg_loss:0.109, val_acc:0.977]
Epoch [98/120    avg_loss:0.145, val_acc:0.969]
Epoch [99/120    avg_loss:0.176, val_acc:0.971]
Epoch [100/120    avg_loss:0.112, val_acc:0.977]
Epoch [101/120    avg_loss:0.098, val_acc:0.981]
Epoch [102/120    avg_loss:0.099, val_acc:0.981]
Epoch [103/120    avg_loss:0.091, val_acc:0.981]
Epoch [104/120    avg_loss:0.080, val_acc:0.983]
Epoch [105/120    avg_loss:0.089, val_acc:0.988]
Epoch [106/120    avg_loss:0.081, val_acc:0.988]
Epoch [107/120    avg_loss:0.081, val_acc:0.988]
Epoch [108/120    avg_loss:0.072, val_acc:0.983]
Epoch [109/120    avg_loss:0.082, val_acc:0.985]
Epoch [110/120    avg_loss:0.076, val_acc:0.988]
Epoch [111/120    avg_loss:0.074, val_acc:0.985]
Epoch [112/120    avg_loss:0.068, val_acc:0.985]
Epoch [113/120    avg_loss:0.078, val_acc:0.988]
Epoch [114/120    avg_loss:0.081, val_acc:0.988]
Epoch [115/120    avg_loss:0.072, val_acc:0.988]
Epoch [116/120    avg_loss:0.066, val_acc:0.988]
Epoch [117/120    avg_loss:0.076, val_acc:0.988]
Epoch [118/120    avg_loss:0.068, val_acc:0.985]
Epoch [119/120    avg_loss:0.073, val_acc:0.985]
Epoch [120/120    avg_loss:0.071, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   3 224   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 0.99854227 0.96263736 0.98678414 0.91629956 0.88054608
 0.99512195 0.91954023 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.986467599536518
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc59d5aaeb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.046]
Epoch [2/120    avg_loss:2.612, val_acc:0.050]
Epoch [3/120    avg_loss:2.590, val_acc:0.133]
Epoch [4/120    avg_loss:2.568, val_acc:0.131]
Epoch [5/120    avg_loss:2.548, val_acc:0.148]
Epoch [6/120    avg_loss:2.527, val_acc:0.250]
Epoch [7/120    avg_loss:2.498, val_acc:0.304]
Epoch [8/120    avg_loss:2.478, val_acc:0.308]
Epoch [9/120    avg_loss:2.453, val_acc:0.308]
Epoch [10/120    avg_loss:2.423, val_acc:0.312]
Epoch [11/120    avg_loss:2.394, val_acc:0.323]
Epoch [12/120    avg_loss:2.368, val_acc:0.350]
Epoch [13/120    avg_loss:2.329, val_acc:0.348]
Epoch [14/120    avg_loss:2.299, val_acc:0.392]
Epoch [15/120    avg_loss:2.258, val_acc:0.448]
Epoch [16/120    avg_loss:2.217, val_acc:0.469]
Epoch [17/120    avg_loss:2.177, val_acc:0.512]
Epoch [18/120    avg_loss:2.129, val_acc:0.519]
Epoch [19/120    avg_loss:2.082, val_acc:0.550]
Epoch [20/120    avg_loss:2.024, val_acc:0.554]
Epoch [21/120    avg_loss:1.980, val_acc:0.579]
Epoch [22/120    avg_loss:1.926, val_acc:0.625]
Epoch [23/120    avg_loss:1.844, val_acc:0.648]
Epoch [24/120    avg_loss:1.767, val_acc:0.642]
Epoch [25/120    avg_loss:1.706, val_acc:0.650]
Epoch [26/120    avg_loss:1.613, val_acc:0.671]
Epoch [27/120    avg_loss:1.564, val_acc:0.681]
Epoch [28/120    avg_loss:1.475, val_acc:0.717]
Epoch [29/120    avg_loss:1.436, val_acc:0.706]
Epoch [30/120    avg_loss:1.351, val_acc:0.740]
Epoch [31/120    avg_loss:1.255, val_acc:0.750]
Epoch [32/120    avg_loss:1.158, val_acc:0.754]
Epoch [33/120    avg_loss:1.102, val_acc:0.827]
Epoch [34/120    avg_loss:1.062, val_acc:0.817]
Epoch [35/120    avg_loss:1.015, val_acc:0.894]
Epoch [36/120    avg_loss:0.941, val_acc:0.883]
Epoch [37/120    avg_loss:0.900, val_acc:0.919]
Epoch [38/120    avg_loss:0.842, val_acc:0.887]
Epoch [39/120    avg_loss:0.782, val_acc:0.915]
Epoch [40/120    avg_loss:0.802, val_acc:0.931]
Epoch [41/120    avg_loss:0.719, val_acc:0.915]
Epoch [42/120    avg_loss:0.658, val_acc:0.929]
Epoch [43/120    avg_loss:0.679, val_acc:0.879]
Epoch [44/120    avg_loss:0.628, val_acc:0.908]
Epoch [45/120    avg_loss:0.544, val_acc:0.879]
Epoch [46/120    avg_loss:0.566, val_acc:0.933]
Epoch [47/120    avg_loss:0.522, val_acc:0.917]
Epoch [48/120    avg_loss:0.492, val_acc:0.952]
Epoch [49/120    avg_loss:0.516, val_acc:0.942]
Epoch [50/120    avg_loss:0.494, val_acc:0.931]
Epoch [51/120    avg_loss:0.444, val_acc:0.908]
Epoch [52/120    avg_loss:0.423, val_acc:0.952]
Epoch [53/120    avg_loss:0.454, val_acc:0.958]
Epoch [54/120    avg_loss:0.459, val_acc:0.912]
Epoch [55/120    avg_loss:0.434, val_acc:0.958]
Epoch [56/120    avg_loss:0.384, val_acc:0.917]
Epoch [57/120    avg_loss:0.342, val_acc:0.956]
Epoch [58/120    avg_loss:0.338, val_acc:0.958]
Epoch [59/120    avg_loss:0.357, val_acc:0.915]
Epoch [60/120    avg_loss:0.374, val_acc:0.963]
Epoch [61/120    avg_loss:0.327, val_acc:0.967]
Epoch [62/120    avg_loss:0.287, val_acc:0.954]
Epoch [63/120    avg_loss:0.365, val_acc:0.971]
Epoch [64/120    avg_loss:0.314, val_acc:0.963]
Epoch [65/120    avg_loss:0.289, val_acc:0.967]
Epoch [66/120    avg_loss:0.296, val_acc:0.960]
Epoch [67/120    avg_loss:0.257, val_acc:0.965]
Epoch [68/120    avg_loss:0.306, val_acc:0.963]
Epoch [69/120    avg_loss:0.268, val_acc:0.956]
Epoch [70/120    avg_loss:0.218, val_acc:0.977]
Epoch [71/120    avg_loss:0.214, val_acc:0.963]
Epoch [72/120    avg_loss:0.256, val_acc:0.950]
Epoch [73/120    avg_loss:0.250, val_acc:0.967]
Epoch [74/120    avg_loss:0.202, val_acc:0.956]
Epoch [75/120    avg_loss:0.195, val_acc:0.971]
Epoch [76/120    avg_loss:0.197, val_acc:0.967]
Epoch [77/120    avg_loss:0.161, val_acc:0.981]
Epoch [78/120    avg_loss:0.182, val_acc:0.979]
Epoch [79/120    avg_loss:0.181, val_acc:0.967]
Epoch [80/120    avg_loss:0.164, val_acc:0.969]
Epoch [81/120    avg_loss:0.177, val_acc:0.969]
Epoch [82/120    avg_loss:0.170, val_acc:0.977]
Epoch [83/120    avg_loss:0.194, val_acc:0.969]
Epoch [84/120    avg_loss:0.190, val_acc:0.985]
Epoch [85/120    avg_loss:0.146, val_acc:0.979]
Epoch [86/120    avg_loss:0.138, val_acc:0.983]
Epoch [87/120    avg_loss:0.166, val_acc:0.981]
Epoch [88/120    avg_loss:0.121, val_acc:0.979]
Epoch [89/120    avg_loss:0.115, val_acc:0.971]
Epoch [90/120    avg_loss:0.131, val_acc:0.981]
Epoch [91/120    avg_loss:0.135, val_acc:0.992]
Epoch [92/120    avg_loss:0.120, val_acc:0.985]
Epoch [93/120    avg_loss:0.144, val_acc:0.977]
Epoch [94/120    avg_loss:0.165, val_acc:0.985]
Epoch [95/120    avg_loss:0.134, val_acc:0.988]
Epoch [96/120    avg_loss:0.115, val_acc:0.985]
Epoch [97/120    avg_loss:0.140, val_acc:0.967]
Epoch [98/120    avg_loss:0.182, val_acc:0.956]
Epoch [99/120    avg_loss:0.185, val_acc:0.979]
Epoch [100/120    avg_loss:0.122, val_acc:0.977]
Epoch [101/120    avg_loss:0.146, val_acc:0.981]
Epoch [102/120    avg_loss:0.166, val_acc:0.985]
Epoch [103/120    avg_loss:0.158, val_acc:0.975]
Epoch [104/120    avg_loss:0.120, val_acc:0.983]
Epoch [105/120    avg_loss:0.095, val_acc:0.983]
Epoch [106/120    avg_loss:0.080, val_acc:0.988]
Epoch [107/120    avg_loss:0.089, val_acc:0.988]
Epoch [108/120    avg_loss:0.080, val_acc:0.988]
Epoch [109/120    avg_loss:0.073, val_acc:0.988]
Epoch [110/120    avg_loss:0.081, val_acc:0.988]
Epoch [111/120    avg_loss:0.065, val_acc:0.988]
Epoch [112/120    avg_loss:0.068, val_acc:0.988]
Epoch [113/120    avg_loss:0.069, val_acc:0.990]
Epoch [114/120    avg_loss:0.068, val_acc:0.988]
Epoch [115/120    avg_loss:0.076, val_acc:0.985]
Epoch [116/120    avg_loss:0.077, val_acc:0.988]
Epoch [117/120    avg_loss:0.058, val_acc:0.990]
Epoch [118/120    avg_loss:0.068, val_acc:0.990]
Epoch [119/120    avg_loss:0.060, val_acc:0.990]
Epoch [120/120    avg_loss:0.069, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.96902655 0.99343545 0.91855204 0.89180328
 1.         0.91954023 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9881306202715137
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff8a1329e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.106]
Epoch [2/120    avg_loss:2.593, val_acc:0.106]
Epoch [3/120    avg_loss:2.563, val_acc:0.135]
Epoch [4/120    avg_loss:2.533, val_acc:0.244]
Epoch [5/120    avg_loss:2.503, val_acc:0.373]
Epoch [6/120    avg_loss:2.470, val_acc:0.425]
Epoch [7/120    avg_loss:2.443, val_acc:0.419]
Epoch [8/120    avg_loss:2.402, val_acc:0.433]
Epoch [9/120    avg_loss:2.374, val_acc:0.444]
Epoch [10/120    avg_loss:2.347, val_acc:0.452]
Epoch [11/120    avg_loss:2.313, val_acc:0.454]
Epoch [12/120    avg_loss:2.279, val_acc:0.454]
Epoch [13/120    avg_loss:2.251, val_acc:0.446]
Epoch [14/120    avg_loss:2.197, val_acc:0.465]
Epoch [15/120    avg_loss:2.132, val_acc:0.481]
Epoch [16/120    avg_loss:2.116, val_acc:0.460]
Epoch [17/120    avg_loss:2.061, val_acc:0.475]
Epoch [18/120    avg_loss:2.007, val_acc:0.510]
Epoch [19/120    avg_loss:1.954, val_acc:0.537]
Epoch [20/120    avg_loss:1.911, val_acc:0.573]
Epoch [21/120    avg_loss:1.849, val_acc:0.571]
Epoch [22/120    avg_loss:1.809, val_acc:0.600]
Epoch [23/120    avg_loss:1.759, val_acc:0.633]
Epoch [24/120    avg_loss:1.703, val_acc:0.658]
Epoch [25/120    avg_loss:1.675, val_acc:0.694]
Epoch [26/120    avg_loss:1.640, val_acc:0.752]
Epoch [27/120    avg_loss:1.583, val_acc:0.681]
Epoch [28/120    avg_loss:1.537, val_acc:0.785]
Epoch [29/120    avg_loss:1.474, val_acc:0.710]
Epoch [30/120    avg_loss:1.476, val_acc:0.842]
Epoch [31/120    avg_loss:1.382, val_acc:0.831]
Epoch [32/120    avg_loss:1.299, val_acc:0.854]
Epoch [33/120    avg_loss:1.227, val_acc:0.844]
Epoch [34/120    avg_loss:1.177, val_acc:0.846]
Epoch [35/120    avg_loss:1.099, val_acc:0.921]
Epoch [36/120    avg_loss:1.053, val_acc:0.900]
Epoch [37/120    avg_loss:1.002, val_acc:0.863]
Epoch [38/120    avg_loss:0.915, val_acc:0.794]
Epoch [39/120    avg_loss:0.901, val_acc:0.863]
Epoch [40/120    avg_loss:0.826, val_acc:0.933]
Epoch [41/120    avg_loss:0.771, val_acc:0.921]
Epoch [42/120    avg_loss:0.711, val_acc:0.908]
Epoch [43/120    avg_loss:0.724, val_acc:0.940]
Epoch [44/120    avg_loss:0.651, val_acc:0.927]
Epoch [45/120    avg_loss:0.600, val_acc:0.946]
Epoch [46/120    avg_loss:0.558, val_acc:0.935]
Epoch [47/120    avg_loss:0.573, val_acc:0.919]
Epoch [48/120    avg_loss:0.546, val_acc:0.940]
Epoch [49/120    avg_loss:0.496, val_acc:0.956]
Epoch [50/120    avg_loss:0.452, val_acc:0.958]
Epoch [51/120    avg_loss:0.438, val_acc:0.958]
Epoch [52/120    avg_loss:0.453, val_acc:0.963]
Epoch [53/120    avg_loss:0.422, val_acc:0.958]
Epoch [54/120    avg_loss:0.405, val_acc:0.944]
Epoch [55/120    avg_loss:0.383, val_acc:0.956]
Epoch [56/120    avg_loss:0.351, val_acc:0.965]
Epoch [57/120    avg_loss:0.330, val_acc:0.958]
Epoch [58/120    avg_loss:0.311, val_acc:0.948]
Epoch [59/120    avg_loss:0.336, val_acc:0.948]
Epoch [60/120    avg_loss:0.299, val_acc:0.969]
Epoch [61/120    avg_loss:0.389, val_acc:0.948]
Epoch [62/120    avg_loss:0.361, val_acc:0.952]
Epoch [63/120    avg_loss:0.295, val_acc:0.960]
Epoch [64/120    avg_loss:0.333, val_acc:0.942]
Epoch [65/120    avg_loss:0.307, val_acc:0.979]
Epoch [66/120    avg_loss:0.270, val_acc:0.967]
Epoch [67/120    avg_loss:0.258, val_acc:0.950]
Epoch [68/120    avg_loss:0.296, val_acc:0.942]
Epoch [69/120    avg_loss:0.377, val_acc:0.931]
Epoch [70/120    avg_loss:0.295, val_acc:0.965]
Epoch [71/120    avg_loss:0.253, val_acc:0.979]
Epoch [72/120    avg_loss:0.263, val_acc:0.969]
Epoch [73/120    avg_loss:0.272, val_acc:0.960]
Epoch [74/120    avg_loss:0.245, val_acc:0.983]
Epoch [75/120    avg_loss:0.232, val_acc:0.979]
Epoch [76/120    avg_loss:0.230, val_acc:0.931]
Epoch [77/120    avg_loss:0.226, val_acc:0.963]
Epoch [78/120    avg_loss:0.196, val_acc:0.988]
Epoch [79/120    avg_loss:0.166, val_acc:0.977]
Epoch [80/120    avg_loss:0.224, val_acc:0.973]
Epoch [81/120    avg_loss:0.221, val_acc:0.967]
Epoch [82/120    avg_loss:0.216, val_acc:0.958]
Epoch [83/120    avg_loss:0.209, val_acc:0.958]
Epoch [84/120    avg_loss:0.176, val_acc:0.948]
Epoch [85/120    avg_loss:0.171, val_acc:0.979]
Epoch [86/120    avg_loss:0.157, val_acc:0.940]
Epoch [87/120    avg_loss:0.148, val_acc:0.983]
Epoch [88/120    avg_loss:0.120, val_acc:0.979]
Epoch [89/120    avg_loss:0.141, val_acc:0.973]
Epoch [90/120    avg_loss:0.144, val_acc:0.946]
Epoch [91/120    avg_loss:0.127, val_acc:0.988]
Epoch [92/120    avg_loss:0.113, val_acc:0.973]
Epoch [93/120    avg_loss:0.110, val_acc:0.988]
Epoch [94/120    avg_loss:0.106, val_acc:0.985]
Epoch [95/120    avg_loss:0.120, val_acc:0.985]
Epoch [96/120    avg_loss:0.114, val_acc:0.983]
Epoch [97/120    avg_loss:0.137, val_acc:0.985]
Epoch [98/120    avg_loss:0.118, val_acc:0.971]
Epoch [99/120    avg_loss:0.112, val_acc:0.979]
Epoch [100/120    avg_loss:0.120, val_acc:0.979]
Epoch [101/120    avg_loss:0.107, val_acc:0.985]
Epoch [102/120    avg_loss:0.089, val_acc:0.994]
Epoch [103/120    avg_loss:0.077, val_acc:0.996]
Epoch [104/120    avg_loss:0.096, val_acc:0.988]
Epoch [105/120    avg_loss:0.115, val_acc:0.971]
Epoch [106/120    avg_loss:0.225, val_acc:0.971]
Epoch [107/120    avg_loss:0.222, val_acc:0.981]
Epoch [108/120    avg_loss:0.156, val_acc:0.981]
Epoch [109/120    avg_loss:0.133, val_acc:0.950]
Epoch [110/120    avg_loss:0.140, val_acc:0.988]
Epoch [111/120    avg_loss:0.126, val_acc:0.977]
Epoch [112/120    avg_loss:0.075, val_acc:0.992]
Epoch [113/120    avg_loss:0.094, val_acc:0.990]
Epoch [114/120    avg_loss:0.123, val_acc:0.975]
Epoch [115/120    avg_loss:0.094, val_acc:0.985]
Epoch [116/120    avg_loss:0.090, val_acc:0.977]
Epoch [117/120    avg_loss:0.083, val_acc:0.990]
Epoch [118/120    avg_loss:0.058, val_acc:0.990]
Epoch [119/120    avg_loss:0.052, val_acc:0.990]
Epoch [120/120    avg_loss:0.047, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99926954 0.97767857 0.99782135 0.95194508 0.93159609
 0.99757869 0.93854749 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9919293716262194
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdfd2ee2e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.656, val_acc:0.000]
Epoch [2/120    avg_loss:2.623, val_acc:0.243]
Epoch [3/120    avg_loss:2.601, val_acc:0.360]
Epoch [4/120    avg_loss:2.577, val_acc:0.325]
Epoch [5/120    avg_loss:2.551, val_acc:0.240]
Epoch [6/120    avg_loss:2.528, val_acc:0.312]
Epoch [7/120    avg_loss:2.497, val_acc:0.377]
Epoch [8/120    avg_loss:2.468, val_acc:0.431]
Epoch [9/120    avg_loss:2.438, val_acc:0.452]
Epoch [10/120    avg_loss:2.400, val_acc:0.512]
Epoch [11/120    avg_loss:2.364, val_acc:0.504]
Epoch [12/120    avg_loss:2.333, val_acc:0.479]
Epoch [13/120    avg_loss:2.291, val_acc:0.487]
Epoch [14/120    avg_loss:2.268, val_acc:0.502]
Epoch [15/120    avg_loss:2.207, val_acc:0.508]
Epoch [16/120    avg_loss:2.169, val_acc:0.490]
Epoch [17/120    avg_loss:2.114, val_acc:0.535]
Epoch [18/120    avg_loss:2.065, val_acc:0.565]
Epoch [19/120    avg_loss:2.017, val_acc:0.581]
Epoch [20/120    avg_loss:1.958, val_acc:0.608]
Epoch [21/120    avg_loss:1.927, val_acc:0.588]
Epoch [22/120    avg_loss:1.883, val_acc:0.600]
Epoch [23/120    avg_loss:1.808, val_acc:0.644]
Epoch [24/120    avg_loss:1.737, val_acc:0.660]
Epoch [25/120    avg_loss:1.670, val_acc:0.715]
Epoch [26/120    avg_loss:1.598, val_acc:0.721]
Epoch [27/120    avg_loss:1.568, val_acc:0.677]
Epoch [28/120    avg_loss:1.482, val_acc:0.754]
Epoch [29/120    avg_loss:1.401, val_acc:0.744]
Epoch [30/120    avg_loss:1.350, val_acc:0.775]
Epoch [31/120    avg_loss:1.243, val_acc:0.777]
Epoch [32/120    avg_loss:1.218, val_acc:0.815]
Epoch [33/120    avg_loss:1.121, val_acc:0.812]
Epoch [34/120    avg_loss:1.068, val_acc:0.865]
Epoch [35/120    avg_loss:1.031, val_acc:0.873]
Epoch [36/120    avg_loss:0.982, val_acc:0.815]
Epoch [37/120    avg_loss:0.955, val_acc:0.892]
Epoch [38/120    avg_loss:0.878, val_acc:0.892]
Epoch [39/120    avg_loss:0.811, val_acc:0.917]
Epoch [40/120    avg_loss:0.763, val_acc:0.925]
Epoch [41/120    avg_loss:0.708, val_acc:0.929]
Epoch [42/120    avg_loss:0.675, val_acc:0.910]
Epoch [43/120    avg_loss:0.659, val_acc:0.923]
Epoch [44/120    avg_loss:0.630, val_acc:0.933]
Epoch [45/120    avg_loss:0.626, val_acc:0.946]
Epoch [46/120    avg_loss:0.564, val_acc:0.933]
Epoch [47/120    avg_loss:0.587, val_acc:0.921]
Epoch [48/120    avg_loss:0.574, val_acc:0.904]
Epoch [49/120    avg_loss:0.508, val_acc:0.950]
Epoch [50/120    avg_loss:0.472, val_acc:0.950]
Epoch [51/120    avg_loss:0.468, val_acc:0.935]
Epoch [52/120    avg_loss:0.478, val_acc:0.954]
Epoch [53/120    avg_loss:0.421, val_acc:0.938]
Epoch [54/120    avg_loss:0.392, val_acc:0.960]
Epoch [55/120    avg_loss:0.351, val_acc:0.956]
Epoch [56/120    avg_loss:0.310, val_acc:0.963]
Epoch [57/120    avg_loss:0.341, val_acc:0.942]
Epoch [58/120    avg_loss:0.328, val_acc:0.958]
Epoch [59/120    avg_loss:0.301, val_acc:0.956]
Epoch [60/120    avg_loss:0.315, val_acc:0.940]
Epoch [61/120    avg_loss:0.403, val_acc:0.952]
Epoch [62/120    avg_loss:0.392, val_acc:0.944]
Epoch [63/120    avg_loss:0.355, val_acc:0.935]
Epoch [64/120    avg_loss:0.298, val_acc:0.963]
Epoch [65/120    avg_loss:0.279, val_acc:0.948]
Epoch [66/120    avg_loss:0.263, val_acc:0.965]
Epoch [67/120    avg_loss:0.245, val_acc:0.940]
Epoch [68/120    avg_loss:0.301, val_acc:0.958]
Epoch [69/120    avg_loss:0.335, val_acc:0.946]
Epoch [70/120    avg_loss:0.253, val_acc:0.965]
Epoch [71/120    avg_loss:0.222, val_acc:0.952]
Epoch [72/120    avg_loss:0.272, val_acc:0.975]
Epoch [73/120    avg_loss:0.238, val_acc:0.954]
Epoch [74/120    avg_loss:0.208, val_acc:0.969]
Epoch [75/120    avg_loss:0.207, val_acc:0.958]
Epoch [76/120    avg_loss:0.208, val_acc:0.973]
Epoch [77/120    avg_loss:0.184, val_acc:0.963]
Epoch [78/120    avg_loss:0.165, val_acc:0.940]
Epoch [79/120    avg_loss:0.237, val_acc:0.967]
Epoch [80/120    avg_loss:0.240, val_acc:0.956]
Epoch [81/120    avg_loss:0.197, val_acc:0.960]
Epoch [82/120    avg_loss:0.172, val_acc:0.981]
Epoch [83/120    avg_loss:0.187, val_acc:0.971]
Epoch [84/120    avg_loss:0.185, val_acc:0.956]
Epoch [85/120    avg_loss:0.202, val_acc:0.967]
Epoch [86/120    avg_loss:0.183, val_acc:0.944]
Epoch [87/120    avg_loss:0.270, val_acc:0.969]
Epoch [88/120    avg_loss:0.255, val_acc:0.979]
Epoch [89/120    avg_loss:0.202, val_acc:0.969]
Epoch [90/120    avg_loss:0.193, val_acc:0.975]
Epoch [91/120    avg_loss:0.131, val_acc:0.985]
Epoch [92/120    avg_loss:0.136, val_acc:0.988]
Epoch [93/120    avg_loss:0.117, val_acc:0.965]
Epoch [94/120    avg_loss:0.132, val_acc:0.975]
Epoch [95/120    avg_loss:0.141, val_acc:0.981]
Epoch [96/120    avg_loss:0.159, val_acc:0.977]
Epoch [97/120    avg_loss:0.216, val_acc:0.963]
Epoch [98/120    avg_loss:0.208, val_acc:0.979]
Epoch [99/120    avg_loss:0.141, val_acc:0.958]
Epoch [100/120    avg_loss:0.227, val_acc:0.960]
Epoch [101/120    avg_loss:0.194, val_acc:0.985]
Epoch [102/120    avg_loss:0.134, val_acc:0.983]
Epoch [103/120    avg_loss:0.115, val_acc:0.994]
Epoch [104/120    avg_loss:0.089, val_acc:0.985]
Epoch [105/120    avg_loss:0.101, val_acc:0.988]
Epoch [106/120    avg_loss:0.088, val_acc:0.988]
Epoch [107/120    avg_loss:0.114, val_acc:0.981]
Epoch [108/120    avg_loss:0.112, val_acc:0.979]
Epoch [109/120    avg_loss:0.102, val_acc:0.981]
Epoch [110/120    avg_loss:0.111, val_acc:0.981]
Epoch [111/120    avg_loss:0.113, val_acc:0.983]
Epoch [112/120    avg_loss:0.143, val_acc:0.965]
Epoch [113/120    avg_loss:0.178, val_acc:0.958]
Epoch [114/120    avg_loss:0.146, val_acc:0.990]
Epoch [115/120    avg_loss:0.085, val_acc:0.992]
Epoch [116/120    avg_loss:0.110, val_acc:0.981]
Epoch [117/120    avg_loss:0.099, val_acc:0.988]
Epoch [118/120    avg_loss:0.078, val_acc:0.992]
Epoch [119/120    avg_loss:0.060, val_acc:0.992]
Epoch [120/120    avg_loss:0.069, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.97333333 0.97777778 0.92273731 0.91694352
 1.         0.93181818 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9876560468920398
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7effffbacef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.283]
Epoch [2/120    avg_loss:2.584, val_acc:0.367]
Epoch [3/120    avg_loss:2.555, val_acc:0.415]
Epoch [4/120    avg_loss:2.523, val_acc:0.415]
Epoch [5/120    avg_loss:2.492, val_acc:0.383]
Epoch [6/120    avg_loss:2.467, val_acc:0.375]
Epoch [7/120    avg_loss:2.438, val_acc:0.377]
Epoch [8/120    avg_loss:2.395, val_acc:0.373]
Epoch [9/120    avg_loss:2.374, val_acc:0.417]
Epoch [10/120    avg_loss:2.327, val_acc:0.433]
Epoch [11/120    avg_loss:2.288, val_acc:0.421]
Epoch [12/120    avg_loss:2.253, val_acc:0.446]
Epoch [13/120    avg_loss:2.220, val_acc:0.473]
Epoch [14/120    avg_loss:2.169, val_acc:0.573]
Epoch [15/120    avg_loss:2.132, val_acc:0.537]
Epoch [16/120    avg_loss:2.085, val_acc:0.600]
Epoch [17/120    avg_loss:2.035, val_acc:0.602]
Epoch [18/120    avg_loss:1.983, val_acc:0.606]
Epoch [19/120    avg_loss:1.938, val_acc:0.625]
Epoch [20/120    avg_loss:1.856, val_acc:0.631]
Epoch [21/120    avg_loss:1.811, val_acc:0.656]
Epoch [22/120    avg_loss:1.726, val_acc:0.681]
Epoch [23/120    avg_loss:1.656, val_acc:0.644]
Epoch [24/120    avg_loss:1.612, val_acc:0.696]
Epoch [25/120    avg_loss:1.556, val_acc:0.725]
Epoch [26/120    avg_loss:1.454, val_acc:0.765]
Epoch [27/120    avg_loss:1.356, val_acc:0.777]
Epoch [28/120    avg_loss:1.316, val_acc:0.779]
Epoch [29/120    avg_loss:1.255, val_acc:0.752]
Epoch [30/120    avg_loss:1.208, val_acc:0.765]
Epoch [31/120    avg_loss:1.129, val_acc:0.758]
Epoch [32/120    avg_loss:1.094, val_acc:0.860]
Epoch [33/120    avg_loss:0.981, val_acc:0.812]
Epoch [34/120    avg_loss:0.950, val_acc:0.846]
Epoch [35/120    avg_loss:0.871, val_acc:0.842]
Epoch [36/120    avg_loss:0.842, val_acc:0.865]
Epoch [37/120    avg_loss:0.749, val_acc:0.887]
Epoch [38/120    avg_loss:0.765, val_acc:0.871]
Epoch [39/120    avg_loss:0.744, val_acc:0.883]
Epoch [40/120    avg_loss:0.672, val_acc:0.908]
Epoch [41/120    avg_loss:0.709, val_acc:0.842]
Epoch [42/120    avg_loss:0.620, val_acc:0.890]
Epoch [43/120    avg_loss:0.605, val_acc:0.908]
Epoch [44/120    avg_loss:0.601, val_acc:0.877]
Epoch [45/120    avg_loss:0.615, val_acc:0.892]
Epoch [46/120    avg_loss:0.531, val_acc:0.896]
Epoch [47/120    avg_loss:0.514, val_acc:0.912]
Epoch [48/120    avg_loss:0.487, val_acc:0.919]
Epoch [49/120    avg_loss:0.442, val_acc:0.919]
Epoch [50/120    avg_loss:0.392, val_acc:0.927]
Epoch [51/120    avg_loss:0.432, val_acc:0.908]
Epoch [52/120    avg_loss:0.484, val_acc:0.923]
Epoch [53/120    avg_loss:0.400, val_acc:0.931]
Epoch [54/120    avg_loss:0.412, val_acc:0.940]
Epoch [55/120    avg_loss:0.360, val_acc:0.942]
Epoch [56/120    avg_loss:0.380, val_acc:0.933]
Epoch [57/120    avg_loss:0.331, val_acc:0.938]
Epoch [58/120    avg_loss:0.380, val_acc:0.933]
Epoch [59/120    avg_loss:0.346, val_acc:0.904]
Epoch [60/120    avg_loss:0.347, val_acc:0.927]
Epoch [61/120    avg_loss:0.446, val_acc:0.917]
Epoch [62/120    avg_loss:0.349, val_acc:0.956]
Epoch [63/120    avg_loss:0.335, val_acc:0.956]
Epoch [64/120    avg_loss:0.296, val_acc:0.958]
Epoch [65/120    avg_loss:0.279, val_acc:0.963]
Epoch [66/120    avg_loss:0.288, val_acc:0.942]
Epoch [67/120    avg_loss:0.246, val_acc:0.969]
Epoch [68/120    avg_loss:0.220, val_acc:0.954]
Epoch [69/120    avg_loss:0.238, val_acc:0.946]
Epoch [70/120    avg_loss:0.271, val_acc:0.925]
Epoch [71/120    avg_loss:0.338, val_acc:0.931]
Epoch [72/120    avg_loss:0.281, val_acc:0.950]
Epoch [73/120    avg_loss:0.230, val_acc:0.952]
Epoch [74/120    avg_loss:0.192, val_acc:0.956]
Epoch [75/120    avg_loss:0.188, val_acc:0.958]
Epoch [76/120    avg_loss:0.215, val_acc:0.933]
Epoch [77/120    avg_loss:0.272, val_acc:0.958]
Epoch [78/120    avg_loss:0.223, val_acc:0.965]
Epoch [79/120    avg_loss:0.240, val_acc:0.960]
Epoch [80/120    avg_loss:0.226, val_acc:0.952]
Epoch [81/120    avg_loss:0.195, val_acc:0.975]
Epoch [82/120    avg_loss:0.157, val_acc:0.977]
Epoch [83/120    avg_loss:0.153, val_acc:0.979]
Epoch [84/120    avg_loss:0.133, val_acc:0.979]
Epoch [85/120    avg_loss:0.151, val_acc:0.977]
Epoch [86/120    avg_loss:0.125, val_acc:0.981]
Epoch [87/120    avg_loss:0.160, val_acc:0.983]
Epoch [88/120    avg_loss:0.128, val_acc:0.983]
Epoch [89/120    avg_loss:0.141, val_acc:0.979]
Epoch [90/120    avg_loss:0.140, val_acc:0.985]
Epoch [91/120    avg_loss:0.124, val_acc:0.985]
Epoch [92/120    avg_loss:0.115, val_acc:0.985]
Epoch [93/120    avg_loss:0.119, val_acc:0.985]
Epoch [94/120    avg_loss:0.117, val_acc:0.990]
Epoch [95/120    avg_loss:0.120, val_acc:0.988]
Epoch [96/120    avg_loss:0.126, val_acc:0.990]
Epoch [97/120    avg_loss:0.116, val_acc:0.985]
Epoch [98/120    avg_loss:0.137, val_acc:0.990]
Epoch [99/120    avg_loss:0.133, val_acc:0.994]
Epoch [100/120    avg_loss:0.115, val_acc:0.992]
Epoch [101/120    avg_loss:0.110, val_acc:0.992]
Epoch [102/120    avg_loss:0.124, val_acc:0.992]
Epoch [103/120    avg_loss:0.118, val_acc:0.992]
Epoch [104/120    avg_loss:0.104, val_acc:0.994]
Epoch [105/120    avg_loss:0.098, val_acc:0.990]
Epoch [106/120    avg_loss:0.108, val_acc:0.994]
Epoch [107/120    avg_loss:0.111, val_acc:0.994]
Epoch [108/120    avg_loss:0.112, val_acc:0.996]
Epoch [109/120    avg_loss:0.100, val_acc:0.994]
Epoch [110/120    avg_loss:0.099, val_acc:0.992]
Epoch [111/120    avg_loss:0.108, val_acc:0.994]
Epoch [112/120    avg_loss:0.097, val_acc:0.994]
Epoch [113/120    avg_loss:0.102, val_acc:0.992]
Epoch [114/120    avg_loss:0.094, val_acc:0.998]
Epoch [115/120    avg_loss:0.095, val_acc:0.994]
Epoch [116/120    avg_loss:0.095, val_acc:0.994]
Epoch [117/120    avg_loss:0.107, val_acc:0.996]
Epoch [118/120    avg_loss:0.107, val_acc:0.994]
Epoch [119/120    avg_loss:0.105, val_acc:0.996]
Epoch [120/120    avg_loss:0.097, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.96347032 0.99563319 0.9543379  0.93464052
 1.         0.92063492 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9912174240476475
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97569afe10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.670, val_acc:0.015]
Epoch [2/120    avg_loss:2.633, val_acc:0.115]
Epoch [3/120    avg_loss:2.600, val_acc:0.208]
Epoch [4/120    avg_loss:2.572, val_acc:0.208]
Epoch [5/120    avg_loss:2.548, val_acc:0.344]
Epoch [6/120    avg_loss:2.524, val_acc:0.419]
Epoch [7/120    avg_loss:2.503, val_acc:0.423]
Epoch [8/120    avg_loss:2.484, val_acc:0.421]
Epoch [9/120    avg_loss:2.451, val_acc:0.419]
Epoch [10/120    avg_loss:2.433, val_acc:0.431]
Epoch [11/120    avg_loss:2.401, val_acc:0.444]
Epoch [12/120    avg_loss:2.378, val_acc:0.450]
Epoch [13/120    avg_loss:2.357, val_acc:0.479]
Epoch [14/120    avg_loss:2.327, val_acc:0.500]
Epoch [15/120    avg_loss:2.294, val_acc:0.525]
Epoch [16/120    avg_loss:2.264, val_acc:0.523]
Epoch [17/120    avg_loss:2.241, val_acc:0.544]
Epoch [18/120    avg_loss:2.202, val_acc:0.560]
Epoch [19/120    avg_loss:2.161, val_acc:0.573]
Epoch [20/120    avg_loss:2.128, val_acc:0.567]
Epoch [21/120    avg_loss:2.091, val_acc:0.573]
Epoch [22/120    avg_loss:2.031, val_acc:0.571]
Epoch [23/120    avg_loss:1.980, val_acc:0.569]
Epoch [24/120    avg_loss:1.946, val_acc:0.573]
Epoch [25/120    avg_loss:1.884, val_acc:0.583]
Epoch [26/120    avg_loss:1.811, val_acc:0.606]
Epoch [27/120    avg_loss:1.739, val_acc:0.610]
Epoch [28/120    avg_loss:1.758, val_acc:0.604]
Epoch [29/120    avg_loss:1.660, val_acc:0.588]
Epoch [30/120    avg_loss:1.656, val_acc:0.594]
Epoch [31/120    avg_loss:1.554, val_acc:0.604]
Epoch [32/120    avg_loss:1.476, val_acc:0.613]
Epoch [33/120    avg_loss:1.426, val_acc:0.692]
Epoch [34/120    avg_loss:1.367, val_acc:0.694]
Epoch [35/120    avg_loss:1.277, val_acc:0.758]
Epoch [36/120    avg_loss:1.214, val_acc:0.777]
Epoch [37/120    avg_loss:1.138, val_acc:0.719]
Epoch [38/120    avg_loss:1.076, val_acc:0.867]
Epoch [39/120    avg_loss:1.031, val_acc:0.833]
Epoch [40/120    avg_loss:1.012, val_acc:0.892]
Epoch [41/120    avg_loss:0.928, val_acc:0.860]
Epoch [42/120    avg_loss:0.939, val_acc:0.894]
Epoch [43/120    avg_loss:0.819, val_acc:0.896]
Epoch [44/120    avg_loss:0.788, val_acc:0.896]
Epoch [45/120    avg_loss:0.778, val_acc:0.881]
Epoch [46/120    avg_loss:0.749, val_acc:0.902]
Epoch [47/120    avg_loss:0.688, val_acc:0.910]
Epoch [48/120    avg_loss:0.645, val_acc:0.910]
Epoch [49/120    avg_loss:0.584, val_acc:0.917]
Epoch [50/120    avg_loss:0.588, val_acc:0.915]
Epoch [51/120    avg_loss:0.566, val_acc:0.912]
Epoch [52/120    avg_loss:0.569, val_acc:0.931]
Epoch [53/120    avg_loss:0.507, val_acc:0.923]
Epoch [54/120    avg_loss:0.496, val_acc:0.927]
Epoch [55/120    avg_loss:0.519, val_acc:0.919]
Epoch [56/120    avg_loss:0.468, val_acc:0.923]
Epoch [57/120    avg_loss:0.435, val_acc:0.940]
Epoch [58/120    avg_loss:0.389, val_acc:0.931]
Epoch [59/120    avg_loss:0.391, val_acc:0.938]
Epoch [60/120    avg_loss:0.412, val_acc:0.944]
Epoch [61/120    avg_loss:0.381, val_acc:0.938]
Epoch [62/120    avg_loss:0.362, val_acc:0.946]
Epoch [63/120    avg_loss:0.349, val_acc:0.940]
Epoch [64/120    avg_loss:0.309, val_acc:0.942]
Epoch [65/120    avg_loss:0.312, val_acc:0.946]
Epoch [66/120    avg_loss:0.303, val_acc:0.933]
Epoch [67/120    avg_loss:0.322, val_acc:0.929]
Epoch [68/120    avg_loss:0.361, val_acc:0.940]
Epoch [69/120    avg_loss:0.370, val_acc:0.933]
Epoch [70/120    avg_loss:0.317, val_acc:0.960]
Epoch [71/120    avg_loss:0.294, val_acc:0.956]
Epoch [72/120    avg_loss:0.259, val_acc:0.956]
Epoch [73/120    avg_loss:0.233, val_acc:0.938]
Epoch [74/120    avg_loss:0.252, val_acc:0.956]
Epoch [75/120    avg_loss:0.266, val_acc:0.971]
Epoch [76/120    avg_loss:0.246, val_acc:0.940]
Epoch [77/120    avg_loss:0.228, val_acc:0.977]
Epoch [78/120    avg_loss:0.217, val_acc:0.971]
Epoch [79/120    avg_loss:0.195, val_acc:0.965]
Epoch [80/120    avg_loss:0.212, val_acc:0.975]
Epoch [81/120    avg_loss:0.168, val_acc:0.981]
Epoch [82/120    avg_loss:0.159, val_acc:0.979]
Epoch [83/120    avg_loss:0.194, val_acc:0.960]
Epoch [84/120    avg_loss:0.190, val_acc:0.971]
Epoch [85/120    avg_loss:0.178, val_acc:0.958]
Epoch [86/120    avg_loss:0.251, val_acc:0.956]
Epoch [87/120    avg_loss:0.210, val_acc:0.971]
Epoch [88/120    avg_loss:0.193, val_acc:0.973]
Epoch [89/120    avg_loss:0.159, val_acc:0.973]
Epoch [90/120    avg_loss:0.194, val_acc:0.958]
Epoch [91/120    avg_loss:0.217, val_acc:0.965]
Epoch [92/120    avg_loss:0.157, val_acc:0.981]
Epoch [93/120    avg_loss:0.137, val_acc:0.979]
Epoch [94/120    avg_loss:0.198, val_acc:0.975]
Epoch [95/120    avg_loss:0.147, val_acc:0.985]
Epoch [96/120    avg_loss:0.126, val_acc:0.996]
Epoch [97/120    avg_loss:0.125, val_acc:0.985]
Epoch [98/120    avg_loss:0.147, val_acc:0.994]
Epoch [99/120    avg_loss:0.138, val_acc:0.975]
Epoch [100/120    avg_loss:0.109, val_acc:0.994]
Epoch [101/120    avg_loss:0.122, val_acc:0.977]
Epoch [102/120    avg_loss:0.121, val_acc:0.971]
Epoch [103/120    avg_loss:0.147, val_acc:0.988]
Epoch [104/120    avg_loss:0.130, val_acc:0.979]
Epoch [105/120    avg_loss:0.136, val_acc:0.983]
Epoch [106/120    avg_loss:0.104, val_acc:0.985]
Epoch [107/120    avg_loss:0.092, val_acc:0.975]
Epoch [108/120    avg_loss:0.111, val_acc:0.981]
Epoch [109/120    avg_loss:0.127, val_acc:0.975]
Epoch [110/120    avg_loss:0.105, val_acc:0.990]
Epoch [111/120    avg_loss:0.079, val_acc:0.998]
Epoch [112/120    avg_loss:0.078, val_acc:0.992]
Epoch [113/120    avg_loss:0.060, val_acc:0.992]
Epoch [114/120    avg_loss:0.059, val_acc:0.992]
Epoch [115/120    avg_loss:0.062, val_acc:0.998]
Epoch [116/120    avg_loss:0.067, val_acc:0.992]
Epoch [117/120    avg_loss:0.059, val_acc:0.994]
Epoch [118/120    avg_loss:0.059, val_acc:0.994]
Epoch [119/120    avg_loss:0.070, val_acc:0.992]
Epoch [120/120    avg_loss:0.064, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.98426966 0.99343545 0.93986637 0.91946309
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919289447497649
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83ab4dfdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.654, val_acc:0.023]
Epoch [2/120    avg_loss:2.612, val_acc:0.087]
Epoch [3/120    avg_loss:2.576, val_acc:0.210]
Epoch [4/120    avg_loss:2.544, val_acc:0.269]
Epoch [5/120    avg_loss:2.513, val_acc:0.294]
Epoch [6/120    avg_loss:2.492, val_acc:0.294]
Epoch [7/120    avg_loss:2.458, val_acc:0.294]
Epoch [8/120    avg_loss:2.429, val_acc:0.294]
Epoch [9/120    avg_loss:2.397, val_acc:0.294]
Epoch [10/120    avg_loss:2.372, val_acc:0.294]
Epoch [11/120    avg_loss:2.345, val_acc:0.294]
Epoch [12/120    avg_loss:2.314, val_acc:0.298]
Epoch [13/120    avg_loss:2.285, val_acc:0.350]
Epoch [14/120    avg_loss:2.233, val_acc:0.360]
Epoch [15/120    avg_loss:2.198, val_acc:0.381]
Epoch [16/120    avg_loss:2.150, val_acc:0.423]
Epoch [17/120    avg_loss:2.108, val_acc:0.494]
Epoch [18/120    avg_loss:2.073, val_acc:0.533]
Epoch [19/120    avg_loss:2.012, val_acc:0.554]
Epoch [20/120    avg_loss:1.944, val_acc:0.537]
Epoch [21/120    avg_loss:1.882, val_acc:0.581]
Epoch [22/120    avg_loss:1.803, val_acc:0.590]
Epoch [23/120    avg_loss:1.748, val_acc:0.579]
Epoch [24/120    avg_loss:1.665, val_acc:0.588]
Epoch [25/120    avg_loss:1.631, val_acc:0.594]
Epoch [26/120    avg_loss:1.557, val_acc:0.588]
Epoch [27/120    avg_loss:1.521, val_acc:0.602]
Epoch [28/120    avg_loss:1.423, val_acc:0.606]
Epoch [29/120    avg_loss:1.352, val_acc:0.592]
Epoch [30/120    avg_loss:1.282, val_acc:0.615]
Epoch [31/120    avg_loss:1.239, val_acc:0.660]
Epoch [32/120    avg_loss:1.193, val_acc:0.671]
Epoch [33/120    avg_loss:1.127, val_acc:0.696]
Epoch [34/120    avg_loss:1.098, val_acc:0.756]
Epoch [35/120    avg_loss:1.038, val_acc:0.742]
Epoch [36/120    avg_loss:0.999, val_acc:0.792]
Epoch [37/120    avg_loss:0.924, val_acc:0.838]
Epoch [38/120    avg_loss:0.877, val_acc:0.873]
Epoch [39/120    avg_loss:0.852, val_acc:0.842]
Epoch [40/120    avg_loss:0.800, val_acc:0.894]
Epoch [41/120    avg_loss:0.763, val_acc:0.896]
Epoch [42/120    avg_loss:0.731, val_acc:0.921]
Epoch [43/120    avg_loss:0.718, val_acc:0.921]
Epoch [44/120    avg_loss:0.680, val_acc:0.925]
Epoch [45/120    avg_loss:0.643, val_acc:0.900]
Epoch [46/120    avg_loss:0.596, val_acc:0.938]
Epoch [47/120    avg_loss:0.529, val_acc:0.921]
Epoch [48/120    avg_loss:0.546, val_acc:0.912]
Epoch [49/120    avg_loss:0.510, val_acc:0.944]
Epoch [50/120    avg_loss:0.452, val_acc:0.931]
Epoch [51/120    avg_loss:0.480, val_acc:0.927]
Epoch [52/120    avg_loss:0.473, val_acc:0.942]
Epoch [53/120    avg_loss:0.504, val_acc:0.919]
Epoch [54/120    avg_loss:0.460, val_acc:0.927]
Epoch [55/120    avg_loss:0.410, val_acc:0.956]
Epoch [56/120    avg_loss:0.388, val_acc:0.933]
Epoch [57/120    avg_loss:0.348, val_acc:0.950]
Epoch [58/120    avg_loss:0.349, val_acc:0.944]
Epoch [59/120    avg_loss:0.378, val_acc:0.948]
Epoch [60/120    avg_loss:0.396, val_acc:0.931]
Epoch [61/120    avg_loss:0.328, val_acc:0.946]
Epoch [62/120    avg_loss:0.332, val_acc:0.944]
Epoch [63/120    avg_loss:0.291, val_acc:0.938]
Epoch [64/120    avg_loss:0.320, val_acc:0.944]
Epoch [65/120    avg_loss:0.317, val_acc:0.952]
Epoch [66/120    avg_loss:0.292, val_acc:0.944]
Epoch [67/120    avg_loss:0.297, val_acc:0.958]
Epoch [68/120    avg_loss:0.308, val_acc:0.954]
Epoch [69/120    avg_loss:0.272, val_acc:0.960]
Epoch [70/120    avg_loss:0.265, val_acc:0.958]
Epoch [71/120    avg_loss:0.249, val_acc:0.969]
Epoch [72/120    avg_loss:0.218, val_acc:0.954]
Epoch [73/120    avg_loss:0.260, val_acc:0.954]
Epoch [74/120    avg_loss:0.255, val_acc:0.954]
Epoch [75/120    avg_loss:0.198, val_acc:0.969]
Epoch [76/120    avg_loss:0.183, val_acc:0.960]
Epoch [77/120    avg_loss:0.227, val_acc:0.963]
Epoch [78/120    avg_loss:0.216, val_acc:0.965]
Epoch [79/120    avg_loss:0.222, val_acc:0.981]
Epoch [80/120    avg_loss:0.211, val_acc:0.965]
Epoch [81/120    avg_loss:0.192, val_acc:0.958]
Epoch [82/120    avg_loss:0.211, val_acc:0.973]
Epoch [83/120    avg_loss:0.194, val_acc:0.963]
Epoch [84/120    avg_loss:0.189, val_acc:0.963]
Epoch [85/120    avg_loss:0.172, val_acc:0.977]
Epoch [86/120    avg_loss:0.196, val_acc:0.975]
Epoch [87/120    avg_loss:0.225, val_acc:0.979]
Epoch [88/120    avg_loss:0.181, val_acc:0.977]
Epoch [89/120    avg_loss:0.156, val_acc:0.969]
Epoch [90/120    avg_loss:0.157, val_acc:0.954]
Epoch [91/120    avg_loss:0.161, val_acc:0.971]
Epoch [92/120    avg_loss:0.153, val_acc:0.975]
Epoch [93/120    avg_loss:0.110, val_acc:0.981]
Epoch [94/120    avg_loss:0.090, val_acc:0.981]
Epoch [95/120    avg_loss:0.111, val_acc:0.983]
Epoch [96/120    avg_loss:0.101, val_acc:0.979]
Epoch [97/120    avg_loss:0.083, val_acc:0.981]
Epoch [98/120    avg_loss:0.075, val_acc:0.981]
Epoch [99/120    avg_loss:0.091, val_acc:0.981]
Epoch [100/120    avg_loss:0.087, val_acc:0.985]
Epoch [101/120    avg_loss:0.101, val_acc:0.985]
Epoch [102/120    avg_loss:0.084, val_acc:0.988]
Epoch [103/120    avg_loss:0.080, val_acc:0.985]
Epoch [104/120    avg_loss:0.081, val_acc:0.988]
Epoch [105/120    avg_loss:0.081, val_acc:0.990]
Epoch [106/120    avg_loss:0.090, val_acc:0.988]
Epoch [107/120    avg_loss:0.081, val_acc:0.988]
Epoch [108/120    avg_loss:0.076, val_acc:0.988]
Epoch [109/120    avg_loss:0.083, val_acc:0.988]
Epoch [110/120    avg_loss:0.077, val_acc:0.990]
Epoch [111/120    avg_loss:0.071, val_acc:0.990]
Epoch [112/120    avg_loss:0.079, val_acc:0.990]
Epoch [113/120    avg_loss:0.070, val_acc:0.992]
Epoch [114/120    avg_loss:0.076, val_acc:0.990]
Epoch [115/120    avg_loss:0.070, val_acc:0.988]
Epoch [116/120    avg_loss:0.082, val_acc:0.985]
Epoch [117/120    avg_loss:0.070, val_acc:0.990]
Epoch [118/120    avg_loss:0.094, val_acc:0.990]
Epoch [119/120    avg_loss:0.074, val_acc:0.985]
Epoch [120/120    avg_loss:0.077, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 214  16   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.97767857 0.96396396 0.90949227 0.91856678
 1.         0.94382022 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9878936634546753
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71ae9f6eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.684, val_acc:0.041]
Epoch [2/120    avg_loss:2.655, val_acc:0.035]
Epoch [3/120    avg_loss:2.626, val_acc:0.044]
Epoch [4/120    avg_loss:2.601, val_acc:0.254]
Epoch [5/120    avg_loss:2.571, val_acc:0.302]
Epoch [6/120    avg_loss:2.550, val_acc:0.304]
Epoch [7/120    avg_loss:2.521, val_acc:0.310]
Epoch [8/120    avg_loss:2.499, val_acc:0.321]
Epoch [9/120    avg_loss:2.478, val_acc:0.325]
Epoch [10/120    avg_loss:2.449, val_acc:0.331]
Epoch [11/120    avg_loss:2.421, val_acc:0.354]
Epoch [12/120    avg_loss:2.395, val_acc:0.381]
Epoch [13/120    avg_loss:2.362, val_acc:0.404]
Epoch [14/120    avg_loss:2.339, val_acc:0.431]
Epoch [15/120    avg_loss:2.299, val_acc:0.440]
Epoch [16/120    avg_loss:2.269, val_acc:0.456]
Epoch [17/120    avg_loss:2.236, val_acc:0.475]
Epoch [18/120    avg_loss:2.191, val_acc:0.512]
Epoch [19/120    avg_loss:2.169, val_acc:0.537]
Epoch [20/120    avg_loss:2.126, val_acc:0.585]
Epoch [21/120    avg_loss:2.062, val_acc:0.613]
Epoch [22/120    avg_loss:2.015, val_acc:0.631]
Epoch [23/120    avg_loss:1.947, val_acc:0.635]
Epoch [24/120    avg_loss:1.905, val_acc:0.654]
Epoch [25/120    avg_loss:1.843, val_acc:0.656]
Epoch [26/120    avg_loss:1.789, val_acc:0.679]
Epoch [27/120    avg_loss:1.689, val_acc:0.694]
Epoch [28/120    avg_loss:1.620, val_acc:0.708]
Epoch [29/120    avg_loss:1.594, val_acc:0.717]
Epoch [30/120    avg_loss:1.515, val_acc:0.727]
Epoch [31/120    avg_loss:1.433, val_acc:0.727]
Epoch [32/120    avg_loss:1.368, val_acc:0.777]
Epoch [33/120    avg_loss:1.308, val_acc:0.750]
Epoch [34/120    avg_loss:1.242, val_acc:0.762]
Epoch [35/120    avg_loss:1.192, val_acc:0.817]
Epoch [36/120    avg_loss:1.102, val_acc:0.852]
Epoch [37/120    avg_loss:1.068, val_acc:0.840]
Epoch [38/120    avg_loss:0.964, val_acc:0.904]
Epoch [39/120    avg_loss:0.921, val_acc:0.904]
Epoch [40/120    avg_loss:0.887, val_acc:0.896]
Epoch [41/120    avg_loss:0.845, val_acc:0.898]
Epoch [42/120    avg_loss:0.811, val_acc:0.873]
Epoch [43/120    avg_loss:0.722, val_acc:0.919]
Epoch [44/120    avg_loss:0.724, val_acc:0.881]
Epoch [45/120    avg_loss:0.699, val_acc:0.835]
Epoch [46/120    avg_loss:0.664, val_acc:0.894]
Epoch [47/120    avg_loss:0.619, val_acc:0.898]
Epoch [48/120    avg_loss:0.599, val_acc:0.923]
Epoch [49/120    avg_loss:0.547, val_acc:0.929]
Epoch [50/120    avg_loss:0.505, val_acc:0.917]
Epoch [51/120    avg_loss:0.456, val_acc:0.931]
Epoch [52/120    avg_loss:0.452, val_acc:0.908]
Epoch [53/120    avg_loss:0.448, val_acc:0.940]
Epoch [54/120    avg_loss:0.409, val_acc:0.935]
Epoch [55/120    avg_loss:0.392, val_acc:0.935]
Epoch [56/120    avg_loss:0.420, val_acc:0.950]
Epoch [57/120    avg_loss:0.413, val_acc:0.940]
Epoch [58/120    avg_loss:0.328, val_acc:0.946]
Epoch [59/120    avg_loss:0.312, val_acc:0.952]
Epoch [60/120    avg_loss:0.322, val_acc:0.935]
Epoch [61/120    avg_loss:0.366, val_acc:0.929]
Epoch [62/120    avg_loss:0.399, val_acc:0.938]
Epoch [63/120    avg_loss:0.356, val_acc:0.908]
Epoch [64/120    avg_loss:0.433, val_acc:0.927]
Epoch [65/120    avg_loss:0.427, val_acc:0.925]
Epoch [66/120    avg_loss:0.333, val_acc:0.965]
Epoch [67/120    avg_loss:0.300, val_acc:0.952]
Epoch [68/120    avg_loss:0.264, val_acc:0.942]
Epoch [69/120    avg_loss:0.237, val_acc:0.948]
Epoch [70/120    avg_loss:0.276, val_acc:0.940]
Epoch [71/120    avg_loss:0.293, val_acc:0.948]
Epoch [72/120    avg_loss:0.296, val_acc:0.952]
Epoch [73/120    avg_loss:0.276, val_acc:0.944]
Epoch [74/120    avg_loss:0.261, val_acc:0.960]
Epoch [75/120    avg_loss:0.211, val_acc:0.975]
Epoch [76/120    avg_loss:0.188, val_acc:0.963]
Epoch [77/120    avg_loss:0.188, val_acc:0.960]
Epoch [78/120    avg_loss:0.179, val_acc:0.963]
Epoch [79/120    avg_loss:0.194, val_acc:0.958]
Epoch [80/120    avg_loss:0.162, val_acc:0.973]
Epoch [81/120    avg_loss:0.159, val_acc:0.977]
Epoch [82/120    avg_loss:0.159, val_acc:0.960]
Epoch [83/120    avg_loss:0.191, val_acc:0.954]
Epoch [84/120    avg_loss:0.145, val_acc:0.963]
Epoch [85/120    avg_loss:0.147, val_acc:0.969]
Epoch [86/120    avg_loss:0.171, val_acc:0.942]
Epoch [87/120    avg_loss:0.180, val_acc:0.977]
Epoch [88/120    avg_loss:0.108, val_acc:0.969]
Epoch [89/120    avg_loss:0.131, val_acc:0.971]
Epoch [90/120    avg_loss:0.103, val_acc:0.973]
Epoch [91/120    avg_loss:0.112, val_acc:0.969]
Epoch [92/120    avg_loss:0.100, val_acc:0.969]
Epoch [93/120    avg_loss:0.099, val_acc:0.975]
Epoch [94/120    avg_loss:0.123, val_acc:0.956]
Epoch [95/120    avg_loss:0.094, val_acc:0.975]
Epoch [96/120    avg_loss:0.141, val_acc:0.975]
Epoch [97/120    avg_loss:0.126, val_acc:0.960]
Epoch [98/120    avg_loss:0.151, val_acc:0.971]
Epoch [99/120    avg_loss:0.108, val_acc:0.979]
Epoch [100/120    avg_loss:0.094, val_acc:0.969]
Epoch [101/120    avg_loss:0.106, val_acc:0.977]
Epoch [102/120    avg_loss:0.095, val_acc:0.979]
Epoch [103/120    avg_loss:0.120, val_acc:0.963]
Epoch [104/120    avg_loss:0.114, val_acc:0.983]
Epoch [105/120    avg_loss:0.107, val_acc:0.977]
Epoch [106/120    avg_loss:0.080, val_acc:0.983]
Epoch [107/120    avg_loss:0.099, val_acc:0.973]
Epoch [108/120    avg_loss:0.093, val_acc:0.979]
Epoch [109/120    avg_loss:0.111, val_acc:0.977]
Epoch [110/120    avg_loss:0.112, val_acc:0.981]
Epoch [111/120    avg_loss:0.114, val_acc:0.965]
Epoch [112/120    avg_loss:0.195, val_acc:0.971]
Epoch [113/120    avg_loss:0.133, val_acc:0.969]
Epoch [114/120    avg_loss:0.126, val_acc:0.975]
Epoch [115/120    avg_loss:0.090, val_acc:0.983]
Epoch [116/120    avg_loss:0.102, val_acc:0.977]
Epoch [117/120    avg_loss:0.075, val_acc:0.992]
Epoch [118/120    avg_loss:0.066, val_acc:0.981]
Epoch [119/120    avg_loss:0.060, val_acc:0.981]
Epoch [120/120    avg_loss:0.076, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 212  16   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99926954 0.98426966 0.95927602 0.90322581 0.90169492
 0.99757869 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9874189955806293
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbabd1f4e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.635, val_acc:0.139]
Epoch [2/120    avg_loss:2.607, val_acc:0.135]
Epoch [3/120    avg_loss:2.582, val_acc:0.215]
Epoch [4/120    avg_loss:2.552, val_acc:0.310]
Epoch [5/120    avg_loss:2.527, val_acc:0.354]
Epoch [6/120    avg_loss:2.508, val_acc:0.362]
Epoch [7/120    avg_loss:2.475, val_acc:0.367]
Epoch [8/120    avg_loss:2.452, val_acc:0.377]
Epoch [9/120    avg_loss:2.424, val_acc:0.381]
Epoch [10/120    avg_loss:2.387, val_acc:0.379]
Epoch [11/120    avg_loss:2.359, val_acc:0.394]
Epoch [12/120    avg_loss:2.325, val_acc:0.427]
Epoch [13/120    avg_loss:2.289, val_acc:0.494]
Epoch [14/120    avg_loss:2.242, val_acc:0.496]
Epoch [15/120    avg_loss:2.197, val_acc:0.498]
Epoch [16/120    avg_loss:2.149, val_acc:0.521]
Epoch [17/120    avg_loss:2.092, val_acc:0.552]
Epoch [18/120    avg_loss:2.035, val_acc:0.562]
Epoch [19/120    avg_loss:2.004, val_acc:0.644]
Epoch [20/120    avg_loss:1.933, val_acc:0.658]
Epoch [21/120    avg_loss:1.867, val_acc:0.617]
Epoch [22/120    avg_loss:1.822, val_acc:0.662]
Epoch [23/120    avg_loss:1.730, val_acc:0.721]
Epoch [24/120    avg_loss:1.674, val_acc:0.696]
Epoch [25/120    avg_loss:1.640, val_acc:0.713]
Epoch [26/120    avg_loss:1.550, val_acc:0.746]
Epoch [27/120    avg_loss:1.468, val_acc:0.729]
Epoch [28/120    avg_loss:1.414, val_acc:0.748]
Epoch [29/120    avg_loss:1.341, val_acc:0.748]
Epoch [30/120    avg_loss:1.258, val_acc:0.767]
Epoch [31/120    avg_loss:1.200, val_acc:0.825]
Epoch [32/120    avg_loss:1.134, val_acc:0.825]
Epoch [33/120    avg_loss:1.042, val_acc:0.860]
Epoch [34/120    avg_loss:0.977, val_acc:0.860]
Epoch [35/120    avg_loss:0.964, val_acc:0.863]
Epoch [36/120    avg_loss:0.872, val_acc:0.912]
Epoch [37/120    avg_loss:0.819, val_acc:0.908]
Epoch [38/120    avg_loss:0.832, val_acc:0.898]
Epoch [39/120    avg_loss:0.815, val_acc:0.919]
Epoch [40/120    avg_loss:0.754, val_acc:0.925]
Epoch [41/120    avg_loss:0.762, val_acc:0.890]
Epoch [42/120    avg_loss:0.785, val_acc:0.910]
Epoch [43/120    avg_loss:0.664, val_acc:0.921]
Epoch [44/120    avg_loss:0.623, val_acc:0.940]
Epoch [45/120    avg_loss:0.577, val_acc:0.931]
Epoch [46/120    avg_loss:0.544, val_acc:0.910]
Epoch [47/120    avg_loss:0.530, val_acc:0.942]
Epoch [48/120    avg_loss:0.522, val_acc:0.942]
Epoch [49/120    avg_loss:0.500, val_acc:0.927]
Epoch [50/120    avg_loss:0.450, val_acc:0.956]
Epoch [51/120    avg_loss:0.432, val_acc:0.958]
Epoch [52/120    avg_loss:0.456, val_acc:0.960]
Epoch [53/120    avg_loss:0.393, val_acc:0.948]
Epoch [54/120    avg_loss:0.369, val_acc:0.963]
Epoch [55/120    avg_loss:0.357, val_acc:0.954]
Epoch [56/120    avg_loss:0.338, val_acc:0.950]
Epoch [57/120    avg_loss:0.315, val_acc:0.950]
Epoch [58/120    avg_loss:0.327, val_acc:0.950]
Epoch [59/120    avg_loss:0.320, val_acc:0.952]
Epoch [60/120    avg_loss:0.297, val_acc:0.879]
Epoch [61/120    avg_loss:0.395, val_acc:0.952]
Epoch [62/120    avg_loss:0.374, val_acc:0.946]
Epoch [63/120    avg_loss:0.316, val_acc:0.960]
Epoch [64/120    avg_loss:0.343, val_acc:0.969]
Epoch [65/120    avg_loss:0.272, val_acc:0.979]
Epoch [66/120    avg_loss:0.244, val_acc:0.969]
Epoch [67/120    avg_loss:0.226, val_acc:0.958]
Epoch [68/120    avg_loss:0.259, val_acc:0.950]
Epoch [69/120    avg_loss:0.247, val_acc:0.971]
Epoch [70/120    avg_loss:0.235, val_acc:0.950]
Epoch [71/120    avg_loss:0.193, val_acc:0.944]
Epoch [72/120    avg_loss:0.221, val_acc:0.963]
Epoch [73/120    avg_loss:0.234, val_acc:0.956]
Epoch [74/120    avg_loss:0.222, val_acc:0.973]
Epoch [75/120    avg_loss:0.172, val_acc:0.958]
Epoch [76/120    avg_loss:0.192, val_acc:0.967]
Epoch [77/120    avg_loss:0.172, val_acc:0.971]
Epoch [78/120    avg_loss:0.211, val_acc:0.908]
Epoch [79/120    avg_loss:0.264, val_acc:0.975]
Epoch [80/120    avg_loss:0.153, val_acc:0.975]
Epoch [81/120    avg_loss:0.139, val_acc:0.981]
Epoch [82/120    avg_loss:0.126, val_acc:0.985]
Epoch [83/120    avg_loss:0.118, val_acc:0.985]
Epoch [84/120    avg_loss:0.125, val_acc:0.988]
Epoch [85/120    avg_loss:0.111, val_acc:0.983]
Epoch [86/120    avg_loss:0.123, val_acc:0.981]
Epoch [87/120    avg_loss:0.113, val_acc:0.985]
Epoch [88/120    avg_loss:0.119, val_acc:0.985]
Epoch [89/120    avg_loss:0.115, val_acc:0.983]
Epoch [90/120    avg_loss:0.111, val_acc:0.990]
Epoch [91/120    avg_loss:0.121, val_acc:0.988]
Epoch [92/120    avg_loss:0.109, val_acc:0.990]
Epoch [93/120    avg_loss:0.111, val_acc:0.990]
Epoch [94/120    avg_loss:0.116, val_acc:0.988]
Epoch [95/120    avg_loss:0.109, val_acc:0.990]
Epoch [96/120    avg_loss:0.112, val_acc:0.990]
Epoch [97/120    avg_loss:0.124, val_acc:0.988]
Epoch [98/120    avg_loss:0.107, val_acc:0.990]
Epoch [99/120    avg_loss:0.103, val_acc:0.988]
Epoch [100/120    avg_loss:0.112, val_acc:0.990]
Epoch [101/120    avg_loss:0.117, val_acc:0.990]
Epoch [102/120    avg_loss:0.118, val_acc:0.992]
Epoch [103/120    avg_loss:0.115, val_acc:0.990]
Epoch [104/120    avg_loss:0.107, val_acc:0.990]
Epoch [105/120    avg_loss:0.105, val_acc:0.988]
Epoch [106/120    avg_loss:0.108, val_acc:0.985]
Epoch [107/120    avg_loss:0.088, val_acc:0.983]
Epoch [108/120    avg_loss:0.088, val_acc:0.988]
Epoch [109/120    avg_loss:0.110, val_acc:0.990]
Epoch [110/120    avg_loss:0.096, val_acc:0.988]
Epoch [111/120    avg_loss:0.101, val_acc:0.990]
Epoch [112/120    avg_loss:0.096, val_acc:0.988]
Epoch [113/120    avg_loss:0.096, val_acc:0.992]
Epoch [114/120    avg_loss:0.089, val_acc:0.990]
Epoch [115/120    avg_loss:0.101, val_acc:0.985]
Epoch [116/120    avg_loss:0.107, val_acc:0.990]
Epoch [117/120    avg_loss:0.111, val_acc:0.985]
Epoch [118/120    avg_loss:0.115, val_acc:0.985]
Epoch [119/120    avg_loss:0.115, val_acc:0.983]
Epoch [120/120    avg_loss:0.107, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.98198198 1.         0.92170022 0.88215488
 1.         0.95604396 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9897925065143822
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0281620ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.622, val_acc:0.079]
Epoch [2/120    avg_loss:2.581, val_acc:0.424]
Epoch [3/120    avg_loss:2.546, val_acc:0.458]
Epoch [4/120    avg_loss:2.506, val_acc:0.427]
Epoch [5/120    avg_loss:2.472, val_acc:0.421]
Epoch [6/120    avg_loss:2.436, val_acc:0.419]
Epoch [7/120    avg_loss:2.399, val_acc:0.417]
Epoch [8/120    avg_loss:2.358, val_acc:0.429]
Epoch [9/120    avg_loss:2.337, val_acc:0.465]
Epoch [10/120    avg_loss:2.296, val_acc:0.471]
Epoch [11/120    avg_loss:2.258, val_acc:0.458]
Epoch [12/120    avg_loss:2.204, val_acc:0.456]
Epoch [13/120    avg_loss:2.166, val_acc:0.460]
Epoch [14/120    avg_loss:2.112, val_acc:0.471]
Epoch [15/120    avg_loss:2.095, val_acc:0.479]
Epoch [16/120    avg_loss:2.059, val_acc:0.481]
Epoch [17/120    avg_loss:1.983, val_acc:0.485]
Epoch [18/120    avg_loss:1.933, val_acc:0.490]
Epoch [19/120    avg_loss:1.911, val_acc:0.502]
Epoch [20/120    avg_loss:1.824, val_acc:0.506]
Epoch [21/120    avg_loss:1.773, val_acc:0.519]
Epoch [22/120    avg_loss:1.715, val_acc:0.523]
Epoch [23/120    avg_loss:1.667, val_acc:0.533]
Epoch [24/120    avg_loss:1.645, val_acc:0.531]
Epoch [25/120    avg_loss:1.609, val_acc:0.550]
Epoch [26/120    avg_loss:1.521, val_acc:0.583]
Epoch [27/120    avg_loss:1.485, val_acc:0.635]
Epoch [28/120    avg_loss:1.427, val_acc:0.652]
Epoch [29/120    avg_loss:1.350, val_acc:0.717]
Epoch [30/120    avg_loss:1.284, val_acc:0.708]
Epoch [31/120    avg_loss:1.214, val_acc:0.725]
Epoch [32/120    avg_loss:1.163, val_acc:0.710]
Epoch [33/120    avg_loss:1.119, val_acc:0.754]
Epoch [34/120    avg_loss:1.039, val_acc:0.783]
Epoch [35/120    avg_loss:0.963, val_acc:0.792]
Epoch [36/120    avg_loss:0.923, val_acc:0.815]
Epoch [37/120    avg_loss:0.894, val_acc:0.804]
Epoch [38/120    avg_loss:0.851, val_acc:0.869]
Epoch [39/120    avg_loss:0.783, val_acc:0.810]
Epoch [40/120    avg_loss:0.708, val_acc:0.869]
Epoch [41/120    avg_loss:0.685, val_acc:0.852]
Epoch [42/120    avg_loss:0.666, val_acc:0.860]
Epoch [43/120    avg_loss:0.614, val_acc:0.806]
Epoch [44/120    avg_loss:0.671, val_acc:0.881]
Epoch [45/120    avg_loss:0.607, val_acc:0.881]
Epoch [46/120    avg_loss:0.578, val_acc:0.902]
Epoch [47/120    avg_loss:0.498, val_acc:0.931]
Epoch [48/120    avg_loss:0.470, val_acc:0.927]
Epoch [49/120    avg_loss:0.468, val_acc:0.919]
Epoch [50/120    avg_loss:0.415, val_acc:0.931]
Epoch [51/120    avg_loss:0.406, val_acc:0.917]
Epoch [52/120    avg_loss:0.360, val_acc:0.927]
Epoch [53/120    avg_loss:0.381, val_acc:0.906]
Epoch [54/120    avg_loss:0.405, val_acc:0.935]
Epoch [55/120    avg_loss:0.372, val_acc:0.940]
Epoch [56/120    avg_loss:0.353, val_acc:0.940]
Epoch [57/120    avg_loss:0.312, val_acc:0.935]
Epoch [58/120    avg_loss:0.305, val_acc:0.965]
Epoch [59/120    avg_loss:0.328, val_acc:0.921]
Epoch [60/120    avg_loss:0.363, val_acc:0.938]
Epoch [61/120    avg_loss:0.357, val_acc:0.942]
Epoch [62/120    avg_loss:0.283, val_acc:0.956]
Epoch [63/120    avg_loss:0.302, val_acc:0.963]
Epoch [64/120    avg_loss:0.298, val_acc:0.944]
Epoch [65/120    avg_loss:0.285, val_acc:0.969]
Epoch [66/120    avg_loss:0.245, val_acc:0.971]
Epoch [67/120    avg_loss:0.222, val_acc:0.956]
Epoch [68/120    avg_loss:0.265, val_acc:0.969]
Epoch [69/120    avg_loss:0.252, val_acc:0.975]
Epoch [70/120    avg_loss:0.212, val_acc:0.942]
Epoch [71/120    avg_loss:0.229, val_acc:0.981]
Epoch [72/120    avg_loss:0.215, val_acc:0.973]
Epoch [73/120    avg_loss:0.232, val_acc:0.975]
Epoch [74/120    avg_loss:0.193, val_acc:0.975]
Epoch [75/120    avg_loss:0.189, val_acc:0.979]
Epoch [76/120    avg_loss:0.197, val_acc:0.910]
Epoch [77/120    avg_loss:0.222, val_acc:0.981]
Epoch [78/120    avg_loss:0.161, val_acc:0.981]
Epoch [79/120    avg_loss:0.164, val_acc:0.985]
Epoch [80/120    avg_loss:0.155, val_acc:0.990]
Epoch [81/120    avg_loss:0.135, val_acc:0.988]
Epoch [82/120    avg_loss:0.127, val_acc:0.994]
Epoch [83/120    avg_loss:0.138, val_acc:0.985]
Epoch [84/120    avg_loss:0.117, val_acc:0.990]
Epoch [85/120    avg_loss:0.145, val_acc:0.977]
Epoch [86/120    avg_loss:0.167, val_acc:0.979]
Epoch [87/120    avg_loss:0.223, val_acc:0.975]
Epoch [88/120    avg_loss:0.173, val_acc:0.985]
Epoch [89/120    avg_loss:0.140, val_acc:0.973]
Epoch [90/120    avg_loss:0.143, val_acc:0.981]
Epoch [91/120    avg_loss:0.118, val_acc:0.990]
Epoch [92/120    avg_loss:0.108, val_acc:0.981]
Epoch [93/120    avg_loss:0.143, val_acc:0.988]
Epoch [94/120    avg_loss:0.109, val_acc:0.996]
Epoch [95/120    avg_loss:0.102, val_acc:0.981]
Epoch [96/120    avg_loss:0.121, val_acc:0.988]
Epoch [97/120    avg_loss:0.108, val_acc:0.994]
Epoch [98/120    avg_loss:0.099, val_acc:0.990]
Epoch [99/120    avg_loss:0.097, val_acc:0.990]
Epoch [100/120    avg_loss:0.087, val_acc:0.977]
Epoch [101/120    avg_loss:0.087, val_acc:0.988]
Epoch [102/120    avg_loss:0.085, val_acc:0.990]
Epoch [103/120    avg_loss:0.076, val_acc:0.994]
Epoch [104/120    avg_loss:0.069, val_acc:0.990]
Epoch [105/120    avg_loss:0.059, val_acc:0.983]
Epoch [106/120    avg_loss:0.127, val_acc:0.992]
Epoch [107/120    avg_loss:0.116, val_acc:0.977]
Epoch [108/120    avg_loss:0.095, val_acc:0.992]
Epoch [109/120    avg_loss:0.066, val_acc:0.994]
Epoch [110/120    avg_loss:0.068, val_acc:0.996]
Epoch [111/120    avg_loss:0.057, val_acc:0.996]
Epoch [112/120    avg_loss:0.056, val_acc:0.996]
Epoch [113/120    avg_loss:0.071, val_acc:0.996]
Epoch [114/120    avg_loss:0.059, val_acc:0.994]
Epoch [115/120    avg_loss:0.055, val_acc:0.996]
Epoch [116/120    avg_loss:0.062, val_acc:0.994]
Epoch [117/120    avg_loss:0.059, val_acc:0.994]
Epoch [118/120    avg_loss:0.051, val_acc:0.994]
Epoch [119/120    avg_loss:0.051, val_acc:0.996]
Epoch [120/120    avg_loss:0.049, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         1.         1.         0.92747253 0.88581315
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9921663794794675
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f754fd19e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.081]
Epoch [2/120    avg_loss:2.600, val_acc:0.081]
Epoch [3/120    avg_loss:2.569, val_acc:0.227]
Epoch [4/120    avg_loss:2.538, val_acc:0.267]
Epoch [5/120    avg_loss:2.505, val_acc:0.267]
Epoch [6/120    avg_loss:2.476, val_acc:0.267]
Epoch [7/120    avg_loss:2.450, val_acc:0.267]
Epoch [8/120    avg_loss:2.422, val_acc:0.287]
Epoch [9/120    avg_loss:2.396, val_acc:0.292]
Epoch [10/120    avg_loss:2.366, val_acc:0.302]
Epoch [11/120    avg_loss:2.350, val_acc:0.319]
Epoch [12/120    avg_loss:2.316, val_acc:0.348]
Epoch [13/120    avg_loss:2.282, val_acc:0.388]
Epoch [14/120    avg_loss:2.248, val_acc:0.410]
Epoch [15/120    avg_loss:2.207, val_acc:0.450]
Epoch [16/120    avg_loss:2.163, val_acc:0.490]
Epoch [17/120    avg_loss:2.115, val_acc:0.477]
Epoch [18/120    avg_loss:2.070, val_acc:0.521]
Epoch [19/120    avg_loss:1.996, val_acc:0.525]
Epoch [20/120    avg_loss:1.972, val_acc:0.525]
Epoch [21/120    avg_loss:1.912, val_acc:0.521]
Epoch [22/120    avg_loss:1.871, val_acc:0.571]
Epoch [23/120    avg_loss:1.804, val_acc:0.590]
Epoch [24/120    avg_loss:1.739, val_acc:0.640]
Epoch [25/120    avg_loss:1.666, val_acc:0.669]
Epoch [26/120    avg_loss:1.587, val_acc:0.685]
Epoch [27/120    avg_loss:1.540, val_acc:0.681]
Epoch [28/120    avg_loss:1.470, val_acc:0.702]
Epoch [29/120    avg_loss:1.398, val_acc:0.708]
Epoch [30/120    avg_loss:1.329, val_acc:0.727]
Epoch [31/120    avg_loss:1.251, val_acc:0.746]
Epoch [32/120    avg_loss:1.164, val_acc:0.785]
Epoch [33/120    avg_loss:1.149, val_acc:0.769]
Epoch [34/120    avg_loss:1.074, val_acc:0.779]
Epoch [35/120    avg_loss:0.991, val_acc:0.804]
Epoch [36/120    avg_loss:0.934, val_acc:0.810]
Epoch [37/120    avg_loss:0.858, val_acc:0.804]
Epoch [38/120    avg_loss:0.811, val_acc:0.823]
Epoch [39/120    avg_loss:0.781, val_acc:0.823]
Epoch [40/120    avg_loss:0.745, val_acc:0.873]
Epoch [41/120    avg_loss:0.718, val_acc:0.835]
Epoch [42/120    avg_loss:0.666, val_acc:0.883]
Epoch [43/120    avg_loss:0.636, val_acc:0.854]
Epoch [44/120    avg_loss:0.623, val_acc:0.875]
Epoch [45/120    avg_loss:0.565, val_acc:0.844]
Epoch [46/120    avg_loss:0.635, val_acc:0.863]
Epoch [47/120    avg_loss:0.589, val_acc:0.894]
Epoch [48/120    avg_loss:0.547, val_acc:0.850]
Epoch [49/120    avg_loss:0.520, val_acc:0.938]
Epoch [50/120    avg_loss:0.441, val_acc:0.942]
Epoch [51/120    avg_loss:0.430, val_acc:0.944]
Epoch [52/120    avg_loss:0.443, val_acc:0.950]
Epoch [53/120    avg_loss:0.384, val_acc:0.958]
Epoch [54/120    avg_loss:0.373, val_acc:0.948]
Epoch [55/120    avg_loss:0.366, val_acc:0.946]
Epoch [56/120    avg_loss:0.393, val_acc:0.952]
Epoch [57/120    avg_loss:0.358, val_acc:0.946]
Epoch [58/120    avg_loss:0.307, val_acc:0.960]
Epoch [59/120    avg_loss:0.323, val_acc:0.938]
Epoch [60/120    avg_loss:0.307, val_acc:0.956]
Epoch [61/120    avg_loss:0.244, val_acc:0.969]
Epoch [62/120    avg_loss:0.233, val_acc:0.975]
Epoch [63/120    avg_loss:0.250, val_acc:0.942]
Epoch [64/120    avg_loss:0.254, val_acc:0.969]
Epoch [65/120    avg_loss:0.205, val_acc:0.935]
Epoch [66/120    avg_loss:0.303, val_acc:0.975]
Epoch [67/120    avg_loss:0.295, val_acc:0.967]
Epoch [68/120    avg_loss:0.224, val_acc:0.973]
Epoch [69/120    avg_loss:0.209, val_acc:0.960]
Epoch [70/120    avg_loss:0.203, val_acc:0.940]
Epoch [71/120    avg_loss:0.194, val_acc:0.977]
Epoch [72/120    avg_loss:0.197, val_acc:0.967]
Epoch [73/120    avg_loss:0.170, val_acc:0.967]
Epoch [74/120    avg_loss:0.401, val_acc:0.971]
Epoch [75/120    avg_loss:0.270, val_acc:0.952]
Epoch [76/120    avg_loss:0.293, val_acc:0.885]
Epoch [77/120    avg_loss:0.252, val_acc:0.973]
Epoch [78/120    avg_loss:0.197, val_acc:0.956]
Epoch [79/120    avg_loss:0.179, val_acc:0.973]
Epoch [80/120    avg_loss:0.163, val_acc:0.977]
Epoch [81/120    avg_loss:0.135, val_acc:0.975]
Epoch [82/120    avg_loss:0.133, val_acc:0.973]
Epoch [83/120    avg_loss:0.137, val_acc:0.971]
Epoch [84/120    avg_loss:0.148, val_acc:0.971]
Epoch [85/120    avg_loss:0.192, val_acc:0.954]
Epoch [86/120    avg_loss:0.183, val_acc:0.971]
Epoch [87/120    avg_loss:0.144, val_acc:0.977]
Epoch [88/120    avg_loss:0.111, val_acc:0.977]
Epoch [89/120    avg_loss:0.106, val_acc:0.988]
Epoch [90/120    avg_loss:0.100, val_acc:0.985]
Epoch [91/120    avg_loss:0.165, val_acc:0.948]
Epoch [92/120    avg_loss:0.126, val_acc:0.988]
Epoch [93/120    avg_loss:0.094, val_acc:0.988]
Epoch [94/120    avg_loss:0.082, val_acc:0.983]
Epoch [95/120    avg_loss:0.079, val_acc:0.983]
Epoch [96/120    avg_loss:0.069, val_acc:0.994]
Epoch [97/120    avg_loss:0.081, val_acc:0.985]
Epoch [98/120    avg_loss:0.087, val_acc:0.990]
Epoch [99/120    avg_loss:0.083, val_acc:0.985]
Epoch [100/120    avg_loss:0.102, val_acc:0.971]
Epoch [101/120    avg_loss:0.099, val_acc:0.981]
Epoch [102/120    avg_loss:0.069, val_acc:0.985]
Epoch [103/120    avg_loss:0.073, val_acc:0.994]
Epoch [104/120    avg_loss:0.092, val_acc:0.973]
Epoch [105/120    avg_loss:0.087, val_acc:0.967]
Epoch [106/120    avg_loss:0.109, val_acc:0.979]
Epoch [107/120    avg_loss:0.087, val_acc:0.985]
Epoch [108/120    avg_loss:0.053, val_acc:0.988]
Epoch [109/120    avg_loss:0.070, val_acc:0.969]
Epoch [110/120    avg_loss:0.087, val_acc:0.985]
Epoch [111/120    avg_loss:0.070, val_acc:0.965]
Epoch [112/120    avg_loss:0.100, val_acc:0.985]
Epoch [113/120    avg_loss:0.075, val_acc:0.981]
Epoch [114/120    avg_loss:0.060, val_acc:0.983]
Epoch [115/120    avg_loss:0.082, val_acc:0.975]
Epoch [116/120    avg_loss:0.066, val_acc:0.990]
Epoch [117/120    avg_loss:0.050, val_acc:0.990]
Epoch [118/120    avg_loss:0.045, val_acc:0.990]
Epoch [119/120    avg_loss:0.044, val_acc:0.992]
Epoch [120/120    avg_loss:0.036, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99780541 0.98866213 1.         0.94977169 0.92810458
 0.99277108 0.97826087 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9928792825839245
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f6e2cbe80>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.254]
Epoch [2/120    avg_loss:2.595, val_acc:0.312]
Epoch [3/120    avg_loss:2.563, val_acc:0.315]
Epoch [4/120    avg_loss:2.532, val_acc:0.312]
Epoch [5/120    avg_loss:2.501, val_acc:0.304]
Epoch [6/120    avg_loss:2.466, val_acc:0.302]
Epoch [7/120    avg_loss:2.440, val_acc:0.298]
Epoch [8/120    avg_loss:2.406, val_acc:0.300]
Epoch [9/120    avg_loss:2.379, val_acc:0.312]
Epoch [10/120    avg_loss:2.354, val_acc:0.321]
Epoch [11/120    avg_loss:2.328, val_acc:0.329]
Epoch [12/120    avg_loss:2.300, val_acc:0.348]
Epoch [13/120    avg_loss:2.261, val_acc:0.350]
Epoch [14/120    avg_loss:2.235, val_acc:0.352]
Epoch [15/120    avg_loss:2.191, val_acc:0.427]
Epoch [16/120    avg_loss:2.127, val_acc:0.429]
Epoch [17/120    avg_loss:2.087, val_acc:0.519]
Epoch [18/120    avg_loss:2.045, val_acc:0.562]
Epoch [19/120    avg_loss:1.979, val_acc:0.565]
Epoch [20/120    avg_loss:1.937, val_acc:0.571]
Epoch [21/120    avg_loss:1.900, val_acc:0.592]
Epoch [22/120    avg_loss:1.833, val_acc:0.646]
Epoch [23/120    avg_loss:1.790, val_acc:0.646]
Epoch [24/120    avg_loss:1.731, val_acc:0.646]
Epoch [25/120    avg_loss:1.663, val_acc:0.675]
Epoch [26/120    avg_loss:1.606, val_acc:0.681]
Epoch [27/120    avg_loss:1.530, val_acc:0.694]
Epoch [28/120    avg_loss:1.456, val_acc:0.710]
Epoch [29/120    avg_loss:1.423, val_acc:0.729]
Epoch [30/120    avg_loss:1.317, val_acc:0.729]
Epoch [31/120    avg_loss:1.270, val_acc:0.767]
Epoch [32/120    avg_loss:1.192, val_acc:0.750]
Epoch [33/120    avg_loss:1.139, val_acc:0.754]
Epoch [34/120    avg_loss:1.112, val_acc:0.752]
Epoch [35/120    avg_loss:1.017, val_acc:0.796]
Epoch [36/120    avg_loss:0.966, val_acc:0.771]
Epoch [37/120    avg_loss:0.915, val_acc:0.887]
Epoch [38/120    avg_loss:0.866, val_acc:0.854]
Epoch [39/120    avg_loss:0.774, val_acc:0.917]
Epoch [40/120    avg_loss:0.743, val_acc:0.927]
Epoch [41/120    avg_loss:0.693, val_acc:0.908]
Epoch [42/120    avg_loss:0.642, val_acc:0.942]
Epoch [43/120    avg_loss:0.661, val_acc:0.942]
Epoch [44/120    avg_loss:0.644, val_acc:0.912]
Epoch [45/120    avg_loss:0.601, val_acc:0.921]
Epoch [46/120    avg_loss:0.591, val_acc:0.917]
Epoch [47/120    avg_loss:0.530, val_acc:0.892]
Epoch [48/120    avg_loss:0.502, val_acc:0.942]
Epoch [49/120    avg_loss:0.481, val_acc:0.948]
Epoch [50/120    avg_loss:0.447, val_acc:0.958]
Epoch [51/120    avg_loss:0.431, val_acc:0.935]
Epoch [52/120    avg_loss:0.443, val_acc:0.954]
Epoch [53/120    avg_loss:0.382, val_acc:0.925]
Epoch [54/120    avg_loss:0.411, val_acc:0.950]
Epoch [55/120    avg_loss:0.392, val_acc:0.963]
Epoch [56/120    avg_loss:0.349, val_acc:0.977]
Epoch [57/120    avg_loss:0.353, val_acc:0.965]
Epoch [58/120    avg_loss:0.322, val_acc:0.950]
Epoch [59/120    avg_loss:0.331, val_acc:0.963]
Epoch [60/120    avg_loss:0.308, val_acc:0.944]
Epoch [61/120    avg_loss:0.295, val_acc:0.971]
Epoch [62/120    avg_loss:0.287, val_acc:0.975]
Epoch [63/120    avg_loss:0.287, val_acc:0.965]
Epoch [64/120    avg_loss:0.273, val_acc:0.950]
Epoch [65/120    avg_loss:0.271, val_acc:0.954]
Epoch [66/120    avg_loss:0.268, val_acc:0.983]
Epoch [67/120    avg_loss:0.233, val_acc:0.983]
Epoch [68/120    avg_loss:0.209, val_acc:0.985]
Epoch [69/120    avg_loss:0.240, val_acc:0.988]
Epoch [70/120    avg_loss:0.241, val_acc:0.979]
Epoch [71/120    avg_loss:0.220, val_acc:0.988]
Epoch [72/120    avg_loss:0.211, val_acc:0.990]
Epoch [73/120    avg_loss:0.204, val_acc:0.994]
Epoch [74/120    avg_loss:0.212, val_acc:0.983]
Epoch [75/120    avg_loss:0.298, val_acc:0.979]
Epoch [76/120    avg_loss:0.186, val_acc:0.988]
Epoch [77/120    avg_loss:0.208, val_acc:0.985]
Epoch [78/120    avg_loss:0.201, val_acc:0.981]
Epoch [79/120    avg_loss:0.186, val_acc:0.977]
Epoch [80/120    avg_loss:0.196, val_acc:0.973]
Epoch [81/120    avg_loss:0.179, val_acc:0.967]
Epoch [82/120    avg_loss:0.174, val_acc:0.975]
Epoch [83/120    avg_loss:0.182, val_acc:0.983]
Epoch [84/120    avg_loss:0.156, val_acc:0.988]
Epoch [85/120    avg_loss:0.146, val_acc:0.971]
Epoch [86/120    avg_loss:0.209, val_acc:0.983]
Epoch [87/120    avg_loss:0.151, val_acc:0.985]
Epoch [88/120    avg_loss:0.129, val_acc:0.988]
Epoch [89/120    avg_loss:0.111, val_acc:0.994]
Epoch [90/120    avg_loss:0.113, val_acc:0.990]
Epoch [91/120    avg_loss:0.103, val_acc:0.992]
Epoch [92/120    avg_loss:0.100, val_acc:0.992]
Epoch [93/120    avg_loss:0.091, val_acc:0.992]
Epoch [94/120    avg_loss:0.113, val_acc:0.992]
Epoch [95/120    avg_loss:0.098, val_acc:0.992]
Epoch [96/120    avg_loss:0.092, val_acc:0.992]
Epoch [97/120    avg_loss:0.096, val_acc:0.992]
Epoch [98/120    avg_loss:0.084, val_acc:0.992]
Epoch [99/120    avg_loss:0.098, val_acc:0.990]
Epoch [100/120    avg_loss:0.089, val_acc:0.990]
Epoch [101/120    avg_loss:0.094, val_acc:0.992]
Epoch [102/120    avg_loss:0.080, val_acc:0.992]
Epoch [103/120    avg_loss:0.104, val_acc:0.994]
Epoch [104/120    avg_loss:0.080, val_acc:0.994]
Epoch [105/120    avg_loss:0.090, val_acc:0.992]
Epoch [106/120    avg_loss:0.089, val_acc:0.992]
Epoch [107/120    avg_loss:0.088, val_acc:0.992]
Epoch [108/120    avg_loss:0.079, val_acc:0.992]
Epoch [109/120    avg_loss:0.084, val_acc:0.992]
Epoch [110/120    avg_loss:0.088, val_acc:0.992]
Epoch [111/120    avg_loss:0.093, val_acc:0.992]
Epoch [112/120    avg_loss:0.083, val_acc:0.992]
Epoch [113/120    avg_loss:0.092, val_acc:0.992]
Epoch [114/120    avg_loss:0.085, val_acc:0.992]
Epoch [115/120    avg_loss:0.087, val_acc:0.994]
Epoch [116/120    avg_loss:0.085, val_acc:0.994]
Epoch [117/120    avg_loss:0.083, val_acc:0.994]
Epoch [118/120    avg_loss:0.098, val_acc:0.994]
Epoch [119/120    avg_loss:0.087, val_acc:0.994]
Epoch [120/120    avg_loss:0.092, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99780541 0.97767857 1.         0.93953488 0.91719745
 0.99277108 0.94382022 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9895565559610654
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8401d06e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.656, val_acc:0.331]
Epoch [2/120    avg_loss:2.623, val_acc:0.571]
Epoch [3/120    avg_loss:2.590, val_acc:0.471]
Epoch [4/120    avg_loss:2.551, val_acc:0.436]
Epoch [5/120    avg_loss:2.515, val_acc:0.434]
Epoch [6/120    avg_loss:2.480, val_acc:0.450]
Epoch [7/120    avg_loss:2.453, val_acc:0.431]
Epoch [8/120    avg_loss:2.398, val_acc:0.419]
Epoch [9/120    avg_loss:2.377, val_acc:0.410]
Epoch [10/120    avg_loss:2.334, val_acc:0.394]
Epoch [11/120    avg_loss:2.297, val_acc:0.402]
Epoch [12/120    avg_loss:2.250, val_acc:0.396]
Epoch [13/120    avg_loss:2.196, val_acc:0.394]
Epoch [14/120    avg_loss:2.168, val_acc:0.417]
Epoch [15/120    avg_loss:2.109, val_acc:0.423]
Epoch [16/120    avg_loss:2.082, val_acc:0.425]
Epoch [17/120    avg_loss:2.070, val_acc:0.433]
Epoch [18/120    avg_loss:2.062, val_acc:0.438]
Epoch [19/120    avg_loss:2.057, val_acc:0.438]
Epoch [20/120    avg_loss:2.039, val_acc:0.444]
Epoch [21/120    avg_loss:2.055, val_acc:0.448]
Epoch [22/120    avg_loss:2.045, val_acc:0.446]
Epoch [23/120    avg_loss:2.022, val_acc:0.442]
Epoch [24/120    avg_loss:2.038, val_acc:0.440]
Epoch [25/120    avg_loss:2.023, val_acc:0.448]
Epoch [26/120    avg_loss:2.026, val_acc:0.448]
Epoch [27/120    avg_loss:2.016, val_acc:0.446]
Epoch [28/120    avg_loss:1.989, val_acc:0.450]
Epoch [29/120    avg_loss:1.998, val_acc:0.448]
Epoch [30/120    avg_loss:2.009, val_acc:0.448]
Epoch [31/120    avg_loss:2.006, val_acc:0.448]
Epoch [32/120    avg_loss:1.989, val_acc:0.448]
Epoch [33/120    avg_loss:1.990, val_acc:0.450]
Epoch [34/120    avg_loss:1.986, val_acc:0.448]
Epoch [35/120    avg_loss:1.993, val_acc:0.448]
Epoch [36/120    avg_loss:1.987, val_acc:0.450]
Epoch [37/120    avg_loss:1.992, val_acc:0.450]
Epoch [38/120    avg_loss:1.990, val_acc:0.450]
Epoch [39/120    avg_loss:1.986, val_acc:0.448]
Epoch [40/120    avg_loss:1.995, val_acc:0.448]
Epoch [41/120    avg_loss:1.983, val_acc:0.448]
Epoch [42/120    avg_loss:1.984, val_acc:0.448]
Epoch [43/120    avg_loss:1.994, val_acc:0.448]
Epoch [44/120    avg_loss:1.978, val_acc:0.448]
Epoch [45/120    avg_loss:1.991, val_acc:0.450]
Epoch [46/120    avg_loss:1.985, val_acc:0.450]
Epoch [47/120    avg_loss:1.994, val_acc:0.450]
Epoch [48/120    avg_loss:1.986, val_acc:0.450]
Epoch [49/120    avg_loss:1.991, val_acc:0.450]
Epoch [50/120    avg_loss:2.009, val_acc:0.448]
Epoch [51/120    avg_loss:1.991, val_acc:0.450]
Epoch [52/120    avg_loss:2.003, val_acc:0.450]
Epoch [53/120    avg_loss:1.993, val_acc:0.450]
Epoch [54/120    avg_loss:1.992, val_acc:0.450]
Epoch [55/120    avg_loss:1.986, val_acc:0.450]
Epoch [56/120    avg_loss:1.991, val_acc:0.450]
Epoch [57/120    avg_loss:1.974, val_acc:0.450]
Epoch [58/120    avg_loss:1.992, val_acc:0.450]
Epoch [59/120    avg_loss:1.988, val_acc:0.450]
Epoch [60/120    avg_loss:1.988, val_acc:0.450]
Epoch [61/120    avg_loss:1.979, val_acc:0.450]
Epoch [62/120    avg_loss:1.992, val_acc:0.450]
Epoch [63/120    avg_loss:2.005, val_acc:0.450]
Epoch [64/120    avg_loss:1.997, val_acc:0.450]
Epoch [65/120    avg_loss:1.986, val_acc:0.450]
Epoch [66/120    avg_loss:1.997, val_acc:0.450]
Epoch [67/120    avg_loss:1.989, val_acc:0.450]
Epoch [68/120    avg_loss:1.969, val_acc:0.450]
Epoch [69/120    avg_loss:1.990, val_acc:0.450]
Epoch [70/120    avg_loss:1.991, val_acc:0.450]
Epoch [71/120    avg_loss:1.987, val_acc:0.450]
Epoch [72/120    avg_loss:1.990, val_acc:0.450]
Epoch [73/120    avg_loss:1.990, val_acc:0.450]
Epoch [74/120    avg_loss:1.991, val_acc:0.450]
Epoch [75/120    avg_loss:1.975, val_acc:0.450]
Epoch [76/120    avg_loss:1.984, val_acc:0.450]
Epoch [77/120    avg_loss:1.996, val_acc:0.450]
Epoch [78/120    avg_loss:1.990, val_acc:0.450]
Epoch [79/120    avg_loss:1.982, val_acc:0.450]
Epoch [80/120    avg_loss:1.978, val_acc:0.450]
Epoch [81/120    avg_loss:1.995, val_acc:0.450]
Epoch [82/120    avg_loss:1.996, val_acc:0.450]
Epoch [83/120    avg_loss:1.988, val_acc:0.450]
Epoch [84/120    avg_loss:1.992, val_acc:0.450]
Epoch [85/120    avg_loss:1.986, val_acc:0.450]
Epoch [86/120    avg_loss:1.978, val_acc:0.450]
Epoch [87/120    avg_loss:1.979, val_acc:0.450]
Epoch [88/120    avg_loss:1.998, val_acc:0.450]
Epoch [89/120    avg_loss:1.978, val_acc:0.450]
Epoch [90/120    avg_loss:1.996, val_acc:0.450]
Epoch [91/120    avg_loss:1.985, val_acc:0.450]
Epoch [92/120    avg_loss:1.995, val_acc:0.450]
Epoch [93/120    avg_loss:1.982, val_acc:0.450]
Epoch [94/120    avg_loss:1.998, val_acc:0.450]
Epoch [95/120    avg_loss:1.997, val_acc:0.450]
Epoch [96/120    avg_loss:1.975, val_acc:0.450]
Epoch [97/120    avg_loss:1.980, val_acc:0.450]
Epoch [98/120    avg_loss:1.994, val_acc:0.450]
Epoch [99/120    avg_loss:1.986, val_acc:0.450]
Epoch [100/120    avg_loss:2.001, val_acc:0.450]
Epoch [101/120    avg_loss:1.981, val_acc:0.450]
Epoch [102/120    avg_loss:1.973, val_acc:0.450]
Epoch [103/120    avg_loss:1.986, val_acc:0.450]
Epoch [104/120    avg_loss:1.984, val_acc:0.450]
Epoch [105/120    avg_loss:1.985, val_acc:0.450]
Epoch [106/120    avg_loss:1.982, val_acc:0.450]
Epoch [107/120    avg_loss:1.981, val_acc:0.450]
Epoch [108/120    avg_loss:1.991, val_acc:0.450]
Epoch [109/120    avg_loss:2.002, val_acc:0.450]
Epoch [110/120    avg_loss:1.976, val_acc:0.450]
Epoch [111/120    avg_loss:1.994, val_acc:0.450]
Epoch [112/120    avg_loss:1.994, val_acc:0.450]
Epoch [113/120    avg_loss:1.984, val_acc:0.450]
Epoch [114/120    avg_loss:1.983, val_acc:0.450]
Epoch [115/120    avg_loss:1.985, val_acc:0.450]
Epoch [116/120    avg_loss:1.985, val_acc:0.450]
Epoch [117/120    avg_loss:1.983, val_acc:0.450]
Epoch [118/120    avg_loss:1.986, val_acc:0.450]
Epoch [119/120    avg_loss:1.993, val_acc:0.450]
Epoch [120/120    avg_loss:1.997, val_acc:0.450]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0  17  65  12   0 560  31   0   0   0   0   0   0]
 [  0   0 103   0   0   0   0 116   0   0   0   0   0   0]
 [  0   0 142  62   0   0   0  26   0   0   0   0   0   0]
 [  0   0  72  60  49  10   0  17   0   0  19   0   0   0]
 [  0   0  35  39  14  26   0  30   0   0   1   0   0   0]
 [  0   0   0   3  15   0  10 178   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0  88 223   0   0   0   0   0   2   0  75   0   0]
 [  0   0   0   8   0   0   0   0   0  76   0 384   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0  16   0   0   0   0   0   0   0  19 341  65  12]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
43.92324093816631

F1 scores:
[       nan 0.         0.29768786 0.17971014 0.30914826 0.28729282
 0.0257732  0.32081911 0.         0.27838828 0.94915254 0.48519949
 0.25096525 0.99285714]

Kappa:
0.3904413642960742
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1ba01ce80>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.176]
Epoch [2/120    avg_loss:2.585, val_acc:0.306]
Epoch [3/120    avg_loss:2.552, val_acc:0.385]
Epoch [4/120    avg_loss:2.525, val_acc:0.400]
Epoch [5/120    avg_loss:2.494, val_acc:0.402]
Epoch [6/120    avg_loss:2.464, val_acc:0.394]
Epoch [7/120    avg_loss:2.432, val_acc:0.400]
Epoch [8/120    avg_loss:2.399, val_acc:0.425]
Epoch [9/120    avg_loss:2.375, val_acc:0.421]
Epoch [10/120    avg_loss:2.340, val_acc:0.429]
Epoch [11/120    avg_loss:2.316, val_acc:0.419]
Epoch [12/120    avg_loss:2.282, val_acc:0.394]
Epoch [13/120    avg_loss:2.242, val_acc:0.396]
Epoch [14/120    avg_loss:2.206, val_acc:0.381]
Epoch [15/120    avg_loss:2.170, val_acc:0.398]
Epoch [16/120    avg_loss:2.126, val_acc:0.404]
Epoch [17/120    avg_loss:2.099, val_acc:0.423]
Epoch [18/120    avg_loss:2.073, val_acc:0.450]
Epoch [19/120    avg_loss:1.991, val_acc:0.471]
Epoch [20/120    avg_loss:1.959, val_acc:0.481]
Epoch [21/120    avg_loss:1.897, val_acc:0.502]
Epoch [22/120    avg_loss:1.914, val_acc:0.494]
Epoch [23/120    avg_loss:1.879, val_acc:0.492]
Epoch [24/120    avg_loss:1.795, val_acc:0.542]
Epoch [25/120    avg_loss:1.756, val_acc:0.542]
Epoch [26/120    avg_loss:1.707, val_acc:0.565]
Epoch [27/120    avg_loss:1.664, val_acc:0.600]
Epoch [28/120    avg_loss:1.611, val_acc:0.598]
Epoch [29/120    avg_loss:1.567, val_acc:0.629]
Epoch [30/120    avg_loss:1.516, val_acc:0.642]
Epoch [31/120    avg_loss:1.465, val_acc:0.665]
Epoch [32/120    avg_loss:1.417, val_acc:0.681]
Epoch [33/120    avg_loss:1.355, val_acc:0.658]
Epoch [34/120    avg_loss:1.308, val_acc:0.698]
Epoch [35/120    avg_loss:1.293, val_acc:0.690]
Epoch [36/120    avg_loss:1.230, val_acc:0.815]
Epoch [37/120    avg_loss:1.172, val_acc:0.806]
Epoch [38/120    avg_loss:1.149, val_acc:0.825]
Epoch [39/120    avg_loss:1.021, val_acc:0.883]
Epoch [40/120    avg_loss:0.996, val_acc:0.833]
Epoch [41/120    avg_loss:0.924, val_acc:0.823]
Epoch [42/120    avg_loss:0.884, val_acc:0.867]
Epoch [43/120    avg_loss:0.808, val_acc:0.881]
Epoch [44/120    avg_loss:0.785, val_acc:0.904]
Epoch [45/120    avg_loss:0.696, val_acc:0.921]
Epoch [46/120    avg_loss:0.705, val_acc:0.877]
Epoch [47/120    avg_loss:0.632, val_acc:0.917]
Epoch [48/120    avg_loss:0.630, val_acc:0.933]
Epoch [49/120    avg_loss:0.564, val_acc:0.944]
Epoch [50/120    avg_loss:0.572, val_acc:0.904]
Epoch [51/120    avg_loss:0.624, val_acc:0.846]
Epoch [52/120    avg_loss:0.578, val_acc:0.921]
Epoch [53/120    avg_loss:0.581, val_acc:0.919]
Epoch [54/120    avg_loss:0.563, val_acc:0.933]
Epoch [55/120    avg_loss:0.481, val_acc:0.946]
Epoch [56/120    avg_loss:0.429, val_acc:0.935]
Epoch [57/120    avg_loss:0.418, val_acc:0.948]
Epoch [58/120    avg_loss:0.422, val_acc:0.946]
Epoch [59/120    avg_loss:0.419, val_acc:0.956]
Epoch [60/120    avg_loss:0.400, val_acc:0.954]
Epoch [61/120    avg_loss:0.408, val_acc:0.948]
Epoch [62/120    avg_loss:0.366, val_acc:0.946]
Epoch [63/120    avg_loss:0.432, val_acc:0.894]
Epoch [64/120    avg_loss:0.400, val_acc:0.963]
Epoch [65/120    avg_loss:0.324, val_acc:0.967]
Epoch [66/120    avg_loss:0.303, val_acc:0.969]
Epoch [67/120    avg_loss:0.269, val_acc:0.969]
Epoch [68/120    avg_loss:0.257, val_acc:0.967]
Epoch [69/120    avg_loss:0.261, val_acc:0.963]
Epoch [70/120    avg_loss:0.319, val_acc:0.946]
Epoch [71/120    avg_loss:0.299, val_acc:0.904]
Epoch [72/120    avg_loss:0.291, val_acc:0.938]
Epoch [73/120    avg_loss:0.304, val_acc:0.935]
Epoch [74/120    avg_loss:0.257, val_acc:0.954]
Epoch [75/120    avg_loss:0.270, val_acc:0.958]
Epoch [76/120    avg_loss:0.209, val_acc:0.958]
Epoch [77/120    avg_loss:0.243, val_acc:0.927]
Epoch [78/120    avg_loss:0.282, val_acc:0.956]
Epoch [79/120    avg_loss:0.196, val_acc:0.956]
Epoch [80/120    avg_loss:0.263, val_acc:0.973]
Epoch [81/120    avg_loss:0.243, val_acc:0.971]
Epoch [82/120    avg_loss:0.247, val_acc:0.975]
Epoch [83/120    avg_loss:0.256, val_acc:0.969]
Epoch [84/120    avg_loss:0.222, val_acc:0.985]
Epoch [85/120    avg_loss:0.197, val_acc:0.963]
Epoch [86/120    avg_loss:0.201, val_acc:0.979]
Epoch [87/120    avg_loss:0.180, val_acc:0.977]
Epoch [88/120    avg_loss:0.153, val_acc:0.963]
Epoch [89/120    avg_loss:0.140, val_acc:0.981]
Epoch [90/120    avg_loss:0.156, val_acc:0.973]
Epoch [91/120    avg_loss:0.152, val_acc:0.985]
Epoch [92/120    avg_loss:0.163, val_acc:0.977]
Epoch [93/120    avg_loss:0.178, val_acc:0.975]
Epoch [94/120    avg_loss:0.198, val_acc:0.971]
Epoch [95/120    avg_loss:0.184, val_acc:0.969]
Epoch [96/120    avg_loss:0.212, val_acc:0.973]
Epoch [97/120    avg_loss:0.230, val_acc:0.971]
Epoch [98/120    avg_loss:0.183, val_acc:0.969]
Epoch [99/120    avg_loss:0.173, val_acc:0.960]
Epoch [100/120    avg_loss:0.144, val_acc:0.977]
Epoch [101/120    avg_loss:0.126, val_acc:0.977]
Epoch [102/120    avg_loss:0.166, val_acc:0.940]
Epoch [103/120    avg_loss:0.193, val_acc:0.988]
Epoch [104/120    avg_loss:0.145, val_acc:0.981]
Epoch [105/120    avg_loss:0.149, val_acc:0.967]
Epoch [106/120    avg_loss:0.167, val_acc:0.979]
Epoch [107/120    avg_loss:0.147, val_acc:0.988]
Epoch [108/120    avg_loss:0.115, val_acc:0.981]
Epoch [109/120    avg_loss:0.107, val_acc:0.983]
Epoch [110/120    avg_loss:0.088, val_acc:0.985]
Epoch [111/120    avg_loss:0.103, val_acc:0.985]
Epoch [112/120    avg_loss:0.082, val_acc:0.983]
Epoch [113/120    avg_loss:0.109, val_acc:0.981]
Epoch [114/120    avg_loss:0.094, val_acc:0.983]
Epoch [115/120    avg_loss:0.080, val_acc:0.988]
Epoch [116/120    avg_loss:0.071, val_acc:0.994]
Epoch [117/120    avg_loss:0.128, val_acc:0.969]
Epoch [118/120    avg_loss:0.111, val_acc:0.967]
Epoch [119/120    avg_loss:0.070, val_acc:0.981]
Epoch [120/120    avg_loss:0.073, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   0   0   0   0   0   0   0   4   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98623853 0.99563319 0.93305439 0.90151515
 1.         0.97916667 1.         1.         1.         1.
 0.99339207 1.        ]

Kappa:
0.9909785631790122
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a23efcef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.617, val_acc:0.056]
Epoch [2/120    avg_loss:2.579, val_acc:0.281]
Epoch [3/120    avg_loss:2.540, val_acc:0.342]
Epoch [4/120    avg_loss:2.504, val_acc:0.329]
Epoch [5/120    avg_loss:2.465, val_acc:0.346]
Epoch [6/120    avg_loss:2.426, val_acc:0.358]
Epoch [7/120    avg_loss:2.392, val_acc:0.404]
Epoch [8/120    avg_loss:2.355, val_acc:0.440]
Epoch [9/120    avg_loss:2.301, val_acc:0.479]
Epoch [10/120    avg_loss:2.254, val_acc:0.510]
Epoch [11/120    avg_loss:2.183, val_acc:0.529]
Epoch [12/120    avg_loss:2.134, val_acc:0.542]
Epoch [13/120    avg_loss:2.087, val_acc:0.579]
Epoch [14/120    avg_loss:2.031, val_acc:0.579]
Epoch [15/120    avg_loss:1.960, val_acc:0.596]
Epoch [16/120    avg_loss:1.908, val_acc:0.600]
Epoch [17/120    avg_loss:1.848, val_acc:0.617]
Epoch [18/120    avg_loss:1.799, val_acc:0.642]
Epoch [19/120    avg_loss:1.707, val_acc:0.671]
Epoch [20/120    avg_loss:1.602, val_acc:0.692]
Epoch [21/120    avg_loss:1.550, val_acc:0.694]
Epoch [22/120    avg_loss:1.473, val_acc:0.694]
Epoch [23/120    avg_loss:1.417, val_acc:0.702]
Epoch [24/120    avg_loss:1.351, val_acc:0.692]
Epoch [25/120    avg_loss:1.280, val_acc:0.725]
Epoch [26/120    avg_loss:1.219, val_acc:0.758]
Epoch [27/120    avg_loss:1.146, val_acc:0.783]
Epoch [28/120    avg_loss:1.087, val_acc:0.800]
Epoch [29/120    avg_loss:1.018, val_acc:0.825]
Epoch [30/120    avg_loss:0.974, val_acc:0.852]
Epoch [31/120    avg_loss:0.924, val_acc:0.890]
Epoch [32/120    avg_loss:0.886, val_acc:0.877]
Epoch [33/120    avg_loss:0.836, val_acc:0.917]
Epoch [34/120    avg_loss:0.777, val_acc:0.848]
Epoch [35/120    avg_loss:0.750, val_acc:0.898]
Epoch [36/120    avg_loss:0.716, val_acc:0.885]
Epoch [37/120    avg_loss:0.745, val_acc:0.921]
Epoch [38/120    avg_loss:0.645, val_acc:0.912]
Epoch [39/120    avg_loss:0.596, val_acc:0.923]
Epoch [40/120    avg_loss:0.562, val_acc:0.919]
Epoch [41/120    avg_loss:0.579, val_acc:0.933]
Epoch [42/120    avg_loss:0.509, val_acc:0.919]
Epoch [43/120    avg_loss:0.489, val_acc:0.933]
Epoch [44/120    avg_loss:0.462, val_acc:0.923]
Epoch [45/120    avg_loss:0.460, val_acc:0.946]
Epoch [46/120    avg_loss:0.423, val_acc:0.950]
Epoch [47/120    avg_loss:0.398, val_acc:0.940]
Epoch [48/120    avg_loss:0.359, val_acc:0.963]
Epoch [49/120    avg_loss:0.362, val_acc:0.952]
Epoch [50/120    avg_loss:0.344, val_acc:0.958]
Epoch [51/120    avg_loss:0.310, val_acc:0.960]
Epoch [52/120    avg_loss:0.310, val_acc:0.963]
Epoch [53/120    avg_loss:0.276, val_acc:0.971]
Epoch [54/120    avg_loss:0.275, val_acc:0.975]
Epoch [55/120    avg_loss:0.259, val_acc:0.979]
Epoch [56/120    avg_loss:0.227, val_acc:0.983]
Epoch [57/120    avg_loss:0.230, val_acc:0.979]
Epoch [58/120    avg_loss:0.237, val_acc:0.973]
Epoch [59/120    avg_loss:0.268, val_acc:0.952]
Epoch [60/120    avg_loss:0.246, val_acc:0.954]
Epoch [61/120    avg_loss:0.226, val_acc:0.971]
Epoch [62/120    avg_loss:0.211, val_acc:0.981]
Epoch [63/120    avg_loss:0.174, val_acc:0.979]
Epoch [64/120    avg_loss:0.236, val_acc:0.954]
Epoch [65/120    avg_loss:0.258, val_acc:0.979]
Epoch [66/120    avg_loss:0.228, val_acc:0.973]
Epoch [67/120    avg_loss:0.190, val_acc:0.979]
Epoch [68/120    avg_loss:0.185, val_acc:0.981]
Epoch [69/120    avg_loss:0.189, val_acc:0.981]
Epoch [70/120    avg_loss:0.154, val_acc:0.981]
Epoch [71/120    avg_loss:0.151, val_acc:0.988]
Epoch [72/120    avg_loss:0.126, val_acc:0.988]
Epoch [73/120    avg_loss:0.127, val_acc:0.988]
Epoch [74/120    avg_loss:0.133, val_acc:0.992]
Epoch [75/120    avg_loss:0.122, val_acc:0.992]
Epoch [76/120    avg_loss:0.146, val_acc:0.992]
Epoch [77/120    avg_loss:0.125, val_acc:0.990]
Epoch [78/120    avg_loss:0.121, val_acc:0.992]
Epoch [79/120    avg_loss:0.122, val_acc:0.992]
Epoch [80/120    avg_loss:0.119, val_acc:0.992]
Epoch [81/120    avg_loss:0.114, val_acc:0.990]
Epoch [82/120    avg_loss:0.107, val_acc:0.992]
Epoch [83/120    avg_loss:0.126, val_acc:0.992]
Epoch [84/120    avg_loss:0.107, val_acc:0.992]
Epoch [85/120    avg_loss:0.108, val_acc:0.992]
Epoch [86/120    avg_loss:0.110, val_acc:0.992]
Epoch [87/120    avg_loss:0.114, val_acc:0.994]
Epoch [88/120    avg_loss:0.111, val_acc:0.994]
Epoch [89/120    avg_loss:0.107, val_acc:0.992]
Epoch [90/120    avg_loss:0.099, val_acc:0.992]
Epoch [91/120    avg_loss:0.107, val_acc:0.990]
Epoch [92/120    avg_loss:0.100, val_acc:0.992]
Epoch [93/120    avg_loss:0.101, val_acc:0.992]
Epoch [94/120    avg_loss:0.109, val_acc:0.992]
Epoch [95/120    avg_loss:0.104, val_acc:0.988]
Epoch [96/120    avg_loss:0.114, val_acc:0.994]
Epoch [97/120    avg_loss:0.098, val_acc:0.994]
Epoch [98/120    avg_loss:0.106, val_acc:0.992]
Epoch [99/120    avg_loss:0.106, val_acc:0.992]
Epoch [100/120    avg_loss:0.105, val_acc:0.994]
Epoch [101/120    avg_loss:0.099, val_acc:0.992]
Epoch [102/120    avg_loss:0.092, val_acc:0.992]
Epoch [103/120    avg_loss:0.097, val_acc:0.996]
Epoch [104/120    avg_loss:0.094, val_acc:0.994]
Epoch [105/120    avg_loss:0.101, val_acc:0.994]
Epoch [106/120    avg_loss:0.096, val_acc:0.994]
Epoch [107/120    avg_loss:0.082, val_acc:0.994]
Epoch [108/120    avg_loss:0.102, val_acc:0.992]
Epoch [109/120    avg_loss:0.098, val_acc:0.992]
Epoch [110/120    avg_loss:0.087, val_acc:0.992]
Epoch [111/120    avg_loss:0.098, val_acc:0.992]
Epoch [112/120    avg_loss:0.097, val_acc:0.992]
Epoch [113/120    avg_loss:0.085, val_acc:0.990]
Epoch [114/120    avg_loss:0.094, val_acc:0.992]
Epoch [115/120    avg_loss:0.084, val_acc:0.992]
Epoch [116/120    avg_loss:0.085, val_acc:0.990]
Epoch [117/120    avg_loss:0.092, val_acc:0.990]
Epoch [118/120    avg_loss:0.087, val_acc:0.990]
Epoch [119/120    avg_loss:0.086, val_acc:0.990]
Epoch [120/120    avg_loss:0.087, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 201  25   0   0   0   0   0   0   1   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.99095023 1.         0.93055556 0.90675241
 1.         0.97826087 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9919294373897737
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc418a8bdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.652, val_acc:0.087]
Epoch [2/120    avg_loss:2.616, val_acc:0.263]
Epoch [3/120    avg_loss:2.585, val_acc:0.365]
Epoch [4/120    avg_loss:2.551, val_acc:0.388]
Epoch [5/120    avg_loss:2.522, val_acc:0.412]
Epoch [6/120    avg_loss:2.479, val_acc:0.415]
Epoch [7/120    avg_loss:2.450, val_acc:0.415]
Epoch [8/120    avg_loss:2.407, val_acc:0.406]
Epoch [9/120    avg_loss:2.370, val_acc:0.402]
Epoch [10/120    avg_loss:2.335, val_acc:0.400]
Epoch [11/120    avg_loss:2.295, val_acc:0.400]
Epoch [12/120    avg_loss:2.250, val_acc:0.406]
Epoch [13/120    avg_loss:2.212, val_acc:0.404]
Epoch [14/120    avg_loss:2.162, val_acc:0.421]
Epoch [15/120    avg_loss:2.118, val_acc:0.438]
Epoch [16/120    avg_loss:2.085, val_acc:0.475]
Epoch [17/120    avg_loss:2.034, val_acc:0.531]
Epoch [18/120    avg_loss:1.983, val_acc:0.558]
Epoch [19/120    avg_loss:1.938, val_acc:0.575]
Epoch [20/120    avg_loss:1.877, val_acc:0.590]
Epoch [21/120    avg_loss:1.844, val_acc:0.575]
Epoch [22/120    avg_loss:1.770, val_acc:0.633]
Epoch [23/120    avg_loss:1.737, val_acc:0.652]
Epoch [24/120    avg_loss:1.669, val_acc:0.660]
Epoch [25/120    avg_loss:1.590, val_acc:0.675]
Epoch [26/120    avg_loss:1.559, val_acc:0.713]
Epoch [27/120    avg_loss:1.493, val_acc:0.719]
Epoch [28/120    avg_loss:1.452, val_acc:0.706]
Epoch [29/120    avg_loss:1.384, val_acc:0.733]
Epoch [30/120    avg_loss:1.281, val_acc:0.756]
Epoch [31/120    avg_loss:1.248, val_acc:0.750]
Epoch [32/120    avg_loss:1.164, val_acc:0.779]
Epoch [33/120    avg_loss:1.109, val_acc:0.790]
Epoch [34/120    avg_loss:1.068, val_acc:0.794]
Epoch [35/120    avg_loss:0.978, val_acc:0.846]
Epoch [36/120    avg_loss:0.921, val_acc:0.838]
Epoch [37/120    avg_loss:0.834, val_acc:0.838]
Epoch [38/120    avg_loss:0.838, val_acc:0.921]
Epoch [39/120    avg_loss:0.745, val_acc:0.871]
Epoch [40/120    avg_loss:0.701, val_acc:0.879]
Epoch [41/120    avg_loss:0.636, val_acc:0.915]
Epoch [42/120    avg_loss:0.613, val_acc:0.875]
Epoch [43/120    avg_loss:0.645, val_acc:0.925]
Epoch [44/120    avg_loss:0.612, val_acc:0.912]
Epoch [45/120    avg_loss:0.594, val_acc:0.940]
Epoch [46/120    avg_loss:0.538, val_acc:0.927]
Epoch [47/120    avg_loss:0.501, val_acc:0.925]
Epoch [48/120    avg_loss:0.447, val_acc:0.931]
Epoch [49/120    avg_loss:0.463, val_acc:0.929]
Epoch [50/120    avg_loss:0.442, val_acc:0.938]
Epoch [51/120    avg_loss:0.422, val_acc:0.950]
Epoch [52/120    avg_loss:0.498, val_acc:0.923]
Epoch [53/120    avg_loss:0.437, val_acc:0.935]
Epoch [54/120    avg_loss:0.417, val_acc:0.912]
Epoch [55/120    avg_loss:0.364, val_acc:0.948]
Epoch [56/120    avg_loss:0.376, val_acc:0.954]
Epoch [57/120    avg_loss:0.386, val_acc:0.931]
Epoch [58/120    avg_loss:0.408, val_acc:0.946]
Epoch [59/120    avg_loss:0.329, val_acc:0.956]
Epoch [60/120    avg_loss:0.332, val_acc:0.956]
Epoch [61/120    avg_loss:0.319, val_acc:0.954]
Epoch [62/120    avg_loss:0.318, val_acc:0.958]
Epoch [63/120    avg_loss:0.321, val_acc:0.946]
Epoch [64/120    avg_loss:0.282, val_acc:0.969]
Epoch [65/120    avg_loss:0.287, val_acc:0.960]
Epoch [66/120    avg_loss:0.255, val_acc:0.965]
Epoch [67/120    avg_loss:0.233, val_acc:0.963]
Epoch [68/120    avg_loss:0.248, val_acc:0.965]
Epoch [69/120    avg_loss:0.236, val_acc:0.969]
Epoch [70/120    avg_loss:0.266, val_acc:0.967]
Epoch [71/120    avg_loss:0.250, val_acc:0.956]
Epoch [72/120    avg_loss:0.205, val_acc:0.967]
Epoch [73/120    avg_loss:0.191, val_acc:0.960]
Epoch [74/120    avg_loss:0.276, val_acc:0.944]
Epoch [75/120    avg_loss:0.256, val_acc:0.958]
Epoch [76/120    avg_loss:0.206, val_acc:0.981]
Epoch [77/120    avg_loss:0.192, val_acc:0.981]
Epoch [78/120    avg_loss:0.164, val_acc:0.967]
Epoch [79/120    avg_loss:0.199, val_acc:0.965]
Epoch [80/120    avg_loss:0.198, val_acc:0.975]
Epoch [81/120    avg_loss:0.201, val_acc:0.977]
Epoch [82/120    avg_loss:0.207, val_acc:0.975]
Epoch [83/120    avg_loss:0.211, val_acc:0.971]
Epoch [84/120    avg_loss:0.203, val_acc:0.973]
Epoch [85/120    avg_loss:0.183, val_acc:0.977]
Epoch [86/120    avg_loss:0.162, val_acc:0.983]
Epoch [87/120    avg_loss:0.189, val_acc:0.979]
Epoch [88/120    avg_loss:0.163, val_acc:0.960]
Epoch [89/120    avg_loss:0.184, val_acc:0.956]
Epoch [90/120    avg_loss:0.146, val_acc:0.983]
Epoch [91/120    avg_loss:0.117, val_acc:0.981]
Epoch [92/120    avg_loss:0.120, val_acc:0.992]
Epoch [93/120    avg_loss:0.109, val_acc:0.983]
Epoch [94/120    avg_loss:0.142, val_acc:0.990]
Epoch [95/120    avg_loss:0.114, val_acc:0.981]
Epoch [96/120    avg_loss:0.099, val_acc:0.979]
Epoch [97/120    avg_loss:0.099, val_acc:0.990]
Epoch [98/120    avg_loss:0.078, val_acc:0.990]
Epoch [99/120    avg_loss:0.082, val_acc:0.990]
Epoch [100/120    avg_loss:0.109, val_acc:0.983]
Epoch [101/120    avg_loss:0.123, val_acc:0.973]
Epoch [102/120    avg_loss:0.175, val_acc:0.973]
Epoch [103/120    avg_loss:0.166, val_acc:0.988]
Epoch [104/120    avg_loss:0.156, val_acc:0.983]
Epoch [105/120    avg_loss:0.141, val_acc:0.985]
Epoch [106/120    avg_loss:0.097, val_acc:0.985]
Epoch [107/120    avg_loss:0.087, val_acc:0.990]
Epoch [108/120    avg_loss:0.097, val_acc:0.990]
Epoch [109/120    avg_loss:0.076, val_acc:0.992]
Epoch [110/120    avg_loss:0.083, val_acc:0.992]
Epoch [111/120    avg_loss:0.079, val_acc:0.994]
Epoch [112/120    avg_loss:0.077, val_acc:0.992]
Epoch [113/120    avg_loss:0.073, val_acc:0.992]
Epoch [114/120    avg_loss:0.073, val_acc:0.992]
Epoch [115/120    avg_loss:0.081, val_acc:0.994]
Epoch [116/120    avg_loss:0.063, val_acc:0.992]
Epoch [117/120    avg_loss:0.069, val_acc:0.994]
Epoch [118/120    avg_loss:0.067, val_acc:0.992]
Epoch [119/120    avg_loss:0.074, val_acc:0.994]
Epoch [120/120    avg_loss:0.063, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99707174 0.98426966 1.         0.95238095 0.93069307
 0.99038462 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924046034540531
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7d7ca6e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.652, val_acc:0.100]
Epoch [2/120    avg_loss:2.614, val_acc:0.156]
Epoch [3/120    avg_loss:2.582, val_acc:0.210]
Epoch [4/120    avg_loss:2.555, val_acc:0.315]
Epoch [5/120    avg_loss:2.530, val_acc:0.371]
Epoch [6/120    avg_loss:2.504, val_acc:0.390]
Epoch [7/120    avg_loss:2.479, val_acc:0.394]
Epoch [8/120    avg_loss:2.450, val_acc:0.410]
Epoch [9/120    avg_loss:2.421, val_acc:0.442]
Epoch [10/120    avg_loss:2.383, val_acc:0.463]
Epoch [11/120    avg_loss:2.356, val_acc:0.485]
Epoch [12/120    avg_loss:2.324, val_acc:0.496]
Epoch [13/120    avg_loss:2.291, val_acc:0.525]
Epoch [14/120    avg_loss:2.240, val_acc:0.562]
Epoch [15/120    avg_loss:2.211, val_acc:0.613]
Epoch [16/120    avg_loss:2.163, val_acc:0.613]
Epoch [17/120    avg_loss:2.117, val_acc:0.667]
Epoch [18/120    avg_loss:2.064, val_acc:0.694]
Epoch [19/120    avg_loss:2.010, val_acc:0.702]
Epoch [20/120    avg_loss:1.947, val_acc:0.698]
Epoch [21/120    avg_loss:1.899, val_acc:0.698]
Epoch [22/120    avg_loss:1.845, val_acc:0.729]
Epoch [23/120    avg_loss:1.782, val_acc:0.754]
Epoch [24/120    avg_loss:1.703, val_acc:0.748]
Epoch [25/120    avg_loss:1.641, val_acc:0.754]
Epoch [26/120    avg_loss:1.532, val_acc:0.787]
Epoch [27/120    avg_loss:1.473, val_acc:0.750]
Epoch [28/120    avg_loss:1.449, val_acc:0.787]
Epoch [29/120    avg_loss:1.358, val_acc:0.760]
Epoch [30/120    avg_loss:1.269, val_acc:0.760]
Epoch [31/120    avg_loss:1.182, val_acc:0.815]
Epoch [32/120    avg_loss:1.106, val_acc:0.785]
Epoch [33/120    avg_loss:1.031, val_acc:0.856]
Epoch [34/120    avg_loss:1.016, val_acc:0.904]
Epoch [35/120    avg_loss:0.977, val_acc:0.887]
Epoch [36/120    avg_loss:0.881, val_acc:0.842]
Epoch [37/120    avg_loss:0.822, val_acc:0.925]
Epoch [38/120    avg_loss:0.744, val_acc:0.915]
Epoch [39/120    avg_loss:0.702, val_acc:0.910]
Epoch [40/120    avg_loss:0.637, val_acc:0.929]
Epoch [41/120    avg_loss:0.593, val_acc:0.946]
Epoch [42/120    avg_loss:0.608, val_acc:0.938]
Epoch [43/120    avg_loss:0.658, val_acc:0.917]
Epoch [44/120    avg_loss:0.596, val_acc:0.923]
Epoch [45/120    avg_loss:0.505, val_acc:0.958]
Epoch [46/120    avg_loss:0.453, val_acc:0.956]
Epoch [47/120    avg_loss:0.415, val_acc:0.946]
Epoch [48/120    avg_loss:0.418, val_acc:0.958]
Epoch [49/120    avg_loss:0.399, val_acc:0.960]
Epoch [50/120    avg_loss:0.469, val_acc:0.967]
Epoch [51/120    avg_loss:0.394, val_acc:0.956]
Epoch [52/120    avg_loss:0.423, val_acc:0.952]
Epoch [53/120    avg_loss:0.425, val_acc:0.963]
Epoch [54/120    avg_loss:0.395, val_acc:0.942]
Epoch [55/120    avg_loss:0.372, val_acc:0.948]
Epoch [56/120    avg_loss:0.332, val_acc:0.963]
Epoch [57/120    avg_loss:0.330, val_acc:0.963]
Epoch [58/120    avg_loss:0.324, val_acc:0.954]
Epoch [59/120    avg_loss:0.346, val_acc:0.960]
Epoch [60/120    avg_loss:0.300, val_acc:0.956]
Epoch [61/120    avg_loss:0.271, val_acc:0.960]
Epoch [62/120    avg_loss:0.296, val_acc:0.963]
Epoch [63/120    avg_loss:0.297, val_acc:0.973]
Epoch [64/120    avg_loss:0.226, val_acc:0.960]
Epoch [65/120    avg_loss:0.203, val_acc:0.975]
Epoch [66/120    avg_loss:0.236, val_acc:0.954]
Epoch [67/120    avg_loss:0.208, val_acc:0.965]
Epoch [68/120    avg_loss:0.243, val_acc:0.954]
Epoch [69/120    avg_loss:0.231, val_acc:0.967]
Epoch [70/120    avg_loss:0.299, val_acc:0.940]
Epoch [71/120    avg_loss:0.302, val_acc:0.921]
Epoch [72/120    avg_loss:0.242, val_acc:0.967]
Epoch [73/120    avg_loss:0.211, val_acc:0.969]
Epoch [74/120    avg_loss:0.215, val_acc:0.979]
Epoch [75/120    avg_loss:0.191, val_acc:0.979]
Epoch [76/120    avg_loss:0.156, val_acc:0.990]
Epoch [77/120    avg_loss:0.150, val_acc:0.973]
Epoch [78/120    avg_loss:0.168, val_acc:0.977]
Epoch [79/120    avg_loss:0.134, val_acc:0.983]
Epoch [80/120    avg_loss:0.156, val_acc:0.973]
Epoch [81/120    avg_loss:0.177, val_acc:0.963]
Epoch [82/120    avg_loss:0.161, val_acc:0.963]
Epoch [83/120    avg_loss:0.141, val_acc:0.973]
Epoch [84/120    avg_loss:0.182, val_acc:0.983]
Epoch [85/120    avg_loss:0.121, val_acc:0.988]
Epoch [86/120    avg_loss:0.125, val_acc:0.971]
Epoch [87/120    avg_loss:0.126, val_acc:0.985]
Epoch [88/120    avg_loss:0.223, val_acc:0.960]
Epoch [89/120    avg_loss:0.219, val_acc:0.958]
Epoch [90/120    avg_loss:0.188, val_acc:0.975]
Epoch [91/120    avg_loss:0.134, val_acc:0.975]
Epoch [92/120    avg_loss:0.135, val_acc:0.981]
Epoch [93/120    avg_loss:0.118, val_acc:0.983]
Epoch [94/120    avg_loss:0.109, val_acc:0.985]
Epoch [95/120    avg_loss:0.107, val_acc:0.988]
Epoch [96/120    avg_loss:0.102, val_acc:0.988]
Epoch [97/120    avg_loss:0.093, val_acc:0.988]
Epoch [98/120    avg_loss:0.092, val_acc:0.985]
Epoch [99/120    avg_loss:0.087, val_acc:0.988]
Epoch [100/120    avg_loss:0.088, val_acc:0.990]
Epoch [101/120    avg_loss:0.087, val_acc:0.992]
Epoch [102/120    avg_loss:0.086, val_acc:0.988]
Epoch [103/120    avg_loss:0.076, val_acc:0.994]
Epoch [104/120    avg_loss:0.078, val_acc:0.992]
Epoch [105/120    avg_loss:0.090, val_acc:0.992]
Epoch [106/120    avg_loss:0.089, val_acc:0.992]
Epoch [107/120    avg_loss:0.074, val_acc:0.988]
Epoch [108/120    avg_loss:0.079, val_acc:0.990]
Epoch [109/120    avg_loss:0.072, val_acc:0.990]
Epoch [110/120    avg_loss:0.080, val_acc:0.990]
Epoch [111/120    avg_loss:0.072, val_acc:0.992]
Epoch [112/120    avg_loss:0.083, val_acc:0.994]
Epoch [113/120    avg_loss:0.077, val_acc:0.992]
Epoch [114/120    avg_loss:0.070, val_acc:0.996]
Epoch [115/120    avg_loss:0.072, val_acc:0.996]
Epoch [116/120    avg_loss:0.080, val_acc:0.990]
Epoch [117/120    avg_loss:0.077, val_acc:0.992]
Epoch [118/120    avg_loss:0.075, val_acc:0.994]
Epoch [119/120    avg_loss:0.072, val_acc:0.992]
Epoch [120/120    avg_loss:0.071, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.98426966 0.99563319 0.91028446 0.85714286
 1.         0.95081967 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9881304172054588
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f534d2b1eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.102]
Epoch [2/120    avg_loss:2.575, val_acc:0.190]
Epoch [3/120    avg_loss:2.545, val_acc:0.302]
Epoch [4/120    avg_loss:2.512, val_acc:0.321]
Epoch [5/120    avg_loss:2.483, val_acc:0.321]
Epoch [6/120    avg_loss:2.447, val_acc:0.321]
Epoch [7/120    avg_loss:2.422, val_acc:0.321]
Epoch [8/120    avg_loss:2.390, val_acc:0.321]
Epoch [9/120    avg_loss:2.365, val_acc:0.321]
Epoch [10/120    avg_loss:2.340, val_acc:0.321]
Epoch [11/120    avg_loss:2.303, val_acc:0.323]
Epoch [12/120    avg_loss:2.274, val_acc:0.331]
Epoch [13/120    avg_loss:2.249, val_acc:0.338]
Epoch [14/120    avg_loss:2.215, val_acc:0.329]
Epoch [15/120    avg_loss:2.182, val_acc:0.333]
Epoch [16/120    avg_loss:2.147, val_acc:0.369]
Epoch [17/120    avg_loss:2.098, val_acc:0.396]
Epoch [18/120    avg_loss:2.040, val_acc:0.402]
Epoch [19/120    avg_loss:1.996, val_acc:0.427]
Epoch [20/120    avg_loss:1.967, val_acc:0.450]
Epoch [21/120    avg_loss:1.926, val_acc:0.446]
Epoch [22/120    avg_loss:1.864, val_acc:0.469]
Epoch [23/120    avg_loss:1.855, val_acc:0.496]
Epoch [24/120    avg_loss:1.768, val_acc:0.512]
Epoch [25/120    avg_loss:1.710, val_acc:0.598]
Epoch [26/120    avg_loss:1.657, val_acc:0.629]
Epoch [27/120    avg_loss:1.599, val_acc:0.642]
Epoch [28/120    avg_loss:1.550, val_acc:0.667]
Epoch [29/120    avg_loss:1.488, val_acc:0.702]
Epoch [30/120    avg_loss:1.420, val_acc:0.715]
Epoch [31/120    avg_loss:1.411, val_acc:0.700]
Epoch [32/120    avg_loss:1.304, val_acc:0.752]
Epoch [33/120    avg_loss:1.223, val_acc:0.758]
Epoch [34/120    avg_loss:1.190, val_acc:0.779]
Epoch [35/120    avg_loss:1.158, val_acc:0.765]
Epoch [36/120    avg_loss:1.043, val_acc:0.773]
Epoch [37/120    avg_loss:0.982, val_acc:0.802]
Epoch [38/120    avg_loss:0.933, val_acc:0.819]
Epoch [39/120    avg_loss:0.884, val_acc:0.817]
Epoch [40/120    avg_loss:0.857, val_acc:0.815]
Epoch [41/120    avg_loss:0.807, val_acc:0.829]
Epoch [42/120    avg_loss:0.723, val_acc:0.894]
Epoch [43/120    avg_loss:0.680, val_acc:0.863]
Epoch [44/120    avg_loss:0.644, val_acc:0.867]
Epoch [45/120    avg_loss:0.626, val_acc:0.881]
Epoch [46/120    avg_loss:0.571, val_acc:0.915]
Epoch [47/120    avg_loss:0.527, val_acc:0.917]
Epoch [48/120    avg_loss:0.544, val_acc:0.931]
Epoch [49/120    avg_loss:0.556, val_acc:0.931]
Epoch [50/120    avg_loss:0.567, val_acc:0.883]
Epoch [51/120    avg_loss:0.474, val_acc:0.927]
Epoch [52/120    avg_loss:0.441, val_acc:0.952]
Epoch [53/120    avg_loss:0.403, val_acc:0.933]
Epoch [54/120    avg_loss:0.414, val_acc:0.906]
Epoch [55/120    avg_loss:0.355, val_acc:0.921]
Epoch [56/120    avg_loss:0.359, val_acc:0.921]
Epoch [57/120    avg_loss:0.410, val_acc:0.952]
Epoch [58/120    avg_loss:0.404, val_acc:0.954]
Epoch [59/120    avg_loss:0.387, val_acc:0.915]
Epoch [60/120    avg_loss:0.364, val_acc:0.958]
Epoch [61/120    avg_loss:0.325, val_acc:0.952]
Epoch [62/120    avg_loss:0.313, val_acc:0.963]
Epoch [63/120    avg_loss:0.294, val_acc:0.967]
Epoch [64/120    avg_loss:0.254, val_acc:0.977]
Epoch [65/120    avg_loss:0.239, val_acc:0.956]
Epoch [66/120    avg_loss:0.226, val_acc:0.971]
Epoch [67/120    avg_loss:0.212, val_acc:0.944]
Epoch [68/120    avg_loss:0.218, val_acc:0.971]
Epoch [69/120    avg_loss:0.236, val_acc:0.971]
Epoch [70/120    avg_loss:0.221, val_acc:0.977]
Epoch [71/120    avg_loss:0.194, val_acc:0.969]
Epoch [72/120    avg_loss:0.198, val_acc:0.965]
Epoch [73/120    avg_loss:0.196, val_acc:0.973]
Epoch [74/120    avg_loss:0.202, val_acc:0.971]
Epoch [75/120    avg_loss:0.214, val_acc:0.979]
Epoch [76/120    avg_loss:0.221, val_acc:0.977]
Epoch [77/120    avg_loss:0.168, val_acc:0.979]
Epoch [78/120    avg_loss:0.157, val_acc:0.965]
Epoch [79/120    avg_loss:0.136, val_acc:0.977]
Epoch [80/120    avg_loss:0.159, val_acc:0.975]
Epoch [81/120    avg_loss:0.129, val_acc:0.977]
Epoch [82/120    avg_loss:0.151, val_acc:0.973]
Epoch [83/120    avg_loss:0.158, val_acc:0.969]
Epoch [84/120    avg_loss:0.186, val_acc:0.954]
Epoch [85/120    avg_loss:0.179, val_acc:0.960]
Epoch [86/120    avg_loss:0.164, val_acc:0.975]
Epoch [87/120    avg_loss:0.162, val_acc:0.977]
Epoch [88/120    avg_loss:0.160, val_acc:0.942]
Epoch [89/120    avg_loss:0.139, val_acc:0.983]
Epoch [90/120    avg_loss:0.109, val_acc:0.985]
Epoch [91/120    avg_loss:0.092, val_acc:0.990]
Epoch [92/120    avg_loss:0.088, val_acc:0.992]
Epoch [93/120    avg_loss:0.075, val_acc:0.994]
Epoch [94/120    avg_loss:0.081, val_acc:0.983]
Epoch [95/120    avg_loss:0.096, val_acc:0.983]
Epoch [96/120    avg_loss:0.123, val_acc:0.971]
Epoch [97/120    avg_loss:0.127, val_acc:0.975]
Epoch [98/120    avg_loss:0.096, val_acc:0.988]
Epoch [99/120    avg_loss:0.079, val_acc:0.979]
Epoch [100/120    avg_loss:0.080, val_acc:0.981]
Epoch [101/120    avg_loss:0.094, val_acc:0.990]
Epoch [102/120    avg_loss:0.083, val_acc:0.988]
Epoch [103/120    avg_loss:0.067, val_acc:0.985]
Epoch [104/120    avg_loss:0.061, val_acc:0.992]
Epoch [105/120    avg_loss:0.050, val_acc:0.994]
Epoch [106/120    avg_loss:0.054, val_acc:0.992]
Epoch [107/120    avg_loss:0.056, val_acc:0.992]
Epoch [108/120    avg_loss:0.054, val_acc:0.979]
Epoch [109/120    avg_loss:0.083, val_acc:0.988]
Epoch [110/120    avg_loss:0.059, val_acc:0.985]
Epoch [111/120    avg_loss:0.116, val_acc:0.963]
Epoch [112/120    avg_loss:0.088, val_acc:0.983]
Epoch [113/120    avg_loss:0.087, val_acc:0.992]
Epoch [114/120    avg_loss:0.075, val_acc:0.983]
Epoch [115/120    avg_loss:0.052, val_acc:0.979]
Epoch [116/120    avg_loss:0.040, val_acc:0.992]
Epoch [117/120    avg_loss:0.076, val_acc:0.988]
Epoch [118/120    avg_loss:0.062, val_acc:0.979]
Epoch [119/120    avg_loss:0.052, val_acc:0.985]
Epoch [120/120    avg_loss:0.045, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  20   0   0   0   0   0   0   1   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99780541 0.97986577 1.         0.94930876 0.93203883
 0.99277108 0.94972067 1.         1.         1.         0.99603699
 0.99557522 1.        ]

Kappa:
0.9912177182950925
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7955100e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.169]
Epoch [2/120    avg_loss:2.600, val_acc:0.188]
Epoch [3/120    avg_loss:2.563, val_acc:0.246]
Epoch [4/120    avg_loss:2.527, val_acc:0.388]
Epoch [5/120    avg_loss:2.496, val_acc:0.388]
Epoch [6/120    avg_loss:2.453, val_acc:0.388]
Epoch [7/120    avg_loss:2.426, val_acc:0.392]
Epoch [8/120    avg_loss:2.388, val_acc:0.419]
Epoch [9/120    avg_loss:2.362, val_acc:0.440]
Epoch [10/120    avg_loss:2.330, val_acc:0.473]
Epoch [11/120    avg_loss:2.282, val_acc:0.531]
Epoch [12/120    avg_loss:2.251, val_acc:0.542]
Epoch [13/120    avg_loss:2.222, val_acc:0.548]
Epoch [14/120    avg_loss:2.158, val_acc:0.562]
Epoch [15/120    avg_loss:2.121, val_acc:0.548]
Epoch [16/120    avg_loss:2.062, val_acc:0.537]
Epoch [17/120    avg_loss:2.045, val_acc:0.560]
Epoch [18/120    avg_loss:1.965, val_acc:0.521]
Epoch [19/120    avg_loss:1.910, val_acc:0.548]
Epoch [20/120    avg_loss:1.879, val_acc:0.546]
Epoch [21/120    avg_loss:1.869, val_acc:0.531]
Epoch [22/120    avg_loss:1.808, val_acc:0.581]
Epoch [23/120    avg_loss:1.754, val_acc:0.625]
Epoch [24/120    avg_loss:1.674, val_acc:0.610]
Epoch [25/120    avg_loss:1.598, val_acc:0.619]
Epoch [26/120    avg_loss:1.563, val_acc:0.640]
Epoch [27/120    avg_loss:1.502, val_acc:0.652]
Epoch [28/120    avg_loss:1.428, val_acc:0.675]
Epoch [29/120    avg_loss:1.378, val_acc:0.669]
Epoch [30/120    avg_loss:1.270, val_acc:0.685]
Epoch [31/120    avg_loss:1.234, val_acc:0.713]
Epoch [32/120    avg_loss:1.153, val_acc:0.725]
Epoch [33/120    avg_loss:1.133, val_acc:0.758]
Epoch [34/120    avg_loss:1.017, val_acc:0.752]
Epoch [35/120    avg_loss:1.017, val_acc:0.731]
Epoch [36/120    avg_loss:0.955, val_acc:0.792]
Epoch [37/120    avg_loss:0.939, val_acc:0.775]
Epoch [38/120    avg_loss:0.911, val_acc:0.817]
Epoch [39/120    avg_loss:0.821, val_acc:0.817]
Epoch [40/120    avg_loss:0.764, val_acc:0.815]
Epoch [41/120    avg_loss:0.720, val_acc:0.885]
Epoch [42/120    avg_loss:0.661, val_acc:0.896]
Epoch [43/120    avg_loss:0.604, val_acc:0.927]
Epoch [44/120    avg_loss:0.602, val_acc:0.892]
Epoch [45/120    avg_loss:0.681, val_acc:0.908]
Epoch [46/120    avg_loss:0.681, val_acc:0.883]
Epoch [47/120    avg_loss:0.633, val_acc:0.898]
Epoch [48/120    avg_loss:0.585, val_acc:0.927]
Epoch [49/120    avg_loss:0.525, val_acc:0.898]
Epoch [50/120    avg_loss:0.495, val_acc:0.910]
Epoch [51/120    avg_loss:0.464, val_acc:0.933]
Epoch [52/120    avg_loss:0.440, val_acc:0.925]
Epoch [53/120    avg_loss:0.401, val_acc:0.873]
Epoch [54/120    avg_loss:0.428, val_acc:0.944]
Epoch [55/120    avg_loss:0.414, val_acc:0.933]
Epoch [56/120    avg_loss:0.425, val_acc:0.940]
Epoch [57/120    avg_loss:0.371, val_acc:0.921]
Epoch [58/120    avg_loss:0.349, val_acc:0.954]
Epoch [59/120    avg_loss:0.341, val_acc:0.942]
Epoch [60/120    avg_loss:0.351, val_acc:0.946]
Epoch [61/120    avg_loss:0.328, val_acc:0.950]
Epoch [62/120    avg_loss:0.355, val_acc:0.960]
Epoch [63/120    avg_loss:0.324, val_acc:0.935]
Epoch [64/120    avg_loss:0.284, val_acc:0.956]
Epoch [65/120    avg_loss:0.311, val_acc:0.965]
Epoch [66/120    avg_loss:0.285, val_acc:0.929]
Epoch [67/120    avg_loss:0.303, val_acc:0.931]
Epoch [68/120    avg_loss:0.303, val_acc:0.935]
Epoch [69/120    avg_loss:0.333, val_acc:0.931]
Epoch [70/120    avg_loss:0.297, val_acc:0.954]
Epoch [71/120    avg_loss:0.350, val_acc:0.958]
Epoch [72/120    avg_loss:0.235, val_acc:0.948]
Epoch [73/120    avg_loss:0.239, val_acc:0.967]
Epoch [74/120    avg_loss:0.239, val_acc:0.969]
Epoch [75/120    avg_loss:0.266, val_acc:0.971]
Epoch [76/120    avg_loss:0.227, val_acc:0.954]
Epoch [77/120    avg_loss:0.239, val_acc:0.975]
Epoch [78/120    avg_loss:0.204, val_acc:0.965]
Epoch [79/120    avg_loss:0.197, val_acc:0.971]
Epoch [80/120    avg_loss:0.238, val_acc:0.931]
Epoch [81/120    avg_loss:0.281, val_acc:0.958]
Epoch [82/120    avg_loss:0.220, val_acc:0.935]
Epoch [83/120    avg_loss:0.206, val_acc:0.965]
Epoch [84/120    avg_loss:0.191, val_acc:0.958]
Epoch [85/120    avg_loss:0.183, val_acc:0.973]
Epoch [86/120    avg_loss:0.166, val_acc:0.971]
Epoch [87/120    avg_loss:0.161, val_acc:0.963]
Epoch [88/120    avg_loss:0.166, val_acc:0.977]
Epoch [89/120    avg_loss:0.147, val_acc:0.965]
Epoch [90/120    avg_loss:0.170, val_acc:0.973]
Epoch [91/120    avg_loss:0.152, val_acc:0.975]
Epoch [92/120    avg_loss:0.156, val_acc:0.975]
Epoch [93/120    avg_loss:0.148, val_acc:0.979]
Epoch [94/120    avg_loss:0.117, val_acc:0.981]
Epoch [95/120    avg_loss:0.153, val_acc:0.967]
Epoch [96/120    avg_loss:0.194, val_acc:0.967]
Epoch [97/120    avg_loss:0.170, val_acc:0.969]
Epoch [98/120    avg_loss:0.187, val_acc:0.960]
Epoch [99/120    avg_loss:0.210, val_acc:0.973]
Epoch [100/120    avg_loss:0.163, val_acc:0.963]
Epoch [101/120    avg_loss:0.163, val_acc:0.979]
Epoch [102/120    avg_loss:0.141, val_acc:0.971]
Epoch [103/120    avg_loss:0.137, val_acc:0.985]
Epoch [104/120    avg_loss:0.139, val_acc:0.981]
Epoch [105/120    avg_loss:0.112, val_acc:0.977]
Epoch [106/120    avg_loss:0.102, val_acc:0.977]
Epoch [107/120    avg_loss:0.114, val_acc:0.985]
Epoch [108/120    avg_loss:0.102, val_acc:0.983]
Epoch [109/120    avg_loss:0.128, val_acc:0.971]
Epoch [110/120    avg_loss:0.128, val_acc:0.969]
Epoch [111/120    avg_loss:0.095, val_acc:0.979]
Epoch [112/120    avg_loss:0.079, val_acc:0.979]
Epoch [113/120    avg_loss:0.084, val_acc:0.983]
Epoch [114/120    avg_loss:0.063, val_acc:0.988]
Epoch [115/120    avg_loss:0.067, val_acc:0.977]
Epoch [116/120    avg_loss:0.093, val_acc:0.963]
Epoch [117/120    avg_loss:0.080, val_acc:0.983]
Epoch [118/120    avg_loss:0.072, val_acc:0.975]
Epoch [119/120    avg_loss:0.070, val_acc:0.990]
Epoch [120/120    avg_loss:0.062, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 572   0   0   0   0 113   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 188  39   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.67377398720681

F1 scores:
[       nan 0.91010342 0.99541284 1.         0.9060241  0.88145897
 0.7847619  0.98947368 1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.963076377102408
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a9d736e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.027]
Epoch [2/120    avg_loss:2.567, val_acc:0.235]
Epoch [3/120    avg_loss:2.521, val_acc:0.335]
Epoch [4/120    avg_loss:2.480, val_acc:0.348]
Epoch [5/120    avg_loss:2.444, val_acc:0.346]
Epoch [6/120    avg_loss:2.395, val_acc:0.362]
Epoch [7/120    avg_loss:2.363, val_acc:0.369]
Epoch [8/120    avg_loss:2.315, val_acc:0.373]
Epoch [9/120    avg_loss:2.268, val_acc:0.388]
Epoch [10/120    avg_loss:2.234, val_acc:0.392]
Epoch [11/120    avg_loss:2.185, val_acc:0.406]
Epoch [12/120    avg_loss:2.142, val_acc:0.410]
Epoch [13/120    avg_loss:2.091, val_acc:0.421]
Epoch [14/120    avg_loss:2.048, val_acc:0.431]
Epoch [15/120    avg_loss:2.015, val_acc:0.460]
Epoch [16/120    avg_loss:1.940, val_acc:0.500]
Epoch [17/120    avg_loss:1.899, val_acc:0.521]
Epoch [18/120    avg_loss:1.837, val_acc:0.554]
Epoch [19/120    avg_loss:1.788, val_acc:0.635]
Epoch [20/120    avg_loss:1.735, val_acc:0.679]
Epoch [21/120    avg_loss:1.661, val_acc:0.698]
Epoch [22/120    avg_loss:1.599, val_acc:0.696]
Epoch [23/120    avg_loss:1.524, val_acc:0.735]
Epoch [24/120    avg_loss:1.460, val_acc:0.767]
Epoch [25/120    avg_loss:1.373, val_acc:0.771]
Epoch [26/120    avg_loss:1.284, val_acc:0.771]
Epoch [27/120    avg_loss:1.194, val_acc:0.798]
Epoch [28/120    avg_loss:1.125, val_acc:0.821]
Epoch [29/120    avg_loss:1.062, val_acc:0.787]
Epoch [30/120    avg_loss:1.026, val_acc:0.821]
Epoch [31/120    avg_loss:0.962, val_acc:0.787]
Epoch [32/120    avg_loss:0.881, val_acc:0.900]
Epoch [33/120    avg_loss:0.856, val_acc:0.785]
Epoch [34/120    avg_loss:0.814, val_acc:0.817]
Epoch [35/120    avg_loss:0.752, val_acc:0.917]
Epoch [36/120    avg_loss:0.764, val_acc:0.869]
Epoch [37/120    avg_loss:0.749, val_acc:0.900]
Epoch [38/120    avg_loss:0.641, val_acc:0.894]
Epoch [39/120    avg_loss:0.603, val_acc:0.865]
Epoch [40/120    avg_loss:0.590, val_acc:0.931]
Epoch [41/120    avg_loss:0.574, val_acc:0.898]
Epoch [42/120    avg_loss:0.589, val_acc:0.931]
Epoch [43/120    avg_loss:0.569, val_acc:0.940]
Epoch [44/120    avg_loss:0.520, val_acc:0.954]
Epoch [45/120    avg_loss:0.430, val_acc:0.942]
Epoch [46/120    avg_loss:0.448, val_acc:0.944]
Epoch [47/120    avg_loss:0.430, val_acc:0.942]
Epoch [48/120    avg_loss:0.415, val_acc:0.942]
Epoch [49/120    avg_loss:0.427, val_acc:0.942]
Epoch [50/120    avg_loss:0.348, val_acc:0.950]
Epoch [51/120    avg_loss:0.344, val_acc:0.942]
Epoch [52/120    avg_loss:0.365, val_acc:0.967]
Epoch [53/120    avg_loss:0.367, val_acc:0.923]
Epoch [54/120    avg_loss:0.390, val_acc:0.950]
Epoch [55/120    avg_loss:0.312, val_acc:0.971]
Epoch [56/120    avg_loss:0.313, val_acc:0.954]
Epoch [57/120    avg_loss:0.280, val_acc:0.960]
Epoch [58/120    avg_loss:0.281, val_acc:0.956]
Epoch [59/120    avg_loss:0.271, val_acc:0.958]
Epoch [60/120    avg_loss:0.295, val_acc:0.956]
Epoch [61/120    avg_loss:0.234, val_acc:0.969]
Epoch [62/120    avg_loss:0.219, val_acc:0.973]
Epoch [63/120    avg_loss:0.237, val_acc:0.969]
Epoch [64/120    avg_loss:0.218, val_acc:0.975]
Epoch [65/120    avg_loss:0.260, val_acc:0.963]
Epoch [66/120    avg_loss:0.221, val_acc:0.973]
Epoch [67/120    avg_loss:0.185, val_acc:0.975]
Epoch [68/120    avg_loss:0.220, val_acc:0.963]
Epoch [69/120    avg_loss:0.183, val_acc:0.969]
Epoch [70/120    avg_loss:0.167, val_acc:0.975]
Epoch [71/120    avg_loss:0.154, val_acc:0.956]
Epoch [72/120    avg_loss:0.207, val_acc:0.971]
Epoch [73/120    avg_loss:0.214, val_acc:0.950]
Epoch [74/120    avg_loss:0.218, val_acc:0.963]
Epoch [75/120    avg_loss:0.189, val_acc:0.977]
Epoch [76/120    avg_loss:0.187, val_acc:0.990]
Epoch [77/120    avg_loss:0.177, val_acc:0.977]
Epoch [78/120    avg_loss:0.139, val_acc:0.973]
Epoch [79/120    avg_loss:0.139, val_acc:0.954]
Epoch [80/120    avg_loss:0.156, val_acc:0.990]
Epoch [81/120    avg_loss:0.119, val_acc:0.977]
Epoch [82/120    avg_loss:0.123, val_acc:0.983]
Epoch [83/120    avg_loss:0.110, val_acc:0.992]
Epoch [84/120    avg_loss:0.137, val_acc:0.973]
Epoch [85/120    avg_loss:0.149, val_acc:0.973]
Epoch [86/120    avg_loss:0.157, val_acc:0.983]
Epoch [87/120    avg_loss:0.142, val_acc:0.981]
Epoch [88/120    avg_loss:0.141, val_acc:0.958]
Epoch [89/120    avg_loss:0.129, val_acc:0.979]
Epoch [90/120    avg_loss:0.139, val_acc:0.983]
Epoch [91/120    avg_loss:0.101, val_acc:0.985]
Epoch [92/120    avg_loss:0.110, val_acc:0.975]
Epoch [93/120    avg_loss:0.123, val_acc:0.985]
Epoch [94/120    avg_loss:0.108, val_acc:0.977]
Epoch [95/120    avg_loss:0.137, val_acc:0.977]
Epoch [96/120    avg_loss:0.125, val_acc:0.969]
Epoch [97/120    avg_loss:0.106, val_acc:0.973]
Epoch [98/120    avg_loss:0.082, val_acc:0.985]
Epoch [99/120    avg_loss:0.077, val_acc:0.988]
Epoch [100/120    avg_loss:0.083, val_acc:0.988]
Epoch [101/120    avg_loss:0.084, val_acc:0.988]
Epoch [102/120    avg_loss:0.076, val_acc:0.990]
Epoch [103/120    avg_loss:0.070, val_acc:0.992]
Epoch [104/120    avg_loss:0.078, val_acc:0.992]
Epoch [105/120    avg_loss:0.071, val_acc:0.992]
Epoch [106/120    avg_loss:0.068, val_acc:0.992]
Epoch [107/120    avg_loss:0.071, val_acc:0.992]
Epoch [108/120    avg_loss:0.072, val_acc:0.990]
Epoch [109/120    avg_loss:0.072, val_acc:0.992]
Epoch [110/120    avg_loss:0.070, val_acc:0.990]
Epoch [111/120    avg_loss:0.064, val_acc:0.990]
Epoch [112/120    avg_loss:0.070, val_acc:0.992]
Epoch [113/120    avg_loss:0.062, val_acc:0.992]
Epoch [114/120    avg_loss:0.061, val_acc:0.992]
Epoch [115/120    avg_loss:0.073, val_acc:0.990]
Epoch [116/120    avg_loss:0.067, val_acc:0.992]
Epoch [117/120    avg_loss:0.059, val_acc:0.992]
Epoch [118/120    avg_loss:0.068, val_acc:0.992]
Epoch [119/120    avg_loss:0.057, val_acc:0.992]
Epoch [120/120    avg_loss:0.073, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 0.99264706 1.         1.         0.92063492 0.88448845
 0.97630332 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9893209997669393
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5696371be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.000]
Epoch [2/120    avg_loss:2.616, val_acc:0.113]
Epoch [3/120    avg_loss:2.588, val_acc:0.190]
Epoch [4/120    avg_loss:2.559, val_acc:0.223]
Epoch [5/120    avg_loss:2.530, val_acc:0.273]
Epoch [6/120    avg_loss:2.504, val_acc:0.306]
Epoch [7/120    avg_loss:2.468, val_acc:0.323]
Epoch [8/120    avg_loss:2.437, val_acc:0.331]
Epoch [9/120    avg_loss:2.409, val_acc:0.340]
Epoch [10/120    avg_loss:2.376, val_acc:0.350]
Epoch [11/120    avg_loss:2.347, val_acc:0.348]
Epoch [12/120    avg_loss:2.314, val_acc:0.354]
Epoch [13/120    avg_loss:2.280, val_acc:0.367]
Epoch [14/120    avg_loss:2.238, val_acc:0.406]
Epoch [15/120    avg_loss:2.204, val_acc:0.431]
Epoch [16/120    avg_loss:2.164, val_acc:0.454]
Epoch [17/120    avg_loss:2.112, val_acc:0.458]
Epoch [18/120    avg_loss:2.052, val_acc:0.454]
Epoch [19/120    avg_loss:2.009, val_acc:0.446]
Epoch [20/120    avg_loss:1.954, val_acc:0.485]
Epoch [21/120    avg_loss:1.911, val_acc:0.565]
Epoch [22/120    avg_loss:1.857, val_acc:0.590]
Epoch [23/120    avg_loss:1.814, val_acc:0.617]
Epoch [24/120    avg_loss:1.751, val_acc:0.658]
Epoch [25/120    avg_loss:1.682, val_acc:0.669]
Epoch [26/120    avg_loss:1.627, val_acc:0.698]
Epoch [27/120    avg_loss:1.564, val_acc:0.679]
Epoch [28/120    avg_loss:1.515, val_acc:0.677]
Epoch [29/120    avg_loss:1.459, val_acc:0.700]
Epoch [30/120    avg_loss:1.412, val_acc:0.727]
Epoch [31/120    avg_loss:1.361, val_acc:0.721]
Epoch [32/120    avg_loss:1.320, val_acc:0.713]
Epoch [33/120    avg_loss:1.261, val_acc:0.754]
Epoch [34/120    avg_loss:1.170, val_acc:0.750]
Epoch [35/120    avg_loss:1.103, val_acc:0.760]
Epoch [36/120    avg_loss:1.047, val_acc:0.775]
Epoch [37/120    avg_loss:1.026, val_acc:0.781]
Epoch [38/120    avg_loss:0.957, val_acc:0.773]
Epoch [39/120    avg_loss:0.908, val_acc:0.777]
Epoch [40/120    avg_loss:0.863, val_acc:0.787]
Epoch [41/120    avg_loss:0.863, val_acc:0.783]
Epoch [42/120    avg_loss:0.800, val_acc:0.790]
Epoch [43/120    avg_loss:0.802, val_acc:0.787]
Epoch [44/120    avg_loss:0.727, val_acc:0.804]
Epoch [45/120    avg_loss:0.707, val_acc:0.821]
Epoch [46/120    avg_loss:0.661, val_acc:0.802]
Epoch [47/120    avg_loss:0.631, val_acc:0.846]
Epoch [48/120    avg_loss:0.587, val_acc:0.900]
Epoch [49/120    avg_loss:0.535, val_acc:0.869]
Epoch [50/120    avg_loss:0.490, val_acc:0.846]
Epoch [51/120    avg_loss:0.502, val_acc:0.877]
Epoch [52/120    avg_loss:0.484, val_acc:0.931]
Epoch [53/120    avg_loss:0.496, val_acc:0.892]
Epoch [54/120    avg_loss:0.538, val_acc:0.915]
Epoch [55/120    avg_loss:0.411, val_acc:0.944]
Epoch [56/120    avg_loss:0.359, val_acc:0.935]
Epoch [57/120    avg_loss:0.330, val_acc:0.946]
Epoch [58/120    avg_loss:0.354, val_acc:0.944]
Epoch [59/120    avg_loss:0.338, val_acc:0.940]
Epoch [60/120    avg_loss:0.337, val_acc:0.940]
Epoch [61/120    avg_loss:0.292, val_acc:0.931]
Epoch [62/120    avg_loss:0.306, val_acc:0.960]
Epoch [63/120    avg_loss:0.257, val_acc:0.956]
Epoch [64/120    avg_loss:0.247, val_acc:0.963]
Epoch [65/120    avg_loss:0.226, val_acc:0.956]
Epoch [66/120    avg_loss:0.258, val_acc:0.942]
Epoch [67/120    avg_loss:0.286, val_acc:0.952]
Epoch [68/120    avg_loss:0.298, val_acc:0.954]
Epoch [69/120    avg_loss:0.282, val_acc:0.958]
Epoch [70/120    avg_loss:0.245, val_acc:0.958]
Epoch [71/120    avg_loss:0.229, val_acc:0.960]
Epoch [72/120    avg_loss:0.211, val_acc:0.946]
Epoch [73/120    avg_loss:0.263, val_acc:0.950]
Epoch [74/120    avg_loss:0.237, val_acc:0.960]
Epoch [75/120    avg_loss:0.174, val_acc:0.971]
Epoch [76/120    avg_loss:0.208, val_acc:0.967]
Epoch [77/120    avg_loss:0.211, val_acc:0.952]
Epoch [78/120    avg_loss:0.230, val_acc:0.971]
Epoch [79/120    avg_loss:0.156, val_acc:0.969]
Epoch [80/120    avg_loss:0.159, val_acc:0.975]
Epoch [81/120    avg_loss:0.196, val_acc:0.979]
Epoch [82/120    avg_loss:0.214, val_acc:0.963]
Epoch [83/120    avg_loss:0.196, val_acc:0.967]
Epoch [84/120    avg_loss:0.162, val_acc:0.977]
Epoch [85/120    avg_loss:0.161, val_acc:0.950]
Epoch [86/120    avg_loss:0.135, val_acc:0.969]
Epoch [87/120    avg_loss:0.147, val_acc:0.985]
Epoch [88/120    avg_loss:0.104, val_acc:0.979]
Epoch [89/120    avg_loss:0.095, val_acc:0.971]
Epoch [90/120    avg_loss:0.117, val_acc:0.967]
Epoch [91/120    avg_loss:0.097, val_acc:0.977]
Epoch [92/120    avg_loss:0.093, val_acc:0.977]
Epoch [93/120    avg_loss:0.120, val_acc:0.960]
Epoch [94/120    avg_loss:0.105, val_acc:0.979]
Epoch [95/120    avg_loss:0.108, val_acc:0.954]
Epoch [96/120    avg_loss:0.135, val_acc:0.963]
Epoch [97/120    avg_loss:0.158, val_acc:0.967]
Epoch [98/120    avg_loss:0.118, val_acc:0.967]
Epoch [99/120    avg_loss:0.093, val_acc:0.969]
Epoch [100/120    avg_loss:0.078, val_acc:0.969]
Epoch [101/120    avg_loss:0.092, val_acc:0.979]
Epoch [102/120    avg_loss:0.074, val_acc:0.983]
Epoch [103/120    avg_loss:0.054, val_acc:0.981]
Epoch [104/120    avg_loss:0.067, val_acc:0.979]
Epoch [105/120    avg_loss:0.064, val_acc:0.977]
Epoch [106/120    avg_loss:0.056, val_acc:0.977]
Epoch [107/120    avg_loss:0.067, val_acc:0.979]
Epoch [108/120    avg_loss:0.054, val_acc:0.981]
Epoch [109/120    avg_loss:0.073, val_acc:0.981]
Epoch [110/120    avg_loss:0.058, val_acc:0.979]
Epoch [111/120    avg_loss:0.050, val_acc:0.979]
Epoch [112/120    avg_loss:0.066, val_acc:0.981]
Epoch [113/120    avg_loss:0.062, val_acc:0.983]
Epoch [114/120    avg_loss:0.051, val_acc:0.983]
Epoch [115/120    avg_loss:0.057, val_acc:0.983]
Epoch [116/120    avg_loss:0.053, val_acc:0.983]
Epoch [117/120    avg_loss:0.049, val_acc:0.983]
Epoch [118/120    avg_loss:0.051, val_acc:0.981]
Epoch [119/120    avg_loss:0.044, val_acc:0.981]
Epoch [120/120    avg_loss:0.055, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   4   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.996337   1.         1.         0.95217391 0.90972222
 0.98800959 1.         1.         1.         1.         1.
 0.99556541 1.        ]

Kappa:
0.992642342385882
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb78084e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.638, val_acc:0.019]
Epoch [2/120    avg_loss:2.594, val_acc:0.146]
Epoch [3/120    avg_loss:2.560, val_acc:0.323]
Epoch [4/120    avg_loss:2.524, val_acc:0.327]
Epoch [5/120    avg_loss:2.492, val_acc:0.335]
Epoch [6/120    avg_loss:2.458, val_acc:0.344]
Epoch [7/120    avg_loss:2.425, val_acc:0.346]
Epoch [8/120    avg_loss:2.396, val_acc:0.342]
Epoch [9/120    avg_loss:2.366, val_acc:0.327]
Epoch [10/120    avg_loss:2.330, val_acc:0.335]
Epoch [11/120    avg_loss:2.305, val_acc:0.346]
Epoch [12/120    avg_loss:2.282, val_acc:0.356]
Epoch [13/120    avg_loss:2.260, val_acc:0.350]
Epoch [14/120    avg_loss:2.230, val_acc:0.356]
Epoch [15/120    avg_loss:2.206, val_acc:0.375]
Epoch [16/120    avg_loss:2.186, val_acc:0.379]
Epoch [17/120    avg_loss:2.141, val_acc:0.394]
Epoch [18/120    avg_loss:2.092, val_acc:0.408]
Epoch [19/120    avg_loss:2.058, val_acc:0.423]
Epoch [20/120    avg_loss:2.007, val_acc:0.431]
Epoch [21/120    avg_loss:1.954, val_acc:0.475]
Epoch [22/120    avg_loss:1.895, val_acc:0.475]
Epoch [23/120    avg_loss:1.837, val_acc:0.487]
Epoch [24/120    avg_loss:1.826, val_acc:0.535]
Epoch [25/120    avg_loss:1.764, val_acc:0.588]
Epoch [26/120    avg_loss:1.699, val_acc:0.623]
Epoch [27/120    avg_loss:1.626, val_acc:0.623]
Epoch [28/120    avg_loss:1.589, val_acc:0.623]
Epoch [29/120    avg_loss:1.525, val_acc:0.673]
Epoch [30/120    avg_loss:1.474, val_acc:0.679]
Epoch [31/120    avg_loss:1.386, val_acc:0.700]
Epoch [32/120    avg_loss:1.317, val_acc:0.688]
Epoch [33/120    avg_loss:1.264, val_acc:0.727]
Epoch [34/120    avg_loss:1.200, val_acc:0.752]
Epoch [35/120    avg_loss:1.129, val_acc:0.767]
Epoch [36/120    avg_loss:1.071, val_acc:0.729]
Epoch [37/120    avg_loss:1.109, val_acc:0.719]
Epoch [38/120    avg_loss:1.037, val_acc:0.773]
Epoch [39/120    avg_loss:0.919, val_acc:0.779]
Epoch [40/120    avg_loss:0.891, val_acc:0.775]
Epoch [41/120    avg_loss:0.814, val_acc:0.783]
Epoch [42/120    avg_loss:0.775, val_acc:0.815]
Epoch [43/120    avg_loss:0.700, val_acc:0.806]
Epoch [44/120    avg_loss:0.701, val_acc:0.779]
Epoch [45/120    avg_loss:0.638, val_acc:0.867]
Epoch [46/120    avg_loss:0.634, val_acc:0.906]
Epoch [47/120    avg_loss:0.575, val_acc:0.885]
Epoch [48/120    avg_loss:0.586, val_acc:0.769]
Epoch [49/120    avg_loss:0.598, val_acc:0.815]
Epoch [50/120    avg_loss:0.575, val_acc:0.885]
Epoch [51/120    avg_loss:0.498, val_acc:0.912]
Epoch [52/120    avg_loss:0.451, val_acc:0.915]
Epoch [53/120    avg_loss:0.439, val_acc:0.900]
Epoch [54/120    avg_loss:0.443, val_acc:0.892]
Epoch [55/120    avg_loss:0.425, val_acc:0.892]
Epoch [56/120    avg_loss:0.390, val_acc:0.927]
Epoch [57/120    avg_loss:0.389, val_acc:0.933]
Epoch [58/120    avg_loss:0.406, val_acc:0.835]
Epoch [59/120    avg_loss:0.366, val_acc:0.950]
Epoch [60/120    avg_loss:0.384, val_acc:0.873]
Epoch [61/120    avg_loss:0.377, val_acc:0.931]
Epoch [62/120    avg_loss:0.317, val_acc:0.917]
Epoch [63/120    avg_loss:0.309, val_acc:0.958]
Epoch [64/120    avg_loss:0.271, val_acc:0.952]
Epoch [65/120    avg_loss:0.314, val_acc:0.952]
Epoch [66/120    avg_loss:0.319, val_acc:0.931]
Epoch [67/120    avg_loss:0.356, val_acc:0.946]
Epoch [68/120    avg_loss:0.290, val_acc:0.944]
Epoch [69/120    avg_loss:0.278, val_acc:0.940]
Epoch [70/120    avg_loss:0.230, val_acc:0.958]
Epoch [71/120    avg_loss:0.248, val_acc:0.958]
Epoch [72/120    avg_loss:0.240, val_acc:0.942]
Epoch [73/120    avg_loss:0.271, val_acc:0.954]
Epoch [74/120    avg_loss:0.217, val_acc:0.938]
Epoch [75/120    avg_loss:0.270, val_acc:0.950]
Epoch [76/120    avg_loss:0.267, val_acc:0.946]
Epoch [77/120    avg_loss:0.241, val_acc:0.969]
Epoch [78/120    avg_loss:0.237, val_acc:0.921]
Epoch [79/120    avg_loss:0.201, val_acc:0.965]
Epoch [80/120    avg_loss:0.206, val_acc:0.965]
Epoch [81/120    avg_loss:0.187, val_acc:0.979]
Epoch [82/120    avg_loss:0.201, val_acc:0.965]
Epoch [83/120    avg_loss:0.197, val_acc:0.973]
Epoch [84/120    avg_loss:0.205, val_acc:0.940]
Epoch [85/120    avg_loss:0.265, val_acc:0.908]
Epoch [86/120    avg_loss:0.191, val_acc:0.967]
Epoch [87/120    avg_loss:0.182, val_acc:0.975]
Epoch [88/120    avg_loss:0.193, val_acc:0.954]
Epoch [89/120    avg_loss:0.171, val_acc:0.965]
Epoch [90/120    avg_loss:0.182, val_acc:0.950]
Epoch [91/120    avg_loss:0.162, val_acc:0.963]
Epoch [92/120    avg_loss:0.140, val_acc:0.973]
Epoch [93/120    avg_loss:0.143, val_acc:0.971]
Epoch [94/120    avg_loss:0.132, val_acc:0.973]
Epoch [95/120    avg_loss:0.129, val_acc:0.977]
Epoch [96/120    avg_loss:0.107, val_acc:0.979]
Epoch [97/120    avg_loss:0.104, val_acc:0.983]
Epoch [98/120    avg_loss:0.114, val_acc:0.979]
Epoch [99/120    avg_loss:0.113, val_acc:0.981]
Epoch [100/120    avg_loss:0.111, val_acc:0.985]
Epoch [101/120    avg_loss:0.095, val_acc:0.985]
Epoch [102/120    avg_loss:0.102, val_acc:0.981]
Epoch [103/120    avg_loss:0.106, val_acc:0.981]
Epoch [104/120    avg_loss:0.097, val_acc:0.983]
Epoch [105/120    avg_loss:0.095, val_acc:0.977]
Epoch [106/120    avg_loss:0.089, val_acc:0.988]
Epoch [107/120    avg_loss:0.104, val_acc:0.988]
Epoch [108/120    avg_loss:0.096, val_acc:0.981]
Epoch [109/120    avg_loss:0.106, val_acc:0.988]
Epoch [110/120    avg_loss:0.094, val_acc:0.990]
Epoch [111/120    avg_loss:0.105, val_acc:0.988]
Epoch [112/120    avg_loss:0.094, val_acc:0.988]
Epoch [113/120    avg_loss:0.093, val_acc:0.988]
Epoch [114/120    avg_loss:0.096, val_acc:0.990]
Epoch [115/120    avg_loss:0.090, val_acc:0.985]
Epoch [116/120    avg_loss:0.098, val_acc:0.988]
Epoch [117/120    avg_loss:0.089, val_acc:0.992]
Epoch [118/120    avg_loss:0.084, val_acc:0.990]
Epoch [119/120    avg_loss:0.092, val_acc:0.985]
Epoch [120/120    avg_loss:0.098, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99560117 0.98426966 1.         0.94418605 0.92356688
 0.98564593 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9912186493561094
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faba68f5dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.271]
Epoch [2/120    avg_loss:2.584, val_acc:0.271]
Epoch [3/120    avg_loss:2.551, val_acc:0.271]
Epoch [4/120    avg_loss:2.518, val_acc:0.271]
Epoch [5/120    avg_loss:2.490, val_acc:0.271]
Epoch [6/120    avg_loss:2.463, val_acc:0.271]
Epoch [7/120    avg_loss:2.436, val_acc:0.271]
Epoch [8/120    avg_loss:2.414, val_acc:0.287]
Epoch [9/120    avg_loss:2.386, val_acc:0.304]
Epoch [10/120    avg_loss:2.354, val_acc:0.317]
Epoch [11/120    avg_loss:2.325, val_acc:0.327]
Epoch [12/120    avg_loss:2.288, val_acc:0.335]
Epoch [13/120    avg_loss:2.252, val_acc:0.350]
Epoch [14/120    avg_loss:2.210, val_acc:0.379]
Epoch [15/120    avg_loss:2.168, val_acc:0.379]
Epoch [16/120    avg_loss:2.130, val_acc:0.404]
Epoch [17/120    avg_loss:2.082, val_acc:0.425]
Epoch [18/120    avg_loss:2.049, val_acc:0.442]
Epoch [19/120    avg_loss:1.971, val_acc:0.477]
Epoch [20/120    avg_loss:1.973, val_acc:0.537]
Epoch [21/120    avg_loss:1.899, val_acc:0.544]
Epoch [22/120    avg_loss:1.846, val_acc:0.567]
Epoch [23/120    avg_loss:1.801, val_acc:0.558]
Epoch [24/120    avg_loss:1.738, val_acc:0.604]
Epoch [25/120    avg_loss:1.680, val_acc:0.617]
Epoch [26/120    avg_loss:1.624, val_acc:0.610]
Epoch [27/120    avg_loss:1.564, val_acc:0.642]
Epoch [28/120    avg_loss:1.520, val_acc:0.671]
Epoch [29/120    avg_loss:1.466, val_acc:0.708]
Epoch [30/120    avg_loss:1.386, val_acc:0.673]
Epoch [31/120    avg_loss:1.348, val_acc:0.742]
Epoch [32/120    avg_loss:1.296, val_acc:0.733]
Epoch [33/120    avg_loss:1.242, val_acc:0.758]
Epoch [34/120    avg_loss:1.189, val_acc:0.794]
Epoch [35/120    avg_loss:1.096, val_acc:0.806]
Epoch [36/120    avg_loss:1.031, val_acc:0.800]
Epoch [37/120    avg_loss:0.971, val_acc:0.810]
Epoch [38/120    avg_loss:0.894, val_acc:0.835]
Epoch [39/120    avg_loss:0.861, val_acc:0.810]
Epoch [40/120    avg_loss:0.919, val_acc:0.800]
Epoch [41/120    avg_loss:0.830, val_acc:0.787]
Epoch [42/120    avg_loss:0.753, val_acc:0.831]
Epoch [43/120    avg_loss:0.679, val_acc:0.833]
Epoch [44/120    avg_loss:0.645, val_acc:0.823]
Epoch [45/120    avg_loss:0.593, val_acc:0.838]
Epoch [46/120    avg_loss:0.557, val_acc:0.840]
Epoch [47/120    avg_loss:0.518, val_acc:0.838]
Epoch [48/120    avg_loss:0.479, val_acc:0.881]
Epoch [49/120    avg_loss:0.477, val_acc:0.844]
Epoch [50/120    avg_loss:0.443, val_acc:0.896]
Epoch [51/120    avg_loss:0.423, val_acc:0.900]
Epoch [52/120    avg_loss:0.381, val_acc:0.892]
Epoch [53/120    avg_loss:0.368, val_acc:0.971]
Epoch [54/120    avg_loss:0.355, val_acc:0.910]
Epoch [55/120    avg_loss:0.360, val_acc:0.931]
Epoch [56/120    avg_loss:0.375, val_acc:0.946]
Epoch [57/120    avg_loss:0.420, val_acc:0.925]
Epoch [58/120    avg_loss:0.403, val_acc:0.910]
Epoch [59/120    avg_loss:0.332, val_acc:0.958]
Epoch [60/120    avg_loss:0.329, val_acc:0.954]
Epoch [61/120    avg_loss:0.291, val_acc:0.963]
Epoch [62/120    avg_loss:0.301, val_acc:0.981]
Epoch [63/120    avg_loss:0.245, val_acc:0.967]
Epoch [64/120    avg_loss:0.249, val_acc:0.960]
Epoch [65/120    avg_loss:0.248, val_acc:0.969]
Epoch [66/120    avg_loss:0.307, val_acc:0.973]
Epoch [67/120    avg_loss:0.243, val_acc:0.948]
Epoch [68/120    avg_loss:0.306, val_acc:0.948]
Epoch [69/120    avg_loss:0.275, val_acc:0.969]
Epoch [70/120    avg_loss:0.228, val_acc:0.969]
Epoch [71/120    avg_loss:0.168, val_acc:0.975]
Epoch [72/120    avg_loss:0.155, val_acc:0.979]
Epoch [73/120    avg_loss:0.145, val_acc:0.977]
Epoch [74/120    avg_loss:0.178, val_acc:0.988]
Epoch [75/120    avg_loss:0.180, val_acc:0.988]
Epoch [76/120    avg_loss:0.150, val_acc:0.971]
Epoch [77/120    avg_loss:0.154, val_acc:0.969]
Epoch [78/120    avg_loss:0.144, val_acc:0.975]
Epoch [79/120    avg_loss:0.152, val_acc:0.988]
Epoch [80/120    avg_loss:0.133, val_acc:0.977]
Epoch [81/120    avg_loss:0.164, val_acc:0.992]
Epoch [82/120    avg_loss:0.123, val_acc:0.990]
Epoch [83/120    avg_loss:0.134, val_acc:0.979]
Epoch [84/120    avg_loss:0.151, val_acc:0.963]
Epoch [85/120    avg_loss:0.128, val_acc:0.985]
Epoch [86/120    avg_loss:0.146, val_acc:0.969]
Epoch [87/120    avg_loss:0.112, val_acc:0.985]
Epoch [88/120    avg_loss:0.120, val_acc:0.990]
Epoch [89/120    avg_loss:0.140, val_acc:0.971]
Epoch [90/120    avg_loss:0.127, val_acc:0.963]
Epoch [91/120    avg_loss:0.120, val_acc:0.985]
Epoch [92/120    avg_loss:0.130, val_acc:0.973]
Epoch [93/120    avg_loss:0.093, val_acc:0.985]
Epoch [94/120    avg_loss:0.096, val_acc:0.981]
Epoch [95/120    avg_loss:0.093, val_acc:0.990]
Epoch [96/120    avg_loss:0.070, val_acc:0.990]
Epoch [97/120    avg_loss:0.066, val_acc:0.990]
Epoch [98/120    avg_loss:0.076, val_acc:0.992]
Epoch [99/120    avg_loss:0.075, val_acc:0.990]
Epoch [100/120    avg_loss:0.068, val_acc:0.990]
Epoch [101/120    avg_loss:0.070, val_acc:0.992]
Epoch [102/120    avg_loss:0.071, val_acc:0.990]
Epoch [103/120    avg_loss:0.070, val_acc:0.990]
Epoch [104/120    avg_loss:0.066, val_acc:0.990]
Epoch [105/120    avg_loss:0.069, val_acc:0.992]
Epoch [106/120    avg_loss:0.066, val_acc:0.992]
Epoch [107/120    avg_loss:0.069, val_acc:0.994]
Epoch [108/120    avg_loss:0.074, val_acc:0.990]
Epoch [109/120    avg_loss:0.070, val_acc:0.992]
Epoch [110/120    avg_loss:0.064, val_acc:0.994]
Epoch [111/120    avg_loss:0.061, val_acc:0.994]
Epoch [112/120    avg_loss:0.066, val_acc:0.990]
Epoch [113/120    avg_loss:0.068, val_acc:0.990]
Epoch [114/120    avg_loss:0.069, val_acc:0.992]
Epoch [115/120    avg_loss:0.062, val_acc:0.996]
Epoch [116/120    avg_loss:0.058, val_acc:0.996]
Epoch [117/120    avg_loss:0.063, val_acc:0.994]
Epoch [118/120    avg_loss:0.072, val_acc:0.994]
Epoch [119/120    avg_loss:0.067, val_acc:0.994]
Epoch [120/120    avg_loss:0.068, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   9   0   0   0   0   0   0   2   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.996337   1.         1.         0.95364238 0.93425606
 0.98800959 1.         1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9938286634744483
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd56127add8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.676, val_acc:0.094]
Epoch [2/120    avg_loss:2.632, val_acc:0.094]
Epoch [3/120    avg_loss:2.589, val_acc:0.200]
Epoch [4/120    avg_loss:2.556, val_acc:0.417]
Epoch [5/120    avg_loss:2.520, val_acc:0.438]
Epoch [6/120    avg_loss:2.488, val_acc:0.506]
Epoch [7/120    avg_loss:2.452, val_acc:0.515]
Epoch [8/120    avg_loss:2.418, val_acc:0.529]
Epoch [9/120    avg_loss:2.376, val_acc:0.523]
Epoch [10/120    avg_loss:2.332, val_acc:0.506]
Epoch [11/120    avg_loss:2.290, val_acc:0.498]
Epoch [12/120    avg_loss:2.237, val_acc:0.504]
Epoch [13/120    avg_loss:2.184, val_acc:0.527]
Epoch [14/120    avg_loss:2.120, val_acc:0.548]
Epoch [15/120    avg_loss:2.072, val_acc:0.556]
Epoch [16/120    avg_loss:2.005, val_acc:0.596]
Epoch [17/120    avg_loss:1.928, val_acc:0.617]
Epoch [18/120    avg_loss:1.868, val_acc:0.648]
Epoch [19/120    avg_loss:1.800, val_acc:0.669]
Epoch [20/120    avg_loss:1.737, val_acc:0.675]
Epoch [21/120    avg_loss:1.706, val_acc:0.683]
Epoch [22/120    avg_loss:1.606, val_acc:0.727]
Epoch [23/120    avg_loss:1.542, val_acc:0.723]
Epoch [24/120    avg_loss:1.468, val_acc:0.723]
Epoch [25/120    avg_loss:1.396, val_acc:0.740]
Epoch [26/120    avg_loss:1.349, val_acc:0.756]
Epoch [27/120    avg_loss:1.291, val_acc:0.744]
Epoch [28/120    avg_loss:1.205, val_acc:0.781]
Epoch [29/120    avg_loss:1.154, val_acc:0.781]
Epoch [30/120    avg_loss:1.060, val_acc:0.779]
Epoch [31/120    avg_loss:1.033, val_acc:0.762]
Epoch [32/120    avg_loss:0.977, val_acc:0.792]
Epoch [33/120    avg_loss:0.928, val_acc:0.798]
Epoch [34/120    avg_loss:0.869, val_acc:0.806]
Epoch [35/120    avg_loss:0.841, val_acc:0.806]
Epoch [36/120    avg_loss:0.747, val_acc:0.804]
Epoch [37/120    avg_loss:0.726, val_acc:0.827]
Epoch [38/120    avg_loss:0.667, val_acc:0.825]
Epoch [39/120    avg_loss:0.656, val_acc:0.844]
Epoch [40/120    avg_loss:0.613, val_acc:0.908]
Epoch [41/120    avg_loss:0.563, val_acc:0.833]
Epoch [42/120    avg_loss:0.530, val_acc:0.938]
Epoch [43/120    avg_loss:0.492, val_acc:0.940]
Epoch [44/120    avg_loss:0.494, val_acc:0.917]
Epoch [45/120    avg_loss:0.518, val_acc:0.921]
Epoch [46/120    avg_loss:0.476, val_acc:0.960]
Epoch [47/120    avg_loss:0.444, val_acc:0.923]
Epoch [48/120    avg_loss:0.437, val_acc:0.929]
Epoch [49/120    avg_loss:0.429, val_acc:0.954]
Epoch [50/120    avg_loss:0.374, val_acc:0.942]
Epoch [51/120    avg_loss:0.371, val_acc:0.954]
Epoch [52/120    avg_loss:0.360, val_acc:0.954]
Epoch [53/120    avg_loss:0.304, val_acc:0.971]
Epoch [54/120    avg_loss:0.256, val_acc:0.967]
Epoch [55/120    avg_loss:0.280, val_acc:0.967]
Epoch [56/120    avg_loss:0.273, val_acc:0.979]
Epoch [57/120    avg_loss:0.249, val_acc:0.965]
Epoch [58/120    avg_loss:0.289, val_acc:0.956]
Epoch [59/120    avg_loss:0.259, val_acc:0.975]
Epoch [60/120    avg_loss:0.282, val_acc:0.965]
Epoch [61/120    avg_loss:0.262, val_acc:0.958]
Epoch [62/120    avg_loss:0.239, val_acc:0.963]
Epoch [63/120    avg_loss:0.216, val_acc:0.979]
Epoch [64/120    avg_loss:0.233, val_acc:0.967]
Epoch [65/120    avg_loss:0.221, val_acc:0.967]
Epoch [66/120    avg_loss:0.193, val_acc:0.975]
Epoch [67/120    avg_loss:0.177, val_acc:0.977]
Epoch [68/120    avg_loss:0.193, val_acc:0.973]
Epoch [69/120    avg_loss:0.194, val_acc:0.975]
Epoch [70/120    avg_loss:0.191, val_acc:0.973]
Epoch [71/120    avg_loss:0.186, val_acc:0.981]
Epoch [72/120    avg_loss:0.136, val_acc:0.973]
Epoch [73/120    avg_loss:0.144, val_acc:0.973]
Epoch [74/120    avg_loss:0.139, val_acc:0.981]
Epoch [75/120    avg_loss:0.120, val_acc:0.983]
Epoch [76/120    avg_loss:0.111, val_acc:0.981]
Epoch [77/120    avg_loss:0.124, val_acc:0.965]
Epoch [78/120    avg_loss:0.123, val_acc:0.967]
Epoch [79/120    avg_loss:0.130, val_acc:0.983]
Epoch [80/120    avg_loss:0.127, val_acc:0.983]
Epoch [81/120    avg_loss:0.117, val_acc:0.969]
Epoch [82/120    avg_loss:0.128, val_acc:0.979]
Epoch [83/120    avg_loss:0.102, val_acc:0.990]
Epoch [84/120    avg_loss:0.097, val_acc:0.988]
Epoch [85/120    avg_loss:0.100, val_acc:0.988]
Epoch [86/120    avg_loss:0.097, val_acc:0.983]
Epoch [87/120    avg_loss:0.106, val_acc:0.985]
Epoch [88/120    avg_loss:0.097, val_acc:0.988]
Epoch [89/120    avg_loss:0.100, val_acc:0.981]
Epoch [90/120    avg_loss:0.111, val_acc:0.973]
Epoch [91/120    avg_loss:0.089, val_acc:0.985]
Epoch [92/120    avg_loss:0.076, val_acc:0.990]
Epoch [93/120    avg_loss:0.093, val_acc:0.992]
Epoch [94/120    avg_loss:0.074, val_acc:0.985]
Epoch [95/120    avg_loss:0.057, val_acc:0.992]
Epoch [96/120    avg_loss:0.051, val_acc:0.985]
Epoch [97/120    avg_loss:0.080, val_acc:0.985]
Epoch [98/120    avg_loss:0.053, val_acc:0.992]
Epoch [99/120    avg_loss:0.054, val_acc:0.990]
Epoch [100/120    avg_loss:0.050, val_acc:0.990]
Epoch [101/120    avg_loss:0.048, val_acc:0.990]
Epoch [102/120    avg_loss:0.047, val_acc:0.992]
Epoch [103/120    avg_loss:0.040, val_acc:0.990]
Epoch [104/120    avg_loss:0.050, val_acc:0.992]
Epoch [105/120    avg_loss:0.072, val_acc:0.988]
Epoch [106/120    avg_loss:0.072, val_acc:0.983]
Epoch [107/120    avg_loss:0.058, val_acc:0.988]
Epoch [108/120    avg_loss:0.044, val_acc:0.992]
Epoch [109/120    avg_loss:0.034, val_acc:0.996]
Epoch [110/120    avg_loss:0.042, val_acc:0.990]
Epoch [111/120    avg_loss:0.037, val_acc:0.990]
Epoch [112/120    avg_loss:0.037, val_acc:1.000]
Epoch [113/120    avg_loss:0.036, val_acc:0.994]
Epoch [114/120    avg_loss:0.047, val_acc:0.985]
Epoch [115/120    avg_loss:0.074, val_acc:0.992]
Epoch [116/120    avg_loss:0.062, val_acc:0.985]
Epoch [117/120    avg_loss:0.061, val_acc:0.988]
Epoch [118/120    avg_loss:0.037, val_acc:0.990]
Epoch [119/120    avg_loss:0.035, val_acc:0.998]
Epoch [120/120    avg_loss:0.050, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 671   0   0   0   0  14   0   0   0   0   0   0   0]
 [  0   0 149   1   0   0   0  69   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   2 204   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.97441364605544

F1 scores:
[       nan 0.98967552 0.80978261 0.9978308  0.98013245 0.96245734
 0.96226415 0.73151751 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9774664672407293
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa93e139e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.085]
Epoch [2/120    avg_loss:2.600, val_acc:0.273]
Epoch [3/120    avg_loss:2.572, val_acc:0.275]
Epoch [4/120    avg_loss:2.552, val_acc:0.275]
Epoch [5/120    avg_loss:2.521, val_acc:0.275]
Epoch [6/120    avg_loss:2.497, val_acc:0.277]
Epoch [7/120    avg_loss:2.469, val_acc:0.281]
Epoch [8/120    avg_loss:2.443, val_acc:0.298]
Epoch [9/120    avg_loss:2.414, val_acc:0.312]
Epoch [10/120    avg_loss:2.388, val_acc:0.317]
Epoch [11/120    avg_loss:2.349, val_acc:0.344]
Epoch [12/120    avg_loss:2.319, val_acc:0.375]
Epoch [13/120    avg_loss:2.290, val_acc:0.419]
Epoch [14/120    avg_loss:2.250, val_acc:0.438]
Epoch [15/120    avg_loss:2.211, val_acc:0.440]
Epoch [16/120    avg_loss:2.168, val_acc:0.442]
Epoch [17/120    avg_loss:2.123, val_acc:0.415]
Epoch [18/120    avg_loss:2.082, val_acc:0.396]
Epoch [19/120    avg_loss:2.039, val_acc:0.450]
Epoch [20/120    avg_loss:1.986, val_acc:0.490]
Epoch [21/120    avg_loss:1.941, val_acc:0.517]
Epoch [22/120    avg_loss:1.878, val_acc:0.546]
Epoch [23/120    avg_loss:1.851, val_acc:0.556]
Epoch [24/120    avg_loss:1.792, val_acc:0.596]
Epoch [25/120    avg_loss:1.753, val_acc:0.629]
Epoch [26/120    avg_loss:1.673, val_acc:0.640]
Epoch [27/120    avg_loss:1.592, val_acc:0.673]
Epoch [28/120    avg_loss:1.564, val_acc:0.654]
Epoch [29/120    avg_loss:1.552, val_acc:0.675]
Epoch [30/120    avg_loss:1.442, val_acc:0.704]
Epoch [31/120    avg_loss:1.392, val_acc:0.694]
Epoch [32/120    avg_loss:1.337, val_acc:0.690]
Epoch [33/120    avg_loss:1.277, val_acc:0.706]
Epoch [34/120    avg_loss:1.212, val_acc:0.727]
Epoch [35/120    avg_loss:1.169, val_acc:0.756]
Epoch [36/120    avg_loss:1.114, val_acc:0.740]
Epoch [37/120    avg_loss:1.061, val_acc:0.771]
Epoch [38/120    avg_loss:0.978, val_acc:0.835]
Epoch [39/120    avg_loss:0.982, val_acc:0.835]
Epoch [40/120    avg_loss:0.942, val_acc:0.867]
Epoch [41/120    avg_loss:0.841, val_acc:0.885]
Epoch [42/120    avg_loss:0.771, val_acc:0.815]
Epoch [43/120    avg_loss:0.765, val_acc:0.898]
Epoch [44/120    avg_loss:0.804, val_acc:0.890]
Epoch [45/120    avg_loss:0.722, val_acc:0.885]
Epoch [46/120    avg_loss:0.618, val_acc:0.915]
Epoch [47/120    avg_loss:0.645, val_acc:0.935]
Epoch [48/120    avg_loss:0.679, val_acc:0.904]
Epoch [49/120    avg_loss:0.547, val_acc:0.921]
Epoch [50/120    avg_loss:0.510, val_acc:0.923]
Epoch [51/120    avg_loss:0.524, val_acc:0.925]
Epoch [52/120    avg_loss:0.456, val_acc:0.927]
Epoch [53/120    avg_loss:0.418, val_acc:0.929]
Epoch [54/120    avg_loss:0.419, val_acc:0.902]
Epoch [55/120    avg_loss:0.388, val_acc:0.898]
Epoch [56/120    avg_loss:0.369, val_acc:0.892]
Epoch [57/120    avg_loss:0.379, val_acc:0.950]
Epoch [58/120    avg_loss:0.352, val_acc:0.902]
Epoch [59/120    avg_loss:0.329, val_acc:0.935]
Epoch [60/120    avg_loss:0.298, val_acc:0.935]
Epoch [61/120    avg_loss:0.290, val_acc:0.935]
Epoch [62/120    avg_loss:0.289, val_acc:0.944]
Epoch [63/120    avg_loss:0.263, val_acc:0.944]
Epoch [64/120    avg_loss:0.297, val_acc:0.952]
Epoch [65/120    avg_loss:0.226, val_acc:0.956]
Epoch [66/120    avg_loss:0.236, val_acc:0.956]
Epoch [67/120    avg_loss:0.249, val_acc:0.944]
Epoch [68/120    avg_loss:0.242, val_acc:0.952]
Epoch [69/120    avg_loss:0.268, val_acc:0.952]
Epoch [70/120    avg_loss:0.238, val_acc:0.954]
Epoch [71/120    avg_loss:0.205, val_acc:0.969]
Epoch [72/120    avg_loss:0.223, val_acc:0.965]
Epoch [73/120    avg_loss:0.221, val_acc:0.963]
Epoch [74/120    avg_loss:0.176, val_acc:0.958]
Epoch [75/120    avg_loss:0.196, val_acc:0.965]
Epoch [76/120    avg_loss:0.182, val_acc:0.960]
Epoch [77/120    avg_loss:0.224, val_acc:0.952]
Epoch [78/120    avg_loss:0.176, val_acc:0.971]
Epoch [79/120    avg_loss:0.181, val_acc:0.960]
Epoch [80/120    avg_loss:0.175, val_acc:0.960]
Epoch [81/120    avg_loss:0.211, val_acc:0.956]
Epoch [82/120    avg_loss:0.197, val_acc:0.977]
Epoch [83/120    avg_loss:0.156, val_acc:0.983]
Epoch [84/120    avg_loss:0.128, val_acc:0.977]
Epoch [85/120    avg_loss:0.152, val_acc:0.965]
Epoch [86/120    avg_loss:0.126, val_acc:0.969]
Epoch [87/120    avg_loss:0.133, val_acc:0.975]
Epoch [88/120    avg_loss:0.119, val_acc:0.985]
Epoch [89/120    avg_loss:0.109, val_acc:0.975]
Epoch [90/120    avg_loss:0.103, val_acc:0.981]
Epoch [91/120    avg_loss:0.108, val_acc:0.975]
Epoch [92/120    avg_loss:0.094, val_acc:0.981]
Epoch [93/120    avg_loss:0.091, val_acc:0.973]
Epoch [94/120    avg_loss:0.108, val_acc:0.975]
Epoch [95/120    avg_loss:0.098, val_acc:0.981]
Epoch [96/120    avg_loss:0.140, val_acc:0.981]
Epoch [97/120    avg_loss:0.111, val_acc:0.983]
Epoch [98/120    avg_loss:0.112, val_acc:0.988]
Epoch [99/120    avg_loss:0.117, val_acc:0.973]
Epoch [100/120    avg_loss:0.116, val_acc:0.979]
Epoch [101/120    avg_loss:0.081, val_acc:0.977]
Epoch [102/120    avg_loss:0.081, val_acc:0.979]
Epoch [103/120    avg_loss:0.095, val_acc:0.981]
Epoch [104/120    avg_loss:0.094, val_acc:0.979]
Epoch [105/120    avg_loss:0.109, val_acc:0.975]
Epoch [106/120    avg_loss:0.099, val_acc:0.956]
Epoch [107/120    avg_loss:0.182, val_acc:0.983]
Epoch [108/120    avg_loss:0.123, val_acc:0.946]
Epoch [109/120    avg_loss:0.146, val_acc:0.975]
Epoch [110/120    avg_loss:0.157, val_acc:0.969]
Epoch [111/120    avg_loss:0.111, val_acc:0.981]
Epoch [112/120    avg_loss:0.075, val_acc:0.981]
Epoch [113/120    avg_loss:0.070, val_acc:0.981]
Epoch [114/120    avg_loss:0.060, val_acc:0.981]
Epoch [115/120    avg_loss:0.067, val_acc:0.977]
Epoch [116/120    avg_loss:0.065, val_acc:0.977]
Epoch [117/120    avg_loss:0.057, val_acc:0.979]
Epoch [118/120    avg_loss:0.052, val_acc:0.979]
Epoch [119/120    avg_loss:0.058, val_acc:0.979]
Epoch [120/120    avg_loss:0.058, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 200  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 0.99707174 0.9977221  0.9978308  0.93457944 0.91428571
 0.99038462 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9921679334335572
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f857202be48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.638, val_acc:0.044]
Epoch [2/120    avg_loss:2.601, val_acc:0.106]
Epoch [3/120    avg_loss:2.572, val_acc:0.227]
Epoch [4/120    avg_loss:2.549, val_acc:0.263]
Epoch [5/120    avg_loss:2.526, val_acc:0.304]
Epoch [6/120    avg_loss:2.504, val_acc:0.310]
Epoch [7/120    avg_loss:2.478, val_acc:0.310]
Epoch [8/120    avg_loss:2.452, val_acc:0.315]
Epoch [9/120    avg_loss:2.420, val_acc:0.312]
Epoch [10/120    avg_loss:2.388, val_acc:0.327]
Epoch [11/120    avg_loss:2.358, val_acc:0.335]
Epoch [12/120    avg_loss:2.331, val_acc:0.358]
Epoch [13/120    avg_loss:2.285, val_acc:0.385]
Epoch [14/120    avg_loss:2.247, val_acc:0.481]
Epoch [15/120    avg_loss:2.200, val_acc:0.527]
Epoch [16/120    avg_loss:2.158, val_acc:0.577]
Epoch [17/120    avg_loss:2.112, val_acc:0.600]
Epoch [18/120    avg_loss:2.067, val_acc:0.644]
Epoch [19/120    avg_loss:2.013, val_acc:0.654]
Epoch [20/120    avg_loss:1.948, val_acc:0.654]
Epoch [21/120    avg_loss:1.896, val_acc:0.698]
Epoch [22/120    avg_loss:1.806, val_acc:0.685]
Epoch [23/120    avg_loss:1.736, val_acc:0.715]
Epoch [24/120    avg_loss:1.653, val_acc:0.727]
Epoch [25/120    avg_loss:1.624, val_acc:0.696]
Epoch [26/120    avg_loss:1.540, val_acc:0.710]
Epoch [27/120    avg_loss:1.440, val_acc:0.738]
Epoch [28/120    avg_loss:1.352, val_acc:0.748]
Epoch [29/120    avg_loss:1.273, val_acc:0.779]
Epoch [30/120    avg_loss:1.191, val_acc:0.787]
Epoch [31/120    avg_loss:1.125, val_acc:0.785]
Epoch [32/120    avg_loss:1.048, val_acc:0.812]
Epoch [33/120    avg_loss:0.944, val_acc:0.769]
Epoch [34/120    avg_loss:0.938, val_acc:0.833]
Epoch [35/120    avg_loss:0.872, val_acc:0.794]
Epoch [36/120    avg_loss:0.780, val_acc:0.852]
Epoch [37/120    avg_loss:0.777, val_acc:0.865]
Epoch [38/120    avg_loss:0.721, val_acc:0.890]
Epoch [39/120    avg_loss:0.702, val_acc:0.887]
Epoch [40/120    avg_loss:0.631, val_acc:0.929]
Epoch [41/120    avg_loss:0.582, val_acc:0.929]
Epoch [42/120    avg_loss:0.637, val_acc:0.915]
Epoch [43/120    avg_loss:0.572, val_acc:0.925]
Epoch [44/120    avg_loss:0.577, val_acc:0.929]
Epoch [45/120    avg_loss:0.477, val_acc:0.923]
Epoch [46/120    avg_loss:0.475, val_acc:0.944]
Epoch [47/120    avg_loss:0.475, val_acc:0.873]
Epoch [48/120    avg_loss:0.426, val_acc:0.933]
Epoch [49/120    avg_loss:0.375, val_acc:0.956]
Epoch [50/120    avg_loss:0.327, val_acc:0.948]
Epoch [51/120    avg_loss:0.313, val_acc:0.965]
Epoch [52/120    avg_loss:0.320, val_acc:0.954]
Epoch [53/120    avg_loss:0.334, val_acc:0.929]
Epoch [54/120    avg_loss:0.283, val_acc:0.971]
Epoch [55/120    avg_loss:0.302, val_acc:0.967]
Epoch [56/120    avg_loss:0.318, val_acc:0.946]
Epoch [57/120    avg_loss:0.314, val_acc:0.963]
Epoch [58/120    avg_loss:0.382, val_acc:0.952]
Epoch [59/120    avg_loss:0.390, val_acc:0.963]
Epoch [60/120    avg_loss:0.300, val_acc:0.956]
Epoch [61/120    avg_loss:0.278, val_acc:0.960]
Epoch [62/120    avg_loss:0.259, val_acc:0.933]
Epoch [63/120    avg_loss:0.261, val_acc:0.963]
Epoch [64/120    avg_loss:0.236, val_acc:0.973]
Epoch [65/120    avg_loss:0.212, val_acc:0.938]
Epoch [66/120    avg_loss:0.250, val_acc:0.960]
Epoch [67/120    avg_loss:0.234, val_acc:0.956]
Epoch [68/120    avg_loss:0.209, val_acc:0.940]
Epoch [69/120    avg_loss:0.212, val_acc:0.967]
Epoch [70/120    avg_loss:0.170, val_acc:0.973]
Epoch [71/120    avg_loss:0.193, val_acc:0.967]
Epoch [72/120    avg_loss:0.157, val_acc:0.963]
Epoch [73/120    avg_loss:0.141, val_acc:0.965]
Epoch [74/120    avg_loss:0.136, val_acc:0.977]
Epoch [75/120    avg_loss:0.130, val_acc:0.975]
Epoch [76/120    avg_loss:0.136, val_acc:0.985]
Epoch [77/120    avg_loss:0.134, val_acc:0.965]
Epoch [78/120    avg_loss:0.143, val_acc:0.975]
Epoch [79/120    avg_loss:0.170, val_acc:0.969]
Epoch [80/120    avg_loss:0.147, val_acc:0.983]
Epoch [81/120    avg_loss:0.112, val_acc:0.988]
Epoch [82/120    avg_loss:0.095, val_acc:0.981]
Epoch [83/120    avg_loss:0.093, val_acc:0.977]
Epoch [84/120    avg_loss:0.100, val_acc:0.992]
Epoch [85/120    avg_loss:0.163, val_acc:0.973]
Epoch [86/120    avg_loss:0.163, val_acc:0.983]
Epoch [87/120    avg_loss:0.128, val_acc:0.973]
Epoch [88/120    avg_loss:0.112, val_acc:0.977]
Epoch [89/120    avg_loss:0.117, val_acc:0.981]
Epoch [90/120    avg_loss:0.081, val_acc:0.981]
Epoch [91/120    avg_loss:0.087, val_acc:0.983]
Epoch [92/120    avg_loss:0.096, val_acc:0.979]
Epoch [93/120    avg_loss:0.100, val_acc:0.981]
Epoch [94/120    avg_loss:0.083, val_acc:0.990]
Epoch [95/120    avg_loss:0.095, val_acc:0.969]
Epoch [96/120    avg_loss:0.228, val_acc:0.960]
Epoch [97/120    avg_loss:0.155, val_acc:0.971]
Epoch [98/120    avg_loss:0.097, val_acc:0.977]
Epoch [99/120    avg_loss:0.082, val_acc:0.981]
Epoch [100/120    avg_loss:0.069, val_acc:0.977]
Epoch [101/120    avg_loss:0.080, val_acc:0.983]
Epoch [102/120    avg_loss:0.076, val_acc:0.979]
Epoch [103/120    avg_loss:0.070, val_acc:0.983]
Epoch [104/120    avg_loss:0.055, val_acc:0.981]
Epoch [105/120    avg_loss:0.063, val_acc:0.983]
Epoch [106/120    avg_loss:0.058, val_acc:0.985]
Epoch [107/120    avg_loss:0.064, val_acc:0.983]
Epoch [108/120    avg_loss:0.057, val_acc:0.983]
Epoch [109/120    avg_loss:0.061, val_acc:0.985]
Epoch [110/120    avg_loss:0.051, val_acc:0.988]
Epoch [111/120    avg_loss:0.062, val_acc:0.988]
Epoch [112/120    avg_loss:0.057, val_acc:0.988]
Epoch [113/120    avg_loss:0.053, val_acc:0.988]
Epoch [114/120    avg_loss:0.063, val_acc:0.985]
Epoch [115/120    avg_loss:0.057, val_acc:0.988]
Epoch [116/120    avg_loss:0.066, val_acc:0.988]
Epoch [117/120    avg_loss:0.051, val_acc:0.988]
Epoch [118/120    avg_loss:0.055, val_acc:0.988]
Epoch [119/120    avg_loss:0.065, val_acc:0.988]
Epoch [120/120    avg_loss:0.062, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99412628 0.98871332 1.         0.9610984  0.94462541
 0.98095238 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9928802143890412
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6499b8ce48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.696]
Epoch [2/120    avg_loss:2.604, val_acc:0.411]
Epoch [3/120    avg_loss:2.573, val_acc:0.281]
Epoch [4/120    avg_loss:2.550, val_acc:0.277]
Epoch [5/120    avg_loss:2.527, val_acc:0.281]
Epoch [6/120    avg_loss:2.499, val_acc:0.287]
Epoch [7/120    avg_loss:2.469, val_acc:0.296]
Epoch [8/120    avg_loss:2.443, val_acc:0.340]
Epoch [9/120    avg_loss:2.415, val_acc:0.369]
Epoch [10/120    avg_loss:2.388, val_acc:0.396]
Epoch [11/120    avg_loss:2.365, val_acc:0.404]
Epoch [12/120    avg_loss:2.329, val_acc:0.425]
Epoch [13/120    avg_loss:2.304, val_acc:0.448]
Epoch [14/120    avg_loss:2.277, val_acc:0.454]
Epoch [15/120    avg_loss:2.262, val_acc:0.458]
Epoch [16/120    avg_loss:2.254, val_acc:0.458]
Epoch [17/120    avg_loss:2.242, val_acc:0.456]
Epoch [18/120    avg_loss:2.245, val_acc:0.463]
Epoch [19/120    avg_loss:2.241, val_acc:0.456]
Epoch [20/120    avg_loss:2.241, val_acc:0.458]
Epoch [21/120    avg_loss:2.243, val_acc:0.458]
Epoch [22/120    avg_loss:2.239, val_acc:0.460]
Epoch [23/120    avg_loss:2.238, val_acc:0.463]
Epoch [24/120    avg_loss:2.227, val_acc:0.463]
Epoch [25/120    avg_loss:2.221, val_acc:0.465]
Epoch [26/120    avg_loss:2.214, val_acc:0.463]
Epoch [27/120    avg_loss:2.212, val_acc:0.467]
Epoch [28/120    avg_loss:2.209, val_acc:0.467]
Epoch [29/120    avg_loss:2.206, val_acc:0.467]
Epoch [30/120    avg_loss:2.215, val_acc:0.467]
Epoch [31/120    avg_loss:2.214, val_acc:0.467]
Epoch [32/120    avg_loss:2.206, val_acc:0.467]
Epoch [33/120    avg_loss:2.218, val_acc:0.469]
Epoch [34/120    avg_loss:2.204, val_acc:0.467]
Epoch [35/120    avg_loss:2.205, val_acc:0.469]
Epoch [36/120    avg_loss:2.208, val_acc:0.469]
Epoch [37/120    avg_loss:2.213, val_acc:0.469]
Epoch [38/120    avg_loss:2.207, val_acc:0.469]
Epoch [39/120    avg_loss:2.210, val_acc:0.469]
Epoch [40/120    avg_loss:2.217, val_acc:0.469]
Epoch [41/120    avg_loss:2.212, val_acc:0.469]
Epoch [42/120    avg_loss:2.215, val_acc:0.469]
Epoch [43/120    avg_loss:2.210, val_acc:0.469]
Epoch [44/120    avg_loss:2.212, val_acc:0.469]
Epoch [45/120    avg_loss:2.207, val_acc:0.469]
Epoch [46/120    avg_loss:2.205, val_acc:0.469]
Epoch [47/120    avg_loss:2.206, val_acc:0.469]
Epoch [48/120    avg_loss:2.202, val_acc:0.469]
Epoch [49/120    avg_loss:2.208, val_acc:0.469]
Epoch [50/120    avg_loss:2.207, val_acc:0.469]
Epoch [51/120    avg_loss:2.206, val_acc:0.469]
Epoch [52/120    avg_loss:2.201, val_acc:0.469]
Epoch [53/120    avg_loss:2.214, val_acc:0.469]
Epoch [54/120    avg_loss:2.208, val_acc:0.469]
Epoch [55/120    avg_loss:2.203, val_acc:0.469]
Epoch [56/120    avg_loss:2.204, val_acc:0.469]
Epoch [57/120    avg_loss:2.206, val_acc:0.469]
Epoch [58/120    avg_loss:2.203, val_acc:0.469]
Epoch [59/120    avg_loss:2.212, val_acc:0.469]
Epoch [60/120    avg_loss:2.211, val_acc:0.469]
Epoch [61/120    avg_loss:2.205, val_acc:0.469]
Epoch [62/120    avg_loss:2.209, val_acc:0.469]
Epoch [63/120    avg_loss:2.213, val_acc:0.469]
Epoch [64/120    avg_loss:2.216, val_acc:0.469]
Epoch [65/120    avg_loss:2.204, val_acc:0.469]
Epoch [66/120    avg_loss:2.212, val_acc:0.469]
Epoch [67/120    avg_loss:2.204, val_acc:0.469]
Epoch [68/120    avg_loss:2.206, val_acc:0.469]
Epoch [69/120    avg_loss:2.208, val_acc:0.469]
Epoch [70/120    avg_loss:2.205, val_acc:0.469]
Epoch [71/120    avg_loss:2.207, val_acc:0.469]
Epoch [72/120    avg_loss:2.212, val_acc:0.469]
Epoch [73/120    avg_loss:2.206, val_acc:0.469]
Epoch [74/120    avg_loss:2.202, val_acc:0.469]
Epoch [75/120    avg_loss:2.207, val_acc:0.469]
Epoch [76/120    avg_loss:2.208, val_acc:0.469]
Epoch [77/120    avg_loss:2.209, val_acc:0.469]
Epoch [78/120    avg_loss:2.204, val_acc:0.469]
Epoch [79/120    avg_loss:2.209, val_acc:0.469]
Epoch [80/120    avg_loss:2.209, val_acc:0.469]
Epoch [81/120    avg_loss:2.211, val_acc:0.469]
Epoch [82/120    avg_loss:2.200, val_acc:0.469]
Epoch [83/120    avg_loss:2.215, val_acc:0.469]
Epoch [84/120    avg_loss:2.212, val_acc:0.469]
Epoch [85/120    avg_loss:2.211, val_acc:0.469]
Epoch [86/120    avg_loss:2.213, val_acc:0.469]
Epoch [87/120    avg_loss:2.206, val_acc:0.469]
Epoch [88/120    avg_loss:2.209, val_acc:0.469]
Epoch [89/120    avg_loss:2.209, val_acc:0.469]
Epoch [90/120    avg_loss:2.207, val_acc:0.469]
Epoch [91/120    avg_loss:2.203, val_acc:0.469]
Epoch [92/120    avg_loss:2.208, val_acc:0.469]
Epoch [93/120    avg_loss:2.204, val_acc:0.469]
Epoch [94/120    avg_loss:2.201, val_acc:0.469]
Epoch [95/120    avg_loss:2.203, val_acc:0.469]
Epoch [96/120    avg_loss:2.207, val_acc:0.469]
Epoch [97/120    avg_loss:2.215, val_acc:0.469]
Epoch [98/120    avg_loss:2.211, val_acc:0.469]
Epoch [99/120    avg_loss:2.206, val_acc:0.469]
Epoch [100/120    avg_loss:2.208, val_acc:0.469]
Epoch [101/120    avg_loss:2.205, val_acc:0.469]
Epoch [102/120    avg_loss:2.203, val_acc:0.469]
Epoch [103/120    avg_loss:2.207, val_acc:0.469]
Epoch [104/120    avg_loss:2.207, val_acc:0.469]
Epoch [105/120    avg_loss:2.212, val_acc:0.469]
Epoch [106/120    avg_loss:2.209, val_acc:0.469]
Epoch [107/120    avg_loss:2.206, val_acc:0.469]
Epoch [108/120    avg_loss:2.202, val_acc:0.469]
Epoch [109/120    avg_loss:2.211, val_acc:0.469]
Epoch [110/120    avg_loss:2.210, val_acc:0.469]
Epoch [111/120    avg_loss:2.216, val_acc:0.469]
Epoch [112/120    avg_loss:2.206, val_acc:0.469]
Epoch [113/120    avg_loss:2.201, val_acc:0.469]
Epoch [114/120    avg_loss:2.209, val_acc:0.469]
Epoch [115/120    avg_loss:2.208, val_acc:0.469]
Epoch [116/120    avg_loss:2.215, val_acc:0.469]
Epoch [117/120    avg_loss:2.213, val_acc:0.469]
Epoch [118/120    avg_loss:2.207, val_acc:0.469]
Epoch [119/120    avg_loss:2.213, val_acc:0.469]
Epoch [120/120    avg_loss:2.211, val_acc:0.469]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 364   0 160   0   2 157   0   0   0   0   0]
 [  0   0 146  24   0  23   0  26   0   0   0   0   0   0]
 [  0   0   1 210   0   2   0  17   0   0   0   0   0   0]
 [  0   0   0 121   0  67   0   0   0   0  32   7   0   0]
 [  0   0   0  12   0 132   0   0   0   0   0   1   0   0]
 [  0   0  27  65   0  15   0  99   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0 216   0  17   0   0   0   0   0 155   0   0]
 [  0   0   0  65   0   0   0   0   0   0   0 403   0   0]
 [  0   0   0   0   2   0   0   0   0   0 356   6   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 399   0  54]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
45.82089552238806

F1 scores:
[       nan 0.         0.73924051 0.3213466  0.         0.47058824
 0.         0.56626506 0.         0.         0.94680851 0.43710145
 0.         0.96864111]

Kappa:
0.4093264572285838
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4bfc033dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.671, val_acc:0.000]
Epoch [2/120    avg_loss:2.635, val_acc:0.021]
Epoch [3/120    avg_loss:2.612, val_acc:0.131]
Epoch [4/120    avg_loss:2.589, val_acc:0.265]
Epoch [5/120    avg_loss:2.569, val_acc:0.385]
Epoch [6/120    avg_loss:2.542, val_acc:0.481]
Epoch [7/120    avg_loss:2.515, val_acc:0.485]
Epoch [8/120    avg_loss:2.485, val_acc:0.496]
Epoch [9/120    avg_loss:2.449, val_acc:0.506]
Epoch [10/120    avg_loss:2.419, val_acc:0.485]
Epoch [11/120    avg_loss:2.386, val_acc:0.498]
Epoch [12/120    avg_loss:2.347, val_acc:0.504]
Epoch [13/120    avg_loss:2.308, val_acc:0.508]
Epoch [14/120    avg_loss:2.259, val_acc:0.515]
Epoch [15/120    avg_loss:2.202, val_acc:0.508]
Epoch [16/120    avg_loss:2.160, val_acc:0.527]
Epoch [17/120    avg_loss:2.092, val_acc:0.515]
Epoch [18/120    avg_loss:2.058, val_acc:0.531]
Epoch [19/120    avg_loss:1.975, val_acc:0.537]
Epoch [20/120    avg_loss:1.923, val_acc:0.558]
Epoch [21/120    avg_loss:1.868, val_acc:0.556]
Epoch [22/120    avg_loss:1.793, val_acc:0.550]
Epoch [23/120    avg_loss:1.751, val_acc:0.565]
Epoch [24/120    avg_loss:1.673, val_acc:0.571]
Epoch [25/120    avg_loss:1.604, val_acc:0.590]
Epoch [26/120    avg_loss:1.539, val_acc:0.615]
Epoch [27/120    avg_loss:1.511, val_acc:0.610]
Epoch [28/120    avg_loss:1.424, val_acc:0.623]
Epoch [29/120    avg_loss:1.342, val_acc:0.654]
Epoch [30/120    avg_loss:1.281, val_acc:0.683]
Epoch [31/120    avg_loss:1.214, val_acc:0.698]
Epoch [32/120    avg_loss:1.124, val_acc:0.710]
Epoch [33/120    avg_loss:1.083, val_acc:0.721]
Epoch [34/120    avg_loss:1.016, val_acc:0.740]
Epoch [35/120    avg_loss:0.964, val_acc:0.752]
Epoch [36/120    avg_loss:0.905, val_acc:0.735]
Epoch [37/120    avg_loss:0.882, val_acc:0.767]
Epoch [38/120    avg_loss:0.843, val_acc:0.773]
Epoch [39/120    avg_loss:0.772, val_acc:0.787]
Epoch [40/120    avg_loss:0.713, val_acc:0.777]
Epoch [41/120    avg_loss:0.667, val_acc:0.792]
Epoch [42/120    avg_loss:0.661, val_acc:0.806]
Epoch [43/120    avg_loss:0.588, val_acc:0.827]
Epoch [44/120    avg_loss:0.551, val_acc:0.869]
Epoch [45/120    avg_loss:0.528, val_acc:0.833]
Epoch [46/120    avg_loss:0.515, val_acc:0.860]
Epoch [47/120    avg_loss:0.500, val_acc:0.944]
Epoch [48/120    avg_loss:0.459, val_acc:0.944]
Epoch [49/120    avg_loss:0.422, val_acc:0.923]
Epoch [50/120    avg_loss:0.436, val_acc:0.948]
Epoch [51/120    avg_loss:0.428, val_acc:0.915]
Epoch [52/120    avg_loss:0.429, val_acc:0.912]
Epoch [53/120    avg_loss:0.388, val_acc:0.973]
Epoch [54/120    avg_loss:0.338, val_acc:0.935]
Epoch [55/120    avg_loss:0.378, val_acc:0.912]
Epoch [56/120    avg_loss:0.364, val_acc:0.912]
Epoch [57/120    avg_loss:0.354, val_acc:0.921]
Epoch [58/120    avg_loss:0.323, val_acc:0.933]
Epoch [59/120    avg_loss:0.342, val_acc:0.938]
Epoch [60/120    avg_loss:0.326, val_acc:0.954]
Epoch [61/120    avg_loss:0.271, val_acc:0.954]
Epoch [62/120    avg_loss:0.247, val_acc:0.971]
Epoch [63/120    avg_loss:0.234, val_acc:0.979]
Epoch [64/120    avg_loss:0.229, val_acc:0.942]
Epoch [65/120    avg_loss:0.219, val_acc:0.973]
Epoch [66/120    avg_loss:0.225, val_acc:0.981]
Epoch [67/120    avg_loss:0.297, val_acc:0.958]
Epoch [68/120    avg_loss:0.240, val_acc:0.971]
Epoch [69/120    avg_loss:0.191, val_acc:0.960]
Epoch [70/120    avg_loss:0.189, val_acc:0.958]
Epoch [71/120    avg_loss:0.178, val_acc:0.948]
Epoch [72/120    avg_loss:0.179, val_acc:0.967]
Epoch [73/120    avg_loss:0.170, val_acc:0.973]
Epoch [74/120    avg_loss:0.162, val_acc:0.973]
Epoch [75/120    avg_loss:0.145, val_acc:0.977]
Epoch [76/120    avg_loss:0.150, val_acc:0.988]
Epoch [77/120    avg_loss:0.132, val_acc:0.965]
Epoch [78/120    avg_loss:0.152, val_acc:0.975]
Epoch [79/120    avg_loss:0.181, val_acc:0.975]
Epoch [80/120    avg_loss:0.128, val_acc:0.979]
Epoch [81/120    avg_loss:0.139, val_acc:0.977]
Epoch [82/120    avg_loss:0.116, val_acc:0.979]
Epoch [83/120    avg_loss:0.119, val_acc:0.973]
Epoch [84/120    avg_loss:0.119, val_acc:0.975]
Epoch [85/120    avg_loss:0.101, val_acc:0.983]
Epoch [86/120    avg_loss:0.096, val_acc:0.988]
Epoch [87/120    avg_loss:0.084, val_acc:0.988]
Epoch [88/120    avg_loss:0.099, val_acc:0.983]
Epoch [89/120    avg_loss:0.120, val_acc:0.975]
Epoch [90/120    avg_loss:0.146, val_acc:0.971]
Epoch [91/120    avg_loss:0.157, val_acc:0.969]
Epoch [92/120    avg_loss:0.120, val_acc:0.967]
Epoch [93/120    avg_loss:0.121, val_acc:0.990]
Epoch [94/120    avg_loss:0.113, val_acc:0.983]
Epoch [95/120    avg_loss:0.142, val_acc:0.960]
Epoch [96/120    avg_loss:0.106, val_acc:0.973]
Epoch [97/120    avg_loss:0.109, val_acc:0.988]
Epoch [98/120    avg_loss:0.079, val_acc:0.975]
Epoch [99/120    avg_loss:0.068, val_acc:0.988]
Epoch [100/120    avg_loss:0.074, val_acc:0.981]
Epoch [101/120    avg_loss:0.098, val_acc:0.977]
Epoch [102/120    avg_loss:0.146, val_acc:0.977]
Epoch [103/120    avg_loss:0.110, val_acc:0.969]
Epoch [104/120    avg_loss:0.152, val_acc:0.975]
Epoch [105/120    avg_loss:0.083, val_acc:0.985]
Epoch [106/120    avg_loss:0.071, val_acc:0.990]
Epoch [107/120    avg_loss:0.053, val_acc:0.985]
Epoch [108/120    avg_loss:0.067, val_acc:0.983]
Epoch [109/120    avg_loss:0.062, val_acc:0.973]
Epoch [110/120    avg_loss:0.105, val_acc:0.975]
Epoch [111/120    avg_loss:0.089, val_acc:0.988]
Epoch [112/120    avg_loss:0.113, val_acc:0.983]
Epoch [113/120    avg_loss:0.082, val_acc:0.983]
Epoch [114/120    avg_loss:0.068, val_acc:0.985]
Epoch [115/120    avg_loss:0.049, val_acc:0.985]
Epoch [116/120    avg_loss:0.054, val_acc:0.985]
Epoch [117/120    avg_loss:0.045, val_acc:0.990]
Epoch [118/120    avg_loss:0.044, val_acc:0.994]
Epoch [119/120    avg_loss:0.039, val_acc:0.990]
Epoch [120/120    avg_loss:0.034, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   0   9   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  11   0   0   0   0   0   0   4   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 0.99338722 0.99545455 1.         0.94432071 0.92783505
 0.97862233 0.98924731 1.         1.         1.         0.99210526
 0.98893805 1.        ]

Kappa:
0.9900318431449945
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f867b459e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.308]
Epoch [2/120    avg_loss:2.596, val_acc:0.331]
Epoch [3/120    avg_loss:2.567, val_acc:0.379]
Epoch [4/120    avg_loss:2.535, val_acc:0.373]
Epoch [5/120    avg_loss:2.509, val_acc:0.367]
Epoch [6/120    avg_loss:2.477, val_acc:0.365]
Epoch [7/120    avg_loss:2.453, val_acc:0.367]
Epoch [8/120    avg_loss:2.426, val_acc:0.365]
Epoch [9/120    avg_loss:2.393, val_acc:0.369]
Epoch [10/120    avg_loss:2.356, val_acc:0.394]
Epoch [11/120    avg_loss:2.334, val_acc:0.423]
Epoch [12/120    avg_loss:2.302, val_acc:0.404]
Epoch [13/120    avg_loss:2.263, val_acc:0.429]
Epoch [14/120    avg_loss:2.234, val_acc:0.450]
Epoch [15/120    avg_loss:2.192, val_acc:0.485]
Epoch [16/120    avg_loss:2.156, val_acc:0.515]
Epoch [17/120    avg_loss:2.102, val_acc:0.546]
Epoch [18/120    avg_loss:2.058, val_acc:0.569]
Epoch [19/120    avg_loss:2.029, val_acc:0.606]
Epoch [20/120    avg_loss:1.981, val_acc:0.594]
Epoch [21/120    avg_loss:1.913, val_acc:0.635]
Epoch [22/120    avg_loss:1.845, val_acc:0.656]
Epoch [23/120    avg_loss:1.787, val_acc:0.694]
Epoch [24/120    avg_loss:1.707, val_acc:0.708]
Epoch [25/120    avg_loss:1.627, val_acc:0.702]
Epoch [26/120    avg_loss:1.580, val_acc:0.735]
Epoch [27/120    avg_loss:1.484, val_acc:0.731]
Epoch [28/120    avg_loss:1.386, val_acc:0.756]
Epoch [29/120    avg_loss:1.321, val_acc:0.748]
Epoch [30/120    avg_loss:1.262, val_acc:0.752]
Epoch [31/120    avg_loss:1.184, val_acc:0.783]
Epoch [32/120    avg_loss:1.110, val_acc:0.779]
Epoch [33/120    avg_loss:1.023, val_acc:0.779]
Epoch [34/120    avg_loss:0.990, val_acc:0.792]
Epoch [35/120    avg_loss:0.947, val_acc:0.775]
Epoch [36/120    avg_loss:0.864, val_acc:0.794]
Epoch [37/120    avg_loss:0.798, val_acc:0.854]
Epoch [38/120    avg_loss:0.764, val_acc:0.856]
Epoch [39/120    avg_loss:0.717, val_acc:0.858]
Epoch [40/120    avg_loss:0.692, val_acc:0.877]
Epoch [41/120    avg_loss:0.642, val_acc:0.885]
Epoch [42/120    avg_loss:0.621, val_acc:0.912]
Epoch [43/120    avg_loss:0.614, val_acc:0.869]
Epoch [44/120    avg_loss:0.539, val_acc:0.902]
Epoch [45/120    avg_loss:0.505, val_acc:0.896]
Epoch [46/120    avg_loss:0.529, val_acc:0.952]
Epoch [47/120    avg_loss:0.464, val_acc:0.946]
Epoch [48/120    avg_loss:0.438, val_acc:0.946]
Epoch [49/120    avg_loss:0.417, val_acc:0.950]
Epoch [50/120    avg_loss:0.363, val_acc:0.950]
Epoch [51/120    avg_loss:0.359, val_acc:0.925]
Epoch [52/120    avg_loss:0.367, val_acc:0.923]
Epoch [53/120    avg_loss:0.355, val_acc:0.912]
Epoch [54/120    avg_loss:0.389, val_acc:0.954]
Epoch [55/120    avg_loss:0.398, val_acc:0.954]
Epoch [56/120    avg_loss:0.305, val_acc:0.938]
Epoch [57/120    avg_loss:0.325, val_acc:0.950]
Epoch [58/120    avg_loss:0.291, val_acc:0.956]
Epoch [59/120    avg_loss:0.335, val_acc:0.950]
Epoch [60/120    avg_loss:0.285, val_acc:0.954]
Epoch [61/120    avg_loss:0.268, val_acc:0.952]
Epoch [62/120    avg_loss:0.271, val_acc:0.927]
Epoch [63/120    avg_loss:0.259, val_acc:0.965]
Epoch [64/120    avg_loss:0.257, val_acc:0.952]
Epoch [65/120    avg_loss:0.243, val_acc:0.944]
Epoch [66/120    avg_loss:0.213, val_acc:0.942]
Epoch [67/120    avg_loss:0.225, val_acc:0.977]
Epoch [68/120    avg_loss:0.243, val_acc:0.948]
Epoch [69/120    avg_loss:0.198, val_acc:0.952]
Epoch [70/120    avg_loss:0.213, val_acc:0.965]
Epoch [71/120    avg_loss:0.204, val_acc:0.973]
Epoch [72/120    avg_loss:0.165, val_acc:0.971]
Epoch [73/120    avg_loss:0.146, val_acc:0.965]
Epoch [74/120    avg_loss:0.157, val_acc:0.981]
Epoch [75/120    avg_loss:0.186, val_acc:0.973]
Epoch [76/120    avg_loss:0.137, val_acc:0.973]
Epoch [77/120    avg_loss:0.156, val_acc:0.967]
Epoch [78/120    avg_loss:0.145, val_acc:0.969]
Epoch [79/120    avg_loss:0.167, val_acc:0.983]
Epoch [80/120    avg_loss:0.173, val_acc:0.975]
Epoch [81/120    avg_loss:0.154, val_acc:0.969]
Epoch [82/120    avg_loss:0.183, val_acc:0.973]
Epoch [83/120    avg_loss:0.158, val_acc:0.971]
Epoch [84/120    avg_loss:0.139, val_acc:0.977]
Epoch [85/120    avg_loss:0.135, val_acc:0.979]
Epoch [86/120    avg_loss:0.142, val_acc:0.973]
Epoch [87/120    avg_loss:0.130, val_acc:0.960]
Epoch [88/120    avg_loss:0.140, val_acc:0.981]
Epoch [89/120    avg_loss:0.098, val_acc:0.977]
Epoch [90/120    avg_loss:0.093, val_acc:0.983]
Epoch [91/120    avg_loss:0.096, val_acc:0.971]
Epoch [92/120    avg_loss:0.117, val_acc:0.977]
Epoch [93/120    avg_loss:0.083, val_acc:0.971]
Epoch [94/120    avg_loss:0.099, val_acc:0.973]
Epoch [95/120    avg_loss:0.102, val_acc:0.981]
Epoch [96/120    avg_loss:0.079, val_acc:0.981]
Epoch [97/120    avg_loss:0.071, val_acc:0.983]
Epoch [98/120    avg_loss:0.067, val_acc:0.977]
Epoch [99/120    avg_loss:0.082, val_acc:0.977]
Epoch [100/120    avg_loss:0.056, val_acc:0.990]
Epoch [101/120    avg_loss:0.070, val_acc:0.981]
Epoch [102/120    avg_loss:0.062, val_acc:0.983]
Epoch [103/120    avg_loss:0.089, val_acc:0.981]
Epoch [104/120    avg_loss:0.066, val_acc:0.985]
Epoch [105/120    avg_loss:0.063, val_acc:0.985]
Epoch [106/120    avg_loss:0.060, val_acc:0.965]
Epoch [107/120    avg_loss:0.046, val_acc:0.985]
Epoch [108/120    avg_loss:0.065, val_acc:0.956]
Epoch [109/120    avg_loss:0.112, val_acc:0.979]
Epoch [110/120    avg_loss:0.086, val_acc:0.956]
Epoch [111/120    avg_loss:0.111, val_acc:0.977]
Epoch [112/120    avg_loss:0.112, val_acc:0.979]
Epoch [113/120    avg_loss:0.090, val_acc:0.977]
Epoch [114/120    avg_loss:0.057, val_acc:0.981]
Epoch [115/120    avg_loss:0.064, val_acc:0.985]
Epoch [116/120    avg_loss:0.044, val_acc:0.985]
Epoch [117/120    avg_loss:0.041, val_acc:0.985]
Epoch [118/120    avg_loss:0.043, val_acc:0.988]
Epoch [119/120    avg_loss:0.043, val_acc:0.988]
Epoch [120/120    avg_loss:0.039, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99560117 0.99095023 1.         0.94877506 0.9220339
 0.98564593 0.97826087 1.         1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9914555702303646
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd1e5b3da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.646, val_acc:0.033]
Epoch [2/120    avg_loss:2.622, val_acc:0.033]
Epoch [3/120    avg_loss:2.600, val_acc:0.098]
Epoch [4/120    avg_loss:2.576, val_acc:0.115]
Epoch [5/120    avg_loss:2.548, val_acc:0.121]
Epoch [6/120    avg_loss:2.522, val_acc:0.254]
Epoch [7/120    avg_loss:2.492, val_acc:0.315]
Epoch [8/120    avg_loss:2.468, val_acc:0.333]
Epoch [9/120    avg_loss:2.444, val_acc:0.348]
Epoch [10/120    avg_loss:2.418, val_acc:0.350]
Epoch [11/120    avg_loss:2.387, val_acc:0.352]
Epoch [12/120    avg_loss:2.368, val_acc:0.340]
Epoch [13/120    avg_loss:2.336, val_acc:0.346]
Epoch [14/120    avg_loss:2.315, val_acc:0.348]
Epoch [15/120    avg_loss:2.295, val_acc:0.358]
Epoch [16/120    avg_loss:2.255, val_acc:0.354]
Epoch [17/120    avg_loss:2.231, val_acc:0.367]
Epoch [18/120    avg_loss:2.180, val_acc:0.392]
Epoch [19/120    avg_loss:2.154, val_acc:0.417]
Epoch [20/120    avg_loss:2.097, val_acc:0.415]
Epoch [21/120    avg_loss:2.066, val_acc:0.485]
Epoch [22/120    avg_loss:2.025, val_acc:0.500]
Epoch [23/120    avg_loss:1.998, val_acc:0.544]
Epoch [24/120    avg_loss:1.936, val_acc:0.592]
Epoch [25/120    avg_loss:1.891, val_acc:0.635]
Epoch [26/120    avg_loss:1.877, val_acc:0.642]
Epoch [27/120    avg_loss:1.833, val_acc:0.646]
Epoch [28/120    avg_loss:1.786, val_acc:0.669]
Epoch [29/120    avg_loss:1.696, val_acc:0.671]
Epoch [30/120    avg_loss:1.643, val_acc:0.677]
Epoch [31/120    avg_loss:1.580, val_acc:0.662]
Epoch [32/120    avg_loss:1.529, val_acc:0.683]
Epoch [33/120    avg_loss:1.472, val_acc:0.723]
Epoch [34/120    avg_loss:1.391, val_acc:0.719]
Epoch [35/120    avg_loss:1.346, val_acc:0.748]
Epoch [36/120    avg_loss:1.301, val_acc:0.773]
Epoch [37/120    avg_loss:1.227, val_acc:0.790]
Epoch [38/120    avg_loss:1.172, val_acc:0.865]
Epoch [39/120    avg_loss:1.127, val_acc:0.892]
Epoch [40/120    avg_loss:1.086, val_acc:0.838]
Epoch [41/120    avg_loss:1.023, val_acc:0.881]
Epoch [42/120    avg_loss:0.996, val_acc:0.910]
Epoch [43/120    avg_loss:0.932, val_acc:0.908]
Epoch [44/120    avg_loss:0.900, val_acc:0.912]
Epoch [45/120    avg_loss:0.872, val_acc:0.929]
Epoch [46/120    avg_loss:0.811, val_acc:0.921]
Epoch [47/120    avg_loss:0.841, val_acc:0.890]
Epoch [48/120    avg_loss:0.848, val_acc:0.894]
Epoch [49/120    avg_loss:0.771, val_acc:0.912]
Epoch [50/120    avg_loss:0.716, val_acc:0.917]
Epoch [51/120    avg_loss:0.673, val_acc:0.933]
Epoch [52/120    avg_loss:0.641, val_acc:0.902]
Epoch [53/120    avg_loss:0.601, val_acc:0.887]
Epoch [54/120    avg_loss:0.580, val_acc:0.917]
Epoch [55/120    avg_loss:0.555, val_acc:0.927]
Epoch [56/120    avg_loss:0.540, val_acc:0.935]
Epoch [57/120    avg_loss:0.545, val_acc:0.915]
Epoch [58/120    avg_loss:0.494, val_acc:0.927]
Epoch [59/120    avg_loss:0.458, val_acc:0.923]
Epoch [60/120    avg_loss:0.463, val_acc:0.950]
Epoch [61/120    avg_loss:0.459, val_acc:0.900]
Epoch [62/120    avg_loss:0.538, val_acc:0.917]
Epoch [63/120    avg_loss:0.488, val_acc:0.931]
Epoch [64/120    avg_loss:0.460, val_acc:0.910]
Epoch [65/120    avg_loss:0.506, val_acc:0.942]
Epoch [66/120    avg_loss:0.466, val_acc:0.933]
Epoch [67/120    avg_loss:0.413, val_acc:0.933]
Epoch [68/120    avg_loss:0.389, val_acc:0.946]
Epoch [69/120    avg_loss:0.402, val_acc:0.946]
Epoch [70/120    avg_loss:0.361, val_acc:0.902]
Epoch [71/120    avg_loss:0.407, val_acc:0.935]
Epoch [72/120    avg_loss:0.348, val_acc:0.935]
Epoch [73/120    avg_loss:0.414, val_acc:0.925]
Epoch [74/120    avg_loss:0.357, val_acc:0.929]
Epoch [75/120    avg_loss:0.305, val_acc:0.950]
Epoch [76/120    avg_loss:0.282, val_acc:0.956]
Epoch [77/120    avg_loss:0.279, val_acc:0.952]
Epoch [78/120    avg_loss:0.276, val_acc:0.956]
Epoch [79/120    avg_loss:0.282, val_acc:0.950]
Epoch [80/120    avg_loss:0.270, val_acc:0.950]
Epoch [81/120    avg_loss:0.262, val_acc:0.954]
Epoch [82/120    avg_loss:0.283, val_acc:0.952]
Epoch [83/120    avg_loss:0.271, val_acc:0.954]
Epoch [84/120    avg_loss:0.260, val_acc:0.952]
Epoch [85/120    avg_loss:0.263, val_acc:0.952]
Epoch [86/120    avg_loss:0.265, val_acc:0.956]
Epoch [87/120    avg_loss:0.248, val_acc:0.956]
Epoch [88/120    avg_loss:0.279, val_acc:0.956]
Epoch [89/120    avg_loss:0.241, val_acc:0.958]
Epoch [90/120    avg_loss:0.257, val_acc:0.963]
Epoch [91/120    avg_loss:0.271, val_acc:0.954]
Epoch [92/120    avg_loss:0.237, val_acc:0.963]
Epoch [93/120    avg_loss:0.250, val_acc:0.960]
Epoch [94/120    avg_loss:0.249, val_acc:0.954]
Epoch [95/120    avg_loss:0.260, val_acc:0.956]
Epoch [96/120    avg_loss:0.237, val_acc:0.963]
Epoch [97/120    avg_loss:0.242, val_acc:0.963]
Epoch [98/120    avg_loss:0.244, val_acc:0.956]
Epoch [99/120    avg_loss:0.232, val_acc:0.963]
Epoch [100/120    avg_loss:0.243, val_acc:0.960]
Epoch [101/120    avg_loss:0.229, val_acc:0.965]
Epoch [102/120    avg_loss:0.224, val_acc:0.967]
Epoch [103/120    avg_loss:0.245, val_acc:0.960]
Epoch [104/120    avg_loss:0.230, val_acc:0.963]
Epoch [105/120    avg_loss:0.237, val_acc:0.967]
Epoch [106/120    avg_loss:0.225, val_acc:0.969]
Epoch [107/120    avg_loss:0.228, val_acc:0.960]
Epoch [108/120    avg_loss:0.233, val_acc:0.969]
Epoch [109/120    avg_loss:0.210, val_acc:0.963]
Epoch [110/120    avg_loss:0.216, val_acc:0.967]
Epoch [111/120    avg_loss:0.219, val_acc:0.965]
Epoch [112/120    avg_loss:0.232, val_acc:0.963]
Epoch [113/120    avg_loss:0.229, val_acc:0.956]
Epoch [114/120    avg_loss:0.214, val_acc:0.960]
Epoch [115/120    avg_loss:0.217, val_acc:0.958]
Epoch [116/120    avg_loss:0.219, val_acc:0.965]
Epoch [117/120    avg_loss:0.222, val_acc:0.956]
Epoch [118/120    avg_loss:0.229, val_acc:0.960]
Epoch [119/120    avg_loss:0.198, val_acc:0.969]
Epoch [120/120    avg_loss:0.220, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   2 213  12   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  55  90   0   0   0   0   0   0   0   0]
 [  0  15   0   0  11  10 170   0   0   0   0   0   0   0]
 [  0   0  25   0   0   0   0  69   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.75906183368869

F1 scores:
[       nan 0.98916968 0.93275488 0.96162528 0.82466281 0.69767442
 0.90425532 0.82634731 0.99614891 0.99893276 1.         0.99867198
 0.99779249 1.        ]

Kappa:
0.9638935632000408
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94b86afe10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.071]
Epoch [2/120    avg_loss:2.594, val_acc:0.190]
Epoch [3/120    avg_loss:2.558, val_acc:0.335]
Epoch [4/120    avg_loss:2.527, val_acc:0.352]
Epoch [5/120    avg_loss:2.490, val_acc:0.354]
Epoch [6/120    avg_loss:2.455, val_acc:0.356]
Epoch [7/120    avg_loss:2.418, val_acc:0.362]
Epoch [8/120    avg_loss:2.384, val_acc:0.375]
Epoch [9/120    avg_loss:2.349, val_acc:0.388]
Epoch [10/120    avg_loss:2.312, val_acc:0.406]
Epoch [11/120    avg_loss:2.287, val_acc:0.419]
Epoch [12/120    avg_loss:2.241, val_acc:0.429]
Epoch [13/120    avg_loss:2.213, val_acc:0.456]
Epoch [14/120    avg_loss:2.168, val_acc:0.471]
Epoch [15/120    avg_loss:2.129, val_acc:0.454]
Epoch [16/120    avg_loss:2.083, val_acc:0.487]
Epoch [17/120    avg_loss:2.064, val_acc:0.496]
Epoch [18/120    avg_loss:2.002, val_acc:0.544]
Epoch [19/120    avg_loss:1.961, val_acc:0.537]
Epoch [20/120    avg_loss:1.916, val_acc:0.565]
Epoch [21/120    avg_loss:1.861, val_acc:0.571]
Epoch [22/120    avg_loss:1.831, val_acc:0.604]
Epoch [23/120    avg_loss:1.777, val_acc:0.596]
Epoch [24/120    avg_loss:1.725, val_acc:0.627]
Epoch [25/120    avg_loss:1.667, val_acc:0.642]
Epoch [26/120    avg_loss:1.615, val_acc:0.652]
Epoch [27/120    avg_loss:1.541, val_acc:0.629]
Epoch [28/120    avg_loss:1.518, val_acc:0.654]
Epoch [29/120    avg_loss:1.455, val_acc:0.679]
Epoch [30/120    avg_loss:1.402, val_acc:0.669]
Epoch [31/120    avg_loss:1.346, val_acc:0.692]
Epoch [32/120    avg_loss:1.282, val_acc:0.692]
Epoch [33/120    avg_loss:1.238, val_acc:0.721]
Epoch [34/120    avg_loss:1.208, val_acc:0.787]
Epoch [35/120    avg_loss:1.148, val_acc:0.848]
Epoch [36/120    avg_loss:1.114, val_acc:0.825]
Epoch [37/120    avg_loss:1.082, val_acc:0.773]
Epoch [38/120    avg_loss:1.007, val_acc:0.762]
Epoch [39/120    avg_loss:1.011, val_acc:0.856]
Epoch [40/120    avg_loss:0.959, val_acc:0.881]
Epoch [41/120    avg_loss:0.925, val_acc:0.787]
Epoch [42/120    avg_loss:0.854, val_acc:0.890]
Epoch [43/120    avg_loss:0.812, val_acc:0.894]
Epoch [44/120    avg_loss:0.804, val_acc:0.885]
Epoch [45/120    avg_loss:0.766, val_acc:0.896]
Epoch [46/120    avg_loss:0.714, val_acc:0.912]
Epoch [47/120    avg_loss:0.677, val_acc:0.915]
Epoch [48/120    avg_loss:0.675, val_acc:0.908]
Epoch [49/120    avg_loss:0.657, val_acc:0.917]
Epoch [50/120    avg_loss:0.620, val_acc:0.915]
Epoch [51/120    avg_loss:0.584, val_acc:0.915]
Epoch [52/120    avg_loss:0.535, val_acc:0.915]
Epoch [53/120    avg_loss:0.532, val_acc:0.929]
Epoch [54/120    avg_loss:0.601, val_acc:0.906]
Epoch [55/120    avg_loss:0.500, val_acc:0.927]
Epoch [56/120    avg_loss:0.500, val_acc:0.925]
Epoch [57/120    avg_loss:0.489, val_acc:0.929]
Epoch [58/120    avg_loss:0.480, val_acc:0.917]
Epoch [59/120    avg_loss:0.447, val_acc:0.925]
Epoch [60/120    avg_loss:0.430, val_acc:0.933]
Epoch [61/120    avg_loss:0.412, val_acc:0.931]
Epoch [62/120    avg_loss:0.418, val_acc:0.931]
Epoch [63/120    avg_loss:0.372, val_acc:0.929]
Epoch [64/120    avg_loss:0.431, val_acc:0.940]
Epoch [65/120    avg_loss:0.366, val_acc:0.923]
Epoch [66/120    avg_loss:0.395, val_acc:0.931]
Epoch [67/120    avg_loss:0.430, val_acc:0.929]
Epoch [68/120    avg_loss:0.394, val_acc:0.929]
Epoch [69/120    avg_loss:0.349, val_acc:0.938]
Epoch [70/120    avg_loss:0.336, val_acc:0.946]
Epoch [71/120    avg_loss:0.329, val_acc:0.946]
Epoch [72/120    avg_loss:0.288, val_acc:0.933]
Epoch [73/120    avg_loss:0.341, val_acc:0.935]
Epoch [74/120    avg_loss:0.348, val_acc:0.933]
Epoch [75/120    avg_loss:0.320, val_acc:0.921]
Epoch [76/120    avg_loss:0.294, val_acc:0.950]
Epoch [77/120    avg_loss:0.263, val_acc:0.940]
Epoch [78/120    avg_loss:0.236, val_acc:0.963]
Epoch [79/120    avg_loss:0.220, val_acc:0.958]
Epoch [80/120    avg_loss:0.215, val_acc:0.950]
Epoch [81/120    avg_loss:0.211, val_acc:0.958]
Epoch [82/120    avg_loss:0.202, val_acc:0.948]
Epoch [83/120    avg_loss:0.229, val_acc:0.969]
Epoch [84/120    avg_loss:0.261, val_acc:0.967]
Epoch [85/120    avg_loss:0.194, val_acc:0.965]
Epoch [86/120    avg_loss:0.177, val_acc:0.958]
Epoch [87/120    avg_loss:0.243, val_acc:0.940]
Epoch [88/120    avg_loss:0.249, val_acc:0.958]
Epoch [89/120    avg_loss:0.212, val_acc:0.965]
Epoch [90/120    avg_loss:0.210, val_acc:0.965]
Epoch [91/120    avg_loss:0.197, val_acc:0.942]
Epoch [92/120    avg_loss:0.213, val_acc:0.958]
Epoch [93/120    avg_loss:0.232, val_acc:0.944]
Epoch [94/120    avg_loss:0.210, val_acc:0.963]
Epoch [95/120    avg_loss:0.174, val_acc:0.969]
Epoch [96/120    avg_loss:0.173, val_acc:0.967]
Epoch [97/120    avg_loss:0.168, val_acc:0.967]
Epoch [98/120    avg_loss:0.137, val_acc:0.975]
Epoch [99/120    avg_loss:0.166, val_acc:0.977]
Epoch [100/120    avg_loss:0.142, val_acc:0.969]
Epoch [101/120    avg_loss:0.123, val_acc:0.979]
Epoch [102/120    avg_loss:0.141, val_acc:0.969]
Epoch [103/120    avg_loss:0.210, val_acc:0.942]
Epoch [104/120    avg_loss:0.225, val_acc:0.950]
Epoch [105/120    avg_loss:0.216, val_acc:0.925]
Epoch [106/120    avg_loss:0.183, val_acc:0.967]
Epoch [107/120    avg_loss:0.148, val_acc:0.971]
Epoch [108/120    avg_loss:0.111, val_acc:0.971]
Epoch [109/120    avg_loss:0.136, val_acc:0.979]
Epoch [110/120    avg_loss:0.117, val_acc:0.973]
Epoch [111/120    avg_loss:0.114, val_acc:0.979]
Epoch [112/120    avg_loss:0.144, val_acc:0.981]
Epoch [113/120    avg_loss:0.112, val_acc:0.977]
Epoch [114/120    avg_loss:0.105, val_acc:0.979]
Epoch [115/120    avg_loss:0.081, val_acc:0.977]
Epoch [116/120    avg_loss:0.089, val_acc:0.983]
Epoch [117/120    avg_loss:0.074, val_acc:0.971]
Epoch [118/120    avg_loss:0.098, val_acc:0.979]
Epoch [119/120    avg_loss:0.101, val_acc:0.979]
Epoch [120/120    avg_loss:0.104, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   2   6   0   0   0   0   0   0]
 [  0   0   0 221   1   0   0   0   6   2   0   0   0   0]
 [  0   0   0  16 200   0  10   0   1   0   0   0   0   0]
 [  0   0   0   9  55  76   5   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  25   0   0   0   4  65   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   5 463   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   7   0   3   0 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.63113006396588

F1 scores:
[       nan 1.         0.92747253 0.92857143 0.82815735 0.68778281
 0.95150115 0.78787879 0.97610063 0.99249732 0.99589603 0.99867198
 0.9877369  1.        ]

Kappa:
0.9624771102625169
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0fa7391e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.632, val_acc:0.071]
Epoch [2/120    avg_loss:2.600, val_acc:0.102]
Epoch [3/120    avg_loss:2.572, val_acc:0.098]
Epoch [4/120    avg_loss:2.537, val_acc:0.167]
Epoch [5/120    avg_loss:2.511, val_acc:0.260]
Epoch [6/120    avg_loss:2.499, val_acc:0.273]
Epoch [7/120    avg_loss:2.469, val_acc:0.273]
Epoch [8/120    avg_loss:2.456, val_acc:0.273]
Epoch [9/120    avg_loss:2.430, val_acc:0.273]
Epoch [10/120    avg_loss:2.414, val_acc:0.271]
Epoch [11/120    avg_loss:2.397, val_acc:0.275]
Epoch [12/120    avg_loss:2.378, val_acc:0.306]
Epoch [13/120    avg_loss:2.351, val_acc:0.335]
Epoch [14/120    avg_loss:2.325, val_acc:0.348]
Epoch [15/120    avg_loss:2.293, val_acc:0.373]
Epoch [16/120    avg_loss:2.275, val_acc:0.425]
Epoch [17/120    avg_loss:2.232, val_acc:0.431]
Epoch [18/120    avg_loss:2.205, val_acc:0.442]
Epoch [19/120    avg_loss:2.161, val_acc:0.446]
Epoch [20/120    avg_loss:2.111, val_acc:0.450]
Epoch [21/120    avg_loss:2.070, val_acc:0.452]
Epoch [22/120    avg_loss:2.020, val_acc:0.460]
Epoch [23/120    avg_loss:1.970, val_acc:0.473]
Epoch [24/120    avg_loss:1.919, val_acc:0.471]
Epoch [25/120    avg_loss:1.865, val_acc:0.490]
Epoch [26/120    avg_loss:1.810, val_acc:0.500]
Epoch [27/120    avg_loss:1.767, val_acc:0.544]
Epoch [28/120    avg_loss:1.703, val_acc:0.633]
Epoch [29/120    avg_loss:1.673, val_acc:0.642]
Epoch [30/120    avg_loss:1.610, val_acc:0.673]
Epoch [31/120    avg_loss:1.584, val_acc:0.706]
Epoch [32/120    avg_loss:1.526, val_acc:0.679]
Epoch [33/120    avg_loss:1.511, val_acc:0.713]
Epoch [34/120    avg_loss:1.456, val_acc:0.771]
Epoch [35/120    avg_loss:1.344, val_acc:0.756]
Epoch [36/120    avg_loss:1.323, val_acc:0.760]
Epoch [37/120    avg_loss:1.257, val_acc:0.804]
Epoch [38/120    avg_loss:1.185, val_acc:0.829]
Epoch [39/120    avg_loss:1.155, val_acc:0.825]
Epoch [40/120    avg_loss:1.101, val_acc:0.835]
Epoch [41/120    avg_loss:1.039, val_acc:0.858]
Epoch [42/120    avg_loss:1.014, val_acc:0.875]
Epoch [43/120    avg_loss:0.932, val_acc:0.892]
Epoch [44/120    avg_loss:0.883, val_acc:0.887]
Epoch [45/120    avg_loss:0.858, val_acc:0.885]
Epoch [46/120    avg_loss:0.841, val_acc:0.892]
Epoch [47/120    avg_loss:0.785, val_acc:0.912]
Epoch [48/120    avg_loss:0.679, val_acc:0.906]
Epoch [49/120    avg_loss:0.671, val_acc:0.904]
Epoch [50/120    avg_loss:0.684, val_acc:0.923]
Epoch [51/120    avg_loss:0.621, val_acc:0.908]
Epoch [52/120    avg_loss:0.559, val_acc:0.915]
Epoch [53/120    avg_loss:0.554, val_acc:0.910]
Epoch [54/120    avg_loss:0.551, val_acc:0.906]
Epoch [55/120    avg_loss:0.571, val_acc:0.917]
Epoch [56/120    avg_loss:0.516, val_acc:0.931]
Epoch [57/120    avg_loss:0.482, val_acc:0.894]
Epoch [58/120    avg_loss:0.447, val_acc:0.929]
Epoch [59/120    avg_loss:0.426, val_acc:0.912]
Epoch [60/120    avg_loss:0.484, val_acc:0.944]
Epoch [61/120    avg_loss:0.438, val_acc:0.929]
Epoch [62/120    avg_loss:0.453, val_acc:0.915]
Epoch [63/120    avg_loss:0.470, val_acc:0.925]
Epoch [64/120    avg_loss:0.433, val_acc:0.931]
Epoch [65/120    avg_loss:0.391, val_acc:0.935]
Epoch [66/120    avg_loss:0.376, val_acc:0.921]
Epoch [67/120    avg_loss:0.379, val_acc:0.940]
Epoch [68/120    avg_loss:0.345, val_acc:0.948]
Epoch [69/120    avg_loss:0.338, val_acc:0.954]
Epoch [70/120    avg_loss:0.305, val_acc:0.942]
Epoch [71/120    avg_loss:0.297, val_acc:0.935]
Epoch [72/120    avg_loss:0.329, val_acc:0.956]
Epoch [73/120    avg_loss:0.360, val_acc:0.946]
Epoch [74/120    avg_loss:0.268, val_acc:0.956]
Epoch [75/120    avg_loss:0.294, val_acc:0.950]
Epoch [76/120    avg_loss:0.274, val_acc:0.960]
Epoch [77/120    avg_loss:0.305, val_acc:0.944]
Epoch [78/120    avg_loss:0.293, val_acc:0.954]
Epoch [79/120    avg_loss:0.254, val_acc:0.944]
Epoch [80/120    avg_loss:0.231, val_acc:0.950]
Epoch [81/120    avg_loss:0.245, val_acc:0.958]
Epoch [82/120    avg_loss:0.246, val_acc:0.954]
Epoch [83/120    avg_loss:0.215, val_acc:0.967]
Epoch [84/120    avg_loss:0.231, val_acc:0.963]
Epoch [85/120    avg_loss:0.207, val_acc:0.969]
Epoch [86/120    avg_loss:0.187, val_acc:0.975]
Epoch [87/120    avg_loss:0.178, val_acc:0.973]
Epoch [88/120    avg_loss:0.174, val_acc:0.973]
Epoch [89/120    avg_loss:0.209, val_acc:0.958]
Epoch [90/120    avg_loss:0.206, val_acc:0.958]
Epoch [91/120    avg_loss:0.253, val_acc:0.960]
Epoch [92/120    avg_loss:0.197, val_acc:0.971]
Epoch [93/120    avg_loss:0.178, val_acc:0.981]
Epoch [94/120    avg_loss:0.184, val_acc:0.956]
Epoch [95/120    avg_loss:0.178, val_acc:0.956]
Epoch [96/120    avg_loss:0.175, val_acc:0.981]
Epoch [97/120    avg_loss:0.189, val_acc:0.969]
Epoch [98/120    avg_loss:0.180, val_acc:0.958]
Epoch [99/120    avg_loss:0.180, val_acc:0.975]
Epoch [100/120    avg_loss:0.146, val_acc:0.958]
Epoch [101/120    avg_loss:0.189, val_acc:0.954]
Epoch [102/120    avg_loss:0.164, val_acc:0.985]
Epoch [103/120    avg_loss:0.143, val_acc:0.988]
Epoch [104/120    avg_loss:0.122, val_acc:0.992]
Epoch [105/120    avg_loss:0.135, val_acc:0.965]
Epoch [106/120    avg_loss:0.139, val_acc:0.981]
Epoch [107/120    avg_loss:0.127, val_acc:0.985]
Epoch [108/120    avg_loss:0.118, val_acc:0.990]
Epoch [109/120    avg_loss:0.108, val_acc:0.990]
Epoch [110/120    avg_loss:0.116, val_acc:0.985]
Epoch [111/120    avg_loss:0.115, val_acc:0.992]
Epoch [112/120    avg_loss:0.097, val_acc:0.985]
Epoch [113/120    avg_loss:0.105, val_acc:0.981]
Epoch [114/120    avg_loss:0.092, val_acc:0.985]
Epoch [115/120    avg_loss:0.076, val_acc:0.985]
Epoch [116/120    avg_loss:0.095, val_acc:0.956]
Epoch [117/120    avg_loss:0.133, val_acc:0.956]
Epoch [118/120    avg_loss:0.115, val_acc:0.990]
Epoch [119/120    avg_loss:0.103, val_acc:0.983]
Epoch [120/120    avg_loss:0.083, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 206   0   0   0   0  13   0   0   0   0   0   0]
 [  0   0   0 224   3   0   0   0   2   1   0   0   0   0]
 [  0   0   0   1 226   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  33 112   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   4   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 0.99927061 0.93636364 0.98461538 0.92433538 0.87159533
 0.99756691 0.84946237 0.99232737 0.99893276 1.         1.
 0.99556541 1.        ]

Kappa:
0.9826678877831668
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7facaa88cdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.667, val_acc:0.000]
Epoch [2/120    avg_loss:2.644, val_acc:0.000]
Epoch [3/120    avg_loss:2.624, val_acc:0.106]
Epoch [4/120    avg_loss:2.606, val_acc:0.135]
Epoch [5/120    avg_loss:2.589, val_acc:0.125]
Epoch [6/120    avg_loss:2.570, val_acc:0.122]
Epoch [7/120    avg_loss:2.551, val_acc:0.129]
Epoch [8/120    avg_loss:2.531, val_acc:0.129]
Epoch [9/120    avg_loss:2.508, val_acc:0.227]
Epoch [10/120    avg_loss:2.487, val_acc:0.296]
Epoch [11/120    avg_loss:2.462, val_acc:0.315]
Epoch [12/120    avg_loss:2.437, val_acc:0.319]
Epoch [13/120    avg_loss:2.410, val_acc:0.329]
Epoch [14/120    avg_loss:2.381, val_acc:0.333]
Epoch [15/120    avg_loss:2.350, val_acc:0.340]
Epoch [16/120    avg_loss:2.319, val_acc:0.365]
Epoch [17/120    avg_loss:2.292, val_acc:0.390]
Epoch [18/120    avg_loss:2.248, val_acc:0.442]
Epoch [19/120    avg_loss:2.211, val_acc:0.498]
Epoch [20/120    avg_loss:2.174, val_acc:0.487]
Epoch [21/120    avg_loss:2.143, val_acc:0.556]
Epoch [22/120    avg_loss:2.092, val_acc:0.544]
Epoch [23/120    avg_loss:2.053, val_acc:0.660]
Epoch [24/120    avg_loss:2.008, val_acc:0.604]
Epoch [25/120    avg_loss:1.949, val_acc:0.710]
Epoch [26/120    avg_loss:1.913, val_acc:0.685]
Epoch [27/120    avg_loss:1.859, val_acc:0.731]
Epoch [28/120    avg_loss:1.810, val_acc:0.742]
Epoch [29/120    avg_loss:1.787, val_acc:0.752]
Epoch [30/120    avg_loss:1.714, val_acc:0.798]
Epoch [31/120    avg_loss:1.671, val_acc:0.754]
Epoch [32/120    avg_loss:1.609, val_acc:0.802]
Epoch [33/120    avg_loss:1.565, val_acc:0.796]
Epoch [34/120    avg_loss:1.492, val_acc:0.831]
Epoch [35/120    avg_loss:1.446, val_acc:0.821]
Epoch [36/120    avg_loss:1.409, val_acc:0.835]
Epoch [37/120    avg_loss:1.333, val_acc:0.840]
Epoch [38/120    avg_loss:1.291, val_acc:0.842]
Epoch [39/120    avg_loss:1.230, val_acc:0.883]
Epoch [40/120    avg_loss:1.172, val_acc:0.887]
Epoch [41/120    avg_loss:1.120, val_acc:0.890]
Epoch [42/120    avg_loss:1.081, val_acc:0.856]
Epoch [43/120    avg_loss:1.032, val_acc:0.900]
Epoch [44/120    avg_loss:0.984, val_acc:0.892]
Epoch [45/120    avg_loss:0.929, val_acc:0.873]
Epoch [46/120    avg_loss:0.887, val_acc:0.896]
Epoch [47/120    avg_loss:0.952, val_acc:0.887]
Epoch [48/120    avg_loss:0.844, val_acc:0.910]
Epoch [49/120    avg_loss:0.777, val_acc:0.900]
Epoch [50/120    avg_loss:0.749, val_acc:0.898]
Epoch [51/120    avg_loss:0.734, val_acc:0.908]
Epoch [52/120    avg_loss:0.690, val_acc:0.906]
Epoch [53/120    avg_loss:0.726, val_acc:0.875]
Epoch [54/120    avg_loss:0.694, val_acc:0.919]
Epoch [55/120    avg_loss:0.670, val_acc:0.915]
Epoch [56/120    avg_loss:0.602, val_acc:0.910]
Epoch [57/120    avg_loss:0.630, val_acc:0.912]
Epoch [58/120    avg_loss:0.571, val_acc:0.925]
Epoch [59/120    avg_loss:0.536, val_acc:0.906]
Epoch [60/120    avg_loss:0.504, val_acc:0.925]
Epoch [61/120    avg_loss:0.461, val_acc:0.919]
Epoch [62/120    avg_loss:0.474, val_acc:0.923]
Epoch [63/120    avg_loss:0.481, val_acc:0.898]
Epoch [64/120    avg_loss:0.457, val_acc:0.927]
Epoch [65/120    avg_loss:0.464, val_acc:0.927]
Epoch [66/120    avg_loss:0.432, val_acc:0.931]
Epoch [67/120    avg_loss:0.410, val_acc:0.929]
Epoch [68/120    avg_loss:0.393, val_acc:0.929]
Epoch [69/120    avg_loss:0.459, val_acc:0.940]
Epoch [70/120    avg_loss:0.466, val_acc:0.919]
Epoch [71/120    avg_loss:0.425, val_acc:0.940]
Epoch [72/120    avg_loss:0.387, val_acc:0.933]
Epoch [73/120    avg_loss:0.358, val_acc:0.931]
Epoch [74/120    avg_loss:0.396, val_acc:0.946]
Epoch [75/120    avg_loss:0.357, val_acc:0.931]
Epoch [76/120    avg_loss:0.328, val_acc:0.948]
Epoch [77/120    avg_loss:0.357, val_acc:0.946]
Epoch [78/120    avg_loss:0.344, val_acc:0.919]
Epoch [79/120    avg_loss:0.357, val_acc:0.927]
Epoch [80/120    avg_loss:0.324, val_acc:0.938]
Epoch [81/120    avg_loss:0.297, val_acc:0.935]
Epoch [82/120    avg_loss:0.284, val_acc:0.954]
Epoch [83/120    avg_loss:0.343, val_acc:0.950]
Epoch [84/120    avg_loss:0.250, val_acc:0.954]
Epoch [85/120    avg_loss:0.292, val_acc:0.946]
Epoch [86/120    avg_loss:0.280, val_acc:0.969]
Epoch [87/120    avg_loss:0.282, val_acc:0.942]
Epoch [88/120    avg_loss:0.300, val_acc:0.938]
Epoch [89/120    avg_loss:0.269, val_acc:0.944]
Epoch [90/120    avg_loss:0.243, val_acc:0.942]
Epoch [91/120    avg_loss:0.237, val_acc:0.952]
Epoch [92/120    avg_loss:0.269, val_acc:0.971]
Epoch [93/120    avg_loss:0.305, val_acc:0.906]
Epoch [94/120    avg_loss:0.342, val_acc:0.950]
Epoch [95/120    avg_loss:0.225, val_acc:0.960]
Epoch [96/120    avg_loss:0.229, val_acc:0.956]
Epoch [97/120    avg_loss:0.227, val_acc:0.956]
Epoch [98/120    avg_loss:0.240, val_acc:0.956]
Epoch [99/120    avg_loss:0.228, val_acc:0.960]
Epoch [100/120    avg_loss:0.196, val_acc:0.952]
Epoch [101/120    avg_loss:0.190, val_acc:0.963]
Epoch [102/120    avg_loss:0.167, val_acc:0.969]
Epoch [103/120    avg_loss:0.175, val_acc:0.958]
Epoch [104/120    avg_loss:0.200, val_acc:0.960]
Epoch [105/120    avg_loss:0.186, val_acc:0.977]
Epoch [106/120    avg_loss:0.173, val_acc:0.944]
Epoch [107/120    avg_loss:0.240, val_acc:0.958]
Epoch [108/120    avg_loss:0.191, val_acc:0.956]
Epoch [109/120    avg_loss:0.188, val_acc:0.954]
Epoch [110/120    avg_loss:0.163, val_acc:0.965]
Epoch [111/120    avg_loss:0.148, val_acc:0.977]
Epoch [112/120    avg_loss:0.141, val_acc:0.965]
Epoch [113/120    avg_loss:0.155, val_acc:0.963]
Epoch [114/120    avg_loss:0.167, val_acc:0.963]
Epoch [115/120    avg_loss:0.166, val_acc:0.965]
Epoch [116/120    avg_loss:0.177, val_acc:0.956]
Epoch [117/120    avg_loss:0.239, val_acc:0.979]
Epoch [118/120    avg_loss:0.175, val_acc:0.965]
Epoch [119/120    avg_loss:0.164, val_acc:0.975]
Epoch [120/120    avg_loss:0.136, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   6   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0  29 116   0   0   0   0   0   0   0   0]
 [  0  31   0   0   0   0 175   0   0   0   0   0   0   0]
 [  0   0  27   0   0   0   0  67   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   5   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.27078891257996

F1 scores:
[       nan 0.97787295 0.94193548 0.98004435 0.87068966 0.81118881
 0.91863517 0.83229814 0.99106003 0.99893276 1.         0.99734043
 0.99224806 1.        ]

Kappa:
0.9695860563579008
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd0ec14dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.100]
Epoch [2/120    avg_loss:2.594, val_acc:0.258]
Epoch [3/120    avg_loss:2.567, val_acc:0.367]
Epoch [4/120    avg_loss:2.539, val_acc:0.388]
Epoch [5/120    avg_loss:2.513, val_acc:0.396]
Epoch [6/120    avg_loss:2.495, val_acc:0.404]
Epoch [7/120    avg_loss:2.460, val_acc:0.408]
Epoch [8/120    avg_loss:2.439, val_acc:0.412]
Epoch [9/120    avg_loss:2.405, val_acc:0.419]
Epoch [10/120    avg_loss:2.380, val_acc:0.419]
Epoch [11/120    avg_loss:2.346, val_acc:0.412]
Epoch [12/120    avg_loss:2.306, val_acc:0.412]
Epoch [13/120    avg_loss:2.278, val_acc:0.398]
Epoch [14/120    avg_loss:2.231, val_acc:0.423]
Epoch [15/120    avg_loss:2.191, val_acc:0.454]
Epoch [16/120    avg_loss:2.146, val_acc:0.475]
Epoch [17/120    avg_loss:2.137, val_acc:0.490]
Epoch [18/120    avg_loss:2.099, val_acc:0.500]
Epoch [19/120    avg_loss:2.059, val_acc:0.590]
Epoch [20/120    avg_loss:2.019, val_acc:0.627]
Epoch [21/120    avg_loss:1.961, val_acc:0.592]
Epoch [22/120    avg_loss:1.910, val_acc:0.637]
Epoch [23/120    avg_loss:1.870, val_acc:0.679]
Epoch [24/120    avg_loss:1.846, val_acc:0.685]
Epoch [25/120    avg_loss:1.816, val_acc:0.640]
Epoch [26/120    avg_loss:1.779, val_acc:0.667]
Epoch [27/120    avg_loss:1.719, val_acc:0.708]
Epoch [28/120    avg_loss:1.639, val_acc:0.700]
Epoch [29/120    avg_loss:1.607, val_acc:0.708]
Epoch [30/120    avg_loss:1.584, val_acc:0.727]
Epoch [31/120    avg_loss:1.526, val_acc:0.748]
Epoch [32/120    avg_loss:1.447, val_acc:0.771]
Epoch [33/120    avg_loss:1.433, val_acc:0.758]
Epoch [34/120    avg_loss:1.357, val_acc:0.769]
Epoch [35/120    avg_loss:1.307, val_acc:0.783]
Epoch [36/120    avg_loss:1.269, val_acc:0.765]
Epoch [37/120    avg_loss:1.210, val_acc:0.808]
Epoch [38/120    avg_loss:1.168, val_acc:0.846]
Epoch [39/120    avg_loss:1.144, val_acc:0.827]
Epoch [40/120    avg_loss:1.043, val_acc:0.854]
Epoch [41/120    avg_loss:0.989, val_acc:0.852]
Epoch [42/120    avg_loss:0.968, val_acc:0.850]
Epoch [43/120    avg_loss:0.940, val_acc:0.890]
Epoch [44/120    avg_loss:0.917, val_acc:0.840]
Epoch [45/120    avg_loss:0.879, val_acc:0.900]
Epoch [46/120    avg_loss:0.790, val_acc:0.908]
Epoch [47/120    avg_loss:0.773, val_acc:0.863]
Epoch [48/120    avg_loss:0.737, val_acc:0.923]
Epoch [49/120    avg_loss:0.687, val_acc:0.929]
Epoch [50/120    avg_loss:0.676, val_acc:0.917]
Epoch [51/120    avg_loss:0.646, val_acc:0.921]
Epoch [52/120    avg_loss:0.635, val_acc:0.902]
Epoch [53/120    avg_loss:0.600, val_acc:0.929]
Epoch [54/120    avg_loss:0.550, val_acc:0.938]
Epoch [55/120    avg_loss:0.556, val_acc:0.917]
Epoch [56/120    avg_loss:0.534, val_acc:0.919]
Epoch [57/120    avg_loss:0.503, val_acc:0.915]
Epoch [58/120    avg_loss:0.532, val_acc:0.894]
Epoch [59/120    avg_loss:0.535, val_acc:0.915]
Epoch [60/120    avg_loss:0.495, val_acc:0.915]
Epoch [61/120    avg_loss:0.454, val_acc:0.910]
Epoch [62/120    avg_loss:0.464, val_acc:0.929]
Epoch [63/120    avg_loss:0.420, val_acc:0.917]
Epoch [64/120    avg_loss:0.422, val_acc:0.938]
Epoch [65/120    avg_loss:0.402, val_acc:0.890]
Epoch [66/120    avg_loss:0.392, val_acc:0.938]
Epoch [67/120    avg_loss:0.370, val_acc:0.921]
Epoch [68/120    avg_loss:0.357, val_acc:0.948]
Epoch [69/120    avg_loss:0.336, val_acc:0.948]
Epoch [70/120    avg_loss:0.439, val_acc:0.946]
Epoch [71/120    avg_loss:0.391, val_acc:0.906]
Epoch [72/120    avg_loss:0.384, val_acc:0.942]
Epoch [73/120    avg_loss:0.387, val_acc:0.950]
Epoch [74/120    avg_loss:0.357, val_acc:0.954]
Epoch [75/120    avg_loss:0.320, val_acc:0.940]
Epoch [76/120    avg_loss:0.383, val_acc:0.923]
Epoch [77/120    avg_loss:0.372, val_acc:0.944]
Epoch [78/120    avg_loss:0.317, val_acc:0.952]
Epoch [79/120    avg_loss:0.290, val_acc:0.942]
Epoch [80/120    avg_loss:0.333, val_acc:0.956]
Epoch [81/120    avg_loss:0.316, val_acc:0.938]
Epoch [82/120    avg_loss:0.278, val_acc:0.919]
Epoch [83/120    avg_loss:0.256, val_acc:0.950]
Epoch [84/120    avg_loss:0.249, val_acc:0.956]
Epoch [85/120    avg_loss:0.217, val_acc:0.979]
Epoch [86/120    avg_loss:0.209, val_acc:0.948]
Epoch [87/120    avg_loss:0.205, val_acc:0.944]
Epoch [88/120    avg_loss:0.252, val_acc:0.971]
Epoch [89/120    avg_loss:0.245, val_acc:0.969]
Epoch [90/120    avg_loss:0.204, val_acc:0.975]
Epoch [91/120    avg_loss:0.189, val_acc:0.958]
Epoch [92/120    avg_loss:0.217, val_acc:0.960]
Epoch [93/120    avg_loss:0.201, val_acc:0.965]
Epoch [94/120    avg_loss:0.193, val_acc:0.973]
Epoch [95/120    avg_loss:0.208, val_acc:0.965]
Epoch [96/120    avg_loss:0.245, val_acc:0.969]
Epoch [97/120    avg_loss:0.224, val_acc:0.946]
Epoch [98/120    avg_loss:0.233, val_acc:0.965]
Epoch [99/120    avg_loss:0.180, val_acc:0.969]
Epoch [100/120    avg_loss:0.170, val_acc:0.975]
Epoch [101/120    avg_loss:0.149, val_acc:0.981]
Epoch [102/120    avg_loss:0.135, val_acc:0.979]
Epoch [103/120    avg_loss:0.146, val_acc:0.975]
Epoch [104/120    avg_loss:0.129, val_acc:0.983]
Epoch [105/120    avg_loss:0.138, val_acc:0.983]
Epoch [106/120    avg_loss:0.145, val_acc:0.983]
Epoch [107/120    avg_loss:0.135, val_acc:0.981]
Epoch [108/120    avg_loss:0.131, val_acc:0.981]
Epoch [109/120    avg_loss:0.127, val_acc:0.983]
Epoch [110/120    avg_loss:0.134, val_acc:0.983]
Epoch [111/120    avg_loss:0.116, val_acc:0.983]
Epoch [112/120    avg_loss:0.128, val_acc:0.985]
Epoch [113/120    avg_loss:0.126, val_acc:0.983]
Epoch [114/120    avg_loss:0.122, val_acc:0.983]
Epoch [115/120    avg_loss:0.116, val_acc:0.983]
Epoch [116/120    avg_loss:0.127, val_acc:0.983]
Epoch [117/120    avg_loss:0.128, val_acc:0.981]
Epoch [118/120    avg_loss:0.121, val_acc:0.983]
Epoch [119/120    avg_loss:0.121, val_acc:0.985]
Epoch [120/120    avg_loss:0.132, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 219   6   0   0   0   2   3   0   0   0   0]
 [  0   0   0   4 202  21   0   0   0   0   0   0   0   0]
 [  0   0   0   3  31 110   0   0   1   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   1   0   1   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.80383795309169

F1 scores:
[       nan 0.99927061 0.93932584 0.95842451 0.86695279 0.79710145
 1.         0.85082873 0.99356499 0.99680511 1.         0.99600533
 0.99669967 1.        ]

Kappa:
0.9755452740218186
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab8a433e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.110]
Epoch [2/120    avg_loss:2.614, val_acc:0.183]
Epoch [3/120    avg_loss:2.583, val_acc:0.204]
Epoch [4/120    avg_loss:2.558, val_acc:0.204]
Epoch [5/120    avg_loss:2.530, val_acc:0.275]
Epoch [6/120    avg_loss:2.497, val_acc:0.377]
Epoch [7/120    avg_loss:2.468, val_acc:0.385]
Epoch [8/120    avg_loss:2.449, val_acc:0.377]
Epoch [9/120    avg_loss:2.430, val_acc:0.379]
Epoch [10/120    avg_loss:2.399, val_acc:0.381]
Epoch [11/120    avg_loss:2.383, val_acc:0.400]
Epoch [12/120    avg_loss:2.349, val_acc:0.456]
Epoch [13/120    avg_loss:2.335, val_acc:0.490]
Epoch [14/120    avg_loss:2.301, val_acc:0.510]
Epoch [15/120    avg_loss:2.274, val_acc:0.494]
Epoch [16/120    avg_loss:2.250, val_acc:0.502]
Epoch [17/120    avg_loss:2.201, val_acc:0.517]
Epoch [18/120    avg_loss:2.177, val_acc:0.540]
Epoch [19/120    avg_loss:2.131, val_acc:0.552]
Epoch [20/120    avg_loss:2.090, val_acc:0.625]
Epoch [21/120    avg_loss:2.045, val_acc:0.652]
Epoch [22/120    avg_loss:2.008, val_acc:0.637]
Epoch [23/120    avg_loss:1.943, val_acc:0.671]
Epoch [24/120    avg_loss:1.880, val_acc:0.656]
Epoch [25/120    avg_loss:1.835, val_acc:0.606]
Epoch [26/120    avg_loss:1.791, val_acc:0.658]
Epoch [27/120    avg_loss:1.713, val_acc:0.623]
Epoch [28/120    avg_loss:1.647, val_acc:0.660]
Epoch [29/120    avg_loss:1.613, val_acc:0.631]
Epoch [30/120    avg_loss:1.537, val_acc:0.694]
Epoch [31/120    avg_loss:1.505, val_acc:0.725]
Epoch [32/120    avg_loss:1.440, val_acc:0.706]
Epoch [33/120    avg_loss:1.367, val_acc:0.762]
Epoch [34/120    avg_loss:1.321, val_acc:0.771]
Epoch [35/120    avg_loss:1.262, val_acc:0.856]
Epoch [36/120    avg_loss:1.185, val_acc:0.869]
Epoch [37/120    avg_loss:1.116, val_acc:0.873]
Epoch [38/120    avg_loss:1.066, val_acc:0.879]
Epoch [39/120    avg_loss:1.000, val_acc:0.896]
Epoch [40/120    avg_loss:0.984, val_acc:0.904]
Epoch [41/120    avg_loss:0.917, val_acc:0.912]
Epoch [42/120    avg_loss:0.879, val_acc:0.900]
Epoch [43/120    avg_loss:0.823, val_acc:0.896]
Epoch [44/120    avg_loss:0.789, val_acc:0.925]
Epoch [45/120    avg_loss:0.747, val_acc:0.925]
Epoch [46/120    avg_loss:0.687, val_acc:0.910]
Epoch [47/120    avg_loss:0.663, val_acc:0.900]
Epoch [48/120    avg_loss:0.647, val_acc:0.892]
Epoch [49/120    avg_loss:0.641, val_acc:0.908]
Epoch [50/120    avg_loss:0.609, val_acc:0.908]
Epoch [51/120    avg_loss:0.597, val_acc:0.904]
Epoch [52/120    avg_loss:0.543, val_acc:0.921]
Epoch [53/120    avg_loss:0.552, val_acc:0.917]
Epoch [54/120    avg_loss:0.497, val_acc:0.927]
Epoch [55/120    avg_loss:0.486, val_acc:0.927]
Epoch [56/120    avg_loss:0.474, val_acc:0.923]
Epoch [57/120    avg_loss:0.465, val_acc:0.919]
Epoch [58/120    avg_loss:0.454, val_acc:0.931]
Epoch [59/120    avg_loss:0.403, val_acc:0.921]
Epoch [60/120    avg_loss:0.432, val_acc:0.917]
Epoch [61/120    avg_loss:0.440, val_acc:0.904]
Epoch [62/120    avg_loss:0.458, val_acc:0.910]
Epoch [63/120    avg_loss:0.430, val_acc:0.931]
Epoch [64/120    avg_loss:0.394, val_acc:0.919]
Epoch [65/120    avg_loss:0.408, val_acc:0.910]
Epoch [66/120    avg_loss:0.363, val_acc:0.929]
Epoch [67/120    avg_loss:0.318, val_acc:0.942]
Epoch [68/120    avg_loss:0.350, val_acc:0.942]
Epoch [69/120    avg_loss:0.304, val_acc:0.915]
Epoch [70/120    avg_loss:0.337, val_acc:0.944]
Epoch [71/120    avg_loss:0.284, val_acc:0.944]
Epoch [72/120    avg_loss:0.284, val_acc:0.946]
Epoch [73/120    avg_loss:0.297, val_acc:0.946]
Epoch [74/120    avg_loss:0.262, val_acc:0.956]
Epoch [75/120    avg_loss:0.267, val_acc:0.935]
Epoch [76/120    avg_loss:0.282, val_acc:0.935]
Epoch [77/120    avg_loss:0.245, val_acc:0.948]
Epoch [78/120    avg_loss:0.277, val_acc:0.954]
Epoch [79/120    avg_loss:0.240, val_acc:0.933]
Epoch [80/120    avg_loss:0.347, val_acc:0.946]
Epoch [81/120    avg_loss:0.278, val_acc:0.931]
Epoch [82/120    avg_loss:0.227, val_acc:0.950]
Epoch [83/120    avg_loss:0.235, val_acc:0.971]
Epoch [84/120    avg_loss:0.205, val_acc:0.942]
Epoch [85/120    avg_loss:0.245, val_acc:0.954]
Epoch [86/120    avg_loss:0.225, val_acc:0.935]
Epoch [87/120    avg_loss:0.222, val_acc:0.946]
Epoch [88/120    avg_loss:0.190, val_acc:0.954]
Epoch [89/120    avg_loss:0.194, val_acc:0.960]
Epoch [90/120    avg_loss:0.165, val_acc:0.956]
Epoch [91/120    avg_loss:0.182, val_acc:0.965]
Epoch [92/120    avg_loss:0.162, val_acc:0.950]
Epoch [93/120    avg_loss:0.216, val_acc:0.944]
Epoch [94/120    avg_loss:0.163, val_acc:0.971]
Epoch [95/120    avg_loss:0.157, val_acc:0.950]
Epoch [96/120    avg_loss:0.152, val_acc:0.967]
Epoch [97/120    avg_loss:0.154, val_acc:0.969]
Epoch [98/120    avg_loss:0.134, val_acc:0.956]
Epoch [99/120    avg_loss:0.161, val_acc:0.965]
Epoch [100/120    avg_loss:0.124, val_acc:0.969]
Epoch [101/120    avg_loss:0.154, val_acc:0.963]
Epoch [102/120    avg_loss:0.150, val_acc:0.958]
Epoch [103/120    avg_loss:0.110, val_acc:0.967]
Epoch [104/120    avg_loss:0.127, val_acc:0.952]
Epoch [105/120    avg_loss:0.164, val_acc:0.967]
Epoch [106/120    avg_loss:0.153, val_acc:0.960]
Epoch [107/120    avg_loss:0.153, val_acc:0.958]
Epoch [108/120    avg_loss:0.106, val_acc:0.965]
Epoch [109/120    avg_loss:0.099, val_acc:0.975]
Epoch [110/120    avg_loss:0.121, val_acc:0.973]
Epoch [111/120    avg_loss:0.108, val_acc:0.973]
Epoch [112/120    avg_loss:0.114, val_acc:0.977]
Epoch [113/120    avg_loss:0.098, val_acc:0.975]
Epoch [114/120    avg_loss:0.095, val_acc:0.973]
Epoch [115/120    avg_loss:0.082, val_acc:0.973]
Epoch [116/120    avg_loss:0.097, val_acc:0.973]
Epoch [117/120    avg_loss:0.090, val_acc:0.973]
Epoch [118/120    avg_loss:0.106, val_acc:0.973]
Epoch [119/120    avg_loss:0.107, val_acc:0.975]
Epoch [120/120    avg_loss:0.085, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 221   8   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 210  16   0   0   1   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  24   0   0   0   0  70   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.35820895522387

F1 scores:
[       nan 1.         0.92239468 0.98004435 0.91106291 0.88965517
 1.         0.8        0.998713   0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9817197774968958
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f288ab66e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.269]
Epoch [2/120    avg_loss:2.607, val_acc:0.281]
Epoch [3/120    avg_loss:2.580, val_acc:0.356]
Epoch [4/120    avg_loss:2.554, val_acc:0.377]
Epoch [5/120    avg_loss:2.525, val_acc:0.383]
Epoch [6/120    avg_loss:2.495, val_acc:0.385]
Epoch [7/120    avg_loss:2.467, val_acc:0.383]
Epoch [8/120    avg_loss:2.434, val_acc:0.394]
Epoch [9/120    avg_loss:2.412, val_acc:0.410]
Epoch [10/120    avg_loss:2.388, val_acc:0.425]
Epoch [11/120    avg_loss:2.352, val_acc:0.454]
Epoch [12/120    avg_loss:2.324, val_acc:0.454]
Epoch [13/120    avg_loss:2.283, val_acc:0.460]
Epoch [14/120    avg_loss:2.261, val_acc:0.442]
Epoch [15/120    avg_loss:2.222, val_acc:0.438]
Epoch [16/120    avg_loss:2.183, val_acc:0.431]
Epoch [17/120    avg_loss:2.136, val_acc:0.481]
Epoch [18/120    avg_loss:2.106, val_acc:0.512]
Epoch [19/120    avg_loss:2.051, val_acc:0.535]
Epoch [20/120    avg_loss:1.995, val_acc:0.531]
Epoch [21/120    avg_loss:1.949, val_acc:0.573]
Epoch [22/120    avg_loss:1.912, val_acc:0.552]
Epoch [23/120    avg_loss:1.857, val_acc:0.575]
Epoch [24/120    avg_loss:1.813, val_acc:0.667]
Epoch [25/120    avg_loss:1.769, val_acc:0.635]
Epoch [26/120    avg_loss:1.713, val_acc:0.648]
Epoch [27/120    avg_loss:1.667, val_acc:0.654]
Epoch [28/120    avg_loss:1.602, val_acc:0.769]
Epoch [29/120    avg_loss:1.569, val_acc:0.719]
Epoch [30/120    avg_loss:1.502, val_acc:0.796]
Epoch [31/120    avg_loss:1.437, val_acc:0.796]
Epoch [32/120    avg_loss:1.377, val_acc:0.829]
Epoch [33/120    avg_loss:1.307, val_acc:0.846]
Epoch [34/120    avg_loss:1.286, val_acc:0.787]
Epoch [35/120    avg_loss:1.231, val_acc:0.863]
Epoch [36/120    avg_loss:1.155, val_acc:0.838]
Epoch [37/120    avg_loss:1.123, val_acc:0.858]
Epoch [38/120    avg_loss:1.061, val_acc:0.881]
Epoch [39/120    avg_loss:1.059, val_acc:0.867]
Epoch [40/120    avg_loss:1.008, val_acc:0.812]
Epoch [41/120    avg_loss:0.957, val_acc:0.885]
Epoch [42/120    avg_loss:0.924, val_acc:0.890]
Epoch [43/120    avg_loss:0.851, val_acc:0.896]
Epoch [44/120    avg_loss:0.830, val_acc:0.896]
Epoch [45/120    avg_loss:0.776, val_acc:0.906]
Epoch [46/120    avg_loss:0.753, val_acc:0.902]
Epoch [47/120    avg_loss:0.751, val_acc:0.912]
Epoch [48/120    avg_loss:0.682, val_acc:0.912]
Epoch [49/120    avg_loss:0.660, val_acc:0.890]
Epoch [50/120    avg_loss:0.626, val_acc:0.917]
Epoch [51/120    avg_loss:0.593, val_acc:0.896]
Epoch [52/120    avg_loss:0.569, val_acc:0.929]
Epoch [53/120    avg_loss:0.524, val_acc:0.929]
Epoch [54/120    avg_loss:0.512, val_acc:0.912]
Epoch [55/120    avg_loss:0.561, val_acc:0.910]
Epoch [56/120    avg_loss:0.511, val_acc:0.925]
Epoch [57/120    avg_loss:0.479, val_acc:0.915]
Epoch [58/120    avg_loss:0.484, val_acc:0.923]
Epoch [59/120    avg_loss:0.459, val_acc:0.910]
Epoch [60/120    avg_loss:0.413, val_acc:0.929]
Epoch [61/120    avg_loss:0.366, val_acc:0.940]
Epoch [62/120    avg_loss:0.383, val_acc:0.923]
Epoch [63/120    avg_loss:0.426, val_acc:0.892]
Epoch [64/120    avg_loss:0.420, val_acc:0.931]
Epoch [65/120    avg_loss:0.381, val_acc:0.946]
Epoch [66/120    avg_loss:0.416, val_acc:0.919]
Epoch [67/120    avg_loss:0.374, val_acc:0.927]
Epoch [68/120    avg_loss:0.440, val_acc:0.950]
Epoch [69/120    avg_loss:0.466, val_acc:0.935]
Epoch [70/120    avg_loss:0.314, val_acc:0.946]
Epoch [71/120    avg_loss:0.310, val_acc:0.938]
Epoch [72/120    avg_loss:0.329, val_acc:0.952]
Epoch [73/120    avg_loss:0.318, val_acc:0.938]
Epoch [74/120    avg_loss:0.314, val_acc:0.938]
Epoch [75/120    avg_loss:0.275, val_acc:0.946]
Epoch [76/120    avg_loss:0.277, val_acc:0.942]
Epoch [77/120    avg_loss:0.240, val_acc:0.958]
Epoch [78/120    avg_loss:0.262, val_acc:0.931]
Epoch [79/120    avg_loss:0.245, val_acc:0.958]
Epoch [80/120    avg_loss:0.255, val_acc:0.940]
Epoch [81/120    avg_loss:0.231, val_acc:0.944]
Epoch [82/120    avg_loss:0.220, val_acc:0.960]
Epoch [83/120    avg_loss:0.217, val_acc:0.952]
Epoch [84/120    avg_loss:0.190, val_acc:0.960]
Epoch [85/120    avg_loss:0.200, val_acc:0.960]
Epoch [86/120    avg_loss:0.186, val_acc:0.965]
Epoch [87/120    avg_loss:0.204, val_acc:0.950]
Epoch [88/120    avg_loss:0.211, val_acc:0.960]
Epoch [89/120    avg_loss:0.221, val_acc:0.952]
Epoch [90/120    avg_loss:0.188, val_acc:0.963]
Epoch [91/120    avg_loss:0.243, val_acc:0.960]
Epoch [92/120    avg_loss:0.196, val_acc:0.948]
Epoch [93/120    avg_loss:0.209, val_acc:0.965]
Epoch [94/120    avg_loss:0.161, val_acc:0.971]
Epoch [95/120    avg_loss:0.143, val_acc:0.952]
Epoch [96/120    avg_loss:0.206, val_acc:0.954]
Epoch [97/120    avg_loss:0.162, val_acc:0.977]
Epoch [98/120    avg_loss:0.151, val_acc:0.969]
Epoch [99/120    avg_loss:0.155, val_acc:0.975]
Epoch [100/120    avg_loss:0.147, val_acc:0.960]
Epoch [101/120    avg_loss:0.188, val_acc:0.965]
Epoch [102/120    avg_loss:0.204, val_acc:0.958]
Epoch [103/120    avg_loss:0.177, val_acc:0.938]
Epoch [104/120    avg_loss:0.180, val_acc:0.965]
Epoch [105/120    avg_loss:0.133, val_acc:0.965]
Epoch [106/120    avg_loss:0.119, val_acc:0.973]
Epoch [107/120    avg_loss:0.127, val_acc:0.958]
Epoch [108/120    avg_loss:0.118, val_acc:0.981]
Epoch [109/120    avg_loss:0.114, val_acc:0.975]
Epoch [110/120    avg_loss:0.101, val_acc:0.985]
Epoch [111/120    avg_loss:0.111, val_acc:0.979]
Epoch [112/120    avg_loss:0.125, val_acc:0.973]
Epoch [113/120    avg_loss:0.131, val_acc:0.971]
Epoch [114/120    avg_loss:0.092, val_acc:0.977]
Epoch [115/120    avg_loss:0.143, val_acc:0.981]
Epoch [116/120    avg_loss:0.122, val_acc:0.977]
Epoch [117/120    avg_loss:0.088, val_acc:0.973]
Epoch [118/120    avg_loss:0.133, val_acc:0.977]
Epoch [119/120    avg_loss:0.101, val_acc:0.975]
Epoch [120/120    avg_loss:0.097, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 193   0   2   0   0  24   0   0   0   0   0   0]
 [  0   0   1 220   8   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  32 113   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   3   0   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.08102345415779

F1 scores:
[       nan 0.99708879 0.91037736 0.97130243 0.90650407 0.86259542
 0.99019608 0.82587065 1.         0.99893276 1.         1.
 0.99667774 1.        ]

Kappa:
0.9786333579902661
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2d8f0bda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.093]
Epoch [2/120    avg_loss:2.586, val_acc:0.085]
Epoch [3/120    avg_loss:2.552, val_acc:0.085]
Epoch [4/120    avg_loss:2.520, val_acc:0.087]
Epoch [5/120    avg_loss:2.491, val_acc:0.206]
Epoch [6/120    avg_loss:2.447, val_acc:0.329]
Epoch [7/120    avg_loss:2.417, val_acc:0.344]
Epoch [8/120    avg_loss:2.377, val_acc:0.348]
Epoch [9/120    avg_loss:2.351, val_acc:0.379]
Epoch [10/120    avg_loss:2.314, val_acc:0.392]
Epoch [11/120    avg_loss:2.272, val_acc:0.402]
Epoch [12/120    avg_loss:2.232, val_acc:0.410]
Epoch [13/120    avg_loss:2.190, val_acc:0.412]
Epoch [14/120    avg_loss:2.156, val_acc:0.419]
Epoch [15/120    avg_loss:2.125, val_acc:0.419]
Epoch [16/120    avg_loss:2.102, val_acc:0.419]
Epoch [17/120    avg_loss:2.038, val_acc:0.425]
Epoch [18/120    avg_loss:2.009, val_acc:0.444]
Epoch [19/120    avg_loss:1.958, val_acc:0.467]
Epoch [20/120    avg_loss:1.904, val_acc:0.519]
Epoch [21/120    avg_loss:1.892, val_acc:0.542]
Epoch [22/120    avg_loss:1.834, val_acc:0.544]
Epoch [23/120    avg_loss:1.788, val_acc:0.565]
Epoch [24/120    avg_loss:1.744, val_acc:0.658]
Epoch [25/120    avg_loss:1.698, val_acc:0.635]
Epoch [26/120    avg_loss:1.662, val_acc:0.688]
Epoch [27/120    avg_loss:1.569, val_acc:0.706]
Epoch [28/120    avg_loss:1.517, val_acc:0.708]
Epoch [29/120    avg_loss:1.474, val_acc:0.706]
Epoch [30/120    avg_loss:1.404, val_acc:0.704]
Epoch [31/120    avg_loss:1.343, val_acc:0.727]
Epoch [32/120    avg_loss:1.282, val_acc:0.706]
Epoch [33/120    avg_loss:1.236, val_acc:0.777]
Epoch [34/120    avg_loss:1.151, val_acc:0.838]
Epoch [35/120    avg_loss:1.105, val_acc:0.850]
Epoch [36/120    avg_loss:1.043, val_acc:0.794]
Epoch [37/120    avg_loss:0.971, val_acc:0.812]
Epoch [38/120    avg_loss:0.961, val_acc:0.792]
Epoch [39/120    avg_loss:0.923, val_acc:0.840]
Epoch [40/120    avg_loss:0.864, val_acc:0.863]
Epoch [41/120    avg_loss:0.829, val_acc:0.865]
Epoch [42/120    avg_loss:0.762, val_acc:0.804]
Epoch [43/120    avg_loss:0.732, val_acc:0.906]
Epoch [44/120    avg_loss:0.676, val_acc:0.844]
Epoch [45/120    avg_loss:0.669, val_acc:0.873]
Epoch [46/120    avg_loss:0.642, val_acc:0.854]
Epoch [47/120    avg_loss:0.647, val_acc:0.875]
Epoch [48/120    avg_loss:0.605, val_acc:0.896]
Epoch [49/120    avg_loss:0.565, val_acc:0.906]
Epoch [50/120    avg_loss:0.530, val_acc:0.896]
Epoch [51/120    avg_loss:0.593, val_acc:0.871]
Epoch [52/120    avg_loss:0.583, val_acc:0.846]
Epoch [53/120    avg_loss:0.536, val_acc:0.940]
Epoch [54/120    avg_loss:0.511, val_acc:0.915]
Epoch [55/120    avg_loss:0.496, val_acc:0.912]
Epoch [56/120    avg_loss:0.454, val_acc:0.927]
Epoch [57/120    avg_loss:0.454, val_acc:0.935]
Epoch [58/120    avg_loss:0.408, val_acc:0.881]
Epoch [59/120    avg_loss:0.423, val_acc:0.933]
Epoch [60/120    avg_loss:0.367, val_acc:0.952]
Epoch [61/120    avg_loss:0.345, val_acc:0.960]
Epoch [62/120    avg_loss:0.329, val_acc:0.954]
Epoch [63/120    avg_loss:0.305, val_acc:0.950]
Epoch [64/120    avg_loss:0.307, val_acc:0.958]
Epoch [65/120    avg_loss:0.290, val_acc:0.958]
Epoch [66/120    avg_loss:0.287, val_acc:0.965]
Epoch [67/120    avg_loss:0.260, val_acc:0.948]
Epoch [68/120    avg_loss:0.293, val_acc:0.927]
Epoch [69/120    avg_loss:0.286, val_acc:0.942]
Epoch [70/120    avg_loss:0.274, val_acc:0.944]
Epoch [71/120    avg_loss:0.249, val_acc:0.963]
Epoch [72/120    avg_loss:0.260, val_acc:0.963]
Epoch [73/120    avg_loss:0.301, val_acc:0.938]
Epoch [74/120    avg_loss:0.361, val_acc:0.929]
Epoch [75/120    avg_loss:0.304, val_acc:0.960]
Epoch [76/120    avg_loss:0.309, val_acc:0.952]
Epoch [77/120    avg_loss:0.260, val_acc:0.963]
Epoch [78/120    avg_loss:0.279, val_acc:0.935]
Epoch [79/120    avg_loss:0.229, val_acc:0.952]
Epoch [80/120    avg_loss:0.220, val_acc:0.960]
Epoch [81/120    avg_loss:0.180, val_acc:0.969]
Epoch [82/120    avg_loss:0.193, val_acc:0.973]
Epoch [83/120    avg_loss:0.184, val_acc:0.973]
Epoch [84/120    avg_loss:0.181, val_acc:0.977]
Epoch [85/120    avg_loss:0.165, val_acc:0.969]
Epoch [86/120    avg_loss:0.170, val_acc:0.969]
Epoch [87/120    avg_loss:0.165, val_acc:0.975]
Epoch [88/120    avg_loss:0.174, val_acc:0.977]
Epoch [89/120    avg_loss:0.174, val_acc:0.977]
Epoch [90/120    avg_loss:0.179, val_acc:0.977]
Epoch [91/120    avg_loss:0.163, val_acc:0.975]
Epoch [92/120    avg_loss:0.160, val_acc:0.975]
Epoch [93/120    avg_loss:0.162, val_acc:0.973]
Epoch [94/120    avg_loss:0.155, val_acc:0.975]
Epoch [95/120    avg_loss:0.160, val_acc:0.969]
Epoch [96/120    avg_loss:0.157, val_acc:0.977]
Epoch [97/120    avg_loss:0.164, val_acc:0.975]
Epoch [98/120    avg_loss:0.148, val_acc:0.979]
Epoch [99/120    avg_loss:0.150, val_acc:0.981]
Epoch [100/120    avg_loss:0.142, val_acc:0.979]
Epoch [101/120    avg_loss:0.145, val_acc:0.979]
Epoch [102/120    avg_loss:0.151, val_acc:0.983]
Epoch [103/120    avg_loss:0.132, val_acc:0.973]
Epoch [104/120    avg_loss:0.141, val_acc:0.973]
Epoch [105/120    avg_loss:0.150, val_acc:0.979]
Epoch [106/120    avg_loss:0.153, val_acc:0.979]
Epoch [107/120    avg_loss:0.151, val_acc:0.983]
Epoch [108/120    avg_loss:0.133, val_acc:0.985]
Epoch [109/120    avg_loss:0.135, val_acc:0.979]
Epoch [110/120    avg_loss:0.144, val_acc:0.981]
Epoch [111/120    avg_loss:0.142, val_acc:0.985]
Epoch [112/120    avg_loss:0.141, val_acc:0.983]
Epoch [113/120    avg_loss:0.135, val_acc:0.983]
Epoch [114/120    avg_loss:0.140, val_acc:0.983]
Epoch [115/120    avg_loss:0.144, val_acc:0.975]
Epoch [116/120    avg_loss:0.137, val_acc:0.983]
Epoch [117/120    avg_loss:0.128, val_acc:0.981]
Epoch [118/120    avg_loss:0.140, val_acc:0.985]
Epoch [119/120    avg_loss:0.131, val_acc:0.985]
Epoch [120/120    avg_loss:0.145, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 198   0   0   0   0  21   0   0   0   0   0   0]
 [  0   0   0 223   5   0   0   0   2   0   0   0   0   0]
 [  0   0   0   1 196  30   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   6   0   0   0 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.2089552238806

F1 scores:
[       nan 1.         0.92307692 0.98237885 0.90114943 0.88178914
 1.         0.83248731 0.98979592 1.         1.         1.
 0.99333333 1.        ]

Kappa:
0.9800630829641636
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f603fe3fdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.615, val_acc:0.083]
Epoch [2/120    avg_loss:2.578, val_acc:0.246]
Epoch [3/120    avg_loss:2.542, val_acc:0.271]
Epoch [4/120    avg_loss:2.516, val_acc:0.279]
Epoch [5/120    avg_loss:2.485, val_acc:0.279]
Epoch [6/120    avg_loss:2.461, val_acc:0.300]
Epoch [7/120    avg_loss:2.434, val_acc:0.321]
Epoch [8/120    avg_loss:2.406, val_acc:0.375]
Epoch [9/120    avg_loss:2.376, val_acc:0.406]
Epoch [10/120    avg_loss:2.357, val_acc:0.421]
Epoch [11/120    avg_loss:2.332, val_acc:0.444]
Epoch [12/120    avg_loss:2.301, val_acc:0.469]
Epoch [13/120    avg_loss:2.273, val_acc:0.485]
Epoch [14/120    avg_loss:2.248, val_acc:0.504]
Epoch [15/120    avg_loss:2.212, val_acc:0.519]
Epoch [16/120    avg_loss:2.170, val_acc:0.550]
Epoch [17/120    avg_loss:2.133, val_acc:0.592]
Epoch [18/120    avg_loss:2.089, val_acc:0.596]
Epoch [19/120    avg_loss:2.050, val_acc:0.619]
Epoch [20/120    avg_loss:2.014, val_acc:0.640]
Epoch [21/120    avg_loss:1.955, val_acc:0.640]
Epoch [22/120    avg_loss:1.908, val_acc:0.677]
Epoch [23/120    avg_loss:1.852, val_acc:0.650]
Epoch [24/120    avg_loss:1.807, val_acc:0.696]
Epoch [25/120    avg_loss:1.741, val_acc:0.679]
Epoch [26/120    avg_loss:1.690, val_acc:0.654]
Epoch [27/120    avg_loss:1.655, val_acc:0.698]
Epoch [28/120    avg_loss:1.603, val_acc:0.694]
Epoch [29/120    avg_loss:1.547, val_acc:0.685]
Epoch [30/120    avg_loss:1.468, val_acc:0.717]
Epoch [31/120    avg_loss:1.425, val_acc:0.733]
Epoch [32/120    avg_loss:1.360, val_acc:0.731]
Epoch [33/120    avg_loss:1.319, val_acc:0.769]
Epoch [34/120    avg_loss:1.294, val_acc:0.762]
Epoch [35/120    avg_loss:1.196, val_acc:0.738]
Epoch [36/120    avg_loss:1.130, val_acc:0.771]
Epoch [37/120    avg_loss:1.102, val_acc:0.767]
Epoch [38/120    avg_loss:1.027, val_acc:0.777]
Epoch [39/120    avg_loss:0.994, val_acc:0.796]
Epoch [40/120    avg_loss:0.910, val_acc:0.860]
Epoch [41/120    avg_loss:0.886, val_acc:0.771]
Epoch [42/120    avg_loss:0.855, val_acc:0.846]
Epoch [43/120    avg_loss:0.811, val_acc:0.873]
Epoch [44/120    avg_loss:0.751, val_acc:0.846]
Epoch [45/120    avg_loss:0.712, val_acc:0.883]
Epoch [46/120    avg_loss:0.690, val_acc:0.925]
Epoch [47/120    avg_loss:0.656, val_acc:0.927]
Epoch [48/120    avg_loss:0.615, val_acc:0.854]
Epoch [49/120    avg_loss:0.626, val_acc:0.925]
Epoch [50/120    avg_loss:0.598, val_acc:0.933]
Epoch [51/120    avg_loss:0.584, val_acc:0.931]
Epoch [52/120    avg_loss:0.541, val_acc:0.877]
Epoch [53/120    avg_loss:0.539, val_acc:0.925]
Epoch [54/120    avg_loss:0.542, val_acc:0.942]
Epoch [55/120    avg_loss:0.510, val_acc:0.927]
Epoch [56/120    avg_loss:0.488, val_acc:0.896]
Epoch [57/120    avg_loss:0.527, val_acc:0.923]
Epoch [58/120    avg_loss:0.466, val_acc:0.935]
Epoch [59/120    avg_loss:0.460, val_acc:0.948]
Epoch [60/120    avg_loss:0.456, val_acc:0.942]
Epoch [61/120    avg_loss:0.455, val_acc:0.833]
Epoch [62/120    avg_loss:0.395, val_acc:0.946]
Epoch [63/120    avg_loss:0.338, val_acc:0.954]
Epoch [64/120    avg_loss:0.367, val_acc:0.944]
Epoch [65/120    avg_loss:0.340, val_acc:0.942]
Epoch [66/120    avg_loss:0.402, val_acc:0.956]
Epoch [67/120    avg_loss:0.405, val_acc:0.960]
Epoch [68/120    avg_loss:0.346, val_acc:0.960]
Epoch [69/120    avg_loss:0.319, val_acc:0.971]
Epoch [70/120    avg_loss:0.292, val_acc:0.971]
Epoch [71/120    avg_loss:0.266, val_acc:0.956]
Epoch [72/120    avg_loss:0.248, val_acc:0.965]
Epoch [73/120    avg_loss:0.227, val_acc:0.950]
Epoch [74/120    avg_loss:0.286, val_acc:0.965]
Epoch [75/120    avg_loss:0.253, val_acc:0.969]
Epoch [76/120    avg_loss:0.236, val_acc:0.963]
Epoch [77/120    avg_loss:0.184, val_acc:0.971]
Epoch [78/120    avg_loss:0.211, val_acc:0.965]
Epoch [79/120    avg_loss:0.196, val_acc:0.973]
Epoch [80/120    avg_loss:0.186, val_acc:0.977]
Epoch [81/120    avg_loss:0.290, val_acc:0.956]
Epoch [82/120    avg_loss:0.254, val_acc:0.971]
Epoch [83/120    avg_loss:0.211, val_acc:0.971]
Epoch [84/120    avg_loss:0.194, val_acc:0.965]
Epoch [85/120    avg_loss:0.190, val_acc:0.952]
Epoch [86/120    avg_loss:0.194, val_acc:0.958]
Epoch [87/120    avg_loss:0.202, val_acc:0.963]
Epoch [88/120    avg_loss:0.164, val_acc:0.973]
Epoch [89/120    avg_loss:0.179, val_acc:0.977]
Epoch [90/120    avg_loss:0.158, val_acc:0.956]
Epoch [91/120    avg_loss:0.255, val_acc:0.954]
Epoch [92/120    avg_loss:0.197, val_acc:0.963]
Epoch [93/120    avg_loss:0.178, val_acc:0.965]
Epoch [94/120    avg_loss:0.223, val_acc:0.960]
Epoch [95/120    avg_loss:0.176, val_acc:0.967]
Epoch [96/120    avg_loss:0.162, val_acc:0.969]
Epoch [97/120    avg_loss:0.142, val_acc:0.981]
Epoch [98/120    avg_loss:0.141, val_acc:0.971]
Epoch [99/120    avg_loss:0.118, val_acc:0.983]
Epoch [100/120    avg_loss:0.110, val_acc:0.975]
Epoch [101/120    avg_loss:0.135, val_acc:0.956]
Epoch [102/120    avg_loss:0.160, val_acc:0.975]
Epoch [103/120    avg_loss:0.129, val_acc:0.973]
Epoch [104/120    avg_loss:0.126, val_acc:0.983]
Epoch [105/120    avg_loss:0.112, val_acc:0.973]
Epoch [106/120    avg_loss:0.130, val_acc:0.973]
Epoch [107/120    avg_loss:0.155, val_acc:0.973]
Epoch [108/120    avg_loss:0.128, val_acc:0.985]
Epoch [109/120    avg_loss:0.120, val_acc:0.981]
Epoch [110/120    avg_loss:0.106, val_acc:0.975]
Epoch [111/120    avg_loss:0.136, val_acc:0.971]
Epoch [112/120    avg_loss:0.146, val_acc:0.977]
Epoch [113/120    avg_loss:0.102, val_acc:0.981]
Epoch [114/120    avg_loss:0.086, val_acc:0.977]
Epoch [115/120    avg_loss:0.088, val_acc:0.979]
Epoch [116/120    avg_loss:0.098, val_acc:0.977]
Epoch [117/120    avg_loss:0.084, val_acc:0.988]
Epoch [118/120    avg_loss:0.082, val_acc:0.981]
Epoch [119/120    avg_loss:0.074, val_acc:0.979]
Epoch [120/120    avg_loss:0.076, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   4   0   0   0   0   1   0]
 [  0   0   0 203  13   0   0   0  14   0   0   0   0   0]
 [  0   0   0   0 186  41   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   7   0   0   0   0 199   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.78251599147121

F1 scores:
[       nan 0.99491649 0.9532294  0.93764434 0.85714286 0.84829721
 0.98271605 0.88636364 0.98227848 1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9753066369336489
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0956bf7dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 26282==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.606, val_acc:0.117]
Epoch [2/120    avg_loss:2.580, val_acc:0.190]
Epoch [3/120    avg_loss:2.556, val_acc:0.323]
Epoch [4/120    avg_loss:2.523, val_acc:0.315]
Epoch [5/120    avg_loss:2.490, val_acc:0.329]
Epoch [6/120    avg_loss:2.455, val_acc:0.344]
Epoch [7/120    avg_loss:2.427, val_acc:0.373]
Epoch [8/120    avg_loss:2.397, val_acc:0.350]
Epoch [9/120    avg_loss:2.365, val_acc:0.367]
Epoch [10/120    avg_loss:2.328, val_acc:0.362]
Epoch [11/120    avg_loss:2.300, val_acc:0.362]
Epoch [12/120    avg_loss:2.263, val_acc:0.352]
Epoch [13/120    avg_loss:2.226, val_acc:0.362]
Epoch [14/120    avg_loss:2.183, val_acc:0.367]
Epoch [15/120    avg_loss:2.145, val_acc:0.367]
Epoch [16/120    avg_loss:2.105, val_acc:0.377]
Epoch [17/120    avg_loss:2.087, val_acc:0.423]
Epoch [18/120    avg_loss:2.024, val_acc:0.427]
Epoch [19/120    avg_loss:1.998, val_acc:0.471]
Epoch [20/120    avg_loss:1.979, val_acc:0.548]
Epoch [21/120    avg_loss:1.901, val_acc:0.577]
Epoch [22/120    avg_loss:1.848, val_acc:0.583]
Epoch [23/120    avg_loss:1.811, val_acc:0.623]
Epoch [24/120    avg_loss:1.751, val_acc:0.648]
Epoch [25/120    avg_loss:1.690, val_acc:0.665]
Epoch [26/120    avg_loss:1.620, val_acc:0.662]
Epoch [27/120    avg_loss:1.605, val_acc:0.692]
Epoch [28/120    avg_loss:1.514, val_acc:0.710]
Epoch [29/120    avg_loss:1.462, val_acc:0.767]
Epoch [30/120    avg_loss:1.400, val_acc:0.796]
Epoch [31/120    avg_loss:1.339, val_acc:0.825]
Epoch [32/120    avg_loss:1.264, val_acc:0.802]
Epoch [33/120    avg_loss:1.238, val_acc:0.802]
Epoch [34/120    avg_loss:1.202, val_acc:0.871]
Epoch [35/120    avg_loss:1.091, val_acc:0.871]
Epoch [36/120    avg_loss:1.039, val_acc:0.883]
Epoch [37/120    avg_loss:1.001, val_acc:0.877]
Epoch [38/120    avg_loss:0.951, val_acc:0.869]
Epoch [39/120    avg_loss:0.914, val_acc:0.873]
Epoch [40/120    avg_loss:0.865, val_acc:0.881]
Epoch [41/120    avg_loss:0.800, val_acc:0.877]
Epoch [42/120    avg_loss:0.771, val_acc:0.900]
Epoch [43/120    avg_loss:0.728, val_acc:0.915]
Epoch [44/120    avg_loss:0.682, val_acc:0.910]
Epoch [45/120    avg_loss:0.654, val_acc:0.908]
Epoch [46/120    avg_loss:0.679, val_acc:0.894]
Epoch [47/120    avg_loss:0.772, val_acc:0.915]
Epoch [48/120    avg_loss:0.636, val_acc:0.912]
Epoch [49/120    avg_loss:0.638, val_acc:0.923]
Epoch [50/120    avg_loss:0.544, val_acc:0.915]
Epoch [51/120    avg_loss:0.556, val_acc:0.879]
Epoch [52/120    avg_loss:0.500, val_acc:0.912]
Epoch [53/120    avg_loss:0.537, val_acc:0.925]
Epoch [54/120    avg_loss:0.472, val_acc:0.927]
Epoch [55/120    avg_loss:0.509, val_acc:0.925]
Epoch [56/120    avg_loss:0.448, val_acc:0.929]
Epoch [57/120    avg_loss:0.420, val_acc:0.923]
Epoch [58/120    avg_loss:0.450, val_acc:0.925]
Epoch [59/120    avg_loss:0.407, val_acc:0.890]
Epoch [60/120    avg_loss:0.388, val_acc:0.931]
Epoch [61/120    avg_loss:0.412, val_acc:0.938]
Epoch [62/120    avg_loss:0.391, val_acc:0.935]
Epoch [63/120    avg_loss:0.344, val_acc:0.935]
Epoch [64/120    avg_loss:0.366, val_acc:0.942]
Epoch [65/120    avg_loss:0.357, val_acc:0.942]
Epoch [66/120    avg_loss:0.335, val_acc:0.940]
Epoch [67/120    avg_loss:0.295, val_acc:0.923]
Epoch [68/120    avg_loss:0.344, val_acc:0.929]
Epoch [69/120    avg_loss:0.335, val_acc:0.946]
Epoch [70/120    avg_loss:0.293, val_acc:0.933]
Epoch [71/120    avg_loss:0.286, val_acc:0.950]
Epoch [72/120    avg_loss:0.259, val_acc:0.960]
Epoch [73/120    avg_loss:0.267, val_acc:0.952]
Epoch [74/120    avg_loss:0.256, val_acc:0.935]
Epoch [75/120    avg_loss:0.250, val_acc:0.948]
Epoch [76/120    avg_loss:0.253, val_acc:0.954]
Epoch [77/120    avg_loss:0.231, val_acc:0.948]
Epoch [78/120    avg_loss:0.287, val_acc:0.946]
Epoch [79/120    avg_loss:0.281, val_acc:0.948]
Epoch [80/120    avg_loss:0.283, val_acc:0.931]
Epoch [81/120    avg_loss:0.258, val_acc:0.956]
Epoch [82/120    avg_loss:0.222, val_acc:0.954]
Epoch [83/120    avg_loss:0.194, val_acc:0.960]
Epoch [84/120    avg_loss:0.182, val_acc:0.952]
Epoch [85/120    avg_loss:0.183, val_acc:0.960]
Epoch [86/120    avg_loss:0.210, val_acc:0.956]
Epoch [87/120    avg_loss:0.243, val_acc:0.952]
Epoch [88/120    avg_loss:0.231, val_acc:0.948]
Epoch [89/120    avg_loss:0.290, val_acc:0.923]
Epoch [90/120    avg_loss:0.332, val_acc:0.923]
Epoch [91/120    avg_loss:0.242, val_acc:0.956]
Epoch [92/120    avg_loss:0.183, val_acc:0.963]
Epoch [93/120    avg_loss:0.158, val_acc:0.958]
Epoch [94/120    avg_loss:0.190, val_acc:0.960]
Epoch [95/120    avg_loss:0.192, val_acc:0.929]
Epoch [96/120    avg_loss:0.220, val_acc:0.960]
Epoch [97/120    avg_loss:0.186, val_acc:0.952]
Epoch [98/120    avg_loss:0.189, val_acc:0.954]
Epoch [99/120    avg_loss:0.199, val_acc:0.954]
Epoch [100/120    avg_loss:0.175, val_acc:0.967]
Epoch [101/120    avg_loss:0.181, val_acc:0.960]
Epoch [102/120    avg_loss:0.150, val_acc:0.967]
Epoch [103/120    avg_loss:0.170, val_acc:0.967]
Epoch [104/120    avg_loss:0.142, val_acc:0.969]
Epoch [105/120    avg_loss:0.123, val_acc:0.956]
Epoch [106/120    avg_loss:0.111, val_acc:0.973]
Epoch [107/120    avg_loss:0.128, val_acc:0.969]
Epoch [108/120    avg_loss:0.141, val_acc:0.954]
Epoch [109/120    avg_loss:0.102, val_acc:0.969]
Epoch [110/120    avg_loss:0.104, val_acc:0.975]
Epoch [111/120    avg_loss:0.112, val_acc:0.963]
Epoch [112/120    avg_loss:0.103, val_acc:0.977]
Epoch [113/120    avg_loss:0.115, val_acc:0.965]
Epoch [114/120    avg_loss:0.110, val_acc:0.971]
Epoch [115/120    avg_loss:0.106, val_acc:0.967]
Epoch [116/120    avg_loss:0.098, val_acc:0.967]
Epoch [117/120    avg_loss:0.122, val_acc:0.977]
Epoch [118/120    avg_loss:0.125, val_acc:0.967]
Epoch [119/120    avg_loss:0.111, val_acc:0.971]
Epoch [120/120    avg_loss:0.148, val_acc:0.950]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   2 221   3   0   0   2   0   2   0   0   0   0]
 [  0   0   1   0 131  94   0   1   0   0   0   0   0   0]
 [  0   0   0   0   1 116   0  28   0   0   0   0   0   0]
 [  0   0   0   0   0   0 186  20   0   0   0   0   0   0]
 [  0   0  22   0   0   0   0  72   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0   0 372   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   2   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
95.73560767590618

F1 scores:
[       nan 1.         0.90063425 0.98004435 0.72375691 0.65352113
 0.94897959 0.64573991 0.97894737 0.9978678  0.99726027 1.
 0.99778761 1.        ]

Kappa:
0.9525528729437767
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e358cbe48>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.177]
Epoch [2/120    avg_loss:2.596, val_acc:0.167]
Epoch [3/120    avg_loss:2.570, val_acc:0.154]
Epoch [4/120    avg_loss:2.553, val_acc:0.229]
Epoch [5/120    avg_loss:2.521, val_acc:0.292]
Epoch [6/120    avg_loss:2.499, val_acc:0.298]
Epoch [7/120    avg_loss:2.472, val_acc:0.298]
Epoch [8/120    avg_loss:2.441, val_acc:0.298]
Epoch [9/120    avg_loss:2.425, val_acc:0.302]
Epoch [10/120    avg_loss:2.411, val_acc:0.302]
Epoch [11/120    avg_loss:2.381, val_acc:0.310]
Epoch [12/120    avg_loss:2.354, val_acc:0.331]
Epoch [13/120    avg_loss:2.356, val_acc:0.329]
Epoch [14/120    avg_loss:2.315, val_acc:0.338]
Epoch [15/120    avg_loss:2.303, val_acc:0.344]
Epoch [16/120    avg_loss:2.276, val_acc:0.365]
Epoch [17/120    avg_loss:2.253, val_acc:0.375]
Epoch [18/120    avg_loss:2.229, val_acc:0.394]
Epoch [19/120    avg_loss:2.195, val_acc:0.400]
Epoch [20/120    avg_loss:2.161, val_acc:0.417]
Epoch [21/120    avg_loss:2.119, val_acc:0.417]
Epoch [22/120    avg_loss:2.106, val_acc:0.427]
Epoch [23/120    avg_loss:2.057, val_acc:0.431]
Epoch [24/120    avg_loss:2.024, val_acc:0.442]
Epoch [25/120    avg_loss:2.008, val_acc:0.444]
Epoch [26/120    avg_loss:1.958, val_acc:0.442]
Epoch [27/120    avg_loss:1.972, val_acc:0.456]
Epoch [28/120    avg_loss:1.965, val_acc:0.465]
Epoch [29/120    avg_loss:1.926, val_acc:0.496]
Epoch [30/120    avg_loss:1.858, val_acc:0.500]
Epoch [31/120    avg_loss:1.846, val_acc:0.519]
Epoch [32/120    avg_loss:1.786, val_acc:0.540]
Epoch [33/120    avg_loss:1.733, val_acc:0.581]
Epoch [34/120    avg_loss:1.706, val_acc:0.573]
Epoch [35/120    avg_loss:1.651, val_acc:0.598]
Epoch [36/120    avg_loss:1.604, val_acc:0.658]
Epoch [37/120    avg_loss:1.581, val_acc:0.702]
Epoch [38/120    avg_loss:1.519, val_acc:0.667]
Epoch [39/120    avg_loss:1.470, val_acc:0.671]
Epoch [40/120    avg_loss:1.426, val_acc:0.704]
Epoch [41/120    avg_loss:1.363, val_acc:0.646]
Epoch [42/120    avg_loss:1.349, val_acc:0.723]
Epoch [43/120    avg_loss:1.236, val_acc:0.740]
Epoch [44/120    avg_loss:1.187, val_acc:0.748]
Epoch [45/120    avg_loss:1.141, val_acc:0.810]
Epoch [46/120    avg_loss:1.101, val_acc:0.790]
Epoch [47/120    avg_loss:1.026, val_acc:0.796]
Epoch [48/120    avg_loss:0.971, val_acc:0.823]
Epoch [49/120    avg_loss:0.886, val_acc:0.827]
Epoch [50/120    avg_loss:0.877, val_acc:0.829]
Epoch [51/120    avg_loss:0.828, val_acc:0.852]
Epoch [52/120    avg_loss:0.760, val_acc:0.904]
Epoch [53/120    avg_loss:0.735, val_acc:0.890]
Epoch [54/120    avg_loss:0.652, val_acc:0.877]
Epoch [55/120    avg_loss:0.665, val_acc:0.908]
Epoch [56/120    avg_loss:0.646, val_acc:0.919]
Epoch [57/120    avg_loss:0.555, val_acc:0.910]
Epoch [58/120    avg_loss:0.531, val_acc:0.931]
Epoch [59/120    avg_loss:0.530, val_acc:0.933]
Epoch [60/120    avg_loss:0.497, val_acc:0.954]
Epoch [61/120    avg_loss:0.476, val_acc:0.963]
Epoch [62/120    avg_loss:0.463, val_acc:0.900]
Epoch [63/120    avg_loss:0.427, val_acc:0.958]
Epoch [64/120    avg_loss:0.432, val_acc:0.973]
Epoch [65/120    avg_loss:0.400, val_acc:0.938]
Epoch [66/120    avg_loss:0.426, val_acc:0.950]
Epoch [67/120    avg_loss:0.416, val_acc:0.935]
Epoch [68/120    avg_loss:0.369, val_acc:0.950]
Epoch [69/120    avg_loss:0.388, val_acc:0.973]
Epoch [70/120    avg_loss:0.339, val_acc:0.944]
Epoch [71/120    avg_loss:0.346, val_acc:0.944]
Epoch [72/120    avg_loss:0.339, val_acc:0.927]
Epoch [73/120    avg_loss:0.312, val_acc:0.967]
Epoch [74/120    avg_loss:0.284, val_acc:0.971]
Epoch [75/120    avg_loss:0.236, val_acc:0.967]
Epoch [76/120    avg_loss:0.247, val_acc:0.963]
Epoch [77/120    avg_loss:0.234, val_acc:0.973]
Epoch [78/120    avg_loss:0.257, val_acc:0.938]
Epoch [79/120    avg_loss:0.282, val_acc:0.912]
Epoch [80/120    avg_loss:0.305, val_acc:0.931]
Epoch [81/120    avg_loss:0.309, val_acc:0.969]
Epoch [82/120    avg_loss:0.259, val_acc:0.975]
Epoch [83/120    avg_loss:0.203, val_acc:0.956]
Epoch [84/120    avg_loss:0.224, val_acc:0.954]
Epoch [85/120    avg_loss:0.251, val_acc:0.933]
Epoch [86/120    avg_loss:0.239, val_acc:0.956]
Epoch [87/120    avg_loss:0.249, val_acc:0.958]
Epoch [88/120    avg_loss:0.238, val_acc:0.971]
Epoch [89/120    avg_loss:0.194, val_acc:0.958]
Epoch [90/120    avg_loss:0.199, val_acc:0.954]
Epoch [91/120    avg_loss:0.193, val_acc:0.965]
Epoch [92/120    avg_loss:0.181, val_acc:0.952]
Epoch [93/120    avg_loss:0.167, val_acc:0.973]
Epoch [94/120    avg_loss:0.171, val_acc:0.973]
Epoch [95/120    avg_loss:0.208, val_acc:0.963]
Epoch [96/120    avg_loss:0.162, val_acc:0.973]
Epoch [97/120    avg_loss:0.160, val_acc:0.977]
Epoch [98/120    avg_loss:0.123, val_acc:0.977]
Epoch [99/120    avg_loss:0.139, val_acc:0.981]
Epoch [100/120    avg_loss:0.126, val_acc:0.981]
Epoch [101/120    avg_loss:0.134, val_acc:0.983]
Epoch [102/120    avg_loss:0.129, val_acc:0.981]
Epoch [103/120    avg_loss:0.121, val_acc:0.983]
Epoch [104/120    avg_loss:0.121, val_acc:0.983]
Epoch [105/120    avg_loss:0.121, val_acc:0.983]
Epoch [106/120    avg_loss:0.124, val_acc:0.983]
Epoch [107/120    avg_loss:0.121, val_acc:0.985]
Epoch [108/120    avg_loss:0.121, val_acc:0.988]
Epoch [109/120    avg_loss:0.129, val_acc:0.985]
Epoch [110/120    avg_loss:0.115, val_acc:0.983]
Epoch [111/120    avg_loss:0.114, val_acc:0.983]
Epoch [112/120    avg_loss:0.120, val_acc:0.981]
Epoch [113/120    avg_loss:0.113, val_acc:0.981]
Epoch [114/120    avg_loss:0.109, val_acc:0.983]
Epoch [115/120    avg_loss:0.108, val_acc:0.981]
Epoch [116/120    avg_loss:0.105, val_acc:0.985]
Epoch [117/120    avg_loss:0.102, val_acc:0.983]
Epoch [118/120    avg_loss:0.110, val_acc:0.988]
Epoch [119/120    avg_loss:0.119, val_acc:0.983]
Epoch [120/120    avg_loss:0.102, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   3 201  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   2   0   0   0   1   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.97333333 0.99134199 0.91571754 0.88741722
 0.99516908 0.92655367 1.         1.         0.99862826 1.
 0.99667774 1.        ]

Kappa:
0.9874189064323365
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f58745dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.015]
Epoch [2/120    avg_loss:2.604, val_acc:0.243]
Epoch [3/120    avg_loss:2.582, val_acc:0.302]
Epoch [4/120    avg_loss:2.556, val_acc:0.310]
Epoch [5/120    avg_loss:2.535, val_acc:0.319]
Epoch [6/120    avg_loss:2.511, val_acc:0.323]
Epoch [7/120    avg_loss:2.481, val_acc:0.325]
Epoch [8/120    avg_loss:2.456, val_acc:0.317]
Epoch [9/120    avg_loss:2.433, val_acc:0.325]
Epoch [10/120    avg_loss:2.410, val_acc:0.319]
Epoch [11/120    avg_loss:2.369, val_acc:0.329]
Epoch [12/120    avg_loss:2.350, val_acc:0.335]
Epoch [13/120    avg_loss:2.315, val_acc:0.375]
Epoch [14/120    avg_loss:2.273, val_acc:0.444]
Epoch [15/120    avg_loss:2.267, val_acc:0.448]
Epoch [16/120    avg_loss:2.227, val_acc:0.471]
Epoch [17/120    avg_loss:2.178, val_acc:0.487]
Epoch [18/120    avg_loss:2.157, val_acc:0.494]
Epoch [19/120    avg_loss:2.122, val_acc:0.512]
Epoch [20/120    avg_loss:2.093, val_acc:0.550]
Epoch [21/120    avg_loss:2.040, val_acc:0.569]
Epoch [22/120    avg_loss:1.992, val_acc:0.573]
Epoch [23/120    avg_loss:1.963, val_acc:0.598]
Epoch [24/120    avg_loss:1.942, val_acc:0.629]
Epoch [25/120    avg_loss:1.887, val_acc:0.637]
Epoch [26/120    avg_loss:1.827, val_acc:0.665]
Epoch [27/120    avg_loss:1.788, val_acc:0.656]
Epoch [28/120    avg_loss:1.720, val_acc:0.690]
Epoch [29/120    avg_loss:1.671, val_acc:0.679]
Epoch [30/120    avg_loss:1.626, val_acc:0.669]
Epoch [31/120    avg_loss:1.563, val_acc:0.721]
Epoch [32/120    avg_loss:1.511, val_acc:0.727]
Epoch [33/120    avg_loss:1.429, val_acc:0.748]
Epoch [34/120    avg_loss:1.390, val_acc:0.750]
Epoch [35/120    avg_loss:1.322, val_acc:0.758]
Epoch [36/120    avg_loss:1.283, val_acc:0.756]
Epoch [37/120    avg_loss:1.233, val_acc:0.775]
Epoch [38/120    avg_loss:1.162, val_acc:0.746]
Epoch [39/120    avg_loss:1.111, val_acc:0.762]
Epoch [40/120    avg_loss:1.064, val_acc:0.781]
Epoch [41/120    avg_loss:0.982, val_acc:0.790]
Epoch [42/120    avg_loss:0.953, val_acc:0.775]
Epoch [43/120    avg_loss:0.920, val_acc:0.792]
Epoch [44/120    avg_loss:0.879, val_acc:0.792]
Epoch [45/120    avg_loss:0.839, val_acc:0.796]
Epoch [46/120    avg_loss:0.816, val_acc:0.781]
Epoch [47/120    avg_loss:0.792, val_acc:0.787]
Epoch [48/120    avg_loss:0.743, val_acc:0.802]
Epoch [49/120    avg_loss:0.733, val_acc:0.775]
Epoch [50/120    avg_loss:0.714, val_acc:0.794]
Epoch [51/120    avg_loss:0.693, val_acc:0.787]
Epoch [52/120    avg_loss:0.665, val_acc:0.804]
Epoch [53/120    avg_loss:0.682, val_acc:0.804]
Epoch [54/120    avg_loss:0.629, val_acc:0.798]
Epoch [55/120    avg_loss:0.608, val_acc:0.798]
Epoch [56/120    avg_loss:0.607, val_acc:0.831]
Epoch [57/120    avg_loss:0.584, val_acc:0.875]
Epoch [58/120    avg_loss:0.554, val_acc:0.821]
Epoch [59/120    avg_loss:0.538, val_acc:0.823]
Epoch [60/120    avg_loss:0.507, val_acc:0.881]
Epoch [61/120    avg_loss:0.505, val_acc:0.908]
Epoch [62/120    avg_loss:0.471, val_acc:0.912]
Epoch [63/120    avg_loss:0.488, val_acc:0.910]
Epoch [64/120    avg_loss:0.448, val_acc:0.919]
Epoch [65/120    avg_loss:0.394, val_acc:0.942]
Epoch [66/120    avg_loss:0.469, val_acc:0.938]
Epoch [67/120    avg_loss:0.420, val_acc:0.927]
Epoch [68/120    avg_loss:0.413, val_acc:0.935]
Epoch [69/120    avg_loss:0.390, val_acc:0.942]
Epoch [70/120    avg_loss:0.396, val_acc:0.952]
Epoch [71/120    avg_loss:0.364, val_acc:0.938]
Epoch [72/120    avg_loss:0.389, val_acc:0.896]
Epoch [73/120    avg_loss:0.380, val_acc:0.927]
Epoch [74/120    avg_loss:0.353, val_acc:0.940]
Epoch [75/120    avg_loss:0.351, val_acc:0.956]
Epoch [76/120    avg_loss:0.315, val_acc:0.954]
Epoch [77/120    avg_loss:0.291, val_acc:0.942]
Epoch [78/120    avg_loss:0.283, val_acc:0.950]
Epoch [79/120    avg_loss:0.252, val_acc:0.948]
Epoch [80/120    avg_loss:0.300, val_acc:0.931]
Epoch [81/120    avg_loss:0.270, val_acc:0.935]
Epoch [82/120    avg_loss:0.274, val_acc:0.952]
Epoch [83/120    avg_loss:0.261, val_acc:0.956]
Epoch [84/120    avg_loss:0.239, val_acc:0.946]
Epoch [85/120    avg_loss:0.242, val_acc:0.965]
Epoch [86/120    avg_loss:0.214, val_acc:0.965]
Epoch [87/120    avg_loss:0.222, val_acc:0.969]
Epoch [88/120    avg_loss:0.232, val_acc:0.935]
Epoch [89/120    avg_loss:0.216, val_acc:0.950]
Epoch [90/120    avg_loss:0.227, val_acc:0.958]
Epoch [91/120    avg_loss:0.228, val_acc:0.965]
Epoch [92/120    avg_loss:0.231, val_acc:0.958]
Epoch [93/120    avg_loss:0.250, val_acc:0.967]
Epoch [94/120    avg_loss:0.180, val_acc:0.967]
Epoch [95/120    avg_loss:0.210, val_acc:0.958]
Epoch [96/120    avg_loss:0.210, val_acc:0.960]
Epoch [97/120    avg_loss:0.205, val_acc:0.958]
Epoch [98/120    avg_loss:0.169, val_acc:0.969]
Epoch [99/120    avg_loss:0.192, val_acc:0.942]
Epoch [100/120    avg_loss:0.202, val_acc:0.969]
Epoch [101/120    avg_loss:0.187, val_acc:0.965]
Epoch [102/120    avg_loss:0.164, val_acc:0.971]
Epoch [103/120    avg_loss:0.152, val_acc:0.965]
Epoch [104/120    avg_loss:0.177, val_acc:0.965]
Epoch [105/120    avg_loss:0.181, val_acc:0.965]
Epoch [106/120    avg_loss:0.207, val_acc:0.971]
Epoch [107/120    avg_loss:0.154, val_acc:0.948]
Epoch [108/120    avg_loss:0.134, val_acc:0.971]
Epoch [109/120    avg_loss:0.127, val_acc:0.975]
Epoch [110/120    avg_loss:0.121, val_acc:0.950]
Epoch [111/120    avg_loss:0.136, val_acc:0.971]
Epoch [112/120    avg_loss:0.112, val_acc:0.963]
Epoch [113/120    avg_loss:0.174, val_acc:0.979]
Epoch [114/120    avg_loss:0.145, val_acc:0.971]
Epoch [115/120    avg_loss:0.151, val_acc:0.965]
Epoch [116/120    avg_loss:0.112, val_acc:0.981]
Epoch [117/120    avg_loss:0.108, val_acc:0.975]
Epoch [118/120    avg_loss:0.122, val_acc:0.969]
Epoch [119/120    avg_loss:0.209, val_acc:0.967]
Epoch [120/120    avg_loss:0.197, val_acc:0.958]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   5   0   0   0   0   1   0]
 [  0   0   0 223   2   0   0   5   0   0   0   0   0   0]
 [  0   0   0   6 175  22  22   0   0   0   0   0   2   0]
 [  0   0   0  12  11 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 366  11   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.50533049040511

F1 scores:
[       nan 1.         0.94666667 0.94692144 0.84337349 0.84429066
 0.94930876 0.84444444 1.         1.         1.         0.98519515
 0.98478261 1.        ]

Kappa:
0.9722233681994442
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77bc343e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.637, val_acc:0.077]
Epoch [2/120    avg_loss:2.610, val_acc:0.077]
Epoch [3/120    avg_loss:2.581, val_acc:0.077]
Epoch [4/120    avg_loss:2.555, val_acc:0.077]
Epoch [5/120    avg_loss:2.527, val_acc:0.098]
Epoch [6/120    avg_loss:2.504, val_acc:0.142]
Epoch [7/120    avg_loss:2.481, val_acc:0.177]
Epoch [8/120    avg_loss:2.453, val_acc:0.244]
Epoch [9/120    avg_loss:2.431, val_acc:0.315]
Epoch [10/120    avg_loss:2.394, val_acc:0.304]
Epoch [11/120    avg_loss:2.362, val_acc:0.327]
Epoch [12/120    avg_loss:2.324, val_acc:0.346]
Epoch [13/120    avg_loss:2.298, val_acc:0.354]
Epoch [14/120    avg_loss:2.273, val_acc:0.365]
Epoch [15/120    avg_loss:2.232, val_acc:0.394]
Epoch [16/120    avg_loss:2.193, val_acc:0.412]
Epoch [17/120    avg_loss:2.156, val_acc:0.431]
Epoch [18/120    avg_loss:2.112, val_acc:0.454]
Epoch [19/120    avg_loss:2.078, val_acc:0.477]
Epoch [20/120    avg_loss:2.015, val_acc:0.496]
Epoch [21/120    avg_loss:1.991, val_acc:0.506]
Epoch [22/120    avg_loss:1.925, val_acc:0.529]
Epoch [23/120    avg_loss:1.907, val_acc:0.527]
Epoch [24/120    avg_loss:1.848, val_acc:0.542]
Epoch [25/120    avg_loss:1.804, val_acc:0.558]
Epoch [26/120    avg_loss:1.788, val_acc:0.567]
Epoch [27/120    avg_loss:1.751, val_acc:0.581]
Epoch [28/120    avg_loss:1.687, val_acc:0.604]
Epoch [29/120    avg_loss:1.646, val_acc:0.621]
Epoch [30/120    avg_loss:1.613, val_acc:0.635]
Epoch [31/120    avg_loss:1.576, val_acc:0.623]
Epoch [32/120    avg_loss:1.538, val_acc:0.660]
Epoch [33/120    avg_loss:1.465, val_acc:0.650]
Epoch [34/120    avg_loss:1.442, val_acc:0.665]
Epoch [35/120    avg_loss:1.388, val_acc:0.665]
Epoch [36/120    avg_loss:1.412, val_acc:0.667]
Epoch [37/120    avg_loss:1.304, val_acc:0.694]
Epoch [38/120    avg_loss:1.275, val_acc:0.679]
Epoch [39/120    avg_loss:1.205, val_acc:0.700]
Epoch [40/120    avg_loss:1.209, val_acc:0.704]
Epoch [41/120    avg_loss:1.144, val_acc:0.704]
Epoch [42/120    avg_loss:1.084, val_acc:0.719]
Epoch [43/120    avg_loss:1.045, val_acc:0.794]
Epoch [44/120    avg_loss:1.021, val_acc:0.729]
Epoch [45/120    avg_loss:0.954, val_acc:0.756]
Epoch [46/120    avg_loss:0.908, val_acc:0.796]
Epoch [47/120    avg_loss:0.883, val_acc:0.823]
Epoch [48/120    avg_loss:0.807, val_acc:0.831]
Epoch [49/120    avg_loss:0.776, val_acc:0.906]
Epoch [50/120    avg_loss:0.733, val_acc:0.904]
Epoch [51/120    avg_loss:0.724, val_acc:0.910]
Epoch [52/120    avg_loss:0.701, val_acc:0.892]
Epoch [53/120    avg_loss:0.668, val_acc:0.917]
Epoch [54/120    avg_loss:0.631, val_acc:0.910]
Epoch [55/120    avg_loss:0.584, val_acc:0.933]
Epoch [56/120    avg_loss:0.563, val_acc:0.938]
Epoch [57/120    avg_loss:0.544, val_acc:0.900]
Epoch [58/120    avg_loss:0.603, val_acc:0.938]
Epoch [59/120    avg_loss:0.530, val_acc:0.935]
Epoch [60/120    avg_loss:0.507, val_acc:0.933]
Epoch [61/120    avg_loss:0.462, val_acc:0.940]
Epoch [62/120    avg_loss:0.444, val_acc:0.950]
Epoch [63/120    avg_loss:0.447, val_acc:0.958]
Epoch [64/120    avg_loss:0.436, val_acc:0.973]
Epoch [65/120    avg_loss:0.392, val_acc:0.963]
Epoch [66/120    avg_loss:0.382, val_acc:0.956]
Epoch [67/120    avg_loss:0.373, val_acc:0.958]
Epoch [68/120    avg_loss:0.352, val_acc:0.948]
Epoch [69/120    avg_loss:0.396, val_acc:0.950]
Epoch [70/120    avg_loss:0.365, val_acc:0.967]
Epoch [71/120    avg_loss:0.331, val_acc:0.967]
Epoch [72/120    avg_loss:0.322, val_acc:0.975]
Epoch [73/120    avg_loss:0.293, val_acc:0.946]
Epoch [74/120    avg_loss:0.325, val_acc:0.971]
Epoch [75/120    avg_loss:0.326, val_acc:0.944]
Epoch [76/120    avg_loss:0.317, val_acc:0.956]
Epoch [77/120    avg_loss:0.265, val_acc:0.983]
Epoch [78/120    avg_loss:0.275, val_acc:0.967]
Epoch [79/120    avg_loss:0.266, val_acc:0.960]
Epoch [80/120    avg_loss:0.228, val_acc:0.958]
Epoch [81/120    avg_loss:0.258, val_acc:0.975]
Epoch [82/120    avg_loss:0.223, val_acc:0.952]
Epoch [83/120    avg_loss:0.221, val_acc:0.981]
Epoch [84/120    avg_loss:0.211, val_acc:0.971]
Epoch [85/120    avg_loss:0.237, val_acc:0.969]
Epoch [86/120    avg_loss:0.274, val_acc:0.975]
Epoch [87/120    avg_loss:0.186, val_acc:0.975]
Epoch [88/120    avg_loss:0.208, val_acc:0.985]
Epoch [89/120    avg_loss:0.171, val_acc:0.979]
Epoch [90/120    avg_loss:0.175, val_acc:0.979]
Epoch [91/120    avg_loss:0.189, val_acc:0.988]
Epoch [92/120    avg_loss:0.144, val_acc:0.979]
Epoch [93/120    avg_loss:0.179, val_acc:0.977]
Epoch [94/120    avg_loss:0.214, val_acc:0.981]
Epoch [95/120    avg_loss:0.231, val_acc:0.946]
Epoch [96/120    avg_loss:0.195, val_acc:0.973]
Epoch [97/120    avg_loss:0.187, val_acc:0.954]
Epoch [98/120    avg_loss:0.164, val_acc:0.988]
Epoch [99/120    avg_loss:0.140, val_acc:0.990]
Epoch [100/120    avg_loss:0.140, val_acc:0.990]
Epoch [101/120    avg_loss:0.145, val_acc:0.981]
Epoch [102/120    avg_loss:0.121, val_acc:0.988]
Epoch [103/120    avg_loss:0.105, val_acc:0.977]
Epoch [104/120    avg_loss:0.107, val_acc:0.988]
Epoch [105/120    avg_loss:0.108, val_acc:0.998]
Epoch [106/120    avg_loss:0.126, val_acc:0.971]
Epoch [107/120    avg_loss:0.131, val_acc:0.988]
Epoch [108/120    avg_loss:0.131, val_acc:0.988]
Epoch [109/120    avg_loss:0.118, val_acc:0.994]
Epoch [110/120    avg_loss:0.103, val_acc:0.985]
Epoch [111/120    avg_loss:0.094, val_acc:0.985]
Epoch [112/120    avg_loss:0.098, val_acc:0.981]
Epoch [113/120    avg_loss:0.083, val_acc:0.994]
Epoch [114/120    avg_loss:0.077, val_acc:0.979]
Epoch [115/120    avg_loss:0.089, val_acc:0.988]
Epoch [116/120    avg_loss:0.102, val_acc:0.990]
Epoch [117/120    avg_loss:0.101, val_acc:0.990]
Epoch [118/120    avg_loss:0.098, val_acc:0.998]
Epoch [119/120    avg_loss:0.082, val_acc:0.996]
Epoch [120/120    avg_loss:0.076, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 227   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  86  59   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0   0 375   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.56929637526652

F1 scores:
[       nan 1.         0.94600432 0.99563319 0.84074074 0.57843137
 1.         0.94382022 0.98296199 1.         1.         0.99600533
 0.99669967 1.        ]

Kappa:
0.9729298672127611
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff57dea2e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.690, val_acc:0.069]
Epoch [2/120    avg_loss:2.664, val_acc:0.077]
Epoch [3/120    avg_loss:2.639, val_acc:0.102]
Epoch [4/120    avg_loss:2.613, val_acc:0.102]
Epoch [5/120    avg_loss:2.586, val_acc:0.142]
Epoch [6/120    avg_loss:2.561, val_acc:0.185]
Epoch [7/120    avg_loss:2.533, val_acc:0.273]
Epoch [8/120    avg_loss:2.508, val_acc:0.344]
Epoch [9/120    avg_loss:2.481, val_acc:0.365]
Epoch [10/120    avg_loss:2.461, val_acc:0.365]
Epoch [11/120    avg_loss:2.434, val_acc:0.392]
Epoch [12/120    avg_loss:2.404, val_acc:0.390]
Epoch [13/120    avg_loss:2.381, val_acc:0.392]
Epoch [14/120    avg_loss:2.353, val_acc:0.408]
Epoch [15/120    avg_loss:2.328, val_acc:0.440]
Epoch [16/120    avg_loss:2.306, val_acc:0.433]
Epoch [17/120    avg_loss:2.272, val_acc:0.448]
Epoch [18/120    avg_loss:2.244, val_acc:0.458]
Epoch [19/120    avg_loss:2.192, val_acc:0.460]
Epoch [20/120    avg_loss:2.161, val_acc:0.481]
Epoch [21/120    avg_loss:2.123, val_acc:0.473]
Epoch [22/120    avg_loss:2.094, val_acc:0.535]
Epoch [23/120    avg_loss:2.039, val_acc:0.498]
Epoch [24/120    avg_loss:2.008, val_acc:0.525]
Epoch [25/120    avg_loss:1.957, val_acc:0.558]
Epoch [26/120    avg_loss:1.902, val_acc:0.571]
Epoch [27/120    avg_loss:1.824, val_acc:0.577]
Epoch [28/120    avg_loss:1.804, val_acc:0.623]
Epoch [29/120    avg_loss:1.721, val_acc:0.623]
Epoch [30/120    avg_loss:1.679, val_acc:0.629]
Epoch [31/120    avg_loss:1.632, val_acc:0.635]
Epoch [32/120    avg_loss:1.566, val_acc:0.656]
Epoch [33/120    avg_loss:1.513, val_acc:0.654]
Epoch [34/120    avg_loss:1.457, val_acc:0.685]
Epoch [35/120    avg_loss:1.390, val_acc:0.706]
Epoch [36/120    avg_loss:1.328, val_acc:0.787]
Epoch [37/120    avg_loss:1.294, val_acc:0.798]
Epoch [38/120    avg_loss:1.225, val_acc:0.775]
Epoch [39/120    avg_loss:1.155, val_acc:0.863]
Epoch [40/120    avg_loss:1.128, val_acc:0.781]
Epoch [41/120    avg_loss:1.043, val_acc:0.908]
Epoch [42/120    avg_loss:0.993, val_acc:0.852]
Epoch [43/120    avg_loss:0.954, val_acc:0.887]
Epoch [44/120    avg_loss:0.914, val_acc:0.906]
Epoch [45/120    avg_loss:0.868, val_acc:0.875]
Epoch [46/120    avg_loss:0.818, val_acc:0.900]
Epoch [47/120    avg_loss:0.810, val_acc:0.915]
Epoch [48/120    avg_loss:0.774, val_acc:0.917]
Epoch [49/120    avg_loss:0.713, val_acc:0.933]
Epoch [50/120    avg_loss:0.674, val_acc:0.929]
Epoch [51/120    avg_loss:0.623, val_acc:0.940]
Epoch [52/120    avg_loss:0.607, val_acc:0.935]
Epoch [53/120    avg_loss:0.634, val_acc:0.927]
Epoch [54/120    avg_loss:0.591, val_acc:0.933]
Epoch [55/120    avg_loss:0.560, val_acc:0.931]
Epoch [56/120    avg_loss:0.519, val_acc:0.938]
Epoch [57/120    avg_loss:0.499, val_acc:0.929]
Epoch [58/120    avg_loss:0.502, val_acc:0.919]
Epoch [59/120    avg_loss:0.493, val_acc:0.929]
Epoch [60/120    avg_loss:0.468, val_acc:0.948]
Epoch [61/120    avg_loss:0.474, val_acc:0.940]
Epoch [62/120    avg_loss:0.474, val_acc:0.935]
Epoch [63/120    avg_loss:0.430, val_acc:0.954]
Epoch [64/120    avg_loss:0.400, val_acc:0.954]
Epoch [65/120    avg_loss:0.369, val_acc:0.938]
Epoch [66/120    avg_loss:0.378, val_acc:0.948]
Epoch [67/120    avg_loss:0.366, val_acc:0.960]
Epoch [68/120    avg_loss:0.337, val_acc:0.948]
Epoch [69/120    avg_loss:0.330, val_acc:0.931]
Epoch [70/120    avg_loss:0.357, val_acc:0.956]
Epoch [71/120    avg_loss:0.350, val_acc:0.952]
Epoch [72/120    avg_loss:0.312, val_acc:0.952]
Epoch [73/120    avg_loss:0.333, val_acc:0.935]
Epoch [74/120    avg_loss:0.338, val_acc:0.954]
Epoch [75/120    avg_loss:0.299, val_acc:0.960]
Epoch [76/120    avg_loss:0.278, val_acc:0.973]
Epoch [77/120    avg_loss:0.300, val_acc:0.946]
Epoch [78/120    avg_loss:0.301, val_acc:0.958]
Epoch [79/120    avg_loss:0.264, val_acc:0.977]
Epoch [80/120    avg_loss:0.269, val_acc:0.969]
Epoch [81/120    avg_loss:0.311, val_acc:0.954]
Epoch [82/120    avg_loss:0.280, val_acc:0.969]
Epoch [83/120    avg_loss:0.272, val_acc:0.960]
Epoch [84/120    avg_loss:0.232, val_acc:0.952]
Epoch [85/120    avg_loss:0.240, val_acc:0.967]
Epoch [86/120    avg_loss:0.231, val_acc:0.977]
Epoch [87/120    avg_loss:0.191, val_acc:0.979]
Epoch [88/120    avg_loss:0.155, val_acc:0.979]
Epoch [89/120    avg_loss:0.158, val_acc:0.979]
Epoch [90/120    avg_loss:0.141, val_acc:0.975]
Epoch [91/120    avg_loss:0.162, val_acc:0.969]
Epoch [92/120    avg_loss:0.207, val_acc:0.981]
Epoch [93/120    avg_loss:0.175, val_acc:0.979]
Epoch [94/120    avg_loss:0.179, val_acc:0.979]
Epoch [95/120    avg_loss:0.182, val_acc:0.983]
Epoch [96/120    avg_loss:0.179, val_acc:0.979]
Epoch [97/120    avg_loss:0.159, val_acc:0.981]
Epoch [98/120    avg_loss:0.166, val_acc:0.975]
Epoch [99/120    avg_loss:0.168, val_acc:0.963]
Epoch [100/120    avg_loss:0.176, val_acc:0.983]
Epoch [101/120    avg_loss:0.161, val_acc:0.973]
Epoch [102/120    avg_loss:0.172, val_acc:0.979]
Epoch [103/120    avg_loss:0.129, val_acc:0.981]
Epoch [104/120    avg_loss:0.113, val_acc:0.969]
Epoch [105/120    avg_loss:0.170, val_acc:0.981]
Epoch [106/120    avg_loss:0.157, val_acc:0.977]
Epoch [107/120    avg_loss:0.203, val_acc:0.950]
Epoch [108/120    avg_loss:0.175, val_acc:0.979]
Epoch [109/120    avg_loss:0.139, val_acc:0.979]
Epoch [110/120    avg_loss:0.131, val_acc:0.977]
Epoch [111/120    avg_loss:0.124, val_acc:0.975]
Epoch [112/120    avg_loss:0.111, val_acc:0.979]
Epoch [113/120    avg_loss:0.079, val_acc:0.983]
Epoch [114/120    avg_loss:0.090, val_acc:0.983]
Epoch [115/120    avg_loss:0.075, val_acc:0.985]
Epoch [116/120    avg_loss:0.130, val_acc:0.942]
Epoch [117/120    avg_loss:0.180, val_acc:0.973]
Epoch [118/120    avg_loss:0.143, val_acc:0.958]
Epoch [119/120    avg_loss:0.165, val_acc:0.971]
Epoch [120/120    avg_loss:0.165, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   3 161  58   0   1   2   5   0   0   0   0   0]
 [  0   0   0   0 195  13  19   0   0   0   0   0   0   0]
 [  0   0   0   0  37  64  44   0   0   0   0   0   0   0]
 [  0  79   0   0   1   0 126   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   1  75   0   0   0   0   0   0]
 [  0   2  11   0   0   0   0   0 375   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
93.64605543710022

F1 scores:
[       nan 0.94417643 0.92274678 0.82352941 0.75289575 0.57657658
 0.63476071 0.85714286 0.9765625  1.         1.         1.
 1.         1.        ]

Kappa:
0.9291060351276194
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd6b8d0e630>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.638, val_acc:0.166]
Epoch [2/120    avg_loss:2.611, val_acc:0.180]
Epoch [3/120    avg_loss:2.588, val_acc:0.117]
Epoch [4/120    avg_loss:2.561, val_acc:0.183]
Epoch [5/120    avg_loss:2.539, val_acc:0.292]
Epoch [6/120    avg_loss:2.517, val_acc:0.296]
Epoch [7/120    avg_loss:2.499, val_acc:0.294]
Epoch [8/120    avg_loss:2.475, val_acc:0.290]
Epoch [9/120    avg_loss:2.452, val_acc:0.290]
Epoch [10/120    avg_loss:2.432, val_acc:0.290]
Epoch [11/120    avg_loss:2.414, val_acc:0.290]
Epoch [12/120    avg_loss:2.399, val_acc:0.294]
Epoch [13/120    avg_loss:2.375, val_acc:0.298]
Epoch [14/120    avg_loss:2.352, val_acc:0.294]
Epoch [15/120    avg_loss:2.324, val_acc:0.312]
Epoch [16/120    avg_loss:2.304, val_acc:0.331]
Epoch [17/120    avg_loss:2.267, val_acc:0.344]
Epoch [18/120    avg_loss:2.239, val_acc:0.369]
Epoch [19/120    avg_loss:2.217, val_acc:0.379]
Epoch [20/120    avg_loss:2.200, val_acc:0.408]
Epoch [21/120    avg_loss:2.139, val_acc:0.465]
Epoch [22/120    avg_loss:2.113, val_acc:0.512]
Epoch [23/120    avg_loss:2.067, val_acc:0.562]
Epoch [24/120    avg_loss:2.036, val_acc:0.585]
Epoch [25/120    avg_loss:1.989, val_acc:0.642]
Epoch [26/120    avg_loss:1.949, val_acc:0.675]
Epoch [27/120    avg_loss:1.923, val_acc:0.696]
Epoch [28/120    avg_loss:1.855, val_acc:0.719]
Epoch [29/120    avg_loss:1.813, val_acc:0.744]
Epoch [30/120    avg_loss:1.769, val_acc:0.775]
Epoch [31/120    avg_loss:1.724, val_acc:0.779]
Epoch [32/120    avg_loss:1.663, val_acc:0.812]
Epoch [33/120    avg_loss:1.630, val_acc:0.802]
Epoch [34/120    avg_loss:1.559, val_acc:0.796]
Epoch [35/120    avg_loss:1.509, val_acc:0.827]
Epoch [36/120    avg_loss:1.470, val_acc:0.817]
Epoch [37/120    avg_loss:1.430, val_acc:0.804]
Epoch [38/120    avg_loss:1.346, val_acc:0.821]
Epoch [39/120    avg_loss:1.298, val_acc:0.798]
Epoch [40/120    avg_loss:1.243, val_acc:0.787]
Epoch [41/120    avg_loss:1.177, val_acc:0.783]
Epoch [42/120    avg_loss:1.125, val_acc:0.887]
Epoch [43/120    avg_loss:1.081, val_acc:0.842]
Epoch [44/120    avg_loss:1.024, val_acc:0.873]
Epoch [45/120    avg_loss:0.978, val_acc:0.850]
Epoch [46/120    avg_loss:0.929, val_acc:0.871]
Epoch [47/120    avg_loss:0.875, val_acc:0.833]
Epoch [48/120    avg_loss:0.821, val_acc:0.921]
Epoch [49/120    avg_loss:0.811, val_acc:0.925]
Epoch [50/120    avg_loss:0.724, val_acc:0.923]
Epoch [51/120    avg_loss:0.699, val_acc:0.908]
Epoch [52/120    avg_loss:0.674, val_acc:0.935]
Epoch [53/120    avg_loss:0.695, val_acc:0.908]
Epoch [54/120    avg_loss:0.653, val_acc:0.946]
Epoch [55/120    avg_loss:0.609, val_acc:0.935]
Epoch [56/120    avg_loss:0.614, val_acc:0.935]
Epoch [57/120    avg_loss:0.539, val_acc:0.946]
Epoch [58/120    avg_loss:0.564, val_acc:0.958]
Epoch [59/120    avg_loss:0.500, val_acc:0.935]
Epoch [60/120    avg_loss:0.482, val_acc:0.958]
Epoch [61/120    avg_loss:0.467, val_acc:0.950]
Epoch [62/120    avg_loss:0.441, val_acc:0.935]
Epoch [63/120    avg_loss:0.430, val_acc:0.929]
Epoch [64/120    avg_loss:0.512, val_acc:0.900]
Epoch [65/120    avg_loss:0.559, val_acc:0.938]
Epoch [66/120    avg_loss:0.411, val_acc:0.944]
Epoch [67/120    avg_loss:0.407, val_acc:0.960]
Epoch [68/120    avg_loss:0.360, val_acc:0.956]
Epoch [69/120    avg_loss:0.365, val_acc:0.960]
Epoch [70/120    avg_loss:0.385, val_acc:0.967]
Epoch [71/120    avg_loss:0.355, val_acc:0.973]
Epoch [72/120    avg_loss:0.336, val_acc:0.973]
Epoch [73/120    avg_loss:0.314, val_acc:0.946]
Epoch [74/120    avg_loss:0.345, val_acc:0.975]
Epoch [75/120    avg_loss:0.287, val_acc:0.973]
Epoch [76/120    avg_loss:0.286, val_acc:0.969]
Epoch [77/120    avg_loss:0.278, val_acc:0.971]
Epoch [78/120    avg_loss:0.267, val_acc:0.965]
Epoch [79/120    avg_loss:0.266, val_acc:0.931]
Epoch [80/120    avg_loss:0.265, val_acc:0.967]
Epoch [81/120    avg_loss:0.231, val_acc:0.950]
Epoch [82/120    avg_loss:0.244, val_acc:0.979]
Epoch [83/120    avg_loss:0.227, val_acc:0.979]
Epoch [84/120    avg_loss:0.227, val_acc:0.973]
Epoch [85/120    avg_loss:0.202, val_acc:0.967]
Epoch [86/120    avg_loss:0.196, val_acc:0.977]
Epoch [87/120    avg_loss:0.219, val_acc:0.975]
Epoch [88/120    avg_loss:0.189, val_acc:0.971]
Epoch [89/120    avg_loss:0.220, val_acc:0.958]
Epoch [90/120    avg_loss:0.206, val_acc:0.979]
Epoch [91/120    avg_loss:0.177, val_acc:0.977]
Epoch [92/120    avg_loss:0.164, val_acc:0.958]
Epoch [93/120    avg_loss:0.180, val_acc:0.969]
Epoch [94/120    avg_loss:0.173, val_acc:0.967]
Epoch [95/120    avg_loss:0.175, val_acc:0.981]
Epoch [96/120    avg_loss:0.174, val_acc:0.969]
Epoch [97/120    avg_loss:0.157, val_acc:0.983]
Epoch [98/120    avg_loss:0.146, val_acc:0.969]
Epoch [99/120    avg_loss:0.136, val_acc:0.981]
Epoch [100/120    avg_loss:0.144, val_acc:0.969]
Epoch [101/120    avg_loss:0.144, val_acc:0.975]
Epoch [102/120    avg_loss:0.125, val_acc:0.985]
Epoch [103/120    avg_loss:0.143, val_acc:0.983]
Epoch [104/120    avg_loss:0.144, val_acc:0.975]
Epoch [105/120    avg_loss:0.106, val_acc:0.985]
Epoch [106/120    avg_loss:0.138, val_acc:0.985]
Epoch [107/120    avg_loss:0.150, val_acc:0.981]
Epoch [108/120    avg_loss:0.112, val_acc:0.977]
Epoch [109/120    avg_loss:0.104, val_acc:0.983]
Epoch [110/120    avg_loss:0.115, val_acc:0.983]
Epoch [111/120    avg_loss:0.120, val_acc:0.960]
Epoch [112/120    avg_loss:0.140, val_acc:0.973]
Epoch [113/120    avg_loss:0.146, val_acc:0.975]
Epoch [114/120    avg_loss:0.175, val_acc:0.973]
Epoch [115/120    avg_loss:0.145, val_acc:0.985]
Epoch [116/120    avg_loss:0.144, val_acc:0.971]
Epoch [117/120    avg_loss:0.119, val_acc:0.981]
Epoch [118/120    avg_loss:0.102, val_acc:0.973]
Epoch [119/120    avg_loss:0.136, val_acc:0.983]
Epoch [120/120    avg_loss:0.138, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 566   0   0   0   0 119   0   0   0   0   0   0   0]
 [  0   0 199   0   0   0   1  19   0   0   0   0   0   0]
 [  0   0   0 128  49   0  41   7   5   0   0   0   0   0]
 [  0   0   0   0 135   3  89   0   0   0   0   0   0   0]
 [  0   0   0   0  43  27  75   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   3   6   0   0   0   2   0 377   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
89.76545842217485

F1 scores:
[       nan 0.90271132 0.90045249 0.7150838  0.59471366 0.30857143
 0.55751015 0.7755102  0.97922078 1.         1.         1.
 1.         1.        ]

Kappa:
0.8863613590660511
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fefb4659e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.056]
Epoch [2/120    avg_loss:2.593, val_acc:0.108]
Epoch [3/120    avg_loss:2.559, val_acc:0.200]
Epoch [4/120    avg_loss:2.537, val_acc:0.300]
Epoch [5/120    avg_loss:2.508, val_acc:0.312]
Epoch [6/120    avg_loss:2.484, val_acc:0.308]
Epoch [7/120    avg_loss:2.452, val_acc:0.304]
Epoch [8/120    avg_loss:2.417, val_acc:0.298]
Epoch [9/120    avg_loss:2.402, val_acc:0.294]
Epoch [10/120    avg_loss:2.363, val_acc:0.290]
Epoch [11/120    avg_loss:2.332, val_acc:0.298]
Epoch [12/120    avg_loss:2.291, val_acc:0.308]
Epoch [13/120    avg_loss:2.268, val_acc:0.323]
Epoch [14/120    avg_loss:2.247, val_acc:0.346]
Epoch [15/120    avg_loss:2.201, val_acc:0.362]
Epoch [16/120    avg_loss:2.167, val_acc:0.406]
Epoch [17/120    avg_loss:2.130, val_acc:0.404]
Epoch [18/120    avg_loss:2.093, val_acc:0.417]
Epoch [19/120    avg_loss:2.066, val_acc:0.452]
Epoch [20/120    avg_loss:2.035, val_acc:0.446]
Epoch [21/120    avg_loss:1.990, val_acc:0.502]
Epoch [22/120    avg_loss:1.943, val_acc:0.554]
Epoch [23/120    avg_loss:1.927, val_acc:0.567]
Epoch [24/120    avg_loss:1.900, val_acc:0.548]
Epoch [25/120    avg_loss:1.836, val_acc:0.596]
Epoch [26/120    avg_loss:1.800, val_acc:0.604]
Epoch [27/120    avg_loss:1.770, val_acc:0.633]
Epoch [28/120    avg_loss:1.705, val_acc:0.627]
Epoch [29/120    avg_loss:1.652, val_acc:0.633]
Epoch [30/120    avg_loss:1.583, val_acc:0.650]
Epoch [31/120    avg_loss:1.513, val_acc:0.679]
Epoch [32/120    avg_loss:1.459, val_acc:0.698]
Epoch [33/120    avg_loss:1.450, val_acc:0.702]
Epoch [34/120    avg_loss:1.381, val_acc:0.706]
Epoch [35/120    avg_loss:1.353, val_acc:0.715]
Epoch [36/120    avg_loss:1.290, val_acc:0.729]
Epoch [37/120    avg_loss:1.219, val_acc:0.758]
Epoch [38/120    avg_loss:1.167, val_acc:0.729]
Epoch [39/120    avg_loss:1.140, val_acc:0.723]
Epoch [40/120    avg_loss:1.114, val_acc:0.840]
Epoch [41/120    avg_loss:1.028, val_acc:0.856]
Epoch [42/120    avg_loss:1.001, val_acc:0.865]
Epoch [43/120    avg_loss:0.989, val_acc:0.873]
Epoch [44/120    avg_loss:0.942, val_acc:0.858]
Epoch [45/120    avg_loss:0.866, val_acc:0.904]
Epoch [46/120    avg_loss:0.860, val_acc:0.865]
Epoch [47/120    avg_loss:0.806, val_acc:0.896]
Epoch [48/120    avg_loss:0.754, val_acc:0.923]
Epoch [49/120    avg_loss:0.729, val_acc:0.894]
Epoch [50/120    avg_loss:0.709, val_acc:0.929]
Epoch [51/120    avg_loss:0.643, val_acc:0.942]
Epoch [52/120    avg_loss:0.639, val_acc:0.921]
Epoch [53/120    avg_loss:0.610, val_acc:0.948]
Epoch [54/120    avg_loss:0.565, val_acc:0.963]
Epoch [55/120    avg_loss:0.531, val_acc:0.952]
Epoch [56/120    avg_loss:0.476, val_acc:0.952]
Epoch [57/120    avg_loss:0.461, val_acc:0.975]
Epoch [58/120    avg_loss:0.463, val_acc:0.944]
Epoch [59/120    avg_loss:0.469, val_acc:0.965]
Epoch [60/120    avg_loss:0.389, val_acc:0.967]
Epoch [61/120    avg_loss:0.436, val_acc:0.958]
Epoch [62/120    avg_loss:0.408, val_acc:0.960]
Epoch [63/120    avg_loss:0.424, val_acc:0.927]
Epoch [64/120    avg_loss:0.452, val_acc:0.960]
Epoch [65/120    avg_loss:0.362, val_acc:0.973]
Epoch [66/120    avg_loss:0.343, val_acc:0.967]
Epoch [67/120    avg_loss:0.369, val_acc:0.971]
Epoch [68/120    avg_loss:0.300, val_acc:0.971]
Epoch [69/120    avg_loss:0.344, val_acc:0.940]
Epoch [70/120    avg_loss:0.342, val_acc:0.973]
Epoch [71/120    avg_loss:0.269, val_acc:0.975]
Epoch [72/120    avg_loss:0.250, val_acc:0.977]
Epoch [73/120    avg_loss:0.248, val_acc:0.979]
Epoch [74/120    avg_loss:0.231, val_acc:0.985]
Epoch [75/120    avg_loss:0.235, val_acc:0.981]
Epoch [76/120    avg_loss:0.233, val_acc:0.985]
Epoch [77/120    avg_loss:0.231, val_acc:0.985]
Epoch [78/120    avg_loss:0.225, val_acc:0.985]
Epoch [79/120    avg_loss:0.227, val_acc:0.985]
Epoch [80/120    avg_loss:0.230, val_acc:0.985]
Epoch [81/120    avg_loss:0.220, val_acc:0.983]
Epoch [82/120    avg_loss:0.236, val_acc:0.983]
Epoch [83/120    avg_loss:0.204, val_acc:0.988]
Epoch [84/120    avg_loss:0.219, val_acc:0.988]
Epoch [85/120    avg_loss:0.205, val_acc:0.990]
Epoch [86/120    avg_loss:0.224, val_acc:0.985]
Epoch [87/120    avg_loss:0.202, val_acc:0.985]
Epoch [88/120    avg_loss:0.203, val_acc:0.988]
Epoch [89/120    avg_loss:0.200, val_acc:0.985]
Epoch [90/120    avg_loss:0.211, val_acc:0.988]
Epoch [91/120    avg_loss:0.185, val_acc:0.983]
Epoch [92/120    avg_loss:0.203, val_acc:0.988]
Epoch [93/120    avg_loss:0.201, val_acc:0.988]
Epoch [94/120    avg_loss:0.184, val_acc:0.985]
Epoch [95/120    avg_loss:0.188, val_acc:0.988]
Epoch [96/120    avg_loss:0.191, val_acc:0.988]
Epoch [97/120    avg_loss:0.205, val_acc:0.990]
Epoch [98/120    avg_loss:0.179, val_acc:0.981]
Epoch [99/120    avg_loss:0.195, val_acc:0.990]
Epoch [100/120    avg_loss:0.185, val_acc:0.985]
Epoch [101/120    avg_loss:0.199, val_acc:0.979]
Epoch [102/120    avg_loss:0.178, val_acc:0.988]
Epoch [103/120    avg_loss:0.196, val_acc:0.992]
Epoch [104/120    avg_loss:0.197, val_acc:0.990]
Epoch [105/120    avg_loss:0.189, val_acc:0.985]
Epoch [106/120    avg_loss:0.195, val_acc:0.985]
Epoch [107/120    avg_loss:0.177, val_acc:0.992]
Epoch [108/120    avg_loss:0.187, val_acc:0.992]
Epoch [109/120    avg_loss:0.160, val_acc:0.985]
Epoch [110/120    avg_loss:0.176, val_acc:0.988]
Epoch [111/120    avg_loss:0.176, val_acc:0.985]
Epoch [112/120    avg_loss:0.180, val_acc:0.985]
Epoch [113/120    avg_loss:0.168, val_acc:0.983]
Epoch [114/120    avg_loss:0.184, val_acc:0.990]
Epoch [115/120    avg_loss:0.168, val_acc:0.985]
Epoch [116/120    avg_loss:0.175, val_acc:0.981]
Epoch [117/120    avg_loss:0.160, val_acc:0.985]
Epoch [118/120    avg_loss:0.174, val_acc:0.990]
Epoch [119/120    avg_loss:0.157, val_acc:0.988]
Epoch [120/120    avg_loss:0.164, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 222   4   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0  31 114   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0  21   0   0   0   0  73   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.10234541577825

F1 scores:
[       nan 1.         0.95010846 0.98230088 0.86451613 0.8
 0.99512195 0.8742515  0.99742931 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.978869376745076
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c6ce42e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.628, val_acc:0.140]
Epoch [2/120    avg_loss:2.606, val_acc:0.256]
Epoch [3/120    avg_loss:2.582, val_acc:0.277]
Epoch [4/120    avg_loss:2.554, val_acc:0.283]
Epoch [5/120    avg_loss:2.533, val_acc:0.285]
Epoch [6/120    avg_loss:2.506, val_acc:0.287]
Epoch [7/120    avg_loss:2.479, val_acc:0.285]
Epoch [8/120    avg_loss:2.455, val_acc:0.285]
Epoch [9/120    avg_loss:2.433, val_acc:0.285]
Epoch [10/120    avg_loss:2.415, val_acc:0.285]
Epoch [11/120    avg_loss:2.394, val_acc:0.285]
Epoch [12/120    avg_loss:2.383, val_acc:0.285]
Epoch [13/120    avg_loss:2.351, val_acc:0.285]
Epoch [14/120    avg_loss:2.328, val_acc:0.285]
Epoch [15/120    avg_loss:2.302, val_acc:0.285]
Epoch [16/120    avg_loss:2.285, val_acc:0.285]
Epoch [17/120    avg_loss:2.251, val_acc:0.292]
Epoch [18/120    avg_loss:2.221, val_acc:0.310]
Epoch [19/120    avg_loss:2.201, val_acc:0.325]
Epoch [20/120    avg_loss:2.169, val_acc:0.352]
Epoch [21/120    avg_loss:2.130, val_acc:0.360]
Epoch [22/120    avg_loss:2.099, val_acc:0.392]
Epoch [23/120    avg_loss:2.071, val_acc:0.412]
Epoch [24/120    avg_loss:2.024, val_acc:0.417]
Epoch [25/120    avg_loss:2.031, val_acc:0.429]
Epoch [26/120    avg_loss:1.970, val_acc:0.446]
Epoch [27/120    avg_loss:1.933, val_acc:0.456]
Epoch [28/120    avg_loss:1.894, val_acc:0.487]
Epoch [29/120    avg_loss:1.870, val_acc:0.479]
Epoch [30/120    avg_loss:1.850, val_acc:0.523]
Epoch [31/120    avg_loss:1.802, val_acc:0.558]
Epoch [32/120    avg_loss:1.743, val_acc:0.573]
Epoch [33/120    avg_loss:1.719, val_acc:0.588]
Epoch [34/120    avg_loss:1.654, val_acc:0.596]
Epoch [35/120    avg_loss:1.628, val_acc:0.652]
Epoch [36/120    avg_loss:1.596, val_acc:0.640]
Epoch [37/120    avg_loss:1.546, val_acc:0.652]
Epoch [38/120    avg_loss:1.485, val_acc:0.673]
Epoch [39/120    avg_loss:1.440, val_acc:0.708]
Epoch [40/120    avg_loss:1.366, val_acc:0.700]
Epoch [41/120    avg_loss:1.310, val_acc:0.729]
Epoch [42/120    avg_loss:1.234, val_acc:0.810]
Epoch [43/120    avg_loss:1.167, val_acc:0.752]
Epoch [44/120    avg_loss:1.131, val_acc:0.796]
Epoch [45/120    avg_loss:1.069, val_acc:0.771]
Epoch [46/120    avg_loss:1.049, val_acc:0.790]
Epoch [47/120    avg_loss:1.007, val_acc:0.800]
Epoch [48/120    avg_loss:0.909, val_acc:0.877]
Epoch [49/120    avg_loss:0.858, val_acc:0.915]
Epoch [50/120    avg_loss:0.846, val_acc:0.883]
Epoch [51/120    avg_loss:0.809, val_acc:0.935]
Epoch [52/120    avg_loss:0.778, val_acc:0.931]
Epoch [53/120    avg_loss:0.705, val_acc:0.921]
Epoch [54/120    avg_loss:0.763, val_acc:0.896]
Epoch [55/120    avg_loss:0.691, val_acc:0.871]
Epoch [56/120    avg_loss:0.658, val_acc:0.952]
Epoch [57/120    avg_loss:0.717, val_acc:0.942]
Epoch [58/120    avg_loss:0.684, val_acc:0.925]
Epoch [59/120    avg_loss:0.587, val_acc:0.929]
Epoch [60/120    avg_loss:0.545, val_acc:0.933]
Epoch [61/120    avg_loss:0.490, val_acc:0.950]
Epoch [62/120    avg_loss:0.489, val_acc:0.929]
Epoch [63/120    avg_loss:0.490, val_acc:0.956]
Epoch [64/120    avg_loss:0.458, val_acc:0.954]
Epoch [65/120    avg_loss:0.448, val_acc:0.915]
Epoch [66/120    avg_loss:0.435, val_acc:0.944]
Epoch [67/120    avg_loss:0.408, val_acc:0.956]
Epoch [68/120    avg_loss:0.378, val_acc:0.946]
Epoch [69/120    avg_loss:0.406, val_acc:0.892]
Epoch [70/120    avg_loss:0.360, val_acc:0.956]
Epoch [71/120    avg_loss:0.369, val_acc:0.963]
Epoch [72/120    avg_loss:0.349, val_acc:0.944]
Epoch [73/120    avg_loss:0.350, val_acc:0.950]
Epoch [74/120    avg_loss:0.346, val_acc:0.946]
Epoch [75/120    avg_loss:0.331, val_acc:0.960]
Epoch [76/120    avg_loss:0.331, val_acc:0.954]
Epoch [77/120    avg_loss:0.290, val_acc:0.954]
Epoch [78/120    avg_loss:0.245, val_acc:0.956]
Epoch [79/120    avg_loss:0.330, val_acc:0.956]
Epoch [80/120    avg_loss:0.410, val_acc:0.908]
Epoch [81/120    avg_loss:0.332, val_acc:0.956]
Epoch [82/120    avg_loss:0.288, val_acc:0.950]
Epoch [83/120    avg_loss:0.336, val_acc:0.938]
Epoch [84/120    avg_loss:0.363, val_acc:0.954]
Epoch [85/120    avg_loss:0.307, val_acc:0.963]
Epoch [86/120    avg_loss:0.248, val_acc:0.967]
Epoch [87/120    avg_loss:0.241, val_acc:0.969]
Epoch [88/120    avg_loss:0.248, val_acc:0.967]
Epoch [89/120    avg_loss:0.230, val_acc:0.971]
Epoch [90/120    avg_loss:0.225, val_acc:0.973]
Epoch [91/120    avg_loss:0.203, val_acc:0.973]
Epoch [92/120    avg_loss:0.197, val_acc:0.971]
Epoch [93/120    avg_loss:0.207, val_acc:0.975]
Epoch [94/120    avg_loss:0.206, val_acc:0.977]
Epoch [95/120    avg_loss:0.207, val_acc:0.977]
Epoch [96/120    avg_loss:0.198, val_acc:0.975]
Epoch [97/120    avg_loss:0.206, val_acc:0.979]
Epoch [98/120    avg_loss:0.217, val_acc:0.979]
Epoch [99/120    avg_loss:0.196, val_acc:0.979]
Epoch [100/120    avg_loss:0.187, val_acc:0.981]
Epoch [101/120    avg_loss:0.190, val_acc:0.979]
Epoch [102/120    avg_loss:0.180, val_acc:0.977]
Epoch [103/120    avg_loss:0.181, val_acc:0.977]
Epoch [104/120    avg_loss:0.184, val_acc:0.981]
Epoch [105/120    avg_loss:0.188, val_acc:0.979]
Epoch [106/120    avg_loss:0.180, val_acc:0.985]
Epoch [107/120    avg_loss:0.178, val_acc:0.981]
Epoch [108/120    avg_loss:0.188, val_acc:0.983]
Epoch [109/120    avg_loss:0.190, val_acc:0.981]
Epoch [110/120    avg_loss:0.164, val_acc:0.981]
Epoch [111/120    avg_loss:0.181, val_acc:0.981]
Epoch [112/120    avg_loss:0.165, val_acc:0.988]
Epoch [113/120    avg_loss:0.176, val_acc:0.983]
Epoch [114/120    avg_loss:0.172, val_acc:0.985]
Epoch [115/120    avg_loss:0.170, val_acc:0.988]
Epoch [116/120    avg_loss:0.164, val_acc:0.985]
Epoch [117/120    avg_loss:0.182, val_acc:0.985]
Epoch [118/120    avg_loss:0.160, val_acc:0.988]
Epoch [119/120    avg_loss:0.153, val_acc:0.983]
Epoch [120/120    avg_loss:0.146, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   0 227   0   0   0   0   0   3   0   0   0   0]
 [  0   0   0   1 203  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 1.         0.95535714 0.99126638 0.89230769 0.83737024
 0.99756691 0.89385475 0.99870968 0.99680511 1.         0.99602649
 0.99668508 1.        ]

Kappa:
0.9821948588572414
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa723fdbdd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.155]
Epoch [2/120    avg_loss:2.576, val_acc:0.223]
Epoch [3/120    avg_loss:2.549, val_acc:0.221]
Epoch [4/120    avg_loss:2.521, val_acc:0.260]
Epoch [5/120    avg_loss:2.495, val_acc:0.290]
Epoch [6/120    avg_loss:2.475, val_acc:0.319]
Epoch [7/120    avg_loss:2.456, val_acc:0.338]
Epoch [8/120    avg_loss:2.432, val_acc:0.342]
Epoch [9/120    avg_loss:2.409, val_acc:0.354]
Epoch [10/120    avg_loss:2.398, val_acc:0.358]
Epoch [11/120    avg_loss:2.368, val_acc:0.371]
Epoch [12/120    avg_loss:2.342, val_acc:0.412]
Epoch [13/120    avg_loss:2.325, val_acc:0.502]
Epoch [14/120    avg_loss:2.296, val_acc:0.604]
Epoch [15/120    avg_loss:2.259, val_acc:0.598]
Epoch [16/120    avg_loss:2.214, val_acc:0.650]
Epoch [17/120    avg_loss:2.183, val_acc:0.665]
Epoch [18/120    avg_loss:2.144, val_acc:0.677]
Epoch [19/120    avg_loss:2.101, val_acc:0.706]
Epoch [20/120    avg_loss:2.041, val_acc:0.706]
Epoch [21/120    avg_loss:1.995, val_acc:0.717]
Epoch [22/120    avg_loss:1.946, val_acc:0.748]
Epoch [23/120    avg_loss:1.884, val_acc:0.752]
Epoch [24/120    avg_loss:1.842, val_acc:0.756]
Epoch [25/120    avg_loss:1.770, val_acc:0.765]
Epoch [26/120    avg_loss:1.738, val_acc:0.802]
Epoch [27/120    avg_loss:1.669, val_acc:0.802]
Epoch [28/120    avg_loss:1.631, val_acc:0.835]
Epoch [29/120    avg_loss:1.558, val_acc:0.821]
Epoch [30/120    avg_loss:1.494, val_acc:0.858]
Epoch [31/120    avg_loss:1.425, val_acc:0.854]
Epoch [32/120    avg_loss:1.371, val_acc:0.854]
Epoch [33/120    avg_loss:1.292, val_acc:0.856]
Epoch [34/120    avg_loss:1.242, val_acc:0.877]
Epoch [35/120    avg_loss:1.160, val_acc:0.881]
Epoch [36/120    avg_loss:1.134, val_acc:0.869]
Epoch [37/120    avg_loss:1.084, val_acc:0.873]
Epoch [38/120    avg_loss:1.085, val_acc:0.840]
Epoch [39/120    avg_loss:1.050, val_acc:0.877]
Epoch [40/120    avg_loss:0.989, val_acc:0.867]
Epoch [41/120    avg_loss:0.903, val_acc:0.894]
Epoch [42/120    avg_loss:0.868, val_acc:0.877]
Epoch [43/120    avg_loss:0.843, val_acc:0.873]
Epoch [44/120    avg_loss:0.784, val_acc:0.900]
Epoch [45/120    avg_loss:0.727, val_acc:0.912]
Epoch [46/120    avg_loss:0.725, val_acc:0.912]
Epoch [47/120    avg_loss:0.752, val_acc:0.885]
Epoch [48/120    avg_loss:0.753, val_acc:0.898]
Epoch [49/120    avg_loss:0.673, val_acc:0.900]
Epoch [50/120    avg_loss:0.649, val_acc:0.906]
Epoch [51/120    avg_loss:0.579, val_acc:0.906]
Epoch [52/120    avg_loss:0.565, val_acc:0.906]
Epoch [53/120    avg_loss:0.563, val_acc:0.917]
Epoch [54/120    avg_loss:0.532, val_acc:0.910]
Epoch [55/120    avg_loss:0.509, val_acc:0.904]
Epoch [56/120    avg_loss:0.475, val_acc:0.927]
Epoch [57/120    avg_loss:0.473, val_acc:0.910]
Epoch [58/120    avg_loss:0.499, val_acc:0.929]
Epoch [59/120    avg_loss:0.487, val_acc:0.921]
Epoch [60/120    avg_loss:0.463, val_acc:0.904]
Epoch [61/120    avg_loss:0.435, val_acc:0.921]
Epoch [62/120    avg_loss:0.430, val_acc:0.912]
Epoch [63/120    avg_loss:0.412, val_acc:0.935]
Epoch [64/120    avg_loss:0.452, val_acc:0.906]
Epoch [65/120    avg_loss:0.415, val_acc:0.938]
Epoch [66/120    avg_loss:0.388, val_acc:0.935]
Epoch [67/120    avg_loss:0.399, val_acc:0.940]
Epoch [68/120    avg_loss:0.368, val_acc:0.938]
Epoch [69/120    avg_loss:0.335, val_acc:0.923]
Epoch [70/120    avg_loss:0.309, val_acc:0.927]
Epoch [71/120    avg_loss:0.393, val_acc:0.944]
Epoch [72/120    avg_loss:0.365, val_acc:0.935]
Epoch [73/120    avg_loss:0.371, val_acc:0.931]
Epoch [74/120    avg_loss:0.324, val_acc:0.925]
Epoch [75/120    avg_loss:0.339, val_acc:0.925]
Epoch [76/120    avg_loss:0.346, val_acc:0.915]
Epoch [77/120    avg_loss:0.323, val_acc:0.931]
Epoch [78/120    avg_loss:0.291, val_acc:0.935]
Epoch [79/120    avg_loss:0.340, val_acc:0.938]
Epoch [80/120    avg_loss:0.294, val_acc:0.933]
Epoch [81/120    avg_loss:0.281, val_acc:0.935]
Epoch [82/120    avg_loss:0.277, val_acc:0.958]
Epoch [83/120    avg_loss:0.290, val_acc:0.938]
Epoch [84/120    avg_loss:0.283, val_acc:0.925]
Epoch [85/120    avg_loss:0.275, val_acc:0.948]
Epoch [86/120    avg_loss:0.276, val_acc:0.946]
Epoch [87/120    avg_loss:0.215, val_acc:0.950]
Epoch [88/120    avg_loss:0.241, val_acc:0.931]
Epoch [89/120    avg_loss:0.237, val_acc:0.952]
Epoch [90/120    avg_loss:0.214, val_acc:0.948]
Epoch [91/120    avg_loss:0.217, val_acc:0.944]
Epoch [92/120    avg_loss:0.212, val_acc:0.950]
Epoch [93/120    avg_loss:0.228, val_acc:0.942]
Epoch [94/120    avg_loss:0.207, val_acc:0.933]
Epoch [95/120    avg_loss:0.256, val_acc:0.956]
Epoch [96/120    avg_loss:0.180, val_acc:0.956]
Epoch [97/120    avg_loss:0.171, val_acc:0.956]
Epoch [98/120    avg_loss:0.170, val_acc:0.958]
Epoch [99/120    avg_loss:0.175, val_acc:0.956]
Epoch [100/120    avg_loss:0.160, val_acc:0.963]
Epoch [101/120    avg_loss:0.162, val_acc:0.960]
Epoch [102/120    avg_loss:0.154, val_acc:0.965]
Epoch [103/120    avg_loss:0.151, val_acc:0.960]
Epoch [104/120    avg_loss:0.157, val_acc:0.963]
Epoch [105/120    avg_loss:0.162, val_acc:0.960]
Epoch [106/120    avg_loss:0.176, val_acc:0.963]
Epoch [107/120    avg_loss:0.165, val_acc:0.963]
Epoch [108/120    avg_loss:0.159, val_acc:0.965]
Epoch [109/120    avg_loss:0.154, val_acc:0.963]
Epoch [110/120    avg_loss:0.155, val_acc:0.965]
Epoch [111/120    avg_loss:0.146, val_acc:0.965]
Epoch [112/120    avg_loss:0.162, val_acc:0.963]
Epoch [113/120    avg_loss:0.161, val_acc:0.960]
Epoch [114/120    avg_loss:0.147, val_acc:0.963]
Epoch [115/120    avg_loss:0.169, val_acc:0.963]
Epoch [116/120    avg_loss:0.150, val_acc:0.969]
Epoch [117/120    avg_loss:0.148, val_acc:0.967]
Epoch [118/120    avg_loss:0.148, val_acc:0.965]
Epoch [119/120    avg_loss:0.150, val_acc:0.967]
Epoch [120/120    avg_loss:0.142, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   1 220   3   1   0   4   0   1   0   0   0   0]
 [  0   0   0   0 199  28   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.272921108742

F1 scores:
[       nan 1.         0.94456763 0.97777778 0.89038031 0.86644951
 0.98522167 0.84444444 1.         0.99893276 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9807721225994447
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d532cce80>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.635, val_acc:0.083]
Epoch [2/120    avg_loss:2.617, val_acc:0.083]
Epoch [3/120    avg_loss:2.593, val_acc:0.210]
Epoch [4/120    avg_loss:2.574, val_acc:0.346]
Epoch [5/120    avg_loss:2.549, val_acc:0.352]
Epoch [6/120    avg_loss:2.528, val_acc:0.356]
Epoch [7/120    avg_loss:2.502, val_acc:0.358]
Epoch [8/120    avg_loss:2.481, val_acc:0.352]
Epoch [9/120    avg_loss:2.457, val_acc:0.365]
Epoch [10/120    avg_loss:2.421, val_acc:0.438]
Epoch [11/120    avg_loss:2.395, val_acc:0.475]
Epoch [12/120    avg_loss:2.361, val_acc:0.487]
Epoch [13/120    avg_loss:2.336, val_acc:0.481]
Epoch [14/120    avg_loss:2.307, val_acc:0.519]
Epoch [15/120    avg_loss:2.273, val_acc:0.521]
Epoch [16/120    avg_loss:2.238, val_acc:0.506]
Epoch [17/120    avg_loss:2.207, val_acc:0.535]
Epoch [18/120    avg_loss:2.169, val_acc:0.508]
Epoch [19/120    avg_loss:2.147, val_acc:0.519]
Epoch [20/120    avg_loss:2.089, val_acc:0.500]
Epoch [21/120    avg_loss:2.049, val_acc:0.504]
Epoch [22/120    avg_loss:2.014, val_acc:0.504]
Epoch [23/120    avg_loss:1.963, val_acc:0.519]
Epoch [24/120    avg_loss:1.907, val_acc:0.525]
Epoch [25/120    avg_loss:1.873, val_acc:0.529]
Epoch [26/120    avg_loss:1.840, val_acc:0.554]
Epoch [27/120    avg_loss:1.778, val_acc:0.562]
Epoch [28/120    avg_loss:1.723, val_acc:0.565]
Epoch [29/120    avg_loss:1.695, val_acc:0.577]
Epoch [30/120    avg_loss:1.624, val_acc:0.613]
Epoch [31/120    avg_loss:1.581, val_acc:0.637]
Epoch [32/120    avg_loss:1.509, val_acc:0.646]
Epoch [33/120    avg_loss:1.474, val_acc:0.640]
Epoch [34/120    avg_loss:1.431, val_acc:0.700]
Epoch [35/120    avg_loss:1.338, val_acc:0.671]
Epoch [36/120    avg_loss:1.295, val_acc:0.721]
Epoch [37/120    avg_loss:1.231, val_acc:0.723]
Epoch [38/120    avg_loss:1.193, val_acc:0.710]
Epoch [39/120    avg_loss:1.126, val_acc:0.735]
Epoch [40/120    avg_loss:1.104, val_acc:0.750]
Epoch [41/120    avg_loss:1.021, val_acc:0.781]
Epoch [42/120    avg_loss:0.966, val_acc:0.744]
Epoch [43/120    avg_loss:0.929, val_acc:0.848]
Epoch [44/120    avg_loss:0.852, val_acc:0.806]
Epoch [45/120    avg_loss:0.818, val_acc:0.842]
Epoch [46/120    avg_loss:0.796, val_acc:0.812]
Epoch [47/120    avg_loss:0.767, val_acc:0.846]
Epoch [48/120    avg_loss:0.715, val_acc:0.827]
Epoch [49/120    avg_loss:0.733, val_acc:0.865]
Epoch [50/120    avg_loss:0.684, val_acc:0.821]
Epoch [51/120    avg_loss:0.638, val_acc:0.906]
Epoch [52/120    avg_loss:0.614, val_acc:0.892]
Epoch [53/120    avg_loss:0.616, val_acc:0.894]
Epoch [54/120    avg_loss:0.594, val_acc:0.927]
Epoch [55/120    avg_loss:0.593, val_acc:0.923]
Epoch [56/120    avg_loss:0.548, val_acc:0.921]
Epoch [57/120    avg_loss:0.548, val_acc:0.917]
Epoch [58/120    avg_loss:0.557, val_acc:0.817]
Epoch [59/120    avg_loss:0.538, val_acc:0.921]
Epoch [60/120    avg_loss:0.504, val_acc:0.929]
Epoch [61/120    avg_loss:0.487, val_acc:0.940]
Epoch [62/120    avg_loss:0.458, val_acc:0.938]
Epoch [63/120    avg_loss:0.421, val_acc:0.933]
Epoch [64/120    avg_loss:0.409, val_acc:0.931]
Epoch [65/120    avg_loss:0.375, val_acc:0.912]
Epoch [66/120    avg_loss:0.495, val_acc:0.927]
Epoch [67/120    avg_loss:0.436, val_acc:0.927]
Epoch [68/120    avg_loss:0.424, val_acc:0.946]
Epoch [69/120    avg_loss:0.373, val_acc:0.942]
Epoch [70/120    avg_loss:0.356, val_acc:0.933]
Epoch [71/120    avg_loss:0.356, val_acc:0.904]
Epoch [72/120    avg_loss:0.388, val_acc:0.948]
Epoch [73/120    avg_loss:0.350, val_acc:0.954]
Epoch [74/120    avg_loss:0.339, val_acc:0.944]
Epoch [75/120    avg_loss:0.368, val_acc:0.942]
Epoch [76/120    avg_loss:0.368, val_acc:0.919]
Epoch [77/120    avg_loss:0.386, val_acc:0.956]
Epoch [78/120    avg_loss:0.316, val_acc:0.954]
Epoch [79/120    avg_loss:0.307, val_acc:0.963]
Epoch [80/120    avg_loss:0.310, val_acc:0.954]
Epoch [81/120    avg_loss:0.290, val_acc:0.967]
Epoch [82/120    avg_loss:0.249, val_acc:0.967]
Epoch [83/120    avg_loss:0.241, val_acc:0.973]
Epoch [84/120    avg_loss:0.250, val_acc:0.952]
Epoch [85/120    avg_loss:0.403, val_acc:0.948]
Epoch [86/120    avg_loss:0.339, val_acc:0.938]
Epoch [87/120    avg_loss:0.283, val_acc:0.967]
Epoch [88/120    avg_loss:0.256, val_acc:0.948]
Epoch [89/120    avg_loss:0.233, val_acc:0.946]
Epoch [90/120    avg_loss:0.214, val_acc:0.969]
Epoch [91/120    avg_loss:0.229, val_acc:0.975]
Epoch [92/120    avg_loss:0.203, val_acc:0.967]
Epoch [93/120    avg_loss:0.186, val_acc:0.967]
Epoch [94/120    avg_loss:0.225, val_acc:0.977]
Epoch [95/120    avg_loss:0.193, val_acc:0.979]
Epoch [96/120    avg_loss:0.189, val_acc:0.971]
Epoch [97/120    avg_loss:0.180, val_acc:0.942]
Epoch [98/120    avg_loss:0.184, val_acc:0.979]
Epoch [99/120    avg_loss:0.163, val_acc:0.975]
Epoch [100/120    avg_loss:0.175, val_acc:0.979]
Epoch [101/120    avg_loss:0.140, val_acc:0.973]
Epoch [102/120    avg_loss:0.158, val_acc:0.983]
Epoch [103/120    avg_loss:0.136, val_acc:0.985]
Epoch [104/120    avg_loss:0.130, val_acc:0.977]
Epoch [105/120    avg_loss:0.152, val_acc:0.975]
Epoch [106/120    avg_loss:0.203, val_acc:0.965]
Epoch [107/120    avg_loss:0.178, val_acc:0.988]
Epoch [108/120    avg_loss:0.165, val_acc:0.979]
Epoch [109/120    avg_loss:0.123, val_acc:0.990]
Epoch [110/120    avg_loss:0.131, val_acc:0.973]
Epoch [111/120    avg_loss:0.127, val_acc:0.981]
Epoch [112/120    avg_loss:0.113, val_acc:0.983]
Epoch [113/120    avg_loss:0.150, val_acc:0.944]
Epoch [114/120    avg_loss:0.193, val_acc:0.975]
Epoch [115/120    avg_loss:0.158, val_acc:0.965]
Epoch [116/120    avg_loss:0.144, val_acc:0.975]
Epoch [117/120    avg_loss:0.167, val_acc:0.940]
Epoch [118/120    avg_loss:0.146, val_acc:0.977]
Epoch [119/120    avg_loss:0.154, val_acc:0.973]
Epoch [120/120    avg_loss:0.132, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 201   0   0   0   0  18   0   0   0   0   0   0]
 [  0   0   2 173  35  18   0   0   0   2   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   6   0   0   0   0   0   0 382   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   1   0   0   0   0   1 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.39872068230277

F1 scores:
[       nan 0.99563953 0.92626728 0.85856079 0.86637931 0.86486486
 0.99757869 0.84536082 0.99220779 0.9978678  1.         0.9986755
 0.99778761 1.        ]

Kappa:
0.9710431588412648
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f799ee54dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27082==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.052]
Epoch [2/120    avg_loss:2.598, val_acc:0.104]
Epoch [3/120    avg_loss:2.577, val_acc:0.148]
Epoch [4/120    avg_loss:2.558, val_acc:0.163]
Epoch [5/120    avg_loss:2.537, val_acc:0.237]
Epoch [6/120    avg_loss:2.509, val_acc:0.352]
Epoch [7/120    avg_loss:2.487, val_acc:0.373]
Epoch [8/120    avg_loss:2.460, val_acc:0.423]
Epoch [9/120    avg_loss:2.428, val_acc:0.440]
Epoch [10/120    avg_loss:2.398, val_acc:0.467]
Epoch [11/120    avg_loss:2.377, val_acc:0.481]
Epoch [12/120    avg_loss:2.337, val_acc:0.487]
Epoch [13/120    avg_loss:2.303, val_acc:0.487]
Epoch [14/120    avg_loss:2.267, val_acc:0.510]
Epoch [15/120    avg_loss:2.240, val_acc:0.519]
Epoch [16/120    avg_loss:2.193, val_acc:0.525]
Epoch [17/120    avg_loss:2.139, val_acc:0.537]
Epoch [18/120    avg_loss:2.100, val_acc:0.548]
Epoch [19/120    avg_loss:2.055, val_acc:0.575]
Epoch [20/120    avg_loss:2.031, val_acc:0.573]
Epoch [21/120    avg_loss:1.997, val_acc:0.613]
Epoch [22/120    avg_loss:1.959, val_acc:0.627]
Epoch [23/120    avg_loss:1.902, val_acc:0.644]
Epoch [24/120    avg_loss:1.862, val_acc:0.652]
Epoch [25/120    avg_loss:1.817, val_acc:0.656]
Epoch [26/120    avg_loss:1.783, val_acc:0.669]
Epoch [27/120    avg_loss:1.743, val_acc:0.677]
Epoch [28/120    avg_loss:1.697, val_acc:0.679]
Epoch [29/120    avg_loss:1.648, val_acc:0.679]
Epoch [30/120    avg_loss:1.587, val_acc:0.706]
Epoch [31/120    avg_loss:1.547, val_acc:0.679]
Epoch [32/120    avg_loss:1.460, val_acc:0.683]
Epoch [33/120    avg_loss:1.421, val_acc:0.708]
Epoch [34/120    avg_loss:1.415, val_acc:0.735]
Epoch [35/120    avg_loss:1.347, val_acc:0.715]
Epoch [36/120    avg_loss:1.305, val_acc:0.746]
Epoch [37/120    avg_loss:1.251, val_acc:0.738]
Epoch [38/120    avg_loss:1.204, val_acc:0.804]
Epoch [39/120    avg_loss:1.182, val_acc:0.819]
Epoch [40/120    avg_loss:1.145, val_acc:0.769]
Epoch [41/120    avg_loss:1.046, val_acc:0.869]
Epoch [42/120    avg_loss:1.029, val_acc:0.821]
Epoch [43/120    avg_loss:0.977, val_acc:0.898]
Epoch [44/120    avg_loss:0.907, val_acc:0.863]
Epoch [45/120    avg_loss:0.882, val_acc:0.887]
Epoch [46/120    avg_loss:0.850, val_acc:0.898]
Epoch [47/120    avg_loss:0.832, val_acc:0.910]
Epoch [48/120    avg_loss:0.761, val_acc:0.933]
Epoch [49/120    avg_loss:0.773, val_acc:0.896]
Epoch [50/120    avg_loss:0.710, val_acc:0.925]
Epoch [51/120    avg_loss:0.697, val_acc:0.902]
Epoch [52/120    avg_loss:0.653, val_acc:0.890]
Epoch [53/120    avg_loss:0.624, val_acc:0.898]
Epoch [54/120    avg_loss:0.562, val_acc:0.935]
Epoch [55/120    avg_loss:0.574, val_acc:0.938]
Epoch [56/120    avg_loss:0.594, val_acc:0.871]
Epoch [57/120    avg_loss:0.555, val_acc:0.929]
Epoch [58/120    avg_loss:0.511, val_acc:0.931]
Epoch [59/120    avg_loss:0.518, val_acc:0.944]
Epoch [60/120    avg_loss:0.455, val_acc:0.927]
Epoch [61/120    avg_loss:0.447, val_acc:0.925]
Epoch [62/120    avg_loss:0.417, val_acc:0.956]
Epoch [63/120    avg_loss:0.393, val_acc:0.931]
Epoch [64/120    avg_loss:0.430, val_acc:0.867]
Epoch [65/120    avg_loss:0.438, val_acc:0.948]
Epoch [66/120    avg_loss:0.388, val_acc:0.954]
Epoch [67/120    avg_loss:0.391, val_acc:0.944]
Epoch [68/120    avg_loss:0.354, val_acc:0.954]
Epoch [69/120    avg_loss:0.343, val_acc:0.952]
Epoch [70/120    avg_loss:0.330, val_acc:0.963]
Epoch [71/120    avg_loss:0.315, val_acc:0.929]
Epoch [72/120    avg_loss:0.313, val_acc:0.942]
Epoch [73/120    avg_loss:0.289, val_acc:0.946]
Epoch [74/120    avg_loss:0.319, val_acc:0.958]
Epoch [75/120    avg_loss:0.274, val_acc:0.960]
Epoch [76/120    avg_loss:0.270, val_acc:0.969]
Epoch [77/120    avg_loss:0.289, val_acc:0.958]
Epoch [78/120    avg_loss:0.268, val_acc:0.975]
Epoch [79/120    avg_loss:0.253, val_acc:0.973]
Epoch [80/120    avg_loss:0.266, val_acc:0.977]
Epoch [81/120    avg_loss:0.254, val_acc:0.960]
Epoch [82/120    avg_loss:0.277, val_acc:0.958]
Epoch [83/120    avg_loss:0.277, val_acc:0.977]
Epoch [84/120    avg_loss:0.218, val_acc:0.952]
Epoch [85/120    avg_loss:0.243, val_acc:0.965]
Epoch [86/120    avg_loss:0.231, val_acc:0.973]
Epoch [87/120    avg_loss:0.224, val_acc:0.973]
Epoch [88/120    avg_loss:0.240, val_acc:0.979]
Epoch [89/120    avg_loss:0.196, val_acc:0.977]
Epoch [90/120    avg_loss:0.199, val_acc:0.981]
Epoch [91/120    avg_loss:0.195, val_acc:0.985]
Epoch [92/120    avg_loss:0.192, val_acc:0.988]
Epoch [93/120    avg_loss:0.177, val_acc:0.983]
Epoch [94/120    avg_loss:0.165, val_acc:0.979]
Epoch [95/120    avg_loss:0.156, val_acc:0.988]
Epoch [96/120    avg_loss:0.131, val_acc:0.981]
Epoch [97/120    avg_loss:0.153, val_acc:0.981]
Epoch [98/120    avg_loss:0.157, val_acc:0.985]
Epoch [99/120    avg_loss:0.169, val_acc:0.981]
Epoch [100/120    avg_loss:0.213, val_acc:0.981]
Epoch [101/120    avg_loss:0.156, val_acc:0.971]
Epoch [102/120    avg_loss:0.162, val_acc:0.988]
Epoch [103/120    avg_loss:0.127, val_acc:0.992]
Epoch [104/120    avg_loss:0.143, val_acc:0.985]
Epoch [105/120    avg_loss:0.124, val_acc:0.983]
Epoch [106/120    avg_loss:0.127, val_acc:0.985]
Epoch [107/120    avg_loss:0.151, val_acc:0.988]
Epoch [108/120    avg_loss:0.112, val_acc:0.985]
Epoch [109/120    avg_loss:0.111, val_acc:0.988]
Epoch [110/120    avg_loss:0.121, val_acc:0.990]
Epoch [111/120    avg_loss:0.103, val_acc:0.988]
Epoch [112/120    avg_loss:0.100, val_acc:0.992]
Epoch [113/120    avg_loss:0.115, val_acc:0.988]
Epoch [114/120    avg_loss:0.126, val_acc:0.990]
Epoch [115/120    avg_loss:0.121, val_acc:0.988]
Epoch [116/120    avg_loss:0.132, val_acc:0.983]
Epoch [117/120    avg_loss:0.135, val_acc:0.971]
Epoch [118/120    avg_loss:0.150, val_acc:0.971]
Epoch [119/120    avg_loss:0.152, val_acc:0.983]
Epoch [120/120    avg_loss:0.244, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   6 198   0   0   0  15   0   0   0   0   0   0   0]
 [  0   0   0 221   0   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0 120  65  42   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  21   0   0   0  31  42   0   0   0   0   0   0]
 [  0  32   0   0   1   0   0   0 355   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
95.11727078891258

F1 scores:
[       nan 0.97301136 0.90410959 0.98004435 0.68965517 0.7967033
 0.824      0.61764706 0.95558546 1.         1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9456106534874843
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28babd9e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.146]
Epoch [2/120    avg_loss:2.594, val_acc:0.142]
Epoch [3/120    avg_loss:2.563, val_acc:0.138]
Epoch [4/120    avg_loss:2.542, val_acc:0.283]
Epoch [5/120    avg_loss:2.515, val_acc:0.302]
Epoch [6/120    avg_loss:2.497, val_acc:0.304]
Epoch [7/120    avg_loss:2.473, val_acc:0.315]
Epoch [8/120    avg_loss:2.450, val_acc:0.319]
Epoch [9/120    avg_loss:2.419, val_acc:0.312]
Epoch [10/120    avg_loss:2.397, val_acc:0.315]
Epoch [11/120    avg_loss:2.366, val_acc:0.308]
Epoch [12/120    avg_loss:2.335, val_acc:0.310]
Epoch [13/120    avg_loss:2.307, val_acc:0.317]
Epoch [14/120    avg_loss:2.265, val_acc:0.308]
Epoch [15/120    avg_loss:2.231, val_acc:0.304]
Epoch [16/120    avg_loss:2.223, val_acc:0.342]
Epoch [17/120    avg_loss:2.147, val_acc:0.321]
Epoch [18/120    avg_loss:2.109, val_acc:0.379]
Epoch [19/120    avg_loss:2.104, val_acc:0.377]
Epoch [20/120    avg_loss:2.035, val_acc:0.425]
Epoch [21/120    avg_loss:2.004, val_acc:0.431]
Epoch [22/120    avg_loss:1.964, val_acc:0.487]
Epoch [23/120    avg_loss:1.940, val_acc:0.504]
Epoch [24/120    avg_loss:1.907, val_acc:0.533]
Epoch [25/120    avg_loss:1.861, val_acc:0.556]
Epoch [26/120    avg_loss:1.851, val_acc:0.600]
Epoch [27/120    avg_loss:1.792, val_acc:0.583]
Epoch [28/120    avg_loss:1.737, val_acc:0.652]
Epoch [29/120    avg_loss:1.685, val_acc:0.658]
Epoch [30/120    avg_loss:1.628, val_acc:0.675]
Epoch [31/120    avg_loss:1.591, val_acc:0.675]
Epoch [32/120    avg_loss:1.520, val_acc:0.694]
Epoch [33/120    avg_loss:1.509, val_acc:0.696]
Epoch [34/120    avg_loss:1.424, val_acc:0.781]
Epoch [35/120    avg_loss:1.382, val_acc:0.708]
Epoch [36/120    avg_loss:1.319, val_acc:0.731]
Epoch [37/120    avg_loss:1.273, val_acc:0.740]
Epoch [38/120    avg_loss:1.251, val_acc:0.773]
Epoch [39/120    avg_loss:1.200, val_acc:0.760]
Epoch [40/120    avg_loss:1.161, val_acc:0.850]
Epoch [41/120    avg_loss:1.070, val_acc:0.748]
Epoch [42/120    avg_loss:1.019, val_acc:0.842]
Epoch [43/120    avg_loss:0.945, val_acc:0.846]
Epoch [44/120    avg_loss:0.888, val_acc:0.794]
Epoch [45/120    avg_loss:0.926, val_acc:0.787]
Epoch [46/120    avg_loss:0.929, val_acc:0.852]
Epoch [47/120    avg_loss:0.819, val_acc:0.892]
Epoch [48/120    avg_loss:0.747, val_acc:0.908]
Epoch [49/120    avg_loss:0.763, val_acc:0.879]
Epoch [50/120    avg_loss:0.699, val_acc:0.912]
Epoch [51/120    avg_loss:0.671, val_acc:0.915]
Epoch [52/120    avg_loss:0.654, val_acc:0.912]
Epoch [53/120    avg_loss:0.595, val_acc:0.921]
Epoch [54/120    avg_loss:0.603, val_acc:0.931]
Epoch [55/120    avg_loss:0.582, val_acc:0.910]
Epoch [56/120    avg_loss:0.540, val_acc:0.938]
Epoch [57/120    avg_loss:0.501, val_acc:0.925]
Epoch [58/120    avg_loss:0.456, val_acc:0.944]
Epoch [59/120    avg_loss:0.451, val_acc:0.954]
Epoch [60/120    avg_loss:0.676, val_acc:0.883]
Epoch [61/120    avg_loss:0.577, val_acc:0.925]
Epoch [62/120    avg_loss:0.503, val_acc:0.925]
Epoch [63/120    avg_loss:0.433, val_acc:0.925]
Epoch [64/120    avg_loss:0.424, val_acc:0.929]
Epoch [65/120    avg_loss:0.369, val_acc:0.931]
Epoch [66/120    avg_loss:0.382, val_acc:0.938]
Epoch [67/120    avg_loss:0.358, val_acc:0.946]
Epoch [68/120    avg_loss:0.322, val_acc:0.956]
Epoch [69/120    avg_loss:0.342, val_acc:0.948]
Epoch [70/120    avg_loss:0.350, val_acc:0.944]
Epoch [71/120    avg_loss:0.328, val_acc:0.948]
Epoch [72/120    avg_loss:0.352, val_acc:0.931]
Epoch [73/120    avg_loss:0.348, val_acc:0.929]
Epoch [74/120    avg_loss:0.315, val_acc:0.965]
Epoch [75/120    avg_loss:0.268, val_acc:0.944]
Epoch [76/120    avg_loss:0.289, val_acc:0.935]
Epoch [77/120    avg_loss:0.271, val_acc:0.946]
Epoch [78/120    avg_loss:0.275, val_acc:0.933]
Epoch [79/120    avg_loss:0.313, val_acc:0.927]
Epoch [80/120    avg_loss:0.298, val_acc:0.938]
Epoch [81/120    avg_loss:0.254, val_acc:0.956]
Epoch [82/120    avg_loss:0.236, val_acc:0.965]
Epoch [83/120    avg_loss:0.245, val_acc:0.942]
Epoch [84/120    avg_loss:0.246, val_acc:0.967]
Epoch [85/120    avg_loss:0.225, val_acc:0.952]
Epoch [86/120    avg_loss:0.231, val_acc:0.973]
Epoch [87/120    avg_loss:0.194, val_acc:0.973]
Epoch [88/120    avg_loss:0.182, val_acc:0.952]
Epoch [89/120    avg_loss:0.226, val_acc:0.967]
Epoch [90/120    avg_loss:0.234, val_acc:0.963]
Epoch [91/120    avg_loss:0.201, val_acc:0.956]
Epoch [92/120    avg_loss:0.252, val_acc:0.954]
Epoch [93/120    avg_loss:0.214, val_acc:0.967]
Epoch [94/120    avg_loss:0.188, val_acc:0.948]
Epoch [95/120    avg_loss:0.166, val_acc:0.960]
Epoch [96/120    avg_loss:0.156, val_acc:0.967]
Epoch [97/120    avg_loss:0.177, val_acc:0.975]
Epoch [98/120    avg_loss:0.214, val_acc:0.965]
Epoch [99/120    avg_loss:0.194, val_acc:0.919]
Epoch [100/120    avg_loss:0.188, val_acc:0.971]
Epoch [101/120    avg_loss:0.165, val_acc:0.969]
Epoch [102/120    avg_loss:0.181, val_acc:0.971]
Epoch [103/120    avg_loss:0.195, val_acc:0.952]
Epoch [104/120    avg_loss:0.255, val_acc:0.963]
Epoch [105/120    avg_loss:0.175, val_acc:0.965]
Epoch [106/120    avg_loss:0.150, val_acc:0.973]
Epoch [107/120    avg_loss:0.161, val_acc:0.973]
Epoch [108/120    avg_loss:0.153, val_acc:0.975]
Epoch [109/120    avg_loss:0.145, val_acc:0.963]
Epoch [110/120    avg_loss:0.201, val_acc:0.963]
Epoch [111/120    avg_loss:0.194, val_acc:0.963]
Epoch [112/120    avg_loss:0.139, val_acc:0.967]
Epoch [113/120    avg_loss:0.174, val_acc:0.969]
Epoch [114/120    avg_loss:0.194, val_acc:0.960]
Epoch [115/120    avg_loss:0.190, val_acc:0.960]
Epoch [116/120    avg_loss:0.186, val_acc:0.979]
Epoch [117/120    avg_loss:0.121, val_acc:0.977]
Epoch [118/120    avg_loss:0.139, val_acc:0.967]
Epoch [119/120    avg_loss:0.148, val_acc:0.956]
Epoch [120/120    avg_loss:0.119, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 638   0   0   0   0  47   0   0   0   0   0   0   0]
 [  0   0 200   0   0   0   0  19   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.29211087420042

F1 scores:
[       nan 0.96447468 0.94339623 0.97777778 0.87982833 0.84027778
 0.89760349 0.88118812 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9698893286351054
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd78296ce10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.640, val_acc:0.108]
Epoch [2/120    avg_loss:2.612, val_acc:0.220]
Epoch [3/120    avg_loss:2.589, val_acc:0.371]
Epoch [4/120    avg_loss:2.567, val_acc:0.396]
Epoch [5/120    avg_loss:2.546, val_acc:0.375]
Epoch [6/120    avg_loss:2.524, val_acc:0.381]
Epoch [7/120    avg_loss:2.506, val_acc:0.390]
Epoch [8/120    avg_loss:2.481, val_acc:0.417]
Epoch [9/120    avg_loss:2.463, val_acc:0.421]
Epoch [10/120    avg_loss:2.437, val_acc:0.448]
Epoch [11/120    avg_loss:2.417, val_acc:0.465]
Epoch [12/120    avg_loss:2.398, val_acc:0.498]
Epoch [13/120    avg_loss:2.361, val_acc:0.540]
Epoch [14/120    avg_loss:2.345, val_acc:0.546]
Epoch [15/120    avg_loss:2.316, val_acc:0.544]
Epoch [16/120    avg_loss:2.289, val_acc:0.544]
Epoch [17/120    avg_loss:2.252, val_acc:0.527]
Epoch [18/120    avg_loss:2.220, val_acc:0.527]
Epoch [19/120    avg_loss:2.188, val_acc:0.515]
Epoch [20/120    avg_loss:2.163, val_acc:0.542]
Epoch [21/120    avg_loss:2.132, val_acc:0.531]
Epoch [22/120    avg_loss:2.086, val_acc:0.498]
Epoch [23/120    avg_loss:2.050, val_acc:0.567]
Epoch [24/120    avg_loss:1.997, val_acc:0.577]
Epoch [25/120    avg_loss:1.931, val_acc:0.637]
Epoch [26/120    avg_loss:1.885, val_acc:0.700]
Epoch [27/120    avg_loss:1.832, val_acc:0.704]
Epoch [28/120    avg_loss:1.767, val_acc:0.783]
Epoch [29/120    avg_loss:1.731, val_acc:0.779]
Epoch [30/120    avg_loss:1.705, val_acc:0.671]
Epoch [31/120    avg_loss:1.637, val_acc:0.781]
Epoch [32/120    avg_loss:1.559, val_acc:0.806]
Epoch [33/120    avg_loss:1.514, val_acc:0.835]
Epoch [34/120    avg_loss:1.438, val_acc:0.885]
Epoch [35/120    avg_loss:1.389, val_acc:0.846]
Epoch [36/120    avg_loss:1.354, val_acc:0.840]
Epoch [37/120    avg_loss:1.290, val_acc:0.804]
Epoch [38/120    avg_loss:1.211, val_acc:0.825]
Epoch [39/120    avg_loss:1.127, val_acc:0.904]
Epoch [40/120    avg_loss:1.089, val_acc:0.885]
Epoch [41/120    avg_loss:1.004, val_acc:0.858]
Epoch [42/120    avg_loss:0.979, val_acc:0.875]
Epoch [43/120    avg_loss:0.970, val_acc:0.885]
Epoch [44/120    avg_loss:0.862, val_acc:0.896]
Epoch [45/120    avg_loss:0.841, val_acc:0.885]
Epoch [46/120    avg_loss:0.773, val_acc:0.908]
Epoch [47/120    avg_loss:0.757, val_acc:0.904]
Epoch [48/120    avg_loss:0.709, val_acc:0.898]
Epoch [49/120    avg_loss:0.650, val_acc:0.919]
Epoch [50/120    avg_loss:0.678, val_acc:0.873]
Epoch [51/120    avg_loss:0.665, val_acc:0.904]
Epoch [52/120    avg_loss:0.618, val_acc:0.894]
Epoch [53/120    avg_loss:0.563, val_acc:0.902]
Epoch [54/120    avg_loss:0.576, val_acc:0.894]
Epoch [55/120    avg_loss:0.583, val_acc:0.923]
Epoch [56/120    avg_loss:0.524, val_acc:0.890]
Epoch [57/120    avg_loss:0.509, val_acc:0.910]
Epoch [58/120    avg_loss:0.467, val_acc:0.917]
Epoch [59/120    avg_loss:0.485, val_acc:0.919]
Epoch [60/120    avg_loss:0.458, val_acc:0.915]
Epoch [61/120    avg_loss:0.452, val_acc:0.921]
Epoch [62/120    avg_loss:0.400, val_acc:0.925]
Epoch [63/120    avg_loss:0.397, val_acc:0.948]
Epoch [64/120    avg_loss:0.397, val_acc:0.931]
Epoch [65/120    avg_loss:0.374, val_acc:0.921]
Epoch [66/120    avg_loss:0.348, val_acc:0.944]
Epoch [67/120    avg_loss:0.331, val_acc:0.946]
Epoch [68/120    avg_loss:0.339, val_acc:0.933]
Epoch [69/120    avg_loss:0.352, val_acc:0.929]
Epoch [70/120    avg_loss:0.325, val_acc:0.944]
Epoch [71/120    avg_loss:0.346, val_acc:0.948]
Epoch [72/120    avg_loss:0.305, val_acc:0.912]
Epoch [73/120    avg_loss:0.348, val_acc:0.912]
Epoch [74/120    avg_loss:0.338, val_acc:0.948]
Epoch [75/120    avg_loss:0.306, val_acc:0.946]
Epoch [76/120    avg_loss:0.269, val_acc:0.925]
Epoch [77/120    avg_loss:0.289, val_acc:0.946]
Epoch [78/120    avg_loss:0.244, val_acc:0.963]
Epoch [79/120    avg_loss:0.324, val_acc:0.890]
Epoch [80/120    avg_loss:0.304, val_acc:0.906]
Epoch [81/120    avg_loss:0.359, val_acc:0.956]
Epoch [82/120    avg_loss:0.284, val_acc:0.956]
Epoch [83/120    avg_loss:0.274, val_acc:0.963]
Epoch [84/120    avg_loss:0.248, val_acc:0.954]
Epoch [85/120    avg_loss:0.228, val_acc:0.952]
Epoch [86/120    avg_loss:0.224, val_acc:0.971]
Epoch [87/120    avg_loss:0.212, val_acc:0.963]
Epoch [88/120    avg_loss:0.217, val_acc:0.963]
Epoch [89/120    avg_loss:0.250, val_acc:0.963]
Epoch [90/120    avg_loss:0.192, val_acc:0.946]
Epoch [91/120    avg_loss:0.218, val_acc:0.971]
Epoch [92/120    avg_loss:0.180, val_acc:0.952]
Epoch [93/120    avg_loss:0.178, val_acc:0.973]
Epoch [94/120    avg_loss:0.172, val_acc:0.958]
Epoch [95/120    avg_loss:0.153, val_acc:0.969]
Epoch [96/120    avg_loss:0.139, val_acc:0.958]
Epoch [97/120    avg_loss:0.139, val_acc:0.967]
Epoch [98/120    avg_loss:0.128, val_acc:0.973]
Epoch [99/120    avg_loss:0.135, val_acc:0.971]
Epoch [100/120    avg_loss:0.120, val_acc:0.967]
Epoch [101/120    avg_loss:0.124, val_acc:0.965]
Epoch [102/120    avg_loss:0.140, val_acc:0.956]
Epoch [103/120    avg_loss:0.131, val_acc:0.975]
Epoch [104/120    avg_loss:0.143, val_acc:0.969]
Epoch [105/120    avg_loss:0.120, val_acc:0.969]
Epoch [106/120    avg_loss:0.121, val_acc:0.979]
Epoch [107/120    avg_loss:0.135, val_acc:0.977]
Epoch [108/120    avg_loss:0.165, val_acc:0.975]
Epoch [109/120    avg_loss:0.185, val_acc:0.963]
Epoch [110/120    avg_loss:0.163, val_acc:0.942]
Epoch [111/120    avg_loss:0.173, val_acc:0.977]
Epoch [112/120    avg_loss:0.146, val_acc:0.952]
Epoch [113/120    avg_loss:0.128, val_acc:0.979]
Epoch [114/120    avg_loss:0.116, val_acc:0.975]
Epoch [115/120    avg_loss:0.130, val_acc:0.979]
Epoch [116/120    avg_loss:0.115, val_acc:0.979]
Epoch [117/120    avg_loss:0.103, val_acc:0.979]
Epoch [118/120    avg_loss:0.102, val_acc:0.977]
Epoch [119/120    avg_loss:0.115, val_acc:0.973]
Epoch [120/120    avg_loss:0.144, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 188  36   5   0   1   0   0   0   0   0   0]
 [  0   0   0   0 201  25   1   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.14498933901919

F1 scores:
[       nan 0.99927061 0.97333333 0.89952153 0.85714286 0.88888889
 0.99757869 0.92655367 0.99870968 1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9793484500000228
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a41c21e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.019]
Epoch [2/120    avg_loss:2.615, val_acc:0.079]
Epoch [3/120    avg_loss:2.585, val_acc:0.129]
Epoch [4/120    avg_loss:2.561, val_acc:0.273]
Epoch [5/120    avg_loss:2.533, val_acc:0.294]
Epoch [6/120    avg_loss:2.507, val_acc:0.296]
Epoch [7/120    avg_loss:2.485, val_acc:0.298]
Epoch [8/120    avg_loss:2.456, val_acc:0.321]
Epoch [9/120    avg_loss:2.436, val_acc:0.333]
Epoch [10/120    avg_loss:2.410, val_acc:0.354]
Epoch [11/120    avg_loss:2.380, val_acc:0.379]
Epoch [12/120    avg_loss:2.349, val_acc:0.398]
Epoch [13/120    avg_loss:2.318, val_acc:0.417]
Epoch [14/120    avg_loss:2.297, val_acc:0.410]
Epoch [15/120    avg_loss:2.258, val_acc:0.408]
Epoch [16/120    avg_loss:2.214, val_acc:0.415]
Epoch [17/120    avg_loss:2.180, val_acc:0.417]
Epoch [18/120    avg_loss:2.124, val_acc:0.425]
Epoch [19/120    avg_loss:2.091, val_acc:0.421]
Epoch [20/120    avg_loss:2.046, val_acc:0.423]
Epoch [21/120    avg_loss:2.017, val_acc:0.442]
Epoch [22/120    avg_loss:1.962, val_acc:0.465]
Epoch [23/120    avg_loss:1.919, val_acc:0.510]
Epoch [24/120    avg_loss:1.890, val_acc:0.525]
Epoch [25/120    avg_loss:1.845, val_acc:0.548]
Epoch [26/120    avg_loss:1.793, val_acc:0.540]
Epoch [27/120    avg_loss:1.745, val_acc:0.577]
Epoch [28/120    avg_loss:1.675, val_acc:0.550]
Epoch [29/120    avg_loss:1.635, val_acc:0.604]
Epoch [30/120    avg_loss:1.575, val_acc:0.581]
Epoch [31/120    avg_loss:1.551, val_acc:0.583]
Epoch [32/120    avg_loss:1.499, val_acc:0.588]
Epoch [33/120    avg_loss:1.449, val_acc:0.644]
Epoch [34/120    avg_loss:1.441, val_acc:0.631]
Epoch [35/120    avg_loss:1.331, val_acc:0.667]
Epoch [36/120    avg_loss:1.291, val_acc:0.648]
Epoch [37/120    avg_loss:1.290, val_acc:0.667]
Epoch [38/120    avg_loss:1.239, val_acc:0.694]
Epoch [39/120    avg_loss:1.188, val_acc:0.692]
Epoch [40/120    avg_loss:1.133, val_acc:0.713]
Epoch [41/120    avg_loss:1.096, val_acc:0.698]
Epoch [42/120    avg_loss:1.067, val_acc:0.765]
Epoch [43/120    avg_loss:1.014, val_acc:0.865]
Epoch [44/120    avg_loss:1.068, val_acc:0.794]
Epoch [45/120    avg_loss:1.007, val_acc:0.787]
Epoch [46/120    avg_loss:0.928, val_acc:0.869]
Epoch [47/120    avg_loss:0.882, val_acc:0.910]
Epoch [48/120    avg_loss:0.843, val_acc:0.869]
Epoch [49/120    avg_loss:0.801, val_acc:0.885]
Epoch [50/120    avg_loss:0.769, val_acc:0.885]
Epoch [51/120    avg_loss:0.804, val_acc:0.896]
Epoch [52/120    avg_loss:0.730, val_acc:0.887]
Epoch [53/120    avg_loss:0.660, val_acc:0.927]
Epoch [54/120    avg_loss:0.652, val_acc:0.912]
Epoch [55/120    avg_loss:0.590, val_acc:0.927]
Epoch [56/120    avg_loss:0.584, val_acc:0.927]
Epoch [57/120    avg_loss:0.589, val_acc:0.919]
Epoch [58/120    avg_loss:0.559, val_acc:0.912]
Epoch [59/120    avg_loss:0.593, val_acc:0.917]
Epoch [60/120    avg_loss:0.507, val_acc:0.952]
Epoch [61/120    avg_loss:0.491, val_acc:0.919]
Epoch [62/120    avg_loss:0.474, val_acc:0.952]
Epoch [63/120    avg_loss:0.432, val_acc:0.935]
Epoch [64/120    avg_loss:0.454, val_acc:0.935]
Epoch [65/120    avg_loss:0.419, val_acc:0.958]
Epoch [66/120    avg_loss:0.396, val_acc:0.942]
Epoch [67/120    avg_loss:0.443, val_acc:0.944]
Epoch [68/120    avg_loss:0.388, val_acc:0.942]
Epoch [69/120    avg_loss:0.452, val_acc:0.960]
Epoch [70/120    avg_loss:0.390, val_acc:0.946]
Epoch [71/120    avg_loss:0.393, val_acc:0.927]
Epoch [72/120    avg_loss:0.334, val_acc:0.952]
Epoch [73/120    avg_loss:0.381, val_acc:0.969]
Epoch [74/120    avg_loss:0.299, val_acc:0.958]
Epoch [75/120    avg_loss:0.332, val_acc:0.938]
Epoch [76/120    avg_loss:0.345, val_acc:0.948]
Epoch [77/120    avg_loss:0.310, val_acc:0.942]
Epoch [78/120    avg_loss:0.296, val_acc:0.965]
Epoch [79/120    avg_loss:0.274, val_acc:0.965]
Epoch [80/120    avg_loss:0.250, val_acc:0.942]
Epoch [81/120    avg_loss:0.301, val_acc:0.940]
Epoch [82/120    avg_loss:0.259, val_acc:0.960]
Epoch [83/120    avg_loss:0.241, val_acc:0.956]
Epoch [84/120    avg_loss:0.260, val_acc:0.967]
Epoch [85/120    avg_loss:0.279, val_acc:0.923]
Epoch [86/120    avg_loss:0.234, val_acc:0.950]
Epoch [87/120    avg_loss:0.230, val_acc:0.960]
Epoch [88/120    avg_loss:0.190, val_acc:0.963]
Epoch [89/120    avg_loss:0.202, val_acc:0.963]
Epoch [90/120    avg_loss:0.188, val_acc:0.969]
Epoch [91/120    avg_loss:0.170, val_acc:0.969]
Epoch [92/120    avg_loss:0.173, val_acc:0.969]
Epoch [93/120    avg_loss:0.175, val_acc:0.971]
Epoch [94/120    avg_loss:0.185, val_acc:0.969]
Epoch [95/120    avg_loss:0.178, val_acc:0.975]
Epoch [96/120    avg_loss:0.173, val_acc:0.971]
Epoch [97/120    avg_loss:0.162, val_acc:0.973]
Epoch [98/120    avg_loss:0.176, val_acc:0.975]
Epoch [99/120    avg_loss:0.164, val_acc:0.977]
Epoch [100/120    avg_loss:0.157, val_acc:0.975]
Epoch [101/120    avg_loss:0.158, val_acc:0.971]
Epoch [102/120    avg_loss:0.167, val_acc:0.975]
Epoch [103/120    avg_loss:0.161, val_acc:0.977]
Epoch [104/120    avg_loss:0.154, val_acc:0.975]
Epoch [105/120    avg_loss:0.163, val_acc:0.975]
Epoch [106/120    avg_loss:0.155, val_acc:0.975]
Epoch [107/120    avg_loss:0.152, val_acc:0.975]
Epoch [108/120    avg_loss:0.150, val_acc:0.977]
Epoch [109/120    avg_loss:0.176, val_acc:0.977]
Epoch [110/120    avg_loss:0.172, val_acc:0.981]
Epoch [111/120    avg_loss:0.143, val_acc:0.979]
Epoch [112/120    avg_loss:0.142, val_acc:0.977]
Epoch [113/120    avg_loss:0.136, val_acc:0.977]
Epoch [114/120    avg_loss:0.152, val_acc:0.977]
Epoch [115/120    avg_loss:0.152, val_acc:0.975]
Epoch [116/120    avg_loss:0.142, val_acc:0.979]
Epoch [117/120    avg_loss:0.140, val_acc:0.983]
Epoch [118/120    avg_loss:0.138, val_acc:0.983]
Epoch [119/120    avg_loss:0.137, val_acc:0.979]
Epoch [120/120    avg_loss:0.146, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 218  11   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 204  22   0   0   0   1   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 1.         0.97117517 0.97321429 0.91071429 0.90849673
 1.         0.92045455 1.         0.99893276 1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9864691495546937
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbbb8c64e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.110]
Epoch [2/120    avg_loss:2.613, val_acc:0.106]
Epoch [3/120    avg_loss:2.590, val_acc:0.285]
Epoch [4/120    avg_loss:2.567, val_acc:0.365]
Epoch [5/120    avg_loss:2.548, val_acc:0.375]
Epoch [6/120    avg_loss:2.524, val_acc:0.373]
Epoch [7/120    avg_loss:2.502, val_acc:0.360]
Epoch [8/120    avg_loss:2.478, val_acc:0.354]
Epoch [9/120    avg_loss:2.461, val_acc:0.350]
Epoch [10/120    avg_loss:2.435, val_acc:0.344]
Epoch [11/120    avg_loss:2.411, val_acc:0.340]
Epoch [12/120    avg_loss:2.383, val_acc:0.338]
Epoch [13/120    avg_loss:2.352, val_acc:0.346]
Epoch [14/120    avg_loss:2.328, val_acc:0.350]
Epoch [15/120    avg_loss:2.302, val_acc:0.348]
Epoch [16/120    avg_loss:2.264, val_acc:0.350]
Epoch [17/120    avg_loss:2.239, val_acc:0.356]
Epoch [18/120    avg_loss:2.208, val_acc:0.362]
Epoch [19/120    avg_loss:2.185, val_acc:0.360]
Epoch [20/120    avg_loss:2.174, val_acc:0.369]
Epoch [21/120    avg_loss:2.163, val_acc:0.367]
Epoch [22/120    avg_loss:2.171, val_acc:0.365]
Epoch [23/120    avg_loss:2.170, val_acc:0.369]
Epoch [24/120    avg_loss:2.166, val_acc:0.375]
Epoch [25/120    avg_loss:2.157, val_acc:0.375]
Epoch [26/120    avg_loss:2.146, val_acc:0.373]
Epoch [27/120    avg_loss:2.149, val_acc:0.371]
Epoch [28/120    avg_loss:2.144, val_acc:0.373]
Epoch [29/120    avg_loss:2.140, val_acc:0.377]
Epoch [30/120    avg_loss:2.131, val_acc:0.375]
Epoch [31/120    avg_loss:2.137, val_acc:0.379]
Epoch [32/120    avg_loss:2.132, val_acc:0.375]
Epoch [33/120    avg_loss:2.121, val_acc:0.379]
Epoch [34/120    avg_loss:2.107, val_acc:0.377]
Epoch [35/120    avg_loss:2.125, val_acc:0.381]
Epoch [36/120    avg_loss:2.121, val_acc:0.383]
Epoch [37/120    avg_loss:2.112, val_acc:0.379]
Epoch [38/120    avg_loss:2.098, val_acc:0.381]
Epoch [39/120    avg_loss:2.106, val_acc:0.381]
Epoch [40/120    avg_loss:2.113, val_acc:0.392]
Epoch [41/120    avg_loss:2.090, val_acc:0.390]
Epoch [42/120    avg_loss:2.096, val_acc:0.396]
Epoch [43/120    avg_loss:2.095, val_acc:0.394]
Epoch [44/120    avg_loss:2.078, val_acc:0.392]
Epoch [45/120    avg_loss:2.091, val_acc:0.396]
Epoch [46/120    avg_loss:2.072, val_acc:0.398]
Epoch [47/120    avg_loss:2.084, val_acc:0.400]
Epoch [48/120    avg_loss:2.058, val_acc:0.398]
Epoch [49/120    avg_loss:2.066, val_acc:0.402]
Epoch [50/120    avg_loss:2.043, val_acc:0.398]
Epoch [51/120    avg_loss:2.057, val_acc:0.402]
Epoch [52/120    avg_loss:2.034, val_acc:0.400]
Epoch [53/120    avg_loss:2.050, val_acc:0.402]
Epoch [54/120    avg_loss:2.057, val_acc:0.404]
Epoch [55/120    avg_loss:2.041, val_acc:0.396]
Epoch [56/120    avg_loss:2.035, val_acc:0.400]
Epoch [57/120    avg_loss:2.018, val_acc:0.398]
Epoch [58/120    avg_loss:2.031, val_acc:0.406]
Epoch [59/120    avg_loss:2.013, val_acc:0.408]
Epoch [60/120    avg_loss:2.023, val_acc:0.417]
Epoch [61/120    avg_loss:2.039, val_acc:0.423]
Epoch [62/120    avg_loss:2.010, val_acc:0.417]
Epoch [63/120    avg_loss:1.996, val_acc:0.425]
Epoch [64/120    avg_loss:1.996, val_acc:0.429]
Epoch [65/120    avg_loss:1.991, val_acc:0.433]
Epoch [66/120    avg_loss:1.986, val_acc:0.431]
Epoch [67/120    avg_loss:1.973, val_acc:0.444]
Epoch [68/120    avg_loss:1.981, val_acc:0.452]
Epoch [69/120    avg_loss:1.970, val_acc:0.438]
Epoch [70/120    avg_loss:1.967, val_acc:0.448]
Epoch [71/120    avg_loss:1.949, val_acc:0.452]
Epoch [72/120    avg_loss:1.948, val_acc:0.454]
Epoch [73/120    avg_loss:1.959, val_acc:0.454]
Epoch [74/120    avg_loss:1.935, val_acc:0.460]
Epoch [75/120    avg_loss:1.940, val_acc:0.460]
Epoch [76/120    avg_loss:1.930, val_acc:0.460]
Epoch [77/120    avg_loss:1.947, val_acc:0.465]
Epoch [78/120    avg_loss:1.919, val_acc:0.475]
Epoch [79/120    avg_loss:1.937, val_acc:0.469]
Epoch [80/120    avg_loss:1.917, val_acc:0.471]
Epoch [81/120    avg_loss:1.914, val_acc:0.469]
Epoch [82/120    avg_loss:1.911, val_acc:0.475]
Epoch [83/120    avg_loss:1.878, val_acc:0.473]
Epoch [84/120    avg_loss:1.897, val_acc:0.469]
Epoch [85/120    avg_loss:1.902, val_acc:0.473]
Epoch [86/120    avg_loss:1.884, val_acc:0.477]
Epoch [87/120    avg_loss:1.885, val_acc:0.475]
Epoch [88/120    avg_loss:1.892, val_acc:0.481]
Epoch [89/120    avg_loss:1.874, val_acc:0.481]
Epoch [90/120    avg_loss:1.873, val_acc:0.479]
Epoch [91/120    avg_loss:1.871, val_acc:0.477]
Epoch [92/120    avg_loss:1.855, val_acc:0.481]
Epoch [93/120    avg_loss:1.850, val_acc:0.481]
Epoch [94/120    avg_loss:1.842, val_acc:0.481]
Epoch [95/120    avg_loss:1.849, val_acc:0.483]
Epoch [96/120    avg_loss:1.841, val_acc:0.485]
Epoch [97/120    avg_loss:1.842, val_acc:0.485]
Epoch [98/120    avg_loss:1.837, val_acc:0.496]
Epoch [99/120    avg_loss:1.832, val_acc:0.494]
Epoch [100/120    avg_loss:1.815, val_acc:0.483]
Epoch [101/120    avg_loss:1.824, val_acc:0.500]
Epoch [102/120    avg_loss:1.798, val_acc:0.496]
Epoch [103/120    avg_loss:1.816, val_acc:0.500]
Epoch [104/120    avg_loss:1.798, val_acc:0.504]
Epoch [105/120    avg_loss:1.789, val_acc:0.510]
Epoch [106/120    avg_loss:1.777, val_acc:0.521]
Epoch [107/120    avg_loss:1.772, val_acc:0.517]
Epoch [108/120    avg_loss:1.765, val_acc:0.517]
Epoch [109/120    avg_loss:1.765, val_acc:0.527]
Epoch [110/120    avg_loss:1.767, val_acc:0.540]
Epoch [111/120    avg_loss:1.755, val_acc:0.531]
Epoch [112/120    avg_loss:1.738, val_acc:0.542]
Epoch [113/120    avg_loss:1.739, val_acc:0.550]
Epoch [114/120    avg_loss:1.760, val_acc:0.554]
Epoch [115/120    avg_loss:1.720, val_acc:0.579]
Epoch [116/120    avg_loss:1.736, val_acc:0.600]
Epoch [117/120    avg_loss:1.723, val_acc:0.615]
Epoch [118/120    avg_loss:1.695, val_acc:0.610]
Epoch [119/120    avg_loss:1.700, val_acc:0.619]
Epoch [120/120    avg_loss:1.694, val_acc:0.613]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0  16   0   0 669   0   0   0   0   0   0   0   0   0]
 [  0   0 206   0   0   0   0  13   0   0   0   0   0   0]
 [  0   0   1   0   0 215   0  14   0   0   0   0   0   0]
 [  0   0   2   0   1 218   0   0   0   0   0   0   6   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0  75  72  27  32   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0 109   0  23  69   0   0  38 149   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 291   0 177   0   0]
 [  0   0   0   0   0   0   0   0   0   0 363   0   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 102 351   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
58.14498933901919

F1 scores:
[       nan 0.04564907 0.74502712 0.         0.00201005 0.33564815
 0.23175966 0.67532468 0.17840376 0.64096916 0.99862448 0.72991288
 0.86559803 1.        ]

Kappa:
0.5432647264924934
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f90f6528da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.680, val_acc:0.048]
Epoch [2/120    avg_loss:2.643, val_acc:0.048]
Epoch [3/120    avg_loss:2.608, val_acc:0.040]
Epoch [4/120    avg_loss:2.575, val_acc:0.110]
Epoch [5/120    avg_loss:2.543, val_acc:0.125]
Epoch [6/120    avg_loss:2.521, val_acc:0.158]
Epoch [7/120    avg_loss:2.490, val_acc:0.212]
Epoch [8/120    avg_loss:2.466, val_acc:0.304]
Epoch [9/120    avg_loss:2.447, val_acc:0.398]
Epoch [10/120    avg_loss:2.425, val_acc:0.421]
Epoch [11/120    avg_loss:2.400, val_acc:0.417]
Epoch [12/120    avg_loss:2.380, val_acc:0.427]
Epoch [13/120    avg_loss:2.346, val_acc:0.433]
Epoch [14/120    avg_loss:2.324, val_acc:0.433]
Epoch [15/120    avg_loss:2.301, val_acc:0.442]
Epoch [16/120    avg_loss:2.271, val_acc:0.454]
Epoch [17/120    avg_loss:2.245, val_acc:0.442]
Epoch [18/120    avg_loss:2.205, val_acc:0.454]
Epoch [19/120    avg_loss:2.191, val_acc:0.442]
Epoch [20/120    avg_loss:2.128, val_acc:0.477]
Epoch [21/120    avg_loss:2.091, val_acc:0.458]
Epoch [22/120    avg_loss:2.057, val_acc:0.471]
Epoch [23/120    avg_loss:2.012, val_acc:0.494]
Epoch [24/120    avg_loss:1.979, val_acc:0.506]
Epoch [25/120    avg_loss:1.910, val_acc:0.515]
Epoch [26/120    avg_loss:1.880, val_acc:0.556]
Epoch [27/120    avg_loss:1.836, val_acc:0.596]
Epoch [28/120    avg_loss:1.796, val_acc:0.608]
Epoch [29/120    avg_loss:1.731, val_acc:0.606]
Epoch [30/120    avg_loss:1.704, val_acc:0.613]
Epoch [31/120    avg_loss:1.640, val_acc:0.648]
Epoch [32/120    avg_loss:1.591, val_acc:0.656]
Epoch [33/120    avg_loss:1.557, val_acc:0.665]
Epoch [34/120    avg_loss:1.483, val_acc:0.665]
Epoch [35/120    avg_loss:1.451, val_acc:0.700]
Epoch [36/120    avg_loss:1.401, val_acc:0.735]
Epoch [37/120    avg_loss:1.310, val_acc:0.777]
Epoch [38/120    avg_loss:1.263, val_acc:0.754]
Epoch [39/120    avg_loss:1.210, val_acc:0.794]
Epoch [40/120    avg_loss:1.195, val_acc:0.794]
Epoch [41/120    avg_loss:1.137, val_acc:0.779]
Epoch [42/120    avg_loss:1.056, val_acc:0.835]
Epoch [43/120    avg_loss:0.990, val_acc:0.858]
Epoch [44/120    avg_loss:0.953, val_acc:0.829]
Epoch [45/120    avg_loss:0.926, val_acc:0.885]
Epoch [46/120    avg_loss:0.862, val_acc:0.900]
Epoch [47/120    avg_loss:0.862, val_acc:0.829]
Epoch [48/120    avg_loss:0.800, val_acc:0.854]
Epoch [49/120    avg_loss:0.739, val_acc:0.844]
Epoch [50/120    avg_loss:0.722, val_acc:0.894]
Epoch [51/120    avg_loss:0.735, val_acc:0.921]
Epoch [52/120    avg_loss:0.685, val_acc:0.883]
Epoch [53/120    avg_loss:0.715, val_acc:0.906]
Epoch [54/120    avg_loss:0.602, val_acc:0.921]
Epoch [55/120    avg_loss:0.579, val_acc:0.917]
Epoch [56/120    avg_loss:0.496, val_acc:0.933]
Epoch [57/120    avg_loss:0.497, val_acc:0.898]
Epoch [58/120    avg_loss:0.516, val_acc:0.938]
Epoch [59/120    avg_loss:0.488, val_acc:0.931]
Epoch [60/120    avg_loss:0.453, val_acc:0.925]
Epoch [61/120    avg_loss:0.484, val_acc:0.919]
Epoch [62/120    avg_loss:0.458, val_acc:0.929]
Epoch [63/120    avg_loss:0.420, val_acc:0.940]
Epoch [64/120    avg_loss:0.426, val_acc:0.929]
Epoch [65/120    avg_loss:0.432, val_acc:0.917]
Epoch [66/120    avg_loss:0.471, val_acc:0.942]
Epoch [67/120    avg_loss:0.402, val_acc:0.940]
Epoch [68/120    avg_loss:0.392, val_acc:0.940]
Epoch [69/120    avg_loss:0.348, val_acc:0.940]
Epoch [70/120    avg_loss:0.331, val_acc:0.944]
Epoch [71/120    avg_loss:0.326, val_acc:0.950]
Epoch [72/120    avg_loss:0.325, val_acc:0.919]
Epoch [73/120    avg_loss:0.354, val_acc:0.969]
Epoch [74/120    avg_loss:0.321, val_acc:0.950]
Epoch [75/120    avg_loss:0.328, val_acc:0.942]
Epoch [76/120    avg_loss:0.373, val_acc:0.925]
Epoch [77/120    avg_loss:0.329, val_acc:0.944]
Epoch [78/120    avg_loss:0.307, val_acc:0.963]
Epoch [79/120    avg_loss:0.275, val_acc:0.965]
Epoch [80/120    avg_loss:0.256, val_acc:0.958]
Epoch [81/120    avg_loss:0.268, val_acc:0.969]
Epoch [82/120    avg_loss:0.269, val_acc:0.967]
Epoch [83/120    avg_loss:0.225, val_acc:0.958]
Epoch [84/120    avg_loss:0.261, val_acc:0.965]
Epoch [85/120    avg_loss:0.228, val_acc:0.958]
Epoch [86/120    avg_loss:0.224, val_acc:0.975]
Epoch [87/120    avg_loss:0.206, val_acc:0.958]
Epoch [88/120    avg_loss:0.193, val_acc:0.952]
Epoch [89/120    avg_loss:0.252, val_acc:0.960]
Epoch [90/120    avg_loss:0.243, val_acc:0.954]
Epoch [91/120    avg_loss:0.249, val_acc:0.985]
Epoch [92/120    avg_loss:0.215, val_acc:0.981]
Epoch [93/120    avg_loss:0.172, val_acc:0.963]
Epoch [94/120    avg_loss:0.181, val_acc:0.983]
Epoch [95/120    avg_loss:0.201, val_acc:0.983]
Epoch [96/120    avg_loss:0.182, val_acc:0.965]
Epoch [97/120    avg_loss:0.189, val_acc:0.954]
Epoch [98/120    avg_loss:0.160, val_acc:0.963]
Epoch [99/120    avg_loss:0.167, val_acc:0.973]
Epoch [100/120    avg_loss:0.180, val_acc:0.979]
Epoch [101/120    avg_loss:0.192, val_acc:0.967]
Epoch [102/120    avg_loss:0.177, val_acc:0.979]
Epoch [103/120    avg_loss:0.146, val_acc:0.981]
Epoch [104/120    avg_loss:0.131, val_acc:0.981]
Epoch [105/120    avg_loss:0.126, val_acc:0.988]
Epoch [106/120    avg_loss:0.110, val_acc:0.985]
Epoch [107/120    avg_loss:0.112, val_acc:0.985]
Epoch [108/120    avg_loss:0.105, val_acc:0.985]
Epoch [109/120    avg_loss:0.105, val_acc:0.988]
Epoch [110/120    avg_loss:0.101, val_acc:0.988]
Epoch [111/120    avg_loss:0.097, val_acc:0.988]
Epoch [112/120    avg_loss:0.113, val_acc:0.990]
Epoch [113/120    avg_loss:0.113, val_acc:0.988]
Epoch [114/120    avg_loss:0.099, val_acc:0.990]
Epoch [115/120    avg_loss:0.091, val_acc:0.990]
Epoch [116/120    avg_loss:0.105, val_acc:0.988]
Epoch [117/120    avg_loss:0.102, val_acc:0.990]
Epoch [118/120    avg_loss:0.099, val_acc:0.992]
Epoch [119/120    avg_loss:0.097, val_acc:0.990]
Epoch [120/120    avg_loss:0.093, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 227   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 200  27   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.97333333 0.99343545 0.92165899 0.8974359
 1.         0.93785311 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9890806995721562
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f300cf89da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.160]
Epoch [2/120    avg_loss:2.630, val_acc:0.319]
Epoch [3/120    avg_loss:2.601, val_acc:0.325]
Epoch [4/120    avg_loss:2.575, val_acc:0.404]
Epoch [5/120    avg_loss:2.552, val_acc:0.417]
Epoch [6/120    avg_loss:2.526, val_acc:0.429]
Epoch [7/120    avg_loss:2.493, val_acc:0.431]
Epoch [8/120    avg_loss:2.466, val_acc:0.442]
Epoch [9/120    avg_loss:2.444, val_acc:0.444]
Epoch [10/120    avg_loss:2.406, val_acc:0.456]
Epoch [11/120    avg_loss:2.380, val_acc:0.463]
Epoch [12/120    avg_loss:2.338, val_acc:0.450]
Epoch [13/120    avg_loss:2.310, val_acc:0.456]
Epoch [14/120    avg_loss:2.251, val_acc:0.479]
Epoch [15/120    avg_loss:2.218, val_acc:0.483]
Epoch [16/120    avg_loss:2.178, val_acc:0.496]
Epoch [17/120    avg_loss:2.144, val_acc:0.500]
Epoch [18/120    avg_loss:2.108, val_acc:0.512]
Epoch [19/120    avg_loss:2.098, val_acc:0.521]
Epoch [20/120    avg_loss:2.038, val_acc:0.517]
Epoch [21/120    avg_loss:1.978, val_acc:0.506]
Epoch [22/120    avg_loss:1.972, val_acc:0.523]
Epoch [23/120    avg_loss:1.924, val_acc:0.527]
Epoch [24/120    avg_loss:1.891, val_acc:0.546]
Epoch [25/120    avg_loss:1.849, val_acc:0.592]
Epoch [26/120    avg_loss:1.812, val_acc:0.562]
Epoch [27/120    avg_loss:1.760, val_acc:0.598]
Epoch [28/120    avg_loss:1.694, val_acc:0.619]
Epoch [29/120    avg_loss:1.657, val_acc:0.621]
Epoch [30/120    avg_loss:1.630, val_acc:0.627]
Epoch [31/120    avg_loss:1.569, val_acc:0.623]
Epoch [32/120    avg_loss:1.544, val_acc:0.637]
Epoch [33/120    avg_loss:1.475, val_acc:0.658]
Epoch [34/120    avg_loss:1.442, val_acc:0.662]
Epoch [35/120    avg_loss:1.345, val_acc:0.692]
Epoch [36/120    avg_loss:1.320, val_acc:0.696]
Epoch [37/120    avg_loss:1.297, val_acc:0.698]
Epoch [38/120    avg_loss:1.204, val_acc:0.733]
Epoch [39/120    avg_loss:1.178, val_acc:0.700]
Epoch [40/120    avg_loss:1.134, val_acc:0.769]
Epoch [41/120    avg_loss:1.091, val_acc:0.760]
Epoch [42/120    avg_loss:1.036, val_acc:0.819]
Epoch [43/120    avg_loss:1.001, val_acc:0.765]
Epoch [44/120    avg_loss:0.984, val_acc:0.769]
Epoch [45/120    avg_loss:0.925, val_acc:0.754]
Epoch [46/120    avg_loss:0.893, val_acc:0.838]
Epoch [47/120    avg_loss:0.801, val_acc:0.835]
Epoch [48/120    avg_loss:0.762, val_acc:0.879]
Epoch [49/120    avg_loss:0.706, val_acc:0.892]
Epoch [50/120    avg_loss:0.688, val_acc:0.912]
Epoch [51/120    avg_loss:0.652, val_acc:0.838]
Epoch [52/120    avg_loss:0.628, val_acc:0.919]
Epoch [53/120    avg_loss:0.592, val_acc:0.938]
Epoch [54/120    avg_loss:0.536, val_acc:0.921]
Epoch [55/120    avg_loss:0.495, val_acc:0.915]
Epoch [56/120    avg_loss:0.490, val_acc:0.950]
Epoch [57/120    avg_loss:0.459, val_acc:0.944]
Epoch [58/120    avg_loss:0.442, val_acc:0.942]
Epoch [59/120    avg_loss:0.447, val_acc:0.940]
Epoch [60/120    avg_loss:0.427, val_acc:0.929]
Epoch [61/120    avg_loss:0.428, val_acc:0.946]
Epoch [62/120    avg_loss:0.352, val_acc:0.944]
Epoch [63/120    avg_loss:0.384, val_acc:0.956]
Epoch [64/120    avg_loss:0.394, val_acc:0.935]
Epoch [65/120    avg_loss:0.387, val_acc:0.942]
Epoch [66/120    avg_loss:0.354, val_acc:0.935]
Epoch [67/120    avg_loss:0.374, val_acc:0.952]
Epoch [68/120    avg_loss:0.424, val_acc:0.846]
Epoch [69/120    avg_loss:0.349, val_acc:0.938]
Epoch [70/120    avg_loss:0.336, val_acc:0.958]
Epoch [71/120    avg_loss:0.264, val_acc:0.963]
Epoch [72/120    avg_loss:0.277, val_acc:0.952]
Epoch [73/120    avg_loss:0.252, val_acc:0.942]
Epoch [74/120    avg_loss:0.260, val_acc:0.956]
Epoch [75/120    avg_loss:0.235, val_acc:0.963]
Epoch [76/120    avg_loss:0.223, val_acc:0.948]
Epoch [77/120    avg_loss:0.294, val_acc:0.958]
Epoch [78/120    avg_loss:0.250, val_acc:0.931]
Epoch [79/120    avg_loss:0.283, val_acc:0.956]
Epoch [80/120    avg_loss:0.261, val_acc:0.927]
Epoch [81/120    avg_loss:0.214, val_acc:0.933]
Epoch [82/120    avg_loss:0.184, val_acc:0.954]
Epoch [83/120    avg_loss:0.186, val_acc:0.952]
Epoch [84/120    avg_loss:0.177, val_acc:0.971]
Epoch [85/120    avg_loss:0.197, val_acc:0.963]
Epoch [86/120    avg_loss:0.172, val_acc:0.948]
Epoch [87/120    avg_loss:0.219, val_acc:0.946]
Epoch [88/120    avg_loss:0.242, val_acc:0.971]
Epoch [89/120    avg_loss:0.207, val_acc:0.963]
Epoch [90/120    avg_loss:0.174, val_acc:0.973]
Epoch [91/120    avg_loss:0.149, val_acc:0.965]
Epoch [92/120    avg_loss:0.135, val_acc:0.977]
Epoch [93/120    avg_loss:0.122, val_acc:0.973]
Epoch [94/120    avg_loss:0.146, val_acc:0.973]
Epoch [95/120    avg_loss:0.135, val_acc:0.965]
Epoch [96/120    avg_loss:0.130, val_acc:0.977]
Epoch [97/120    avg_loss:0.100, val_acc:0.975]
Epoch [98/120    avg_loss:0.129, val_acc:0.969]
Epoch [99/120    avg_loss:0.110, val_acc:0.975]
Epoch [100/120    avg_loss:0.102, val_acc:0.981]
Epoch [101/120    avg_loss:0.089, val_acc:0.973]
Epoch [102/120    avg_loss:0.077, val_acc:0.975]
Epoch [103/120    avg_loss:0.114, val_acc:0.958]
Epoch [104/120    avg_loss:0.168, val_acc:0.971]
Epoch [105/120    avg_loss:0.125, val_acc:0.969]
Epoch [106/120    avg_loss:0.126, val_acc:0.977]
Epoch [107/120    avg_loss:0.117, val_acc:0.979]
Epoch [108/120    avg_loss:0.124, val_acc:0.950]
Epoch [109/120    avg_loss:0.155, val_acc:0.952]
Epoch [110/120    avg_loss:0.134, val_acc:0.952]
Epoch [111/120    avg_loss:0.114, val_acc:0.977]
Epoch [112/120    avg_loss:0.091, val_acc:0.977]
Epoch [113/120    avg_loss:0.092, val_acc:0.977]
Epoch [114/120    avg_loss:0.062, val_acc:0.981]
Epoch [115/120    avg_loss:0.070, val_acc:0.981]
Epoch [116/120    avg_loss:0.065, val_acc:0.983]
Epoch [117/120    avg_loss:0.065, val_acc:0.983]
Epoch [118/120    avg_loss:0.068, val_acc:0.981]
Epoch [119/120    avg_loss:0.063, val_acc:0.981]
Epoch [120/120    avg_loss:0.062, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 1.         0.97767857 0.98230088 0.89427313 0.86577181
 1.         0.94382022 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9862314783976643
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd030b9be10>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.185]
Epoch [2/120    avg_loss:2.605, val_acc:0.198]
Epoch [3/120    avg_loss:2.582, val_acc:0.210]
Epoch [4/120    avg_loss:2.561, val_acc:0.256]
Epoch [5/120    avg_loss:2.543, val_acc:0.267]
Epoch [6/120    avg_loss:2.516, val_acc:0.267]
Epoch [7/120    avg_loss:2.501, val_acc:0.267]
Epoch [8/120    avg_loss:2.476, val_acc:0.267]
Epoch [9/120    avg_loss:2.462, val_acc:0.271]
Epoch [10/120    avg_loss:2.437, val_acc:0.277]
Epoch [11/120    avg_loss:2.419, val_acc:0.290]
Epoch [12/120    avg_loss:2.394, val_acc:0.323]
Epoch [13/120    avg_loss:2.365, val_acc:0.362]
Epoch [14/120    avg_loss:2.354, val_acc:0.425]
Epoch [15/120    avg_loss:2.322, val_acc:0.433]
Epoch [16/120    avg_loss:2.304, val_acc:0.448]
Epoch [17/120    avg_loss:2.274, val_acc:0.475]
Epoch [18/120    avg_loss:2.240, val_acc:0.463]
Epoch [19/120    avg_loss:2.220, val_acc:0.487]
Epoch [20/120    avg_loss:2.198, val_acc:0.487]
Epoch [21/120    avg_loss:2.159, val_acc:0.483]
Epoch [22/120    avg_loss:2.119, val_acc:0.508]
Epoch [23/120    avg_loss:2.069, val_acc:0.581]
Epoch [24/120    avg_loss:2.032, val_acc:0.600]
Epoch [25/120    avg_loss:2.004, val_acc:0.631]
Epoch [26/120    avg_loss:1.946, val_acc:0.673]
Epoch [27/120    avg_loss:1.906, val_acc:0.698]
Epoch [28/120    avg_loss:1.868, val_acc:0.729]
Epoch [29/120    avg_loss:1.795, val_acc:0.717]
Epoch [30/120    avg_loss:1.766, val_acc:0.733]
Epoch [31/120    avg_loss:1.699, val_acc:0.823]
Epoch [32/120    avg_loss:1.656, val_acc:0.721]
Epoch [33/120    avg_loss:1.606, val_acc:0.756]
Epoch [34/120    avg_loss:1.573, val_acc:0.748]
Epoch [35/120    avg_loss:1.476, val_acc:0.819]
Epoch [36/120    avg_loss:1.413, val_acc:0.842]
Epoch [37/120    avg_loss:1.367, val_acc:0.835]
Epoch [38/120    avg_loss:1.310, val_acc:0.881]
Epoch [39/120    avg_loss:1.241, val_acc:0.892]
Epoch [40/120    avg_loss:1.172, val_acc:0.881]
Epoch [41/120    avg_loss:1.120, val_acc:0.898]
Epoch [42/120    avg_loss:1.074, val_acc:0.890]
Epoch [43/120    avg_loss:1.031, val_acc:0.883]
Epoch [44/120    avg_loss:0.976, val_acc:0.883]
Epoch [45/120    avg_loss:0.895, val_acc:0.894]
Epoch [46/120    avg_loss:0.888, val_acc:0.887]
Epoch [47/120    avg_loss:0.849, val_acc:0.898]
Epoch [48/120    avg_loss:0.810, val_acc:0.900]
Epoch [49/120    avg_loss:0.769, val_acc:0.908]
Epoch [50/120    avg_loss:0.749, val_acc:0.871]
Epoch [51/120    avg_loss:0.712, val_acc:0.902]
Epoch [52/120    avg_loss:0.699, val_acc:0.887]
Epoch [53/120    avg_loss:0.682, val_acc:0.910]
Epoch [54/120    avg_loss:0.654, val_acc:0.902]
Epoch [55/120    avg_loss:0.630, val_acc:0.902]
Epoch [56/120    avg_loss:0.593, val_acc:0.925]
Epoch [57/120    avg_loss:0.557, val_acc:0.925]
Epoch [58/120    avg_loss:0.546, val_acc:0.908]
Epoch [59/120    avg_loss:0.536, val_acc:0.927]
Epoch [60/120    avg_loss:0.544, val_acc:0.908]
Epoch [61/120    avg_loss:0.494, val_acc:0.933]
Epoch [62/120    avg_loss:0.454, val_acc:0.927]
Epoch [63/120    avg_loss:0.467, val_acc:0.902]
Epoch [64/120    avg_loss:0.480, val_acc:0.929]
Epoch [65/120    avg_loss:0.464, val_acc:0.890]
Epoch [66/120    avg_loss:0.499, val_acc:0.908]
Epoch [67/120    avg_loss:0.404, val_acc:0.925]
Epoch [68/120    avg_loss:0.414, val_acc:0.929]
Epoch [69/120    avg_loss:0.398, val_acc:0.912]
Epoch [70/120    avg_loss:0.382, val_acc:0.931]
Epoch [71/120    avg_loss:0.389, val_acc:0.944]
Epoch [72/120    avg_loss:0.367, val_acc:0.954]
Epoch [73/120    avg_loss:0.330, val_acc:0.965]
Epoch [74/120    avg_loss:0.333, val_acc:0.948]
Epoch [75/120    avg_loss:0.291, val_acc:0.940]
Epoch [76/120    avg_loss:0.395, val_acc:0.925]
Epoch [77/120    avg_loss:0.401, val_acc:0.935]
Epoch [78/120    avg_loss:0.326, val_acc:0.946]
Epoch [79/120    avg_loss:0.305, val_acc:0.954]
Epoch [80/120    avg_loss:0.283, val_acc:0.963]
Epoch [81/120    avg_loss:0.279, val_acc:0.950]
Epoch [82/120    avg_loss:0.272, val_acc:0.940]
Epoch [83/120    avg_loss:0.294, val_acc:0.921]
Epoch [84/120    avg_loss:0.286, val_acc:0.960]
Epoch [85/120    avg_loss:0.250, val_acc:0.935]
Epoch [86/120    avg_loss:0.223, val_acc:0.973]
Epoch [87/120    avg_loss:0.226, val_acc:0.938]
Epoch [88/120    avg_loss:0.219, val_acc:0.946]
Epoch [89/120    avg_loss:0.277, val_acc:0.931]
Epoch [90/120    avg_loss:0.335, val_acc:0.910]
Epoch [91/120    avg_loss:0.303, val_acc:0.950]
Epoch [92/120    avg_loss:0.303, val_acc:0.954]
Epoch [93/120    avg_loss:0.269, val_acc:0.969]
Epoch [94/120    avg_loss:0.210, val_acc:0.971]
Epoch [95/120    avg_loss:0.196, val_acc:0.952]
Epoch [96/120    avg_loss:0.192, val_acc:0.979]
Epoch [97/120    avg_loss:0.184, val_acc:0.967]
Epoch [98/120    avg_loss:0.183, val_acc:0.979]
Epoch [99/120    avg_loss:0.173, val_acc:0.963]
Epoch [100/120    avg_loss:0.152, val_acc:0.983]
Epoch [101/120    avg_loss:0.153, val_acc:0.981]
Epoch [102/120    avg_loss:0.135, val_acc:0.973]
Epoch [103/120    avg_loss:0.186, val_acc:0.963]
Epoch [104/120    avg_loss:0.193, val_acc:0.971]
Epoch [105/120    avg_loss:0.124, val_acc:0.977]
Epoch [106/120    avg_loss:0.137, val_acc:0.973]
Epoch [107/120    avg_loss:0.111, val_acc:0.967]
Epoch [108/120    avg_loss:0.139, val_acc:0.977]
Epoch [109/120    avg_loss:0.133, val_acc:0.975]
Epoch [110/120    avg_loss:0.115, val_acc:0.971]
Epoch [111/120    avg_loss:0.128, val_acc:0.979]
Epoch [112/120    avg_loss:0.144, val_acc:0.975]
Epoch [113/120    avg_loss:0.148, val_acc:0.952]
Epoch [114/120    avg_loss:0.116, val_acc:0.975]
Epoch [115/120    avg_loss:0.089, val_acc:0.977]
Epoch [116/120    avg_loss:0.095, val_acc:0.983]
Epoch [117/120    avg_loss:0.090, val_acc:0.985]
Epoch [118/120    avg_loss:0.090, val_acc:0.983]
Epoch [119/120    avg_loss:0.096, val_acc:0.985]
Epoch [120/120    avg_loss:0.113, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.97550111 1.         0.90868597 0.86101695
 1.         0.93785311 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9876555614130504
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f6ae10f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.647, val_acc:0.065]
Epoch [2/120    avg_loss:2.628, val_acc:0.067]
Epoch [3/120    avg_loss:2.610, val_acc:0.113]
Epoch [4/120    avg_loss:2.590, val_acc:0.152]
Epoch [5/120    avg_loss:2.573, val_acc:0.175]
Epoch [6/120    avg_loss:2.548, val_acc:0.340]
Epoch [7/120    avg_loss:2.525, val_acc:0.346]
Epoch [8/120    avg_loss:2.506, val_acc:0.346]
Epoch [9/120    avg_loss:2.484, val_acc:0.340]
Epoch [10/120    avg_loss:2.463, val_acc:0.350]
Epoch [11/120    avg_loss:2.443, val_acc:0.369]
Epoch [12/120    avg_loss:2.426, val_acc:0.394]
Epoch [13/120    avg_loss:2.400, val_acc:0.406]
Epoch [14/120    avg_loss:2.381, val_acc:0.425]
Epoch [15/120    avg_loss:2.359, val_acc:0.448]
Epoch [16/120    avg_loss:2.338, val_acc:0.460]
Epoch [17/120    avg_loss:2.316, val_acc:0.454]
Epoch [18/120    avg_loss:2.294, val_acc:0.463]
Epoch [19/120    avg_loss:2.257, val_acc:0.454]
Epoch [20/120    avg_loss:2.235, val_acc:0.452]
Epoch [21/120    avg_loss:2.211, val_acc:0.446]
Epoch [22/120    avg_loss:2.169, val_acc:0.440]
Epoch [23/120    avg_loss:2.142, val_acc:0.433]
Epoch [24/120    avg_loss:2.110, val_acc:0.442]
Epoch [25/120    avg_loss:2.064, val_acc:0.440]
Epoch [26/120    avg_loss:2.043, val_acc:0.456]
Epoch [27/120    avg_loss:2.014, val_acc:0.477]
Epoch [28/120    avg_loss:1.987, val_acc:0.465]
Epoch [29/120    avg_loss:1.938, val_acc:0.477]
Epoch [30/120    avg_loss:1.894, val_acc:0.525]
Epoch [31/120    avg_loss:1.870, val_acc:0.556]
Epoch [32/120    avg_loss:1.827, val_acc:0.581]
Epoch [33/120    avg_loss:1.770, val_acc:0.642]
Epoch [34/120    avg_loss:1.718, val_acc:0.690]
Epoch [35/120    avg_loss:1.680, val_acc:0.729]
Epoch [36/120    avg_loss:1.634, val_acc:0.738]
Epoch [37/120    avg_loss:1.612, val_acc:0.727]
Epoch [38/120    avg_loss:1.558, val_acc:0.679]
Epoch [39/120    avg_loss:1.519, val_acc:0.721]
Epoch [40/120    avg_loss:1.453, val_acc:0.738]
Epoch [41/120    avg_loss:1.396, val_acc:0.746]
Epoch [42/120    avg_loss:1.340, val_acc:0.777]
Epoch [43/120    avg_loss:1.301, val_acc:0.742]
Epoch [44/120    avg_loss:1.272, val_acc:0.756]
Epoch [45/120    avg_loss:1.193, val_acc:0.883]
Epoch [46/120    avg_loss:1.147, val_acc:0.796]
Epoch [47/120    avg_loss:1.127, val_acc:0.875]
Epoch [48/120    avg_loss:1.051, val_acc:0.854]
Epoch [49/120    avg_loss:0.985, val_acc:0.833]
Epoch [50/120    avg_loss:0.984, val_acc:0.815]
Epoch [51/120    avg_loss:0.935, val_acc:0.877]
Epoch [52/120    avg_loss:0.852, val_acc:0.858]
Epoch [53/120    avg_loss:0.835, val_acc:0.844]
Epoch [54/120    avg_loss:0.802, val_acc:0.871]
Epoch [55/120    avg_loss:0.758, val_acc:0.892]
Epoch [56/120    avg_loss:0.699, val_acc:0.938]
Epoch [57/120    avg_loss:0.692, val_acc:0.912]
Epoch [58/120    avg_loss:0.653, val_acc:0.923]
Epoch [59/120    avg_loss:0.615, val_acc:0.915]
Epoch [60/120    avg_loss:0.574, val_acc:0.925]
Epoch [61/120    avg_loss:0.599, val_acc:0.925]
Epoch [62/120    avg_loss:0.592, val_acc:0.852]
Epoch [63/120    avg_loss:0.682, val_acc:0.931]
Epoch [64/120    avg_loss:0.522, val_acc:0.946]
Epoch [65/120    avg_loss:0.473, val_acc:0.952]
Epoch [66/120    avg_loss:0.496, val_acc:0.935]
Epoch [67/120    avg_loss:0.490, val_acc:0.925]
Epoch [68/120    avg_loss:0.460, val_acc:0.944]
Epoch [69/120    avg_loss:0.417, val_acc:0.954]
Epoch [70/120    avg_loss:0.418, val_acc:0.956]
Epoch [71/120    avg_loss:0.382, val_acc:0.971]
Epoch [72/120    avg_loss:0.361, val_acc:0.969]
Epoch [73/120    avg_loss:0.313, val_acc:0.940]
Epoch [74/120    avg_loss:0.298, val_acc:0.954]
Epoch [75/120    avg_loss:0.306, val_acc:0.956]
Epoch [76/120    avg_loss:0.289, val_acc:0.938]
Epoch [77/120    avg_loss:0.346, val_acc:0.965]
Epoch [78/120    avg_loss:0.309, val_acc:0.950]
Epoch [79/120    avg_loss:0.289, val_acc:0.942]
Epoch [80/120    avg_loss:0.294, val_acc:0.956]
Epoch [81/120    avg_loss:0.257, val_acc:0.952]
Epoch [82/120    avg_loss:0.249, val_acc:0.963]
Epoch [83/120    avg_loss:0.238, val_acc:0.973]
Epoch [84/120    avg_loss:0.209, val_acc:0.977]
Epoch [85/120    avg_loss:0.265, val_acc:0.960]
Epoch [86/120    avg_loss:0.222, val_acc:0.946]
Epoch [87/120    avg_loss:0.247, val_acc:0.958]
Epoch [88/120    avg_loss:0.245, val_acc:0.971]
Epoch [89/120    avg_loss:0.205, val_acc:0.969]
Epoch [90/120    avg_loss:0.191, val_acc:0.960]
Epoch [91/120    avg_loss:0.203, val_acc:0.967]
Epoch [92/120    avg_loss:0.202, val_acc:0.948]
Epoch [93/120    avg_loss:0.252, val_acc:0.960]
Epoch [94/120    avg_loss:0.236, val_acc:0.977]
Epoch [95/120    avg_loss:0.234, val_acc:0.956]
Epoch [96/120    avg_loss:0.205, val_acc:0.973]
Epoch [97/120    avg_loss:0.209, val_acc:0.981]
Epoch [98/120    avg_loss:0.215, val_acc:0.975]
Epoch [99/120    avg_loss:0.169, val_acc:0.973]
Epoch [100/120    avg_loss:0.164, val_acc:0.983]
Epoch [101/120    avg_loss:0.139, val_acc:0.979]
Epoch [102/120    avg_loss:0.133, val_acc:0.977]
Epoch [103/120    avg_loss:0.119, val_acc:0.973]
Epoch [104/120    avg_loss:0.148, val_acc:0.963]
Epoch [105/120    avg_loss:0.181, val_acc:0.973]
Epoch [106/120    avg_loss:0.133, val_acc:0.979]
Epoch [107/120    avg_loss:0.164, val_acc:0.977]
Epoch [108/120    avg_loss:0.149, val_acc:0.979]
Epoch [109/120    avg_loss:0.118, val_acc:0.981]
Epoch [110/120    avg_loss:0.121, val_acc:0.981]
Epoch [111/120    avg_loss:0.093, val_acc:0.973]
Epoch [112/120    avg_loss:0.123, val_acc:0.981]
Epoch [113/120    avg_loss:0.095, val_acc:0.979]
Epoch [114/120    avg_loss:0.083, val_acc:0.985]
Epoch [115/120    avg_loss:0.085, val_acc:0.983]
Epoch [116/120    avg_loss:0.079, val_acc:0.985]
Epoch [117/120    avg_loss:0.074, val_acc:0.985]
Epoch [118/120    avg_loss:0.076, val_acc:0.985]
Epoch [119/120    avg_loss:0.069, val_acc:0.985]
Epoch [120/120    avg_loss:0.069, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98426966 0.98901099 0.93033708 0.91803279
 0.99756691 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9909796640570183
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f62f4226d30>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.073]
Epoch [2/120    avg_loss:2.603, val_acc:0.223]
Epoch [3/120    avg_loss:2.578, val_acc:0.304]
Epoch [4/120    avg_loss:2.562, val_acc:0.342]
Epoch [5/120    avg_loss:2.540, val_acc:0.352]
Epoch [6/120    avg_loss:2.522, val_acc:0.352]
Epoch [7/120    avg_loss:2.498, val_acc:0.383]
Epoch [8/120    avg_loss:2.481, val_acc:0.444]
Epoch [9/120    avg_loss:2.453, val_acc:0.500]
Epoch [10/120    avg_loss:2.435, val_acc:0.506]
Epoch [11/120    avg_loss:2.410, val_acc:0.531]
Epoch [12/120    avg_loss:2.384, val_acc:0.533]
Epoch [13/120    avg_loss:2.354, val_acc:0.552]
Epoch [14/120    avg_loss:2.333, val_acc:0.556]
Epoch [15/120    avg_loss:2.301, val_acc:0.565]
Epoch [16/120    avg_loss:2.273, val_acc:0.558]
Epoch [17/120    avg_loss:2.256, val_acc:0.560]
Epoch [18/120    avg_loss:2.211, val_acc:0.567]
Epoch [19/120    avg_loss:2.170, val_acc:0.575]
Epoch [20/120    avg_loss:2.137, val_acc:0.575]
Epoch [21/120    avg_loss:2.089, val_acc:0.569]
Epoch [22/120    avg_loss:2.048, val_acc:0.567]
Epoch [23/120    avg_loss:1.997, val_acc:0.585]
Epoch [24/120    avg_loss:1.958, val_acc:0.588]
Epoch [25/120    avg_loss:1.878, val_acc:0.592]
Epoch [26/120    avg_loss:1.844, val_acc:0.625]
Epoch [27/120    avg_loss:1.797, val_acc:0.662]
Epoch [28/120    avg_loss:1.745, val_acc:0.629]
Epoch [29/120    avg_loss:1.724, val_acc:0.654]
Epoch [30/120    avg_loss:1.637, val_acc:0.710]
Epoch [31/120    avg_loss:1.587, val_acc:0.681]
Epoch [32/120    avg_loss:1.540, val_acc:0.769]
Epoch [33/120    avg_loss:1.467, val_acc:0.719]
Epoch [34/120    avg_loss:1.423, val_acc:0.744]
Epoch [35/120    avg_loss:1.346, val_acc:0.794]
Epoch [36/120    avg_loss:1.315, val_acc:0.800]
Epoch [37/120    avg_loss:1.254, val_acc:0.750]
Epoch [38/120    avg_loss:1.204, val_acc:0.838]
Epoch [39/120    avg_loss:1.198, val_acc:0.854]
Epoch [40/120    avg_loss:1.097, val_acc:0.852]
Epoch [41/120    avg_loss:1.069, val_acc:0.867]
Epoch [42/120    avg_loss:1.020, val_acc:0.900]
Epoch [43/120    avg_loss:0.964, val_acc:0.902]
Epoch [44/120    avg_loss:0.878, val_acc:0.898]
Epoch [45/120    avg_loss:0.873, val_acc:0.842]
Epoch [46/120    avg_loss:0.876, val_acc:0.915]
Epoch [47/120    avg_loss:0.808, val_acc:0.904]
Epoch [48/120    avg_loss:0.777, val_acc:0.921]
Epoch [49/120    avg_loss:0.721, val_acc:0.925]
Epoch [50/120    avg_loss:0.687, val_acc:0.925]
Epoch [51/120    avg_loss:0.675, val_acc:0.887]
Epoch [52/120    avg_loss:0.657, val_acc:0.910]
Epoch [53/120    avg_loss:0.738, val_acc:0.921]
Epoch [54/120    avg_loss:0.625, val_acc:0.927]
Epoch [55/120    avg_loss:0.560, val_acc:0.933]
Epoch [56/120    avg_loss:0.525, val_acc:0.929]
Epoch [57/120    avg_loss:0.486, val_acc:0.940]
Epoch [58/120    avg_loss:0.495, val_acc:0.952]
Epoch [59/120    avg_loss:0.485, val_acc:0.942]
Epoch [60/120    avg_loss:0.450, val_acc:0.940]
Epoch [61/120    avg_loss:0.452, val_acc:0.942]
Epoch [62/120    avg_loss:0.420, val_acc:0.946]
Epoch [63/120    avg_loss:0.401, val_acc:0.946]
Epoch [64/120    avg_loss:0.386, val_acc:0.948]
Epoch [65/120    avg_loss:0.352, val_acc:0.954]
Epoch [66/120    avg_loss:0.335, val_acc:0.960]
Epoch [67/120    avg_loss:0.347, val_acc:0.923]
Epoch [68/120    avg_loss:0.372, val_acc:0.954]
Epoch [69/120    avg_loss:0.380, val_acc:0.935]
Epoch [70/120    avg_loss:0.384, val_acc:0.946]
Epoch [71/120    avg_loss:0.391, val_acc:0.956]
Epoch [72/120    avg_loss:0.339, val_acc:0.956]
Epoch [73/120    avg_loss:0.339, val_acc:0.958]
Epoch [74/120    avg_loss:0.322, val_acc:0.950]
Epoch [75/120    avg_loss:0.273, val_acc:0.973]
Epoch [76/120    avg_loss:0.281, val_acc:0.967]
Epoch [77/120    avg_loss:0.282, val_acc:0.967]
Epoch [78/120    avg_loss:0.281, val_acc:0.971]
Epoch [79/120    avg_loss:0.255, val_acc:0.960]
Epoch [80/120    avg_loss:0.252, val_acc:0.973]
Epoch [81/120    avg_loss:0.243, val_acc:0.975]
Epoch [82/120    avg_loss:0.221, val_acc:0.967]
Epoch [83/120    avg_loss:0.244, val_acc:0.973]
Epoch [84/120    avg_loss:0.239, val_acc:0.969]
Epoch [85/120    avg_loss:0.199, val_acc:0.956]
Epoch [86/120    avg_loss:0.224, val_acc:0.977]
Epoch [87/120    avg_loss:0.219, val_acc:0.975]
Epoch [88/120    avg_loss:0.206, val_acc:0.965]
Epoch [89/120    avg_loss:0.168, val_acc:0.981]
Epoch [90/120    avg_loss:0.172, val_acc:0.975]
Epoch [91/120    avg_loss:0.174, val_acc:0.977]
Epoch [92/120    avg_loss:0.182, val_acc:0.977]
Epoch [93/120    avg_loss:0.153, val_acc:0.975]
Epoch [94/120    avg_loss:0.168, val_acc:0.969]
Epoch [95/120    avg_loss:0.164, val_acc:0.963]
Epoch [96/120    avg_loss:0.165, val_acc:0.979]
Epoch [97/120    avg_loss:0.176, val_acc:0.979]
Epoch [98/120    avg_loss:0.144, val_acc:0.988]
Epoch [99/120    avg_loss:0.128, val_acc:0.983]
Epoch [100/120    avg_loss:0.127, val_acc:0.988]
Epoch [101/120    avg_loss:0.152, val_acc:0.971]
Epoch [102/120    avg_loss:0.145, val_acc:0.983]
Epoch [103/120    avg_loss:0.143, val_acc:0.988]
Epoch [104/120    avg_loss:0.133, val_acc:0.981]
Epoch [105/120    avg_loss:0.123, val_acc:0.992]
Epoch [106/120    avg_loss:0.125, val_acc:0.973]
Epoch [107/120    avg_loss:0.137, val_acc:0.981]
Epoch [108/120    avg_loss:0.151, val_acc:0.981]
Epoch [109/120    avg_loss:0.135, val_acc:0.990]
Epoch [110/120    avg_loss:0.157, val_acc:0.983]
Epoch [111/120    avg_loss:0.141, val_acc:0.985]
Epoch [112/120    avg_loss:0.142, val_acc:0.983]
Epoch [113/120    avg_loss:0.127, val_acc:0.979]
Epoch [114/120    avg_loss:0.115, val_acc:0.988]
Epoch [115/120    avg_loss:0.092, val_acc:0.992]
Epoch [116/120    avg_loss:0.085, val_acc:0.994]
Epoch [117/120    avg_loss:0.099, val_acc:0.994]
Epoch [118/120    avg_loss:0.078, val_acc:0.994]
Epoch [119/120    avg_loss:0.069, val_acc:0.992]
Epoch [120/120    avg_loss:0.090, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   2 225   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  41 104   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.98871332 0.98468271 0.90361446 0.83534137
 1.         0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9874162492332907
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb0dab8de48>
supervision:full
center_pixel:True
Network :
Number of parameter: 28042==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.129]
Epoch [2/120    avg_loss:2.601, val_acc:0.204]
Epoch [3/120    avg_loss:2.574, val_acc:0.196]
Epoch [4/120    avg_loss:2.557, val_acc:0.173]
Epoch [5/120    avg_loss:2.538, val_acc:0.242]
Epoch [6/120    avg_loss:2.517, val_acc:0.317]
Epoch [7/120    avg_loss:2.499, val_acc:0.338]
Epoch [8/120    avg_loss:2.481, val_acc:0.360]
Epoch [9/120    avg_loss:2.462, val_acc:0.373]
Epoch [10/120    avg_loss:2.437, val_acc:0.381]
Epoch [11/120    avg_loss:2.428, val_acc:0.381]
Epoch [12/120    avg_loss:2.400, val_acc:0.392]
Epoch [13/120    avg_loss:2.384, val_acc:0.404]
Epoch [14/120    avg_loss:2.363, val_acc:0.408]
Epoch [15/120    avg_loss:2.345, val_acc:0.419]
Epoch [16/120    avg_loss:2.331, val_acc:0.429]
Epoch [17/120    avg_loss:2.302, val_acc:0.438]
Epoch [18/120    avg_loss:2.275, val_acc:0.454]
Epoch [19/120    avg_loss:2.256, val_acc:0.481]
Epoch [20/120    avg_loss:2.234, val_acc:0.515]
Epoch [21/120    avg_loss:2.195, val_acc:0.537]
Epoch [22/120    avg_loss:2.159, val_acc:0.531]
Epoch [23/120    avg_loss:2.132, val_acc:0.537]
Epoch [24/120    avg_loss:2.095, val_acc:0.519]
Epoch [25/120    avg_loss:2.039, val_acc:0.533]
Epoch [26/120    avg_loss:2.004, val_acc:0.546]
Epoch [27/120    avg_loss:1.967, val_acc:0.554]
Epoch [28/120    avg_loss:1.935, val_acc:0.581]
Epoch [29/120    avg_loss:1.868, val_acc:0.585]
Epoch [30/120    avg_loss:1.818, val_acc:0.588]
Epoch [31/120    avg_loss:1.748, val_acc:0.615]
Epoch [32/120    avg_loss:1.713, val_acc:0.594]
Epoch [33/120    avg_loss:1.680, val_acc:0.592]
Epoch [34/120    avg_loss:1.598, val_acc:0.671]
Epoch [35/120    avg_loss:1.530, val_acc:0.675]
Epoch [36/120    avg_loss:1.483, val_acc:0.683]
Epoch [37/120    avg_loss:1.423, val_acc:0.758]
Epoch [38/120    avg_loss:1.365, val_acc:0.765]
Epoch [39/120    avg_loss:1.302, val_acc:0.812]
Epoch [40/120    avg_loss:1.257, val_acc:0.817]
Epoch [41/120    avg_loss:1.177, val_acc:0.871]
Epoch [42/120    avg_loss:1.127, val_acc:0.840]
Epoch [43/120    avg_loss:1.085, val_acc:0.881]
Epoch [44/120    avg_loss:1.048, val_acc:0.900]
Epoch [45/120    avg_loss:0.970, val_acc:0.877]
Epoch [46/120    avg_loss:0.929, val_acc:0.931]
Epoch [47/120    avg_loss:0.936, val_acc:0.904]
Epoch [48/120    avg_loss:0.838, val_acc:0.910]
Epoch [49/120    avg_loss:0.842, val_acc:0.917]
Epoch [50/120    avg_loss:0.794, val_acc:0.935]
Epoch [51/120    avg_loss:0.720, val_acc:0.921]
Epoch [52/120    avg_loss:0.734, val_acc:0.940]
Epoch [53/120    avg_loss:0.646, val_acc:0.925]
Epoch [54/120    avg_loss:0.656, val_acc:0.950]
Epoch [55/120    avg_loss:0.682, val_acc:0.908]
Epoch [56/120    avg_loss:0.624, val_acc:0.946]
Epoch [57/120    avg_loss:0.559, val_acc:0.931]
Epoch [58/120    avg_loss:0.558, val_acc:0.938]
Epoch [59/120    avg_loss:0.479, val_acc:0.948]
Epoch [60/120    avg_loss:0.471, val_acc:0.948]
Epoch [61/120    avg_loss:0.444, val_acc:0.933]
Epoch [62/120    avg_loss:0.466, val_acc:0.948]
Epoch [63/120    avg_loss:0.425, val_acc:0.946]
Epoch [64/120    avg_loss:0.440, val_acc:0.933]
Epoch [65/120    avg_loss:0.369, val_acc:0.940]
Epoch [66/120    avg_loss:0.334, val_acc:0.956]
Epoch [67/120    avg_loss:0.368, val_acc:0.958]
Epoch [68/120    avg_loss:0.338, val_acc:0.969]
Epoch [69/120    avg_loss:0.334, val_acc:0.967]
Epoch [70/120    avg_loss:0.337, val_acc:0.948]
Epoch [71/120    avg_loss:0.306, val_acc:0.954]
Epoch [72/120    avg_loss:0.272, val_acc:0.969]
Epoch [73/120    avg_loss:0.276, val_acc:0.956]
Epoch [74/120    avg_loss:0.251, val_acc:0.935]
Epoch [75/120    avg_loss:0.289, val_acc:0.960]
Epoch [76/120    avg_loss:0.285, val_acc:0.960]
Epoch [77/120    avg_loss:0.255, val_acc:0.971]
Epoch [78/120    avg_loss:0.268, val_acc:0.981]
Epoch [79/120    avg_loss:0.216, val_acc:0.971]
Epoch [80/120    avg_loss:0.228, val_acc:0.983]
Epoch [81/120    avg_loss:0.238, val_acc:0.988]
Epoch [82/120    avg_loss:0.190, val_acc:0.985]
Epoch [83/120    avg_loss:0.196, val_acc:0.981]
Epoch [84/120    avg_loss:0.187, val_acc:0.981]
Epoch [85/120    avg_loss:0.204, val_acc:0.969]
Epoch [86/120    avg_loss:0.197, val_acc:0.979]
Epoch [87/120    avg_loss:0.183, val_acc:0.983]
Epoch [88/120    avg_loss:0.170, val_acc:0.988]
Epoch [89/120    avg_loss:0.139, val_acc:0.985]
Epoch [90/120    avg_loss:0.137, val_acc:0.983]
Epoch [91/120    avg_loss:0.171, val_acc:0.981]
Epoch [92/120    avg_loss:0.167, val_acc:0.967]
Epoch [93/120    avg_loss:0.266, val_acc:0.971]
Epoch [94/120    avg_loss:0.200, val_acc:0.969]
Epoch [95/120    avg_loss:0.154, val_acc:0.969]
Epoch [96/120    avg_loss:0.206, val_acc:0.977]
Epoch [97/120    avg_loss:0.184, val_acc:0.963]
Epoch [98/120    avg_loss:0.136, val_acc:0.981]
Epoch [99/120    avg_loss:0.130, val_acc:0.971]
Epoch [100/120    avg_loss:0.130, val_acc:0.975]
Epoch [101/120    avg_loss:0.191, val_acc:0.967]
Epoch [102/120    avg_loss:0.128, val_acc:0.983]
Epoch [103/120    avg_loss:0.111, val_acc:0.990]
Epoch [104/120    avg_loss:0.096, val_acc:0.992]
Epoch [105/120    avg_loss:0.093, val_acc:0.992]
Epoch [106/120    avg_loss:0.086, val_acc:0.992]
Epoch [107/120    avg_loss:0.086, val_acc:0.992]
Epoch [108/120    avg_loss:0.077, val_acc:0.992]
Epoch [109/120    avg_loss:0.085, val_acc:0.992]
Epoch [110/120    avg_loss:0.074, val_acc:0.992]
Epoch [111/120    avg_loss:0.081, val_acc:0.992]
Epoch [112/120    avg_loss:0.074, val_acc:0.992]
Epoch [113/120    avg_loss:0.084, val_acc:0.992]
Epoch [114/120    avg_loss:0.076, val_acc:0.992]
Epoch [115/120    avg_loss:0.066, val_acc:0.992]
Epoch [116/120    avg_loss:0.064, val_acc:0.992]
Epoch [117/120    avg_loss:0.069, val_acc:0.992]
Epoch [118/120    avg_loss:0.072, val_acc:0.994]
Epoch [119/120    avg_loss:0.072, val_acc:0.992]
Epoch [120/120    avg_loss:0.071, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   1   0   0   2   0   0   0   0   0   0]
 [  0   0   0   1 207  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.98648649 0.99126638 0.92       0.88435374
 1.         0.95652174 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895551366902396
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67c9f91da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.190]
Epoch [2/120    avg_loss:2.591, val_acc:0.212]
Epoch [3/120    avg_loss:2.551, val_acc:0.252]
Epoch [4/120    avg_loss:2.528, val_acc:0.385]
Epoch [5/120    avg_loss:2.497, val_acc:0.415]
Epoch [6/120    avg_loss:2.468, val_acc:0.406]
Epoch [7/120    avg_loss:2.441, val_acc:0.400]
Epoch [8/120    avg_loss:2.423, val_acc:0.400]
Epoch [9/120    avg_loss:2.391, val_acc:0.390]
Epoch [10/120    avg_loss:2.359, val_acc:0.392]
Epoch [11/120    avg_loss:2.338, val_acc:0.392]
Epoch [12/120    avg_loss:2.310, val_acc:0.394]
Epoch [13/120    avg_loss:2.259, val_acc:0.398]
Epoch [14/120    avg_loss:2.218, val_acc:0.408]
Epoch [15/120    avg_loss:2.200, val_acc:0.438]
Epoch [16/120    avg_loss:2.159, val_acc:0.448]
Epoch [17/120    avg_loss:2.118, val_acc:0.475]
Epoch [18/120    avg_loss:2.077, val_acc:0.471]
Epoch [19/120    avg_loss:2.050, val_acc:0.485]
Epoch [20/120    avg_loss:1.986, val_acc:0.519]
Epoch [21/120    avg_loss:1.951, val_acc:0.531]
Epoch [22/120    avg_loss:1.914, val_acc:0.575]
Epoch [23/120    avg_loss:1.864, val_acc:0.571]
Epoch [24/120    avg_loss:1.837, val_acc:0.585]
Epoch [25/120    avg_loss:1.793, val_acc:0.623]
Epoch [26/120    avg_loss:1.730, val_acc:0.621]
Epoch [27/120    avg_loss:1.718, val_acc:0.656]
Epoch [28/120    avg_loss:1.653, val_acc:0.671]
Epoch [29/120    avg_loss:1.582, val_acc:0.698]
Epoch [30/120    avg_loss:1.525, val_acc:0.721]
Epoch [31/120    avg_loss:1.451, val_acc:0.721]
Epoch [32/120    avg_loss:1.420, val_acc:0.738]
Epoch [33/120    avg_loss:1.361, val_acc:0.744]
Epoch [34/120    avg_loss:1.336, val_acc:0.762]
Epoch [35/120    avg_loss:1.266, val_acc:0.750]
Epoch [36/120    avg_loss:1.189, val_acc:0.760]
Epoch [37/120    avg_loss:1.146, val_acc:0.744]
Epoch [38/120    avg_loss:1.151, val_acc:0.771]
Epoch [39/120    avg_loss:1.074, val_acc:0.781]
Epoch [40/120    avg_loss:0.992, val_acc:0.812]
Epoch [41/120    avg_loss:0.960, val_acc:0.827]
Epoch [42/120    avg_loss:0.920, val_acc:0.844]
Epoch [43/120    avg_loss:0.842, val_acc:0.867]
Epoch [44/120    avg_loss:0.813, val_acc:0.867]
Epoch [45/120    avg_loss:0.772, val_acc:0.923]
Epoch [46/120    avg_loss:0.772, val_acc:0.865]
Epoch [47/120    avg_loss:0.738, val_acc:0.896]
Epoch [48/120    avg_loss:0.693, val_acc:0.927]
Epoch [49/120    avg_loss:0.628, val_acc:0.946]
Epoch [50/120    avg_loss:0.591, val_acc:0.948]
Epoch [51/120    avg_loss:0.566, val_acc:0.942]
Epoch [52/120    avg_loss:0.573, val_acc:0.954]
Epoch [53/120    avg_loss:0.548, val_acc:0.946]
Epoch [54/120    avg_loss:0.535, val_acc:0.952]
Epoch [55/120    avg_loss:0.489, val_acc:0.938]
Epoch [56/120    avg_loss:0.471, val_acc:0.950]
Epoch [57/120    avg_loss:0.481, val_acc:0.923]
Epoch [58/120    avg_loss:0.461, val_acc:0.952]
Epoch [59/120    avg_loss:0.421, val_acc:0.950]
Epoch [60/120    avg_loss:0.450, val_acc:0.933]
Epoch [61/120    avg_loss:0.423, val_acc:0.958]
Epoch [62/120    avg_loss:0.372, val_acc:0.954]
Epoch [63/120    avg_loss:0.361, val_acc:0.950]
Epoch [64/120    avg_loss:0.327, val_acc:0.969]
Epoch [65/120    avg_loss:0.339, val_acc:0.965]
Epoch [66/120    avg_loss:0.358, val_acc:0.971]
Epoch [67/120    avg_loss:0.339, val_acc:0.969]
Epoch [68/120    avg_loss:0.320, val_acc:0.971]
Epoch [69/120    avg_loss:0.303, val_acc:0.948]
Epoch [70/120    avg_loss:0.349, val_acc:0.965]
Epoch [71/120    avg_loss:0.324, val_acc:0.969]
Epoch [72/120    avg_loss:0.294, val_acc:0.963]
Epoch [73/120    avg_loss:0.313, val_acc:0.979]
Epoch [74/120    avg_loss:0.257, val_acc:0.973]
Epoch [75/120    avg_loss:0.232, val_acc:0.977]
Epoch [76/120    avg_loss:0.242, val_acc:0.973]
Epoch [77/120    avg_loss:0.212, val_acc:0.975]
Epoch [78/120    avg_loss:0.190, val_acc:0.992]
Epoch [79/120    avg_loss:0.183, val_acc:0.981]
Epoch [80/120    avg_loss:0.191, val_acc:0.973]
Epoch [81/120    avg_loss:0.220, val_acc:0.975]
Epoch [82/120    avg_loss:0.176, val_acc:0.979]
Epoch [83/120    avg_loss:0.181, val_acc:0.969]
Epoch [84/120    avg_loss:0.166, val_acc:0.983]
Epoch [85/120    avg_loss:0.178, val_acc:0.956]
Epoch [86/120    avg_loss:0.258, val_acc:0.933]
Epoch [87/120    avg_loss:0.324, val_acc:0.965]
Epoch [88/120    avg_loss:0.222, val_acc:0.981]
Epoch [89/120    avg_loss:0.197, val_acc:0.975]
Epoch [90/120    avg_loss:0.189, val_acc:0.977]
Epoch [91/120    avg_loss:0.150, val_acc:0.985]
Epoch [92/120    avg_loss:0.138, val_acc:0.990]
Epoch [93/120    avg_loss:0.135, val_acc:0.990]
Epoch [94/120    avg_loss:0.126, val_acc:0.985]
Epoch [95/120    avg_loss:0.119, val_acc:0.990]
Epoch [96/120    avg_loss:0.119, val_acc:0.994]
Epoch [97/120    avg_loss:0.122, val_acc:0.994]
Epoch [98/120    avg_loss:0.124, val_acc:0.992]
Epoch [99/120    avg_loss:0.113, val_acc:0.992]
Epoch [100/120    avg_loss:0.110, val_acc:0.994]
Epoch [101/120    avg_loss:0.119, val_acc:0.996]
Epoch [102/120    avg_loss:0.115, val_acc:0.994]
Epoch [103/120    avg_loss:0.106, val_acc:0.994]
Epoch [104/120    avg_loss:0.109, val_acc:0.994]
Epoch [105/120    avg_loss:0.117, val_acc:0.994]
Epoch [106/120    avg_loss:0.108, val_acc:0.994]
Epoch [107/120    avg_loss:0.111, val_acc:0.994]
Epoch [108/120    avg_loss:0.114, val_acc:0.994]
Epoch [109/120    avg_loss:0.107, val_acc:0.990]
Epoch [110/120    avg_loss:0.107, val_acc:0.996]
Epoch [111/120    avg_loss:0.110, val_acc:0.996]
Epoch [112/120    avg_loss:0.105, val_acc:0.996]
Epoch [113/120    avg_loss:0.105, val_acc:0.990]
Epoch [114/120    avg_loss:0.100, val_acc:0.996]
Epoch [115/120    avg_loss:0.091, val_acc:0.996]
Epoch [116/120    avg_loss:0.093, val_acc:0.994]
Epoch [117/120    avg_loss:0.096, val_acc:0.994]
Epoch [118/120    avg_loss:0.094, val_acc:0.996]
Epoch [119/120    avg_loss:0.097, val_acc:0.992]
Epoch [120/120    avg_loss:0.094, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99707174 0.98206278 1.         0.94977169 0.92810458
 0.99038462 0.95555556 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919299405884513
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f071b3d2dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.327]
Epoch [2/120    avg_loss:2.559, val_acc:0.365]
Epoch [3/120    avg_loss:2.521, val_acc:0.362]
Epoch [4/120    avg_loss:2.497, val_acc:0.350]
Epoch [5/120    avg_loss:2.478, val_acc:0.342]
Epoch [6/120    avg_loss:2.446, val_acc:0.325]
Epoch [7/120    avg_loss:2.427, val_acc:0.321]
Epoch [8/120    avg_loss:2.399, val_acc:0.315]
Epoch [9/120    avg_loss:2.377, val_acc:0.327]
Epoch [10/120    avg_loss:2.355, val_acc:0.329]
Epoch [11/120    avg_loss:2.322, val_acc:0.338]
Epoch [12/120    avg_loss:2.297, val_acc:0.333]
Epoch [13/120    avg_loss:2.265, val_acc:0.317]
Epoch [14/120    avg_loss:2.249, val_acc:0.327]
Epoch [15/120    avg_loss:2.198, val_acc:0.344]
Epoch [16/120    avg_loss:2.178, val_acc:0.338]
Epoch [17/120    avg_loss:2.187, val_acc:0.338]
Epoch [18/120    avg_loss:2.169, val_acc:0.344]
Epoch [19/120    avg_loss:2.168, val_acc:0.338]
Epoch [20/120    avg_loss:2.152, val_acc:0.342]
Epoch [21/120    avg_loss:2.174, val_acc:0.346]
Epoch [22/120    avg_loss:2.156, val_acc:0.348]
Epoch [23/120    avg_loss:2.150, val_acc:0.350]
Epoch [24/120    avg_loss:2.144, val_acc:0.352]
Epoch [25/120    avg_loss:2.148, val_acc:0.352]
Epoch [26/120    avg_loss:2.128, val_acc:0.352]
Epoch [27/120    avg_loss:2.133, val_acc:0.352]
Epoch [28/120    avg_loss:2.123, val_acc:0.352]
Epoch [29/120    avg_loss:2.137, val_acc:0.352]
Epoch [30/120    avg_loss:2.136, val_acc:0.352]
Epoch [31/120    avg_loss:2.117, val_acc:0.352]
Epoch [32/120    avg_loss:2.132, val_acc:0.352]
Epoch [33/120    avg_loss:2.120, val_acc:0.352]
Epoch [34/120    avg_loss:2.118, val_acc:0.352]
Epoch [35/120    avg_loss:2.122, val_acc:0.352]
Epoch [36/120    avg_loss:2.133, val_acc:0.352]
Epoch [37/120    avg_loss:2.128, val_acc:0.352]
Epoch [38/120    avg_loss:2.133, val_acc:0.354]
Epoch [39/120    avg_loss:2.117, val_acc:0.354]
Epoch [40/120    avg_loss:2.126, val_acc:0.354]
Epoch [41/120    avg_loss:2.123, val_acc:0.354]
Epoch [42/120    avg_loss:2.115, val_acc:0.354]
Epoch [43/120    avg_loss:2.128, val_acc:0.354]
Epoch [44/120    avg_loss:2.116, val_acc:0.354]
Epoch [45/120    avg_loss:2.127, val_acc:0.354]
Epoch [46/120    avg_loss:2.124, val_acc:0.354]
Epoch [47/120    avg_loss:2.119, val_acc:0.354]
Epoch [48/120    avg_loss:2.116, val_acc:0.354]
Epoch [49/120    avg_loss:2.118, val_acc:0.354]
Epoch [50/120    avg_loss:2.125, val_acc:0.354]
Epoch [51/120    avg_loss:2.143, val_acc:0.354]
Epoch [52/120    avg_loss:2.125, val_acc:0.354]
Epoch [53/120    avg_loss:2.130, val_acc:0.354]
Epoch [54/120    avg_loss:2.138, val_acc:0.354]
Epoch [55/120    avg_loss:2.136, val_acc:0.354]
Epoch [56/120    avg_loss:2.125, val_acc:0.354]
Epoch [57/120    avg_loss:2.121, val_acc:0.354]
Epoch [58/120    avg_loss:2.127, val_acc:0.354]
Epoch [59/120    avg_loss:2.116, val_acc:0.354]
Epoch [60/120    avg_loss:2.111, val_acc:0.354]
Epoch [61/120    avg_loss:2.123, val_acc:0.354]
Epoch [62/120    avg_loss:2.123, val_acc:0.354]
Epoch [63/120    avg_loss:2.118, val_acc:0.354]
Epoch [64/120    avg_loss:2.143, val_acc:0.354]
Epoch [65/120    avg_loss:2.118, val_acc:0.354]
Epoch [66/120    avg_loss:2.136, val_acc:0.354]
Epoch [67/120    avg_loss:2.128, val_acc:0.354]
Epoch [68/120    avg_loss:2.132, val_acc:0.354]
Epoch [69/120    avg_loss:2.128, val_acc:0.354]
Epoch [70/120    avg_loss:2.122, val_acc:0.354]
Epoch [71/120    avg_loss:2.128, val_acc:0.354]
Epoch [72/120    avg_loss:2.123, val_acc:0.354]
Epoch [73/120    avg_loss:2.118, val_acc:0.354]
Epoch [74/120    avg_loss:2.110, val_acc:0.354]
Epoch [75/120    avg_loss:2.135, val_acc:0.354]
Epoch [76/120    avg_loss:2.118, val_acc:0.354]
Epoch [77/120    avg_loss:2.116, val_acc:0.354]
Epoch [78/120    avg_loss:2.120, val_acc:0.354]
Epoch [79/120    avg_loss:2.123, val_acc:0.354]
Epoch [80/120    avg_loss:2.117, val_acc:0.354]
Epoch [81/120    avg_loss:2.116, val_acc:0.354]
Epoch [82/120    avg_loss:2.109, val_acc:0.354]
Epoch [83/120    avg_loss:2.125, val_acc:0.354]
Epoch [84/120    avg_loss:2.131, val_acc:0.354]
Epoch [85/120    avg_loss:2.113, val_acc:0.354]
Epoch [86/120    avg_loss:2.129, val_acc:0.354]
Epoch [87/120    avg_loss:2.117, val_acc:0.354]
Epoch [88/120    avg_loss:2.124, val_acc:0.354]
Epoch [89/120    avg_loss:2.128, val_acc:0.354]
Epoch [90/120    avg_loss:2.130, val_acc:0.354]
Epoch [91/120    avg_loss:2.142, val_acc:0.354]
Epoch [92/120    avg_loss:2.119, val_acc:0.354]
Epoch [93/120    avg_loss:2.150, val_acc:0.354]
Epoch [94/120    avg_loss:2.119, val_acc:0.354]
Epoch [95/120    avg_loss:2.117, val_acc:0.354]
Epoch [96/120    avg_loss:2.118, val_acc:0.354]
Epoch [97/120    avg_loss:2.132, val_acc:0.354]
Epoch [98/120    avg_loss:2.138, val_acc:0.354]
Epoch [99/120    avg_loss:2.125, val_acc:0.354]
Epoch [100/120    avg_loss:2.119, val_acc:0.354]
Epoch [101/120    avg_loss:2.117, val_acc:0.354]
Epoch [102/120    avg_loss:2.114, val_acc:0.354]
Epoch [103/120    avg_loss:2.116, val_acc:0.354]
Epoch [104/120    avg_loss:2.116, val_acc:0.354]
Epoch [105/120    avg_loss:2.110, val_acc:0.354]
Epoch [106/120    avg_loss:2.121, val_acc:0.354]
Epoch [107/120    avg_loss:2.101, val_acc:0.354]
Epoch [108/120    avg_loss:2.137, val_acc:0.354]
Epoch [109/120    avg_loss:2.128, val_acc:0.354]
Epoch [110/120    avg_loss:2.118, val_acc:0.354]
Epoch [111/120    avg_loss:2.119, val_acc:0.354]
Epoch [112/120    avg_loss:2.119, val_acc:0.354]
Epoch [113/120    avg_loss:2.127, val_acc:0.354]
Epoch [114/120    avg_loss:2.116, val_acc:0.354]
Epoch [115/120    avg_loss:2.120, val_acc:0.354]
Epoch [116/120    avg_loss:2.141, val_acc:0.354]
Epoch [117/120    avg_loss:2.112, val_acc:0.354]
Epoch [118/120    avg_loss:2.133, val_acc:0.354]
Epoch [119/120    avg_loss:2.119, val_acc:0.354]
Epoch [120/120    avg_loss:2.111, val_acc:0.354]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0  64   0 455  32   0   0   0   0 134]
 [  0   0   0   0   0   0   0 219   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  60 170   0   0   0   0   0]
 [  0   0   0   0   0  50   0  75  89   0   0   0   2  11]
 [  0   0   0   0   0  55   0  52  34   0   0   1   0   3]
 [  0   0   0   0   0   5   0 184   4   0   0   0   0  13]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   1 219   0   0 168   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 468   0   0]
 [  0   0   0   0   0   0   0   0   0  96   0   9 195  64]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0  22   0   0   3   0   0   0 326  46  56]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
34.64818763326226

F1 scores:
[       nan 0.         0.         0.         0.         0.34482759
 0.         0.1519806  0.46794872 0.         0.         0.4368482
 0.13218391 0.8558235 ]

Kappa:
0.2831328245310269
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3452484dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.645, val_acc:0.073]
Epoch [2/120    avg_loss:2.621, val_acc:0.079]
Epoch [3/120    avg_loss:2.599, val_acc:0.185]
Epoch [4/120    avg_loss:2.571, val_acc:0.181]
Epoch [5/120    avg_loss:2.551, val_acc:0.181]
Epoch [6/120    avg_loss:2.524, val_acc:0.173]
Epoch [7/120    avg_loss:2.503, val_acc:0.183]
Epoch [8/120    avg_loss:2.476, val_acc:0.275]
Epoch [9/120    avg_loss:2.449, val_acc:0.362]
Epoch [10/120    avg_loss:2.424, val_acc:0.385]
Epoch [11/120    avg_loss:2.406, val_acc:0.427]
Epoch [12/120    avg_loss:2.373, val_acc:0.458]
Epoch [13/120    avg_loss:2.350, val_acc:0.473]
Epoch [14/120    avg_loss:2.327, val_acc:0.481]
Epoch [15/120    avg_loss:2.298, val_acc:0.498]
Epoch [16/120    avg_loss:2.264, val_acc:0.523]
Epoch [17/120    avg_loss:2.223, val_acc:0.529]
Epoch [18/120    avg_loss:2.186, val_acc:0.535]
Epoch [19/120    avg_loss:2.147, val_acc:0.550]
Epoch [20/120    avg_loss:2.106, val_acc:0.565]
Epoch [21/120    avg_loss:2.073, val_acc:0.596]
Epoch [22/120    avg_loss:2.034, val_acc:0.610]
Epoch [23/120    avg_loss:1.984, val_acc:0.606]
Epoch [24/120    avg_loss:1.931, val_acc:0.588]
Epoch [25/120    avg_loss:1.893, val_acc:0.604]
Epoch [26/120    avg_loss:1.829, val_acc:0.621]
Epoch [27/120    avg_loss:1.781, val_acc:0.656]
Epoch [28/120    avg_loss:1.723, val_acc:0.665]
Epoch [29/120    avg_loss:1.695, val_acc:0.619]
Epoch [30/120    avg_loss:1.643, val_acc:0.671]
Epoch [31/120    avg_loss:1.603, val_acc:0.696]
Epoch [32/120    avg_loss:1.529, val_acc:0.729]
Epoch [33/120    avg_loss:1.471, val_acc:0.738]
Epoch [34/120    avg_loss:1.426, val_acc:0.796]
Epoch [35/120    avg_loss:1.350, val_acc:0.777]
Epoch [36/120    avg_loss:1.294, val_acc:0.848]
Epoch [37/120    avg_loss:1.236, val_acc:0.819]
Epoch [38/120    avg_loss:1.143, val_acc:0.852]
Epoch [39/120    avg_loss:1.077, val_acc:0.890]
Epoch [40/120    avg_loss:1.002, val_acc:0.904]
Epoch [41/120    avg_loss:0.998, val_acc:0.902]
Epoch [42/120    avg_loss:0.915, val_acc:0.912]
Epoch [43/120    avg_loss:0.889, val_acc:0.906]
Epoch [44/120    avg_loss:0.820, val_acc:0.931]
Epoch [45/120    avg_loss:0.731, val_acc:0.938]
Epoch [46/120    avg_loss:0.716, val_acc:0.919]
Epoch [47/120    avg_loss:0.707, val_acc:0.860]
Epoch [48/120    avg_loss:0.708, val_acc:0.946]
Epoch [49/120    avg_loss:0.611, val_acc:0.958]
Epoch [50/120    avg_loss:0.590, val_acc:0.925]
Epoch [51/120    avg_loss:0.530, val_acc:0.942]
Epoch [52/120    avg_loss:0.495, val_acc:0.940]
Epoch [53/120    avg_loss:0.528, val_acc:0.960]
Epoch [54/120    avg_loss:0.474, val_acc:0.938]
Epoch [55/120    avg_loss:0.478, val_acc:0.921]
Epoch [56/120    avg_loss:0.472, val_acc:0.906]
Epoch [57/120    avg_loss:0.448, val_acc:0.944]
Epoch [58/120    avg_loss:0.427, val_acc:0.958]
Epoch [59/120    avg_loss:0.369, val_acc:0.956]
Epoch [60/120    avg_loss:0.363, val_acc:0.948]
Epoch [61/120    avg_loss:0.386, val_acc:0.963]
Epoch [62/120    avg_loss:0.349, val_acc:0.938]
Epoch [63/120    avg_loss:0.311, val_acc:0.960]
Epoch [64/120    avg_loss:0.362, val_acc:0.958]
Epoch [65/120    avg_loss:0.331, val_acc:0.935]
Epoch [66/120    avg_loss:0.305, val_acc:0.946]
Epoch [67/120    avg_loss:0.319, val_acc:0.960]
Epoch [68/120    avg_loss:0.345, val_acc:0.969]
Epoch [69/120    avg_loss:0.284, val_acc:0.969]
Epoch [70/120    avg_loss:0.267, val_acc:0.963]
Epoch [71/120    avg_loss:0.277, val_acc:0.973]
Epoch [72/120    avg_loss:0.230, val_acc:0.975]
Epoch [73/120    avg_loss:0.206, val_acc:0.981]
Epoch [74/120    avg_loss:0.212, val_acc:0.967]
Epoch [75/120    avg_loss:0.224, val_acc:0.971]
Epoch [76/120    avg_loss:0.196, val_acc:0.977]
Epoch [77/120    avg_loss:0.207, val_acc:0.967]
Epoch [78/120    avg_loss:0.234, val_acc:0.983]
Epoch [79/120    avg_loss:0.213, val_acc:0.971]
Epoch [80/120    avg_loss:0.201, val_acc:0.925]
Epoch [81/120    avg_loss:0.226, val_acc:0.967]
Epoch [82/120    avg_loss:0.201, val_acc:0.971]
Epoch [83/120    avg_loss:0.182, val_acc:0.981]
Epoch [84/120    avg_loss:0.182, val_acc:0.983]
Epoch [85/120    avg_loss:0.210, val_acc:0.952]
Epoch [86/120    avg_loss:0.357, val_acc:0.938]
Epoch [87/120    avg_loss:0.367, val_acc:0.956]
Epoch [88/120    avg_loss:0.291, val_acc:0.965]
Epoch [89/120    avg_loss:0.219, val_acc:0.969]
Epoch [90/120    avg_loss:0.208, val_acc:0.963]
Epoch [91/120    avg_loss:0.188, val_acc:0.963]
Epoch [92/120    avg_loss:0.151, val_acc:0.983]
Epoch [93/120    avg_loss:0.132, val_acc:0.977]
Epoch [94/120    avg_loss:0.138, val_acc:0.979]
Epoch [95/120    avg_loss:0.171, val_acc:0.948]
Epoch [96/120    avg_loss:0.149, val_acc:0.983]
Epoch [97/120    avg_loss:0.140, val_acc:0.985]
Epoch [98/120    avg_loss:0.153, val_acc:0.921]
Epoch [99/120    avg_loss:0.186, val_acc:0.979]
Epoch [100/120    avg_loss:0.149, val_acc:0.963]
Epoch [101/120    avg_loss:0.184, val_acc:0.977]
Epoch [102/120    avg_loss:0.143, val_acc:0.979]
Epoch [103/120    avg_loss:0.169, val_acc:0.983]
Epoch [104/120    avg_loss:0.122, val_acc:0.977]
Epoch [105/120    avg_loss:0.157, val_acc:0.929]
Epoch [106/120    avg_loss:0.159, val_acc:0.983]
Epoch [107/120    avg_loss:0.152, val_acc:0.965]
Epoch [108/120    avg_loss:0.166, val_acc:0.979]
Epoch [109/120    avg_loss:0.129, val_acc:0.981]
Epoch [110/120    avg_loss:0.112, val_acc:0.979]
Epoch [111/120    avg_loss:0.090, val_acc:0.983]
Epoch [112/120    avg_loss:0.083, val_acc:0.985]
Epoch [113/120    avg_loss:0.077, val_acc:0.985]
Epoch [114/120    avg_loss:0.080, val_acc:0.992]
Epoch [115/120    avg_loss:0.080, val_acc:0.992]
Epoch [116/120    avg_loss:0.079, val_acc:0.992]
Epoch [117/120    avg_loss:0.086, val_acc:0.992]
Epoch [118/120    avg_loss:0.075, val_acc:0.992]
Epoch [119/120    avg_loss:0.072, val_acc:0.992]
Epoch [120/120    avg_loss:0.073, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 198  27   0   0   0   0   0   0   2   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.98648649 1.         0.92523364 0.9044586
 1.         0.96703297 1.         1.         1.         0.98691099
 0.98663697 1.        ]

Kappa:
0.9886065701707132
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f495ef73e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.056]
Epoch [2/120    avg_loss:2.616, val_acc:0.212]
Epoch [3/120    avg_loss:2.586, val_acc:0.335]
Epoch [4/120    avg_loss:2.560, val_acc:0.371]
Epoch [5/120    avg_loss:2.535, val_acc:0.390]
Epoch [6/120    avg_loss:2.511, val_acc:0.392]
Epoch [7/120    avg_loss:2.488, val_acc:0.402]
Epoch [8/120    avg_loss:2.464, val_acc:0.408]
Epoch [9/120    avg_loss:2.429, val_acc:0.440]
Epoch [10/120    avg_loss:2.405, val_acc:0.483]
Epoch [11/120    avg_loss:2.372, val_acc:0.579]
Epoch [12/120    avg_loss:2.339, val_acc:0.619]
Epoch [13/120    avg_loss:2.309, val_acc:0.640]
Epoch [14/120    avg_loss:2.271, val_acc:0.648]
Epoch [15/120    avg_loss:2.241, val_acc:0.644]
Epoch [16/120    avg_loss:2.192, val_acc:0.633]
Epoch [17/120    avg_loss:2.152, val_acc:0.660]
Epoch [18/120    avg_loss:2.105, val_acc:0.669]
Epoch [19/120    avg_loss:2.068, val_acc:0.650]
Epoch [20/120    avg_loss:2.008, val_acc:0.658]
Epoch [21/120    avg_loss:1.952, val_acc:0.650]
Epoch [22/120    avg_loss:1.905, val_acc:0.656]
Epoch [23/120    avg_loss:1.846, val_acc:0.656]
Epoch [24/120    avg_loss:1.798, val_acc:0.646]
Epoch [25/120    avg_loss:1.733, val_acc:0.671]
Epoch [26/120    avg_loss:1.681, val_acc:0.715]
Epoch [27/120    avg_loss:1.621, val_acc:0.685]
Epoch [28/120    avg_loss:1.556, val_acc:0.681]
Epoch [29/120    avg_loss:1.498, val_acc:0.706]
Epoch [30/120    avg_loss:1.417, val_acc:0.727]
Epoch [31/120    avg_loss:1.373, val_acc:0.756]
Epoch [32/120    avg_loss:1.314, val_acc:0.748]
Epoch [33/120    avg_loss:1.310, val_acc:0.787]
Epoch [34/120    avg_loss:1.258, val_acc:0.787]
Epoch [35/120    avg_loss:1.169, val_acc:0.758]
Epoch [36/120    avg_loss:1.112, val_acc:0.823]
Epoch [37/120    avg_loss:1.065, val_acc:0.827]
Epoch [38/120    avg_loss:0.983, val_acc:0.850]
Epoch [39/120    avg_loss:0.959, val_acc:0.800]
Epoch [40/120    avg_loss:0.919, val_acc:0.863]
Epoch [41/120    avg_loss:0.865, val_acc:0.892]
Epoch [42/120    avg_loss:0.790, val_acc:0.931]
Epoch [43/120    avg_loss:0.746, val_acc:0.898]
Epoch [44/120    avg_loss:0.730, val_acc:0.873]
Epoch [45/120    avg_loss:0.659, val_acc:0.956]
Epoch [46/120    avg_loss:0.613, val_acc:0.948]
Epoch [47/120    avg_loss:0.573, val_acc:0.938]
Epoch [48/120    avg_loss:0.599, val_acc:0.925]
Epoch [49/120    avg_loss:0.558, val_acc:0.954]
Epoch [50/120    avg_loss:0.590, val_acc:0.883]
Epoch [51/120    avg_loss:0.570, val_acc:0.948]
Epoch [52/120    avg_loss:0.471, val_acc:0.956]
Epoch [53/120    avg_loss:0.474, val_acc:0.925]
Epoch [54/120    avg_loss:0.467, val_acc:0.969]
Epoch [55/120    avg_loss:0.430, val_acc:0.956]
Epoch [56/120    avg_loss:0.407, val_acc:0.942]
Epoch [57/120    avg_loss:0.395, val_acc:0.956]
Epoch [58/120    avg_loss:0.398, val_acc:0.927]
Epoch [59/120    avg_loss:0.368, val_acc:0.963]
Epoch [60/120    avg_loss:0.380, val_acc:0.969]
Epoch [61/120    avg_loss:0.367, val_acc:0.958]
Epoch [62/120    avg_loss:0.352, val_acc:0.954]
Epoch [63/120    avg_loss:0.341, val_acc:0.940]
Epoch [64/120    avg_loss:0.317, val_acc:0.952]
Epoch [65/120    avg_loss:0.345, val_acc:0.967]
Epoch [66/120    avg_loss:0.257, val_acc:0.977]
Epoch [67/120    avg_loss:0.225, val_acc:0.958]
Epoch [68/120    avg_loss:0.266, val_acc:0.963]
Epoch [69/120    avg_loss:0.278, val_acc:0.967]
Epoch [70/120    avg_loss:0.293, val_acc:0.965]
Epoch [71/120    avg_loss:0.242, val_acc:0.975]
Epoch [72/120    avg_loss:0.226, val_acc:0.960]
Epoch [73/120    avg_loss:0.220, val_acc:0.977]
Epoch [74/120    avg_loss:0.244, val_acc:0.971]
Epoch [75/120    avg_loss:0.263, val_acc:0.977]
Epoch [76/120    avg_loss:0.261, val_acc:0.956]
Epoch [77/120    avg_loss:0.247, val_acc:0.979]
Epoch [78/120    avg_loss:0.211, val_acc:0.956]
Epoch [79/120    avg_loss:0.250, val_acc:0.950]
Epoch [80/120    avg_loss:0.207, val_acc:0.952]
Epoch [81/120    avg_loss:0.198, val_acc:0.985]
Epoch [82/120    avg_loss:0.238, val_acc:0.967]
Epoch [83/120    avg_loss:0.239, val_acc:0.971]
Epoch [84/120    avg_loss:0.194, val_acc:0.985]
Epoch [85/120    avg_loss:0.152, val_acc:0.988]
Epoch [86/120    avg_loss:0.137, val_acc:0.979]
Epoch [87/120    avg_loss:0.162, val_acc:0.948]
Epoch [88/120    avg_loss:0.156, val_acc:0.988]
Epoch [89/120    avg_loss:0.154, val_acc:0.990]
Epoch [90/120    avg_loss:0.118, val_acc:0.992]
Epoch [91/120    avg_loss:0.104, val_acc:0.992]
Epoch [92/120    avg_loss:0.131, val_acc:0.985]
Epoch [93/120    avg_loss:0.132, val_acc:0.979]
Epoch [94/120    avg_loss:0.128, val_acc:0.985]
Epoch [95/120    avg_loss:0.140, val_acc:0.994]
Epoch [96/120    avg_loss:0.096, val_acc:0.992]
Epoch [97/120    avg_loss:0.117, val_acc:0.992]
Epoch [98/120    avg_loss:0.100, val_acc:0.985]
Epoch [99/120    avg_loss:0.102, val_acc:0.992]
Epoch [100/120    avg_loss:0.105, val_acc:0.985]
Epoch [101/120    avg_loss:0.087, val_acc:0.983]
Epoch [102/120    avg_loss:0.144, val_acc:0.988]
Epoch [103/120    avg_loss:0.101, val_acc:0.971]
Epoch [104/120    avg_loss:0.148, val_acc:0.979]
Epoch [105/120    avg_loss:0.112, val_acc:0.985]
Epoch [106/120    avg_loss:0.090, val_acc:0.992]
Epoch [107/120    avg_loss:0.079, val_acc:0.990]
Epoch [108/120    avg_loss:0.097, val_acc:0.985]
Epoch [109/120    avg_loss:0.086, val_acc:0.992]
Epoch [110/120    avg_loss:0.065, val_acc:0.994]
Epoch [111/120    avg_loss:0.058, val_acc:0.994]
Epoch [112/120    avg_loss:0.056, val_acc:0.994]
Epoch [113/120    avg_loss:0.061, val_acc:0.994]
Epoch [114/120    avg_loss:0.052, val_acc:0.994]
Epoch [115/120    avg_loss:0.050, val_acc:0.994]
Epoch [116/120    avg_loss:0.053, val_acc:0.996]
Epoch [117/120    avg_loss:0.053, val_acc:0.994]
Epoch [118/120    avg_loss:0.064, val_acc:0.996]
Epoch [119/120    avg_loss:0.051, val_acc:0.994]
Epoch [120/120    avg_loss:0.051, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99853801 0.98426966 1.         0.95652174 0.93811075
 0.99516908 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9933538161176235
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd4e5495e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.171]
Epoch [2/120    avg_loss:2.601, val_acc:0.217]
Epoch [3/120    avg_loss:2.579, val_acc:0.225]
Epoch [4/120    avg_loss:2.559, val_acc:0.300]
Epoch [5/120    avg_loss:2.538, val_acc:0.308]
Epoch [6/120    avg_loss:2.515, val_acc:0.310]
Epoch [7/120    avg_loss:2.490, val_acc:0.329]
Epoch [8/120    avg_loss:2.471, val_acc:0.362]
Epoch [9/120    avg_loss:2.437, val_acc:0.373]
Epoch [10/120    avg_loss:2.416, val_acc:0.383]
Epoch [11/120    avg_loss:2.388, val_acc:0.412]
Epoch [12/120    avg_loss:2.349, val_acc:0.446]
Epoch [13/120    avg_loss:2.322, val_acc:0.481]
Epoch [14/120    avg_loss:2.300, val_acc:0.504]
Epoch [15/120    avg_loss:2.261, val_acc:0.548]
Epoch [16/120    avg_loss:2.232, val_acc:0.575]
Epoch [17/120    avg_loss:2.181, val_acc:0.596]
Epoch [18/120    avg_loss:2.143, val_acc:0.600]
Epoch [19/120    avg_loss:2.122, val_acc:0.592]
Epoch [20/120    avg_loss:2.069, val_acc:0.598]
Epoch [21/120    avg_loss:2.034, val_acc:0.613]
Epoch [22/120    avg_loss:1.987, val_acc:0.642]
Epoch [23/120    avg_loss:1.932, val_acc:0.654]
Epoch [24/120    avg_loss:1.882, val_acc:0.675]
Epoch [25/120    avg_loss:1.826, val_acc:0.710]
Epoch [26/120    avg_loss:1.773, val_acc:0.710]
Epoch [27/120    avg_loss:1.740, val_acc:0.740]
Epoch [28/120    avg_loss:1.661, val_acc:0.796]
Epoch [29/120    avg_loss:1.601, val_acc:0.790]
Epoch [30/120    avg_loss:1.590, val_acc:0.746]
Epoch [31/120    avg_loss:1.533, val_acc:0.790]
Epoch [32/120    avg_loss:1.495, val_acc:0.831]
Epoch [33/120    avg_loss:1.415, val_acc:0.810]
Epoch [34/120    avg_loss:1.369, val_acc:0.769]
Epoch [35/120    avg_loss:1.286, val_acc:0.806]
Epoch [36/120    avg_loss:1.232, val_acc:0.869]
Epoch [37/120    avg_loss:1.171, val_acc:0.863]
Epoch [38/120    avg_loss:1.122, val_acc:0.854]
Epoch [39/120    avg_loss:1.083, val_acc:0.838]
Epoch [40/120    avg_loss:1.020, val_acc:0.852]
Epoch [41/120    avg_loss:1.009, val_acc:0.879]
Epoch [42/120    avg_loss:0.902, val_acc:0.877]
Epoch [43/120    avg_loss:0.851, val_acc:0.879]
Epoch [44/120    avg_loss:0.806, val_acc:0.912]
Epoch [45/120    avg_loss:0.771, val_acc:0.933]
Epoch [46/120    avg_loss:0.753, val_acc:0.908]
Epoch [47/120    avg_loss:0.719, val_acc:0.931]
Epoch [48/120    avg_loss:0.666, val_acc:0.887]
Epoch [49/120    avg_loss:0.622, val_acc:0.912]
Epoch [50/120    avg_loss:0.596, val_acc:0.935]
Epoch [51/120    avg_loss:0.632, val_acc:0.863]
Epoch [52/120    avg_loss:0.578, val_acc:0.912]
Epoch [53/120    avg_loss:0.520, val_acc:0.929]
Epoch [54/120    avg_loss:0.495, val_acc:0.917]
Epoch [55/120    avg_loss:0.497, val_acc:0.925]
Epoch [56/120    avg_loss:0.447, val_acc:0.933]
Epoch [57/120    avg_loss:0.444, val_acc:0.927]
Epoch [58/120    avg_loss:0.478, val_acc:0.933]
Epoch [59/120    avg_loss:0.421, val_acc:0.908]
Epoch [60/120    avg_loss:0.450, val_acc:0.935]
Epoch [61/120    avg_loss:0.432, val_acc:0.933]
Epoch [62/120    avg_loss:0.379, val_acc:0.946]
Epoch [63/120    avg_loss:0.429, val_acc:0.931]
Epoch [64/120    avg_loss:0.356, val_acc:0.944]
Epoch [65/120    avg_loss:0.420, val_acc:0.929]
Epoch [66/120    avg_loss:0.441, val_acc:0.933]
Epoch [67/120    avg_loss:0.327, val_acc:0.933]
Epoch [68/120    avg_loss:0.351, val_acc:0.963]
Epoch [69/120    avg_loss:0.307, val_acc:0.965]
Epoch [70/120    avg_loss:0.305, val_acc:0.942]
Epoch [71/120    avg_loss:0.299, val_acc:0.944]
Epoch [72/120    avg_loss:0.324, val_acc:0.929]
Epoch [73/120    avg_loss:0.292, val_acc:0.938]
Epoch [74/120    avg_loss:0.318, val_acc:0.954]
Epoch [75/120    avg_loss:0.358, val_acc:0.971]
Epoch [76/120    avg_loss:0.294, val_acc:0.942]
Epoch [77/120    avg_loss:0.333, val_acc:0.958]
Epoch [78/120    avg_loss:0.252, val_acc:0.967]
Epoch [79/120    avg_loss:0.217, val_acc:0.975]
Epoch [80/120    avg_loss:0.205, val_acc:0.960]
Epoch [81/120    avg_loss:0.197, val_acc:0.979]
Epoch [82/120    avg_loss:0.187, val_acc:0.983]
Epoch [83/120    avg_loss:0.202, val_acc:0.981]
Epoch [84/120    avg_loss:0.185, val_acc:0.983]
Epoch [85/120    avg_loss:0.216, val_acc:0.977]
Epoch [86/120    avg_loss:0.182, val_acc:0.960]
Epoch [87/120    avg_loss:0.174, val_acc:0.977]
Epoch [88/120    avg_loss:0.159, val_acc:0.985]
Epoch [89/120    avg_loss:0.137, val_acc:0.985]
Epoch [90/120    avg_loss:0.140, val_acc:0.983]
Epoch [91/120    avg_loss:0.145, val_acc:0.977]
Epoch [92/120    avg_loss:0.131, val_acc:0.981]
Epoch [93/120    avg_loss:0.133, val_acc:0.983]
Epoch [94/120    avg_loss:0.130, val_acc:0.985]
Epoch [95/120    avg_loss:0.150, val_acc:0.969]
Epoch [96/120    avg_loss:0.135, val_acc:0.994]
Epoch [97/120    avg_loss:0.141, val_acc:0.981]
Epoch [98/120    avg_loss:0.129, val_acc:0.969]
Epoch [99/120    avg_loss:0.127, val_acc:0.985]
Epoch [100/120    avg_loss:0.145, val_acc:0.985]
Epoch [101/120    avg_loss:0.120, val_acc:0.992]
Epoch [102/120    avg_loss:0.100, val_acc:0.990]
Epoch [103/120    avg_loss:0.117, val_acc:0.983]
Epoch [104/120    avg_loss:0.150, val_acc:0.990]
Epoch [105/120    avg_loss:0.132, val_acc:0.983]
Epoch [106/120    avg_loss:0.152, val_acc:0.963]
Epoch [107/120    avg_loss:0.194, val_acc:0.967]
Epoch [108/120    avg_loss:0.178, val_acc:0.983]
Epoch [109/120    avg_loss:0.152, val_acc:0.973]
Epoch [110/120    avg_loss:0.130, val_acc:0.985]
Epoch [111/120    avg_loss:0.100, val_acc:0.988]
Epoch [112/120    avg_loss:0.094, val_acc:0.990]
Epoch [113/120    avg_loss:0.105, val_acc:0.992]
Epoch [114/120    avg_loss:0.109, val_acc:0.994]
Epoch [115/120    avg_loss:0.080, val_acc:0.992]
Epoch [116/120    avg_loss:0.097, val_acc:0.992]
Epoch [117/120    avg_loss:0.083, val_acc:0.992]
Epoch [118/120    avg_loss:0.079, val_acc:0.990]
Epoch [119/120    avg_loss:0.085, val_acc:0.992]
Epoch [120/120    avg_loss:0.077, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99780541 0.97986577 1.         0.95652174 0.93811075
 0.99277108 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9926418102675164
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6b58c8dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.653, val_acc:0.081]
Epoch [2/120    avg_loss:2.620, val_acc:0.117]
Epoch [3/120    avg_loss:2.594, val_acc:0.146]
Epoch [4/120    avg_loss:2.566, val_acc:0.246]
Epoch [5/120    avg_loss:2.548, val_acc:0.371]
Epoch [6/120    avg_loss:2.517, val_acc:0.423]
Epoch [7/120    avg_loss:2.497, val_acc:0.444]
Epoch [8/120    avg_loss:2.477, val_acc:0.473]
Epoch [9/120    avg_loss:2.454, val_acc:0.483]
Epoch [10/120    avg_loss:2.426, val_acc:0.508]
Epoch [11/120    avg_loss:2.402, val_acc:0.519]
Epoch [12/120    avg_loss:2.373, val_acc:0.525]
Epoch [13/120    avg_loss:2.351, val_acc:0.527]
Epoch [14/120    avg_loss:2.322, val_acc:0.537]
Epoch [15/120    avg_loss:2.292, val_acc:0.546]
Epoch [16/120    avg_loss:2.254, val_acc:0.556]
Epoch [17/120    avg_loss:2.219, val_acc:0.562]
Epoch [18/120    avg_loss:2.184, val_acc:0.579]
Epoch [19/120    avg_loss:2.126, val_acc:0.598]
Epoch [20/120    avg_loss:2.092, val_acc:0.585]
Epoch [21/120    avg_loss:2.039, val_acc:0.613]
Epoch [22/120    avg_loss:1.980, val_acc:0.635]
Epoch [23/120    avg_loss:1.938, val_acc:0.619]
Epoch [24/120    avg_loss:1.870, val_acc:0.644]
Epoch [25/120    avg_loss:1.822, val_acc:0.671]
Epoch [26/120    avg_loss:1.748, val_acc:0.708]
Epoch [27/120    avg_loss:1.709, val_acc:0.721]
Epoch [28/120    avg_loss:1.629, val_acc:0.729]
Epoch [29/120    avg_loss:1.553, val_acc:0.798]
Epoch [30/120    avg_loss:1.516, val_acc:0.738]
Epoch [31/120    avg_loss:1.447, val_acc:0.777]
Epoch [32/120    avg_loss:1.401, val_acc:0.787]
Epoch [33/120    avg_loss:1.357, val_acc:0.860]
Epoch [34/120    avg_loss:1.276, val_acc:0.773]
Epoch [35/120    avg_loss:1.217, val_acc:0.858]
Epoch [36/120    avg_loss:1.127, val_acc:0.900]
Epoch [37/120    avg_loss:1.083, val_acc:0.863]
Epoch [38/120    avg_loss:1.036, val_acc:0.908]
Epoch [39/120    avg_loss:0.966, val_acc:0.904]
Epoch [40/120    avg_loss:0.922, val_acc:0.877]
Epoch [41/120    avg_loss:0.878, val_acc:0.873]
Epoch [42/120    avg_loss:0.863, val_acc:0.910]
Epoch [43/120    avg_loss:0.775, val_acc:0.931]
Epoch [44/120    avg_loss:0.730, val_acc:0.942]
Epoch [45/120    avg_loss:0.676, val_acc:0.944]
Epoch [46/120    avg_loss:0.643, val_acc:0.938]
Epoch [47/120    avg_loss:0.635, val_acc:0.923]
Epoch [48/120    avg_loss:0.624, val_acc:0.942]
Epoch [49/120    avg_loss:0.595, val_acc:0.958]
Epoch [50/120    avg_loss:0.593, val_acc:0.942]
Epoch [51/120    avg_loss:0.550, val_acc:0.954]
Epoch [52/120    avg_loss:0.472, val_acc:0.956]
Epoch [53/120    avg_loss:0.454, val_acc:0.950]
Epoch [54/120    avg_loss:0.432, val_acc:0.933]
Epoch [55/120    avg_loss:0.468, val_acc:0.952]
Epoch [56/120    avg_loss:0.431, val_acc:0.954]
Epoch [57/120    avg_loss:0.430, val_acc:0.942]
Epoch [58/120    avg_loss:0.402, val_acc:0.933]
Epoch [59/120    avg_loss:0.375, val_acc:0.954]
Epoch [60/120    avg_loss:0.387, val_acc:0.956]
Epoch [61/120    avg_loss:0.354, val_acc:0.956]
Epoch [62/120    avg_loss:0.372, val_acc:0.965]
Epoch [63/120    avg_loss:0.308, val_acc:0.965]
Epoch [64/120    avg_loss:0.291, val_acc:0.965]
Epoch [65/120    avg_loss:0.294, val_acc:0.946]
Epoch [66/120    avg_loss:0.322, val_acc:0.960]
Epoch [67/120    avg_loss:0.272, val_acc:0.969]
Epoch [68/120    avg_loss:0.260, val_acc:0.956]
Epoch [69/120    avg_loss:0.272, val_acc:0.956]
Epoch [70/120    avg_loss:0.254, val_acc:0.965]
Epoch [71/120    avg_loss:0.304, val_acc:0.971]
Epoch [72/120    avg_loss:0.245, val_acc:0.948]
Epoch [73/120    avg_loss:0.231, val_acc:0.958]
Epoch [74/120    avg_loss:0.246, val_acc:0.960]
Epoch [75/120    avg_loss:0.264, val_acc:0.919]
Epoch [76/120    avg_loss:0.261, val_acc:0.963]
Epoch [77/120    avg_loss:0.241, val_acc:0.973]
Epoch [78/120    avg_loss:0.223, val_acc:0.975]
Epoch [79/120    avg_loss:0.188, val_acc:0.979]
Epoch [80/120    avg_loss:0.171, val_acc:0.963]
Epoch [81/120    avg_loss:0.155, val_acc:0.967]
Epoch [82/120    avg_loss:0.181, val_acc:0.969]
Epoch [83/120    avg_loss:0.154, val_acc:0.975]
Epoch [84/120    avg_loss:0.207, val_acc:0.971]
Epoch [85/120    avg_loss:0.184, val_acc:0.952]
Epoch [86/120    avg_loss:0.179, val_acc:0.975]
Epoch [87/120    avg_loss:0.148, val_acc:0.958]
Epoch [88/120    avg_loss:0.164, val_acc:0.981]
Epoch [89/120    avg_loss:0.146, val_acc:0.960]
Epoch [90/120    avg_loss:0.153, val_acc:0.977]
Epoch [91/120    avg_loss:0.143, val_acc:0.973]
Epoch [92/120    avg_loss:0.144, val_acc:0.958]
Epoch [93/120    avg_loss:0.159, val_acc:0.973]
Epoch [94/120    avg_loss:0.138, val_acc:0.985]
Epoch [95/120    avg_loss:0.117, val_acc:0.971]
Epoch [96/120    avg_loss:0.137, val_acc:0.981]
Epoch [97/120    avg_loss:0.135, val_acc:0.979]
Epoch [98/120    avg_loss:0.125, val_acc:0.977]
Epoch [99/120    avg_loss:0.147, val_acc:0.967]
Epoch [100/120    avg_loss:0.156, val_acc:0.981]
Epoch [101/120    avg_loss:0.128, val_acc:0.946]
Epoch [102/120    avg_loss:0.169, val_acc:0.958]
Epoch [103/120    avg_loss:0.131, val_acc:0.973]
Epoch [104/120    avg_loss:0.164, val_acc:0.971]
Epoch [105/120    avg_loss:0.168, val_acc:0.973]
Epoch [106/120    avg_loss:0.128, val_acc:0.981]
Epoch [107/120    avg_loss:0.100, val_acc:0.981]
Epoch [108/120    avg_loss:0.086, val_acc:0.985]
Epoch [109/120    avg_loss:0.076, val_acc:0.983]
Epoch [110/120    avg_loss:0.075, val_acc:0.983]
Epoch [111/120    avg_loss:0.084, val_acc:0.983]
Epoch [112/120    avg_loss:0.074, val_acc:0.983]
Epoch [113/120    avg_loss:0.084, val_acc:0.983]
Epoch [114/120    avg_loss:0.073, val_acc:0.983]
Epoch [115/120    avg_loss:0.070, val_acc:0.983]
Epoch [116/120    avg_loss:0.072, val_acc:0.983]
Epoch [117/120    avg_loss:0.076, val_acc:0.983]
Epoch [118/120    avg_loss:0.076, val_acc:0.981]
Epoch [119/120    avg_loss:0.074, val_acc:0.981]
Epoch [120/120    avg_loss:0.070, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99780541 0.98426966 1.         0.93793103 0.91262136
 0.99277108 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9912178285216872
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc225c1e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.102]
Epoch [2/120    avg_loss:2.624, val_acc:0.135]
Epoch [3/120    avg_loss:2.597, val_acc:0.169]
Epoch [4/120    avg_loss:2.577, val_acc:0.183]
Epoch [5/120    avg_loss:2.553, val_acc:0.202]
Epoch [6/120    avg_loss:2.524, val_acc:0.233]
Epoch [7/120    avg_loss:2.498, val_acc:0.294]
Epoch [8/120    avg_loss:2.470, val_acc:0.315]
Epoch [9/120    avg_loss:2.451, val_acc:0.360]
Epoch [10/120    avg_loss:2.420, val_acc:0.392]
Epoch [11/120    avg_loss:2.397, val_acc:0.415]
Epoch [12/120    avg_loss:2.373, val_acc:0.431]
Epoch [13/120    avg_loss:2.348, val_acc:0.433]
Epoch [14/120    avg_loss:2.319, val_acc:0.440]
Epoch [15/120    avg_loss:2.290, val_acc:0.446]
Epoch [16/120    avg_loss:2.251, val_acc:0.448]
Epoch [17/120    avg_loss:2.222, val_acc:0.467]
Epoch [18/120    avg_loss:2.176, val_acc:0.483]
Epoch [19/120    avg_loss:2.143, val_acc:0.502]
Epoch [20/120    avg_loss:2.090, val_acc:0.535]
Epoch [21/120    avg_loss:2.048, val_acc:0.544]
Epoch [22/120    avg_loss:2.000, val_acc:0.567]
Epoch [23/120    avg_loss:1.955, val_acc:0.596]
Epoch [24/120    avg_loss:1.907, val_acc:0.604]
Epoch [25/120    avg_loss:1.867, val_acc:0.579]
Epoch [26/120    avg_loss:1.831, val_acc:0.592]
Epoch [27/120    avg_loss:1.774, val_acc:0.592]
Epoch [28/120    avg_loss:1.706, val_acc:0.617]
Epoch [29/120    avg_loss:1.670, val_acc:0.610]
Epoch [30/120    avg_loss:1.615, val_acc:0.627]
Epoch [31/120    avg_loss:1.581, val_acc:0.606]
Epoch [32/120    avg_loss:1.550, val_acc:0.621]
Epoch [33/120    avg_loss:1.475, val_acc:0.637]
Epoch [34/120    avg_loss:1.430, val_acc:0.627]
Epoch [35/120    avg_loss:1.397, val_acc:0.675]
Epoch [36/120    avg_loss:1.330, val_acc:0.702]
Epoch [37/120    avg_loss:1.305, val_acc:0.717]
Epoch [38/120    avg_loss:1.238, val_acc:0.727]
Epoch [39/120    avg_loss:1.191, val_acc:0.721]
Epoch [40/120    avg_loss:1.118, val_acc:0.754]
Epoch [41/120    avg_loss:1.082, val_acc:0.756]
Epoch [42/120    avg_loss:1.049, val_acc:0.727]
Epoch [43/120    avg_loss:1.050, val_acc:0.815]
Epoch [44/120    avg_loss:0.967, val_acc:0.860]
Epoch [45/120    avg_loss:0.909, val_acc:0.875]
Epoch [46/120    avg_loss:0.834, val_acc:0.906]
Epoch [47/120    avg_loss:0.856, val_acc:0.910]
Epoch [48/120    avg_loss:0.800, val_acc:0.927]
Epoch [49/120    avg_loss:0.772, val_acc:0.879]
Epoch [50/120    avg_loss:0.708, val_acc:0.890]
Epoch [51/120    avg_loss:0.672, val_acc:0.929]
Epoch [52/120    avg_loss:0.632, val_acc:0.940]
Epoch [53/120    avg_loss:0.654, val_acc:0.871]
Epoch [54/120    avg_loss:0.592, val_acc:0.917]
Epoch [55/120    avg_loss:0.585, val_acc:0.942]
Epoch [56/120    avg_loss:0.521, val_acc:0.929]
Epoch [57/120    avg_loss:0.506, val_acc:0.946]
Epoch [58/120    avg_loss:0.462, val_acc:0.948]
Epoch [59/120    avg_loss:0.492, val_acc:0.940]
Epoch [60/120    avg_loss:0.465, val_acc:0.942]
Epoch [61/120    avg_loss:0.426, val_acc:0.935]
Epoch [62/120    avg_loss:0.420, val_acc:0.938]
Epoch [63/120    avg_loss:0.405, val_acc:0.952]
Epoch [64/120    avg_loss:0.378, val_acc:0.971]
Epoch [65/120    avg_loss:0.332, val_acc:0.960]
Epoch [66/120    avg_loss:0.311, val_acc:0.948]
Epoch [67/120    avg_loss:0.326, val_acc:0.977]
Epoch [68/120    avg_loss:0.302, val_acc:0.958]
Epoch [69/120    avg_loss:0.305, val_acc:0.958]
Epoch [70/120    avg_loss:0.294, val_acc:0.967]
Epoch [71/120    avg_loss:0.270, val_acc:0.969]
Epoch [72/120    avg_loss:0.325, val_acc:0.954]
Epoch [73/120    avg_loss:0.331, val_acc:0.935]
Epoch [74/120    avg_loss:0.310, val_acc:0.969]
Epoch [75/120    avg_loss:0.276, val_acc:0.981]
Epoch [76/120    avg_loss:0.264, val_acc:0.983]
Epoch [77/120    avg_loss:0.256, val_acc:0.985]
Epoch [78/120    avg_loss:0.238, val_acc:0.977]
Epoch [79/120    avg_loss:0.208, val_acc:0.977]
Epoch [80/120    avg_loss:0.195, val_acc:0.983]
Epoch [81/120    avg_loss:0.221, val_acc:0.977]
Epoch [82/120    avg_loss:0.199, val_acc:0.990]
Epoch [83/120    avg_loss:0.194, val_acc:0.967]
Epoch [84/120    avg_loss:0.180, val_acc:0.983]
Epoch [85/120    avg_loss:0.197, val_acc:0.977]
Epoch [86/120    avg_loss:0.218, val_acc:0.963]
Epoch [87/120    avg_loss:0.293, val_acc:0.948]
Epoch [88/120    avg_loss:0.242, val_acc:0.921]
Epoch [89/120    avg_loss:0.224, val_acc:0.958]
Epoch [90/120    avg_loss:0.176, val_acc:0.985]
Epoch [91/120    avg_loss:0.173, val_acc:0.992]
Epoch [92/120    avg_loss:0.162, val_acc:0.983]
Epoch [93/120    avg_loss:0.176, val_acc:0.985]
Epoch [94/120    avg_loss:0.202, val_acc:0.963]
Epoch [95/120    avg_loss:0.180, val_acc:0.994]
Epoch [96/120    avg_loss:0.159, val_acc:0.969]
Epoch [97/120    avg_loss:0.163, val_acc:0.969]
Epoch [98/120    avg_loss:0.145, val_acc:0.979]
Epoch [99/120    avg_loss:0.147, val_acc:0.985]
Epoch [100/120    avg_loss:0.130, val_acc:0.992]
Epoch [101/120    avg_loss:0.123, val_acc:0.981]
Epoch [102/120    avg_loss:0.116, val_acc:0.990]
Epoch [103/120    avg_loss:0.101, val_acc:0.988]
Epoch [104/120    avg_loss:0.109, val_acc:0.981]
Epoch [105/120    avg_loss:0.151, val_acc:0.994]
Epoch [106/120    avg_loss:0.102, val_acc:0.981]
Epoch [107/120    avg_loss:0.112, val_acc:0.977]
Epoch [108/120    avg_loss:0.111, val_acc:0.988]
Epoch [109/120    avg_loss:0.114, val_acc:0.938]
Epoch [110/120    avg_loss:0.118, val_acc:0.971]
Epoch [111/120    avg_loss:0.095, val_acc:0.979]
Epoch [112/120    avg_loss:0.103, val_acc:0.994]
Epoch [113/120    avg_loss:0.082, val_acc:0.994]
Epoch [114/120    avg_loss:0.088, val_acc:0.985]
Epoch [115/120    avg_loss:0.091, val_acc:0.990]
Epoch [116/120    avg_loss:0.079, val_acc:0.985]
Epoch [117/120    avg_loss:0.088, val_acc:0.992]
Epoch [118/120    avg_loss:0.066, val_acc:0.994]
Epoch [119/120    avg_loss:0.068, val_acc:0.985]
Epoch [120/120    avg_loss:0.073, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 199   0   0   0   0  20   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 0.99780541 0.94988067 0.99782135 0.95744681 0.9270073
 0.99277108 0.89952153 1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.989319453599339
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb525c4ce80>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.281]
Epoch [2/120    avg_loss:2.571, val_acc:0.308]
Epoch [3/120    avg_loss:2.543, val_acc:0.315]
Epoch [4/120    avg_loss:2.525, val_acc:0.319]
Epoch [5/120    avg_loss:2.506, val_acc:0.319]
Epoch [6/120    avg_loss:2.481, val_acc:0.319]
Epoch [7/120    avg_loss:2.460, val_acc:0.321]
Epoch [8/120    avg_loss:2.443, val_acc:0.323]
Epoch [9/120    avg_loss:2.416, val_acc:0.325]
Epoch [10/120    avg_loss:2.389, val_acc:0.338]
Epoch [11/120    avg_loss:2.364, val_acc:0.352]
Epoch [12/120    avg_loss:2.343, val_acc:0.362]
Epoch [13/120    avg_loss:2.314, val_acc:0.379]
Epoch [14/120    avg_loss:2.287, val_acc:0.421]
Epoch [15/120    avg_loss:2.254, val_acc:0.446]
Epoch [16/120    avg_loss:2.220, val_acc:0.481]
Epoch [17/120    avg_loss:2.198, val_acc:0.519]
Epoch [18/120    avg_loss:2.155, val_acc:0.519]
Epoch [19/120    avg_loss:2.123, val_acc:0.552]
Epoch [20/120    avg_loss:2.073, val_acc:0.554]
Epoch [21/120    avg_loss:2.048, val_acc:0.550]
Epoch [22/120    avg_loss:2.001, val_acc:0.567]
Epoch [23/120    avg_loss:1.956, val_acc:0.581]
Epoch [24/120    avg_loss:1.928, val_acc:0.581]
Epoch [25/120    avg_loss:1.880, val_acc:0.602]
Epoch [26/120    avg_loss:1.825, val_acc:0.615]
Epoch [27/120    avg_loss:1.792, val_acc:0.637]
Epoch [28/120    avg_loss:1.738, val_acc:0.696]
Epoch [29/120    avg_loss:1.686, val_acc:0.677]
Epoch [30/120    avg_loss:1.641, val_acc:0.652]
Epoch [31/120    avg_loss:1.606, val_acc:0.675]
Epoch [32/120    avg_loss:1.538, val_acc:0.702]
Epoch [33/120    avg_loss:1.498, val_acc:0.719]
Epoch [34/120    avg_loss:1.430, val_acc:0.706]
Epoch [35/120    avg_loss:1.389, val_acc:0.723]
Epoch [36/120    avg_loss:1.329, val_acc:0.723]
Epoch [37/120    avg_loss:1.309, val_acc:0.740]
Epoch [38/120    avg_loss:1.235, val_acc:0.750]
Epoch [39/120    avg_loss:1.189, val_acc:0.740]
Epoch [40/120    avg_loss:1.128, val_acc:0.773]
Epoch [41/120    avg_loss:1.118, val_acc:0.767]
Epoch [42/120    avg_loss:1.126, val_acc:0.731]
Epoch [43/120    avg_loss:1.064, val_acc:0.829]
Epoch [44/120    avg_loss:0.998, val_acc:0.863]
Epoch [45/120    avg_loss:0.934, val_acc:0.894]
Epoch [46/120    avg_loss:0.880, val_acc:0.879]
Epoch [47/120    avg_loss:0.849, val_acc:0.898]
Epoch [48/120    avg_loss:0.821, val_acc:0.904]
Epoch [49/120    avg_loss:0.787, val_acc:0.885]
Epoch [50/120    avg_loss:0.778, val_acc:0.844]
Epoch [51/120    avg_loss:0.730, val_acc:0.933]
Epoch [52/120    avg_loss:0.657, val_acc:0.927]
Epoch [53/120    avg_loss:0.640, val_acc:0.927]
Epoch [54/120    avg_loss:0.594, val_acc:0.919]
Epoch [55/120    avg_loss:0.594, val_acc:0.842]
Epoch [56/120    avg_loss:0.594, val_acc:0.906]
Epoch [57/120    avg_loss:0.569, val_acc:0.927]
Epoch [58/120    avg_loss:0.523, val_acc:0.929]
Epoch [59/120    avg_loss:0.492, val_acc:0.946]
Epoch [60/120    avg_loss:0.493, val_acc:0.921]
Epoch [61/120    avg_loss:0.462, val_acc:0.948]
Epoch [62/120    avg_loss:0.427, val_acc:0.946]
Epoch [63/120    avg_loss:0.410, val_acc:0.944]
Epoch [64/120    avg_loss:0.397, val_acc:0.942]
Epoch [65/120    avg_loss:0.381, val_acc:0.948]
Epoch [66/120    avg_loss:0.367, val_acc:0.942]
Epoch [67/120    avg_loss:0.340, val_acc:0.950]
Epoch [68/120    avg_loss:0.356, val_acc:0.940]
Epoch [69/120    avg_loss:0.354, val_acc:0.942]
Epoch [70/120    avg_loss:0.316, val_acc:0.971]
Epoch [71/120    avg_loss:0.282, val_acc:0.935]
Epoch [72/120    avg_loss:0.319, val_acc:0.954]
Epoch [73/120    avg_loss:0.340, val_acc:0.950]
Epoch [74/120    avg_loss:0.306, val_acc:0.942]
Epoch [75/120    avg_loss:0.274, val_acc:0.969]
Epoch [76/120    avg_loss:0.316, val_acc:0.954]
Epoch [77/120    avg_loss:0.265, val_acc:0.958]
Epoch [78/120    avg_loss:0.246, val_acc:0.963]
Epoch [79/120    avg_loss:0.225, val_acc:0.963]
Epoch [80/120    avg_loss:0.223, val_acc:0.915]
Epoch [81/120    avg_loss:0.213, val_acc:0.971]
Epoch [82/120    avg_loss:0.237, val_acc:0.971]
Epoch [83/120    avg_loss:0.222, val_acc:0.967]
Epoch [84/120    avg_loss:0.242, val_acc:0.960]
Epoch [85/120    avg_loss:0.214, val_acc:0.965]
Epoch [86/120    avg_loss:0.186, val_acc:0.963]
Epoch [87/120    avg_loss:0.216, val_acc:0.977]
Epoch [88/120    avg_loss:0.183, val_acc:0.971]
Epoch [89/120    avg_loss:0.149, val_acc:0.973]
Epoch [90/120    avg_loss:0.147, val_acc:0.973]
Epoch [91/120    avg_loss:0.217, val_acc:0.948]
Epoch [92/120    avg_loss:0.247, val_acc:0.948]
Epoch [93/120    avg_loss:0.242, val_acc:0.975]
Epoch [94/120    avg_loss:0.209, val_acc:0.952]
Epoch [95/120    avg_loss:0.221, val_acc:0.960]
Epoch [96/120    avg_loss:0.170, val_acc:0.977]
Epoch [97/120    avg_loss:0.173, val_acc:0.967]
Epoch [98/120    avg_loss:0.172, val_acc:0.983]
Epoch [99/120    avg_loss:0.155, val_acc:0.975]
Epoch [100/120    avg_loss:0.145, val_acc:0.981]
Epoch [101/120    avg_loss:0.106, val_acc:0.985]
Epoch [102/120    avg_loss:0.113, val_acc:0.983]
Epoch [103/120    avg_loss:0.104, val_acc:0.975]
Epoch [104/120    avg_loss:0.123, val_acc:0.977]
Epoch [105/120    avg_loss:0.128, val_acc:0.981]
Epoch [106/120    avg_loss:0.111, val_acc:0.979]
Epoch [107/120    avg_loss:0.098, val_acc:0.983]
Epoch [108/120    avg_loss:0.109, val_acc:0.969]
Epoch [109/120    avg_loss:0.131, val_acc:0.977]
Epoch [110/120    avg_loss:0.113, val_acc:0.981]
Epoch [111/120    avg_loss:0.117, val_acc:0.983]
Epoch [112/120    avg_loss:0.116, val_acc:0.981]
Epoch [113/120    avg_loss:0.101, val_acc:0.981]
Epoch [114/120    avg_loss:0.106, val_acc:0.983]
Epoch [115/120    avg_loss:0.076, val_acc:0.983]
Epoch [116/120    avg_loss:0.074, val_acc:0.988]
Epoch [117/120    avg_loss:0.059, val_acc:0.988]
Epoch [118/120    avg_loss:0.061, val_acc:0.988]
Epoch [119/120    avg_loss:0.064, val_acc:0.985]
Epoch [120/120    avg_loss:0.061, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99926954 0.98871332 1.         0.93598234 0.90034364
 0.99757869 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.991691622168888
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc0cbbe4e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.160]
Epoch [2/120    avg_loss:2.598, val_acc:0.308]
Epoch [3/120    avg_loss:2.573, val_acc:0.306]
Epoch [4/120    avg_loss:2.554, val_acc:0.306]
Epoch [5/120    avg_loss:2.534, val_acc:0.306]
Epoch [6/120    avg_loss:2.514, val_acc:0.306]
Epoch [7/120    avg_loss:2.485, val_acc:0.308]
Epoch [8/120    avg_loss:2.463, val_acc:0.312]
Epoch [9/120    avg_loss:2.431, val_acc:0.335]
Epoch [10/120    avg_loss:2.405, val_acc:0.369]
Epoch [11/120    avg_loss:2.383, val_acc:0.383]
Epoch [12/120    avg_loss:2.355, val_acc:0.396]
Epoch [13/120    avg_loss:2.321, val_acc:0.400]
Epoch [14/120    avg_loss:2.301, val_acc:0.404]
Epoch [15/120    avg_loss:2.272, val_acc:0.402]
Epoch [16/120    avg_loss:2.253, val_acc:0.406]
Epoch [17/120    avg_loss:2.220, val_acc:0.427]
Epoch [18/120    avg_loss:2.185, val_acc:0.438]
Epoch [19/120    avg_loss:2.166, val_acc:0.463]
Epoch [20/120    avg_loss:2.130, val_acc:0.469]
Epoch [21/120    avg_loss:2.100, val_acc:0.477]
Epoch [22/120    avg_loss:2.059, val_acc:0.498]
Epoch [23/120    avg_loss:2.025, val_acc:0.525]
Epoch [24/120    avg_loss:1.994, val_acc:0.552]
Epoch [25/120    avg_loss:1.961, val_acc:0.554]
Epoch [26/120    avg_loss:1.915, val_acc:0.583]
Epoch [27/120    avg_loss:1.871, val_acc:0.596]
Epoch [28/120    avg_loss:1.837, val_acc:0.592]
Epoch [29/120    avg_loss:1.793, val_acc:0.608]
Epoch [30/120    avg_loss:1.752, val_acc:0.640]
Epoch [31/120    avg_loss:1.701, val_acc:0.669]
Epoch [32/120    avg_loss:1.652, val_acc:0.669]
Epoch [33/120    avg_loss:1.585, val_acc:0.708]
Epoch [34/120    avg_loss:1.542, val_acc:0.723]
Epoch [35/120    avg_loss:1.505, val_acc:0.725]
Epoch [36/120    avg_loss:1.448, val_acc:0.781]
Epoch [37/120    avg_loss:1.380, val_acc:0.846]
Epoch [38/120    avg_loss:1.299, val_acc:0.815]
Epoch [39/120    avg_loss:1.267, val_acc:0.790]
Epoch [40/120    avg_loss:1.229, val_acc:0.806]
Epoch [41/120    avg_loss:1.156, val_acc:0.865]
Epoch [42/120    avg_loss:1.144, val_acc:0.860]
Epoch [43/120    avg_loss:1.075, val_acc:0.898]
Epoch [44/120    avg_loss:1.003, val_acc:0.873]
Epoch [45/120    avg_loss:0.936, val_acc:0.898]
Epoch [46/120    avg_loss:0.866, val_acc:0.850]
Epoch [47/120    avg_loss:0.820, val_acc:0.863]
Epoch [48/120    avg_loss:0.788, val_acc:0.952]
Epoch [49/120    avg_loss:0.737, val_acc:0.894]
Epoch [50/120    avg_loss:0.718, val_acc:0.948]
Epoch [51/120    avg_loss:0.707, val_acc:0.908]
Epoch [52/120    avg_loss:0.672, val_acc:0.946]
Epoch [53/120    avg_loss:0.603, val_acc:0.944]
Epoch [54/120    avg_loss:0.687, val_acc:0.929]
Epoch [55/120    avg_loss:0.654, val_acc:0.956]
Epoch [56/120    avg_loss:0.553, val_acc:0.956]
Epoch [57/120    avg_loss:0.509, val_acc:0.958]
Epoch [58/120    avg_loss:0.493, val_acc:0.921]
Epoch [59/120    avg_loss:0.500, val_acc:0.956]
Epoch [60/120    avg_loss:0.467, val_acc:0.948]
Epoch [61/120    avg_loss:0.440, val_acc:0.938]
Epoch [62/120    avg_loss:0.442, val_acc:0.940]
Epoch [63/120    avg_loss:0.449, val_acc:0.963]
Epoch [64/120    avg_loss:0.375, val_acc:0.971]
Epoch [65/120    avg_loss:0.376, val_acc:0.927]
Epoch [66/120    avg_loss:0.375, val_acc:0.960]
Epoch [67/120    avg_loss:0.339, val_acc:0.900]
Epoch [68/120    avg_loss:0.331, val_acc:0.969]
Epoch [69/120    avg_loss:0.328, val_acc:0.973]
Epoch [70/120    avg_loss:0.337, val_acc:0.935]
Epoch [71/120    avg_loss:0.343, val_acc:0.971]
Epoch [72/120    avg_loss:0.377, val_acc:0.958]
Epoch [73/120    avg_loss:0.325, val_acc:0.963]
Epoch [74/120    avg_loss:0.280, val_acc:0.954]
Epoch [75/120    avg_loss:0.311, val_acc:0.969]
Epoch [76/120    avg_loss:0.303, val_acc:0.948]
Epoch [77/120    avg_loss:0.272, val_acc:0.973]
Epoch [78/120    avg_loss:0.250, val_acc:0.977]
Epoch [79/120    avg_loss:0.218, val_acc:0.975]
Epoch [80/120    avg_loss:0.221, val_acc:0.969]
Epoch [81/120    avg_loss:0.221, val_acc:0.977]
Epoch [82/120    avg_loss:0.222, val_acc:0.960]
Epoch [83/120    avg_loss:0.228, val_acc:0.979]
Epoch [84/120    avg_loss:0.227, val_acc:0.965]
Epoch [85/120    avg_loss:0.190, val_acc:0.973]
Epoch [86/120    avg_loss:0.192, val_acc:0.973]
Epoch [87/120    avg_loss:0.194, val_acc:0.967]
Epoch [88/120    avg_loss:0.207, val_acc:0.946]
Epoch [89/120    avg_loss:0.267, val_acc:0.973]
Epoch [90/120    avg_loss:0.161, val_acc:0.979]
Epoch [91/120    avg_loss:0.179, val_acc:0.973]
Epoch [92/120    avg_loss:0.261, val_acc:0.946]
Epoch [93/120    avg_loss:0.265, val_acc:0.981]
Epoch [94/120    avg_loss:0.142, val_acc:0.981]
Epoch [95/120    avg_loss:0.152, val_acc:0.979]
Epoch [96/120    avg_loss:0.140, val_acc:0.981]
Epoch [97/120    avg_loss:0.183, val_acc:0.981]
Epoch [98/120    avg_loss:0.164, val_acc:0.979]
Epoch [99/120    avg_loss:0.152, val_acc:0.965]
Epoch [100/120    avg_loss:0.151, val_acc:0.983]
Epoch [101/120    avg_loss:0.123, val_acc:0.975]
Epoch [102/120    avg_loss:0.140, val_acc:0.979]
Epoch [103/120    avg_loss:0.161, val_acc:0.977]
Epoch [104/120    avg_loss:0.166, val_acc:0.977]
Epoch [105/120    avg_loss:0.149, val_acc:0.981]
Epoch [106/120    avg_loss:0.129, val_acc:0.971]
Epoch [107/120    avg_loss:0.144, val_acc:0.990]
Epoch [108/120    avg_loss:0.129, val_acc:0.985]
Epoch [109/120    avg_loss:0.154, val_acc:0.979]
Epoch [110/120    avg_loss:0.146, val_acc:0.975]
Epoch [111/120    avg_loss:0.102, val_acc:0.985]
Epoch [112/120    avg_loss:0.099, val_acc:0.985]
Epoch [113/120    avg_loss:0.115, val_acc:0.985]
Epoch [114/120    avg_loss:0.106, val_acc:0.981]
Epoch [115/120    avg_loss:0.126, val_acc:0.971]
Epoch [116/120    avg_loss:0.151, val_acc:0.990]
Epoch [117/120    avg_loss:0.116, val_acc:0.990]
Epoch [118/120    avg_loss:0.111, val_acc:0.985]
Epoch [119/120    avg_loss:0.118, val_acc:0.983]
Epoch [120/120    avg_loss:0.122, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 0.99927061 0.98206278 0.99343545 0.92482916 0.9025974
 0.99756691 0.95555556 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9900299819266217
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9036b5de10>
supervision:full
center_pixel:True
Network :
Number of parameter: 29162==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.069]
Epoch [2/120    avg_loss:2.600, val_acc:0.160]
Epoch [3/120    avg_loss:2.564, val_acc:0.175]
Epoch [4/120    avg_loss:2.528, val_acc:0.329]
Epoch [5/120    avg_loss:2.499, val_acc:0.358]
Epoch [6/120    avg_loss:2.473, val_acc:0.350]
Epoch [7/120    avg_loss:2.452, val_acc:0.346]
Epoch [8/120    avg_loss:2.427, val_acc:0.344]
Epoch [9/120    avg_loss:2.404, val_acc:0.344]
Epoch [10/120    avg_loss:2.380, val_acc:0.344]
Epoch [11/120    avg_loss:2.356, val_acc:0.344]
Epoch [12/120    avg_loss:2.332, val_acc:0.346]
Epoch [13/120    avg_loss:2.306, val_acc:0.377]
Epoch [14/120    avg_loss:2.275, val_acc:0.400]
Epoch [15/120    avg_loss:2.243, val_acc:0.442]
Epoch [16/120    avg_loss:2.219, val_acc:0.425]
Epoch [17/120    avg_loss:2.186, val_acc:0.452]
Epoch [18/120    avg_loss:2.160, val_acc:0.546]
Epoch [19/120    avg_loss:2.113, val_acc:0.588]
Epoch [20/120    avg_loss:2.074, val_acc:0.598]
Epoch [21/120    avg_loss:2.037, val_acc:0.621]
Epoch [22/120    avg_loss:1.965, val_acc:0.646]
Epoch [23/120    avg_loss:1.923, val_acc:0.673]
Epoch [24/120    avg_loss:1.879, val_acc:0.700]
Epoch [25/120    avg_loss:1.799, val_acc:0.681]
Epoch [26/120    avg_loss:1.763, val_acc:0.706]
Epoch [27/120    avg_loss:1.670, val_acc:0.696]
Epoch [28/120    avg_loss:1.607, val_acc:0.713]
Epoch [29/120    avg_loss:1.545, val_acc:0.725]
Epoch [30/120    avg_loss:1.460, val_acc:0.733]
Epoch [31/120    avg_loss:1.411, val_acc:0.738]
Epoch [32/120    avg_loss:1.347, val_acc:0.744]
Epoch [33/120    avg_loss:1.238, val_acc:0.756]
Epoch [34/120    avg_loss:1.178, val_acc:0.746]
Epoch [35/120    avg_loss:1.125, val_acc:0.754]
Epoch [36/120    avg_loss:1.061, val_acc:0.752]
Epoch [37/120    avg_loss:0.998, val_acc:0.767]
Epoch [38/120    avg_loss:0.973, val_acc:0.756]
Epoch [39/120    avg_loss:0.905, val_acc:0.777]
Epoch [40/120    avg_loss:0.885, val_acc:0.790]
Epoch [41/120    avg_loss:0.822, val_acc:0.775]
Epoch [42/120    avg_loss:0.760, val_acc:0.785]
Epoch [43/120    avg_loss:0.736, val_acc:0.790]
Epoch [44/120    avg_loss:0.656, val_acc:0.877]
Epoch [45/120    avg_loss:0.711, val_acc:0.894]
Epoch [46/120    avg_loss:0.664, val_acc:0.877]
Epoch [47/120    avg_loss:0.586, val_acc:0.867]
Epoch [48/120    avg_loss:0.555, val_acc:0.910]
Epoch [49/120    avg_loss:0.588, val_acc:0.873]
Epoch [50/120    avg_loss:0.550, val_acc:0.912]
Epoch [51/120    avg_loss:0.543, val_acc:0.904]
Epoch [52/120    avg_loss:0.557, val_acc:0.883]
Epoch [53/120    avg_loss:0.501, val_acc:0.917]
Epoch [54/120    avg_loss:0.475, val_acc:0.921]
Epoch [55/120    avg_loss:0.433, val_acc:0.940]
Epoch [56/120    avg_loss:0.440, val_acc:0.929]
Epoch [57/120    avg_loss:0.470, val_acc:0.948]
Epoch [58/120    avg_loss:0.388, val_acc:0.904]
Epoch [59/120    avg_loss:0.388, val_acc:0.952]
Epoch [60/120    avg_loss:0.352, val_acc:0.933]
Epoch [61/120    avg_loss:0.334, val_acc:0.952]
Epoch [62/120    avg_loss:0.362, val_acc:0.960]
Epoch [63/120    avg_loss:0.339, val_acc:0.921]
Epoch [64/120    avg_loss:0.333, val_acc:0.925]
Epoch [65/120    avg_loss:0.355, val_acc:0.952]
Epoch [66/120    avg_loss:0.340, val_acc:0.879]
Epoch [67/120    avg_loss:0.362, val_acc:0.965]
Epoch [68/120    avg_loss:0.322, val_acc:0.942]
Epoch [69/120    avg_loss:0.298, val_acc:0.915]
Epoch [70/120    avg_loss:0.305, val_acc:0.915]
Epoch [71/120    avg_loss:0.296, val_acc:0.915]
Epoch [72/120    avg_loss:0.283, val_acc:0.946]
Epoch [73/120    avg_loss:0.244, val_acc:0.971]
Epoch [74/120    avg_loss:0.233, val_acc:0.942]
Epoch [75/120    avg_loss:0.241, val_acc:0.971]
Epoch [76/120    avg_loss:0.232, val_acc:0.971]
Epoch [77/120    avg_loss:0.226, val_acc:0.969]
Epoch [78/120    avg_loss:0.220, val_acc:0.977]
Epoch [79/120    avg_loss:0.202, val_acc:0.971]
Epoch [80/120    avg_loss:0.189, val_acc:0.977]
Epoch [81/120    avg_loss:0.184, val_acc:0.946]
Epoch [82/120    avg_loss:0.225, val_acc:0.965]
Epoch [83/120    avg_loss:0.240, val_acc:0.952]
Epoch [84/120    avg_loss:0.223, val_acc:0.973]
Epoch [85/120    avg_loss:0.222, val_acc:0.977]
Epoch [86/120    avg_loss:0.185, val_acc:0.971]
Epoch [87/120    avg_loss:0.205, val_acc:0.950]
Epoch [88/120    avg_loss:0.236, val_acc:0.981]
Epoch [89/120    avg_loss:0.185, val_acc:0.971]
Epoch [90/120    avg_loss:0.183, val_acc:0.973]
Epoch [91/120    avg_loss:0.149, val_acc:0.971]
Epoch [92/120    avg_loss:0.134, val_acc:0.973]
Epoch [93/120    avg_loss:0.159, val_acc:0.975]
Epoch [94/120    avg_loss:0.150, val_acc:0.983]
Epoch [95/120    avg_loss:0.138, val_acc:0.969]
Epoch [96/120    avg_loss:0.148, val_acc:0.981]
Epoch [97/120    avg_loss:0.185, val_acc:0.985]
Epoch [98/120    avg_loss:0.123, val_acc:0.977]
Epoch [99/120    avg_loss:0.113, val_acc:0.985]
Epoch [100/120    avg_loss:0.119, val_acc:0.979]
Epoch [101/120    avg_loss:0.134, val_acc:0.977]
Epoch [102/120    avg_loss:0.150, val_acc:0.983]
Epoch [103/120    avg_loss:0.105, val_acc:0.981]
Epoch [104/120    avg_loss:0.110, val_acc:0.985]
Epoch [105/120    avg_loss:0.092, val_acc:0.988]
Epoch [106/120    avg_loss:0.098, val_acc:0.983]
Epoch [107/120    avg_loss:0.091, val_acc:0.975]
Epoch [108/120    avg_loss:0.118, val_acc:0.981]
Epoch [109/120    avg_loss:0.094, val_acc:0.990]
Epoch [110/120    avg_loss:0.093, val_acc:0.973]
Epoch [111/120    avg_loss:0.096, val_acc:0.988]
Epoch [112/120    avg_loss:0.100, val_acc:0.988]
Epoch [113/120    avg_loss:0.135, val_acc:0.965]
Epoch [114/120    avg_loss:0.166, val_acc:0.946]
Epoch [115/120    avg_loss:0.140, val_acc:0.963]
Epoch [116/120    avg_loss:0.149, val_acc:0.973]
Epoch [117/120    avg_loss:0.099, val_acc:0.985]
Epoch [118/120    avg_loss:0.078, val_acc:0.983]
Epoch [119/120    avg_loss:0.084, val_acc:0.983]
Epoch [120/120    avg_loss:0.091, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   0   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  83 143   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15   3 188   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.26865671641791

F1 scores:
[       nan 1.         0.98206278 0.98901099 0.51076923 0.65759637
 0.95431472 0.95555556 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9584809264942681
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f210421fe80>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.217]
Epoch [2/120    avg_loss:2.608, val_acc:0.223]
Epoch [3/120    avg_loss:2.583, val_acc:0.290]
Epoch [4/120    avg_loss:2.564, val_acc:0.304]
Epoch [5/120    avg_loss:2.544, val_acc:0.292]
Epoch [6/120    avg_loss:2.517, val_acc:0.292]
Epoch [7/120    avg_loss:2.493, val_acc:0.294]
Epoch [8/120    avg_loss:2.467, val_acc:0.302]
Epoch [9/120    avg_loss:2.448, val_acc:0.302]
Epoch [10/120    avg_loss:2.414, val_acc:0.329]
Epoch [11/120    avg_loss:2.391, val_acc:0.369]
Epoch [12/120    avg_loss:2.368, val_acc:0.390]
Epoch [13/120    avg_loss:2.347, val_acc:0.417]
Epoch [14/120    avg_loss:2.322, val_acc:0.423]
Epoch [15/120    avg_loss:2.284, val_acc:0.444]
Epoch [16/120    avg_loss:2.260, val_acc:0.463]
Epoch [17/120    avg_loss:2.222, val_acc:0.492]
Epoch [18/120    avg_loss:2.193, val_acc:0.485]
Epoch [19/120    avg_loss:2.151, val_acc:0.515]
Epoch [20/120    avg_loss:2.118, val_acc:0.527]
Epoch [21/120    avg_loss:2.084, val_acc:0.525]
Epoch [22/120    avg_loss:2.046, val_acc:0.540]
Epoch [23/120    avg_loss:2.008, val_acc:0.550]
Epoch [24/120    avg_loss:1.946, val_acc:0.556]
Epoch [25/120    avg_loss:1.900, val_acc:0.567]
Epoch [26/120    avg_loss:1.854, val_acc:0.581]
Epoch [27/120    avg_loss:1.799, val_acc:0.588]
Epoch [28/120    avg_loss:1.754, val_acc:0.631]
Epoch [29/120    avg_loss:1.703, val_acc:0.621]
Epoch [30/120    avg_loss:1.651, val_acc:0.673]
Epoch [31/120    avg_loss:1.597, val_acc:0.685]
Epoch [32/120    avg_loss:1.555, val_acc:0.681]
Epoch [33/120    avg_loss:1.497, val_acc:0.698]
Epoch [34/120    avg_loss:1.469, val_acc:0.700]
Epoch [35/120    avg_loss:1.419, val_acc:0.706]
Epoch [36/120    avg_loss:1.361, val_acc:0.721]
Epoch [37/120    avg_loss:1.293, val_acc:0.738]
Epoch [38/120    avg_loss:1.236, val_acc:0.735]
Epoch [39/120    avg_loss:1.181, val_acc:0.740]
Epoch [40/120    avg_loss:1.146, val_acc:0.785]
Epoch [41/120    avg_loss:1.102, val_acc:0.781]
Epoch [42/120    avg_loss:1.082, val_acc:0.785]
Epoch [43/120    avg_loss:1.025, val_acc:0.790]
Epoch [44/120    avg_loss:0.969, val_acc:0.787]
Epoch [45/120    avg_loss:0.915, val_acc:0.810]
Epoch [46/120    avg_loss:0.876, val_acc:0.827]
Epoch [47/120    avg_loss:0.852, val_acc:0.827]
Epoch [48/120    avg_loss:0.811, val_acc:0.825]
Epoch [49/120    avg_loss:0.755, val_acc:0.817]
Epoch [50/120    avg_loss:0.715, val_acc:0.804]
Epoch [51/120    avg_loss:0.667, val_acc:0.858]
Epoch [52/120    avg_loss:0.647, val_acc:0.850]
Epoch [53/120    avg_loss:0.649, val_acc:0.867]
Epoch [54/120    avg_loss:0.638, val_acc:0.854]
Epoch [55/120    avg_loss:0.606, val_acc:0.873]
Epoch [56/120    avg_loss:0.581, val_acc:0.877]
Epoch [57/120    avg_loss:0.517, val_acc:0.881]
Epoch [58/120    avg_loss:0.572, val_acc:0.892]
Epoch [59/120    avg_loss:0.598, val_acc:0.871]
Epoch [60/120    avg_loss:0.501, val_acc:0.900]
Epoch [61/120    avg_loss:0.535, val_acc:0.852]
Epoch [62/120    avg_loss:0.533, val_acc:0.881]
Epoch [63/120    avg_loss:0.452, val_acc:0.869]
Epoch [64/120    avg_loss:0.443, val_acc:0.940]
Epoch [65/120    avg_loss:0.390, val_acc:0.963]
Epoch [66/120    avg_loss:0.411, val_acc:0.965]
Epoch [67/120    avg_loss:0.405, val_acc:0.885]
Epoch [68/120    avg_loss:0.358, val_acc:0.958]
Epoch [69/120    avg_loss:0.394, val_acc:0.963]
Epoch [70/120    avg_loss:0.403, val_acc:0.960]
Epoch [71/120    avg_loss:0.396, val_acc:0.967]
Epoch [72/120    avg_loss:0.341, val_acc:0.973]
Epoch [73/120    avg_loss:0.332, val_acc:0.967]
Epoch [74/120    avg_loss:0.288, val_acc:0.969]
Epoch [75/120    avg_loss:0.297, val_acc:0.973]
Epoch [76/120    avg_loss:0.280, val_acc:0.965]
Epoch [77/120    avg_loss:0.235, val_acc:0.983]
Epoch [78/120    avg_loss:0.249, val_acc:0.958]
Epoch [79/120    avg_loss:0.240, val_acc:0.954]
Epoch [80/120    avg_loss:0.242, val_acc:0.975]
Epoch [81/120    avg_loss:0.228, val_acc:0.979]
Epoch [82/120    avg_loss:0.239, val_acc:0.952]
Epoch [83/120    avg_loss:0.305, val_acc:0.896]
Epoch [84/120    avg_loss:0.268, val_acc:0.969]
Epoch [85/120    avg_loss:0.247, val_acc:0.971]
Epoch [86/120    avg_loss:0.222, val_acc:0.973]
Epoch [87/120    avg_loss:0.192, val_acc:0.973]
Epoch [88/120    avg_loss:0.249, val_acc:0.942]
Epoch [89/120    avg_loss:0.258, val_acc:0.979]
Epoch [90/120    avg_loss:0.195, val_acc:0.983]
Epoch [91/120    avg_loss:0.171, val_acc:0.967]
Epoch [92/120    avg_loss:0.196, val_acc:0.985]
Epoch [93/120    avg_loss:0.175, val_acc:0.983]
Epoch [94/120    avg_loss:0.166, val_acc:0.983]
Epoch [95/120    avg_loss:0.154, val_acc:0.950]
Epoch [96/120    avg_loss:0.227, val_acc:0.990]
Epoch [97/120    avg_loss:0.180, val_acc:0.979]
Epoch [98/120    avg_loss:0.170, val_acc:0.983]
Epoch [99/120    avg_loss:0.142, val_acc:0.992]
Epoch [100/120    avg_loss:0.189, val_acc:0.969]
Epoch [101/120    avg_loss:0.158, val_acc:0.985]
Epoch [102/120    avg_loss:0.121, val_acc:0.973]
Epoch [103/120    avg_loss:0.117, val_acc:0.994]
Epoch [104/120    avg_loss:0.100, val_acc:0.992]
Epoch [105/120    avg_loss:0.100, val_acc:0.981]
Epoch [106/120    avg_loss:0.093, val_acc:0.979]
Epoch [107/120    avg_loss:0.097, val_acc:0.963]
Epoch [108/120    avg_loss:0.180, val_acc:0.967]
Epoch [109/120    avg_loss:0.188, val_acc:0.975]
Epoch [110/120    avg_loss:0.152, val_acc:0.975]
Epoch [111/120    avg_loss:0.153, val_acc:0.975]
Epoch [112/120    avg_loss:0.133, val_acc:0.960]
Epoch [113/120    avg_loss:0.127, val_acc:0.985]
Epoch [114/120    avg_loss:0.114, val_acc:0.983]
Epoch [115/120    avg_loss:0.176, val_acc:0.977]
Epoch [116/120    avg_loss:0.126, val_acc:0.977]
Epoch [117/120    avg_loss:0.109, val_acc:0.985]
Epoch [118/120    avg_loss:0.099, val_acc:0.990]
Epoch [119/120    avg_loss:0.086, val_acc:0.992]
Epoch [120/120    avg_loss:0.078, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 0.99707174 0.99319728 1.         0.95670996 0.92907801
 0.99038462 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9935909878042516
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc761a09e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.644, val_acc:0.044]
Epoch [2/120    avg_loss:2.618, val_acc:0.077]
Epoch [3/120    avg_loss:2.591, val_acc:0.131]
Epoch [4/120    avg_loss:2.563, val_acc:0.144]
Epoch [5/120    avg_loss:2.533, val_acc:0.210]
Epoch [6/120    avg_loss:2.509, val_acc:0.348]
Epoch [7/120    avg_loss:2.476, val_acc:0.479]
Epoch [8/120    avg_loss:2.445, val_acc:0.517]
Epoch [9/120    avg_loss:2.417, val_acc:0.519]
Epoch [10/120    avg_loss:2.384, val_acc:0.521]
Epoch [11/120    avg_loss:2.350, val_acc:0.542]
Epoch [12/120    avg_loss:2.314, val_acc:0.560]
Epoch [13/120    avg_loss:2.273, val_acc:0.569]
Epoch [14/120    avg_loss:2.227, val_acc:0.588]
Epoch [15/120    avg_loss:2.190, val_acc:0.598]
Epoch [16/120    avg_loss:2.140, val_acc:0.608]
Epoch [17/120    avg_loss:2.094, val_acc:0.621]
Epoch [18/120    avg_loss:2.047, val_acc:0.617]
Epoch [19/120    avg_loss:1.994, val_acc:0.633]
Epoch [20/120    avg_loss:1.958, val_acc:0.648]
Epoch [21/120    avg_loss:1.890, val_acc:0.644]
Epoch [22/120    avg_loss:1.829, val_acc:0.658]
Epoch [23/120    avg_loss:1.790, val_acc:0.667]
Epoch [24/120    avg_loss:1.742, val_acc:0.656]
Epoch [25/120    avg_loss:1.675, val_acc:0.658]
Epoch [26/120    avg_loss:1.632, val_acc:0.665]
Epoch [27/120    avg_loss:1.553, val_acc:0.669]
Epoch [28/120    avg_loss:1.515, val_acc:0.669]
Epoch [29/120    avg_loss:1.456, val_acc:0.677]
Epoch [30/120    avg_loss:1.407, val_acc:0.685]
Epoch [31/120    avg_loss:1.353, val_acc:0.692]
Epoch [32/120    avg_loss:1.306, val_acc:0.771]
Epoch [33/120    avg_loss:1.236, val_acc:0.713]
Epoch [34/120    avg_loss:1.158, val_acc:0.731]
Epoch [35/120    avg_loss:1.097, val_acc:0.775]
Epoch [36/120    avg_loss:1.045, val_acc:0.835]
Epoch [37/120    avg_loss:0.984, val_acc:0.840]
Epoch [38/120    avg_loss:0.938, val_acc:0.867]
Epoch [39/120    avg_loss:0.887, val_acc:0.881]
Epoch [40/120    avg_loss:0.849, val_acc:0.910]
Epoch [41/120    avg_loss:0.833, val_acc:0.877]
Epoch [42/120    avg_loss:0.725, val_acc:0.931]
Epoch [43/120    avg_loss:0.682, val_acc:0.871]
Epoch [44/120    avg_loss:0.778, val_acc:0.906]
Epoch [45/120    avg_loss:0.672, val_acc:0.935]
Epoch [46/120    avg_loss:0.619, val_acc:0.912]
Epoch [47/120    avg_loss:0.652, val_acc:0.890]
Epoch [48/120    avg_loss:0.568, val_acc:0.925]
Epoch [49/120    avg_loss:0.575, val_acc:0.835]
Epoch [50/120    avg_loss:0.608, val_acc:0.906]
Epoch [51/120    avg_loss:0.491, val_acc:0.956]
Epoch [52/120    avg_loss:0.459, val_acc:0.946]
Epoch [53/120    avg_loss:0.458, val_acc:0.944]
Epoch [54/120    avg_loss:0.451, val_acc:0.929]
Epoch [55/120    avg_loss:0.413, val_acc:0.950]
Epoch [56/120    avg_loss:0.385, val_acc:0.948]
Epoch [57/120    avg_loss:0.402, val_acc:0.967]
Epoch [58/120    avg_loss:0.389, val_acc:0.942]
Epoch [59/120    avg_loss:0.375, val_acc:0.956]
Epoch [60/120    avg_loss:0.325, val_acc:0.948]
Epoch [61/120    avg_loss:0.341, val_acc:0.954]
Epoch [62/120    avg_loss:0.300, val_acc:0.954]
Epoch [63/120    avg_loss:0.322, val_acc:0.965]
Epoch [64/120    avg_loss:0.337, val_acc:0.969]
Epoch [65/120    avg_loss:0.299, val_acc:0.956]
Epoch [66/120    avg_loss:0.233, val_acc:0.965]
Epoch [67/120    avg_loss:0.211, val_acc:0.971]
Epoch [68/120    avg_loss:0.230, val_acc:0.963]
Epoch [69/120    avg_loss:0.241, val_acc:0.965]
Epoch [70/120    avg_loss:0.256, val_acc:0.960]
Epoch [71/120    avg_loss:0.247, val_acc:0.958]
Epoch [72/120    avg_loss:0.274, val_acc:0.977]
Epoch [73/120    avg_loss:0.241, val_acc:0.952]
Epoch [74/120    avg_loss:0.243, val_acc:0.971]
Epoch [75/120    avg_loss:0.193, val_acc:0.971]
Epoch [76/120    avg_loss:0.174, val_acc:0.967]
Epoch [77/120    avg_loss:0.188, val_acc:0.985]
Epoch [78/120    avg_loss:0.214, val_acc:0.963]
Epoch [79/120    avg_loss:0.175, val_acc:0.985]
Epoch [80/120    avg_loss:0.181, val_acc:0.969]
Epoch [81/120    avg_loss:0.151, val_acc:0.975]
Epoch [82/120    avg_loss:0.170, val_acc:0.975]
Epoch [83/120    avg_loss:0.152, val_acc:0.975]
Epoch [84/120    avg_loss:0.165, val_acc:0.973]
Epoch [85/120    avg_loss:0.180, val_acc:0.958]
Epoch [86/120    avg_loss:0.219, val_acc:0.979]
Epoch [87/120    avg_loss:0.168, val_acc:0.981]
Epoch [88/120    avg_loss:0.160, val_acc:0.965]
Epoch [89/120    avg_loss:0.146, val_acc:0.983]
Epoch [90/120    avg_loss:0.140, val_acc:0.971]
Epoch [91/120    avg_loss:0.111, val_acc:0.992]
Epoch [92/120    avg_loss:0.130, val_acc:0.994]
Epoch [93/120    avg_loss:0.109, val_acc:0.990]
Epoch [94/120    avg_loss:0.123, val_acc:0.990]
Epoch [95/120    avg_loss:0.116, val_acc:0.985]
Epoch [96/120    avg_loss:0.163, val_acc:0.969]
Epoch [97/120    avg_loss:0.126, val_acc:0.985]
Epoch [98/120    avg_loss:0.147, val_acc:0.969]
Epoch [99/120    avg_loss:0.140, val_acc:0.979]
Epoch [100/120    avg_loss:0.134, val_acc:0.992]
Epoch [101/120    avg_loss:0.095, val_acc:0.988]
Epoch [102/120    avg_loss:0.114, val_acc:0.990]
Epoch [103/120    avg_loss:0.114, val_acc:0.990]
Epoch [104/120    avg_loss:0.092, val_acc:0.990]
Epoch [105/120    avg_loss:0.085, val_acc:0.992]
Epoch [106/120    avg_loss:0.071, val_acc:0.992]
Epoch [107/120    avg_loss:0.064, val_acc:0.992]
Epoch [108/120    avg_loss:0.076, val_acc:0.992]
Epoch [109/120    avg_loss:0.073, val_acc:0.994]
Epoch [110/120    avg_loss:0.065, val_acc:0.992]
Epoch [111/120    avg_loss:0.063, val_acc:0.996]
Epoch [112/120    avg_loss:0.078, val_acc:0.994]
Epoch [113/120    avg_loss:0.064, val_acc:0.996]
Epoch [114/120    avg_loss:0.068, val_acc:0.996]
Epoch [115/120    avg_loss:0.074, val_acc:0.996]
Epoch [116/120    avg_loss:0.063, val_acc:0.996]
Epoch [117/120    avg_loss:0.069, val_acc:0.996]
Epoch [118/120    avg_loss:0.060, val_acc:0.994]
Epoch [119/120    avg_loss:0.069, val_acc:0.992]
Epoch [120/120    avg_loss:0.059, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99412628 1.         1.         0.93959732 0.90909091
 0.98095238 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9916935014894516
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f829d0c2e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.670, val_acc:0.118]
Epoch [2/120    avg_loss:2.650, val_acc:0.123]
Epoch [3/120    avg_loss:2.630, val_acc:0.125]
Epoch [4/120    avg_loss:2.607, val_acc:0.125]
Epoch [5/120    avg_loss:2.584, val_acc:0.188]
Epoch [6/120    avg_loss:2.566, val_acc:0.315]
Epoch [7/120    avg_loss:2.547, val_acc:0.348]
Epoch [8/120    avg_loss:2.527, val_acc:0.385]
Epoch [9/120    avg_loss:2.509, val_acc:0.398]
Epoch [10/120    avg_loss:2.491, val_acc:0.398]
Epoch [11/120    avg_loss:2.470, val_acc:0.398]
Epoch [12/120    avg_loss:2.447, val_acc:0.398]
Epoch [13/120    avg_loss:2.430, val_acc:0.398]
Epoch [14/120    avg_loss:2.412, val_acc:0.398]
Epoch [15/120    avg_loss:2.391, val_acc:0.398]
Epoch [16/120    avg_loss:2.372, val_acc:0.398]
Epoch [17/120    avg_loss:2.356, val_acc:0.398]
Epoch [18/120    avg_loss:2.332, val_acc:0.398]
Epoch [19/120    avg_loss:2.309, val_acc:0.400]
Epoch [20/120    avg_loss:2.288, val_acc:0.419]
Epoch [21/120    avg_loss:2.270, val_acc:0.433]
Epoch [22/120    avg_loss:2.245, val_acc:0.456]
Epoch [23/120    avg_loss:2.222, val_acc:0.479]
Epoch [24/120    avg_loss:2.185, val_acc:0.483]
Epoch [25/120    avg_loss:2.160, val_acc:0.502]
Epoch [26/120    avg_loss:2.133, val_acc:0.519]
Epoch [27/120    avg_loss:2.093, val_acc:0.521]
Epoch [28/120    avg_loss:2.060, val_acc:0.544]
Epoch [29/120    avg_loss:2.026, val_acc:0.556]
Epoch [30/120    avg_loss:1.982, val_acc:0.546]
Epoch [31/120    avg_loss:1.966, val_acc:0.579]
Epoch [32/120    avg_loss:1.907, val_acc:0.577]
Epoch [33/120    avg_loss:1.882, val_acc:0.579]
Epoch [34/120    avg_loss:1.848, val_acc:0.590]
Epoch [35/120    avg_loss:1.808, val_acc:0.642]
Epoch [36/120    avg_loss:1.777, val_acc:0.640]
Epoch [37/120    avg_loss:1.732, val_acc:0.679]
Epoch [38/120    avg_loss:1.712, val_acc:0.673]
Epoch [39/120    avg_loss:1.685, val_acc:0.675]
Epoch [40/120    avg_loss:1.627, val_acc:0.677]
Epoch [41/120    avg_loss:1.585, val_acc:0.731]
Epoch [42/120    avg_loss:1.539, val_acc:0.740]
Epoch [43/120    avg_loss:1.509, val_acc:0.738]
Epoch [44/120    avg_loss:1.443, val_acc:0.752]
Epoch [45/120    avg_loss:1.402, val_acc:0.840]
Epoch [46/120    avg_loss:1.347, val_acc:0.815]
Epoch [47/120    avg_loss:1.301, val_acc:0.800]
Epoch [48/120    avg_loss:1.230, val_acc:0.835]
Epoch [49/120    avg_loss:1.200, val_acc:0.762]
Epoch [50/120    avg_loss:1.154, val_acc:0.865]
Epoch [51/120    avg_loss:1.108, val_acc:0.852]
Epoch [52/120    avg_loss:1.021, val_acc:0.904]
Epoch [53/120    avg_loss:1.000, val_acc:0.806]
Epoch [54/120    avg_loss:0.955, val_acc:0.902]
Epoch [55/120    avg_loss:0.918, val_acc:0.917]
Epoch [56/120    avg_loss:0.884, val_acc:0.894]
Epoch [57/120    avg_loss:0.844, val_acc:0.917]
Epoch [58/120    avg_loss:0.803, val_acc:0.892]
Epoch [59/120    avg_loss:0.726, val_acc:0.902]
Epoch [60/120    avg_loss:0.732, val_acc:0.879]
Epoch [61/120    avg_loss:0.728, val_acc:0.917]
Epoch [62/120    avg_loss:0.707, val_acc:0.883]
Epoch [63/120    avg_loss:0.641, val_acc:0.942]
Epoch [64/120    avg_loss:0.574, val_acc:0.940]
Epoch [65/120    avg_loss:0.556, val_acc:0.917]
Epoch [66/120    avg_loss:0.535, val_acc:0.940]
Epoch [67/120    avg_loss:0.481, val_acc:0.944]
Epoch [68/120    avg_loss:0.459, val_acc:0.960]
Epoch [69/120    avg_loss:0.415, val_acc:0.952]
Epoch [70/120    avg_loss:0.451, val_acc:0.950]
Epoch [71/120    avg_loss:0.455, val_acc:0.948]
Epoch [72/120    avg_loss:0.397, val_acc:0.960]
Epoch [73/120    avg_loss:0.356, val_acc:0.969]
Epoch [74/120    avg_loss:0.402, val_acc:0.956]
Epoch [75/120    avg_loss:0.338, val_acc:0.944]
Epoch [76/120    avg_loss:0.361, val_acc:0.958]
Epoch [77/120    avg_loss:0.331, val_acc:0.948]
Epoch [78/120    avg_loss:0.297, val_acc:0.967]
Epoch [79/120    avg_loss:0.262, val_acc:0.963]
Epoch [80/120    avg_loss:0.259, val_acc:0.952]
Epoch [81/120    avg_loss:0.251, val_acc:0.973]
Epoch [82/120    avg_loss:0.220, val_acc:0.979]
Epoch [83/120    avg_loss:0.225, val_acc:0.952]
Epoch [84/120    avg_loss:0.262, val_acc:0.973]
Epoch [85/120    avg_loss:0.221, val_acc:0.975]
Epoch [86/120    avg_loss:0.253, val_acc:0.977]
Epoch [87/120    avg_loss:0.248, val_acc:0.894]
Epoch [88/120    avg_loss:0.271, val_acc:0.954]
Epoch [89/120    avg_loss:0.292, val_acc:0.952]
Epoch [90/120    avg_loss:0.222, val_acc:0.969]
Epoch [91/120    avg_loss:0.217, val_acc:0.954]
Epoch [92/120    avg_loss:0.200, val_acc:0.975]
Epoch [93/120    avg_loss:0.266, val_acc:0.927]
Epoch [94/120    avg_loss:0.260, val_acc:0.944]
Epoch [95/120    avg_loss:0.221, val_acc:0.944]
Epoch [96/120    avg_loss:0.233, val_acc:0.973]
Epoch [97/120    avg_loss:0.182, val_acc:0.975]
Epoch [98/120    avg_loss:0.173, val_acc:0.973]
Epoch [99/120    avg_loss:0.161, val_acc:0.979]
Epoch [100/120    avg_loss:0.153, val_acc:0.979]
Epoch [101/120    avg_loss:0.154, val_acc:0.983]
Epoch [102/120    avg_loss:0.145, val_acc:0.983]
Epoch [103/120    avg_loss:0.146, val_acc:0.983]
Epoch [104/120    avg_loss:0.135, val_acc:0.981]
Epoch [105/120    avg_loss:0.130, val_acc:0.983]
Epoch [106/120    avg_loss:0.140, val_acc:0.981]
Epoch [107/120    avg_loss:0.134, val_acc:0.981]
Epoch [108/120    avg_loss:0.142, val_acc:0.985]
Epoch [109/120    avg_loss:0.156, val_acc:0.985]
Epoch [110/120    avg_loss:0.141, val_acc:0.985]
Epoch [111/120    avg_loss:0.125, val_acc:0.983]
Epoch [112/120    avg_loss:0.135, val_acc:0.983]
Epoch [113/120    avg_loss:0.124, val_acc:0.981]
Epoch [114/120    avg_loss:0.126, val_acc:0.981]
Epoch [115/120    avg_loss:0.125, val_acc:0.985]
Epoch [116/120    avg_loss:0.117, val_acc:0.983]
Epoch [117/120    avg_loss:0.146, val_acc:0.985]
Epoch [118/120    avg_loss:0.128, val_acc:0.983]
Epoch [119/120    avg_loss:0.114, val_acc:0.985]
Epoch [120/120    avg_loss:0.125, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99486427 0.99319728 1.         0.9476082  0.92459016
 0.98329356 0.98378378 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.991930780202367
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4ca64e2dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.607, val_acc:0.133]
Epoch [2/120    avg_loss:2.567, val_acc:0.146]
Epoch [3/120    avg_loss:2.534, val_acc:0.194]
Epoch [4/120    avg_loss:2.501, val_acc:0.337]
Epoch [5/120    avg_loss:2.479, val_acc:0.346]
Epoch [6/120    avg_loss:2.454, val_acc:0.354]
Epoch [7/120    avg_loss:2.431, val_acc:0.360]
Epoch [8/120    avg_loss:2.412, val_acc:0.379]
Epoch [9/120    avg_loss:2.389, val_acc:0.388]
Epoch [10/120    avg_loss:2.361, val_acc:0.390]
Epoch [11/120    avg_loss:2.344, val_acc:0.394]
Epoch [12/120    avg_loss:2.312, val_acc:0.417]
Epoch [13/120    avg_loss:2.291, val_acc:0.415]
Epoch [14/120    avg_loss:2.268, val_acc:0.419]
Epoch [15/120    avg_loss:2.233, val_acc:0.404]
Epoch [16/120    avg_loss:2.195, val_acc:0.410]
Epoch [17/120    avg_loss:2.168, val_acc:0.410]
Epoch [18/120    avg_loss:2.125, val_acc:0.421]
Epoch [19/120    avg_loss:2.089, val_acc:0.421]
Epoch [20/120    avg_loss:2.063, val_acc:0.477]
Epoch [21/120    avg_loss:2.011, val_acc:0.504]
Epoch [22/120    avg_loss:1.951, val_acc:0.525]
Epoch [23/120    avg_loss:1.905, val_acc:0.565]
Epoch [24/120    avg_loss:1.851, val_acc:0.546]
Epoch [25/120    avg_loss:1.827, val_acc:0.575]
Epoch [26/120    avg_loss:1.794, val_acc:0.575]
Epoch [27/120    avg_loss:1.756, val_acc:0.573]
Epoch [28/120    avg_loss:1.702, val_acc:0.633]
Epoch [29/120    avg_loss:1.649, val_acc:0.602]
Epoch [30/120    avg_loss:1.597, val_acc:0.613]
Epoch [31/120    avg_loss:1.574, val_acc:0.654]
Epoch [32/120    avg_loss:1.500, val_acc:0.685]
Epoch [33/120    avg_loss:1.418, val_acc:0.675]
Epoch [34/120    avg_loss:1.424, val_acc:0.671]
Epoch [35/120    avg_loss:1.367, val_acc:0.681]
Epoch [36/120    avg_loss:1.321, val_acc:0.708]
Epoch [37/120    avg_loss:1.238, val_acc:0.762]
Epoch [38/120    avg_loss:1.206, val_acc:0.754]
Epoch [39/120    avg_loss:1.175, val_acc:0.748]
Epoch [40/120    avg_loss:1.142, val_acc:0.731]
Epoch [41/120    avg_loss:1.047, val_acc:0.798]
Epoch [42/120    avg_loss:1.016, val_acc:0.831]
Epoch [43/120    avg_loss:0.950, val_acc:0.802]
Epoch [44/120    avg_loss:0.882, val_acc:0.827]
Epoch [45/120    avg_loss:0.893, val_acc:0.825]
Epoch [46/120    avg_loss:0.843, val_acc:0.812]
Epoch [47/120    avg_loss:0.775, val_acc:0.827]
Epoch [48/120    avg_loss:0.755, val_acc:0.821]
Epoch [49/120    avg_loss:0.697, val_acc:0.846]
Epoch [50/120    avg_loss:0.662, val_acc:0.821]
Epoch [51/120    avg_loss:0.721, val_acc:0.806]
Epoch [52/120    avg_loss:0.626, val_acc:0.852]
Epoch [53/120    avg_loss:0.601, val_acc:0.877]
Epoch [54/120    avg_loss:0.541, val_acc:0.823]
Epoch [55/120    avg_loss:0.541, val_acc:0.831]
Epoch [56/120    avg_loss:0.561, val_acc:0.844]
Epoch [57/120    avg_loss:0.514, val_acc:0.844]
Epoch [58/120    avg_loss:0.468, val_acc:0.863]
Epoch [59/120    avg_loss:0.466, val_acc:0.948]
Epoch [60/120    avg_loss:0.449, val_acc:0.860]
Epoch [61/120    avg_loss:0.424, val_acc:0.925]
Epoch [62/120    avg_loss:0.438, val_acc:0.885]
Epoch [63/120    avg_loss:0.426, val_acc:0.892]
Epoch [64/120    avg_loss:0.401, val_acc:0.923]
Epoch [65/120    avg_loss:0.421, val_acc:0.912]
Epoch [66/120    avg_loss:0.348, val_acc:0.938]
Epoch [67/120    avg_loss:0.326, val_acc:0.969]
Epoch [68/120    avg_loss:0.340, val_acc:0.950]
Epoch [69/120    avg_loss:0.325, val_acc:0.938]
Epoch [70/120    avg_loss:0.321, val_acc:0.960]
Epoch [71/120    avg_loss:0.309, val_acc:0.956]
Epoch [72/120    avg_loss:0.310, val_acc:0.979]
Epoch [73/120    avg_loss:0.302, val_acc:0.963]
Epoch [74/120    avg_loss:0.250, val_acc:0.973]
Epoch [75/120    avg_loss:0.326, val_acc:0.944]
Epoch [76/120    avg_loss:0.261, val_acc:0.935]
Epoch [77/120    avg_loss:0.251, val_acc:0.988]
Epoch [78/120    avg_loss:0.217, val_acc:0.971]
Epoch [79/120    avg_loss:0.195, val_acc:0.971]
Epoch [80/120    avg_loss:0.169, val_acc:0.985]
Epoch [81/120    avg_loss:0.190, val_acc:0.975]
Epoch [82/120    avg_loss:0.185, val_acc:0.952]
Epoch [83/120    avg_loss:0.192, val_acc:0.977]
Epoch [84/120    avg_loss:0.214, val_acc:0.954]
Epoch [85/120    avg_loss:0.175, val_acc:0.985]
Epoch [86/120    avg_loss:0.144, val_acc:0.988]
Epoch [87/120    avg_loss:0.152, val_acc:0.983]
Epoch [88/120    avg_loss:0.180, val_acc:0.985]
Epoch [89/120    avg_loss:0.261, val_acc:0.954]
Epoch [90/120    avg_loss:0.224, val_acc:0.965]
Epoch [91/120    avg_loss:0.350, val_acc:0.917]
Epoch [92/120    avg_loss:0.247, val_acc:0.979]
Epoch [93/120    avg_loss:0.205, val_acc:0.990]
Epoch [94/120    avg_loss:0.151, val_acc:0.985]
Epoch [95/120    avg_loss:0.154, val_acc:0.992]
Epoch [96/120    avg_loss:0.162, val_acc:0.992]
Epoch [97/120    avg_loss:0.142, val_acc:0.988]
Epoch [98/120    avg_loss:0.126, val_acc:0.992]
Epoch [99/120    avg_loss:0.160, val_acc:0.983]
Epoch [100/120    avg_loss:0.138, val_acc:0.971]
Epoch [101/120    avg_loss:0.152, val_acc:0.988]
Epoch [102/120    avg_loss:0.220, val_acc:0.981]
Epoch [103/120    avg_loss:0.147, val_acc:0.975]
Epoch [104/120    avg_loss:0.182, val_acc:0.983]
Epoch [105/120    avg_loss:0.164, val_acc:0.990]
Epoch [106/120    avg_loss:0.111, val_acc:0.994]
Epoch [107/120    avg_loss:0.094, val_acc:0.983]
Epoch [108/120    avg_loss:0.101, val_acc:0.979]
Epoch [109/120    avg_loss:0.126, val_acc:0.975]
Epoch [110/120    avg_loss:0.093, val_acc:0.990]
Epoch [111/120    avg_loss:0.110, val_acc:0.979]
Epoch [112/120    avg_loss:0.077, val_acc:0.994]
Epoch [113/120    avg_loss:0.121, val_acc:0.988]
Epoch [114/120    avg_loss:0.104, val_acc:0.985]
Epoch [115/120    avg_loss:0.098, val_acc:0.990]
Epoch [116/120    avg_loss:0.102, val_acc:0.977]
Epoch [117/120    avg_loss:0.119, val_acc:0.981]
Epoch [118/120    avg_loss:0.095, val_acc:0.988]
Epoch [119/120    avg_loss:0.082, val_acc:0.985]
Epoch [120/120    avg_loss:0.070, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  14   0   0   0   0   0   0   2   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99780541 1.         0.98678414 0.93986637 0.93645485
 0.99277108 1.         1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9928791838400896
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff8bce19e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.661, val_acc:0.021]
Epoch [2/120    avg_loss:2.641, val_acc:0.079]
Epoch [3/120    avg_loss:2.615, val_acc:0.103]
Epoch [4/120    avg_loss:2.593, val_acc:0.108]
Epoch [5/120    avg_loss:2.570, val_acc:0.110]
Epoch [6/120    avg_loss:2.544, val_acc:0.117]
Epoch [7/120    avg_loss:2.523, val_acc:0.181]
Epoch [8/120    avg_loss:2.495, val_acc:0.242]
Epoch [9/120    avg_loss:2.474, val_acc:0.265]
Epoch [10/120    avg_loss:2.442, val_acc:0.306]
Epoch [11/120    avg_loss:2.410, val_acc:0.348]
Epoch [12/120    avg_loss:2.376, val_acc:0.352]
Epoch [13/120    avg_loss:2.347, val_acc:0.365]
Epoch [14/120    avg_loss:2.315, val_acc:0.379]
Epoch [15/120    avg_loss:2.279, val_acc:0.390]
Epoch [16/120    avg_loss:2.243, val_acc:0.419]
Epoch [17/120    avg_loss:2.190, val_acc:0.402]
Epoch [18/120    avg_loss:2.143, val_acc:0.429]
Epoch [19/120    avg_loss:2.124, val_acc:0.398]
Epoch [20/120    avg_loss:2.067, val_acc:0.438]
Epoch [21/120    avg_loss:2.027, val_acc:0.429]
Epoch [22/120    avg_loss:2.008, val_acc:0.487]
Epoch [23/120    avg_loss:1.929, val_acc:0.496]
Epoch [24/120    avg_loss:1.917, val_acc:0.496]
Epoch [25/120    avg_loss:1.848, val_acc:0.508]
Epoch [26/120    avg_loss:1.821, val_acc:0.506]
Epoch [27/120    avg_loss:1.773, val_acc:0.527]
Epoch [28/120    avg_loss:1.741, val_acc:0.533]
Epoch [29/120    avg_loss:1.706, val_acc:0.540]
Epoch [30/120    avg_loss:1.649, val_acc:0.567]
Epoch [31/120    avg_loss:1.590, val_acc:0.565]
Epoch [32/120    avg_loss:1.543, val_acc:0.627]
Epoch [33/120    avg_loss:1.507, val_acc:0.633]
Epoch [34/120    avg_loss:1.471, val_acc:0.662]
Epoch [35/120    avg_loss:1.436, val_acc:0.652]
Epoch [36/120    avg_loss:1.377, val_acc:0.667]
Epoch [37/120    avg_loss:1.309, val_acc:0.667]
Epoch [38/120    avg_loss:1.278, val_acc:0.673]
Epoch [39/120    avg_loss:1.246, val_acc:0.675]
Epoch [40/120    avg_loss:1.225, val_acc:0.683]
Epoch [41/120    avg_loss:1.135, val_acc:0.688]
Epoch [42/120    avg_loss:1.074, val_acc:0.679]
Epoch [43/120    avg_loss:1.054, val_acc:0.694]
Epoch [44/120    avg_loss:0.997, val_acc:0.708]
Epoch [45/120    avg_loss:0.939, val_acc:0.719]
Epoch [46/120    avg_loss:0.890, val_acc:0.715]
Epoch [47/120    avg_loss:0.873, val_acc:0.727]
Epoch [48/120    avg_loss:0.871, val_acc:0.769]
Epoch [49/120    avg_loss:0.843, val_acc:0.740]
Epoch [50/120    avg_loss:0.863, val_acc:0.717]
Epoch [51/120    avg_loss:0.788, val_acc:0.760]
Epoch [52/120    avg_loss:0.713, val_acc:0.787]
Epoch [53/120    avg_loss:0.691, val_acc:0.825]
Epoch [54/120    avg_loss:0.635, val_acc:0.854]
Epoch [55/120    avg_loss:0.615, val_acc:0.835]
Epoch [56/120    avg_loss:0.587, val_acc:0.896]
Epoch [57/120    avg_loss:0.592, val_acc:0.902]
Epoch [58/120    avg_loss:0.597, val_acc:0.887]
Epoch [59/120    avg_loss:0.505, val_acc:0.919]
Epoch [60/120    avg_loss:0.488, val_acc:0.933]
Epoch [61/120    avg_loss:0.451, val_acc:0.942]
Epoch [62/120    avg_loss:0.457, val_acc:0.933]
Epoch [63/120    avg_loss:0.423, val_acc:0.921]
Epoch [64/120    avg_loss:0.439, val_acc:0.948]
Epoch [65/120    avg_loss:0.425, val_acc:0.923]
Epoch [66/120    avg_loss:0.509, val_acc:0.921]
Epoch [67/120    avg_loss:0.430, val_acc:0.910]
Epoch [68/120    avg_loss:0.388, val_acc:0.942]
Epoch [69/120    avg_loss:0.358, val_acc:0.940]
Epoch [70/120    avg_loss:0.398, val_acc:0.931]
Epoch [71/120    avg_loss:0.399, val_acc:0.938]
Epoch [72/120    avg_loss:0.344, val_acc:0.956]
Epoch [73/120    avg_loss:0.330, val_acc:0.923]
Epoch [74/120    avg_loss:0.342, val_acc:0.946]
Epoch [75/120    avg_loss:0.277, val_acc:0.952]
Epoch [76/120    avg_loss:0.312, val_acc:0.952]
Epoch [77/120    avg_loss:0.289, val_acc:0.950]
Epoch [78/120    avg_loss:0.239, val_acc:0.950]
Epoch [79/120    avg_loss:0.233, val_acc:0.971]
Epoch [80/120    avg_loss:0.250, val_acc:0.956]
Epoch [81/120    avg_loss:0.232, val_acc:0.963]
Epoch [82/120    avg_loss:0.217, val_acc:0.975]
Epoch [83/120    avg_loss:0.200, val_acc:0.975]
Epoch [84/120    avg_loss:0.212, val_acc:0.973]
Epoch [85/120    avg_loss:0.204, val_acc:0.971]
Epoch [86/120    avg_loss:0.209, val_acc:0.915]
Epoch [87/120    avg_loss:0.217, val_acc:0.971]
Epoch [88/120    avg_loss:0.256, val_acc:0.946]
Epoch [89/120    avg_loss:0.358, val_acc:0.915]
Epoch [90/120    avg_loss:0.268, val_acc:0.956]
Epoch [91/120    avg_loss:0.223, val_acc:0.963]
Epoch [92/120    avg_loss:0.185, val_acc:0.981]
Epoch [93/120    avg_loss:0.191, val_acc:0.973]
Epoch [94/120    avg_loss:0.160, val_acc:0.977]
Epoch [95/120    avg_loss:0.150, val_acc:0.971]
Epoch [96/120    avg_loss:0.213, val_acc:0.971]
Epoch [97/120    avg_loss:0.273, val_acc:0.956]
Epoch [98/120    avg_loss:0.220, val_acc:0.958]
Epoch [99/120    avg_loss:0.225, val_acc:0.977]
Epoch [100/120    avg_loss:0.161, val_acc:0.977]
Epoch [101/120    avg_loss:0.151, val_acc:0.971]
Epoch [102/120    avg_loss:0.136, val_acc:0.981]
Epoch [103/120    avg_loss:0.152, val_acc:0.977]
Epoch [104/120    avg_loss:0.151, val_acc:0.979]
Epoch [105/120    avg_loss:0.134, val_acc:0.979]
Epoch [106/120    avg_loss:0.156, val_acc:0.979]
Epoch [107/120    avg_loss:0.169, val_acc:0.983]
Epoch [108/120    avg_loss:0.120, val_acc:0.985]
Epoch [109/120    avg_loss:0.128, val_acc:0.973]
Epoch [110/120    avg_loss:0.116, val_acc:0.979]
Epoch [111/120    avg_loss:0.108, val_acc:0.979]
Epoch [112/120    avg_loss:0.105, val_acc:0.977]
Epoch [113/120    avg_loss:0.093, val_acc:0.981]
Epoch [114/120    avg_loss:0.122, val_acc:0.979]
Epoch [115/120    avg_loss:0.117, val_acc:0.975]
Epoch [116/120    avg_loss:0.104, val_acc:0.985]
Epoch [117/120    avg_loss:0.141, val_acc:0.981]
Epoch [118/120    avg_loss:0.107, val_acc:0.983]
Epoch [119/120    avg_loss:0.105, val_acc:0.973]
Epoch [120/120    avg_loss:0.114, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 121 106   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.48400852878464

F1 scores:
[       nan 0.99560117 1.         0.98678414 0.68361582 0.73232323
 0.98564593 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9720054020438454
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f727b816dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.100]
Epoch [2/120    avg_loss:2.617, val_acc:0.129]
Epoch [3/120    avg_loss:2.598, val_acc:0.302]
Epoch [4/120    avg_loss:2.583, val_acc:0.308]
Epoch [5/120    avg_loss:2.567, val_acc:0.359]
Epoch [6/120    avg_loss:2.548, val_acc:0.406]
Epoch [7/120    avg_loss:2.528, val_acc:0.435]
Epoch [8/120    avg_loss:2.510, val_acc:0.450]
Epoch [9/120    avg_loss:2.486, val_acc:0.456]
Epoch [10/120    avg_loss:2.463, val_acc:0.487]
Epoch [11/120    avg_loss:2.437, val_acc:0.496]
Epoch [12/120    avg_loss:2.418, val_acc:0.500]
Epoch [13/120    avg_loss:2.400, val_acc:0.498]
Epoch [14/120    avg_loss:2.372, val_acc:0.492]
Epoch [15/120    avg_loss:2.349, val_acc:0.515]
Epoch [16/120    avg_loss:2.323, val_acc:0.523]
Epoch [17/120    avg_loss:2.289, val_acc:0.533]
Epoch [18/120    avg_loss:2.260, val_acc:0.542]
Epoch [19/120    avg_loss:2.222, val_acc:0.550]
Epoch [20/120    avg_loss:2.191, val_acc:0.558]
Epoch [21/120    avg_loss:2.153, val_acc:0.560]
Epoch [22/120    avg_loss:2.111, val_acc:0.550]
Epoch [23/120    avg_loss:2.064, val_acc:0.552]
Epoch [24/120    avg_loss:2.016, val_acc:0.554]
Epoch [25/120    avg_loss:1.975, val_acc:0.562]
Epoch [26/120    avg_loss:1.922, val_acc:0.556]
Epoch [27/120    avg_loss:1.868, val_acc:0.588]
Epoch [28/120    avg_loss:1.821, val_acc:0.594]
Epoch [29/120    avg_loss:1.773, val_acc:0.606]
Epoch [30/120    avg_loss:1.722, val_acc:0.629]
Epoch [31/120    avg_loss:1.675, val_acc:0.613]
Epoch [32/120    avg_loss:1.636, val_acc:0.637]
Epoch [33/120    avg_loss:1.569, val_acc:0.665]
Epoch [34/120    avg_loss:1.495, val_acc:0.685]
Epoch [35/120    avg_loss:1.433, val_acc:0.675]
Epoch [36/120    avg_loss:1.381, val_acc:0.698]
Epoch [37/120    avg_loss:1.328, val_acc:0.694]
Epoch [38/120    avg_loss:1.313, val_acc:0.698]
Epoch [39/120    avg_loss:1.226, val_acc:0.719]
Epoch [40/120    avg_loss:1.165, val_acc:0.715]
Epoch [41/120    avg_loss:1.116, val_acc:0.735]
Epoch [42/120    avg_loss:1.067, val_acc:0.744]
Epoch [43/120    avg_loss:1.064, val_acc:0.750]
Epoch [44/120    avg_loss:1.064, val_acc:0.748]
Epoch [45/120    avg_loss:1.000, val_acc:0.754]
Epoch [46/120    avg_loss:0.932, val_acc:0.771]
Epoch [47/120    avg_loss:0.875, val_acc:0.812]
Epoch [48/120    avg_loss:0.805, val_acc:0.848]
Epoch [49/120    avg_loss:0.778, val_acc:0.831]
Epoch [50/120    avg_loss:0.703, val_acc:0.881]
Epoch [51/120    avg_loss:0.720, val_acc:0.871]
Epoch [52/120    avg_loss:0.677, val_acc:0.885]
Epoch [53/120    avg_loss:0.657, val_acc:0.917]
Epoch [54/120    avg_loss:0.643, val_acc:0.925]
Epoch [55/120    avg_loss:0.603, val_acc:0.933]
Epoch [56/120    avg_loss:0.564, val_acc:0.923]
Epoch [57/120    avg_loss:0.557, val_acc:0.935]
Epoch [58/120    avg_loss:0.518, val_acc:0.958]
Epoch [59/120    avg_loss:0.490, val_acc:0.890]
Epoch [60/120    avg_loss:0.461, val_acc:0.973]
Epoch [61/120    avg_loss:0.475, val_acc:0.952]
Epoch [62/120    avg_loss:0.395, val_acc:0.948]
Epoch [63/120    avg_loss:0.401, val_acc:0.944]
Epoch [64/120    avg_loss:0.413, val_acc:0.950]
Epoch [65/120    avg_loss:0.409, val_acc:0.940]
Epoch [66/120    avg_loss:0.347, val_acc:0.958]
Epoch [67/120    avg_loss:0.382, val_acc:0.965]
Epoch [68/120    avg_loss:0.354, val_acc:0.973]
Epoch [69/120    avg_loss:0.329, val_acc:0.954]
Epoch [70/120    avg_loss:0.291, val_acc:0.975]
Epoch [71/120    avg_loss:0.316, val_acc:0.946]
Epoch [72/120    avg_loss:0.278, val_acc:0.969]
Epoch [73/120    avg_loss:0.255, val_acc:0.944]
Epoch [74/120    avg_loss:0.324, val_acc:0.958]
Epoch [75/120    avg_loss:0.281, val_acc:0.944]
Epoch [76/120    avg_loss:0.225, val_acc:0.977]
Epoch [77/120    avg_loss:0.233, val_acc:0.977]
Epoch [78/120    avg_loss:0.251, val_acc:0.965]
Epoch [79/120    avg_loss:0.273, val_acc:0.954]
Epoch [80/120    avg_loss:0.226, val_acc:0.977]
Epoch [81/120    avg_loss:0.205, val_acc:0.967]
Epoch [82/120    avg_loss:0.184, val_acc:0.960]
Epoch [83/120    avg_loss:0.212, val_acc:0.985]
Epoch [84/120    avg_loss:0.195, val_acc:0.969]
Epoch [85/120    avg_loss:0.147, val_acc:0.977]
Epoch [86/120    avg_loss:0.148, val_acc:0.975]
Epoch [87/120    avg_loss:0.168, val_acc:0.979]
Epoch [88/120    avg_loss:0.144, val_acc:0.977]
Epoch [89/120    avg_loss:0.118, val_acc:0.979]
Epoch [90/120    avg_loss:0.128, val_acc:0.979]
Epoch [91/120    avg_loss:0.121, val_acc:0.975]
Epoch [92/120    avg_loss:0.109, val_acc:0.981]
Epoch [93/120    avg_loss:0.133, val_acc:0.977]
Epoch [94/120    avg_loss:0.114, val_acc:0.979]
Epoch [95/120    avg_loss:0.122, val_acc:0.981]
Epoch [96/120    avg_loss:0.156, val_acc:0.973]
Epoch [97/120    avg_loss:0.156, val_acc:0.988]
Epoch [98/120    avg_loss:0.108, val_acc:0.990]
Epoch [99/120    avg_loss:0.098, val_acc:0.988]
Epoch [100/120    avg_loss:0.097, val_acc:0.985]
Epoch [101/120    avg_loss:0.079, val_acc:0.985]
Epoch [102/120    avg_loss:0.089, val_acc:0.985]
Epoch [103/120    avg_loss:0.086, val_acc:0.985]
Epoch [104/120    avg_loss:0.081, val_acc:0.988]
Epoch [105/120    avg_loss:0.081, val_acc:0.988]
Epoch [106/120    avg_loss:0.086, val_acc:0.990]
Epoch [107/120    avg_loss:0.080, val_acc:0.990]
Epoch [108/120    avg_loss:0.079, val_acc:0.988]
Epoch [109/120    avg_loss:0.084, val_acc:0.988]
Epoch [110/120    avg_loss:0.077, val_acc:0.988]
Epoch [111/120    avg_loss:0.089, val_acc:0.988]
Epoch [112/120    avg_loss:0.082, val_acc:0.988]
Epoch [113/120    avg_loss:0.077, val_acc:0.988]
Epoch [114/120    avg_loss:0.074, val_acc:0.988]
Epoch [115/120    avg_loss:0.072, val_acc:0.988]
Epoch [116/120    avg_loss:0.076, val_acc:0.988]
Epoch [117/120    avg_loss:0.072, val_acc:0.988]
Epoch [118/120    avg_loss:0.077, val_acc:0.990]
Epoch [119/120    avg_loss:0.070, val_acc:0.992]
Epoch [120/120    avg_loss:0.066, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.996337   0.99095023 1.         0.94065934 0.90657439
 0.98800959 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.991455051745168
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7dc1707dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.637, val_acc:0.121]
Epoch [2/120    avg_loss:2.606, val_acc:0.329]
Epoch [3/120    avg_loss:2.583, val_acc:0.371]
Epoch [4/120    avg_loss:2.558, val_acc:0.373]
Epoch [5/120    avg_loss:2.535, val_acc:0.375]
Epoch [6/120    avg_loss:2.508, val_acc:0.379]
Epoch [7/120    avg_loss:2.483, val_acc:0.379]
Epoch [8/120    avg_loss:2.456, val_acc:0.390]
Epoch [9/120    avg_loss:2.429, val_acc:0.404]
Epoch [10/120    avg_loss:2.402, val_acc:0.400]
Epoch [11/120    avg_loss:2.382, val_acc:0.390]
Epoch [12/120    avg_loss:2.354, val_acc:0.398]
Epoch [13/120    avg_loss:2.330, val_acc:0.369]
Epoch [14/120    avg_loss:2.305, val_acc:0.360]
Epoch [15/120    avg_loss:2.266, val_acc:0.356]
Epoch [16/120    avg_loss:2.242, val_acc:0.369]
Epoch [17/120    avg_loss:2.201, val_acc:0.369]
Epoch [18/120    avg_loss:2.163, val_acc:0.377]
Epoch [19/120    avg_loss:2.136, val_acc:0.381]
Epoch [20/120    avg_loss:2.081, val_acc:0.419]
Epoch [21/120    avg_loss:2.032, val_acc:0.423]
Epoch [22/120    avg_loss:2.005, val_acc:0.452]
Epoch [23/120    avg_loss:1.963, val_acc:0.492]
Epoch [24/120    avg_loss:1.918, val_acc:0.554]
Epoch [25/120    avg_loss:1.874, val_acc:0.571]
Epoch [26/120    avg_loss:1.846, val_acc:0.575]
Epoch [27/120    avg_loss:1.766, val_acc:0.583]
Epoch [28/120    avg_loss:1.723, val_acc:0.588]
Epoch [29/120    avg_loss:1.677, val_acc:0.608]
Epoch [30/120    avg_loss:1.661, val_acc:0.594]
Epoch [31/120    avg_loss:1.597, val_acc:0.604]
Epoch [32/120    avg_loss:1.521, val_acc:0.613]
Epoch [33/120    avg_loss:1.459, val_acc:0.642]
Epoch [34/120    avg_loss:1.416, val_acc:0.648]
Epoch [35/120    avg_loss:1.349, val_acc:0.679]
Epoch [36/120    avg_loss:1.318, val_acc:0.677]
Epoch [37/120    avg_loss:1.260, val_acc:0.725]
Epoch [38/120    avg_loss:1.213, val_acc:0.725]
Epoch [39/120    avg_loss:1.192, val_acc:0.762]
Epoch [40/120    avg_loss:1.135, val_acc:0.785]
Epoch [41/120    avg_loss:1.097, val_acc:0.781]
Epoch [42/120    avg_loss:1.030, val_acc:0.787]
Epoch [43/120    avg_loss:0.977, val_acc:0.802]
Epoch [44/120    avg_loss:0.962, val_acc:0.796]
Epoch [45/120    avg_loss:0.914, val_acc:0.848]
Epoch [46/120    avg_loss:0.879, val_acc:0.825]
Epoch [47/120    avg_loss:0.846, val_acc:0.844]
Epoch [48/120    avg_loss:0.756, val_acc:0.829]
Epoch [49/120    avg_loss:0.711, val_acc:0.858]
Epoch [50/120    avg_loss:0.681, val_acc:0.863]
Epoch [51/120    avg_loss:0.638, val_acc:0.863]
Epoch [52/120    avg_loss:0.632, val_acc:0.865]
Epoch [53/120    avg_loss:0.620, val_acc:0.840]
Epoch [54/120    avg_loss:0.607, val_acc:0.858]
Epoch [55/120    avg_loss:0.578, val_acc:0.852]
Epoch [56/120    avg_loss:0.519, val_acc:0.842]
Epoch [57/120    avg_loss:0.516, val_acc:0.856]
Epoch [58/120    avg_loss:0.509, val_acc:0.856]
Epoch [59/120    avg_loss:0.485, val_acc:0.856]
Epoch [60/120    avg_loss:0.412, val_acc:0.885]
Epoch [61/120    avg_loss:0.418, val_acc:0.896]
Epoch [62/120    avg_loss:0.408, val_acc:0.863]
Epoch [63/120    avg_loss:0.347, val_acc:0.933]
Epoch [64/120    avg_loss:0.352, val_acc:0.952]
Epoch [65/120    avg_loss:0.331, val_acc:0.921]
Epoch [66/120    avg_loss:0.327, val_acc:0.956]
Epoch [67/120    avg_loss:0.299, val_acc:0.958]
Epoch [68/120    avg_loss:0.310, val_acc:0.960]
Epoch [69/120    avg_loss:0.260, val_acc:0.956]
Epoch [70/120    avg_loss:0.295, val_acc:0.971]
Epoch [71/120    avg_loss:0.271, val_acc:0.967]
Epoch [72/120    avg_loss:0.247, val_acc:0.944]
Epoch [73/120    avg_loss:0.296, val_acc:0.965]
Epoch [74/120    avg_loss:0.329, val_acc:0.963]
Epoch [75/120    avg_loss:0.251, val_acc:0.954]
Epoch [76/120    avg_loss:0.277, val_acc:0.963]
Epoch [77/120    avg_loss:0.213, val_acc:0.975]
Epoch [78/120    avg_loss:0.218, val_acc:0.950]
Epoch [79/120    avg_loss:0.239, val_acc:0.960]
Epoch [80/120    avg_loss:0.199, val_acc:0.983]
Epoch [81/120    avg_loss:0.176, val_acc:0.979]
Epoch [82/120    avg_loss:0.165, val_acc:0.965]
Epoch [83/120    avg_loss:0.187, val_acc:0.981]
Epoch [84/120    avg_loss:0.155, val_acc:0.979]
Epoch [85/120    avg_loss:0.196, val_acc:0.979]
Epoch [86/120    avg_loss:0.161, val_acc:0.975]
Epoch [87/120    avg_loss:0.197, val_acc:0.967]
Epoch [88/120    avg_loss:0.198, val_acc:0.960]
Epoch [89/120    avg_loss:0.221, val_acc:0.977]
Epoch [90/120    avg_loss:0.167, val_acc:0.985]
Epoch [91/120    avg_loss:0.136, val_acc:0.983]
Epoch [92/120    avg_loss:0.119, val_acc:0.985]
Epoch [93/120    avg_loss:0.103, val_acc:0.983]
Epoch [94/120    avg_loss:0.108, val_acc:0.988]
Epoch [95/120    avg_loss:0.115, val_acc:0.981]
Epoch [96/120    avg_loss:0.103, val_acc:0.985]
Epoch [97/120    avg_loss:0.144, val_acc:0.988]
Epoch [98/120    avg_loss:0.107, val_acc:0.990]
Epoch [99/120    avg_loss:0.103, val_acc:0.981]
Epoch [100/120    avg_loss:0.151, val_acc:0.969]
Epoch [101/120    avg_loss:0.135, val_acc:0.983]
Epoch [102/120    avg_loss:0.111, val_acc:0.985]
Epoch [103/120    avg_loss:0.117, val_acc:0.983]
Epoch [104/120    avg_loss:0.090, val_acc:0.988]
Epoch [105/120    avg_loss:0.101, val_acc:0.990]
Epoch [106/120    avg_loss:0.094, val_acc:0.983]
Epoch [107/120    avg_loss:0.090, val_acc:0.990]
Epoch [108/120    avg_loss:0.084, val_acc:0.983]
Epoch [109/120    avg_loss:0.099, val_acc:0.981]
Epoch [110/120    avg_loss:0.083, val_acc:0.985]
Epoch [111/120    avg_loss:0.104, val_acc:0.990]
Epoch [112/120    avg_loss:0.067, val_acc:0.985]
Epoch [113/120    avg_loss:0.055, val_acc:0.992]
Epoch [114/120    avg_loss:0.055, val_acc:0.994]
Epoch [115/120    avg_loss:0.072, val_acc:0.992]
Epoch [116/120    avg_loss:0.094, val_acc:0.979]
Epoch [117/120    avg_loss:0.061, val_acc:0.990]
Epoch [118/120    avg_loss:0.065, val_acc:0.981]
Epoch [119/120    avg_loss:0.067, val_acc:0.992]
Epoch [120/120    avg_loss:0.059, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 670   0   0   0   0  15   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219  11   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   4   7   0   0   0   0   0 377   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 0.98601913 0.97986577 0.97550111 0.92307692 0.92651757
 0.96487119 0.98924731 0.98562092 1.         1.         1.
 1.         1.        ]

Kappa:
0.9852889386913813
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7cfeacd30>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.081]
Epoch [2/120    avg_loss:2.617, val_acc:0.073]
Epoch [3/120    avg_loss:2.593, val_acc:0.092]
Epoch [4/120    avg_loss:2.574, val_acc:0.233]
Epoch [5/120    avg_loss:2.550, val_acc:0.344]
Epoch [6/120    avg_loss:2.529, val_acc:0.369]
Epoch [7/120    avg_loss:2.503, val_acc:0.375]
Epoch [8/120    avg_loss:2.484, val_acc:0.379]
Epoch [9/120    avg_loss:2.456, val_acc:0.381]
Epoch [10/120    avg_loss:2.437, val_acc:0.390]
Epoch [11/120    avg_loss:2.416, val_acc:0.394]
Epoch [12/120    avg_loss:2.394, val_acc:0.404]
Epoch [13/120    avg_loss:2.367, val_acc:0.404]
Epoch [14/120    avg_loss:2.345, val_acc:0.404]
Epoch [15/120    avg_loss:2.321, val_acc:0.404]
Epoch [16/120    avg_loss:2.303, val_acc:0.404]
Epoch [17/120    avg_loss:2.277, val_acc:0.406]
Epoch [18/120    avg_loss:2.254, val_acc:0.444]
Epoch [19/120    avg_loss:2.233, val_acc:0.440]
Epoch [20/120    avg_loss:2.202, val_acc:0.494]
Epoch [21/120    avg_loss:2.165, val_acc:0.500]
Epoch [22/120    avg_loss:2.141, val_acc:0.490]
Epoch [23/120    avg_loss:2.110, val_acc:0.504]
Epoch [24/120    avg_loss:2.076, val_acc:0.502]
Epoch [25/120    avg_loss:2.043, val_acc:0.537]
Epoch [26/120    avg_loss:1.993, val_acc:0.567]
Epoch [27/120    avg_loss:1.953, val_acc:0.554]
Epoch [28/120    avg_loss:1.902, val_acc:0.567]
Epoch [29/120    avg_loss:1.858, val_acc:0.592]
Epoch [30/120    avg_loss:1.825, val_acc:0.610]
Epoch [31/120    avg_loss:1.776, val_acc:0.625]
Epoch [32/120    avg_loss:1.735, val_acc:0.623]
Epoch [33/120    avg_loss:1.689, val_acc:0.631]
Epoch [34/120    avg_loss:1.615, val_acc:0.654]
Epoch [35/120    avg_loss:1.566, val_acc:0.660]
Epoch [36/120    avg_loss:1.515, val_acc:0.692]
Epoch [37/120    avg_loss:1.463, val_acc:0.719]
Epoch [38/120    avg_loss:1.428, val_acc:0.744]
Epoch [39/120    avg_loss:1.387, val_acc:0.744]
Epoch [40/120    avg_loss:1.307, val_acc:0.754]
Epoch [41/120    avg_loss:1.289, val_acc:0.758]
Epoch [42/120    avg_loss:1.208, val_acc:0.767]
Epoch [43/120    avg_loss:1.156, val_acc:0.762]
Epoch [44/120    avg_loss:1.081, val_acc:0.794]
Epoch [45/120    avg_loss:1.050, val_acc:0.796]
Epoch [46/120    avg_loss:1.011, val_acc:0.806]
Epoch [47/120    avg_loss:0.955, val_acc:0.796]
Epoch [48/120    avg_loss:0.934, val_acc:0.823]
Epoch [49/120    avg_loss:0.891, val_acc:0.819]
Epoch [50/120    avg_loss:0.869, val_acc:0.804]
Epoch [51/120    avg_loss:0.815, val_acc:0.806]
Epoch [52/120    avg_loss:0.779, val_acc:0.808]
Epoch [53/120    avg_loss:0.729, val_acc:0.825]
Epoch [54/120    avg_loss:0.675, val_acc:0.844]
Epoch [55/120    avg_loss:0.661, val_acc:0.815]
Epoch [56/120    avg_loss:0.692, val_acc:0.833]
Epoch [57/120    avg_loss:0.664, val_acc:0.800]
Epoch [58/120    avg_loss:0.695, val_acc:0.800]
Epoch [59/120    avg_loss:0.654, val_acc:0.827]
Epoch [60/120    avg_loss:0.644, val_acc:0.850]
Epoch [61/120    avg_loss:0.574, val_acc:0.885]
Epoch [62/120    avg_loss:0.527, val_acc:0.877]
Epoch [63/120    avg_loss:0.549, val_acc:0.929]
Epoch [64/120    avg_loss:0.524, val_acc:0.852]
Epoch [65/120    avg_loss:0.447, val_acc:0.929]
Epoch [66/120    avg_loss:0.465, val_acc:0.933]
Epoch [67/120    avg_loss:0.435, val_acc:0.956]
Epoch [68/120    avg_loss:0.438, val_acc:0.963]
Epoch [69/120    avg_loss:0.380, val_acc:0.946]
Epoch [70/120    avg_loss:0.394, val_acc:0.927]
Epoch [71/120    avg_loss:0.328, val_acc:0.958]
Epoch [72/120    avg_loss:0.328, val_acc:0.965]
Epoch [73/120    avg_loss:0.326, val_acc:0.967]
Epoch [74/120    avg_loss:0.269, val_acc:0.967]
Epoch [75/120    avg_loss:0.332, val_acc:0.942]
Epoch [76/120    avg_loss:0.291, val_acc:0.942]
Epoch [77/120    avg_loss:0.252, val_acc:0.979]
Epoch [78/120    avg_loss:0.245, val_acc:0.967]
Epoch [79/120    avg_loss:0.256, val_acc:0.967]
Epoch [80/120    avg_loss:0.264, val_acc:0.975]
Epoch [81/120    avg_loss:0.231, val_acc:0.952]
Epoch [82/120    avg_loss:0.274, val_acc:0.940]
Epoch [83/120    avg_loss:0.229, val_acc:0.973]
Epoch [84/120    avg_loss:0.264, val_acc:0.960]
Epoch [85/120    avg_loss:0.214, val_acc:0.967]
Epoch [86/120    avg_loss:0.199, val_acc:0.963]
Epoch [87/120    avg_loss:0.198, val_acc:0.985]
Epoch [88/120    avg_loss:0.203, val_acc:0.973]
Epoch [89/120    avg_loss:0.239, val_acc:0.965]
Epoch [90/120    avg_loss:0.195, val_acc:0.975]
Epoch [91/120    avg_loss:0.172, val_acc:0.977]
Epoch [92/120    avg_loss:0.205, val_acc:0.960]
Epoch [93/120    avg_loss:0.158, val_acc:0.981]
Epoch [94/120    avg_loss:0.164, val_acc:0.979]
Epoch [95/120    avg_loss:0.163, val_acc:0.983]
Epoch [96/120    avg_loss:0.120, val_acc:0.992]
Epoch [97/120    avg_loss:0.112, val_acc:0.988]
Epoch [98/120    avg_loss:0.113, val_acc:0.979]
Epoch [99/120    avg_loss:0.142, val_acc:0.977]
Epoch [100/120    avg_loss:0.157, val_acc:0.967]
Epoch [101/120    avg_loss:0.131, val_acc:0.977]
Epoch [102/120    avg_loss:0.133, val_acc:0.985]
Epoch [103/120    avg_loss:0.119, val_acc:0.979]
Epoch [104/120    avg_loss:0.142, val_acc:0.979]
Epoch [105/120    avg_loss:0.159, val_acc:0.988]
Epoch [106/120    avg_loss:0.127, val_acc:0.973]
Epoch [107/120    avg_loss:0.190, val_acc:0.952]
Epoch [108/120    avg_loss:0.253, val_acc:0.963]
Epoch [109/120    avg_loss:0.154, val_acc:0.977]
Epoch [110/120    avg_loss:0.138, val_acc:0.985]
Epoch [111/120    avg_loss:0.112, val_acc:0.994]
Epoch [112/120    avg_loss:0.110, val_acc:0.988]
Epoch [113/120    avg_loss:0.100, val_acc:0.992]
Epoch [114/120    avg_loss:0.082, val_acc:0.992]
Epoch [115/120    avg_loss:0.084, val_acc:0.990]
Epoch [116/120    avg_loss:0.095, val_acc:0.988]
Epoch [117/120    avg_loss:0.097, val_acc:0.992]
Epoch [118/120    avg_loss:0.085, val_acc:0.988]
Epoch [119/120    avg_loss:0.086, val_acc:0.988]
Epoch [120/120    avg_loss:0.082, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99560117 0.9977221  1.         0.94666667 0.91836735
 0.98564593 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.992642320789854
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f964b434e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.083]
Epoch [2/120    avg_loss:2.587, val_acc:0.152]
Epoch [3/120    avg_loss:2.551, val_acc:0.135]
Epoch [4/120    avg_loss:2.517, val_acc:0.300]
Epoch [5/120    avg_loss:2.482, val_acc:0.327]
Epoch [6/120    avg_loss:2.455, val_acc:0.335]
Epoch [7/120    avg_loss:2.432, val_acc:0.329]
Epoch [8/120    avg_loss:2.394, val_acc:0.348]
Epoch [9/120    avg_loss:2.368, val_acc:0.350]
Epoch [10/120    avg_loss:2.334, val_acc:0.379]
Epoch [11/120    avg_loss:2.303, val_acc:0.429]
Epoch [12/120    avg_loss:2.262, val_acc:0.460]
Epoch [13/120    avg_loss:2.227, val_acc:0.490]
Epoch [14/120    avg_loss:2.189, val_acc:0.494]
Epoch [15/120    avg_loss:2.142, val_acc:0.510]
Epoch [16/120    avg_loss:2.098, val_acc:0.508]
Epoch [17/120    avg_loss:2.045, val_acc:0.510]
Epoch [18/120    avg_loss:2.002, val_acc:0.517]
Epoch [19/120    avg_loss:1.962, val_acc:0.521]
Epoch [20/120    avg_loss:1.923, val_acc:0.521]
Epoch [21/120    avg_loss:1.867, val_acc:0.527]
Epoch [22/120    avg_loss:1.815, val_acc:0.548]
Epoch [23/120    avg_loss:1.786, val_acc:0.567]
Epoch [24/120    avg_loss:1.714, val_acc:0.558]
Epoch [25/120    avg_loss:1.679, val_acc:0.577]
Epoch [26/120    avg_loss:1.631, val_acc:0.594]
Epoch [27/120    avg_loss:1.607, val_acc:0.621]
Epoch [28/120    avg_loss:1.554, val_acc:0.644]
Epoch [29/120    avg_loss:1.522, val_acc:0.656]
Epoch [30/120    avg_loss:1.474, val_acc:0.673]
Epoch [31/120    avg_loss:1.459, val_acc:0.673]
Epoch [32/120    avg_loss:1.431, val_acc:0.688]
Epoch [33/120    avg_loss:1.376, val_acc:0.688]
Epoch [34/120    avg_loss:1.335, val_acc:0.692]
Epoch [35/120    avg_loss:1.295, val_acc:0.721]
Epoch [36/120    avg_loss:1.238, val_acc:0.742]
Epoch [37/120    avg_loss:1.201, val_acc:0.727]
Epoch [38/120    avg_loss:1.157, val_acc:0.748]
Epoch [39/120    avg_loss:1.122, val_acc:0.733]
Epoch [40/120    avg_loss:1.081, val_acc:0.773]
Epoch [41/120    avg_loss:1.072, val_acc:0.769]
Epoch [42/120    avg_loss:0.988, val_acc:0.771]
Epoch [43/120    avg_loss:0.960, val_acc:0.798]
Epoch [44/120    avg_loss:0.887, val_acc:0.792]
Epoch [45/120    avg_loss:0.884, val_acc:0.773]
Epoch [46/120    avg_loss:0.832, val_acc:0.804]
Epoch [47/120    avg_loss:0.794, val_acc:0.787]
Epoch [48/120    avg_loss:0.770, val_acc:0.815]
Epoch [49/120    avg_loss:0.687, val_acc:0.827]
Epoch [50/120    avg_loss:0.633, val_acc:0.838]
Epoch [51/120    avg_loss:0.598, val_acc:0.908]
Epoch [52/120    avg_loss:0.558, val_acc:0.833]
Epoch [53/120    avg_loss:0.526, val_acc:0.906]
Epoch [54/120    avg_loss:0.497, val_acc:0.923]
Epoch [55/120    avg_loss:0.490, val_acc:0.935]
Epoch [56/120    avg_loss:0.439, val_acc:0.919]
Epoch [57/120    avg_loss:0.444, val_acc:0.935]
Epoch [58/120    avg_loss:0.423, val_acc:0.942]
Epoch [59/120    avg_loss:0.479, val_acc:0.912]
Epoch [60/120    avg_loss:0.454, val_acc:0.946]
Epoch [61/120    avg_loss:0.447, val_acc:0.935]
Epoch [62/120    avg_loss:0.427, val_acc:0.925]
Epoch [63/120    avg_loss:0.381, val_acc:0.940]
Epoch [64/120    avg_loss:0.348, val_acc:0.944]
Epoch [65/120    avg_loss:0.313, val_acc:0.956]
Epoch [66/120    avg_loss:0.323, val_acc:0.942]
Epoch [67/120    avg_loss:0.349, val_acc:0.960]
Epoch [68/120    avg_loss:0.310, val_acc:0.963]
Epoch [69/120    avg_loss:0.264, val_acc:0.940]
Epoch [70/120    avg_loss:0.278, val_acc:0.944]
Epoch [71/120    avg_loss:0.247, val_acc:0.954]
Epoch [72/120    avg_loss:0.239, val_acc:0.946]
Epoch [73/120    avg_loss:0.214, val_acc:0.971]
Epoch [74/120    avg_loss:0.239, val_acc:0.942]
Epoch [75/120    avg_loss:0.239, val_acc:0.940]
Epoch [76/120    avg_loss:0.212, val_acc:0.965]
Epoch [77/120    avg_loss:0.211, val_acc:0.965]
Epoch [78/120    avg_loss:0.266, val_acc:0.942]
Epoch [79/120    avg_loss:0.231, val_acc:0.963]
Epoch [80/120    avg_loss:0.227, val_acc:0.973]
Epoch [81/120    avg_loss:0.190, val_acc:0.967]
Epoch [82/120    avg_loss:0.180, val_acc:0.969]
Epoch [83/120    avg_loss:0.180, val_acc:0.969]
Epoch [84/120    avg_loss:0.211, val_acc:0.971]
Epoch [85/120    avg_loss:0.219, val_acc:0.952]
Epoch [86/120    avg_loss:0.188, val_acc:0.967]
Epoch [87/120    avg_loss:0.154, val_acc:0.958]
Epoch [88/120    avg_loss:0.184, val_acc:0.958]
Epoch [89/120    avg_loss:0.175, val_acc:0.973]
Epoch [90/120    avg_loss:0.144, val_acc:0.979]
Epoch [91/120    avg_loss:0.152, val_acc:0.956]
Epoch [92/120    avg_loss:0.155, val_acc:0.975]
Epoch [93/120    avg_loss:0.127, val_acc:0.969]
Epoch [94/120    avg_loss:0.135, val_acc:0.971]
Epoch [95/120    avg_loss:0.139, val_acc:0.965]
Epoch [96/120    avg_loss:0.191, val_acc:0.963]
Epoch [97/120    avg_loss:0.138, val_acc:0.967]
Epoch [98/120    avg_loss:0.141, val_acc:0.969]
Epoch [99/120    avg_loss:0.131, val_acc:0.975]
Epoch [100/120    avg_loss:0.115, val_acc:0.981]
Epoch [101/120    avg_loss:0.103, val_acc:0.977]
Epoch [102/120    avg_loss:0.124, val_acc:0.981]
Epoch [103/120    avg_loss:0.115, val_acc:0.965]
Epoch [104/120    avg_loss:0.109, val_acc:0.973]
Epoch [105/120    avg_loss:0.109, val_acc:0.975]
Epoch [106/120    avg_loss:0.103, val_acc:0.988]
Epoch [107/120    avg_loss:0.097, val_acc:0.983]
Epoch [108/120    avg_loss:0.082, val_acc:0.981]
Epoch [109/120    avg_loss:0.084, val_acc:0.973]
Epoch [110/120    avg_loss:0.100, val_acc:0.983]
Epoch [111/120    avg_loss:0.098, val_acc:0.979]
Epoch [112/120    avg_loss:0.105, val_acc:0.942]
Epoch [113/120    avg_loss:0.187, val_acc:0.969]
Epoch [114/120    avg_loss:0.135, val_acc:0.979]
Epoch [115/120    avg_loss:0.111, val_acc:0.981]
Epoch [116/120    avg_loss:0.098, val_acc:0.969]
Epoch [117/120    avg_loss:0.077, val_acc:0.981]
Epoch [118/120    avg_loss:0.094, val_acc:0.983]
Epoch [119/120    avg_loss:0.090, val_acc:0.977]
Epoch [120/120    avg_loss:0.075, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0  12   0   0   0   0   0   0 376   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.98396501 1.         0.99782135 0.95945946 0.94
 0.97630332 0.99470899 0.98429319 1.         1.         1.
 1.         1.        ]

Kappa:
0.9902684351133526
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe342ba3f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 30442==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.662, val_acc:0.096]
Epoch [2/120    avg_loss:2.629, val_acc:0.110]
Epoch [3/120    avg_loss:2.601, val_acc:0.273]
Epoch [4/120    avg_loss:2.572, val_acc:0.333]
Epoch [5/120    avg_loss:2.542, val_acc:0.329]
Epoch [6/120    avg_loss:2.516, val_acc:0.317]
Epoch [7/120    avg_loss:2.487, val_acc:0.308]
Epoch [8/120    avg_loss:2.462, val_acc:0.302]
Epoch [9/120    avg_loss:2.437, val_acc:0.300]
Epoch [10/120    avg_loss:2.414, val_acc:0.317]
Epoch [11/120    avg_loss:2.384, val_acc:0.315]
Epoch [12/120    avg_loss:2.359, val_acc:0.302]
Epoch [13/120    avg_loss:2.335, val_acc:0.306]
Epoch [14/120    avg_loss:2.297, val_acc:0.323]
Epoch [15/120    avg_loss:2.269, val_acc:0.327]
Epoch [16/120    avg_loss:2.230, val_acc:0.331]
Epoch [17/120    avg_loss:2.191, val_acc:0.346]
Epoch [18/120    avg_loss:2.152, val_acc:0.356]
Epoch [19/120    avg_loss:2.111, val_acc:0.367]
Epoch [20/120    avg_loss:2.078, val_acc:0.360]
Epoch [21/120    avg_loss:2.056, val_acc:0.398]
Epoch [22/120    avg_loss:2.005, val_acc:0.450]
Epoch [23/120    avg_loss:1.956, val_acc:0.492]
Epoch [24/120    avg_loss:1.923, val_acc:0.537]
Epoch [25/120    avg_loss:1.892, val_acc:0.537]
Epoch [26/120    avg_loss:1.847, val_acc:0.569]
Epoch [27/120    avg_loss:1.796, val_acc:0.590]
Epoch [28/120    avg_loss:1.747, val_acc:0.606]
Epoch [29/120    avg_loss:1.712, val_acc:0.629]
Epoch [30/120    avg_loss:1.636, val_acc:0.644]
Epoch [31/120    avg_loss:1.628, val_acc:0.650]
Epoch [32/120    avg_loss:1.559, val_acc:0.665]
Epoch [33/120    avg_loss:1.483, val_acc:0.706]
Epoch [34/120    avg_loss:1.426, val_acc:0.685]
Epoch [35/120    avg_loss:1.385, val_acc:0.727]
Epoch [36/120    avg_loss:1.318, val_acc:0.731]
Epoch [37/120    avg_loss:1.283, val_acc:0.715]
Epoch [38/120    avg_loss:1.226, val_acc:0.738]
Epoch [39/120    avg_loss:1.162, val_acc:0.758]
Epoch [40/120    avg_loss:1.139, val_acc:0.762]
Epoch [41/120    avg_loss:1.053, val_acc:0.781]
Epoch [42/120    avg_loss:1.005, val_acc:0.777]
Epoch [43/120    avg_loss:0.961, val_acc:0.781]
Epoch [44/120    avg_loss:0.895, val_acc:0.806]
Epoch [45/120    avg_loss:0.909, val_acc:0.781]
Epoch [46/120    avg_loss:0.842, val_acc:0.779]
Epoch [47/120    avg_loss:0.800, val_acc:0.806]
Epoch [48/120    avg_loss:0.746, val_acc:0.808]
Epoch [49/120    avg_loss:0.742, val_acc:0.829]
Epoch [50/120    avg_loss:0.718, val_acc:0.812]
Epoch [51/120    avg_loss:0.673, val_acc:0.821]
Epoch [52/120    avg_loss:0.657, val_acc:0.835]
Epoch [53/120    avg_loss:0.687, val_acc:0.825]
Epoch [54/120    avg_loss:0.615, val_acc:0.821]
Epoch [55/120    avg_loss:0.596, val_acc:0.817]
Epoch [56/120    avg_loss:0.560, val_acc:0.840]
Epoch [57/120    avg_loss:0.526, val_acc:0.881]
Epoch [58/120    avg_loss:0.558, val_acc:0.865]
Epoch [59/120    avg_loss:0.553, val_acc:0.858]
Epoch [60/120    avg_loss:0.569, val_acc:0.900]
Epoch [61/120    avg_loss:0.481, val_acc:0.883]
Epoch [62/120    avg_loss:0.497, val_acc:0.921]
Epoch [63/120    avg_loss:0.440, val_acc:0.925]
Epoch [64/120    avg_loss:0.430, val_acc:0.906]
Epoch [65/120    avg_loss:0.434, val_acc:0.838]
Epoch [66/120    avg_loss:0.409, val_acc:0.931]
Epoch [67/120    avg_loss:0.423, val_acc:0.929]
Epoch [68/120    avg_loss:0.373, val_acc:0.948]
Epoch [69/120    avg_loss:0.339, val_acc:0.958]
Epoch [70/120    avg_loss:0.322, val_acc:0.956]
Epoch [71/120    avg_loss:0.313, val_acc:0.956]
Epoch [72/120    avg_loss:0.328, val_acc:0.952]
Epoch [73/120    avg_loss:0.293, val_acc:0.923]
Epoch [74/120    avg_loss:0.333, val_acc:0.965]
Epoch [75/120    avg_loss:0.270, val_acc:0.917]
Epoch [76/120    avg_loss:0.313, val_acc:0.885]
Epoch [77/120    avg_loss:0.285, val_acc:0.950]
Epoch [78/120    avg_loss:0.279, val_acc:0.958]
Epoch [79/120    avg_loss:0.259, val_acc:0.960]
Epoch [80/120    avg_loss:0.307, val_acc:0.965]
Epoch [81/120    avg_loss:0.347, val_acc:0.963]
Epoch [82/120    avg_loss:0.277, val_acc:0.969]
Epoch [83/120    avg_loss:0.254, val_acc:0.979]
Epoch [84/120    avg_loss:0.229, val_acc:0.969]
Epoch [85/120    avg_loss:0.192, val_acc:0.985]
Epoch [86/120    avg_loss:0.205, val_acc:0.990]
Epoch [87/120    avg_loss:0.175, val_acc:0.979]
Epoch [88/120    avg_loss:0.188, val_acc:0.981]
Epoch [89/120    avg_loss:0.188, val_acc:0.969]
Epoch [90/120    avg_loss:0.172, val_acc:0.965]
Epoch [91/120    avg_loss:0.206, val_acc:0.906]
Epoch [92/120    avg_loss:0.216, val_acc:0.977]
Epoch [93/120    avg_loss:0.183, val_acc:0.950]
Epoch [94/120    avg_loss:0.173, val_acc:0.967]
Epoch [95/120    avg_loss:0.208, val_acc:0.977]
Epoch [96/120    avg_loss:0.202, val_acc:0.977]
Epoch [97/120    avg_loss:0.203, val_acc:0.969]
Epoch [98/120    avg_loss:0.226, val_acc:0.971]
Epoch [99/120    avg_loss:0.202, val_acc:0.973]
Epoch [100/120    avg_loss:0.204, val_acc:0.992]
Epoch [101/120    avg_loss:0.157, val_acc:0.990]
Epoch [102/120    avg_loss:0.131, val_acc:0.994]
Epoch [103/120    avg_loss:0.150, val_acc:0.994]
Epoch [104/120    avg_loss:0.134, val_acc:0.996]
Epoch [105/120    avg_loss:0.140, val_acc:0.990]
Epoch [106/120    avg_loss:0.141, val_acc:0.994]
Epoch [107/120    avg_loss:0.145, val_acc:0.994]
Epoch [108/120    avg_loss:0.131, val_acc:0.996]
Epoch [109/120    avg_loss:0.131, val_acc:0.994]
Epoch [110/120    avg_loss:0.119, val_acc:0.994]
Epoch [111/120    avg_loss:0.117, val_acc:0.990]
Epoch [112/120    avg_loss:0.122, val_acc:0.996]
Epoch [113/120    avg_loss:0.119, val_acc:0.996]
Epoch [114/120    avg_loss:0.129, val_acc:0.994]
Epoch [115/120    avg_loss:0.114, val_acc:0.992]
Epoch [116/120    avg_loss:0.106, val_acc:0.994]
Epoch [117/120    avg_loss:0.116, val_acc:0.994]
Epoch [118/120    avg_loss:0.113, val_acc:0.994]
Epoch [119/120    avg_loss:0.116, val_acc:0.994]
Epoch [120/120    avg_loss:0.117, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99264706 0.99545455 0.99782135 0.93577982 0.91262136
 0.97630332 0.98924731 1.         1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9897958662202492
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16c573fa20>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.492, val_acc:0.297]
Epoch [2/120    avg_loss:2.214, val_acc:0.578]
Epoch [3/120    avg_loss:1.995, val_acc:0.617]
Epoch [4/120    avg_loss:1.748, val_acc:0.648]
Epoch [5/120    avg_loss:1.503, val_acc:0.707]
Epoch [6/120    avg_loss:1.316, val_acc:0.771]
Epoch [7/120    avg_loss:1.198, val_acc:0.699]
Epoch [8/120    avg_loss:0.984, val_acc:0.803]
Epoch [9/120    avg_loss:0.928, val_acc:0.826]
Epoch [10/120    avg_loss:0.822, val_acc:0.852]
Epoch [11/120    avg_loss:0.757, val_acc:0.855]
Epoch [12/120    avg_loss:0.728, val_acc:0.865]
Epoch [13/120    avg_loss:0.638, val_acc:0.863]
Epoch [14/120    avg_loss:0.629, val_acc:0.881]
Epoch [15/120    avg_loss:0.622, val_acc:0.822]
Epoch [16/120    avg_loss:0.563, val_acc:0.873]
Epoch [17/120    avg_loss:0.583, val_acc:0.830]
Epoch [18/120    avg_loss:0.546, val_acc:0.840]
Epoch [19/120    avg_loss:0.506, val_acc:0.877]
Epoch [20/120    avg_loss:0.471, val_acc:0.854]
Epoch [21/120    avg_loss:0.447, val_acc:0.842]
Epoch [22/120    avg_loss:0.469, val_acc:0.877]
Epoch [23/120    avg_loss:0.472, val_acc:0.885]
Epoch [24/120    avg_loss:0.436, val_acc:0.891]
Epoch [25/120    avg_loss:0.393, val_acc:0.906]
Epoch [26/120    avg_loss:0.438, val_acc:0.885]
Epoch [27/120    avg_loss:0.357, val_acc:0.893]
Epoch [28/120    avg_loss:0.305, val_acc:0.920]
Epoch [29/120    avg_loss:0.310, val_acc:0.906]
Epoch [30/120    avg_loss:0.312, val_acc:0.916]
Epoch [31/120    avg_loss:0.305, val_acc:0.902]
Epoch [32/120    avg_loss:0.322, val_acc:0.918]
Epoch [33/120    avg_loss:0.274, val_acc:0.928]
Epoch [34/120    avg_loss:0.323, val_acc:0.875]
Epoch [35/120    avg_loss:0.330, val_acc:0.926]
Epoch [36/120    avg_loss:0.302, val_acc:0.945]
Epoch [37/120    avg_loss:0.293, val_acc:0.922]
Epoch [38/120    avg_loss:0.298, val_acc:0.928]
Epoch [39/120    avg_loss:0.307, val_acc:0.934]
Epoch [40/120    avg_loss:0.255, val_acc:0.918]
Epoch [41/120    avg_loss:0.257, val_acc:0.910]
Epoch [42/120    avg_loss:0.267, val_acc:0.938]
Epoch [43/120    avg_loss:0.230, val_acc:0.916]
Epoch [44/120    avg_loss:0.214, val_acc:0.928]
Epoch [45/120    avg_loss:0.237, val_acc:0.922]
Epoch [46/120    avg_loss:0.212, val_acc:0.951]
Epoch [47/120    avg_loss:0.237, val_acc:0.938]
Epoch [48/120    avg_loss:0.207, val_acc:0.945]
Epoch [49/120    avg_loss:0.175, val_acc:0.939]
Epoch [50/120    avg_loss:0.191, val_acc:0.934]
Epoch [51/120    avg_loss:0.148, val_acc:0.951]
Epoch [52/120    avg_loss:0.212, val_acc:0.953]
Epoch [53/120    avg_loss:0.173, val_acc:0.941]
Epoch [54/120    avg_loss:0.250, val_acc:0.936]
Epoch [55/120    avg_loss:0.186, val_acc:0.957]
Epoch [56/120    avg_loss:0.207, val_acc:0.918]
Epoch [57/120    avg_loss:0.193, val_acc:0.961]
Epoch [58/120    avg_loss:0.188, val_acc:0.941]
Epoch [59/120    avg_loss:0.193, val_acc:0.959]
Epoch [60/120    avg_loss:0.140, val_acc:0.967]
Epoch [61/120    avg_loss:0.139, val_acc:0.947]
Epoch [62/120    avg_loss:0.110, val_acc:0.965]
Epoch [63/120    avg_loss:0.179, val_acc:0.967]
Epoch [64/120    avg_loss:0.225, val_acc:0.975]
Epoch [65/120    avg_loss:0.130, val_acc:0.957]
Epoch [66/120    avg_loss:0.107, val_acc:0.973]
Epoch [67/120    avg_loss:0.144, val_acc:0.957]
Epoch [68/120    avg_loss:0.148, val_acc:0.961]
Epoch [69/120    avg_loss:0.116, val_acc:0.959]
Epoch [70/120    avg_loss:0.106, val_acc:0.967]
Epoch [71/120    avg_loss:0.091, val_acc:0.980]
Epoch [72/120    avg_loss:0.083, val_acc:0.971]
Epoch [73/120    avg_loss:0.143, val_acc:0.963]
Epoch [74/120    avg_loss:0.133, val_acc:0.941]
Epoch [75/120    avg_loss:0.163, val_acc:0.951]
Epoch [76/120    avg_loss:0.151, val_acc:0.947]
Epoch [77/120    avg_loss:0.122, val_acc:0.965]
Epoch [78/120    avg_loss:0.139, val_acc:0.922]
Epoch [79/120    avg_loss:0.167, val_acc:0.951]
Epoch [80/120    avg_loss:0.133, val_acc:0.979]
Epoch [81/120    avg_loss:0.094, val_acc:0.943]
Epoch [82/120    avg_loss:0.129, val_acc:0.945]
Epoch [83/120    avg_loss:0.091, val_acc:0.955]
Epoch [84/120    avg_loss:0.095, val_acc:0.979]
Epoch [85/120    avg_loss:0.053, val_acc:0.980]
Epoch [86/120    avg_loss:0.071, val_acc:0.979]
Epoch [87/120    avg_loss:0.048, val_acc:0.982]
Epoch [88/120    avg_loss:0.050, val_acc:0.984]
Epoch [89/120    avg_loss:0.042, val_acc:0.982]
Epoch [90/120    avg_loss:0.047, val_acc:0.982]
Epoch [91/120    avg_loss:0.047, val_acc:0.982]
Epoch [92/120    avg_loss:0.043, val_acc:0.982]
Epoch [93/120    avg_loss:0.041, val_acc:0.982]
Epoch [94/120    avg_loss:0.046, val_acc:0.984]
Epoch [95/120    avg_loss:0.044, val_acc:0.982]
Epoch [96/120    avg_loss:0.044, val_acc:0.982]
Epoch [97/120    avg_loss:0.048, val_acc:0.980]
Epoch [98/120    avg_loss:0.046, val_acc:0.980]
Epoch [99/120    avg_loss:0.046, val_acc:0.980]
Epoch [100/120    avg_loss:0.037, val_acc:0.982]
Epoch [101/120    avg_loss:0.030, val_acc:0.979]
Epoch [102/120    avg_loss:0.037, val_acc:0.984]
Epoch [103/120    avg_loss:0.042, val_acc:0.984]
Epoch [104/120    avg_loss:0.036, val_acc:0.982]
Epoch [105/120    avg_loss:0.040, val_acc:0.980]
Epoch [106/120    avg_loss:0.039, val_acc:0.984]
Epoch [107/120    avg_loss:0.035, val_acc:0.982]
Epoch [108/120    avg_loss:0.038, val_acc:0.984]
Epoch [109/120    avg_loss:0.034, val_acc:0.982]
Epoch [110/120    avg_loss:0.039, val_acc:0.982]
Epoch [111/120    avg_loss:0.029, val_acc:0.986]
Epoch [112/120    avg_loss:0.039, val_acc:0.979]
Epoch [113/120    avg_loss:0.041, val_acc:0.982]
Epoch [114/120    avg_loss:0.039, val_acc:0.980]
Epoch [115/120    avg_loss:0.029, val_acc:0.982]
Epoch [116/120    avg_loss:0.031, val_acc:0.982]
Epoch [117/120    avg_loss:0.044, val_acc:0.980]
Epoch [118/120    avg_loss:0.030, val_acc:0.980]
Epoch [119/120    avg_loss:0.034, val_acc:0.992]
Epoch [120/120    avg_loss:0.043, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 219   7   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 208  18   0   0   1   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.50746268656717

F1 scores:
[       nan 0.99708879 0.93512304 0.97550111 0.92650334 0.91694352
 0.99019608 0.83798883 0.99487179 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9833807822209704
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97a57139b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.544, val_acc:0.414]
Epoch [2/120    avg_loss:2.251, val_acc:0.582]
Epoch [3/120    avg_loss:2.032, val_acc:0.609]
Epoch [4/120    avg_loss:1.792, val_acc:0.631]
Epoch [5/120    avg_loss:1.508, val_acc:0.664]
Epoch [6/120    avg_loss:1.261, val_acc:0.715]
Epoch [7/120    avg_loss:1.105, val_acc:0.775]
Epoch [8/120    avg_loss:0.949, val_acc:0.797]
Epoch [9/120    avg_loss:0.876, val_acc:0.820]
Epoch [10/120    avg_loss:0.758, val_acc:0.799]
Epoch [11/120    avg_loss:0.637, val_acc:0.814]
Epoch [12/120    avg_loss:0.604, val_acc:0.855]
Epoch [13/120    avg_loss:0.569, val_acc:0.873]
Epoch [14/120    avg_loss:0.526, val_acc:0.789]
Epoch [15/120    avg_loss:0.530, val_acc:0.805]
Epoch [16/120    avg_loss:0.464, val_acc:0.879]
Epoch [17/120    avg_loss:0.529, val_acc:0.840]
Epoch [18/120    avg_loss:0.472, val_acc:0.889]
Epoch [19/120    avg_loss:0.420, val_acc:0.908]
Epoch [20/120    avg_loss:0.415, val_acc:0.887]
Epoch [21/120    avg_loss:0.430, val_acc:0.865]
Epoch [22/120    avg_loss:0.396, val_acc:0.875]
Epoch [23/120    avg_loss:0.381, val_acc:0.893]
Epoch [24/120    avg_loss:0.375, val_acc:0.914]
Epoch [25/120    avg_loss:0.348, val_acc:0.891]
Epoch [26/120    avg_loss:0.324, val_acc:0.893]
Epoch [27/120    avg_loss:0.304, val_acc:0.883]
Epoch [28/120    avg_loss:0.338, val_acc:0.910]
Epoch [29/120    avg_loss:0.304, val_acc:0.912]
Epoch [30/120    avg_loss:0.275, val_acc:0.912]
Epoch [31/120    avg_loss:0.279, val_acc:0.928]
Epoch [32/120    avg_loss:0.265, val_acc:0.924]
Epoch [33/120    avg_loss:0.348, val_acc:0.914]
Epoch [34/120    avg_loss:0.326, val_acc:0.898]
Epoch [35/120    avg_loss:0.291, val_acc:0.898]
Epoch [36/120    avg_loss:0.259, val_acc:0.912]
Epoch [37/120    avg_loss:0.286, val_acc:0.914]
Epoch [38/120    avg_loss:0.244, val_acc:0.928]
Epoch [39/120    avg_loss:0.359, val_acc:0.914]
Epoch [40/120    avg_loss:0.218, val_acc:0.936]
Epoch [41/120    avg_loss:0.230, val_acc:0.910]
Epoch [42/120    avg_loss:0.239, val_acc:0.941]
Epoch [43/120    avg_loss:0.237, val_acc:0.930]
Epoch [44/120    avg_loss:0.205, val_acc:0.930]
Epoch [45/120    avg_loss:0.170, val_acc:0.941]
Epoch [46/120    avg_loss:0.214, val_acc:0.934]
Epoch [47/120    avg_loss:0.192, val_acc:0.943]
Epoch [48/120    avg_loss:0.172, val_acc:0.936]
Epoch [49/120    avg_loss:0.204, val_acc:0.939]
Epoch [50/120    avg_loss:0.169, val_acc:0.949]
Epoch [51/120    avg_loss:0.153, val_acc:0.959]
Epoch [52/120    avg_loss:0.140, val_acc:0.949]
Epoch [53/120    avg_loss:0.146, val_acc:0.947]
Epoch [54/120    avg_loss:0.170, val_acc:0.941]
Epoch [55/120    avg_loss:0.153, val_acc:0.949]
Epoch [56/120    avg_loss:0.182, val_acc:0.957]
Epoch [57/120    avg_loss:0.135, val_acc:0.953]
Epoch [58/120    avg_loss:0.140, val_acc:0.957]
Epoch [59/120    avg_loss:0.115, val_acc:0.920]
Epoch [60/120    avg_loss:0.175, val_acc:0.953]
Epoch [61/120    avg_loss:0.205, val_acc:0.951]
Epoch [62/120    avg_loss:0.130, val_acc:0.938]
Epoch [63/120    avg_loss:0.108, val_acc:0.947]
Epoch [64/120    avg_loss:0.141, val_acc:0.904]
Epoch [65/120    avg_loss:0.180, val_acc:0.930]
Epoch [66/120    avg_loss:0.122, val_acc:0.949]
Epoch [67/120    avg_loss:0.110, val_acc:0.957]
Epoch [68/120    avg_loss:0.093, val_acc:0.965]
Epoch [69/120    avg_loss:0.092, val_acc:0.967]
Epoch [70/120    avg_loss:0.091, val_acc:0.965]
Epoch [71/120    avg_loss:0.076, val_acc:0.967]
Epoch [72/120    avg_loss:0.077, val_acc:0.973]
Epoch [73/120    avg_loss:0.064, val_acc:0.973]
Epoch [74/120    avg_loss:0.073, val_acc:0.971]
Epoch [75/120    avg_loss:0.068, val_acc:0.973]
Epoch [76/120    avg_loss:0.061, val_acc:0.971]
Epoch [77/120    avg_loss:0.051, val_acc:0.969]
Epoch [78/120    avg_loss:0.054, val_acc:0.969]
Epoch [79/120    avg_loss:0.058, val_acc:0.971]
Epoch [80/120    avg_loss:0.059, val_acc:0.965]
Epoch [81/120    avg_loss:0.056, val_acc:0.967]
Epoch [82/120    avg_loss:0.062, val_acc:0.971]
Epoch [83/120    avg_loss:0.057, val_acc:0.969]
Epoch [84/120    avg_loss:0.061, val_acc:0.969]
Epoch [85/120    avg_loss:0.062, val_acc:0.969]
Epoch [86/120    avg_loss:0.054, val_acc:0.971]
Epoch [87/120    avg_loss:0.059, val_acc:0.971]
Epoch [88/120    avg_loss:0.060, val_acc:0.973]
Epoch [89/120    avg_loss:0.059, val_acc:0.973]
Epoch [90/120    avg_loss:0.065, val_acc:0.975]
Epoch [91/120    avg_loss:0.054, val_acc:0.975]
Epoch [92/120    avg_loss:0.051, val_acc:0.975]
Epoch [93/120    avg_loss:0.057, val_acc:0.973]
Epoch [94/120    avg_loss:0.052, val_acc:0.971]
Epoch [95/120    avg_loss:0.053, val_acc:0.973]
Epoch [96/120    avg_loss:0.049, val_acc:0.971]
Epoch [97/120    avg_loss:0.062, val_acc:0.973]
Epoch [98/120    avg_loss:0.047, val_acc:0.973]
Epoch [99/120    avg_loss:0.046, val_acc:0.973]
Epoch [100/120    avg_loss:0.048, val_acc:0.971]
Epoch [101/120    avg_loss:0.051, val_acc:0.969]
Epoch [102/120    avg_loss:0.054, val_acc:0.971]
Epoch [103/120    avg_loss:0.055, val_acc:0.971]
Epoch [104/120    avg_loss:0.042, val_acc:0.973]
Epoch [105/120    avg_loss:0.056, val_acc:0.971]
Epoch [106/120    avg_loss:0.040, val_acc:0.971]
Epoch [107/120    avg_loss:0.049, val_acc:0.971]
Epoch [108/120    avg_loss:0.044, val_acc:0.973]
Epoch [109/120    avg_loss:0.051, val_acc:0.973]
Epoch [110/120    avg_loss:0.050, val_acc:0.973]
Epoch [111/120    avg_loss:0.049, val_acc:0.973]
Epoch [112/120    avg_loss:0.044, val_acc:0.973]
Epoch [113/120    avg_loss:0.041, val_acc:0.973]
Epoch [114/120    avg_loss:0.048, val_acc:0.973]
Epoch [115/120    avg_loss:0.050, val_acc:0.973]
Epoch [116/120    avg_loss:0.045, val_acc:0.973]
Epoch [117/120    avg_loss:0.041, val_acc:0.973]
Epoch [118/120    avg_loss:0.045, val_acc:0.973]
Epoch [119/120    avg_loss:0.044, val_acc:0.973]
Epoch [120/120    avg_loss:0.038, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   3   1   0   0   0   0   0]
 [  0   0   1 217   4   0   0   0   4   4   0   0   0   0]
 [  0   0   1   4 209  13   0   0   0   0   0   0   0   0]
 [  0   0   0   1  25 119   0   0   0   0   0   0   0   0]
 [  0   6   0   0   0   0 200   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   6   0   0   0   0   0   0   0   0   2 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.12366737739872

F1 scores:
[       nan 0.99563953 0.94505495 0.96017699 0.89892473 0.85920578
 0.98522167 0.91011236 0.99359795 0.99574468 1.         0.9973545
 0.99109131 1.        ]

Kappa:
0.9791047516084435
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feb6e7bba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.450, val_acc:0.404]
Epoch [2/120    avg_loss:2.170, val_acc:0.531]
Epoch [3/120    avg_loss:1.955, val_acc:0.648]
Epoch [4/120    avg_loss:1.746, val_acc:0.740]
Epoch [5/120    avg_loss:1.494, val_acc:0.748]
Epoch [6/120    avg_loss:1.295, val_acc:0.799]
Epoch [7/120    avg_loss:1.099, val_acc:0.801]
Epoch [8/120    avg_loss:0.943, val_acc:0.869]
Epoch [9/120    avg_loss:0.881, val_acc:0.693]
Epoch [10/120    avg_loss:0.779, val_acc:0.836]
Epoch [11/120    avg_loss:0.711, val_acc:0.883]
Epoch [12/120    avg_loss:0.687, val_acc:0.863]
Epoch [13/120    avg_loss:0.589, val_acc:0.820]
Epoch [14/120    avg_loss:0.566, val_acc:0.896]
Epoch [15/120    avg_loss:0.548, val_acc:0.881]
Epoch [16/120    avg_loss:0.505, val_acc:0.883]
Epoch [17/120    avg_loss:0.516, val_acc:0.883]
Epoch [18/120    avg_loss:0.586, val_acc:0.887]
Epoch [19/120    avg_loss:0.459, val_acc:0.893]
Epoch [20/120    avg_loss:0.462, val_acc:0.887]
Epoch [21/120    avg_loss:0.397, val_acc:0.904]
Epoch [22/120    avg_loss:0.420, val_acc:0.848]
Epoch [23/120    avg_loss:0.408, val_acc:0.902]
Epoch [24/120    avg_loss:0.356, val_acc:0.910]
Epoch [25/120    avg_loss:0.343, val_acc:0.904]
Epoch [26/120    avg_loss:0.349, val_acc:0.924]
Epoch [27/120    avg_loss:0.300, val_acc:0.928]
Epoch [28/120    avg_loss:0.326, val_acc:0.934]
Epoch [29/120    avg_loss:0.371, val_acc:0.906]
Epoch [30/120    avg_loss:0.330, val_acc:0.914]
Epoch [31/120    avg_loss:0.324, val_acc:0.910]
Epoch [32/120    avg_loss:0.339, val_acc:0.951]
Epoch [33/120    avg_loss:0.309, val_acc:0.932]
Epoch [34/120    avg_loss:0.263, val_acc:0.918]
Epoch [35/120    avg_loss:0.302, val_acc:0.924]
Epoch [36/120    avg_loss:0.321, val_acc:0.918]
Epoch [37/120    avg_loss:0.267, val_acc:0.912]
Epoch [38/120    avg_loss:0.261, val_acc:0.953]
Epoch [39/120    avg_loss:0.243, val_acc:0.953]
Epoch [40/120    avg_loss:0.288, val_acc:0.943]
Epoch [41/120    avg_loss:0.262, val_acc:0.924]
Epoch [42/120    avg_loss:0.215, val_acc:0.945]
Epoch [43/120    avg_loss:0.230, val_acc:0.934]
Epoch [44/120    avg_loss:0.168, val_acc:0.941]
Epoch [45/120    avg_loss:0.183, val_acc:0.949]
Epoch [46/120    avg_loss:0.173, val_acc:0.947]
Epoch [47/120    avg_loss:0.184, val_acc:0.934]
Epoch [48/120    avg_loss:0.180, val_acc:0.908]
Epoch [49/120    avg_loss:0.168, val_acc:0.934]
Epoch [50/120    avg_loss:0.178, val_acc:0.947]
Epoch [51/120    avg_loss:0.172, val_acc:0.936]
Epoch [52/120    avg_loss:0.152, val_acc:0.955]
Epoch [53/120    avg_loss:0.154, val_acc:0.943]
Epoch [54/120    avg_loss:0.134, val_acc:0.949]
Epoch [55/120    avg_loss:0.153, val_acc:0.961]
Epoch [56/120    avg_loss:0.151, val_acc:0.965]
Epoch [57/120    avg_loss:0.161, val_acc:0.959]
Epoch [58/120    avg_loss:0.162, val_acc:0.957]
Epoch [59/120    avg_loss:0.108, val_acc:0.965]
Epoch [60/120    avg_loss:0.162, val_acc:0.961]
Epoch [61/120    avg_loss:0.105, val_acc:0.953]
Epoch [62/120    avg_loss:0.118, val_acc:0.965]
Epoch [63/120    avg_loss:0.168, val_acc:0.943]
Epoch [64/120    avg_loss:0.153, val_acc:0.953]
Epoch [65/120    avg_loss:0.193, val_acc:0.945]
Epoch [66/120    avg_loss:0.165, val_acc:0.953]
Epoch [67/120    avg_loss:0.137, val_acc:0.916]
Epoch [68/120    avg_loss:0.155, val_acc:0.965]
Epoch [69/120    avg_loss:0.076, val_acc:0.967]
Epoch [70/120    avg_loss:0.072, val_acc:0.953]
Epoch [71/120    avg_loss:0.085, val_acc:0.967]
Epoch [72/120    avg_loss:0.082, val_acc:0.961]
Epoch [73/120    avg_loss:0.075, val_acc:0.961]
Epoch [74/120    avg_loss:0.057, val_acc:0.971]
Epoch [75/120    avg_loss:0.085, val_acc:0.955]
Epoch [76/120    avg_loss:0.112, val_acc:0.959]
Epoch [77/120    avg_loss:0.127, val_acc:0.963]
Epoch [78/120    avg_loss:0.096, val_acc:0.953]
Epoch [79/120    avg_loss:0.088, val_acc:0.965]
Epoch [80/120    avg_loss:0.048, val_acc:0.977]
Epoch [81/120    avg_loss:0.040, val_acc:0.967]
Epoch [82/120    avg_loss:0.062, val_acc:0.955]
Epoch [83/120    avg_loss:0.058, val_acc:0.961]
Epoch [84/120    avg_loss:0.064, val_acc:0.957]
Epoch [85/120    avg_loss:0.042, val_acc:0.969]
Epoch [86/120    avg_loss:0.052, val_acc:0.961]
Epoch [87/120    avg_loss:0.060, val_acc:0.959]
Epoch [88/120    avg_loss:0.071, val_acc:0.965]
Epoch [89/120    avg_loss:0.046, val_acc:0.955]
Epoch [90/120    avg_loss:0.040, val_acc:0.953]
Epoch [91/120    avg_loss:0.084, val_acc:0.961]
Epoch [92/120    avg_loss:0.049, val_acc:0.955]
Epoch [93/120    avg_loss:0.048, val_acc:0.955]
Epoch [94/120    avg_loss:0.056, val_acc:0.967]
Epoch [95/120    avg_loss:0.022, val_acc:0.963]
Epoch [96/120    avg_loss:0.023, val_acc:0.963]
Epoch [97/120    avg_loss:0.021, val_acc:0.965]
Epoch [98/120    avg_loss:0.022, val_acc:0.965]
Epoch [99/120    avg_loss:0.020, val_acc:0.965]
Epoch [100/120    avg_loss:0.020, val_acc:0.967]
Epoch [101/120    avg_loss:0.022, val_acc:0.969]
Epoch [102/120    avg_loss:0.018, val_acc:0.967]
Epoch [103/120    avg_loss:0.016, val_acc:0.969]
Epoch [104/120    avg_loss:0.021, val_acc:0.969]
Epoch [105/120    avg_loss:0.019, val_acc:0.965]
Epoch [106/120    avg_loss:0.016, val_acc:0.969]
Epoch [107/120    avg_loss:0.017, val_acc:0.969]
Epoch [108/120    avg_loss:0.019, val_acc:0.969]
Epoch [109/120    avg_loss:0.017, val_acc:0.969]
Epoch [110/120    avg_loss:0.017, val_acc:0.969]
Epoch [111/120    avg_loss:0.019, val_acc:0.969]
Epoch [112/120    avg_loss:0.013, val_acc:0.969]
Epoch [113/120    avg_loss:0.016, val_acc:0.969]
Epoch [114/120    avg_loss:0.019, val_acc:0.969]
Epoch [115/120    avg_loss:0.020, val_acc:0.969]
Epoch [116/120    avg_loss:0.020, val_acc:0.969]
Epoch [117/120    avg_loss:0.014, val_acc:0.969]
Epoch [118/120    avg_loss:0.017, val_acc:0.969]
Epoch [119/120    avg_loss:0.013, val_acc:0.969]
Epoch [120/120    avg_loss:0.013, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 206   0   0   0   0  13   0   0   0   0   0   0]
 [  0   0   0 215   5   0   0   0  10   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   1   0   0   0   0   0]
 [  0   0   0   1  18 126   0   0   0   0   0   0   0   0]
 [  0   5   0   0   1   0 199   0   1   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   1   2   1   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   1   0   0   0   0   0   0  23 429   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.99573560767591

F1 scores:
[       nan 0.99563953 0.96487119 0.96196868 0.92077088 0.89361702
 0.98271605 0.93532338 0.97959184 1.         1.         0.97039897
 0.97278912 1.        ]

Kappa:
0.9776848048280918
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1296ac9a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.502, val_acc:0.301]
Epoch [2/120    avg_loss:2.241, val_acc:0.381]
Epoch [3/120    avg_loss:2.033, val_acc:0.523]
Epoch [4/120    avg_loss:1.830, val_acc:0.600]
Epoch [5/120    avg_loss:1.624, val_acc:0.705]
Epoch [6/120    avg_loss:1.407, val_acc:0.719]
Epoch [7/120    avg_loss:1.185, val_acc:0.750]
Epoch [8/120    avg_loss:1.063, val_acc:0.779]
Epoch [9/120    avg_loss:0.961, val_acc:0.783]
Epoch [10/120    avg_loss:0.868, val_acc:0.826]
Epoch [11/120    avg_loss:0.783, val_acc:0.822]
Epoch [12/120    avg_loss:0.727, val_acc:0.850]
Epoch [13/120    avg_loss:0.626, val_acc:0.842]
Epoch [14/120    avg_loss:0.637, val_acc:0.842]
Epoch [15/120    avg_loss:0.596, val_acc:0.877]
Epoch [16/120    avg_loss:0.561, val_acc:0.885]
Epoch [17/120    avg_loss:0.515, val_acc:0.822]
Epoch [18/120    avg_loss:0.479, val_acc:0.906]
Epoch [19/120    avg_loss:0.462, val_acc:0.838]
Epoch [20/120    avg_loss:0.468, val_acc:0.910]
Epoch [21/120    avg_loss:0.434, val_acc:0.889]
Epoch [22/120    avg_loss:0.412, val_acc:0.871]
Epoch [23/120    avg_loss:0.436, val_acc:0.898]
Epoch [24/120    avg_loss:0.427, val_acc:0.879]
Epoch [25/120    avg_loss:0.402, val_acc:0.914]
Epoch [26/120    avg_loss:0.331, val_acc:0.924]
Epoch [27/120    avg_loss:0.370, val_acc:0.902]
Epoch [28/120    avg_loss:0.383, val_acc:0.799]
Epoch [29/120    avg_loss:0.515, val_acc:0.906]
Epoch [30/120    avg_loss:0.361, val_acc:0.910]
Epoch [31/120    avg_loss:0.317, val_acc:0.916]
Epoch [32/120    avg_loss:0.280, val_acc:0.918]
Epoch [33/120    avg_loss:0.279, val_acc:0.922]
Epoch [34/120    avg_loss:0.267, val_acc:0.887]
Epoch [35/120    avg_loss:0.304, val_acc:0.914]
Epoch [36/120    avg_loss:0.264, val_acc:0.914]
Epoch [37/120    avg_loss:0.307, val_acc:0.920]
Epoch [38/120    avg_loss:0.273, val_acc:0.936]
Epoch [39/120    avg_loss:0.351, val_acc:0.895]
Epoch [40/120    avg_loss:0.289, val_acc:0.932]
Epoch [41/120    avg_loss:0.232, val_acc:0.891]
Epoch [42/120    avg_loss:0.264, val_acc:0.902]
Epoch [43/120    avg_loss:0.219, val_acc:0.920]
Epoch [44/120    avg_loss:0.224, val_acc:0.926]
Epoch [45/120    avg_loss:0.242, val_acc:0.939]
Epoch [46/120    avg_loss:0.169, val_acc:0.953]
Epoch [47/120    avg_loss:0.199, val_acc:0.949]
Epoch [48/120    avg_loss:0.192, val_acc:0.873]
Epoch [49/120    avg_loss:0.231, val_acc:0.949]
Epoch [50/120    avg_loss:0.184, val_acc:0.936]
Epoch [51/120    avg_loss:0.196, val_acc:0.951]
Epoch [52/120    avg_loss:0.154, val_acc:0.938]
Epoch [53/120    avg_loss:0.215, val_acc:0.938]
Epoch [54/120    avg_loss:0.172, val_acc:0.934]
Epoch [55/120    avg_loss:0.254, val_acc:0.893]
Epoch [56/120    avg_loss:0.145, val_acc:0.957]
Epoch [57/120    avg_loss:0.192, val_acc:0.939]
Epoch [58/120    avg_loss:0.126, val_acc:0.957]
Epoch [59/120    avg_loss:0.123, val_acc:0.963]
Epoch [60/120    avg_loss:0.142, val_acc:0.939]
Epoch [61/120    avg_loss:0.147, val_acc:0.910]
Epoch [62/120    avg_loss:0.154, val_acc:0.920]
Epoch [63/120    avg_loss:0.140, val_acc:0.967]
Epoch [64/120    avg_loss:0.139, val_acc:0.939]
Epoch [65/120    avg_loss:0.112, val_acc:0.945]
Epoch [66/120    avg_loss:0.122, val_acc:0.955]
Epoch [67/120    avg_loss:0.122, val_acc:0.965]
Epoch [68/120    avg_loss:0.095, val_acc:0.953]
Epoch [69/120    avg_loss:0.105, val_acc:0.953]
Epoch [70/120    avg_loss:0.118, val_acc:0.955]
Epoch [71/120    avg_loss:0.090, val_acc:0.949]
Epoch [72/120    avg_loss:0.109, val_acc:0.955]
Epoch [73/120    avg_loss:0.091, val_acc:0.961]
Epoch [74/120    avg_loss:0.091, val_acc:0.959]
Epoch [75/120    avg_loss:0.085, val_acc:0.955]
Epoch [76/120    avg_loss:0.116, val_acc:0.945]
Epoch [77/120    avg_loss:0.105, val_acc:0.959]
Epoch [78/120    avg_loss:0.067, val_acc:0.967]
Epoch [79/120    avg_loss:0.051, val_acc:0.971]
Epoch [80/120    avg_loss:0.050, val_acc:0.969]
Epoch [81/120    avg_loss:0.056, val_acc:0.969]
Epoch [82/120    avg_loss:0.054, val_acc:0.971]
Epoch [83/120    avg_loss:0.055, val_acc:0.973]
Epoch [84/120    avg_loss:0.045, val_acc:0.973]
Epoch [85/120    avg_loss:0.048, val_acc:0.969]
Epoch [86/120    avg_loss:0.051, val_acc:0.969]
Epoch [87/120    avg_loss:0.037, val_acc:0.973]
Epoch [88/120    avg_loss:0.044, val_acc:0.973]
Epoch [89/120    avg_loss:0.042, val_acc:0.971]
Epoch [90/120    avg_loss:0.045, val_acc:0.973]
Epoch [91/120    avg_loss:0.040, val_acc:0.973]
Epoch [92/120    avg_loss:0.039, val_acc:0.975]
Epoch [93/120    avg_loss:0.049, val_acc:0.975]
Epoch [94/120    avg_loss:0.042, val_acc:0.975]
Epoch [95/120    avg_loss:0.036, val_acc:0.975]
Epoch [96/120    avg_loss:0.044, val_acc:0.975]
Epoch [97/120    avg_loss:0.046, val_acc:0.975]
Epoch [98/120    avg_loss:0.042, val_acc:0.977]
Epoch [99/120    avg_loss:0.033, val_acc:0.973]
Epoch [100/120    avg_loss:0.035, val_acc:0.977]
Epoch [101/120    avg_loss:0.036, val_acc:0.977]
Epoch [102/120    avg_loss:0.042, val_acc:0.967]
Epoch [103/120    avg_loss:0.043, val_acc:0.975]
Epoch [104/120    avg_loss:0.036, val_acc:0.977]
Epoch [105/120    avg_loss:0.036, val_acc:0.973]
Epoch [106/120    avg_loss:0.034, val_acc:0.973]
Epoch [107/120    avg_loss:0.035, val_acc:0.973]
Epoch [108/120    avg_loss:0.040, val_acc:0.975]
Epoch [109/120    avg_loss:0.037, val_acc:0.975]
Epoch [110/120    avg_loss:0.032, val_acc:0.977]
Epoch [111/120    avg_loss:0.033, val_acc:0.977]
Epoch [112/120    avg_loss:0.031, val_acc:0.973]
Epoch [113/120    avg_loss:0.036, val_acc:0.975]
Epoch [114/120    avg_loss:0.037, val_acc:0.975]
Epoch [115/120    avg_loss:0.037, val_acc:0.975]
Epoch [116/120    avg_loss:0.035, val_acc:0.971]
Epoch [117/120    avg_loss:0.034, val_acc:0.973]
Epoch [118/120    avg_loss:0.048, val_acc:0.971]
Epoch [119/120    avg_loss:0.024, val_acc:0.973]
Epoch [120/120    avg_loss:0.039, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 194   0   0   0   0  23   1   0   0   0   0   0]
 [  0   0   0 213  12   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 210  15   1   0   1   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   4   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.97441364605544

F1 scores:
[       nan 0.997815   0.91079812 0.96162528 0.90322581 0.89273356
 0.99270073 0.8358209  0.98339719 0.99893276 1.         0.99867198
 0.9944629  1.        ]

Kappa:
0.9774483816967816
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f98cc467a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.491, val_acc:0.504]
Epoch [2/120    avg_loss:2.228, val_acc:0.588]
Epoch [3/120    avg_loss:1.991, val_acc:0.682]
Epoch [4/120    avg_loss:1.797, val_acc:0.680]
Epoch [5/120    avg_loss:1.551, val_acc:0.754]
Epoch [6/120    avg_loss:1.322, val_acc:0.779]
Epoch [7/120    avg_loss:1.160, val_acc:0.795]
Epoch [8/120    avg_loss:0.976, val_acc:0.814]
Epoch [9/120    avg_loss:0.859, val_acc:0.828]
Epoch [10/120    avg_loss:0.786, val_acc:0.846]
Epoch [11/120    avg_loss:0.719, val_acc:0.836]
Epoch [12/120    avg_loss:0.642, val_acc:0.859]
Epoch [13/120    avg_loss:0.625, val_acc:0.854]
Epoch [14/120    avg_loss:0.575, val_acc:0.865]
Epoch [15/120    avg_loss:0.562, val_acc:0.854]
Epoch [16/120    avg_loss:0.488, val_acc:0.912]
Epoch [17/120    avg_loss:0.441, val_acc:0.900]
Epoch [18/120    avg_loss:0.443, val_acc:0.908]
Epoch [19/120    avg_loss:0.446, val_acc:0.865]
Epoch [20/120    avg_loss:0.392, val_acc:0.879]
Epoch [21/120    avg_loss:0.362, val_acc:0.896]
Epoch [22/120    avg_loss:0.410, val_acc:0.898]
Epoch [23/120    avg_loss:0.454, val_acc:0.873]
Epoch [24/120    avg_loss:0.350, val_acc:0.928]
Epoch [25/120    avg_loss:0.357, val_acc:0.912]
Epoch [26/120    avg_loss:0.382, val_acc:0.924]
Epoch [27/120    avg_loss:0.356, val_acc:0.914]
Epoch [28/120    avg_loss:0.296, val_acc:0.926]
Epoch [29/120    avg_loss:0.268, val_acc:0.918]
Epoch [30/120    avg_loss:0.247, val_acc:0.934]
Epoch [31/120    avg_loss:0.322, val_acc:0.926]
Epoch [32/120    avg_loss:0.287, val_acc:0.920]
Epoch [33/120    avg_loss:0.273, val_acc:0.939]
Epoch [34/120    avg_loss:0.222, val_acc:0.914]
Epoch [35/120    avg_loss:0.252, val_acc:0.932]
Epoch [36/120    avg_loss:0.260, val_acc:0.934]
Epoch [37/120    avg_loss:0.213, val_acc:0.941]
Epoch [38/120    avg_loss:0.234, val_acc:0.945]
Epoch [39/120    avg_loss:0.222, val_acc:0.928]
Epoch [40/120    avg_loss:0.214, val_acc:0.951]
Epoch [41/120    avg_loss:0.222, val_acc:0.938]
Epoch [42/120    avg_loss:0.221, val_acc:0.914]
Epoch [43/120    avg_loss:0.217, val_acc:0.938]
Epoch [44/120    avg_loss:0.242, val_acc:0.941]
Epoch [45/120    avg_loss:0.255, val_acc:0.957]
Epoch [46/120    avg_loss:0.211, val_acc:0.934]
Epoch [47/120    avg_loss:0.197, val_acc:0.936]
Epoch [48/120    avg_loss:0.179, val_acc:0.949]
Epoch [49/120    avg_loss:0.168, val_acc:0.957]
Epoch [50/120    avg_loss:0.155, val_acc:0.961]
Epoch [51/120    avg_loss:0.136, val_acc:0.951]
Epoch [52/120    avg_loss:0.171, val_acc:0.961]
Epoch [53/120    avg_loss:0.145, val_acc:0.949]
Epoch [54/120    avg_loss:0.141, val_acc:0.932]
Epoch [55/120    avg_loss:0.147, val_acc:0.949]
Epoch [56/120    avg_loss:0.170, val_acc:0.953]
Epoch [57/120    avg_loss:0.128, val_acc:0.961]
Epoch [58/120    avg_loss:0.109, val_acc:0.949]
Epoch [59/120    avg_loss:0.138, val_acc:0.965]
Epoch [60/120    avg_loss:0.127, val_acc:0.969]
Epoch [61/120    avg_loss:0.127, val_acc:0.957]
Epoch [62/120    avg_loss:0.148, val_acc:0.965]
Epoch [63/120    avg_loss:0.092, val_acc:0.971]
Epoch [64/120    avg_loss:0.092, val_acc:0.959]
Epoch [65/120    avg_loss:0.099, val_acc:0.980]
Epoch [66/120    avg_loss:0.090, val_acc:0.971]
Epoch [67/120    avg_loss:0.071, val_acc:0.941]
Epoch [68/120    avg_loss:0.112, val_acc:0.969]
Epoch [69/120    avg_loss:0.143, val_acc:0.941]
Epoch [70/120    avg_loss:0.123, val_acc:0.963]
Epoch [71/120    avg_loss:0.144, val_acc:0.947]
Epoch [72/120    avg_loss:0.132, val_acc:0.973]
Epoch [73/120    avg_loss:0.110, val_acc:0.975]
Epoch [74/120    avg_loss:0.085, val_acc:0.971]
Epoch [75/120    avg_loss:0.080, val_acc:0.963]
Epoch [76/120    avg_loss:0.085, val_acc:0.977]
Epoch [77/120    avg_loss:0.076, val_acc:0.963]
Epoch [78/120    avg_loss:0.074, val_acc:0.967]
Epoch [79/120    avg_loss:0.056, val_acc:0.982]
Epoch [80/120    avg_loss:0.053, val_acc:0.982]
Epoch [81/120    avg_loss:0.040, val_acc:0.984]
Epoch [82/120    avg_loss:0.040, val_acc:0.982]
Epoch [83/120    avg_loss:0.052, val_acc:0.982]
Epoch [84/120    avg_loss:0.040, val_acc:0.984]
Epoch [85/120    avg_loss:0.034, val_acc:0.980]
Epoch [86/120    avg_loss:0.033, val_acc:0.982]
Epoch [87/120    avg_loss:0.037, val_acc:0.984]
Epoch [88/120    avg_loss:0.028, val_acc:0.984]
Epoch [89/120    avg_loss:0.040, val_acc:0.979]
Epoch [90/120    avg_loss:0.035, val_acc:0.977]
Epoch [91/120    avg_loss:0.029, val_acc:0.980]
Epoch [92/120    avg_loss:0.034, val_acc:0.982]
Epoch [93/120    avg_loss:0.035, val_acc:0.982]
Epoch [94/120    avg_loss:0.044, val_acc:0.982]
Epoch [95/120    avg_loss:0.031, val_acc:0.982]
Epoch [96/120    avg_loss:0.032, val_acc:0.982]
Epoch [97/120    avg_loss:0.029, val_acc:0.980]
Epoch [98/120    avg_loss:0.032, val_acc:0.980]
Epoch [99/120    avg_loss:0.037, val_acc:0.980]
Epoch [100/120    avg_loss:0.031, val_acc:0.980]
Epoch [101/120    avg_loss:0.033, val_acc:0.980]
Epoch [102/120    avg_loss:0.032, val_acc:0.980]
Epoch [103/120    avg_loss:0.028, val_acc:0.980]
Epoch [104/120    avg_loss:0.027, val_acc:0.980]
Epoch [105/120    avg_loss:0.028, val_acc:0.980]
Epoch [106/120    avg_loss:0.031, val_acc:0.980]
Epoch [107/120    avg_loss:0.035, val_acc:0.980]
Epoch [108/120    avg_loss:0.027, val_acc:0.980]
Epoch [109/120    avg_loss:0.031, val_acc:0.980]
Epoch [110/120    avg_loss:0.035, val_acc:0.980]
Epoch [111/120    avg_loss:0.040, val_acc:0.982]
Epoch [112/120    avg_loss:0.034, val_acc:0.982]
Epoch [113/120    avg_loss:0.040, val_acc:0.982]
Epoch [114/120    avg_loss:0.025, val_acc:0.982]
Epoch [115/120    avg_loss:0.025, val_acc:0.982]
Epoch [116/120    avg_loss:0.032, val_acc:0.982]
Epoch [117/120    avg_loss:0.030, val_acc:0.982]
Epoch [118/120    avg_loss:0.029, val_acc:0.982]
Epoch [119/120    avg_loss:0.031, val_acc:0.982]
Epoch [120/120    avg_loss:0.032, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   1   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 204   0   0   0   0  15   0   0   0   0   0   0]
 [  0   0   0 210  12   0   0   0   6   2   0   0   0   0]
 [  0   0   0   6 213   7   0   0   1   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 0.99708455 0.93793103 0.94170404 0.92407809 0.94444444
 0.99266504 0.86458333 0.99106003 0.9978678  1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9824318819075819
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ff64c39b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.504, val_acc:0.369]
Epoch [2/120    avg_loss:2.235, val_acc:0.521]
Epoch [3/120    avg_loss:1.992, val_acc:0.648]
Epoch [4/120    avg_loss:1.697, val_acc:0.732]
Epoch [5/120    avg_loss:1.414, val_acc:0.777]
Epoch [6/120    avg_loss:1.169, val_acc:0.740]
Epoch [7/120    avg_loss:1.013, val_acc:0.799]
Epoch [8/120    avg_loss:0.862, val_acc:0.820]
Epoch [9/120    avg_loss:0.867, val_acc:0.818]
Epoch [10/120    avg_loss:0.812, val_acc:0.744]
Epoch [11/120    avg_loss:0.706, val_acc:0.844]
Epoch [12/120    avg_loss:0.641, val_acc:0.867]
Epoch [13/120    avg_loss:0.602, val_acc:0.883]
Epoch [14/120    avg_loss:0.518, val_acc:0.893]
Epoch [15/120    avg_loss:0.509, val_acc:0.834]
Epoch [16/120    avg_loss:0.545, val_acc:0.859]
Epoch [17/120    avg_loss:0.458, val_acc:0.891]
Epoch [18/120    avg_loss:0.442, val_acc:0.889]
Epoch [19/120    avg_loss:0.427, val_acc:0.910]
Epoch [20/120    avg_loss:0.402, val_acc:0.863]
Epoch [21/120    avg_loss:0.425, val_acc:0.885]
Epoch [22/120    avg_loss:0.465, val_acc:0.891]
Epoch [23/120    avg_loss:0.396, val_acc:0.855]
Epoch [24/120    avg_loss:0.377, val_acc:0.900]
Epoch [25/120    avg_loss:0.357, val_acc:0.896]
Epoch [26/120    avg_loss:0.341, val_acc:0.895]
Epoch [27/120    avg_loss:0.312, val_acc:0.932]
Epoch [28/120    avg_loss:0.297, val_acc:0.943]
Epoch [29/120    avg_loss:0.318, val_acc:0.920]
Epoch [30/120    avg_loss:0.295, val_acc:0.941]
Epoch [31/120    avg_loss:0.242, val_acc:0.916]
Epoch [32/120    avg_loss:0.291, val_acc:0.930]
Epoch [33/120    avg_loss:0.246, val_acc:0.945]
Epoch [34/120    avg_loss:0.262, val_acc:0.949]
Epoch [35/120    avg_loss:0.205, val_acc:0.947]
Epoch [36/120    avg_loss:0.201, val_acc:0.936]
Epoch [37/120    avg_loss:0.209, val_acc:0.930]
Epoch [38/120    avg_loss:0.210, val_acc:0.953]
Epoch [39/120    avg_loss:0.216, val_acc:0.953]
Epoch [40/120    avg_loss:0.208, val_acc:0.920]
Epoch [41/120    avg_loss:0.201, val_acc:0.945]
Epoch [42/120    avg_loss:0.186, val_acc:0.924]
Epoch [43/120    avg_loss:0.193, val_acc:0.947]
Epoch [44/120    avg_loss:0.164, val_acc:0.955]
Epoch [45/120    avg_loss:0.165, val_acc:0.951]
Epoch [46/120    avg_loss:0.159, val_acc:0.963]
Epoch [47/120    avg_loss:0.149, val_acc:0.967]
Epoch [48/120    avg_loss:0.174, val_acc:0.949]
Epoch [49/120    avg_loss:0.172, val_acc:0.941]
Epoch [50/120    avg_loss:0.154, val_acc:0.953]
Epoch [51/120    avg_loss:0.130, val_acc:0.957]
Epoch [52/120    avg_loss:0.257, val_acc:0.934]
Epoch [53/120    avg_loss:0.158, val_acc:0.951]
Epoch [54/120    avg_loss:0.213, val_acc:0.943]
Epoch [55/120    avg_loss:0.173, val_acc:0.959]
Epoch [56/120    avg_loss:0.171, val_acc:0.939]
Epoch [57/120    avg_loss:0.114, val_acc:0.967]
Epoch [58/120    avg_loss:0.114, val_acc:0.953]
Epoch [59/120    avg_loss:0.127, val_acc:0.965]
Epoch [60/120    avg_loss:0.118, val_acc:0.965]
Epoch [61/120    avg_loss:0.080, val_acc:0.963]
Epoch [62/120    avg_loss:0.101, val_acc:0.969]
Epoch [63/120    avg_loss:0.108, val_acc:0.951]
Epoch [64/120    avg_loss:0.180, val_acc:0.928]
Epoch [65/120    avg_loss:0.141, val_acc:0.973]
Epoch [66/120    avg_loss:0.079, val_acc:0.969]
Epoch [67/120    avg_loss:0.076, val_acc:0.967]
Epoch [68/120    avg_loss:0.072, val_acc:0.957]
Epoch [69/120    avg_loss:0.069, val_acc:0.973]
Epoch [70/120    avg_loss:0.091, val_acc:0.963]
Epoch [71/120    avg_loss:0.065, val_acc:0.963]
Epoch [72/120    avg_loss:0.112, val_acc:0.955]
Epoch [73/120    avg_loss:0.073, val_acc:0.977]
Epoch [74/120    avg_loss:0.044, val_acc:0.973]
Epoch [75/120    avg_loss:0.062, val_acc:0.965]
Epoch [76/120    avg_loss:0.066, val_acc:0.980]
Epoch [77/120    avg_loss:0.089, val_acc:0.947]
Epoch [78/120    avg_loss:0.105, val_acc:0.971]
Epoch [79/120    avg_loss:0.055, val_acc:0.975]
Epoch [80/120    avg_loss:0.058, val_acc:0.982]
Epoch [81/120    avg_loss:0.089, val_acc:0.975]
Epoch [82/120    avg_loss:0.095, val_acc:0.961]
Epoch [83/120    avg_loss:0.047, val_acc:0.973]
Epoch [84/120    avg_loss:0.049, val_acc:0.980]
Epoch [85/120    avg_loss:0.034, val_acc:0.982]
Epoch [86/120    avg_loss:0.026, val_acc:0.975]
Epoch [87/120    avg_loss:0.051, val_acc:0.973]
Epoch [88/120    avg_loss:0.066, val_acc:0.986]
Epoch [89/120    avg_loss:0.057, val_acc:0.969]
Epoch [90/120    avg_loss:0.066, val_acc:0.963]
Epoch [91/120    avg_loss:0.038, val_acc:0.961]
Epoch [92/120    avg_loss:0.058, val_acc:0.963]
Epoch [93/120    avg_loss:0.058, val_acc:0.975]
Epoch [94/120    avg_loss:0.055, val_acc:0.975]
Epoch [95/120    avg_loss:0.032, val_acc:0.984]
Epoch [96/120    avg_loss:0.039, val_acc:0.977]
Epoch [97/120    avg_loss:0.038, val_acc:0.984]
Epoch [98/120    avg_loss:0.044, val_acc:0.949]
Epoch [99/120    avg_loss:0.076, val_acc:0.973]
Epoch [100/120    avg_loss:0.027, val_acc:0.965]
Epoch [101/120    avg_loss:0.042, val_acc:0.973]
Epoch [102/120    avg_loss:0.023, val_acc:0.979]
Epoch [103/120    avg_loss:0.019, val_acc:0.982]
Epoch [104/120    avg_loss:0.019, val_acc:0.986]
Epoch [105/120    avg_loss:0.015, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.026, val_acc:0.988]
Epoch [108/120    avg_loss:0.012, val_acc:0.990]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.028, val_acc:0.990]
Epoch [111/120    avg_loss:0.015, val_acc:0.988]
Epoch [112/120    avg_loss:0.012, val_acc:0.990]
Epoch [113/120    avg_loss:0.013, val_acc:0.990]
Epoch [114/120    avg_loss:0.014, val_acc:0.988]
Epoch [115/120    avg_loss:0.018, val_acc:0.988]
Epoch [116/120    avg_loss:0.013, val_acc:0.988]
Epoch [117/120    avg_loss:0.016, val_acc:0.986]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.016, val_acc:0.990]
Epoch [120/120    avg_loss:0.013, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   1   2   0   0   0   0   0   0]
 [  0   0   1 226   2   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 122   0   0   1   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   3   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 0.99636364 0.97737557 0.99122807 0.91576674 0.86524823
 0.9902439  0.96216216 0.99101412 1.         1.         1.
 0.99556541 1.        ]

Kappa:
0.9859924225136107
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5272602a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.526, val_acc:0.471]
Epoch [2/120    avg_loss:2.227, val_acc:0.457]
Epoch [3/120    avg_loss:2.002, val_acc:0.590]
Epoch [4/120    avg_loss:1.776, val_acc:0.652]
Epoch [5/120    avg_loss:1.559, val_acc:0.725]
Epoch [6/120    avg_loss:1.344, val_acc:0.680]
Epoch [7/120    avg_loss:1.172, val_acc:0.734]
Epoch [8/120    avg_loss:1.064, val_acc:0.799]
Epoch [9/120    avg_loss:0.904, val_acc:0.818]
Epoch [10/120    avg_loss:0.774, val_acc:0.867]
Epoch [11/120    avg_loss:0.718, val_acc:0.758]
Epoch [12/120    avg_loss:0.723, val_acc:0.861]
Epoch [13/120    avg_loss:0.664, val_acc:0.885]
Epoch [14/120    avg_loss:0.591, val_acc:0.887]
Epoch [15/120    avg_loss:0.470, val_acc:0.900]
Epoch [16/120    avg_loss:0.516, val_acc:0.889]
Epoch [17/120    avg_loss:0.478, val_acc:0.898]
Epoch [18/120    avg_loss:0.432, val_acc:0.912]
Epoch [19/120    avg_loss:0.423, val_acc:0.922]
Epoch [20/120    avg_loss:0.418, val_acc:0.922]
Epoch [21/120    avg_loss:0.517, val_acc:0.906]
Epoch [22/120    avg_loss:0.424, val_acc:0.908]
Epoch [23/120    avg_loss:0.364, val_acc:0.936]
Epoch [24/120    avg_loss:0.336, val_acc:0.918]
Epoch [25/120    avg_loss:0.389, val_acc:0.895]
Epoch [26/120    avg_loss:0.348, val_acc:0.945]
Epoch [27/120    avg_loss:0.290, val_acc:0.955]
Epoch [28/120    avg_loss:0.297, val_acc:0.939]
Epoch [29/120    avg_loss:0.316, val_acc:0.922]
Epoch [30/120    avg_loss:0.287, val_acc:0.959]
Epoch [31/120    avg_loss:0.289, val_acc:0.949]
Epoch [32/120    avg_loss:0.263, val_acc:0.951]
Epoch [33/120    avg_loss:0.261, val_acc:0.957]
Epoch [34/120    avg_loss:0.271, val_acc:0.957]
Epoch [35/120    avg_loss:0.281, val_acc:0.963]
Epoch [36/120    avg_loss:0.227, val_acc:0.906]
Epoch [37/120    avg_loss:0.360, val_acc:0.938]
Epoch [38/120    avg_loss:0.316, val_acc:0.947]
Epoch [39/120    avg_loss:0.273, val_acc:0.953]
Epoch [40/120    avg_loss:0.235, val_acc:0.957]
Epoch [41/120    avg_loss:0.209, val_acc:0.953]
Epoch [42/120    avg_loss:0.179, val_acc:0.934]
Epoch [43/120    avg_loss:0.182, val_acc:0.963]
Epoch [44/120    avg_loss:0.302, val_acc:0.953]
Epoch [45/120    avg_loss:0.212, val_acc:0.955]
Epoch [46/120    avg_loss:0.190, val_acc:0.922]
Epoch [47/120    avg_loss:0.284, val_acc:0.914]
Epoch [48/120    avg_loss:0.207, val_acc:0.953]
Epoch [49/120    avg_loss:0.176, val_acc:0.941]
Epoch [50/120    avg_loss:0.183, val_acc:0.945]
Epoch [51/120    avg_loss:0.195, val_acc:0.945]
Epoch [52/120    avg_loss:0.241, val_acc:0.941]
Epoch [53/120    avg_loss:0.156, val_acc:0.973]
Epoch [54/120    avg_loss:0.153, val_acc:0.963]
Epoch [55/120    avg_loss:0.182, val_acc:0.961]
Epoch [56/120    avg_loss:0.175, val_acc:0.920]
Epoch [57/120    avg_loss:0.199, val_acc:0.963]
Epoch [58/120    avg_loss:0.173, val_acc:0.965]
Epoch [59/120    avg_loss:0.226, val_acc:0.934]
Epoch [60/120    avg_loss:0.134, val_acc:0.955]
Epoch [61/120    avg_loss:0.134, val_acc:0.973]
Epoch [62/120    avg_loss:0.111, val_acc:0.982]
Epoch [63/120    avg_loss:0.234, val_acc:0.967]
Epoch [64/120    avg_loss:0.140, val_acc:0.965]
Epoch [65/120    avg_loss:0.187, val_acc:0.965]
Epoch [66/120    avg_loss:0.160, val_acc:0.936]
Epoch [67/120    avg_loss:0.123, val_acc:0.963]
Epoch [68/120    avg_loss:0.110, val_acc:0.973]
Epoch [69/120    avg_loss:0.111, val_acc:0.975]
Epoch [70/120    avg_loss:0.119, val_acc:0.967]
Epoch [71/120    avg_loss:0.162, val_acc:0.973]
Epoch [72/120    avg_loss:0.116, val_acc:0.961]
Epoch [73/120    avg_loss:0.120, val_acc:0.969]
Epoch [74/120    avg_loss:0.123, val_acc:0.977]
Epoch [75/120    avg_loss:0.094, val_acc:0.977]
Epoch [76/120    avg_loss:0.081, val_acc:0.984]
Epoch [77/120    avg_loss:0.072, val_acc:0.979]
Epoch [78/120    avg_loss:0.086, val_acc:0.984]
Epoch [79/120    avg_loss:0.080, val_acc:0.979]
Epoch [80/120    avg_loss:0.062, val_acc:0.982]
Epoch [81/120    avg_loss:0.083, val_acc:0.984]
Epoch [82/120    avg_loss:0.065, val_acc:0.984]
Epoch [83/120    avg_loss:0.059, val_acc:0.988]
Epoch [84/120    avg_loss:0.063, val_acc:0.984]
Epoch [85/120    avg_loss:0.060, val_acc:0.986]
Epoch [86/120    avg_loss:0.064, val_acc:0.988]
Epoch [87/120    avg_loss:0.062, val_acc:0.984]
Epoch [88/120    avg_loss:0.056, val_acc:0.988]
Epoch [89/120    avg_loss:0.059, val_acc:0.984]
Epoch [90/120    avg_loss:0.061, val_acc:0.988]
Epoch [91/120    avg_loss:0.066, val_acc:0.986]
Epoch [92/120    avg_loss:0.049, val_acc:0.988]
Epoch [93/120    avg_loss:0.049, val_acc:0.988]
Epoch [94/120    avg_loss:0.054, val_acc:0.986]
Epoch [95/120    avg_loss:0.059, val_acc:0.984]
Epoch [96/120    avg_loss:0.056, val_acc:0.988]
Epoch [97/120    avg_loss:0.062, val_acc:0.988]
Epoch [98/120    avg_loss:0.055, val_acc:0.986]
Epoch [99/120    avg_loss:0.060, val_acc:0.990]
Epoch [100/120    avg_loss:0.065, val_acc:0.990]
Epoch [101/120    avg_loss:0.054, val_acc:0.990]
Epoch [102/120    avg_loss:0.066, val_acc:0.990]
Epoch [103/120    avg_loss:0.060, val_acc:0.988]
Epoch [104/120    avg_loss:0.060, val_acc:0.988]
Epoch [105/120    avg_loss:0.058, val_acc:0.988]
Epoch [106/120    avg_loss:0.062, val_acc:0.990]
Epoch [107/120    avg_loss:0.049, val_acc:0.988]
Epoch [108/120    avg_loss:0.058, val_acc:0.988]
Epoch [109/120    avg_loss:0.052, val_acc:0.988]
Epoch [110/120    avg_loss:0.053, val_acc:0.988]
Epoch [111/120    avg_loss:0.057, val_acc:0.990]
Epoch [112/120    avg_loss:0.051, val_acc:0.988]
Epoch [113/120    avg_loss:0.047, val_acc:0.990]
Epoch [114/120    avg_loss:0.053, val_acc:0.988]
Epoch [115/120    avg_loss:0.052, val_acc:0.988]
Epoch [116/120    avg_loss:0.062, val_acc:0.990]
Epoch [117/120    avg_loss:0.046, val_acc:0.990]
Epoch [118/120    avg_loss:0.059, val_acc:0.988]
Epoch [119/120    avg_loss:0.044, val_acc:0.990]
Epoch [120/120    avg_loss:0.040, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   0 219   5   0   0   0   1   5   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   4   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   2   0   0 832]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 1.         0.93453725 0.97550111 0.93512304 0.9205298
 1.         0.84153005 0.99742931 0.99361702 0.99182561 0.99867198
 0.9944629  0.99879952]

Kappa:
0.9829088172646454
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74d9131a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.516, val_acc:0.477]
Epoch [2/120    avg_loss:2.216, val_acc:0.562]
Epoch [3/120    avg_loss:1.985, val_acc:0.604]
Epoch [4/120    avg_loss:1.742, val_acc:0.686]
Epoch [5/120    avg_loss:1.490, val_acc:0.688]
Epoch [6/120    avg_loss:1.295, val_acc:0.664]
Epoch [7/120    avg_loss:1.170, val_acc:0.773]
Epoch [8/120    avg_loss:0.959, val_acc:0.814]
Epoch [9/120    avg_loss:0.849, val_acc:0.838]
Epoch [10/120    avg_loss:0.829, val_acc:0.832]
Epoch [11/120    avg_loss:0.766, val_acc:0.807]
Epoch [12/120    avg_loss:0.653, val_acc:0.832]
Epoch [13/120    avg_loss:0.643, val_acc:0.834]
Epoch [14/120    avg_loss:0.656, val_acc:0.881]
Epoch [15/120    avg_loss:0.573, val_acc:0.879]
Epoch [16/120    avg_loss:0.599, val_acc:0.885]
Epoch [17/120    avg_loss:0.545, val_acc:0.900]
Epoch [18/120    avg_loss:0.476, val_acc:0.904]
Epoch [19/120    avg_loss:0.433, val_acc:0.916]
Epoch [20/120    avg_loss:0.440, val_acc:0.904]
Epoch [21/120    avg_loss:0.434, val_acc:0.914]
Epoch [22/120    avg_loss:0.421, val_acc:0.914]
Epoch [23/120    avg_loss:0.349, val_acc:0.939]
Epoch [24/120    avg_loss:0.354, val_acc:0.906]
Epoch [25/120    avg_loss:0.297, val_acc:0.898]
Epoch [26/120    avg_loss:0.309, val_acc:0.928]
Epoch [27/120    avg_loss:0.363, val_acc:0.893]
Epoch [28/120    avg_loss:0.333, val_acc:0.914]
Epoch [29/120    avg_loss:0.320, val_acc:0.947]
Epoch [30/120    avg_loss:0.249, val_acc:0.934]
Epoch [31/120    avg_loss:0.255, val_acc:0.943]
Epoch [32/120    avg_loss:0.267, val_acc:0.957]
Epoch [33/120    avg_loss:0.216, val_acc:0.939]
Epoch [34/120    avg_loss:0.286, val_acc:0.951]
Epoch [35/120    avg_loss:0.242, val_acc:0.949]
Epoch [36/120    avg_loss:0.214, val_acc:0.947]
Epoch [37/120    avg_loss:0.195, val_acc:0.953]
Epoch [38/120    avg_loss:0.190, val_acc:0.920]
Epoch [39/120    avg_loss:0.264, val_acc:0.945]
Epoch [40/120    avg_loss:0.177, val_acc:0.969]
Epoch [41/120    avg_loss:0.219, val_acc:0.926]
Epoch [42/120    avg_loss:0.197, val_acc:0.969]
Epoch [43/120    avg_loss:0.233, val_acc:0.941]
Epoch [44/120    avg_loss:0.155, val_acc:0.967]
Epoch [45/120    avg_loss:0.153, val_acc:0.957]
Epoch [46/120    avg_loss:0.177, val_acc:0.965]
Epoch [47/120    avg_loss:0.169, val_acc:0.975]
Epoch [48/120    avg_loss:0.138, val_acc:0.908]
Epoch [49/120    avg_loss:0.210, val_acc:0.965]
Epoch [50/120    avg_loss:0.163, val_acc:0.967]
Epoch [51/120    avg_loss:0.179, val_acc:0.973]
Epoch [52/120    avg_loss:0.118, val_acc:0.971]
Epoch [53/120    avg_loss:0.115, val_acc:0.979]
Epoch [54/120    avg_loss:0.113, val_acc:0.982]
Epoch [55/120    avg_loss:0.201, val_acc:0.969]
Epoch [56/120    avg_loss:0.118, val_acc:0.979]
Epoch [57/120    avg_loss:0.087, val_acc:0.965]
Epoch [58/120    avg_loss:0.119, val_acc:0.955]
Epoch [59/120    avg_loss:0.108, val_acc:0.971]
Epoch [60/120    avg_loss:0.141, val_acc:0.973]
Epoch [61/120    avg_loss:0.142, val_acc:0.957]
Epoch [62/120    avg_loss:0.116, val_acc:0.965]
Epoch [63/120    avg_loss:0.105, val_acc:0.982]
Epoch [64/120    avg_loss:0.191, val_acc:0.971]
Epoch [65/120    avg_loss:0.124, val_acc:0.959]
Epoch [66/120    avg_loss:0.097, val_acc:0.973]
Epoch [67/120    avg_loss:0.107, val_acc:0.980]
Epoch [68/120    avg_loss:0.085, val_acc:0.988]
Epoch [69/120    avg_loss:0.073, val_acc:0.986]
Epoch [70/120    avg_loss:0.090, val_acc:0.967]
Epoch [71/120    avg_loss:0.104, val_acc:0.980]
Epoch [72/120    avg_loss:0.139, val_acc:0.965]
Epoch [73/120    avg_loss:0.116, val_acc:0.973]
Epoch [74/120    avg_loss:0.164, val_acc:0.959]
Epoch [75/120    avg_loss:0.123, val_acc:0.969]
Epoch [76/120    avg_loss:0.103, val_acc:0.979]
Epoch [77/120    avg_loss:0.078, val_acc:0.965]
Epoch [78/120    avg_loss:0.147, val_acc:0.975]
Epoch [79/120    avg_loss:0.106, val_acc:0.957]
Epoch [80/120    avg_loss:0.076, val_acc:0.980]
Epoch [81/120    avg_loss:0.071, val_acc:0.986]
Epoch [82/120    avg_loss:0.059, val_acc:0.984]
Epoch [83/120    avg_loss:0.052, val_acc:0.986]
Epoch [84/120    avg_loss:0.050, val_acc:0.990]
Epoch [85/120    avg_loss:0.039, val_acc:0.990]
Epoch [86/120    avg_loss:0.040, val_acc:0.990]
Epoch [87/120    avg_loss:0.047, val_acc:0.990]
Epoch [88/120    avg_loss:0.038, val_acc:0.992]
Epoch [89/120    avg_loss:0.042, val_acc:0.990]
Epoch [90/120    avg_loss:0.048, val_acc:0.992]
Epoch [91/120    avg_loss:0.038, val_acc:0.992]
Epoch [92/120    avg_loss:0.023, val_acc:0.992]
Epoch [93/120    avg_loss:0.049, val_acc:0.992]
Epoch [94/120    avg_loss:0.044, val_acc:0.992]
Epoch [95/120    avg_loss:0.034, val_acc:0.992]
Epoch [96/120    avg_loss:0.037, val_acc:0.994]
Epoch [97/120    avg_loss:0.040, val_acc:0.994]
Epoch [98/120    avg_loss:0.036, val_acc:0.994]
Epoch [99/120    avg_loss:0.042, val_acc:0.994]
Epoch [100/120    avg_loss:0.031, val_acc:0.992]
Epoch [101/120    avg_loss:0.036, val_acc:0.992]
Epoch [102/120    avg_loss:0.040, val_acc:0.994]
Epoch [103/120    avg_loss:0.031, val_acc:0.992]
Epoch [104/120    avg_loss:0.038, val_acc:0.992]
Epoch [105/120    avg_loss:0.035, val_acc:0.992]
Epoch [106/120    avg_loss:0.032, val_acc:0.992]
Epoch [107/120    avg_loss:0.034, val_acc:0.992]
Epoch [108/120    avg_loss:0.037, val_acc:0.992]
Epoch [109/120    avg_loss:0.038, val_acc:0.992]
Epoch [110/120    avg_loss:0.037, val_acc:0.994]
Epoch [111/120    avg_loss:0.031, val_acc:0.994]
Epoch [112/120    avg_loss:0.041, val_acc:0.994]
Epoch [113/120    avg_loss:0.033, val_acc:0.994]
Epoch [114/120    avg_loss:0.038, val_acc:0.992]
Epoch [115/120    avg_loss:0.030, val_acc:0.992]
Epoch [116/120    avg_loss:0.033, val_acc:0.992]
Epoch [117/120    avg_loss:0.032, val_acc:0.994]
Epoch [118/120    avg_loss:0.030, val_acc:0.992]
Epoch [119/120    avg_loss:0.028, val_acc:0.992]
Epoch [120/120    avg_loss:0.027, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 201   0   0   0   0  18   0   0   0   0   0   0]
 [  0   0   0 223   5   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   4   0   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   4   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 0.99419448 0.93271462 0.98454746 0.95535714 0.95016611
 0.99019608 0.85128205 0.98841699 0.99893276 1.         1.
 0.99556541 1.        ]

Kappa:
0.9850438722192825
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7733fae9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.523, val_acc:0.371]
Epoch [2/120    avg_loss:2.260, val_acc:0.531]
Epoch [3/120    avg_loss:1.981, val_acc:0.607]
Epoch [4/120    avg_loss:1.708, val_acc:0.721]
Epoch [5/120    avg_loss:1.480, val_acc:0.725]
Epoch [6/120    avg_loss:1.282, val_acc:0.760]
Epoch [7/120    avg_loss:1.167, val_acc:0.779]
Epoch [8/120    avg_loss:0.936, val_acc:0.828]
Epoch [9/120    avg_loss:0.818, val_acc:0.814]
Epoch [10/120    avg_loss:0.762, val_acc:0.828]
Epoch [11/120    avg_loss:0.719, val_acc:0.822]
Epoch [12/120    avg_loss:0.665, val_acc:0.854]
Epoch [13/120    avg_loss:0.625, val_acc:0.861]
Epoch [14/120    avg_loss:0.623, val_acc:0.848]
Epoch [15/120    avg_loss:0.534, val_acc:0.883]
Epoch [16/120    avg_loss:0.559, val_acc:0.904]
Epoch [17/120    avg_loss:0.539, val_acc:0.865]
Epoch [18/120    avg_loss:0.459, val_acc:0.900]
Epoch [19/120    avg_loss:0.466, val_acc:0.867]
Epoch [20/120    avg_loss:0.453, val_acc:0.877]
Epoch [21/120    avg_loss:0.402, val_acc:0.883]
Epoch [22/120    avg_loss:0.453, val_acc:0.898]
Epoch [23/120    avg_loss:0.363, val_acc:0.900]
Epoch [24/120    avg_loss:0.412, val_acc:0.906]
Epoch [25/120    avg_loss:0.355, val_acc:0.900]
Epoch [26/120    avg_loss:0.378, val_acc:0.873]
Epoch [27/120    avg_loss:0.334, val_acc:0.916]
Epoch [28/120    avg_loss:0.312, val_acc:0.932]
Epoch [29/120    avg_loss:0.315, val_acc:0.922]
Epoch [30/120    avg_loss:0.261, val_acc:0.910]
Epoch [31/120    avg_loss:0.249, val_acc:0.938]
Epoch [32/120    avg_loss:0.300, val_acc:0.930]
Epoch [33/120    avg_loss:0.254, val_acc:0.939]
Epoch [34/120    avg_loss:0.283, val_acc:0.945]
Epoch [35/120    avg_loss:0.244, val_acc:0.941]
Epoch [36/120    avg_loss:0.238, val_acc:0.924]
Epoch [37/120    avg_loss:0.348, val_acc:0.943]
Epoch [38/120    avg_loss:0.244, val_acc:0.934]
Epoch [39/120    avg_loss:0.221, val_acc:0.941]
Epoch [40/120    avg_loss:0.256, val_acc:0.922]
Epoch [41/120    avg_loss:0.214, val_acc:0.943]
Epoch [42/120    avg_loss:0.257, val_acc:0.926]
Epoch [43/120    avg_loss:0.197, val_acc:0.949]
Epoch [44/120    avg_loss:0.184, val_acc:0.930]
Epoch [45/120    avg_loss:0.194, val_acc:0.939]
Epoch [46/120    avg_loss:0.153, val_acc:0.920]
Epoch [47/120    avg_loss:0.241, val_acc:0.939]
Epoch [48/120    avg_loss:0.190, val_acc:0.926]
Epoch [49/120    avg_loss:0.215, val_acc:0.943]
Epoch [50/120    avg_loss:0.166, val_acc:0.934]
Epoch [51/120    avg_loss:0.199, val_acc:0.941]
Epoch [52/120    avg_loss:0.192, val_acc:0.932]
Epoch [53/120    avg_loss:0.174, val_acc:0.945]
Epoch [54/120    avg_loss:0.166, val_acc:0.938]
Epoch [55/120    avg_loss:0.135, val_acc:0.928]
Epoch [56/120    avg_loss:0.181, val_acc:0.963]
Epoch [57/120    avg_loss:0.167, val_acc:0.963]
Epoch [58/120    avg_loss:0.180, val_acc:0.949]
Epoch [59/120    avg_loss:0.197, val_acc:0.941]
Epoch [60/120    avg_loss:0.167, val_acc:0.943]
Epoch [61/120    avg_loss:0.159, val_acc:0.939]
Epoch [62/120    avg_loss:0.148, val_acc:0.932]
Epoch [63/120    avg_loss:0.156, val_acc:0.971]
Epoch [64/120    avg_loss:0.112, val_acc:0.977]
Epoch [65/120    avg_loss:0.124, val_acc:0.973]
Epoch [66/120    avg_loss:0.107, val_acc:0.975]
Epoch [67/120    avg_loss:0.131, val_acc:0.969]
Epoch [68/120    avg_loss:0.128, val_acc:0.959]
Epoch [69/120    avg_loss:0.087, val_acc:0.969]
Epoch [70/120    avg_loss:0.103, val_acc:0.971]
Epoch [71/120    avg_loss:0.136, val_acc:0.947]
Epoch [72/120    avg_loss:0.108, val_acc:0.965]
Epoch [73/120    avg_loss:0.080, val_acc:0.957]
Epoch [74/120    avg_loss:0.091, val_acc:0.965]
Epoch [75/120    avg_loss:0.070, val_acc:0.967]
Epoch [76/120    avg_loss:0.091, val_acc:0.959]
Epoch [77/120    avg_loss:0.104, val_acc:0.961]
Epoch [78/120    avg_loss:0.079, val_acc:0.971]
Epoch [79/120    avg_loss:0.053, val_acc:0.973]
Epoch [80/120    avg_loss:0.051, val_acc:0.975]
Epoch [81/120    avg_loss:0.046, val_acc:0.975]
Epoch [82/120    avg_loss:0.048, val_acc:0.977]
Epoch [83/120    avg_loss:0.045, val_acc:0.977]
Epoch [84/120    avg_loss:0.036, val_acc:0.977]
Epoch [85/120    avg_loss:0.033, val_acc:0.977]
Epoch [86/120    avg_loss:0.046, val_acc:0.975]
Epoch [87/120    avg_loss:0.041, val_acc:0.973]
Epoch [88/120    avg_loss:0.037, val_acc:0.973]
Epoch [89/120    avg_loss:0.040, val_acc:0.975]
Epoch [90/120    avg_loss:0.049, val_acc:0.975]
Epoch [91/120    avg_loss:0.038, val_acc:0.979]
Epoch [92/120    avg_loss:0.040, val_acc:0.975]
Epoch [93/120    avg_loss:0.052, val_acc:0.973]
Epoch [94/120    avg_loss:0.039, val_acc:0.975]
Epoch [95/120    avg_loss:0.047, val_acc:0.975]
Epoch [96/120    avg_loss:0.043, val_acc:0.973]
Epoch [97/120    avg_loss:0.035, val_acc:0.975]
Epoch [98/120    avg_loss:0.048, val_acc:0.971]
Epoch [99/120    avg_loss:0.035, val_acc:0.973]
Epoch [100/120    avg_loss:0.040, val_acc:0.975]
Epoch [101/120    avg_loss:0.047, val_acc:0.977]
Epoch [102/120    avg_loss:0.036, val_acc:0.971]
Epoch [103/120    avg_loss:0.036, val_acc:0.971]
Epoch [104/120    avg_loss:0.032, val_acc:0.975]
Epoch [105/120    avg_loss:0.035, val_acc:0.975]
Epoch [106/120    avg_loss:0.040, val_acc:0.975]
Epoch [107/120    avg_loss:0.033, val_acc:0.975]
Epoch [108/120    avg_loss:0.028, val_acc:0.975]
Epoch [109/120    avg_loss:0.035, val_acc:0.975]
Epoch [110/120    avg_loss:0.036, val_acc:0.975]
Epoch [111/120    avg_loss:0.033, val_acc:0.975]
Epoch [112/120    avg_loss:0.040, val_acc:0.975]
Epoch [113/120    avg_loss:0.033, val_acc:0.975]
Epoch [114/120    avg_loss:0.031, val_acc:0.975]
Epoch [115/120    avg_loss:0.029, val_acc:0.975]
Epoch [116/120    avg_loss:0.042, val_acc:0.975]
Epoch [117/120    avg_loss:0.034, val_acc:0.975]
Epoch [118/120    avg_loss:0.034, val_acc:0.975]
Epoch [119/120    avg_loss:0.040, val_acc:0.975]
Epoch [120/120    avg_loss:0.032, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 220   7   0   0   0   3   0   0   0   0   0]
 [  0   0   1   2 212  12   0   0   0   0   0   0   0   0]
 [  0   0   0   1  18 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   1   0   0   0   0   0   0   0   0   0 375   1   0]
 [  0   0   1   0   0   0   0   0   1   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 0.99927061 0.93959732 0.97130243 0.9137931  0.89045936
 1.         0.86187845 0.99487179 1.         1.         0.99734043
 0.99668508 1.        ]

Kappa:
0.9826694099723
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6e57f19b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.535, val_acc:0.303]
Epoch [2/120    avg_loss:2.261, val_acc:0.494]
Epoch [3/120    avg_loss:2.021, val_acc:0.641]
Epoch [4/120    avg_loss:1.777, val_acc:0.693]
Epoch [5/120    avg_loss:1.538, val_acc:0.695]
Epoch [6/120    avg_loss:1.336, val_acc:0.777]
Epoch [7/120    avg_loss:1.146, val_acc:0.787]
Epoch [8/120    avg_loss:1.030, val_acc:0.695]
Epoch [9/120    avg_loss:0.895, val_acc:0.764]
Epoch [10/120    avg_loss:0.875, val_acc:0.834]
Epoch [11/120    avg_loss:0.842, val_acc:0.818]
Epoch [12/120    avg_loss:0.777, val_acc:0.848]
Epoch [13/120    avg_loss:0.725, val_acc:0.865]
Epoch [14/120    avg_loss:0.629, val_acc:0.854]
Epoch [15/120    avg_loss:0.610, val_acc:0.893]
Epoch [16/120    avg_loss:0.615, val_acc:0.855]
Epoch [17/120    avg_loss:0.570, val_acc:0.865]
Epoch [18/120    avg_loss:0.486, val_acc:0.869]
Epoch [19/120    avg_loss:0.507, val_acc:0.889]
Epoch [20/120    avg_loss:0.484, val_acc:0.863]
Epoch [21/120    avg_loss:0.429, val_acc:0.859]
Epoch [22/120    avg_loss:0.411, val_acc:0.863]
Epoch [23/120    avg_loss:0.406, val_acc:0.838]
Epoch [24/120    avg_loss:0.362, val_acc:0.895]
Epoch [25/120    avg_loss:0.418, val_acc:0.879]
Epoch [26/120    avg_loss:0.446, val_acc:0.900]
Epoch [27/120    avg_loss:0.384, val_acc:0.941]
Epoch [28/120    avg_loss:0.328, val_acc:0.924]
Epoch [29/120    avg_loss:0.333, val_acc:0.902]
Epoch [30/120    avg_loss:0.287, val_acc:0.930]
Epoch [31/120    avg_loss:0.318, val_acc:0.902]
Epoch [32/120    avg_loss:0.343, val_acc:0.934]
Epoch [33/120    avg_loss:0.318, val_acc:0.910]
Epoch [34/120    avg_loss:0.281, val_acc:0.941]
Epoch [35/120    avg_loss:0.292, val_acc:0.938]
Epoch [36/120    avg_loss:0.248, val_acc:0.924]
Epoch [37/120    avg_loss:0.244, val_acc:0.928]
Epoch [38/120    avg_loss:0.303, val_acc:0.938]
Epoch [39/120    avg_loss:0.273, val_acc:0.936]
Epoch [40/120    avg_loss:0.249, val_acc:0.949]
Epoch [41/120    avg_loss:0.223, val_acc:0.914]
Epoch [42/120    avg_loss:0.285, val_acc:0.941]
Epoch [43/120    avg_loss:0.233, val_acc:0.914]
Epoch [44/120    avg_loss:0.238, val_acc:0.939]
Epoch [45/120    avg_loss:0.229, val_acc:0.967]
Epoch [46/120    avg_loss:0.210, val_acc:0.943]
Epoch [47/120    avg_loss:0.228, val_acc:0.939]
Epoch [48/120    avg_loss:0.204, val_acc:0.934]
Epoch [49/120    avg_loss:0.178, val_acc:0.936]
Epoch [50/120    avg_loss:0.171, val_acc:0.965]
Epoch [51/120    avg_loss:0.141, val_acc:0.961]
Epoch [52/120    avg_loss:0.158, val_acc:0.961]
Epoch [53/120    avg_loss:0.136, val_acc:0.973]
Epoch [54/120    avg_loss:0.103, val_acc:0.971]
Epoch [55/120    avg_loss:0.114, val_acc:0.969]
Epoch [56/120    avg_loss:0.173, val_acc:0.936]
Epoch [57/120    avg_loss:0.184, val_acc:0.953]
Epoch [58/120    avg_loss:0.225, val_acc:0.936]
Epoch [59/120    avg_loss:0.152, val_acc:0.953]
Epoch [60/120    avg_loss:0.136, val_acc:0.975]
Epoch [61/120    avg_loss:0.133, val_acc:0.963]
Epoch [62/120    avg_loss:0.150, val_acc:0.951]
Epoch [63/120    avg_loss:0.085, val_acc:0.963]
Epoch [64/120    avg_loss:0.120, val_acc:0.971]
Epoch [65/120    avg_loss:0.118, val_acc:0.957]
Epoch [66/120    avg_loss:0.106, val_acc:0.969]
Epoch [67/120    avg_loss:0.132, val_acc:0.957]
Epoch [68/120    avg_loss:0.085, val_acc:0.980]
Epoch [69/120    avg_loss:0.074, val_acc:0.971]
Epoch [70/120    avg_loss:0.093, val_acc:0.943]
Epoch [71/120    avg_loss:0.110, val_acc:0.953]
Epoch [72/120    avg_loss:0.131, val_acc:0.924]
Epoch [73/120    avg_loss:0.173, val_acc:0.973]
Epoch [74/120    avg_loss:0.100, val_acc:0.967]
Epoch [75/120    avg_loss:0.106, val_acc:0.965]
Epoch [76/120    avg_loss:0.068, val_acc:0.973]
Epoch [77/120    avg_loss:0.059, val_acc:0.977]
Epoch [78/120    avg_loss:0.075, val_acc:0.953]
Epoch [79/120    avg_loss:0.068, val_acc:0.973]
Epoch [80/120    avg_loss:0.121, val_acc:0.959]
Epoch [81/120    avg_loss:0.196, val_acc:0.953]
Epoch [82/120    avg_loss:0.078, val_acc:0.963]
Epoch [83/120    avg_loss:0.068, val_acc:0.965]
Epoch [84/120    avg_loss:0.061, val_acc:0.973]
Epoch [85/120    avg_loss:0.048, val_acc:0.975]
Epoch [86/120    avg_loss:0.060, val_acc:0.973]
Epoch [87/120    avg_loss:0.062, val_acc:0.973]
Epoch [88/120    avg_loss:0.057, val_acc:0.973]
Epoch [89/120    avg_loss:0.050, val_acc:0.975]
Epoch [90/120    avg_loss:0.047, val_acc:0.980]
Epoch [91/120    avg_loss:0.038, val_acc:0.975]
Epoch [92/120    avg_loss:0.047, val_acc:0.969]
Epoch [93/120    avg_loss:0.043, val_acc:0.975]
Epoch [94/120    avg_loss:0.044, val_acc:0.977]
Epoch [95/120    avg_loss:0.045, val_acc:0.971]
Epoch [96/120    avg_loss:0.043, val_acc:0.977]
Epoch [97/120    avg_loss:0.044, val_acc:0.975]
Epoch [98/120    avg_loss:0.037, val_acc:0.977]
Epoch [99/120    avg_loss:0.034, val_acc:0.975]
Epoch [100/120    avg_loss:0.036, val_acc:0.975]
Epoch [101/120    avg_loss:0.033, val_acc:0.977]
Epoch [102/120    avg_loss:0.038, val_acc:0.977]
Epoch [103/120    avg_loss:0.039, val_acc:0.980]
Epoch [104/120    avg_loss:0.038, val_acc:0.977]
Epoch [105/120    avg_loss:0.034, val_acc:0.977]
Epoch [106/120    avg_loss:0.033, val_acc:0.979]
Epoch [107/120    avg_loss:0.042, val_acc:0.979]
Epoch [108/120    avg_loss:0.040, val_acc:0.979]
Epoch [109/120    avg_loss:0.035, val_acc:0.977]
Epoch [110/120    avg_loss:0.031, val_acc:0.980]
Epoch [111/120    avg_loss:0.037, val_acc:0.979]
Epoch [112/120    avg_loss:0.035, val_acc:0.980]
Epoch [113/120    avg_loss:0.037, val_acc:0.979]
Epoch [114/120    avg_loss:0.030, val_acc:0.980]
Epoch [115/120    avg_loss:0.031, val_acc:0.979]
Epoch [116/120    avg_loss:0.036, val_acc:0.979]
Epoch [117/120    avg_loss:0.032, val_acc:0.979]
Epoch [118/120    avg_loss:0.029, val_acc:0.979]
Epoch [119/120    avg_loss:0.033, val_acc:0.980]
Epoch [120/120    avg_loss:0.037, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   7   0   0   0   0   1   0]
 [  0   0   0 217   6   0   0   0   7   0   0   0   0   0]
 [  0   0   0   5 195  26   0   0   1   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   2   0   0   2   0 202   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.23027718550107

F1 scores:
[       nan 0.99854227 0.95045045 0.96017699 0.88636364 0.88235294
 0.99019608 0.8839779  0.98979592 1.         0.99862826 0.99867198
 0.99669239 1.        ]

Kappa:
0.9802952966973425
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a9f685a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.503, val_acc:0.319]
Epoch [2/120    avg_loss:2.227, val_acc:0.460]
Epoch [3/120    avg_loss:1.995, val_acc:0.589]
Epoch [4/120    avg_loss:1.810, val_acc:0.619]
Epoch [5/120    avg_loss:1.587, val_acc:0.716]
Epoch [6/120    avg_loss:1.354, val_acc:0.726]
Epoch [7/120    avg_loss:1.176, val_acc:0.734]
Epoch [8/120    avg_loss:1.032, val_acc:0.813]
Epoch [9/120    avg_loss:0.955, val_acc:0.808]
Epoch [10/120    avg_loss:0.879, val_acc:0.841]
Epoch [11/120    avg_loss:0.839, val_acc:0.829]
Epoch [12/120    avg_loss:0.721, val_acc:0.841]
Epoch [13/120    avg_loss:0.702, val_acc:0.853]
Epoch [14/120    avg_loss:0.599, val_acc:0.823]
Epoch [15/120    avg_loss:0.581, val_acc:0.853]
Epoch [16/120    avg_loss:0.595, val_acc:0.877]
Epoch [17/120    avg_loss:0.524, val_acc:0.885]
Epoch [18/120    avg_loss:0.549, val_acc:0.895]
Epoch [19/120    avg_loss:0.460, val_acc:0.810]
Epoch [20/120    avg_loss:0.512, val_acc:0.901]
Epoch [21/120    avg_loss:0.427, val_acc:0.919]
Epoch [22/120    avg_loss:0.406, val_acc:0.821]
Epoch [23/120    avg_loss:0.430, val_acc:0.919]
Epoch [24/120    avg_loss:0.382, val_acc:0.883]
Epoch [25/120    avg_loss:0.342, val_acc:0.911]
Epoch [26/120    avg_loss:0.416, val_acc:0.919]
Epoch [27/120    avg_loss:0.375, val_acc:0.925]
Epoch [28/120    avg_loss:0.285, val_acc:0.923]
Epoch [29/120    avg_loss:0.269, val_acc:0.901]
Epoch [30/120    avg_loss:0.294, val_acc:0.895]
Epoch [31/120    avg_loss:0.267, val_acc:0.944]
Epoch [32/120    avg_loss:0.244, val_acc:0.946]
Epoch [33/120    avg_loss:0.314, val_acc:0.931]
Epoch [34/120    avg_loss:0.268, val_acc:0.927]
Epoch [35/120    avg_loss:0.271, val_acc:0.931]
Epoch [36/120    avg_loss:0.209, val_acc:0.948]
Epoch [37/120    avg_loss:0.183, val_acc:0.923]
Epoch [38/120    avg_loss:0.272, val_acc:0.948]
Epoch [39/120    avg_loss:0.275, val_acc:0.940]
Epoch [40/120    avg_loss:0.220, val_acc:0.940]
Epoch [41/120    avg_loss:0.174, val_acc:0.952]
Epoch [42/120    avg_loss:0.194, val_acc:0.962]
Epoch [43/120    avg_loss:0.167, val_acc:0.944]
Epoch [44/120    avg_loss:0.178, val_acc:0.942]
Epoch [45/120    avg_loss:0.165, val_acc:0.960]
Epoch [46/120    avg_loss:0.204, val_acc:0.950]
Epoch [47/120    avg_loss:0.154, val_acc:0.958]
Epoch [48/120    avg_loss:0.205, val_acc:0.960]
Epoch [49/120    avg_loss:0.136, val_acc:0.972]
Epoch [50/120    avg_loss:0.136, val_acc:0.956]
Epoch [51/120    avg_loss:0.160, val_acc:0.946]
Epoch [52/120    avg_loss:0.201, val_acc:0.964]
Epoch [53/120    avg_loss:0.151, val_acc:0.956]
Epoch [54/120    avg_loss:0.108, val_acc:0.972]
Epoch [55/120    avg_loss:0.126, val_acc:0.887]
Epoch [56/120    avg_loss:0.124, val_acc:0.962]
Epoch [57/120    avg_loss:0.123, val_acc:0.958]
Epoch [58/120    avg_loss:0.102, val_acc:0.976]
Epoch [59/120    avg_loss:0.120, val_acc:0.958]
Epoch [60/120    avg_loss:0.078, val_acc:0.968]
Epoch [61/120    avg_loss:0.091, val_acc:0.980]
Epoch [62/120    avg_loss:0.129, val_acc:0.952]
Epoch [63/120    avg_loss:0.246, val_acc:0.948]
Epoch [64/120    avg_loss:0.194, val_acc:0.954]
Epoch [65/120    avg_loss:0.107, val_acc:0.966]
Epoch [66/120    avg_loss:0.104, val_acc:0.966]
Epoch [67/120    avg_loss:0.102, val_acc:0.972]
Epoch [68/120    avg_loss:0.091, val_acc:0.972]
Epoch [69/120    avg_loss:0.085, val_acc:0.950]
Epoch [70/120    avg_loss:0.111, val_acc:0.974]
Epoch [71/120    avg_loss:0.077, val_acc:0.960]
Epoch [72/120    avg_loss:0.083, val_acc:0.960]
Epoch [73/120    avg_loss:0.055, val_acc:0.972]
Epoch [74/120    avg_loss:0.074, val_acc:0.968]
Epoch [75/120    avg_loss:0.054, val_acc:0.982]
Epoch [76/120    avg_loss:0.044, val_acc:0.984]
Epoch [77/120    avg_loss:0.035, val_acc:0.978]
Epoch [78/120    avg_loss:0.030, val_acc:0.984]
Epoch [79/120    avg_loss:0.041, val_acc:0.986]
Epoch [80/120    avg_loss:0.033, val_acc:0.984]
Epoch [81/120    avg_loss:0.040, val_acc:0.982]
Epoch [82/120    avg_loss:0.032, val_acc:0.980]
Epoch [83/120    avg_loss:0.036, val_acc:0.982]
Epoch [84/120    avg_loss:0.029, val_acc:0.982]
Epoch [85/120    avg_loss:0.028, val_acc:0.982]
Epoch [86/120    avg_loss:0.028, val_acc:0.982]
Epoch [87/120    avg_loss:0.032, val_acc:0.984]
Epoch [88/120    avg_loss:0.029, val_acc:0.982]
Epoch [89/120    avg_loss:0.032, val_acc:0.982]
Epoch [90/120    avg_loss:0.035, val_acc:0.982]
Epoch [91/120    avg_loss:0.029, val_acc:0.982]
Epoch [92/120    avg_loss:0.029, val_acc:0.984]
Epoch [93/120    avg_loss:0.026, val_acc:0.984]
Epoch [94/120    avg_loss:0.030, val_acc:0.984]
Epoch [95/120    avg_loss:0.032, val_acc:0.984]
Epoch [96/120    avg_loss:0.026, val_acc:0.984]
Epoch [97/120    avg_loss:0.025, val_acc:0.984]
Epoch [98/120    avg_loss:0.029, val_acc:0.984]
Epoch [99/120    avg_loss:0.031, val_acc:0.984]
Epoch [100/120    avg_loss:0.031, val_acc:0.982]
Epoch [101/120    avg_loss:0.032, val_acc:0.984]
Epoch [102/120    avg_loss:0.036, val_acc:0.984]
Epoch [103/120    avg_loss:0.031, val_acc:0.984]
Epoch [104/120    avg_loss:0.026, val_acc:0.984]
Epoch [105/120    avg_loss:0.031, val_acc:0.984]
Epoch [106/120    avg_loss:0.029, val_acc:0.984]
Epoch [107/120    avg_loss:0.032, val_acc:0.984]
Epoch [108/120    avg_loss:0.028, val_acc:0.984]
Epoch [109/120    avg_loss:0.028, val_acc:0.984]
Epoch [110/120    avg_loss:0.028, val_acc:0.984]
Epoch [111/120    avg_loss:0.031, val_acc:0.984]
Epoch [112/120    avg_loss:0.025, val_acc:0.984]
Epoch [113/120    avg_loss:0.033, val_acc:0.984]
Epoch [114/120    avg_loss:0.030, val_acc:0.984]
Epoch [115/120    avg_loss:0.031, val_acc:0.984]
Epoch [116/120    avg_loss:0.031, val_acc:0.984]
Epoch [117/120    avg_loss:0.031, val_acc:0.984]
Epoch [118/120    avg_loss:0.029, val_acc:0.984]
Epoch [119/120    avg_loss:0.024, val_acc:0.984]
Epoch [120/120    avg_loss:0.029, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   8   0   0   0   0   0   0   1   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.97977528 1.         0.93162393 0.88727273
 1.         0.95027624 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9902662119406432
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efead246978>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.507, val_acc:0.313]
Epoch [2/120    avg_loss:2.202, val_acc:0.419]
Epoch [3/120    avg_loss:1.977, val_acc:0.546]
Epoch [4/120    avg_loss:1.776, val_acc:0.613]
Epoch [5/120    avg_loss:1.557, val_acc:0.659]
Epoch [6/120    avg_loss:1.309, val_acc:0.800]
Epoch [7/120    avg_loss:1.102, val_acc:0.849]
Epoch [8/120    avg_loss:0.964, val_acc:0.837]
Epoch [9/120    avg_loss:0.844, val_acc:0.853]
Epoch [10/120    avg_loss:0.724, val_acc:0.863]
Epoch [11/120    avg_loss:0.670, val_acc:0.859]
Epoch [12/120    avg_loss:0.607, val_acc:0.869]
Epoch [13/120    avg_loss:0.556, val_acc:0.885]
Epoch [14/120    avg_loss:0.551, val_acc:0.855]
Epoch [15/120    avg_loss:0.541, val_acc:0.897]
Epoch [16/120    avg_loss:0.497, val_acc:0.899]
Epoch [17/120    avg_loss:0.488, val_acc:0.841]
Epoch [18/120    avg_loss:0.468, val_acc:0.879]
Epoch [19/120    avg_loss:0.436, val_acc:0.909]
Epoch [20/120    avg_loss:0.461, val_acc:0.883]
Epoch [21/120    avg_loss:0.441, val_acc:0.873]
Epoch [22/120    avg_loss:0.423, val_acc:0.893]
Epoch [23/120    avg_loss:0.339, val_acc:0.905]
Epoch [24/120    avg_loss:0.341, val_acc:0.905]
Epoch [25/120    avg_loss:0.318, val_acc:0.909]
Epoch [26/120    avg_loss:0.325, val_acc:0.899]
Epoch [27/120    avg_loss:0.319, val_acc:0.929]
Epoch [28/120    avg_loss:0.319, val_acc:0.897]
Epoch [29/120    avg_loss:0.319, val_acc:0.887]
Epoch [30/120    avg_loss:0.307, val_acc:0.915]
Epoch [31/120    avg_loss:0.348, val_acc:0.915]
Epoch [32/120    avg_loss:0.378, val_acc:0.923]
Epoch [33/120    avg_loss:0.278, val_acc:0.933]
Epoch [34/120    avg_loss:0.291, val_acc:0.937]
Epoch [35/120    avg_loss:0.272, val_acc:0.938]
Epoch [36/120    avg_loss:0.210, val_acc:0.948]
Epoch [37/120    avg_loss:0.217, val_acc:0.958]
Epoch [38/120    avg_loss:0.302, val_acc:0.931]
Epoch [39/120    avg_loss:0.226, val_acc:0.950]
Epoch [40/120    avg_loss:0.191, val_acc:0.958]
Epoch [41/120    avg_loss:0.190, val_acc:0.937]
Epoch [42/120    avg_loss:0.191, val_acc:0.954]
Epoch [43/120    avg_loss:0.198, val_acc:0.952]
Epoch [44/120    avg_loss:0.177, val_acc:0.931]
Epoch [45/120    avg_loss:0.169, val_acc:0.946]
Epoch [46/120    avg_loss:0.204, val_acc:0.970]
Epoch [47/120    avg_loss:0.163, val_acc:0.946]
Epoch [48/120    avg_loss:0.148, val_acc:0.948]
Epoch [49/120    avg_loss:0.185, val_acc:0.929]
Epoch [50/120    avg_loss:0.176, val_acc:0.958]
Epoch [51/120    avg_loss:0.140, val_acc:0.925]
Epoch [52/120    avg_loss:0.123, val_acc:0.962]
Epoch [53/120    avg_loss:0.115, val_acc:0.966]
Epoch [54/120    avg_loss:0.110, val_acc:0.972]
Epoch [55/120    avg_loss:0.094, val_acc:0.970]
Epoch [56/120    avg_loss:0.091, val_acc:0.948]
Epoch [57/120    avg_loss:0.098, val_acc:0.960]
Epoch [58/120    avg_loss:0.096, val_acc:0.931]
Epoch [59/120    avg_loss:0.101, val_acc:0.964]
Epoch [60/120    avg_loss:0.114, val_acc:0.966]
Epoch [61/120    avg_loss:0.163, val_acc:0.962]
Epoch [62/120    avg_loss:0.145, val_acc:0.968]
Epoch [63/120    avg_loss:0.081, val_acc:0.970]
Epoch [64/120    avg_loss:0.098, val_acc:0.938]
Epoch [65/120    avg_loss:0.154, val_acc:0.972]
Epoch [66/120    avg_loss:0.067, val_acc:0.980]
Epoch [67/120    avg_loss:0.060, val_acc:0.982]
Epoch [68/120    avg_loss:0.104, val_acc:0.960]
Epoch [69/120    avg_loss:0.092, val_acc:0.978]
Epoch [70/120    avg_loss:0.062, val_acc:0.968]
Epoch [71/120    avg_loss:0.060, val_acc:0.927]
Epoch [72/120    avg_loss:0.114, val_acc:0.978]
Epoch [73/120    avg_loss:0.134, val_acc:0.966]
Epoch [74/120    avg_loss:0.076, val_acc:0.972]
Epoch [75/120    avg_loss:0.075, val_acc:0.978]
Epoch [76/120    avg_loss:0.096, val_acc:0.956]
Epoch [77/120    avg_loss:0.088, val_acc:0.972]
Epoch [78/120    avg_loss:0.065, val_acc:0.980]
Epoch [79/120    avg_loss:0.050, val_acc:0.980]
Epoch [80/120    avg_loss:0.066, val_acc:0.980]
Epoch [81/120    avg_loss:0.039, val_acc:0.986]
Epoch [82/120    avg_loss:0.025, val_acc:0.986]
Epoch [83/120    avg_loss:0.024, val_acc:0.988]
Epoch [84/120    avg_loss:0.027, val_acc:0.988]
Epoch [85/120    avg_loss:0.037, val_acc:0.988]
Epoch [86/120    avg_loss:0.022, val_acc:0.992]
Epoch [87/120    avg_loss:0.027, val_acc:0.990]
Epoch [88/120    avg_loss:0.033, val_acc:0.990]
Epoch [89/120    avg_loss:0.021, val_acc:0.988]
Epoch [90/120    avg_loss:0.030, val_acc:0.992]
Epoch [91/120    avg_loss:0.024, val_acc:0.992]
Epoch [92/120    avg_loss:0.021, val_acc:0.992]
Epoch [93/120    avg_loss:0.019, val_acc:0.992]
Epoch [94/120    avg_loss:0.021, val_acc:0.996]
Epoch [95/120    avg_loss:0.020, val_acc:0.992]
Epoch [96/120    avg_loss:0.018, val_acc:0.994]
Epoch [97/120    avg_loss:0.021, val_acc:0.990]
Epoch [98/120    avg_loss:0.020, val_acc:0.994]
Epoch [99/120    avg_loss:0.017, val_acc:0.994]
Epoch [100/120    avg_loss:0.019, val_acc:0.994]
Epoch [101/120    avg_loss:0.019, val_acc:0.990]
Epoch [102/120    avg_loss:0.022, val_acc:0.990]
Epoch [103/120    avg_loss:0.017, val_acc:0.990]
Epoch [104/120    avg_loss:0.017, val_acc:0.990]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.020, val_acc:0.992]
Epoch [107/120    avg_loss:0.018, val_acc:0.994]
Epoch [108/120    avg_loss:0.017, val_acc:0.994]
Epoch [109/120    avg_loss:0.016, val_acc:0.994]
Epoch [110/120    avg_loss:0.018, val_acc:0.994]
Epoch [111/120    avg_loss:0.018, val_acc:0.994]
Epoch [112/120    avg_loss:0.020, val_acc:0.994]
Epoch [113/120    avg_loss:0.017, val_acc:0.994]
Epoch [114/120    avg_loss:0.018, val_acc:0.994]
Epoch [115/120    avg_loss:0.019, val_acc:0.994]
Epoch [116/120    avg_loss:0.015, val_acc:0.994]
Epoch [117/120    avg_loss:0.019, val_acc:0.994]
Epoch [118/120    avg_loss:0.019, val_acc:0.994]
Epoch [119/120    avg_loss:0.019, val_acc:0.994]
Epoch [120/120    avg_loss:0.015, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 218   9   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1   0 363   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.99319728 0.97321429 0.90517241 0.87889273
 1.         0.99465241 0.998713   0.99893276 0.99862448 0.9973545
 0.99778761 1.        ]

Kappa:
0.9878933815836665
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f17849b6a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.544, val_acc:0.393]
Epoch [2/120    avg_loss:2.261, val_acc:0.461]
Epoch [3/120    avg_loss:2.047, val_acc:0.582]
Epoch [4/120    avg_loss:1.814, val_acc:0.725]
Epoch [5/120    avg_loss:1.540, val_acc:0.781]
Epoch [6/120    avg_loss:1.315, val_acc:0.770]
Epoch [7/120    avg_loss:1.138, val_acc:0.799]
Epoch [8/120    avg_loss:1.014, val_acc:0.803]
Epoch [9/120    avg_loss:0.904, val_acc:0.756]
Epoch [10/120    avg_loss:0.803, val_acc:0.865]
Epoch [11/120    avg_loss:0.738, val_acc:0.844]
Epoch [12/120    avg_loss:0.667, val_acc:0.879]
Epoch [13/120    avg_loss:0.601, val_acc:0.863]
Epoch [14/120    avg_loss:0.603, val_acc:0.900]
Epoch [15/120    avg_loss:0.634, val_acc:0.867]
Epoch [16/120    avg_loss:0.531, val_acc:0.889]
Epoch [17/120    avg_loss:0.446, val_acc:0.912]
Epoch [18/120    avg_loss:0.448, val_acc:0.928]
Epoch [19/120    avg_loss:0.499, val_acc:0.863]
Epoch [20/120    avg_loss:0.457, val_acc:0.914]
Epoch [21/120    avg_loss:0.360, val_acc:0.859]
Epoch [22/120    avg_loss:0.374, val_acc:0.934]
Epoch [23/120    avg_loss:0.304, val_acc:0.939]
Epoch [24/120    avg_loss:0.348, val_acc:0.930]
Epoch [25/120    avg_loss:0.439, val_acc:0.928]
Epoch [26/120    avg_loss:0.334, val_acc:0.936]
Epoch [27/120    avg_loss:0.294, val_acc:0.916]
Epoch [28/120    avg_loss:0.311, val_acc:0.953]
Epoch [29/120    avg_loss:0.269, val_acc:0.932]
Epoch [30/120    avg_loss:0.314, val_acc:0.961]
Epoch [31/120    avg_loss:0.256, val_acc:0.941]
Epoch [32/120    avg_loss:0.238, val_acc:0.934]
Epoch [33/120    avg_loss:0.209, val_acc:0.951]
Epoch [34/120    avg_loss:0.213, val_acc:0.957]
Epoch [35/120    avg_loss:0.210, val_acc:0.932]
Epoch [36/120    avg_loss:0.206, val_acc:0.967]
Epoch [37/120    avg_loss:0.161, val_acc:0.949]
Epoch [38/120    avg_loss:0.201, val_acc:0.945]
Epoch [39/120    avg_loss:0.145, val_acc:0.953]
Epoch [40/120    avg_loss:0.214, val_acc:0.953]
Epoch [41/120    avg_loss:0.199, val_acc:0.943]
Epoch [42/120    avg_loss:0.258, val_acc:0.943]
Epoch [43/120    avg_loss:0.200, val_acc:0.957]
Epoch [44/120    avg_loss:0.166, val_acc:0.969]
Epoch [45/120    avg_loss:0.189, val_acc:0.961]
Epoch [46/120    avg_loss:0.159, val_acc:0.961]
Epoch [47/120    avg_loss:0.133, val_acc:0.969]
Epoch [48/120    avg_loss:0.164, val_acc:0.963]
Epoch [49/120    avg_loss:0.135, val_acc:0.977]
Epoch [50/120    avg_loss:0.151, val_acc:0.949]
Epoch [51/120    avg_loss:0.225, val_acc:0.971]
Epoch [52/120    avg_loss:0.127, val_acc:0.979]
Epoch [53/120    avg_loss:0.109, val_acc:0.971]
Epoch [54/120    avg_loss:0.138, val_acc:0.949]
Epoch [55/120    avg_loss:0.131, val_acc:0.971]
Epoch [56/120    avg_loss:0.149, val_acc:0.965]
Epoch [57/120    avg_loss:0.146, val_acc:0.980]
Epoch [58/120    avg_loss:0.113, val_acc:0.979]
Epoch [59/120    avg_loss:0.092, val_acc:0.967]
Epoch [60/120    avg_loss:0.102, val_acc:0.984]
Epoch [61/120    avg_loss:0.113, val_acc:0.973]
Epoch [62/120    avg_loss:0.117, val_acc:0.975]
Epoch [63/120    avg_loss:0.099, val_acc:0.986]
Epoch [64/120    avg_loss:0.137, val_acc:0.975]
Epoch [65/120    avg_loss:0.164, val_acc:0.951]
Epoch [66/120    avg_loss:0.130, val_acc:0.959]
Epoch [67/120    avg_loss:0.091, val_acc:0.980]
Epoch [68/120    avg_loss:0.065, val_acc:0.975]
Epoch [69/120    avg_loss:0.071, val_acc:0.977]
Epoch [70/120    avg_loss:0.062, val_acc:0.979]
Epoch [71/120    avg_loss:0.062, val_acc:0.973]
Epoch [72/120    avg_loss:0.053, val_acc:0.963]
Epoch [73/120    avg_loss:0.056, val_acc:0.988]
Epoch [74/120    avg_loss:0.052, val_acc:0.988]
Epoch [75/120    avg_loss:0.058, val_acc:0.980]
Epoch [76/120    avg_loss:0.035, val_acc:0.973]
Epoch [77/120    avg_loss:0.051, val_acc:0.982]
Epoch [78/120    avg_loss:0.051, val_acc:0.963]
Epoch [79/120    avg_loss:0.035, val_acc:0.984]
Epoch [80/120    avg_loss:0.040, val_acc:0.986]
Epoch [81/120    avg_loss:0.084, val_acc:0.984]
Epoch [82/120    avg_loss:0.072, val_acc:0.984]
Epoch [83/120    avg_loss:0.055, val_acc:0.953]
Epoch [84/120    avg_loss:0.052, val_acc:0.986]
Epoch [85/120    avg_loss:0.044, val_acc:0.980]
Epoch [86/120    avg_loss:0.077, val_acc:0.986]
Epoch [87/120    avg_loss:0.061, val_acc:0.986]
Epoch [88/120    avg_loss:0.045, val_acc:0.990]
Epoch [89/120    avg_loss:0.033, val_acc:0.992]
Epoch [90/120    avg_loss:0.022, val_acc:0.992]
Epoch [91/120    avg_loss:0.023, val_acc:0.990]
Epoch [92/120    avg_loss:0.020, val_acc:0.990]
Epoch [93/120    avg_loss:0.020, val_acc:0.992]
Epoch [94/120    avg_loss:0.023, val_acc:0.994]
Epoch [95/120    avg_loss:0.022, val_acc:0.994]
Epoch [96/120    avg_loss:0.026, val_acc:0.992]
Epoch [97/120    avg_loss:0.028, val_acc:0.994]
Epoch [98/120    avg_loss:0.023, val_acc:0.992]
Epoch [99/120    avg_loss:0.018, val_acc:0.992]
Epoch [100/120    avg_loss:0.018, val_acc:0.994]
Epoch [101/120    avg_loss:0.017, val_acc:0.996]
Epoch [102/120    avg_loss:0.016, val_acc:0.996]
Epoch [103/120    avg_loss:0.018, val_acc:0.996]
Epoch [104/120    avg_loss:0.023, val_acc:0.996]
Epoch [105/120    avg_loss:0.018, val_acc:0.996]
Epoch [106/120    avg_loss:0.023, val_acc:0.994]
Epoch [107/120    avg_loss:0.019, val_acc:0.994]
Epoch [108/120    avg_loss:0.021, val_acc:0.992]
Epoch [109/120    avg_loss:0.018, val_acc:0.994]
Epoch [110/120    avg_loss:0.015, val_acc:0.994]
Epoch [111/120    avg_loss:0.013, val_acc:0.994]
Epoch [112/120    avg_loss:0.018, val_acc:0.994]
Epoch [113/120    avg_loss:0.016, val_acc:0.992]
Epoch [114/120    avg_loss:0.013, val_acc:0.992]
Epoch [115/120    avg_loss:0.026, val_acc:0.996]
Epoch [116/120    avg_loss:0.018, val_acc:0.992]
Epoch [117/120    avg_loss:0.013, val_acc:0.992]
Epoch [118/120    avg_loss:0.020, val_acc:0.994]
Epoch [119/120    avg_loss:0.017, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   1   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99927061 0.99095023 0.99122807 0.95302013 0.93288591
 0.99756691 0.97826087 0.99614891 1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9924035758263642
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa4327faa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.544, val_acc:0.375]
Epoch [2/120    avg_loss:2.228, val_acc:0.397]
Epoch [3/120    avg_loss:2.015, val_acc:0.550]
Epoch [4/120    avg_loss:1.785, val_acc:0.702]
Epoch [5/120    avg_loss:1.527, val_acc:0.772]
Epoch [6/120    avg_loss:1.261, val_acc:0.790]
Epoch [7/120    avg_loss:1.041, val_acc:0.810]
Epoch [8/120    avg_loss:0.935, val_acc:0.845]
Epoch [9/120    avg_loss:0.851, val_acc:0.863]
Epoch [10/120    avg_loss:0.706, val_acc:0.812]
Epoch [11/120    avg_loss:0.674, val_acc:0.861]
Epoch [12/120    avg_loss:0.704, val_acc:0.831]
Epoch [13/120    avg_loss:0.629, val_acc:0.897]
Epoch [14/120    avg_loss:0.518, val_acc:0.893]
Epoch [15/120    avg_loss:0.569, val_acc:0.895]
Epoch [16/120    avg_loss:0.534, val_acc:0.839]
Epoch [17/120    avg_loss:0.437, val_acc:0.901]
Epoch [18/120    avg_loss:0.455, val_acc:0.913]
Epoch [19/120    avg_loss:0.391, val_acc:0.927]
Epoch [20/120    avg_loss:0.371, val_acc:0.849]
Epoch [21/120    avg_loss:0.409, val_acc:0.905]
Epoch [22/120    avg_loss:0.418, val_acc:0.935]
Epoch [23/120    avg_loss:0.375, val_acc:0.923]
Epoch [24/120    avg_loss:0.350, val_acc:0.938]
Epoch [25/120    avg_loss:0.278, val_acc:0.946]
Epoch [26/120    avg_loss:0.363, val_acc:0.923]
Epoch [27/120    avg_loss:0.294, val_acc:0.938]
Epoch [28/120    avg_loss:0.252, val_acc:0.950]
Epoch [29/120    avg_loss:0.308, val_acc:0.937]
Epoch [30/120    avg_loss:0.261, val_acc:0.927]
Epoch [31/120    avg_loss:0.269, val_acc:0.952]
Epoch [32/120    avg_loss:0.242, val_acc:0.956]
Epoch [33/120    avg_loss:0.252, val_acc:0.948]
Epoch [34/120    avg_loss:0.234, val_acc:0.923]
Epoch [35/120    avg_loss:0.282, val_acc:0.929]
Epoch [36/120    avg_loss:0.237, val_acc:0.954]
Epoch [37/120    avg_loss:0.252, val_acc:0.950]
Epoch [38/120    avg_loss:0.253, val_acc:0.921]
Epoch [39/120    avg_loss:0.262, val_acc:0.958]
Epoch [40/120    avg_loss:0.203, val_acc:0.960]
Epoch [41/120    avg_loss:0.240, val_acc:0.968]
Epoch [42/120    avg_loss:0.217, val_acc:0.956]
Epoch [43/120    avg_loss:0.186, val_acc:0.970]
Epoch [44/120    avg_loss:0.192, val_acc:0.970]
Epoch [45/120    avg_loss:0.164, val_acc:0.944]
Epoch [46/120    avg_loss:0.195, val_acc:0.966]
Epoch [47/120    avg_loss:0.154, val_acc:0.970]
Epoch [48/120    avg_loss:0.149, val_acc:0.978]
Epoch [49/120    avg_loss:0.149, val_acc:0.978]
Epoch [50/120    avg_loss:0.135, val_acc:0.976]
Epoch [51/120    avg_loss:0.128, val_acc:0.966]
Epoch [52/120    avg_loss:0.141, val_acc:0.972]
Epoch [53/120    avg_loss:0.138, val_acc:0.966]
Epoch [54/120    avg_loss:0.120, val_acc:0.978]
Epoch [55/120    avg_loss:0.095, val_acc:0.978]
Epoch [56/120    avg_loss:0.096, val_acc:0.978]
Epoch [57/120    avg_loss:0.088, val_acc:0.978]
Epoch [58/120    avg_loss:0.149, val_acc:0.964]
Epoch [59/120    avg_loss:0.124, val_acc:0.901]
Epoch [60/120    avg_loss:0.159, val_acc:0.937]
Epoch [61/120    avg_loss:0.112, val_acc:0.982]
Epoch [62/120    avg_loss:0.104, val_acc:0.978]
Epoch [63/120    avg_loss:0.099, val_acc:0.974]
Epoch [64/120    avg_loss:0.097, val_acc:0.968]
Epoch [65/120    avg_loss:0.081, val_acc:0.956]
Epoch [66/120    avg_loss:0.121, val_acc:0.974]
Epoch [67/120    avg_loss:0.085, val_acc:0.964]
Epoch [68/120    avg_loss:0.110, val_acc:0.972]
Epoch [69/120    avg_loss:0.078, val_acc:0.970]
Epoch [70/120    avg_loss:0.128, val_acc:0.962]
Epoch [71/120    avg_loss:0.113, val_acc:0.970]
Epoch [72/120    avg_loss:0.108, val_acc:0.984]
Epoch [73/120    avg_loss:0.064, val_acc:0.976]
Epoch [74/120    avg_loss:0.075, val_acc:0.974]
Epoch [75/120    avg_loss:0.058, val_acc:0.978]
Epoch [76/120    avg_loss:0.065, val_acc:0.986]
Epoch [77/120    avg_loss:0.069, val_acc:0.974]
Epoch [78/120    avg_loss:0.062, val_acc:0.968]
Epoch [79/120    avg_loss:0.068, val_acc:0.986]
Epoch [80/120    avg_loss:0.114, val_acc:0.974]
Epoch [81/120    avg_loss:0.101, val_acc:0.984]
Epoch [82/120    avg_loss:0.065, val_acc:0.972]
Epoch [83/120    avg_loss:0.062, val_acc:0.982]
Epoch [84/120    avg_loss:0.052, val_acc:0.984]
Epoch [85/120    avg_loss:0.053, val_acc:0.978]
Epoch [86/120    avg_loss:0.027, val_acc:0.986]
Epoch [87/120    avg_loss:0.049, val_acc:0.970]
Epoch [88/120    avg_loss:0.035, val_acc:0.986]
Epoch [89/120    avg_loss:0.032, val_acc:0.986]
Epoch [90/120    avg_loss:0.046, val_acc:0.980]
Epoch [91/120    avg_loss:0.044, val_acc:0.982]
Epoch [92/120    avg_loss:0.059, val_acc:0.990]
Epoch [93/120    avg_loss:0.055, val_acc:0.984]
Epoch [94/120    avg_loss:0.047, val_acc:0.992]
Epoch [95/120    avg_loss:0.099, val_acc:0.931]
Epoch [96/120    avg_loss:0.255, val_acc:0.925]
Epoch [97/120    avg_loss:0.152, val_acc:0.968]
Epoch [98/120    avg_loss:0.095, val_acc:0.962]
Epoch [99/120    avg_loss:0.140, val_acc:0.982]
Epoch [100/120    avg_loss:0.065, val_acc:0.978]
Epoch [101/120    avg_loss:0.040, val_acc:0.976]
Epoch [102/120    avg_loss:0.053, val_acc:0.988]
Epoch [103/120    avg_loss:0.058, val_acc:0.976]
Epoch [104/120    avg_loss:0.075, val_acc:0.988]
Epoch [105/120    avg_loss:0.059, val_acc:0.960]
Epoch [106/120    avg_loss:0.067, val_acc:0.976]
Epoch [107/120    avg_loss:0.063, val_acc:0.992]
Epoch [108/120    avg_loss:0.046, val_acc:0.992]
Epoch [109/120    avg_loss:0.027, val_acc:0.978]
Epoch [110/120    avg_loss:0.025, val_acc:0.990]
Epoch [111/120    avg_loss:0.045, val_acc:0.978]
Epoch [112/120    avg_loss:0.040, val_acc:0.982]
Epoch [113/120    avg_loss:0.025, val_acc:0.982]
Epoch [114/120    avg_loss:0.035, val_acc:0.990]
Epoch [115/120    avg_loss:0.043, val_acc:0.982]
Epoch [116/120    avg_loss:0.083, val_acc:0.980]
Epoch [117/120    avg_loss:0.065, val_acc:0.986]
Epoch [118/120    avg_loss:0.029, val_acc:0.990]
Epoch [119/120    avg_loss:0.023, val_acc:0.990]
Epoch [120/120    avg_loss:0.023, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 221   4   0   0   0   3   2   0   0   0   0]
 [  0   0   0   0 212  14   0   0   1   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   7   0   0   0   0 199   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.99491649 0.99316629 0.98004435 0.92374728 0.89583333
 0.98271605 0.98395722 0.99487179 0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.9881280784411524
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe039334ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.479, val_acc:0.403]
Epoch [2/120    avg_loss:2.214, val_acc:0.605]
Epoch [3/120    avg_loss:1.983, val_acc:0.694]
Epoch [4/120    avg_loss:1.745, val_acc:0.704]
Epoch [5/120    avg_loss:1.498, val_acc:0.732]
Epoch [6/120    avg_loss:1.256, val_acc:0.760]
Epoch [7/120    avg_loss:1.110, val_acc:0.762]
Epoch [8/120    avg_loss:0.983, val_acc:0.772]
Epoch [9/120    avg_loss:0.866, val_acc:0.855]
Epoch [10/120    avg_loss:0.796, val_acc:0.794]
Epoch [11/120    avg_loss:0.787, val_acc:0.788]
Epoch [12/120    avg_loss:0.686, val_acc:0.863]
Epoch [13/120    avg_loss:0.672, val_acc:0.798]
Epoch [14/120    avg_loss:0.678, val_acc:0.875]
Epoch [15/120    avg_loss:0.565, val_acc:0.893]
Epoch [16/120    avg_loss:0.482, val_acc:0.911]
Epoch [17/120    avg_loss:0.409, val_acc:0.893]
Epoch [18/120    avg_loss:0.456, val_acc:0.895]
Epoch [19/120    avg_loss:0.439, val_acc:0.881]
Epoch [20/120    avg_loss:0.421, val_acc:0.873]
Epoch [21/120    avg_loss:0.427, val_acc:0.875]
Epoch [22/120    avg_loss:0.406, val_acc:0.895]
Epoch [23/120    avg_loss:0.448, val_acc:0.895]
Epoch [24/120    avg_loss:0.409, val_acc:0.907]
Epoch [25/120    avg_loss:0.369, val_acc:0.897]
Epoch [26/120    avg_loss:0.325, val_acc:0.933]
Epoch [27/120    avg_loss:0.312, val_acc:0.891]
Epoch [28/120    avg_loss:0.311, val_acc:0.933]
Epoch [29/120    avg_loss:0.342, val_acc:0.927]
Epoch [30/120    avg_loss:0.320, val_acc:0.927]
Epoch [31/120    avg_loss:0.265, val_acc:0.935]
Epoch [32/120    avg_loss:0.275, val_acc:0.944]
Epoch [33/120    avg_loss:0.241, val_acc:0.911]
Epoch [34/120    avg_loss:0.332, val_acc:0.913]
Epoch [35/120    avg_loss:0.321, val_acc:0.927]
Epoch [36/120    avg_loss:0.263, val_acc:0.942]
Epoch [37/120    avg_loss:0.274, val_acc:0.915]
Epoch [38/120    avg_loss:0.245, val_acc:0.927]
Epoch [39/120    avg_loss:0.253, val_acc:0.933]
Epoch [40/120    avg_loss:0.235, val_acc:0.940]
Epoch [41/120    avg_loss:0.203, val_acc:0.935]
Epoch [42/120    avg_loss:0.189, val_acc:0.942]
Epoch [43/120    avg_loss:0.199, val_acc:0.938]
Epoch [44/120    avg_loss:0.180, val_acc:0.948]
Epoch [45/120    avg_loss:0.203, val_acc:0.956]
Epoch [46/120    avg_loss:0.201, val_acc:0.919]
Epoch [47/120    avg_loss:0.184, val_acc:0.927]
Epoch [48/120    avg_loss:0.186, val_acc:0.956]
Epoch [49/120    avg_loss:0.178, val_acc:0.962]
Epoch [50/120    avg_loss:0.151, val_acc:0.962]
Epoch [51/120    avg_loss:0.128, val_acc:0.974]
Epoch [52/120    avg_loss:0.143, val_acc:0.956]
Epoch [53/120    avg_loss:0.166, val_acc:0.958]
Epoch [54/120    avg_loss:0.098, val_acc:0.962]
Epoch [55/120    avg_loss:0.166, val_acc:0.968]
Epoch [56/120    avg_loss:0.149, val_acc:0.958]
Epoch [57/120    avg_loss:0.163, val_acc:0.940]
Epoch [58/120    avg_loss:0.098, val_acc:0.966]
Epoch [59/120    avg_loss:0.088, val_acc:0.964]
Epoch [60/120    avg_loss:0.112, val_acc:0.942]
Epoch [61/120    avg_loss:0.133, val_acc:0.946]
Epoch [62/120    avg_loss:0.093, val_acc:0.974]
Epoch [63/120    avg_loss:0.138, val_acc:0.964]
Epoch [64/120    avg_loss:0.096, val_acc:0.962]
Epoch [65/120    avg_loss:0.187, val_acc:0.948]
Epoch [66/120    avg_loss:0.135, val_acc:0.962]
Epoch [67/120    avg_loss:0.123, val_acc:0.952]
Epoch [68/120    avg_loss:0.086, val_acc:0.964]
Epoch [69/120    avg_loss:0.058, val_acc:0.972]
Epoch [70/120    avg_loss:0.103, val_acc:0.929]
Epoch [71/120    avg_loss:0.064, val_acc:0.976]
Epoch [72/120    avg_loss:0.079, val_acc:0.968]
Epoch [73/120    avg_loss:0.051, val_acc:0.976]
Epoch [74/120    avg_loss:0.063, val_acc:0.960]
Epoch [75/120    avg_loss:0.095, val_acc:0.968]
Epoch [76/120    avg_loss:0.079, val_acc:0.984]
Epoch [77/120    avg_loss:0.144, val_acc:0.956]
Epoch [78/120    avg_loss:0.084, val_acc:0.980]
Epoch [79/120    avg_loss:0.080, val_acc:0.978]
Epoch [80/120    avg_loss:0.066, val_acc:0.974]
Epoch [81/120    avg_loss:0.043, val_acc:0.990]
Epoch [82/120    avg_loss:0.040, val_acc:0.976]
Epoch [83/120    avg_loss:0.046, val_acc:0.982]
Epoch [84/120    avg_loss:0.046, val_acc:0.974]
Epoch [85/120    avg_loss:0.103, val_acc:0.984]
Epoch [86/120    avg_loss:0.041, val_acc:0.976]
Epoch [87/120    avg_loss:0.145, val_acc:0.968]
Epoch [88/120    avg_loss:0.057, val_acc:0.972]
Epoch [89/120    avg_loss:0.060, val_acc:0.978]
Epoch [90/120    avg_loss:0.051, val_acc:0.988]
Epoch [91/120    avg_loss:0.033, val_acc:0.980]
Epoch [92/120    avg_loss:0.043, val_acc:0.982]
Epoch [93/120    avg_loss:0.087, val_acc:0.980]
Epoch [94/120    avg_loss:0.052, val_acc:0.968]
Epoch [95/120    avg_loss:0.036, val_acc:0.984]
Epoch [96/120    avg_loss:0.031, val_acc:0.986]
Epoch [97/120    avg_loss:0.023, val_acc:0.986]
Epoch [98/120    avg_loss:0.015, val_acc:0.988]
Epoch [99/120    avg_loss:0.022, val_acc:0.984]
Epoch [100/120    avg_loss:0.020, val_acc:0.986]
Epoch [101/120    avg_loss:0.018, val_acc:0.986]
Epoch [102/120    avg_loss:0.021, val_acc:0.988]
Epoch [103/120    avg_loss:0.022, val_acc:0.988]
Epoch [104/120    avg_loss:0.020, val_acc:0.988]
Epoch [105/120    avg_loss:0.018, val_acc:0.986]
Epoch [106/120    avg_loss:0.013, val_acc:0.986]
Epoch [107/120    avg_loss:0.022, val_acc:0.986]
Epoch [108/120    avg_loss:0.013, val_acc:0.986]
Epoch [109/120    avg_loss:0.018, val_acc:0.988]
Epoch [110/120    avg_loss:0.016, val_acc:0.988]
Epoch [111/120    avg_loss:0.017, val_acc:0.988]
Epoch [112/120    avg_loss:0.014, val_acc:0.988]
Epoch [113/120    avg_loss:0.019, val_acc:0.988]
Epoch [114/120    avg_loss:0.017, val_acc:0.988]
Epoch [115/120    avg_loss:0.021, val_acc:0.988]
Epoch [116/120    avg_loss:0.017, val_acc:0.988]
Epoch [117/120    avg_loss:0.020, val_acc:0.988]
Epoch [118/120    avg_loss:0.016, val_acc:0.988]
Epoch [119/120    avg_loss:0.016, val_acc:0.988]
Epoch [120/120    avg_loss:0.019, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 216  12   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.98426966 0.96860987 0.93361884 0.93425606
 1.         0.96703297 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9907416438793135
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f41c86c4b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.524, val_acc:0.321]
Epoch [2/120    avg_loss:2.243, val_acc:0.417]
Epoch [3/120    avg_loss:2.064, val_acc:0.639]
Epoch [4/120    avg_loss:1.886, val_acc:0.714]
Epoch [5/120    avg_loss:1.636, val_acc:0.673]
Epoch [6/120    avg_loss:1.413, val_acc:0.740]
Epoch [7/120    avg_loss:1.200, val_acc:0.667]
Epoch [8/120    avg_loss:1.083, val_acc:0.790]
Epoch [9/120    avg_loss:0.956, val_acc:0.748]
Epoch [10/120    avg_loss:0.795, val_acc:0.740]
Epoch [11/120    avg_loss:0.824, val_acc:0.819]
Epoch [12/120    avg_loss:0.666, val_acc:0.867]
Epoch [13/120    avg_loss:0.637, val_acc:0.835]
Epoch [14/120    avg_loss:0.613, val_acc:0.873]
Epoch [15/120    avg_loss:0.514, val_acc:0.835]
Epoch [16/120    avg_loss:0.625, val_acc:0.833]
Epoch [17/120    avg_loss:0.476, val_acc:0.913]
Epoch [18/120    avg_loss:0.431, val_acc:0.863]
Epoch [19/120    avg_loss:0.493, val_acc:0.891]
Epoch [20/120    avg_loss:0.408, val_acc:0.887]
Epoch [21/120    avg_loss:0.412, val_acc:0.877]
Epoch [22/120    avg_loss:0.371, val_acc:0.911]
Epoch [23/120    avg_loss:0.335, val_acc:0.917]
Epoch [24/120    avg_loss:0.324, val_acc:0.909]
Epoch [25/120    avg_loss:0.373, val_acc:0.917]
Epoch [26/120    avg_loss:0.287, val_acc:0.907]
Epoch [27/120    avg_loss:0.326, val_acc:0.929]
Epoch [28/120    avg_loss:0.327, val_acc:0.919]
Epoch [29/120    avg_loss:0.265, val_acc:0.909]
Epoch [30/120    avg_loss:0.266, val_acc:0.925]
Epoch [31/120    avg_loss:0.204, val_acc:0.931]
Epoch [32/120    avg_loss:0.249, val_acc:0.919]
Epoch [33/120    avg_loss:0.245, val_acc:0.925]
Epoch [34/120    avg_loss:0.243, val_acc:0.907]
Epoch [35/120    avg_loss:0.182, val_acc:0.931]
Epoch [36/120    avg_loss:0.209, val_acc:0.942]
Epoch [37/120    avg_loss:0.222, val_acc:0.927]
Epoch [38/120    avg_loss:0.201, val_acc:0.938]
Epoch [39/120    avg_loss:0.209, val_acc:0.946]
Epoch [40/120    avg_loss:0.191, val_acc:0.944]
Epoch [41/120    avg_loss:0.207, val_acc:0.946]
Epoch [42/120    avg_loss:0.228, val_acc:0.909]
Epoch [43/120    avg_loss:0.210, val_acc:0.950]
Epoch [44/120    avg_loss:0.191, val_acc:0.931]
Epoch [45/120    avg_loss:0.168, val_acc:0.946]
Epoch [46/120    avg_loss:0.197, val_acc:0.929]
Epoch [47/120    avg_loss:0.185, val_acc:0.954]
Epoch [48/120    avg_loss:0.131, val_acc:0.952]
Epoch [49/120    avg_loss:0.123, val_acc:0.952]
Epoch [50/120    avg_loss:0.101, val_acc:0.948]
Epoch [51/120    avg_loss:0.127, val_acc:0.952]
Epoch [52/120    avg_loss:0.116, val_acc:0.919]
Epoch [53/120    avg_loss:0.116, val_acc:0.948]
Epoch [54/120    avg_loss:0.112, val_acc:0.968]
Epoch [55/120    avg_loss:0.116, val_acc:0.935]
Epoch [56/120    avg_loss:0.106, val_acc:0.954]
Epoch [57/120    avg_loss:0.087, val_acc:0.962]
Epoch [58/120    avg_loss:0.073, val_acc:0.958]
Epoch [59/120    avg_loss:0.088, val_acc:0.931]
Epoch [60/120    avg_loss:0.109, val_acc:0.933]
Epoch [61/120    avg_loss:0.212, val_acc:0.944]
Epoch [62/120    avg_loss:0.125, val_acc:0.944]
Epoch [63/120    avg_loss:0.168, val_acc:0.948]
Epoch [64/120    avg_loss:0.082, val_acc:0.966]
Epoch [65/120    avg_loss:0.099, val_acc:0.942]
Epoch [66/120    avg_loss:0.096, val_acc:0.950]
Epoch [67/120    avg_loss:0.086, val_acc:0.960]
Epoch [68/120    avg_loss:0.063, val_acc:0.974]
Epoch [69/120    avg_loss:0.050, val_acc:0.972]
Epoch [70/120    avg_loss:0.037, val_acc:0.976]
Epoch [71/120    avg_loss:0.041, val_acc:0.978]
Epoch [72/120    avg_loss:0.044, val_acc:0.976]
Epoch [73/120    avg_loss:0.038, val_acc:0.976]
Epoch [74/120    avg_loss:0.036, val_acc:0.976]
Epoch [75/120    avg_loss:0.038, val_acc:0.976]
Epoch [76/120    avg_loss:0.034, val_acc:0.972]
Epoch [77/120    avg_loss:0.039, val_acc:0.974]
Epoch [78/120    avg_loss:0.038, val_acc:0.976]
Epoch [79/120    avg_loss:0.032, val_acc:0.978]
Epoch [80/120    avg_loss:0.032, val_acc:0.972]
Epoch [81/120    avg_loss:0.032, val_acc:0.972]
Epoch [82/120    avg_loss:0.040, val_acc:0.976]
Epoch [83/120    avg_loss:0.031, val_acc:0.976]
Epoch [84/120    avg_loss:0.036, val_acc:0.978]
Epoch [85/120    avg_loss:0.035, val_acc:0.978]
Epoch [86/120    avg_loss:0.038, val_acc:0.976]
Epoch [87/120    avg_loss:0.036, val_acc:0.976]
Epoch [88/120    avg_loss:0.029, val_acc:0.976]
Epoch [89/120    avg_loss:0.027, val_acc:0.976]
Epoch [90/120    avg_loss:0.030, val_acc:0.976]
Epoch [91/120    avg_loss:0.033, val_acc:0.980]
Epoch [92/120    avg_loss:0.032, val_acc:0.978]
Epoch [93/120    avg_loss:0.031, val_acc:0.980]
Epoch [94/120    avg_loss:0.035, val_acc:0.976]
Epoch [95/120    avg_loss:0.035, val_acc:0.974]
Epoch [96/120    avg_loss:0.039, val_acc:0.976]
Epoch [97/120    avg_loss:0.027, val_acc:0.974]
Epoch [98/120    avg_loss:0.034, val_acc:0.976]
Epoch [99/120    avg_loss:0.028, val_acc:0.976]
Epoch [100/120    avg_loss:0.032, val_acc:0.978]
Epoch [101/120    avg_loss:0.028, val_acc:0.974]
Epoch [102/120    avg_loss:0.026, val_acc:0.976]
Epoch [103/120    avg_loss:0.031, val_acc:0.976]
Epoch [104/120    avg_loss:0.029, val_acc:0.978]
Epoch [105/120    avg_loss:0.028, val_acc:0.976]
Epoch [106/120    avg_loss:0.026, val_acc:0.978]
Epoch [107/120    avg_loss:0.033, val_acc:0.978]
Epoch [108/120    avg_loss:0.025, val_acc:0.978]
Epoch [109/120    avg_loss:0.029, val_acc:0.976]
Epoch [110/120    avg_loss:0.028, val_acc:0.976]
Epoch [111/120    avg_loss:0.022, val_acc:0.978]
Epoch [112/120    avg_loss:0.027, val_acc:0.976]
Epoch [113/120    avg_loss:0.031, val_acc:0.976]
Epoch [114/120    avg_loss:0.031, val_acc:0.976]
Epoch [115/120    avg_loss:0.024, val_acc:0.976]
Epoch [116/120    avg_loss:0.030, val_acc:0.976]
Epoch [117/120    avg_loss:0.029, val_acc:0.976]
Epoch [118/120    avg_loss:0.028, val_acc:0.978]
Epoch [119/120    avg_loss:0.025, val_acc:0.976]
Epoch [120/120    avg_loss:0.022, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   2 221   3   0   0   0   2   2   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.97297297 0.98004435 0.93589744 0.91489362
 0.99266504 0.94565217 0.99742931 0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.9890793586514978
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fedec931a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.549, val_acc:0.339]
Epoch [2/120    avg_loss:2.299, val_acc:0.548]
Epoch [3/120    avg_loss:2.054, val_acc:0.631]
Epoch [4/120    avg_loss:1.824, val_acc:0.683]
Epoch [5/120    avg_loss:1.583, val_acc:0.746]
Epoch [6/120    avg_loss:1.342, val_acc:0.776]
Epoch [7/120    avg_loss:1.152, val_acc:0.776]
Epoch [8/120    avg_loss:1.006, val_acc:0.833]
Epoch [9/120    avg_loss:0.872, val_acc:0.839]
Epoch [10/120    avg_loss:0.823, val_acc:0.835]
Epoch [11/120    avg_loss:0.770, val_acc:0.843]
Epoch [12/120    avg_loss:0.619, val_acc:0.833]
Epoch [13/120    avg_loss:0.617, val_acc:0.855]
Epoch [14/120    avg_loss:0.531, val_acc:0.837]
Epoch [15/120    avg_loss:0.510, val_acc:0.859]
Epoch [16/120    avg_loss:0.572, val_acc:0.835]
Epoch [17/120    avg_loss:0.459, val_acc:0.853]
Epoch [18/120    avg_loss:0.442, val_acc:0.825]
Epoch [19/120    avg_loss:0.475, val_acc:0.901]
Epoch [20/120    avg_loss:0.410, val_acc:0.887]
Epoch [21/120    avg_loss:0.466, val_acc:0.895]
Epoch [22/120    avg_loss:0.423, val_acc:0.879]
Epoch [23/120    avg_loss:0.358, val_acc:0.889]
Epoch [24/120    avg_loss:0.349, val_acc:0.907]
Epoch [25/120    avg_loss:0.374, val_acc:0.863]
Epoch [26/120    avg_loss:0.366, val_acc:0.903]
Epoch [27/120    avg_loss:0.347, val_acc:0.907]
Epoch [28/120    avg_loss:0.321, val_acc:0.911]
Epoch [29/120    avg_loss:0.336, val_acc:0.901]
Epoch [30/120    avg_loss:0.283, val_acc:0.897]
Epoch [31/120    avg_loss:0.324, val_acc:0.905]
Epoch [32/120    avg_loss:0.395, val_acc:0.857]
Epoch [33/120    avg_loss:0.327, val_acc:0.933]
Epoch [34/120    avg_loss:0.269, val_acc:0.927]
Epoch [35/120    avg_loss:0.287, val_acc:0.927]
Epoch [36/120    avg_loss:0.319, val_acc:0.901]
Epoch [37/120    avg_loss:0.270, val_acc:0.865]
Epoch [38/120    avg_loss:0.301, val_acc:0.923]
Epoch [39/120    avg_loss:0.266, val_acc:0.937]
Epoch [40/120    avg_loss:0.257, val_acc:0.923]
Epoch [41/120    avg_loss:0.265, val_acc:0.929]
Epoch [42/120    avg_loss:0.275, val_acc:0.875]
Epoch [43/120    avg_loss:0.210, val_acc:0.919]
Epoch [44/120    avg_loss:0.227, val_acc:0.917]
Epoch [45/120    avg_loss:0.229, val_acc:0.925]
Epoch [46/120    avg_loss:0.206, val_acc:0.937]
Epoch [47/120    avg_loss:0.204, val_acc:0.933]
Epoch [48/120    avg_loss:0.191, val_acc:0.944]
Epoch [49/120    avg_loss:0.167, val_acc:0.933]
Epoch [50/120    avg_loss:0.143, val_acc:0.931]
Epoch [51/120    avg_loss:0.179, val_acc:0.923]
Epoch [52/120    avg_loss:0.177, val_acc:0.940]
Epoch [53/120    avg_loss:0.208, val_acc:0.948]
Epoch [54/120    avg_loss:0.166, val_acc:0.929]
Epoch [55/120    avg_loss:0.150, val_acc:0.937]
Epoch [56/120    avg_loss:0.152, val_acc:0.940]
Epoch [57/120    avg_loss:0.124, val_acc:0.952]
Epoch [58/120    avg_loss:0.148, val_acc:0.944]
Epoch [59/120    avg_loss:0.153, val_acc:0.954]
Epoch [60/120    avg_loss:0.138, val_acc:0.958]
Epoch [61/120    avg_loss:0.129, val_acc:0.962]
Epoch [62/120    avg_loss:0.178, val_acc:0.962]
Epoch [63/120    avg_loss:0.146, val_acc:0.964]
Epoch [64/120    avg_loss:0.091, val_acc:0.966]
Epoch [65/120    avg_loss:0.105, val_acc:0.970]
Epoch [66/120    avg_loss:0.097, val_acc:0.952]
Epoch [67/120    avg_loss:0.088, val_acc:0.937]
Epoch [68/120    avg_loss:0.128, val_acc:0.952]
Epoch [69/120    avg_loss:0.090, val_acc:0.962]
Epoch [70/120    avg_loss:0.093, val_acc:0.960]
Epoch [71/120    avg_loss:0.069, val_acc:0.960]
Epoch [72/120    avg_loss:0.082, val_acc:0.946]
Epoch [73/120    avg_loss:0.104, val_acc:0.958]
Epoch [74/120    avg_loss:0.059, val_acc:0.968]
Epoch [75/120    avg_loss:0.083, val_acc:0.956]
Epoch [76/120    avg_loss:0.073, val_acc:0.962]
Epoch [77/120    avg_loss:0.059, val_acc:0.964]
Epoch [78/120    avg_loss:0.051, val_acc:0.960]
Epoch [79/120    avg_loss:0.055, val_acc:0.968]
Epoch [80/120    avg_loss:0.043, val_acc:0.966]
Epoch [81/120    avg_loss:0.048, val_acc:0.966]
Epoch [82/120    avg_loss:0.042, val_acc:0.972]
Epoch [83/120    avg_loss:0.030, val_acc:0.970]
Epoch [84/120    avg_loss:0.033, val_acc:0.976]
Epoch [85/120    avg_loss:0.033, val_acc:0.978]
Epoch [86/120    avg_loss:0.032, val_acc:0.970]
Epoch [87/120    avg_loss:0.040, val_acc:0.972]
Epoch [88/120    avg_loss:0.029, val_acc:0.976]
Epoch [89/120    avg_loss:0.032, val_acc:0.976]
Epoch [90/120    avg_loss:0.029, val_acc:0.972]
Epoch [91/120    avg_loss:0.033, val_acc:0.974]
Epoch [92/120    avg_loss:0.032, val_acc:0.972]
Epoch [93/120    avg_loss:0.037, val_acc:0.972]
Epoch [94/120    avg_loss:0.031, val_acc:0.970]
Epoch [95/120    avg_loss:0.031, val_acc:0.970]
Epoch [96/120    avg_loss:0.035, val_acc:0.972]
Epoch [97/120    avg_loss:0.030, val_acc:0.976]
Epoch [98/120    avg_loss:0.026, val_acc:0.974]
Epoch [99/120    avg_loss:0.028, val_acc:0.974]
Epoch [100/120    avg_loss:0.026, val_acc:0.972]
Epoch [101/120    avg_loss:0.029, val_acc:0.972]
Epoch [102/120    avg_loss:0.024, val_acc:0.972]
Epoch [103/120    avg_loss:0.026, val_acc:0.972]
Epoch [104/120    avg_loss:0.027, val_acc:0.972]
Epoch [105/120    avg_loss:0.027, val_acc:0.972]
Epoch [106/120    avg_loss:0.024, val_acc:0.972]
Epoch [107/120    avg_loss:0.027, val_acc:0.972]
Epoch [108/120    avg_loss:0.031, val_acc:0.972]
Epoch [109/120    avg_loss:0.026, val_acc:0.972]
Epoch [110/120    avg_loss:0.023, val_acc:0.974]
Epoch [111/120    avg_loss:0.034, val_acc:0.972]
Epoch [112/120    avg_loss:0.026, val_acc:0.972]
Epoch [113/120    avg_loss:0.035, val_acc:0.972]
Epoch [114/120    avg_loss:0.031, val_acc:0.972]
Epoch [115/120    avg_loss:0.021, val_acc:0.972]
Epoch [116/120    avg_loss:0.028, val_acc:0.972]
Epoch [117/120    avg_loss:0.032, val_acc:0.972]
Epoch [118/120    avg_loss:0.024, val_acc:0.972]
Epoch [119/120    avg_loss:0.024, val_acc:0.972]
Epoch [120/120    avg_loss:0.025, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 208  13   0   0   0   6   3   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   3   1   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 0.997815   0.97767857 0.94977169 0.88935282 0.85611511
 1.         0.94972067 0.98714653 0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9821923764031951
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fead981c9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.491, val_acc:0.396]
Epoch [2/120    avg_loss:2.216, val_acc:0.500]
Epoch [3/120    avg_loss:2.017, val_acc:0.666]
Epoch [4/120    avg_loss:1.784, val_acc:0.713]
Epoch [5/120    avg_loss:1.511, val_acc:0.734]
Epoch [6/120    avg_loss:1.325, val_acc:0.797]
Epoch [7/120    avg_loss:1.188, val_acc:0.822]
Epoch [8/120    avg_loss:1.018, val_acc:0.834]
Epoch [9/120    avg_loss:0.901, val_acc:0.875]
Epoch [10/120    avg_loss:0.799, val_acc:0.865]
Epoch [11/120    avg_loss:0.786, val_acc:0.832]
Epoch [12/120    avg_loss:0.720, val_acc:0.867]
Epoch [13/120    avg_loss:0.656, val_acc:0.883]
Epoch [14/120    avg_loss:0.582, val_acc:0.811]
Epoch [15/120    avg_loss:0.550, val_acc:0.883]
Epoch [16/120    avg_loss:0.497, val_acc:0.883]
Epoch [17/120    avg_loss:0.560, val_acc:0.859]
Epoch [18/120    avg_loss:0.508, val_acc:0.889]
Epoch [19/120    avg_loss:0.466, val_acc:0.873]
Epoch [20/120    avg_loss:0.444, val_acc:0.902]
Epoch [21/120    avg_loss:0.381, val_acc:0.924]
Epoch [22/120    avg_loss:0.407, val_acc:0.904]
Epoch [23/120    avg_loss:0.366, val_acc:0.900]
Epoch [24/120    avg_loss:0.369, val_acc:0.914]
Epoch [25/120    avg_loss:0.429, val_acc:0.926]
Epoch [26/120    avg_loss:0.393, val_acc:0.914]
Epoch [27/120    avg_loss:0.335, val_acc:0.896]
Epoch [28/120    avg_loss:0.345, val_acc:0.924]
Epoch [29/120    avg_loss:0.328, val_acc:0.930]
Epoch [30/120    avg_loss:0.341, val_acc:0.918]
Epoch [31/120    avg_loss:0.276, val_acc:0.916]
Epoch [32/120    avg_loss:0.235, val_acc:0.943]
Epoch [33/120    avg_loss:0.255, val_acc:0.963]
Epoch [34/120    avg_loss:0.274, val_acc:0.941]
Epoch [35/120    avg_loss:0.251, val_acc:0.936]
Epoch [36/120    avg_loss:0.207, val_acc:0.932]
Epoch [37/120    avg_loss:0.258, val_acc:0.955]
Epoch [38/120    avg_loss:0.210, val_acc:0.916]
Epoch [39/120    avg_loss:0.275, val_acc:0.953]
Epoch [40/120    avg_loss:0.173, val_acc:0.951]
Epoch [41/120    avg_loss:0.173, val_acc:0.965]
Epoch [42/120    avg_loss:0.228, val_acc:0.895]
Epoch [43/120    avg_loss:0.218, val_acc:0.959]
Epoch [44/120    avg_loss:0.159, val_acc:0.971]
Epoch [45/120    avg_loss:0.156, val_acc:0.953]
Epoch [46/120    avg_loss:0.190, val_acc:0.955]
Epoch [47/120    avg_loss:0.177, val_acc:0.953]
Epoch [48/120    avg_loss:0.139, val_acc:0.979]
Epoch [49/120    avg_loss:0.126, val_acc:0.967]
Epoch [50/120    avg_loss:0.167, val_acc:0.979]
Epoch [51/120    avg_loss:0.161, val_acc:0.961]
Epoch [52/120    avg_loss:0.205, val_acc:0.973]
Epoch [53/120    avg_loss:0.141, val_acc:0.969]
Epoch [54/120    avg_loss:0.149, val_acc:0.971]
Epoch [55/120    avg_loss:0.100, val_acc:0.971]
Epoch [56/120    avg_loss:0.132, val_acc:0.932]
Epoch [57/120    avg_loss:0.158, val_acc:0.949]
Epoch [58/120    avg_loss:0.150, val_acc:0.969]
Epoch [59/120    avg_loss:0.141, val_acc:0.941]
Epoch [60/120    avg_loss:0.152, val_acc:0.967]
Epoch [61/120    avg_loss:0.099, val_acc:0.980]
Epoch [62/120    avg_loss:0.085, val_acc:0.984]
Epoch [63/120    avg_loss:0.082, val_acc:0.961]
Epoch [64/120    avg_loss:0.085, val_acc:0.982]
Epoch [65/120    avg_loss:0.162, val_acc:0.959]
Epoch [66/120    avg_loss:0.123, val_acc:0.980]
Epoch [67/120    avg_loss:0.064, val_acc:0.980]
Epoch [68/120    avg_loss:0.064, val_acc:0.959]
Epoch [69/120    avg_loss:0.089, val_acc:0.984]
Epoch [70/120    avg_loss:0.070, val_acc:0.975]
Epoch [71/120    avg_loss:0.067, val_acc:0.975]
Epoch [72/120    avg_loss:0.073, val_acc:0.973]
Epoch [73/120    avg_loss:0.107, val_acc:0.932]
Epoch [74/120    avg_loss:0.083, val_acc:0.973]
Epoch [75/120    avg_loss:0.039, val_acc:0.973]
Epoch [76/120    avg_loss:0.064, val_acc:0.971]
Epoch [77/120    avg_loss:0.081, val_acc:0.938]
Epoch [78/120    avg_loss:0.070, val_acc:0.984]
Epoch [79/120    avg_loss:0.041, val_acc:0.982]
Epoch [80/120    avg_loss:0.083, val_acc:0.980]
Epoch [81/120    avg_loss:0.086, val_acc:0.973]
Epoch [82/120    avg_loss:0.154, val_acc:0.975]
Epoch [83/120    avg_loss:0.083, val_acc:0.975]
Epoch [84/120    avg_loss:0.038, val_acc:0.982]
Epoch [85/120    avg_loss:0.038, val_acc:0.986]
Epoch [86/120    avg_loss:0.019, val_acc:0.988]
Epoch [87/120    avg_loss:0.020, val_acc:0.988]
Epoch [88/120    avg_loss:0.023, val_acc:0.990]
Epoch [89/120    avg_loss:0.019, val_acc:0.988]
Epoch [90/120    avg_loss:0.038, val_acc:0.980]
Epoch [91/120    avg_loss:0.022, val_acc:0.982]
Epoch [92/120    avg_loss:0.031, val_acc:0.986]
Epoch [93/120    avg_loss:0.065, val_acc:0.984]
Epoch [94/120    avg_loss:0.060, val_acc:0.977]
Epoch [95/120    avg_loss:0.060, val_acc:0.963]
Epoch [96/120    avg_loss:0.035, val_acc:0.988]
Epoch [97/120    avg_loss:0.019, val_acc:0.984]
Epoch [98/120    avg_loss:0.021, val_acc:0.984]
Epoch [99/120    avg_loss:0.015, val_acc:0.988]
Epoch [100/120    avg_loss:0.015, val_acc:0.988]
Epoch [101/120    avg_loss:0.017, val_acc:0.988]
Epoch [102/120    avg_loss:0.013, val_acc:0.988]
Epoch [103/120    avg_loss:0.013, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 213  11   0   0   0   6   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   1   3   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 0.99927061 0.98412698 0.96162528 0.92341357 0.93023256
 0.99266504 0.9787234  0.98714653 1.         1.         1.
 1.         1.        ]

Kappa:
0.9883683876047432
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc57accdb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.499, val_acc:0.383]
Epoch [2/120    avg_loss:2.215, val_acc:0.436]
Epoch [3/120    avg_loss:1.975, val_acc:0.648]
Epoch [4/120    avg_loss:1.747, val_acc:0.719]
Epoch [5/120    avg_loss:1.504, val_acc:0.727]
Epoch [6/120    avg_loss:1.252, val_acc:0.766]
Epoch [7/120    avg_loss:1.050, val_acc:0.773]
Epoch [8/120    avg_loss:0.935, val_acc:0.818]
Epoch [9/120    avg_loss:0.902, val_acc:0.787]
Epoch [10/120    avg_loss:0.763, val_acc:0.816]
Epoch [11/120    avg_loss:0.695, val_acc:0.879]
Epoch [12/120    avg_loss:0.701, val_acc:0.848]
Epoch [13/120    avg_loss:0.632, val_acc:0.850]
Epoch [14/120    avg_loss:0.574, val_acc:0.852]
Epoch [15/120    avg_loss:0.508, val_acc:0.873]
Epoch [16/120    avg_loss:0.499, val_acc:0.912]
Epoch [17/120    avg_loss:0.414, val_acc:0.924]
Epoch [18/120    avg_loss:0.469, val_acc:0.879]
Epoch [19/120    avg_loss:0.449, val_acc:0.908]
Epoch [20/120    avg_loss:0.407, val_acc:0.912]
Epoch [21/120    avg_loss:0.355, val_acc:0.906]
Epoch [22/120    avg_loss:0.366, val_acc:0.938]
Epoch [23/120    avg_loss:0.367, val_acc:0.912]
Epoch [24/120    avg_loss:0.328, val_acc:0.924]
Epoch [25/120    avg_loss:0.351, val_acc:0.893]
Epoch [26/120    avg_loss:0.358, val_acc:0.926]
Epoch [27/120    avg_loss:0.329, val_acc:0.912]
Epoch [28/120    avg_loss:0.336, val_acc:0.914]
Epoch [29/120    avg_loss:0.385, val_acc:0.936]
Epoch [30/120    avg_loss:0.293, val_acc:0.939]
Epoch [31/120    avg_loss:0.315, val_acc:0.963]
Epoch [32/120    avg_loss:0.236, val_acc:0.936]
Epoch [33/120    avg_loss:0.248, val_acc:0.938]
Epoch [34/120    avg_loss:0.228, val_acc:0.938]
Epoch [35/120    avg_loss:0.260, val_acc:0.912]
Epoch [36/120    avg_loss:0.284, val_acc:0.918]
Epoch [37/120    avg_loss:0.235, val_acc:0.945]
Epoch [38/120    avg_loss:0.178, val_acc:0.951]
Epoch [39/120    avg_loss:0.220, val_acc:0.959]
Epoch [40/120    avg_loss:0.193, val_acc:0.951]
Epoch [41/120    avg_loss:0.176, val_acc:0.969]
Epoch [42/120    avg_loss:0.231, val_acc:0.941]
Epoch [43/120    avg_loss:0.225, val_acc:0.963]
Epoch [44/120    avg_loss:0.259, val_acc:0.943]
Epoch [45/120    avg_loss:0.167, val_acc:0.971]
Epoch [46/120    avg_loss:0.205, val_acc:0.951]
Epoch [47/120    avg_loss:0.217, val_acc:0.959]
Epoch [48/120    avg_loss:0.170, val_acc:0.973]
Epoch [49/120    avg_loss:0.186, val_acc:0.971]
Epoch [50/120    avg_loss:0.169, val_acc:0.932]
Epoch [51/120    avg_loss:0.151, val_acc:0.967]
Epoch [52/120    avg_loss:0.177, val_acc:0.967]
Epoch [53/120    avg_loss:0.159, val_acc:0.947]
Epoch [54/120    avg_loss:0.170, val_acc:0.971]
Epoch [55/120    avg_loss:0.138, val_acc:0.979]
Epoch [56/120    avg_loss:0.116, val_acc:0.967]
Epoch [57/120    avg_loss:0.126, val_acc:0.969]
Epoch [58/120    avg_loss:0.195, val_acc:0.893]
Epoch [59/120    avg_loss:0.192, val_acc:0.953]
Epoch [60/120    avg_loss:0.126, val_acc:0.975]
Epoch [61/120    avg_loss:0.095, val_acc:0.971]
Epoch [62/120    avg_loss:0.110, val_acc:0.949]
Epoch [63/120    avg_loss:0.140, val_acc:0.969]
Epoch [64/120    avg_loss:0.144, val_acc:0.975]
Epoch [65/120    avg_loss:0.128, val_acc:0.971]
Epoch [66/120    avg_loss:0.110, val_acc:0.973]
Epoch [67/120    avg_loss:0.086, val_acc:0.969]
Epoch [68/120    avg_loss:0.090, val_acc:0.980]
Epoch [69/120    avg_loss:0.112, val_acc:0.977]
Epoch [70/120    avg_loss:0.102, val_acc:0.975]
Epoch [71/120    avg_loss:0.106, val_acc:0.959]
Epoch [72/120    avg_loss:0.108, val_acc:0.943]
Epoch [73/120    avg_loss:0.130, val_acc:0.979]
Epoch [74/120    avg_loss:0.104, val_acc:0.975]
Epoch [75/120    avg_loss:0.101, val_acc:0.969]
Epoch [76/120    avg_loss:0.065, val_acc:0.977]
Epoch [77/120    avg_loss:0.045, val_acc:0.980]
Epoch [78/120    avg_loss:0.062, val_acc:0.965]
Epoch [79/120    avg_loss:0.084, val_acc:0.971]
Epoch [80/120    avg_loss:0.079, val_acc:0.969]
Epoch [81/120    avg_loss:0.090, val_acc:0.979]
Epoch [82/120    avg_loss:0.099, val_acc:0.973]
Epoch [83/120    avg_loss:0.086, val_acc:0.977]
Epoch [84/120    avg_loss:0.108, val_acc:0.971]
Epoch [85/120    avg_loss:0.055, val_acc:0.965]
Epoch [86/120    avg_loss:0.053, val_acc:0.986]
Epoch [87/120    avg_loss:0.122, val_acc:0.957]
Epoch [88/120    avg_loss:0.096, val_acc:0.979]
Epoch [89/120    avg_loss:0.049, val_acc:0.980]
Epoch [90/120    avg_loss:0.071, val_acc:0.957]
Epoch [91/120    avg_loss:0.118, val_acc:0.975]
Epoch [92/120    avg_loss:0.079, val_acc:0.975]
Epoch [93/120    avg_loss:0.069, val_acc:0.982]
Epoch [94/120    avg_loss:0.042, val_acc:0.979]
Epoch [95/120    avg_loss:0.052, val_acc:0.982]
Epoch [96/120    avg_loss:0.035, val_acc:0.982]
Epoch [97/120    avg_loss:0.034, val_acc:0.980]
Epoch [98/120    avg_loss:0.043, val_acc:0.984]
Epoch [99/120    avg_loss:0.029, val_acc:0.979]
Epoch [100/120    avg_loss:0.021, val_acc:0.982]
Epoch [101/120    avg_loss:0.026, val_acc:0.984]
Epoch [102/120    avg_loss:0.019, val_acc:0.984]
Epoch [103/120    avg_loss:0.023, val_acc:0.984]
Epoch [104/120    avg_loss:0.021, val_acc:0.986]
Epoch [105/120    avg_loss:0.026, val_acc:0.984]
Epoch [106/120    avg_loss:0.017, val_acc:0.984]
Epoch [107/120    avg_loss:0.021, val_acc:0.984]
Epoch [108/120    avg_loss:0.023, val_acc:0.984]
Epoch [109/120    avg_loss:0.019, val_acc:0.984]
Epoch [110/120    avg_loss:0.029, val_acc:0.988]
Epoch [111/120    avg_loss:0.025, val_acc:0.986]
Epoch [112/120    avg_loss:0.033, val_acc:0.982]
Epoch [113/120    avg_loss:0.026, val_acc:0.986]
Epoch [114/120    avg_loss:0.021, val_acc:0.986]
Epoch [115/120    avg_loss:0.020, val_acc:0.984]
Epoch [116/120    avg_loss:0.019, val_acc:0.984]
Epoch [117/120    avg_loss:0.022, val_acc:0.984]
Epoch [118/120    avg_loss:0.022, val_acc:0.982]
Epoch [119/120    avg_loss:0.020, val_acc:0.984]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 200  25   0   0   0   0   0   0   2   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.97038724 0.98678414 0.91324201 0.90675241
 0.99756691 0.94736842 1.         1.         1.         1.
 0.99447514 1.        ]

Kappa:
0.9878949053690718
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f009c870ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.518, val_acc:0.417]
Epoch [2/120    avg_loss:2.250, val_acc:0.621]
Epoch [3/120    avg_loss:2.008, val_acc:0.683]
Epoch [4/120    avg_loss:1.754, val_acc:0.675]
Epoch [5/120    avg_loss:1.474, val_acc:0.714]
Epoch [6/120    avg_loss:1.258, val_acc:0.766]
Epoch [7/120    avg_loss:1.110, val_acc:0.730]
Epoch [8/120    avg_loss:0.966, val_acc:0.806]
Epoch [9/120    avg_loss:0.827, val_acc:0.815]
Epoch [10/120    avg_loss:0.780, val_acc:0.782]
Epoch [11/120    avg_loss:0.759, val_acc:0.821]
Epoch [12/120    avg_loss:0.667, val_acc:0.817]
Epoch [13/120    avg_loss:0.734, val_acc:0.839]
Epoch [14/120    avg_loss:0.586, val_acc:0.875]
Epoch [15/120    avg_loss:0.524, val_acc:0.849]
Epoch [16/120    avg_loss:0.516, val_acc:0.792]
Epoch [17/120    avg_loss:0.520, val_acc:0.883]
Epoch [18/120    avg_loss:0.496, val_acc:0.911]
Epoch [19/120    avg_loss:0.475, val_acc:0.873]
Epoch [20/120    avg_loss:0.526, val_acc:0.869]
Epoch [21/120    avg_loss:0.484, val_acc:0.873]
Epoch [22/120    avg_loss:0.412, val_acc:0.909]
Epoch [23/120    avg_loss:0.381, val_acc:0.917]
Epoch [24/120    avg_loss:0.380, val_acc:0.893]
Epoch [25/120    avg_loss:0.413, val_acc:0.897]
Epoch [26/120    avg_loss:0.380, val_acc:0.931]
Epoch [27/120    avg_loss:0.316, val_acc:0.929]
Epoch [28/120    avg_loss:0.340, val_acc:0.861]
Epoch [29/120    avg_loss:0.337, val_acc:0.917]
Epoch [30/120    avg_loss:0.311, val_acc:0.903]
Epoch [31/120    avg_loss:0.292, val_acc:0.915]
Epoch [32/120    avg_loss:0.317, val_acc:0.909]
Epoch [33/120    avg_loss:0.300, val_acc:0.938]
Epoch [34/120    avg_loss:0.317, val_acc:0.938]
Epoch [35/120    avg_loss:0.328, val_acc:0.893]
Epoch [36/120    avg_loss:0.294, val_acc:0.923]
Epoch [37/120    avg_loss:0.257, val_acc:0.935]
Epoch [38/120    avg_loss:0.258, val_acc:0.937]
Epoch [39/120    avg_loss:0.319, val_acc:0.938]
Epoch [40/120    avg_loss:0.230, val_acc:0.950]
Epoch [41/120    avg_loss:0.241, val_acc:0.946]
Epoch [42/120    avg_loss:0.233, val_acc:0.954]
Epoch [43/120    avg_loss:0.212, val_acc:0.937]
Epoch [44/120    avg_loss:0.167, val_acc:0.925]
Epoch [45/120    avg_loss:0.241, val_acc:0.946]
Epoch [46/120    avg_loss:0.234, val_acc:0.960]
Epoch [47/120    avg_loss:0.157, val_acc:0.960]
Epoch [48/120    avg_loss:0.182, val_acc:0.946]
Epoch [49/120    avg_loss:0.163, val_acc:0.964]
Epoch [50/120    avg_loss:0.123, val_acc:0.958]
Epoch [51/120    avg_loss:0.160, val_acc:0.948]
Epoch [52/120    avg_loss:0.220, val_acc:0.962]
Epoch [53/120    avg_loss:0.181, val_acc:0.942]
Epoch [54/120    avg_loss:0.136, val_acc:0.956]
Epoch [55/120    avg_loss:0.138, val_acc:0.970]
Epoch [56/120    avg_loss:0.092, val_acc:0.978]
Epoch [57/120    avg_loss:0.110, val_acc:0.968]
Epoch [58/120    avg_loss:0.108, val_acc:0.966]
Epoch [59/120    avg_loss:0.127, val_acc:0.968]
Epoch [60/120    avg_loss:0.105, val_acc:0.974]
Epoch [61/120    avg_loss:0.125, val_acc:0.956]
Epoch [62/120    avg_loss:0.185, val_acc:0.919]
Epoch [63/120    avg_loss:0.237, val_acc:0.950]
Epoch [64/120    avg_loss:0.154, val_acc:0.966]
Epoch [65/120    avg_loss:0.178, val_acc:0.964]
Epoch [66/120    avg_loss:0.123, val_acc:0.970]
Epoch [67/120    avg_loss:0.085, val_acc:0.952]
Epoch [68/120    avg_loss:0.067, val_acc:0.897]
Epoch [69/120    avg_loss:0.089, val_acc:0.966]
Epoch [70/120    avg_loss:0.067, val_acc:0.982]
Epoch [71/120    avg_loss:0.051, val_acc:0.980]
Epoch [72/120    avg_loss:0.048, val_acc:0.982]
Epoch [73/120    avg_loss:0.054, val_acc:0.986]
Epoch [74/120    avg_loss:0.049, val_acc:0.988]
Epoch [75/120    avg_loss:0.044, val_acc:0.988]
Epoch [76/120    avg_loss:0.049, val_acc:0.988]
Epoch [77/120    avg_loss:0.045, val_acc:0.986]
Epoch [78/120    avg_loss:0.034, val_acc:0.988]
Epoch [79/120    avg_loss:0.042, val_acc:0.988]
Epoch [80/120    avg_loss:0.045, val_acc:0.988]
Epoch [81/120    avg_loss:0.059, val_acc:0.988]
Epoch [82/120    avg_loss:0.040, val_acc:0.988]
Epoch [83/120    avg_loss:0.040, val_acc:0.986]
Epoch [84/120    avg_loss:0.050, val_acc:0.986]
Epoch [85/120    avg_loss:0.038, val_acc:0.988]
Epoch [86/120    avg_loss:0.039, val_acc:0.986]
Epoch [87/120    avg_loss:0.033, val_acc:0.986]
Epoch [88/120    avg_loss:0.037, val_acc:0.988]
Epoch [89/120    avg_loss:0.039, val_acc:0.988]
Epoch [90/120    avg_loss:0.031, val_acc:0.988]
Epoch [91/120    avg_loss:0.038, val_acc:0.986]
Epoch [92/120    avg_loss:0.029, val_acc:0.988]
Epoch [93/120    avg_loss:0.040, val_acc:0.988]
Epoch [94/120    avg_loss:0.037, val_acc:0.988]
Epoch [95/120    avg_loss:0.029, val_acc:0.988]
Epoch [96/120    avg_loss:0.041, val_acc:0.990]
Epoch [97/120    avg_loss:0.034, val_acc:0.988]
Epoch [98/120    avg_loss:0.044, val_acc:0.990]
Epoch [99/120    avg_loss:0.032, val_acc:0.990]
Epoch [100/120    avg_loss:0.044, val_acc:0.990]
Epoch [101/120    avg_loss:0.032, val_acc:0.990]
Epoch [102/120    avg_loss:0.030, val_acc:0.988]
Epoch [103/120    avg_loss:0.032, val_acc:0.990]
Epoch [104/120    avg_loss:0.037, val_acc:0.988]
Epoch [105/120    avg_loss:0.031, val_acc:0.990]
Epoch [106/120    avg_loss:0.037, val_acc:0.988]
Epoch [107/120    avg_loss:0.028, val_acc:0.990]
Epoch [108/120    avg_loss:0.028, val_acc:0.990]
Epoch [109/120    avg_loss:0.031, val_acc:0.990]
Epoch [110/120    avg_loss:0.035, val_acc:0.990]
Epoch [111/120    avg_loss:0.024, val_acc:0.990]
Epoch [112/120    avg_loss:0.026, val_acc:0.990]
Epoch [113/120    avg_loss:0.036, val_acc:0.990]
Epoch [114/120    avg_loss:0.031, val_acc:0.990]
Epoch [115/120    avg_loss:0.030, val_acc:0.990]
Epoch [116/120    avg_loss:0.027, val_acc:0.990]
Epoch [117/120    avg_loss:0.024, val_acc:0.990]
Epoch [118/120    avg_loss:0.028, val_acc:0.990]
Epoch [119/120    avg_loss:0.027, val_acc:0.990]
Epoch [120/120    avg_loss:0.025, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 217  11   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.97767857 0.97091723 0.91983122 0.90391459
 1.         0.94382022 0.998713   0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9881295147391184
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7feb43aa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.506, val_acc:0.440]
Epoch [2/120    avg_loss:2.211, val_acc:0.532]
Epoch [3/120    avg_loss:1.969, val_acc:0.639]
Epoch [4/120    avg_loss:1.717, val_acc:0.679]
Epoch [5/120    avg_loss:1.423, val_acc:0.710]
Epoch [6/120    avg_loss:1.215, val_acc:0.800]
Epoch [7/120    avg_loss:1.033, val_acc:0.835]
Epoch [8/120    avg_loss:0.860, val_acc:0.746]
Epoch [9/120    avg_loss:0.805, val_acc:0.847]
Epoch [10/120    avg_loss:0.677, val_acc:0.885]
Epoch [11/120    avg_loss:0.593, val_acc:0.889]
Epoch [12/120    avg_loss:0.554, val_acc:0.867]
Epoch [13/120    avg_loss:0.515, val_acc:0.889]
Epoch [14/120    avg_loss:0.421, val_acc:0.903]
Epoch [15/120    avg_loss:0.398, val_acc:0.931]
Epoch [16/120    avg_loss:0.421, val_acc:0.919]
Epoch [17/120    avg_loss:0.417, val_acc:0.935]
Epoch [18/120    avg_loss:0.408, val_acc:0.952]
Epoch [19/120    avg_loss:0.300, val_acc:0.950]
Epoch [20/120    avg_loss:0.310, val_acc:0.917]
Epoch [21/120    avg_loss:0.452, val_acc:0.925]
Epoch [22/120    avg_loss:0.362, val_acc:0.929]
Epoch [23/120    avg_loss:0.290, val_acc:0.938]
Epoch [24/120    avg_loss:0.266, val_acc:0.940]
Epoch [25/120    avg_loss:0.303, val_acc:0.921]
Epoch [26/120    avg_loss:0.326, val_acc:0.942]
Epoch [27/120    avg_loss:0.258, val_acc:0.897]
Epoch [28/120    avg_loss:0.294, val_acc:0.958]
Epoch [29/120    avg_loss:0.196, val_acc:0.948]
Epoch [30/120    avg_loss:0.187, val_acc:0.952]
Epoch [31/120    avg_loss:0.222, val_acc:0.948]
Epoch [32/120    avg_loss:0.205, val_acc:0.929]
Epoch [33/120    avg_loss:0.200, val_acc:0.964]
Epoch [34/120    avg_loss:0.182, val_acc:0.962]
Epoch [35/120    avg_loss:0.154, val_acc:0.956]
Epoch [36/120    avg_loss:0.202, val_acc:0.931]
Epoch [37/120    avg_loss:0.201, val_acc:0.970]
Epoch [38/120    avg_loss:0.163, val_acc:0.962]
Epoch [39/120    avg_loss:0.175, val_acc:0.960]
Epoch [40/120    avg_loss:0.114, val_acc:0.974]
Epoch [41/120    avg_loss:0.132, val_acc:0.976]
Epoch [42/120    avg_loss:0.116, val_acc:0.970]
Epoch [43/120    avg_loss:0.134, val_acc:0.980]
Epoch [44/120    avg_loss:0.119, val_acc:0.974]
Epoch [45/120    avg_loss:0.135, val_acc:0.950]
Epoch [46/120    avg_loss:0.100, val_acc:0.986]
Epoch [47/120    avg_loss:0.109, val_acc:0.970]
Epoch [48/120    avg_loss:0.123, val_acc:0.972]
Epoch [49/120    avg_loss:0.092, val_acc:0.972]
Epoch [50/120    avg_loss:0.095, val_acc:0.984]
Epoch [51/120    avg_loss:0.094, val_acc:0.933]
Epoch [52/120    avg_loss:0.091, val_acc:0.966]
Epoch [53/120    avg_loss:0.077, val_acc:0.966]
Epoch [54/120    avg_loss:0.085, val_acc:0.984]
Epoch [55/120    avg_loss:0.088, val_acc:0.978]
Epoch [56/120    avg_loss:0.058, val_acc:0.988]
Epoch [57/120    avg_loss:0.090, val_acc:0.980]
Epoch [58/120    avg_loss:0.061, val_acc:0.984]
Epoch [59/120    avg_loss:0.076, val_acc:0.982]
Epoch [60/120    avg_loss:0.069, val_acc:0.978]
Epoch [61/120    avg_loss:0.049, val_acc:0.982]
Epoch [62/120    avg_loss:0.051, val_acc:0.990]
Epoch [63/120    avg_loss:0.050, val_acc:0.988]
Epoch [64/120    avg_loss:0.038, val_acc:0.980]
Epoch [65/120    avg_loss:0.041, val_acc:0.990]
Epoch [66/120    avg_loss:0.054, val_acc:0.984]
Epoch [67/120    avg_loss:0.162, val_acc:0.972]
Epoch [68/120    avg_loss:0.046, val_acc:0.984]
Epoch [69/120    avg_loss:0.039, val_acc:0.976]
Epoch [70/120    avg_loss:0.047, val_acc:0.988]
Epoch [71/120    avg_loss:0.059, val_acc:0.956]
Epoch [72/120    avg_loss:0.067, val_acc:0.982]
Epoch [73/120    avg_loss:0.048, val_acc:0.988]
Epoch [74/120    avg_loss:0.038, val_acc:0.990]
Epoch [75/120    avg_loss:0.115, val_acc:0.976]
Epoch [76/120    avg_loss:0.054, val_acc:0.986]
Epoch [77/120    avg_loss:0.029, val_acc:0.974]
Epoch [78/120    avg_loss:0.025, val_acc:0.982]
Epoch [79/120    avg_loss:0.023, val_acc:0.980]
Epoch [80/120    avg_loss:0.043, val_acc:0.980]
Epoch [81/120    avg_loss:0.025, val_acc:0.988]
Epoch [82/120    avg_loss:0.016, val_acc:0.988]
Epoch [83/120    avg_loss:0.014, val_acc:0.990]
Epoch [84/120    avg_loss:0.016, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.988]
Epoch [86/120    avg_loss:0.015, val_acc:0.992]
Epoch [87/120    avg_loss:0.048, val_acc:0.980]
Epoch [88/120    avg_loss:0.031, val_acc:0.994]
Epoch [89/120    avg_loss:0.044, val_acc:0.984]
Epoch [90/120    avg_loss:0.028, val_acc:0.986]
Epoch [91/120    avg_loss:0.035, val_acc:0.978]
Epoch [92/120    avg_loss:0.059, val_acc:0.960]
Epoch [93/120    avg_loss:0.061, val_acc:0.982]
Epoch [94/120    avg_loss:0.031, val_acc:0.986]
Epoch [95/120    avg_loss:0.023, val_acc:0.982]
Epoch [96/120    avg_loss:0.029, val_acc:0.984]
Epoch [97/120    avg_loss:0.021, val_acc:0.996]
Epoch [98/120    avg_loss:0.012, val_acc:0.964]
Epoch [99/120    avg_loss:0.023, val_acc:0.980]
Epoch [100/120    avg_loss:0.145, val_acc:0.970]
Epoch [101/120    avg_loss:0.125, val_acc:0.978]
Epoch [102/120    avg_loss:0.039, val_acc:0.980]
Epoch [103/120    avg_loss:0.056, val_acc:0.990]
Epoch [104/120    avg_loss:0.021, val_acc:0.992]
Epoch [105/120    avg_loss:0.021, val_acc:0.992]
Epoch [106/120    avg_loss:0.014, val_acc:0.992]
Epoch [107/120    avg_loss:0.036, val_acc:0.982]
Epoch [108/120    avg_loss:0.036, val_acc:0.982]
Epoch [109/120    avg_loss:0.022, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.011, val_acc:0.992]
Epoch [112/120    avg_loss:0.010, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.008, val_acc:0.992]
Epoch [117/120    avg_loss:0.009, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   1   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   3   0   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.997815   0.99095023 0.98901099 0.96396396 0.95016611
 1.         0.97826087 0.99099099 1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.992878235543628
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa80593da20>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.446, val_acc:0.520]
Epoch [2/120    avg_loss:2.160, val_acc:0.607]
Epoch [3/120    avg_loss:1.914, val_acc:0.655]
Epoch [4/120    avg_loss:1.644, val_acc:0.673]
Epoch [5/120    avg_loss:1.383, val_acc:0.732]
Epoch [6/120    avg_loss:1.168, val_acc:0.730]
Epoch [7/120    avg_loss:1.036, val_acc:0.730]
Epoch [8/120    avg_loss:0.886, val_acc:0.847]
Epoch [9/120    avg_loss:0.778, val_acc:0.831]
Epoch [10/120    avg_loss:0.724, val_acc:0.867]
Epoch [11/120    avg_loss:0.609, val_acc:0.889]
Epoch [12/120    avg_loss:0.588, val_acc:0.873]
Epoch [13/120    avg_loss:0.508, val_acc:0.841]
Epoch [14/120    avg_loss:0.548, val_acc:0.875]
Epoch [15/120    avg_loss:0.445, val_acc:0.915]
Epoch [16/120    avg_loss:0.399, val_acc:0.903]
Epoch [17/120    avg_loss:0.388, val_acc:0.909]
Epoch [18/120    avg_loss:0.370, val_acc:0.929]
Epoch [19/120    avg_loss:0.336, val_acc:0.913]
Epoch [20/120    avg_loss:0.407, val_acc:0.927]
Epoch [21/120    avg_loss:0.323, val_acc:0.891]
Epoch [22/120    avg_loss:0.362, val_acc:0.933]
Epoch [23/120    avg_loss:0.331, val_acc:0.917]
Epoch [24/120    avg_loss:0.311, val_acc:0.935]
Epoch [25/120    avg_loss:0.339, val_acc:0.944]
Epoch [26/120    avg_loss:0.254, val_acc:0.933]
Epoch [27/120    avg_loss:0.287, val_acc:0.919]
Epoch [28/120    avg_loss:0.235, val_acc:0.950]
Epoch [29/120    avg_loss:0.248, val_acc:0.931]
Epoch [30/120    avg_loss:0.264, val_acc:0.935]
Epoch [31/120    avg_loss:0.225, val_acc:0.956]
Epoch [32/120    avg_loss:0.249, val_acc:0.938]
Epoch [33/120    avg_loss:0.198, val_acc:0.962]
Epoch [34/120    avg_loss:0.222, val_acc:0.960]
Epoch [35/120    avg_loss:0.287, val_acc:0.948]
Epoch [36/120    avg_loss:0.204, val_acc:0.956]
Epoch [37/120    avg_loss:0.212, val_acc:0.940]
Epoch [38/120    avg_loss:0.199, val_acc:0.923]
Epoch [39/120    avg_loss:0.191, val_acc:0.974]
Epoch [40/120    avg_loss:0.168, val_acc:0.954]
Epoch [41/120    avg_loss:0.172, val_acc:0.964]
Epoch [42/120    avg_loss:0.154, val_acc:0.974]
Epoch [43/120    avg_loss:0.160, val_acc:0.960]
Epoch [44/120    avg_loss:0.184, val_acc:0.968]
Epoch [45/120    avg_loss:0.098, val_acc:0.970]
Epoch [46/120    avg_loss:0.145, val_acc:0.966]
Epoch [47/120    avg_loss:0.115, val_acc:0.972]
Epoch [48/120    avg_loss:0.129, val_acc:0.954]
Epoch [49/120    avg_loss:0.118, val_acc:0.956]
Epoch [50/120    avg_loss:0.127, val_acc:0.972]
Epoch [51/120    avg_loss:0.103, val_acc:0.966]
Epoch [52/120    avg_loss:0.208, val_acc:0.923]
Epoch [53/120    avg_loss:0.224, val_acc:0.954]
Epoch [54/120    avg_loss:0.166, val_acc:0.972]
Epoch [55/120    avg_loss:0.097, val_acc:0.974]
Epoch [56/120    avg_loss:0.133, val_acc:0.982]
Epoch [57/120    avg_loss:0.097, val_acc:0.978]
Epoch [58/120    avg_loss:0.112, val_acc:0.956]
Epoch [59/120    avg_loss:0.110, val_acc:0.974]
Epoch [60/120    avg_loss:0.137, val_acc:0.976]
Epoch [61/120    avg_loss:0.098, val_acc:0.980]
Epoch [62/120    avg_loss:0.090, val_acc:0.982]
Epoch [63/120    avg_loss:0.062, val_acc:0.978]
Epoch [64/120    avg_loss:0.063, val_acc:0.980]
Epoch [65/120    avg_loss:0.061, val_acc:0.970]
Epoch [66/120    avg_loss:0.123, val_acc:0.968]
Epoch [67/120    avg_loss:0.102, val_acc:0.974]
Epoch [68/120    avg_loss:0.068, val_acc:0.980]
Epoch [69/120    avg_loss:0.060, val_acc:0.972]
Epoch [70/120    avg_loss:0.072, val_acc:0.990]
Epoch [71/120    avg_loss:0.060, val_acc:0.980]
Epoch [72/120    avg_loss:0.042, val_acc:0.982]
Epoch [73/120    avg_loss:0.047, val_acc:0.978]
Epoch [74/120    avg_loss:0.030, val_acc:0.990]
Epoch [75/120    avg_loss:0.052, val_acc:0.992]
Epoch [76/120    avg_loss:0.037, val_acc:0.994]
Epoch [77/120    avg_loss:0.045, val_acc:0.986]
Epoch [78/120    avg_loss:0.036, val_acc:0.990]
Epoch [79/120    avg_loss:0.042, val_acc:0.978]
Epoch [80/120    avg_loss:0.029, val_acc:0.988]
Epoch [81/120    avg_loss:0.023, val_acc:0.994]
Epoch [82/120    avg_loss:0.074, val_acc:0.982]
Epoch [83/120    avg_loss:0.073, val_acc:0.978]
Epoch [84/120    avg_loss:0.099, val_acc:0.990]
Epoch [85/120    avg_loss:0.077, val_acc:0.988]
Epoch [86/120    avg_loss:0.040, val_acc:0.990]
Epoch [87/120    avg_loss:0.039, val_acc:0.988]
Epoch [88/120    avg_loss:0.035, val_acc:0.978]
Epoch [89/120    avg_loss:0.034, val_acc:0.984]
Epoch [90/120    avg_loss:0.034, val_acc:0.972]
Epoch [91/120    avg_loss:0.052, val_acc:0.988]
Epoch [92/120    avg_loss:0.071, val_acc:0.992]
Epoch [93/120    avg_loss:0.019, val_acc:0.994]
Epoch [94/120    avg_loss:0.041, val_acc:0.976]
Epoch [95/120    avg_loss:0.041, val_acc:0.990]
Epoch [96/120    avg_loss:0.021, val_acc:0.988]
Epoch [97/120    avg_loss:0.018, val_acc:0.990]
Epoch [98/120    avg_loss:0.013, val_acc:0.992]
Epoch [99/120    avg_loss:0.019, val_acc:0.994]
Epoch [100/120    avg_loss:0.014, val_acc:0.996]
Epoch [101/120    avg_loss:0.017, val_acc:0.994]
Epoch [102/120    avg_loss:0.051, val_acc:0.988]
Epoch [103/120    avg_loss:0.043, val_acc:1.000]
Epoch [104/120    avg_loss:0.040, val_acc:0.980]
Epoch [105/120    avg_loss:0.037, val_acc:0.992]
Epoch [106/120    avg_loss:0.015, val_acc:0.998]
Epoch [107/120    avg_loss:0.010, val_acc:0.996]
Epoch [108/120    avg_loss:0.010, val_acc:0.996]
Epoch [109/120    avg_loss:0.010, val_acc:0.996]
Epoch [110/120    avg_loss:0.008, val_acc:0.998]
Epoch [111/120    avg_loss:0.012, val_acc:0.996]
Epoch [112/120    avg_loss:0.044, val_acc:0.992]
Epoch [113/120    avg_loss:0.018, val_acc:0.996]
Epoch [114/120    avg_loss:0.013, val_acc:0.994]
Epoch [115/120    avg_loss:0.013, val_acc:0.986]
Epoch [116/120    avg_loss:0.020, val_acc:0.984]
Epoch [117/120    avg_loss:0.016, val_acc:0.996]
Epoch [118/120    avg_loss:0.009, val_acc:0.996]
Epoch [119/120    avg_loss:0.009, val_acc:0.996]
Epoch [120/120    avg_loss:0.008, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  15   0   0   0   0   0   0   2   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 362   0   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.99545455 1.         0.93541203 0.91525424
 0.99512195 0.98924731 1.         1.         0.99724518 0.9986755
 0.99449945 1.        ]

Kappa:
0.9919288524241332
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8196cfac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.460, val_acc:0.349]
Epoch [2/120    avg_loss:2.186, val_acc:0.442]
Epoch [3/120    avg_loss:1.991, val_acc:0.619]
Epoch [4/120    avg_loss:1.783, val_acc:0.667]
Epoch [5/120    avg_loss:1.579, val_acc:0.710]
Epoch [6/120    avg_loss:1.363, val_acc:0.724]
Epoch [7/120    avg_loss:1.149, val_acc:0.762]
Epoch [8/120    avg_loss:1.036, val_acc:0.847]
Epoch [9/120    avg_loss:0.867, val_acc:0.752]
Epoch [10/120    avg_loss:0.833, val_acc:0.841]
Epoch [11/120    avg_loss:0.730, val_acc:0.879]
Epoch [12/120    avg_loss:0.692, val_acc:0.829]
Epoch [13/120    avg_loss:0.644, val_acc:0.855]
Epoch [14/120    avg_loss:0.533, val_acc:0.861]
Epoch [15/120    avg_loss:0.521, val_acc:0.887]
Epoch [16/120    avg_loss:0.568, val_acc:0.887]
Epoch [17/120    avg_loss:0.498, val_acc:0.895]
Epoch [18/120    avg_loss:0.452, val_acc:0.917]
Epoch [19/120    avg_loss:0.465, val_acc:0.879]
Epoch [20/120    avg_loss:0.457, val_acc:0.813]
Epoch [21/120    avg_loss:0.463, val_acc:0.839]
Epoch [22/120    avg_loss:0.442, val_acc:0.919]
Epoch [23/120    avg_loss:0.322, val_acc:0.929]
Epoch [24/120    avg_loss:0.352, val_acc:0.925]
Epoch [25/120    avg_loss:0.344, val_acc:0.937]
Epoch [26/120    avg_loss:0.357, val_acc:0.938]
Epoch [27/120    avg_loss:0.350, val_acc:0.895]
Epoch [28/120    avg_loss:0.288, val_acc:0.927]
Epoch [29/120    avg_loss:0.251, val_acc:0.897]
Epoch [30/120    avg_loss:0.289, val_acc:0.917]
Epoch [31/120    avg_loss:0.334, val_acc:0.950]
Epoch [32/120    avg_loss:0.251, val_acc:0.950]
Epoch [33/120    avg_loss:0.250, val_acc:0.933]
Epoch [34/120    avg_loss:0.227, val_acc:0.950]
Epoch [35/120    avg_loss:0.141, val_acc:0.929]
Epoch [36/120    avg_loss:0.253, val_acc:0.942]
Epoch [37/120    avg_loss:0.311, val_acc:0.899]
Epoch [38/120    avg_loss:0.234, val_acc:0.952]
Epoch [39/120    avg_loss:0.221, val_acc:0.907]
Epoch [40/120    avg_loss:0.197, val_acc:0.982]
Epoch [41/120    avg_loss:0.162, val_acc:0.938]
Epoch [42/120    avg_loss:0.234, val_acc:0.950]
Epoch [43/120    avg_loss:0.167, val_acc:0.940]
Epoch [44/120    avg_loss:0.137, val_acc:0.956]
Epoch [45/120    avg_loss:0.120, val_acc:0.976]
Epoch [46/120    avg_loss:0.116, val_acc:0.970]
Epoch [47/120    avg_loss:0.121, val_acc:0.976]
Epoch [48/120    avg_loss:0.099, val_acc:0.974]
Epoch [49/120    avg_loss:0.110, val_acc:0.984]
Epoch [50/120    avg_loss:0.092, val_acc:0.964]
Epoch [51/120    avg_loss:0.172, val_acc:0.972]
Epoch [52/120    avg_loss:0.096, val_acc:0.974]
Epoch [53/120    avg_loss:0.096, val_acc:0.984]
Epoch [54/120    avg_loss:0.082, val_acc:0.933]
Epoch [55/120    avg_loss:0.135, val_acc:0.970]
Epoch [56/120    avg_loss:0.120, val_acc:0.974]
Epoch [57/120    avg_loss:0.104, val_acc:0.972]
Epoch [58/120    avg_loss:0.085, val_acc:0.988]
Epoch [59/120    avg_loss:0.145, val_acc:0.952]
Epoch [60/120    avg_loss:0.164, val_acc:0.964]
Epoch [61/120    avg_loss:0.188, val_acc:0.950]
Epoch [62/120    avg_loss:0.138, val_acc:0.966]
Epoch [63/120    avg_loss:0.084, val_acc:0.970]
Epoch [64/120    avg_loss:0.068, val_acc:0.986]
Epoch [65/120    avg_loss:0.066, val_acc:0.976]
Epoch [66/120    avg_loss:0.060, val_acc:0.982]
Epoch [67/120    avg_loss:0.054, val_acc:0.980]
Epoch [68/120    avg_loss:0.069, val_acc:0.990]
Epoch [69/120    avg_loss:0.053, val_acc:0.982]
Epoch [70/120    avg_loss:0.050, val_acc:0.986]
Epoch [71/120    avg_loss:0.063, val_acc:0.962]
Epoch [72/120    avg_loss:0.083, val_acc:0.956]
Epoch [73/120    avg_loss:0.151, val_acc:0.980]
Epoch [74/120    avg_loss:0.058, val_acc:0.986]
Epoch [75/120    avg_loss:0.046, val_acc:0.990]
Epoch [76/120    avg_loss:0.026, val_acc:0.988]
Epoch [77/120    avg_loss:0.034, val_acc:0.988]
Epoch [78/120    avg_loss:0.032, val_acc:0.990]
Epoch [79/120    avg_loss:0.030, val_acc:0.992]
Epoch [80/120    avg_loss:0.037, val_acc:0.984]
Epoch [81/120    avg_loss:0.041, val_acc:0.990]
Epoch [82/120    avg_loss:0.024, val_acc:0.990]
Epoch [83/120    avg_loss:0.019, val_acc:0.992]
Epoch [84/120    avg_loss:0.015, val_acc:0.990]
Epoch [85/120    avg_loss:0.040, val_acc:0.978]
Epoch [86/120    avg_loss:0.037, val_acc:0.984]
Epoch [87/120    avg_loss:0.046, val_acc:0.992]
Epoch [88/120    avg_loss:0.028, val_acc:0.992]
Epoch [89/120    avg_loss:0.020, val_acc:0.996]
Epoch [90/120    avg_loss:0.016, val_acc:0.994]
Epoch [91/120    avg_loss:0.016, val_acc:0.990]
Epoch [92/120    avg_loss:0.010, val_acc:0.990]
Epoch [93/120    avg_loss:0.017, val_acc:0.990]
Epoch [94/120    avg_loss:0.027, val_acc:0.998]
Epoch [95/120    avg_loss:0.016, val_acc:0.996]
Epoch [96/120    avg_loss:0.013, val_acc:0.996]
Epoch [97/120    avg_loss:0.010, val_acc:0.996]
Epoch [98/120    avg_loss:0.009, val_acc:0.994]
Epoch [99/120    avg_loss:0.007, val_acc:0.994]
Epoch [100/120    avg_loss:0.008, val_acc:0.992]
Epoch [101/120    avg_loss:0.018, val_acc:0.988]
Epoch [102/120    avg_loss:0.205, val_acc:0.921]
Epoch [103/120    avg_loss:0.138, val_acc:0.974]
Epoch [104/120    avg_loss:0.054, val_acc:0.990]
Epoch [105/120    avg_loss:0.030, val_acc:0.982]
Epoch [106/120    avg_loss:0.032, val_acc:0.986]
Epoch [107/120    avg_loss:0.060, val_acc:0.986]
Epoch [108/120    avg_loss:0.029, val_acc:0.986]
Epoch [109/120    avg_loss:0.025, val_acc:0.986]
Epoch [110/120    avg_loss:0.018, val_acc:0.986]
Epoch [111/120    avg_loss:0.024, val_acc:0.988]
Epoch [112/120    avg_loss:0.018, val_acc:0.988]
Epoch [113/120    avg_loss:0.019, val_acc:0.984]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.016, val_acc:0.988]
Epoch [116/120    avg_loss:0.014, val_acc:0.988]
Epoch [117/120    avg_loss:0.012, val_acc:0.990]
Epoch [118/120    avg_loss:0.017, val_acc:0.988]
Epoch [119/120    avg_loss:0.013, val_acc:0.988]
Epoch [120/120    avg_loss:0.011, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 224   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  14   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.99095023 0.98678414 0.94642857 0.95394737
 0.98771499 0.98924731 1.         1.         1.         0.98562092
 0.98660714 1.        ]

Kappa:
0.9907427965661854
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcaceb73a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.532, val_acc:0.464]
Epoch [2/120    avg_loss:2.257, val_acc:0.605]
Epoch [3/120    avg_loss:2.018, val_acc:0.639]
Epoch [4/120    avg_loss:1.788, val_acc:0.641]
Epoch [5/120    avg_loss:1.510, val_acc:0.732]
Epoch [6/120    avg_loss:1.319, val_acc:0.752]
Epoch [7/120    avg_loss:1.095, val_acc:0.800]
Epoch [8/120    avg_loss:0.963, val_acc:0.812]
Epoch [9/120    avg_loss:0.803, val_acc:0.774]
Epoch [10/120    avg_loss:0.741, val_acc:0.786]
Epoch [11/120    avg_loss:0.704, val_acc:0.792]
Epoch [12/120    avg_loss:0.691, val_acc:0.851]
Epoch [13/120    avg_loss:0.599, val_acc:0.831]
Epoch [14/120    avg_loss:0.594, val_acc:0.859]
Epoch [15/120    avg_loss:0.588, val_acc:0.859]
Epoch [16/120    avg_loss:0.558, val_acc:0.889]
Epoch [17/120    avg_loss:0.512, val_acc:0.903]
Epoch [18/120    avg_loss:0.495, val_acc:0.873]
Epoch [19/120    avg_loss:0.463, val_acc:0.909]
Epoch [20/120    avg_loss:0.438, val_acc:0.897]
Epoch [21/120    avg_loss:0.360, val_acc:0.893]
Epoch [22/120    avg_loss:0.355, val_acc:0.927]
Epoch [23/120    avg_loss:0.376, val_acc:0.915]
Epoch [24/120    avg_loss:0.387, val_acc:0.931]
Epoch [25/120    avg_loss:0.324, val_acc:0.925]
Epoch [26/120    avg_loss:0.303, val_acc:0.937]
Epoch [27/120    avg_loss:0.288, val_acc:0.940]
Epoch [28/120    avg_loss:0.362, val_acc:0.863]
Epoch [29/120    avg_loss:0.323, val_acc:0.933]
Epoch [30/120    avg_loss:0.271, val_acc:0.911]
Epoch [31/120    avg_loss:0.238, val_acc:0.960]
Epoch [32/120    avg_loss:0.260, val_acc:0.917]
Epoch [33/120    avg_loss:0.231, val_acc:0.940]
Epoch [34/120    avg_loss:0.260, val_acc:0.972]
Epoch [35/120    avg_loss:0.237, val_acc:0.937]
Epoch [36/120    avg_loss:0.208, val_acc:0.933]
Epoch [37/120    avg_loss:0.249, val_acc:0.962]
Epoch [38/120    avg_loss:0.189, val_acc:0.958]
Epoch [39/120    avg_loss:0.166, val_acc:0.931]
Epoch [40/120    avg_loss:0.243, val_acc:0.948]
Epoch [41/120    avg_loss:0.228, val_acc:0.954]
Epoch [42/120    avg_loss:0.170, val_acc:0.944]
Epoch [43/120    avg_loss:0.130, val_acc:0.964]
Epoch [44/120    avg_loss:0.162, val_acc:0.970]
Epoch [45/120    avg_loss:0.156, val_acc:0.952]
Epoch [46/120    avg_loss:0.124, val_acc:0.976]
Epoch [47/120    avg_loss:0.118, val_acc:0.964]
Epoch [48/120    avg_loss:0.113, val_acc:0.968]
Epoch [49/120    avg_loss:0.096, val_acc:0.970]
Epoch [50/120    avg_loss:0.118, val_acc:0.952]
Epoch [51/120    avg_loss:0.148, val_acc:0.980]
Epoch [52/120    avg_loss:0.092, val_acc:0.980]
Epoch [53/120    avg_loss:0.102, val_acc:0.964]
Epoch [54/120    avg_loss:0.116, val_acc:0.948]
Epoch [55/120    avg_loss:0.115, val_acc:0.980]
Epoch [56/120    avg_loss:0.107, val_acc:0.946]
Epoch [57/120    avg_loss:0.121, val_acc:0.988]
Epoch [58/120    avg_loss:0.093, val_acc:0.964]
Epoch [59/120    avg_loss:0.061, val_acc:0.942]
Epoch [60/120    avg_loss:0.077, val_acc:0.954]
Epoch [61/120    avg_loss:0.121, val_acc:0.970]
Epoch [62/120    avg_loss:0.104, val_acc:0.974]
Epoch [63/120    avg_loss:0.085, val_acc:0.966]
Epoch [64/120    avg_loss:0.063, val_acc:0.964]
Epoch [65/120    avg_loss:0.051, val_acc:0.968]
Epoch [66/120    avg_loss:0.040, val_acc:0.984]
Epoch [67/120    avg_loss:0.032, val_acc:0.982]
Epoch [68/120    avg_loss:0.035, val_acc:0.954]
Epoch [69/120    avg_loss:0.083, val_acc:0.962]
Epoch [70/120    avg_loss:0.059, val_acc:0.974]
Epoch [71/120    avg_loss:0.042, val_acc:0.982]
Epoch [72/120    avg_loss:0.028, val_acc:0.980]
Epoch [73/120    avg_loss:0.024, val_acc:0.982]
Epoch [74/120    avg_loss:0.031, val_acc:0.982]
Epoch [75/120    avg_loss:0.032, val_acc:0.984]
Epoch [76/120    avg_loss:0.029, val_acc:0.984]
Epoch [77/120    avg_loss:0.024, val_acc:0.984]
Epoch [78/120    avg_loss:0.023, val_acc:0.984]
Epoch [79/120    avg_loss:0.023, val_acc:0.986]
Epoch [80/120    avg_loss:0.019, val_acc:0.986]
Epoch [81/120    avg_loss:0.017, val_acc:0.986]
Epoch [82/120    avg_loss:0.021, val_acc:0.986]
Epoch [83/120    avg_loss:0.022, val_acc:0.986]
Epoch [84/120    avg_loss:0.017, val_acc:0.986]
Epoch [85/120    avg_loss:0.018, val_acc:0.986]
Epoch [86/120    avg_loss:0.023, val_acc:0.986]
Epoch [87/120    avg_loss:0.017, val_acc:0.986]
Epoch [88/120    avg_loss:0.017, val_acc:0.986]
Epoch [89/120    avg_loss:0.024, val_acc:0.986]
Epoch [90/120    avg_loss:0.019, val_acc:0.986]
Epoch [91/120    avg_loss:0.020, val_acc:0.986]
Epoch [92/120    avg_loss:0.025, val_acc:0.986]
Epoch [93/120    avg_loss:0.020, val_acc:0.986]
Epoch [94/120    avg_loss:0.026, val_acc:0.986]
Epoch [95/120    avg_loss:0.019, val_acc:0.986]
Epoch [96/120    avg_loss:0.027, val_acc:0.986]
Epoch [97/120    avg_loss:0.018, val_acc:0.986]
Epoch [98/120    avg_loss:0.018, val_acc:0.986]
Epoch [99/120    avg_loss:0.017, val_acc:0.986]
Epoch [100/120    avg_loss:0.019, val_acc:0.986]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.020, val_acc:0.986]
Epoch [103/120    avg_loss:0.018, val_acc:0.986]
Epoch [104/120    avg_loss:0.019, val_acc:0.986]
Epoch [105/120    avg_loss:0.016, val_acc:0.986]
Epoch [106/120    avg_loss:0.017, val_acc:0.986]
Epoch [107/120    avg_loss:0.018, val_acc:0.986]
Epoch [108/120    avg_loss:0.020, val_acc:0.986]
Epoch [109/120    avg_loss:0.018, val_acc:0.986]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.021, val_acc:0.986]
Epoch [112/120    avg_loss:0.018, val_acc:0.986]
Epoch [113/120    avg_loss:0.018, val_acc:0.986]
Epoch [114/120    avg_loss:0.017, val_acc:0.986]
Epoch [115/120    avg_loss:0.014, val_acc:0.986]
Epoch [116/120    avg_loss:0.018, val_acc:0.986]
Epoch [117/120    avg_loss:0.021, val_acc:0.986]
Epoch [118/120    avg_loss:0.017, val_acc:0.986]
Epoch [119/120    avg_loss:0.019, val_acc:0.986]
Epoch [120/120    avg_loss:0.017, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.98871332 0.99563319 0.93569845 0.90102389
 1.         0.97826087 1.         0.99893276 1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9914540924465343
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3067f9eac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.521, val_acc:0.442]
Epoch [2/120    avg_loss:2.239, val_acc:0.587]
Epoch [3/120    avg_loss:2.008, val_acc:0.617]
Epoch [4/120    avg_loss:1.773, val_acc:0.649]
Epoch [5/120    avg_loss:1.537, val_acc:0.724]
Epoch [6/120    avg_loss:1.273, val_acc:0.688]
Epoch [7/120    avg_loss:1.111, val_acc:0.722]
Epoch [8/120    avg_loss:0.995, val_acc:0.790]
Epoch [9/120    avg_loss:0.850, val_acc:0.821]
Epoch [10/120    avg_loss:0.786, val_acc:0.819]
Epoch [11/120    avg_loss:0.793, val_acc:0.776]
Epoch [12/120    avg_loss:0.660, val_acc:0.855]
Epoch [13/120    avg_loss:0.650, val_acc:0.768]
Epoch [14/120    avg_loss:0.585, val_acc:0.873]
Epoch [15/120    avg_loss:0.558, val_acc:0.893]
Epoch [16/120    avg_loss:0.526, val_acc:0.869]
Epoch [17/120    avg_loss:0.441, val_acc:0.863]
Epoch [18/120    avg_loss:0.397, val_acc:0.925]
Epoch [19/120    avg_loss:0.403, val_acc:0.887]
Epoch [20/120    avg_loss:0.426, val_acc:0.923]
Epoch [21/120    avg_loss:0.393, val_acc:0.919]
Epoch [22/120    avg_loss:0.371, val_acc:0.940]
Epoch [23/120    avg_loss:0.271, val_acc:0.937]
Epoch [24/120    avg_loss:0.350, val_acc:0.861]
Epoch [25/120    avg_loss:0.379, val_acc:0.913]
Epoch [26/120    avg_loss:0.344, val_acc:0.938]
Epoch [27/120    avg_loss:0.270, val_acc:0.944]
Epoch [28/120    avg_loss:0.225, val_acc:0.925]
Epoch [29/120    avg_loss:0.255, val_acc:0.940]
Epoch [30/120    avg_loss:0.244, val_acc:0.940]
Epoch [31/120    avg_loss:0.189, val_acc:0.960]
Epoch [32/120    avg_loss:0.222, val_acc:0.946]
Epoch [33/120    avg_loss:0.218, val_acc:0.940]
Epoch [34/120    avg_loss:0.210, val_acc:0.946]
Epoch [35/120    avg_loss:0.180, val_acc:0.940]
Epoch [36/120    avg_loss:0.171, val_acc:0.962]
Epoch [37/120    avg_loss:0.205, val_acc:0.917]
Epoch [38/120    avg_loss:0.161, val_acc:0.956]
Epoch [39/120    avg_loss:0.169, val_acc:0.948]
Epoch [40/120    avg_loss:0.224, val_acc:0.970]
Epoch [41/120    avg_loss:0.139, val_acc:0.938]
Epoch [42/120    avg_loss:0.196, val_acc:0.976]
Epoch [43/120    avg_loss:0.216, val_acc:0.948]
Epoch [44/120    avg_loss:0.181, val_acc:0.976]
Epoch [45/120    avg_loss:0.151, val_acc:0.982]
Epoch [46/120    avg_loss:0.102, val_acc:0.976]
Epoch [47/120    avg_loss:0.145, val_acc:0.958]
Epoch [48/120    avg_loss:0.149, val_acc:0.966]
Epoch [49/120    avg_loss:0.097, val_acc:0.978]
Epoch [50/120    avg_loss:0.088, val_acc:0.980]
Epoch [51/120    avg_loss:0.074, val_acc:0.988]
Epoch [52/120    avg_loss:0.070, val_acc:0.986]
Epoch [53/120    avg_loss:0.072, val_acc:0.988]
Epoch [54/120    avg_loss:0.098, val_acc:0.982]
Epoch [55/120    avg_loss:0.062, val_acc:0.980]
Epoch [56/120    avg_loss:0.094, val_acc:0.994]
Epoch [57/120    avg_loss:0.125, val_acc:0.964]
Epoch [58/120    avg_loss:0.100, val_acc:0.960]
Epoch [59/120    avg_loss:0.090, val_acc:0.988]
Epoch [60/120    avg_loss:0.071, val_acc:0.962]
Epoch [61/120    avg_loss:0.156, val_acc:0.974]
Epoch [62/120    avg_loss:0.076, val_acc:0.980]
Epoch [63/120    avg_loss:0.054, val_acc:0.978]
Epoch [64/120    avg_loss:0.039, val_acc:0.986]
Epoch [65/120    avg_loss:0.046, val_acc:0.984]
Epoch [66/120    avg_loss:0.047, val_acc:0.994]
Epoch [67/120    avg_loss:0.043, val_acc:0.978]
Epoch [68/120    avg_loss:0.048, val_acc:0.966]
Epoch [69/120    avg_loss:0.061, val_acc:0.980]
Epoch [70/120    avg_loss:0.034, val_acc:0.992]
Epoch [71/120    avg_loss:0.114, val_acc:0.958]
Epoch [72/120    avg_loss:0.086, val_acc:0.980]
Epoch [73/120    avg_loss:0.092, val_acc:0.980]
Epoch [74/120    avg_loss:0.047, val_acc:0.986]
Epoch [75/120    avg_loss:0.031, val_acc:0.988]
Epoch [76/120    avg_loss:0.071, val_acc:0.992]
Epoch [77/120    avg_loss:0.042, val_acc:0.994]
Epoch [78/120    avg_loss:0.026, val_acc:0.976]
Epoch [79/120    avg_loss:0.041, val_acc:0.988]
Epoch [80/120    avg_loss:0.073, val_acc:0.976]
Epoch [81/120    avg_loss:0.052, val_acc:0.984]
Epoch [82/120    avg_loss:0.021, val_acc:0.990]
Epoch [83/120    avg_loss:0.028, val_acc:0.990]
Epoch [84/120    avg_loss:0.022, val_acc:0.992]
Epoch [85/120    avg_loss:0.013, val_acc:0.994]
Epoch [86/120    avg_loss:0.014, val_acc:0.994]
Epoch [87/120    avg_loss:0.014, val_acc:0.994]
Epoch [88/120    avg_loss:0.016, val_acc:0.992]
Epoch [89/120    avg_loss:0.015, val_acc:0.994]
Epoch [90/120    avg_loss:0.012, val_acc:0.994]
Epoch [91/120    avg_loss:0.027, val_acc:0.974]
Epoch [92/120    avg_loss:0.019, val_acc:0.994]
Epoch [93/120    avg_loss:0.009, val_acc:0.994]
Epoch [94/120    avg_loss:0.020, val_acc:0.992]
Epoch [95/120    avg_loss:0.010, val_acc:0.994]
Epoch [96/120    avg_loss:0.014, val_acc:0.990]
Epoch [97/120    avg_loss:0.016, val_acc:0.992]
Epoch [98/120    avg_loss:0.017, val_acc:0.994]
Epoch [99/120    avg_loss:0.013, val_acc:0.996]
Epoch [100/120    avg_loss:0.011, val_acc:0.996]
Epoch [101/120    avg_loss:0.008, val_acc:0.996]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.026, val_acc:0.998]
Epoch [104/120    avg_loss:0.028, val_acc:0.994]
Epoch [105/120    avg_loss:0.032, val_acc:0.988]
Epoch [106/120    avg_loss:0.014, val_acc:0.996]
Epoch [107/120    avg_loss:0.011, val_acc:0.994]
Epoch [108/120    avg_loss:0.011, val_acc:0.994]
Epoch [109/120    avg_loss:0.014, val_acc:0.994]
Epoch [110/120    avg_loss:0.010, val_acc:0.992]
Epoch [111/120    avg_loss:0.010, val_acc:0.994]
Epoch [112/120    avg_loss:0.007, val_acc:0.996]
Epoch [113/120    avg_loss:0.008, val_acc:0.996]
Epoch [114/120    avg_loss:0.007, val_acc:0.994]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.012, val_acc:0.996]
Epoch [117/120    avg_loss:0.008, val_acc:0.996]
Epoch [118/120    avg_loss:0.010, val_acc:0.996]
Epoch [119/120    avg_loss:0.010, val_acc:0.996]
Epoch [120/120    avg_loss:0.009, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 1.         1.         0.99782135 0.969163   0.95532646
 1.         1.         1.         1.         1.         0.99472296
 0.99556541 1.        ]

Kappa:
0.9957272179405828
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a1b09aa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.513, val_acc:0.419]
Epoch [2/120    avg_loss:2.231, val_acc:0.625]
Epoch [3/120    avg_loss:1.971, val_acc:0.619]
Epoch [4/120    avg_loss:1.710, val_acc:0.675]
Epoch [5/120    avg_loss:1.433, val_acc:0.766]
Epoch [6/120    avg_loss:1.212, val_acc:0.823]
Epoch [7/120    avg_loss:1.053, val_acc:0.819]
Epoch [8/120    avg_loss:0.882, val_acc:0.831]
Epoch [9/120    avg_loss:0.802, val_acc:0.865]
Epoch [10/120    avg_loss:0.767, val_acc:0.903]
Epoch [11/120    avg_loss:0.643, val_acc:0.823]
Epoch [12/120    avg_loss:0.612, val_acc:0.881]
Epoch [13/120    avg_loss:0.569, val_acc:0.901]
Epoch [14/120    avg_loss:0.529, val_acc:0.881]
Epoch [15/120    avg_loss:0.537, val_acc:0.917]
Epoch [16/120    avg_loss:0.461, val_acc:0.929]
Epoch [17/120    avg_loss:0.413, val_acc:0.933]
Epoch [18/120    avg_loss:0.359, val_acc:0.954]
Epoch [19/120    avg_loss:0.349, val_acc:0.937]
Epoch [20/120    avg_loss:0.349, val_acc:0.948]
Epoch [21/120    avg_loss:0.359, val_acc:0.923]
Epoch [22/120    avg_loss:0.278, val_acc:0.956]
Epoch [23/120    avg_loss:0.302, val_acc:0.931]
Epoch [24/120    avg_loss:0.342, val_acc:0.948]
Epoch [25/120    avg_loss:0.262, val_acc:0.950]
Epoch [26/120    avg_loss:0.239, val_acc:0.958]
Epoch [27/120    avg_loss:0.315, val_acc:0.964]
Epoch [28/120    avg_loss:0.230, val_acc:0.956]
Epoch [29/120    avg_loss:0.266, val_acc:0.952]
Epoch [30/120    avg_loss:0.208, val_acc:0.950]
Epoch [31/120    avg_loss:0.261, val_acc:0.883]
Epoch [32/120    avg_loss:0.387, val_acc:0.942]
Epoch [33/120    avg_loss:0.240, val_acc:0.958]
Epoch [34/120    avg_loss:0.211, val_acc:0.931]
Epoch [35/120    avg_loss:0.278, val_acc:0.966]
Epoch [36/120    avg_loss:0.226, val_acc:0.960]
Epoch [37/120    avg_loss:0.164, val_acc:0.944]
Epoch [38/120    avg_loss:0.174, val_acc:0.923]
Epoch [39/120    avg_loss:0.263, val_acc:0.964]
Epoch [40/120    avg_loss:0.188, val_acc:0.962]
Epoch [41/120    avg_loss:0.211, val_acc:0.942]
Epoch [42/120    avg_loss:0.150, val_acc:0.980]
Epoch [43/120    avg_loss:0.218, val_acc:0.966]
Epoch [44/120    avg_loss:0.157, val_acc:0.948]
Epoch [45/120    avg_loss:0.130, val_acc:0.982]
Epoch [46/120    avg_loss:0.155, val_acc:0.970]
Epoch [47/120    avg_loss:0.140, val_acc:0.937]
Epoch [48/120    avg_loss:0.111, val_acc:0.972]
Epoch [49/120    avg_loss:0.179, val_acc:0.978]
Epoch [50/120    avg_loss:0.106, val_acc:0.984]
Epoch [51/120    avg_loss:0.085, val_acc:0.986]
Epoch [52/120    avg_loss:0.116, val_acc:0.972]
Epoch [53/120    avg_loss:0.123, val_acc:0.956]
Epoch [54/120    avg_loss:0.096, val_acc:0.978]
Epoch [55/120    avg_loss:0.078, val_acc:0.978]
Epoch [56/120    avg_loss:0.112, val_acc:0.944]
Epoch [57/120    avg_loss:0.130, val_acc:0.970]
Epoch [58/120    avg_loss:0.102, val_acc:0.901]
Epoch [59/120    avg_loss:0.113, val_acc:0.980]
Epoch [60/120    avg_loss:0.120, val_acc:0.978]
Epoch [61/120    avg_loss:0.094, val_acc:0.984]
Epoch [62/120    avg_loss:0.078, val_acc:0.976]
Epoch [63/120    avg_loss:0.055, val_acc:0.994]
Epoch [64/120    avg_loss:0.052, val_acc:0.994]
Epoch [65/120    avg_loss:0.054, val_acc:0.992]
Epoch [66/120    avg_loss:0.038, val_acc:0.990]
Epoch [67/120    avg_loss:0.084, val_acc:0.980]
Epoch [68/120    avg_loss:0.084, val_acc:0.986]
Epoch [69/120    avg_loss:0.062, val_acc:0.982]
Epoch [70/120    avg_loss:0.059, val_acc:0.994]
Epoch [71/120    avg_loss:0.054, val_acc:0.974]
Epoch [72/120    avg_loss:0.052, val_acc:0.972]
Epoch [73/120    avg_loss:0.086, val_acc:0.970]
Epoch [74/120    avg_loss:0.050, val_acc:0.988]
Epoch [75/120    avg_loss:0.055, val_acc:0.984]
Epoch [76/120    avg_loss:0.047, val_acc:0.990]
Epoch [77/120    avg_loss:0.038, val_acc:0.994]
Epoch [78/120    avg_loss:0.027, val_acc:0.988]
Epoch [79/120    avg_loss:0.023, val_acc:0.992]
Epoch [80/120    avg_loss:0.020, val_acc:0.986]
Epoch [81/120    avg_loss:0.026, val_acc:0.992]
Epoch [82/120    avg_loss:0.022, val_acc:0.996]
Epoch [83/120    avg_loss:0.026, val_acc:0.992]
Epoch [84/120    avg_loss:0.032, val_acc:0.990]
Epoch [85/120    avg_loss:0.033, val_acc:0.992]
Epoch [86/120    avg_loss:0.035, val_acc:0.994]
Epoch [87/120    avg_loss:0.020, val_acc:0.994]
Epoch [88/120    avg_loss:0.016, val_acc:0.992]
Epoch [89/120    avg_loss:0.020, val_acc:0.994]
Epoch [90/120    avg_loss:0.016, val_acc:0.980]
Epoch [91/120    avg_loss:0.024, val_acc:0.988]
Epoch [92/120    avg_loss:0.023, val_acc:0.992]
Epoch [93/120    avg_loss:0.021, val_acc:0.990]
Epoch [94/120    avg_loss:0.013, val_acc:0.992]
Epoch [95/120    avg_loss:0.015, val_acc:0.992]
Epoch [96/120    avg_loss:0.014, val_acc:0.992]
Epoch [97/120    avg_loss:0.014, val_acc:0.992]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.030, val_acc:0.992]
Epoch [100/120    avg_loss:0.009, val_acc:0.992]
Epoch [101/120    avg_loss:0.014, val_acc:0.994]
Epoch [102/120    avg_loss:0.013, val_acc:0.994]
Epoch [103/120    avg_loss:0.010, val_acc:0.992]
Epoch [104/120    avg_loss:0.009, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.992]
Epoch [106/120    avg_loss:0.009, val_acc:0.992]
Epoch [107/120    avg_loss:0.012, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.009, val_acc:0.992]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.012, val_acc:0.992]
Epoch [116/120    avg_loss:0.008, val_acc:0.992]
Epoch [117/120    avg_loss:0.012, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.009, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.98426966 1.         0.94273128 0.91034483
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9921660650459055
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47d26e8ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.497, val_acc:0.327]
Epoch [2/120    avg_loss:2.225, val_acc:0.526]
Epoch [3/120    avg_loss:2.004, val_acc:0.651]
Epoch [4/120    avg_loss:1.801, val_acc:0.681]
Epoch [5/120    avg_loss:1.570, val_acc:0.696]
Epoch [6/120    avg_loss:1.347, val_acc:0.685]
Epoch [7/120    avg_loss:1.107, val_acc:0.788]
Epoch [8/120    avg_loss:0.990, val_acc:0.817]
Epoch [9/120    avg_loss:0.820, val_acc:0.823]
Epoch [10/120    avg_loss:0.824, val_acc:0.833]
Epoch [11/120    avg_loss:0.733, val_acc:0.883]
Epoch [12/120    avg_loss:0.688, val_acc:0.901]
Epoch [13/120    avg_loss:0.598, val_acc:0.911]
Epoch [14/120    avg_loss:0.611, val_acc:0.873]
Epoch [15/120    avg_loss:0.586, val_acc:0.893]
Epoch [16/120    avg_loss:0.525, val_acc:0.891]
Epoch [17/120    avg_loss:0.498, val_acc:0.919]
Epoch [18/120    avg_loss:0.448, val_acc:0.897]
Epoch [19/120    avg_loss:0.441, val_acc:0.907]
Epoch [20/120    avg_loss:0.395, val_acc:0.927]
Epoch [21/120    avg_loss:0.457, val_acc:0.909]
Epoch [22/120    avg_loss:0.416, val_acc:0.935]
Epoch [23/120    avg_loss:0.450, val_acc:0.909]
Epoch [24/120    avg_loss:0.429, val_acc:0.909]
Epoch [25/120    avg_loss:0.433, val_acc:0.915]
Epoch [26/120    avg_loss:0.347, val_acc:0.925]
Epoch [27/120    avg_loss:0.375, val_acc:0.917]
Epoch [28/120    avg_loss:0.295, val_acc:0.952]
Epoch [29/120    avg_loss:0.245, val_acc:0.942]
Epoch [30/120    avg_loss:0.303, val_acc:0.927]
Epoch [31/120    avg_loss:0.288, val_acc:0.942]
Epoch [32/120    avg_loss:0.310, val_acc:0.956]
Epoch [33/120    avg_loss:0.273, val_acc:0.948]
Epoch [34/120    avg_loss:0.283, val_acc:0.960]
Epoch [35/120    avg_loss:0.314, val_acc:0.954]
Epoch [36/120    avg_loss:0.261, val_acc:0.935]
Epoch [37/120    avg_loss:0.272, val_acc:0.946]
Epoch [38/120    avg_loss:0.192, val_acc:0.950]
Epoch [39/120    avg_loss:0.217, val_acc:0.940]
Epoch [40/120    avg_loss:0.185, val_acc:0.968]
Epoch [41/120    avg_loss:0.172, val_acc:0.966]
Epoch [42/120    avg_loss:0.178, val_acc:0.966]
Epoch [43/120    avg_loss:0.176, val_acc:0.958]
Epoch [44/120    avg_loss:0.226, val_acc:0.929]
Epoch [45/120    avg_loss:0.220, val_acc:0.946]
Epoch [46/120    avg_loss:0.195, val_acc:0.982]
Epoch [47/120    avg_loss:0.161, val_acc:0.968]
Epoch [48/120    avg_loss:0.143, val_acc:0.938]
Epoch [49/120    avg_loss:0.171, val_acc:0.976]
Epoch [50/120    avg_loss:0.111, val_acc:0.970]
Epoch [51/120    avg_loss:0.131, val_acc:0.980]
Epoch [52/120    avg_loss:0.121, val_acc:0.970]
Epoch [53/120    avg_loss:0.140, val_acc:0.962]
Epoch [54/120    avg_loss:0.133, val_acc:0.980]
Epoch [55/120    avg_loss:0.099, val_acc:0.942]
Epoch [56/120    avg_loss:0.173, val_acc:0.978]
Epoch [57/120    avg_loss:0.131, val_acc:0.962]
Epoch [58/120    avg_loss:0.126, val_acc:0.982]
Epoch [59/120    avg_loss:0.116, val_acc:0.952]
Epoch [60/120    avg_loss:0.092, val_acc:0.978]
Epoch [61/120    avg_loss:0.073, val_acc:0.990]
Epoch [62/120    avg_loss:0.061, val_acc:0.982]
Epoch [63/120    avg_loss:0.069, val_acc:0.972]
Epoch [64/120    avg_loss:0.128, val_acc:0.978]
Epoch [65/120    avg_loss:0.140, val_acc:0.984]
Epoch [66/120    avg_loss:0.074, val_acc:0.970]
Epoch [67/120    avg_loss:0.137, val_acc:0.958]
Epoch [68/120    avg_loss:0.084, val_acc:0.970]
Epoch [69/120    avg_loss:0.074, val_acc:0.988]
Epoch [70/120    avg_loss:0.065, val_acc:0.982]
Epoch [71/120    avg_loss:0.043, val_acc:0.986]
Epoch [72/120    avg_loss:0.040, val_acc:0.990]
Epoch [73/120    avg_loss:0.050, val_acc:0.982]
Epoch [74/120    avg_loss:0.068, val_acc:0.988]
Epoch [75/120    avg_loss:0.099, val_acc:0.984]
Epoch [76/120    avg_loss:0.050, val_acc:0.990]
Epoch [77/120    avg_loss:0.030, val_acc:0.992]
Epoch [78/120    avg_loss:0.043, val_acc:0.994]
Epoch [79/120    avg_loss:0.035, val_acc:0.992]
Epoch [80/120    avg_loss:0.030, val_acc:0.992]
Epoch [81/120    avg_loss:0.077, val_acc:0.986]
Epoch [82/120    avg_loss:0.094, val_acc:0.984]
Epoch [83/120    avg_loss:0.042, val_acc:0.990]
Epoch [84/120    avg_loss:0.042, val_acc:0.976]
Epoch [85/120    avg_loss:0.065, val_acc:0.982]
Epoch [86/120    avg_loss:0.052, val_acc:0.988]
Epoch [87/120    avg_loss:0.052, val_acc:0.986]
Epoch [88/120    avg_loss:0.044, val_acc:0.992]
Epoch [89/120    avg_loss:0.075, val_acc:0.986]
Epoch [90/120    avg_loss:0.045, val_acc:0.990]
Epoch [91/120    avg_loss:0.048, val_acc:0.982]
Epoch [92/120    avg_loss:0.035, val_acc:0.990]
Epoch [93/120    avg_loss:0.019, val_acc:0.994]
Epoch [94/120    avg_loss:0.017, val_acc:0.994]
Epoch [95/120    avg_loss:0.015, val_acc:0.994]
Epoch [96/120    avg_loss:0.015, val_acc:0.994]
Epoch [97/120    avg_loss:0.014, val_acc:0.994]
Epoch [98/120    avg_loss:0.013, val_acc:0.996]
Epoch [99/120    avg_loss:0.017, val_acc:0.996]
Epoch [100/120    avg_loss:0.020, val_acc:0.996]
Epoch [101/120    avg_loss:0.021, val_acc:0.996]
Epoch [102/120    avg_loss:0.023, val_acc:0.996]
Epoch [103/120    avg_loss:0.011, val_acc:0.996]
Epoch [104/120    avg_loss:0.019, val_acc:0.996]
Epoch [105/120    avg_loss:0.017, val_acc:0.996]
Epoch [106/120    avg_loss:0.014, val_acc:0.996]
Epoch [107/120    avg_loss:0.016, val_acc:0.996]
Epoch [108/120    avg_loss:0.011, val_acc:0.996]
Epoch [109/120    avg_loss:0.015, val_acc:0.996]
Epoch [110/120    avg_loss:0.016, val_acc:0.996]
Epoch [111/120    avg_loss:0.011, val_acc:0.996]
Epoch [112/120    avg_loss:0.012, val_acc:0.996]
Epoch [113/120    avg_loss:0.011, val_acc:0.996]
Epoch [114/120    avg_loss:0.015, val_acc:0.996]
Epoch [115/120    avg_loss:0.012, val_acc:0.996]
Epoch [116/120    avg_loss:0.012, val_acc:0.996]
Epoch [117/120    avg_loss:0.013, val_acc:0.996]
Epoch [118/120    avg_loss:0.012, val_acc:0.996]
Epoch [119/120    avg_loss:0.013, val_acc:0.996]
Epoch [120/120    avg_loss:0.012, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.99319728 0.99782135 0.9379015  0.89530686
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924032916707266
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f883e827a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.456, val_acc:0.448]
Epoch [2/120    avg_loss:2.151, val_acc:0.514]
Epoch [3/120    avg_loss:1.902, val_acc:0.562]
Epoch [4/120    avg_loss:1.655, val_acc:0.643]
Epoch [5/120    avg_loss:1.409, val_acc:0.734]
Epoch [6/120    avg_loss:1.226, val_acc:0.742]
Epoch [7/120    avg_loss:1.058, val_acc:0.768]
Epoch [8/120    avg_loss:0.907, val_acc:0.810]
Epoch [9/120    avg_loss:0.817, val_acc:0.827]
Epoch [10/120    avg_loss:0.732, val_acc:0.798]
Epoch [11/120    avg_loss:0.654, val_acc:0.758]
Epoch [12/120    avg_loss:0.637, val_acc:0.736]
Epoch [13/120    avg_loss:0.730, val_acc:0.839]
Epoch [14/120    avg_loss:0.539, val_acc:0.875]
Epoch [15/120    avg_loss:0.508, val_acc:0.887]
Epoch [16/120    avg_loss:0.464, val_acc:0.899]
Epoch [17/120    avg_loss:0.455, val_acc:0.891]
Epoch [18/120    avg_loss:0.439, val_acc:0.889]
Epoch [19/120    avg_loss:0.461, val_acc:0.871]
Epoch [20/120    avg_loss:0.467, val_acc:0.865]
Epoch [21/120    avg_loss:0.394, val_acc:0.923]
Epoch [22/120    avg_loss:0.337, val_acc:0.903]
Epoch [23/120    avg_loss:0.404, val_acc:0.925]
Epoch [24/120    avg_loss:0.320, val_acc:0.925]
Epoch [25/120    avg_loss:0.321, val_acc:0.921]
Epoch [26/120    avg_loss:0.271, val_acc:0.925]
Epoch [27/120    avg_loss:0.320, val_acc:0.903]
Epoch [28/120    avg_loss:0.330, val_acc:0.931]
Epoch [29/120    avg_loss:0.263, val_acc:0.952]
Epoch [30/120    avg_loss:0.247, val_acc:0.925]
Epoch [31/120    avg_loss:0.260, val_acc:0.927]
Epoch [32/120    avg_loss:0.266, val_acc:0.960]
Epoch [33/120    avg_loss:0.209, val_acc:0.946]
Epoch [34/120    avg_loss:0.255, val_acc:0.919]
Epoch [35/120    avg_loss:0.249, val_acc:0.933]
Epoch [36/120    avg_loss:0.233, val_acc:0.964]
Epoch [37/120    avg_loss:0.235, val_acc:0.960]
Epoch [38/120    avg_loss:0.210, val_acc:0.978]
Epoch [39/120    avg_loss:0.180, val_acc:0.956]
Epoch [40/120    avg_loss:0.190, val_acc:0.960]
Epoch [41/120    avg_loss:0.161, val_acc:0.966]
Epoch [42/120    avg_loss:0.133, val_acc:0.960]
Epoch [43/120    avg_loss:0.152, val_acc:0.980]
Epoch [44/120    avg_loss:0.178, val_acc:0.923]
Epoch [45/120    avg_loss:0.147, val_acc:0.978]
Epoch [46/120    avg_loss:0.137, val_acc:0.966]
Epoch [47/120    avg_loss:0.161, val_acc:0.966]
Epoch [48/120    avg_loss:0.113, val_acc:0.990]
Epoch [49/120    avg_loss:0.086, val_acc:0.978]
Epoch [50/120    avg_loss:0.082, val_acc:0.976]
Epoch [51/120    avg_loss:0.122, val_acc:0.968]
Epoch [52/120    avg_loss:0.125, val_acc:0.970]
Epoch [53/120    avg_loss:0.094, val_acc:0.978]
Epoch [54/120    avg_loss:0.125, val_acc:0.980]
Epoch [55/120    avg_loss:0.158, val_acc:0.972]
Epoch [56/120    avg_loss:0.086, val_acc:0.986]
Epoch [57/120    avg_loss:0.118, val_acc:0.966]
Epoch [58/120    avg_loss:0.100, val_acc:0.978]
Epoch [59/120    avg_loss:0.117, val_acc:0.956]
Epoch [60/120    avg_loss:0.082, val_acc:0.978]
Epoch [61/120    avg_loss:0.086, val_acc:0.952]
Epoch [62/120    avg_loss:0.078, val_acc:0.984]
Epoch [63/120    avg_loss:0.043, val_acc:0.988]
Epoch [64/120    avg_loss:0.051, val_acc:0.990]
Epoch [65/120    avg_loss:0.043, val_acc:0.990]
Epoch [66/120    avg_loss:0.044, val_acc:0.990]
Epoch [67/120    avg_loss:0.043, val_acc:0.990]
Epoch [68/120    avg_loss:0.043, val_acc:0.986]
Epoch [69/120    avg_loss:0.044, val_acc:0.992]
Epoch [70/120    avg_loss:0.036, val_acc:0.992]
Epoch [71/120    avg_loss:0.036, val_acc:0.994]
Epoch [72/120    avg_loss:0.036, val_acc:0.994]
Epoch [73/120    avg_loss:0.035, val_acc:0.992]
Epoch [74/120    avg_loss:0.043, val_acc:0.994]
Epoch [75/120    avg_loss:0.035, val_acc:0.992]
Epoch [76/120    avg_loss:0.044, val_acc:0.994]
Epoch [77/120    avg_loss:0.037, val_acc:0.994]
Epoch [78/120    avg_loss:0.035, val_acc:0.994]
Epoch [79/120    avg_loss:0.041, val_acc:0.994]
Epoch [80/120    avg_loss:0.034, val_acc:0.990]
Epoch [81/120    avg_loss:0.049, val_acc:0.994]
Epoch [82/120    avg_loss:0.040, val_acc:0.992]
Epoch [83/120    avg_loss:0.032, val_acc:0.994]
Epoch [84/120    avg_loss:0.041, val_acc:0.994]
Epoch [85/120    avg_loss:0.032, val_acc:0.994]
Epoch [86/120    avg_loss:0.039, val_acc:0.994]
Epoch [87/120    avg_loss:0.037, val_acc:0.994]
Epoch [88/120    avg_loss:0.035, val_acc:0.994]
Epoch [89/120    avg_loss:0.030, val_acc:0.994]
Epoch [90/120    avg_loss:0.045, val_acc:0.992]
Epoch [91/120    avg_loss:0.033, val_acc:0.992]
Epoch [92/120    avg_loss:0.034, val_acc:0.992]
Epoch [93/120    avg_loss:0.028, val_acc:0.992]
Epoch [94/120    avg_loss:0.036, val_acc:0.992]
Epoch [95/120    avg_loss:0.033, val_acc:0.990]
Epoch [96/120    avg_loss:0.029, val_acc:0.990]
Epoch [97/120    avg_loss:0.028, val_acc:0.992]
Epoch [98/120    avg_loss:0.036, val_acc:0.992]
Epoch [99/120    avg_loss:0.028, val_acc:0.992]
Epoch [100/120    avg_loss:0.029, val_acc:0.992]
Epoch [101/120    avg_loss:0.028, val_acc:0.994]
Epoch [102/120    avg_loss:0.030, val_acc:0.994]
Epoch [103/120    avg_loss:0.025, val_acc:0.994]
Epoch [104/120    avg_loss:0.029, val_acc:0.996]
Epoch [105/120    avg_loss:0.029, val_acc:0.994]
Epoch [106/120    avg_loss:0.029, val_acc:0.994]
Epoch [107/120    avg_loss:0.028, val_acc:0.994]
Epoch [108/120    avg_loss:0.025, val_acc:0.994]
Epoch [109/120    avg_loss:0.028, val_acc:0.992]
Epoch [110/120    avg_loss:0.026, val_acc:0.992]
Epoch [111/120    avg_loss:0.023, val_acc:0.992]
Epoch [112/120    avg_loss:0.027, val_acc:0.994]
Epoch [113/120    avg_loss:0.026, val_acc:0.992]
Epoch [114/120    avg_loss:0.028, val_acc:0.992]
Epoch [115/120    avg_loss:0.023, val_acc:0.992]
Epoch [116/120    avg_loss:0.029, val_acc:0.992]
Epoch [117/120    avg_loss:0.026, val_acc:0.992]
Epoch [118/120    avg_loss:0.030, val_acc:0.992]
Epoch [119/120    avg_loss:0.027, val_acc:0.992]
Epoch [120/120    avg_loss:0.028, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 225   0   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.99545455 0.98901099 0.93569845 0.90847458
 0.99512195 1.         0.99614891 1.         1.         0.98950131
 0.99109131 1.        ]

Kappa:
0.9900302433161179
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b21799a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.528, val_acc:0.460]
Epoch [2/120    avg_loss:2.277, val_acc:0.589]
Epoch [3/120    avg_loss:2.078, val_acc:0.621]
Epoch [4/120    avg_loss:1.852, val_acc:0.675]
Epoch [5/120    avg_loss:1.568, val_acc:0.736]
Epoch [6/120    avg_loss:1.388, val_acc:0.732]
Epoch [7/120    avg_loss:1.169, val_acc:0.774]
Epoch [8/120    avg_loss:1.029, val_acc:0.794]
Epoch [9/120    avg_loss:0.929, val_acc:0.829]
Epoch [10/120    avg_loss:0.805, val_acc:0.825]
Epoch [11/120    avg_loss:0.754, val_acc:0.853]
Epoch [12/120    avg_loss:0.701, val_acc:0.819]
Epoch [13/120    avg_loss:0.631, val_acc:0.895]
Epoch [14/120    avg_loss:0.554, val_acc:0.841]
Epoch [15/120    avg_loss:0.477, val_acc:0.893]
Epoch [16/120    avg_loss:0.495, val_acc:0.881]
Epoch [17/120    avg_loss:0.431, val_acc:0.925]
Epoch [18/120    avg_loss:0.422, val_acc:0.889]
Epoch [19/120    avg_loss:0.406, val_acc:0.871]
Epoch [20/120    avg_loss:0.439, val_acc:0.919]
Epoch [21/120    avg_loss:0.467, val_acc:0.887]
Epoch [22/120    avg_loss:0.402, val_acc:0.913]
Epoch [23/120    avg_loss:0.372, val_acc:0.938]
Epoch [24/120    avg_loss:0.327, val_acc:0.931]
Epoch [25/120    avg_loss:0.323, val_acc:0.940]
Epoch [26/120    avg_loss:0.341, val_acc:0.913]
Epoch [27/120    avg_loss:0.299, val_acc:0.923]
Epoch [28/120    avg_loss:0.341, val_acc:0.925]
Epoch [29/120    avg_loss:0.363, val_acc:0.923]
Epoch [30/120    avg_loss:0.291, val_acc:0.901]
Epoch [31/120    avg_loss:0.284, val_acc:0.940]
Epoch [32/120    avg_loss:0.279, val_acc:0.956]
Epoch [33/120    avg_loss:0.228, val_acc:0.956]
Epoch [34/120    avg_loss:0.328, val_acc:0.940]
Epoch [35/120    avg_loss:0.284, val_acc:0.929]
Epoch [36/120    avg_loss:0.303, val_acc:0.950]
Epoch [37/120    avg_loss:0.271, val_acc:0.942]
Epoch [38/120    avg_loss:0.201, val_acc:0.948]
Epoch [39/120    avg_loss:0.205, val_acc:0.952]
Epoch [40/120    avg_loss:0.215, val_acc:0.929]
Epoch [41/120    avg_loss:0.198, val_acc:0.944]
Epoch [42/120    avg_loss:0.184, val_acc:0.964]
Epoch [43/120    avg_loss:0.200, val_acc:0.877]
Epoch [44/120    avg_loss:0.209, val_acc:0.950]
Epoch [45/120    avg_loss:0.156, val_acc:0.952]
Epoch [46/120    avg_loss:0.199, val_acc:0.938]
Epoch [47/120    avg_loss:0.155, val_acc:0.958]
Epoch [48/120    avg_loss:0.194, val_acc:0.942]
Epoch [49/120    avg_loss:0.149, val_acc:0.962]
Epoch [50/120    avg_loss:0.141, val_acc:0.956]
Epoch [51/120    avg_loss:0.151, val_acc:0.968]
Epoch [52/120    avg_loss:0.122, val_acc:0.958]
Epoch [53/120    avg_loss:0.137, val_acc:0.968]
Epoch [54/120    avg_loss:0.129, val_acc:0.964]
Epoch [55/120    avg_loss:0.159, val_acc:0.950]
Epoch [56/120    avg_loss:0.109, val_acc:0.964]
Epoch [57/120    avg_loss:0.124, val_acc:0.962]
Epoch [58/120    avg_loss:0.093, val_acc:0.974]
Epoch [59/120    avg_loss:0.118, val_acc:0.976]
Epoch [60/120    avg_loss:0.095, val_acc:0.935]
Epoch [61/120    avg_loss:0.138, val_acc:0.978]
Epoch [62/120    avg_loss:0.130, val_acc:0.980]
Epoch [63/120    avg_loss:0.140, val_acc:0.982]
Epoch [64/120    avg_loss:0.100, val_acc:0.970]
Epoch [65/120    avg_loss:0.108, val_acc:0.958]
Epoch [66/120    avg_loss:0.151, val_acc:0.968]
Epoch [67/120    avg_loss:0.083, val_acc:0.982]
Epoch [68/120    avg_loss:0.077, val_acc:0.978]
Epoch [69/120    avg_loss:0.099, val_acc:0.978]
Epoch [70/120    avg_loss:0.057, val_acc:0.980]
Epoch [71/120    avg_loss:0.069, val_acc:0.976]
Epoch [72/120    avg_loss:0.153, val_acc:0.956]
Epoch [73/120    avg_loss:0.098, val_acc:0.976]
Epoch [74/120    avg_loss:0.103, val_acc:0.970]
Epoch [75/120    avg_loss:0.086, val_acc:0.952]
Epoch [76/120    avg_loss:0.181, val_acc:0.968]
Epoch [77/120    avg_loss:0.100, val_acc:0.978]
Epoch [78/120    avg_loss:0.049, val_acc:0.982]
Epoch [79/120    avg_loss:0.069, val_acc:0.988]
Epoch [80/120    avg_loss:0.070, val_acc:0.976]
Epoch [81/120    avg_loss:0.082, val_acc:0.984]
Epoch [82/120    avg_loss:0.076, val_acc:0.984]
Epoch [83/120    avg_loss:0.106, val_acc:0.990]
Epoch [84/120    avg_loss:0.061, val_acc:0.986]
Epoch [85/120    avg_loss:0.043, val_acc:0.992]
Epoch [86/120    avg_loss:0.060, val_acc:0.956]
Epoch [87/120    avg_loss:0.054, val_acc:0.982]
Epoch [88/120    avg_loss:0.034, val_acc:0.986]
Epoch [89/120    avg_loss:0.041, val_acc:0.978]
Epoch [90/120    avg_loss:0.048, val_acc:0.980]
Epoch [91/120    avg_loss:0.109, val_acc:0.962]
Epoch [92/120    avg_loss:0.046, val_acc:0.978]
Epoch [93/120    avg_loss:0.032, val_acc:0.984]
Epoch [94/120    avg_loss:0.053, val_acc:0.980]
Epoch [95/120    avg_loss:0.040, val_acc:0.982]
Epoch [96/120    avg_loss:0.037, val_acc:0.984]
Epoch [97/120    avg_loss:0.023, val_acc:0.992]
Epoch [98/120    avg_loss:0.025, val_acc:0.984]
Epoch [99/120    avg_loss:0.037, val_acc:0.992]
Epoch [100/120    avg_loss:0.034, val_acc:0.992]
Epoch [101/120    avg_loss:0.017, val_acc:0.988]
Epoch [102/120    avg_loss:0.023, val_acc:0.984]
Epoch [103/120    avg_loss:0.029, val_acc:0.994]
Epoch [104/120    avg_loss:0.024, val_acc:0.992]
Epoch [105/120    avg_loss:0.017, val_acc:0.990]
Epoch [106/120    avg_loss:0.016, val_acc:0.992]
Epoch [107/120    avg_loss:0.045, val_acc:0.984]
Epoch [108/120    avg_loss:0.030, val_acc:0.982]
Epoch [109/120    avg_loss:0.045, val_acc:0.994]
Epoch [110/120    avg_loss:0.023, val_acc:0.990]
Epoch [111/120    avg_loss:0.028, val_acc:0.992]
Epoch [112/120    avg_loss:0.040, val_acc:0.988]
Epoch [113/120    avg_loss:0.021, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.990]
Epoch [115/120    avg_loss:0.018, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.013, val_acc:0.992]
Epoch [118/120    avg_loss:0.017, val_acc:0.992]
Epoch [119/120    avg_loss:0.019, val_acc:0.992]
Epoch [120/120    avg_loss:0.017, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0 227   2   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.997815   0.99090909 0.99343545 0.95454545 0.94117647
 0.99266504 0.98378378 0.998713   0.99893276 1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9926409242258152
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb1da891b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.540, val_acc:0.411]
Epoch [2/120    avg_loss:2.258, val_acc:0.490]
Epoch [3/120    avg_loss:2.007, val_acc:0.554]
Epoch [4/120    avg_loss:1.776, val_acc:0.655]
Epoch [5/120    avg_loss:1.531, val_acc:0.675]
Epoch [6/120    avg_loss:1.304, val_acc:0.730]
Epoch [7/120    avg_loss:1.155, val_acc:0.778]
Epoch [8/120    avg_loss:1.029, val_acc:0.776]
Epoch [9/120    avg_loss:0.904, val_acc:0.829]
Epoch [10/120    avg_loss:0.804, val_acc:0.849]
Epoch [11/120    avg_loss:0.718, val_acc:0.869]
Epoch [12/120    avg_loss:0.699, val_acc:0.776]
Epoch [13/120    avg_loss:0.712, val_acc:0.861]
Epoch [14/120    avg_loss:0.581, val_acc:0.859]
Epoch [15/120    avg_loss:0.561, val_acc:0.891]
Epoch [16/120    avg_loss:0.488, val_acc:0.909]
Epoch [17/120    avg_loss:0.506, val_acc:0.895]
Epoch [18/120    avg_loss:0.431, val_acc:0.881]
Epoch [19/120    avg_loss:0.415, val_acc:0.869]
Epoch [20/120    avg_loss:0.400, val_acc:0.887]
Epoch [21/120    avg_loss:0.436, val_acc:0.913]
Epoch [22/120    avg_loss:0.356, val_acc:0.923]
Epoch [23/120    avg_loss:0.330, val_acc:0.938]
Epoch [24/120    avg_loss:0.342, val_acc:0.950]
Epoch [25/120    avg_loss:0.330, val_acc:0.899]
Epoch [26/120    avg_loss:0.297, val_acc:0.925]
Epoch [27/120    avg_loss:0.279, val_acc:0.952]
Epoch [28/120    avg_loss:0.291, val_acc:0.899]
Epoch [29/120    avg_loss:0.319, val_acc:0.917]
Epoch [30/120    avg_loss:0.252, val_acc:0.933]
Epoch [31/120    avg_loss:0.297, val_acc:0.931]
Epoch [32/120    avg_loss:0.281, val_acc:0.911]
Epoch [33/120    avg_loss:0.235, val_acc:0.935]
Epoch [34/120    avg_loss:0.229, val_acc:0.927]
Epoch [35/120    avg_loss:0.281, val_acc:0.966]
Epoch [36/120    avg_loss:0.196, val_acc:0.954]
Epoch [37/120    avg_loss:0.239, val_acc:0.960]
Epoch [38/120    avg_loss:0.332, val_acc:0.940]
Epoch [39/120    avg_loss:0.183, val_acc:0.962]
Epoch [40/120    avg_loss:0.203, val_acc:0.960]
Epoch [41/120    avg_loss:0.203, val_acc:0.925]
Epoch [42/120    avg_loss:0.181, val_acc:0.962]
Epoch [43/120    avg_loss:0.175, val_acc:0.962]
Epoch [44/120    avg_loss:0.151, val_acc:0.958]
Epoch [45/120    avg_loss:0.157, val_acc:0.974]
Epoch [46/120    avg_loss:0.113, val_acc:0.978]
Epoch [47/120    avg_loss:0.219, val_acc:0.966]
Epoch [48/120    avg_loss:0.142, val_acc:0.958]
Epoch [49/120    avg_loss:0.136, val_acc:0.966]
Epoch [50/120    avg_loss:0.126, val_acc:0.978]
Epoch [51/120    avg_loss:0.149, val_acc:0.966]
Epoch [52/120    avg_loss:0.106, val_acc:0.976]
Epoch [53/120    avg_loss:0.124, val_acc:0.966]
Epoch [54/120    avg_loss:0.121, val_acc:0.948]
Epoch [55/120    avg_loss:0.144, val_acc:0.968]
Epoch [56/120    avg_loss:0.097, val_acc:0.978]
Epoch [57/120    avg_loss:0.113, val_acc:0.974]
Epoch [58/120    avg_loss:0.090, val_acc:0.978]
Epoch [59/120    avg_loss:0.131, val_acc:0.948]
Epoch [60/120    avg_loss:0.118, val_acc:0.952]
Epoch [61/120    avg_loss:0.103, val_acc:0.972]
Epoch [62/120    avg_loss:0.087, val_acc:0.974]
Epoch [63/120    avg_loss:0.077, val_acc:0.972]
Epoch [64/120    avg_loss:0.101, val_acc:0.972]
Epoch [65/120    avg_loss:0.083, val_acc:0.972]
Epoch [66/120    avg_loss:0.105, val_acc:0.978]
Epoch [67/120    avg_loss:0.069, val_acc:0.980]
Epoch [68/120    avg_loss:0.134, val_acc:0.970]
Epoch [69/120    avg_loss:0.069, val_acc:0.980]
Epoch [70/120    avg_loss:0.077, val_acc:0.972]
Epoch [71/120    avg_loss:0.047, val_acc:0.976]
Epoch [72/120    avg_loss:0.075, val_acc:0.980]
Epoch [73/120    avg_loss:0.055, val_acc:0.980]
Epoch [74/120    avg_loss:0.047, val_acc:0.988]
Epoch [75/120    avg_loss:0.029, val_acc:0.986]
Epoch [76/120    avg_loss:0.080, val_acc:0.984]
Epoch [77/120    avg_loss:0.077, val_acc:0.988]
Epoch [78/120    avg_loss:0.034, val_acc:0.974]
Epoch [79/120    avg_loss:0.045, val_acc:0.954]
Epoch [80/120    avg_loss:0.095, val_acc:0.984]
Epoch [81/120    avg_loss:0.048, val_acc:0.976]
Epoch [82/120    avg_loss:0.065, val_acc:0.986]
Epoch [83/120    avg_loss:0.038, val_acc:0.988]
Epoch [84/120    avg_loss:0.056, val_acc:0.982]
Epoch [85/120    avg_loss:0.031, val_acc:0.988]
Epoch [86/120    avg_loss:0.021, val_acc:0.986]
Epoch [87/120    avg_loss:0.024, val_acc:0.990]
Epoch [88/120    avg_loss:0.032, val_acc:0.986]
Epoch [89/120    avg_loss:0.025, val_acc:0.992]
Epoch [90/120    avg_loss:0.018, val_acc:0.990]
Epoch [91/120    avg_loss:0.017, val_acc:0.986]
Epoch [92/120    avg_loss:0.020, val_acc:0.982]
Epoch [93/120    avg_loss:0.021, val_acc:0.986]
Epoch [94/120    avg_loss:0.026, val_acc:0.982]
Epoch [95/120    avg_loss:0.014, val_acc:0.990]
Epoch [96/120    avg_loss:0.020, val_acc:0.990]
Epoch [97/120    avg_loss:0.015, val_acc:0.988]
Epoch [98/120    avg_loss:0.018, val_acc:0.954]
Epoch [99/120    avg_loss:0.060, val_acc:0.984]
Epoch [100/120    avg_loss:0.025, val_acc:0.986]
Epoch [101/120    avg_loss:0.033, val_acc:0.984]
Epoch [102/120    avg_loss:0.073, val_acc:0.976]
Epoch [103/120    avg_loss:0.041, val_acc:0.984]
Epoch [104/120    avg_loss:0.020, val_acc:0.986]
Epoch [105/120    avg_loss:0.021, val_acc:0.986]
Epoch [106/120    avg_loss:0.019, val_acc:0.986]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.016, val_acc:0.986]
Epoch [109/120    avg_loss:0.020, val_acc:0.986]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.016, val_acc:0.986]
Epoch [112/120    avg_loss:0.020, val_acc:0.986]
Epoch [113/120    avg_loss:0.012, val_acc:0.986]
Epoch [114/120    avg_loss:0.013, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.019, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.011, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99926954 0.98871332 0.99563319 0.94432071 0.92255892
 0.99757869 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9926413365466686
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f82ef95fa20>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.520, val_acc:0.405]
Epoch [2/120    avg_loss:2.211, val_acc:0.438]
Epoch [3/120    avg_loss:2.026, val_acc:0.508]
Epoch [4/120    avg_loss:1.799, val_acc:0.585]
Epoch [5/120    avg_loss:1.590, val_acc:0.643]
Epoch [6/120    avg_loss:1.375, val_acc:0.702]
Epoch [7/120    avg_loss:1.191, val_acc:0.736]
Epoch [8/120    avg_loss:1.006, val_acc:0.802]
Epoch [9/120    avg_loss:0.851, val_acc:0.849]
Epoch [10/120    avg_loss:0.762, val_acc:0.881]
Epoch [11/120    avg_loss:0.617, val_acc:0.883]
Epoch [12/120    avg_loss:0.595, val_acc:0.863]
Epoch [13/120    avg_loss:0.559, val_acc:0.887]
Epoch [14/120    avg_loss:0.493, val_acc:0.845]
Epoch [15/120    avg_loss:0.529, val_acc:0.851]
Epoch [16/120    avg_loss:0.465, val_acc:0.889]
Epoch [17/120    avg_loss:0.442, val_acc:0.899]
Epoch [18/120    avg_loss:0.365, val_acc:0.889]
Epoch [19/120    avg_loss:0.447, val_acc:0.923]
Epoch [20/120    avg_loss:0.366, val_acc:0.929]
Epoch [21/120    avg_loss:0.380, val_acc:0.909]
Epoch [22/120    avg_loss:0.299, val_acc:0.929]
Epoch [23/120    avg_loss:0.299, val_acc:0.911]
Epoch [24/120    avg_loss:0.333, val_acc:0.855]
Epoch [25/120    avg_loss:0.341, val_acc:0.925]
Epoch [26/120    avg_loss:0.319, val_acc:0.911]
Epoch [27/120    avg_loss:0.263, val_acc:0.946]
Epoch [28/120    avg_loss:0.201, val_acc:0.946]
Epoch [29/120    avg_loss:0.247, val_acc:0.948]
Epoch [30/120    avg_loss:0.271, val_acc:0.905]
Epoch [31/120    avg_loss:0.224, val_acc:0.929]
Epoch [32/120    avg_loss:0.217, val_acc:0.952]
Epoch [33/120    avg_loss:0.162, val_acc:0.948]
Epoch [34/120    avg_loss:0.244, val_acc:0.861]
Epoch [35/120    avg_loss:0.232, val_acc:0.946]
Epoch [36/120    avg_loss:0.217, val_acc:0.950]
Epoch [37/120    avg_loss:0.230, val_acc:0.946]
Epoch [38/120    avg_loss:0.197, val_acc:0.950]
Epoch [39/120    avg_loss:0.161, val_acc:0.946]
Epoch [40/120    avg_loss:0.195, val_acc:0.960]
Epoch [41/120    avg_loss:0.152, val_acc:0.956]
Epoch [42/120    avg_loss:0.170, val_acc:0.968]
Epoch [43/120    avg_loss:0.116, val_acc:0.976]
Epoch [44/120    avg_loss:0.102, val_acc:0.976]
Epoch [45/120    avg_loss:0.109, val_acc:0.978]
Epoch [46/120    avg_loss:0.092, val_acc:0.986]
Epoch [47/120    avg_loss:0.087, val_acc:0.980]
Epoch [48/120    avg_loss:0.107, val_acc:0.970]
Epoch [49/120    avg_loss:0.098, val_acc:0.964]
Epoch [50/120    avg_loss:0.163, val_acc:0.946]
Epoch [51/120    avg_loss:0.132, val_acc:0.976]
Epoch [52/120    avg_loss:0.101, val_acc:0.984]
Epoch [53/120    avg_loss:0.085, val_acc:0.984]
Epoch [54/120    avg_loss:0.089, val_acc:0.980]
Epoch [55/120    avg_loss:0.078, val_acc:0.982]
Epoch [56/120    avg_loss:0.085, val_acc:0.980]
Epoch [57/120    avg_loss:0.075, val_acc:0.968]
Epoch [58/120    avg_loss:0.069, val_acc:0.988]
Epoch [59/120    avg_loss:0.068, val_acc:0.986]
Epoch [60/120    avg_loss:0.075, val_acc:0.986]
Epoch [61/120    avg_loss:0.051, val_acc:0.984]
Epoch [62/120    avg_loss:0.119, val_acc:0.976]
Epoch [63/120    avg_loss:0.118, val_acc:0.909]
Epoch [64/120    avg_loss:0.100, val_acc:0.982]
Epoch [65/120    avg_loss:0.059, val_acc:0.988]
Epoch [66/120    avg_loss:0.072, val_acc:0.982]
Epoch [67/120    avg_loss:0.068, val_acc:0.986]
Epoch [68/120    avg_loss:0.064, val_acc:0.984]
Epoch [69/120    avg_loss:0.052, val_acc:0.988]
Epoch [70/120    avg_loss:0.041, val_acc:0.988]
Epoch [71/120    avg_loss:0.050, val_acc:0.988]
Epoch [72/120    avg_loss:0.089, val_acc:0.970]
Epoch [73/120    avg_loss:0.113, val_acc:0.978]
Epoch [74/120    avg_loss:0.050, val_acc:0.982]
Epoch [75/120    avg_loss:0.036, val_acc:0.990]
Epoch [76/120    avg_loss:0.032, val_acc:0.986]
Epoch [77/120    avg_loss:0.081, val_acc:0.990]
Epoch [78/120    avg_loss:0.049, val_acc:0.990]
Epoch [79/120    avg_loss:0.034, val_acc:0.990]
Epoch [80/120    avg_loss:0.028, val_acc:0.990]
Epoch [81/120    avg_loss:0.020, val_acc:0.990]
Epoch [82/120    avg_loss:0.018, val_acc:0.994]
Epoch [83/120    avg_loss:0.015, val_acc:0.990]
Epoch [84/120    avg_loss:0.020, val_acc:0.990]
Epoch [85/120    avg_loss:0.018, val_acc:0.994]
Epoch [86/120    avg_loss:0.022, val_acc:0.984]
Epoch [87/120    avg_loss:0.022, val_acc:0.982]
Epoch [88/120    avg_loss:0.018, val_acc:0.990]
Epoch [89/120    avg_loss:0.025, val_acc:0.988]
Epoch [90/120    avg_loss:0.017, val_acc:0.992]
Epoch [91/120    avg_loss:0.012, val_acc:0.992]
Epoch [92/120    avg_loss:0.017, val_acc:0.990]
Epoch [93/120    avg_loss:0.016, val_acc:0.994]
Epoch [94/120    avg_loss:0.029, val_acc:0.986]
Epoch [95/120    avg_loss:0.037, val_acc:0.980]
Epoch [96/120    avg_loss:0.093, val_acc:0.978]
Epoch [97/120    avg_loss:0.031, val_acc:0.988]
Epoch [98/120    avg_loss:0.027, val_acc:0.992]
Epoch [99/120    avg_loss:0.018, val_acc:0.992]
Epoch [100/120    avg_loss:0.014, val_acc:0.992]
Epoch [101/120    avg_loss:0.017, val_acc:0.990]
Epoch [102/120    avg_loss:0.012, val_acc:0.990]
Epoch [103/120    avg_loss:0.046, val_acc:0.986]
Epoch [104/120    avg_loss:0.027, val_acc:0.992]
Epoch [105/120    avg_loss:0.016, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.990]
Epoch [107/120    avg_loss:0.015, val_acc:0.990]
Epoch [108/120    avg_loss:0.012, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.007, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  14   0   0   0   0   0   0   8   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.98426966 1.         0.9255079  0.91467577
 1.         0.96132597 1.         1.         1.         1.
 0.99124726 1.        ]

Kappa:
0.9905035704499663
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc03fed8ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.541, val_acc:0.347]
Epoch [2/120    avg_loss:2.275, val_acc:0.480]
Epoch [3/120    avg_loss:2.052, val_acc:0.611]
Epoch [4/120    avg_loss:1.812, val_acc:0.655]
Epoch [5/120    avg_loss:1.525, val_acc:0.702]
Epoch [6/120    avg_loss:1.317, val_acc:0.742]
Epoch [7/120    avg_loss:1.125, val_acc:0.704]
Epoch [8/120    avg_loss:1.016, val_acc:0.815]
Epoch [9/120    avg_loss:0.891, val_acc:0.738]
Epoch [10/120    avg_loss:0.800, val_acc:0.794]
Epoch [11/120    avg_loss:0.753, val_acc:0.839]
Epoch [12/120    avg_loss:0.706, val_acc:0.851]
Epoch [13/120    avg_loss:0.623, val_acc:0.802]
Epoch [14/120    avg_loss:0.562, val_acc:0.865]
Epoch [15/120    avg_loss:0.509, val_acc:0.887]
Epoch [16/120    avg_loss:0.477, val_acc:0.889]
Epoch [17/120    avg_loss:0.444, val_acc:0.871]
Epoch [18/120    avg_loss:0.428, val_acc:0.909]
Epoch [19/120    avg_loss:0.378, val_acc:0.897]
Epoch [20/120    avg_loss:0.375, val_acc:0.923]
Epoch [21/120    avg_loss:0.355, val_acc:0.907]
Epoch [22/120    avg_loss:0.337, val_acc:0.931]
Epoch [23/120    avg_loss:0.317, val_acc:0.917]
Epoch [24/120    avg_loss:0.314, val_acc:0.905]
Epoch [25/120    avg_loss:0.273, val_acc:0.931]
Epoch [26/120    avg_loss:0.294, val_acc:0.929]
Epoch [27/120    avg_loss:0.279, val_acc:0.938]
Epoch [28/120    avg_loss:0.238, val_acc:0.929]
Epoch [29/120    avg_loss:0.232, val_acc:0.944]
Epoch [30/120    avg_loss:0.211, val_acc:0.942]
Epoch [31/120    avg_loss:0.231, val_acc:0.948]
Epoch [32/120    avg_loss:0.193, val_acc:0.948]
Epoch [33/120    avg_loss:0.208, val_acc:0.883]
Epoch [34/120    avg_loss:0.342, val_acc:0.925]
Epoch [35/120    avg_loss:0.230, val_acc:0.931]
Epoch [36/120    avg_loss:0.217, val_acc:0.946]
Epoch [37/120    avg_loss:0.198, val_acc:0.921]
Epoch [38/120    avg_loss:0.188, val_acc:0.956]
Epoch [39/120    avg_loss:0.186, val_acc:0.952]
Epoch [40/120    avg_loss:0.183, val_acc:0.970]
Epoch [41/120    avg_loss:0.148, val_acc:0.956]
Epoch [42/120    avg_loss:0.181, val_acc:0.942]
Epoch [43/120    avg_loss:0.161, val_acc:0.948]
Epoch [44/120    avg_loss:0.148, val_acc:0.948]
Epoch [45/120    avg_loss:0.121, val_acc:0.958]
Epoch [46/120    avg_loss:0.139, val_acc:0.956]
Epoch [47/120    avg_loss:0.102, val_acc:0.974]
Epoch [48/120    avg_loss:0.104, val_acc:0.954]
Epoch [49/120    avg_loss:0.124, val_acc:0.964]
Epoch [50/120    avg_loss:0.136, val_acc:0.972]
Epoch [51/120    avg_loss:0.113, val_acc:0.978]
Epoch [52/120    avg_loss:0.107, val_acc:0.978]
Epoch [53/120    avg_loss:0.080, val_acc:0.986]
Epoch [54/120    avg_loss:0.108, val_acc:0.970]
Epoch [55/120    avg_loss:0.082, val_acc:0.970]
Epoch [56/120    avg_loss:0.113, val_acc:0.952]
Epoch [57/120    avg_loss:0.139, val_acc:0.970]
Epoch [58/120    avg_loss:0.105, val_acc:0.970]
Epoch [59/120    avg_loss:0.112, val_acc:0.956]
Epoch [60/120    avg_loss:0.094, val_acc:0.976]
Epoch [61/120    avg_loss:0.091, val_acc:0.980]
Epoch [62/120    avg_loss:0.097, val_acc:0.986]
Epoch [63/120    avg_loss:0.116, val_acc:0.970]
Epoch [64/120    avg_loss:0.086, val_acc:0.968]
Epoch [65/120    avg_loss:0.110, val_acc:0.980]
Epoch [66/120    avg_loss:0.139, val_acc:0.974]
Epoch [67/120    avg_loss:0.116, val_acc:0.948]
Epoch [68/120    avg_loss:0.123, val_acc:0.964]
Epoch [69/120    avg_loss:0.100, val_acc:0.982]
Epoch [70/120    avg_loss:0.091, val_acc:0.980]
Epoch [71/120    avg_loss:0.089, val_acc:0.984]
Epoch [72/120    avg_loss:0.048, val_acc:0.986]
Epoch [73/120    avg_loss:0.081, val_acc:0.982]
Epoch [74/120    avg_loss:0.054, val_acc:0.980]
Epoch [75/120    avg_loss:0.060, val_acc:0.984]
Epoch [76/120    avg_loss:0.049, val_acc:0.988]
Epoch [77/120    avg_loss:0.049, val_acc:0.982]
Epoch [78/120    avg_loss:0.061, val_acc:0.986]
Epoch [79/120    avg_loss:0.053, val_acc:0.984]
Epoch [80/120    avg_loss:0.045, val_acc:0.986]
Epoch [81/120    avg_loss:0.089, val_acc:0.950]
Epoch [82/120    avg_loss:0.105, val_acc:0.988]
Epoch [83/120    avg_loss:0.073, val_acc:0.968]
Epoch [84/120    avg_loss:0.044, val_acc:0.988]
Epoch [85/120    avg_loss:0.038, val_acc:0.988]
Epoch [86/120    avg_loss:0.058, val_acc:0.990]
Epoch [87/120    avg_loss:0.096, val_acc:0.984]
Epoch [88/120    avg_loss:0.054, val_acc:0.992]
Epoch [89/120    avg_loss:0.039, val_acc:0.992]
Epoch [90/120    avg_loss:0.035, val_acc:0.990]
Epoch [91/120    avg_loss:0.028, val_acc:0.986]
Epoch [92/120    avg_loss:0.026, val_acc:0.992]
Epoch [93/120    avg_loss:0.027, val_acc:0.994]
Epoch [94/120    avg_loss:0.047, val_acc:0.980]
Epoch [95/120    avg_loss:0.073, val_acc:0.972]
Epoch [96/120    avg_loss:0.041, val_acc:0.984]
Epoch [97/120    avg_loss:0.032, val_acc:0.986]
Epoch [98/120    avg_loss:0.033, val_acc:0.984]
Epoch [99/120    avg_loss:0.026, val_acc:0.978]
Epoch [100/120    avg_loss:0.026, val_acc:0.988]
Epoch [101/120    avg_loss:0.032, val_acc:0.990]
Epoch [102/120    avg_loss:0.035, val_acc:0.982]
Epoch [103/120    avg_loss:0.035, val_acc:0.988]
Epoch [104/120    avg_loss:0.017, val_acc:0.990]
Epoch [105/120    avg_loss:0.032, val_acc:0.988]
Epoch [106/120    avg_loss:0.040, val_acc:0.986]
Epoch [107/120    avg_loss:0.019, val_acc:0.986]
Epoch [108/120    avg_loss:0.015, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.986]
Epoch [110/120    avg_loss:0.017, val_acc:0.984]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.014, val_acc:0.984]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.013, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.986]
Epoch [116/120    avg_loss:0.015, val_acc:0.986]
Epoch [117/120    avg_loss:0.018, val_acc:0.986]
Epoch [118/120    avg_loss:0.014, val_acc:0.986]
Epoch [119/120    avg_loss:0.014, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  17   0   0   0   0   0   0   5   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99854227 0.98426966 0.99343545 0.91722595 0.90169492
 1.         0.96132597 0.99741602 1.         1.         0.99734043
 0.99233297 1.        ]

Kappa:
0.9886043451097546
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2819979a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.533, val_acc:0.357]
Epoch [2/120    avg_loss:2.271, val_acc:0.510]
Epoch [3/120    avg_loss:2.063, val_acc:0.567]
Epoch [4/120    avg_loss:1.844, val_acc:0.653]
Epoch [5/120    avg_loss:1.570, val_acc:0.698]
Epoch [6/120    avg_loss:1.362, val_acc:0.754]
Epoch [7/120    avg_loss:1.148, val_acc:0.782]
Epoch [8/120    avg_loss:0.959, val_acc:0.778]
Epoch [9/120    avg_loss:0.919, val_acc:0.819]
Epoch [10/120    avg_loss:0.817, val_acc:0.825]
Epoch [11/120    avg_loss:0.736, val_acc:0.877]
Epoch [12/120    avg_loss:0.621, val_acc:0.853]
Epoch [13/120    avg_loss:0.551, val_acc:0.845]
Epoch [14/120    avg_loss:0.495, val_acc:0.885]
Epoch [15/120    avg_loss:0.529, val_acc:0.859]
Epoch [16/120    avg_loss:0.512, val_acc:0.812]
Epoch [17/120    avg_loss:0.456, val_acc:0.903]
Epoch [18/120    avg_loss:0.387, val_acc:0.861]
Epoch [19/120    avg_loss:0.444, val_acc:0.903]
Epoch [20/120    avg_loss:0.378, val_acc:0.944]
Epoch [21/120    avg_loss:0.369, val_acc:0.927]
Epoch [22/120    avg_loss:0.303, val_acc:0.948]
Epoch [23/120    avg_loss:0.308, val_acc:0.933]
Epoch [24/120    avg_loss:0.233, val_acc:0.942]
Epoch [25/120    avg_loss:0.211, val_acc:0.923]
Epoch [26/120    avg_loss:0.242, val_acc:0.859]
Epoch [27/120    avg_loss:0.274, val_acc:0.903]
Epoch [28/120    avg_loss:0.223, val_acc:0.944]
Epoch [29/120    avg_loss:0.236, val_acc:0.889]
Epoch [30/120    avg_loss:0.349, val_acc:0.923]
Epoch [31/120    avg_loss:0.215, val_acc:0.954]
Epoch [32/120    avg_loss:0.237, val_acc:0.919]
Epoch [33/120    avg_loss:0.252, val_acc:0.933]
Epoch [34/120    avg_loss:0.188, val_acc:0.968]
Epoch [35/120    avg_loss:0.202, val_acc:0.958]
Epoch [36/120    avg_loss:0.201, val_acc:0.944]
Epoch [37/120    avg_loss:0.211, val_acc:0.956]
Epoch [38/120    avg_loss:0.197, val_acc:0.954]
Epoch [39/120    avg_loss:0.252, val_acc:0.958]
Epoch [40/120    avg_loss:0.152, val_acc:0.944]
Epoch [41/120    avg_loss:0.150, val_acc:0.970]
Epoch [42/120    avg_loss:0.139, val_acc:0.976]
Epoch [43/120    avg_loss:0.156, val_acc:0.980]
Epoch [44/120    avg_loss:0.135, val_acc:0.958]
Epoch [45/120    avg_loss:0.088, val_acc:0.980]
Epoch [46/120    avg_loss:0.087, val_acc:0.968]
Epoch [47/120    avg_loss:0.157, val_acc:0.935]
Epoch [48/120    avg_loss:0.195, val_acc:0.964]
Epoch [49/120    avg_loss:0.159, val_acc:0.935]
Epoch [50/120    avg_loss:0.149, val_acc:0.974]
Epoch [51/120    avg_loss:0.113, val_acc:0.948]
Epoch [52/120    avg_loss:0.090, val_acc:0.978]
Epoch [53/120    avg_loss:0.116, val_acc:0.976]
Epoch [54/120    avg_loss:0.085, val_acc:0.942]
Epoch [55/120    avg_loss:0.109, val_acc:0.976]
Epoch [56/120    avg_loss:0.103, val_acc:0.978]
Epoch [57/120    avg_loss:0.170, val_acc:0.976]
Epoch [58/120    avg_loss:0.131, val_acc:0.976]
Epoch [59/120    avg_loss:0.092, val_acc:0.986]
Epoch [60/120    avg_loss:0.066, val_acc:0.990]
Epoch [61/120    avg_loss:0.069, val_acc:0.986]
Epoch [62/120    avg_loss:0.051, val_acc:0.988]
Epoch [63/120    avg_loss:0.049, val_acc:0.990]
Epoch [64/120    avg_loss:0.064, val_acc:0.994]
Epoch [65/120    avg_loss:0.064, val_acc:0.986]
Epoch [66/120    avg_loss:0.054, val_acc:0.986]
Epoch [67/120    avg_loss:0.054, val_acc:0.992]
Epoch [68/120    avg_loss:0.052, val_acc:0.992]
Epoch [69/120    avg_loss:0.049, val_acc:0.988]
Epoch [70/120    avg_loss:0.048, val_acc:0.990]
Epoch [71/120    avg_loss:0.046, val_acc:0.992]
Epoch [72/120    avg_loss:0.052, val_acc:0.988]
Epoch [73/120    avg_loss:0.044, val_acc:0.992]
Epoch [74/120    avg_loss:0.044, val_acc:0.996]
Epoch [75/120    avg_loss:0.042, val_acc:0.996]
Epoch [76/120    avg_loss:0.057, val_acc:0.990]
Epoch [77/120    avg_loss:0.042, val_acc:0.990]
Epoch [78/120    avg_loss:0.039, val_acc:0.992]
Epoch [79/120    avg_loss:0.045, val_acc:0.990]
Epoch [80/120    avg_loss:0.047, val_acc:0.992]
Epoch [81/120    avg_loss:0.041, val_acc:0.994]
Epoch [82/120    avg_loss:0.052, val_acc:0.992]
Epoch [83/120    avg_loss:0.040, val_acc:0.994]
Epoch [84/120    avg_loss:0.038, val_acc:0.992]
Epoch [85/120    avg_loss:0.049, val_acc:0.990]
Epoch [86/120    avg_loss:0.032, val_acc:0.990]
Epoch [87/120    avg_loss:0.039, val_acc:0.992]
Epoch [88/120    avg_loss:0.036, val_acc:0.990]
Epoch [89/120    avg_loss:0.035, val_acc:0.990]
Epoch [90/120    avg_loss:0.043, val_acc:0.992]
Epoch [91/120    avg_loss:0.040, val_acc:0.992]
Epoch [92/120    avg_loss:0.045, val_acc:0.992]
Epoch [93/120    avg_loss:0.035, val_acc:0.994]
Epoch [94/120    avg_loss:0.036, val_acc:0.994]
Epoch [95/120    avg_loss:0.035, val_acc:0.994]
Epoch [96/120    avg_loss:0.033, val_acc:0.994]
Epoch [97/120    avg_loss:0.035, val_acc:0.992]
Epoch [98/120    avg_loss:0.040, val_acc:0.992]
Epoch [99/120    avg_loss:0.034, val_acc:0.992]
Epoch [100/120    avg_loss:0.035, val_acc:0.992]
Epoch [101/120    avg_loss:0.042, val_acc:0.992]
Epoch [102/120    avg_loss:0.040, val_acc:0.992]
Epoch [103/120    avg_loss:0.033, val_acc:0.992]
Epoch [104/120    avg_loss:0.035, val_acc:0.992]
Epoch [105/120    avg_loss:0.039, val_acc:0.992]
Epoch [106/120    avg_loss:0.038, val_acc:0.992]
Epoch [107/120    avg_loss:0.040, val_acc:0.992]
Epoch [108/120    avg_loss:0.039, val_acc:0.992]
Epoch [109/120    avg_loss:0.033, val_acc:0.992]
Epoch [110/120    avg_loss:0.044, val_acc:0.992]
Epoch [111/120    avg_loss:0.038, val_acc:0.992]
Epoch [112/120    avg_loss:0.037, val_acc:0.992]
Epoch [113/120    avg_loss:0.043, val_acc:0.992]
Epoch [114/120    avg_loss:0.034, val_acc:0.992]
Epoch [115/120    avg_loss:0.039, val_acc:0.992]
Epoch [116/120    avg_loss:0.036, val_acc:0.992]
Epoch [117/120    avg_loss:0.032, val_acc:0.992]
Epoch [118/120    avg_loss:0.035, val_acc:0.992]
Epoch [119/120    avg_loss:0.045, val_acc:0.992]
Epoch [120/120    avg_loss:0.045, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 0.99926954 0.98426966 1.         0.93275488 0.89045936
 0.99757869 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9907416687166498
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99491b1a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.480, val_acc:0.425]
Epoch [2/120    avg_loss:2.206, val_acc:0.450]
Epoch [3/120    avg_loss:2.033, val_acc:0.518]
Epoch [4/120    avg_loss:1.855, val_acc:0.601]
Epoch [5/120    avg_loss:1.632, val_acc:0.653]
Epoch [6/120    avg_loss:1.427, val_acc:0.700]
Epoch [7/120    avg_loss:1.232, val_acc:0.768]
Epoch [8/120    avg_loss:1.089, val_acc:0.782]
Epoch [9/120    avg_loss:0.957, val_acc:0.825]
Epoch [10/120    avg_loss:0.781, val_acc:0.831]
Epoch [11/120    avg_loss:0.694, val_acc:0.815]
Epoch [12/120    avg_loss:0.680, val_acc:0.845]
Epoch [13/120    avg_loss:0.595, val_acc:0.873]
Epoch [14/120    avg_loss:0.502, val_acc:0.881]
Epoch [15/120    avg_loss:0.508, val_acc:0.893]
Epoch [16/120    avg_loss:0.497, val_acc:0.867]
Epoch [17/120    avg_loss:0.424, val_acc:0.901]
Epoch [18/120    avg_loss:0.427, val_acc:0.897]
Epoch [19/120    avg_loss:0.454, val_acc:0.865]
Epoch [20/120    avg_loss:0.401, val_acc:0.871]
Epoch [21/120    avg_loss:0.414, val_acc:0.911]
Epoch [22/120    avg_loss:0.381, val_acc:0.929]
Epoch [23/120    avg_loss:0.332, val_acc:0.895]
Epoch [24/120    avg_loss:0.292, val_acc:0.919]
Epoch [25/120    avg_loss:0.338, val_acc:0.913]
Epoch [26/120    avg_loss:0.289, val_acc:0.938]
Epoch [27/120    avg_loss:0.289, val_acc:0.933]
Epoch [28/120    avg_loss:0.306, val_acc:0.913]
Epoch [29/120    avg_loss:0.311, val_acc:0.935]
Epoch [30/120    avg_loss:0.259, val_acc:0.972]
Epoch [31/120    avg_loss:0.270, val_acc:0.919]
Epoch [32/120    avg_loss:0.267, val_acc:0.923]
Epoch [33/120    avg_loss:0.286, val_acc:0.952]
Epoch [34/120    avg_loss:0.274, val_acc:0.948]
Epoch [35/120    avg_loss:0.246, val_acc:0.931]
Epoch [36/120    avg_loss:0.246, val_acc:0.962]
Epoch [37/120    avg_loss:0.232, val_acc:0.935]
Epoch [38/120    avg_loss:0.253, val_acc:0.952]
Epoch [39/120    avg_loss:0.188, val_acc:0.960]
Epoch [40/120    avg_loss:0.171, val_acc:0.974]
Epoch [41/120    avg_loss:0.148, val_acc:0.974]
Epoch [42/120    avg_loss:0.179, val_acc:0.948]
Epoch [43/120    avg_loss:0.153, val_acc:0.966]
Epoch [44/120    avg_loss:0.156, val_acc:0.976]
Epoch [45/120    avg_loss:0.123, val_acc:0.964]
Epoch [46/120    avg_loss:0.139, val_acc:0.954]
Epoch [47/120    avg_loss:0.162, val_acc:0.976]
Epoch [48/120    avg_loss:0.134, val_acc:0.942]
Epoch [49/120    avg_loss:0.123, val_acc:0.974]
Epoch [50/120    avg_loss:0.104, val_acc:0.968]
Epoch [51/120    avg_loss:0.145, val_acc:0.956]
Epoch [52/120    avg_loss:0.108, val_acc:0.968]
Epoch [53/120    avg_loss:0.092, val_acc:0.980]
Epoch [54/120    avg_loss:0.109, val_acc:0.972]
Epoch [55/120    avg_loss:0.114, val_acc:0.978]
Epoch [56/120    avg_loss:0.092, val_acc:0.976]
Epoch [57/120    avg_loss:0.105, val_acc:0.986]
Epoch [58/120    avg_loss:0.137, val_acc:0.962]
Epoch [59/120    avg_loss:0.080, val_acc:0.986]
Epoch [60/120    avg_loss:0.098, val_acc:0.982]
Epoch [61/120    avg_loss:0.082, val_acc:0.988]
Epoch [62/120    avg_loss:0.077, val_acc:0.974]
Epoch [63/120    avg_loss:0.103, val_acc:0.972]
Epoch [64/120    avg_loss:0.067, val_acc:0.980]
Epoch [65/120    avg_loss:0.065, val_acc:0.976]
Epoch [66/120    avg_loss:0.052, val_acc:0.984]
Epoch [67/120    avg_loss:0.058, val_acc:0.990]
Epoch [68/120    avg_loss:0.050, val_acc:0.982]
Epoch [69/120    avg_loss:0.063, val_acc:0.990]
Epoch [70/120    avg_loss:0.039, val_acc:0.982]
Epoch [71/120    avg_loss:0.051, val_acc:0.986]
Epoch [72/120    avg_loss:0.110, val_acc:0.980]
Epoch [73/120    avg_loss:0.051, val_acc:0.974]
Epoch [74/120    avg_loss:0.032, val_acc:0.984]
Epoch [75/120    avg_loss:0.044, val_acc:0.988]
Epoch [76/120    avg_loss:0.038, val_acc:0.992]
Epoch [77/120    avg_loss:0.058, val_acc:0.986]
Epoch [78/120    avg_loss:0.061, val_acc:0.968]
Epoch [79/120    avg_loss:0.106, val_acc:0.982]
Epoch [80/120    avg_loss:0.067, val_acc:0.984]
Epoch [81/120    avg_loss:0.047, val_acc:0.990]
Epoch [82/120    avg_loss:0.031, val_acc:0.992]
Epoch [83/120    avg_loss:0.028, val_acc:0.994]
Epoch [84/120    avg_loss:0.028, val_acc:0.992]
Epoch [85/120    avg_loss:0.027, val_acc:0.992]
Epoch [86/120    avg_loss:0.024, val_acc:0.992]
Epoch [87/120    avg_loss:0.043, val_acc:0.992]
Epoch [88/120    avg_loss:0.030, val_acc:0.992]
Epoch [89/120    avg_loss:0.029, val_acc:0.992]
Epoch [90/120    avg_loss:0.022, val_acc:0.996]
Epoch [91/120    avg_loss:0.049, val_acc:0.986]
Epoch [92/120    avg_loss:0.063, val_acc:0.976]
Epoch [93/120    avg_loss:0.049, val_acc:0.992]
Epoch [94/120    avg_loss:0.053, val_acc:0.984]
Epoch [95/120    avg_loss:0.041, val_acc:0.990]
Epoch [96/120    avg_loss:0.047, val_acc:0.990]
Epoch [97/120    avg_loss:0.036, val_acc:0.988]
Epoch [98/120    avg_loss:0.039, val_acc:0.992]
Epoch [99/120    avg_loss:0.021, val_acc:0.992]
Epoch [100/120    avg_loss:0.020, val_acc:0.994]
Epoch [101/120    avg_loss:0.022, val_acc:0.992]
Epoch [102/120    avg_loss:0.020, val_acc:0.994]
Epoch [103/120    avg_loss:0.017, val_acc:0.994]
Epoch [104/120    avg_loss:0.017, val_acc:0.994]
Epoch [105/120    avg_loss:0.010, val_acc:0.994]
Epoch [106/120    avg_loss:0.010, val_acc:0.994]
Epoch [107/120    avg_loss:0.013, val_acc:0.996]
Epoch [108/120    avg_loss:0.010, val_acc:0.996]
Epoch [109/120    avg_loss:0.014, val_acc:0.996]
Epoch [110/120    avg_loss:0.012, val_acc:0.996]
Epoch [111/120    avg_loss:0.011, val_acc:0.996]
Epoch [112/120    avg_loss:0.008, val_acc:0.996]
Epoch [113/120    avg_loss:0.014, val_acc:0.996]
Epoch [114/120    avg_loss:0.013, val_acc:0.996]
Epoch [115/120    avg_loss:0.010, val_acc:0.996]
Epoch [116/120    avg_loss:0.009, val_acc:0.996]
Epoch [117/120    avg_loss:0.009, val_acc:0.996]
Epoch [118/120    avg_loss:0.009, val_acc:0.996]
Epoch [119/120    avg_loss:0.010, val_acc:0.994]
Epoch [120/120    avg_loss:0.013, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  12   0   0   0   0   0   0   4   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 0.99927061 0.99545455 1.         0.96127563 0.95681063
 1.         0.98924731 0.99870968 1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9952522389767795
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff413ae7a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.498, val_acc:0.373]
Epoch [2/120    avg_loss:2.222, val_acc:0.534]
Epoch [3/120    avg_loss:2.059, val_acc:0.544]
Epoch [4/120    avg_loss:1.839, val_acc:0.585]
Epoch [5/120    avg_loss:1.591, val_acc:0.681]
Epoch [6/120    avg_loss:1.358, val_acc:0.704]
Epoch [7/120    avg_loss:1.213, val_acc:0.754]
Epoch [8/120    avg_loss:1.058, val_acc:0.798]
Epoch [9/120    avg_loss:0.927, val_acc:0.740]
Epoch [10/120    avg_loss:0.817, val_acc:0.819]
Epoch [11/120    avg_loss:0.766, val_acc:0.851]
Epoch [12/120    avg_loss:0.660, val_acc:0.815]
Epoch [13/120    avg_loss:0.683, val_acc:0.877]
Epoch [14/120    avg_loss:0.596, val_acc:0.867]
Epoch [15/120    avg_loss:0.525, val_acc:0.895]
Epoch [16/120    avg_loss:0.516, val_acc:0.877]
Epoch [17/120    avg_loss:0.476, val_acc:0.913]
Epoch [18/120    avg_loss:0.465, val_acc:0.901]
Epoch [19/120    avg_loss:0.397, val_acc:0.861]
Epoch [20/120    avg_loss:0.432, val_acc:0.913]
Epoch [21/120    avg_loss:0.369, val_acc:0.883]
Epoch [22/120    avg_loss:0.330, val_acc:0.855]
Epoch [23/120    avg_loss:0.373, val_acc:0.938]
Epoch [24/120    avg_loss:0.338, val_acc:0.913]
Epoch [25/120    avg_loss:0.286, val_acc:0.911]
Epoch [26/120    avg_loss:0.350, val_acc:0.927]
Epoch [27/120    avg_loss:0.303, val_acc:0.829]
Epoch [28/120    avg_loss:0.345, val_acc:0.940]
Epoch [29/120    avg_loss:0.334, val_acc:0.944]
Epoch [30/120    avg_loss:0.225, val_acc:0.956]
Epoch [31/120    avg_loss:0.271, val_acc:0.923]
Epoch [32/120    avg_loss:0.224, val_acc:0.911]
Epoch [33/120    avg_loss:0.205, val_acc:0.931]
Epoch [34/120    avg_loss:0.213, val_acc:0.962]
Epoch [35/120    avg_loss:0.171, val_acc:0.929]
Epoch [36/120    avg_loss:0.196, val_acc:0.942]
Epoch [37/120    avg_loss:0.163, val_acc:0.879]
Epoch [38/120    avg_loss:0.179, val_acc:0.942]
Epoch [39/120    avg_loss:0.161, val_acc:0.940]
Epoch [40/120    avg_loss:0.157, val_acc:0.944]
Epoch [41/120    avg_loss:0.147, val_acc:0.960]
Epoch [42/120    avg_loss:0.137, val_acc:0.962]
Epoch [43/120    avg_loss:0.141, val_acc:0.962]
Epoch [44/120    avg_loss:0.129, val_acc:0.946]
Epoch [45/120    avg_loss:0.108, val_acc:0.962]
Epoch [46/120    avg_loss:0.124, val_acc:0.974]
Epoch [47/120    avg_loss:0.097, val_acc:0.970]
Epoch [48/120    avg_loss:0.100, val_acc:0.970]
Epoch [49/120    avg_loss:0.072, val_acc:0.968]
Epoch [50/120    avg_loss:0.135, val_acc:0.921]
Epoch [51/120    avg_loss:0.116, val_acc:0.978]
Epoch [52/120    avg_loss:0.091, val_acc:0.950]
Epoch [53/120    avg_loss:0.121, val_acc:0.972]
Epoch [54/120    avg_loss:0.100, val_acc:0.974]
Epoch [55/120    avg_loss:0.129, val_acc:0.946]
Epoch [56/120    avg_loss:0.106, val_acc:0.980]
Epoch [57/120    avg_loss:0.070, val_acc:0.978]
Epoch [58/120    avg_loss:0.076, val_acc:0.974]
Epoch [59/120    avg_loss:0.089, val_acc:0.972]
Epoch [60/120    avg_loss:0.079, val_acc:0.986]
Epoch [61/120    avg_loss:0.056, val_acc:0.966]
Epoch [62/120    avg_loss:0.070, val_acc:0.974]
Epoch [63/120    avg_loss:0.080, val_acc:0.982]
Epoch [64/120    avg_loss:0.075, val_acc:0.966]
Epoch [65/120    avg_loss:0.048, val_acc:0.980]
Epoch [66/120    avg_loss:0.063, val_acc:0.970]
Epoch [67/120    avg_loss:0.083, val_acc:0.980]
Epoch [68/120    avg_loss:0.072, val_acc:0.942]
Epoch [69/120    avg_loss:0.081, val_acc:0.988]
Epoch [70/120    avg_loss:0.037, val_acc:0.984]
Epoch [71/120    avg_loss:0.035, val_acc:0.974]
Epoch [72/120    avg_loss:0.038, val_acc:0.976]
Epoch [73/120    avg_loss:0.049, val_acc:0.974]
Epoch [74/120    avg_loss:0.104, val_acc:0.970]
Epoch [75/120    avg_loss:0.083, val_acc:0.972]
Epoch [76/120    avg_loss:0.079, val_acc:0.978]
Epoch [77/120    avg_loss:0.048, val_acc:0.978]
Epoch [78/120    avg_loss:0.043, val_acc:0.972]
Epoch [79/120    avg_loss:0.029, val_acc:0.978]
Epoch [80/120    avg_loss:0.021, val_acc:0.986]
Epoch [81/120    avg_loss:0.025, val_acc:0.982]
Epoch [82/120    avg_loss:0.029, val_acc:0.986]
Epoch [83/120    avg_loss:0.019, val_acc:0.986]
Epoch [84/120    avg_loss:0.017, val_acc:0.988]
Epoch [85/120    avg_loss:0.018, val_acc:0.986]
Epoch [86/120    avg_loss:0.016, val_acc:0.988]
Epoch [87/120    avg_loss:0.019, val_acc:0.986]
Epoch [88/120    avg_loss:0.019, val_acc:0.986]
Epoch [89/120    avg_loss:0.016, val_acc:0.986]
Epoch [90/120    avg_loss:0.017, val_acc:0.986]
Epoch [91/120    avg_loss:0.013, val_acc:0.986]
Epoch [92/120    avg_loss:0.015, val_acc:0.988]
Epoch [93/120    avg_loss:0.019, val_acc:0.988]
Epoch [94/120    avg_loss:0.020, val_acc:0.986]
Epoch [95/120    avg_loss:0.022, val_acc:0.986]
Epoch [96/120    avg_loss:0.017, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.988]
Epoch [98/120    avg_loss:0.017, val_acc:0.988]
Epoch [99/120    avg_loss:0.023, val_acc:0.988]
Epoch [100/120    avg_loss:0.015, val_acc:0.988]
Epoch [101/120    avg_loss:0.018, val_acc:0.988]
Epoch [102/120    avg_loss:0.018, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.030, val_acc:0.984]
Epoch [106/120    avg_loss:0.014, val_acc:0.986]
Epoch [107/120    avg_loss:0.012, val_acc:0.986]
Epoch [108/120    avg_loss:0.017, val_acc:0.984]
Epoch [109/120    avg_loss:0.018, val_acc:0.986]
Epoch [110/120    avg_loss:0.013, val_acc:0.986]
Epoch [111/120    avg_loss:0.013, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.988]
Epoch [113/120    avg_loss:0.015, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.014, val_acc:0.988]
Epoch [116/120    avg_loss:0.014, val_acc:0.988]
Epoch [117/120    avg_loss:0.013, val_acc:0.988]
Epoch [118/120    avg_loss:0.014, val_acc:0.988]
Epoch [119/120    avg_loss:0.015, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   1   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  10   0   0   0   0   0   0   1   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.99560117 0.98206278 0.99782135 0.93913043 0.9084507
 0.98800959 0.96132597 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9902679864226542
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ee137fa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.487, val_acc:0.343]
Epoch [2/120    avg_loss:2.207, val_acc:0.490]
Epoch [3/120    avg_loss:1.987, val_acc:0.573]
Epoch [4/120    avg_loss:1.765, val_acc:0.621]
Epoch [5/120    avg_loss:1.502, val_acc:0.708]
Epoch [6/120    avg_loss:1.254, val_acc:0.706]
Epoch [7/120    avg_loss:1.046, val_acc:0.776]
Epoch [8/120    avg_loss:0.895, val_acc:0.736]
Epoch [9/120    avg_loss:0.848, val_acc:0.829]
Epoch [10/120    avg_loss:0.684, val_acc:0.873]
Epoch [11/120    avg_loss:0.703, val_acc:0.780]
Epoch [12/120    avg_loss:0.558, val_acc:0.893]
Epoch [13/120    avg_loss:0.543, val_acc:0.849]
Epoch [14/120    avg_loss:0.499, val_acc:0.817]
Epoch [15/120    avg_loss:0.528, val_acc:0.835]
Epoch [16/120    avg_loss:0.453, val_acc:0.903]
Epoch [17/120    avg_loss:0.416, val_acc:0.919]
Epoch [18/120    avg_loss:0.361, val_acc:0.885]
Epoch [19/120    avg_loss:0.536, val_acc:0.897]
Epoch [20/120    avg_loss:0.389, val_acc:0.913]
Epoch [21/120    avg_loss:0.357, val_acc:0.883]
Epoch [22/120    avg_loss:0.344, val_acc:0.889]
Epoch [23/120    avg_loss:0.330, val_acc:0.929]
Epoch [24/120    avg_loss:0.304, val_acc:0.933]
Epoch [25/120    avg_loss:0.256, val_acc:0.948]
Epoch [26/120    avg_loss:0.252, val_acc:0.938]
Epoch [27/120    avg_loss:0.282, val_acc:0.935]
Epoch [28/120    avg_loss:0.280, val_acc:0.952]
Epoch [29/120    avg_loss:0.243, val_acc:0.929]
Epoch [30/120    avg_loss:0.271, val_acc:0.933]
Epoch [31/120    avg_loss:0.237, val_acc:0.964]
Epoch [32/120    avg_loss:0.242, val_acc:0.952]
Epoch [33/120    avg_loss:0.211, val_acc:0.935]
Epoch [34/120    avg_loss:0.176, val_acc:0.915]
Epoch [35/120    avg_loss:0.265, val_acc:0.885]
Epoch [36/120    avg_loss:0.297, val_acc:0.927]
Epoch [37/120    avg_loss:0.227, val_acc:0.950]
Epoch [38/120    avg_loss:0.211, val_acc:0.956]
Epoch [39/120    avg_loss:0.200, val_acc:0.968]
Epoch [40/120    avg_loss:0.175, val_acc:0.966]
Epoch [41/120    avg_loss:0.191, val_acc:0.962]
Epoch [42/120    avg_loss:0.141, val_acc:0.978]
Epoch [43/120    avg_loss:0.164, val_acc:0.948]
Epoch [44/120    avg_loss:0.140, val_acc:0.970]
Epoch [45/120    avg_loss:0.133, val_acc:0.913]
Epoch [46/120    avg_loss:0.181, val_acc:0.962]
Epoch [47/120    avg_loss:0.125, val_acc:0.964]
Epoch [48/120    avg_loss:0.129, val_acc:0.978]
Epoch [49/120    avg_loss:0.128, val_acc:0.978]
Epoch [50/120    avg_loss:0.099, val_acc:0.976]
Epoch [51/120    avg_loss:0.127, val_acc:0.974]
Epoch [52/120    avg_loss:0.098, val_acc:0.978]
Epoch [53/120    avg_loss:0.141, val_acc:0.976]
Epoch [54/120    avg_loss:0.130, val_acc:0.968]
Epoch [55/120    avg_loss:0.097, val_acc:0.980]
Epoch [56/120    avg_loss:0.127, val_acc:0.960]
Epoch [57/120    avg_loss:0.146, val_acc:0.972]
Epoch [58/120    avg_loss:0.134, val_acc:0.978]
Epoch [59/120    avg_loss:0.095, val_acc:0.976]
Epoch [60/120    avg_loss:0.074, val_acc:0.974]
Epoch [61/120    avg_loss:0.108, val_acc:0.964]
Epoch [62/120    avg_loss:0.065, val_acc:0.994]
Epoch [63/120    avg_loss:0.050, val_acc:0.988]
Epoch [64/120    avg_loss:0.062, val_acc:0.990]
Epoch [65/120    avg_loss:0.138, val_acc:0.948]
Epoch [66/120    avg_loss:0.066, val_acc:0.986]
Epoch [67/120    avg_loss:0.056, val_acc:0.984]
Epoch [68/120    avg_loss:0.042, val_acc:0.984]
Epoch [69/120    avg_loss:0.050, val_acc:0.988]
Epoch [70/120    avg_loss:0.040, val_acc:0.994]
Epoch [71/120    avg_loss:0.058, val_acc:0.978]
Epoch [72/120    avg_loss:0.065, val_acc:0.978]
Epoch [73/120    avg_loss:0.066, val_acc:0.986]
Epoch [74/120    avg_loss:0.058, val_acc:0.988]
Epoch [75/120    avg_loss:0.046, val_acc:0.998]
Epoch [76/120    avg_loss:0.063, val_acc:0.994]
Epoch [77/120    avg_loss:0.108, val_acc:0.968]
Epoch [78/120    avg_loss:0.058, val_acc:0.996]
Epoch [79/120    avg_loss:0.054, val_acc:0.994]
Epoch [80/120    avg_loss:0.071, val_acc:0.984]
Epoch [81/120    avg_loss:0.035, val_acc:0.990]
Epoch [82/120    avg_loss:0.057, val_acc:0.988]
Epoch [83/120    avg_loss:0.035, val_acc:0.992]
Epoch [84/120    avg_loss:0.040, val_acc:0.996]
Epoch [85/120    avg_loss:0.028, val_acc:0.996]
Epoch [86/120    avg_loss:0.025, val_acc:0.998]
Epoch [87/120    avg_loss:0.021, val_acc:0.996]
Epoch [88/120    avg_loss:0.023, val_acc:0.994]
Epoch [89/120    avg_loss:0.014, val_acc:0.996]
Epoch [90/120    avg_loss:0.029, val_acc:0.996]
Epoch [91/120    avg_loss:0.045, val_acc:0.994]
Epoch [92/120    avg_loss:0.022, val_acc:0.990]
Epoch [93/120    avg_loss:0.016, val_acc:0.996]
Epoch [94/120    avg_loss:0.015, val_acc:0.996]
Epoch [95/120    avg_loss:0.135, val_acc:0.938]
Epoch [96/120    avg_loss:0.141, val_acc:0.974]
Epoch [97/120    avg_loss:0.052, val_acc:0.986]
Epoch [98/120    avg_loss:0.041, val_acc:0.976]
Epoch [99/120    avg_loss:0.051, val_acc:0.990]
Epoch [100/120    avg_loss:0.031, val_acc:0.996]
Epoch [101/120    avg_loss:0.023, val_acc:0.994]
Epoch [102/120    avg_loss:0.016, val_acc:0.998]
Epoch [103/120    avg_loss:0.016, val_acc:0.998]
Epoch [104/120    avg_loss:0.020, val_acc:0.998]
Epoch [105/120    avg_loss:0.016, val_acc:0.998]
Epoch [106/120    avg_loss:0.020, val_acc:0.998]
Epoch [107/120    avg_loss:0.017, val_acc:0.998]
Epoch [108/120    avg_loss:0.015, val_acc:0.998]
Epoch [109/120    avg_loss:0.023, val_acc:0.998]
Epoch [110/120    avg_loss:0.015, val_acc:0.998]
Epoch [111/120    avg_loss:0.015, val_acc:0.998]
Epoch [112/120    avg_loss:0.012, val_acc:0.998]
Epoch [113/120    avg_loss:0.013, val_acc:0.998]
Epoch [114/120    avg_loss:0.013, val_acc:0.998]
Epoch [115/120    avg_loss:0.013, val_acc:0.998]
Epoch [116/120    avg_loss:0.013, val_acc:0.998]
Epoch [117/120    avg_loss:0.015, val_acc:0.998]
Epoch [118/120    avg_loss:0.016, val_acc:0.998]
Epoch [119/120    avg_loss:0.013, val_acc:0.998]
Epoch [120/120    avg_loss:0.011, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  16   0   0   0   0   0   0   3   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.98871332 1.         0.91028446 0.86619718
 1.         0.9726776  1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9890794609179702
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9d0cafaa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.543, val_acc:0.321]
Epoch [2/120    avg_loss:2.266, val_acc:0.433]
Epoch [3/120    avg_loss:2.047, val_acc:0.558]
Epoch [4/120    avg_loss:1.835, val_acc:0.611]
Epoch [5/120    avg_loss:1.598, val_acc:0.645]
Epoch [6/120    avg_loss:1.436, val_acc:0.732]
Epoch [7/120    avg_loss:1.229, val_acc:0.760]
Epoch [8/120    avg_loss:1.043, val_acc:0.774]
Epoch [9/120    avg_loss:0.921, val_acc:0.831]
Epoch [10/120    avg_loss:0.858, val_acc:0.865]
Epoch [11/120    avg_loss:0.750, val_acc:0.861]
Epoch [12/120    avg_loss:0.675, val_acc:0.863]
Epoch [13/120    avg_loss:0.628, val_acc:0.883]
Epoch [14/120    avg_loss:0.540, val_acc:0.881]
Epoch [15/120    avg_loss:0.578, val_acc:0.865]
Epoch [16/120    avg_loss:0.516, val_acc:0.877]
Epoch [17/120    avg_loss:0.461, val_acc:0.903]
Epoch [18/120    avg_loss:0.436, val_acc:0.903]
Epoch [19/120    avg_loss:0.369, val_acc:0.935]
Epoch [20/120    avg_loss:0.447, val_acc:0.889]
Epoch [21/120    avg_loss:0.340, val_acc:0.897]
Epoch [22/120    avg_loss:0.350, val_acc:0.897]
Epoch [23/120    avg_loss:0.386, val_acc:0.913]
Epoch [24/120    avg_loss:0.345, val_acc:0.915]
Epoch [25/120    avg_loss:0.368, val_acc:0.935]
Epoch [26/120    avg_loss:0.281, val_acc:0.948]
Epoch [27/120    avg_loss:0.292, val_acc:0.942]
Epoch [28/120    avg_loss:0.314, val_acc:0.903]
Epoch [29/120    avg_loss:0.283, val_acc:0.925]
Epoch [30/120    avg_loss:0.267, val_acc:0.933]
Epoch [31/120    avg_loss:0.300, val_acc:0.893]
Epoch [32/120    avg_loss:0.230, val_acc:0.935]
Epoch [33/120    avg_loss:0.223, val_acc:0.929]
Epoch [34/120    avg_loss:0.251, val_acc:0.938]
Epoch [35/120    avg_loss:0.216, val_acc:0.929]
Epoch [36/120    avg_loss:0.208, val_acc:0.960]
Epoch [37/120    avg_loss:0.164, val_acc:0.964]
Epoch [38/120    avg_loss:0.183, val_acc:0.948]
Epoch [39/120    avg_loss:0.206, val_acc:0.913]
Epoch [40/120    avg_loss:0.232, val_acc:0.956]
Epoch [41/120    avg_loss:0.170, val_acc:0.952]
Epoch [42/120    avg_loss:0.172, val_acc:0.931]
Epoch [43/120    avg_loss:0.150, val_acc:0.966]
Epoch [44/120    avg_loss:0.176, val_acc:0.958]
Epoch [45/120    avg_loss:0.210, val_acc:0.956]
Epoch [46/120    avg_loss:0.152, val_acc:0.950]
Epoch [47/120    avg_loss:0.135, val_acc:0.974]
Epoch [48/120    avg_loss:0.125, val_acc:0.980]
Epoch [49/120    avg_loss:0.136, val_acc:0.972]
Epoch [50/120    avg_loss:0.132, val_acc:0.970]
Epoch [51/120    avg_loss:0.115, val_acc:0.978]
Epoch [52/120    avg_loss:0.115, val_acc:0.962]
Epoch [53/120    avg_loss:0.120, val_acc:0.970]
Epoch [54/120    avg_loss:0.108, val_acc:0.970]
Epoch [55/120    avg_loss:0.127, val_acc:0.972]
Epoch [56/120    avg_loss:0.141, val_acc:0.970]
Epoch [57/120    avg_loss:0.107, val_acc:0.966]
Epoch [58/120    avg_loss:0.096, val_acc:0.964]
Epoch [59/120    avg_loss:0.122, val_acc:0.964]
Epoch [60/120    avg_loss:0.159, val_acc:0.974]
Epoch [61/120    avg_loss:0.104, val_acc:0.958]
Epoch [62/120    avg_loss:0.085, val_acc:0.976]
Epoch [63/120    avg_loss:0.064, val_acc:0.974]
Epoch [64/120    avg_loss:0.053, val_acc:0.978]
Epoch [65/120    avg_loss:0.060, val_acc:0.978]
Epoch [66/120    avg_loss:0.053, val_acc:0.980]
Epoch [67/120    avg_loss:0.058, val_acc:0.980]
Epoch [68/120    avg_loss:0.051, val_acc:0.984]
Epoch [69/120    avg_loss:0.057, val_acc:0.984]
Epoch [70/120    avg_loss:0.053, val_acc:0.984]
Epoch [71/120    avg_loss:0.052, val_acc:0.984]
Epoch [72/120    avg_loss:0.039, val_acc:0.984]
Epoch [73/120    avg_loss:0.044, val_acc:0.986]
Epoch [74/120    avg_loss:0.049, val_acc:0.986]
Epoch [75/120    avg_loss:0.045, val_acc:0.986]
Epoch [76/120    avg_loss:0.044, val_acc:0.986]
Epoch [77/120    avg_loss:0.037, val_acc:0.986]
Epoch [78/120    avg_loss:0.047, val_acc:0.986]
Epoch [79/120    avg_loss:0.048, val_acc:0.988]
Epoch [80/120    avg_loss:0.045, val_acc:0.990]
Epoch [81/120    avg_loss:0.048, val_acc:0.992]
Epoch [82/120    avg_loss:0.044, val_acc:0.992]
Epoch [83/120    avg_loss:0.040, val_acc:0.990]
Epoch [84/120    avg_loss:0.035, val_acc:0.988]
Epoch [85/120    avg_loss:0.039, val_acc:0.988]
Epoch [86/120    avg_loss:0.041, val_acc:0.988]
Epoch [87/120    avg_loss:0.035, val_acc:0.988]
Epoch [88/120    avg_loss:0.039, val_acc:0.992]
Epoch [89/120    avg_loss:0.043, val_acc:0.988]
Epoch [90/120    avg_loss:0.036, val_acc:0.990]
Epoch [91/120    avg_loss:0.039, val_acc:0.986]
Epoch [92/120    avg_loss:0.046, val_acc:0.988]
Epoch [93/120    avg_loss:0.039, val_acc:0.992]
Epoch [94/120    avg_loss:0.038, val_acc:0.988]
Epoch [95/120    avg_loss:0.038, val_acc:0.990]
Epoch [96/120    avg_loss:0.037, val_acc:0.988]
Epoch [97/120    avg_loss:0.033, val_acc:0.988]
Epoch [98/120    avg_loss:0.037, val_acc:0.988]
Epoch [99/120    avg_loss:0.036, val_acc:0.992]
Epoch [100/120    avg_loss:0.035, val_acc:0.992]
Epoch [101/120    avg_loss:0.045, val_acc:0.992]
Epoch [102/120    avg_loss:0.036, val_acc:0.988]
Epoch [103/120    avg_loss:0.033, val_acc:0.988]
Epoch [104/120    avg_loss:0.037, val_acc:0.990]
Epoch [105/120    avg_loss:0.043, val_acc:0.994]
Epoch [106/120    avg_loss:0.039, val_acc:0.992]
Epoch [107/120    avg_loss:0.037, val_acc:0.992]
Epoch [108/120    avg_loss:0.039, val_acc:0.992]
Epoch [109/120    avg_loss:0.042, val_acc:0.992]
Epoch [110/120    avg_loss:0.031, val_acc:0.992]
Epoch [111/120    avg_loss:0.037, val_acc:0.992]
Epoch [112/120    avg_loss:0.025, val_acc:0.994]
Epoch [113/120    avg_loss:0.031, val_acc:0.992]
Epoch [114/120    avg_loss:0.032, val_acc:0.992]
Epoch [115/120    avg_loss:0.033, val_acc:0.988]
Epoch [116/120    avg_loss:0.029, val_acc:0.990]
Epoch [117/120    avg_loss:0.035, val_acc:0.992]
Epoch [118/120    avg_loss:0.034, val_acc:0.992]
Epoch [119/120    avg_loss:0.044, val_acc:0.992]
Epoch [120/120    avg_loss:0.030, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99926954 0.98206278 1.         0.95391705 0.93548387
 0.99757869 0.95555556 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9931163276850713
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4180e51a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.485, val_acc:0.373]
Epoch [2/120    avg_loss:2.209, val_acc:0.401]
Epoch [3/120    avg_loss:2.039, val_acc:0.532]
Epoch [4/120    avg_loss:1.847, val_acc:0.623]
Epoch [5/120    avg_loss:1.637, val_acc:0.698]
Epoch [6/120    avg_loss:1.369, val_acc:0.724]
Epoch [7/120    avg_loss:1.157, val_acc:0.766]
Epoch [8/120    avg_loss:1.015, val_acc:0.825]
Epoch [9/120    avg_loss:0.820, val_acc:0.857]
Epoch [10/120    avg_loss:0.693, val_acc:0.861]
Epoch [11/120    avg_loss:0.672, val_acc:0.774]
Epoch [12/120    avg_loss:0.555, val_acc:0.861]
Epoch [13/120    avg_loss:0.576, val_acc:0.877]
Epoch [14/120    avg_loss:0.555, val_acc:0.867]
Epoch [15/120    avg_loss:0.492, val_acc:0.889]
Epoch [16/120    avg_loss:0.466, val_acc:0.827]
Epoch [17/120    avg_loss:0.455, val_acc:0.919]
Epoch [18/120    avg_loss:0.408, val_acc:0.925]
Epoch [19/120    avg_loss:0.432, val_acc:0.792]
Epoch [20/120    avg_loss:0.436, val_acc:0.875]
Epoch [21/120    avg_loss:0.410, val_acc:0.917]
Epoch [22/120    avg_loss:0.335, val_acc:0.919]
Epoch [23/120    avg_loss:0.310, val_acc:0.940]
Epoch [24/120    avg_loss:0.307, val_acc:0.935]
Epoch [25/120    avg_loss:0.313, val_acc:0.917]
Epoch [26/120    avg_loss:0.315, val_acc:0.931]
Epoch [27/120    avg_loss:0.336, val_acc:0.909]
Epoch [28/120    avg_loss:0.282, val_acc:0.944]
Epoch [29/120    avg_loss:0.257, val_acc:0.946]
Epoch [30/120    avg_loss:0.244, val_acc:0.907]
Epoch [31/120    avg_loss:0.227, val_acc:0.948]
Epoch [32/120    avg_loss:0.292, val_acc:0.907]
Epoch [33/120    avg_loss:0.315, val_acc:0.927]
Epoch [34/120    avg_loss:0.351, val_acc:0.923]
Epoch [35/120    avg_loss:0.319, val_acc:0.931]
Epoch [36/120    avg_loss:0.238, val_acc:0.946]
Epoch [37/120    avg_loss:0.192, val_acc:0.958]
Epoch [38/120    avg_loss:0.172, val_acc:0.944]
Epoch [39/120    avg_loss:0.170, val_acc:0.940]
Epoch [40/120    avg_loss:0.203, val_acc:0.899]
Epoch [41/120    avg_loss:0.204, val_acc:0.938]
Epoch [42/120    avg_loss:0.178, val_acc:0.954]
Epoch [43/120    avg_loss:0.230, val_acc:0.915]
Epoch [44/120    avg_loss:0.241, val_acc:0.962]
Epoch [45/120    avg_loss:0.192, val_acc:0.956]
Epoch [46/120    avg_loss:0.131, val_acc:0.970]
Epoch [47/120    avg_loss:0.148, val_acc:0.962]
Epoch [48/120    avg_loss:0.198, val_acc:0.954]
Epoch [49/120    avg_loss:0.122, val_acc:0.946]
Epoch [50/120    avg_loss:0.117, val_acc:0.952]
Epoch [51/120    avg_loss:0.152, val_acc:0.946]
Epoch [52/120    avg_loss:0.145, val_acc:0.954]
Epoch [53/120    avg_loss:0.145, val_acc:0.968]
Epoch [54/120    avg_loss:0.126, val_acc:0.966]
Epoch [55/120    avg_loss:0.110, val_acc:0.964]
Epoch [56/120    avg_loss:0.122, val_acc:0.972]
Epoch [57/120    avg_loss:0.121, val_acc:0.944]
Epoch [58/120    avg_loss:0.100, val_acc:0.982]
Epoch [59/120    avg_loss:0.198, val_acc:0.915]
Epoch [60/120    avg_loss:0.098, val_acc:0.964]
Epoch [61/120    avg_loss:0.076, val_acc:0.966]
Epoch [62/120    avg_loss:0.096, val_acc:0.952]
Epoch [63/120    avg_loss:0.146, val_acc:0.964]
Epoch [64/120    avg_loss:0.124, val_acc:0.952]
Epoch [65/120    avg_loss:0.075, val_acc:0.970]
Epoch [66/120    avg_loss:0.095, val_acc:0.966]
Epoch [67/120    avg_loss:0.089, val_acc:0.970]
Epoch [68/120    avg_loss:0.090, val_acc:0.956]
Epoch [69/120    avg_loss:0.170, val_acc:0.966]
Epoch [70/120    avg_loss:0.105, val_acc:0.962]
Epoch [71/120    avg_loss:0.076, val_acc:0.992]
Epoch [72/120    avg_loss:0.085, val_acc:0.982]
Epoch [73/120    avg_loss:0.065, val_acc:0.986]
Epoch [74/120    avg_loss:0.056, val_acc:0.982]
Epoch [75/120    avg_loss:0.048, val_acc:0.978]
Epoch [76/120    avg_loss:0.037, val_acc:0.984]
Epoch [77/120    avg_loss:0.077, val_acc:0.980]
Epoch [78/120    avg_loss:0.094, val_acc:0.988]
Epoch [79/120    avg_loss:0.046, val_acc:0.984]
Epoch [80/120    avg_loss:0.038, val_acc:0.988]
Epoch [81/120    avg_loss:0.027, val_acc:0.982]
Epoch [82/120    avg_loss:0.059, val_acc:0.966]
Epoch [83/120    avg_loss:0.072, val_acc:0.988]
Epoch [84/120    avg_loss:0.038, val_acc:0.986]
Epoch [85/120    avg_loss:0.034, val_acc:0.988]
Epoch [86/120    avg_loss:0.034, val_acc:0.988]
Epoch [87/120    avg_loss:0.031, val_acc:0.990]
Epoch [88/120    avg_loss:0.024, val_acc:0.986]
Epoch [89/120    avg_loss:0.024, val_acc:0.986]
Epoch [90/120    avg_loss:0.024, val_acc:0.988]
Epoch [91/120    avg_loss:0.021, val_acc:0.988]
Epoch [92/120    avg_loss:0.022, val_acc:0.990]
Epoch [93/120    avg_loss:0.020, val_acc:0.990]
Epoch [94/120    avg_loss:0.021, val_acc:0.990]
Epoch [95/120    avg_loss:0.023, val_acc:0.990]
Epoch [96/120    avg_loss:0.022, val_acc:0.990]
Epoch [97/120    avg_loss:0.018, val_acc:0.988]
Epoch [98/120    avg_loss:0.027, val_acc:0.990]
Epoch [99/120    avg_loss:0.018, val_acc:0.988]
Epoch [100/120    avg_loss:0.020, val_acc:0.988]
Epoch [101/120    avg_loss:0.021, val_acc:0.988]
Epoch [102/120    avg_loss:0.022, val_acc:0.988]
Epoch [103/120    avg_loss:0.018, val_acc:0.988]
Epoch [104/120    avg_loss:0.023, val_acc:0.988]
Epoch [105/120    avg_loss:0.020, val_acc:0.990]
Epoch [106/120    avg_loss:0.021, val_acc:0.990]
Epoch [107/120    avg_loss:0.023, val_acc:0.990]
Epoch [108/120    avg_loss:0.022, val_acc:0.990]
Epoch [109/120    avg_loss:0.017, val_acc:0.990]
Epoch [110/120    avg_loss:0.021, val_acc:0.990]
Epoch [111/120    avg_loss:0.024, val_acc:0.990]
Epoch [112/120    avg_loss:0.024, val_acc:0.990]
Epoch [113/120    avg_loss:0.025, val_acc:0.990]
Epoch [114/120    avg_loss:0.021, val_acc:0.990]
Epoch [115/120    avg_loss:0.017, val_acc:0.990]
Epoch [116/120    avg_loss:0.022, val_acc:0.990]
Epoch [117/120    avg_loss:0.022, val_acc:0.990]
Epoch [118/120    avg_loss:0.022, val_acc:0.990]
Epoch [119/120    avg_loss:0.021, val_acc:0.990]
Epoch [120/120    avg_loss:0.020, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0  13   0   0   0   0   0   0 375   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 0.99060014 0.99545455 0.99563319 0.93421053 0.90657439
 1.         0.98924731 0.98296199 1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9893154480193889
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3992857a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.288]
Epoch [2/120    avg_loss:2.327, val_acc:0.498]
Epoch [3/120    avg_loss:2.096, val_acc:0.631]
Epoch [4/120    avg_loss:1.828, val_acc:0.675]
Epoch [5/120    avg_loss:1.564, val_acc:0.685]
Epoch [6/120    avg_loss:1.280, val_acc:0.758]
Epoch [7/120    avg_loss:1.038, val_acc:0.756]
Epoch [8/120    avg_loss:0.886, val_acc:0.869]
Epoch [9/120    avg_loss:0.770, val_acc:0.800]
Epoch [10/120    avg_loss:0.785, val_acc:0.812]
Epoch [11/120    avg_loss:0.666, val_acc:0.893]
Epoch [12/120    avg_loss:0.579, val_acc:0.768]
Epoch [13/120    avg_loss:0.553, val_acc:0.911]
Epoch [14/120    avg_loss:0.516, val_acc:0.915]
Epoch [15/120    avg_loss:0.513, val_acc:0.903]
Epoch [16/120    avg_loss:0.418, val_acc:0.873]
Epoch [17/120    avg_loss:0.448, val_acc:0.867]
Epoch [18/120    avg_loss:0.402, val_acc:0.940]
Epoch [19/120    avg_loss:0.332, val_acc:0.931]
Epoch [20/120    avg_loss:0.356, val_acc:0.925]
Epoch [21/120    avg_loss:0.393, val_acc:0.919]
Epoch [22/120    avg_loss:0.310, val_acc:0.933]
Epoch [23/120    avg_loss:0.383, val_acc:0.921]
Epoch [24/120    avg_loss:0.315, val_acc:0.933]
Epoch [25/120    avg_loss:0.312, val_acc:0.938]
Epoch [26/120    avg_loss:0.262, val_acc:0.958]
Epoch [27/120    avg_loss:0.229, val_acc:0.950]
Epoch [28/120    avg_loss:0.289, val_acc:0.950]
Epoch [29/120    avg_loss:0.255, val_acc:0.946]
Epoch [30/120    avg_loss:0.270, val_acc:0.942]
Epoch [31/120    avg_loss:0.227, val_acc:0.942]
Epoch [32/120    avg_loss:0.216, val_acc:0.966]
Epoch [33/120    avg_loss:0.229, val_acc:0.958]
Epoch [34/120    avg_loss:0.213, val_acc:0.952]
Epoch [35/120    avg_loss:0.181, val_acc:0.966]
Epoch [36/120    avg_loss:0.176, val_acc:0.968]
Epoch [37/120    avg_loss:0.169, val_acc:0.958]
Epoch [38/120    avg_loss:0.140, val_acc:0.974]
Epoch [39/120    avg_loss:0.227, val_acc:0.970]
Epoch [40/120    avg_loss:0.162, val_acc:0.960]
Epoch [41/120    avg_loss:0.127, val_acc:0.978]
Epoch [42/120    avg_loss:0.142, val_acc:0.976]
Epoch [43/120    avg_loss:0.125, val_acc:0.980]
Epoch [44/120    avg_loss:0.117, val_acc:0.956]
Epoch [45/120    avg_loss:0.162, val_acc:0.960]
Epoch [46/120    avg_loss:0.117, val_acc:0.972]
Epoch [47/120    avg_loss:0.144, val_acc:0.962]
Epoch [48/120    avg_loss:0.174, val_acc:0.982]
Epoch [49/120    avg_loss:0.123, val_acc:0.980]
Epoch [50/120    avg_loss:0.094, val_acc:0.982]
Epoch [51/120    avg_loss:0.123, val_acc:0.972]
Epoch [52/120    avg_loss:0.131, val_acc:0.978]
Epoch [53/120    avg_loss:0.106, val_acc:0.988]
Epoch [54/120    avg_loss:0.097, val_acc:0.980]
Epoch [55/120    avg_loss:0.106, val_acc:0.976]
Epoch [56/120    avg_loss:0.101, val_acc:0.990]
Epoch [57/120    avg_loss:0.086, val_acc:0.974]
Epoch [58/120    avg_loss:0.107, val_acc:0.986]
Epoch [59/120    avg_loss:0.106, val_acc:0.970]
Epoch [60/120    avg_loss:0.063, val_acc:0.986]
Epoch [61/120    avg_loss:0.063, val_acc:0.982]
Epoch [62/120    avg_loss:0.060, val_acc:0.992]
Epoch [63/120    avg_loss:0.075, val_acc:0.982]
Epoch [64/120    avg_loss:0.067, val_acc:0.996]
Epoch [65/120    avg_loss:0.049, val_acc:0.984]
Epoch [66/120    avg_loss:0.083, val_acc:0.992]
Epoch [67/120    avg_loss:0.054, val_acc:0.994]
Epoch [68/120    avg_loss:0.062, val_acc:0.974]
Epoch [69/120    avg_loss:0.083, val_acc:0.972]
Epoch [70/120    avg_loss:0.066, val_acc:0.984]
Epoch [71/120    avg_loss:0.092, val_acc:0.909]
Epoch [72/120    avg_loss:0.200, val_acc:0.974]
Epoch [73/120    avg_loss:0.097, val_acc:0.992]
Epoch [74/120    avg_loss:0.077, val_acc:0.990]
Epoch [75/120    avg_loss:0.110, val_acc:0.972]
Epoch [76/120    avg_loss:0.094, val_acc:0.986]
Epoch [77/120    avg_loss:0.066, val_acc:0.972]
Epoch [78/120    avg_loss:0.079, val_acc:0.992]
Epoch [79/120    avg_loss:0.042, val_acc:0.992]
Epoch [80/120    avg_loss:0.044, val_acc:0.992]
Epoch [81/120    avg_loss:0.041, val_acc:0.992]
Epoch [82/120    avg_loss:0.032, val_acc:0.994]
Epoch [83/120    avg_loss:0.034, val_acc:0.992]
Epoch [84/120    avg_loss:0.036, val_acc:0.992]
Epoch [85/120    avg_loss:0.039, val_acc:0.992]
Epoch [86/120    avg_loss:0.035, val_acc:0.992]
Epoch [87/120    avg_loss:0.030, val_acc:0.992]
Epoch [88/120    avg_loss:0.029, val_acc:0.992]
Epoch [89/120    avg_loss:0.029, val_acc:0.992]
Epoch [90/120    avg_loss:0.027, val_acc:0.996]
Epoch [91/120    avg_loss:0.031, val_acc:0.996]
Epoch [92/120    avg_loss:0.030, val_acc:0.996]
Epoch [93/120    avg_loss:0.037, val_acc:0.994]
Epoch [94/120    avg_loss:0.030, val_acc:0.992]
Epoch [95/120    avg_loss:0.030, val_acc:0.994]
Epoch [96/120    avg_loss:0.026, val_acc:0.994]
Epoch [97/120    avg_loss:0.022, val_acc:0.994]
Epoch [98/120    avg_loss:0.022, val_acc:0.994]
Epoch [99/120    avg_loss:0.022, val_acc:0.994]
Epoch [100/120    avg_loss:0.027, val_acc:0.994]
Epoch [101/120    avg_loss:0.028, val_acc:0.994]
Epoch [102/120    avg_loss:0.032, val_acc:0.994]
Epoch [103/120    avg_loss:0.029, val_acc:0.994]
Epoch [104/120    avg_loss:0.026, val_acc:0.994]
Epoch [105/120    avg_loss:0.025, val_acc:0.994]
Epoch [106/120    avg_loss:0.021, val_acc:0.994]
Epoch [107/120    avg_loss:0.025, val_acc:0.994]
Epoch [108/120    avg_loss:0.024, val_acc:0.994]
Epoch [109/120    avg_loss:0.022, val_acc:0.994]
Epoch [110/120    avg_loss:0.024, val_acc:0.994]
Epoch [111/120    avg_loss:0.027, val_acc:0.994]
Epoch [112/120    avg_loss:0.028, val_acc:0.994]
Epoch [113/120    avg_loss:0.022, val_acc:0.994]
Epoch [114/120    avg_loss:0.030, val_acc:0.994]
Epoch [115/120    avg_loss:0.023, val_acc:0.994]
Epoch [116/120    avg_loss:0.026, val_acc:0.994]
Epoch [117/120    avg_loss:0.027, val_acc:0.994]
Epoch [118/120    avg_loss:0.025, val_acc:0.994]
Epoch [119/120    avg_loss:0.024, val_acc:0.994]
Epoch [120/120    avg_loss:0.024, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   9   1   0   0   0   0   0 378   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0  11 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99127907 0.97550111 0.99782135 0.95890411 0.94117647
 0.99277108 0.95555556 0.98694517 1.         1.         0.98429319
 0.98547486 1.        ]

Kappa:
0.9874189420918053
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16162c3ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.535, val_acc:0.365]
Epoch [2/120    avg_loss:2.294, val_acc:0.438]
Epoch [3/120    avg_loss:2.090, val_acc:0.536]
Epoch [4/120    avg_loss:1.863, val_acc:0.617]
Epoch [5/120    avg_loss:1.633, val_acc:0.645]
Epoch [6/120    avg_loss:1.412, val_acc:0.649]
Epoch [7/120    avg_loss:1.223, val_acc:0.776]
Epoch [8/120    avg_loss:1.047, val_acc:0.806]
Epoch [9/120    avg_loss:0.944, val_acc:0.794]
Epoch [10/120    avg_loss:0.875, val_acc:0.770]
Epoch [11/120    avg_loss:0.762, val_acc:0.776]
Epoch [12/120    avg_loss:0.723, val_acc:0.812]
Epoch [13/120    avg_loss:0.694, val_acc:0.827]
Epoch [14/120    avg_loss:0.658, val_acc:0.758]
Epoch [15/120    avg_loss:0.564, val_acc:0.847]
Epoch [16/120    avg_loss:0.507, val_acc:0.871]
Epoch [17/120    avg_loss:0.508, val_acc:0.871]
Epoch [18/120    avg_loss:0.474, val_acc:0.879]
Epoch [19/120    avg_loss:0.436, val_acc:0.883]
Epoch [20/120    avg_loss:0.377, val_acc:0.927]
Epoch [21/120    avg_loss:0.354, val_acc:0.915]
Epoch [22/120    avg_loss:0.388, val_acc:0.903]
Epoch [23/120    avg_loss:0.354, val_acc:0.929]
Epoch [24/120    avg_loss:0.366, val_acc:0.913]
Epoch [25/120    avg_loss:0.332, val_acc:0.919]
Epoch [26/120    avg_loss:0.306, val_acc:0.919]
Epoch [27/120    avg_loss:0.327, val_acc:0.909]
Epoch [28/120    avg_loss:0.301, val_acc:0.877]
Epoch [29/120    avg_loss:0.320, val_acc:0.915]
Epoch [30/120    avg_loss:0.298, val_acc:0.929]
Epoch [31/120    avg_loss:0.254, val_acc:0.952]
Epoch [32/120    avg_loss:0.249, val_acc:0.954]
Epoch [33/120    avg_loss:0.208, val_acc:0.958]
Epoch [34/120    avg_loss:0.230, val_acc:0.942]
Epoch [35/120    avg_loss:0.297, val_acc:0.952]
Epoch [36/120    avg_loss:0.340, val_acc:0.940]
Epoch [37/120    avg_loss:0.213, val_acc:0.960]
Epoch [38/120    avg_loss:0.210, val_acc:0.950]
Epoch [39/120    avg_loss:0.205, val_acc:0.956]
Epoch [40/120    avg_loss:0.206, val_acc:0.946]
Epoch [41/120    avg_loss:0.184, val_acc:0.956]
Epoch [42/120    avg_loss:0.152, val_acc:0.950]
Epoch [43/120    avg_loss:0.195, val_acc:0.952]
Epoch [44/120    avg_loss:0.156, val_acc:0.952]
Epoch [45/120    avg_loss:0.167, val_acc:0.966]
Epoch [46/120    avg_loss:0.169, val_acc:0.962]
Epoch [47/120    avg_loss:0.127, val_acc:0.968]
Epoch [48/120    avg_loss:0.109, val_acc:0.976]
Epoch [49/120    avg_loss:0.130, val_acc:0.976]
Epoch [50/120    avg_loss:0.113, val_acc:0.960]
Epoch [51/120    avg_loss:0.087, val_acc:0.978]
Epoch [52/120    avg_loss:0.089, val_acc:0.972]
Epoch [53/120    avg_loss:0.098, val_acc:0.958]
Epoch [54/120    avg_loss:0.152, val_acc:0.970]
Epoch [55/120    avg_loss:0.160, val_acc:0.954]
Epoch [56/120    avg_loss:0.209, val_acc:0.956]
Epoch [57/120    avg_loss:0.145, val_acc:0.974]
Epoch [58/120    avg_loss:0.094, val_acc:0.938]
Epoch [59/120    avg_loss:0.128, val_acc:0.964]
Epoch [60/120    avg_loss:0.077, val_acc:0.980]
Epoch [61/120    avg_loss:0.068, val_acc:0.978]
Epoch [62/120    avg_loss:0.112, val_acc:0.986]
Epoch [63/120    avg_loss:0.060, val_acc:0.982]
Epoch [64/120    avg_loss:0.049, val_acc:0.978]
Epoch [65/120    avg_loss:0.118, val_acc:0.897]
Epoch [66/120    avg_loss:0.168, val_acc:0.984]
Epoch [67/120    avg_loss:0.056, val_acc:0.982]
Epoch [68/120    avg_loss:0.065, val_acc:0.980]
Epoch [69/120    avg_loss:0.075, val_acc:0.980]
Epoch [70/120    avg_loss:0.059, val_acc:0.984]
Epoch [71/120    avg_loss:0.055, val_acc:0.982]
Epoch [72/120    avg_loss:0.041, val_acc:0.986]
Epoch [73/120    avg_loss:0.070, val_acc:0.970]
Epoch [74/120    avg_loss:0.084, val_acc:0.962]
Epoch [75/120    avg_loss:0.080, val_acc:0.962]
Epoch [76/120    avg_loss:0.063, val_acc:0.976]
Epoch [77/120    avg_loss:0.066, val_acc:0.960]
Epoch [78/120    avg_loss:0.135, val_acc:0.988]
Epoch [79/120    avg_loss:0.061, val_acc:0.982]
Epoch [80/120    avg_loss:0.039, val_acc:0.978]
Epoch [81/120    avg_loss:0.041, val_acc:0.982]
Epoch [82/120    avg_loss:0.025, val_acc:0.986]
Epoch [83/120    avg_loss:0.043, val_acc:0.982]
Epoch [84/120    avg_loss:0.065, val_acc:0.978]
Epoch [85/120    avg_loss:0.042, val_acc:0.984]
Epoch [86/120    avg_loss:0.037, val_acc:0.986]
Epoch [87/120    avg_loss:0.028, val_acc:0.986]
Epoch [88/120    avg_loss:0.054, val_acc:0.978]
Epoch [89/120    avg_loss:0.060, val_acc:0.976]
Epoch [90/120    avg_loss:0.032, val_acc:0.982]
Epoch [91/120    avg_loss:0.031, val_acc:0.978]
Epoch [92/120    avg_loss:0.040, val_acc:0.984]
Epoch [93/120    avg_loss:0.021, val_acc:0.986]
Epoch [94/120    avg_loss:0.019, val_acc:0.986]
Epoch [95/120    avg_loss:0.017, val_acc:0.986]
Epoch [96/120    avg_loss:0.015, val_acc:0.986]
Epoch [97/120    avg_loss:0.017, val_acc:0.986]
Epoch [98/120    avg_loss:0.017, val_acc:0.986]
Epoch [99/120    avg_loss:0.016, val_acc:0.986]
Epoch [100/120    avg_loss:0.015, val_acc:0.986]
Epoch [101/120    avg_loss:0.018, val_acc:0.988]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.015, val_acc:0.986]
Epoch [104/120    avg_loss:0.016, val_acc:0.986]
Epoch [105/120    avg_loss:0.014, val_acc:0.986]
Epoch [106/120    avg_loss:0.015, val_acc:0.986]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.016, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.986]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.013, val_acc:0.986]
Epoch [112/120    avg_loss:0.019, val_acc:0.986]
Epoch [113/120    avg_loss:0.013, val_acc:0.988]
Epoch [114/120    avg_loss:0.013, val_acc:0.988]
Epoch [115/120    avg_loss:0.013, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.013, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.986]
Epoch [119/120    avg_loss:0.013, val_acc:0.986]
Epoch [120/120    avg_loss:0.022, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 1.         0.98426966 1.         0.96428571 0.94594595
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.994540120698409
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4869b7a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.485, val_acc:0.371]
Epoch [2/120    avg_loss:2.196, val_acc:0.446]
Epoch [3/120    avg_loss:2.001, val_acc:0.556]
Epoch [4/120    avg_loss:1.799, val_acc:0.603]
Epoch [5/120    avg_loss:1.567, val_acc:0.637]
Epoch [6/120    avg_loss:1.315, val_acc:0.675]
Epoch [7/120    avg_loss:1.167, val_acc:0.718]
Epoch [8/120    avg_loss:1.007, val_acc:0.768]
Epoch [9/120    avg_loss:0.894, val_acc:0.843]
Epoch [10/120    avg_loss:0.765, val_acc:0.899]
Epoch [11/120    avg_loss:0.662, val_acc:0.802]
Epoch [12/120    avg_loss:0.601, val_acc:0.899]
Epoch [13/120    avg_loss:0.496, val_acc:0.889]
Epoch [14/120    avg_loss:0.484, val_acc:0.883]
Epoch [15/120    avg_loss:0.427, val_acc:0.895]
Epoch [16/120    avg_loss:0.378, val_acc:0.891]
Epoch [17/120    avg_loss:0.352, val_acc:0.919]
Epoch [18/120    avg_loss:0.362, val_acc:0.927]
Epoch [19/120    avg_loss:0.365, val_acc:0.901]
Epoch [20/120    avg_loss:0.358, val_acc:0.879]
Epoch [21/120    avg_loss:0.338, val_acc:0.927]
Epoch [22/120    avg_loss:0.262, val_acc:0.962]
Epoch [23/120    avg_loss:0.262, val_acc:0.944]
Epoch [24/120    avg_loss:0.229, val_acc:0.958]
Epoch [25/120    avg_loss:0.286, val_acc:0.921]
Epoch [26/120    avg_loss:0.217, val_acc:0.958]
Epoch [27/120    avg_loss:0.240, val_acc:0.944]
Epoch [28/120    avg_loss:0.200, val_acc:0.954]
Epoch [29/120    avg_loss:0.186, val_acc:0.925]
Epoch [30/120    avg_loss:0.178, val_acc:0.938]
Epoch [31/120    avg_loss:0.168, val_acc:0.966]
Epoch [32/120    avg_loss:0.178, val_acc:0.911]
Epoch [33/120    avg_loss:0.208, val_acc:0.962]
Epoch [34/120    avg_loss:0.200, val_acc:0.938]
Epoch [35/120    avg_loss:0.162, val_acc:0.968]
Epoch [36/120    avg_loss:0.136, val_acc:0.962]
Epoch [37/120    avg_loss:0.131, val_acc:0.960]
Epoch [38/120    avg_loss:0.143, val_acc:0.986]
Epoch [39/120    avg_loss:0.133, val_acc:0.956]
Epoch [40/120    avg_loss:0.098, val_acc:0.970]
Epoch [41/120    avg_loss:0.089, val_acc:0.978]
Epoch [42/120    avg_loss:0.083, val_acc:0.964]
Epoch [43/120    avg_loss:0.071, val_acc:0.978]
Epoch [44/120    avg_loss:0.100, val_acc:0.980]
Epoch [45/120    avg_loss:0.109, val_acc:0.956]
Epoch [46/120    avg_loss:0.137, val_acc:0.974]
Epoch [47/120    avg_loss:0.074, val_acc:0.978]
Epoch [48/120    avg_loss:0.146, val_acc:0.974]
Epoch [49/120    avg_loss:0.108, val_acc:0.984]
Epoch [50/120    avg_loss:0.069, val_acc:0.964]
Epoch [51/120    avg_loss:0.105, val_acc:0.976]
Epoch [52/120    avg_loss:0.052, val_acc:0.982]
Epoch [53/120    avg_loss:0.046, val_acc:0.984]
Epoch [54/120    avg_loss:0.042, val_acc:0.986]
Epoch [55/120    avg_loss:0.039, val_acc:0.986]
Epoch [56/120    avg_loss:0.041, val_acc:0.988]
Epoch [57/120    avg_loss:0.035, val_acc:0.988]
Epoch [58/120    avg_loss:0.043, val_acc:0.988]
Epoch [59/120    avg_loss:0.038, val_acc:0.988]
Epoch [60/120    avg_loss:0.033, val_acc:0.988]
Epoch [61/120    avg_loss:0.041, val_acc:0.988]
Epoch [62/120    avg_loss:0.039, val_acc:0.988]
Epoch [63/120    avg_loss:0.031, val_acc:0.988]
Epoch [64/120    avg_loss:0.035, val_acc:0.988]
Epoch [65/120    avg_loss:0.032, val_acc:0.988]
Epoch [66/120    avg_loss:0.029, val_acc:0.988]
Epoch [67/120    avg_loss:0.035, val_acc:0.988]
Epoch [68/120    avg_loss:0.040, val_acc:0.988]
Epoch [69/120    avg_loss:0.032, val_acc:0.988]
Epoch [70/120    avg_loss:0.039, val_acc:0.988]
Epoch [71/120    avg_loss:0.038, val_acc:0.988]
Epoch [72/120    avg_loss:0.035, val_acc:0.988]
Epoch [73/120    avg_loss:0.033, val_acc:0.988]
Epoch [74/120    avg_loss:0.037, val_acc:0.988]
Epoch [75/120    avg_loss:0.034, val_acc:0.988]
Epoch [76/120    avg_loss:0.034, val_acc:0.988]
Epoch [77/120    avg_loss:0.029, val_acc:0.988]
Epoch [78/120    avg_loss:0.031, val_acc:0.988]
Epoch [79/120    avg_loss:0.031, val_acc:0.988]
Epoch [80/120    avg_loss:0.028, val_acc:0.988]
Epoch [81/120    avg_loss:0.033, val_acc:0.988]
Epoch [82/120    avg_loss:0.035, val_acc:0.988]
Epoch [83/120    avg_loss:0.031, val_acc:0.988]
Epoch [84/120    avg_loss:0.033, val_acc:0.988]
Epoch [85/120    avg_loss:0.032, val_acc:0.988]
Epoch [86/120    avg_loss:0.029, val_acc:0.988]
Epoch [87/120    avg_loss:0.036, val_acc:0.986]
Epoch [88/120    avg_loss:0.029, val_acc:0.986]
Epoch [89/120    avg_loss:0.033, val_acc:0.988]
Epoch [90/120    avg_loss:0.028, val_acc:0.988]
Epoch [91/120    avg_loss:0.028, val_acc:0.988]
Epoch [92/120    avg_loss:0.026, val_acc:0.988]
Epoch [93/120    avg_loss:0.026, val_acc:0.988]
Epoch [94/120    avg_loss:0.026, val_acc:0.988]
Epoch [95/120    avg_loss:0.025, val_acc:0.988]
Epoch [96/120    avg_loss:0.031, val_acc:0.988]
Epoch [97/120    avg_loss:0.027, val_acc:0.988]
Epoch [98/120    avg_loss:0.028, val_acc:0.988]
Epoch [99/120    avg_loss:0.025, val_acc:0.988]
Epoch [100/120    avg_loss:0.021, val_acc:0.988]
Epoch [101/120    avg_loss:0.022, val_acc:0.988]
Epoch [102/120    avg_loss:0.027, val_acc:0.988]
Epoch [103/120    avg_loss:0.027, val_acc:0.988]
Epoch [104/120    avg_loss:0.021, val_acc:0.988]
Epoch [105/120    avg_loss:0.025, val_acc:0.988]
Epoch [106/120    avg_loss:0.028, val_acc:0.986]
Epoch [107/120    avg_loss:0.028, val_acc:0.986]
Epoch [108/120    avg_loss:0.023, val_acc:0.988]
Epoch [109/120    avg_loss:0.020, val_acc:0.988]
Epoch [110/120    avg_loss:0.027, val_acc:0.988]
Epoch [111/120    avg_loss:0.023, val_acc:0.988]
Epoch [112/120    avg_loss:0.023, val_acc:0.988]
Epoch [113/120    avg_loss:0.023, val_acc:0.988]
Epoch [114/120    avg_loss:0.022, val_acc:0.988]
Epoch [115/120    avg_loss:0.025, val_acc:0.988]
Epoch [116/120    avg_loss:0.024, val_acc:0.988]
Epoch [117/120    avg_loss:0.023, val_acc:0.988]
Epoch [118/120    avg_loss:0.019, val_acc:0.988]
Epoch [119/120    avg_loss:0.020, val_acc:0.988]
Epoch [120/120    avg_loss:0.027, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99780541 0.98871332 1.         0.95067265 0.9261745
 0.99277108 0.9726776  1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9926416933326625
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff30854a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.499, val_acc:0.333]
Epoch [2/120    avg_loss:2.217, val_acc:0.381]
Epoch [3/120    avg_loss:2.009, val_acc:0.554]
Epoch [4/120    avg_loss:1.780, val_acc:0.637]
Epoch [5/120    avg_loss:1.525, val_acc:0.692]
Epoch [6/120    avg_loss:1.305, val_acc:0.704]
Epoch [7/120    avg_loss:1.155, val_acc:0.736]
Epoch [8/120    avg_loss:0.990, val_acc:0.716]
Epoch [9/120    avg_loss:0.890, val_acc:0.812]
Epoch [10/120    avg_loss:0.805, val_acc:0.790]
Epoch [11/120    avg_loss:0.742, val_acc:0.857]
Epoch [12/120    avg_loss:0.644, val_acc:0.792]
Epoch [13/120    avg_loss:0.686, val_acc:0.837]
Epoch [14/120    avg_loss:0.590, val_acc:0.849]
Epoch [15/120    avg_loss:0.519, val_acc:0.863]
Epoch [16/120    avg_loss:0.545, val_acc:0.893]
Epoch [17/120    avg_loss:0.478, val_acc:0.885]
Epoch [18/120    avg_loss:0.533, val_acc:0.889]
Epoch [19/120    avg_loss:0.427, val_acc:0.885]
Epoch [20/120    avg_loss:0.403, val_acc:0.839]
Epoch [21/120    avg_loss:0.438, val_acc:0.861]
Epoch [22/120    avg_loss:0.369, val_acc:0.925]
Epoch [23/120    avg_loss:0.333, val_acc:0.927]
Epoch [24/120    avg_loss:0.291, val_acc:0.935]
Epoch [25/120    avg_loss:0.343, val_acc:0.948]
Epoch [26/120    avg_loss:0.314, val_acc:0.913]
Epoch [27/120    avg_loss:0.269, val_acc:0.948]
Epoch [28/120    avg_loss:0.321, val_acc:0.942]
Epoch [29/120    avg_loss:0.327, val_acc:0.946]
Epoch [30/120    avg_loss:0.252, val_acc:0.940]
Epoch [31/120    avg_loss:0.215, val_acc:0.933]
Epoch [32/120    avg_loss:0.254, val_acc:0.923]
Epoch [33/120    avg_loss:0.222, val_acc:0.968]
Epoch [34/120    avg_loss:0.202, val_acc:0.964]
Epoch [35/120    avg_loss:0.232, val_acc:0.962]
Epoch [36/120    avg_loss:0.175, val_acc:0.960]
Epoch [37/120    avg_loss:0.201, val_acc:0.968]
Epoch [38/120    avg_loss:0.130, val_acc:0.976]
Epoch [39/120    avg_loss:0.143, val_acc:0.948]
Epoch [40/120    avg_loss:0.165, val_acc:0.948]
Epoch [41/120    avg_loss:0.132, val_acc:0.972]
Epoch [42/120    avg_loss:0.156, val_acc:0.958]
Epoch [43/120    avg_loss:0.141, val_acc:0.954]
Epoch [44/120    avg_loss:0.155, val_acc:0.974]
Epoch [45/120    avg_loss:0.137, val_acc:0.964]
Epoch [46/120    avg_loss:0.111, val_acc:0.978]
Epoch [47/120    avg_loss:0.155, val_acc:0.938]
Epoch [48/120    avg_loss:0.145, val_acc:0.978]
Epoch [49/120    avg_loss:0.107, val_acc:0.986]
Epoch [50/120    avg_loss:0.103, val_acc:0.978]
Epoch [51/120    avg_loss:0.173, val_acc:0.970]
Epoch [52/120    avg_loss:0.110, val_acc:0.978]
Epoch [53/120    avg_loss:0.136, val_acc:0.966]
Epoch [54/120    avg_loss:0.120, val_acc:0.966]
Epoch [55/120    avg_loss:0.088, val_acc:0.984]
Epoch [56/120    avg_loss:0.064, val_acc:0.990]
Epoch [57/120    avg_loss:0.078, val_acc:0.978]
Epoch [58/120    avg_loss:0.092, val_acc:0.986]
Epoch [59/120    avg_loss:0.156, val_acc:0.972]
Epoch [60/120    avg_loss:0.086, val_acc:0.980]
Epoch [61/120    avg_loss:0.087, val_acc:0.982]
Epoch [62/120    avg_loss:0.100, val_acc:0.980]
Epoch [63/120    avg_loss:0.047, val_acc:0.988]
Epoch [64/120    avg_loss:0.045, val_acc:0.980]
Epoch [65/120    avg_loss:0.036, val_acc:0.990]
Epoch [66/120    avg_loss:0.038, val_acc:0.970]
Epoch [67/120    avg_loss:0.038, val_acc:0.988]
Epoch [68/120    avg_loss:0.026, val_acc:0.992]
Epoch [69/120    avg_loss:0.034, val_acc:0.990]
Epoch [70/120    avg_loss:0.051, val_acc:0.992]
Epoch [71/120    avg_loss:0.047, val_acc:0.980]
Epoch [72/120    avg_loss:0.080, val_acc:0.986]
Epoch [73/120    avg_loss:0.049, val_acc:0.984]
Epoch [74/120    avg_loss:0.044, val_acc:0.986]
Epoch [75/120    avg_loss:0.035, val_acc:0.988]
Epoch [76/120    avg_loss:0.038, val_acc:0.970]
Epoch [77/120    avg_loss:0.057, val_acc:0.988]
Epoch [78/120    avg_loss:0.028, val_acc:0.982]
Epoch [79/120    avg_loss:0.027, val_acc:0.990]
Epoch [80/120    avg_loss:0.024, val_acc:0.988]
Epoch [81/120    avg_loss:0.025, val_acc:0.988]
Epoch [82/120    avg_loss:0.062, val_acc:0.972]
Epoch [83/120    avg_loss:0.099, val_acc:0.978]
Epoch [84/120    avg_loss:0.062, val_acc:0.984]
Epoch [85/120    avg_loss:0.036, val_acc:0.986]
Epoch [86/120    avg_loss:0.048, val_acc:0.984]
Epoch [87/120    avg_loss:0.033, val_acc:0.988]
Epoch [88/120    avg_loss:0.040, val_acc:0.986]
Epoch [89/120    avg_loss:0.020, val_acc:0.986]
Epoch [90/120    avg_loss:0.025, val_acc:0.986]
Epoch [91/120    avg_loss:0.021, val_acc:0.986]
Epoch [92/120    avg_loss:0.025, val_acc:0.986]
Epoch [93/120    avg_loss:0.024, val_acc:0.986]
Epoch [94/120    avg_loss:0.031, val_acc:0.984]
Epoch [95/120    avg_loss:0.021, val_acc:0.986]
Epoch [96/120    avg_loss:0.019, val_acc:0.990]
Epoch [97/120    avg_loss:0.016, val_acc:0.990]
Epoch [98/120    avg_loss:0.020, val_acc:0.990]
Epoch [99/120    avg_loss:0.018, val_acc:0.990]
Epoch [100/120    avg_loss:0.018, val_acc:0.990]
Epoch [101/120    avg_loss:0.015, val_acc:0.990]
Epoch [102/120    avg_loss:0.019, val_acc:0.990]
Epoch [103/120    avg_loss:0.016, val_acc:0.990]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.022, val_acc:0.990]
Epoch [106/120    avg_loss:0.017, val_acc:0.990]
Epoch [107/120    avg_loss:0.026, val_acc:0.990]
Epoch [108/120    avg_loss:0.017, val_acc:0.990]
Epoch [109/120    avg_loss:0.022, val_acc:0.990]
Epoch [110/120    avg_loss:0.024, val_acc:0.990]
Epoch [111/120    avg_loss:0.020, val_acc:0.990]
Epoch [112/120    avg_loss:0.017, val_acc:0.990]
Epoch [113/120    avg_loss:0.014, val_acc:0.990]
Epoch [114/120    avg_loss:0.021, val_acc:0.990]
Epoch [115/120    avg_loss:0.018, val_acc:0.990]
Epoch [116/120    avg_loss:0.016, val_acc:0.990]
Epoch [117/120    avg_loss:0.019, val_acc:0.990]
Epoch [118/120    avg_loss:0.017, val_acc:0.990]
Epoch [119/120    avg_loss:0.019, val_acc:0.990]
Epoch [120/120    avg_loss:0.015, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99780541 0.98871332 1.         0.9610984  0.94462541
 0.99277108 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9940661261789051
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff62948dac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.523, val_acc:0.498]
Epoch [2/120    avg_loss:2.245, val_acc:0.535]
Epoch [3/120    avg_loss:2.020, val_acc:0.545]
Epoch [4/120    avg_loss:1.786, val_acc:0.582]
Epoch [5/120    avg_loss:1.586, val_acc:0.602]
Epoch [6/120    avg_loss:1.354, val_acc:0.670]
Epoch [7/120    avg_loss:1.212, val_acc:0.736]
Epoch [8/120    avg_loss:1.033, val_acc:0.793]
Epoch [9/120    avg_loss:0.926, val_acc:0.754]
Epoch [10/120    avg_loss:0.785, val_acc:0.814]
Epoch [11/120    avg_loss:0.716, val_acc:0.791]
Epoch [12/120    avg_loss:0.685, val_acc:0.863]
Epoch [13/120    avg_loss:0.608, val_acc:0.797]
Epoch [14/120    avg_loss:0.545, val_acc:0.820]
Epoch [15/120    avg_loss:0.571, val_acc:0.857]
Epoch [16/120    avg_loss:0.490, val_acc:0.875]
Epoch [17/120    avg_loss:0.439, val_acc:0.902]
Epoch [18/120    avg_loss:0.447, val_acc:0.898]
Epoch [19/120    avg_loss:0.420, val_acc:0.918]
Epoch [20/120    avg_loss:0.428, val_acc:0.906]
Epoch [21/120    avg_loss:0.341, val_acc:0.879]
Epoch [22/120    avg_loss:0.421, val_acc:0.893]
Epoch [23/120    avg_loss:0.361, val_acc:0.934]
Epoch [24/120    avg_loss:0.323, val_acc:0.922]
Epoch [25/120    avg_loss:0.276, val_acc:0.918]
Epoch [26/120    avg_loss:0.345, val_acc:0.883]
Epoch [27/120    avg_loss:0.320, val_acc:0.857]
Epoch [28/120    avg_loss:0.384, val_acc:0.939]
Epoch [29/120    avg_loss:0.323, val_acc:0.891]
Epoch [30/120    avg_loss:0.302, val_acc:0.895]
Epoch [31/120    avg_loss:0.254, val_acc:0.904]
Epoch [32/120    avg_loss:0.337, val_acc:0.893]
Epoch [33/120    avg_loss:0.280, val_acc:0.939]
Epoch [34/120    avg_loss:0.255, val_acc:0.926]
Epoch [35/120    avg_loss:0.247, val_acc:0.945]
Epoch [36/120    avg_loss:0.216, val_acc:0.932]
Epoch [37/120    avg_loss:0.207, val_acc:0.910]
Epoch [38/120    avg_loss:0.275, val_acc:0.881]
Epoch [39/120    avg_loss:0.272, val_acc:0.939]
Epoch [40/120    avg_loss:0.291, val_acc:0.975]
Epoch [41/120    avg_loss:0.201, val_acc:0.965]
Epoch [42/120    avg_loss:0.156, val_acc:0.955]
Epoch [43/120    avg_loss:0.164, val_acc:0.982]
Epoch [44/120    avg_loss:0.158, val_acc:0.973]
Epoch [45/120    avg_loss:0.210, val_acc:0.945]
Epoch [46/120    avg_loss:0.187, val_acc:0.955]
Epoch [47/120    avg_loss:0.164, val_acc:0.959]
Epoch [48/120    avg_loss:0.132, val_acc:0.973]
Epoch [49/120    avg_loss:0.180, val_acc:0.965]
Epoch [50/120    avg_loss:0.132, val_acc:0.977]
Epoch [51/120    avg_loss:0.131, val_acc:0.967]
Epoch [52/120    avg_loss:0.131, val_acc:0.971]
Epoch [53/120    avg_loss:0.133, val_acc:0.977]
Epoch [54/120    avg_loss:0.128, val_acc:0.977]
Epoch [55/120    avg_loss:0.106, val_acc:0.990]
Epoch [56/120    avg_loss:0.116, val_acc:0.971]
Epoch [57/120    avg_loss:0.112, val_acc:0.955]
Epoch [58/120    avg_loss:0.168, val_acc:0.973]
Epoch [59/120    avg_loss:0.126, val_acc:0.982]
Epoch [60/120    avg_loss:0.128, val_acc:0.982]
Epoch [61/120    avg_loss:0.144, val_acc:0.973]
Epoch [62/120    avg_loss:0.104, val_acc:0.988]
Epoch [63/120    avg_loss:0.111, val_acc:0.967]
Epoch [64/120    avg_loss:0.122, val_acc:0.980]
Epoch [65/120    avg_loss:0.089, val_acc:0.990]
Epoch [66/120    avg_loss:0.081, val_acc:0.953]
Epoch [67/120    avg_loss:0.141, val_acc:0.949]
Epoch [68/120    avg_loss:0.103, val_acc:0.975]
Epoch [69/120    avg_loss:0.102, val_acc:0.990]
Epoch [70/120    avg_loss:0.071, val_acc:0.980]
Epoch [71/120    avg_loss:0.120, val_acc:0.963]
Epoch [72/120    avg_loss:0.101, val_acc:0.980]
Epoch [73/120    avg_loss:0.076, val_acc:0.975]
Epoch [74/120    avg_loss:0.065, val_acc:0.961]
Epoch [75/120    avg_loss:0.068, val_acc:0.990]
Epoch [76/120    avg_loss:0.062, val_acc:0.982]
Epoch [77/120    avg_loss:0.085, val_acc:0.980]
Epoch [78/120    avg_loss:0.057, val_acc:0.984]
Epoch [79/120    avg_loss:0.052, val_acc:0.988]
Epoch [80/120    avg_loss:0.037, val_acc:0.990]
Epoch [81/120    avg_loss:0.048, val_acc:0.992]
Epoch [82/120    avg_loss:0.039, val_acc:0.980]
Epoch [83/120    avg_loss:0.043, val_acc:0.967]
Epoch [84/120    avg_loss:0.085, val_acc:0.969]
Epoch [85/120    avg_loss:0.057, val_acc:0.992]
Epoch [86/120    avg_loss:0.047, val_acc:0.984]
Epoch [87/120    avg_loss:0.070, val_acc:0.975]
Epoch [88/120    avg_loss:0.043, val_acc:0.984]
Epoch [89/120    avg_loss:0.030, val_acc:0.992]
Epoch [90/120    avg_loss:0.051, val_acc:0.992]
Epoch [91/120    avg_loss:0.088, val_acc:0.984]
Epoch [92/120    avg_loss:0.083, val_acc:0.984]
Epoch [93/120    avg_loss:0.087, val_acc:0.971]
Epoch [94/120    avg_loss:0.037, val_acc:0.992]
Epoch [95/120    avg_loss:0.027, val_acc:0.977]
Epoch [96/120    avg_loss:0.055, val_acc:0.990]
Epoch [97/120    avg_loss:0.037, val_acc:0.988]
Epoch [98/120    avg_loss:0.035, val_acc:0.996]
Epoch [99/120    avg_loss:0.065, val_acc:0.992]
Epoch [100/120    avg_loss:0.032, val_acc:0.984]
Epoch [101/120    avg_loss:0.064, val_acc:0.984]
Epoch [102/120    avg_loss:0.045, val_acc:0.992]
Epoch [103/120    avg_loss:0.051, val_acc:0.965]
Epoch [104/120    avg_loss:0.056, val_acc:0.994]
Epoch [105/120    avg_loss:0.058, val_acc:0.994]
Epoch [106/120    avg_loss:0.033, val_acc:0.992]
Epoch [107/120    avg_loss:0.029, val_acc:0.996]
Epoch [108/120    avg_loss:0.023, val_acc:0.998]
Epoch [109/120    avg_loss:0.014, val_acc:0.996]
Epoch [110/120    avg_loss:0.031, val_acc:0.973]
Epoch [111/120    avg_loss:0.023, val_acc:0.994]
Epoch [112/120    avg_loss:0.028, val_acc:0.990]
Epoch [113/120    avg_loss:0.029, val_acc:0.994]
Epoch [114/120    avg_loss:0.048, val_acc:0.982]
Epoch [115/120    avg_loss:0.060, val_acc:0.988]
Epoch [116/120    avg_loss:0.032, val_acc:0.996]
Epoch [117/120    avg_loss:0.015, val_acc:0.996]
Epoch [118/120    avg_loss:0.025, val_acc:0.996]
Epoch [119/120    avg_loss:0.020, val_acc:0.996]
Epoch [120/120    avg_loss:0.013, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   6   1   0   0   0   0   0 381   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99417758 0.99545455 1.         0.96832579 0.95364238
 0.99516908 0.99465241 0.99089727 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9940655020777576
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f589f84fa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.538, val_acc:0.452]
Epoch [2/120    avg_loss:2.221, val_acc:0.546]
Epoch [3/120    avg_loss:1.979, val_acc:0.613]
Epoch [4/120    avg_loss:1.729, val_acc:0.667]
Epoch [5/120    avg_loss:1.519, val_acc:0.685]
Epoch [6/120    avg_loss:1.313, val_acc:0.738]
Epoch [7/120    avg_loss:1.097, val_acc:0.762]
Epoch [8/120    avg_loss:0.998, val_acc:0.821]
Epoch [9/120    avg_loss:0.833, val_acc:0.819]
Epoch [10/120    avg_loss:0.773, val_acc:0.829]
Epoch [11/120    avg_loss:0.687, val_acc:0.812]
Epoch [12/120    avg_loss:0.770, val_acc:0.792]
Epoch [13/120    avg_loss:0.634, val_acc:0.927]
Epoch [14/120    avg_loss:0.481, val_acc:0.905]
Epoch [15/120    avg_loss:0.500, val_acc:0.823]
Epoch [16/120    avg_loss:0.446, val_acc:0.859]
Epoch [17/120    avg_loss:0.435, val_acc:0.915]
Epoch [18/120    avg_loss:0.346, val_acc:0.944]
Epoch [19/120    avg_loss:0.363, val_acc:0.960]
Epoch [20/120    avg_loss:0.333, val_acc:0.907]
Epoch [21/120    avg_loss:0.283, val_acc:0.869]
Epoch [22/120    avg_loss:0.272, val_acc:0.952]
Epoch [23/120    avg_loss:0.278, val_acc:0.954]
Epoch [24/120    avg_loss:0.242, val_acc:0.944]
Epoch [25/120    avg_loss:0.233, val_acc:0.948]
Epoch [26/120    avg_loss:0.297, val_acc:0.881]
Epoch [27/120    avg_loss:0.224, val_acc:0.940]
Epoch [28/120    avg_loss:0.225, val_acc:0.980]
Epoch [29/120    avg_loss:0.166, val_acc:0.976]
Epoch [30/120    avg_loss:0.181, val_acc:0.982]
Epoch [31/120    avg_loss:0.209, val_acc:0.960]
Epoch [32/120    avg_loss:0.194, val_acc:0.962]
Epoch [33/120    avg_loss:0.186, val_acc:0.974]
Epoch [34/120    avg_loss:0.150, val_acc:0.978]
Epoch [35/120    avg_loss:0.165, val_acc:0.982]
Epoch [36/120    avg_loss:0.195, val_acc:0.986]
Epoch [37/120    avg_loss:0.156, val_acc:0.970]
Epoch [38/120    avg_loss:0.159, val_acc:0.946]
Epoch [39/120    avg_loss:0.161, val_acc:0.956]
Epoch [40/120    avg_loss:0.148, val_acc:0.988]
Epoch [41/120    avg_loss:0.113, val_acc:0.944]
Epoch [42/120    avg_loss:0.152, val_acc:0.988]
Epoch [43/120    avg_loss:0.107, val_acc:0.966]
Epoch [44/120    avg_loss:0.103, val_acc:0.986]
Epoch [45/120    avg_loss:0.117, val_acc:0.992]
Epoch [46/120    avg_loss:0.108, val_acc:0.988]
Epoch [47/120    avg_loss:0.072, val_acc:0.988]
Epoch [48/120    avg_loss:0.061, val_acc:0.992]
Epoch [49/120    avg_loss:0.104, val_acc:0.964]
Epoch [50/120    avg_loss:0.121, val_acc:0.980]
Epoch [51/120    avg_loss:0.105, val_acc:0.982]
Epoch [52/120    avg_loss:0.076, val_acc:0.992]
Epoch [53/120    avg_loss:0.067, val_acc:0.992]
Epoch [54/120    avg_loss:0.062, val_acc:0.990]
Epoch [55/120    avg_loss:0.089, val_acc:0.990]
Epoch [56/120    avg_loss:0.093, val_acc:0.986]
Epoch [57/120    avg_loss:0.076, val_acc:0.980]
Epoch [58/120    avg_loss:0.073, val_acc:0.992]
Epoch [59/120    avg_loss:0.043, val_acc:0.980]
Epoch [60/120    avg_loss:0.088, val_acc:0.958]
Epoch [61/120    avg_loss:0.095, val_acc:0.986]
Epoch [62/120    avg_loss:0.056, val_acc:0.986]
Epoch [63/120    avg_loss:0.040, val_acc:0.990]
Epoch [64/120    avg_loss:0.047, val_acc:0.984]
Epoch [65/120    avg_loss:0.053, val_acc:0.992]
Epoch [66/120    avg_loss:0.032, val_acc:0.996]
Epoch [67/120    avg_loss:0.035, val_acc:0.994]
Epoch [68/120    avg_loss:0.030, val_acc:0.994]
Epoch [69/120    avg_loss:0.024, val_acc:0.994]
Epoch [70/120    avg_loss:0.043, val_acc:0.992]
Epoch [71/120    avg_loss:0.033, val_acc:0.994]
Epoch [72/120    avg_loss:0.041, val_acc:0.998]
Epoch [73/120    avg_loss:0.025, val_acc:0.996]
Epoch [74/120    avg_loss:0.017, val_acc:0.992]
Epoch [75/120    avg_loss:0.022, val_acc:0.990]
Epoch [76/120    avg_loss:0.013, val_acc:0.994]
Epoch [77/120    avg_loss:0.015, val_acc:0.992]
Epoch [78/120    avg_loss:0.016, val_acc:0.994]
Epoch [79/120    avg_loss:0.018, val_acc:0.992]
Epoch [80/120    avg_loss:0.014, val_acc:0.994]
Epoch [81/120    avg_loss:0.014, val_acc:0.992]
Epoch [82/120    avg_loss:0.015, val_acc:0.988]
Epoch [83/120    avg_loss:0.016, val_acc:0.994]
Epoch [84/120    avg_loss:0.015, val_acc:0.992]
Epoch [85/120    avg_loss:0.015, val_acc:0.996]
Epoch [86/120    avg_loss:0.015, val_acc:0.996]
Epoch [87/120    avg_loss:0.014, val_acc:0.994]
Epoch [88/120    avg_loss:0.011, val_acc:0.994]
Epoch [89/120    avg_loss:0.011, val_acc:0.994]
Epoch [90/120    avg_loss:0.011, val_acc:0.994]
Epoch [91/120    avg_loss:0.009, val_acc:0.994]
Epoch [92/120    avg_loss:0.009, val_acc:0.994]
Epoch [93/120    avg_loss:0.009, val_acc:0.994]
Epoch [94/120    avg_loss:0.032, val_acc:0.994]
Epoch [95/120    avg_loss:0.044, val_acc:0.994]
Epoch [96/120    avg_loss:0.019, val_acc:0.994]
Epoch [97/120    avg_loss:0.011, val_acc:0.994]
Epoch [98/120    avg_loss:0.010, val_acc:0.994]
Epoch [99/120    avg_loss:0.009, val_acc:0.994]
Epoch [100/120    avg_loss:0.010, val_acc:0.994]
Epoch [101/120    avg_loss:0.009, val_acc:0.994]
Epoch [102/120    avg_loss:0.012, val_acc:0.994]
Epoch [103/120    avg_loss:0.009, val_acc:0.994]
Epoch [104/120    avg_loss:0.009, val_acc:0.994]
Epoch [105/120    avg_loss:0.011, val_acc:0.994]
Epoch [106/120    avg_loss:0.013, val_acc:0.994]
Epoch [107/120    avg_loss:0.010, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.011, val_acc:0.994]
Epoch [110/120    avg_loss:0.009, val_acc:0.994]
Epoch [111/120    avg_loss:0.011, val_acc:0.994]
Epoch [112/120    avg_loss:0.008, val_acc:0.994]
Epoch [113/120    avg_loss:0.008, val_acc:0.994]
Epoch [114/120    avg_loss:0.017, val_acc:0.994]
Epoch [115/120    avg_loss:0.013, val_acc:0.994]
Epoch [116/120    avg_loss:0.009, val_acc:0.994]
Epoch [117/120    avg_loss:0.009, val_acc:0.994]
Epoch [118/120    avg_loss:0.010, val_acc:0.994]
Epoch [119/120    avg_loss:0.008, val_acc:0.994]
Epoch [120/120    avg_loss:0.011, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  15   0   0   0   0   1   0   1   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99926954 0.99095023 1.         0.94382022 0.92255892
 0.99757869 0.97826087 1.         1.         0.99862826 0.9986755
 0.99779249 1.        ]

Kappa:
0.9926412739752042
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbd1cf4da58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.502, val_acc:0.498]
Epoch [2/120    avg_loss:2.252, val_acc:0.551]
Epoch [3/120    avg_loss:2.037, val_acc:0.631]
Epoch [4/120    avg_loss:1.806, val_acc:0.635]
Epoch [5/120    avg_loss:1.535, val_acc:0.662]
Epoch [6/120    avg_loss:1.310, val_acc:0.699]
Epoch [7/120    avg_loss:1.140, val_acc:0.734]
Epoch [8/120    avg_loss:0.933, val_acc:0.770]
Epoch [9/120    avg_loss:0.861, val_acc:0.730]
Epoch [10/120    avg_loss:0.727, val_acc:0.805]
Epoch [11/120    avg_loss:0.673, val_acc:0.803]
Epoch [12/120    avg_loss:0.652, val_acc:0.822]
Epoch [13/120    avg_loss:0.665, val_acc:0.871]
Epoch [14/120    avg_loss:0.554, val_acc:0.824]
Epoch [15/120    avg_loss:0.571, val_acc:0.869]
Epoch [16/120    avg_loss:0.551, val_acc:0.869]
Epoch [17/120    avg_loss:0.510, val_acc:0.881]
Epoch [18/120    avg_loss:0.406, val_acc:0.887]
Epoch [19/120    avg_loss:0.474, val_acc:0.867]
Epoch [20/120    avg_loss:0.421, val_acc:0.904]
Epoch [21/120    avg_loss:0.391, val_acc:0.914]
Epoch [22/120    avg_loss:0.350, val_acc:0.914]
Epoch [23/120    avg_loss:0.335, val_acc:0.895]
Epoch [24/120    avg_loss:0.338, val_acc:0.922]
Epoch [25/120    avg_loss:0.367, val_acc:0.912]
Epoch [26/120    avg_loss:0.341, val_acc:0.918]
Epoch [27/120    avg_loss:0.292, val_acc:0.932]
Epoch [28/120    avg_loss:0.244, val_acc:0.930]
Epoch [29/120    avg_loss:0.267, val_acc:0.924]
Epoch [30/120    avg_loss:0.266, val_acc:0.926]
Epoch [31/120    avg_loss:0.239, val_acc:0.924]
Epoch [32/120    avg_loss:0.256, val_acc:0.910]
Epoch [33/120    avg_loss:0.272, val_acc:0.949]
Epoch [34/120    avg_loss:0.189, val_acc:0.949]
Epoch [35/120    avg_loss:0.277, val_acc:0.957]
Epoch [36/120    avg_loss:0.215, val_acc:0.959]
Epoch [37/120    avg_loss:0.245, val_acc:0.924]
Epoch [38/120    avg_loss:0.257, val_acc:0.963]
Epoch [39/120    avg_loss:0.152, val_acc:0.961]
Epoch [40/120    avg_loss:0.167, val_acc:0.945]
Epoch [41/120    avg_loss:0.170, val_acc:0.945]
Epoch [42/120    avg_loss:0.167, val_acc:0.982]
Epoch [43/120    avg_loss:0.143, val_acc:0.926]
Epoch [44/120    avg_loss:0.184, val_acc:0.967]
Epoch [45/120    avg_loss:0.117, val_acc:0.977]
Epoch [46/120    avg_loss:0.155, val_acc:0.961]
Epoch [47/120    avg_loss:0.126, val_acc:0.973]
Epoch [48/120    avg_loss:0.115, val_acc:0.969]
Epoch [49/120    avg_loss:0.121, val_acc:0.947]
Epoch [50/120    avg_loss:0.134, val_acc:0.965]
Epoch [51/120    avg_loss:0.113, val_acc:0.990]
Epoch [52/120    avg_loss:0.099, val_acc:0.982]
Epoch [53/120    avg_loss:0.142, val_acc:0.961]
Epoch [54/120    avg_loss:0.167, val_acc:0.943]
Epoch [55/120    avg_loss:0.128, val_acc:0.971]
Epoch [56/120    avg_loss:0.106, val_acc:0.973]
Epoch [57/120    avg_loss:0.126, val_acc:0.961]
Epoch [58/120    avg_loss:0.109, val_acc:0.975]
Epoch [59/120    avg_loss:0.087, val_acc:0.975]
Epoch [60/120    avg_loss:0.093, val_acc:0.990]
Epoch [61/120    avg_loss:0.072, val_acc:0.982]
Epoch [62/120    avg_loss:0.118, val_acc:0.955]
Epoch [63/120    avg_loss:0.145, val_acc:0.975]
Epoch [64/120    avg_loss:0.075, val_acc:0.980]
Epoch [65/120    avg_loss:0.078, val_acc:0.963]
Epoch [66/120    avg_loss:0.093, val_acc:0.988]
Epoch [67/120    avg_loss:0.108, val_acc:0.959]
Epoch [68/120    avg_loss:0.113, val_acc:0.969]
Epoch [69/120    avg_loss:0.060, val_acc:0.957]
Epoch [70/120    avg_loss:0.072, val_acc:0.953]
Epoch [71/120    avg_loss:0.130, val_acc:0.959]
Epoch [72/120    avg_loss:0.089, val_acc:0.975]
Epoch [73/120    avg_loss:0.070, val_acc:0.971]
Epoch [74/120    avg_loss:0.053, val_acc:0.980]
Epoch [75/120    avg_loss:0.044, val_acc:0.982]
Epoch [76/120    avg_loss:0.048, val_acc:0.986]
Epoch [77/120    avg_loss:0.042, val_acc:0.984]
Epoch [78/120    avg_loss:0.056, val_acc:0.988]
Epoch [79/120    avg_loss:0.048, val_acc:0.988]
Epoch [80/120    avg_loss:0.046, val_acc:0.990]
Epoch [81/120    avg_loss:0.051, val_acc:0.992]
Epoch [82/120    avg_loss:0.036, val_acc:0.996]
Epoch [83/120    avg_loss:0.041, val_acc:0.994]
Epoch [84/120    avg_loss:0.039, val_acc:0.996]
Epoch [85/120    avg_loss:0.028, val_acc:0.996]
Epoch [86/120    avg_loss:0.041, val_acc:0.994]
Epoch [87/120    avg_loss:0.031, val_acc:0.992]
Epoch [88/120    avg_loss:0.030, val_acc:0.994]
Epoch [89/120    avg_loss:0.036, val_acc:0.996]
Epoch [90/120    avg_loss:0.032, val_acc:0.992]
Epoch [91/120    avg_loss:0.036, val_acc:0.996]
Epoch [92/120    avg_loss:0.037, val_acc:0.994]
Epoch [93/120    avg_loss:0.026, val_acc:0.996]
Epoch [94/120    avg_loss:0.026, val_acc:0.996]
Epoch [95/120    avg_loss:0.022, val_acc:0.996]
Epoch [96/120    avg_loss:0.029, val_acc:0.996]
Epoch [97/120    avg_loss:0.034, val_acc:0.996]
Epoch [98/120    avg_loss:0.028, val_acc:0.996]
Epoch [99/120    avg_loss:0.030, val_acc:0.996]
Epoch [100/120    avg_loss:0.026, val_acc:0.996]
Epoch [101/120    avg_loss:0.027, val_acc:0.996]
Epoch [102/120    avg_loss:0.025, val_acc:0.996]
Epoch [103/120    avg_loss:0.026, val_acc:0.996]
Epoch [104/120    avg_loss:0.023, val_acc:0.996]
Epoch [105/120    avg_loss:0.029, val_acc:0.996]
Epoch [106/120    avg_loss:0.026, val_acc:0.996]
Epoch [107/120    avg_loss:0.024, val_acc:0.996]
Epoch [108/120    avg_loss:0.026, val_acc:0.994]
Epoch [109/120    avg_loss:0.023, val_acc:0.992]
Epoch [110/120    avg_loss:0.029, val_acc:0.994]
Epoch [111/120    avg_loss:0.034, val_acc:0.992]
Epoch [112/120    avg_loss:0.024, val_acc:0.994]
Epoch [113/120    avg_loss:0.027, val_acc:0.992]
Epoch [114/120    avg_loss:0.026, val_acc:0.996]
Epoch [115/120    avg_loss:0.021, val_acc:0.996]
Epoch [116/120    avg_loss:0.021, val_acc:0.996]
Epoch [117/120    avg_loss:0.021, val_acc:0.996]
Epoch [118/120    avg_loss:0.025, val_acc:0.996]
Epoch [119/120    avg_loss:0.027, val_acc:0.996]
Epoch [120/120    avg_loss:0.028, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.99545455 1.         0.94954128 0.92857143
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9943031985900113
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d71570a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.451, val_acc:0.432]
Epoch [2/120    avg_loss:2.189, val_acc:0.582]
Epoch [3/120    avg_loss:2.010, val_acc:0.637]
Epoch [4/120    avg_loss:1.806, val_acc:0.635]
Epoch [5/120    avg_loss:1.540, val_acc:0.682]
Epoch [6/120    avg_loss:1.309, val_acc:0.707]
Epoch [7/120    avg_loss:1.138, val_acc:0.787]
Epoch [8/120    avg_loss:0.954, val_acc:0.764]
Epoch [9/120    avg_loss:0.848, val_acc:0.775]
Epoch [10/120    avg_loss:0.748, val_acc:0.820]
Epoch [11/120    avg_loss:0.739, val_acc:0.779]
Epoch [12/120    avg_loss:0.714, val_acc:0.859]
Epoch [13/120    avg_loss:0.609, val_acc:0.850]
Epoch [14/120    avg_loss:0.528, val_acc:0.895]
Epoch [15/120    avg_loss:0.491, val_acc:0.865]
Epoch [16/120    avg_loss:0.509, val_acc:0.916]
Epoch [17/120    avg_loss:0.458, val_acc:0.924]
Epoch [18/120    avg_loss:0.406, val_acc:0.924]
Epoch [19/120    avg_loss:0.396, val_acc:0.908]
Epoch [20/120    avg_loss:0.413, val_acc:0.910]
Epoch [21/120    avg_loss:0.389, val_acc:0.922]
Epoch [22/120    avg_loss:0.350, val_acc:0.883]
Epoch [23/120    avg_loss:0.408, val_acc:0.943]
Epoch [24/120    avg_loss:0.276, val_acc:0.945]
Epoch [25/120    avg_loss:0.239, val_acc:0.930]
Epoch [26/120    avg_loss:0.247, val_acc:0.945]
Epoch [27/120    avg_loss:0.251, val_acc:0.965]
Epoch [28/120    avg_loss:0.242, val_acc:0.945]
Epoch [29/120    avg_loss:0.213, val_acc:0.955]
Epoch [30/120    avg_loss:0.209, val_acc:0.932]
Epoch [31/120    avg_loss:0.249, val_acc:0.945]
Epoch [32/120    avg_loss:0.222, val_acc:0.947]
Epoch [33/120    avg_loss:0.175, val_acc:0.965]
Epoch [34/120    avg_loss:0.142, val_acc:0.951]
Epoch [35/120    avg_loss:0.156, val_acc:0.947]
Epoch [36/120    avg_loss:0.198, val_acc:0.926]
Epoch [37/120    avg_loss:0.244, val_acc:0.953]
Epoch [38/120    avg_loss:0.203, val_acc:0.977]
Epoch [39/120    avg_loss:0.162, val_acc:0.977]
Epoch [40/120    avg_loss:0.144, val_acc:0.980]
Epoch [41/120    avg_loss:0.161, val_acc:0.949]
Epoch [42/120    avg_loss:0.183, val_acc:0.986]
Epoch [43/120    avg_loss:0.117, val_acc:0.973]
Epoch [44/120    avg_loss:0.157, val_acc:0.975]
Epoch [45/120    avg_loss:0.166, val_acc:0.975]
Epoch [46/120    avg_loss:0.110, val_acc:0.977]
Epoch [47/120    avg_loss:0.121, val_acc:0.963]
Epoch [48/120    avg_loss:0.102, val_acc:0.969]
Epoch [49/120    avg_loss:0.102, val_acc:0.980]
Epoch [50/120    avg_loss:0.111, val_acc:0.977]
Epoch [51/120    avg_loss:0.140, val_acc:0.943]
Epoch [52/120    avg_loss:0.116, val_acc:0.990]
Epoch [53/120    avg_loss:0.121, val_acc:0.973]
Epoch [54/120    avg_loss:0.098, val_acc:0.963]
Epoch [55/120    avg_loss:0.100, val_acc:0.977]
Epoch [56/120    avg_loss:0.163, val_acc:0.971]
Epoch [57/120    avg_loss:0.143, val_acc:0.980]
Epoch [58/120    avg_loss:0.092, val_acc:0.969]
Epoch [59/120    avg_loss:0.086, val_acc:0.990]
Epoch [60/120    avg_loss:0.087, val_acc:0.986]
Epoch [61/120    avg_loss:0.054, val_acc:0.990]
Epoch [62/120    avg_loss:0.076, val_acc:0.982]
Epoch [63/120    avg_loss:0.051, val_acc:0.980]
Epoch [64/120    avg_loss:0.076, val_acc:0.984]
Epoch [65/120    avg_loss:0.059, val_acc:0.955]
Epoch [66/120    avg_loss:0.050, val_acc:0.990]
Epoch [67/120    avg_loss:0.044, val_acc:0.988]
Epoch [68/120    avg_loss:0.044, val_acc:0.988]
Epoch [69/120    avg_loss:0.042, val_acc:0.990]
Epoch [70/120    avg_loss:0.060, val_acc:0.992]
Epoch [71/120    avg_loss:0.059, val_acc:0.986]
Epoch [72/120    avg_loss:0.045, val_acc:0.984]
Epoch [73/120    avg_loss:0.035, val_acc:0.992]
Epoch [74/120    avg_loss:0.044, val_acc:0.990]
Epoch [75/120    avg_loss:0.067, val_acc:0.988]
Epoch [76/120    avg_loss:0.059, val_acc:0.984]
Epoch [77/120    avg_loss:0.067, val_acc:0.967]
Epoch [78/120    avg_loss:0.158, val_acc:0.961]
Epoch [79/120    avg_loss:0.107, val_acc:0.988]
Epoch [80/120    avg_loss:0.040, val_acc:0.988]
Epoch [81/120    avg_loss:0.026, val_acc:0.988]
Epoch [82/120    avg_loss:0.027, val_acc:0.988]
Epoch [83/120    avg_loss:0.024, val_acc:0.992]
Epoch [84/120    avg_loss:0.023, val_acc:0.996]
Epoch [85/120    avg_loss:0.023, val_acc:0.990]
Epoch [86/120    avg_loss:0.025, val_acc:0.992]
Epoch [87/120    avg_loss:0.016, val_acc:0.992]
Epoch [88/120    avg_loss:0.018, val_acc:0.990]
Epoch [89/120    avg_loss:0.015, val_acc:0.994]
Epoch [90/120    avg_loss:0.019, val_acc:0.996]
Epoch [91/120    avg_loss:0.015, val_acc:0.992]
Epoch [92/120    avg_loss:0.019, val_acc:0.986]
Epoch [93/120    avg_loss:0.043, val_acc:0.996]
Epoch [94/120    avg_loss:0.026, val_acc:0.994]
Epoch [95/120    avg_loss:0.020, val_acc:0.990]
Epoch [96/120    avg_loss:0.028, val_acc:0.994]
Epoch [97/120    avg_loss:0.016, val_acc:0.990]
Epoch [98/120    avg_loss:0.029, val_acc:0.994]
Epoch [99/120    avg_loss:0.039, val_acc:0.973]
Epoch [100/120    avg_loss:0.032, val_acc:0.992]
Epoch [101/120    avg_loss:0.022, val_acc:0.992]
Epoch [102/120    avg_loss:0.017, val_acc:0.994]
Epoch [103/120    avg_loss:0.013, val_acc:0.994]
Epoch [104/120    avg_loss:0.012, val_acc:0.996]
Epoch [105/120    avg_loss:0.014, val_acc:0.994]
Epoch [106/120    avg_loss:0.011, val_acc:0.994]
Epoch [107/120    avg_loss:0.014, val_acc:0.992]
Epoch [108/120    avg_loss:0.010, val_acc:0.998]
Epoch [109/120    avg_loss:0.014, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.011, val_acc:0.996]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.015, val_acc:0.992]
Epoch [114/120    avg_loss:0.017, val_acc:0.994]
Epoch [115/120    avg_loss:0.010, val_acc:0.994]
Epoch [116/120    avg_loss:0.009, val_acc:0.994]
Epoch [117/120    avg_loss:0.009, val_acc:0.996]
Epoch [118/120    avg_loss:0.008, val_acc:0.996]
Epoch [119/120    avg_loss:0.008, val_acc:0.996]
Epoch [120/120    avg_loss:0.008, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.68017057569296

F1 scores:
[       nan 0.99926954 1.         1.         0.96818182 0.95709571
 0.99757869 1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9964395158774058
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99d9241a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.492, val_acc:0.436]
Epoch [2/120    avg_loss:2.202, val_acc:0.568]
Epoch [3/120    avg_loss:1.979, val_acc:0.598]
Epoch [4/120    avg_loss:1.756, val_acc:0.668]
Epoch [5/120    avg_loss:1.518, val_acc:0.668]
Epoch [6/120    avg_loss:1.301, val_acc:0.686]
Epoch [7/120    avg_loss:1.108, val_acc:0.740]
Epoch [8/120    avg_loss:0.952, val_acc:0.773]
Epoch [9/120    avg_loss:0.855, val_acc:0.820]
Epoch [10/120    avg_loss:0.684, val_acc:0.883]
Epoch [11/120    avg_loss:0.657, val_acc:0.834]
Epoch [12/120    avg_loss:0.619, val_acc:0.848]
Epoch [13/120    avg_loss:0.538, val_acc:0.910]
Epoch [14/120    avg_loss:0.484, val_acc:0.873]
Epoch [15/120    avg_loss:0.552, val_acc:0.885]
Epoch [16/120    avg_loss:0.496, val_acc:0.902]
Epoch [17/120    avg_loss:0.460, val_acc:0.885]
Epoch [18/120    avg_loss:0.438, val_acc:0.906]
Epoch [19/120    avg_loss:0.395, val_acc:0.910]
Epoch [20/120    avg_loss:0.356, val_acc:0.873]
Epoch [21/120    avg_loss:0.393, val_acc:0.850]
Epoch [22/120    avg_loss:0.392, val_acc:0.928]
Epoch [23/120    avg_loss:0.357, val_acc:0.930]
Epoch [24/120    avg_loss:0.312, val_acc:0.932]
Epoch [25/120    avg_loss:0.325, val_acc:0.943]
Epoch [26/120    avg_loss:0.285, val_acc:0.943]
Epoch [27/120    avg_loss:0.274, val_acc:0.887]
Epoch [28/120    avg_loss:0.244, val_acc:0.947]
Epoch [29/120    avg_loss:0.302, val_acc:0.951]
Epoch [30/120    avg_loss:0.225, val_acc:0.934]
Epoch [31/120    avg_loss:0.276, val_acc:0.963]
Epoch [32/120    avg_loss:0.252, val_acc:0.949]
Epoch [33/120    avg_loss:0.182, val_acc:0.957]
Epoch [34/120    avg_loss:0.183, val_acc:0.943]
Epoch [35/120    avg_loss:0.235, val_acc:0.945]
Epoch [36/120    avg_loss:0.176, val_acc:0.955]
Epoch [37/120    avg_loss:0.146, val_acc:0.965]
Epoch [38/120    avg_loss:0.142, val_acc:0.955]
Epoch [39/120    avg_loss:0.204, val_acc:0.953]
Epoch [40/120    avg_loss:0.207, val_acc:0.959]
Epoch [41/120    avg_loss:0.160, val_acc:0.957]
Epoch [42/120    avg_loss:0.169, val_acc:0.963]
Epoch [43/120    avg_loss:0.125, val_acc:0.973]
Epoch [44/120    avg_loss:0.124, val_acc:0.961]
Epoch [45/120    avg_loss:0.134, val_acc:0.967]
Epoch [46/120    avg_loss:0.134, val_acc:0.965]
Epoch [47/120    avg_loss:0.119, val_acc:0.971]
Epoch [48/120    avg_loss:0.115, val_acc:0.980]
Epoch [49/120    avg_loss:0.114, val_acc:0.982]
Epoch [50/120    avg_loss:0.089, val_acc:0.973]
Epoch [51/120    avg_loss:0.100, val_acc:0.975]
Epoch [52/120    avg_loss:0.117, val_acc:0.953]
Epoch [53/120    avg_loss:0.094, val_acc:0.977]
Epoch [54/120    avg_loss:0.118, val_acc:0.982]
Epoch [55/120    avg_loss:0.084, val_acc:0.971]
Epoch [56/120    avg_loss:0.131, val_acc:0.969]
Epoch [57/120    avg_loss:0.123, val_acc:0.977]
Epoch [58/120    avg_loss:0.106, val_acc:0.949]
Epoch [59/120    avg_loss:0.148, val_acc:0.980]
Epoch [60/120    avg_loss:0.066, val_acc:0.986]
Epoch [61/120    avg_loss:0.045, val_acc:0.984]
Epoch [62/120    avg_loss:0.132, val_acc:0.873]
Epoch [63/120    avg_loss:0.131, val_acc:0.975]
Epoch [64/120    avg_loss:0.086, val_acc:0.980]
Epoch [65/120    avg_loss:0.054, val_acc:0.984]
Epoch [66/120    avg_loss:0.079, val_acc:0.990]
Epoch [67/120    avg_loss:0.078, val_acc:0.984]
Epoch [68/120    avg_loss:0.047, val_acc:0.990]
Epoch [69/120    avg_loss:0.050, val_acc:0.988]
Epoch [70/120    avg_loss:0.035, val_acc:0.992]
Epoch [71/120    avg_loss:0.025, val_acc:0.996]
Epoch [72/120    avg_loss:0.028, val_acc:0.996]
Epoch [73/120    avg_loss:0.024, val_acc:0.992]
Epoch [74/120    avg_loss:0.029, val_acc:0.996]
Epoch [75/120    avg_loss:0.021, val_acc:0.994]
Epoch [76/120    avg_loss:0.022, val_acc:0.994]
Epoch [77/120    avg_loss:0.024, val_acc:0.994]
Epoch [78/120    avg_loss:0.017, val_acc:0.992]
Epoch [79/120    avg_loss:0.024, val_acc:0.992]
Epoch [80/120    avg_loss:0.035, val_acc:0.994]
Epoch [81/120    avg_loss:0.046, val_acc:0.988]
Epoch [82/120    avg_loss:0.056, val_acc:0.984]
Epoch [83/120    avg_loss:0.054, val_acc:0.988]
Epoch [84/120    avg_loss:0.048, val_acc:0.988]
Epoch [85/120    avg_loss:0.021, val_acc:0.996]
Epoch [86/120    avg_loss:0.017, val_acc:0.994]
Epoch [87/120    avg_loss:0.023, val_acc:0.992]
Epoch [88/120    avg_loss:0.043, val_acc:0.980]
Epoch [89/120    avg_loss:0.053, val_acc:0.996]
Epoch [90/120    avg_loss:0.030, val_acc:0.996]
Epoch [91/120    avg_loss:0.024, val_acc:0.996]
Epoch [92/120    avg_loss:0.053, val_acc:0.988]
Epoch [93/120    avg_loss:0.048, val_acc:0.988]
Epoch [94/120    avg_loss:0.019, val_acc:0.992]
Epoch [95/120    avg_loss:0.027, val_acc:0.990]
Epoch [96/120    avg_loss:0.029, val_acc:0.992]
Epoch [97/120    avg_loss:0.021, val_acc:0.990]
Epoch [98/120    avg_loss:0.017, val_acc:0.992]
Epoch [99/120    avg_loss:0.016, val_acc:0.996]
Epoch [100/120    avg_loss:0.012, val_acc:0.994]
Epoch [101/120    avg_loss:0.015, val_acc:0.994]
Epoch [102/120    avg_loss:0.019, val_acc:0.990]
Epoch [103/120    avg_loss:0.037, val_acc:0.994]
Epoch [104/120    avg_loss:0.017, val_acc:0.996]
Epoch [105/120    avg_loss:0.018, val_acc:0.998]
Epoch [106/120    avg_loss:0.027, val_acc:0.984]
Epoch [107/120    avg_loss:0.027, val_acc:0.994]
Epoch [108/120    avg_loss:0.029, val_acc:0.992]
Epoch [109/120    avg_loss:0.014, val_acc:0.992]
Epoch [110/120    avg_loss:0.051, val_acc:0.988]
Epoch [111/120    avg_loss:0.053, val_acc:0.973]
Epoch [112/120    avg_loss:0.070, val_acc:0.984]
Epoch [113/120    avg_loss:0.021, val_acc:0.994]
Epoch [114/120    avg_loss:0.014, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.031, val_acc:0.984]
Epoch [118/120    avg_loss:0.027, val_acc:0.988]
Epoch [119/120    avg_loss:0.016, val_acc:0.996]
Epoch [120/120    avg_loss:0.011, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99780541 0.99319728 1.         0.96396396 0.94666667
 0.99277108 0.98378378 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9945407255698102
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f02b1a5aac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.534, val_acc:0.441]
Epoch [2/120    avg_loss:2.275, val_acc:0.523]
Epoch [3/120    avg_loss:2.056, val_acc:0.592]
Epoch [4/120    avg_loss:1.859, val_acc:0.650]
Epoch [5/120    avg_loss:1.663, val_acc:0.658]
Epoch [6/120    avg_loss:1.402, val_acc:0.709]
Epoch [7/120    avg_loss:1.198, val_acc:0.742]
Epoch [8/120    avg_loss:1.006, val_acc:0.773]
Epoch [9/120    avg_loss:0.921, val_acc:0.770]
Epoch [10/120    avg_loss:0.796, val_acc:0.805]
Epoch [11/120    avg_loss:0.686, val_acc:0.811]
Epoch [12/120    avg_loss:0.649, val_acc:0.846]
Epoch [13/120    avg_loss:0.611, val_acc:0.840]
Epoch [14/120    avg_loss:0.535, val_acc:0.855]
Epoch [15/120    avg_loss:0.640, val_acc:0.865]
Epoch [16/120    avg_loss:0.476, val_acc:0.887]
Epoch [17/120    avg_loss:0.462, val_acc:0.920]
Epoch [18/120    avg_loss:0.406, val_acc:0.910]
Epoch [19/120    avg_loss:0.415, val_acc:0.924]
Epoch [20/120    avg_loss:0.402, val_acc:0.955]
Epoch [21/120    avg_loss:0.346, val_acc:0.945]
Epoch [22/120    avg_loss:0.290, val_acc:0.930]
Epoch [23/120    avg_loss:0.300, val_acc:0.943]
Epoch [24/120    avg_loss:0.258, val_acc:0.947]
Epoch [25/120    avg_loss:0.242, val_acc:0.943]
Epoch [26/120    avg_loss:0.206, val_acc:0.906]
Epoch [27/120    avg_loss:0.243, val_acc:0.955]
Epoch [28/120    avg_loss:0.248, val_acc:0.949]
Epoch [29/120    avg_loss:0.206, val_acc:0.951]
Epoch [30/120    avg_loss:0.166, val_acc:0.955]
Epoch [31/120    avg_loss:0.243, val_acc:0.932]
Epoch [32/120    avg_loss:0.248, val_acc:0.963]
Epoch [33/120    avg_loss:0.167, val_acc:0.957]
Epoch [34/120    avg_loss:0.150, val_acc:0.969]
Epoch [35/120    avg_loss:0.164, val_acc:0.980]
Epoch [36/120    avg_loss:0.184, val_acc:0.969]
Epoch [37/120    avg_loss:0.151, val_acc:0.982]
Epoch [38/120    avg_loss:0.200, val_acc:0.965]
Epoch [39/120    avg_loss:0.150, val_acc:0.951]
Epoch [40/120    avg_loss:0.122, val_acc:0.988]
Epoch [41/120    avg_loss:0.153, val_acc:0.963]
Epoch [42/120    avg_loss:0.128, val_acc:0.990]
Epoch [43/120    avg_loss:0.079, val_acc:0.994]
Epoch [44/120    avg_loss:0.064, val_acc:0.977]
Epoch [45/120    avg_loss:0.071, val_acc:0.988]
Epoch [46/120    avg_loss:0.084, val_acc:0.986]
Epoch [47/120    avg_loss:0.108, val_acc:0.984]
Epoch [48/120    avg_loss:0.099, val_acc:0.986]
Epoch [49/120    avg_loss:0.085, val_acc:0.994]
Epoch [50/120    avg_loss:0.059, val_acc:0.986]
Epoch [51/120    avg_loss:0.104, val_acc:0.990]
Epoch [52/120    avg_loss:0.071, val_acc:0.977]
Epoch [53/120    avg_loss:0.058, val_acc:0.977]
Epoch [54/120    avg_loss:0.051, val_acc:0.986]
Epoch [55/120    avg_loss:0.071, val_acc:0.982]
Epoch [56/120    avg_loss:0.064, val_acc:0.988]
Epoch [57/120    avg_loss:0.078, val_acc:0.996]
Epoch [58/120    avg_loss:0.052, val_acc:0.992]
Epoch [59/120    avg_loss:0.122, val_acc:0.969]
Epoch [60/120    avg_loss:0.071, val_acc:0.996]
Epoch [61/120    avg_loss:0.048, val_acc:0.992]
Epoch [62/120    avg_loss:0.044, val_acc:0.992]
Epoch [63/120    avg_loss:0.033, val_acc:0.990]
Epoch [64/120    avg_loss:0.071, val_acc:0.996]
Epoch [65/120    avg_loss:0.053, val_acc:0.984]
Epoch [66/120    avg_loss:0.041, val_acc:0.992]
Epoch [67/120    avg_loss:0.045, val_acc:0.992]
Epoch [68/120    avg_loss:0.025, val_acc:0.998]
Epoch [69/120    avg_loss:0.026, val_acc:0.996]
Epoch [70/120    avg_loss:0.043, val_acc:0.988]
Epoch [71/120    avg_loss:0.065, val_acc:0.973]
Epoch [72/120    avg_loss:0.029, val_acc:0.992]
Epoch [73/120    avg_loss:0.025, val_acc:0.992]
Epoch [74/120    avg_loss:0.053, val_acc:0.988]
Epoch [75/120    avg_loss:0.027, val_acc:0.992]
Epoch [76/120    avg_loss:0.024, val_acc:0.998]
Epoch [77/120    avg_loss:0.018, val_acc:0.996]
Epoch [78/120    avg_loss:0.010, val_acc:0.994]
Epoch [79/120    avg_loss:0.013, val_acc:0.996]
Epoch [80/120    avg_loss:0.018, val_acc:0.990]
Epoch [81/120    avg_loss:0.014, val_acc:0.994]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.020, val_acc:0.996]
Epoch [84/120    avg_loss:0.011, val_acc:0.994]
Epoch [85/120    avg_loss:0.032, val_acc:0.986]
Epoch [86/120    avg_loss:0.030, val_acc:0.986]
Epoch [87/120    avg_loss:0.037, val_acc:0.990]
Epoch [88/120    avg_loss:0.028, val_acc:0.996]
Epoch [89/120    avg_loss:0.021, val_acc:0.994]
Epoch [90/120    avg_loss:0.018, val_acc:0.998]
Epoch [91/120    avg_loss:0.019, val_acc:0.998]
Epoch [92/120    avg_loss:0.010, val_acc:0.998]
Epoch [93/120    avg_loss:0.010, val_acc:0.998]
Epoch [94/120    avg_loss:0.014, val_acc:0.998]
Epoch [95/120    avg_loss:0.012, val_acc:0.996]
Epoch [96/120    avg_loss:0.008, val_acc:0.998]
Epoch [97/120    avg_loss:0.009, val_acc:0.996]
Epoch [98/120    avg_loss:0.012, val_acc:0.996]
Epoch [99/120    avg_loss:0.009, val_acc:0.996]
Epoch [100/120    avg_loss:0.008, val_acc:0.996]
Epoch [101/120    avg_loss:0.010, val_acc:0.998]
Epoch [102/120    avg_loss:0.008, val_acc:0.998]
Epoch [103/120    avg_loss:0.012, val_acc:0.998]
Epoch [104/120    avg_loss:0.010, val_acc:0.998]
Epoch [105/120    avg_loss:0.009, val_acc:0.998]
Epoch [106/120    avg_loss:0.008, val_acc:0.998]
Epoch [107/120    avg_loss:0.009, val_acc:0.998]
Epoch [108/120    avg_loss:0.008, val_acc:0.996]
Epoch [109/120    avg_loss:0.011, val_acc:0.996]
Epoch [110/120    avg_loss:0.014, val_acc:0.996]
Epoch [111/120    avg_loss:0.011, val_acc:0.996]
Epoch [112/120    avg_loss:0.010, val_acc:0.994]
Epoch [113/120    avg_loss:0.009, val_acc:0.996]
Epoch [114/120    avg_loss:0.009, val_acc:0.994]
Epoch [115/120    avg_loss:0.010, val_acc:0.994]
Epoch [116/120    avg_loss:0.010, val_acc:0.994]
Epoch [117/120    avg_loss:0.009, val_acc:0.996]
Epoch [118/120    avg_loss:0.010, val_acc:0.996]
Epoch [119/120    avg_loss:0.010, val_acc:0.996]
Epoch [120/120    avg_loss:0.009, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99707174 0.99545455 1.         0.94945055 0.92041522
 0.99038462 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9931164896840469
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff76c7b0ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.472, val_acc:0.389]
Epoch [2/120    avg_loss:2.206, val_acc:0.466]
Epoch [3/120    avg_loss:2.003, val_acc:0.573]
Epoch [4/120    avg_loss:1.785, val_acc:0.643]
Epoch [5/120    avg_loss:1.594, val_acc:0.716]
Epoch [6/120    avg_loss:1.345, val_acc:0.722]
Epoch [7/120    avg_loss:1.227, val_acc:0.728]
Epoch [8/120    avg_loss:1.015, val_acc:0.851]
Epoch [9/120    avg_loss:0.917, val_acc:0.804]
Epoch [10/120    avg_loss:0.845, val_acc:0.879]
Epoch [11/120    avg_loss:0.698, val_acc:0.861]
Epoch [12/120    avg_loss:0.607, val_acc:0.883]
Epoch [13/120    avg_loss:0.628, val_acc:0.865]
Epoch [14/120    avg_loss:0.560, val_acc:0.889]
Epoch [15/120    avg_loss:0.544, val_acc:0.897]
Epoch [16/120    avg_loss:0.502, val_acc:0.861]
Epoch [17/120    avg_loss:0.503, val_acc:0.879]
Epoch [18/120    avg_loss:0.470, val_acc:0.871]
Epoch [19/120    avg_loss:0.460, val_acc:0.911]
Epoch [20/120    avg_loss:0.374, val_acc:0.893]
Epoch [21/120    avg_loss:0.328, val_acc:0.921]
Epoch [22/120    avg_loss:0.389, val_acc:0.944]
Epoch [23/120    avg_loss:0.384, val_acc:0.917]
Epoch [24/120    avg_loss:0.381, val_acc:0.927]
Epoch [25/120    avg_loss:0.297, val_acc:0.933]
Epoch [26/120    avg_loss:0.265, val_acc:0.935]
Epoch [27/120    avg_loss:0.301, val_acc:0.966]
Epoch [28/120    avg_loss:0.304, val_acc:0.952]
Epoch [29/120    avg_loss:0.271, val_acc:0.946]
Epoch [30/120    avg_loss:0.217, val_acc:0.948]
Epoch [31/120    avg_loss:0.216, val_acc:0.946]
Epoch [32/120    avg_loss:0.260, val_acc:0.865]
Epoch [33/120    avg_loss:0.269, val_acc:0.938]
Epoch [34/120    avg_loss:0.192, val_acc:0.958]
Epoch [35/120    avg_loss:0.223, val_acc:0.942]
Epoch [36/120    avg_loss:0.183, val_acc:0.966]
Epoch [37/120    avg_loss:0.225, val_acc:0.958]
Epoch [38/120    avg_loss:0.200, val_acc:0.978]
Epoch [39/120    avg_loss:0.171, val_acc:0.950]
Epoch [40/120    avg_loss:0.173, val_acc:0.964]
Epoch [41/120    avg_loss:0.145, val_acc:0.964]
Epoch [42/120    avg_loss:0.140, val_acc:0.982]
Epoch [43/120    avg_loss:0.158, val_acc:0.956]
Epoch [44/120    avg_loss:0.139, val_acc:0.966]
Epoch [45/120    avg_loss:0.167, val_acc:0.972]
Epoch [46/120    avg_loss:0.169, val_acc:0.956]
Epoch [47/120    avg_loss:0.148, val_acc:0.984]
Epoch [48/120    avg_loss:0.124, val_acc:0.954]
Epoch [49/120    avg_loss:0.110, val_acc:0.970]
Epoch [50/120    avg_loss:0.092, val_acc:0.982]
Epoch [51/120    avg_loss:0.091, val_acc:0.976]
Epoch [52/120    avg_loss:0.134, val_acc:0.968]
Epoch [53/120    avg_loss:0.111, val_acc:0.984]
Epoch [54/120    avg_loss:0.096, val_acc:0.970]
Epoch [55/120    avg_loss:0.117, val_acc:0.966]
Epoch [56/120    avg_loss:0.075, val_acc:0.984]
Epoch [57/120    avg_loss:0.094, val_acc:0.976]
Epoch [58/120    avg_loss:0.107, val_acc:0.970]
Epoch [59/120    avg_loss:0.095, val_acc:0.972]
Epoch [60/120    avg_loss:0.073, val_acc:0.988]
Epoch [61/120    avg_loss:0.041, val_acc:0.986]
Epoch [62/120    avg_loss:0.049, val_acc:0.982]
Epoch [63/120    avg_loss:0.119, val_acc:0.946]
Epoch [64/120    avg_loss:0.082, val_acc:0.972]
Epoch [65/120    avg_loss:0.068, val_acc:0.974]
Epoch [66/120    avg_loss:0.053, val_acc:0.982]
Epoch [67/120    avg_loss:0.065, val_acc:0.986]
Epoch [68/120    avg_loss:0.079, val_acc:0.978]
Epoch [69/120    avg_loss:0.113, val_acc:0.968]
Epoch [70/120    avg_loss:0.072, val_acc:0.980]
Epoch [71/120    avg_loss:0.052, val_acc:0.984]
Epoch [72/120    avg_loss:0.057, val_acc:0.990]
Epoch [73/120    avg_loss:0.061, val_acc:0.984]
Epoch [74/120    avg_loss:0.037, val_acc:0.984]
Epoch [75/120    avg_loss:0.030, val_acc:0.990]
Epoch [76/120    avg_loss:0.031, val_acc:0.990]
Epoch [77/120    avg_loss:0.032, val_acc:0.986]
Epoch [78/120    avg_loss:0.053, val_acc:0.980]
Epoch [79/120    avg_loss:0.032, val_acc:0.982]
Epoch [80/120    avg_loss:0.065, val_acc:0.990]
Epoch [81/120    avg_loss:0.025, val_acc:0.984]
Epoch [82/120    avg_loss:0.025, val_acc:0.988]
Epoch [83/120    avg_loss:0.025, val_acc:0.988]
Epoch [84/120    avg_loss:0.023, val_acc:0.988]
Epoch [85/120    avg_loss:0.015, val_acc:0.990]
Epoch [86/120    avg_loss:0.019, val_acc:0.986]
Epoch [87/120    avg_loss:0.019, val_acc:0.990]
Epoch [88/120    avg_loss:0.028, val_acc:0.984]
Epoch [89/120    avg_loss:0.020, val_acc:0.990]
Epoch [90/120    avg_loss:0.035, val_acc:0.966]
Epoch [91/120    avg_loss:0.055, val_acc:0.988]
Epoch [92/120    avg_loss:0.079, val_acc:0.952]
Epoch [93/120    avg_loss:0.051, val_acc:0.986]
Epoch [94/120    avg_loss:0.030, val_acc:0.990]
Epoch [95/120    avg_loss:0.030, val_acc:0.990]
Epoch [96/120    avg_loss:0.035, val_acc:0.982]
Epoch [97/120    avg_loss:0.073, val_acc:0.976]
Epoch [98/120    avg_loss:0.022, val_acc:0.992]
Epoch [99/120    avg_loss:0.038, val_acc:0.988]
Epoch [100/120    avg_loss:0.041, val_acc:0.986]
Epoch [101/120    avg_loss:0.020, val_acc:0.992]
Epoch [102/120    avg_loss:0.018, val_acc:0.990]
Epoch [103/120    avg_loss:0.013, val_acc:0.990]
Epoch [104/120    avg_loss:0.012, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.990]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.012, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.054, val_acc:0.988]
Epoch [110/120    avg_loss:0.050, val_acc:0.962]
Epoch [111/120    avg_loss:0.043, val_acc:0.978]
Epoch [112/120    avg_loss:0.051, val_acc:0.978]
Epoch [113/120    avg_loss:0.032, val_acc:0.984]
Epoch [114/120    avg_loss:0.020, val_acc:0.982]
Epoch [115/120    avg_loss:0.019, val_acc:0.986]
Epoch [116/120    avg_loss:0.035, val_acc:0.972]
Epoch [117/120    avg_loss:0.046, val_acc:0.990]
Epoch [118/120    avg_loss:0.016, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   7   0   0   0   0   0   0   2   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 0.99560117 0.99319728 0.99782135 0.96035242 0.94444444
 0.98564593 0.98924731 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9935913556228783
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48952cbb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.518, val_acc:0.409]
Epoch [2/120    avg_loss:2.272, val_acc:0.526]
Epoch [3/120    avg_loss:2.069, val_acc:0.500]
Epoch [4/120    avg_loss:1.866, val_acc:0.534]
Epoch [5/120    avg_loss:1.670, val_acc:0.619]
Epoch [6/120    avg_loss:1.432, val_acc:0.629]
Epoch [7/120    avg_loss:1.294, val_acc:0.679]
Epoch [8/120    avg_loss:1.087, val_acc:0.742]
Epoch [9/120    avg_loss:0.960, val_acc:0.778]
Epoch [10/120    avg_loss:0.909, val_acc:0.796]
Epoch [11/120    avg_loss:0.772, val_acc:0.762]
Epoch [12/120    avg_loss:0.751, val_acc:0.871]
Epoch [13/120    avg_loss:0.643, val_acc:0.863]
Epoch [14/120    avg_loss:0.625, val_acc:0.891]
Epoch [15/120    avg_loss:0.557, val_acc:0.732]
Epoch [16/120    avg_loss:0.572, val_acc:0.905]
Epoch [17/120    avg_loss:0.518, val_acc:0.925]
Epoch [18/120    avg_loss:0.449, val_acc:0.915]
Epoch [19/120    avg_loss:0.421, val_acc:0.950]
Epoch [20/120    avg_loss:0.392, val_acc:0.935]
Epoch [21/120    avg_loss:0.413, val_acc:0.944]
Epoch [22/120    avg_loss:0.340, val_acc:0.944]
Epoch [23/120    avg_loss:0.305, val_acc:0.960]
Epoch [24/120    avg_loss:0.340, val_acc:0.942]
Epoch [25/120    avg_loss:0.281, val_acc:0.952]
Epoch [26/120    avg_loss:0.277, val_acc:0.923]
Epoch [27/120    avg_loss:0.244, val_acc:0.956]
Epoch [28/120    avg_loss:0.234, val_acc:0.966]
Epoch [29/120    avg_loss:0.212, val_acc:0.984]
Epoch [30/120    avg_loss:0.261, val_acc:0.933]
Epoch [31/120    avg_loss:0.271, val_acc:0.968]
Epoch [32/120    avg_loss:0.182, val_acc:0.956]
Epoch [33/120    avg_loss:0.181, val_acc:0.929]
Epoch [34/120    avg_loss:0.187, val_acc:0.905]
Epoch [35/120    avg_loss:0.251, val_acc:0.974]
Epoch [36/120    avg_loss:0.162, val_acc:0.982]
Epoch [37/120    avg_loss:0.177, val_acc:0.980]
Epoch [38/120    avg_loss:0.133, val_acc:0.968]
Epoch [39/120    avg_loss:0.115, val_acc:0.980]
Epoch [40/120    avg_loss:0.132, val_acc:0.966]
Epoch [41/120    avg_loss:0.160, val_acc:0.933]
Epoch [42/120    avg_loss:0.161, val_acc:0.982]
Epoch [43/120    avg_loss:0.088, val_acc:0.980]
Epoch [44/120    avg_loss:0.066, val_acc:0.986]
Epoch [45/120    avg_loss:0.076, val_acc:0.982]
Epoch [46/120    avg_loss:0.077, val_acc:0.988]
Epoch [47/120    avg_loss:0.072, val_acc:0.988]
Epoch [48/120    avg_loss:0.061, val_acc:0.988]
Epoch [49/120    avg_loss:0.062, val_acc:0.990]
Epoch [50/120    avg_loss:0.070, val_acc:0.990]
Epoch [51/120    avg_loss:0.055, val_acc:0.990]
Epoch [52/120    avg_loss:0.056, val_acc:0.990]
Epoch [53/120    avg_loss:0.060, val_acc:0.988]
Epoch [54/120    avg_loss:0.063, val_acc:0.990]
Epoch [55/120    avg_loss:0.054, val_acc:0.990]
Epoch [56/120    avg_loss:0.064, val_acc:0.990]
Epoch [57/120    avg_loss:0.061, val_acc:0.990]
Epoch [58/120    avg_loss:0.059, val_acc:0.990]
Epoch [59/120    avg_loss:0.051, val_acc:0.990]
Epoch [60/120    avg_loss:0.063, val_acc:0.990]
Epoch [61/120    avg_loss:0.058, val_acc:0.992]
Epoch [62/120    avg_loss:0.050, val_acc:0.992]
Epoch [63/120    avg_loss:0.054, val_acc:0.992]
Epoch [64/120    avg_loss:0.053, val_acc:0.992]
Epoch [65/120    avg_loss:0.065, val_acc:0.992]
Epoch [66/120    avg_loss:0.061, val_acc:0.992]
Epoch [67/120    avg_loss:0.054, val_acc:0.992]
Epoch [68/120    avg_loss:0.045, val_acc:0.992]
Epoch [69/120    avg_loss:0.045, val_acc:0.992]
Epoch [70/120    avg_loss:0.045, val_acc:0.992]
Epoch [71/120    avg_loss:0.053, val_acc:0.992]
Epoch [72/120    avg_loss:0.046, val_acc:0.992]
Epoch [73/120    avg_loss:0.049, val_acc:0.992]
Epoch [74/120    avg_loss:0.048, val_acc:0.992]
Epoch [75/120    avg_loss:0.045, val_acc:0.992]
Epoch [76/120    avg_loss:0.042, val_acc:0.992]
Epoch [77/120    avg_loss:0.044, val_acc:0.992]
Epoch [78/120    avg_loss:0.043, val_acc:0.992]
Epoch [79/120    avg_loss:0.043, val_acc:0.992]
Epoch [80/120    avg_loss:0.048, val_acc:0.992]
Epoch [81/120    avg_loss:0.041, val_acc:0.992]
Epoch [82/120    avg_loss:0.037, val_acc:0.992]
Epoch [83/120    avg_loss:0.041, val_acc:0.992]
Epoch [84/120    avg_loss:0.046, val_acc:0.992]
Epoch [85/120    avg_loss:0.047, val_acc:0.992]
Epoch [86/120    avg_loss:0.047, val_acc:0.992]
Epoch [87/120    avg_loss:0.048, val_acc:0.992]
Epoch [88/120    avg_loss:0.040, val_acc:0.992]
Epoch [89/120    avg_loss:0.037, val_acc:0.992]
Epoch [90/120    avg_loss:0.036, val_acc:0.992]
Epoch [91/120    avg_loss:0.044, val_acc:0.992]
Epoch [92/120    avg_loss:0.037, val_acc:0.992]
Epoch [93/120    avg_loss:0.047, val_acc:0.992]
Epoch [94/120    avg_loss:0.041, val_acc:0.992]
Epoch [95/120    avg_loss:0.036, val_acc:0.992]
Epoch [96/120    avg_loss:0.040, val_acc:0.992]
Epoch [97/120    avg_loss:0.041, val_acc:0.992]
Epoch [98/120    avg_loss:0.037, val_acc:0.992]
Epoch [99/120    avg_loss:0.038, val_acc:0.992]
Epoch [100/120    avg_loss:0.034, val_acc:0.992]
Epoch [101/120    avg_loss:0.034, val_acc:0.992]
Epoch [102/120    avg_loss:0.030, val_acc:0.992]
Epoch [103/120    avg_loss:0.036, val_acc:0.992]
Epoch [104/120    avg_loss:0.038, val_acc:0.992]
Epoch [105/120    avg_loss:0.035, val_acc:0.992]
Epoch [106/120    avg_loss:0.051, val_acc:0.992]
Epoch [107/120    avg_loss:0.039, val_acc:0.992]
Epoch [108/120    avg_loss:0.033, val_acc:0.992]
Epoch [109/120    avg_loss:0.033, val_acc:0.992]
Epoch [110/120    avg_loss:0.040, val_acc:0.990]
Epoch [111/120    avg_loss:0.034, val_acc:0.992]
Epoch [112/120    avg_loss:0.035, val_acc:0.992]
Epoch [113/120    avg_loss:0.035, val_acc:0.992]
Epoch [114/120    avg_loss:0.033, val_acc:0.992]
Epoch [115/120    avg_loss:0.032, val_acc:0.992]
Epoch [116/120    avg_loss:0.029, val_acc:0.992]
Epoch [117/120    avg_loss:0.030, val_acc:0.992]
Epoch [118/120    avg_loss:0.035, val_acc:0.992]
Epoch [119/120    avg_loss:0.032, val_acc:0.990]
Epoch [120/120    avg_loss:0.027, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  16   0   0   0   0   0   0   4   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 0.99927061 0.99095023 0.99563319 0.92410714 0.9047619
 1.         0.97826087 0.99870968 1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9907414826677022
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c362c6a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.605, val_acc:0.367]
Epoch [2/120    avg_loss:2.425, val_acc:0.357]
Epoch [3/120    avg_loss:2.264, val_acc:0.367]
Epoch [4/120    avg_loss:2.144, val_acc:0.434]
Epoch [5/120    avg_loss:2.012, val_acc:0.492]
Epoch [6/120    avg_loss:1.901, val_acc:0.510]
Epoch [7/120    avg_loss:1.732, val_acc:0.617]
Epoch [8/120    avg_loss:1.630, val_acc:0.744]
Epoch [9/120    avg_loss:1.478, val_acc:0.760]
Epoch [10/120    avg_loss:1.340, val_acc:0.797]
Epoch [11/120    avg_loss:1.187, val_acc:0.775]
Epoch [12/120    avg_loss:1.069, val_acc:0.852]
Epoch [13/120    avg_loss:0.977, val_acc:0.840]
Epoch [14/120    avg_loss:0.859, val_acc:0.846]
Epoch [15/120    avg_loss:0.796, val_acc:0.842]
Epoch [16/120    avg_loss:0.675, val_acc:0.875]
Epoch [17/120    avg_loss:0.662, val_acc:0.832]
Epoch [18/120    avg_loss:0.605, val_acc:0.871]
Epoch [19/120    avg_loss:0.652, val_acc:0.875]
Epoch [20/120    avg_loss:0.542, val_acc:0.865]
Epoch [21/120    avg_loss:0.560, val_acc:0.891]
Epoch [22/120    avg_loss:0.555, val_acc:0.863]
Epoch [23/120    avg_loss:0.527, val_acc:0.885]
Epoch [24/120    avg_loss:0.518, val_acc:0.883]
Epoch [25/120    avg_loss:0.431, val_acc:0.895]
Epoch [26/120    avg_loss:0.492, val_acc:0.893]
Epoch [27/120    avg_loss:0.431, val_acc:0.898]
Epoch [28/120    avg_loss:0.442, val_acc:0.920]
Epoch [29/120    avg_loss:0.369, val_acc:0.920]
Epoch [30/120    avg_loss:0.342, val_acc:0.889]
Epoch [31/120    avg_loss:0.409, val_acc:0.910]
Epoch [32/120    avg_loss:0.423, val_acc:0.871]
Epoch [33/120    avg_loss:0.413, val_acc:0.912]
Epoch [34/120    avg_loss:0.410, val_acc:0.887]
Epoch [35/120    avg_loss:0.363, val_acc:0.924]
Epoch [36/120    avg_loss:0.312, val_acc:0.926]
Epoch [37/120    avg_loss:0.315, val_acc:0.900]
Epoch [38/120    avg_loss:0.300, val_acc:0.922]
Epoch [39/120    avg_loss:0.258, val_acc:0.904]
Epoch [40/120    avg_loss:0.302, val_acc:0.924]
Epoch [41/120    avg_loss:0.337, val_acc:0.953]
Epoch [42/120    avg_loss:0.265, val_acc:0.928]
Epoch [43/120    avg_loss:0.300, val_acc:0.910]
Epoch [44/120    avg_loss:0.279, val_acc:0.914]
Epoch [45/120    avg_loss:0.214, val_acc:0.938]
Epoch [46/120    avg_loss:0.222, val_acc:0.932]
Epoch [47/120    avg_loss:0.233, val_acc:0.930]
Epoch [48/120    avg_loss:0.313, val_acc:0.893]
Epoch [49/120    avg_loss:0.365, val_acc:0.930]
Epoch [50/120    avg_loss:0.237, val_acc:0.924]
Epoch [51/120    avg_loss:0.216, val_acc:0.898]
Epoch [52/120    avg_loss:0.264, val_acc:0.941]
Epoch [53/120    avg_loss:0.291, val_acc:0.918]
Epoch [54/120    avg_loss:0.210, val_acc:0.963]
Epoch [55/120    avg_loss:0.192, val_acc:0.912]
Epoch [56/120    avg_loss:0.254, val_acc:0.928]
Epoch [57/120    avg_loss:0.242, val_acc:0.941]
Epoch [58/120    avg_loss:0.196, val_acc:0.932]
Epoch [59/120    avg_loss:0.199, val_acc:0.934]
Epoch [60/120    avg_loss:0.222, val_acc:0.941]
Epoch [61/120    avg_loss:0.167, val_acc:0.959]
Epoch [62/120    avg_loss:0.191, val_acc:0.936]
Epoch [63/120    avg_loss:0.227, val_acc:0.953]
Epoch [64/120    avg_loss:0.200, val_acc:0.959]
Epoch [65/120    avg_loss:0.151, val_acc:0.955]
Epoch [66/120    avg_loss:0.201, val_acc:0.943]
Epoch [67/120    avg_loss:0.175, val_acc:0.930]
Epoch [68/120    avg_loss:0.155, val_acc:0.961]
Epoch [69/120    avg_loss:0.123, val_acc:0.967]
Epoch [70/120    avg_loss:0.120, val_acc:0.971]
Epoch [71/120    avg_loss:0.114, val_acc:0.977]
Epoch [72/120    avg_loss:0.092, val_acc:0.977]
Epoch [73/120    avg_loss:0.094, val_acc:0.975]
Epoch [74/120    avg_loss:0.098, val_acc:0.973]
Epoch [75/120    avg_loss:0.098, val_acc:0.977]
Epoch [76/120    avg_loss:0.105, val_acc:0.977]
Epoch [77/120    avg_loss:0.096, val_acc:0.977]
Epoch [78/120    avg_loss:0.087, val_acc:0.977]
Epoch [79/120    avg_loss:0.097, val_acc:0.975]
Epoch [80/120    avg_loss:0.086, val_acc:0.977]
Epoch [81/120    avg_loss:0.086, val_acc:0.975]
Epoch [82/120    avg_loss:0.090, val_acc:0.975]
Epoch [83/120    avg_loss:0.095, val_acc:0.977]
Epoch [84/120    avg_loss:0.087, val_acc:0.975]
Epoch [85/120    avg_loss:0.108, val_acc:0.975]
Epoch [86/120    avg_loss:0.093, val_acc:0.975]
Epoch [87/120    avg_loss:0.098, val_acc:0.977]
Epoch [88/120    avg_loss:0.084, val_acc:0.973]
Epoch [89/120    avg_loss:0.087, val_acc:0.975]
Epoch [90/120    avg_loss:0.081, val_acc:0.975]
Epoch [91/120    avg_loss:0.087, val_acc:0.979]
Epoch [92/120    avg_loss:0.082, val_acc:0.977]
Epoch [93/120    avg_loss:0.085, val_acc:0.979]
Epoch [94/120    avg_loss:0.080, val_acc:0.980]
Epoch [95/120    avg_loss:0.078, val_acc:0.980]
Epoch [96/120    avg_loss:0.081, val_acc:0.975]
Epoch [97/120    avg_loss:0.078, val_acc:0.980]
Epoch [98/120    avg_loss:0.090, val_acc:0.973]
Epoch [99/120    avg_loss:0.094, val_acc:0.979]
Epoch [100/120    avg_loss:0.074, val_acc:0.980]
Epoch [101/120    avg_loss:0.080, val_acc:0.975]
Epoch [102/120    avg_loss:0.072, val_acc:0.977]
Epoch [103/120    avg_loss:0.085, val_acc:0.975]
Epoch [104/120    avg_loss:0.079, val_acc:0.979]
Epoch [105/120    avg_loss:0.075, val_acc:0.980]
Epoch [106/120    avg_loss:0.083, val_acc:0.975]
Epoch [107/120    avg_loss:0.067, val_acc:0.977]
Epoch [108/120    avg_loss:0.077, val_acc:0.979]
Epoch [109/120    avg_loss:0.077, val_acc:0.979]
Epoch [110/120    avg_loss:0.081, val_acc:0.975]
Epoch [111/120    avg_loss:0.063, val_acc:0.979]
Epoch [112/120    avg_loss:0.079, val_acc:0.979]
Epoch [113/120    avg_loss:0.086, val_acc:0.979]
Epoch [114/120    avg_loss:0.061, val_acc:0.980]
Epoch [115/120    avg_loss:0.070, val_acc:0.975]
Epoch [116/120    avg_loss:0.087, val_acc:0.977]
Epoch [117/120    avg_loss:0.068, val_acc:0.977]
Epoch [118/120    avg_loss:0.085, val_acc:0.977]
Epoch [119/120    avg_loss:0.068, val_acc:0.977]
Epoch [120/120    avg_loss:0.069, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0   9   0   0   0   0   2   0]
 [  0   0   0 224   4   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  32 113   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.52878464818762

F1 scores:
[       nan 1.         0.94545455 0.98678414 0.91097308 0.85283019
 1.         0.88648649 0.99870968 0.9978678  1.         1.
 0.99779736 1.        ]

Kappa:
0.983618036247974
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7adaeada90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.599, val_acc:0.365]
Epoch [2/120    avg_loss:2.417, val_acc:0.477]
Epoch [3/120    avg_loss:2.240, val_acc:0.488]
Epoch [4/120    avg_loss:2.097, val_acc:0.508]
Epoch [5/120    avg_loss:1.972, val_acc:0.541]
Epoch [6/120    avg_loss:1.834, val_acc:0.615]
Epoch [7/120    avg_loss:1.688, val_acc:0.629]
Epoch [8/120    avg_loss:1.553, val_acc:0.631]
Epoch [9/120    avg_loss:1.413, val_acc:0.699]
Epoch [10/120    avg_loss:1.248, val_acc:0.697]
Epoch [11/120    avg_loss:1.126, val_acc:0.795]
Epoch [12/120    avg_loss:1.102, val_acc:0.730]
Epoch [13/120    avg_loss:0.974, val_acc:0.758]
Epoch [14/120    avg_loss:0.926, val_acc:0.844]
Epoch [15/120    avg_loss:0.771, val_acc:0.850]
Epoch [16/120    avg_loss:0.745, val_acc:0.881]
Epoch [17/120    avg_loss:0.724, val_acc:0.832]
Epoch [18/120    avg_loss:0.651, val_acc:0.877]
Epoch [19/120    avg_loss:0.579, val_acc:0.871]
Epoch [20/120    avg_loss:0.581, val_acc:0.820]
Epoch [21/120    avg_loss:0.572, val_acc:0.863]
Epoch [22/120    avg_loss:0.530, val_acc:0.881]
Epoch [23/120    avg_loss:0.486, val_acc:0.896]
Epoch [24/120    avg_loss:0.455, val_acc:0.893]
Epoch [25/120    avg_loss:0.480, val_acc:0.906]
Epoch [26/120    avg_loss:0.488, val_acc:0.885]
Epoch [27/120    avg_loss:0.453, val_acc:0.900]
Epoch [28/120    avg_loss:0.438, val_acc:0.904]
Epoch [29/120    avg_loss:0.373, val_acc:0.836]
Epoch [30/120    avg_loss:0.431, val_acc:0.883]
Epoch [31/120    avg_loss:0.432, val_acc:0.914]
Epoch [32/120    avg_loss:0.378, val_acc:0.836]
Epoch [33/120    avg_loss:0.424, val_acc:0.912]
Epoch [34/120    avg_loss:0.361, val_acc:0.908]
Epoch [35/120    avg_loss:0.321, val_acc:0.932]
Epoch [36/120    avg_loss:0.317, val_acc:0.887]
Epoch [37/120    avg_loss:0.354, val_acc:0.875]
Epoch [38/120    avg_loss:0.312, val_acc:0.934]
Epoch [39/120    avg_loss:0.279, val_acc:0.932]
Epoch [40/120    avg_loss:0.238, val_acc:0.941]
Epoch [41/120    avg_loss:0.238, val_acc:0.934]
Epoch [42/120    avg_loss:0.320, val_acc:0.910]
Epoch [43/120    avg_loss:0.294, val_acc:0.934]
Epoch [44/120    avg_loss:0.222, val_acc:0.945]
Epoch [45/120    avg_loss:0.215, val_acc:0.912]
Epoch [46/120    avg_loss:0.248, val_acc:0.941]
Epoch [47/120    avg_loss:0.248, val_acc:0.920]
Epoch [48/120    avg_loss:0.265, val_acc:0.953]
Epoch [49/120    avg_loss:0.205, val_acc:0.938]
Epoch [50/120    avg_loss:0.211, val_acc:0.947]
Epoch [51/120    avg_loss:0.222, val_acc:0.920]
Epoch [52/120    avg_loss:0.216, val_acc:0.947]
Epoch [53/120    avg_loss:0.209, val_acc:0.947]
Epoch [54/120    avg_loss:0.196, val_acc:0.945]
Epoch [55/120    avg_loss:0.160, val_acc:0.934]
Epoch [56/120    avg_loss:0.203, val_acc:0.928]
Epoch [57/120    avg_loss:0.187, val_acc:0.955]
Epoch [58/120    avg_loss:0.144, val_acc:0.961]
Epoch [59/120    avg_loss:0.158, val_acc:0.947]
Epoch [60/120    avg_loss:0.166, val_acc:0.932]
Epoch [61/120    avg_loss:0.168, val_acc:0.949]
Epoch [62/120    avg_loss:0.169, val_acc:0.959]
Epoch [63/120    avg_loss:0.189, val_acc:0.955]
Epoch [64/120    avg_loss:0.138, val_acc:0.951]
Epoch [65/120    avg_loss:0.145, val_acc:0.936]
Epoch [66/120    avg_loss:0.188, val_acc:0.938]
Epoch [67/120    avg_loss:0.136, val_acc:0.959]
Epoch [68/120    avg_loss:0.147, val_acc:0.945]
Epoch [69/120    avg_loss:0.147, val_acc:0.945]
Epoch [70/120    avg_loss:0.131, val_acc:0.947]
Epoch [71/120    avg_loss:0.183, val_acc:0.949]
Epoch [72/120    avg_loss:0.140, val_acc:0.953]
Epoch [73/120    avg_loss:0.097, val_acc:0.957]
Epoch [74/120    avg_loss:0.090, val_acc:0.965]
Epoch [75/120    avg_loss:0.090, val_acc:0.963]
Epoch [76/120    avg_loss:0.087, val_acc:0.965]
Epoch [77/120    avg_loss:0.083, val_acc:0.965]
Epoch [78/120    avg_loss:0.090, val_acc:0.967]
Epoch [79/120    avg_loss:0.085, val_acc:0.971]
Epoch [80/120    avg_loss:0.074, val_acc:0.971]
Epoch [81/120    avg_loss:0.083, val_acc:0.971]
Epoch [82/120    avg_loss:0.081, val_acc:0.971]
Epoch [83/120    avg_loss:0.084, val_acc:0.965]
Epoch [84/120    avg_loss:0.078, val_acc:0.967]
Epoch [85/120    avg_loss:0.072, val_acc:0.969]
Epoch [86/120    avg_loss:0.066, val_acc:0.969]
Epoch [87/120    avg_loss:0.072, val_acc:0.969]
Epoch [88/120    avg_loss:0.075, val_acc:0.967]
Epoch [89/120    avg_loss:0.062, val_acc:0.969]
Epoch [90/120    avg_loss:0.066, val_acc:0.969]
Epoch [91/120    avg_loss:0.064, val_acc:0.967]
Epoch [92/120    avg_loss:0.065, val_acc:0.969]
Epoch [93/120    avg_loss:0.070, val_acc:0.967]
Epoch [94/120    avg_loss:0.067, val_acc:0.967]
Epoch [95/120    avg_loss:0.072, val_acc:0.965]
Epoch [96/120    avg_loss:0.063, val_acc:0.965]
Epoch [97/120    avg_loss:0.070, val_acc:0.965]
Epoch [98/120    avg_loss:0.069, val_acc:0.965]
Epoch [99/120    avg_loss:0.067, val_acc:0.965]
Epoch [100/120    avg_loss:0.075, val_acc:0.965]
Epoch [101/120    avg_loss:0.062, val_acc:0.965]
Epoch [102/120    avg_loss:0.067, val_acc:0.965]
Epoch [103/120    avg_loss:0.074, val_acc:0.965]
Epoch [104/120    avg_loss:0.068, val_acc:0.965]
Epoch [105/120    avg_loss:0.060, val_acc:0.965]
Epoch [106/120    avg_loss:0.053, val_acc:0.967]
Epoch [107/120    avg_loss:0.063, val_acc:0.967]
Epoch [108/120    avg_loss:0.068, val_acc:0.967]
Epoch [109/120    avg_loss:0.060, val_acc:0.967]
Epoch [110/120    avg_loss:0.059, val_acc:0.967]
Epoch [111/120    avg_loss:0.071, val_acc:0.967]
Epoch [112/120    avg_loss:0.060, val_acc:0.967]
Epoch [113/120    avg_loss:0.063, val_acc:0.967]
Epoch [114/120    avg_loss:0.061, val_acc:0.967]
Epoch [115/120    avg_loss:0.069, val_acc:0.967]
Epoch [116/120    avg_loss:0.075, val_acc:0.967]
Epoch [117/120    avg_loss:0.062, val_acc:0.967]
Epoch [118/120    avg_loss:0.058, val_acc:0.967]
Epoch [119/120    avg_loss:0.064, val_acc:0.967]
Epoch [120/120    avg_loss:0.065, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   0 214  11   0   0   0   1   4   0   0   0   0]
 [  0   0   0   1 218   7   0   0   0   0   0   0   1   0]
 [  0   0   0   0  28 117   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   1   0   0   0   0   0 363   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.2089552238806

F1 scores:
[       nan 1.         0.93986637 0.96179775 0.89896907 0.86988848
 1.         0.84745763 0.998713   0.99574468 0.99862448 0.99601594
 0.99559471 1.        ]

Kappa:
0.9800555746582474
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f521413fb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.568, val_acc:0.283]
Epoch [2/120    avg_loss:2.366, val_acc:0.326]
Epoch [3/120    avg_loss:2.208, val_acc:0.367]
Epoch [4/120    avg_loss:2.070, val_acc:0.426]
Epoch [5/120    avg_loss:1.975, val_acc:0.555]
Epoch [6/120    avg_loss:1.845, val_acc:0.639]
Epoch [7/120    avg_loss:1.705, val_acc:0.707]
Epoch [8/120    avg_loss:1.579, val_acc:0.736]
Epoch [9/120    avg_loss:1.418, val_acc:0.717]
Epoch [10/120    avg_loss:1.280, val_acc:0.787]
Epoch [11/120    avg_loss:1.159, val_acc:0.783]
Epoch [12/120    avg_loss:1.062, val_acc:0.803]
Epoch [13/120    avg_loss:0.983, val_acc:0.826]
Epoch [14/120    avg_loss:0.886, val_acc:0.826]
Epoch [15/120    avg_loss:0.832, val_acc:0.857]
Epoch [16/120    avg_loss:0.706, val_acc:0.881]
Epoch [17/120    avg_loss:0.679, val_acc:0.828]
Epoch [18/120    avg_loss:0.680, val_acc:0.871]
Epoch [19/120    avg_loss:0.605, val_acc:0.879]
Epoch [20/120    avg_loss:0.648, val_acc:0.883]
Epoch [21/120    avg_loss:0.565, val_acc:0.869]
Epoch [22/120    avg_loss:0.591, val_acc:0.889]
Epoch [23/120    avg_loss:0.574, val_acc:0.908]
Epoch [24/120    avg_loss:0.495, val_acc:0.896]
Epoch [25/120    avg_loss:0.499, val_acc:0.896]
Epoch [26/120    avg_loss:0.462, val_acc:0.920]
Epoch [27/120    avg_loss:0.509, val_acc:0.924]
Epoch [28/120    avg_loss:0.484, val_acc:0.908]
Epoch [29/120    avg_loss:0.436, val_acc:0.875]
Epoch [30/120    avg_loss:0.422, val_acc:0.916]
Epoch [31/120    avg_loss:0.372, val_acc:0.898]
Epoch [32/120    avg_loss:0.436, val_acc:0.895]
Epoch [33/120    avg_loss:0.395, val_acc:0.904]
Epoch [34/120    avg_loss:0.377, val_acc:0.924]
Epoch [35/120    avg_loss:0.351, val_acc:0.926]
Epoch [36/120    avg_loss:0.400, val_acc:0.930]
Epoch [37/120    avg_loss:0.340, val_acc:0.936]
Epoch [38/120    avg_loss:0.324, val_acc:0.912]
Epoch [39/120    avg_loss:0.404, val_acc:0.932]
Epoch [40/120    avg_loss:0.281, val_acc:0.945]
Epoch [41/120    avg_loss:0.367, val_acc:0.932]
Epoch [42/120    avg_loss:0.324, val_acc:0.953]
Epoch [43/120    avg_loss:0.287, val_acc:0.949]
Epoch [44/120    avg_loss:0.274, val_acc:0.941]
Epoch [45/120    avg_loss:0.300, val_acc:0.955]
Epoch [46/120    avg_loss:0.271, val_acc:0.926]
Epoch [47/120    avg_loss:0.279, val_acc:0.955]
Epoch [48/120    avg_loss:0.341, val_acc:0.951]
Epoch [49/120    avg_loss:0.228, val_acc:0.973]
Epoch [50/120    avg_loss:0.226, val_acc:0.949]
Epoch [51/120    avg_loss:0.271, val_acc:0.939]
Epoch [52/120    avg_loss:0.247, val_acc:0.947]
Epoch [53/120    avg_loss:0.221, val_acc:0.957]
Epoch [54/120    avg_loss:0.221, val_acc:0.906]
Epoch [55/120    avg_loss:0.237, val_acc:0.953]
Epoch [56/120    avg_loss:0.182, val_acc:0.967]
Epoch [57/120    avg_loss:0.179, val_acc:0.953]
Epoch [58/120    avg_loss:0.166, val_acc:0.895]
Epoch [59/120    avg_loss:0.253, val_acc:0.926]
Epoch [60/120    avg_loss:0.202, val_acc:0.936]
Epoch [61/120    avg_loss:0.190, val_acc:0.941]
Epoch [62/120    avg_loss:0.173, val_acc:0.973]
Epoch [63/120    avg_loss:0.214, val_acc:0.941]
Epoch [64/120    avg_loss:0.205, val_acc:0.969]
Epoch [65/120    avg_loss:0.172, val_acc:0.959]
Epoch [66/120    avg_loss:0.185, val_acc:0.951]
Epoch [67/120    avg_loss:0.221, val_acc:0.969]
Epoch [68/120    avg_loss:0.165, val_acc:0.953]
Epoch [69/120    avg_loss:0.219, val_acc:0.953]
Epoch [70/120    avg_loss:0.210, val_acc:0.939]
Epoch [71/120    avg_loss:0.184, val_acc:0.957]
Epoch [72/120    avg_loss:0.145, val_acc:0.967]
Epoch [73/120    avg_loss:0.145, val_acc:0.961]
Epoch [74/120    avg_loss:0.163, val_acc:0.967]
Epoch [75/120    avg_loss:0.162, val_acc:0.965]
Epoch [76/120    avg_loss:0.129, val_acc:0.969]
Epoch [77/120    avg_loss:0.110, val_acc:0.975]
Epoch [78/120    avg_loss:0.091, val_acc:0.969]
Epoch [79/120    avg_loss:0.100, val_acc:0.973]
Epoch [80/120    avg_loss:0.100, val_acc:0.973]
Epoch [81/120    avg_loss:0.095, val_acc:0.977]
Epoch [82/120    avg_loss:0.090, val_acc:0.980]
Epoch [83/120    avg_loss:0.099, val_acc:0.980]
Epoch [84/120    avg_loss:0.098, val_acc:0.980]
Epoch [85/120    avg_loss:0.071, val_acc:0.979]
Epoch [86/120    avg_loss:0.078, val_acc:0.982]
Epoch [87/120    avg_loss:0.084, val_acc:0.977]
Epoch [88/120    avg_loss:0.092, val_acc:0.979]
Epoch [89/120    avg_loss:0.081, val_acc:0.973]
Epoch [90/120    avg_loss:0.081, val_acc:0.980]
Epoch [91/120    avg_loss:0.084, val_acc:0.984]
Epoch [92/120    avg_loss:0.093, val_acc:0.980]
Epoch [93/120    avg_loss:0.087, val_acc:0.977]
Epoch [94/120    avg_loss:0.071, val_acc:0.984]
Epoch [95/120    avg_loss:0.081, val_acc:0.982]
Epoch [96/120    avg_loss:0.086, val_acc:0.982]
Epoch [97/120    avg_loss:0.082, val_acc:0.982]
Epoch [98/120    avg_loss:0.074, val_acc:0.982]
Epoch [99/120    avg_loss:0.081, val_acc:0.984]
Epoch [100/120    avg_loss:0.077, val_acc:0.982]
Epoch [101/120    avg_loss:0.072, val_acc:0.982]
Epoch [102/120    avg_loss:0.074, val_acc:0.979]
Epoch [103/120    avg_loss:0.079, val_acc:0.980]
Epoch [104/120    avg_loss:0.082, val_acc:0.982]
Epoch [105/120    avg_loss:0.068, val_acc:0.984]
Epoch [106/120    avg_loss:0.086, val_acc:0.982]
Epoch [107/120    avg_loss:0.066, val_acc:0.984]
Epoch [108/120    avg_loss:0.068, val_acc:0.984]
Epoch [109/120    avg_loss:0.070, val_acc:0.982]
Epoch [110/120    avg_loss:0.083, val_acc:0.980]
Epoch [111/120    avg_loss:0.064, val_acc:0.982]
Epoch [112/120    avg_loss:0.075, val_acc:0.980]
Epoch [113/120    avg_loss:0.080, val_acc:0.984]
Epoch [114/120    avg_loss:0.077, val_acc:0.977]
Epoch [115/120    avg_loss:0.067, val_acc:0.982]
Epoch [116/120    avg_loss:0.064, val_acc:0.982]
Epoch [117/120    avg_loss:0.072, val_acc:0.980]
Epoch [118/120    avg_loss:0.073, val_acc:0.982]
Epoch [119/120    avg_loss:0.093, val_acc:0.982]
Epoch [120/120    avg_loss:0.075, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 203   0   0   0   0  16   0   0   0   0   0   0]
 [  0   0   2 215   4   0   0   0   9   0   0   0   0   0]
 [  0   0   0   0 203  22   0   0   2   0   0   0   0   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   1   0   0   0   0 376   0   0]
 [  0   0   0   0   0   0   2   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.84648187633262

F1 scores:
[       nan 0.99708879 0.93119266 0.96629213 0.88069414 0.82807018
 0.98296837 0.85416667 0.98602287 1.         1.         0.99867198
 0.99778761 1.        ]

Kappa:
0.9760209885828237
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbec4c50da0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.550, val_acc:0.268]
Epoch [2/120    avg_loss:2.377, val_acc:0.309]
Epoch [3/120    avg_loss:2.237, val_acc:0.314]
Epoch [4/120    avg_loss:2.110, val_acc:0.385]
Epoch [5/120    avg_loss:2.017, val_acc:0.467]
Epoch [6/120    avg_loss:1.940, val_acc:0.512]
Epoch [7/120    avg_loss:1.829, val_acc:0.607]
Epoch [8/120    avg_loss:1.718, val_acc:0.668]
Epoch [9/120    avg_loss:1.623, val_acc:0.701]
Epoch [10/120    avg_loss:1.480, val_acc:0.717]
Epoch [11/120    avg_loss:1.393, val_acc:0.688]
Epoch [12/120    avg_loss:1.230, val_acc:0.746]
Epoch [13/120    avg_loss:1.125, val_acc:0.809]
Epoch [14/120    avg_loss:0.971, val_acc:0.814]
Epoch [15/120    avg_loss:0.953, val_acc:0.814]
Epoch [16/120    avg_loss:0.816, val_acc:0.850]
Epoch [17/120    avg_loss:0.778, val_acc:0.867]
Epoch [18/120    avg_loss:0.671, val_acc:0.844]
Epoch [19/120    avg_loss:0.652, val_acc:0.885]
Epoch [20/120    avg_loss:0.613, val_acc:0.898]
Epoch [21/120    avg_loss:0.540, val_acc:0.898]
Epoch [22/120    avg_loss:0.558, val_acc:0.891]
Epoch [23/120    avg_loss:0.491, val_acc:0.910]
Epoch [24/120    avg_loss:0.485, val_acc:0.906]
Epoch [25/120    avg_loss:0.505, val_acc:0.881]
Epoch [26/120    avg_loss:0.500, val_acc:0.896]
Epoch [27/120    avg_loss:0.466, val_acc:0.898]
Epoch [28/120    avg_loss:0.420, val_acc:0.904]
Epoch [29/120    avg_loss:0.416, val_acc:0.900]
Epoch [30/120    avg_loss:0.435, val_acc:0.904]
Epoch [31/120    avg_loss:0.451, val_acc:0.914]
Epoch [32/120    avg_loss:0.341, val_acc:0.898]
Epoch [33/120    avg_loss:0.429, val_acc:0.875]
Epoch [34/120    avg_loss:0.357, val_acc:0.924]
Epoch [35/120    avg_loss:0.336, val_acc:0.939]
Epoch [36/120    avg_loss:0.379, val_acc:0.924]
Epoch [37/120    avg_loss:0.305, val_acc:0.934]
Epoch [38/120    avg_loss:0.314, val_acc:0.912]
Epoch [39/120    avg_loss:0.339, val_acc:0.910]
Epoch [40/120    avg_loss:0.362, val_acc:0.926]
Epoch [41/120    avg_loss:0.243, val_acc:0.930]
Epoch [42/120    avg_loss:0.273, val_acc:0.906]
Epoch [43/120    avg_loss:0.263, val_acc:0.910]
Epoch [44/120    avg_loss:0.276, val_acc:0.949]
Epoch [45/120    avg_loss:0.249, val_acc:0.943]
Epoch [46/120    avg_loss:0.226, val_acc:0.914]
Epoch [47/120    avg_loss:0.244, val_acc:0.951]
Epoch [48/120    avg_loss:0.238, val_acc:0.904]
Epoch [49/120    avg_loss:0.302, val_acc:0.939]
Epoch [50/120    avg_loss:0.259, val_acc:0.945]
Epoch [51/120    avg_loss:0.191, val_acc:0.943]
Epoch [52/120    avg_loss:0.176, val_acc:0.949]
Epoch [53/120    avg_loss:0.296, val_acc:0.930]
Epoch [54/120    avg_loss:0.200, val_acc:0.947]
Epoch [55/120    avg_loss:0.213, val_acc:0.953]
Epoch [56/120    avg_loss:0.237, val_acc:0.934]
Epoch [57/120    avg_loss:0.245, val_acc:0.951]
Epoch [58/120    avg_loss:0.172, val_acc:0.955]
Epoch [59/120    avg_loss:0.166, val_acc:0.947]
Epoch [60/120    avg_loss:0.184, val_acc:0.941]
Epoch [61/120    avg_loss:0.206, val_acc:0.930]
Epoch [62/120    avg_loss:0.224, val_acc:0.953]
Epoch [63/120    avg_loss:0.188, val_acc:0.922]
Epoch [64/120    avg_loss:0.187, val_acc:0.953]
Epoch [65/120    avg_loss:0.203, val_acc:0.941]
Epoch [66/120    avg_loss:0.129, val_acc:0.951]
Epoch [67/120    avg_loss:0.139, val_acc:0.953]
Epoch [68/120    avg_loss:0.155, val_acc:0.959]
Epoch [69/120    avg_loss:0.168, val_acc:0.959]
Epoch [70/120    avg_loss:0.154, val_acc:0.932]
Epoch [71/120    avg_loss:0.168, val_acc:0.955]
Epoch [72/120    avg_loss:0.228, val_acc:0.930]
Epoch [73/120    avg_loss:0.215, val_acc:0.922]
Epoch [74/120    avg_loss:0.159, val_acc:0.955]
Epoch [75/120    avg_loss:0.123, val_acc:0.965]
Epoch [76/120    avg_loss:0.134, val_acc:0.965]
Epoch [77/120    avg_loss:0.121, val_acc:0.963]
Epoch [78/120    avg_loss:0.110, val_acc:0.965]
Epoch [79/120    avg_loss:0.083, val_acc:0.973]
Epoch [80/120    avg_loss:0.131, val_acc:0.975]
Epoch [81/120    avg_loss:0.112, val_acc:0.947]
Epoch [82/120    avg_loss:0.083, val_acc:0.971]
Epoch [83/120    avg_loss:0.069, val_acc:0.920]
Epoch [84/120    avg_loss:0.242, val_acc:0.947]
Epoch [85/120    avg_loss:0.131, val_acc:0.957]
Epoch [86/120    avg_loss:0.101, val_acc:0.932]
Epoch [87/120    avg_loss:0.147, val_acc:0.961]
Epoch [88/120    avg_loss:0.091, val_acc:0.955]
Epoch [89/120    avg_loss:0.088, val_acc:0.957]
Epoch [90/120    avg_loss:0.079, val_acc:0.963]
Epoch [91/120    avg_loss:0.108, val_acc:0.961]
Epoch [92/120    avg_loss:0.069, val_acc:0.969]
Epoch [93/120    avg_loss:0.058, val_acc:0.963]
Epoch [94/120    avg_loss:0.061, val_acc:0.969]
Epoch [95/120    avg_loss:0.061, val_acc:0.971]
Epoch [96/120    avg_loss:0.057, val_acc:0.977]
Epoch [97/120    avg_loss:0.056, val_acc:0.977]
Epoch [98/120    avg_loss:0.063, val_acc:0.977]
Epoch [99/120    avg_loss:0.039, val_acc:0.977]
Epoch [100/120    avg_loss:0.041, val_acc:0.979]
Epoch [101/120    avg_loss:0.058, val_acc:0.975]
Epoch [102/120    avg_loss:0.044, val_acc:0.975]
Epoch [103/120    avg_loss:0.047, val_acc:0.980]
Epoch [104/120    avg_loss:0.041, val_acc:0.977]
Epoch [105/120    avg_loss:0.036, val_acc:0.977]
Epoch [106/120    avg_loss:0.042, val_acc:0.975]
Epoch [107/120    avg_loss:0.053, val_acc:0.977]
Epoch [108/120    avg_loss:0.043, val_acc:0.977]
Epoch [109/120    avg_loss:0.041, val_acc:0.977]
Epoch [110/120    avg_loss:0.041, val_acc:0.979]
Epoch [111/120    avg_loss:0.037, val_acc:0.977]
Epoch [112/120    avg_loss:0.034, val_acc:0.979]
Epoch [113/120    avg_loss:0.045, val_acc:0.977]
Epoch [114/120    avg_loss:0.037, val_acc:0.977]
Epoch [115/120    avg_loss:0.049, val_acc:0.979]
Epoch [116/120    avg_loss:0.034, val_acc:0.977]
Epoch [117/120    avg_loss:0.032, val_acc:0.977]
Epoch [118/120    avg_loss:0.035, val_acc:0.977]
Epoch [119/120    avg_loss:0.036, val_acc:0.977]
Epoch [120/120    avg_loss:0.039, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 212  16   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 200  26   1   0   0   0   0   0   0   0]
 [  0   0   0   1   5 135   3   0   1   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  20   0   0   0   0  74   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.08102345415779

F1 scores:
[       nan 0.997815   0.93303571 0.95711061 0.89285714 0.88235294
 0.98305085 0.83146067 0.998713   0.9978678  1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9786337971012526
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd28acafb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.592, val_acc:0.348]
Epoch [2/120    avg_loss:2.450, val_acc:0.432]
Epoch [3/120    avg_loss:2.297, val_acc:0.457]
Epoch [4/120    avg_loss:2.131, val_acc:0.547]
Epoch [5/120    avg_loss:2.003, val_acc:0.570]
Epoch [6/120    avg_loss:1.824, val_acc:0.602]
Epoch [7/120    avg_loss:1.679, val_acc:0.609]
Epoch [8/120    avg_loss:1.524, val_acc:0.623]
Epoch [9/120    avg_loss:1.364, val_acc:0.707]
Epoch [10/120    avg_loss:1.226, val_acc:0.686]
Epoch [11/120    avg_loss:1.100, val_acc:0.789]
Epoch [12/120    avg_loss:1.009, val_acc:0.865]
Epoch [13/120    avg_loss:0.899, val_acc:0.873]
Epoch [14/120    avg_loss:0.827, val_acc:0.854]
Epoch [15/120    avg_loss:0.789, val_acc:0.758]
Epoch [16/120    avg_loss:0.745, val_acc:0.863]
Epoch [17/120    avg_loss:0.654, val_acc:0.834]
Epoch [18/120    avg_loss:0.643, val_acc:0.873]
Epoch [19/120    avg_loss:0.544, val_acc:0.879]
Epoch [20/120    avg_loss:0.572, val_acc:0.871]
Epoch [21/120    avg_loss:0.520, val_acc:0.912]
Epoch [22/120    avg_loss:0.481, val_acc:0.916]
Epoch [23/120    avg_loss:0.445, val_acc:0.871]
Epoch [24/120    avg_loss:0.441, val_acc:0.871]
Epoch [25/120    avg_loss:0.454, val_acc:0.912]
Epoch [26/120    avg_loss:0.445, val_acc:0.920]
Epoch [27/120    avg_loss:0.423, val_acc:0.895]
Epoch [28/120    avg_loss:0.400, val_acc:0.906]
Epoch [29/120    avg_loss:0.401, val_acc:0.904]
Epoch [30/120    avg_loss:0.392, val_acc:0.861]
Epoch [31/120    avg_loss:0.363, val_acc:0.910]
Epoch [32/120    avg_loss:0.333, val_acc:0.926]
Epoch [33/120    avg_loss:0.279, val_acc:0.939]
Epoch [34/120    avg_loss:0.284, val_acc:0.951]
Epoch [35/120    avg_loss:0.285, val_acc:0.916]
Epoch [36/120    avg_loss:0.248, val_acc:0.932]
Epoch [37/120    avg_loss:0.265, val_acc:0.941]
Epoch [38/120    avg_loss:0.232, val_acc:0.941]
Epoch [39/120    avg_loss:0.234, val_acc:0.934]
Epoch [40/120    avg_loss:0.235, val_acc:0.904]
Epoch [41/120    avg_loss:0.263, val_acc:0.943]
Epoch [42/120    avg_loss:0.242, val_acc:0.926]
Epoch [43/120    avg_loss:0.236, val_acc:0.924]
Epoch [44/120    avg_loss:0.285, val_acc:0.916]
Epoch [45/120    avg_loss:0.238, val_acc:0.910]
Epoch [46/120    avg_loss:0.341, val_acc:0.936]
Epoch [47/120    avg_loss:0.284, val_acc:0.920]
Epoch [48/120    avg_loss:0.220, val_acc:0.951]
Epoch [49/120    avg_loss:0.174, val_acc:0.957]
Epoch [50/120    avg_loss:0.153, val_acc:0.959]
Epoch [51/120    avg_loss:0.157, val_acc:0.961]
Epoch [52/120    avg_loss:0.151, val_acc:0.963]
Epoch [53/120    avg_loss:0.145, val_acc:0.959]
Epoch [54/120    avg_loss:0.152, val_acc:0.961]
Epoch [55/120    avg_loss:0.135, val_acc:0.963]
Epoch [56/120    avg_loss:0.133, val_acc:0.965]
Epoch [57/120    avg_loss:0.138, val_acc:0.965]
Epoch [58/120    avg_loss:0.124, val_acc:0.967]
Epoch [59/120    avg_loss:0.142, val_acc:0.967]
Epoch [60/120    avg_loss:0.134, val_acc:0.961]
Epoch [61/120    avg_loss:0.126, val_acc:0.965]
Epoch [62/120    avg_loss:0.122, val_acc:0.963]
Epoch [63/120    avg_loss:0.117, val_acc:0.965]
Epoch [64/120    avg_loss:0.136, val_acc:0.965]
Epoch [65/120    avg_loss:0.129, val_acc:0.965]
Epoch [66/120    avg_loss:0.117, val_acc:0.965]
Epoch [67/120    avg_loss:0.114, val_acc:0.967]
Epoch [68/120    avg_loss:0.113, val_acc:0.967]
Epoch [69/120    avg_loss:0.117, val_acc:0.971]
Epoch [70/120    avg_loss:0.118, val_acc:0.965]
Epoch [71/120    avg_loss:0.121, val_acc:0.959]
Epoch [72/120    avg_loss:0.112, val_acc:0.965]
Epoch [73/120    avg_loss:0.103, val_acc:0.963]
Epoch [74/120    avg_loss:0.112, val_acc:0.971]
Epoch [75/120    avg_loss:0.111, val_acc:0.967]
Epoch [76/120    avg_loss:0.103, val_acc:0.965]
Epoch [77/120    avg_loss:0.090, val_acc:0.969]
Epoch [78/120    avg_loss:0.105, val_acc:0.967]
Epoch [79/120    avg_loss:0.122, val_acc:0.973]
Epoch [80/120    avg_loss:0.105, val_acc:0.969]
Epoch [81/120    avg_loss:0.112, val_acc:0.963]
Epoch [82/120    avg_loss:0.100, val_acc:0.963]
Epoch [83/120    avg_loss:0.108, val_acc:0.969]
Epoch [84/120    avg_loss:0.106, val_acc:0.963]
Epoch [85/120    avg_loss:0.099, val_acc:0.967]
Epoch [86/120    avg_loss:0.103, val_acc:0.965]
Epoch [87/120    avg_loss:0.098, val_acc:0.967]
Epoch [88/120    avg_loss:0.099, val_acc:0.967]
Epoch [89/120    avg_loss:0.093, val_acc:0.967]
Epoch [90/120    avg_loss:0.105, val_acc:0.969]
Epoch [91/120    avg_loss:0.103, val_acc:0.967]
Epoch [92/120    avg_loss:0.098, val_acc:0.967]
Epoch [93/120    avg_loss:0.082, val_acc:0.967]
Epoch [94/120    avg_loss:0.089, val_acc:0.967]
Epoch [95/120    avg_loss:0.090, val_acc:0.967]
Epoch [96/120    avg_loss:0.091, val_acc:0.967]
Epoch [97/120    avg_loss:0.094, val_acc:0.967]
Epoch [98/120    avg_loss:0.097, val_acc:0.967]
Epoch [99/120    avg_loss:0.084, val_acc:0.967]
Epoch [100/120    avg_loss:0.082, val_acc:0.967]
Epoch [101/120    avg_loss:0.096, val_acc:0.967]
Epoch [102/120    avg_loss:0.093, val_acc:0.965]
Epoch [103/120    avg_loss:0.091, val_acc:0.967]
Epoch [104/120    avg_loss:0.093, val_acc:0.965]
Epoch [105/120    avg_loss:0.091, val_acc:0.967]
Epoch [106/120    avg_loss:0.073, val_acc:0.967]
Epoch [107/120    avg_loss:0.087, val_acc:0.967]
Epoch [108/120    avg_loss:0.098, val_acc:0.967]
Epoch [109/120    avg_loss:0.092, val_acc:0.967]
Epoch [110/120    avg_loss:0.087, val_acc:0.967]
Epoch [111/120    avg_loss:0.090, val_acc:0.967]
Epoch [112/120    avg_loss:0.085, val_acc:0.967]
Epoch [113/120    avg_loss:0.092, val_acc:0.967]
Epoch [114/120    avg_loss:0.085, val_acc:0.967]
Epoch [115/120    avg_loss:0.083, val_acc:0.967]
Epoch [116/120    avg_loss:0.090, val_acc:0.967]
Epoch [117/120    avg_loss:0.084, val_acc:0.967]
Epoch [118/120    avg_loss:0.076, val_acc:0.967]
Epoch [119/120    avg_loss:0.084, val_acc:0.967]
Epoch [120/120    avg_loss:0.085, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 203   0   1   0   0  15   0   0   0   0   0   0]
 [  0   0   0 219  10   0   0   0   0   1   0   0   0   0]
 [  0   0   0   1 203  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.97441364605544

F1 scores:
[       nan 0.99926954 0.92482916 0.97333333 0.875      0.84137931
 0.99757869 0.8342246  0.99870968 0.99893276 1.         0.99601594
 0.99669239 1.        ]

Kappa:
0.9774490220351221
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fa6c510b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.281]
Epoch [2/120    avg_loss:2.469, val_acc:0.443]
Epoch [3/120    avg_loss:2.326, val_acc:0.459]
Epoch [4/120    avg_loss:2.180, val_acc:0.479]
Epoch [5/120    avg_loss:2.053, val_acc:0.520]
Epoch [6/120    avg_loss:1.905, val_acc:0.596]
Epoch [7/120    avg_loss:1.762, val_acc:0.648]
Epoch [8/120    avg_loss:1.620, val_acc:0.701]
Epoch [9/120    avg_loss:1.466, val_acc:0.775]
Epoch [10/120    avg_loss:1.307, val_acc:0.795]
Epoch [11/120    avg_loss:1.180, val_acc:0.822]
Epoch [12/120    avg_loss:1.052, val_acc:0.832]
Epoch [13/120    avg_loss:0.947, val_acc:0.885]
Epoch [14/120    avg_loss:0.902, val_acc:0.871]
Epoch [15/120    avg_loss:0.783, val_acc:0.875]
Epoch [16/120    avg_loss:0.733, val_acc:0.891]
Epoch [17/120    avg_loss:0.686, val_acc:0.895]
Epoch [18/120    avg_loss:0.662, val_acc:0.900]
Epoch [19/120    avg_loss:0.634, val_acc:0.877]
Epoch [20/120    avg_loss:0.591, val_acc:0.881]
Epoch [21/120    avg_loss:0.553, val_acc:0.900]
Epoch [22/120    avg_loss:0.535, val_acc:0.891]
Epoch [23/120    avg_loss:0.440, val_acc:0.914]
Epoch [24/120    avg_loss:0.456, val_acc:0.910]
Epoch [25/120    avg_loss:0.423, val_acc:0.936]
Epoch [26/120    avg_loss:0.573, val_acc:0.850]
Epoch [27/120    avg_loss:0.489, val_acc:0.916]
Epoch [28/120    avg_loss:0.449, val_acc:0.924]
Epoch [29/120    avg_loss:0.422, val_acc:0.918]
Epoch [30/120    avg_loss:0.434, val_acc:0.904]
Epoch [31/120    avg_loss:0.422, val_acc:0.928]
Epoch [32/120    avg_loss:0.405, val_acc:0.902]
Epoch [33/120    avg_loss:0.429, val_acc:0.875]
Epoch [34/120    avg_loss:0.354, val_acc:0.920]
Epoch [35/120    avg_loss:0.365, val_acc:0.918]
Epoch [36/120    avg_loss:0.371, val_acc:0.912]
Epoch [37/120    avg_loss:0.345, val_acc:0.922]
Epoch [38/120    avg_loss:0.317, val_acc:0.912]
Epoch [39/120    avg_loss:0.293, val_acc:0.943]
Epoch [40/120    avg_loss:0.266, val_acc:0.949]
Epoch [41/120    avg_loss:0.263, val_acc:0.941]
Epoch [42/120    avg_loss:0.260, val_acc:0.941]
Epoch [43/120    avg_loss:0.232, val_acc:0.941]
Epoch [44/120    avg_loss:0.236, val_acc:0.949]
Epoch [45/120    avg_loss:0.228, val_acc:0.947]
Epoch [46/120    avg_loss:0.231, val_acc:0.949]
Epoch [47/120    avg_loss:0.210, val_acc:0.947]
Epoch [48/120    avg_loss:0.226, val_acc:0.951]
Epoch [49/120    avg_loss:0.210, val_acc:0.945]
Epoch [50/120    avg_loss:0.223, val_acc:0.953]
Epoch [51/120    avg_loss:0.202, val_acc:0.943]
Epoch [52/120    avg_loss:0.238, val_acc:0.945]
Epoch [53/120    avg_loss:0.221, val_acc:0.943]
Epoch [54/120    avg_loss:0.215, val_acc:0.939]
Epoch [55/120    avg_loss:0.180, val_acc:0.951]
Epoch [56/120    avg_loss:0.237, val_acc:0.955]
Epoch [57/120    avg_loss:0.206, val_acc:0.945]
Epoch [58/120    avg_loss:0.195, val_acc:0.947]
Epoch [59/120    avg_loss:0.222, val_acc:0.941]
Epoch [60/120    avg_loss:0.197, val_acc:0.949]
Epoch [61/120    avg_loss:0.223, val_acc:0.957]
Epoch [62/120    avg_loss:0.219, val_acc:0.949]
Epoch [63/120    avg_loss:0.208, val_acc:0.949]
Epoch [64/120    avg_loss:0.204, val_acc:0.951]
Epoch [65/120    avg_loss:0.182, val_acc:0.957]
Epoch [66/120    avg_loss:0.205, val_acc:0.953]
Epoch [67/120    avg_loss:0.207, val_acc:0.949]
Epoch [68/120    avg_loss:0.184, val_acc:0.949]
Epoch [69/120    avg_loss:0.200, val_acc:0.947]
Epoch [70/120    avg_loss:0.181, val_acc:0.941]
Epoch [71/120    avg_loss:0.198, val_acc:0.959]
Epoch [72/120    avg_loss:0.188, val_acc:0.951]
Epoch [73/120    avg_loss:0.182, val_acc:0.957]
Epoch [74/120    avg_loss:0.180, val_acc:0.949]
Epoch [75/120    avg_loss:0.147, val_acc:0.961]
Epoch [76/120    avg_loss:0.169, val_acc:0.953]
Epoch [77/120    avg_loss:0.169, val_acc:0.959]
Epoch [78/120    avg_loss:0.187, val_acc:0.953]
Epoch [79/120    avg_loss:0.181, val_acc:0.955]
Epoch [80/120    avg_loss:0.175, val_acc:0.959]
Epoch [81/120    avg_loss:0.179, val_acc:0.951]
Epoch [82/120    avg_loss:0.184, val_acc:0.959]
Epoch [83/120    avg_loss:0.165, val_acc:0.957]
Epoch [84/120    avg_loss:0.161, val_acc:0.963]
Epoch [85/120    avg_loss:0.154, val_acc:0.961]
Epoch [86/120    avg_loss:0.145, val_acc:0.955]
Epoch [87/120    avg_loss:0.167, val_acc:0.955]
Epoch [88/120    avg_loss:0.173, val_acc:0.955]
Epoch [89/120    avg_loss:0.167, val_acc:0.963]
Epoch [90/120    avg_loss:0.148, val_acc:0.959]
Epoch [91/120    avg_loss:0.179, val_acc:0.967]
Epoch [92/120    avg_loss:0.154, val_acc:0.965]
Epoch [93/120    avg_loss:0.163, val_acc:0.961]
Epoch [94/120    avg_loss:0.140, val_acc:0.957]
Epoch [95/120    avg_loss:0.140, val_acc:0.955]
Epoch [96/120    avg_loss:0.173, val_acc:0.965]
Epoch [97/120    avg_loss:0.151, val_acc:0.965]
Epoch [98/120    avg_loss:0.158, val_acc:0.961]
Epoch [99/120    avg_loss:0.139, val_acc:0.963]
Epoch [100/120    avg_loss:0.132, val_acc:0.969]
Epoch [101/120    avg_loss:0.146, val_acc:0.967]
Epoch [102/120    avg_loss:0.158, val_acc:0.965]
Epoch [103/120    avg_loss:0.140, val_acc:0.963]
Epoch [104/120    avg_loss:0.171, val_acc:0.959]
Epoch [105/120    avg_loss:0.164, val_acc:0.971]
Epoch [106/120    avg_loss:0.140, val_acc:0.965]
Epoch [107/120    avg_loss:0.145, val_acc:0.963]
Epoch [108/120    avg_loss:0.142, val_acc:0.959]
Epoch [109/120    avg_loss:0.132, val_acc:0.967]
Epoch [110/120    avg_loss:0.157, val_acc:0.965]
Epoch [111/120    avg_loss:0.145, val_acc:0.957]
Epoch [112/120    avg_loss:0.130, val_acc:0.959]
Epoch [113/120    avg_loss:0.133, val_acc:0.961]
Epoch [114/120    avg_loss:0.141, val_acc:0.957]
Epoch [115/120    avg_loss:0.130, val_acc:0.961]
Epoch [116/120    avg_loss:0.132, val_acc:0.971]
Epoch [117/120    avg_loss:0.152, val_acc:0.967]
Epoch [118/120    avg_loss:0.132, val_acc:0.971]
Epoch [119/120    avg_loss:0.138, val_acc:0.969]
Epoch [120/120    avg_loss:0.127, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 196   0   0   0   0  23   0   0   0   0   0   0]
 [  0   0   1 212  10   0   0   0   7   0   0   0   0   0]
 [  0   0   2   1 200  24   0   0   0   0   0   0   0   0]
 [  0   0   0   1  10 134   0   0   0   0   0   0   0   0]
 [  0   9   0   0   1   0 196   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.69722814498934

F1 scores:
[       nan 0.99347353 0.89702517 0.95495495 0.89285714 0.88448845
 0.97512438 0.8        0.98717949 1.         1.         1.
 1.         1.        ]

Kappa:
0.9743589886369498
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fddd0fa8ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.606, val_acc:0.250]
Epoch [2/120    avg_loss:2.469, val_acc:0.305]
Epoch [3/120    avg_loss:2.333, val_acc:0.383]
Epoch [4/120    avg_loss:2.205, val_acc:0.414]
Epoch [5/120    avg_loss:2.093, val_acc:0.510]
Epoch [6/120    avg_loss:1.945, val_acc:0.592]
Epoch [7/120    avg_loss:1.788, val_acc:0.684]
Epoch [8/120    avg_loss:1.645, val_acc:0.693]
Epoch [9/120    avg_loss:1.485, val_acc:0.736]
Epoch [10/120    avg_loss:1.345, val_acc:0.748]
Epoch [11/120    avg_loss:1.183, val_acc:0.785]
Epoch [12/120    avg_loss:1.068, val_acc:0.824]
Epoch [13/120    avg_loss:0.955, val_acc:0.814]
Epoch [14/120    avg_loss:0.896, val_acc:0.857]
Epoch [15/120    avg_loss:0.812, val_acc:0.816]
Epoch [16/120    avg_loss:0.778, val_acc:0.865]
Epoch [17/120    avg_loss:0.724, val_acc:0.844]
Epoch [18/120    avg_loss:0.743, val_acc:0.865]
Epoch [19/120    avg_loss:0.666, val_acc:0.893]
Epoch [20/120    avg_loss:0.595, val_acc:0.877]
Epoch [21/120    avg_loss:0.594, val_acc:0.846]
Epoch [22/120    avg_loss:0.550, val_acc:0.906]
Epoch [23/120    avg_loss:0.478, val_acc:0.895]
Epoch [24/120    avg_loss:0.464, val_acc:0.912]
Epoch [25/120    avg_loss:0.460, val_acc:0.922]
Epoch [26/120    avg_loss:0.472, val_acc:0.918]
Epoch [27/120    avg_loss:0.433, val_acc:0.910]
Epoch [28/120    avg_loss:0.404, val_acc:0.920]
Epoch [29/120    avg_loss:0.434, val_acc:0.922]
Epoch [30/120    avg_loss:0.414, val_acc:0.932]
Epoch [31/120    avg_loss:0.362, val_acc:0.912]
Epoch [32/120    avg_loss:0.424, val_acc:0.908]
Epoch [33/120    avg_loss:0.338, val_acc:0.863]
Epoch [34/120    avg_loss:0.344, val_acc:0.908]
Epoch [35/120    avg_loss:0.328, val_acc:0.941]
Epoch [36/120    avg_loss:0.350, val_acc:0.938]
Epoch [37/120    avg_loss:0.253, val_acc:0.936]
Epoch [38/120    avg_loss:0.294, val_acc:0.932]
Epoch [39/120    avg_loss:0.280, val_acc:0.930]
Epoch [40/120    avg_loss:0.308, val_acc:0.947]
Epoch [41/120    avg_loss:0.259, val_acc:0.961]
Epoch [42/120    avg_loss:0.274, val_acc:0.932]
Epoch [43/120    avg_loss:0.261, val_acc:0.930]
Epoch [44/120    avg_loss:0.255, val_acc:0.930]
Epoch [45/120    avg_loss:0.236, val_acc:0.949]
Epoch [46/120    avg_loss:0.218, val_acc:0.955]
Epoch [47/120    avg_loss:0.239, val_acc:0.941]
Epoch [48/120    avg_loss:0.239, val_acc:0.951]
Epoch [49/120    avg_loss:0.220, val_acc:0.965]
Epoch [50/120    avg_loss:0.219, val_acc:0.941]
Epoch [51/120    avg_loss:0.253, val_acc:0.951]
Epoch [52/120    avg_loss:0.205, val_acc:0.949]
Epoch [53/120    avg_loss:0.165, val_acc:0.951]
Epoch [54/120    avg_loss:0.181, val_acc:0.951]
Epoch [55/120    avg_loss:0.233, val_acc:0.965]
Epoch [56/120    avg_loss:0.194, val_acc:0.945]
Epoch [57/120    avg_loss:0.164, val_acc:0.967]
Epoch [58/120    avg_loss:0.188, val_acc:0.955]
Epoch [59/120    avg_loss:0.136, val_acc:0.977]
Epoch [60/120    avg_loss:0.156, val_acc:0.963]
Epoch [61/120    avg_loss:0.124, val_acc:0.967]
Epoch [62/120    avg_loss:0.118, val_acc:0.961]
Epoch [63/120    avg_loss:0.141, val_acc:0.975]
Epoch [64/120    avg_loss:0.164, val_acc:0.959]
Epoch [65/120    avg_loss:0.179, val_acc:0.957]
Epoch [66/120    avg_loss:0.188, val_acc:0.945]
Epoch [67/120    avg_loss:0.178, val_acc:0.945]
Epoch [68/120    avg_loss:0.137, val_acc:0.959]
Epoch [69/120    avg_loss:0.116, val_acc:0.969]
Epoch [70/120    avg_loss:0.140, val_acc:0.975]
Epoch [71/120    avg_loss:0.140, val_acc:0.957]
Epoch [72/120    avg_loss:0.163, val_acc:0.975]
Epoch [73/120    avg_loss:0.130, val_acc:0.980]
Epoch [74/120    avg_loss:0.089, val_acc:0.982]
Epoch [75/120    avg_loss:0.076, val_acc:0.982]
Epoch [76/120    avg_loss:0.084, val_acc:0.982]
Epoch [77/120    avg_loss:0.082, val_acc:0.982]
Epoch [78/120    avg_loss:0.080, val_acc:0.984]
Epoch [79/120    avg_loss:0.073, val_acc:0.982]
Epoch [80/120    avg_loss:0.063, val_acc:0.984]
Epoch [81/120    avg_loss:0.066, val_acc:0.990]
Epoch [82/120    avg_loss:0.065, val_acc:0.990]
Epoch [83/120    avg_loss:0.078, val_acc:0.990]
Epoch [84/120    avg_loss:0.073, val_acc:0.990]
Epoch [85/120    avg_loss:0.064, val_acc:0.990]
Epoch [86/120    avg_loss:0.061, val_acc:0.986]
Epoch [87/120    avg_loss:0.063, val_acc:0.984]
Epoch [88/120    avg_loss:0.065, val_acc:0.986]
Epoch [89/120    avg_loss:0.064, val_acc:0.980]
Epoch [90/120    avg_loss:0.062, val_acc:0.986]
Epoch [91/120    avg_loss:0.055, val_acc:0.986]
Epoch [92/120    avg_loss:0.072, val_acc:0.984]
Epoch [93/120    avg_loss:0.061, val_acc:0.984]
Epoch [94/120    avg_loss:0.069, val_acc:0.990]
Epoch [95/120    avg_loss:0.075, val_acc:0.988]
Epoch [96/120    avg_loss:0.067, val_acc:0.992]
Epoch [97/120    avg_loss:0.068, val_acc:0.984]
Epoch [98/120    avg_loss:0.064, val_acc:0.986]
Epoch [99/120    avg_loss:0.060, val_acc:0.992]
Epoch [100/120    avg_loss:0.061, val_acc:0.992]
Epoch [101/120    avg_loss:0.057, val_acc:0.992]
Epoch [102/120    avg_loss:0.068, val_acc:0.992]
Epoch [103/120    avg_loss:0.057, val_acc:0.992]
Epoch [104/120    avg_loss:0.058, val_acc:0.990]
Epoch [105/120    avg_loss:0.071, val_acc:0.992]
Epoch [106/120    avg_loss:0.064, val_acc:0.992]
Epoch [107/120    avg_loss:0.054, val_acc:0.992]
Epoch [108/120    avg_loss:0.062, val_acc:0.992]
Epoch [109/120    avg_loss:0.073, val_acc:0.996]
Epoch [110/120    avg_loss:0.056, val_acc:0.992]
Epoch [111/120    avg_loss:0.050, val_acc:0.992]
Epoch [112/120    avg_loss:0.054, val_acc:0.992]
Epoch [113/120    avg_loss:0.069, val_acc:0.992]
Epoch [114/120    avg_loss:0.055, val_acc:0.994]
Epoch [115/120    avg_loss:0.055, val_acc:0.992]
Epoch [116/120    avg_loss:0.059, val_acc:0.990]
Epoch [117/120    avg_loss:0.047, val_acc:0.992]
Epoch [118/120    avg_loss:0.058, val_acc:0.994]
Epoch [119/120    avg_loss:0.065, val_acc:0.992]
Epoch [120/120    avg_loss:0.060, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 211   8   0   0   0   8   3   0   0   0   0]
 [  0   0   0   0 211  15   0   0   1   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.10234541577825

F1 scores:
[       nan 1.         0.93832599 0.9569161  0.89596603 0.87323944
 0.99019608 0.86363636 0.98335467 0.99680511 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9788692098706471
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f17af3a6a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.563, val_acc:0.309]
Epoch [2/120    avg_loss:2.381, val_acc:0.352]
Epoch [3/120    avg_loss:2.248, val_acc:0.484]
Epoch [4/120    avg_loss:2.094, val_acc:0.490]
Epoch [5/120    avg_loss:1.966, val_acc:0.658]
Epoch [6/120    avg_loss:1.827, val_acc:0.709]
Epoch [7/120    avg_loss:1.695, val_acc:0.725]
Epoch [8/120    avg_loss:1.558, val_acc:0.768]
Epoch [9/120    avg_loss:1.355, val_acc:0.811]
Epoch [10/120    avg_loss:1.201, val_acc:0.818]
Epoch [11/120    avg_loss:1.076, val_acc:0.852]
Epoch [12/120    avg_loss:0.957, val_acc:0.867]
Epoch [13/120    avg_loss:0.855, val_acc:0.881]
Epoch [14/120    avg_loss:0.794, val_acc:0.873]
Epoch [15/120    avg_loss:0.756, val_acc:0.885]
Epoch [16/120    avg_loss:0.669, val_acc:0.873]
Epoch [17/120    avg_loss:0.589, val_acc:0.906]
Epoch [18/120    avg_loss:0.605, val_acc:0.906]
Epoch [19/120    avg_loss:0.567, val_acc:0.877]
Epoch [20/120    avg_loss:0.640, val_acc:0.871]
Epoch [21/120    avg_loss:0.579, val_acc:0.910]
Epoch [22/120    avg_loss:0.523, val_acc:0.918]
Epoch [23/120    avg_loss:0.454, val_acc:0.910]
Epoch [24/120    avg_loss:0.459, val_acc:0.916]
Epoch [25/120    avg_loss:0.429, val_acc:0.893]
Epoch [26/120    avg_loss:0.436, val_acc:0.922]
Epoch [27/120    avg_loss:0.378, val_acc:0.922]
Epoch [28/120    avg_loss:0.428, val_acc:0.896]
Epoch [29/120    avg_loss:0.372, val_acc:0.900]
Epoch [30/120    avg_loss:0.394, val_acc:0.889]
Epoch [31/120    avg_loss:0.338, val_acc:0.908]
Epoch [32/120    avg_loss:0.363, val_acc:0.938]
Epoch [33/120    avg_loss:0.346, val_acc:0.900]
Epoch [34/120    avg_loss:0.329, val_acc:0.930]
Epoch [35/120    avg_loss:0.314, val_acc:0.955]
Epoch [36/120    avg_loss:0.288, val_acc:0.932]
Epoch [37/120    avg_loss:0.304, val_acc:0.949]
Epoch [38/120    avg_loss:0.291, val_acc:0.945]
Epoch [39/120    avg_loss:0.280, val_acc:0.934]
Epoch [40/120    avg_loss:0.268, val_acc:0.941]
Epoch [41/120    avg_loss:0.309, val_acc:0.924]
Epoch [42/120    avg_loss:0.358, val_acc:0.908]
Epoch [43/120    avg_loss:0.302, val_acc:0.945]
Epoch [44/120    avg_loss:0.261, val_acc:0.922]
Epoch [45/120    avg_loss:0.232, val_acc:0.945]
Epoch [46/120    avg_loss:0.236, val_acc:0.924]
Epoch [47/120    avg_loss:0.234, val_acc:0.949]
Epoch [48/120    avg_loss:0.222, val_acc:0.963]
Epoch [49/120    avg_loss:0.211, val_acc:0.889]
Epoch [50/120    avg_loss:0.217, val_acc:0.920]
Epoch [51/120    avg_loss:0.223, val_acc:0.941]
Epoch [52/120    avg_loss:0.178, val_acc:0.936]
Epoch [53/120    avg_loss:0.163, val_acc:0.963]
Epoch [54/120    avg_loss:0.181, val_acc:0.951]
Epoch [55/120    avg_loss:0.197, val_acc:0.945]
Epoch [56/120    avg_loss:0.199, val_acc:0.945]
Epoch [57/120    avg_loss:0.142, val_acc:0.963]
Epoch [58/120    avg_loss:0.166, val_acc:0.953]
Epoch [59/120    avg_loss:0.318, val_acc:0.957]
Epoch [60/120    avg_loss:0.206, val_acc:0.975]
Epoch [61/120    avg_loss:0.250, val_acc:0.963]
Epoch [62/120    avg_loss:0.184, val_acc:0.975]
Epoch [63/120    avg_loss:0.178, val_acc:0.955]
Epoch [64/120    avg_loss:0.121, val_acc:0.977]
Epoch [65/120    avg_loss:0.164, val_acc:0.967]
Epoch [66/120    avg_loss:0.180, val_acc:0.959]
Epoch [67/120    avg_loss:0.166, val_acc:0.975]
Epoch [68/120    avg_loss:0.147, val_acc:0.955]
Epoch [69/120    avg_loss:0.136, val_acc:0.961]
Epoch [70/120    avg_loss:0.104, val_acc:0.963]
Epoch [71/120    avg_loss:0.102, val_acc:0.971]
Epoch [72/120    avg_loss:0.100, val_acc:0.963]
Epoch [73/120    avg_loss:0.083, val_acc:0.984]
Epoch [74/120    avg_loss:0.114, val_acc:0.977]
Epoch [75/120    avg_loss:0.112, val_acc:0.934]
Epoch [76/120    avg_loss:0.112, val_acc:0.967]
Epoch [77/120    avg_loss:0.144, val_acc:0.971]
Epoch [78/120    avg_loss:0.087, val_acc:0.973]
Epoch [79/120    avg_loss:0.101, val_acc:0.969]
Epoch [80/120    avg_loss:0.101, val_acc:0.973]
Epoch [81/120    avg_loss:0.107, val_acc:0.963]
Epoch [82/120    avg_loss:0.093, val_acc:0.979]
Epoch [83/120    avg_loss:0.063, val_acc:0.973]
Epoch [84/120    avg_loss:0.070, val_acc:0.986]
Epoch [85/120    avg_loss:0.055, val_acc:0.977]
Epoch [86/120    avg_loss:0.055, val_acc:0.982]
Epoch [87/120    avg_loss:0.119, val_acc:0.953]
Epoch [88/120    avg_loss:0.185, val_acc:0.939]
Epoch [89/120    avg_loss:0.102, val_acc:0.971]
Epoch [90/120    avg_loss:0.089, val_acc:0.980]
Epoch [91/120    avg_loss:0.070, val_acc:0.975]
Epoch [92/120    avg_loss:0.101, val_acc:0.967]
Epoch [93/120    avg_loss:0.088, val_acc:0.971]
Epoch [94/120    avg_loss:0.110, val_acc:0.971]
Epoch [95/120    avg_loss:0.085, val_acc:0.975]
Epoch [96/120    avg_loss:0.079, val_acc:0.977]
Epoch [97/120    avg_loss:0.069, val_acc:0.971]
Epoch [98/120    avg_loss:0.074, val_acc:0.986]
Epoch [99/120    avg_loss:0.046, val_acc:0.984]
Epoch [100/120    avg_loss:0.040, val_acc:0.986]
Epoch [101/120    avg_loss:0.048, val_acc:0.986]
Epoch [102/120    avg_loss:0.041, val_acc:0.982]
Epoch [103/120    avg_loss:0.038, val_acc:0.982]
Epoch [104/120    avg_loss:0.041, val_acc:0.984]
Epoch [105/120    avg_loss:0.027, val_acc:0.984]
Epoch [106/120    avg_loss:0.042, val_acc:0.984]
Epoch [107/120    avg_loss:0.036, val_acc:0.984]
Epoch [108/120    avg_loss:0.046, val_acc:0.984]
Epoch [109/120    avg_loss:0.037, val_acc:0.984]
Epoch [110/120    avg_loss:0.033, val_acc:0.984]
Epoch [111/120    avg_loss:0.035, val_acc:0.984]
Epoch [112/120    avg_loss:0.032, val_acc:0.984]
Epoch [113/120    avg_loss:0.036, val_acc:0.984]
Epoch [114/120    avg_loss:0.038, val_acc:0.984]
Epoch [115/120    avg_loss:0.042, val_acc:0.984]
Epoch [116/120    avg_loss:0.040, val_acc:0.984]
Epoch [117/120    avg_loss:0.039, val_acc:0.984]
Epoch [118/120    avg_loss:0.029, val_acc:0.984]
Epoch [119/120    avg_loss:0.039, val_acc:0.984]
Epoch [120/120    avg_loss:0.036, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 219   7   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0   0 383   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.37953091684435

F1 scores:
[       nan 0.99854227 0.93693694 0.97550111 0.91176471 0.87272727
 0.99512195 0.87700535 0.98966408 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9819570377651389
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5643386a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.563, val_acc:0.328]
Epoch [2/120    avg_loss:2.401, val_acc:0.338]
Epoch [3/120    avg_loss:2.264, val_acc:0.330]
Epoch [4/120    avg_loss:2.144, val_acc:0.389]
Epoch [5/120    avg_loss:2.014, val_acc:0.496]
Epoch [6/120    avg_loss:1.889, val_acc:0.635]
Epoch [7/120    avg_loss:1.784, val_acc:0.621]
Epoch [8/120    avg_loss:1.661, val_acc:0.586]
Epoch [9/120    avg_loss:1.523, val_acc:0.762]
Epoch [10/120    avg_loss:1.417, val_acc:0.691]
Epoch [11/120    avg_loss:1.281, val_acc:0.686]
Epoch [12/120    avg_loss:1.157, val_acc:0.750]
Epoch [13/120    avg_loss:1.024, val_acc:0.826]
Epoch [14/120    avg_loss:0.944, val_acc:0.812]
Epoch [15/120    avg_loss:0.898, val_acc:0.842]
Epoch [16/120    avg_loss:0.810, val_acc:0.826]
Epoch [17/120    avg_loss:0.724, val_acc:0.885]
Epoch [18/120    avg_loss:0.709, val_acc:0.846]
Epoch [19/120    avg_loss:0.666, val_acc:0.850]
Epoch [20/120    avg_loss:0.711, val_acc:0.893]
Epoch [21/120    avg_loss:0.571, val_acc:0.885]
Epoch [22/120    avg_loss:0.585, val_acc:0.906]
Epoch [23/120    avg_loss:0.534, val_acc:0.887]
Epoch [24/120    avg_loss:0.488, val_acc:0.893]
Epoch [25/120    avg_loss:0.497, val_acc:0.896]
Epoch [26/120    avg_loss:0.517, val_acc:0.863]
Epoch [27/120    avg_loss:0.510, val_acc:0.871]
Epoch [28/120    avg_loss:0.462, val_acc:0.812]
Epoch [29/120    avg_loss:0.429, val_acc:0.885]
Epoch [30/120    avg_loss:0.469, val_acc:0.906]
Epoch [31/120    avg_loss:0.394, val_acc:0.914]
Epoch [32/120    avg_loss:0.417, val_acc:0.920]
Epoch [33/120    avg_loss:0.409, val_acc:0.871]
Epoch [34/120    avg_loss:0.431, val_acc:0.896]
Epoch [35/120    avg_loss:0.432, val_acc:0.924]
Epoch [36/120    avg_loss:0.328, val_acc:0.924]
Epoch [37/120    avg_loss:0.339, val_acc:0.928]
Epoch [38/120    avg_loss:0.301, val_acc:0.916]
Epoch [39/120    avg_loss:0.376, val_acc:0.924]
Epoch [40/120    avg_loss:0.282, val_acc:0.900]
Epoch [41/120    avg_loss:0.324, val_acc:0.957]
Epoch [42/120    avg_loss:0.262, val_acc:0.932]
Epoch [43/120    avg_loss:0.236, val_acc:0.945]
Epoch [44/120    avg_loss:0.255, val_acc:0.945]
Epoch [45/120    avg_loss:0.216, val_acc:0.938]
Epoch [46/120    avg_loss:0.253, val_acc:0.943]
Epoch [47/120    avg_loss:0.209, val_acc:0.930]
Epoch [48/120    avg_loss:0.302, val_acc:0.920]
Epoch [49/120    avg_loss:0.245, val_acc:0.943]
Epoch [50/120    avg_loss:0.324, val_acc:0.941]
Epoch [51/120    avg_loss:0.311, val_acc:0.926]
Epoch [52/120    avg_loss:0.221, val_acc:0.951]
Epoch [53/120    avg_loss:0.193, val_acc:0.926]
Epoch [54/120    avg_loss:0.247, val_acc:0.947]
Epoch [55/120    avg_loss:0.177, val_acc:0.961]
Epoch [56/120    avg_loss:0.161, val_acc:0.961]
Epoch [57/120    avg_loss:0.153, val_acc:0.965]
Epoch [58/120    avg_loss:0.168, val_acc:0.963]
Epoch [59/120    avg_loss:0.146, val_acc:0.965]
Epoch [60/120    avg_loss:0.143, val_acc:0.963]
Epoch [61/120    avg_loss:0.146, val_acc:0.965]
Epoch [62/120    avg_loss:0.146, val_acc:0.963]
Epoch [63/120    avg_loss:0.142, val_acc:0.965]
Epoch [64/120    avg_loss:0.138, val_acc:0.963]
Epoch [65/120    avg_loss:0.127, val_acc:0.965]
Epoch [66/120    avg_loss:0.153, val_acc:0.965]
Epoch [67/120    avg_loss:0.147, val_acc:0.967]
Epoch [68/120    avg_loss:0.141, val_acc:0.969]
Epoch [69/120    avg_loss:0.129, val_acc:0.965]
Epoch [70/120    avg_loss:0.118, val_acc:0.969]
Epoch [71/120    avg_loss:0.120, val_acc:0.969]
Epoch [72/120    avg_loss:0.115, val_acc:0.965]
Epoch [73/120    avg_loss:0.129, val_acc:0.965]
Epoch [74/120    avg_loss:0.129, val_acc:0.967]
Epoch [75/120    avg_loss:0.116, val_acc:0.967]
Epoch [76/120    avg_loss:0.107, val_acc:0.967]
Epoch [77/120    avg_loss:0.125, val_acc:0.965]
Epoch [78/120    avg_loss:0.126, val_acc:0.971]
Epoch [79/120    avg_loss:0.123, val_acc:0.975]
Epoch [80/120    avg_loss:0.129, val_acc:0.967]
Epoch [81/120    avg_loss:0.134, val_acc:0.969]
Epoch [82/120    avg_loss:0.104, val_acc:0.967]
Epoch [83/120    avg_loss:0.111, val_acc:0.965]
Epoch [84/120    avg_loss:0.129, val_acc:0.969]
Epoch [85/120    avg_loss:0.127, val_acc:0.969]
Epoch [86/120    avg_loss:0.094, val_acc:0.971]
Epoch [87/120    avg_loss:0.112, val_acc:0.971]
Epoch [88/120    avg_loss:0.110, val_acc:0.973]
Epoch [89/120    avg_loss:0.108, val_acc:0.971]
Epoch [90/120    avg_loss:0.109, val_acc:0.973]
Epoch [91/120    avg_loss:0.118, val_acc:0.971]
Epoch [92/120    avg_loss:0.106, val_acc:0.971]
Epoch [93/120    avg_loss:0.104, val_acc:0.971]
Epoch [94/120    avg_loss:0.106, val_acc:0.971]
Epoch [95/120    avg_loss:0.104, val_acc:0.969]
Epoch [96/120    avg_loss:0.099, val_acc:0.971]
Epoch [97/120    avg_loss:0.095, val_acc:0.971]
Epoch [98/120    avg_loss:0.089, val_acc:0.969]
Epoch [99/120    avg_loss:0.106, val_acc:0.969]
Epoch [100/120    avg_loss:0.101, val_acc:0.969]
Epoch [101/120    avg_loss:0.108, val_acc:0.969]
Epoch [102/120    avg_loss:0.101, val_acc:0.969]
Epoch [103/120    avg_loss:0.100, val_acc:0.969]
Epoch [104/120    avg_loss:0.096, val_acc:0.969]
Epoch [105/120    avg_loss:0.098, val_acc:0.969]
Epoch [106/120    avg_loss:0.094, val_acc:0.969]
Epoch [107/120    avg_loss:0.087, val_acc:0.969]
Epoch [108/120    avg_loss:0.093, val_acc:0.969]
Epoch [109/120    avg_loss:0.091, val_acc:0.969]
Epoch [110/120    avg_loss:0.090, val_acc:0.969]
Epoch [111/120    avg_loss:0.107, val_acc:0.969]
Epoch [112/120    avg_loss:0.097, val_acc:0.969]
Epoch [113/120    avg_loss:0.103, val_acc:0.969]
Epoch [114/120    avg_loss:0.096, val_acc:0.969]
Epoch [115/120    avg_loss:0.095, val_acc:0.969]
Epoch [116/120    avg_loss:0.107, val_acc:0.969]
Epoch [117/120    avg_loss:0.093, val_acc:0.969]
Epoch [118/120    avg_loss:0.091, val_acc:0.969]
Epoch [119/120    avg_loss:0.100, val_acc:0.969]
Epoch [120/120    avg_loss:0.101, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 206   0   0   0   0  13   0   0   0   0   0   0]
 [  0   0   0 219   4   0   0   0   7   0   0   0   0   0]
 [  0   0   0   0 201  24   0   0   2   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   4   0   0   0   0   0   0   2 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.99573560767591

F1 scores:
[       nan 0.99927061 0.9321267  0.97550111 0.88157895 0.85034014
 0.99756691 0.84324324 0.98853503 1.         1.         0.9973545
 0.99221357 1.        ]

Kappa:
0.9776854068553774
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a08050a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.592, val_acc:0.289]
Epoch [2/120    avg_loss:2.407, val_acc:0.293]
Epoch [3/120    avg_loss:2.274, val_acc:0.428]
Epoch [4/120    avg_loss:2.147, val_acc:0.557]
Epoch [5/120    avg_loss:2.009, val_acc:0.650]
Epoch [6/120    avg_loss:1.865, val_acc:0.680]
Epoch [7/120    avg_loss:1.723, val_acc:0.684]
Epoch [8/120    avg_loss:1.574, val_acc:0.756]
Epoch [9/120    avg_loss:1.411, val_acc:0.719]
Epoch [10/120    avg_loss:1.264, val_acc:0.744]
Epoch [11/120    avg_loss:1.136, val_acc:0.797]
Epoch [12/120    avg_loss:1.043, val_acc:0.801]
Epoch [13/120    avg_loss:0.939, val_acc:0.797]
Epoch [14/120    avg_loss:0.834, val_acc:0.846]
Epoch [15/120    avg_loss:0.745, val_acc:0.861]
Epoch [16/120    avg_loss:0.703, val_acc:0.867]
Epoch [17/120    avg_loss:0.645, val_acc:0.846]
Epoch [18/120    avg_loss:0.625, val_acc:0.875]
Epoch [19/120    avg_loss:0.564, val_acc:0.875]
Epoch [20/120    avg_loss:0.563, val_acc:0.883]
Epoch [21/120    avg_loss:0.595, val_acc:0.889]
Epoch [22/120    avg_loss:0.526, val_acc:0.893]
Epoch [23/120    avg_loss:0.518, val_acc:0.881]
Epoch [24/120    avg_loss:0.448, val_acc:0.900]
Epoch [25/120    avg_loss:0.444, val_acc:0.848]
Epoch [26/120    avg_loss:0.485, val_acc:0.881]
Epoch [27/120    avg_loss:0.404, val_acc:0.930]
Epoch [28/120    avg_loss:0.404, val_acc:0.893]
Epoch [29/120    avg_loss:0.367, val_acc:0.869]
Epoch [30/120    avg_loss:0.370, val_acc:0.926]
Epoch [31/120    avg_loss:0.285, val_acc:0.910]
Epoch [32/120    avg_loss:0.313, val_acc:0.904]
Epoch [33/120    avg_loss:0.313, val_acc:0.916]
Epoch [34/120    avg_loss:0.390, val_acc:0.877]
Epoch [35/120    avg_loss:0.389, val_acc:0.910]
Epoch [36/120    avg_loss:0.301, val_acc:0.922]
Epoch [37/120    avg_loss:0.329, val_acc:0.881]
Epoch [38/120    avg_loss:0.441, val_acc:0.898]
Epoch [39/120    avg_loss:0.354, val_acc:0.904]
Epoch [40/120    avg_loss:0.316, val_acc:0.896]
Epoch [41/120    avg_loss:0.283, val_acc:0.922]
Epoch [42/120    avg_loss:0.252, val_acc:0.930]
Epoch [43/120    avg_loss:0.255, val_acc:0.930]
Epoch [44/120    avg_loss:0.214, val_acc:0.930]
Epoch [45/120    avg_loss:0.230, val_acc:0.928]
Epoch [46/120    avg_loss:0.205, val_acc:0.934]
Epoch [47/120    avg_loss:0.219, val_acc:0.936]
Epoch [48/120    avg_loss:0.217, val_acc:0.928]
Epoch [49/120    avg_loss:0.207, val_acc:0.928]
Epoch [50/120    avg_loss:0.213, val_acc:0.928]
Epoch [51/120    avg_loss:0.219, val_acc:0.932]
Epoch [52/120    avg_loss:0.228, val_acc:0.934]
Epoch [53/120    avg_loss:0.216, val_acc:0.936]
Epoch [54/120    avg_loss:0.198, val_acc:0.934]
Epoch [55/120    avg_loss:0.205, val_acc:0.938]
Epoch [56/120    avg_loss:0.208, val_acc:0.938]
Epoch [57/120    avg_loss:0.183, val_acc:0.941]
Epoch [58/120    avg_loss:0.178, val_acc:0.941]
Epoch [59/120    avg_loss:0.194, val_acc:0.945]
Epoch [60/120    avg_loss:0.202, val_acc:0.941]
Epoch [61/120    avg_loss:0.166, val_acc:0.939]
Epoch [62/120    avg_loss:0.182, val_acc:0.947]
Epoch [63/120    avg_loss:0.173, val_acc:0.947]
Epoch [64/120    avg_loss:0.177, val_acc:0.955]
Epoch [65/120    avg_loss:0.171, val_acc:0.949]
Epoch [66/120    avg_loss:0.165, val_acc:0.947]
Epoch [67/120    avg_loss:0.178, val_acc:0.951]
Epoch [68/120    avg_loss:0.159, val_acc:0.951]
Epoch [69/120    avg_loss:0.174, val_acc:0.955]
Epoch [70/120    avg_loss:0.164, val_acc:0.953]
Epoch [71/120    avg_loss:0.163, val_acc:0.955]
Epoch [72/120    avg_loss:0.153, val_acc:0.955]
Epoch [73/120    avg_loss:0.146, val_acc:0.957]
Epoch [74/120    avg_loss:0.158, val_acc:0.961]
Epoch [75/120    avg_loss:0.147, val_acc:0.963]
Epoch [76/120    avg_loss:0.163, val_acc:0.957]
Epoch [77/120    avg_loss:0.156, val_acc:0.953]
Epoch [78/120    avg_loss:0.144, val_acc:0.957]
Epoch [79/120    avg_loss:0.136, val_acc:0.959]
Epoch [80/120    avg_loss:0.127, val_acc:0.955]
Epoch [81/120    avg_loss:0.137, val_acc:0.957]
Epoch [82/120    avg_loss:0.142, val_acc:0.959]
Epoch [83/120    avg_loss:0.135, val_acc:0.961]
Epoch [84/120    avg_loss:0.148, val_acc:0.945]
Epoch [85/120    avg_loss:0.136, val_acc:0.957]
Epoch [86/120    avg_loss:0.143, val_acc:0.955]
Epoch [87/120    avg_loss:0.119, val_acc:0.963]
Epoch [88/120    avg_loss:0.132, val_acc:0.963]
Epoch [89/120    avg_loss:0.131, val_acc:0.965]
Epoch [90/120    avg_loss:0.134, val_acc:0.959]
Epoch [91/120    avg_loss:0.116, val_acc:0.961]
Epoch [92/120    avg_loss:0.131, val_acc:0.961]
Epoch [93/120    avg_loss:0.159, val_acc:0.957]
Epoch [94/120    avg_loss:0.130, val_acc:0.963]
Epoch [95/120    avg_loss:0.119, val_acc:0.951]
Epoch [96/120    avg_loss:0.129, val_acc:0.959]
Epoch [97/120    avg_loss:0.123, val_acc:0.961]
Epoch [98/120    avg_loss:0.123, val_acc:0.963]
Epoch [99/120    avg_loss:0.113, val_acc:0.963]
Epoch [100/120    avg_loss:0.135, val_acc:0.959]
Epoch [101/120    avg_loss:0.115, val_acc:0.961]
Epoch [102/120    avg_loss:0.113, val_acc:0.965]
Epoch [103/120    avg_loss:0.114, val_acc:0.965]
Epoch [104/120    avg_loss:0.114, val_acc:0.967]
Epoch [105/120    avg_loss:0.127, val_acc:0.957]
Epoch [106/120    avg_loss:0.117, val_acc:0.949]
Epoch [107/120    avg_loss:0.114, val_acc:0.961]
Epoch [108/120    avg_loss:0.117, val_acc:0.963]
Epoch [109/120    avg_loss:0.112, val_acc:0.961]
Epoch [110/120    avg_loss:0.108, val_acc:0.967]
Epoch [111/120    avg_loss:0.111, val_acc:0.965]
Epoch [112/120    avg_loss:0.119, val_acc:0.973]
Epoch [113/120    avg_loss:0.108, val_acc:0.967]
Epoch [114/120    avg_loss:0.095, val_acc:0.967]
Epoch [115/120    avg_loss:0.116, val_acc:0.967]
Epoch [116/120    avg_loss:0.108, val_acc:0.959]
Epoch [117/120    avg_loss:0.113, val_acc:0.959]
Epoch [118/120    avg_loss:0.103, val_acc:0.969]
Epoch [119/120    avg_loss:0.089, val_acc:0.965]
Epoch [120/120    avg_loss:0.115, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 201   0   0   0   0  18   0   0   0   0   0   0]
 [  0   0   0 222   7   0   0   0   0   1   0   0   0   0]
 [  0   0   1   0 194  31   0   0   0   0   1   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 372   5   0]
 [  0   0   0   0   0   0   0   0   0   0   3   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.08102345415779

F1 scores:
[       nan 1.         0.92626728 0.98230088 0.88584475 0.8681672
 1.         0.83937824 1.         0.99893276 0.99453552 0.99332443
 0.99118943 1.        ]

Kappa:
0.9786377104484116
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb249d07b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.533, val_acc:0.310]
Epoch [2/120    avg_loss:2.382, val_acc:0.310]
Epoch [3/120    avg_loss:2.271, val_acc:0.345]
Epoch [4/120    avg_loss:2.165, val_acc:0.387]
Epoch [5/120    avg_loss:2.040, val_acc:0.506]
Epoch [6/120    avg_loss:1.947, val_acc:0.544]
Epoch [7/120    avg_loss:1.809, val_acc:0.581]
Epoch [8/120    avg_loss:1.684, val_acc:0.651]
Epoch [9/120    avg_loss:1.565, val_acc:0.690]
Epoch [10/120    avg_loss:1.417, val_acc:0.690]
Epoch [11/120    avg_loss:1.301, val_acc:0.758]
Epoch [12/120    avg_loss:1.164, val_acc:0.730]
Epoch [13/120    avg_loss:1.038, val_acc:0.794]
Epoch [14/120    avg_loss:0.933, val_acc:0.778]
Epoch [15/120    avg_loss:0.870, val_acc:0.762]
Epoch [16/120    avg_loss:0.815, val_acc:0.853]
Epoch [17/120    avg_loss:0.755, val_acc:0.869]
Epoch [18/120    avg_loss:0.670, val_acc:0.845]
Epoch [19/120    avg_loss:0.649, val_acc:0.827]
Epoch [20/120    avg_loss:0.636, val_acc:0.883]
Epoch [21/120    avg_loss:0.595, val_acc:0.881]
Epoch [22/120    avg_loss:0.524, val_acc:0.821]
Epoch [23/120    avg_loss:0.529, val_acc:0.895]
Epoch [24/120    avg_loss:0.539, val_acc:0.887]
Epoch [25/120    avg_loss:0.539, val_acc:0.881]
Epoch [26/120    avg_loss:0.457, val_acc:0.879]
Epoch [27/120    avg_loss:0.488, val_acc:0.903]
Epoch [28/120    avg_loss:0.438, val_acc:0.915]
Epoch [29/120    avg_loss:0.370, val_acc:0.931]
Epoch [30/120    avg_loss:0.396, val_acc:0.903]
Epoch [31/120    avg_loss:0.372, val_acc:0.917]
Epoch [32/120    avg_loss:0.398, val_acc:0.917]
Epoch [33/120    avg_loss:0.414, val_acc:0.913]
Epoch [34/120    avg_loss:0.335, val_acc:0.913]
Epoch [35/120    avg_loss:0.326, val_acc:0.915]
Epoch [36/120    avg_loss:0.338, val_acc:0.903]
Epoch [37/120    avg_loss:0.329, val_acc:0.903]
Epoch [38/120    avg_loss:0.326, val_acc:0.931]
Epoch [39/120    avg_loss:0.333, val_acc:0.931]
Epoch [40/120    avg_loss:0.311, val_acc:0.917]
Epoch [41/120    avg_loss:0.294, val_acc:0.950]
Epoch [42/120    avg_loss:0.288, val_acc:0.944]
Epoch [43/120    avg_loss:0.252, val_acc:0.933]
Epoch [44/120    avg_loss:0.270, val_acc:0.927]
Epoch [45/120    avg_loss:0.293, val_acc:0.946]
Epoch [46/120    avg_loss:0.218, val_acc:0.962]
Epoch [47/120    avg_loss:0.225, val_acc:0.933]
Epoch [48/120    avg_loss:0.303, val_acc:0.927]
Epoch [49/120    avg_loss:0.238, val_acc:0.946]
Epoch [50/120    avg_loss:0.241, val_acc:0.909]
Epoch [51/120    avg_loss:0.338, val_acc:0.950]
Epoch [52/120    avg_loss:0.242, val_acc:0.929]
Epoch [53/120    avg_loss:0.239, val_acc:0.946]
Epoch [54/120    avg_loss:0.217, val_acc:0.944]
Epoch [55/120    avg_loss:0.196, val_acc:0.946]
Epoch [56/120    avg_loss:0.161, val_acc:0.946]
Epoch [57/120    avg_loss:0.172, val_acc:0.952]
Epoch [58/120    avg_loss:0.170, val_acc:0.956]
Epoch [59/120    avg_loss:0.199, val_acc:0.895]
Epoch [60/120    avg_loss:0.242, val_acc:0.958]
Epoch [61/120    avg_loss:0.126, val_acc:0.960]
Epoch [62/120    avg_loss:0.110, val_acc:0.964]
Epoch [63/120    avg_loss:0.109, val_acc:0.966]
Epoch [64/120    avg_loss:0.111, val_acc:0.966]
Epoch [65/120    avg_loss:0.106, val_acc:0.962]
Epoch [66/120    avg_loss:0.121, val_acc:0.968]
Epoch [67/120    avg_loss:0.104, val_acc:0.968]
Epoch [68/120    avg_loss:0.116, val_acc:0.968]
Epoch [69/120    avg_loss:0.102, val_acc:0.966]
Epoch [70/120    avg_loss:0.096, val_acc:0.968]
Epoch [71/120    avg_loss:0.091, val_acc:0.968]
Epoch [72/120    avg_loss:0.113, val_acc:0.966]
Epoch [73/120    avg_loss:0.097, val_acc:0.970]
Epoch [74/120    avg_loss:0.091, val_acc:0.968]
Epoch [75/120    avg_loss:0.100, val_acc:0.968]
Epoch [76/120    avg_loss:0.097, val_acc:0.970]
Epoch [77/120    avg_loss:0.104, val_acc:0.972]
Epoch [78/120    avg_loss:0.099, val_acc:0.968]
Epoch [79/120    avg_loss:0.096, val_acc:0.970]
Epoch [80/120    avg_loss:0.089, val_acc:0.970]
Epoch [81/120    avg_loss:0.092, val_acc:0.968]
Epoch [82/120    avg_loss:0.092, val_acc:0.970]
Epoch [83/120    avg_loss:0.080, val_acc:0.970]
Epoch [84/120    avg_loss:0.079, val_acc:0.968]
Epoch [85/120    avg_loss:0.096, val_acc:0.968]
Epoch [86/120    avg_loss:0.083, val_acc:0.966]
Epoch [87/120    avg_loss:0.079, val_acc:0.968]
Epoch [88/120    avg_loss:0.093, val_acc:0.968]
Epoch [89/120    avg_loss:0.093, val_acc:0.972]
Epoch [90/120    avg_loss:0.099, val_acc:0.972]
Epoch [91/120    avg_loss:0.084, val_acc:0.972]
Epoch [92/120    avg_loss:0.081, val_acc:0.970]
Epoch [93/120    avg_loss:0.067, val_acc:0.968]
Epoch [94/120    avg_loss:0.087, val_acc:0.972]
Epoch [95/120    avg_loss:0.081, val_acc:0.968]
Epoch [96/120    avg_loss:0.084, val_acc:0.972]
Epoch [97/120    avg_loss:0.090, val_acc:0.972]
Epoch [98/120    avg_loss:0.074, val_acc:0.970]
Epoch [99/120    avg_loss:0.076, val_acc:0.972]
Epoch [100/120    avg_loss:0.074, val_acc:0.972]
Epoch [101/120    avg_loss:0.092, val_acc:0.972]
Epoch [102/120    avg_loss:0.079, val_acc:0.974]
Epoch [103/120    avg_loss:0.073, val_acc:0.968]
Epoch [104/120    avg_loss:0.081, val_acc:0.972]
Epoch [105/120    avg_loss:0.081, val_acc:0.974]
Epoch [106/120    avg_loss:0.094, val_acc:0.974]
Epoch [107/120    avg_loss:0.089, val_acc:0.970]
Epoch [108/120    avg_loss:0.073, val_acc:0.974]
Epoch [109/120    avg_loss:0.078, val_acc:0.976]
Epoch [110/120    avg_loss:0.075, val_acc:0.976]
Epoch [111/120    avg_loss:0.074, val_acc:0.974]
Epoch [112/120    avg_loss:0.085, val_acc:0.974]
Epoch [113/120    avg_loss:0.075, val_acc:0.974]
Epoch [114/120    avg_loss:0.077, val_acc:0.974]
Epoch [115/120    avg_loss:0.077, val_acc:0.974]
Epoch [116/120    avg_loss:0.072, val_acc:0.974]
Epoch [117/120    avg_loss:0.076, val_acc:0.974]
Epoch [118/120    avg_loss:0.082, val_acc:0.974]
Epoch [119/120    avg_loss:0.072, val_acc:0.974]
Epoch [120/120    avg_loss:0.074, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   2 220   2   0   0   0   6   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   6   0   0   2   0 198   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   1   0   0   0   0   0   0   0   0   0  12 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.14498933901919

F1 scores:
[       nan 0.99491649 0.95302013 0.97777778 0.91067538 0.87197232
 0.98019802 0.89502762 0.99232737 1.         1.         0.9843342
 0.98544233 1.        ]

Kappa:
0.9793435132288236
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4ac69fba58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.574, val_acc:0.516]
Epoch [2/120    avg_loss:2.393, val_acc:0.412]
Epoch [3/120    avg_loss:2.260, val_acc:0.418]
Epoch [4/120    avg_loss:2.156, val_acc:0.537]
Epoch [5/120    avg_loss:2.059, val_acc:0.572]
Epoch [6/120    avg_loss:1.918, val_acc:0.586]
Epoch [7/120    avg_loss:1.785, val_acc:0.596]
Epoch [8/120    avg_loss:1.656, val_acc:0.604]
Epoch [9/120    avg_loss:1.535, val_acc:0.666]
Epoch [10/120    avg_loss:1.392, val_acc:0.721]
Epoch [11/120    avg_loss:1.275, val_acc:0.729]
Epoch [12/120    avg_loss:1.159, val_acc:0.758]
Epoch [13/120    avg_loss:1.062, val_acc:0.799]
Epoch [14/120    avg_loss:0.996, val_acc:0.814]
Epoch [15/120    avg_loss:0.915, val_acc:0.809]
Epoch [16/120    avg_loss:0.828, val_acc:0.850]
Epoch [17/120    avg_loss:0.773, val_acc:0.875]
Epoch [18/120    avg_loss:0.733, val_acc:0.873]
Epoch [19/120    avg_loss:0.638, val_acc:0.783]
Epoch [20/120    avg_loss:0.613, val_acc:0.865]
Epoch [21/120    avg_loss:0.557, val_acc:0.896]
Epoch [22/120    avg_loss:0.552, val_acc:0.873]
Epoch [23/120    avg_loss:0.537, val_acc:0.891]
Epoch [24/120    avg_loss:0.526, val_acc:0.889]
Epoch [25/120    avg_loss:0.524, val_acc:0.867]
Epoch [26/120    avg_loss:0.467, val_acc:0.912]
Epoch [27/120    avg_loss:0.461, val_acc:0.912]
Epoch [28/120    avg_loss:0.417, val_acc:0.916]
Epoch [29/120    avg_loss:0.433, val_acc:0.889]
Epoch [30/120    avg_loss:0.347, val_acc:0.932]
Epoch [31/120    avg_loss:0.362, val_acc:0.932]
Epoch [32/120    avg_loss:0.383, val_acc:0.930]
Epoch [33/120    avg_loss:0.303, val_acc:0.945]
Epoch [34/120    avg_loss:0.317, val_acc:0.930]
Epoch [35/120    avg_loss:0.272, val_acc:0.938]
Epoch [36/120    avg_loss:0.236, val_acc:0.932]
Epoch [37/120    avg_loss:0.256, val_acc:0.949]
Epoch [38/120    avg_loss:0.227, val_acc:0.918]
Epoch [39/120    avg_loss:0.220, val_acc:0.947]
Epoch [40/120    avg_loss:0.228, val_acc:0.943]
Epoch [41/120    avg_loss:0.274, val_acc:0.947]
Epoch [42/120    avg_loss:0.274, val_acc:0.939]
Epoch [43/120    avg_loss:0.202, val_acc:0.953]
Epoch [44/120    avg_loss:0.220, val_acc:0.957]
Epoch [45/120    avg_loss:0.219, val_acc:0.938]
Epoch [46/120    avg_loss:0.225, val_acc:0.947]
Epoch [47/120    avg_loss:0.244, val_acc:0.957]
Epoch [48/120    avg_loss:0.253, val_acc:0.904]
Epoch [49/120    avg_loss:0.179, val_acc:0.941]
Epoch [50/120    avg_loss:0.270, val_acc:0.924]
Epoch [51/120    avg_loss:0.221, val_acc:0.941]
Epoch [52/120    avg_loss:0.190, val_acc:0.949]
Epoch [53/120    avg_loss:0.162, val_acc:0.959]
Epoch [54/120    avg_loss:0.210, val_acc:0.953]
Epoch [55/120    avg_loss:0.175, val_acc:0.967]
Epoch [56/120    avg_loss:0.159, val_acc:0.943]
Epoch [57/120    avg_loss:0.195, val_acc:0.951]
Epoch [58/120    avg_loss:0.146, val_acc:0.967]
Epoch [59/120    avg_loss:0.122, val_acc:0.969]
Epoch [60/120    avg_loss:0.131, val_acc:0.975]
Epoch [61/120    avg_loss:0.120, val_acc:0.957]
Epoch [62/120    avg_loss:0.148, val_acc:0.963]
Epoch [63/120    avg_loss:0.147, val_acc:0.922]
Epoch [64/120    avg_loss:0.093, val_acc:0.971]
Epoch [65/120    avg_loss:0.134, val_acc:0.963]
Epoch [66/120    avg_loss:0.114, val_acc:0.953]
Epoch [67/120    avg_loss:0.143, val_acc:0.959]
Epoch [68/120    avg_loss:0.134, val_acc:0.961]
Epoch [69/120    avg_loss:0.117, val_acc:0.947]
Epoch [70/120    avg_loss:0.152, val_acc:0.916]
Epoch [71/120    avg_loss:0.127, val_acc:0.945]
Epoch [72/120    avg_loss:0.121, val_acc:0.975]
Epoch [73/120    avg_loss:0.159, val_acc:0.967]
Epoch [74/120    avg_loss:0.143, val_acc:0.969]
Epoch [75/120    avg_loss:0.075, val_acc:0.977]
Epoch [76/120    avg_loss:0.103, val_acc:0.963]
Epoch [77/120    avg_loss:0.084, val_acc:0.977]
Epoch [78/120    avg_loss:0.113, val_acc:0.955]
Epoch [79/120    avg_loss:0.106, val_acc:0.957]
Epoch [80/120    avg_loss:0.077, val_acc:0.975]
Epoch [81/120    avg_loss:0.088, val_acc:0.973]
Epoch [82/120    avg_loss:0.136, val_acc:0.973]
Epoch [83/120    avg_loss:0.099, val_acc:0.957]
Epoch [84/120    avg_loss:0.097, val_acc:0.975]
Epoch [85/120    avg_loss:0.109, val_acc:0.973]
Epoch [86/120    avg_loss:0.092, val_acc:0.963]
Epoch [87/120    avg_loss:0.075, val_acc:0.977]
Epoch [88/120    avg_loss:0.057, val_acc:0.975]
Epoch [89/120    avg_loss:0.044, val_acc:0.975]
Epoch [90/120    avg_loss:0.034, val_acc:0.982]
Epoch [91/120    avg_loss:0.046, val_acc:0.969]
Epoch [92/120    avg_loss:0.067, val_acc:0.969]
Epoch [93/120    avg_loss:0.048, val_acc:0.982]
Epoch [94/120    avg_loss:0.033, val_acc:0.988]
Epoch [95/120    avg_loss:0.044, val_acc:0.984]
Epoch [96/120    avg_loss:0.033, val_acc:0.988]
Epoch [97/120    avg_loss:0.037, val_acc:0.977]
Epoch [98/120    avg_loss:0.042, val_acc:0.986]
Epoch [99/120    avg_loss:0.032, val_acc:0.988]
Epoch [100/120    avg_loss:0.026, val_acc:0.975]
Epoch [101/120    avg_loss:0.043, val_acc:0.973]
Epoch [102/120    avg_loss:0.067, val_acc:0.977]
Epoch [103/120    avg_loss:0.059, val_acc:0.980]
Epoch [104/120    avg_loss:0.044, val_acc:0.984]
Epoch [105/120    avg_loss:0.034, val_acc:0.984]
Epoch [106/120    avg_loss:0.066, val_acc:0.980]
Epoch [107/120    avg_loss:0.030, val_acc:0.986]
Epoch [108/120    avg_loss:0.027, val_acc:0.988]
Epoch [109/120    avg_loss:0.049, val_acc:0.982]
Epoch [110/120    avg_loss:0.094, val_acc:0.865]
Epoch [111/120    avg_loss:0.298, val_acc:0.961]
Epoch [112/120    avg_loss:0.182, val_acc:0.969]
Epoch [113/120    avg_loss:0.107, val_acc:0.967]
Epoch [114/120    avg_loss:0.062, val_acc:0.963]
Epoch [115/120    avg_loss:0.060, val_acc:0.969]
Epoch [116/120    avg_loss:0.042, val_acc:0.984]
Epoch [117/120    avg_loss:0.031, val_acc:0.986]
Epoch [118/120    avg_loss:0.047, val_acc:0.973]
Epoch [119/120    avg_loss:0.048, val_acc:0.986]
Epoch [120/120    avg_loss:0.035, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 204   0   0   0   0  15   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   7 218   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.96226415 0.98283262 0.93562232 0.91512915
 1.         0.92610837 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9890806227517125
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14eed69ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.587, val_acc:0.312]
Epoch [2/120    avg_loss:2.462, val_acc:0.430]
Epoch [3/120    avg_loss:2.334, val_acc:0.516]
Epoch [4/120    avg_loss:2.202, val_acc:0.516]
Epoch [5/120    avg_loss:2.049, val_acc:0.574]
Epoch [6/120    avg_loss:1.889, val_acc:0.592]
Epoch [7/120    avg_loss:1.774, val_acc:0.670]
Epoch [8/120    avg_loss:1.640, val_acc:0.709]
Epoch [9/120    avg_loss:1.469, val_acc:0.773]
Epoch [10/120    avg_loss:1.323, val_acc:0.791]
Epoch [11/120    avg_loss:1.156, val_acc:0.842]
Epoch [12/120    avg_loss:1.053, val_acc:0.830]
Epoch [13/120    avg_loss:0.914, val_acc:0.869]
Epoch [14/120    avg_loss:0.860, val_acc:0.871]
Epoch [15/120    avg_loss:0.783, val_acc:0.883]
Epoch [16/120    avg_loss:0.726, val_acc:0.828]
Epoch [17/120    avg_loss:0.655, val_acc:0.875]
Epoch [18/120    avg_loss:0.680, val_acc:0.873]
Epoch [19/120    avg_loss:0.592, val_acc:0.840]
Epoch [20/120    avg_loss:0.571, val_acc:0.896]
Epoch [21/120    avg_loss:0.551, val_acc:0.863]
Epoch [22/120    avg_loss:0.519, val_acc:0.896]
Epoch [23/120    avg_loss:0.492, val_acc:0.924]
Epoch [24/120    avg_loss:0.479, val_acc:0.906]
Epoch [25/120    avg_loss:0.448, val_acc:0.898]
Epoch [26/120    avg_loss:0.418, val_acc:0.912]
Epoch [27/120    avg_loss:0.436, val_acc:0.879]
Epoch [28/120    avg_loss:0.379, val_acc:0.908]
Epoch [29/120    avg_loss:0.342, val_acc:0.930]
Epoch [30/120    avg_loss:0.355, val_acc:0.908]
Epoch [31/120    avg_loss:0.365, val_acc:0.918]
Epoch [32/120    avg_loss:0.355, val_acc:0.902]
Epoch [33/120    avg_loss:0.310, val_acc:0.930]
Epoch [34/120    avg_loss:0.332, val_acc:0.916]
Epoch [35/120    avg_loss:0.277, val_acc:0.939]
Epoch [36/120    avg_loss:0.274, val_acc:0.914]
Epoch [37/120    avg_loss:0.270, val_acc:0.916]
Epoch [38/120    avg_loss:0.283, val_acc:0.936]
Epoch [39/120    avg_loss:0.309, val_acc:0.928]
Epoch [40/120    avg_loss:0.275, val_acc:0.934]
Epoch [41/120    avg_loss:0.223, val_acc:0.957]
Epoch [42/120    avg_loss:0.223, val_acc:0.943]
Epoch [43/120    avg_loss:0.319, val_acc:0.926]
Epoch [44/120    avg_loss:0.267, val_acc:0.939]
Epoch [45/120    avg_loss:0.205, val_acc:0.938]
Epoch [46/120    avg_loss:0.256, val_acc:0.930]
Epoch [47/120    avg_loss:0.237, val_acc:0.955]
Epoch [48/120    avg_loss:0.205, val_acc:0.949]
Epoch [49/120    avg_loss:0.265, val_acc:0.955]
Epoch [50/120    avg_loss:0.201, val_acc:0.949]
Epoch [51/120    avg_loss:0.180, val_acc:0.936]
Epoch [52/120    avg_loss:0.233, val_acc:0.943]
Epoch [53/120    avg_loss:0.193, val_acc:0.947]
Epoch [54/120    avg_loss:0.175, val_acc:0.945]
Epoch [55/120    avg_loss:0.139, val_acc:0.957]
Epoch [56/120    avg_loss:0.129, val_acc:0.967]
Epoch [57/120    avg_loss:0.113, val_acc:0.969]
Epoch [58/120    avg_loss:0.117, val_acc:0.967]
Epoch [59/120    avg_loss:0.121, val_acc:0.971]
Epoch [60/120    avg_loss:0.123, val_acc:0.973]
Epoch [61/120    avg_loss:0.129, val_acc:0.973]
Epoch [62/120    avg_loss:0.122, val_acc:0.973]
Epoch [63/120    avg_loss:0.114, val_acc:0.965]
Epoch [64/120    avg_loss:0.095, val_acc:0.971]
Epoch [65/120    avg_loss:0.118, val_acc:0.975]
Epoch [66/120    avg_loss:0.124, val_acc:0.967]
Epoch [67/120    avg_loss:0.099, val_acc:0.969]
Epoch [68/120    avg_loss:0.103, val_acc:0.967]
Epoch [69/120    avg_loss:0.113, val_acc:0.971]
Epoch [70/120    avg_loss:0.091, val_acc:0.971]
Epoch [71/120    avg_loss:0.107, val_acc:0.969]
Epoch [72/120    avg_loss:0.118, val_acc:0.971]
Epoch [73/120    avg_loss:0.103, val_acc:0.967]
Epoch [74/120    avg_loss:0.094, val_acc:0.971]
Epoch [75/120    avg_loss:0.084, val_acc:0.971]
Epoch [76/120    avg_loss:0.117, val_acc:0.967]
Epoch [77/120    avg_loss:0.099, val_acc:0.967]
Epoch [78/120    avg_loss:0.097, val_acc:0.971]
Epoch [79/120    avg_loss:0.093, val_acc:0.971]
Epoch [80/120    avg_loss:0.109, val_acc:0.971]
Epoch [81/120    avg_loss:0.082, val_acc:0.971]
Epoch [82/120    avg_loss:0.081, val_acc:0.971]
Epoch [83/120    avg_loss:0.091, val_acc:0.971]
Epoch [84/120    avg_loss:0.087, val_acc:0.971]
Epoch [85/120    avg_loss:0.083, val_acc:0.971]
Epoch [86/120    avg_loss:0.093, val_acc:0.971]
Epoch [87/120    avg_loss:0.102, val_acc:0.971]
Epoch [88/120    avg_loss:0.096, val_acc:0.971]
Epoch [89/120    avg_loss:0.108, val_acc:0.971]
Epoch [90/120    avg_loss:0.109, val_acc:0.971]
Epoch [91/120    avg_loss:0.092, val_acc:0.971]
Epoch [92/120    avg_loss:0.084, val_acc:0.971]
Epoch [93/120    avg_loss:0.108, val_acc:0.971]
Epoch [94/120    avg_loss:0.100, val_acc:0.971]
Epoch [95/120    avg_loss:0.090, val_acc:0.971]
Epoch [96/120    avg_loss:0.085, val_acc:0.971]
Epoch [97/120    avg_loss:0.088, val_acc:0.971]
Epoch [98/120    avg_loss:0.096, val_acc:0.971]
Epoch [99/120    avg_loss:0.092, val_acc:0.971]
Epoch [100/120    avg_loss:0.092, val_acc:0.971]
Epoch [101/120    avg_loss:0.087, val_acc:0.971]
Epoch [102/120    avg_loss:0.101, val_acc:0.971]
Epoch [103/120    avg_loss:0.100, val_acc:0.971]
Epoch [104/120    avg_loss:0.088, val_acc:0.971]
Epoch [105/120    avg_loss:0.095, val_acc:0.971]
Epoch [106/120    avg_loss:0.098, val_acc:0.971]
Epoch [107/120    avg_loss:0.095, val_acc:0.971]
Epoch [108/120    avg_loss:0.089, val_acc:0.971]
Epoch [109/120    avg_loss:0.095, val_acc:0.971]
Epoch [110/120    avg_loss:0.089, val_acc:0.971]
Epoch [111/120    avg_loss:0.094, val_acc:0.971]
Epoch [112/120    avg_loss:0.093, val_acc:0.971]
Epoch [113/120    avg_loss:0.086, val_acc:0.971]
Epoch [114/120    avg_loss:0.079, val_acc:0.971]
Epoch [115/120    avg_loss:0.094, val_acc:0.971]
Epoch [116/120    avg_loss:0.089, val_acc:0.971]
Epoch [117/120    avg_loss:0.090, val_acc:0.971]
Epoch [118/120    avg_loss:0.095, val_acc:0.971]
Epoch [119/120    avg_loss:0.101, val_acc:0.971]
Epoch [120/120    avg_loss:0.103, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   3 221   3   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 1.         0.96263736 0.98004435 0.88695652 0.82926829
 1.         0.91954023 1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9829062269262158
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f944905f9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.550, val_acc:0.321]
Epoch [2/120    avg_loss:2.390, val_acc:0.319]
Epoch [3/120    avg_loss:2.267, val_acc:0.333]
Epoch [4/120    avg_loss:2.161, val_acc:0.421]
Epoch [5/120    avg_loss:2.073, val_acc:0.480]
Epoch [6/120    avg_loss:1.966, val_acc:0.546]
Epoch [7/120    avg_loss:1.838, val_acc:0.583]
Epoch [8/120    avg_loss:1.729, val_acc:0.647]
Epoch [9/120    avg_loss:1.573, val_acc:0.692]
Epoch [10/120    avg_loss:1.440, val_acc:0.732]
Epoch [11/120    avg_loss:1.270, val_acc:0.728]
Epoch [12/120    avg_loss:1.166, val_acc:0.726]
Epoch [13/120    avg_loss:1.050, val_acc:0.748]
Epoch [14/120    avg_loss:0.973, val_acc:0.752]
Epoch [15/120    avg_loss:0.856, val_acc:0.756]
Epoch [16/120    avg_loss:0.781, val_acc:0.817]
Epoch [17/120    avg_loss:0.750, val_acc:0.812]
Epoch [18/120    avg_loss:0.729, val_acc:0.845]
Epoch [19/120    avg_loss:0.726, val_acc:0.780]
Epoch [20/120    avg_loss:0.656, val_acc:0.819]
Epoch [21/120    avg_loss:0.605, val_acc:0.909]
Epoch [22/120    avg_loss:0.645, val_acc:0.885]
Epoch [23/120    avg_loss:0.581, val_acc:0.859]
Epoch [24/120    avg_loss:0.550, val_acc:0.861]
Epoch [25/120    avg_loss:0.500, val_acc:0.879]
Epoch [26/120    avg_loss:0.494, val_acc:0.885]
Epoch [27/120    avg_loss:0.497, val_acc:0.907]
Epoch [28/120    avg_loss:0.463, val_acc:0.891]
Epoch [29/120    avg_loss:0.476, val_acc:0.913]
Epoch [30/120    avg_loss:0.463, val_acc:0.905]
Epoch [31/120    avg_loss:0.441, val_acc:0.899]
Epoch [32/120    avg_loss:0.432, val_acc:0.871]
Epoch [33/120    avg_loss:0.394, val_acc:0.921]
Epoch [34/120    avg_loss:0.395, val_acc:0.867]
Epoch [35/120    avg_loss:0.400, val_acc:0.925]
Epoch [36/120    avg_loss:0.364, val_acc:0.903]
Epoch [37/120    avg_loss:0.355, val_acc:0.944]
Epoch [38/120    avg_loss:0.351, val_acc:0.929]
Epoch [39/120    avg_loss:0.370, val_acc:0.944]
Epoch [40/120    avg_loss:0.370, val_acc:0.931]
Epoch [41/120    avg_loss:0.370, val_acc:0.931]
Epoch [42/120    avg_loss:0.301, val_acc:0.942]
Epoch [43/120    avg_loss:0.363, val_acc:0.905]
Epoch [44/120    avg_loss:0.331, val_acc:0.940]
Epoch [45/120    avg_loss:0.284, val_acc:0.929]
Epoch [46/120    avg_loss:0.283, val_acc:0.948]
Epoch [47/120    avg_loss:0.222, val_acc:0.950]
Epoch [48/120    avg_loss:0.228, val_acc:0.950]
Epoch [49/120    avg_loss:0.240, val_acc:0.948]
Epoch [50/120    avg_loss:0.233, val_acc:0.958]
Epoch [51/120    avg_loss:0.224, val_acc:0.907]
Epoch [52/120    avg_loss:0.198, val_acc:0.954]
Epoch [53/120    avg_loss:0.207, val_acc:0.933]
Epoch [54/120    avg_loss:0.266, val_acc:0.933]
Epoch [55/120    avg_loss:0.224, val_acc:0.950]
Epoch [56/120    avg_loss:0.234, val_acc:0.940]
Epoch [57/120    avg_loss:0.208, val_acc:0.913]
Epoch [58/120    avg_loss:0.241, val_acc:0.966]
Epoch [59/120    avg_loss:0.192, val_acc:0.952]
Epoch [60/120    avg_loss:0.175, val_acc:0.915]
Epoch [61/120    avg_loss:0.196, val_acc:0.950]
Epoch [62/120    avg_loss:0.150, val_acc:0.960]
Epoch [63/120    avg_loss:0.156, val_acc:0.940]
Epoch [64/120    avg_loss:0.158, val_acc:0.933]
Epoch [65/120    avg_loss:0.157, val_acc:0.964]
Epoch [66/120    avg_loss:0.229, val_acc:0.968]
Epoch [67/120    avg_loss:0.180, val_acc:0.966]
Epoch [68/120    avg_loss:0.109, val_acc:0.960]
Epoch [69/120    avg_loss:0.199, val_acc:0.942]
Epoch [70/120    avg_loss:0.164, val_acc:0.962]
Epoch [71/120    avg_loss:0.145, val_acc:0.966]
Epoch [72/120    avg_loss:0.104, val_acc:0.962]
Epoch [73/120    avg_loss:0.110, val_acc:0.964]
Epoch [74/120    avg_loss:0.127, val_acc:0.954]
Epoch [75/120    avg_loss:0.129, val_acc:0.974]
Epoch [76/120    avg_loss:0.108, val_acc:0.966]
Epoch [77/120    avg_loss:0.069, val_acc:0.978]
Epoch [78/120    avg_loss:0.094, val_acc:0.960]
Epoch [79/120    avg_loss:0.093, val_acc:0.976]
Epoch [80/120    avg_loss:0.084, val_acc:0.978]
Epoch [81/120    avg_loss:0.094, val_acc:0.978]
Epoch [82/120    avg_loss:0.083, val_acc:0.970]
Epoch [83/120    avg_loss:0.110, val_acc:0.960]
Epoch [84/120    avg_loss:0.111, val_acc:0.958]
Epoch [85/120    avg_loss:0.098, val_acc:0.980]
Epoch [86/120    avg_loss:0.097, val_acc:0.966]
Epoch [87/120    avg_loss:0.134, val_acc:0.905]
Epoch [88/120    avg_loss:0.128, val_acc:0.968]
Epoch [89/120    avg_loss:0.075, val_acc:0.978]
Epoch [90/120    avg_loss:0.099, val_acc:0.942]
Epoch [91/120    avg_loss:0.097, val_acc:0.986]
Epoch [92/120    avg_loss:0.060, val_acc:0.982]
Epoch [93/120    avg_loss:0.083, val_acc:0.966]
Epoch [94/120    avg_loss:0.091, val_acc:0.980]
Epoch [95/120    avg_loss:0.058, val_acc:0.976]
Epoch [96/120    avg_loss:0.065, val_acc:0.972]
Epoch [97/120    avg_loss:0.080, val_acc:0.976]
Epoch [98/120    avg_loss:0.039, val_acc:0.982]
Epoch [99/120    avg_loss:0.043, val_acc:0.984]
Epoch [100/120    avg_loss:0.100, val_acc:0.972]
Epoch [101/120    avg_loss:0.051, val_acc:0.980]
Epoch [102/120    avg_loss:0.070, val_acc:0.976]
Epoch [103/120    avg_loss:0.062, val_acc:0.980]
Epoch [104/120    avg_loss:0.040, val_acc:0.982]
Epoch [105/120    avg_loss:0.030, val_acc:0.982]
Epoch [106/120    avg_loss:0.028, val_acc:0.986]
Epoch [107/120    avg_loss:0.032, val_acc:0.984]
Epoch [108/120    avg_loss:0.028, val_acc:0.984]
Epoch [109/120    avg_loss:0.029, val_acc:0.984]
Epoch [110/120    avg_loss:0.025, val_acc:0.984]
Epoch [111/120    avg_loss:0.023, val_acc:0.984]
Epoch [112/120    avg_loss:0.032, val_acc:0.982]
Epoch [113/120    avg_loss:0.021, val_acc:0.984]
Epoch [114/120    avg_loss:0.024, val_acc:0.984]
Epoch [115/120    avg_loss:0.023, val_acc:0.984]
Epoch [116/120    avg_loss:0.023, val_acc:0.982]
Epoch [117/120    avg_loss:0.021, val_acc:0.982]
Epoch [118/120    avg_loss:0.023, val_acc:0.984]
Epoch [119/120    avg_loss:0.020, val_acc:0.984]
Epoch [120/120    avg_loss:0.022, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   2   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   1   0 376   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.99545455 0.98901099 0.92993631 0.88727273
 1.         0.98924731 0.99614891 0.99893276 1.         0.98947368
 0.99221357 1.        ]

Kappa:
0.9890796201181805
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16add1eac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.351]
Epoch [2/120    avg_loss:2.373, val_acc:0.393]
Epoch [3/120    avg_loss:2.246, val_acc:0.409]
Epoch [4/120    avg_loss:2.121, val_acc:0.454]
Epoch [5/120    avg_loss:2.007, val_acc:0.538]
Epoch [6/120    avg_loss:1.889, val_acc:0.571]
Epoch [7/120    avg_loss:1.746, val_acc:0.585]
Epoch [8/120    avg_loss:1.598, val_acc:0.601]
Epoch [9/120    avg_loss:1.465, val_acc:0.637]
Epoch [10/120    avg_loss:1.365, val_acc:0.651]
Epoch [11/120    avg_loss:1.200, val_acc:0.718]
Epoch [12/120    avg_loss:1.106, val_acc:0.808]
Epoch [13/120    avg_loss:1.035, val_acc:0.730]
Epoch [14/120    avg_loss:0.942, val_acc:0.812]
Epoch [15/120    avg_loss:0.852, val_acc:0.867]
Epoch [16/120    avg_loss:0.789, val_acc:0.889]
Epoch [17/120    avg_loss:0.719, val_acc:0.861]
Epoch [18/120    avg_loss:0.646, val_acc:0.893]
Epoch [19/120    avg_loss:0.642, val_acc:0.879]
Epoch [20/120    avg_loss:0.595, val_acc:0.903]
Epoch [21/120    avg_loss:0.559, val_acc:0.907]
Epoch [22/120    avg_loss:0.504, val_acc:0.929]
Epoch [23/120    avg_loss:0.537, val_acc:0.778]
Epoch [24/120    avg_loss:0.554, val_acc:0.863]
Epoch [25/120    avg_loss:0.530, val_acc:0.881]
Epoch [26/120    avg_loss:0.515, val_acc:0.879]
Epoch [27/120    avg_loss:0.549, val_acc:0.899]
Epoch [28/120    avg_loss:0.495, val_acc:0.917]
Epoch [29/120    avg_loss:0.402, val_acc:0.948]
Epoch [30/120    avg_loss:0.390, val_acc:0.944]
Epoch [31/120    avg_loss:0.384, val_acc:0.962]
Epoch [32/120    avg_loss:0.375, val_acc:0.933]
Epoch [33/120    avg_loss:0.348, val_acc:0.927]
Epoch [34/120    avg_loss:0.305, val_acc:0.950]
Epoch [35/120    avg_loss:0.339, val_acc:0.917]
Epoch [36/120    avg_loss:0.307, val_acc:0.919]
Epoch [37/120    avg_loss:0.321, val_acc:0.950]
Epoch [38/120    avg_loss:0.288, val_acc:0.927]
Epoch [39/120    avg_loss:0.310, val_acc:0.942]
Epoch [40/120    avg_loss:0.248, val_acc:0.966]
Epoch [41/120    avg_loss:0.261, val_acc:0.972]
Epoch [42/120    avg_loss:0.240, val_acc:0.942]
Epoch [43/120    avg_loss:0.337, val_acc:0.966]
Epoch [44/120    avg_loss:0.215, val_acc:0.966]
Epoch [45/120    avg_loss:0.258, val_acc:0.970]
Epoch [46/120    avg_loss:0.287, val_acc:0.954]
Epoch [47/120    avg_loss:0.232, val_acc:0.950]
Epoch [48/120    avg_loss:0.255, val_acc:0.956]
Epoch [49/120    avg_loss:0.257, val_acc:0.950]
Epoch [50/120    avg_loss:0.243, val_acc:0.933]
Epoch [51/120    avg_loss:0.187, val_acc:0.970]
Epoch [52/120    avg_loss:0.178, val_acc:0.984]
Epoch [53/120    avg_loss:0.185, val_acc:0.962]
Epoch [54/120    avg_loss:0.184, val_acc:0.978]
Epoch [55/120    avg_loss:0.159, val_acc:0.982]
Epoch [56/120    avg_loss:0.149, val_acc:0.952]
Epoch [57/120    avg_loss:0.204, val_acc:0.966]
Epoch [58/120    avg_loss:0.171, val_acc:0.978]
Epoch [59/120    avg_loss:0.131, val_acc:0.958]
Epoch [60/120    avg_loss:0.117, val_acc:0.982]
Epoch [61/120    avg_loss:0.120, val_acc:0.988]
Epoch [62/120    avg_loss:0.106, val_acc:0.980]
Epoch [63/120    avg_loss:0.105, val_acc:0.978]
Epoch [64/120    avg_loss:0.105, val_acc:0.931]
Epoch [65/120    avg_loss:0.176, val_acc:0.956]
Epoch [66/120    avg_loss:0.129, val_acc:0.980]
Epoch [67/120    avg_loss:0.102, val_acc:0.990]
Epoch [68/120    avg_loss:0.078, val_acc:0.986]
Epoch [69/120    avg_loss:0.072, val_acc:0.998]
Epoch [70/120    avg_loss:0.075, val_acc:0.990]
Epoch [71/120    avg_loss:0.112, val_acc:0.996]
Epoch [72/120    avg_loss:0.140, val_acc:0.974]
Epoch [73/120    avg_loss:0.111, val_acc:0.992]
Epoch [74/120    avg_loss:0.104, val_acc:0.992]
Epoch [75/120    avg_loss:0.074, val_acc:0.996]
Epoch [76/120    avg_loss:0.073, val_acc:0.996]
Epoch [77/120    avg_loss:0.067, val_acc:0.994]
Epoch [78/120    avg_loss:0.077, val_acc:0.986]
Epoch [79/120    avg_loss:0.104, val_acc:0.980]
Epoch [80/120    avg_loss:0.084, val_acc:0.998]
Epoch [81/120    avg_loss:0.067, val_acc:0.994]
Epoch [82/120    avg_loss:0.058, val_acc:0.998]
Epoch [83/120    avg_loss:0.052, val_acc:0.996]
Epoch [84/120    avg_loss:0.077, val_acc:0.994]
Epoch [85/120    avg_loss:0.061, val_acc:0.998]
Epoch [86/120    avg_loss:0.055, val_acc:0.994]
Epoch [87/120    avg_loss:0.071, val_acc:0.994]
Epoch [88/120    avg_loss:0.044, val_acc:0.998]
Epoch [89/120    avg_loss:0.045, val_acc:0.990]
Epoch [90/120    avg_loss:0.041, val_acc:0.996]
Epoch [91/120    avg_loss:0.035, val_acc:0.992]
Epoch [92/120    avg_loss:0.026, val_acc:0.998]
Epoch [93/120    avg_loss:0.032, val_acc:0.998]
Epoch [94/120    avg_loss:0.047, val_acc:0.992]
Epoch [95/120    avg_loss:0.054, val_acc:0.982]
Epoch [96/120    avg_loss:0.047, val_acc:0.988]
Epoch [97/120    avg_loss:0.080, val_acc:0.984]
Epoch [98/120    avg_loss:0.121, val_acc:0.978]
Epoch [99/120    avg_loss:0.095, val_acc:0.980]
Epoch [100/120    avg_loss:0.058, val_acc:0.990]
Epoch [101/120    avg_loss:0.053, val_acc:0.994]
Epoch [102/120    avg_loss:0.068, val_acc:0.994]
Epoch [103/120    avg_loss:0.055, val_acc:0.982]
Epoch [104/120    avg_loss:0.054, val_acc:0.998]
Epoch [105/120    avg_loss:0.025, val_acc:0.994]
Epoch [106/120    avg_loss:0.044, val_acc:0.990]
Epoch [107/120    avg_loss:0.036, val_acc:0.988]
Epoch [108/120    avg_loss:0.061, val_acc:0.980]
Epoch [109/120    avg_loss:0.053, val_acc:0.988]
Epoch [110/120    avg_loss:0.050, val_acc:0.998]
Epoch [111/120    avg_loss:0.045, val_acc:0.994]
Epoch [112/120    avg_loss:0.029, val_acc:0.994]
Epoch [113/120    avg_loss:0.020, val_acc:0.996]
Epoch [114/120    avg_loss:0.025, val_acc:0.998]
Epoch [115/120    avg_loss:0.022, val_acc:0.992]
Epoch [116/120    avg_loss:0.072, val_acc:0.970]
Epoch [117/120    avg_loss:0.040, val_acc:0.986]
Epoch [118/120    avg_loss:0.021, val_acc:0.990]
Epoch [119/120    avg_loss:0.022, val_acc:0.996]
Epoch [120/120    avg_loss:0.019, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   3 212  12   0   0   0   0   0   0   0   0]
 [  0   0   0   1  13 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.99319728 0.98920086 0.9380531  0.90972222
 1.         0.98378378 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9921660991471085
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c22e0aa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.234]
Epoch [2/120    avg_loss:2.441, val_acc:0.357]
Epoch [3/120    avg_loss:2.269, val_acc:0.454]
Epoch [4/120    avg_loss:2.131, val_acc:0.506]
Epoch [5/120    avg_loss:1.986, val_acc:0.560]
Epoch [6/120    avg_loss:1.838, val_acc:0.601]
Epoch [7/120    avg_loss:1.699, val_acc:0.696]
Epoch [8/120    avg_loss:1.565, val_acc:0.657]
Epoch [9/120    avg_loss:1.410, val_acc:0.726]
Epoch [10/120    avg_loss:1.254, val_acc:0.768]
Epoch [11/120    avg_loss:1.127, val_acc:0.798]
Epoch [12/120    avg_loss:1.027, val_acc:0.815]
Epoch [13/120    avg_loss:0.946, val_acc:0.857]
Epoch [14/120    avg_loss:0.845, val_acc:0.877]
Epoch [15/120    avg_loss:0.818, val_acc:0.829]
Epoch [16/120    avg_loss:0.747, val_acc:0.865]
Epoch [17/120    avg_loss:0.674, val_acc:0.879]
Epoch [18/120    avg_loss:0.593, val_acc:0.881]
Epoch [19/120    avg_loss:0.593, val_acc:0.821]
Epoch [20/120    avg_loss:0.600, val_acc:0.879]
Epoch [21/120    avg_loss:0.540, val_acc:0.879]
Epoch [22/120    avg_loss:0.582, val_acc:0.905]
Epoch [23/120    avg_loss:0.513, val_acc:0.899]
Epoch [24/120    avg_loss:0.518, val_acc:0.905]
Epoch [25/120    avg_loss:0.472, val_acc:0.917]
Epoch [26/120    avg_loss:0.479, val_acc:0.903]
Epoch [27/120    avg_loss:0.422, val_acc:0.935]
Epoch [28/120    avg_loss:0.428, val_acc:0.921]
Epoch [29/120    avg_loss:0.447, val_acc:0.923]
Epoch [30/120    avg_loss:0.358, val_acc:0.944]
Epoch [31/120    avg_loss:0.349, val_acc:0.921]
Epoch [32/120    avg_loss:0.373, val_acc:0.917]
Epoch [33/120    avg_loss:0.370, val_acc:0.917]
Epoch [34/120    avg_loss:0.377, val_acc:0.927]
Epoch [35/120    avg_loss:0.357, val_acc:0.929]
Epoch [36/120    avg_loss:0.363, val_acc:0.944]
Epoch [37/120    avg_loss:0.326, val_acc:0.925]
Epoch [38/120    avg_loss:0.362, val_acc:0.899]
Epoch [39/120    avg_loss:0.365, val_acc:0.942]
Epoch [40/120    avg_loss:0.293, val_acc:0.917]
Epoch [41/120    avg_loss:0.325, val_acc:0.933]
Epoch [42/120    avg_loss:0.305, val_acc:0.933]
Epoch [43/120    avg_loss:0.268, val_acc:0.933]
Epoch [44/120    avg_loss:0.243, val_acc:0.944]
Epoch [45/120    avg_loss:0.257, val_acc:0.946]
Epoch [46/120    avg_loss:0.247, val_acc:0.962]
Epoch [47/120    avg_loss:0.229, val_acc:0.925]
Epoch [48/120    avg_loss:0.257, val_acc:0.929]
Epoch [49/120    avg_loss:0.223, val_acc:0.933]
Epoch [50/120    avg_loss:0.193, val_acc:0.972]
Epoch [51/120    avg_loss:0.197, val_acc:0.956]
Epoch [52/120    avg_loss:0.199, val_acc:0.956]
Epoch [53/120    avg_loss:0.156, val_acc:0.976]
Epoch [54/120    avg_loss:0.174, val_acc:0.935]
Epoch [55/120    avg_loss:0.167, val_acc:0.960]
Epoch [56/120    avg_loss:0.145, val_acc:0.964]
Epoch [57/120    avg_loss:0.177, val_acc:0.927]
Epoch [58/120    avg_loss:0.263, val_acc:0.958]
Epoch [59/120    avg_loss:0.176, val_acc:0.966]
Epoch [60/120    avg_loss:0.133, val_acc:0.966]
Epoch [61/120    avg_loss:0.156, val_acc:0.980]
Epoch [62/120    avg_loss:0.154, val_acc:0.968]
Epoch [63/120    avg_loss:0.148, val_acc:0.940]
Epoch [64/120    avg_loss:0.204, val_acc:0.954]
Epoch [65/120    avg_loss:0.157, val_acc:0.970]
Epoch [66/120    avg_loss:0.177, val_acc:0.970]
Epoch [67/120    avg_loss:0.131, val_acc:0.964]
Epoch [68/120    avg_loss:0.133, val_acc:0.954]
Epoch [69/120    avg_loss:0.184, val_acc:0.966]
Epoch [70/120    avg_loss:0.137, val_acc:0.966]
Epoch [71/120    avg_loss:0.140, val_acc:0.960]
Epoch [72/120    avg_loss:0.129, val_acc:0.968]
Epoch [73/120    avg_loss:0.151, val_acc:0.950]
Epoch [74/120    avg_loss:0.165, val_acc:0.968]
Epoch [75/120    avg_loss:0.104, val_acc:0.974]
Epoch [76/120    avg_loss:0.078, val_acc:0.972]
Epoch [77/120    avg_loss:0.084, val_acc:0.974]
Epoch [78/120    avg_loss:0.070, val_acc:0.974]
Epoch [79/120    avg_loss:0.072, val_acc:0.974]
Epoch [80/120    avg_loss:0.059, val_acc:0.974]
Epoch [81/120    avg_loss:0.067, val_acc:0.976]
Epoch [82/120    avg_loss:0.064, val_acc:0.976]
Epoch [83/120    avg_loss:0.073, val_acc:0.976]
Epoch [84/120    avg_loss:0.071, val_acc:0.978]
Epoch [85/120    avg_loss:0.069, val_acc:0.974]
Epoch [86/120    avg_loss:0.061, val_acc:0.976]
Epoch [87/120    avg_loss:0.059, val_acc:0.976]
Epoch [88/120    avg_loss:0.070, val_acc:0.976]
Epoch [89/120    avg_loss:0.058, val_acc:0.976]
Epoch [90/120    avg_loss:0.065, val_acc:0.976]
Epoch [91/120    avg_loss:0.060, val_acc:0.976]
Epoch [92/120    avg_loss:0.067, val_acc:0.976]
Epoch [93/120    avg_loss:0.056, val_acc:0.976]
Epoch [94/120    avg_loss:0.060, val_acc:0.976]
Epoch [95/120    avg_loss:0.064, val_acc:0.976]
Epoch [96/120    avg_loss:0.065, val_acc:0.976]
Epoch [97/120    avg_loss:0.069, val_acc:0.976]
Epoch [98/120    avg_loss:0.056, val_acc:0.976]
Epoch [99/120    avg_loss:0.065, val_acc:0.976]
Epoch [100/120    avg_loss:0.059, val_acc:0.976]
Epoch [101/120    avg_loss:0.055, val_acc:0.976]
Epoch [102/120    avg_loss:0.057, val_acc:0.976]
Epoch [103/120    avg_loss:0.068, val_acc:0.976]
Epoch [104/120    avg_loss:0.066, val_acc:0.976]
Epoch [105/120    avg_loss:0.058, val_acc:0.976]
Epoch [106/120    avg_loss:0.056, val_acc:0.976]
Epoch [107/120    avg_loss:0.061, val_acc:0.976]
Epoch [108/120    avg_loss:0.057, val_acc:0.976]
Epoch [109/120    avg_loss:0.064, val_acc:0.976]
Epoch [110/120    avg_loss:0.060, val_acc:0.976]
Epoch [111/120    avg_loss:0.055, val_acc:0.976]
Epoch [112/120    avg_loss:0.065, val_acc:0.976]
Epoch [113/120    avg_loss:0.057, val_acc:0.976]
Epoch [114/120    avg_loss:0.057, val_acc:0.976]
Epoch [115/120    avg_loss:0.068, val_acc:0.976]
Epoch [116/120    avg_loss:0.079, val_acc:0.976]
Epoch [117/120    avg_loss:0.055, val_acc:0.976]
Epoch [118/120    avg_loss:0.069, val_acc:0.976]
Epoch [119/120    avg_loss:0.068, val_acc:0.976]
Epoch [120/120    avg_loss:0.057, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 209  18   0   0   1   2   0   0   0   0   0]
 [  0   0   0   0 199  28   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.96674058 0.95216401 0.89237668 0.90506329
 1.         0.90909091 0.99742931 1.         1.         1.
 1.         1.        ]

Kappa:
0.9843331261051754
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0960fccb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.351]
Epoch [2/120    avg_loss:2.418, val_acc:0.355]
Epoch [3/120    avg_loss:2.272, val_acc:0.452]
Epoch [4/120    avg_loss:2.154, val_acc:0.548]
Epoch [5/120    avg_loss:2.034, val_acc:0.552]
Epoch [6/120    avg_loss:1.900, val_acc:0.609]
Epoch [7/120    avg_loss:1.790, val_acc:0.621]
Epoch [8/120    avg_loss:1.630, val_acc:0.613]
Epoch [9/120    avg_loss:1.493, val_acc:0.665]
Epoch [10/120    avg_loss:1.365, val_acc:0.690]
Epoch [11/120    avg_loss:1.215, val_acc:0.728]
Epoch [12/120    avg_loss:1.091, val_acc:0.786]
Epoch [13/120    avg_loss:0.948, val_acc:0.831]
Epoch [14/120    avg_loss:0.879, val_acc:0.772]
Epoch [15/120    avg_loss:0.836, val_acc:0.903]
Epoch [16/120    avg_loss:0.743, val_acc:0.857]
Epoch [17/120    avg_loss:0.654, val_acc:0.887]
Epoch [18/120    avg_loss:0.634, val_acc:0.867]
Epoch [19/120    avg_loss:0.631, val_acc:0.851]
Epoch [20/120    avg_loss:0.645, val_acc:0.909]
Epoch [21/120    avg_loss:0.540, val_acc:0.851]
Epoch [22/120    avg_loss:0.488, val_acc:0.897]
Epoch [23/120    avg_loss:0.474, val_acc:0.891]
Epoch [24/120    avg_loss:0.422, val_acc:0.899]
Epoch [25/120    avg_loss:0.476, val_acc:0.883]
Epoch [26/120    avg_loss:0.414, val_acc:0.903]
Epoch [27/120    avg_loss:0.434, val_acc:0.901]
Epoch [28/120    avg_loss:0.405, val_acc:0.907]
Epoch [29/120    avg_loss:0.403, val_acc:0.925]
Epoch [30/120    avg_loss:0.390, val_acc:0.923]
Epoch [31/120    avg_loss:0.402, val_acc:0.903]
Epoch [32/120    avg_loss:0.431, val_acc:0.931]
Epoch [33/120    avg_loss:0.310, val_acc:0.911]
Epoch [34/120    avg_loss:0.317, val_acc:0.921]
Epoch [35/120    avg_loss:0.291, val_acc:0.940]
Epoch [36/120    avg_loss:0.324, val_acc:0.915]
Epoch [37/120    avg_loss:0.288, val_acc:0.931]
Epoch [38/120    avg_loss:0.282, val_acc:0.933]
Epoch [39/120    avg_loss:0.230, val_acc:0.944]
Epoch [40/120    avg_loss:0.266, val_acc:0.935]
Epoch [41/120    avg_loss:0.283, val_acc:0.944]
Epoch [42/120    avg_loss:0.315, val_acc:0.917]
Epoch [43/120    avg_loss:0.284, val_acc:0.966]
Epoch [44/120    avg_loss:0.279, val_acc:0.952]
Epoch [45/120    avg_loss:0.227, val_acc:0.956]
Epoch [46/120    avg_loss:0.224, val_acc:0.917]
Epoch [47/120    avg_loss:0.282, val_acc:0.921]
Epoch [48/120    avg_loss:0.189, val_acc:0.974]
Epoch [49/120    avg_loss:0.168, val_acc:0.940]
Epoch [50/120    avg_loss:0.152, val_acc:0.923]
Epoch [51/120    avg_loss:0.198, val_acc:0.964]
Epoch [52/120    avg_loss:0.160, val_acc:0.980]
Epoch [53/120    avg_loss:0.149, val_acc:0.972]
Epoch [54/120    avg_loss:0.184, val_acc:0.972]
Epoch [55/120    avg_loss:0.162, val_acc:0.968]
Epoch [56/120    avg_loss:0.163, val_acc:0.956]
Epoch [57/120    avg_loss:0.162, val_acc:0.962]
Epoch [58/120    avg_loss:0.138, val_acc:0.917]
Epoch [59/120    avg_loss:0.216, val_acc:0.976]
Epoch [60/120    avg_loss:0.124, val_acc:0.982]
Epoch [61/120    avg_loss:0.114, val_acc:0.960]
Epoch [62/120    avg_loss:0.181, val_acc:0.960]
Epoch [63/120    avg_loss:0.140, val_acc:0.968]
Epoch [64/120    avg_loss:0.110, val_acc:0.980]
Epoch [65/120    avg_loss:0.124, val_acc:0.970]
Epoch [66/120    avg_loss:0.119, val_acc:0.972]
Epoch [67/120    avg_loss:0.127, val_acc:0.960]
Epoch [68/120    avg_loss:0.088, val_acc:0.978]
Epoch [69/120    avg_loss:0.102, val_acc:0.970]
Epoch [70/120    avg_loss:0.134, val_acc:0.972]
Epoch [71/120    avg_loss:0.095, val_acc:0.978]
Epoch [72/120    avg_loss:0.106, val_acc:0.970]
Epoch [73/120    avg_loss:0.095, val_acc:0.988]
Epoch [74/120    avg_loss:0.114, val_acc:0.956]
Epoch [75/120    avg_loss:0.141, val_acc:0.986]
Epoch [76/120    avg_loss:0.101, val_acc:0.974]
Epoch [77/120    avg_loss:0.082, val_acc:0.990]
Epoch [78/120    avg_loss:0.098, val_acc:0.978]
Epoch [79/120    avg_loss:0.099, val_acc:0.966]
Epoch [80/120    avg_loss:0.092, val_acc:0.946]
Epoch [81/120    avg_loss:0.106, val_acc:0.970]
Epoch [82/120    avg_loss:0.131, val_acc:0.968]
Epoch [83/120    avg_loss:0.092, val_acc:0.970]
Epoch [84/120    avg_loss:0.090, val_acc:0.978]
Epoch [85/120    avg_loss:0.073, val_acc:0.982]
Epoch [86/120    avg_loss:0.074, val_acc:0.982]
Epoch [87/120    avg_loss:0.062, val_acc:0.974]
Epoch [88/120    avg_loss:0.080, val_acc:0.986]
Epoch [89/120    avg_loss:0.082, val_acc:0.988]
Epoch [90/120    avg_loss:0.065, val_acc:0.984]
Epoch [91/120    avg_loss:0.054, val_acc:0.988]
Epoch [92/120    avg_loss:0.046, val_acc:0.996]
Epoch [93/120    avg_loss:0.038, val_acc:0.994]
Epoch [94/120    avg_loss:0.040, val_acc:0.994]
Epoch [95/120    avg_loss:0.036, val_acc:0.990]
Epoch [96/120    avg_loss:0.035, val_acc:0.992]
Epoch [97/120    avg_loss:0.036, val_acc:0.996]
Epoch [98/120    avg_loss:0.036, val_acc:0.996]
Epoch [99/120    avg_loss:0.032, val_acc:0.994]
Epoch [100/120    avg_loss:0.028, val_acc:0.994]
Epoch [101/120    avg_loss:0.031, val_acc:0.996]
Epoch [102/120    avg_loss:0.031, val_acc:0.994]
Epoch [103/120    avg_loss:0.031, val_acc:0.994]
Epoch [104/120    avg_loss:0.028, val_acc:0.994]
Epoch [105/120    avg_loss:0.024, val_acc:0.994]
Epoch [106/120    avg_loss:0.034, val_acc:0.996]
Epoch [107/120    avg_loss:0.034, val_acc:0.996]
Epoch [108/120    avg_loss:0.035, val_acc:0.994]
Epoch [109/120    avg_loss:0.026, val_acc:0.992]
Epoch [110/120    avg_loss:0.029, val_acc:0.994]
Epoch [111/120    avg_loss:0.031, val_acc:0.994]
Epoch [112/120    avg_loss:0.024, val_acc:0.992]
Epoch [113/120    avg_loss:0.032, val_acc:0.994]
Epoch [114/120    avg_loss:0.030, val_acc:0.994]
Epoch [115/120    avg_loss:0.026, val_acc:0.992]
Epoch [116/120    avg_loss:0.030, val_acc:0.994]
Epoch [117/120    avg_loss:0.036, val_acc:0.992]
Epoch [118/120    avg_loss:0.031, val_acc:0.994]
Epoch [119/120    avg_loss:0.033, val_acc:0.994]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 211  17   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 0.99927061 0.97986577 0.9569161  0.89655172 0.8956229
 0.99756691 0.95555556 0.996139   1.         1.         1.
 1.         1.        ]

Kappa:
0.9857563910602575
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f118eed7a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.576, val_acc:0.295]
Epoch [2/120    avg_loss:2.407, val_acc:0.357]
Epoch [3/120    avg_loss:2.290, val_acc:0.381]
Epoch [4/120    avg_loss:2.194, val_acc:0.438]
Epoch [5/120    avg_loss:2.086, val_acc:0.512]
Epoch [6/120    avg_loss:1.966, val_acc:0.582]
Epoch [7/120    avg_loss:1.848, val_acc:0.582]
Epoch [8/120    avg_loss:1.745, val_acc:0.615]
Epoch [9/120    avg_loss:1.616, val_acc:0.652]
Epoch [10/120    avg_loss:1.508, val_acc:0.650]
Epoch [11/120    avg_loss:1.342, val_acc:0.684]
Epoch [12/120    avg_loss:1.266, val_acc:0.652]
Epoch [13/120    avg_loss:1.072, val_acc:0.861]
Epoch [14/120    avg_loss:0.955, val_acc:0.857]
Epoch [15/120    avg_loss:0.896, val_acc:0.857]
Epoch [16/120    avg_loss:0.833, val_acc:0.871]
Epoch [17/120    avg_loss:0.714, val_acc:0.873]
Epoch [18/120    avg_loss:0.701, val_acc:0.834]
Epoch [19/120    avg_loss:0.613, val_acc:0.895]
Epoch [20/120    avg_loss:0.536, val_acc:0.914]
Epoch [21/120    avg_loss:0.543, val_acc:0.904]
Epoch [22/120    avg_loss:0.553, val_acc:0.893]
Epoch [23/120    avg_loss:0.598, val_acc:0.900]
Epoch [24/120    avg_loss:0.471, val_acc:0.902]
Epoch [25/120    avg_loss:0.477, val_acc:0.924]
Epoch [26/120    avg_loss:0.460, val_acc:0.908]
Epoch [27/120    avg_loss:0.424, val_acc:0.916]
Epoch [28/120    avg_loss:0.389, val_acc:0.922]
Epoch [29/120    avg_loss:0.389, val_acc:0.900]
Epoch [30/120    avg_loss:0.418, val_acc:0.914]
Epoch [31/120    avg_loss:0.419, val_acc:0.920]
Epoch [32/120    avg_loss:0.390, val_acc:0.898]
Epoch [33/120    avg_loss:0.408, val_acc:0.922]
Epoch [34/120    avg_loss:0.342, val_acc:0.896]
Epoch [35/120    avg_loss:0.314, val_acc:0.939]
Epoch [36/120    avg_loss:0.304, val_acc:0.916]
Epoch [37/120    avg_loss:0.332, val_acc:0.893]
Epoch [38/120    avg_loss:0.286, val_acc:0.939]
Epoch [39/120    avg_loss:0.249, val_acc:0.949]
Epoch [40/120    avg_loss:0.238, val_acc:0.895]
Epoch [41/120    avg_loss:0.213, val_acc:0.957]
Epoch [42/120    avg_loss:0.206, val_acc:0.938]
Epoch [43/120    avg_loss:0.211, val_acc:0.963]
Epoch [44/120    avg_loss:0.251, val_acc:0.965]
Epoch [45/120    avg_loss:0.229, val_acc:0.938]
Epoch [46/120    avg_loss:0.173, val_acc:0.959]
Epoch [47/120    avg_loss:0.215, val_acc:0.943]
Epoch [48/120    avg_loss:0.153, val_acc:0.941]
Epoch [49/120    avg_loss:0.209, val_acc:0.924]
Epoch [50/120    avg_loss:0.218, val_acc:0.967]
Epoch [51/120    avg_loss:0.197, val_acc:0.967]
Epoch [52/120    avg_loss:0.205, val_acc:0.928]
Epoch [53/120    avg_loss:0.209, val_acc:0.941]
Epoch [54/120    avg_loss:0.210, val_acc:0.943]
Epoch [55/120    avg_loss:0.151, val_acc:0.971]
Epoch [56/120    avg_loss:0.138, val_acc:0.959]
Epoch [57/120    avg_loss:0.126, val_acc:0.955]
Epoch [58/120    avg_loss:0.133, val_acc:0.961]
Epoch [59/120    avg_loss:0.131, val_acc:0.945]
Epoch [60/120    avg_loss:0.128, val_acc:0.951]
Epoch [61/120    avg_loss:0.117, val_acc:0.959]
Epoch [62/120    avg_loss:0.161, val_acc:0.955]
Epoch [63/120    avg_loss:0.134, val_acc:0.975]
Epoch [64/120    avg_loss:0.127, val_acc:0.969]
Epoch [65/120    avg_loss:0.116, val_acc:0.961]
Epoch [66/120    avg_loss:0.115, val_acc:0.953]
Epoch [67/120    avg_loss:0.094, val_acc:0.977]
Epoch [68/120    avg_loss:0.079, val_acc:0.967]
Epoch [69/120    avg_loss:0.078, val_acc:0.982]
Epoch [70/120    avg_loss:0.090, val_acc:0.963]
Epoch [71/120    avg_loss:0.124, val_acc:0.975]
Epoch [72/120    avg_loss:0.086, val_acc:0.975]
Epoch [73/120    avg_loss:0.106, val_acc:0.959]
Epoch [74/120    avg_loss:0.083, val_acc:0.967]
Epoch [75/120    avg_loss:0.079, val_acc:0.982]
Epoch [76/120    avg_loss:0.096, val_acc:0.961]
Epoch [77/120    avg_loss:0.068, val_acc:0.986]
Epoch [78/120    avg_loss:0.066, val_acc:0.977]
Epoch [79/120    avg_loss:0.058, val_acc:0.984]
Epoch [80/120    avg_loss:0.050, val_acc:0.971]
Epoch [81/120    avg_loss:0.102, val_acc:0.953]
Epoch [82/120    avg_loss:0.120, val_acc:0.973]
Epoch [83/120    avg_loss:0.093, val_acc:0.979]
Epoch [84/120    avg_loss:0.127, val_acc:0.965]
Epoch [85/120    avg_loss:0.135, val_acc:0.975]
Epoch [86/120    avg_loss:0.106, val_acc:0.961]
Epoch [87/120    avg_loss:0.086, val_acc:0.973]
Epoch [88/120    avg_loss:0.072, val_acc:0.963]
Epoch [89/120    avg_loss:0.076, val_acc:0.969]
Epoch [90/120    avg_loss:0.105, val_acc:0.959]
Epoch [91/120    avg_loss:0.070, val_acc:0.988]
Epoch [92/120    avg_loss:0.040, val_acc:0.990]
Epoch [93/120    avg_loss:0.042, val_acc:0.990]
Epoch [94/120    avg_loss:0.049, val_acc:0.988]
Epoch [95/120    avg_loss:0.041, val_acc:0.986]
Epoch [96/120    avg_loss:0.034, val_acc:0.988]
Epoch [97/120    avg_loss:0.037, val_acc:0.988]
Epoch [98/120    avg_loss:0.040, val_acc:0.986]
Epoch [99/120    avg_loss:0.034, val_acc:0.988]
Epoch [100/120    avg_loss:0.038, val_acc:0.986]
Epoch [101/120    avg_loss:0.040, val_acc:0.986]
Epoch [102/120    avg_loss:0.034, val_acc:0.986]
Epoch [103/120    avg_loss:0.034, val_acc:0.986]
Epoch [104/120    avg_loss:0.038, val_acc:0.988]
Epoch [105/120    avg_loss:0.031, val_acc:0.988]
Epoch [106/120    avg_loss:0.033, val_acc:0.986]
Epoch [107/120    avg_loss:0.037, val_acc:0.986]
Epoch [108/120    avg_loss:0.037, val_acc:0.986]
Epoch [109/120    avg_loss:0.030, val_acc:0.986]
Epoch [110/120    avg_loss:0.031, val_acc:0.986]
Epoch [111/120    avg_loss:0.032, val_acc:0.986]
Epoch [112/120    avg_loss:0.032, val_acc:0.986]
Epoch [113/120    avg_loss:0.034, val_acc:0.986]
Epoch [114/120    avg_loss:0.029, val_acc:0.986]
Epoch [115/120    avg_loss:0.029, val_acc:0.986]
Epoch [116/120    avg_loss:0.027, val_acc:0.986]
Epoch [117/120    avg_loss:0.038, val_acc:0.986]
Epoch [118/120    avg_loss:0.045, val_acc:0.986]
Epoch [119/120    avg_loss:0.030, val_acc:0.986]
Epoch [120/120    avg_loss:0.030, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   1 213  13   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   2   0   0  11 440   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0   0 832]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 1.         0.97757848 0.96162528 0.91466083 0.91333333
 1.         0.95604396 0.99358151 0.99680511 1.         0.98562092
 0.98544233 0.99879952]

Kappa:
0.984096596009558
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f33c3961a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.568, val_acc:0.324]
Epoch [2/120    avg_loss:2.385, val_acc:0.354]
Epoch [3/120    avg_loss:2.232, val_acc:0.383]
Epoch [4/120    avg_loss:2.103, val_acc:0.514]
Epoch [5/120    avg_loss:1.976, val_acc:0.582]
Epoch [6/120    avg_loss:1.829, val_acc:0.646]
Epoch [7/120    avg_loss:1.697, val_acc:0.672]
Epoch [8/120    avg_loss:1.534, val_acc:0.719]
Epoch [9/120    avg_loss:1.346, val_acc:0.715]
Epoch [10/120    avg_loss:1.235, val_acc:0.725]
Epoch [11/120    avg_loss:1.105, val_acc:0.730]
Epoch [12/120    avg_loss:0.982, val_acc:0.787]
Epoch [13/120    avg_loss:0.900, val_acc:0.803]
Epoch [14/120    avg_loss:0.826, val_acc:0.873]
Epoch [15/120    avg_loss:0.744, val_acc:0.885]
Epoch [16/120    avg_loss:0.690, val_acc:0.883]
Epoch [17/120    avg_loss:0.639, val_acc:0.898]
Epoch [18/120    avg_loss:0.638, val_acc:0.885]
Epoch [19/120    avg_loss:0.603, val_acc:0.873]
Epoch [20/120    avg_loss:0.535, val_acc:0.885]
Epoch [21/120    avg_loss:0.551, val_acc:0.881]
Epoch [22/120    avg_loss:0.535, val_acc:0.904]
Epoch [23/120    avg_loss:0.533, val_acc:0.920]
Epoch [24/120    avg_loss:0.465, val_acc:0.906]
Epoch [25/120    avg_loss:0.451, val_acc:0.918]
Epoch [26/120    avg_loss:0.426, val_acc:0.936]
Epoch [27/120    avg_loss:0.424, val_acc:0.930]
Epoch [28/120    avg_loss:0.409, val_acc:0.938]
Epoch [29/120    avg_loss:0.388, val_acc:0.895]
Epoch [30/120    avg_loss:0.438, val_acc:0.932]
Epoch [31/120    avg_loss:0.389, val_acc:0.920]
Epoch [32/120    avg_loss:0.463, val_acc:0.863]
Epoch [33/120    avg_loss:0.412, val_acc:0.955]
Epoch [34/120    avg_loss:0.358, val_acc:0.900]
Epoch [35/120    avg_loss:0.350, val_acc:0.922]
Epoch [36/120    avg_loss:0.366, val_acc:0.920]
Epoch [37/120    avg_loss:0.307, val_acc:0.928]
Epoch [38/120    avg_loss:0.303, val_acc:0.934]
Epoch [39/120    avg_loss:0.297, val_acc:0.910]
Epoch [40/120    avg_loss:0.297, val_acc:0.928]
Epoch [41/120    avg_loss:0.341, val_acc:0.934]
Epoch [42/120    avg_loss:0.291, val_acc:0.967]
Epoch [43/120    avg_loss:0.289, val_acc:0.938]
Epoch [44/120    avg_loss:0.226, val_acc:0.953]
Epoch [45/120    avg_loss:0.243, val_acc:0.930]
Epoch [46/120    avg_loss:0.255, val_acc:0.957]
Epoch [47/120    avg_loss:0.277, val_acc:0.945]
Epoch [48/120    avg_loss:0.240, val_acc:0.963]
Epoch [49/120    avg_loss:0.210, val_acc:0.963]
Epoch [50/120    avg_loss:0.238, val_acc:0.959]
Epoch [51/120    avg_loss:0.166, val_acc:0.967]
Epoch [52/120    avg_loss:0.212, val_acc:0.955]
Epoch [53/120    avg_loss:0.205, val_acc:0.959]
Epoch [54/120    avg_loss:0.207, val_acc:0.977]
Epoch [55/120    avg_loss:0.195, val_acc:0.963]
Epoch [56/120    avg_loss:0.224, val_acc:0.943]
Epoch [57/120    avg_loss:0.214, val_acc:0.957]
Epoch [58/120    avg_loss:0.178, val_acc:0.953]
Epoch [59/120    avg_loss:0.182, val_acc:0.973]
Epoch [60/120    avg_loss:0.149, val_acc:0.973]
Epoch [61/120    avg_loss:0.159, val_acc:0.969]
Epoch [62/120    avg_loss:0.132, val_acc:0.975]
Epoch [63/120    avg_loss:0.179, val_acc:0.965]
Epoch [64/120    avg_loss:0.167, val_acc:0.963]
Epoch [65/120    avg_loss:0.142, val_acc:0.979]
Epoch [66/120    avg_loss:0.137, val_acc:0.955]
Epoch [67/120    avg_loss:0.133, val_acc:0.973]
Epoch [68/120    avg_loss:0.160, val_acc:0.979]
Epoch [69/120    avg_loss:0.121, val_acc:0.973]
Epoch [70/120    avg_loss:0.141, val_acc:0.971]
Epoch [71/120    avg_loss:0.109, val_acc:0.980]
Epoch [72/120    avg_loss:0.134, val_acc:0.975]
Epoch [73/120    avg_loss:0.083, val_acc:0.965]
Epoch [74/120    avg_loss:0.078, val_acc:0.980]
Epoch [75/120    avg_loss:0.194, val_acc:0.965]
Epoch [76/120    avg_loss:0.129, val_acc:0.973]
Epoch [77/120    avg_loss:0.113, val_acc:0.955]
Epoch [78/120    avg_loss:0.098, val_acc:0.980]
Epoch [79/120    avg_loss:0.126, val_acc:0.963]
Epoch [80/120    avg_loss:0.090, val_acc:0.977]
Epoch [81/120    avg_loss:0.074, val_acc:0.979]
Epoch [82/120    avg_loss:0.074, val_acc:0.969]
Epoch [83/120    avg_loss:0.061, val_acc:0.979]
Epoch [84/120    avg_loss:0.069, val_acc:0.959]
Epoch [85/120    avg_loss:0.102, val_acc:0.979]
Epoch [86/120    avg_loss:0.068, val_acc:0.977]
Epoch [87/120    avg_loss:0.074, val_acc:0.980]
Epoch [88/120    avg_loss:0.088, val_acc:0.955]
Epoch [89/120    avg_loss:0.147, val_acc:0.967]
Epoch [90/120    avg_loss:0.094, val_acc:0.975]
Epoch [91/120    avg_loss:0.095, val_acc:0.965]
Epoch [92/120    avg_loss:0.074, val_acc:0.982]
Epoch [93/120    avg_loss:0.102, val_acc:0.971]
Epoch [94/120    avg_loss:0.176, val_acc:0.959]
Epoch [95/120    avg_loss:0.172, val_acc:0.932]
Epoch [96/120    avg_loss:0.093, val_acc:0.977]
Epoch [97/120    avg_loss:0.071, val_acc:0.979]
Epoch [98/120    avg_loss:0.084, val_acc:0.977]
Epoch [99/120    avg_loss:0.048, val_acc:0.986]
Epoch [100/120    avg_loss:0.053, val_acc:0.984]
Epoch [101/120    avg_loss:0.043, val_acc:0.980]
Epoch [102/120    avg_loss:0.050, val_acc:0.975]
Epoch [103/120    avg_loss:0.052, val_acc:0.994]
Epoch [104/120    avg_loss:0.045, val_acc:0.984]
Epoch [105/120    avg_loss:0.074, val_acc:0.973]
Epoch [106/120    avg_loss:0.046, val_acc:0.971]
Epoch [107/120    avg_loss:0.048, val_acc:0.998]
Epoch [108/120    avg_loss:0.044, val_acc:0.984]
Epoch [109/120    avg_loss:0.049, val_acc:0.982]
Epoch [110/120    avg_loss:0.037, val_acc:0.986]
Epoch [111/120    avg_loss:0.038, val_acc:0.984]
Epoch [112/120    avg_loss:0.049, val_acc:0.979]
Epoch [113/120    avg_loss:0.050, val_acc:0.992]
Epoch [114/120    avg_loss:0.045, val_acc:0.992]
Epoch [115/120    avg_loss:0.054, val_acc:0.992]
Epoch [116/120    avg_loss:0.049, val_acc:0.996]
Epoch [117/120    avg_loss:0.031, val_acc:0.994]
Epoch [118/120    avg_loss:0.017, val_acc:0.994]
Epoch [119/120    avg_loss:0.031, val_acc:0.988]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 219   5   0   0   0   4   0   0   0   0   0]
 [  0   0   0   2 208  17   0   0   0   0   0   0   0   0]
 [  0   0   0   7  14 123   1   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0  16 437   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.272921108742

F1 scores:
[       nan 1.         0.98206278 0.95633188 0.90434783 0.86315789
 0.98280098 0.96703297 0.99487179 1.         1.         0.97789337
 0.98092031 1.        ]

Kappa:
0.980771375148097
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7befeafa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.413]
Epoch [2/120    avg_loss:2.430, val_acc:0.399]
Epoch [3/120    avg_loss:2.303, val_acc:0.401]
Epoch [4/120    avg_loss:2.184, val_acc:0.438]
Epoch [5/120    avg_loss:2.073, val_acc:0.492]
Epoch [6/120    avg_loss:1.956, val_acc:0.542]
Epoch [7/120    avg_loss:1.850, val_acc:0.629]
Epoch [8/120    avg_loss:1.746, val_acc:0.700]
Epoch [9/120    avg_loss:1.571, val_acc:0.728]
Epoch [10/120    avg_loss:1.446, val_acc:0.663]
Epoch [11/120    avg_loss:1.362, val_acc:0.726]
Epoch [12/120    avg_loss:1.246, val_acc:0.764]
Epoch [13/120    avg_loss:1.135, val_acc:0.750]
Epoch [14/120    avg_loss:1.076, val_acc:0.714]
Epoch [15/120    avg_loss:0.969, val_acc:0.823]
Epoch [16/120    avg_loss:0.861, val_acc:0.792]
Epoch [17/120    avg_loss:0.757, val_acc:0.873]
Epoch [18/120    avg_loss:0.731, val_acc:0.851]
Epoch [19/120    avg_loss:0.730, val_acc:0.851]
Epoch [20/120    avg_loss:0.653, val_acc:0.883]
Epoch [21/120    avg_loss:0.596, val_acc:0.913]
Epoch [22/120    avg_loss:0.596, val_acc:0.839]
Epoch [23/120    avg_loss:0.529, val_acc:0.895]
Epoch [24/120    avg_loss:0.519, val_acc:0.893]
Epoch [25/120    avg_loss:0.534, val_acc:0.881]
Epoch [26/120    avg_loss:0.529, val_acc:0.877]
Epoch [27/120    avg_loss:0.476, val_acc:0.889]
Epoch [28/120    avg_loss:0.419, val_acc:0.911]
Epoch [29/120    avg_loss:0.483, val_acc:0.907]
Epoch [30/120    avg_loss:0.444, val_acc:0.905]
Epoch [31/120    avg_loss:0.436, val_acc:0.873]
Epoch [32/120    avg_loss:0.414, val_acc:0.879]
Epoch [33/120    avg_loss:0.420, val_acc:0.903]
Epoch [34/120    avg_loss:0.382, val_acc:0.887]
Epoch [35/120    avg_loss:0.350, val_acc:0.931]
Epoch [36/120    avg_loss:0.288, val_acc:0.917]
Epoch [37/120    avg_loss:0.284, val_acc:0.929]
Epoch [38/120    avg_loss:0.287, val_acc:0.940]
Epoch [39/120    avg_loss:0.276, val_acc:0.938]
Epoch [40/120    avg_loss:0.275, val_acc:0.940]
Epoch [41/120    avg_loss:0.269, val_acc:0.940]
Epoch [42/120    avg_loss:0.290, val_acc:0.942]
Epoch [43/120    avg_loss:0.289, val_acc:0.948]
Epoch [44/120    avg_loss:0.288, val_acc:0.944]
Epoch [45/120    avg_loss:0.270, val_acc:0.950]
Epoch [46/120    avg_loss:0.272, val_acc:0.946]
Epoch [47/120    avg_loss:0.257, val_acc:0.942]
Epoch [48/120    avg_loss:0.262, val_acc:0.950]
Epoch [49/120    avg_loss:0.263, val_acc:0.950]
Epoch [50/120    avg_loss:0.242, val_acc:0.954]
Epoch [51/120    avg_loss:0.266, val_acc:0.948]
Epoch [52/120    avg_loss:0.248, val_acc:0.944]
Epoch [53/120    avg_loss:0.270, val_acc:0.952]
Epoch [54/120    avg_loss:0.250, val_acc:0.948]
Epoch [55/120    avg_loss:0.248, val_acc:0.948]
Epoch [56/120    avg_loss:0.247, val_acc:0.944]
Epoch [57/120    avg_loss:0.258, val_acc:0.950]
Epoch [58/120    avg_loss:0.224, val_acc:0.948]
Epoch [59/120    avg_loss:0.244, val_acc:0.950]
Epoch [60/120    avg_loss:0.268, val_acc:0.960]
Epoch [61/120    avg_loss:0.221, val_acc:0.958]
Epoch [62/120    avg_loss:0.238, val_acc:0.954]
Epoch [63/120    avg_loss:0.239, val_acc:0.952]
Epoch [64/120    avg_loss:0.219, val_acc:0.954]
Epoch [65/120    avg_loss:0.218, val_acc:0.958]
Epoch [66/120    avg_loss:0.211, val_acc:0.956]
Epoch [67/120    avg_loss:0.238, val_acc:0.956]
Epoch [68/120    avg_loss:0.218, val_acc:0.956]
Epoch [69/120    avg_loss:0.238, val_acc:0.952]
Epoch [70/120    avg_loss:0.205, val_acc:0.954]
Epoch [71/120    avg_loss:0.245, val_acc:0.962]
Epoch [72/120    avg_loss:0.229, val_acc:0.960]
Epoch [73/120    avg_loss:0.250, val_acc:0.960]
Epoch [74/120    avg_loss:0.223, val_acc:0.964]
Epoch [75/120    avg_loss:0.214, val_acc:0.960]
Epoch [76/120    avg_loss:0.201, val_acc:0.968]
Epoch [77/120    avg_loss:0.194, val_acc:0.966]
Epoch [78/120    avg_loss:0.214, val_acc:0.954]
Epoch [79/120    avg_loss:0.196, val_acc:0.954]
Epoch [80/120    avg_loss:0.200, val_acc:0.974]
Epoch [81/120    avg_loss:0.194, val_acc:0.966]
Epoch [82/120    avg_loss:0.205, val_acc:0.970]
Epoch [83/120    avg_loss:0.185, val_acc:0.974]
Epoch [84/120    avg_loss:0.217, val_acc:0.962]
Epoch [85/120    avg_loss:0.187, val_acc:0.974]
Epoch [86/120    avg_loss:0.178, val_acc:0.978]
Epoch [87/120    avg_loss:0.186, val_acc:0.964]
Epoch [88/120    avg_loss:0.186, val_acc:0.970]
Epoch [89/120    avg_loss:0.200, val_acc:0.974]
Epoch [90/120    avg_loss:0.203, val_acc:0.976]
Epoch [91/120    avg_loss:0.193, val_acc:0.972]
Epoch [92/120    avg_loss:0.167, val_acc:0.972]
Epoch [93/120    avg_loss:0.166, val_acc:0.980]
Epoch [94/120    avg_loss:0.179, val_acc:0.970]
Epoch [95/120    avg_loss:0.163, val_acc:0.976]
Epoch [96/120    avg_loss:0.165, val_acc:0.974]
Epoch [97/120    avg_loss:0.171, val_acc:0.964]
Epoch [98/120    avg_loss:0.178, val_acc:0.980]
Epoch [99/120    avg_loss:0.172, val_acc:0.950]
Epoch [100/120    avg_loss:0.203, val_acc:0.962]
Epoch [101/120    avg_loss:0.147, val_acc:0.976]
Epoch [102/120    avg_loss:0.169, val_acc:0.966]
Epoch [103/120    avg_loss:0.171, val_acc:0.972]
Epoch [104/120    avg_loss:0.158, val_acc:0.970]
Epoch [105/120    avg_loss:0.169, val_acc:0.974]
Epoch [106/120    avg_loss:0.182, val_acc:0.956]
Epoch [107/120    avg_loss:0.156, val_acc:0.978]
Epoch [108/120    avg_loss:0.167, val_acc:0.974]
Epoch [109/120    avg_loss:0.155, val_acc:0.976]
Epoch [110/120    avg_loss:0.147, val_acc:0.968]
Epoch [111/120    avg_loss:0.153, val_acc:0.964]
Epoch [112/120    avg_loss:0.136, val_acc:0.970]
Epoch [113/120    avg_loss:0.145, val_acc:0.970]
Epoch [114/120    avg_loss:0.144, val_acc:0.976]
Epoch [115/120    avg_loss:0.132, val_acc:0.974]
Epoch [116/120    avg_loss:0.129, val_acc:0.974]
Epoch [117/120    avg_loss:0.139, val_acc:0.974]
Epoch [118/120    avg_loss:0.151, val_acc:0.974]
Epoch [119/120    avg_loss:0.125, val_acc:0.974]
Epoch [120/120    avg_loss:0.128, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   1 206  15   4   0   0   4   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   5   0   0  11   0 190   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.91044776119402

F1 scores:
[       nan 0.99636364 0.95089286 0.94495413 0.87048832 0.87128713
 0.95959596 0.88268156 0.99487179 1.         1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9767326121938184
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7d1d77b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.550, val_acc:0.417]
Epoch [2/120    avg_loss:2.390, val_acc:0.413]
Epoch [3/120    avg_loss:2.266, val_acc:0.446]
Epoch [4/120    avg_loss:2.156, val_acc:0.544]
Epoch [5/120    avg_loss:2.051, val_acc:0.597]
Epoch [6/120    avg_loss:1.941, val_acc:0.597]
Epoch [7/120    avg_loss:1.808, val_acc:0.625]
Epoch [8/120    avg_loss:1.676, val_acc:0.619]
Epoch [9/120    avg_loss:1.544, val_acc:0.649]
Epoch [10/120    avg_loss:1.480, val_acc:0.675]
Epoch [11/120    avg_loss:1.301, val_acc:0.798]
Epoch [12/120    avg_loss:1.222, val_acc:0.744]
Epoch [13/120    avg_loss:1.085, val_acc:0.736]
Epoch [14/120    avg_loss:0.960, val_acc:0.736]
Epoch [15/120    avg_loss:0.910, val_acc:0.825]
Epoch [16/120    avg_loss:0.826, val_acc:0.792]
Epoch [17/120    avg_loss:0.773, val_acc:0.792]
Epoch [18/120    avg_loss:0.721, val_acc:0.837]
Epoch [19/120    avg_loss:0.683, val_acc:0.831]
Epoch [20/120    avg_loss:0.602, val_acc:0.865]
Epoch [21/120    avg_loss:0.593, val_acc:0.899]
Epoch [22/120    avg_loss:0.583, val_acc:0.913]
Epoch [23/120    avg_loss:0.541, val_acc:0.885]
Epoch [24/120    avg_loss:0.508, val_acc:0.907]
Epoch [25/120    avg_loss:0.468, val_acc:0.933]
Epoch [26/120    avg_loss:0.418, val_acc:0.857]
Epoch [27/120    avg_loss:0.508, val_acc:0.899]
Epoch [28/120    avg_loss:0.454, val_acc:0.923]
Epoch [29/120    avg_loss:0.422, val_acc:0.901]
Epoch [30/120    avg_loss:0.383, val_acc:0.952]
Epoch [31/120    avg_loss:0.420, val_acc:0.905]
Epoch [32/120    avg_loss:0.362, val_acc:0.913]
Epoch [33/120    avg_loss:0.352, val_acc:0.940]
Epoch [34/120    avg_loss:0.335, val_acc:0.913]
Epoch [35/120    avg_loss:0.319, val_acc:0.915]
Epoch [36/120    avg_loss:0.328, val_acc:0.944]
Epoch [37/120    avg_loss:0.256, val_acc:0.938]
Epoch [38/120    avg_loss:0.268, val_acc:0.935]
Epoch [39/120    avg_loss:0.291, val_acc:0.921]
Epoch [40/120    avg_loss:0.260, val_acc:0.966]
Epoch [41/120    avg_loss:0.264, val_acc:0.923]
Epoch [42/120    avg_loss:0.309, val_acc:0.921]
Epoch [43/120    avg_loss:0.258, val_acc:0.960]
Epoch [44/120    avg_loss:0.204, val_acc:0.956]
Epoch [45/120    avg_loss:0.195, val_acc:0.944]
Epoch [46/120    avg_loss:0.265, val_acc:0.954]
Epoch [47/120    avg_loss:0.222, val_acc:0.956]
Epoch [48/120    avg_loss:0.162, val_acc:0.958]
Epoch [49/120    avg_loss:0.182, val_acc:0.962]
Epoch [50/120    avg_loss:0.201, val_acc:0.948]
Epoch [51/120    avg_loss:0.181, val_acc:0.964]
Epoch [52/120    avg_loss:0.173, val_acc:0.976]
Epoch [53/120    avg_loss:0.151, val_acc:0.974]
Epoch [54/120    avg_loss:0.179, val_acc:0.935]
Epoch [55/120    avg_loss:0.220, val_acc:0.968]
Epoch [56/120    avg_loss:0.173, val_acc:0.962]
Epoch [57/120    avg_loss:0.133, val_acc:0.964]
Epoch [58/120    avg_loss:0.126, val_acc:0.948]
Epoch [59/120    avg_loss:0.150, val_acc:0.964]
Epoch [60/120    avg_loss:0.212, val_acc:0.960]
Epoch [61/120    avg_loss:0.157, val_acc:0.978]
Epoch [62/120    avg_loss:0.145, val_acc:0.940]
Epoch [63/120    avg_loss:0.164, val_acc:0.978]
Epoch [64/120    avg_loss:0.137, val_acc:0.954]
Epoch [65/120    avg_loss:0.151, val_acc:0.984]
Epoch [66/120    avg_loss:0.090, val_acc:0.986]
Epoch [67/120    avg_loss:0.100, val_acc:0.980]
Epoch [68/120    avg_loss:0.093, val_acc:0.988]
Epoch [69/120    avg_loss:0.093, val_acc:0.976]
Epoch [70/120    avg_loss:0.088, val_acc:0.980]
Epoch [71/120    avg_loss:0.125, val_acc:0.944]
Epoch [72/120    avg_loss:0.102, val_acc:0.988]
Epoch [73/120    avg_loss:0.083, val_acc:0.984]
Epoch [74/120    avg_loss:0.084, val_acc:0.986]
Epoch [75/120    avg_loss:0.095, val_acc:0.980]
Epoch [76/120    avg_loss:0.123, val_acc:0.984]
Epoch [77/120    avg_loss:0.089, val_acc:0.988]
Epoch [78/120    avg_loss:0.072, val_acc:0.986]
Epoch [79/120    avg_loss:0.083, val_acc:0.964]
Epoch [80/120    avg_loss:0.086, val_acc:0.970]
Epoch [81/120    avg_loss:0.058, val_acc:0.988]
Epoch [82/120    avg_loss:0.058, val_acc:0.988]
Epoch [83/120    avg_loss:0.051, val_acc:0.992]
Epoch [84/120    avg_loss:0.047, val_acc:0.988]
Epoch [85/120    avg_loss:0.039, val_acc:0.994]
Epoch [86/120    avg_loss:0.052, val_acc:0.990]
Epoch [87/120    avg_loss:0.046, val_acc:0.988]
Epoch [88/120    avg_loss:0.047, val_acc:0.990]
Epoch [89/120    avg_loss:0.049, val_acc:0.984]
Epoch [90/120    avg_loss:0.073, val_acc:0.984]
Epoch [91/120    avg_loss:0.052, val_acc:0.990]
Epoch [92/120    avg_loss:0.061, val_acc:0.990]
Epoch [93/120    avg_loss:0.049, val_acc:0.990]
Epoch [94/120    avg_loss:0.029, val_acc:0.992]
Epoch [95/120    avg_loss:0.030, val_acc:0.992]
Epoch [96/120    avg_loss:0.055, val_acc:0.980]
Epoch [97/120    avg_loss:0.072, val_acc:0.986]
Epoch [98/120    avg_loss:0.037, val_acc:0.990]
Epoch [99/120    avg_loss:0.029, val_acc:0.992]
Epoch [100/120    avg_loss:0.025, val_acc:0.992]
Epoch [101/120    avg_loss:0.017, val_acc:0.992]
Epoch [102/120    avg_loss:0.020, val_acc:0.992]
Epoch [103/120    avg_loss:0.019, val_acc:0.992]
Epoch [104/120    avg_loss:0.024, val_acc:0.992]
Epoch [105/120    avg_loss:0.025, val_acc:0.992]
Epoch [106/120    avg_loss:0.020, val_acc:0.992]
Epoch [107/120    avg_loss:0.019, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.992]
Epoch [110/120    avg_loss:0.022, val_acc:0.992]
Epoch [111/120    avg_loss:0.018, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.992]
Epoch [113/120    avg_loss:0.019, val_acc:0.992]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.015, val_acc:0.992]
Epoch [116/120    avg_loss:0.020, val_acc:0.992]
Epoch [117/120    avg_loss:0.019, val_acc:0.992]
Epoch [118/120    avg_loss:0.020, val_acc:0.992]
Epoch [119/120    avg_loss:0.021, val_acc:0.992]
Epoch [120/120    avg_loss:0.021, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 225   0   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.97986577 0.98901099 0.96363636 0.94736842
 1.         0.96132597 0.99614891 1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9921664916860902
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f54cfd12ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.589, val_acc:0.312]
Epoch [2/120    avg_loss:2.429, val_acc:0.387]
Epoch [3/120    avg_loss:2.305, val_acc:0.448]
Epoch [4/120    avg_loss:2.202, val_acc:0.502]
Epoch [5/120    avg_loss:2.072, val_acc:0.500]
Epoch [6/120    avg_loss:1.955, val_acc:0.554]
Epoch [7/120    avg_loss:1.830, val_acc:0.565]
Epoch [8/120    avg_loss:1.695, val_acc:0.653]
Epoch [9/120    avg_loss:1.551, val_acc:0.738]
Epoch [10/120    avg_loss:1.426, val_acc:0.714]
Epoch [11/120    avg_loss:1.289, val_acc:0.744]
Epoch [12/120    avg_loss:1.175, val_acc:0.724]
Epoch [13/120    avg_loss:1.054, val_acc:0.823]
Epoch [14/120    avg_loss:0.946, val_acc:0.817]
Epoch [15/120    avg_loss:0.910, val_acc:0.798]
Epoch [16/120    avg_loss:0.839, val_acc:0.827]
Epoch [17/120    avg_loss:0.736, val_acc:0.855]
Epoch [18/120    avg_loss:0.660, val_acc:0.800]
Epoch [19/120    avg_loss:0.638, val_acc:0.841]
Epoch [20/120    avg_loss:0.610, val_acc:0.873]
Epoch [21/120    avg_loss:0.634, val_acc:0.849]
Epoch [22/120    avg_loss:0.538, val_acc:0.909]
Epoch [23/120    avg_loss:0.508, val_acc:0.887]
Epoch [24/120    avg_loss:0.467, val_acc:0.893]
Epoch [25/120    avg_loss:0.476, val_acc:0.859]
Epoch [26/120    avg_loss:0.531, val_acc:0.907]
Epoch [27/120    avg_loss:0.467, val_acc:0.881]
Epoch [28/120    avg_loss:0.491, val_acc:0.897]
Epoch [29/120    avg_loss:0.453, val_acc:0.911]
Epoch [30/120    avg_loss:0.390, val_acc:0.913]
Epoch [31/120    avg_loss:0.432, val_acc:0.938]
Epoch [32/120    avg_loss:0.404, val_acc:0.897]
Epoch [33/120    avg_loss:0.366, val_acc:0.927]
Epoch [34/120    avg_loss:0.420, val_acc:0.897]
Epoch [35/120    avg_loss:0.367, val_acc:0.911]
Epoch [36/120    avg_loss:0.313, val_acc:0.929]
Epoch [37/120    avg_loss:0.384, val_acc:0.905]
Epoch [38/120    avg_loss:0.310, val_acc:0.921]
Epoch [39/120    avg_loss:0.344, val_acc:0.911]
Epoch [40/120    avg_loss:0.303, val_acc:0.933]
Epoch [41/120    avg_loss:0.313, val_acc:0.905]
Epoch [42/120    avg_loss:0.327, val_acc:0.929]
Epoch [43/120    avg_loss:0.306, val_acc:0.875]
Epoch [44/120    avg_loss:0.289, val_acc:0.929]
Epoch [45/120    avg_loss:0.226, val_acc:0.950]
Epoch [46/120    avg_loss:0.207, val_acc:0.944]
Epoch [47/120    avg_loss:0.212, val_acc:0.954]
Epoch [48/120    avg_loss:0.186, val_acc:0.950]
Epoch [49/120    avg_loss:0.172, val_acc:0.958]
Epoch [50/120    avg_loss:0.193, val_acc:0.964]
Epoch [51/120    avg_loss:0.203, val_acc:0.960]
Epoch [52/120    avg_loss:0.176, val_acc:0.960]
Epoch [53/120    avg_loss:0.163, val_acc:0.960]
Epoch [54/120    avg_loss:0.186, val_acc:0.964]
Epoch [55/120    avg_loss:0.174, val_acc:0.964]
Epoch [56/120    avg_loss:0.156, val_acc:0.964]
Epoch [57/120    avg_loss:0.152, val_acc:0.962]
Epoch [58/120    avg_loss:0.154, val_acc:0.964]
Epoch [59/120    avg_loss:0.140, val_acc:0.960]
Epoch [60/120    avg_loss:0.174, val_acc:0.962]
Epoch [61/120    avg_loss:0.143, val_acc:0.962]
Epoch [62/120    avg_loss:0.166, val_acc:0.964]
Epoch [63/120    avg_loss:0.162, val_acc:0.964]
Epoch [64/120    avg_loss:0.153, val_acc:0.964]
Epoch [65/120    avg_loss:0.166, val_acc:0.964]
Epoch [66/120    avg_loss:0.162, val_acc:0.960]
Epoch [67/120    avg_loss:0.174, val_acc:0.960]
Epoch [68/120    avg_loss:0.139, val_acc:0.966]
Epoch [69/120    avg_loss:0.144, val_acc:0.962]
Epoch [70/120    avg_loss:0.167, val_acc:0.968]
Epoch [71/120    avg_loss:0.148, val_acc:0.964]
Epoch [72/120    avg_loss:0.156, val_acc:0.960]
Epoch [73/120    avg_loss:0.158, val_acc:0.968]
Epoch [74/120    avg_loss:0.144, val_acc:0.966]
Epoch [75/120    avg_loss:0.140, val_acc:0.968]
Epoch [76/120    avg_loss:0.147, val_acc:0.968]
Epoch [77/120    avg_loss:0.138, val_acc:0.964]
Epoch [78/120    avg_loss:0.143, val_acc:0.966]
Epoch [79/120    avg_loss:0.122, val_acc:0.962]
Epoch [80/120    avg_loss:0.133, val_acc:0.966]
Epoch [81/120    avg_loss:0.126, val_acc:0.970]
Epoch [82/120    avg_loss:0.123, val_acc:0.970]
Epoch [83/120    avg_loss:0.128, val_acc:0.970]
Epoch [84/120    avg_loss:0.145, val_acc:0.968]
Epoch [85/120    avg_loss:0.139, val_acc:0.970]
Epoch [86/120    avg_loss:0.128, val_acc:0.972]
Epoch [87/120    avg_loss:0.133, val_acc:0.970]
Epoch [88/120    avg_loss:0.143, val_acc:0.972]
Epoch [89/120    avg_loss:0.130, val_acc:0.974]
Epoch [90/120    avg_loss:0.119, val_acc:0.966]
Epoch [91/120    avg_loss:0.123, val_acc:0.970]
Epoch [92/120    avg_loss:0.110, val_acc:0.972]
Epoch [93/120    avg_loss:0.118, val_acc:0.972]
Epoch [94/120    avg_loss:0.121, val_acc:0.976]
Epoch [95/120    avg_loss:0.126, val_acc:0.970]
Epoch [96/120    avg_loss:0.129, val_acc:0.970]
Epoch [97/120    avg_loss:0.139, val_acc:0.974]
Epoch [98/120    avg_loss:0.119, val_acc:0.972]
Epoch [99/120    avg_loss:0.106, val_acc:0.976]
Epoch [100/120    avg_loss:0.110, val_acc:0.974]
Epoch [101/120    avg_loss:0.122, val_acc:0.972]
Epoch [102/120    avg_loss:0.127, val_acc:0.970]
Epoch [103/120    avg_loss:0.120, val_acc:0.974]
Epoch [104/120    avg_loss:0.122, val_acc:0.972]
Epoch [105/120    avg_loss:0.125, val_acc:0.976]
Epoch [106/120    avg_loss:0.110, val_acc:0.972]
Epoch [107/120    avg_loss:0.111, val_acc:0.982]
Epoch [108/120    avg_loss:0.118, val_acc:0.980]
Epoch [109/120    avg_loss:0.102, val_acc:0.976]
Epoch [110/120    avg_loss:0.114, val_acc:0.974]
Epoch [111/120    avg_loss:0.106, val_acc:0.972]
Epoch [112/120    avg_loss:0.110, val_acc:0.978]
Epoch [113/120    avg_loss:0.124, val_acc:0.976]
Epoch [114/120    avg_loss:0.123, val_acc:0.974]
Epoch [115/120    avg_loss:0.114, val_acc:0.976]
Epoch [116/120    avg_loss:0.106, val_acc:0.974]
Epoch [117/120    avg_loss:0.109, val_acc:0.982]
Epoch [118/120    avg_loss:0.091, val_acc:0.980]
Epoch [119/120    avg_loss:0.132, val_acc:0.970]
Epoch [120/120    avg_loss:0.105, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   1   0   0   1   0 204   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 0.99927061 0.96052632 0.98454746 0.91363636 0.90384615
 0.99512195 0.89411765 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9862311780249156
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96d44e5ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.538, val_acc:0.206]
Epoch [2/120    avg_loss:2.397, val_acc:0.288]
Epoch [3/120    avg_loss:2.280, val_acc:0.321]
Epoch [4/120    avg_loss:2.193, val_acc:0.448]
Epoch [5/120    avg_loss:2.060, val_acc:0.524]
Epoch [6/120    avg_loss:1.941, val_acc:0.558]
Epoch [7/120    avg_loss:1.824, val_acc:0.593]
Epoch [8/120    avg_loss:1.702, val_acc:0.635]
Epoch [9/120    avg_loss:1.610, val_acc:0.649]
Epoch [10/120    avg_loss:1.513, val_acc:0.655]
Epoch [11/120    avg_loss:1.391, val_acc:0.692]
Epoch [12/120    avg_loss:1.258, val_acc:0.698]
Epoch [13/120    avg_loss:1.154, val_acc:0.736]
Epoch [14/120    avg_loss:1.050, val_acc:0.841]
Epoch [15/120    avg_loss:0.928, val_acc:0.877]
Epoch [16/120    avg_loss:0.865, val_acc:0.851]
Epoch [17/120    avg_loss:0.861, val_acc:0.800]
Epoch [18/120    avg_loss:0.745, val_acc:0.867]
Epoch [19/120    avg_loss:0.665, val_acc:0.917]
Epoch [20/120    avg_loss:0.629, val_acc:0.885]
Epoch [21/120    avg_loss:0.564, val_acc:0.869]
Epoch [22/120    avg_loss:0.611, val_acc:0.899]
Epoch [23/120    avg_loss:0.459, val_acc:0.913]
Epoch [24/120    avg_loss:0.513, val_acc:0.923]
Epoch [25/120    avg_loss:0.493, val_acc:0.911]
Epoch [26/120    avg_loss:0.433, val_acc:0.923]
Epoch [27/120    avg_loss:0.401, val_acc:0.913]
Epoch [28/120    avg_loss:0.409, val_acc:0.929]
Epoch [29/120    avg_loss:0.377, val_acc:0.857]
Epoch [30/120    avg_loss:0.391, val_acc:0.925]
Epoch [31/120    avg_loss:0.413, val_acc:0.927]
Epoch [32/120    avg_loss:0.327, val_acc:0.933]
Epoch [33/120    avg_loss:0.396, val_acc:0.905]
Epoch [34/120    avg_loss:0.385, val_acc:0.931]
Epoch [35/120    avg_loss:0.429, val_acc:0.923]
Epoch [36/120    avg_loss:0.397, val_acc:0.925]
Epoch [37/120    avg_loss:0.345, val_acc:0.917]
Epoch [38/120    avg_loss:0.342, val_acc:0.948]
Epoch [39/120    avg_loss:0.286, val_acc:0.935]
Epoch [40/120    avg_loss:0.295, val_acc:0.933]
Epoch [41/120    avg_loss:0.277, val_acc:0.964]
Epoch [42/120    avg_loss:0.222, val_acc:0.946]
Epoch [43/120    avg_loss:0.254, val_acc:0.964]
Epoch [44/120    avg_loss:0.227, val_acc:0.950]
Epoch [45/120    avg_loss:0.202, val_acc:0.960]
Epoch [46/120    avg_loss:0.212, val_acc:0.942]
Epoch [47/120    avg_loss:0.238, val_acc:0.938]
Epoch [48/120    avg_loss:0.268, val_acc:0.962]
Epoch [49/120    avg_loss:0.252, val_acc:0.962]
Epoch [50/120    avg_loss:0.256, val_acc:0.966]
Epoch [51/120    avg_loss:0.193, val_acc:0.984]
Epoch [52/120    avg_loss:0.200, val_acc:0.921]
Epoch [53/120    avg_loss:0.234, val_acc:0.958]
Epoch [54/120    avg_loss:0.202, val_acc:0.970]
Epoch [55/120    avg_loss:0.230, val_acc:0.976]
Epoch [56/120    avg_loss:0.155, val_acc:0.988]
Epoch [57/120    avg_loss:0.156, val_acc:0.986]
Epoch [58/120    avg_loss:0.130, val_acc:0.982]
Epoch [59/120    avg_loss:0.122, val_acc:0.986]
Epoch [60/120    avg_loss:0.136, val_acc:0.976]
Epoch [61/120    avg_loss:0.110, val_acc:0.984]
Epoch [62/120    avg_loss:0.094, val_acc:0.992]
Epoch [63/120    avg_loss:0.116, val_acc:0.982]
Epoch [64/120    avg_loss:0.122, val_acc:0.980]
Epoch [65/120    avg_loss:0.109, val_acc:0.933]
Epoch [66/120    avg_loss:0.166, val_acc:0.944]
Epoch [67/120    avg_loss:0.171, val_acc:0.972]
Epoch [68/120    avg_loss:0.130, val_acc:0.944]
Epoch [69/120    avg_loss:0.130, val_acc:0.984]
Epoch [70/120    avg_loss:0.098, val_acc:0.980]
Epoch [71/120    avg_loss:0.089, val_acc:0.964]
Epoch [72/120    avg_loss:0.114, val_acc:0.984]
Epoch [73/120    avg_loss:0.110, val_acc:0.992]
Epoch [74/120    avg_loss:0.104, val_acc:0.984]
Epoch [75/120    avg_loss:0.117, val_acc:0.986]
Epoch [76/120    avg_loss:0.094, val_acc:0.988]
Epoch [77/120    avg_loss:0.071, val_acc:0.978]
Epoch [78/120    avg_loss:0.108, val_acc:0.996]
Epoch [79/120    avg_loss:0.071, val_acc:0.994]
Epoch [80/120    avg_loss:0.048, val_acc:0.992]
Epoch [81/120    avg_loss:0.043, val_acc:0.996]
Epoch [82/120    avg_loss:0.070, val_acc:0.990]
Epoch [83/120    avg_loss:0.066, val_acc:0.998]
Epoch [84/120    avg_loss:0.055, val_acc:0.996]
Epoch [85/120    avg_loss:0.041, val_acc:0.998]
Epoch [86/120    avg_loss:0.051, val_acc:1.000]
Epoch [87/120    avg_loss:0.033, val_acc:0.994]
Epoch [88/120    avg_loss:0.033, val_acc:0.998]
Epoch [89/120    avg_loss:0.031, val_acc:0.996]
Epoch [90/120    avg_loss:0.043, val_acc:0.986]
Epoch [91/120    avg_loss:0.037, val_acc:0.998]
Epoch [92/120    avg_loss:0.022, val_acc:1.000]
Epoch [93/120    avg_loss:0.026, val_acc:0.994]
Epoch [94/120    avg_loss:0.036, val_acc:0.992]
Epoch [95/120    avg_loss:0.047, val_acc:0.994]
Epoch [96/120    avg_loss:0.050, val_acc:0.988]
Epoch [97/120    avg_loss:0.141, val_acc:0.974]
Epoch [98/120    avg_loss:0.071, val_acc:0.992]
Epoch [99/120    avg_loss:0.070, val_acc:0.974]
Epoch [100/120    avg_loss:0.101, val_acc:0.988]
Epoch [101/120    avg_loss:0.109, val_acc:0.964]
Epoch [102/120    avg_loss:0.176, val_acc:0.984]
Epoch [103/120    avg_loss:0.075, val_acc:0.994]
Epoch [104/120    avg_loss:0.046, val_acc:0.996]
Epoch [105/120    avg_loss:0.035, val_acc:0.984]
Epoch [106/120    avg_loss:0.040, val_acc:0.998]
Epoch [107/120    avg_loss:0.024, val_acc:0.998]
Epoch [108/120    avg_loss:0.027, val_acc:0.998]
Epoch [109/120    avg_loss:0.023, val_acc:0.998]
Epoch [110/120    avg_loss:0.021, val_acc:0.998]
Epoch [111/120    avg_loss:0.024, val_acc:0.998]
Epoch [112/120    avg_loss:0.024, val_acc:0.998]
Epoch [113/120    avg_loss:0.020, val_acc:0.998]
Epoch [114/120    avg_loss:0.020, val_acc:0.998]
Epoch [115/120    avg_loss:0.022, val_acc:0.998]
Epoch [116/120    avg_loss:0.028, val_acc:0.998]
Epoch [117/120    avg_loss:0.028, val_acc:0.998]
Epoch [118/120    avg_loss:0.017, val_acc:0.998]
Epoch [119/120    avg_loss:0.020, val_acc:0.998]
Epoch [120/120    avg_loss:0.024, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.98648649 0.99782135 0.94323144 0.90909091
 1.         0.96703297 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9921658901734438
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f55b0630a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.595, val_acc:0.105]
Epoch [2/120    avg_loss:2.434, val_acc:0.321]
Epoch [3/120    avg_loss:2.311, val_acc:0.333]
Epoch [4/120    avg_loss:2.209, val_acc:0.458]
Epoch [5/120    avg_loss:2.092, val_acc:0.508]
Epoch [6/120    avg_loss:1.983, val_acc:0.544]
Epoch [7/120    avg_loss:1.849, val_acc:0.591]
Epoch [8/120    avg_loss:1.751, val_acc:0.625]
Epoch [9/120    avg_loss:1.621, val_acc:0.653]
Epoch [10/120    avg_loss:1.465, val_acc:0.728]
Epoch [11/120    avg_loss:1.357, val_acc:0.748]
Epoch [12/120    avg_loss:1.221, val_acc:0.774]
Epoch [13/120    avg_loss:1.090, val_acc:0.758]
Epoch [14/120    avg_loss:0.991, val_acc:0.821]
Epoch [15/120    avg_loss:0.885, val_acc:0.794]
Epoch [16/120    avg_loss:0.795, val_acc:0.889]
Epoch [17/120    avg_loss:0.761, val_acc:0.867]
Epoch [18/120    avg_loss:0.653, val_acc:0.897]
Epoch [19/120    avg_loss:0.703, val_acc:0.901]
Epoch [20/120    avg_loss:0.602, val_acc:0.927]
Epoch [21/120    avg_loss:0.576, val_acc:0.915]
Epoch [22/120    avg_loss:0.521, val_acc:0.901]
Epoch [23/120    avg_loss:0.536, val_acc:0.919]
Epoch [24/120    avg_loss:0.423, val_acc:0.923]
Epoch [25/120    avg_loss:0.461, val_acc:0.921]
Epoch [26/120    avg_loss:0.494, val_acc:0.905]
Epoch [27/120    avg_loss:0.426, val_acc:0.919]
Epoch [28/120    avg_loss:0.428, val_acc:0.933]
Epoch [29/120    avg_loss:0.428, val_acc:0.921]
Epoch [30/120    avg_loss:0.409, val_acc:0.931]
Epoch [31/120    avg_loss:0.380, val_acc:0.929]
Epoch [32/120    avg_loss:0.410, val_acc:0.931]
Epoch [33/120    avg_loss:0.372, val_acc:0.929]
Epoch [34/120    avg_loss:0.346, val_acc:0.877]
Epoch [35/120    avg_loss:0.366, val_acc:0.958]
Epoch [36/120    avg_loss:0.322, val_acc:0.935]
Epoch [37/120    avg_loss:0.280, val_acc:0.960]
Epoch [38/120    avg_loss:0.303, val_acc:0.962]
Epoch [39/120    avg_loss:0.368, val_acc:0.913]
Epoch [40/120    avg_loss:0.345, val_acc:0.944]
Epoch [41/120    avg_loss:0.323, val_acc:0.954]
Epoch [42/120    avg_loss:0.270, val_acc:0.972]
Epoch [43/120    avg_loss:0.272, val_acc:0.938]
Epoch [44/120    avg_loss:0.235, val_acc:0.942]
Epoch [45/120    avg_loss:0.258, val_acc:0.966]
Epoch [46/120    avg_loss:0.247, val_acc:0.974]
Epoch [47/120    avg_loss:0.244, val_acc:0.956]
Epoch [48/120    avg_loss:0.217, val_acc:0.960]
Epoch [49/120    avg_loss:0.202, val_acc:0.966]
Epoch [50/120    avg_loss:0.237, val_acc:0.899]
Epoch [51/120    avg_loss:0.238, val_acc:0.972]
Epoch [52/120    avg_loss:0.180, val_acc:0.974]
Epoch [53/120    avg_loss:0.162, val_acc:0.972]
Epoch [54/120    avg_loss:0.155, val_acc:0.966]
Epoch [55/120    avg_loss:0.172, val_acc:0.960]
Epoch [56/120    avg_loss:0.140, val_acc:0.974]
Epoch [57/120    avg_loss:0.163, val_acc:0.950]
Epoch [58/120    avg_loss:0.189, val_acc:0.980]
Epoch [59/120    avg_loss:0.185, val_acc:0.968]
Epoch [60/120    avg_loss:0.149, val_acc:0.964]
Epoch [61/120    avg_loss:0.123, val_acc:0.984]
Epoch [62/120    avg_loss:0.142, val_acc:0.968]
Epoch [63/120    avg_loss:0.106, val_acc:0.980]
Epoch [64/120    avg_loss:0.088, val_acc:0.980]
Epoch [65/120    avg_loss:0.159, val_acc:0.933]
Epoch [66/120    avg_loss:0.120, val_acc:0.980]
Epoch [67/120    avg_loss:0.102, val_acc:0.980]
Epoch [68/120    avg_loss:0.114, val_acc:0.976]
Epoch [69/120    avg_loss:0.184, val_acc:0.948]
Epoch [70/120    avg_loss:0.102, val_acc:0.974]
Epoch [71/120    avg_loss:0.119, val_acc:0.974]
Epoch [72/120    avg_loss:0.084, val_acc:0.972]
Epoch [73/120    avg_loss:0.088, val_acc:0.972]
Epoch [74/120    avg_loss:0.102, val_acc:0.980]
Epoch [75/120    avg_loss:0.076, val_acc:0.982]
Epoch [76/120    avg_loss:0.056, val_acc:0.982]
Epoch [77/120    avg_loss:0.060, val_acc:0.982]
Epoch [78/120    avg_loss:0.058, val_acc:0.980]
Epoch [79/120    avg_loss:0.051, val_acc:0.984]
Epoch [80/120    avg_loss:0.050, val_acc:0.986]
Epoch [81/120    avg_loss:0.048, val_acc:0.986]
Epoch [82/120    avg_loss:0.044, val_acc:0.986]
Epoch [83/120    avg_loss:0.052, val_acc:0.986]
Epoch [84/120    avg_loss:0.046, val_acc:0.986]
Epoch [85/120    avg_loss:0.046, val_acc:0.986]
Epoch [86/120    avg_loss:0.048, val_acc:0.986]
Epoch [87/120    avg_loss:0.050, val_acc:0.986]
Epoch [88/120    avg_loss:0.050, val_acc:0.984]
Epoch [89/120    avg_loss:0.051, val_acc:0.984]
Epoch [90/120    avg_loss:0.047, val_acc:0.984]
Epoch [91/120    avg_loss:0.040, val_acc:0.988]
Epoch [92/120    avg_loss:0.046, val_acc:0.988]
Epoch [93/120    avg_loss:0.040, val_acc:0.986]
Epoch [94/120    avg_loss:0.040, val_acc:0.986]
Epoch [95/120    avg_loss:0.047, val_acc:0.990]
Epoch [96/120    avg_loss:0.039, val_acc:0.988]
Epoch [97/120    avg_loss:0.044, val_acc:0.988]
Epoch [98/120    avg_loss:0.041, val_acc:0.990]
Epoch [99/120    avg_loss:0.039, val_acc:0.988]
Epoch [100/120    avg_loss:0.034, val_acc:0.988]
Epoch [101/120    avg_loss:0.036, val_acc:0.988]
Epoch [102/120    avg_loss:0.044, val_acc:0.990]
Epoch [103/120    avg_loss:0.032, val_acc:0.988]
Epoch [104/120    avg_loss:0.038, val_acc:0.988]
Epoch [105/120    avg_loss:0.038, val_acc:0.988]
Epoch [106/120    avg_loss:0.039, val_acc:0.988]
Epoch [107/120    avg_loss:0.037, val_acc:0.988]
Epoch [108/120    avg_loss:0.039, val_acc:0.988]
Epoch [109/120    avg_loss:0.033, val_acc:0.988]
Epoch [110/120    avg_loss:0.036, val_acc:0.984]
Epoch [111/120    avg_loss:0.038, val_acc:0.986]
Epoch [112/120    avg_loss:0.040, val_acc:0.990]
Epoch [113/120    avg_loss:0.033, val_acc:0.990]
Epoch [114/120    avg_loss:0.033, val_acc:0.990]
Epoch [115/120    avg_loss:0.037, val_acc:0.990]
Epoch [116/120    avg_loss:0.033, val_acc:0.990]
Epoch [117/120    avg_loss:0.037, val_acc:0.988]
Epoch [118/120    avg_loss:0.034, val_acc:0.988]
Epoch [119/120    avg_loss:0.029, val_acc:0.988]
Epoch [120/120    avg_loss:0.030, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99854227 0.98426966 0.98454746 0.94432071 0.94039735
 0.99512195 0.96132597 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9916912689130558
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb6218ef978>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.419]
Epoch [2/120    avg_loss:2.377, val_acc:0.438]
Epoch [3/120    avg_loss:2.216, val_acc:0.417]
Epoch [4/120    avg_loss:2.086, val_acc:0.508]
Epoch [5/120    avg_loss:1.967, val_acc:0.544]
Epoch [6/120    avg_loss:1.828, val_acc:0.558]
Epoch [7/120    avg_loss:1.707, val_acc:0.532]
Epoch [8/120    avg_loss:1.552, val_acc:0.639]
Epoch [9/120    avg_loss:1.424, val_acc:0.683]
Epoch [10/120    avg_loss:1.295, val_acc:0.716]
Epoch [11/120    avg_loss:1.217, val_acc:0.744]
Epoch [12/120    avg_loss:1.114, val_acc:0.746]
Epoch [13/120    avg_loss:0.979, val_acc:0.855]
Epoch [14/120    avg_loss:0.876, val_acc:0.825]
Epoch [15/120    avg_loss:0.847, val_acc:0.871]
Epoch [16/120    avg_loss:0.728, val_acc:0.887]
Epoch [17/120    avg_loss:0.666, val_acc:0.909]
Epoch [18/120    avg_loss:0.610, val_acc:0.893]
Epoch [19/120    avg_loss:0.565, val_acc:0.899]
Epoch [20/120    avg_loss:0.504, val_acc:0.911]
Epoch [21/120    avg_loss:0.518, val_acc:0.903]
Epoch [22/120    avg_loss:0.459, val_acc:0.917]
Epoch [23/120    avg_loss:0.416, val_acc:0.857]
Epoch [24/120    avg_loss:0.474, val_acc:0.919]
Epoch [25/120    avg_loss:0.415, val_acc:0.929]
Epoch [26/120    avg_loss:0.408, val_acc:0.935]
Epoch [27/120    avg_loss:0.412, val_acc:0.831]
Epoch [28/120    avg_loss:0.489, val_acc:0.909]
Epoch [29/120    avg_loss:0.395, val_acc:0.915]
Epoch [30/120    avg_loss:0.362, val_acc:0.907]
Epoch [31/120    avg_loss:0.436, val_acc:0.917]
Epoch [32/120    avg_loss:0.348, val_acc:0.938]
Epoch [33/120    avg_loss:0.370, val_acc:0.877]
Epoch [34/120    avg_loss:0.356, val_acc:0.897]
Epoch [35/120    avg_loss:0.379, val_acc:0.942]
Epoch [36/120    avg_loss:0.362, val_acc:0.942]
Epoch [37/120    avg_loss:0.322, val_acc:0.940]
Epoch [38/120    avg_loss:0.286, val_acc:0.956]
Epoch [39/120    avg_loss:0.254, val_acc:0.966]
Epoch [40/120    avg_loss:0.250, val_acc:0.958]
Epoch [41/120    avg_loss:0.297, val_acc:0.927]
Epoch [42/120    avg_loss:0.291, val_acc:0.946]
Epoch [43/120    avg_loss:0.230, val_acc:0.974]
Epoch [44/120    avg_loss:0.248, val_acc:0.956]
Epoch [45/120    avg_loss:0.233, val_acc:0.952]
Epoch [46/120    avg_loss:0.220, val_acc:0.962]
Epoch [47/120    avg_loss:0.197, val_acc:0.972]
Epoch [48/120    avg_loss:0.148, val_acc:0.964]
Epoch [49/120    avg_loss:0.277, val_acc:0.962]
Epoch [50/120    avg_loss:0.205, val_acc:0.950]
Epoch [51/120    avg_loss:0.217, val_acc:0.946]
Epoch [52/120    avg_loss:0.202, val_acc:0.948]
Epoch [53/120    avg_loss:0.233, val_acc:0.958]
Epoch [54/120    avg_loss:0.218, val_acc:0.972]
Epoch [55/120    avg_loss:0.289, val_acc:0.942]
Epoch [56/120    avg_loss:0.200, val_acc:0.974]
Epoch [57/120    avg_loss:0.196, val_acc:0.970]
Epoch [58/120    avg_loss:0.188, val_acc:0.978]
Epoch [59/120    avg_loss:0.160, val_acc:0.962]
Epoch [60/120    avg_loss:0.202, val_acc:0.966]
Epoch [61/120    avg_loss:0.136, val_acc:0.962]
Epoch [62/120    avg_loss:0.124, val_acc:0.988]
Epoch [63/120    avg_loss:0.158, val_acc:0.956]
Epoch [64/120    avg_loss:0.193, val_acc:0.970]
Epoch [65/120    avg_loss:0.121, val_acc:0.966]
Epoch [66/120    avg_loss:0.130, val_acc:0.974]
Epoch [67/120    avg_loss:0.115, val_acc:0.978]
Epoch [68/120    avg_loss:0.147, val_acc:0.974]
Epoch [69/120    avg_loss:0.189, val_acc:0.976]
Epoch [70/120    avg_loss:0.164, val_acc:0.958]
Epoch [71/120    avg_loss:0.144, val_acc:0.978]
Epoch [72/120    avg_loss:0.113, val_acc:0.982]
Epoch [73/120    avg_loss:0.101, val_acc:0.990]
Epoch [74/120    avg_loss:0.098, val_acc:0.976]
Epoch [75/120    avg_loss:0.123, val_acc:0.980]
Epoch [76/120    avg_loss:0.125, val_acc:0.950]
Epoch [77/120    avg_loss:0.171, val_acc:0.974]
Epoch [78/120    avg_loss:0.084, val_acc:0.970]
Epoch [79/120    avg_loss:0.128, val_acc:0.980]
Epoch [80/120    avg_loss:0.082, val_acc:0.978]
Epoch [81/120    avg_loss:0.078, val_acc:0.978]
Epoch [82/120    avg_loss:0.071, val_acc:0.980]
Epoch [83/120    avg_loss:0.087, val_acc:0.994]
Epoch [84/120    avg_loss:0.105, val_acc:0.988]
Epoch [85/120    avg_loss:0.085, val_acc:0.990]
Epoch [86/120    avg_loss:0.092, val_acc:0.984]
Epoch [87/120    avg_loss:0.088, val_acc:0.982]
Epoch [88/120    avg_loss:0.057, val_acc:0.988]
Epoch [89/120    avg_loss:0.053, val_acc:0.988]
Epoch [90/120    avg_loss:0.047, val_acc:0.988]
Epoch [91/120    avg_loss:0.065, val_acc:0.982]
Epoch [92/120    avg_loss:0.062, val_acc:0.990]
Epoch [93/120    avg_loss:0.054, val_acc:0.986]
Epoch [94/120    avg_loss:0.057, val_acc:0.988]
Epoch [95/120    avg_loss:0.038, val_acc:0.992]
Epoch [96/120    avg_loss:0.058, val_acc:0.988]
Epoch [97/120    avg_loss:0.041, val_acc:0.994]
Epoch [98/120    avg_loss:0.029, val_acc:0.994]
Epoch [99/120    avg_loss:0.026, val_acc:0.994]
Epoch [100/120    avg_loss:0.027, val_acc:0.994]
Epoch [101/120    avg_loss:0.028, val_acc:0.992]
Epoch [102/120    avg_loss:0.025, val_acc:0.994]
Epoch [103/120    avg_loss:0.024, val_acc:0.994]
Epoch [104/120    avg_loss:0.022, val_acc:0.994]
Epoch [105/120    avg_loss:0.025, val_acc:0.994]
Epoch [106/120    avg_loss:0.028, val_acc:0.994]
Epoch [107/120    avg_loss:0.024, val_acc:0.994]
Epoch [108/120    avg_loss:0.025, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.022, val_acc:0.994]
Epoch [111/120    avg_loss:0.031, val_acc:0.992]
Epoch [112/120    avg_loss:0.020, val_acc:0.994]
Epoch [113/120    avg_loss:0.028, val_acc:0.994]
Epoch [114/120    avg_loss:0.028, val_acc:0.994]
Epoch [115/120    avg_loss:0.018, val_acc:0.994]
Epoch [116/120    avg_loss:0.021, val_acc:0.992]
Epoch [117/120    avg_loss:0.027, val_acc:0.992]
Epoch [118/120    avg_loss:0.018, val_acc:0.992]
Epoch [119/120    avg_loss:0.020, val_acc:0.994]
Epoch [120/120    avg_loss:0.025, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.98206278 0.99782135 0.94117647 0.91390728
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919290795577341
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa02d72bac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.562, val_acc:0.472]
Epoch [2/120    avg_loss:2.368, val_acc:0.415]
Epoch [3/120    avg_loss:2.227, val_acc:0.435]
Epoch [4/120    avg_loss:2.122, val_acc:0.482]
Epoch [5/120    avg_loss:1.990, val_acc:0.530]
Epoch [6/120    avg_loss:1.870, val_acc:0.558]
Epoch [7/120    avg_loss:1.730, val_acc:0.569]
Epoch [8/120    avg_loss:1.595, val_acc:0.633]
Epoch [9/120    avg_loss:1.495, val_acc:0.679]
Epoch [10/120    avg_loss:1.361, val_acc:0.716]
Epoch [11/120    avg_loss:1.224, val_acc:0.712]
Epoch [12/120    avg_loss:1.127, val_acc:0.744]
Epoch [13/120    avg_loss:1.023, val_acc:0.732]
Epoch [14/120    avg_loss:0.990, val_acc:0.758]
Epoch [15/120    avg_loss:0.911, val_acc:0.762]
Epoch [16/120    avg_loss:0.821, val_acc:0.792]
Epoch [17/120    avg_loss:0.761, val_acc:0.869]
Epoch [18/120    avg_loss:0.680, val_acc:0.869]
Epoch [19/120    avg_loss:0.632, val_acc:0.871]
Epoch [20/120    avg_loss:0.614, val_acc:0.887]
Epoch [21/120    avg_loss:0.571, val_acc:0.895]
Epoch [22/120    avg_loss:0.527, val_acc:0.889]
Epoch [23/120    avg_loss:0.493, val_acc:0.899]
Epoch [24/120    avg_loss:0.443, val_acc:0.913]
Epoch [25/120    avg_loss:0.458, val_acc:0.909]
Epoch [26/120    avg_loss:0.464, val_acc:0.929]
Epoch [27/120    avg_loss:0.426, val_acc:0.907]
Epoch [28/120    avg_loss:0.435, val_acc:0.905]
Epoch [29/120    avg_loss:0.421, val_acc:0.915]
Epoch [30/120    avg_loss:0.381, val_acc:0.907]
Epoch [31/120    avg_loss:0.342, val_acc:0.935]
Epoch [32/120    avg_loss:0.338, val_acc:0.911]
Epoch [33/120    avg_loss:0.379, val_acc:0.891]
Epoch [34/120    avg_loss:0.347, val_acc:0.942]
Epoch [35/120    avg_loss:0.312, val_acc:0.931]
Epoch [36/120    avg_loss:0.284, val_acc:0.942]
Epoch [37/120    avg_loss:0.351, val_acc:0.919]
Epoch [38/120    avg_loss:0.292, val_acc:0.942]
Epoch [39/120    avg_loss:0.292, val_acc:0.923]
Epoch [40/120    avg_loss:0.271, val_acc:0.948]
Epoch [41/120    avg_loss:0.260, val_acc:0.948]
Epoch [42/120    avg_loss:0.213, val_acc:0.942]
Epoch [43/120    avg_loss:0.255, val_acc:0.942]
Epoch [44/120    avg_loss:0.271, val_acc:0.933]
Epoch [45/120    avg_loss:0.237, val_acc:0.950]
Epoch [46/120    avg_loss:0.212, val_acc:0.962]
Epoch [47/120    avg_loss:0.170, val_acc:0.944]
Epoch [48/120    avg_loss:0.210, val_acc:0.935]
Epoch [49/120    avg_loss:0.259, val_acc:0.915]
Epoch [50/120    avg_loss:0.298, val_acc:0.923]
Epoch [51/120    avg_loss:0.247, val_acc:0.946]
Epoch [52/120    avg_loss:0.248, val_acc:0.958]
Epoch [53/120    avg_loss:0.172, val_acc:0.950]
Epoch [54/120    avg_loss:0.174, val_acc:0.958]
Epoch [55/120    avg_loss:0.175, val_acc:0.966]
Epoch [56/120    avg_loss:0.193, val_acc:0.958]
Epoch [57/120    avg_loss:0.145, val_acc:0.968]
Epoch [58/120    avg_loss:0.180, val_acc:0.956]
Epoch [59/120    avg_loss:0.198, val_acc:0.942]
Epoch [60/120    avg_loss:0.182, val_acc:0.933]
Epoch [61/120    avg_loss:0.171, val_acc:0.974]
Epoch [62/120    avg_loss:0.169, val_acc:0.958]
Epoch [63/120    avg_loss:0.129, val_acc:0.968]
Epoch [64/120    avg_loss:0.178, val_acc:0.948]
Epoch [65/120    avg_loss:0.135, val_acc:0.954]
Epoch [66/120    avg_loss:0.155, val_acc:0.956]
Epoch [67/120    avg_loss:0.109, val_acc:0.974]
Epoch [68/120    avg_loss:0.116, val_acc:0.966]
Epoch [69/120    avg_loss:0.097, val_acc:0.962]
Epoch [70/120    avg_loss:0.105, val_acc:0.952]
Epoch [71/120    avg_loss:0.095, val_acc:0.964]
Epoch [72/120    avg_loss:0.135, val_acc:0.954]
Epoch [73/120    avg_loss:0.160, val_acc:0.970]
Epoch [74/120    avg_loss:0.127, val_acc:0.964]
Epoch [75/120    avg_loss:0.084, val_acc:0.964]
Epoch [76/120    avg_loss:0.111, val_acc:0.980]
Epoch [77/120    avg_loss:0.081, val_acc:0.974]
Epoch [78/120    avg_loss:0.083, val_acc:0.972]
Epoch [79/120    avg_loss:0.080, val_acc:0.968]
Epoch [80/120    avg_loss:0.094, val_acc:0.964]
Epoch [81/120    avg_loss:0.093, val_acc:0.974]
Epoch [82/120    avg_loss:0.075, val_acc:0.978]
Epoch [83/120    avg_loss:0.062, val_acc:0.972]
Epoch [84/120    avg_loss:0.068, val_acc:0.970]
Epoch [85/120    avg_loss:0.095, val_acc:0.968]
Epoch [86/120    avg_loss:0.091, val_acc:0.974]
Epoch [87/120    avg_loss:0.072, val_acc:0.972]
Epoch [88/120    avg_loss:0.055, val_acc:0.976]
Epoch [89/120    avg_loss:0.053, val_acc:0.970]
Epoch [90/120    avg_loss:0.067, val_acc:0.976]
Epoch [91/120    avg_loss:0.036, val_acc:0.984]
Epoch [92/120    avg_loss:0.040, val_acc:0.980]
Epoch [93/120    avg_loss:0.041, val_acc:0.980]
Epoch [94/120    avg_loss:0.038, val_acc:0.980]
Epoch [95/120    avg_loss:0.038, val_acc:0.978]
Epoch [96/120    avg_loss:0.036, val_acc:0.980]
Epoch [97/120    avg_loss:0.026, val_acc:0.980]
Epoch [98/120    avg_loss:0.034, val_acc:0.980]
Epoch [99/120    avg_loss:0.036, val_acc:0.984]
Epoch [100/120    avg_loss:0.035, val_acc:0.984]
Epoch [101/120    avg_loss:0.040, val_acc:0.980]
Epoch [102/120    avg_loss:0.044, val_acc:0.980]
Epoch [103/120    avg_loss:0.027, val_acc:0.980]
Epoch [104/120    avg_loss:0.029, val_acc:0.980]
Epoch [105/120    avg_loss:0.031, val_acc:0.982]
Epoch [106/120    avg_loss:0.025, val_acc:0.978]
Epoch [107/120    avg_loss:0.032, val_acc:0.978]
Epoch [108/120    avg_loss:0.028, val_acc:0.978]
Epoch [109/120    avg_loss:0.027, val_acc:0.982]
Epoch [110/120    avg_loss:0.032, val_acc:0.980]
Epoch [111/120    avg_loss:0.032, val_acc:0.982]
Epoch [112/120    avg_loss:0.029, val_acc:0.982]
Epoch [113/120    avg_loss:0.025, val_acc:0.980]
Epoch [114/120    avg_loss:0.026, val_acc:0.980]
Epoch [115/120    avg_loss:0.024, val_acc:0.980]
Epoch [116/120    avg_loss:0.026, val_acc:0.980]
Epoch [117/120    avg_loss:0.030, val_acc:0.980]
Epoch [118/120    avg_loss:0.027, val_acc:0.980]
Epoch [119/120    avg_loss:0.023, val_acc:0.980]
Epoch [120/120    avg_loss:0.030, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 209  14   0   0   0   5   1   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.97767857 0.95216401 0.92715232 0.93770492
 1.         0.94972067 0.99359795 0.99893276 1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9881304502492251
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa18299ea90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.588, val_acc:0.331]
Epoch [2/120    avg_loss:2.451, val_acc:0.355]
Epoch [3/120    avg_loss:2.332, val_acc:0.333]
Epoch [4/120    avg_loss:2.219, val_acc:0.470]
Epoch [5/120    avg_loss:2.107, val_acc:0.565]
Epoch [6/120    avg_loss:1.998, val_acc:0.573]
Epoch [7/120    avg_loss:1.860, val_acc:0.641]
Epoch [8/120    avg_loss:1.709, val_acc:0.712]
Epoch [9/120    avg_loss:1.563, val_acc:0.752]
Epoch [10/120    avg_loss:1.422, val_acc:0.776]
Epoch [11/120    avg_loss:1.296, val_acc:0.784]
Epoch [12/120    avg_loss:1.150, val_acc:0.819]
Epoch [13/120    avg_loss:1.027, val_acc:0.855]
Epoch [14/120    avg_loss:0.906, val_acc:0.865]
Epoch [15/120    avg_loss:0.826, val_acc:0.855]
Epoch [16/120    avg_loss:0.732, val_acc:0.897]
Epoch [17/120    avg_loss:0.681, val_acc:0.875]
Epoch [18/120    avg_loss:0.630, val_acc:0.845]
Epoch [19/120    avg_loss:0.589, val_acc:0.909]
Epoch [20/120    avg_loss:0.490, val_acc:0.873]
Epoch [21/120    avg_loss:0.518, val_acc:0.881]
Epoch [22/120    avg_loss:0.508, val_acc:0.895]
Epoch [23/120    avg_loss:0.459, val_acc:0.911]
Epoch [24/120    avg_loss:0.452, val_acc:0.905]
Epoch [25/120    avg_loss:0.453, val_acc:0.905]
Epoch [26/120    avg_loss:0.463, val_acc:0.909]
Epoch [27/120    avg_loss:0.450, val_acc:0.925]
Epoch [28/120    avg_loss:0.387, val_acc:0.913]
Epoch [29/120    avg_loss:0.420, val_acc:0.919]
Epoch [30/120    avg_loss:0.374, val_acc:0.921]
Epoch [31/120    avg_loss:0.362, val_acc:0.913]
Epoch [32/120    avg_loss:0.296, val_acc:0.923]
Epoch [33/120    avg_loss:0.326, val_acc:0.929]
Epoch [34/120    avg_loss:0.293, val_acc:0.938]
Epoch [35/120    avg_loss:0.319, val_acc:0.950]
Epoch [36/120    avg_loss:0.272, val_acc:0.940]
Epoch [37/120    avg_loss:0.237, val_acc:0.954]
Epoch [38/120    avg_loss:0.271, val_acc:0.931]
Epoch [39/120    avg_loss:0.281, val_acc:0.938]
Epoch [40/120    avg_loss:0.218, val_acc:0.952]
Epoch [41/120    avg_loss:0.239, val_acc:0.935]
Epoch [42/120    avg_loss:0.291, val_acc:0.964]
Epoch [43/120    avg_loss:0.203, val_acc:0.962]
Epoch [44/120    avg_loss:0.189, val_acc:0.968]
Epoch [45/120    avg_loss:0.183, val_acc:0.946]
Epoch [46/120    avg_loss:0.217, val_acc:0.952]
Epoch [47/120    avg_loss:0.203, val_acc:0.962]
Epoch [48/120    avg_loss:0.195, val_acc:0.956]
Epoch [49/120    avg_loss:0.179, val_acc:0.974]
Epoch [50/120    avg_loss:0.149, val_acc:0.980]
Epoch [51/120    avg_loss:0.133, val_acc:0.974]
Epoch [52/120    avg_loss:0.170, val_acc:0.976]
Epoch [53/120    avg_loss:0.173, val_acc:0.952]
Epoch [54/120    avg_loss:0.167, val_acc:0.986]
Epoch [55/120    avg_loss:0.129, val_acc:0.968]
Epoch [56/120    avg_loss:0.247, val_acc:0.970]
Epoch [57/120    avg_loss:0.206, val_acc:0.968]
Epoch [58/120    avg_loss:0.131, val_acc:0.964]
Epoch [59/120    avg_loss:0.116, val_acc:0.968]
Epoch [60/120    avg_loss:0.097, val_acc:0.972]
Epoch [61/120    avg_loss:0.110, val_acc:0.982]
Epoch [62/120    avg_loss:0.104, val_acc:0.974]
Epoch [63/120    avg_loss:0.142, val_acc:0.986]
Epoch [64/120    avg_loss:0.109, val_acc:0.976]
Epoch [65/120    avg_loss:0.091, val_acc:0.966]
Epoch [66/120    avg_loss:0.091, val_acc:0.986]
Epoch [67/120    avg_loss:0.092, val_acc:0.962]
Epoch [68/120    avg_loss:0.102, val_acc:0.982]
Epoch [69/120    avg_loss:0.096, val_acc:0.970]
Epoch [70/120    avg_loss:0.089, val_acc:0.970]
Epoch [71/120    avg_loss:0.109, val_acc:0.980]
Epoch [72/120    avg_loss:0.136, val_acc:0.919]
Epoch [73/120    avg_loss:0.125, val_acc:0.976]
Epoch [74/120    avg_loss:0.068, val_acc:0.988]
Epoch [75/120    avg_loss:0.077, val_acc:0.972]
Epoch [76/120    avg_loss:0.166, val_acc:0.970]
Epoch [77/120    avg_loss:0.096, val_acc:0.980]
Epoch [78/120    avg_loss:0.089, val_acc:0.984]
Epoch [79/120    avg_loss:0.093, val_acc:0.956]
Epoch [80/120    avg_loss:0.093, val_acc:0.978]
Epoch [81/120    avg_loss:0.064, val_acc:0.992]
Epoch [82/120    avg_loss:0.052, val_acc:0.980]
Epoch [83/120    avg_loss:0.056, val_acc:0.982]
Epoch [84/120    avg_loss:0.069, val_acc:0.982]
Epoch [85/120    avg_loss:0.052, val_acc:0.988]
Epoch [86/120    avg_loss:0.059, val_acc:0.986]
Epoch [87/120    avg_loss:0.061, val_acc:0.986]
Epoch [88/120    avg_loss:0.069, val_acc:0.994]
Epoch [89/120    avg_loss:0.047, val_acc:0.978]
Epoch [90/120    avg_loss:0.071, val_acc:0.978]
Epoch [91/120    avg_loss:0.074, val_acc:0.984]
Epoch [92/120    avg_loss:0.050, val_acc:0.990]
Epoch [93/120    avg_loss:0.040, val_acc:0.980]
Epoch [94/120    avg_loss:0.069, val_acc:0.992]
Epoch [95/120    avg_loss:0.059, val_acc:0.988]
Epoch [96/120    avg_loss:0.068, val_acc:0.982]
Epoch [97/120    avg_loss:0.083, val_acc:0.980]
Epoch [98/120    avg_loss:0.067, val_acc:0.986]
Epoch [99/120    avg_loss:0.032, val_acc:0.990]
Epoch [100/120    avg_loss:0.038, val_acc:0.986]
Epoch [101/120    avg_loss:0.023, val_acc:0.992]
Epoch [102/120    avg_loss:0.020, val_acc:0.992]
Epoch [103/120    avg_loss:0.018, val_acc:0.992]
Epoch [104/120    avg_loss:0.018, val_acc:0.992]
Epoch [105/120    avg_loss:0.018, val_acc:0.992]
Epoch [106/120    avg_loss:0.018, val_acc:0.992]
Epoch [107/120    avg_loss:0.015, val_acc:0.992]
Epoch [108/120    avg_loss:0.016, val_acc:0.992]
Epoch [109/120    avg_loss:0.020, val_acc:0.994]
Epoch [110/120    avg_loss:0.017, val_acc:0.994]
Epoch [111/120    avg_loss:0.016, val_acc:0.994]
Epoch [112/120    avg_loss:0.016, val_acc:0.994]
Epoch [113/120    avg_loss:0.014, val_acc:0.994]
Epoch [114/120    avg_loss:0.016, val_acc:0.996]
Epoch [115/120    avg_loss:0.015, val_acc:0.996]
Epoch [116/120    avg_loss:0.015, val_acc:0.994]
Epoch [117/120    avg_loss:0.018, val_acc:0.996]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.014, val_acc:0.994]
Epoch [120/120    avg_loss:0.014, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215  15   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.98648649 0.96629213 0.94168467 0.95945946
 1.         0.96703297 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9921663275382108
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa62bc3ab70>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.597, val_acc:0.395]
Epoch [2/120    avg_loss:2.423, val_acc:0.492]
Epoch [3/120    avg_loss:2.302, val_acc:0.458]
Epoch [4/120    avg_loss:2.196, val_acc:0.472]
Epoch [5/120    avg_loss:2.065, val_acc:0.530]
Epoch [6/120    avg_loss:1.944, val_acc:0.579]
Epoch [7/120    avg_loss:1.816, val_acc:0.595]
Epoch [8/120    avg_loss:1.679, val_acc:0.637]
Epoch [9/120    avg_loss:1.492, val_acc:0.688]
Epoch [10/120    avg_loss:1.397, val_acc:0.714]
Epoch [11/120    avg_loss:1.269, val_acc:0.772]
Epoch [12/120    avg_loss:1.154, val_acc:0.720]
Epoch [13/120    avg_loss:1.118, val_acc:0.712]
Epoch [14/120    avg_loss:1.002, val_acc:0.774]
Epoch [15/120    avg_loss:0.892, val_acc:0.815]
Epoch [16/120    avg_loss:0.832, val_acc:0.843]
Epoch [17/120    avg_loss:0.811, val_acc:0.857]
Epoch [18/120    avg_loss:0.738, val_acc:0.901]
Epoch [19/120    avg_loss:0.673, val_acc:0.889]
Epoch [20/120    avg_loss:0.609, val_acc:0.903]
Epoch [21/120    avg_loss:0.550, val_acc:0.877]
Epoch [22/120    avg_loss:0.577, val_acc:0.895]
Epoch [23/120    avg_loss:0.532, val_acc:0.815]
Epoch [24/120    avg_loss:0.556, val_acc:0.919]
Epoch [25/120    avg_loss:0.477, val_acc:0.938]
Epoch [26/120    avg_loss:0.476, val_acc:0.935]
Epoch [27/120    avg_loss:0.436, val_acc:0.903]
Epoch [28/120    avg_loss:0.479, val_acc:0.917]
Epoch [29/120    avg_loss:0.421, val_acc:0.931]
Epoch [30/120    avg_loss:0.400, val_acc:0.925]
Epoch [31/120    avg_loss:0.343, val_acc:0.929]
Epoch [32/120    avg_loss:0.391, val_acc:0.931]
Epoch [33/120    avg_loss:0.331, val_acc:0.958]
Epoch [34/120    avg_loss:0.273, val_acc:0.909]
Epoch [35/120    avg_loss:0.324, val_acc:0.909]
Epoch [36/120    avg_loss:0.346, val_acc:0.954]
Epoch [37/120    avg_loss:0.316, val_acc:0.931]
Epoch [38/120    avg_loss:0.270, val_acc:0.933]
Epoch [39/120    avg_loss:0.255, val_acc:0.956]
Epoch [40/120    avg_loss:0.241, val_acc:0.950]
Epoch [41/120    avg_loss:0.242, val_acc:0.952]
Epoch [42/120    avg_loss:0.263, val_acc:0.917]
Epoch [43/120    avg_loss:0.266, val_acc:0.944]
Epoch [44/120    avg_loss:0.296, val_acc:0.950]
Epoch [45/120    avg_loss:0.210, val_acc:0.968]
Epoch [46/120    avg_loss:0.192, val_acc:0.964]
Epoch [47/120    avg_loss:0.186, val_acc:0.972]
Epoch [48/120    avg_loss:0.232, val_acc:0.927]
Epoch [49/120    avg_loss:0.252, val_acc:0.956]
Epoch [50/120    avg_loss:0.298, val_acc:0.960]
Epoch [51/120    avg_loss:0.165, val_acc:0.960]
Epoch [52/120    avg_loss:0.190, val_acc:0.978]
Epoch [53/120    avg_loss:0.165, val_acc:0.974]
Epoch [54/120    avg_loss:0.164, val_acc:0.885]
Epoch [55/120    avg_loss:0.159, val_acc:0.974]
Epoch [56/120    avg_loss:0.132, val_acc:0.974]
Epoch [57/120    avg_loss:0.185, val_acc:0.966]
Epoch [58/120    avg_loss:0.144, val_acc:0.978]
Epoch [59/120    avg_loss:0.108, val_acc:0.976]
Epoch [60/120    avg_loss:0.099, val_acc:0.984]
Epoch [61/120    avg_loss:0.118, val_acc:0.972]
Epoch [62/120    avg_loss:0.116, val_acc:0.978]
Epoch [63/120    avg_loss:0.084, val_acc:0.970]
Epoch [64/120    avg_loss:0.092, val_acc:0.982]
Epoch [65/120    avg_loss:0.124, val_acc:0.980]
Epoch [66/120    avg_loss:0.122, val_acc:0.978]
Epoch [67/120    avg_loss:0.139, val_acc:0.986]
Epoch [68/120    avg_loss:0.079, val_acc:0.988]
Epoch [69/120    avg_loss:0.115, val_acc:0.962]
Epoch [70/120    avg_loss:0.088, val_acc:0.982]
Epoch [71/120    avg_loss:0.060, val_acc:0.984]
Epoch [72/120    avg_loss:0.090, val_acc:0.986]
Epoch [73/120    avg_loss:0.116, val_acc:0.986]
Epoch [74/120    avg_loss:0.088, val_acc:0.990]
Epoch [75/120    avg_loss:0.065, val_acc:0.986]
Epoch [76/120    avg_loss:0.073, val_acc:0.980]
Epoch [77/120    avg_loss:0.084, val_acc:0.986]
Epoch [78/120    avg_loss:0.072, val_acc:0.978]
Epoch [79/120    avg_loss:0.062, val_acc:0.982]
Epoch [80/120    avg_loss:0.062, val_acc:0.988]
Epoch [81/120    avg_loss:0.070, val_acc:0.982]
Epoch [82/120    avg_loss:0.070, val_acc:0.984]
Epoch [83/120    avg_loss:0.052, val_acc:0.992]
Epoch [84/120    avg_loss:0.067, val_acc:0.990]
Epoch [85/120    avg_loss:0.054, val_acc:0.984]
Epoch [86/120    avg_loss:0.046, val_acc:0.990]
Epoch [87/120    avg_loss:0.060, val_acc:0.990]
Epoch [88/120    avg_loss:0.061, val_acc:0.984]
Epoch [89/120    avg_loss:0.071, val_acc:0.970]
Epoch [90/120    avg_loss:0.110, val_acc:0.982]
Epoch [91/120    avg_loss:0.049, val_acc:0.982]
Epoch [92/120    avg_loss:0.043, val_acc:0.986]
Epoch [93/120    avg_loss:0.048, val_acc:0.980]
Epoch [94/120    avg_loss:0.080, val_acc:0.990]
Epoch [95/120    avg_loss:0.049, val_acc:0.986]
Epoch [96/120    avg_loss:0.047, val_acc:0.990]
Epoch [97/120    avg_loss:0.033, val_acc:0.992]
Epoch [98/120    avg_loss:0.028, val_acc:0.992]
Epoch [99/120    avg_loss:0.021, val_acc:0.992]
Epoch [100/120    avg_loss:0.028, val_acc:0.992]
Epoch [101/120    avg_loss:0.021, val_acc:0.992]
Epoch [102/120    avg_loss:0.024, val_acc:0.992]
Epoch [103/120    avg_loss:0.041, val_acc:0.992]
Epoch [104/120    avg_loss:0.035, val_acc:0.992]
Epoch [105/120    avg_loss:0.026, val_acc:0.992]
Epoch [106/120    avg_loss:0.027, val_acc:0.992]
Epoch [107/120    avg_loss:0.026, val_acc:0.992]
Epoch [108/120    avg_loss:0.023, val_acc:0.992]
Epoch [109/120    avg_loss:0.021, val_acc:0.992]
Epoch [110/120    avg_loss:0.022, val_acc:0.992]
Epoch [111/120    avg_loss:0.021, val_acc:0.992]
Epoch [112/120    avg_loss:0.024, val_acc:0.992]
Epoch [113/120    avg_loss:0.022, val_acc:0.992]
Epoch [114/120    avg_loss:0.019, val_acc:0.992]
Epoch [115/120    avg_loss:0.019, val_acc:0.992]
Epoch [116/120    avg_loss:0.017, val_acc:0.992]
Epoch [117/120    avg_loss:0.021, val_acc:0.992]
Epoch [118/120    avg_loss:0.016, val_acc:0.992]
Epoch [119/120    avg_loss:0.018, val_acc:0.992]
Epoch [120/120    avg_loss:0.024, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 1.         0.98648649 0.99782135 0.95768374 0.93559322
 1.         0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9940654023535729
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60c84cbac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.561, val_acc:0.272]
Epoch [2/120    avg_loss:2.411, val_acc:0.353]
Epoch [3/120    avg_loss:2.292, val_acc:0.480]
Epoch [4/120    avg_loss:2.190, val_acc:0.526]
Epoch [5/120    avg_loss:2.071, val_acc:0.567]
Epoch [6/120    avg_loss:1.928, val_acc:0.571]
Epoch [7/120    avg_loss:1.786, val_acc:0.597]
Epoch [8/120    avg_loss:1.646, val_acc:0.613]
Epoch [9/120    avg_loss:1.522, val_acc:0.665]
Epoch [10/120    avg_loss:1.419, val_acc:0.683]
Epoch [11/120    avg_loss:1.273, val_acc:0.704]
Epoch [12/120    avg_loss:1.153, val_acc:0.704]
Epoch [13/120    avg_loss:1.054, val_acc:0.726]
Epoch [14/120    avg_loss:0.967, val_acc:0.760]
Epoch [15/120    avg_loss:0.831, val_acc:0.746]
Epoch [16/120    avg_loss:0.838, val_acc:0.784]
Epoch [17/120    avg_loss:0.766, val_acc:0.782]
Epoch [18/120    avg_loss:0.690, val_acc:0.786]
Epoch [19/120    avg_loss:0.643, val_acc:0.839]
Epoch [20/120    avg_loss:0.604, val_acc:0.857]
Epoch [21/120    avg_loss:0.608, val_acc:0.877]
Epoch [22/120    avg_loss:0.553, val_acc:0.800]
Epoch [23/120    avg_loss:0.453, val_acc:0.849]
Epoch [24/120    avg_loss:0.511, val_acc:0.905]
Epoch [25/120    avg_loss:0.399, val_acc:0.903]
Epoch [26/120    avg_loss:0.441, val_acc:0.915]
Epoch [27/120    avg_loss:0.415, val_acc:0.940]
Epoch [28/120    avg_loss:0.332, val_acc:0.938]
Epoch [29/120    avg_loss:0.368, val_acc:0.901]
Epoch [30/120    avg_loss:0.320, val_acc:0.964]
Epoch [31/120    avg_loss:0.297, val_acc:0.927]
Epoch [32/120    avg_loss:0.311, val_acc:0.905]
Epoch [33/120    avg_loss:0.298, val_acc:0.897]
Epoch [34/120    avg_loss:0.309, val_acc:0.948]
Epoch [35/120    avg_loss:0.254, val_acc:0.942]
Epoch [36/120    avg_loss:0.302, val_acc:0.958]
Epoch [37/120    avg_loss:0.265, val_acc:0.964]
Epoch [38/120    avg_loss:0.225, val_acc:0.940]
Epoch [39/120    avg_loss:0.217, val_acc:0.931]
Epoch [40/120    avg_loss:0.208, val_acc:0.964]
Epoch [41/120    avg_loss:0.211, val_acc:0.946]
Epoch [42/120    avg_loss:0.253, val_acc:0.946]
Epoch [43/120    avg_loss:0.214, val_acc:0.946]
Epoch [44/120    avg_loss:0.146, val_acc:0.964]
Epoch [45/120    avg_loss:0.137, val_acc:0.978]
Epoch [46/120    avg_loss:0.133, val_acc:0.929]
Epoch [47/120    avg_loss:0.201, val_acc:0.933]
Epoch [48/120    avg_loss:0.200, val_acc:0.958]
Epoch [49/120    avg_loss:0.149, val_acc:0.927]
Epoch [50/120    avg_loss:0.113, val_acc:0.972]
Epoch [51/120    avg_loss:0.150, val_acc:0.978]
Epoch [52/120    avg_loss:0.177, val_acc:0.960]
Epoch [53/120    avg_loss:0.170, val_acc:0.982]
Epoch [54/120    avg_loss:0.147, val_acc:0.970]
Epoch [55/120    avg_loss:0.133, val_acc:0.974]
Epoch [56/120    avg_loss:0.147, val_acc:0.972]
Epoch [57/120    avg_loss:0.123, val_acc:0.986]
Epoch [58/120    avg_loss:0.144, val_acc:0.976]
Epoch [59/120    avg_loss:0.116, val_acc:0.966]
Epoch [60/120    avg_loss:0.123, val_acc:0.966]
Epoch [61/120    avg_loss:0.148, val_acc:0.980]
Epoch [62/120    avg_loss:0.134, val_acc:0.964]
Epoch [63/120    avg_loss:0.101, val_acc:0.954]
Epoch [64/120    avg_loss:0.154, val_acc:0.980]
Epoch [65/120    avg_loss:0.089, val_acc:0.978]
Epoch [66/120    avg_loss:0.088, val_acc:0.972]
Epoch [67/120    avg_loss:0.090, val_acc:0.972]
Epoch [68/120    avg_loss:0.071, val_acc:0.984]
Epoch [69/120    avg_loss:0.057, val_acc:0.976]
Epoch [70/120    avg_loss:0.052, val_acc:0.986]
Epoch [71/120    avg_loss:0.064, val_acc:0.988]
Epoch [72/120    avg_loss:0.077, val_acc:0.970]
Epoch [73/120    avg_loss:0.078, val_acc:0.978]
Epoch [74/120    avg_loss:0.057, val_acc:0.972]
Epoch [75/120    avg_loss:0.125, val_acc:0.966]
Epoch [76/120    avg_loss:0.141, val_acc:0.974]
Epoch [77/120    avg_loss:0.137, val_acc:0.974]
Epoch [78/120    avg_loss:0.105, val_acc:0.990]
Epoch [79/120    avg_loss:0.060, val_acc:0.988]
Epoch [80/120    avg_loss:0.066, val_acc:0.986]
Epoch [81/120    avg_loss:0.046, val_acc:0.982]
Epoch [82/120    avg_loss:0.051, val_acc:0.992]
Epoch [83/120    avg_loss:0.049, val_acc:0.982]
Epoch [84/120    avg_loss:0.050, val_acc:0.994]
Epoch [85/120    avg_loss:0.042, val_acc:0.988]
Epoch [86/120    avg_loss:0.028, val_acc:0.986]
Epoch [87/120    avg_loss:0.030, val_acc:0.988]
Epoch [88/120    avg_loss:0.036, val_acc:0.986]
Epoch [89/120    avg_loss:0.040, val_acc:0.968]
Epoch [90/120    avg_loss:0.034, val_acc:0.988]
Epoch [91/120    avg_loss:0.034, val_acc:0.986]
Epoch [92/120    avg_loss:0.027, val_acc:0.988]
Epoch [93/120    avg_loss:0.088, val_acc:0.966]
Epoch [94/120    avg_loss:0.131, val_acc:0.972]
Epoch [95/120    avg_loss:0.098, val_acc:0.982]
Epoch [96/120    avg_loss:0.069, val_acc:0.988]
Epoch [97/120    avg_loss:0.146, val_acc:0.960]
Epoch [98/120    avg_loss:0.107, val_acc:0.978]
Epoch [99/120    avg_loss:0.079, val_acc:0.978]
Epoch [100/120    avg_loss:0.060, val_acc:0.980]
Epoch [101/120    avg_loss:0.060, val_acc:0.982]
Epoch [102/120    avg_loss:0.044, val_acc:0.980]
Epoch [103/120    avg_loss:0.045, val_acc:0.982]
Epoch [104/120    avg_loss:0.037, val_acc:0.986]
Epoch [105/120    avg_loss:0.042, val_acc:0.988]
Epoch [106/120    avg_loss:0.037, val_acc:0.988]
Epoch [107/120    avg_loss:0.041, val_acc:0.988]
Epoch [108/120    avg_loss:0.035, val_acc:0.986]
Epoch [109/120    avg_loss:0.046, val_acc:0.990]
Epoch [110/120    avg_loss:0.043, val_acc:0.990]
Epoch [111/120    avg_loss:0.042, val_acc:0.990]
Epoch [112/120    avg_loss:0.028, val_acc:0.990]
Epoch [113/120    avg_loss:0.029, val_acc:0.990]
Epoch [114/120    avg_loss:0.034, val_acc:0.990]
Epoch [115/120    avg_loss:0.033, val_acc:0.990]
Epoch [116/120    avg_loss:0.033, val_acc:0.990]
Epoch [117/120    avg_loss:0.038, val_acc:0.990]
Epoch [118/120    avg_loss:0.031, val_acc:0.990]
Epoch [119/120    avg_loss:0.034, val_acc:0.990]
Epoch [120/120    avg_loss:0.029, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.97986577 1.         0.93959732 0.90909091
 1.         0.94972067 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9912166750055866
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61dca3db70>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.274]
Epoch [2/120    avg_loss:2.422, val_acc:0.363]
Epoch [3/120    avg_loss:2.254, val_acc:0.405]
Epoch [4/120    avg_loss:2.108, val_acc:0.526]
Epoch [5/120    avg_loss:1.981, val_acc:0.609]
Epoch [6/120    avg_loss:1.845, val_acc:0.605]
Epoch [7/120    avg_loss:1.756, val_acc:0.637]
Epoch [8/120    avg_loss:1.626, val_acc:0.633]
Epoch [9/120    avg_loss:1.472, val_acc:0.675]
Epoch [10/120    avg_loss:1.332, val_acc:0.671]
Epoch [11/120    avg_loss:1.196, val_acc:0.708]
Epoch [12/120    avg_loss:1.157, val_acc:0.806]
Epoch [13/120    avg_loss:1.028, val_acc:0.815]
Epoch [14/120    avg_loss:0.916, val_acc:0.766]
Epoch [15/120    avg_loss:0.835, val_acc:0.901]
Epoch [16/120    avg_loss:0.712, val_acc:0.867]
Epoch [17/120    avg_loss:0.679, val_acc:0.885]
Epoch [18/120    avg_loss:0.614, val_acc:0.802]
Epoch [19/120    avg_loss:0.617, val_acc:0.899]
Epoch [20/120    avg_loss:0.573, val_acc:0.911]
Epoch [21/120    avg_loss:0.520, val_acc:0.911]
Epoch [22/120    avg_loss:0.489, val_acc:0.907]
Epoch [23/120    avg_loss:0.459, val_acc:0.887]
Epoch [24/120    avg_loss:0.423, val_acc:0.931]
Epoch [25/120    avg_loss:0.399, val_acc:0.847]
Epoch [26/120    avg_loss:0.424, val_acc:0.938]
Epoch [27/120    avg_loss:0.381, val_acc:0.946]
Epoch [28/120    avg_loss:0.403, val_acc:0.940]
Epoch [29/120    avg_loss:0.342, val_acc:0.909]
Epoch [30/120    avg_loss:0.450, val_acc:0.927]
Epoch [31/120    avg_loss:0.347, val_acc:0.931]
Epoch [32/120    avg_loss:0.332, val_acc:0.909]
Epoch [33/120    avg_loss:0.338, val_acc:0.929]
Epoch [34/120    avg_loss:0.315, val_acc:0.940]
Epoch [35/120    avg_loss:0.317, val_acc:0.948]
Epoch [36/120    avg_loss:0.321, val_acc:0.942]
Epoch [37/120    avg_loss:0.244, val_acc:0.944]
Epoch [38/120    avg_loss:0.270, val_acc:0.970]
Epoch [39/120    avg_loss:0.244, val_acc:0.919]
Epoch [40/120    avg_loss:0.248, val_acc:0.962]
Epoch [41/120    avg_loss:0.218, val_acc:0.944]
Epoch [42/120    avg_loss:0.234, val_acc:0.946]
Epoch [43/120    avg_loss:0.236, val_acc:0.944]
Epoch [44/120    avg_loss:0.211, val_acc:0.927]
Epoch [45/120    avg_loss:0.276, val_acc:0.923]
Epoch [46/120    avg_loss:0.221, val_acc:0.970]
Epoch [47/120    avg_loss:0.182, val_acc:0.962]
Epoch [48/120    avg_loss:0.153, val_acc:0.966]
Epoch [49/120    avg_loss:0.165, val_acc:0.972]
Epoch [50/120    avg_loss:0.172, val_acc:0.964]
Epoch [51/120    avg_loss:0.176, val_acc:0.966]
Epoch [52/120    avg_loss:0.152, val_acc:0.982]
Epoch [53/120    avg_loss:0.151, val_acc:0.956]
Epoch [54/120    avg_loss:0.152, val_acc:0.982]
Epoch [55/120    avg_loss:0.126, val_acc:0.960]
Epoch [56/120    avg_loss:0.215, val_acc:0.944]
Epoch [57/120    avg_loss:0.202, val_acc:0.968]
Epoch [58/120    avg_loss:0.127, val_acc:0.958]
Epoch [59/120    avg_loss:0.146, val_acc:0.966]
Epoch [60/120    avg_loss:0.119, val_acc:0.978]
Epoch [61/120    avg_loss:0.137, val_acc:0.974]
Epoch [62/120    avg_loss:0.103, val_acc:0.966]
Epoch [63/120    avg_loss:0.107, val_acc:0.976]
Epoch [64/120    avg_loss:0.169, val_acc:0.974]
Epoch [65/120    avg_loss:0.135, val_acc:0.974]
Epoch [66/120    avg_loss:0.120, val_acc:0.984]
Epoch [67/120    avg_loss:0.108, val_acc:0.984]
Epoch [68/120    avg_loss:0.086, val_acc:0.986]
Epoch [69/120    avg_loss:0.091, val_acc:0.982]
Epoch [70/120    avg_loss:0.083, val_acc:0.976]
Epoch [71/120    avg_loss:0.106, val_acc:0.978]
Epoch [72/120    avg_loss:0.101, val_acc:0.982]
Epoch [73/120    avg_loss:0.122, val_acc:0.980]
Epoch [74/120    avg_loss:0.087, val_acc:0.984]
Epoch [75/120    avg_loss:0.078, val_acc:0.978]
Epoch [76/120    avg_loss:0.064, val_acc:0.986]
Epoch [77/120    avg_loss:0.080, val_acc:0.980]
Epoch [78/120    avg_loss:0.091, val_acc:0.978]
Epoch [79/120    avg_loss:0.077, val_acc:0.990]
Epoch [80/120    avg_loss:0.050, val_acc:0.992]
Epoch [81/120    avg_loss:0.042, val_acc:0.992]
Epoch [82/120    avg_loss:0.092, val_acc:0.988]
Epoch [83/120    avg_loss:0.061, val_acc:0.988]
Epoch [84/120    avg_loss:0.059, val_acc:0.976]
Epoch [85/120    avg_loss:0.052, val_acc:0.996]
Epoch [86/120    avg_loss:0.039, val_acc:0.996]
Epoch [87/120    avg_loss:0.058, val_acc:0.988]
Epoch [88/120    avg_loss:0.094, val_acc:0.990]
Epoch [89/120    avg_loss:0.048, val_acc:0.992]
Epoch [90/120    avg_loss:0.047, val_acc:0.988]
Epoch [91/120    avg_loss:0.048, val_acc:0.992]
Epoch [92/120    avg_loss:0.043, val_acc:0.988]
Epoch [93/120    avg_loss:0.043, val_acc:0.992]
Epoch [94/120    avg_loss:0.029, val_acc:0.992]
Epoch [95/120    avg_loss:0.043, val_acc:0.986]
Epoch [96/120    avg_loss:0.060, val_acc:0.980]
Epoch [97/120    avg_loss:0.060, val_acc:0.992]
Epoch [98/120    avg_loss:0.074, val_acc:0.972]
Epoch [99/120    avg_loss:0.046, val_acc:0.984]
Epoch [100/120    avg_loss:0.044, val_acc:0.990]
Epoch [101/120    avg_loss:0.027, val_acc:0.990]
Epoch [102/120    avg_loss:0.028, val_acc:0.992]
Epoch [103/120    avg_loss:0.035, val_acc:0.992]
Epoch [104/120    avg_loss:0.023, val_acc:0.992]
Epoch [105/120    avg_loss:0.023, val_acc:0.992]
Epoch [106/120    avg_loss:0.020, val_acc:0.992]
Epoch [107/120    avg_loss:0.027, val_acc:0.994]
Epoch [108/120    avg_loss:0.023, val_acc:0.990]
Epoch [109/120    avg_loss:0.024, val_acc:0.992]
Epoch [110/120    avg_loss:0.024, val_acc:0.994]
Epoch [111/120    avg_loss:0.027, val_acc:0.990]
Epoch [112/120    avg_loss:0.027, val_acc:0.992]
Epoch [113/120    avg_loss:0.024, val_acc:0.992]
Epoch [114/120    avg_loss:0.021, val_acc:0.992]
Epoch [115/120    avg_loss:0.022, val_acc:0.992]
Epoch [116/120    avg_loss:0.026, val_acc:0.992]
Epoch [117/120    avg_loss:0.022, val_acc:0.994]
Epoch [118/120    avg_loss:0.026, val_acc:0.994]
Epoch [119/120    avg_loss:0.022, val_acc:0.992]
Epoch [120/120    avg_loss:0.028, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   0   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.98206278 0.99343545 0.95890411 0.94117647
 1.         0.95555556 0.99614891 1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9928785253577136
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f06a677fa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.564, val_acc:0.385]
Epoch [2/120    avg_loss:2.408, val_acc:0.344]
Epoch [3/120    avg_loss:2.282, val_acc:0.404]
Epoch [4/120    avg_loss:2.179, val_acc:0.498]
Epoch [5/120    avg_loss:2.046, val_acc:0.583]
Epoch [6/120    avg_loss:1.907, val_acc:0.581]
Epoch [7/120    avg_loss:1.753, val_acc:0.613]
Epoch [8/120    avg_loss:1.598, val_acc:0.646]
Epoch [9/120    avg_loss:1.478, val_acc:0.685]
Epoch [10/120    avg_loss:1.346, val_acc:0.681]
Epoch [11/120    avg_loss:1.262, val_acc:0.700]
Epoch [12/120    avg_loss:1.115, val_acc:0.710]
Epoch [13/120    avg_loss:1.027, val_acc:0.752]
Epoch [14/120    avg_loss:0.910, val_acc:0.762]
Epoch [15/120    avg_loss:0.848, val_acc:0.769]
Epoch [16/120    avg_loss:0.801, val_acc:0.869]
Epoch [17/120    avg_loss:0.718, val_acc:0.863]
Epoch [18/120    avg_loss:0.653, val_acc:0.823]
Epoch [19/120    avg_loss:0.626, val_acc:0.898]
Epoch [20/120    avg_loss:0.631, val_acc:0.912]
Epoch [21/120    avg_loss:0.614, val_acc:0.940]
Epoch [22/120    avg_loss:0.547, val_acc:0.931]
Epoch [23/120    avg_loss:0.505, val_acc:0.917]
Epoch [24/120    avg_loss:0.467, val_acc:0.869]
Epoch [25/120    avg_loss:0.463, val_acc:0.908]
Epoch [26/120    avg_loss:0.456, val_acc:0.944]
Epoch [27/120    avg_loss:0.438, val_acc:0.954]
Epoch [28/120    avg_loss:0.430, val_acc:0.931]
Epoch [29/120    avg_loss:0.444, val_acc:0.950]
Epoch [30/120    avg_loss:0.406, val_acc:0.850]
Epoch [31/120    avg_loss:0.398, val_acc:0.954]
Epoch [32/120    avg_loss:0.325, val_acc:0.952]
Epoch [33/120    avg_loss:0.318, val_acc:0.915]
Epoch [34/120    avg_loss:0.312, val_acc:0.958]
Epoch [35/120    avg_loss:0.309, val_acc:0.952]
Epoch [36/120    avg_loss:0.311, val_acc:0.948]
Epoch [37/120    avg_loss:0.283, val_acc:0.956]
Epoch [38/120    avg_loss:0.255, val_acc:0.952]
Epoch [39/120    avg_loss:0.238, val_acc:0.956]
Epoch [40/120    avg_loss:0.246, val_acc:0.942]
Epoch [41/120    avg_loss:0.232, val_acc:0.958]
Epoch [42/120    avg_loss:0.236, val_acc:0.942]
Epoch [43/120    avg_loss:0.247, val_acc:0.958]
Epoch [44/120    avg_loss:0.224, val_acc:0.973]
Epoch [45/120    avg_loss:0.218, val_acc:0.960]
Epoch [46/120    avg_loss:0.206, val_acc:0.977]
Epoch [47/120    avg_loss:0.248, val_acc:0.950]
Epoch [48/120    avg_loss:0.248, val_acc:0.963]
Epoch [49/120    avg_loss:0.238, val_acc:0.944]
Epoch [50/120    avg_loss:0.271, val_acc:0.963]
Epoch [51/120    avg_loss:0.301, val_acc:0.898]
Epoch [52/120    avg_loss:0.258, val_acc:0.965]
Epoch [53/120    avg_loss:0.259, val_acc:0.963]
Epoch [54/120    avg_loss:0.210, val_acc:0.965]
Epoch [55/120    avg_loss:0.191, val_acc:0.960]
Epoch [56/120    avg_loss:0.198, val_acc:0.958]
Epoch [57/120    avg_loss:0.173, val_acc:0.979]
Epoch [58/120    avg_loss:0.163, val_acc:0.954]
Epoch [59/120    avg_loss:0.163, val_acc:0.958]
Epoch [60/120    avg_loss:0.133, val_acc:0.973]
Epoch [61/120    avg_loss:0.153, val_acc:0.965]
Epoch [62/120    avg_loss:0.162, val_acc:0.985]
Epoch [63/120    avg_loss:0.141, val_acc:0.990]
Epoch [64/120    avg_loss:0.147, val_acc:0.956]
Epoch [65/120    avg_loss:0.164, val_acc:0.967]
Epoch [66/120    avg_loss:0.180, val_acc:0.992]
Epoch [67/120    avg_loss:0.127, val_acc:0.977]
Epoch [68/120    avg_loss:0.204, val_acc:0.942]
Epoch [69/120    avg_loss:0.168, val_acc:0.969]
Epoch [70/120    avg_loss:0.138, val_acc:0.950]
Epoch [71/120    avg_loss:0.125, val_acc:0.975]
Epoch [72/120    avg_loss:0.120, val_acc:0.996]
Epoch [73/120    avg_loss:0.092, val_acc:0.981]
Epoch [74/120    avg_loss:0.080, val_acc:0.988]
Epoch [75/120    avg_loss:0.107, val_acc:0.965]
Epoch [76/120    avg_loss:0.198, val_acc:0.969]
Epoch [77/120    avg_loss:0.141, val_acc:0.967]
Epoch [78/120    avg_loss:0.116, val_acc:0.988]
Epoch [79/120    avg_loss:0.093, val_acc:0.979]
Epoch [80/120    avg_loss:0.113, val_acc:0.977]
Epoch [81/120    avg_loss:0.120, val_acc:0.894]
Epoch [82/120    avg_loss:0.113, val_acc:0.990]
Epoch [83/120    avg_loss:0.113, val_acc:0.973]
Epoch [84/120    avg_loss:0.091, val_acc:0.990]
Epoch [85/120    avg_loss:0.082, val_acc:0.981]
Epoch [86/120    avg_loss:0.062, val_acc:0.992]
Epoch [87/120    avg_loss:0.061, val_acc:0.994]
Epoch [88/120    avg_loss:0.052, val_acc:0.994]
Epoch [89/120    avg_loss:0.060, val_acc:0.994]
Epoch [90/120    avg_loss:0.046, val_acc:0.994]
Epoch [91/120    avg_loss:0.053, val_acc:0.994]
Epoch [92/120    avg_loss:0.048, val_acc:0.994]
Epoch [93/120    avg_loss:0.049, val_acc:0.996]
Epoch [94/120    avg_loss:0.047, val_acc:0.994]
Epoch [95/120    avg_loss:0.050, val_acc:0.996]
Epoch [96/120    avg_loss:0.049, val_acc:0.996]
Epoch [97/120    avg_loss:0.037, val_acc:0.996]
Epoch [98/120    avg_loss:0.047, val_acc:0.992]
Epoch [99/120    avg_loss:0.046, val_acc:0.994]
Epoch [100/120    avg_loss:0.045, val_acc:0.994]
Epoch [101/120    avg_loss:0.050, val_acc:0.996]
Epoch [102/120    avg_loss:0.050, val_acc:0.996]
Epoch [103/120    avg_loss:0.049, val_acc:0.992]
Epoch [104/120    avg_loss:0.043, val_acc:0.996]
Epoch [105/120    avg_loss:0.043, val_acc:0.992]
Epoch [106/120    avg_loss:0.041, val_acc:0.994]
Epoch [107/120    avg_loss:0.046, val_acc:0.994]
Epoch [108/120    avg_loss:0.034, val_acc:0.994]
Epoch [109/120    avg_loss:0.045, val_acc:0.994]
Epoch [110/120    avg_loss:0.038, val_acc:0.994]
Epoch [111/120    avg_loss:0.042, val_acc:0.994]
Epoch [112/120    avg_loss:0.043, val_acc:0.994]
Epoch [113/120    avg_loss:0.049, val_acc:0.996]
Epoch [114/120    avg_loss:0.039, val_acc:0.996]
Epoch [115/120    avg_loss:0.037, val_acc:0.996]
Epoch [116/120    avg_loss:0.048, val_acc:0.994]
Epoch [117/120    avg_loss:0.044, val_acc:0.996]
Epoch [118/120    avg_loss:0.033, val_acc:0.996]
Epoch [119/120    avg_loss:0.040, val_acc:0.996]
Epoch [120/120    avg_loss:0.039, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   2   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99926954 0.99095023 0.98901099 0.93512304 0.909699
 0.99757869 0.97826087 0.99742931 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9912169613014344
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7c15ff9ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.585, val_acc:0.312]
Epoch [2/120    avg_loss:2.419, val_acc:0.444]
Epoch [3/120    avg_loss:2.284, val_acc:0.454]
Epoch [4/120    avg_loss:2.168, val_acc:0.454]
Epoch [5/120    avg_loss:2.027, val_acc:0.528]
Epoch [6/120    avg_loss:1.880, val_acc:0.617]
Epoch [7/120    avg_loss:1.788, val_acc:0.635]
Epoch [8/120    avg_loss:1.659, val_acc:0.649]
Epoch [9/120    avg_loss:1.597, val_acc:0.685]
Epoch [10/120    avg_loss:1.453, val_acc:0.694]
Epoch [11/120    avg_loss:1.328, val_acc:0.744]
Epoch [12/120    avg_loss:1.263, val_acc:0.760]
Epoch [13/120    avg_loss:1.118, val_acc:0.780]
Epoch [14/120    avg_loss:0.988, val_acc:0.815]
Epoch [15/120    avg_loss:0.880, val_acc:0.829]
Epoch [16/120    avg_loss:0.886, val_acc:0.810]
Epoch [17/120    avg_loss:0.788, val_acc:0.887]
Epoch [18/120    avg_loss:0.726, val_acc:0.893]
Epoch [19/120    avg_loss:0.637, val_acc:0.903]
Epoch [20/120    avg_loss:0.558, val_acc:0.907]
Epoch [21/120    avg_loss:0.620, val_acc:0.897]
Epoch [22/120    avg_loss:0.559, val_acc:0.933]
Epoch [23/120    avg_loss:0.477, val_acc:0.903]
Epoch [24/120    avg_loss:0.422, val_acc:0.935]
Epoch [25/120    avg_loss:0.424, val_acc:0.931]
Epoch [26/120    avg_loss:0.380, val_acc:0.913]
Epoch [27/120    avg_loss:0.377, val_acc:0.942]
Epoch [28/120    avg_loss:0.364, val_acc:0.954]
Epoch [29/120    avg_loss:0.331, val_acc:0.944]
Epoch [30/120    avg_loss:0.379, val_acc:0.938]
Epoch [31/120    avg_loss:0.307, val_acc:0.919]
Epoch [32/120    avg_loss:0.342, val_acc:0.962]
Epoch [33/120    avg_loss:0.302, val_acc:0.933]
Epoch [34/120    avg_loss:0.318, val_acc:0.933]
Epoch [35/120    avg_loss:0.330, val_acc:0.966]
Epoch [36/120    avg_loss:0.275, val_acc:0.944]
Epoch [37/120    avg_loss:0.235, val_acc:0.972]
Epoch [38/120    avg_loss:0.285, val_acc:0.964]
Epoch [39/120    avg_loss:0.319, val_acc:0.948]
Epoch [40/120    avg_loss:0.241, val_acc:0.958]
Epoch [41/120    avg_loss:0.280, val_acc:0.960]
Epoch [42/120    avg_loss:0.245, val_acc:0.956]
Epoch [43/120    avg_loss:0.282, val_acc:0.958]
Epoch [44/120    avg_loss:0.201, val_acc:0.958]
Epoch [45/120    avg_loss:0.226, val_acc:0.966]
Epoch [46/120    avg_loss:0.220, val_acc:0.980]
Epoch [47/120    avg_loss:0.159, val_acc:0.984]
Epoch [48/120    avg_loss:0.165, val_acc:0.962]
Epoch [49/120    avg_loss:0.133, val_acc:0.964]
Epoch [50/120    avg_loss:0.184, val_acc:0.960]
Epoch [51/120    avg_loss:0.184, val_acc:0.974]
Epoch [52/120    avg_loss:0.155, val_acc:0.956]
Epoch [53/120    avg_loss:0.169, val_acc:0.978]
Epoch [54/120    avg_loss:0.132, val_acc:0.978]
Epoch [55/120    avg_loss:0.169, val_acc:0.960]
Epoch [56/120    avg_loss:0.161, val_acc:0.978]
Epoch [57/120    avg_loss:0.165, val_acc:0.974]
Epoch [58/120    avg_loss:0.157, val_acc:0.978]
Epoch [59/120    avg_loss:0.129, val_acc:0.984]
Epoch [60/120    avg_loss:0.122, val_acc:0.972]
Epoch [61/120    avg_loss:0.102, val_acc:0.980]
Epoch [62/120    avg_loss:0.116, val_acc:0.972]
Epoch [63/120    avg_loss:0.126, val_acc:0.982]
Epoch [64/120    avg_loss:0.189, val_acc:0.968]
Epoch [65/120    avg_loss:0.136, val_acc:0.980]
Epoch [66/120    avg_loss:0.103, val_acc:0.990]
Epoch [67/120    avg_loss:0.112, val_acc:0.984]
Epoch [68/120    avg_loss:0.110, val_acc:0.986]
Epoch [69/120    avg_loss:0.091, val_acc:0.990]
Epoch [70/120    avg_loss:0.088, val_acc:0.988]
Epoch [71/120    avg_loss:0.102, val_acc:0.992]
Epoch [72/120    avg_loss:0.087, val_acc:0.990]
Epoch [73/120    avg_loss:0.075, val_acc:0.982]
Epoch [74/120    avg_loss:0.112, val_acc:0.978]
Epoch [75/120    avg_loss:0.088, val_acc:0.990]
Epoch [76/120    avg_loss:0.083, val_acc:0.990]
Epoch [77/120    avg_loss:0.060, val_acc:0.984]
Epoch [78/120    avg_loss:0.095, val_acc:0.994]
Epoch [79/120    avg_loss:0.082, val_acc:0.984]
Epoch [80/120    avg_loss:0.092, val_acc:0.982]
Epoch [81/120    avg_loss:0.073, val_acc:0.992]
Epoch [82/120    avg_loss:0.064, val_acc:0.994]
Epoch [83/120    avg_loss:0.053, val_acc:0.984]
Epoch [84/120    avg_loss:0.036, val_acc:0.998]
Epoch [85/120    avg_loss:0.054, val_acc:0.996]
Epoch [86/120    avg_loss:0.039, val_acc:0.992]
Epoch [87/120    avg_loss:0.044, val_acc:0.998]
Epoch [88/120    avg_loss:0.051, val_acc:0.996]
Epoch [89/120    avg_loss:0.045, val_acc:0.998]
Epoch [90/120    avg_loss:0.058, val_acc:0.992]
Epoch [91/120    avg_loss:0.057, val_acc:0.984]
Epoch [92/120    avg_loss:0.112, val_acc:0.988]
Epoch [93/120    avg_loss:0.050, val_acc:0.994]
Epoch [94/120    avg_loss:0.038, val_acc:0.994]
Epoch [95/120    avg_loss:0.076, val_acc:0.984]
Epoch [96/120    avg_loss:0.048, val_acc:0.992]
Epoch [97/120    avg_loss:0.070, val_acc:0.972]
Epoch [98/120    avg_loss:0.074, val_acc:0.982]
Epoch [99/120    avg_loss:0.078, val_acc:0.994]
Epoch [100/120    avg_loss:0.036, val_acc:0.996]
Epoch [101/120    avg_loss:0.045, val_acc:0.994]
Epoch [102/120    avg_loss:0.068, val_acc:0.992]
Epoch [103/120    avg_loss:0.041, val_acc:0.996]
Epoch [104/120    avg_loss:0.030, val_acc:0.996]
Epoch [105/120    avg_loss:0.023, val_acc:0.996]
Epoch [106/120    avg_loss:0.030, val_acc:0.996]
Epoch [107/120    avg_loss:0.027, val_acc:0.996]
Epoch [108/120    avg_loss:0.033, val_acc:0.994]
Epoch [109/120    avg_loss:0.027, val_acc:0.994]
Epoch [110/120    avg_loss:0.021, val_acc:0.994]
Epoch [111/120    avg_loss:0.020, val_acc:0.996]
Epoch [112/120    avg_loss:0.020, val_acc:0.996]
Epoch [113/120    avg_loss:0.025, val_acc:0.996]
Epoch [114/120    avg_loss:0.019, val_acc:0.996]
Epoch [115/120    avg_loss:0.018, val_acc:0.996]
Epoch [116/120    avg_loss:0.019, val_acc:0.996]
Epoch [117/120    avg_loss:0.020, val_acc:0.996]
Epoch [118/120    avg_loss:0.021, val_acc:0.996]
Epoch [119/120    avg_loss:0.019, val_acc:0.996]
Epoch [120/120    avg_loss:0.031, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  16   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.99545455 1.         0.93541203 0.9047619
 1.         0.98924731 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9926411197766957
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c021aeb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.562, val_acc:0.343]
Epoch [2/120    avg_loss:2.408, val_acc:0.359]
Epoch [3/120    avg_loss:2.261, val_acc:0.409]
Epoch [4/120    avg_loss:2.131, val_acc:0.458]
Epoch [5/120    avg_loss:1.986, val_acc:0.522]
Epoch [6/120    avg_loss:1.886, val_acc:0.516]
Epoch [7/120    avg_loss:1.742, val_acc:0.579]
Epoch [8/120    avg_loss:1.614, val_acc:0.639]
Epoch [9/120    avg_loss:1.475, val_acc:0.720]
Epoch [10/120    avg_loss:1.352, val_acc:0.740]
Epoch [11/120    avg_loss:1.241, val_acc:0.798]
Epoch [12/120    avg_loss:1.145, val_acc:0.825]
Epoch [13/120    avg_loss:1.039, val_acc:0.766]
Epoch [14/120    avg_loss:0.975, val_acc:0.837]
Epoch [15/120    avg_loss:0.871, val_acc:0.831]
Epoch [16/120    avg_loss:0.742, val_acc:0.833]
Epoch [17/120    avg_loss:0.732, val_acc:0.873]
Epoch [18/120    avg_loss:0.725, val_acc:0.879]
Epoch [19/120    avg_loss:0.654, val_acc:0.837]
Epoch [20/120    avg_loss:0.599, val_acc:0.883]
Epoch [21/120    avg_loss:0.543, val_acc:0.927]
Epoch [22/120    avg_loss:0.502, val_acc:0.903]
Epoch [23/120    avg_loss:0.483, val_acc:0.909]
Epoch [24/120    avg_loss:0.421, val_acc:0.927]
Epoch [25/120    avg_loss:0.438, val_acc:0.893]
Epoch [26/120    avg_loss:0.504, val_acc:0.907]
Epoch [27/120    avg_loss:0.397, val_acc:0.946]
Epoch [28/120    avg_loss:0.361, val_acc:0.944]
Epoch [29/120    avg_loss:0.342, val_acc:0.944]
Epoch [30/120    avg_loss:0.390, val_acc:0.944]
Epoch [31/120    avg_loss:0.328, val_acc:0.875]
Epoch [32/120    avg_loss:0.432, val_acc:0.948]
Epoch [33/120    avg_loss:0.303, val_acc:0.911]
Epoch [34/120    avg_loss:0.324, val_acc:0.960]
Epoch [35/120    avg_loss:0.271, val_acc:0.940]
Epoch [36/120    avg_loss:0.297, val_acc:0.942]
Epoch [37/120    avg_loss:0.216, val_acc:0.946]
Epoch [38/120    avg_loss:0.278, val_acc:0.970]
Epoch [39/120    avg_loss:0.240, val_acc:0.940]
Epoch [40/120    avg_loss:0.223, val_acc:0.958]
Epoch [41/120    avg_loss:0.217, val_acc:0.964]
Epoch [42/120    avg_loss:0.208, val_acc:0.935]
Epoch [43/120    avg_loss:0.213, val_acc:0.968]
Epoch [44/120    avg_loss:0.163, val_acc:0.966]
Epoch [45/120    avg_loss:0.166, val_acc:0.970]
Epoch [46/120    avg_loss:0.204, val_acc:0.956]
Epoch [47/120    avg_loss:0.170, val_acc:0.968]
Epoch [48/120    avg_loss:0.129, val_acc:0.970]
Epoch [49/120    avg_loss:0.149, val_acc:0.982]
Epoch [50/120    avg_loss:0.153, val_acc:0.964]
Epoch [51/120    avg_loss:0.159, val_acc:0.978]
Epoch [52/120    avg_loss:0.125, val_acc:0.960]
Epoch [53/120    avg_loss:0.099, val_acc:0.980]
Epoch [54/120    avg_loss:0.087, val_acc:0.984]
Epoch [55/120    avg_loss:0.076, val_acc:0.964]
Epoch [56/120    avg_loss:0.092, val_acc:0.982]
Epoch [57/120    avg_loss:0.116, val_acc:0.978]
Epoch [58/120    avg_loss:0.112, val_acc:0.964]
Epoch [59/120    avg_loss:0.126, val_acc:0.978]
Epoch [60/120    avg_loss:0.128, val_acc:0.966]
Epoch [61/120    avg_loss:0.131, val_acc:0.958]
Epoch [62/120    avg_loss:0.143, val_acc:0.978]
Epoch [63/120    avg_loss:0.093, val_acc:0.952]
Epoch [64/120    avg_loss:0.116, val_acc:0.984]
Epoch [65/120    avg_loss:0.063, val_acc:0.986]
Epoch [66/120    avg_loss:0.056, val_acc:0.968]
Epoch [67/120    avg_loss:0.076, val_acc:0.990]
Epoch [68/120    avg_loss:0.062, val_acc:0.984]
Epoch [69/120    avg_loss:0.071, val_acc:0.976]
Epoch [70/120    avg_loss:0.086, val_acc:0.974]
Epoch [71/120    avg_loss:0.057, val_acc:0.990]
Epoch [72/120    avg_loss:0.042, val_acc:0.988]
Epoch [73/120    avg_loss:0.044, val_acc:0.980]
Epoch [74/120    avg_loss:0.039, val_acc:0.988]
Epoch [75/120    avg_loss:0.047, val_acc:0.980]
Epoch [76/120    avg_loss:0.085, val_acc:0.984]
Epoch [77/120    avg_loss:0.073, val_acc:0.984]
Epoch [78/120    avg_loss:0.054, val_acc:0.994]
Epoch [79/120    avg_loss:0.067, val_acc:0.992]
Epoch [80/120    avg_loss:0.045, val_acc:0.988]
Epoch [81/120    avg_loss:0.075, val_acc:0.988]
Epoch [82/120    avg_loss:0.064, val_acc:0.986]
Epoch [83/120    avg_loss:0.052, val_acc:0.982]
Epoch [84/120    avg_loss:0.034, val_acc:0.990]
Epoch [85/120    avg_loss:0.035, val_acc:0.988]
Epoch [86/120    avg_loss:0.024, val_acc:0.980]
Epoch [87/120    avg_loss:0.043, val_acc:0.990]
Epoch [88/120    avg_loss:0.039, val_acc:0.994]
Epoch [89/120    avg_loss:0.032, val_acc:0.984]
Epoch [90/120    avg_loss:0.029, val_acc:0.984]
Epoch [91/120    avg_loss:0.026, val_acc:0.986]
Epoch [92/120    avg_loss:0.031, val_acc:0.990]
Epoch [93/120    avg_loss:0.032, val_acc:0.984]
Epoch [94/120    avg_loss:0.020, val_acc:0.990]
Epoch [95/120    avg_loss:0.026, val_acc:0.988]
Epoch [96/120    avg_loss:0.038, val_acc:0.978]
Epoch [97/120    avg_loss:0.043, val_acc:0.990]
Epoch [98/120    avg_loss:0.033, val_acc:0.982]
Epoch [99/120    avg_loss:0.030, val_acc:0.992]
Epoch [100/120    avg_loss:0.055, val_acc:0.986]
Epoch [101/120    avg_loss:0.040, val_acc:0.992]
Epoch [102/120    avg_loss:0.023, val_acc:0.994]
Epoch [103/120    avg_loss:0.017, val_acc:0.994]
Epoch [104/120    avg_loss:0.016, val_acc:0.994]
Epoch [105/120    avg_loss:0.020, val_acc:0.994]
Epoch [106/120    avg_loss:0.034, val_acc:0.992]
Epoch [107/120    avg_loss:0.015, val_acc:0.992]
Epoch [108/120    avg_loss:0.013, val_acc:0.992]
Epoch [109/120    avg_loss:0.014, val_acc:0.992]
Epoch [110/120    avg_loss:0.013, val_acc:0.992]
Epoch [111/120    avg_loss:0.013, val_acc:0.992]
Epoch [112/120    avg_loss:0.018, val_acc:0.992]
Epoch [113/120    avg_loss:0.015, val_acc:0.992]
Epoch [114/120    avg_loss:0.013, val_acc:0.992]
Epoch [115/120    avg_loss:0.012, val_acc:0.992]
Epoch [116/120    avg_loss:0.013, val_acc:0.992]
Epoch [117/120    avg_loss:0.012, val_acc:0.992]
Epoch [118/120    avg_loss:0.014, val_acc:0.992]
Epoch [119/120    avg_loss:0.011, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.98426966 0.98901099 0.95824176 0.95238095
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9938279159806969
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9846bdcb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.552, val_acc:0.395]
Epoch [2/120    avg_loss:2.389, val_acc:0.423]
Epoch [3/120    avg_loss:2.261, val_acc:0.490]
Epoch [4/120    avg_loss:2.128, val_acc:0.542]
Epoch [5/120    avg_loss:1.984, val_acc:0.562]
Epoch [6/120    avg_loss:1.829, val_acc:0.593]
Epoch [7/120    avg_loss:1.681, val_acc:0.591]
Epoch [8/120    avg_loss:1.517, val_acc:0.643]
Epoch [9/120    avg_loss:1.390, val_acc:0.663]
Epoch [10/120    avg_loss:1.292, val_acc:0.700]
Epoch [11/120    avg_loss:1.124, val_acc:0.718]
Epoch [12/120    avg_loss:1.020, val_acc:0.718]
Epoch [13/120    avg_loss:0.975, val_acc:0.742]
Epoch [14/120    avg_loss:0.905, val_acc:0.724]
Epoch [15/120    avg_loss:0.825, val_acc:0.802]
Epoch [16/120    avg_loss:0.759, val_acc:0.845]
Epoch [17/120    avg_loss:0.686, val_acc:0.806]
Epoch [18/120    avg_loss:0.688, val_acc:0.790]
Epoch [19/120    avg_loss:0.662, val_acc:0.863]
Epoch [20/120    avg_loss:0.579, val_acc:0.873]
Epoch [21/120    avg_loss:0.513, val_acc:0.911]
Epoch [22/120    avg_loss:0.488, val_acc:0.923]
Epoch [23/120    avg_loss:0.487, val_acc:0.917]
Epoch [24/120    avg_loss:0.447, val_acc:0.909]
Epoch [25/120    avg_loss:0.404, val_acc:0.915]
Epoch [26/120    avg_loss:0.392, val_acc:0.927]
Epoch [27/120    avg_loss:0.385, val_acc:0.919]
Epoch [28/120    avg_loss:0.415, val_acc:0.929]
Epoch [29/120    avg_loss:0.373, val_acc:0.915]
Epoch [30/120    avg_loss:0.386, val_acc:0.925]
Epoch [31/120    avg_loss:0.345, val_acc:0.940]
Epoch [32/120    avg_loss:0.330, val_acc:0.940]
Epoch [33/120    avg_loss:0.403, val_acc:0.905]
Epoch [34/120    avg_loss:0.324, val_acc:0.946]
Epoch [35/120    avg_loss:0.252, val_acc:0.958]
Epoch [36/120    avg_loss:0.272, val_acc:0.948]
Epoch [37/120    avg_loss:0.297, val_acc:0.935]
Epoch [38/120    avg_loss:0.251, val_acc:0.958]
Epoch [39/120    avg_loss:0.261, val_acc:0.950]
Epoch [40/120    avg_loss:0.276, val_acc:0.968]
Epoch [41/120    avg_loss:0.220, val_acc:0.962]
Epoch [42/120    avg_loss:0.239, val_acc:0.952]
Epoch [43/120    avg_loss:0.283, val_acc:0.948]
Epoch [44/120    avg_loss:0.180, val_acc:0.970]
Epoch [45/120    avg_loss:0.185, val_acc:0.964]
Epoch [46/120    avg_loss:0.199, val_acc:0.931]
Epoch [47/120    avg_loss:0.222, val_acc:0.964]
Epoch [48/120    avg_loss:0.216, val_acc:0.962]
Epoch [49/120    avg_loss:0.180, val_acc:0.968]
Epoch [50/120    avg_loss:0.190, val_acc:0.948]
Epoch [51/120    avg_loss:0.149, val_acc:0.962]
Epoch [52/120    avg_loss:0.190, val_acc:0.962]
Epoch [53/120    avg_loss:0.176, val_acc:0.952]
Epoch [54/120    avg_loss:0.148, val_acc:0.968]
Epoch [55/120    avg_loss:0.195, val_acc:0.958]
Epoch [56/120    avg_loss:0.184, val_acc:0.966]
Epoch [57/120    avg_loss:0.142, val_acc:0.974]
Epoch [58/120    avg_loss:0.132, val_acc:0.978]
Epoch [59/120    avg_loss:0.161, val_acc:0.958]
Epoch [60/120    avg_loss:0.126, val_acc:0.968]
Epoch [61/120    avg_loss:0.171, val_acc:0.970]
Epoch [62/120    avg_loss:0.116, val_acc:0.978]
Epoch [63/120    avg_loss:0.100, val_acc:0.956]
Epoch [64/120    avg_loss:0.127, val_acc:0.970]
Epoch [65/120    avg_loss:0.113, val_acc:0.972]
Epoch [66/120    avg_loss:0.116, val_acc:0.972]
Epoch [67/120    avg_loss:0.097, val_acc:0.982]
Epoch [68/120    avg_loss:0.081, val_acc:0.978]
Epoch [69/120    avg_loss:0.102, val_acc:0.976]
Epoch [70/120    avg_loss:0.121, val_acc:0.974]
Epoch [71/120    avg_loss:0.130, val_acc:0.974]
Epoch [72/120    avg_loss:0.140, val_acc:0.972]
Epoch [73/120    avg_loss:0.138, val_acc:0.980]
Epoch [74/120    avg_loss:0.118, val_acc:0.982]
Epoch [75/120    avg_loss:0.110, val_acc:0.980]
Epoch [76/120    avg_loss:0.073, val_acc:0.978]
Epoch [77/120    avg_loss:0.089, val_acc:0.946]
Epoch [78/120    avg_loss:0.102, val_acc:0.982]
Epoch [79/120    avg_loss:0.076, val_acc:0.986]
Epoch [80/120    avg_loss:0.081, val_acc:0.984]
Epoch [81/120    avg_loss:0.084, val_acc:0.984]
Epoch [82/120    avg_loss:0.058, val_acc:0.984]
Epoch [83/120    avg_loss:0.045, val_acc:0.978]
Epoch [84/120    avg_loss:0.037, val_acc:0.986]
Epoch [85/120    avg_loss:0.046, val_acc:0.990]
Epoch [86/120    avg_loss:0.053, val_acc:0.978]
Epoch [87/120    avg_loss:0.044, val_acc:0.978]
Epoch [88/120    avg_loss:0.059, val_acc:0.974]
Epoch [89/120    avg_loss:0.080, val_acc:0.980]
Epoch [90/120    avg_loss:0.048, val_acc:0.990]
Epoch [91/120    avg_loss:0.039, val_acc:0.980]
Epoch [92/120    avg_loss:0.031, val_acc:0.986]
Epoch [93/120    avg_loss:0.032, val_acc:0.988]
Epoch [94/120    avg_loss:0.035, val_acc:0.982]
Epoch [95/120    avg_loss:0.044, val_acc:0.990]
Epoch [96/120    avg_loss:0.066, val_acc:0.980]
Epoch [97/120    avg_loss:0.058, val_acc:0.984]
Epoch [98/120    avg_loss:0.033, val_acc:0.984]
Epoch [99/120    avg_loss:0.025, val_acc:0.988]
Epoch [100/120    avg_loss:0.035, val_acc:0.976]
Epoch [101/120    avg_loss:0.034, val_acc:0.984]
Epoch [102/120    avg_loss:0.024, val_acc:0.986]
Epoch [103/120    avg_loss:0.026, val_acc:0.986]
Epoch [104/120    avg_loss:0.027, val_acc:0.988]
Epoch [105/120    avg_loss:0.021, val_acc:0.988]
Epoch [106/120    avg_loss:0.021, val_acc:0.986]
Epoch [107/120    avg_loss:0.018, val_acc:0.986]
Epoch [108/120    avg_loss:0.025, val_acc:0.990]
Epoch [109/120    avg_loss:0.025, val_acc:0.984]
Epoch [110/120    avg_loss:0.033, val_acc:0.988]
Epoch [111/120    avg_loss:0.049, val_acc:0.982]
Epoch [112/120    avg_loss:0.072, val_acc:0.976]
Epoch [113/120    avg_loss:0.046, val_acc:0.968]
Epoch [114/120    avg_loss:0.047, val_acc:0.980]
Epoch [115/120    avg_loss:0.052, val_acc:0.980]
Epoch [116/120    avg_loss:0.035, val_acc:0.986]
Epoch [117/120    avg_loss:0.030, val_acc:0.986]
Epoch [118/120    avg_loss:0.024, val_acc:0.980]
Epoch [119/120    avg_loss:0.017, val_acc:0.986]
Epoch [120/120    avg_loss:0.020, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   1   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  14   0   0   0   0   0   0   2   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99853801 0.98206278 1.         0.9254386  0.89198606
 0.99757869 0.95555556 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9897922042648409
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc11832db38>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.547, val_acc:0.296]
Epoch [2/120    avg_loss:2.391, val_acc:0.312]
Epoch [3/120    avg_loss:2.284, val_acc:0.367]
Epoch [4/120    avg_loss:2.181, val_acc:0.490]
Epoch [5/120    avg_loss:2.069, val_acc:0.548]
Epoch [6/120    avg_loss:1.963, val_acc:0.623]
Epoch [7/120    avg_loss:1.819, val_acc:0.621]
Epoch [8/120    avg_loss:1.686, val_acc:0.617]
Epoch [9/120    avg_loss:1.561, val_acc:0.655]
Epoch [10/120    avg_loss:1.375, val_acc:0.710]
Epoch [11/120    avg_loss:1.264, val_acc:0.688]
Epoch [12/120    avg_loss:1.183, val_acc:0.720]
Epoch [13/120    avg_loss:1.057, val_acc:0.752]
Epoch [14/120    avg_loss:0.981, val_acc:0.764]
Epoch [15/120    avg_loss:0.890, val_acc:0.760]
Epoch [16/120    avg_loss:0.824, val_acc:0.831]
Epoch [17/120    avg_loss:0.828, val_acc:0.833]
Epoch [18/120    avg_loss:0.794, val_acc:0.786]
Epoch [19/120    avg_loss:0.773, val_acc:0.847]
Epoch [20/120    avg_loss:0.678, val_acc:0.873]
Epoch [21/120    avg_loss:0.613, val_acc:0.875]
Epoch [22/120    avg_loss:0.561, val_acc:0.881]
Epoch [23/120    avg_loss:0.555, val_acc:0.861]
Epoch [24/120    avg_loss:0.577, val_acc:0.869]
Epoch [25/120    avg_loss:0.530, val_acc:0.929]
Epoch [26/120    avg_loss:0.533, val_acc:0.917]
Epoch [27/120    avg_loss:0.465, val_acc:0.927]
Epoch [28/120    avg_loss:0.425, val_acc:0.923]
Epoch [29/120    avg_loss:0.441, val_acc:0.909]
Epoch [30/120    avg_loss:0.392, val_acc:0.925]
Epoch [31/120    avg_loss:0.382, val_acc:0.933]
Epoch [32/120    avg_loss:0.390, val_acc:0.875]
Epoch [33/120    avg_loss:0.460, val_acc:0.913]
Epoch [34/120    avg_loss:0.333, val_acc:0.909]
Epoch [35/120    avg_loss:0.373, val_acc:0.942]
Epoch [36/120    avg_loss:0.343, val_acc:0.946]
Epoch [37/120    avg_loss:0.299, val_acc:0.929]
Epoch [38/120    avg_loss:0.338, val_acc:0.958]
Epoch [39/120    avg_loss:0.277, val_acc:0.942]
Epoch [40/120    avg_loss:0.304, val_acc:0.944]
Epoch [41/120    avg_loss:0.281, val_acc:0.956]
Epoch [42/120    avg_loss:0.230, val_acc:0.948]
Epoch [43/120    avg_loss:0.213, val_acc:0.968]
Epoch [44/120    avg_loss:0.236, val_acc:0.944]
Epoch [45/120    avg_loss:0.286, val_acc:0.938]
Epoch [46/120    avg_loss:0.347, val_acc:0.909]
Epoch [47/120    avg_loss:0.305, val_acc:0.946]
Epoch [48/120    avg_loss:0.247, val_acc:0.938]
Epoch [49/120    avg_loss:0.195, val_acc:0.970]
Epoch [50/120    avg_loss:0.176, val_acc:0.968]
Epoch [51/120    avg_loss:0.150, val_acc:0.972]
Epoch [52/120    avg_loss:0.158, val_acc:0.968]
Epoch [53/120    avg_loss:0.168, val_acc:0.964]
Epoch [54/120    avg_loss:0.232, val_acc:0.958]
Epoch [55/120    avg_loss:0.178, val_acc:0.958]
Epoch [56/120    avg_loss:0.134, val_acc:0.964]
Epoch [57/120    avg_loss:0.134, val_acc:0.960]
Epoch [58/120    avg_loss:0.147, val_acc:0.976]
Epoch [59/120    avg_loss:0.093, val_acc:0.976]
Epoch [60/120    avg_loss:0.102, val_acc:0.984]
Epoch [61/120    avg_loss:0.138, val_acc:0.956]
Epoch [62/120    avg_loss:0.200, val_acc:0.966]
Epoch [63/120    avg_loss:0.145, val_acc:0.962]
Epoch [64/120    avg_loss:0.107, val_acc:0.972]
Epoch [65/120    avg_loss:0.084, val_acc:0.976]
Epoch [66/120    avg_loss:0.101, val_acc:0.982]
Epoch [67/120    avg_loss:0.118, val_acc:0.968]
Epoch [68/120    avg_loss:0.107, val_acc:0.974]
Epoch [69/120    avg_loss:0.081, val_acc:0.960]
Epoch [70/120    avg_loss:0.100, val_acc:0.974]
Epoch [71/120    avg_loss:0.141, val_acc:0.984]
Epoch [72/120    avg_loss:0.088, val_acc:0.960]
Epoch [73/120    avg_loss:0.126, val_acc:0.982]
Epoch [74/120    avg_loss:0.090, val_acc:0.976]
Epoch [75/120    avg_loss:0.053, val_acc:0.984]
Epoch [76/120    avg_loss:0.073, val_acc:0.982]
Epoch [77/120    avg_loss:0.068, val_acc:0.968]
Epoch [78/120    avg_loss:0.092, val_acc:0.982]
Epoch [79/120    avg_loss:0.070, val_acc:0.982]
Epoch [80/120    avg_loss:0.050, val_acc:0.984]
Epoch [81/120    avg_loss:0.063, val_acc:0.962]
Epoch [82/120    avg_loss:0.069, val_acc:0.986]
Epoch [83/120    avg_loss:0.052, val_acc:0.980]
Epoch [84/120    avg_loss:0.064, val_acc:0.966]
Epoch [85/120    avg_loss:0.105, val_acc:0.970]
Epoch [86/120    avg_loss:0.090, val_acc:0.984]
Epoch [87/120    avg_loss:0.088, val_acc:0.982]
Epoch [88/120    avg_loss:0.058, val_acc:0.982]
Epoch [89/120    avg_loss:0.049, val_acc:0.974]
Epoch [90/120    avg_loss:0.049, val_acc:0.982]
Epoch [91/120    avg_loss:0.044, val_acc:0.982]
Epoch [92/120    avg_loss:0.040, val_acc:0.990]
Epoch [93/120    avg_loss:0.050, val_acc:0.980]
Epoch [94/120    avg_loss:0.055, val_acc:0.980]
Epoch [95/120    avg_loss:0.037, val_acc:0.980]
Epoch [96/120    avg_loss:0.040, val_acc:0.980]
Epoch [97/120    avg_loss:0.111, val_acc:0.978]
Epoch [98/120    avg_loss:0.055, val_acc:0.982]
Epoch [99/120    avg_loss:0.041, val_acc:0.962]
Epoch [100/120    avg_loss:0.058, val_acc:0.982]
Epoch [101/120    avg_loss:0.051, val_acc:0.978]
Epoch [102/120    avg_loss:0.060, val_acc:0.978]
Epoch [103/120    avg_loss:0.114, val_acc:0.974]
Epoch [104/120    avg_loss:0.057, val_acc:0.978]
Epoch [105/120    avg_loss:0.091, val_acc:0.962]
Epoch [106/120    avg_loss:0.060, val_acc:0.980]
Epoch [107/120    avg_loss:0.028, val_acc:0.984]
Epoch [108/120    avg_loss:0.032, val_acc:0.980]
Epoch [109/120    avg_loss:0.028, val_acc:0.982]
Epoch [110/120    avg_loss:0.031, val_acc:0.980]
Epoch [111/120    avg_loss:0.022, val_acc:0.980]
Epoch [112/120    avg_loss:0.027, val_acc:0.982]
Epoch [113/120    avg_loss:0.026, val_acc:0.982]
Epoch [114/120    avg_loss:0.020, val_acc:0.980]
Epoch [115/120    avg_loss:0.019, val_acc:0.980]
Epoch [116/120    avg_loss:0.023, val_acc:0.980]
Epoch [117/120    avg_loss:0.018, val_acc:0.980]
Epoch [118/120    avg_loss:0.022, val_acc:0.982]
Epoch [119/120    avg_loss:0.017, val_acc:0.982]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99780541 0.99545455 0.99563319 0.93654267 0.90657439
 0.99277108 0.98924731 1.         1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9912174813874426
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7c1ed37b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.541, val_acc:0.337]
Epoch [2/120    avg_loss:2.361, val_acc:0.429]
Epoch [3/120    avg_loss:2.232, val_acc:0.442]
Epoch [4/120    avg_loss:2.119, val_acc:0.460]
Epoch [5/120    avg_loss:2.019, val_acc:0.502]
Epoch [6/120    avg_loss:1.922, val_acc:0.597]
Epoch [7/120    avg_loss:1.783, val_acc:0.617]
Epoch [8/120    avg_loss:1.671, val_acc:0.643]
Epoch [9/120    avg_loss:1.489, val_acc:0.669]
Epoch [10/120    avg_loss:1.374, val_acc:0.698]
Epoch [11/120    avg_loss:1.230, val_acc:0.722]
Epoch [12/120    avg_loss:1.096, val_acc:0.774]
Epoch [13/120    avg_loss:0.997, val_acc:0.883]
Epoch [14/120    avg_loss:0.867, val_acc:0.829]
Epoch [15/120    avg_loss:0.790, val_acc:0.843]
Epoch [16/120    avg_loss:0.699, val_acc:0.845]
Epoch [17/120    avg_loss:0.623, val_acc:0.905]
Epoch [18/120    avg_loss:0.567, val_acc:0.879]
Epoch [19/120    avg_loss:0.604, val_acc:0.875]
Epoch [20/120    avg_loss:0.538, val_acc:0.897]
Epoch [21/120    avg_loss:0.486, val_acc:0.897]
Epoch [22/120    avg_loss:0.455, val_acc:0.895]
Epoch [23/120    avg_loss:0.446, val_acc:0.933]
Epoch [24/120    avg_loss:0.475, val_acc:0.849]
Epoch [25/120    avg_loss:0.431, val_acc:0.952]
Epoch [26/120    avg_loss:0.377, val_acc:0.923]
Epoch [27/120    avg_loss:0.374, val_acc:0.903]
Epoch [28/120    avg_loss:0.430, val_acc:0.962]
Epoch [29/120    avg_loss:0.440, val_acc:0.964]
Epoch [30/120    avg_loss:0.337, val_acc:0.962]
Epoch [31/120    avg_loss:0.288, val_acc:0.966]
Epoch [32/120    avg_loss:0.281, val_acc:0.968]
Epoch [33/120    avg_loss:0.259, val_acc:0.954]
Epoch [34/120    avg_loss:0.303, val_acc:0.948]
Epoch [35/120    avg_loss:0.332, val_acc:0.950]
Epoch [36/120    avg_loss:0.254, val_acc:0.978]
Epoch [37/120    avg_loss:0.277, val_acc:0.950]
Epoch [38/120    avg_loss:0.263, val_acc:0.980]
Epoch [39/120    avg_loss:0.217, val_acc:0.962]
Epoch [40/120    avg_loss:0.215, val_acc:0.960]
Epoch [41/120    avg_loss:0.193, val_acc:0.986]
Epoch [42/120    avg_loss:0.174, val_acc:0.976]
Epoch [43/120    avg_loss:0.163, val_acc:0.978]
Epoch [44/120    avg_loss:0.138, val_acc:0.988]
Epoch [45/120    avg_loss:0.197, val_acc:0.954]
Epoch [46/120    avg_loss:0.211, val_acc:0.974]
Epoch [47/120    avg_loss:0.203, val_acc:0.972]
Epoch [48/120    avg_loss:0.166, val_acc:0.998]
Epoch [49/120    avg_loss:0.165, val_acc:0.986]
Epoch [50/120    avg_loss:0.144, val_acc:0.982]
Epoch [51/120    avg_loss:0.131, val_acc:0.994]
Epoch [52/120    avg_loss:0.108, val_acc:0.978]
Epoch [53/120    avg_loss:0.111, val_acc:0.990]
Epoch [54/120    avg_loss:0.109, val_acc:0.988]
Epoch [55/120    avg_loss:0.100, val_acc:0.994]
Epoch [56/120    avg_loss:0.096, val_acc:0.984]
Epoch [57/120    avg_loss:0.113, val_acc:0.986]
Epoch [58/120    avg_loss:0.140, val_acc:0.970]
Epoch [59/120    avg_loss:0.234, val_acc:0.948]
Epoch [60/120    avg_loss:0.148, val_acc:0.996]
Epoch [61/120    avg_loss:0.111, val_acc:0.972]
Epoch [62/120    avg_loss:0.100, val_acc:0.990]
Epoch [63/120    avg_loss:0.067, val_acc:0.990]
Epoch [64/120    avg_loss:0.059, val_acc:0.992]
Epoch [65/120    avg_loss:0.065, val_acc:0.994]
Epoch [66/120    avg_loss:0.067, val_acc:0.994]
Epoch [67/120    avg_loss:0.069, val_acc:0.996]
Epoch [68/120    avg_loss:0.061, val_acc:0.996]
Epoch [69/120    avg_loss:0.064, val_acc:0.998]
Epoch [70/120    avg_loss:0.063, val_acc:0.996]
Epoch [71/120    avg_loss:0.063, val_acc:0.996]
Epoch [72/120    avg_loss:0.049, val_acc:0.998]
Epoch [73/120    avg_loss:0.053, val_acc:0.996]
Epoch [74/120    avg_loss:0.055, val_acc:0.996]
Epoch [75/120    avg_loss:0.054, val_acc:0.998]
Epoch [76/120    avg_loss:0.052, val_acc:0.996]
Epoch [77/120    avg_loss:0.049, val_acc:0.996]
Epoch [78/120    avg_loss:0.047, val_acc:0.996]
Epoch [79/120    avg_loss:0.052, val_acc:0.996]
Epoch [80/120    avg_loss:0.062, val_acc:0.996]
Epoch [81/120    avg_loss:0.051, val_acc:0.996]
Epoch [82/120    avg_loss:0.048, val_acc:0.996]
Epoch [83/120    avg_loss:0.049, val_acc:0.996]
Epoch [84/120    avg_loss:0.051, val_acc:0.996]
Epoch [85/120    avg_loss:0.045, val_acc:0.996]
Epoch [86/120    avg_loss:0.051, val_acc:0.996]
Epoch [87/120    avg_loss:0.049, val_acc:0.998]
Epoch [88/120    avg_loss:0.047, val_acc:0.996]
Epoch [89/120    avg_loss:0.047, val_acc:0.996]
Epoch [90/120    avg_loss:0.047, val_acc:0.998]
Epoch [91/120    avg_loss:0.045, val_acc:0.998]
Epoch [92/120    avg_loss:0.046, val_acc:0.998]
Epoch [93/120    avg_loss:0.041, val_acc:0.998]
Epoch [94/120    avg_loss:0.042, val_acc:0.998]
Epoch [95/120    avg_loss:0.044, val_acc:0.996]
Epoch [96/120    avg_loss:0.046, val_acc:0.996]
Epoch [97/120    avg_loss:0.047, val_acc:0.996]
Epoch [98/120    avg_loss:0.037, val_acc:0.998]
Epoch [99/120    avg_loss:0.042, val_acc:0.998]
Epoch [100/120    avg_loss:0.038, val_acc:0.998]
Epoch [101/120    avg_loss:0.040, val_acc:0.998]
Epoch [102/120    avg_loss:0.041, val_acc:0.996]
Epoch [103/120    avg_loss:0.036, val_acc:0.996]
Epoch [104/120    avg_loss:0.039, val_acc:0.996]
Epoch [105/120    avg_loss:0.059, val_acc:0.996]
Epoch [106/120    avg_loss:0.036, val_acc:0.996]
Epoch [107/120    avg_loss:0.048, val_acc:0.996]
Epoch [108/120    avg_loss:0.039, val_acc:0.994]
Epoch [109/120    avg_loss:0.037, val_acc:0.996]
Epoch [110/120    avg_loss:0.047, val_acc:0.996]
Epoch [111/120    avg_loss:0.039, val_acc:0.994]
Epoch [112/120    avg_loss:0.035, val_acc:0.996]
Epoch [113/120    avg_loss:0.033, val_acc:0.998]
Epoch [114/120    avg_loss:0.043, val_acc:0.996]
Epoch [115/120    avg_loss:0.039, val_acc:0.996]
Epoch [116/120    avg_loss:0.038, val_acc:0.996]
Epoch [117/120    avg_loss:0.033, val_acc:0.996]
Epoch [118/120    avg_loss:0.039, val_acc:0.996]
Epoch [119/120    avg_loss:0.036, val_acc:0.998]
Epoch [120/120    avg_loss:0.032, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.99853801 0.98426966 1.         0.93763441 0.89605735
 0.99516908 0.96132597 1.         1.         1.         0.9843342
 0.98657718 1.        ]

Kappa:
0.9881309831296476
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ff3fdea20>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.581, val_acc:0.379]
Epoch [2/120    avg_loss:2.438, val_acc:0.440]
Epoch [3/120    avg_loss:2.312, val_acc:0.421]
Epoch [4/120    avg_loss:2.197, val_acc:0.446]
Epoch [5/120    avg_loss:2.071, val_acc:0.506]
Epoch [6/120    avg_loss:1.955, val_acc:0.565]
Epoch [7/120    avg_loss:1.825, val_acc:0.596]
Epoch [8/120    avg_loss:1.704, val_acc:0.615]
Epoch [9/120    avg_loss:1.618, val_acc:0.681]
Epoch [10/120    avg_loss:1.466, val_acc:0.710]
Epoch [11/120    avg_loss:1.346, val_acc:0.725]
Epoch [12/120    avg_loss:1.218, val_acc:0.744]
Epoch [13/120    avg_loss:1.117, val_acc:0.754]
Epoch [14/120    avg_loss:1.008, val_acc:0.785]
Epoch [15/120    avg_loss:0.996, val_acc:0.835]
Epoch [16/120    avg_loss:0.862, val_acc:0.892]
Epoch [17/120    avg_loss:0.738, val_acc:0.879]
Epoch [18/120    avg_loss:0.757, val_acc:0.892]
Epoch [19/120    avg_loss:0.648, val_acc:0.908]
Epoch [20/120    avg_loss:0.605, val_acc:0.890]
Epoch [21/120    avg_loss:0.557, val_acc:0.915]
Epoch [22/120    avg_loss:0.531, val_acc:0.910]
Epoch [23/120    avg_loss:0.535, val_acc:0.929]
Epoch [24/120    avg_loss:0.487, val_acc:0.935]
Epoch [25/120    avg_loss:0.512, val_acc:0.912]
Epoch [26/120    avg_loss:0.413, val_acc:0.904]
Epoch [27/120    avg_loss:0.416, val_acc:0.952]
Epoch [28/120    avg_loss:0.356, val_acc:0.944]
Epoch [29/120    avg_loss:0.367, val_acc:0.885]
Epoch [30/120    avg_loss:0.406, val_acc:0.921]
Epoch [31/120    avg_loss:0.369, val_acc:0.938]
Epoch [32/120    avg_loss:0.318, val_acc:0.963]
Epoch [33/120    avg_loss:0.304, val_acc:0.956]
Epoch [34/120    avg_loss:0.237, val_acc:0.950]
Epoch [35/120    avg_loss:0.251, val_acc:0.958]
Epoch [36/120    avg_loss:0.238, val_acc:0.931]
Epoch [37/120    avg_loss:0.365, val_acc:0.940]
Epoch [38/120    avg_loss:0.281, val_acc:0.952]
Epoch [39/120    avg_loss:0.258, val_acc:0.948]
Epoch [40/120    avg_loss:0.268, val_acc:0.969]
Epoch [41/120    avg_loss:0.253, val_acc:0.948]
Epoch [42/120    avg_loss:0.278, val_acc:0.965]
Epoch [43/120    avg_loss:0.208, val_acc:0.952]
Epoch [44/120    avg_loss:0.185, val_acc:0.967]
Epoch [45/120    avg_loss:0.187, val_acc:0.948]
Epoch [46/120    avg_loss:0.201, val_acc:0.971]
Epoch [47/120    avg_loss:0.222, val_acc:0.969]
Epoch [48/120    avg_loss:0.175, val_acc:0.977]
Epoch [49/120    avg_loss:0.198, val_acc:0.975]
Epoch [50/120    avg_loss:0.156, val_acc:0.956]
Epoch [51/120    avg_loss:0.245, val_acc:0.958]
Epoch [52/120    avg_loss:0.155, val_acc:0.973]
Epoch [53/120    avg_loss:0.148, val_acc:0.948]
Epoch [54/120    avg_loss:0.215, val_acc:0.971]
Epoch [55/120    avg_loss:0.228, val_acc:0.931]
Epoch [56/120    avg_loss:0.153, val_acc:0.960]
Epoch [57/120    avg_loss:0.126, val_acc:0.977]
Epoch [58/120    avg_loss:0.140, val_acc:0.975]
Epoch [59/120    avg_loss:0.115, val_acc:0.969]
Epoch [60/120    avg_loss:0.120, val_acc:0.977]
Epoch [61/120    avg_loss:0.137, val_acc:0.965]
Epoch [62/120    avg_loss:0.097, val_acc:0.981]
Epoch [63/120    avg_loss:0.086, val_acc:0.979]
Epoch [64/120    avg_loss:0.089, val_acc:0.975]
Epoch [65/120    avg_loss:0.108, val_acc:0.967]
Epoch [66/120    avg_loss:0.107, val_acc:0.992]
Epoch [67/120    avg_loss:0.095, val_acc:0.979]
Epoch [68/120    avg_loss:0.115, val_acc:0.969]
Epoch [69/120    avg_loss:0.170, val_acc:0.965]
Epoch [70/120    avg_loss:0.153, val_acc:0.977]
Epoch [71/120    avg_loss:0.084, val_acc:0.977]
Epoch [72/120    avg_loss:0.071, val_acc:0.990]
Epoch [73/120    avg_loss:0.060, val_acc:0.985]
Epoch [74/120    avg_loss:0.105, val_acc:0.977]
Epoch [75/120    avg_loss:0.104, val_acc:0.979]
Epoch [76/120    avg_loss:0.071, val_acc:0.985]
Epoch [77/120    avg_loss:0.055, val_acc:0.975]
Epoch [78/120    avg_loss:0.099, val_acc:0.969]
Epoch [79/120    avg_loss:0.104, val_acc:0.977]
Epoch [80/120    avg_loss:0.064, val_acc:0.977]
Epoch [81/120    avg_loss:0.044, val_acc:0.977]
Epoch [82/120    avg_loss:0.049, val_acc:0.977]
Epoch [83/120    avg_loss:0.048, val_acc:0.977]
Epoch [84/120    avg_loss:0.044, val_acc:0.981]
Epoch [85/120    avg_loss:0.046, val_acc:0.979]
Epoch [86/120    avg_loss:0.033, val_acc:0.979]
Epoch [87/120    avg_loss:0.046, val_acc:0.979]
Epoch [88/120    avg_loss:0.036, val_acc:0.985]
Epoch [89/120    avg_loss:0.038, val_acc:0.983]
Epoch [90/120    avg_loss:0.037, val_acc:0.979]
Epoch [91/120    avg_loss:0.042, val_acc:0.985]
Epoch [92/120    avg_loss:0.036, val_acc:0.985]
Epoch [93/120    avg_loss:0.043, val_acc:0.985]
Epoch [94/120    avg_loss:0.044, val_acc:0.985]
Epoch [95/120    avg_loss:0.036, val_acc:0.985]
Epoch [96/120    avg_loss:0.040, val_acc:0.985]
Epoch [97/120    avg_loss:0.038, val_acc:0.985]
Epoch [98/120    avg_loss:0.029, val_acc:0.985]
Epoch [99/120    avg_loss:0.037, val_acc:0.985]
Epoch [100/120    avg_loss:0.037, val_acc:0.985]
Epoch [101/120    avg_loss:0.037, val_acc:0.985]
Epoch [102/120    avg_loss:0.037, val_acc:0.985]
Epoch [103/120    avg_loss:0.029, val_acc:0.985]
Epoch [104/120    avg_loss:0.036, val_acc:0.985]
Epoch [105/120    avg_loss:0.037, val_acc:0.983]
Epoch [106/120    avg_loss:0.034, val_acc:0.983]
Epoch [107/120    avg_loss:0.039, val_acc:0.983]
Epoch [108/120    avg_loss:0.033, val_acc:0.983]
Epoch [109/120    avg_loss:0.030, val_acc:0.983]
Epoch [110/120    avg_loss:0.029, val_acc:0.983]
Epoch [111/120    avg_loss:0.032, val_acc:0.983]
Epoch [112/120    avg_loss:0.037, val_acc:0.983]
Epoch [113/120    avg_loss:0.037, val_acc:0.983]
Epoch [114/120    avg_loss:0.034, val_acc:0.983]
Epoch [115/120    avg_loss:0.034, val_acc:0.983]
Epoch [116/120    avg_loss:0.039, val_acc:0.983]
Epoch [117/120    avg_loss:0.030, val_acc:0.983]
Epoch [118/120    avg_loss:0.029, val_acc:0.983]
Epoch [119/120    avg_loss:0.031, val_acc:0.983]
Epoch [120/120    avg_loss:0.033, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  15   0   0   0   0   0   0   2   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 0.99926954 0.97986577 0.99782135 0.91503268 0.86925795
 0.99757869 0.95555556 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9883674043544104
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4d10fcb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.568, val_acc:0.226]
Epoch [2/120    avg_loss:2.414, val_acc:0.371]
Epoch [3/120    avg_loss:2.310, val_acc:0.387]
Epoch [4/120    avg_loss:2.187, val_acc:0.423]
Epoch [5/120    avg_loss:2.059, val_acc:0.435]
Epoch [6/120    avg_loss:1.902, val_acc:0.476]
Epoch [7/120    avg_loss:1.792, val_acc:0.575]
Epoch [8/120    avg_loss:1.668, val_acc:0.609]
Epoch [9/120    avg_loss:1.563, val_acc:0.645]
Epoch [10/120    avg_loss:1.476, val_acc:0.720]
Epoch [11/120    avg_loss:1.389, val_acc:0.700]
Epoch [12/120    avg_loss:1.250, val_acc:0.714]
Epoch [13/120    avg_loss:1.100, val_acc:0.728]
Epoch [14/120    avg_loss:0.988, val_acc:0.770]
Epoch [15/120    avg_loss:0.929, val_acc:0.782]
Epoch [16/120    avg_loss:0.858, val_acc:0.802]
Epoch [17/120    avg_loss:0.759, val_acc:0.810]
Epoch [18/120    avg_loss:0.718, val_acc:0.881]
Epoch [19/120    avg_loss:0.619, val_acc:0.861]
Epoch [20/120    avg_loss:0.571, val_acc:0.855]
Epoch [21/120    avg_loss:0.647, val_acc:0.855]
Epoch [22/120    avg_loss:0.590, val_acc:0.869]
Epoch [23/120    avg_loss:0.498, val_acc:0.909]
Epoch [24/120    avg_loss:0.451, val_acc:0.879]
Epoch [25/120    avg_loss:0.408, val_acc:0.889]
Epoch [26/120    avg_loss:0.435, val_acc:0.917]
Epoch [27/120    avg_loss:0.417, val_acc:0.915]
Epoch [28/120    avg_loss:0.398, val_acc:0.901]
Epoch [29/120    avg_loss:0.406, val_acc:0.913]
Epoch [30/120    avg_loss:0.326, val_acc:0.933]
Epoch [31/120    avg_loss:0.397, val_acc:0.933]
Epoch [32/120    avg_loss:0.357, val_acc:0.919]
Epoch [33/120    avg_loss:0.367, val_acc:0.942]
Epoch [34/120    avg_loss:0.292, val_acc:0.917]
Epoch [35/120    avg_loss:0.279, val_acc:0.891]
Epoch [36/120    avg_loss:0.311, val_acc:0.923]
Epoch [37/120    avg_loss:0.256, val_acc:0.946]
Epoch [38/120    avg_loss:0.213, val_acc:0.938]
Epoch [39/120    avg_loss:0.238, val_acc:0.911]
Epoch [40/120    avg_loss:0.246, val_acc:0.915]
Epoch [41/120    avg_loss:0.225, val_acc:0.954]
Epoch [42/120    avg_loss:0.205, val_acc:0.950]
Epoch [43/120    avg_loss:0.198, val_acc:0.946]
Epoch [44/120    avg_loss:0.240, val_acc:0.942]
Epoch [45/120    avg_loss:0.248, val_acc:0.913]
Epoch [46/120    avg_loss:0.253, val_acc:0.938]
Epoch [47/120    avg_loss:0.203, val_acc:0.931]
Epoch [48/120    avg_loss:0.202, val_acc:0.948]
Epoch [49/120    avg_loss:0.162, val_acc:0.948]
Epoch [50/120    avg_loss:0.169, val_acc:0.970]
Epoch [51/120    avg_loss:0.187, val_acc:0.970]
Epoch [52/120    avg_loss:0.176, val_acc:0.952]
Epoch [53/120    avg_loss:0.189, val_acc:0.966]
Epoch [54/120    avg_loss:0.147, val_acc:0.972]
Epoch [55/120    avg_loss:0.147, val_acc:0.948]
Epoch [56/120    avg_loss:0.158, val_acc:0.960]
Epoch [57/120    avg_loss:0.141, val_acc:0.956]
Epoch [58/120    avg_loss:0.141, val_acc:0.972]
Epoch [59/120    avg_loss:0.155, val_acc:0.935]
Epoch [60/120    avg_loss:0.136, val_acc:0.970]
Epoch [61/120    avg_loss:0.125, val_acc:0.974]
Epoch [62/120    avg_loss:0.085, val_acc:0.970]
Epoch [63/120    avg_loss:0.100, val_acc:0.984]
Epoch [64/120    avg_loss:0.087, val_acc:0.978]
Epoch [65/120    avg_loss:0.077, val_acc:0.986]
Epoch [66/120    avg_loss:0.087, val_acc:0.976]
Epoch [67/120    avg_loss:0.088, val_acc:0.978]
Epoch [68/120    avg_loss:0.104, val_acc:0.919]
Epoch [69/120    avg_loss:0.153, val_acc:0.980]
Epoch [70/120    avg_loss:0.090, val_acc:0.978]
Epoch [71/120    avg_loss:0.087, val_acc:0.960]
Epoch [72/120    avg_loss:0.159, val_acc:0.923]
Epoch [73/120    avg_loss:0.081, val_acc:0.980]
Epoch [74/120    avg_loss:0.110, val_acc:0.946]
Epoch [75/120    avg_loss:0.107, val_acc:0.940]
Epoch [76/120    avg_loss:0.080, val_acc:0.960]
Epoch [77/120    avg_loss:0.064, val_acc:0.960]
Epoch [78/120    avg_loss:0.055, val_acc:0.978]
Epoch [79/120    avg_loss:0.066, val_acc:0.986]
Epoch [80/120    avg_loss:0.036, val_acc:0.988]
Epoch [81/120    avg_loss:0.042, val_acc:0.990]
Epoch [82/120    avg_loss:0.042, val_acc:0.992]
Epoch [83/120    avg_loss:0.037, val_acc:0.992]
Epoch [84/120    avg_loss:0.034, val_acc:0.992]
Epoch [85/120    avg_loss:0.044, val_acc:0.990]
Epoch [86/120    avg_loss:0.035, val_acc:0.990]
Epoch [87/120    avg_loss:0.036, val_acc:0.990]
Epoch [88/120    avg_loss:0.038, val_acc:0.992]
Epoch [89/120    avg_loss:0.034, val_acc:0.990]
Epoch [90/120    avg_loss:0.036, val_acc:0.990]
Epoch [91/120    avg_loss:0.038, val_acc:0.992]
Epoch [92/120    avg_loss:0.036, val_acc:0.990]
Epoch [93/120    avg_loss:0.041, val_acc:0.990]
Epoch [94/120    avg_loss:0.032, val_acc:0.992]
Epoch [95/120    avg_loss:0.034, val_acc:0.990]
Epoch [96/120    avg_loss:0.031, val_acc:0.990]
Epoch [97/120    avg_loss:0.042, val_acc:0.990]
Epoch [98/120    avg_loss:0.031, val_acc:0.992]
Epoch [99/120    avg_loss:0.035, val_acc:0.992]
Epoch [100/120    avg_loss:0.031, val_acc:0.992]
Epoch [101/120    avg_loss:0.036, val_acc:0.992]
Epoch [102/120    avg_loss:0.036, val_acc:0.992]
Epoch [103/120    avg_loss:0.033, val_acc:0.992]
Epoch [104/120    avg_loss:0.034, val_acc:0.992]
Epoch [105/120    avg_loss:0.040, val_acc:0.994]
Epoch [106/120    avg_loss:0.036, val_acc:0.994]
Epoch [107/120    avg_loss:0.032, val_acc:0.992]
Epoch [108/120    avg_loss:0.031, val_acc:0.992]
Epoch [109/120    avg_loss:0.042, val_acc:0.994]
Epoch [110/120    avg_loss:0.031, val_acc:0.994]
Epoch [111/120    avg_loss:0.037, val_acc:0.992]
Epoch [112/120    avg_loss:0.033, val_acc:0.992]
Epoch [113/120    avg_loss:0.030, val_acc:0.992]
Epoch [114/120    avg_loss:0.029, val_acc:0.992]
Epoch [115/120    avg_loss:0.028, val_acc:0.992]
Epoch [116/120    avg_loss:0.027, val_acc:0.994]
Epoch [117/120    avg_loss:0.032, val_acc:0.994]
Epoch [118/120    avg_loss:0.028, val_acc:0.994]
Epoch [119/120    avg_loss:0.027, val_acc:0.994]
Epoch [120/120    avg_loss:0.032, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   1   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 0.99927061 0.97986577 0.99782135 0.95132743 0.92783505
 0.99756691 0.95604396 1.         1.         0.99862826 1.
 0.99778761 1.        ]

Kappa:
0.9921660928027211
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f912c5bea20>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.639, val_acc:0.441]
Epoch [2/120    avg_loss:2.435, val_acc:0.472]
Epoch [3/120    avg_loss:2.296, val_acc:0.536]
Epoch [4/120    avg_loss:2.169, val_acc:0.562]
Epoch [5/120    avg_loss:2.039, val_acc:0.565]
Epoch [6/120    avg_loss:1.896, val_acc:0.585]
Epoch [7/120    avg_loss:1.758, val_acc:0.583]
Epoch [8/120    avg_loss:1.628, val_acc:0.625]
Epoch [9/120    avg_loss:1.489, val_acc:0.681]
Epoch [10/120    avg_loss:1.387, val_acc:0.653]
Epoch [11/120    avg_loss:1.277, val_acc:0.702]
Epoch [12/120    avg_loss:1.131, val_acc:0.750]
Epoch [13/120    avg_loss:1.035, val_acc:0.770]
Epoch [14/120    avg_loss:0.928, val_acc:0.768]
Epoch [15/120    avg_loss:0.867, val_acc:0.782]
Epoch [16/120    avg_loss:0.788, val_acc:0.772]
Epoch [17/120    avg_loss:0.810, val_acc:0.815]
Epoch [18/120    avg_loss:0.693, val_acc:0.796]
Epoch [19/120    avg_loss:0.646, val_acc:0.792]
Epoch [20/120    avg_loss:0.598, val_acc:0.794]
Epoch [21/120    avg_loss:0.645, val_acc:0.802]
Epoch [22/120    avg_loss:0.586, val_acc:0.815]
Epoch [23/120    avg_loss:0.563, val_acc:0.845]
Epoch [24/120    avg_loss:0.536, val_acc:0.927]
Epoch [25/120    avg_loss:0.499, val_acc:0.909]
Epoch [26/120    avg_loss:0.511, val_acc:0.865]
Epoch [27/120    avg_loss:0.417, val_acc:0.931]
Epoch [28/120    avg_loss:0.441, val_acc:0.800]
Epoch [29/120    avg_loss:0.387, val_acc:0.915]
Epoch [30/120    avg_loss:0.403, val_acc:0.923]
Epoch [31/120    avg_loss:0.351, val_acc:0.944]
Epoch [32/120    avg_loss:0.371, val_acc:0.933]
Epoch [33/120    avg_loss:0.347, val_acc:0.923]
Epoch [34/120    avg_loss:0.337, val_acc:0.946]
Epoch [35/120    avg_loss:0.297, val_acc:0.960]
Epoch [36/120    avg_loss:0.290, val_acc:0.946]
Epoch [37/120    avg_loss:0.324, val_acc:0.944]
Epoch [38/120    avg_loss:0.349, val_acc:0.875]
Epoch [39/120    avg_loss:0.297, val_acc:0.944]
Epoch [40/120    avg_loss:0.261, val_acc:0.968]
Epoch [41/120    avg_loss:0.244, val_acc:0.960]
Epoch [42/120    avg_loss:0.261, val_acc:0.966]
Epoch [43/120    avg_loss:0.205, val_acc:0.956]
Epoch [44/120    avg_loss:0.289, val_acc:0.962]
Epoch [45/120    avg_loss:0.263, val_acc:0.960]
Epoch [46/120    avg_loss:0.235, val_acc:0.958]
Epoch [47/120    avg_loss:0.188, val_acc:0.978]
Epoch [48/120    avg_loss:0.170, val_acc:0.935]
Epoch [49/120    avg_loss:0.189, val_acc:0.950]
Epoch [50/120    avg_loss:0.231, val_acc:0.938]
Epoch [51/120    avg_loss:0.224, val_acc:0.956]
Epoch [52/120    avg_loss:0.181, val_acc:0.980]
Epoch [53/120    avg_loss:0.145, val_acc:0.976]
Epoch [54/120    avg_loss:0.163, val_acc:0.972]
Epoch [55/120    avg_loss:0.132, val_acc:0.976]
Epoch [56/120    avg_loss:0.119, val_acc:0.976]
Epoch [57/120    avg_loss:0.160, val_acc:0.956]
Epoch [58/120    avg_loss:0.157, val_acc:0.927]
Epoch [59/120    avg_loss:0.122, val_acc:0.966]
Epoch [60/120    avg_loss:0.121, val_acc:0.968]
Epoch [61/120    avg_loss:0.105, val_acc:0.970]
Epoch [62/120    avg_loss:0.097, val_acc:0.960]
Epoch [63/120    avg_loss:0.118, val_acc:0.982]
Epoch [64/120    avg_loss:0.107, val_acc:0.978]
Epoch [65/120    avg_loss:0.114, val_acc:0.984]
Epoch [66/120    avg_loss:0.092, val_acc:0.974]
Epoch [67/120    avg_loss:0.086, val_acc:0.976]
Epoch [68/120    avg_loss:0.119, val_acc:0.964]
Epoch [69/120    avg_loss:0.106, val_acc:0.972]
Epoch [70/120    avg_loss:0.110, val_acc:0.968]
Epoch [71/120    avg_loss:0.093, val_acc:0.986]
Epoch [72/120    avg_loss:0.074, val_acc:0.988]
Epoch [73/120    avg_loss:0.054, val_acc:0.986]
Epoch [74/120    avg_loss:0.055, val_acc:0.990]
Epoch [75/120    avg_loss:0.071, val_acc:0.974]
Epoch [76/120    avg_loss:0.069, val_acc:0.980]
Epoch [77/120    avg_loss:0.076, val_acc:0.976]
Epoch [78/120    avg_loss:0.104, val_acc:0.982]
Epoch [79/120    avg_loss:0.087, val_acc:0.966]
Epoch [80/120    avg_loss:0.126, val_acc:0.980]
Epoch [81/120    avg_loss:0.178, val_acc:0.980]
Epoch [82/120    avg_loss:0.154, val_acc:0.960]
Epoch [83/120    avg_loss:0.102, val_acc:0.984]
Epoch [84/120    avg_loss:0.061, val_acc:0.982]
Epoch [85/120    avg_loss:0.062, val_acc:0.978]
Epoch [86/120    avg_loss:0.088, val_acc:0.974]
Epoch [87/120    avg_loss:0.109, val_acc:0.984]
Epoch [88/120    avg_loss:0.049, val_acc:0.986]
Epoch [89/120    avg_loss:0.058, val_acc:0.988]
Epoch [90/120    avg_loss:0.036, val_acc:0.986]
Epoch [91/120    avg_loss:0.031, val_acc:0.988]
Epoch [92/120    avg_loss:0.036, val_acc:0.988]
Epoch [93/120    avg_loss:0.040, val_acc:0.988]
Epoch [94/120    avg_loss:0.035, val_acc:0.988]
Epoch [95/120    avg_loss:0.033, val_acc:0.988]
Epoch [96/120    avg_loss:0.033, val_acc:0.988]
Epoch [97/120    avg_loss:0.031, val_acc:0.988]
Epoch [98/120    avg_loss:0.035, val_acc:0.988]
Epoch [99/120    avg_loss:0.031, val_acc:0.988]
Epoch [100/120    avg_loss:0.033, val_acc:0.988]
Epoch [101/120    avg_loss:0.030, val_acc:0.988]
Epoch [102/120    avg_loss:0.033, val_acc:0.988]
Epoch [103/120    avg_loss:0.042, val_acc:0.988]
Epoch [104/120    avg_loss:0.032, val_acc:0.988]
Epoch [105/120    avg_loss:0.029, val_acc:0.988]
Epoch [106/120    avg_loss:0.030, val_acc:0.988]
Epoch [107/120    avg_loss:0.030, val_acc:0.988]
Epoch [108/120    avg_loss:0.027, val_acc:0.988]
Epoch [109/120    avg_loss:0.030, val_acc:0.990]
Epoch [110/120    avg_loss:0.031, val_acc:0.990]
Epoch [111/120    avg_loss:0.031, val_acc:0.990]
Epoch [112/120    avg_loss:0.030, val_acc:0.990]
Epoch [113/120    avg_loss:0.032, val_acc:0.990]
Epoch [114/120    avg_loss:0.027, val_acc:0.990]
Epoch [115/120    avg_loss:0.025, val_acc:0.990]
Epoch [116/120    avg_loss:0.028, val_acc:0.990]
Epoch [117/120    avg_loss:0.033, val_acc:0.990]
Epoch [118/120    avg_loss:0.029, val_acc:0.990]
Epoch [119/120    avg_loss:0.029, val_acc:0.990]
Epoch [120/120    avg_loss:0.033, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  16   0   0   0   0   0   0   3   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 0.99926954 0.97117517 0.99563319 0.94545455 0.93023256
 0.99757869 0.93785311 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9909792079398357
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff28dc47ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.568, val_acc:0.333]
Epoch [2/120    avg_loss:2.404, val_acc:0.423]
Epoch [3/120    avg_loss:2.291, val_acc:0.456]
Epoch [4/120    avg_loss:2.163, val_acc:0.546]
Epoch [5/120    avg_loss:2.038, val_acc:0.599]
Epoch [6/120    avg_loss:1.897, val_acc:0.625]
Epoch [7/120    avg_loss:1.739, val_acc:0.663]
Epoch [8/120    avg_loss:1.614, val_acc:0.690]
Epoch [9/120    avg_loss:1.480, val_acc:0.702]
Epoch [10/120    avg_loss:1.337, val_acc:0.732]
Epoch [11/120    avg_loss:1.180, val_acc:0.720]
Epoch [12/120    avg_loss:1.049, val_acc:0.810]
Epoch [13/120    avg_loss:0.972, val_acc:0.808]
Epoch [14/120    avg_loss:0.901, val_acc:0.871]
Epoch [15/120    avg_loss:0.808, val_acc:0.861]
Epoch [16/120    avg_loss:0.730, val_acc:0.889]
Epoch [17/120    avg_loss:0.690, val_acc:0.843]
Epoch [18/120    avg_loss:0.634, val_acc:0.913]
Epoch [19/120    avg_loss:0.539, val_acc:0.849]
Epoch [20/120    avg_loss:0.562, val_acc:0.929]
Epoch [21/120    avg_loss:0.498, val_acc:0.915]
Epoch [22/120    avg_loss:0.464, val_acc:0.909]
Epoch [23/120    avg_loss:0.417, val_acc:0.907]
Epoch [24/120    avg_loss:0.469, val_acc:0.897]
Epoch [25/120    avg_loss:0.425, val_acc:0.917]
Epoch [26/120    avg_loss:0.391, val_acc:0.944]
Epoch [27/120    avg_loss:0.370, val_acc:0.929]
Epoch [28/120    avg_loss:0.424, val_acc:0.933]
Epoch [29/120    avg_loss:0.371, val_acc:0.927]
Epoch [30/120    avg_loss:0.366, val_acc:0.923]
Epoch [31/120    avg_loss:0.388, val_acc:0.946]
Epoch [32/120    avg_loss:0.279, val_acc:0.952]
Epoch [33/120    avg_loss:0.263, val_acc:0.938]
Epoch [34/120    avg_loss:0.286, val_acc:0.950]
Epoch [35/120    avg_loss:0.305, val_acc:0.907]
Epoch [36/120    avg_loss:0.322, val_acc:0.909]
Epoch [37/120    avg_loss:0.358, val_acc:0.931]
Epoch [38/120    avg_loss:0.287, val_acc:0.954]
Epoch [39/120    avg_loss:0.264, val_acc:0.962]
Epoch [40/120    avg_loss:0.244, val_acc:0.968]
Epoch [41/120    avg_loss:0.279, val_acc:0.958]
Epoch [42/120    avg_loss:0.305, val_acc:0.917]
Epoch [43/120    avg_loss:0.356, val_acc:0.909]
Epoch [44/120    avg_loss:0.246, val_acc:0.966]
Epoch [45/120    avg_loss:0.242, val_acc:0.968]
Epoch [46/120    avg_loss:0.246, val_acc:0.980]
Epoch [47/120    avg_loss:0.172, val_acc:0.964]
Epoch [48/120    avg_loss:0.171, val_acc:0.964]
Epoch [49/120    avg_loss:0.179, val_acc:0.952]
Epoch [50/120    avg_loss:0.183, val_acc:0.976]
Epoch [51/120    avg_loss:0.129, val_acc:0.958]
Epoch [52/120    avg_loss:0.168, val_acc:0.968]
Epoch [53/120    avg_loss:0.189, val_acc:0.942]
Epoch [54/120    avg_loss:0.185, val_acc:0.980]
Epoch [55/120    avg_loss:0.214, val_acc:0.966]
Epoch [56/120    avg_loss:0.171, val_acc:0.972]
Epoch [57/120    avg_loss:0.156, val_acc:0.952]
Epoch [58/120    avg_loss:0.260, val_acc:0.964]
Epoch [59/120    avg_loss:0.142, val_acc:0.960]
Epoch [60/120    avg_loss:0.138, val_acc:0.976]
Epoch [61/120    avg_loss:0.140, val_acc:0.980]
Epoch [62/120    avg_loss:0.129, val_acc:0.988]
Epoch [63/120    avg_loss:0.107, val_acc:0.974]
Epoch [64/120    avg_loss:0.120, val_acc:0.974]
Epoch [65/120    avg_loss:0.131, val_acc:0.952]
Epoch [66/120    avg_loss:0.099, val_acc:0.970]
Epoch [67/120    avg_loss:0.147, val_acc:0.976]
Epoch [68/120    avg_loss:0.103, val_acc:0.992]
Epoch [69/120    avg_loss:0.072, val_acc:0.982]
Epoch [70/120    avg_loss:0.118, val_acc:0.966]
Epoch [71/120    avg_loss:0.073, val_acc:0.990]
Epoch [72/120    avg_loss:0.068, val_acc:0.988]
Epoch [73/120    avg_loss:0.092, val_acc:0.984]
Epoch [74/120    avg_loss:0.109, val_acc:0.972]
Epoch [75/120    avg_loss:0.095, val_acc:0.990]
Epoch [76/120    avg_loss:0.052, val_acc:0.990]
Epoch [77/120    avg_loss:0.046, val_acc:0.988]
Epoch [78/120    avg_loss:0.085, val_acc:0.972]
Epoch [79/120    avg_loss:0.080, val_acc:0.984]
Epoch [80/120    avg_loss:0.091, val_acc:0.984]
Epoch [81/120    avg_loss:0.070, val_acc:0.988]
Epoch [82/120    avg_loss:0.042, val_acc:0.992]
Epoch [83/120    avg_loss:0.041, val_acc:0.996]
Epoch [84/120    avg_loss:0.055, val_acc:0.994]
Epoch [85/120    avg_loss:0.040, val_acc:0.994]
Epoch [86/120    avg_loss:0.037, val_acc:0.994]
Epoch [87/120    avg_loss:0.033, val_acc:0.994]
Epoch [88/120    avg_loss:0.032, val_acc:0.996]
Epoch [89/120    avg_loss:0.038, val_acc:0.994]
Epoch [90/120    avg_loss:0.037, val_acc:0.994]
Epoch [91/120    avg_loss:0.037, val_acc:0.994]
Epoch [92/120    avg_loss:0.035, val_acc:0.996]
Epoch [93/120    avg_loss:0.028, val_acc:0.994]
Epoch [94/120    avg_loss:0.034, val_acc:0.994]
Epoch [95/120    avg_loss:0.040, val_acc:0.994]
Epoch [96/120    avg_loss:0.033, val_acc:0.994]
Epoch [97/120    avg_loss:0.041, val_acc:0.994]
Epoch [98/120    avg_loss:0.034, val_acc:0.994]
Epoch [99/120    avg_loss:0.030, val_acc:0.994]
Epoch [100/120    avg_loss:0.032, val_acc:0.994]
Epoch [101/120    avg_loss:0.023, val_acc:0.994]
Epoch [102/120    avg_loss:0.029, val_acc:0.994]
Epoch [103/120    avg_loss:0.031, val_acc:0.994]
Epoch [104/120    avg_loss:0.024, val_acc:0.994]
Epoch [105/120    avg_loss:0.031, val_acc:0.994]
Epoch [106/120    avg_loss:0.036, val_acc:0.994]
Epoch [107/120    avg_loss:0.032, val_acc:0.994]
Epoch [108/120    avg_loss:0.031, val_acc:0.994]
Epoch [109/120    avg_loss:0.038, val_acc:0.994]
Epoch [110/120    avg_loss:0.032, val_acc:0.994]
Epoch [111/120    avg_loss:0.027, val_acc:0.994]
Epoch [112/120    avg_loss:0.026, val_acc:0.994]
Epoch [113/120    avg_loss:0.028, val_acc:0.994]
Epoch [114/120    avg_loss:0.028, val_acc:0.994]
Epoch [115/120    avg_loss:0.025, val_acc:0.994]
Epoch [116/120    avg_loss:0.025, val_acc:0.994]
Epoch [117/120    avg_loss:0.024, val_acc:0.994]
Epoch [118/120    avg_loss:0.029, val_acc:0.994]
Epoch [119/120    avg_loss:0.029, val_acc:0.994]
Epoch [120/120    avg_loss:0.025, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99853801 0.98648649 1.         0.94065934 0.90657439
 0.99516908 0.96703297 1.         1.         1.         0.98820446
 0.98996656 1.        ]

Kappa:
0.98955564418333
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efbb6bf8a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.578, val_acc:0.304]
Epoch [2/120    avg_loss:2.417, val_acc:0.419]
Epoch [3/120    avg_loss:2.278, val_acc:0.462]
Epoch [4/120    avg_loss:2.158, val_acc:0.528]
Epoch [5/120    avg_loss:2.017, val_acc:0.581]
Epoch [6/120    avg_loss:1.870, val_acc:0.615]
Epoch [7/120    avg_loss:1.713, val_acc:0.657]
Epoch [8/120    avg_loss:1.585, val_acc:0.647]
Epoch [9/120    avg_loss:1.436, val_acc:0.683]
Epoch [10/120    avg_loss:1.322, val_acc:0.736]
Epoch [11/120    avg_loss:1.160, val_acc:0.748]
Epoch [12/120    avg_loss:1.078, val_acc:0.758]
Epoch [13/120    avg_loss:0.971, val_acc:0.764]
Epoch [14/120    avg_loss:0.902, val_acc:0.792]
Epoch [15/120    avg_loss:0.840, val_acc:0.748]
Epoch [16/120    avg_loss:0.757, val_acc:0.833]
Epoch [17/120    avg_loss:0.704, val_acc:0.821]
Epoch [18/120    avg_loss:0.648, val_acc:0.821]
Epoch [19/120    avg_loss:0.667, val_acc:0.831]
Epoch [20/120    avg_loss:0.590, val_acc:0.855]
Epoch [21/120    avg_loss:0.578, val_acc:0.861]
Epoch [22/120    avg_loss:0.493, val_acc:0.895]
Epoch [23/120    avg_loss:0.514, val_acc:0.899]
Epoch [24/120    avg_loss:0.514, val_acc:0.909]
Epoch [25/120    avg_loss:0.470, val_acc:0.885]
Epoch [26/120    avg_loss:0.425, val_acc:0.931]
Epoch [27/120    avg_loss:0.388, val_acc:0.933]
Epoch [28/120    avg_loss:0.342, val_acc:0.913]
Epoch [29/120    avg_loss:0.311, val_acc:0.938]
Epoch [30/120    avg_loss:0.338, val_acc:0.940]
Epoch [31/120    avg_loss:0.336, val_acc:0.935]
Epoch [32/120    avg_loss:0.300, val_acc:0.933]
Epoch [33/120    avg_loss:0.277, val_acc:0.938]
Epoch [34/120    avg_loss:0.269, val_acc:0.929]
Epoch [35/120    avg_loss:0.256, val_acc:0.935]
Epoch [36/120    avg_loss:0.229, val_acc:0.966]
Epoch [37/120    avg_loss:0.250, val_acc:0.907]
Epoch [38/120    avg_loss:0.274, val_acc:0.956]
Epoch [39/120    avg_loss:0.272, val_acc:0.931]
Epoch [40/120    avg_loss:0.254, val_acc:0.966]
Epoch [41/120    avg_loss:0.228, val_acc:0.948]
Epoch [42/120    avg_loss:0.280, val_acc:0.948]
Epoch [43/120    avg_loss:0.240, val_acc:0.968]
Epoch [44/120    avg_loss:0.180, val_acc:0.972]
Epoch [45/120    avg_loss:0.208, val_acc:0.923]
Epoch [46/120    avg_loss:0.217, val_acc:0.968]
Epoch [47/120    avg_loss:0.200, val_acc:0.958]
Epoch [48/120    avg_loss:0.210, val_acc:0.952]
Epoch [49/120    avg_loss:0.249, val_acc:0.972]
Epoch [50/120    avg_loss:0.164, val_acc:0.966]
Epoch [51/120    avg_loss:0.178, val_acc:0.974]
Epoch [52/120    avg_loss:0.131, val_acc:0.978]
Epoch [53/120    avg_loss:0.192, val_acc:0.952]
Epoch [54/120    avg_loss:0.179, val_acc:0.972]
Epoch [55/120    avg_loss:0.128, val_acc:0.966]
Epoch [56/120    avg_loss:0.147, val_acc:0.966]
Epoch [57/120    avg_loss:0.180, val_acc:0.952]
Epoch [58/120    avg_loss:0.177, val_acc:0.946]
Epoch [59/120    avg_loss:0.149, val_acc:0.962]
Epoch [60/120    avg_loss:0.137, val_acc:0.976]
Epoch [61/120    avg_loss:0.162, val_acc:0.972]
Epoch [62/120    avg_loss:0.165, val_acc:0.952]
Epoch [63/120    avg_loss:0.122, val_acc:0.978]
Epoch [64/120    avg_loss:0.126, val_acc:0.964]
Epoch [65/120    avg_loss:0.130, val_acc:0.984]
Epoch [66/120    avg_loss:0.104, val_acc:0.976]
Epoch [67/120    avg_loss:0.135, val_acc:0.974]
Epoch [68/120    avg_loss:0.157, val_acc:0.970]
Epoch [69/120    avg_loss:0.095, val_acc:0.968]
Epoch [70/120    avg_loss:0.166, val_acc:0.988]
Epoch [71/120    avg_loss:0.115, val_acc:0.974]
Epoch [72/120    avg_loss:0.124, val_acc:0.974]
Epoch [73/120    avg_loss:0.144, val_acc:0.964]
Epoch [74/120    avg_loss:0.100, val_acc:0.986]
Epoch [75/120    avg_loss:0.089, val_acc:0.978]
Epoch [76/120    avg_loss:0.109, val_acc:0.974]
Epoch [77/120    avg_loss:0.093, val_acc:0.970]
Epoch [78/120    avg_loss:0.099, val_acc:0.976]
Epoch [79/120    avg_loss:0.062, val_acc:0.974]
Epoch [80/120    avg_loss:0.066, val_acc:0.976]
Epoch [81/120    avg_loss:0.080, val_acc:0.968]
Epoch [82/120    avg_loss:0.086, val_acc:0.964]
Epoch [83/120    avg_loss:0.082, val_acc:0.974]
Epoch [84/120    avg_loss:0.062, val_acc:0.982]
Epoch [85/120    avg_loss:0.034, val_acc:0.982]
Epoch [86/120    avg_loss:0.040, val_acc:0.980]
Epoch [87/120    avg_loss:0.037, val_acc:0.980]
Epoch [88/120    avg_loss:0.036, val_acc:0.982]
Epoch [89/120    avg_loss:0.050, val_acc:0.982]
Epoch [90/120    avg_loss:0.054, val_acc:0.984]
Epoch [91/120    avg_loss:0.034, val_acc:0.984]
Epoch [92/120    avg_loss:0.037, val_acc:0.986]
Epoch [93/120    avg_loss:0.044, val_acc:0.984]
Epoch [94/120    avg_loss:0.033, val_acc:0.986]
Epoch [95/120    avg_loss:0.050, val_acc:0.988]
Epoch [96/120    avg_loss:0.034, val_acc:0.988]
Epoch [97/120    avg_loss:0.038, val_acc:0.988]
Epoch [98/120    avg_loss:0.040, val_acc:0.988]
Epoch [99/120    avg_loss:0.036, val_acc:0.988]
Epoch [100/120    avg_loss:0.032, val_acc:0.988]
Epoch [101/120    avg_loss:0.035, val_acc:0.988]
Epoch [102/120    avg_loss:0.035, val_acc:0.988]
Epoch [103/120    avg_loss:0.027, val_acc:0.988]
Epoch [104/120    avg_loss:0.030, val_acc:0.988]
Epoch [105/120    avg_loss:0.028, val_acc:0.988]
Epoch [106/120    avg_loss:0.035, val_acc:0.988]
Epoch [107/120    avg_loss:0.029, val_acc:0.988]
Epoch [108/120    avg_loss:0.029, val_acc:0.988]
Epoch [109/120    avg_loss:0.040, val_acc:0.988]
Epoch [110/120    avg_loss:0.034, val_acc:0.988]
Epoch [111/120    avg_loss:0.029, val_acc:0.988]
Epoch [112/120    avg_loss:0.033, val_acc:0.988]
Epoch [113/120    avg_loss:0.028, val_acc:0.988]
Epoch [114/120    avg_loss:0.045, val_acc:0.988]
Epoch [115/120    avg_loss:0.034, val_acc:0.986]
Epoch [116/120    avg_loss:0.028, val_acc:0.988]
Epoch [117/120    avg_loss:0.036, val_acc:0.988]
Epoch [118/120    avg_loss:0.033, val_acc:0.988]
Epoch [119/120    avg_loss:0.036, val_acc:0.988]
Epoch [120/120    avg_loss:0.026, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  12   0   0   0   0   0   0   2   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99264706 1.         1.         0.95089286 0.93197279
 0.97630332 1.         1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9924055869942843
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7ac10ca90>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.584, val_acc:0.302]
Epoch [2/120    avg_loss:2.431, val_acc:0.425]
Epoch [3/120    avg_loss:2.288, val_acc:0.502]
Epoch [4/120    avg_loss:2.156, val_acc:0.571]
Epoch [5/120    avg_loss:2.066, val_acc:0.588]
Epoch [6/120    avg_loss:1.940, val_acc:0.629]
Epoch [7/120    avg_loss:1.828, val_acc:0.642]
Epoch [8/120    avg_loss:1.691, val_acc:0.688]
Epoch [9/120    avg_loss:1.579, val_acc:0.688]
Epoch [10/120    avg_loss:1.459, val_acc:0.706]
Epoch [11/120    avg_loss:1.325, val_acc:0.738]
Epoch [12/120    avg_loss:1.214, val_acc:0.738]
Epoch [13/120    avg_loss:1.071, val_acc:0.779]
Epoch [14/120    avg_loss:0.976, val_acc:0.794]
Epoch [15/120    avg_loss:0.863, val_acc:0.777]
Epoch [16/120    avg_loss:0.757, val_acc:0.842]
Epoch [17/120    avg_loss:0.722, val_acc:0.873]
Epoch [18/120    avg_loss:0.711, val_acc:0.848]
Epoch [19/120    avg_loss:0.607, val_acc:0.817]
Epoch [20/120    avg_loss:0.587, val_acc:0.944]
Epoch [21/120    avg_loss:0.525, val_acc:0.910]
Epoch [22/120    avg_loss:0.465, val_acc:0.917]
Epoch [23/120    avg_loss:0.519, val_acc:0.929]
Epoch [24/120    avg_loss:0.430, val_acc:0.923]
Epoch [25/120    avg_loss:0.466, val_acc:0.904]
Epoch [26/120    avg_loss:0.393, val_acc:0.940]
Epoch [27/120    avg_loss:0.377, val_acc:0.885]
Epoch [28/120    avg_loss:0.490, val_acc:0.883]
Epoch [29/120    avg_loss:0.405, val_acc:0.923]
Epoch [30/120    avg_loss:0.344, val_acc:0.946]
Epoch [31/120    avg_loss:0.298, val_acc:0.950]
Epoch [32/120    avg_loss:0.278, val_acc:0.954]
Epoch [33/120    avg_loss:0.271, val_acc:0.971]
Epoch [34/120    avg_loss:0.295, val_acc:0.946]
Epoch [35/120    avg_loss:0.220, val_acc:0.954]
Epoch [36/120    avg_loss:0.262, val_acc:0.948]
Epoch [37/120    avg_loss:0.241, val_acc:0.958]
Epoch [38/120    avg_loss:0.270, val_acc:0.969]
Epoch [39/120    avg_loss:0.182, val_acc:0.946]
Epoch [40/120    avg_loss:0.205, val_acc:0.942]
Epoch [41/120    avg_loss:0.245, val_acc:0.960]
Epoch [42/120    avg_loss:0.174, val_acc:0.948]
Epoch [43/120    avg_loss:0.195, val_acc:0.954]
Epoch [44/120    avg_loss:0.167, val_acc:0.960]
Epoch [45/120    avg_loss:0.157, val_acc:0.942]
Epoch [46/120    avg_loss:0.173, val_acc:0.973]
Epoch [47/120    avg_loss:0.167, val_acc:0.950]
Epoch [48/120    avg_loss:0.164, val_acc:0.952]
Epoch [49/120    avg_loss:0.171, val_acc:0.960]
Epoch [50/120    avg_loss:0.139, val_acc:0.971]
Epoch [51/120    avg_loss:0.133, val_acc:0.963]
Epoch [52/120    avg_loss:0.139, val_acc:0.979]
Epoch [53/120    avg_loss:0.142, val_acc:0.977]
Epoch [54/120    avg_loss:0.125, val_acc:0.967]
Epoch [55/120    avg_loss:0.126, val_acc:0.958]
Epoch [56/120    avg_loss:0.104, val_acc:0.963]
Epoch [57/120    avg_loss:0.153, val_acc:0.952]
Epoch [58/120    avg_loss:0.201, val_acc:0.975]
Epoch [59/120    avg_loss:0.196, val_acc:0.942]
Epoch [60/120    avg_loss:0.171, val_acc:0.956]
Epoch [61/120    avg_loss:0.144, val_acc:0.956]
Epoch [62/120    avg_loss:0.167, val_acc:0.979]
Epoch [63/120    avg_loss:0.122, val_acc:0.954]
Epoch [64/120    avg_loss:0.127, val_acc:0.952]
Epoch [65/120    avg_loss:0.101, val_acc:0.981]
Epoch [66/120    avg_loss:0.095, val_acc:0.950]
Epoch [67/120    avg_loss:0.147, val_acc:0.975]
Epoch [68/120    avg_loss:0.081, val_acc:0.988]
Epoch [69/120    avg_loss:0.093, val_acc:0.969]
Epoch [70/120    avg_loss:0.080, val_acc:0.985]
Epoch [71/120    avg_loss:0.047, val_acc:0.988]
Epoch [72/120    avg_loss:0.057, val_acc:0.975]
Epoch [73/120    avg_loss:0.057, val_acc:0.983]
Epoch [74/120    avg_loss:0.052, val_acc:0.985]
Epoch [75/120    avg_loss:0.065, val_acc:0.981]
Epoch [76/120    avg_loss:0.043, val_acc:0.988]
Epoch [77/120    avg_loss:0.046, val_acc:0.979]
Epoch [78/120    avg_loss:0.047, val_acc:0.977]
Epoch [79/120    avg_loss:0.112, val_acc:0.994]
Epoch [80/120    avg_loss:0.050, val_acc:0.988]
Epoch [81/120    avg_loss:0.053, val_acc:0.979]
Epoch [82/120    avg_loss:0.042, val_acc:0.990]
Epoch [83/120    avg_loss:0.045, val_acc:0.977]
Epoch [84/120    avg_loss:0.034, val_acc:0.988]
Epoch [85/120    avg_loss:0.020, val_acc:0.988]
Epoch [86/120    avg_loss:0.030, val_acc:0.990]
Epoch [87/120    avg_loss:0.036, val_acc:0.994]
Epoch [88/120    avg_loss:0.028, val_acc:0.981]
Epoch [89/120    avg_loss:0.027, val_acc:0.990]
Epoch [90/120    avg_loss:0.029, val_acc:0.994]
Epoch [91/120    avg_loss:0.030, val_acc:0.983]
Epoch [92/120    avg_loss:0.123, val_acc:0.981]
Epoch [93/120    avg_loss:0.069, val_acc:0.973]
Epoch [94/120    avg_loss:0.093, val_acc:0.981]
Epoch [95/120    avg_loss:0.115, val_acc:0.975]
Epoch [96/120    avg_loss:0.045, val_acc:0.994]
Epoch [97/120    avg_loss:0.035, val_acc:0.983]
Epoch [98/120    avg_loss:0.048, val_acc:0.988]
Epoch [99/120    avg_loss:0.029, val_acc:0.994]
Epoch [100/120    avg_loss:0.037, val_acc:0.981]
Epoch [101/120    avg_loss:0.047, val_acc:0.981]
Epoch [102/120    avg_loss:0.039, val_acc:0.990]
Epoch [103/120    avg_loss:0.039, val_acc:0.988]
Epoch [104/120    avg_loss:0.044, val_acc:0.985]
Epoch [105/120    avg_loss:0.024, val_acc:0.992]
Epoch [106/120    avg_loss:0.018, val_acc:0.990]
Epoch [107/120    avg_loss:0.013, val_acc:0.990]
Epoch [108/120    avg_loss:0.012, val_acc:0.990]
Epoch [109/120    avg_loss:0.015, val_acc:0.992]
Epoch [110/120    avg_loss:0.013, val_acc:0.992]
Epoch [111/120    avg_loss:0.014, val_acc:0.992]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.014, val_acc:0.992]
Epoch [114/120    avg_loss:0.009, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.008, val_acc:0.992]
Epoch [117/120    avg_loss:0.011, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.992]
Epoch [119/120    avg_loss:0.011, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99926954 0.99545455 0.99782135 0.96137339 0.93571429
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9950148357859373
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6ff878ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.553, val_acc:0.373]
Epoch [2/120    avg_loss:2.416, val_acc:0.321]
Epoch [3/120    avg_loss:2.295, val_acc:0.423]
Epoch [4/120    avg_loss:2.190, val_acc:0.448]
Epoch [5/120    avg_loss:2.098, val_acc:0.485]
Epoch [6/120    avg_loss:1.984, val_acc:0.531]
Epoch [7/120    avg_loss:1.885, val_acc:0.544]
Epoch [8/120    avg_loss:1.779, val_acc:0.573]
Epoch [9/120    avg_loss:1.696, val_acc:0.621]
Epoch [10/120    avg_loss:1.584, val_acc:0.646]
Epoch [11/120    avg_loss:1.485, val_acc:0.683]
Epoch [12/120    avg_loss:1.392, val_acc:0.717]
Epoch [13/120    avg_loss:1.246, val_acc:0.733]
Epoch [14/120    avg_loss:1.120, val_acc:0.731]
Epoch [15/120    avg_loss:1.040, val_acc:0.767]
Epoch [16/120    avg_loss:0.918, val_acc:0.794]
Epoch [17/120    avg_loss:0.883, val_acc:0.867]
Epoch [18/120    avg_loss:0.802, val_acc:0.762]
Epoch [19/120    avg_loss:0.746, val_acc:0.831]
Epoch [20/120    avg_loss:0.674, val_acc:0.867]
Epoch [21/120    avg_loss:0.644, val_acc:0.877]
Epoch [22/120    avg_loss:0.588, val_acc:0.900]
Epoch [23/120    avg_loss:0.515, val_acc:0.896]
Epoch [24/120    avg_loss:0.522, val_acc:0.902]
Epoch [25/120    avg_loss:0.466, val_acc:0.881]
Epoch [26/120    avg_loss:0.516, val_acc:0.940]
Epoch [27/120    avg_loss:0.399, val_acc:0.938]
Epoch [28/120    avg_loss:0.401, val_acc:0.906]
Epoch [29/120    avg_loss:0.353, val_acc:0.908]
Epoch [30/120    avg_loss:0.363, val_acc:0.917]
Epoch [31/120    avg_loss:0.331, val_acc:0.923]
Epoch [32/120    avg_loss:0.403, val_acc:0.919]
Epoch [33/120    avg_loss:0.337, val_acc:0.915]
Epoch [34/120    avg_loss:0.306, val_acc:0.929]
Epoch [35/120    avg_loss:0.293, val_acc:0.948]
Epoch [36/120    avg_loss:0.261, val_acc:0.931]
Epoch [37/120    avg_loss:0.238, val_acc:0.929]
Epoch [38/120    avg_loss:0.278, val_acc:0.952]
Epoch [39/120    avg_loss:0.262, val_acc:0.931]
Epoch [40/120    avg_loss:0.253, val_acc:0.963]
Epoch [41/120    avg_loss:0.224, val_acc:0.965]
Epoch [42/120    avg_loss:0.228, val_acc:0.952]
Epoch [43/120    avg_loss:0.204, val_acc:0.973]
Epoch [44/120    avg_loss:0.207, val_acc:0.933]
Epoch [45/120    avg_loss:0.154, val_acc:0.981]
Epoch [46/120    avg_loss:0.167, val_acc:0.963]
Epoch [47/120    avg_loss:0.191, val_acc:0.971]
Epoch [48/120    avg_loss:0.166, val_acc:0.954]
Epoch [49/120    avg_loss:0.151, val_acc:0.977]
Epoch [50/120    avg_loss:0.164, val_acc:0.975]
Epoch [51/120    avg_loss:0.176, val_acc:0.971]
Epoch [52/120    avg_loss:0.163, val_acc:0.981]
Epoch [53/120    avg_loss:0.150, val_acc:0.963]
Epoch [54/120    avg_loss:0.151, val_acc:0.969]
Epoch [55/120    avg_loss:0.103, val_acc:0.969]
Epoch [56/120    avg_loss:0.121, val_acc:0.988]
Epoch [57/120    avg_loss:0.133, val_acc:0.990]
Epoch [58/120    avg_loss:0.127, val_acc:0.979]
Epoch [59/120    avg_loss:0.139, val_acc:0.994]
Epoch [60/120    avg_loss:0.083, val_acc:0.988]
Epoch [61/120    avg_loss:0.089, val_acc:0.988]
Epoch [62/120    avg_loss:0.089, val_acc:0.975]
Epoch [63/120    avg_loss:0.119, val_acc:0.985]
Epoch [64/120    avg_loss:0.097, val_acc:0.979]
Epoch [65/120    avg_loss:0.089, val_acc:0.983]
Epoch [66/120    avg_loss:0.098, val_acc:0.990]
Epoch [67/120    avg_loss:0.068, val_acc:0.988]
Epoch [68/120    avg_loss:0.097, val_acc:0.983]
Epoch [69/120    avg_loss:0.086, val_acc:0.988]
Epoch [70/120    avg_loss:0.061, val_acc:0.990]
Epoch [71/120    avg_loss:0.145, val_acc:0.990]
Epoch [72/120    avg_loss:0.073, val_acc:0.988]
Epoch [73/120    avg_loss:0.058, val_acc:0.992]
Epoch [74/120    avg_loss:0.046, val_acc:0.992]
Epoch [75/120    avg_loss:0.042, val_acc:0.990]
Epoch [76/120    avg_loss:0.043, val_acc:0.992]
Epoch [77/120    avg_loss:0.046, val_acc:0.990]
Epoch [78/120    avg_loss:0.041, val_acc:0.990]
Epoch [79/120    avg_loss:0.048, val_acc:0.992]
Epoch [80/120    avg_loss:0.048, val_acc:0.990]
Epoch [81/120    avg_loss:0.030, val_acc:0.992]
Epoch [82/120    avg_loss:0.050, val_acc:0.992]
Epoch [83/120    avg_loss:0.034, val_acc:0.992]
Epoch [84/120    avg_loss:0.035, val_acc:0.990]
Epoch [85/120    avg_loss:0.043, val_acc:0.990]
Epoch [86/120    avg_loss:0.036, val_acc:0.990]
Epoch [87/120    avg_loss:0.043, val_acc:0.990]
Epoch [88/120    avg_loss:0.039, val_acc:0.990]
Epoch [89/120    avg_loss:0.041, val_acc:0.990]
Epoch [90/120    avg_loss:0.037, val_acc:0.990]
Epoch [91/120    avg_loss:0.041, val_acc:0.990]
Epoch [92/120    avg_loss:0.037, val_acc:0.990]
Epoch [93/120    avg_loss:0.035, val_acc:0.990]
Epoch [94/120    avg_loss:0.037, val_acc:0.990]
Epoch [95/120    avg_loss:0.031, val_acc:0.990]
Epoch [96/120    avg_loss:0.038, val_acc:0.990]
Epoch [97/120    avg_loss:0.041, val_acc:0.990]
Epoch [98/120    avg_loss:0.038, val_acc:0.990]
Epoch [99/120    avg_loss:0.036, val_acc:0.990]
Epoch [100/120    avg_loss:0.032, val_acc:0.990]
Epoch [101/120    avg_loss:0.035, val_acc:0.990]
Epoch [102/120    avg_loss:0.032, val_acc:0.990]
Epoch [103/120    avg_loss:0.039, val_acc:0.990]
Epoch [104/120    avg_loss:0.041, val_acc:0.990]
Epoch [105/120    avg_loss:0.035, val_acc:0.990]
Epoch [106/120    avg_loss:0.032, val_acc:0.990]
Epoch [107/120    avg_loss:0.032, val_acc:0.990]
Epoch [108/120    avg_loss:0.032, val_acc:0.990]
Epoch [109/120    avg_loss:0.032, val_acc:0.990]
Epoch [110/120    avg_loss:0.034, val_acc:0.990]
Epoch [111/120    avg_loss:0.045, val_acc:0.990]
Epoch [112/120    avg_loss:0.033, val_acc:0.990]
Epoch [113/120    avg_loss:0.029, val_acc:0.990]
Epoch [114/120    avg_loss:0.038, val_acc:0.990]
Epoch [115/120    avg_loss:0.035, val_acc:0.990]
Epoch [116/120    avg_loss:0.051, val_acc:0.990]
Epoch [117/120    avg_loss:0.037, val_acc:0.990]
Epoch [118/120    avg_loss:0.034, val_acc:0.990]
Epoch [119/120    avg_loss:0.027, val_acc:0.990]
Epoch [120/120    avg_loss:0.039, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 0.996337   0.99545455 1.         0.97797357 0.96551724
 0.98800959 0.98924731 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9957275812264392
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5bf8372b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.325]
Epoch [2/120    avg_loss:2.476, val_acc:0.319]
Epoch [3/120    avg_loss:2.357, val_acc:0.438]
Epoch [4/120    avg_loss:2.239, val_acc:0.521]
Epoch [5/120    avg_loss:2.132, val_acc:0.594]
Epoch [6/120    avg_loss:2.018, val_acc:0.627]
Epoch [7/120    avg_loss:1.892, val_acc:0.658]
Epoch [8/120    avg_loss:1.746, val_acc:0.667]
Epoch [9/120    avg_loss:1.645, val_acc:0.683]
Epoch [10/120    avg_loss:1.511, val_acc:0.698]
Epoch [11/120    avg_loss:1.391, val_acc:0.733]
Epoch [12/120    avg_loss:1.214, val_acc:0.758]
Epoch [13/120    avg_loss:1.093, val_acc:0.783]
Epoch [14/120    avg_loss:1.011, val_acc:0.754]
Epoch [15/120    avg_loss:0.921, val_acc:0.802]
Epoch [16/120    avg_loss:0.816, val_acc:0.858]
Epoch [17/120    avg_loss:0.741, val_acc:0.804]
Epoch [18/120    avg_loss:0.711, val_acc:0.838]
Epoch [19/120    avg_loss:0.643, val_acc:0.896]
Epoch [20/120    avg_loss:0.566, val_acc:0.900]
Epoch [21/120    avg_loss:0.540, val_acc:0.827]
Epoch [22/120    avg_loss:0.554, val_acc:0.873]
Epoch [23/120    avg_loss:0.483, val_acc:0.923]
Epoch [24/120    avg_loss:0.475, val_acc:0.904]
Epoch [25/120    avg_loss:0.447, val_acc:0.921]
Epoch [26/120    avg_loss:0.465, val_acc:0.812]
Epoch [27/120    avg_loss:0.495, val_acc:0.885]
Epoch [28/120    avg_loss:0.412, val_acc:0.900]
Epoch [29/120    avg_loss:0.437, val_acc:0.921]
Epoch [30/120    avg_loss:0.354, val_acc:0.927]
Epoch [31/120    avg_loss:0.320, val_acc:0.931]
Epoch [32/120    avg_loss:0.384, val_acc:0.938]
Epoch [33/120    avg_loss:0.298, val_acc:0.933]
Epoch [34/120    avg_loss:0.347, val_acc:0.950]
Epoch [35/120    avg_loss:0.288, val_acc:0.948]
Epoch [36/120    avg_loss:0.278, val_acc:0.969]
Epoch [37/120    avg_loss:0.341, val_acc:0.944]
Epoch [38/120    avg_loss:0.304, val_acc:0.923]
Epoch [39/120    avg_loss:0.251, val_acc:0.910]
Epoch [40/120    avg_loss:0.306, val_acc:0.906]
Epoch [41/120    avg_loss:0.313, val_acc:0.935]
Epoch [42/120    avg_loss:0.309, val_acc:0.952]
Epoch [43/120    avg_loss:0.278, val_acc:0.960]
Epoch [44/120    avg_loss:0.217, val_acc:0.950]
Epoch [45/120    avg_loss:0.233, val_acc:0.965]
Epoch [46/120    avg_loss:0.212, val_acc:0.967]
Epoch [47/120    avg_loss:0.249, val_acc:0.963]
Epoch [48/120    avg_loss:0.249, val_acc:0.971]
Epoch [49/120    avg_loss:0.174, val_acc:0.967]
Epoch [50/120    avg_loss:0.243, val_acc:0.956]
Epoch [51/120    avg_loss:0.199, val_acc:0.975]
Epoch [52/120    avg_loss:0.128, val_acc:0.977]
Epoch [53/120    avg_loss:0.170, val_acc:0.963]
Epoch [54/120    avg_loss:0.208, val_acc:0.967]
Epoch [55/120    avg_loss:0.222, val_acc:0.963]
Epoch [56/120    avg_loss:0.175, val_acc:0.973]
Epoch [57/120    avg_loss:0.152, val_acc:0.977]
Epoch [58/120    avg_loss:0.260, val_acc:0.956]
Epoch [59/120    avg_loss:0.229, val_acc:0.971]
Epoch [60/120    avg_loss:0.184, val_acc:0.963]
Epoch [61/120    avg_loss:0.127, val_acc:0.969]
Epoch [62/120    avg_loss:0.166, val_acc:0.971]
Epoch [63/120    avg_loss:0.148, val_acc:0.971]
Epoch [64/120    avg_loss:0.124, val_acc:0.971]
Epoch [65/120    avg_loss:0.101, val_acc:0.975]
Epoch [66/120    avg_loss:0.116, val_acc:0.979]
Epoch [67/120    avg_loss:0.110, val_acc:0.965]
Epoch [68/120    avg_loss:0.109, val_acc:0.958]
Epoch [69/120    avg_loss:0.106, val_acc:0.975]
Epoch [70/120    avg_loss:0.129, val_acc:0.975]
Epoch [71/120    avg_loss:0.074, val_acc:0.956]
Epoch [72/120    avg_loss:0.114, val_acc:0.975]
Epoch [73/120    avg_loss:0.084, val_acc:0.990]
Epoch [74/120    avg_loss:0.086, val_acc:0.969]
Epoch [75/120    avg_loss:0.095, val_acc:0.988]
Epoch [76/120    avg_loss:0.077, val_acc:0.983]
Epoch [77/120    avg_loss:0.075, val_acc:0.988]
Epoch [78/120    avg_loss:0.065, val_acc:0.988]
Epoch [79/120    avg_loss:0.048, val_acc:0.977]
Epoch [80/120    avg_loss:0.110, val_acc:0.988]
Epoch [81/120    avg_loss:0.068, val_acc:0.990]
Epoch [82/120    avg_loss:0.069, val_acc:0.981]
Epoch [83/120    avg_loss:0.056, val_acc:0.985]
Epoch [84/120    avg_loss:0.075, val_acc:0.981]
Epoch [85/120    avg_loss:0.072, val_acc:0.979]
Epoch [86/120    avg_loss:0.057, val_acc:0.990]
Epoch [87/120    avg_loss:0.048, val_acc:0.988]
Epoch [88/120    avg_loss:0.073, val_acc:0.975]
Epoch [89/120    avg_loss:0.087, val_acc:0.981]
Epoch [90/120    avg_loss:0.043, val_acc:0.988]
Epoch [91/120    avg_loss:0.043, val_acc:0.990]
Epoch [92/120    avg_loss:0.048, val_acc:0.969]
Epoch [93/120    avg_loss:0.056, val_acc:0.988]
Epoch [94/120    avg_loss:0.078, val_acc:0.973]
Epoch [95/120    avg_loss:0.041, val_acc:0.990]
Epoch [96/120    avg_loss:0.040, val_acc:0.981]
Epoch [97/120    avg_loss:0.021, val_acc:0.990]
Epoch [98/120    avg_loss:0.027, val_acc:0.992]
Epoch [99/120    avg_loss:0.044, val_acc:0.985]
Epoch [100/120    avg_loss:0.035, val_acc:0.985]
Epoch [101/120    avg_loss:0.023, val_acc:0.988]
Epoch [102/120    avg_loss:0.028, val_acc:0.988]
Epoch [103/120    avg_loss:0.021, val_acc:0.992]
Epoch [104/120    avg_loss:0.020, val_acc:0.990]
Epoch [105/120    avg_loss:0.032, val_acc:0.990]
Epoch [106/120    avg_loss:0.036, val_acc:0.994]
Epoch [107/120    avg_loss:0.043, val_acc:0.981]
Epoch [108/120    avg_loss:0.032, val_acc:0.990]
Epoch [109/120    avg_loss:0.035, val_acc:0.969]
Epoch [110/120    avg_loss:0.054, val_acc:0.988]
Epoch [111/120    avg_loss:0.035, val_acc:0.944]
Epoch [112/120    avg_loss:0.110, val_acc:0.956]
Epoch [113/120    avg_loss:0.073, val_acc:0.983]
Epoch [114/120    avg_loss:0.077, val_acc:0.973]
Epoch [115/120    avg_loss:0.072, val_acc:0.981]
Epoch [116/120    avg_loss:0.106, val_acc:0.975]
Epoch [117/120    avg_loss:0.102, val_acc:0.990]
Epoch [118/120    avg_loss:0.060, val_acc:0.973]
Epoch [119/120    avg_loss:0.060, val_acc:0.981]
Epoch [120/120    avg_loss:0.036, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 0.996337   0.98648649 0.99122807 0.94039735 0.9220339
 0.98800959 0.9726776  0.99870968 1.         1.         1.
 1.         1.        ]

Kappa:
0.9909805825434342
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40773eba20>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.589, val_acc:0.147]
Epoch [2/120    avg_loss:2.450, val_acc:0.442]
Epoch [3/120    avg_loss:2.300, val_acc:0.516]
Epoch [4/120    avg_loss:2.158, val_acc:0.560]
Epoch [5/120    avg_loss:2.021, val_acc:0.585]
Epoch [6/120    avg_loss:1.861, val_acc:0.609]
Epoch [7/120    avg_loss:1.734, val_acc:0.663]
Epoch [8/120    avg_loss:1.584, val_acc:0.657]
Epoch [9/120    avg_loss:1.406, val_acc:0.722]
Epoch [10/120    avg_loss:1.279, val_acc:0.692]
Epoch [11/120    avg_loss:1.188, val_acc:0.732]
Epoch [12/120    avg_loss:1.043, val_acc:0.754]
Epoch [13/120    avg_loss:0.990, val_acc:0.754]
Epoch [14/120    avg_loss:0.888, val_acc:0.778]
Epoch [15/120    avg_loss:0.809, val_acc:0.815]
Epoch [16/120    avg_loss:0.715, val_acc:0.867]
Epoch [17/120    avg_loss:0.690, val_acc:0.879]
Epoch [18/120    avg_loss:0.642, val_acc:0.893]
Epoch [19/120    avg_loss:0.544, val_acc:0.913]
Epoch [20/120    avg_loss:0.522, val_acc:0.909]
Epoch [21/120    avg_loss:0.452, val_acc:0.895]
Epoch [22/120    avg_loss:0.447, val_acc:0.905]
Epoch [23/120    avg_loss:0.469, val_acc:0.913]
Epoch [24/120    avg_loss:0.438, val_acc:0.929]
Epoch [25/120    avg_loss:0.383, val_acc:0.933]
Epoch [26/120    avg_loss:0.514, val_acc:0.889]
Epoch [27/120    avg_loss:0.426, val_acc:0.897]
Epoch [28/120    avg_loss:0.416, val_acc:0.897]
Epoch [29/120    avg_loss:0.355, val_acc:0.857]
Epoch [30/120    avg_loss:0.305, val_acc:0.915]
Epoch [31/120    avg_loss:0.311, val_acc:0.885]
Epoch [32/120    avg_loss:0.307, val_acc:0.925]
Epoch [33/120    avg_loss:0.265, val_acc:0.905]
Epoch [34/120    avg_loss:0.285, val_acc:0.915]
Epoch [35/120    avg_loss:0.281, val_acc:0.946]
Epoch [36/120    avg_loss:0.223, val_acc:0.962]
Epoch [37/120    avg_loss:0.209, val_acc:0.956]
Epoch [38/120    avg_loss:0.225, val_acc:0.933]
Epoch [39/120    avg_loss:0.195, val_acc:0.948]
Epoch [40/120    avg_loss:0.185, val_acc:0.950]
Epoch [41/120    avg_loss:0.166, val_acc:0.962]
Epoch [42/120    avg_loss:0.167, val_acc:0.952]
Epoch [43/120    avg_loss:0.174, val_acc:0.942]
Epoch [44/120    avg_loss:0.178, val_acc:0.935]
Epoch [45/120    avg_loss:0.156, val_acc:0.942]
Epoch [46/120    avg_loss:0.177, val_acc:0.970]
Epoch [47/120    avg_loss:0.156, val_acc:0.952]
Epoch [48/120    avg_loss:0.131, val_acc:0.972]
Epoch [49/120    avg_loss:0.135, val_acc:0.954]
Epoch [50/120    avg_loss:0.133, val_acc:0.966]
Epoch [51/120    avg_loss:0.150, val_acc:0.952]
Epoch [52/120    avg_loss:0.138, val_acc:0.970]
Epoch [53/120    avg_loss:0.131, val_acc:0.956]
Epoch [54/120    avg_loss:0.116, val_acc:0.966]
Epoch [55/120    avg_loss:0.129, val_acc:0.982]
Epoch [56/120    avg_loss:0.117, val_acc:0.972]
Epoch [57/120    avg_loss:0.099, val_acc:0.976]
Epoch [58/120    avg_loss:0.092, val_acc:0.984]
Epoch [59/120    avg_loss:0.088, val_acc:0.972]
Epoch [60/120    avg_loss:0.087, val_acc:0.982]
Epoch [61/120    avg_loss:0.118, val_acc:0.974]
Epoch [62/120    avg_loss:0.106, val_acc:0.976]
Epoch [63/120    avg_loss:0.084, val_acc:0.986]
Epoch [64/120    avg_loss:0.069, val_acc:0.982]
Epoch [65/120    avg_loss:0.070, val_acc:0.984]
Epoch [66/120    avg_loss:0.088, val_acc:0.978]
Epoch [67/120    avg_loss:0.096, val_acc:0.956]
Epoch [68/120    avg_loss:0.059, val_acc:0.984]
Epoch [69/120    avg_loss:0.062, val_acc:0.984]
Epoch [70/120    avg_loss:0.051, val_acc:0.990]
Epoch [71/120    avg_loss:0.066, val_acc:0.974]
Epoch [72/120    avg_loss:0.054, val_acc:0.984]
Epoch [73/120    avg_loss:0.069, val_acc:0.978]
Epoch [74/120    avg_loss:0.058, val_acc:0.984]
Epoch [75/120    avg_loss:0.050, val_acc:0.978]
Epoch [76/120    avg_loss:0.046, val_acc:0.972]
Epoch [77/120    avg_loss:0.082, val_acc:0.986]
Epoch [78/120    avg_loss:0.068, val_acc:0.978]
Epoch [79/120    avg_loss:0.055, val_acc:0.988]
Epoch [80/120    avg_loss:0.111, val_acc:0.978]
Epoch [81/120    avg_loss:0.070, val_acc:0.966]
Epoch [82/120    avg_loss:0.057, val_acc:0.978]
Epoch [83/120    avg_loss:0.055, val_acc:0.986]
Epoch [84/120    avg_loss:0.032, val_acc:0.988]
Epoch [85/120    avg_loss:0.031, val_acc:0.988]
Epoch [86/120    avg_loss:0.031, val_acc:0.990]
Epoch [87/120    avg_loss:0.029, val_acc:0.992]
Epoch [88/120    avg_loss:0.028, val_acc:0.990]
Epoch [89/120    avg_loss:0.032, val_acc:0.992]
Epoch [90/120    avg_loss:0.024, val_acc:0.990]
Epoch [91/120    avg_loss:0.028, val_acc:0.988]
Epoch [92/120    avg_loss:0.025, val_acc:0.988]
Epoch [93/120    avg_loss:0.031, val_acc:0.988]
Epoch [94/120    avg_loss:0.026, val_acc:0.988]
Epoch [95/120    avg_loss:0.027, val_acc:0.988]
Epoch [96/120    avg_loss:0.034, val_acc:0.992]
Epoch [97/120    avg_loss:0.026, val_acc:0.990]
Epoch [98/120    avg_loss:0.025, val_acc:0.990]
Epoch [99/120    avg_loss:0.030, val_acc:0.990]
Epoch [100/120    avg_loss:0.025, val_acc:0.990]
Epoch [101/120    avg_loss:0.023, val_acc:0.990]
Epoch [102/120    avg_loss:0.026, val_acc:0.990]
Epoch [103/120    avg_loss:0.025, val_acc:0.988]
Epoch [104/120    avg_loss:0.025, val_acc:0.988]
Epoch [105/120    avg_loss:0.023, val_acc:0.988]
Epoch [106/120    avg_loss:0.023, val_acc:0.988]
Epoch [107/120    avg_loss:0.022, val_acc:0.990]
Epoch [108/120    avg_loss:0.022, val_acc:0.990]
Epoch [109/120    avg_loss:0.022, val_acc:0.988]
Epoch [110/120    avg_loss:0.030, val_acc:0.988]
Epoch [111/120    avg_loss:0.020, val_acc:0.988]
Epoch [112/120    avg_loss:0.027, val_acc:0.988]
Epoch [113/120    avg_loss:0.023, val_acc:0.988]
Epoch [114/120    avg_loss:0.023, val_acc:0.988]
Epoch [115/120    avg_loss:0.023, val_acc:0.988]
Epoch [116/120    avg_loss:0.020, val_acc:0.988]
Epoch [117/120    avg_loss:0.023, val_acc:0.988]
Epoch [118/120    avg_loss:0.025, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.026, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99560117 0.98871332 1.         0.96103896 0.93617021
 0.98564593 0.97826087 0.99870968 1.         1.         1.
 1.         1.        ]

Kappa:
0.9931165952405667
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbeee28fcf8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.319]
Epoch [2/120    avg_loss:2.455, val_acc:0.365]
Epoch [3/120    avg_loss:2.333, val_acc:0.468]
Epoch [4/120    avg_loss:2.241, val_acc:0.544]
Epoch [5/120    avg_loss:2.116, val_acc:0.565]
Epoch [6/120    avg_loss:2.002, val_acc:0.613]
Epoch [7/120    avg_loss:1.851, val_acc:0.661]
Epoch [8/120    avg_loss:1.716, val_acc:0.673]
Epoch [9/120    avg_loss:1.565, val_acc:0.661]
Epoch [10/120    avg_loss:1.408, val_acc:0.690]
Epoch [11/120    avg_loss:1.263, val_acc:0.736]
Epoch [12/120    avg_loss:1.163, val_acc:0.718]
Epoch [13/120    avg_loss:1.030, val_acc:0.736]
Epoch [14/120    avg_loss:0.923, val_acc:0.750]
Epoch [15/120    avg_loss:0.840, val_acc:0.756]
Epoch [16/120    avg_loss:0.854, val_acc:0.770]
Epoch [17/120    avg_loss:0.733, val_acc:0.804]
Epoch [18/120    avg_loss:0.652, val_acc:0.869]
Epoch [19/120    avg_loss:0.637, val_acc:0.837]
Epoch [20/120    avg_loss:0.591, val_acc:0.853]
Epoch [21/120    avg_loss:0.564, val_acc:0.921]
Epoch [22/120    avg_loss:0.509, val_acc:0.865]
Epoch [23/120    avg_loss:0.515, val_acc:0.883]
Epoch [24/120    avg_loss:0.437, val_acc:0.907]
Epoch [25/120    avg_loss:0.422, val_acc:0.893]
Epoch [26/120    avg_loss:0.526, val_acc:0.915]
Epoch [27/120    avg_loss:0.438, val_acc:0.883]
Epoch [28/120    avg_loss:0.349, val_acc:0.923]
Epoch [29/120    avg_loss:0.358, val_acc:0.909]
Epoch [30/120    avg_loss:0.418, val_acc:0.907]
Epoch [31/120    avg_loss:0.382, val_acc:0.954]
Epoch [32/120    avg_loss:0.276, val_acc:0.948]
Epoch [33/120    avg_loss:0.265, val_acc:0.940]
Epoch [34/120    avg_loss:0.263, val_acc:0.923]
Epoch [35/120    avg_loss:0.297, val_acc:0.899]
Epoch [36/120    avg_loss:0.250, val_acc:0.929]
Epoch [37/120    avg_loss:0.376, val_acc:0.899]
Epoch [38/120    avg_loss:0.282, val_acc:0.942]
Epoch [39/120    avg_loss:0.256, val_acc:0.946]
Epoch [40/120    avg_loss:0.225, val_acc:0.974]
Epoch [41/120    avg_loss:0.192, val_acc:0.968]
Epoch [42/120    avg_loss:0.196, val_acc:0.970]
Epoch [43/120    avg_loss:0.171, val_acc:0.972]
Epoch [44/120    avg_loss:0.156, val_acc:0.984]
Epoch [45/120    avg_loss:0.139, val_acc:0.974]
Epoch [46/120    avg_loss:0.178, val_acc:0.972]
Epoch [47/120    avg_loss:0.204, val_acc:0.978]
Epoch [48/120    avg_loss:0.183, val_acc:0.972]
Epoch [49/120    avg_loss:0.182, val_acc:0.972]
Epoch [50/120    avg_loss:0.179, val_acc:0.923]
Epoch [51/120    avg_loss:0.166, val_acc:0.974]
Epoch [52/120    avg_loss:0.238, val_acc:0.958]
Epoch [53/120    avg_loss:0.170, val_acc:0.982]
Epoch [54/120    avg_loss:0.174, val_acc:0.978]
Epoch [55/120    avg_loss:0.162, val_acc:0.968]
Epoch [56/120    avg_loss:0.126, val_acc:0.986]
Epoch [57/120    avg_loss:0.134, val_acc:0.974]
Epoch [58/120    avg_loss:0.182, val_acc:0.976]
Epoch [59/120    avg_loss:0.146, val_acc:0.980]
Epoch [60/120    avg_loss:0.132, val_acc:0.978]
Epoch [61/120    avg_loss:0.115, val_acc:0.980]
Epoch [62/120    avg_loss:0.101, val_acc:0.982]
Epoch [63/120    avg_loss:0.106, val_acc:0.881]
Epoch [64/120    avg_loss:0.153, val_acc:0.984]
Epoch [65/120    avg_loss:0.080, val_acc:0.976]
Epoch [66/120    avg_loss:0.084, val_acc:0.968]
Epoch [67/120    avg_loss:0.156, val_acc:0.978]
Epoch [68/120    avg_loss:0.109, val_acc:0.982]
Epoch [69/120    avg_loss:0.125, val_acc:0.984]
Epoch [70/120    avg_loss:0.067, val_acc:0.986]
Epoch [71/120    avg_loss:0.069, val_acc:0.986]
Epoch [72/120    avg_loss:0.055, val_acc:0.988]
Epoch [73/120    avg_loss:0.068, val_acc:0.986]
Epoch [74/120    avg_loss:0.052, val_acc:0.992]
Epoch [75/120    avg_loss:0.057, val_acc:0.990]
Epoch [76/120    avg_loss:0.051, val_acc:0.990]
Epoch [77/120    avg_loss:0.053, val_acc:0.992]
Epoch [78/120    avg_loss:0.060, val_acc:0.992]
Epoch [79/120    avg_loss:0.063, val_acc:0.992]
Epoch [80/120    avg_loss:0.062, val_acc:0.992]
Epoch [81/120    avg_loss:0.049, val_acc:0.990]
Epoch [82/120    avg_loss:0.045, val_acc:0.992]
Epoch [83/120    avg_loss:0.048, val_acc:0.992]
Epoch [84/120    avg_loss:0.041, val_acc:0.992]
Epoch [85/120    avg_loss:0.055, val_acc:0.990]
Epoch [86/120    avg_loss:0.048, val_acc:0.992]
Epoch [87/120    avg_loss:0.049, val_acc:0.992]
Epoch [88/120    avg_loss:0.043, val_acc:0.992]
Epoch [89/120    avg_loss:0.047, val_acc:0.992]
Epoch [90/120    avg_loss:0.049, val_acc:0.992]
Epoch [91/120    avg_loss:0.055, val_acc:0.992]
Epoch [92/120    avg_loss:0.044, val_acc:0.992]
Epoch [93/120    avg_loss:0.049, val_acc:0.992]
Epoch [94/120    avg_loss:0.041, val_acc:0.992]
Epoch [95/120    avg_loss:0.048, val_acc:0.992]
Epoch [96/120    avg_loss:0.041, val_acc:0.992]
Epoch [97/120    avg_loss:0.046, val_acc:0.992]
Epoch [98/120    avg_loss:0.043, val_acc:0.992]
Epoch [99/120    avg_loss:0.038, val_acc:0.992]
Epoch [100/120    avg_loss:0.033, val_acc:0.992]
Epoch [101/120    avg_loss:0.045, val_acc:0.992]
Epoch [102/120    avg_loss:0.037, val_acc:0.992]
Epoch [103/120    avg_loss:0.039, val_acc:0.992]
Epoch [104/120    avg_loss:0.044, val_acc:0.992]
Epoch [105/120    avg_loss:0.037, val_acc:0.992]
Epoch [106/120    avg_loss:0.037, val_acc:0.992]
Epoch [107/120    avg_loss:0.049, val_acc:0.992]
Epoch [108/120    avg_loss:0.042, val_acc:0.992]
Epoch [109/120    avg_loss:0.042, val_acc:0.992]
Epoch [110/120    avg_loss:0.037, val_acc:0.992]
Epoch [111/120    avg_loss:0.036, val_acc:0.992]
Epoch [112/120    avg_loss:0.038, val_acc:0.992]
Epoch [113/120    avg_loss:0.044, val_acc:0.992]
Epoch [114/120    avg_loss:0.040, val_acc:0.992]
Epoch [115/120    avg_loss:0.041, val_acc:0.992]
Epoch [116/120    avg_loss:0.038, val_acc:0.992]
Epoch [117/120    avg_loss:0.033, val_acc:0.992]
Epoch [118/120    avg_loss:0.036, val_acc:0.992]
Epoch [119/120    avg_loss:0.033, val_acc:0.992]
Epoch [120/120    avg_loss:0.034, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99780541 0.98648649 1.         0.96196868 0.94276094
 0.99277108 0.96703297 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9938284760713761
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f30b085fa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.547, val_acc:0.458]
Epoch [2/120    avg_loss:2.349, val_acc:0.498]
Epoch [3/120    avg_loss:2.200, val_acc:0.481]
Epoch [4/120    avg_loss:2.076, val_acc:0.498]
Epoch [5/120    avg_loss:1.954, val_acc:0.540]
Epoch [6/120    avg_loss:1.813, val_acc:0.573]
Epoch [7/120    avg_loss:1.662, val_acc:0.621]
Epoch [8/120    avg_loss:1.522, val_acc:0.648]
Epoch [9/120    avg_loss:1.361, val_acc:0.679]
Epoch [10/120    avg_loss:1.258, val_acc:0.708]
Epoch [11/120    avg_loss:1.131, val_acc:0.685]
Epoch [12/120    avg_loss:1.096, val_acc:0.756]
Epoch [13/120    avg_loss:0.975, val_acc:0.775]
Epoch [14/120    avg_loss:0.872, val_acc:0.723]
Epoch [15/120    avg_loss:0.791, val_acc:0.825]
Epoch [16/120    avg_loss:0.760, val_acc:0.825]
Epoch [17/120    avg_loss:0.674, val_acc:0.787]
Epoch [18/120    avg_loss:0.602, val_acc:0.840]
Epoch [19/120    avg_loss:0.581, val_acc:0.892]
Epoch [20/120    avg_loss:0.532, val_acc:0.885]
Epoch [21/120    avg_loss:0.494, val_acc:0.919]
Epoch [22/120    avg_loss:0.462, val_acc:0.885]
Epoch [23/120    avg_loss:0.418, val_acc:0.925]
Epoch [24/120    avg_loss:0.392, val_acc:0.892]
Epoch [25/120    avg_loss:0.393, val_acc:0.931]
Epoch [26/120    avg_loss:0.368, val_acc:0.931]
Epoch [27/120    avg_loss:0.385, val_acc:0.927]
Epoch [28/120    avg_loss:0.409, val_acc:0.873]
Epoch [29/120    avg_loss:0.327, val_acc:0.935]
Epoch [30/120    avg_loss:0.294, val_acc:0.894]
Epoch [31/120    avg_loss:0.314, val_acc:0.948]
Epoch [32/120    avg_loss:0.246, val_acc:0.925]
Epoch [33/120    avg_loss:0.296, val_acc:0.940]
Epoch [34/120    avg_loss:0.293, val_acc:0.929]
Epoch [35/120    avg_loss:0.237, val_acc:0.931]
Epoch [36/120    avg_loss:0.247, val_acc:0.923]
Epoch [37/120    avg_loss:0.258, val_acc:0.946]
Epoch [38/120    avg_loss:0.187, val_acc:0.952]
Epoch [39/120    avg_loss:0.191, val_acc:0.912]
Epoch [40/120    avg_loss:0.211, val_acc:0.944]
Epoch [41/120    avg_loss:0.231, val_acc:0.944]
Epoch [42/120    avg_loss:0.243, val_acc:0.958]
Epoch [43/120    avg_loss:0.193, val_acc:0.948]
Epoch [44/120    avg_loss:0.225, val_acc:0.956]
Epoch [45/120    avg_loss:0.152, val_acc:0.965]
Epoch [46/120    avg_loss:0.203, val_acc:0.963]
Epoch [47/120    avg_loss:0.200, val_acc:0.958]
Epoch [48/120    avg_loss:0.150, val_acc:0.958]
Epoch [49/120    avg_loss:0.128, val_acc:0.969]
Epoch [50/120    avg_loss:0.114, val_acc:0.960]
Epoch [51/120    avg_loss:0.138, val_acc:0.971]
Epoch [52/120    avg_loss:0.132, val_acc:0.967]
Epoch [53/120    avg_loss:0.237, val_acc:0.944]
Epoch [54/120    avg_loss:0.167, val_acc:0.940]
Epoch [55/120    avg_loss:0.174, val_acc:0.952]
Epoch [56/120    avg_loss:0.155, val_acc:0.977]
Epoch [57/120    avg_loss:0.140, val_acc:0.971]
Epoch [58/120    avg_loss:0.109, val_acc:0.977]
Epoch [59/120    avg_loss:0.091, val_acc:0.975]
Epoch [60/120    avg_loss:0.082, val_acc:0.967]
Epoch [61/120    avg_loss:0.121, val_acc:0.971]
Epoch [62/120    avg_loss:0.106, val_acc:0.967]
Epoch [63/120    avg_loss:0.076, val_acc:0.965]
Epoch [64/120    avg_loss:0.087, val_acc:0.979]
Epoch [65/120    avg_loss:0.065, val_acc:0.985]
Epoch [66/120    avg_loss:0.087, val_acc:0.979]
Epoch [67/120    avg_loss:0.220, val_acc:0.958]
Epoch [68/120    avg_loss:0.172, val_acc:0.942]
Epoch [69/120    avg_loss:0.133, val_acc:0.967]
Epoch [70/120    avg_loss:0.093, val_acc:0.965]
Epoch [71/120    avg_loss:0.114, val_acc:0.960]
Epoch [72/120    avg_loss:0.077, val_acc:0.963]
Epoch [73/120    avg_loss:0.062, val_acc:0.954]
Epoch [74/120    avg_loss:0.135, val_acc:0.971]
Epoch [75/120    avg_loss:0.084, val_acc:0.967]
Epoch [76/120    avg_loss:0.081, val_acc:0.975]
Epoch [77/120    avg_loss:0.073, val_acc:0.977]
Epoch [78/120    avg_loss:0.054, val_acc:0.973]
Epoch [79/120    avg_loss:0.049, val_acc:0.975]
Epoch [80/120    avg_loss:0.048, val_acc:0.983]
Epoch [81/120    avg_loss:0.042, val_acc:0.985]
Epoch [82/120    avg_loss:0.040, val_acc:0.985]
Epoch [83/120    avg_loss:0.046, val_acc:0.983]
Epoch [84/120    avg_loss:0.039, val_acc:0.985]
Epoch [85/120    avg_loss:0.047, val_acc:0.985]
Epoch [86/120    avg_loss:0.036, val_acc:0.983]
Epoch [87/120    avg_loss:0.040, val_acc:0.985]
Epoch [88/120    avg_loss:0.044, val_acc:0.985]
Epoch [89/120    avg_loss:0.036, val_acc:0.985]
Epoch [90/120    avg_loss:0.034, val_acc:0.985]
Epoch [91/120    avg_loss:0.037, val_acc:0.985]
Epoch [92/120    avg_loss:0.039, val_acc:0.985]
Epoch [93/120    avg_loss:0.038, val_acc:0.985]
Epoch [94/120    avg_loss:0.035, val_acc:0.985]
Epoch [95/120    avg_loss:0.036, val_acc:0.985]
Epoch [96/120    avg_loss:0.037, val_acc:0.985]
Epoch [97/120    avg_loss:0.043, val_acc:0.985]
Epoch [98/120    avg_loss:0.036, val_acc:0.985]
Epoch [99/120    avg_loss:0.040, val_acc:0.988]
Epoch [100/120    avg_loss:0.029, val_acc:0.985]
Epoch [101/120    avg_loss:0.036, val_acc:0.985]
Epoch [102/120    avg_loss:0.043, val_acc:0.988]
Epoch [103/120    avg_loss:0.030, val_acc:0.988]
Epoch [104/120    avg_loss:0.037, val_acc:0.985]
Epoch [105/120    avg_loss:0.037, val_acc:0.988]
Epoch [106/120    avg_loss:0.032, val_acc:0.988]
Epoch [107/120    avg_loss:0.030, val_acc:0.985]
Epoch [108/120    avg_loss:0.030, val_acc:0.985]
Epoch [109/120    avg_loss:0.035, val_acc:0.985]
Epoch [110/120    avg_loss:0.030, val_acc:0.988]
Epoch [111/120    avg_loss:0.034, val_acc:0.985]
Epoch [112/120    avg_loss:0.031, val_acc:0.985]
Epoch [113/120    avg_loss:0.026, val_acc:0.985]
Epoch [114/120    avg_loss:0.038, val_acc:0.985]
Epoch [115/120    avg_loss:0.033, val_acc:0.985]
Epoch [116/120    avg_loss:0.035, val_acc:0.985]
Epoch [117/120    avg_loss:0.030, val_acc:0.988]
Epoch [118/120    avg_loss:0.028, val_acc:0.988]
Epoch [119/120    avg_loss:0.038, val_acc:0.985]
Epoch [120/120    avg_loss:0.027, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99487179 0.97986577 1.         0.95927602 0.94039735
 0.98564593 0.94972067 0.99870968 1.         1.         1.
 1.         1.        ]

Kappa:
0.991930025537881
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fedc74dfb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.555, val_acc:0.272]
Epoch [2/120    avg_loss:2.418, val_acc:0.298]
Epoch [3/120    avg_loss:2.298, val_acc:0.349]
Epoch [4/120    avg_loss:2.173, val_acc:0.460]
Epoch [5/120    avg_loss:2.065, val_acc:0.510]
Epoch [6/120    avg_loss:1.958, val_acc:0.536]
Epoch [7/120    avg_loss:1.827, val_acc:0.595]
Epoch [8/120    avg_loss:1.701, val_acc:0.667]
Epoch [9/120    avg_loss:1.598, val_acc:0.673]
Epoch [10/120    avg_loss:1.468, val_acc:0.671]
Epoch [11/120    avg_loss:1.393, val_acc:0.696]
Epoch [12/120    avg_loss:1.261, val_acc:0.712]
Epoch [13/120    avg_loss:1.168, val_acc:0.708]
Epoch [14/120    avg_loss:1.061, val_acc:0.772]
Epoch [15/120    avg_loss:0.981, val_acc:0.724]
Epoch [16/120    avg_loss:0.947, val_acc:0.780]
Epoch [17/120    avg_loss:0.844, val_acc:0.841]
Epoch [18/120    avg_loss:0.811, val_acc:0.760]
Epoch [19/120    avg_loss:0.736, val_acc:0.899]
Epoch [20/120    avg_loss:0.718, val_acc:0.849]
Epoch [21/120    avg_loss:0.648, val_acc:0.893]
Epoch [22/120    avg_loss:0.609, val_acc:0.903]
Epoch [23/120    avg_loss:0.527, val_acc:0.893]
Epoch [24/120    avg_loss:0.524, val_acc:0.891]
Epoch [25/120    avg_loss:0.482, val_acc:0.827]
Epoch [26/120    avg_loss:0.404, val_acc:0.923]
Epoch [27/120    avg_loss:0.429, val_acc:0.909]
Epoch [28/120    avg_loss:0.369, val_acc:0.909]
Epoch [29/120    avg_loss:0.427, val_acc:0.925]
Epoch [30/120    avg_loss:0.372, val_acc:0.917]
Epoch [31/120    avg_loss:0.360, val_acc:0.917]
Epoch [32/120    avg_loss:0.325, val_acc:0.917]
Epoch [33/120    avg_loss:0.380, val_acc:0.938]
Epoch [34/120    avg_loss:0.305, val_acc:0.911]
Epoch [35/120    avg_loss:0.335, val_acc:0.921]
Epoch [36/120    avg_loss:0.259, val_acc:0.938]
Epoch [37/120    avg_loss:0.291, val_acc:0.954]
Epoch [38/120    avg_loss:0.265, val_acc:0.956]
Epoch [39/120    avg_loss:0.245, val_acc:0.940]
Epoch [40/120    avg_loss:0.260, val_acc:0.956]
Epoch [41/120    avg_loss:0.219, val_acc:0.944]
Epoch [42/120    avg_loss:0.226, val_acc:0.913]
Epoch [43/120    avg_loss:0.229, val_acc:0.960]
Epoch [44/120    avg_loss:0.271, val_acc:0.952]
Epoch [45/120    avg_loss:0.201, val_acc:0.958]
Epoch [46/120    avg_loss:0.191, val_acc:0.946]
Epoch [47/120    avg_loss:0.229, val_acc:0.964]
Epoch [48/120    avg_loss:0.213, val_acc:0.954]
Epoch [49/120    avg_loss:0.168, val_acc:0.966]
Epoch [50/120    avg_loss:0.162, val_acc:0.966]
Epoch [51/120    avg_loss:0.133, val_acc:0.968]
Epoch [52/120    avg_loss:0.140, val_acc:0.966]
Epoch [53/120    avg_loss:0.149, val_acc:0.972]
Epoch [54/120    avg_loss:0.163, val_acc:0.962]
Epoch [55/120    avg_loss:0.156, val_acc:0.958]
Epoch [56/120    avg_loss:0.124, val_acc:0.988]
Epoch [57/120    avg_loss:0.130, val_acc:0.964]
Epoch [58/120    avg_loss:0.123, val_acc:0.982]
Epoch [59/120    avg_loss:0.113, val_acc:0.982]
Epoch [60/120    avg_loss:0.093, val_acc:0.986]
Epoch [61/120    avg_loss:0.110, val_acc:0.966]
Epoch [62/120    avg_loss:0.117, val_acc:0.960]
Epoch [63/120    avg_loss:0.141, val_acc:0.974]
Epoch [64/120    avg_loss:0.131, val_acc:0.980]
Epoch [65/120    avg_loss:0.131, val_acc:0.976]
Epoch [66/120    avg_loss:0.116, val_acc:0.964]
Epoch [67/120    avg_loss:0.109, val_acc:0.972]
Epoch [68/120    avg_loss:0.108, val_acc:0.978]
Epoch [69/120    avg_loss:0.087, val_acc:0.980]
Epoch [70/120    avg_loss:0.071, val_acc:0.980]
Epoch [71/120    avg_loss:0.064, val_acc:0.988]
Epoch [72/120    avg_loss:0.060, val_acc:0.988]
Epoch [73/120    avg_loss:0.062, val_acc:0.988]
Epoch [74/120    avg_loss:0.058, val_acc:0.990]
Epoch [75/120    avg_loss:0.055, val_acc:0.992]
Epoch [76/120    avg_loss:0.048, val_acc:0.990]
Epoch [77/120    avg_loss:0.051, val_acc:0.992]
Epoch [78/120    avg_loss:0.049, val_acc:0.990]
Epoch [79/120    avg_loss:0.049, val_acc:0.992]
Epoch [80/120    avg_loss:0.049, val_acc:0.994]
Epoch [81/120    avg_loss:0.055, val_acc:0.992]
Epoch [82/120    avg_loss:0.048, val_acc:0.992]
Epoch [83/120    avg_loss:0.047, val_acc:0.990]
Epoch [84/120    avg_loss:0.053, val_acc:0.988]
Epoch [85/120    avg_loss:0.055, val_acc:0.992]
Epoch [86/120    avg_loss:0.061, val_acc:0.994]
Epoch [87/120    avg_loss:0.051, val_acc:0.990]
Epoch [88/120    avg_loss:0.055, val_acc:0.996]
Epoch [89/120    avg_loss:0.049, val_acc:0.994]
Epoch [90/120    avg_loss:0.047, val_acc:0.994]
Epoch [91/120    avg_loss:0.063, val_acc:0.994]
Epoch [92/120    avg_loss:0.040, val_acc:0.994]
Epoch [93/120    avg_loss:0.064, val_acc:0.992]
Epoch [94/120    avg_loss:0.050, val_acc:0.994]
Epoch [95/120    avg_loss:0.047, val_acc:0.994]
Epoch [96/120    avg_loss:0.039, val_acc:0.994]
Epoch [97/120    avg_loss:0.042, val_acc:0.994]
Epoch [98/120    avg_loss:0.044, val_acc:0.992]
Epoch [99/120    avg_loss:0.049, val_acc:0.992]
Epoch [100/120    avg_loss:0.043, val_acc:0.994]
Epoch [101/120    avg_loss:0.047, val_acc:0.996]
Epoch [102/120    avg_loss:0.041, val_acc:0.986]
Epoch [103/120    avg_loss:0.039, val_acc:0.994]
Epoch [104/120    avg_loss:0.047, val_acc:0.992]
Epoch [105/120    avg_loss:0.046, val_acc:0.992]
Epoch [106/120    avg_loss:0.048, val_acc:0.994]
Epoch [107/120    avg_loss:0.045, val_acc:0.992]
Epoch [108/120    avg_loss:0.039, val_acc:0.992]
Epoch [109/120    avg_loss:0.036, val_acc:0.992]
Epoch [110/120    avg_loss:0.045, val_acc:0.992]
Epoch [111/120    avg_loss:0.052, val_acc:0.992]
Epoch [112/120    avg_loss:0.039, val_acc:0.994]
Epoch [113/120    avg_loss:0.034, val_acc:0.992]
Epoch [114/120    avg_loss:0.040, val_acc:0.994]
Epoch [115/120    avg_loss:0.040, val_acc:0.994]
Epoch [116/120    avg_loss:0.050, val_acc:0.994]
Epoch [117/120    avg_loss:0.038, val_acc:0.994]
Epoch [118/120    avg_loss:0.039, val_acc:0.994]
Epoch [119/120    avg_loss:0.035, val_acc:0.994]
Epoch [120/120    avg_loss:0.042, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.9977221  1.         0.93693694 0.90666667
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.99311617996323
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4fab144b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.568, val_acc:0.399]
Epoch [2/120    avg_loss:2.417, val_acc:0.393]
Epoch [3/120    avg_loss:2.284, val_acc:0.440]
Epoch [4/120    avg_loss:2.148, val_acc:0.520]
Epoch [5/120    avg_loss:2.024, val_acc:0.569]
Epoch [6/120    avg_loss:1.901, val_acc:0.633]
Epoch [7/120    avg_loss:1.762, val_acc:0.665]
Epoch [8/120    avg_loss:1.614, val_acc:0.643]
Epoch [9/120    avg_loss:1.516, val_acc:0.696]
Epoch [10/120    avg_loss:1.345, val_acc:0.710]
Epoch [11/120    avg_loss:1.223, val_acc:0.736]
Epoch [12/120    avg_loss:1.077, val_acc:0.756]
Epoch [13/120    avg_loss:1.015, val_acc:0.756]
Epoch [14/120    avg_loss:0.872, val_acc:0.875]
Epoch [15/120    avg_loss:0.856, val_acc:0.819]
Epoch [16/120    avg_loss:0.837, val_acc:0.792]
Epoch [17/120    avg_loss:0.704, val_acc:0.845]
Epoch [18/120    avg_loss:0.617, val_acc:0.887]
Epoch [19/120    avg_loss:0.647, val_acc:0.823]
Epoch [20/120    avg_loss:0.559, val_acc:0.889]
Epoch [21/120    avg_loss:0.532, val_acc:0.923]
Epoch [22/120    avg_loss:0.516, val_acc:0.911]
Epoch [23/120    avg_loss:0.495, val_acc:0.865]
Epoch [24/120    avg_loss:0.439, val_acc:0.938]
Epoch [25/120    avg_loss:0.392, val_acc:0.921]
Epoch [26/120    avg_loss:0.491, val_acc:0.853]
Epoch [27/120    avg_loss:0.442, val_acc:0.938]
Epoch [28/120    avg_loss:0.371, val_acc:0.923]
Epoch [29/120    avg_loss:0.331, val_acc:0.933]
Epoch [30/120    avg_loss:0.342, val_acc:0.931]
Epoch [31/120    avg_loss:0.349, val_acc:0.935]
Epoch [32/120    avg_loss:0.315, val_acc:0.929]
Epoch [33/120    avg_loss:0.262, val_acc:0.954]
Epoch [34/120    avg_loss:0.269, val_acc:0.950]
Epoch [35/120    avg_loss:0.232, val_acc:0.968]
Epoch [36/120    avg_loss:0.243, val_acc:0.954]
Epoch [37/120    avg_loss:0.255, val_acc:0.956]
Epoch [38/120    avg_loss:0.245, val_acc:0.956]
Epoch [39/120    avg_loss:0.289, val_acc:0.966]
Epoch [40/120    avg_loss:0.276, val_acc:0.946]
Epoch [41/120    avg_loss:0.258, val_acc:0.933]
Epoch [42/120    avg_loss:0.298, val_acc:0.919]
Epoch [43/120    avg_loss:0.205, val_acc:0.962]
Epoch [44/120    avg_loss:0.167, val_acc:0.933]
Epoch [45/120    avg_loss:0.204, val_acc:0.976]
Epoch [46/120    avg_loss:0.266, val_acc:0.935]
Epoch [47/120    avg_loss:0.239, val_acc:0.984]
Epoch [48/120    avg_loss:0.173, val_acc:0.982]
Epoch [49/120    avg_loss:0.167, val_acc:0.978]
Epoch [50/120    avg_loss:0.184, val_acc:0.962]
Epoch [51/120    avg_loss:0.138, val_acc:0.968]
Epoch [52/120    avg_loss:0.161, val_acc:0.948]
Epoch [53/120    avg_loss:0.254, val_acc:0.978]
Epoch [54/120    avg_loss:0.161, val_acc:0.984]
Epoch [55/120    avg_loss:0.158, val_acc:0.984]
Epoch [56/120    avg_loss:0.139, val_acc:0.972]
Epoch [57/120    avg_loss:0.121, val_acc:0.974]
Epoch [58/120    avg_loss:0.147, val_acc:0.978]
Epoch [59/120    avg_loss:0.129, val_acc:0.976]
Epoch [60/120    avg_loss:0.170, val_acc:0.974]
Epoch [61/120    avg_loss:0.127, val_acc:0.966]
Epoch [62/120    avg_loss:0.166, val_acc:0.968]
Epoch [63/120    avg_loss:0.161, val_acc:0.976]
Epoch [64/120    avg_loss:0.126, val_acc:0.986]
Epoch [65/120    avg_loss:0.131, val_acc:0.964]
Epoch [66/120    avg_loss:0.171, val_acc:0.980]
Epoch [67/120    avg_loss:0.085, val_acc:0.976]
Epoch [68/120    avg_loss:0.083, val_acc:0.986]
Epoch [69/120    avg_loss:0.080, val_acc:0.978]
Epoch [70/120    avg_loss:0.077, val_acc:0.992]
Epoch [71/120    avg_loss:0.066, val_acc:0.984]
Epoch [72/120    avg_loss:0.096, val_acc:0.976]
Epoch [73/120    avg_loss:0.078, val_acc:0.986]
Epoch [74/120    avg_loss:0.085, val_acc:0.982]
Epoch [75/120    avg_loss:0.097, val_acc:0.988]
Epoch [76/120    avg_loss:0.088, val_acc:0.984]
Epoch [77/120    avg_loss:0.090, val_acc:0.986]
Epoch [78/120    avg_loss:0.069, val_acc:0.980]
Epoch [79/120    avg_loss:0.085, val_acc:0.988]
Epoch [80/120    avg_loss:0.063, val_acc:0.988]
Epoch [81/120    avg_loss:0.065, val_acc:0.978]
Epoch [82/120    avg_loss:0.103, val_acc:0.976]
Epoch [83/120    avg_loss:0.052, val_acc:0.978]
Epoch [84/120    avg_loss:0.066, val_acc:0.988]
Epoch [85/120    avg_loss:0.047, val_acc:0.988]
Epoch [86/120    avg_loss:0.037, val_acc:0.988]
Epoch [87/120    avg_loss:0.038, val_acc:0.990]
Epoch [88/120    avg_loss:0.052, val_acc:0.990]
Epoch [89/120    avg_loss:0.043, val_acc:0.992]
Epoch [90/120    avg_loss:0.037, val_acc:0.992]
Epoch [91/120    avg_loss:0.031, val_acc:0.992]
Epoch [92/120    avg_loss:0.039, val_acc:0.992]
Epoch [93/120    avg_loss:0.032, val_acc:0.992]
Epoch [94/120    avg_loss:0.031, val_acc:0.994]
Epoch [95/120    avg_loss:0.029, val_acc:0.994]
Epoch [96/120    avg_loss:0.031, val_acc:0.992]
Epoch [97/120    avg_loss:0.031, val_acc:0.994]
Epoch [98/120    avg_loss:0.033, val_acc:0.994]
Epoch [99/120    avg_loss:0.029, val_acc:0.992]
Epoch [100/120    avg_loss:0.033, val_acc:0.994]
Epoch [101/120    avg_loss:0.036, val_acc:0.994]
Epoch [102/120    avg_loss:0.032, val_acc:0.994]
Epoch [103/120    avg_loss:0.032, val_acc:0.994]
Epoch [104/120    avg_loss:0.031, val_acc:0.992]
Epoch [105/120    avg_loss:0.031, val_acc:0.992]
Epoch [106/120    avg_loss:0.033, val_acc:0.992]
Epoch [107/120    avg_loss:0.027, val_acc:0.994]
Epoch [108/120    avg_loss:0.035, val_acc:0.992]
Epoch [109/120    avg_loss:0.035, val_acc:0.992]
Epoch [110/120    avg_loss:0.029, val_acc:0.994]
Epoch [111/120    avg_loss:0.027, val_acc:0.994]
Epoch [112/120    avg_loss:0.029, val_acc:0.994]
Epoch [113/120    avg_loss:0.028, val_acc:0.994]
Epoch [114/120    avg_loss:0.026, val_acc:0.994]
Epoch [115/120    avg_loss:0.027, val_acc:0.994]
Epoch [116/120    avg_loss:0.025, val_acc:0.994]
Epoch [117/120    avg_loss:0.026, val_acc:0.994]
Epoch [118/120    avg_loss:0.032, val_acc:0.994]
Epoch [119/120    avg_loss:0.033, val_acc:0.994]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99412628 0.99319728 1.         0.92682927 0.88737201
 0.98095238 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.989557173236709
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc0a7f9fac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.565, val_acc:0.412]
Epoch [2/120    avg_loss:2.385, val_acc:0.403]
Epoch [3/120    avg_loss:2.250, val_acc:0.427]
Epoch [4/120    avg_loss:2.125, val_acc:0.472]
Epoch [5/120    avg_loss:2.009, val_acc:0.496]
Epoch [6/120    avg_loss:1.884, val_acc:0.516]
Epoch [7/120    avg_loss:1.785, val_acc:0.577]
Epoch [8/120    avg_loss:1.665, val_acc:0.623]
Epoch [9/120    avg_loss:1.512, val_acc:0.669]
Epoch [10/120    avg_loss:1.384, val_acc:0.692]
Epoch [11/120    avg_loss:1.274, val_acc:0.720]
Epoch [12/120    avg_loss:1.142, val_acc:0.768]
Epoch [13/120    avg_loss:0.999, val_acc:0.768]
Epoch [14/120    avg_loss:0.899, val_acc:0.857]
Epoch [15/120    avg_loss:0.786, val_acc:0.897]
Epoch [16/120    avg_loss:0.670, val_acc:0.915]
Epoch [17/120    avg_loss:0.635, val_acc:0.843]
Epoch [18/120    avg_loss:0.605, val_acc:0.895]
Epoch [19/120    avg_loss:0.574, val_acc:0.927]
Epoch [20/120    avg_loss:0.483, val_acc:0.911]
Epoch [21/120    avg_loss:0.501, val_acc:0.933]
Epoch [22/120    avg_loss:0.481, val_acc:0.877]
Epoch [23/120    avg_loss:0.501, val_acc:0.927]
Epoch [24/120    avg_loss:0.418, val_acc:0.927]
Epoch [25/120    avg_loss:0.368, val_acc:0.927]
Epoch [26/120    avg_loss:0.391, val_acc:0.948]
Epoch [27/120    avg_loss:0.322, val_acc:0.917]
Epoch [28/120    avg_loss:0.403, val_acc:0.905]
Epoch [29/120    avg_loss:0.321, val_acc:0.962]
Epoch [30/120    avg_loss:0.298, val_acc:0.956]
Epoch [31/120    avg_loss:0.245, val_acc:0.962]
Epoch [32/120    avg_loss:0.217, val_acc:0.952]
Epoch [33/120    avg_loss:0.227, val_acc:0.964]
Epoch [34/120    avg_loss:0.215, val_acc:0.966]
Epoch [35/120    avg_loss:0.230, val_acc:0.950]
Epoch [36/120    avg_loss:0.282, val_acc:0.964]
Epoch [37/120    avg_loss:0.210, val_acc:0.972]
Epoch [38/120    avg_loss:0.173, val_acc:0.968]
Epoch [39/120    avg_loss:0.196, val_acc:0.962]
Epoch [40/120    avg_loss:0.201, val_acc:0.952]
Epoch [41/120    avg_loss:0.221, val_acc:0.958]
Epoch [42/120    avg_loss:0.196, val_acc:0.954]
Epoch [43/120    avg_loss:0.162, val_acc:0.984]
Epoch [44/120    avg_loss:0.172, val_acc:0.962]
Epoch [45/120    avg_loss:0.177, val_acc:0.972]
Epoch [46/120    avg_loss:0.205, val_acc:0.954]
Epoch [47/120    avg_loss:0.164, val_acc:0.974]
Epoch [48/120    avg_loss:0.133, val_acc:0.966]
Epoch [49/120    avg_loss:0.115, val_acc:0.966]
Epoch [50/120    avg_loss:0.134, val_acc:0.972]
Epoch [51/120    avg_loss:0.152, val_acc:0.988]
Epoch [52/120    avg_loss:0.114, val_acc:0.980]
Epoch [53/120    avg_loss:0.103, val_acc:0.984]
Epoch [54/120    avg_loss:0.212, val_acc:0.933]
Epoch [55/120    avg_loss:0.195, val_acc:0.976]
Epoch [56/120    avg_loss:0.144, val_acc:0.972]
Epoch [57/120    avg_loss:0.112, val_acc:0.986]
Epoch [58/120    avg_loss:0.089, val_acc:0.990]
Epoch [59/120    avg_loss:0.076, val_acc:0.978]
Epoch [60/120    avg_loss:0.147, val_acc:0.956]
Epoch [61/120    avg_loss:0.096, val_acc:0.970]
Epoch [62/120    avg_loss:0.084, val_acc:0.990]
Epoch [63/120    avg_loss:0.103, val_acc:0.980]
Epoch [64/120    avg_loss:0.067, val_acc:0.988]
Epoch [65/120    avg_loss:0.094, val_acc:0.986]
Epoch [66/120    avg_loss:0.082, val_acc:0.982]
Epoch [67/120    avg_loss:0.055, val_acc:0.988]
Epoch [68/120    avg_loss:0.080, val_acc:0.986]
Epoch [69/120    avg_loss:0.081, val_acc:0.980]
Epoch [70/120    avg_loss:0.083, val_acc:0.978]
Epoch [71/120    avg_loss:0.063, val_acc:0.988]
Epoch [72/120    avg_loss:0.043, val_acc:0.988]
Epoch [73/120    avg_loss:0.064, val_acc:0.986]
Epoch [74/120    avg_loss:0.090, val_acc:0.972]
Epoch [75/120    avg_loss:0.091, val_acc:0.986]
Epoch [76/120    avg_loss:0.049, val_acc:0.990]
Epoch [77/120    avg_loss:0.037, val_acc:0.992]
Epoch [78/120    avg_loss:0.042, val_acc:0.988]
Epoch [79/120    avg_loss:0.038, val_acc:0.990]
Epoch [80/120    avg_loss:0.047, val_acc:0.990]
Epoch [81/120    avg_loss:0.042, val_acc:0.990]
Epoch [82/120    avg_loss:0.037, val_acc:0.992]
Epoch [83/120    avg_loss:0.034, val_acc:0.988]
Epoch [84/120    avg_loss:0.034, val_acc:0.990]
Epoch [85/120    avg_loss:0.039, val_acc:0.992]
Epoch [86/120    avg_loss:0.036, val_acc:0.990]
Epoch [87/120    avg_loss:0.030, val_acc:0.992]
Epoch [88/120    avg_loss:0.038, val_acc:0.992]
Epoch [89/120    avg_loss:0.030, val_acc:0.992]
Epoch [90/120    avg_loss:0.028, val_acc:0.992]
Epoch [91/120    avg_loss:0.025, val_acc:0.992]
Epoch [92/120    avg_loss:0.035, val_acc:0.992]
Epoch [93/120    avg_loss:0.027, val_acc:0.992]
Epoch [94/120    avg_loss:0.030, val_acc:0.992]
Epoch [95/120    avg_loss:0.028, val_acc:0.992]
Epoch [96/120    avg_loss:0.031, val_acc:0.992]
Epoch [97/120    avg_loss:0.031, val_acc:0.992]
Epoch [98/120    avg_loss:0.027, val_acc:0.992]
Epoch [99/120    avg_loss:0.033, val_acc:0.994]
Epoch [100/120    avg_loss:0.034, val_acc:0.992]
Epoch [101/120    avg_loss:0.034, val_acc:0.992]
Epoch [102/120    avg_loss:0.030, val_acc:0.992]
Epoch [103/120    avg_loss:0.025, val_acc:0.992]
Epoch [104/120    avg_loss:0.026, val_acc:0.992]
Epoch [105/120    avg_loss:0.029, val_acc:0.992]
Epoch [106/120    avg_loss:0.028, val_acc:0.992]
Epoch [107/120    avg_loss:0.026, val_acc:0.992]
Epoch [108/120    avg_loss:0.031, val_acc:0.992]
Epoch [109/120    avg_loss:0.023, val_acc:0.992]
Epoch [110/120    avg_loss:0.025, val_acc:0.992]
Epoch [111/120    avg_loss:0.028, val_acc:0.992]
Epoch [112/120    avg_loss:0.025, val_acc:0.992]
Epoch [113/120    avg_loss:0.026, val_acc:0.992]
Epoch [114/120    avg_loss:0.028, val_acc:0.992]
Epoch [115/120    avg_loss:0.032, val_acc:0.992]
Epoch [116/120    avg_loss:0.029, val_acc:0.992]
Epoch [117/120    avg_loss:0.024, val_acc:0.992]
Epoch [118/120    avg_loss:0.026, val_acc:0.992]
Epoch [119/120    avg_loss:0.029, val_acc:0.992]
Epoch [120/120    avg_loss:0.023, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.996337   0.99319728 1.         0.96717724 0.94773519
 0.98800959 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9945407167284344
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8158af8a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.626, val_acc:0.404]
Epoch [2/120    avg_loss:2.493, val_acc:0.490]
Epoch [3/120    avg_loss:2.386, val_acc:0.488]
Epoch [4/120    avg_loss:2.308, val_acc:0.508]
Epoch [5/120    avg_loss:2.234, val_acc:0.550]
Epoch [6/120    avg_loss:2.145, val_acc:0.589]
Epoch [7/120    avg_loss:2.076, val_acc:0.625]
Epoch [8/120    avg_loss:1.984, val_acc:0.530]
Epoch [9/120    avg_loss:1.889, val_acc:0.595]
Epoch [10/120    avg_loss:1.786, val_acc:0.581]
Epoch [11/120    avg_loss:1.700, val_acc:0.619]
Epoch [12/120    avg_loss:1.574, val_acc:0.635]
Epoch [13/120    avg_loss:1.483, val_acc:0.702]
Epoch [14/120    avg_loss:1.394, val_acc:0.738]
Epoch [15/120    avg_loss:1.308, val_acc:0.788]
Epoch [16/120    avg_loss:1.205, val_acc:0.784]
Epoch [17/120    avg_loss:1.120, val_acc:0.823]
Epoch [18/120    avg_loss:1.008, val_acc:0.835]
Epoch [19/120    avg_loss:0.960, val_acc:0.893]
Epoch [20/120    avg_loss:0.874, val_acc:0.859]
Epoch [21/120    avg_loss:0.817, val_acc:0.855]
Epoch [22/120    avg_loss:0.773, val_acc:0.897]
Epoch [23/120    avg_loss:0.705, val_acc:0.881]
Epoch [24/120    avg_loss:0.643, val_acc:0.871]
Epoch [25/120    avg_loss:0.640, val_acc:0.901]
Epoch [26/120    avg_loss:0.532, val_acc:0.909]
Epoch [27/120    avg_loss:0.535, val_acc:0.891]
Epoch [28/120    avg_loss:0.516, val_acc:0.907]
Epoch [29/120    avg_loss:0.544, val_acc:0.909]
Epoch [30/120    avg_loss:0.498, val_acc:0.923]
Epoch [31/120    avg_loss:0.465, val_acc:0.907]
Epoch [32/120    avg_loss:0.473, val_acc:0.917]
Epoch [33/120    avg_loss:0.466, val_acc:0.875]
Epoch [34/120    avg_loss:0.434, val_acc:0.909]
Epoch [35/120    avg_loss:0.380, val_acc:0.921]
Epoch [36/120    avg_loss:0.408, val_acc:0.909]
Epoch [37/120    avg_loss:0.377, val_acc:0.915]
Epoch [38/120    avg_loss:0.422, val_acc:0.917]
Epoch [39/120    avg_loss:0.379, val_acc:0.917]
Epoch [40/120    avg_loss:0.406, val_acc:0.917]
Epoch [41/120    avg_loss:0.370, val_acc:0.911]
Epoch [42/120    avg_loss:0.325, val_acc:0.937]
Epoch [43/120    avg_loss:0.312, val_acc:0.913]
Epoch [44/120    avg_loss:0.304, val_acc:0.917]
Epoch [45/120    avg_loss:0.336, val_acc:0.917]
Epoch [46/120    avg_loss:0.353, val_acc:0.935]
Epoch [47/120    avg_loss:0.306, val_acc:0.927]
Epoch [48/120    avg_loss:0.282, val_acc:0.940]
Epoch [49/120    avg_loss:0.304, val_acc:0.927]
Epoch [50/120    avg_loss:0.345, val_acc:0.933]
Epoch [51/120    avg_loss:0.346, val_acc:0.911]
Epoch [52/120    avg_loss:0.296, val_acc:0.915]
Epoch [53/120    avg_loss:0.282, val_acc:0.927]
Epoch [54/120    avg_loss:0.268, val_acc:0.933]
Epoch [55/120    avg_loss:0.245, val_acc:0.940]
Epoch [56/120    avg_loss:0.243, val_acc:0.954]
Epoch [57/120    avg_loss:0.250, val_acc:0.915]
Epoch [58/120    avg_loss:0.222, val_acc:0.948]
Epoch [59/120    avg_loss:0.192, val_acc:0.956]
Epoch [60/120    avg_loss:0.272, val_acc:0.960]
Epoch [61/120    avg_loss:0.230, val_acc:0.952]
Epoch [62/120    avg_loss:0.206, val_acc:0.962]
Epoch [63/120    avg_loss:0.207, val_acc:0.958]
Epoch [64/120    avg_loss:0.203, val_acc:0.950]
Epoch [65/120    avg_loss:0.203, val_acc:0.952]
Epoch [66/120    avg_loss:0.202, val_acc:0.942]
Epoch [67/120    avg_loss:0.210, val_acc:0.921]
Epoch [68/120    avg_loss:0.191, val_acc:0.950]
Epoch [69/120    avg_loss:0.195, val_acc:0.960]
Epoch [70/120    avg_loss:0.276, val_acc:0.931]
Epoch [71/120    avg_loss:0.262, val_acc:0.954]
Epoch [72/120    avg_loss:0.220, val_acc:0.935]
Epoch [73/120    avg_loss:0.189, val_acc:0.960]
Epoch [74/120    avg_loss:0.168, val_acc:0.954]
Epoch [75/120    avg_loss:0.207, val_acc:0.931]
Epoch [76/120    avg_loss:0.166, val_acc:0.962]
Epoch [77/120    avg_loss:0.133, val_acc:0.970]
Epoch [78/120    avg_loss:0.122, val_acc:0.972]
Epoch [79/120    avg_loss:0.113, val_acc:0.976]
Epoch [80/120    avg_loss:0.109, val_acc:0.974]
Epoch [81/120    avg_loss:0.129, val_acc:0.976]
Epoch [82/120    avg_loss:0.133, val_acc:0.976]
Epoch [83/120    avg_loss:0.109, val_acc:0.978]
Epoch [84/120    avg_loss:0.114, val_acc:0.976]
Epoch [85/120    avg_loss:0.109, val_acc:0.974]
Epoch [86/120    avg_loss:0.089, val_acc:0.976]
Epoch [87/120    avg_loss:0.113, val_acc:0.974]
Epoch [88/120    avg_loss:0.108, val_acc:0.976]
Epoch [89/120    avg_loss:0.115, val_acc:0.976]
Epoch [90/120    avg_loss:0.118, val_acc:0.974]
Epoch [91/120    avg_loss:0.113, val_acc:0.974]
Epoch [92/120    avg_loss:0.097, val_acc:0.972]
Epoch [93/120    avg_loss:0.108, val_acc:0.976]
Epoch [94/120    avg_loss:0.112, val_acc:0.976]
Epoch [95/120    avg_loss:0.099, val_acc:0.972]
Epoch [96/120    avg_loss:0.107, val_acc:0.972]
Epoch [97/120    avg_loss:0.103, val_acc:0.972]
Epoch [98/120    avg_loss:0.091, val_acc:0.972]
Epoch [99/120    avg_loss:0.114, val_acc:0.972]
Epoch [100/120    avg_loss:0.100, val_acc:0.972]
Epoch [101/120    avg_loss:0.116, val_acc:0.974]
Epoch [102/120    avg_loss:0.099, val_acc:0.974]
Epoch [103/120    avg_loss:0.108, val_acc:0.974]
Epoch [104/120    avg_loss:0.097, val_acc:0.974]
Epoch [105/120    avg_loss:0.090, val_acc:0.974]
Epoch [106/120    avg_loss:0.089, val_acc:0.974]
Epoch [107/120    avg_loss:0.107, val_acc:0.974]
Epoch [108/120    avg_loss:0.089, val_acc:0.974]
Epoch [109/120    avg_loss:0.094, val_acc:0.974]
Epoch [110/120    avg_loss:0.101, val_acc:0.974]
Epoch [111/120    avg_loss:0.106, val_acc:0.974]
Epoch [112/120    avg_loss:0.096, val_acc:0.974]
Epoch [113/120    avg_loss:0.106, val_acc:0.974]
Epoch [114/120    avg_loss:0.087, val_acc:0.974]
Epoch [115/120    avg_loss:0.089, val_acc:0.974]
Epoch [116/120    avg_loss:0.092, val_acc:0.974]
Epoch [117/120    avg_loss:0.083, val_acc:0.974]
Epoch [118/120    avg_loss:0.112, val_acc:0.974]
Epoch [119/120    avg_loss:0.104, val_acc:0.974]
Epoch [120/120    avg_loss:0.090, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 0.997815   0.94666667 0.98230088 0.89912281 0.87162162
 0.99266504 0.86363636 1.         1.         1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9821940720104632
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4362610ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.652, val_acc:0.131]
Epoch [2/120    avg_loss:2.535, val_acc:0.419]
Epoch [3/120    avg_loss:2.420, val_acc:0.458]
Epoch [4/120    avg_loss:2.332, val_acc:0.518]
Epoch [5/120    avg_loss:2.247, val_acc:0.542]
Epoch [6/120    avg_loss:2.165, val_acc:0.579]
Epoch [7/120    avg_loss:2.066, val_acc:0.623]
Epoch [8/120    avg_loss:1.967, val_acc:0.683]
Epoch [9/120    avg_loss:1.851, val_acc:0.696]
Epoch [10/120    avg_loss:1.741, val_acc:0.685]
Epoch [11/120    avg_loss:1.639, val_acc:0.724]
Epoch [12/120    avg_loss:1.504, val_acc:0.800]
Epoch [13/120    avg_loss:1.397, val_acc:0.806]
Epoch [14/120    avg_loss:1.298, val_acc:0.784]
Epoch [15/120    avg_loss:1.166, val_acc:0.833]
Epoch [16/120    avg_loss:1.078, val_acc:0.837]
Epoch [17/120    avg_loss:0.980, val_acc:0.841]
Epoch [18/120    avg_loss:0.915, val_acc:0.877]
Epoch [19/120    avg_loss:0.846, val_acc:0.889]
Epoch [20/120    avg_loss:0.812, val_acc:0.851]
Epoch [21/120    avg_loss:0.759, val_acc:0.859]
Epoch [22/120    avg_loss:0.706, val_acc:0.875]
Epoch [23/120    avg_loss:0.699, val_acc:0.869]
Epoch [24/120    avg_loss:0.610, val_acc:0.889]
Epoch [25/120    avg_loss:0.606, val_acc:0.871]
Epoch [26/120    avg_loss:0.567, val_acc:0.889]
Epoch [27/120    avg_loss:0.594, val_acc:0.905]
Epoch [28/120    avg_loss:0.607, val_acc:0.885]
Epoch [29/120    avg_loss:0.503, val_acc:0.913]
Epoch [30/120    avg_loss:0.493, val_acc:0.881]
Epoch [31/120    avg_loss:0.516, val_acc:0.917]
Epoch [32/120    avg_loss:0.456, val_acc:0.911]
Epoch [33/120    avg_loss:0.447, val_acc:0.901]
Epoch [34/120    avg_loss:0.480, val_acc:0.915]
Epoch [35/120    avg_loss:0.444, val_acc:0.917]
Epoch [36/120    avg_loss:0.380, val_acc:0.909]
Epoch [37/120    avg_loss:0.457, val_acc:0.935]
Epoch [38/120    avg_loss:0.396, val_acc:0.903]
Epoch [39/120    avg_loss:0.384, val_acc:0.883]
Epoch [40/120    avg_loss:0.418, val_acc:0.923]
Epoch [41/120    avg_loss:0.396, val_acc:0.907]
Epoch [42/120    avg_loss:0.356, val_acc:0.940]
Epoch [43/120    avg_loss:0.327, val_acc:0.915]
Epoch [44/120    avg_loss:0.341, val_acc:0.925]
Epoch [45/120    avg_loss:0.313, val_acc:0.944]
Epoch [46/120    avg_loss:0.304, val_acc:0.942]
Epoch [47/120    avg_loss:0.305, val_acc:0.937]
Epoch [48/120    avg_loss:0.297, val_acc:0.915]
Epoch [49/120    avg_loss:0.314, val_acc:0.903]
Epoch [50/120    avg_loss:0.317, val_acc:0.942]
Epoch [51/120    avg_loss:0.345, val_acc:0.903]
Epoch [52/120    avg_loss:0.292, val_acc:0.944]
Epoch [53/120    avg_loss:0.248, val_acc:0.940]
Epoch [54/120    avg_loss:0.246, val_acc:0.946]
Epoch [55/120    avg_loss:0.241, val_acc:0.938]
Epoch [56/120    avg_loss:0.231, val_acc:0.940]
Epoch [57/120    avg_loss:0.262, val_acc:0.940]
Epoch [58/120    avg_loss:0.236, val_acc:0.942]
Epoch [59/120    avg_loss:0.188, val_acc:0.940]
Epoch [60/120    avg_loss:0.205, val_acc:0.915]
Epoch [61/120    avg_loss:0.231, val_acc:0.944]
Epoch [62/120    avg_loss:0.297, val_acc:0.937]
Epoch [63/120    avg_loss:0.241, val_acc:0.954]
Epoch [64/120    avg_loss:0.187, val_acc:0.948]
Epoch [65/120    avg_loss:0.174, val_acc:0.946]
Epoch [66/120    avg_loss:0.148, val_acc:0.958]
Epoch [67/120    avg_loss:0.180, val_acc:0.937]
Epoch [68/120    avg_loss:0.216, val_acc:0.942]
Epoch [69/120    avg_loss:0.154, val_acc:0.952]
Epoch [70/120    avg_loss:0.158, val_acc:0.968]
Epoch [71/120    avg_loss:0.174, val_acc:0.960]
Epoch [72/120    avg_loss:0.152, val_acc:0.960]
Epoch [73/120    avg_loss:0.117, val_acc:0.968]
Epoch [74/120    avg_loss:0.100, val_acc:0.962]
Epoch [75/120    avg_loss:0.118, val_acc:0.968]
Epoch [76/120    avg_loss:0.116, val_acc:0.960]
Epoch [77/120    avg_loss:0.154, val_acc:0.964]
Epoch [78/120    avg_loss:0.170, val_acc:0.970]
Epoch [79/120    avg_loss:0.090, val_acc:0.976]
Epoch [80/120    avg_loss:0.079, val_acc:0.976]
Epoch [81/120    avg_loss:0.128, val_acc:0.938]
Epoch [82/120    avg_loss:0.131, val_acc:0.948]
Epoch [83/120    avg_loss:0.198, val_acc:0.962]
Epoch [84/120    avg_loss:0.201, val_acc:0.954]
Epoch [85/120    avg_loss:0.173, val_acc:0.964]
Epoch [86/120    avg_loss:0.148, val_acc:0.956]
Epoch [87/120    avg_loss:0.105, val_acc:0.960]
Epoch [88/120    avg_loss:0.100, val_acc:0.968]
Epoch [89/120    avg_loss:0.106, val_acc:0.954]
Epoch [90/120    avg_loss:0.105, val_acc:0.958]
Epoch [91/120    avg_loss:0.127, val_acc:0.958]
Epoch [92/120    avg_loss:0.119, val_acc:0.954]
Epoch [93/120    avg_loss:0.095, val_acc:0.982]
Epoch [94/120    avg_loss:0.120, val_acc:0.974]
Epoch [95/120    avg_loss:0.077, val_acc:0.984]
Epoch [96/120    avg_loss:0.105, val_acc:0.970]
Epoch [97/120    avg_loss:0.090, val_acc:0.976]
Epoch [98/120    avg_loss:0.078, val_acc:0.974]
Epoch [99/120    avg_loss:0.069, val_acc:0.970]
Epoch [100/120    avg_loss:0.065, val_acc:0.972]
Epoch [101/120    avg_loss:0.094, val_acc:0.972]
Epoch [102/120    avg_loss:0.179, val_acc:0.950]
Epoch [103/120    avg_loss:0.181, val_acc:0.948]
Epoch [104/120    avg_loss:0.113, val_acc:0.964]
Epoch [105/120    avg_loss:0.146, val_acc:0.958]
Epoch [106/120    avg_loss:0.119, val_acc:0.950]
Epoch [107/120    avg_loss:0.098, val_acc:0.964]
Epoch [108/120    avg_loss:0.164, val_acc:0.962]
Epoch [109/120    avg_loss:0.128, val_acc:0.978]
Epoch [110/120    avg_loss:0.088, val_acc:0.970]
Epoch [111/120    avg_loss:0.069, val_acc:0.972]
Epoch [112/120    avg_loss:0.080, val_acc:0.974]
Epoch [113/120    avg_loss:0.069, val_acc:0.974]
Epoch [114/120    avg_loss:0.058, val_acc:0.976]
Epoch [115/120    avg_loss:0.049, val_acc:0.974]
Epoch [116/120    avg_loss:0.055, val_acc:0.976]
Epoch [117/120    avg_loss:0.057, val_acc:0.976]
Epoch [118/120    avg_loss:0.052, val_acc:0.976]
Epoch [119/120    avg_loss:0.047, val_acc:0.976]
Epoch [120/120    avg_loss:0.043, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   5   0   0   3   0   0   0   0   0   0]
 [  0   0   1 214   6   0   0   0   6   3   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  28 117   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 449   3]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.99573560767591

F1 scores:
[       nan 0.99708879 0.93777778 0.96396396 0.88935282 0.84782609
 0.99019608 0.89772727 0.98714653 0.99680511 1.         0.99734748
 0.9944629  0.99820467]

Kappa:
0.9776785691682338
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f682d235a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.617, val_acc:0.353]
Epoch [2/120    avg_loss:2.466, val_acc:0.345]
Epoch [3/120    avg_loss:2.343, val_acc:0.323]
Epoch [4/120    avg_loss:2.227, val_acc:0.357]
Epoch [5/120    avg_loss:2.119, val_acc:0.433]
Epoch [6/120    avg_loss:2.040, val_acc:0.516]
Epoch [7/120    avg_loss:1.953, val_acc:0.554]
Epoch [8/120    avg_loss:1.866, val_acc:0.532]
Epoch [9/120    avg_loss:1.769, val_acc:0.587]
Epoch [10/120    avg_loss:1.696, val_acc:0.627]
Epoch [11/120    avg_loss:1.583, val_acc:0.665]
Epoch [12/120    avg_loss:1.499, val_acc:0.708]
Epoch [13/120    avg_loss:1.411, val_acc:0.750]
Epoch [14/120    avg_loss:1.331, val_acc:0.700]
Epoch [15/120    avg_loss:1.236, val_acc:0.806]
Epoch [16/120    avg_loss:1.127, val_acc:0.829]
Epoch [17/120    avg_loss:1.021, val_acc:0.746]
Epoch [18/120    avg_loss:0.959, val_acc:0.861]
Epoch [19/120    avg_loss:0.847, val_acc:0.855]
Epoch [20/120    avg_loss:0.800, val_acc:0.851]
Epoch [21/120    avg_loss:0.721, val_acc:0.875]
Epoch [22/120    avg_loss:0.681, val_acc:0.875]
Epoch [23/120    avg_loss:0.642, val_acc:0.869]
Epoch [24/120    avg_loss:0.662, val_acc:0.895]
Epoch [25/120    avg_loss:0.565, val_acc:0.893]
Epoch [26/120    avg_loss:0.520, val_acc:0.895]
Epoch [27/120    avg_loss:0.520, val_acc:0.897]
Epoch [28/120    avg_loss:0.559, val_acc:0.899]
Epoch [29/120    avg_loss:0.505, val_acc:0.895]
Epoch [30/120    avg_loss:0.469, val_acc:0.905]
Epoch [31/120    avg_loss:0.428, val_acc:0.905]
Epoch [32/120    avg_loss:0.454, val_acc:0.855]
Epoch [33/120    avg_loss:0.565, val_acc:0.867]
Epoch [34/120    avg_loss:0.446, val_acc:0.899]
Epoch [35/120    avg_loss:0.470, val_acc:0.893]
Epoch [36/120    avg_loss:0.450, val_acc:0.905]
Epoch [37/120    avg_loss:0.376, val_acc:0.907]
Epoch [38/120    avg_loss:0.362, val_acc:0.885]
Epoch [39/120    avg_loss:0.421, val_acc:0.835]
Epoch [40/120    avg_loss:0.383, val_acc:0.895]
Epoch [41/120    avg_loss:0.382, val_acc:0.915]
Epoch [42/120    avg_loss:0.331, val_acc:0.907]
Epoch [43/120    avg_loss:0.332, val_acc:0.917]
Epoch [44/120    avg_loss:0.322, val_acc:0.927]
Epoch [45/120    avg_loss:0.306, val_acc:0.937]
Epoch [46/120    avg_loss:0.286, val_acc:0.937]
Epoch [47/120    avg_loss:0.287, val_acc:0.935]
Epoch [48/120    avg_loss:0.288, val_acc:0.938]
Epoch [49/120    avg_loss:0.269, val_acc:0.950]
Epoch [50/120    avg_loss:0.273, val_acc:0.940]
Epoch [51/120    avg_loss:0.269, val_acc:0.921]
Epoch [52/120    avg_loss:0.252, val_acc:0.938]
Epoch [53/120    avg_loss:0.241, val_acc:0.946]
Epoch [54/120    avg_loss:0.254, val_acc:0.885]
Epoch [55/120    avg_loss:0.244, val_acc:0.950]
Epoch [56/120    avg_loss:0.267, val_acc:0.940]
Epoch [57/120    avg_loss:0.258, val_acc:0.925]
Epoch [58/120    avg_loss:0.239, val_acc:0.952]
Epoch [59/120    avg_loss:0.218, val_acc:0.942]
Epoch [60/120    avg_loss:0.195, val_acc:0.952]
Epoch [61/120    avg_loss:0.156, val_acc:0.962]
Epoch [62/120    avg_loss:0.180, val_acc:0.937]
Epoch [63/120    avg_loss:0.232, val_acc:0.952]
Epoch [64/120    avg_loss:0.195, val_acc:0.931]
Epoch [65/120    avg_loss:0.178, val_acc:0.944]
Epoch [66/120    avg_loss:0.192, val_acc:0.960]
Epoch [67/120    avg_loss:0.167, val_acc:0.950]
Epoch [68/120    avg_loss:0.172, val_acc:0.956]
Epoch [69/120    avg_loss:0.167, val_acc:0.950]
Epoch [70/120    avg_loss:0.188, val_acc:0.952]
Epoch [71/120    avg_loss:0.224, val_acc:0.942]
Epoch [72/120    avg_loss:0.206, val_acc:0.954]
Epoch [73/120    avg_loss:0.161, val_acc:0.952]
Epoch [74/120    avg_loss:0.121, val_acc:0.960]
Epoch [75/120    avg_loss:0.114, val_acc:0.962]
Epoch [76/120    avg_loss:0.096, val_acc:0.964]
Epoch [77/120    avg_loss:0.096, val_acc:0.964]
Epoch [78/120    avg_loss:0.086, val_acc:0.968]
Epoch [79/120    avg_loss:0.106, val_acc:0.964]
Epoch [80/120    avg_loss:0.095, val_acc:0.968]
Epoch [81/120    avg_loss:0.087, val_acc:0.968]
Epoch [82/120    avg_loss:0.088, val_acc:0.970]
Epoch [83/120    avg_loss:0.088, val_acc:0.972]
Epoch [84/120    avg_loss:0.091, val_acc:0.970]
Epoch [85/120    avg_loss:0.088, val_acc:0.970]
Epoch [86/120    avg_loss:0.084, val_acc:0.972]
Epoch [87/120    avg_loss:0.089, val_acc:0.970]
Epoch [88/120    avg_loss:0.092, val_acc:0.968]
Epoch [89/120    avg_loss:0.079, val_acc:0.974]
Epoch [90/120    avg_loss:0.078, val_acc:0.974]
Epoch [91/120    avg_loss:0.086, val_acc:0.972]
Epoch [92/120    avg_loss:0.072, val_acc:0.968]
Epoch [93/120    avg_loss:0.088, val_acc:0.974]
Epoch [94/120    avg_loss:0.083, val_acc:0.970]
Epoch [95/120    avg_loss:0.081, val_acc:0.972]
Epoch [96/120    avg_loss:0.077, val_acc:0.972]
Epoch [97/120    avg_loss:0.073, val_acc:0.976]
Epoch [98/120    avg_loss:0.075, val_acc:0.974]
Epoch [99/120    avg_loss:0.072, val_acc:0.970]
Epoch [100/120    avg_loss:0.073, val_acc:0.972]
Epoch [101/120    avg_loss:0.077, val_acc:0.972]
Epoch [102/120    avg_loss:0.080, val_acc:0.974]
Epoch [103/120    avg_loss:0.081, val_acc:0.974]
Epoch [104/120    avg_loss:0.073, val_acc:0.970]
Epoch [105/120    avg_loss:0.089, val_acc:0.972]
Epoch [106/120    avg_loss:0.089, val_acc:0.972]
Epoch [107/120    avg_loss:0.083, val_acc:0.970]
Epoch [108/120    avg_loss:0.068, val_acc:0.976]
Epoch [109/120    avg_loss:0.072, val_acc:0.972]
Epoch [110/120    avg_loss:0.075, val_acc:0.974]
Epoch [111/120    avg_loss:0.082, val_acc:0.976]
Epoch [112/120    avg_loss:0.065, val_acc:0.974]
Epoch [113/120    avg_loss:0.066, val_acc:0.976]
Epoch [114/120    avg_loss:0.065, val_acc:0.974]
Epoch [115/120    avg_loss:0.068, val_acc:0.972]
Epoch [116/120    avg_loss:0.066, val_acc:0.972]
Epoch [117/120    avg_loss:0.077, val_acc:0.972]
Epoch [118/120    avg_loss:0.079, val_acc:0.972]
Epoch [119/120    avg_loss:0.071, val_acc:0.974]
Epoch [120/120    avg_loss:0.062, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 206   0   0   0   0  13   0   0   0   0   0   0]
 [  0   0   0 218   6   0   0   0   6   0   0   0   0   0]
 [  0   0   1   1 202  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 1.         0.93636364 0.97104677 0.90786517 0.89108911
 1.         0.85561497 0.99232737 1.         0.99862826 1.
 0.99889503 1.        ]

Kappa:
0.9821964855077874
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6361e1ab00>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.588, val_acc:0.230]
Epoch [2/120    avg_loss:2.477, val_acc:0.458]
Epoch [3/120    avg_loss:2.379, val_acc:0.502]
Epoch [4/120    avg_loss:2.302, val_acc:0.504]
Epoch [5/120    avg_loss:2.240, val_acc:0.496]
Epoch [6/120    avg_loss:2.164, val_acc:0.508]
Epoch [7/120    avg_loss:2.074, val_acc:0.573]
Epoch [8/120    avg_loss:1.995, val_acc:0.613]
Epoch [9/120    avg_loss:1.906, val_acc:0.643]
Epoch [10/120    avg_loss:1.840, val_acc:0.667]
Epoch [11/120    avg_loss:1.759, val_acc:0.669]
Epoch [12/120    avg_loss:1.690, val_acc:0.685]
Epoch [13/120    avg_loss:1.567, val_acc:0.698]
Epoch [14/120    avg_loss:1.472, val_acc:0.681]
Epoch [15/120    avg_loss:1.381, val_acc:0.683]
Epoch [16/120    avg_loss:1.296, val_acc:0.730]
Epoch [17/120    avg_loss:1.207, val_acc:0.768]
Epoch [18/120    avg_loss:1.140, val_acc:0.734]
Epoch [19/120    avg_loss:1.103, val_acc:0.792]
Epoch [20/120    avg_loss:1.022, val_acc:0.825]
Epoch [21/120    avg_loss:0.989, val_acc:0.855]
Epoch [22/120    avg_loss:0.894, val_acc:0.847]
Epoch [23/120    avg_loss:0.843, val_acc:0.899]
Epoch [24/120    avg_loss:0.754, val_acc:0.903]
Epoch [25/120    avg_loss:0.811, val_acc:0.863]
Epoch [26/120    avg_loss:0.701, val_acc:0.911]
Epoch [27/120    avg_loss:0.646, val_acc:0.881]
Epoch [28/120    avg_loss:0.623, val_acc:0.905]
Epoch [29/120    avg_loss:0.619, val_acc:0.891]
Epoch [30/120    avg_loss:0.570, val_acc:0.889]
Epoch [31/120    avg_loss:0.545, val_acc:0.881]
Epoch [32/120    avg_loss:0.545, val_acc:0.911]
Epoch [33/120    avg_loss:0.502, val_acc:0.897]
Epoch [34/120    avg_loss:0.470, val_acc:0.931]
Epoch [35/120    avg_loss:0.433, val_acc:0.927]
Epoch [36/120    avg_loss:0.412, val_acc:0.925]
Epoch [37/120    avg_loss:0.406, val_acc:0.940]
Epoch [38/120    avg_loss:0.358, val_acc:0.931]
Epoch [39/120    avg_loss:0.376, val_acc:0.911]
Epoch [40/120    avg_loss:0.445, val_acc:0.907]
Epoch [41/120    avg_loss:0.371, val_acc:0.925]
Epoch [42/120    avg_loss:0.381, val_acc:0.923]
Epoch [43/120    avg_loss:0.375, val_acc:0.927]
Epoch [44/120    avg_loss:0.376, val_acc:0.940]
Epoch [45/120    avg_loss:0.312, val_acc:0.966]
Epoch [46/120    avg_loss:0.263, val_acc:0.942]
Epoch [47/120    avg_loss:0.267, val_acc:0.927]
Epoch [48/120    avg_loss:0.309, val_acc:0.960]
Epoch [49/120    avg_loss:0.310, val_acc:0.946]
Epoch [50/120    avg_loss:0.280, val_acc:0.944]
Epoch [51/120    avg_loss:0.254, val_acc:0.933]
Epoch [52/120    avg_loss:0.241, val_acc:0.968]
Epoch [53/120    avg_loss:0.226, val_acc:0.956]
Epoch [54/120    avg_loss:0.239, val_acc:0.962]
Epoch [55/120    avg_loss:0.203, val_acc:0.935]
Epoch [56/120    avg_loss:0.203, val_acc:0.970]
Epoch [57/120    avg_loss:0.197, val_acc:0.935]
Epoch [58/120    avg_loss:0.232, val_acc:0.923]
Epoch [59/120    avg_loss:0.224, val_acc:0.962]
Epoch [60/120    avg_loss:0.162, val_acc:0.972]
Epoch [61/120    avg_loss:0.202, val_acc:0.966]
Epoch [62/120    avg_loss:0.236, val_acc:0.980]
Epoch [63/120    avg_loss:0.168, val_acc:0.966]
Epoch [64/120    avg_loss:0.145, val_acc:0.984]
Epoch [65/120    avg_loss:0.143, val_acc:0.958]
Epoch [66/120    avg_loss:0.181, val_acc:0.966]
Epoch [67/120    avg_loss:0.210, val_acc:0.978]
Epoch [68/120    avg_loss:0.169, val_acc:0.972]
Epoch [69/120    avg_loss:0.145, val_acc:0.968]
Epoch [70/120    avg_loss:0.138, val_acc:0.976]
Epoch [71/120    avg_loss:0.147, val_acc:0.958]
Epoch [72/120    avg_loss:0.189, val_acc:0.946]
Epoch [73/120    avg_loss:0.189, val_acc:0.954]
Epoch [74/120    avg_loss:0.189, val_acc:0.958]
Epoch [75/120    avg_loss:0.204, val_acc:0.972]
Epoch [76/120    avg_loss:0.139, val_acc:0.970]
Epoch [77/120    avg_loss:0.135, val_acc:0.944]
Epoch [78/120    avg_loss:0.123, val_acc:0.960]
Epoch [79/120    avg_loss:0.091, val_acc:0.978]
Epoch [80/120    avg_loss:0.095, val_acc:0.980]
Epoch [81/120    avg_loss:0.086, val_acc:0.984]
Epoch [82/120    avg_loss:0.084, val_acc:0.984]
Epoch [83/120    avg_loss:0.083, val_acc:0.984]
Epoch [84/120    avg_loss:0.080, val_acc:0.982]
Epoch [85/120    avg_loss:0.080, val_acc:0.982]
Epoch [86/120    avg_loss:0.084, val_acc:0.980]
Epoch [87/120    avg_loss:0.074, val_acc:0.986]
Epoch [88/120    avg_loss:0.086, val_acc:0.984]
Epoch [89/120    avg_loss:0.084, val_acc:0.984]
Epoch [90/120    avg_loss:0.078, val_acc:0.980]
Epoch [91/120    avg_loss:0.066, val_acc:0.986]
Epoch [92/120    avg_loss:0.082, val_acc:0.984]
Epoch [93/120    avg_loss:0.075, val_acc:0.984]
Epoch [94/120    avg_loss:0.075, val_acc:0.984]
Epoch [95/120    avg_loss:0.065, val_acc:0.984]
Epoch [96/120    avg_loss:0.070, val_acc:0.982]
Epoch [97/120    avg_loss:0.067, val_acc:0.982]
Epoch [98/120    avg_loss:0.070, val_acc:0.982]
Epoch [99/120    avg_loss:0.063, val_acc:0.982]
Epoch [100/120    avg_loss:0.068, val_acc:0.988]
Epoch [101/120    avg_loss:0.055, val_acc:0.982]
Epoch [102/120    avg_loss:0.058, val_acc:0.982]
Epoch [103/120    avg_loss:0.059, val_acc:0.982]
Epoch [104/120    avg_loss:0.057, val_acc:0.982]
Epoch [105/120    avg_loss:0.071, val_acc:0.984]
Epoch [106/120    avg_loss:0.061, val_acc:0.984]
Epoch [107/120    avg_loss:0.062, val_acc:0.984]
Epoch [108/120    avg_loss:0.062, val_acc:0.984]
Epoch [109/120    avg_loss:0.062, val_acc:0.982]
Epoch [110/120    avg_loss:0.062, val_acc:0.982]
Epoch [111/120    avg_loss:0.054, val_acc:0.984]
Epoch [112/120    avg_loss:0.076, val_acc:0.984]
Epoch [113/120    avg_loss:0.057, val_acc:0.976]
Epoch [114/120    avg_loss:0.058, val_acc:0.976]
Epoch [115/120    avg_loss:0.075, val_acc:0.976]
Epoch [116/120    avg_loss:0.058, val_acc:0.976]
Epoch [117/120    avg_loss:0.068, val_acc:0.978]
Epoch [118/120    avg_loss:0.061, val_acc:0.980]
Epoch [119/120    avg_loss:0.065, val_acc:0.980]
Epoch [120/120    avg_loss:0.059, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 225   2   0   0   0   0   3   0   0   0   0]
 [  0   0   1   0 208  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   7   0   0   0   0 199   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 373   4   0]
 [  0   0   0   0   0   0   0   0   3   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 0.99491649 0.93905192 0.98901099 0.92857143 0.9023569
 0.98271605 0.85869565 0.99614891 0.99680511 1.         0.99466667
 0.99228225 1.        ]

Kappa:
0.9821927063552724
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1f80daa20>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.274]
Epoch [2/120    avg_loss:2.490, val_acc:0.288]
Epoch [3/120    avg_loss:2.397, val_acc:0.317]
Epoch [4/120    avg_loss:2.327, val_acc:0.313]
Epoch [5/120    avg_loss:2.256, val_acc:0.409]
Epoch [6/120    avg_loss:2.186, val_acc:0.480]
Epoch [7/120    avg_loss:2.098, val_acc:0.506]
Epoch [8/120    avg_loss:2.012, val_acc:0.577]
Epoch [9/120    avg_loss:1.929, val_acc:0.579]
Epoch [10/120    avg_loss:1.846, val_acc:0.599]
Epoch [11/120    avg_loss:1.767, val_acc:0.603]
Epoch [12/120    avg_loss:1.680, val_acc:0.641]
Epoch [13/120    avg_loss:1.593, val_acc:0.641]
Epoch [14/120    avg_loss:1.527, val_acc:0.683]
Epoch [15/120    avg_loss:1.438, val_acc:0.694]
Epoch [16/120    avg_loss:1.338, val_acc:0.716]
Epoch [17/120    avg_loss:1.262, val_acc:0.718]
Epoch [18/120    avg_loss:1.196, val_acc:0.788]
Epoch [19/120    avg_loss:1.089, val_acc:0.790]
Epoch [20/120    avg_loss:1.044, val_acc:0.782]
Epoch [21/120    avg_loss:0.937, val_acc:0.857]
Epoch [22/120    avg_loss:0.866, val_acc:0.893]
Epoch [23/120    avg_loss:0.832, val_acc:0.855]
Epoch [24/120    avg_loss:0.767, val_acc:0.867]
Epoch [25/120    avg_loss:0.682, val_acc:0.871]
Epoch [26/120    avg_loss:0.691, val_acc:0.893]
Epoch [27/120    avg_loss:0.586, val_acc:0.901]
Epoch [28/120    avg_loss:0.623, val_acc:0.895]
Epoch [29/120    avg_loss:0.550, val_acc:0.863]
Epoch [30/120    avg_loss:0.579, val_acc:0.915]
Epoch [31/120    avg_loss:0.560, val_acc:0.921]
Epoch [32/120    avg_loss:0.521, val_acc:0.887]
Epoch [33/120    avg_loss:0.430, val_acc:0.942]
Epoch [34/120    avg_loss:0.405, val_acc:0.921]
Epoch [35/120    avg_loss:0.363, val_acc:0.915]
Epoch [36/120    avg_loss:0.431, val_acc:0.883]
Epoch [37/120    avg_loss:0.379, val_acc:0.919]
Epoch [38/120    avg_loss:0.339, val_acc:0.938]
Epoch [39/120    avg_loss:0.312, val_acc:0.897]
Epoch [40/120    avg_loss:0.304, val_acc:0.925]
Epoch [41/120    avg_loss:0.308, val_acc:0.903]
Epoch [42/120    avg_loss:0.397, val_acc:0.913]
Epoch [43/120    avg_loss:0.351, val_acc:0.935]
Epoch [44/120    avg_loss:0.294, val_acc:0.925]
Epoch [45/120    avg_loss:0.320, val_acc:0.905]
Epoch [46/120    avg_loss:0.335, val_acc:0.905]
Epoch [47/120    avg_loss:0.311, val_acc:0.946]
Epoch [48/120    avg_loss:0.257, val_acc:0.962]
Epoch [49/120    avg_loss:0.236, val_acc:0.960]
Epoch [50/120    avg_loss:0.242, val_acc:0.960]
Epoch [51/120    avg_loss:0.227, val_acc:0.956]
Epoch [52/120    avg_loss:0.224, val_acc:0.956]
Epoch [53/120    avg_loss:0.222, val_acc:0.958]
Epoch [54/120    avg_loss:0.219, val_acc:0.958]
Epoch [55/120    avg_loss:0.215, val_acc:0.960]
Epoch [56/120    avg_loss:0.196, val_acc:0.964]
Epoch [57/120    avg_loss:0.211, val_acc:0.962]
Epoch [58/120    avg_loss:0.199, val_acc:0.962]
Epoch [59/120    avg_loss:0.211, val_acc:0.958]
Epoch [60/120    avg_loss:0.181, val_acc:0.962]
Epoch [61/120    avg_loss:0.187, val_acc:0.962]
Epoch [62/120    avg_loss:0.177, val_acc:0.962]
Epoch [63/120    avg_loss:0.182, val_acc:0.962]
Epoch [64/120    avg_loss:0.205, val_acc:0.962]
Epoch [65/120    avg_loss:0.209, val_acc:0.960]
Epoch [66/120    avg_loss:0.183, val_acc:0.960]
Epoch [67/120    avg_loss:0.185, val_acc:0.962]
Epoch [68/120    avg_loss:0.168, val_acc:0.960]
Epoch [69/120    avg_loss:0.181, val_acc:0.960]
Epoch [70/120    avg_loss:0.182, val_acc:0.960]
Epoch [71/120    avg_loss:0.165, val_acc:0.958]
Epoch [72/120    avg_loss:0.190, val_acc:0.958]
Epoch [73/120    avg_loss:0.182, val_acc:0.962]
Epoch [74/120    avg_loss:0.165, val_acc:0.960]
Epoch [75/120    avg_loss:0.207, val_acc:0.960]
Epoch [76/120    avg_loss:0.169, val_acc:0.960]
Epoch [77/120    avg_loss:0.178, val_acc:0.960]
Epoch [78/120    avg_loss:0.187, val_acc:0.962]
Epoch [79/120    avg_loss:0.171, val_acc:0.960]
Epoch [80/120    avg_loss:0.176, val_acc:0.960]
Epoch [81/120    avg_loss:0.169, val_acc:0.960]
Epoch [82/120    avg_loss:0.163, val_acc:0.960]
Epoch [83/120    avg_loss:0.172, val_acc:0.960]
Epoch [84/120    avg_loss:0.172, val_acc:0.960]
Epoch [85/120    avg_loss:0.184, val_acc:0.960]
Epoch [86/120    avg_loss:0.172, val_acc:0.960]
Epoch [87/120    avg_loss:0.186, val_acc:0.960]
Epoch [88/120    avg_loss:0.160, val_acc:0.960]
Epoch [89/120    avg_loss:0.181, val_acc:0.960]
Epoch [90/120    avg_loss:0.162, val_acc:0.960]
Epoch [91/120    avg_loss:0.188, val_acc:0.960]
Epoch [92/120    avg_loss:0.172, val_acc:0.960]
Epoch [93/120    avg_loss:0.168, val_acc:0.960]
Epoch [94/120    avg_loss:0.192, val_acc:0.960]
Epoch [95/120    avg_loss:0.164, val_acc:0.960]
Epoch [96/120    avg_loss:0.180, val_acc:0.960]
Epoch [97/120    avg_loss:0.175, val_acc:0.960]
Epoch [98/120    avg_loss:0.169, val_acc:0.960]
Epoch [99/120    avg_loss:0.181, val_acc:0.960]
Epoch [100/120    avg_loss:0.171, val_acc:0.960]
Epoch [101/120    avg_loss:0.167, val_acc:0.960]
Epoch [102/120    avg_loss:0.165, val_acc:0.960]
Epoch [103/120    avg_loss:0.153, val_acc:0.960]
Epoch [104/120    avg_loss:0.174, val_acc:0.960]
Epoch [105/120    avg_loss:0.179, val_acc:0.960]
Epoch [106/120    avg_loss:0.190, val_acc:0.960]
Epoch [107/120    avg_loss:0.162, val_acc:0.960]
Epoch [108/120    avg_loss:0.180, val_acc:0.960]
Epoch [109/120    avg_loss:0.177, val_acc:0.960]
Epoch [110/120    avg_loss:0.170, val_acc:0.960]
Epoch [111/120    avg_loss:0.171, val_acc:0.960]
Epoch [112/120    avg_loss:0.176, val_acc:0.960]
Epoch [113/120    avg_loss:0.171, val_acc:0.960]
Epoch [114/120    avg_loss:0.176, val_acc:0.960]
Epoch [115/120    avg_loss:0.179, val_acc:0.960]
Epoch [116/120    avg_loss:0.163, val_acc:0.960]
Epoch [117/120    avg_loss:0.186, val_acc:0.960]
Epoch [118/120    avg_loss:0.183, val_acc:0.960]
Epoch [119/120    avg_loss:0.171, val_acc:0.960]
Epoch [120/120    avg_loss:0.165, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   1 223   0   0   0   0   4   2   0   0   0   0]
 [  0   0   1   0 174  49   0   0   0   0   0   0   3   0]
 [  0   0   0   0  48  97   0   0   0   0   0   0   0   0]
 [  0   2   0   0  11   0 193   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 373   4   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.78038379530916

F1 scores:
[       nan 0.99854227 0.9375     0.98454746 0.75652174 0.66666667
 0.96741855 0.85555556 0.99487179 0.9978678  1.         0.99466667
 0.99233297 1.        ]

Kappa:
0.9641478187775742
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f59c3dc2a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.587, val_acc:0.121]
Epoch [2/120    avg_loss:2.474, val_acc:0.278]
Epoch [3/120    avg_loss:2.396, val_acc:0.292]
Epoch [4/120    avg_loss:2.323, val_acc:0.315]
Epoch [5/120    avg_loss:2.249, val_acc:0.381]
Epoch [6/120    avg_loss:2.168, val_acc:0.415]
Epoch [7/120    avg_loss:2.084, val_acc:0.466]
Epoch [8/120    avg_loss:2.019, val_acc:0.502]
Epoch [9/120    avg_loss:1.906, val_acc:0.524]
Epoch [10/120    avg_loss:1.821, val_acc:0.567]
Epoch [11/120    avg_loss:1.736, val_acc:0.675]
Epoch [12/120    avg_loss:1.642, val_acc:0.683]
Epoch [13/120    avg_loss:1.562, val_acc:0.764]
Epoch [14/120    avg_loss:1.455, val_acc:0.780]
Epoch [15/120    avg_loss:1.363, val_acc:0.796]
Epoch [16/120    avg_loss:1.269, val_acc:0.760]
Epoch [17/120    avg_loss:1.209, val_acc:0.829]
Epoch [18/120    avg_loss:1.125, val_acc:0.843]
Epoch [19/120    avg_loss:1.073, val_acc:0.831]
Epoch [20/120    avg_loss:0.939, val_acc:0.857]
Epoch [21/120    avg_loss:0.877, val_acc:0.875]
Epoch [22/120    avg_loss:0.817, val_acc:0.893]
Epoch [23/120    avg_loss:0.743, val_acc:0.885]
Epoch [24/120    avg_loss:0.712, val_acc:0.895]
Epoch [25/120    avg_loss:0.662, val_acc:0.871]
Epoch [26/120    avg_loss:0.640, val_acc:0.839]
Epoch [27/120    avg_loss:0.629, val_acc:0.798]
Epoch [28/120    avg_loss:0.587, val_acc:0.905]
Epoch [29/120    avg_loss:0.576, val_acc:0.893]
Epoch [30/120    avg_loss:0.567, val_acc:0.833]
Epoch [31/120    avg_loss:0.549, val_acc:0.915]
Epoch [32/120    avg_loss:0.491, val_acc:0.895]
Epoch [33/120    avg_loss:0.469, val_acc:0.905]
Epoch [34/120    avg_loss:0.458, val_acc:0.911]
Epoch [35/120    avg_loss:0.511, val_acc:0.913]
Epoch [36/120    avg_loss:0.474, val_acc:0.921]
Epoch [37/120    avg_loss:0.433, val_acc:0.921]
Epoch [38/120    avg_loss:0.434, val_acc:0.877]
Epoch [39/120    avg_loss:0.418, val_acc:0.919]
Epoch [40/120    avg_loss:0.358, val_acc:0.897]
Epoch [41/120    avg_loss:0.362, val_acc:0.927]
Epoch [42/120    avg_loss:0.361, val_acc:0.911]
Epoch [43/120    avg_loss:0.407, val_acc:0.919]
Epoch [44/120    avg_loss:0.334, val_acc:0.903]
Epoch [45/120    avg_loss:0.362, val_acc:0.925]
Epoch [46/120    avg_loss:0.366, val_acc:0.899]
Epoch [47/120    avg_loss:0.345, val_acc:0.927]
Epoch [48/120    avg_loss:0.291, val_acc:0.915]
Epoch [49/120    avg_loss:0.302, val_acc:0.911]
Epoch [50/120    avg_loss:0.295, val_acc:0.913]
Epoch [51/120    avg_loss:0.319, val_acc:0.931]
Epoch [52/120    avg_loss:0.375, val_acc:0.909]
Epoch [53/120    avg_loss:0.324, val_acc:0.933]
Epoch [54/120    avg_loss:0.249, val_acc:0.931]
Epoch [55/120    avg_loss:0.251, val_acc:0.944]
Epoch [56/120    avg_loss:0.245, val_acc:0.942]
Epoch [57/120    avg_loss:0.228, val_acc:0.931]
Epoch [58/120    avg_loss:0.217, val_acc:0.935]
Epoch [59/120    avg_loss:0.308, val_acc:0.935]
Epoch [60/120    avg_loss:0.257, val_acc:0.938]
Epoch [61/120    avg_loss:0.235, val_acc:0.929]
Epoch [62/120    avg_loss:0.287, val_acc:0.938]
Epoch [63/120    avg_loss:0.233, val_acc:0.940]
Epoch [64/120    avg_loss:0.188, val_acc:0.946]
Epoch [65/120    avg_loss:0.183, val_acc:0.935]
Epoch [66/120    avg_loss:0.233, val_acc:0.954]
Epoch [67/120    avg_loss:0.199, val_acc:0.972]
Epoch [68/120    avg_loss:0.213, val_acc:0.962]
Epoch [69/120    avg_loss:0.146, val_acc:0.962]
Epoch [70/120    avg_loss:0.144, val_acc:0.950]
Epoch [71/120    avg_loss:0.141, val_acc:0.944]
Epoch [72/120    avg_loss:0.180, val_acc:0.948]
Epoch [73/120    avg_loss:0.163, val_acc:0.964]
Epoch [74/120    avg_loss:0.146, val_acc:0.958]
Epoch [75/120    avg_loss:0.270, val_acc:0.948]
Epoch [76/120    avg_loss:0.208, val_acc:0.946]
Epoch [77/120    avg_loss:0.181, val_acc:0.950]
Epoch [78/120    avg_loss:0.172, val_acc:0.966]
Epoch [79/120    avg_loss:0.153, val_acc:0.968]
Epoch [80/120    avg_loss:0.173, val_acc:0.944]
Epoch [81/120    avg_loss:0.173, val_acc:0.972]
Epoch [82/120    avg_loss:0.104, val_acc:0.976]
Epoch [83/120    avg_loss:0.095, val_acc:0.982]
Epoch [84/120    avg_loss:0.100, val_acc:0.980]
Epoch [85/120    avg_loss:0.088, val_acc:0.982]
Epoch [86/120    avg_loss:0.092, val_acc:0.980]
Epoch [87/120    avg_loss:0.095, val_acc:0.972]
Epoch [88/120    avg_loss:0.094, val_acc:0.976]
Epoch [89/120    avg_loss:0.087, val_acc:0.974]
Epoch [90/120    avg_loss:0.087, val_acc:0.976]
Epoch [91/120    avg_loss:0.095, val_acc:0.974]
Epoch [92/120    avg_loss:0.079, val_acc:0.976]
Epoch [93/120    avg_loss:0.078, val_acc:0.978]
Epoch [94/120    avg_loss:0.093, val_acc:0.980]
Epoch [95/120    avg_loss:0.075, val_acc:0.982]
Epoch [96/120    avg_loss:0.100, val_acc:0.980]
Epoch [97/120    avg_loss:0.077, val_acc:0.982]
Epoch [98/120    avg_loss:0.077, val_acc:0.976]
Epoch [99/120    avg_loss:0.080, val_acc:0.980]
Epoch [100/120    avg_loss:0.075, val_acc:0.982]
Epoch [101/120    avg_loss:0.080, val_acc:0.980]
Epoch [102/120    avg_loss:0.079, val_acc:0.980]
Epoch [103/120    avg_loss:0.069, val_acc:0.982]
Epoch [104/120    avg_loss:0.072, val_acc:0.982]
Epoch [105/120    avg_loss:0.079, val_acc:0.982]
Epoch [106/120    avg_loss:0.068, val_acc:0.982]
Epoch [107/120    avg_loss:0.068, val_acc:0.982]
Epoch [108/120    avg_loss:0.075, val_acc:0.980]
Epoch [109/120    avg_loss:0.075, val_acc:0.982]
Epoch [110/120    avg_loss:0.073, val_acc:0.982]
Epoch [111/120    avg_loss:0.071, val_acc:0.980]
Epoch [112/120    avg_loss:0.101, val_acc:0.982]
Epoch [113/120    avg_loss:0.074, val_acc:0.978]
Epoch [114/120    avg_loss:0.079, val_acc:0.978]
Epoch [115/120    avg_loss:0.068, val_acc:0.978]
Epoch [116/120    avg_loss:0.076, val_acc:0.980]
Epoch [117/120    avg_loss:0.064, val_acc:0.980]
Epoch [118/120    avg_loss:0.079, val_acc:0.980]
Epoch [119/120    avg_loss:0.082, val_acc:0.982]
Epoch [120/120    avg_loss:0.063, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 216   7   0   0   0   6   1   0   0   0   0]
 [  0   0   0   0 197  30   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 1.         0.95154185 0.96860987 0.90366972 0.88888889
 1.         0.87209302 0.99232737 0.99893276 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9829075048900238
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f35b165fb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.022]
Epoch [2/120    avg_loss:2.499, val_acc:0.286]
Epoch [3/120    avg_loss:2.384, val_acc:0.310]
Epoch [4/120    avg_loss:2.296, val_acc:0.345]
Epoch [5/120    avg_loss:2.228, val_acc:0.375]
Epoch [6/120    avg_loss:2.138, val_acc:0.442]
Epoch [7/120    avg_loss:2.094, val_acc:0.480]
Epoch [8/120    avg_loss:1.999, val_acc:0.522]
Epoch [9/120    avg_loss:1.919, val_acc:0.587]
Epoch [10/120    avg_loss:1.823, val_acc:0.603]
Epoch [11/120    avg_loss:1.718, val_acc:0.627]
Epoch [12/120    avg_loss:1.627, val_acc:0.663]
Epoch [13/120    avg_loss:1.521, val_acc:0.673]
Epoch [14/120    avg_loss:1.382, val_acc:0.730]
Epoch [15/120    avg_loss:1.260, val_acc:0.698]
Epoch [16/120    avg_loss:1.177, val_acc:0.754]
Epoch [17/120    avg_loss:1.076, val_acc:0.853]
Epoch [18/120    avg_loss:0.988, val_acc:0.863]
Epoch [19/120    avg_loss:0.938, val_acc:0.774]
Epoch [20/120    avg_loss:0.855, val_acc:0.845]
Epoch [21/120    avg_loss:0.778, val_acc:0.887]
Epoch [22/120    avg_loss:0.759, val_acc:0.847]
Epoch [23/120    avg_loss:0.725, val_acc:0.869]
Epoch [24/120    avg_loss:0.709, val_acc:0.889]
Epoch [25/120    avg_loss:0.640, val_acc:0.887]
Epoch [26/120    avg_loss:0.619, val_acc:0.867]
Epoch [27/120    avg_loss:0.587, val_acc:0.907]
Epoch [28/120    avg_loss:0.553, val_acc:0.873]
Epoch [29/120    avg_loss:0.522, val_acc:0.895]
Epoch [30/120    avg_loss:0.504, val_acc:0.917]
Epoch [31/120    avg_loss:0.474, val_acc:0.919]
Epoch [32/120    avg_loss:0.476, val_acc:0.891]
Epoch [33/120    avg_loss:0.488, val_acc:0.915]
Epoch [34/120    avg_loss:0.426, val_acc:0.923]
Epoch [35/120    avg_loss:0.408, val_acc:0.915]
Epoch [36/120    avg_loss:0.416, val_acc:0.935]
Epoch [37/120    avg_loss:0.413, val_acc:0.921]
Epoch [38/120    avg_loss:0.384, val_acc:0.911]
Epoch [39/120    avg_loss:0.361, val_acc:0.935]
Epoch [40/120    avg_loss:0.368, val_acc:0.927]
Epoch [41/120    avg_loss:0.315, val_acc:0.917]
Epoch [42/120    avg_loss:0.383, val_acc:0.931]
Epoch [43/120    avg_loss:0.324, val_acc:0.937]
Epoch [44/120    avg_loss:0.359, val_acc:0.942]
Epoch [45/120    avg_loss:0.351, val_acc:0.921]
Epoch [46/120    avg_loss:0.313, val_acc:0.946]
Epoch [47/120    avg_loss:0.282, val_acc:0.944]
Epoch [48/120    avg_loss:0.269, val_acc:0.958]
Epoch [49/120    avg_loss:0.252, val_acc:0.956]
Epoch [50/120    avg_loss:0.291, val_acc:0.925]
Epoch [51/120    avg_loss:0.279, val_acc:0.929]
Epoch [52/120    avg_loss:0.301, val_acc:0.925]
Epoch [53/120    avg_loss:0.280, val_acc:0.952]
Epoch [54/120    avg_loss:0.281, val_acc:0.921]
Epoch [55/120    avg_loss:0.274, val_acc:0.954]
Epoch [56/120    avg_loss:0.248, val_acc:0.948]
Epoch [57/120    avg_loss:0.240, val_acc:0.950]
Epoch [58/120    avg_loss:0.196, val_acc:0.968]
Epoch [59/120    avg_loss:0.224, val_acc:0.968]
Epoch [60/120    avg_loss:0.214, val_acc:0.944]
Epoch [61/120    avg_loss:0.225, val_acc:0.935]
Epoch [62/120    avg_loss:0.236, val_acc:0.952]
Epoch [63/120    avg_loss:0.190, val_acc:0.976]
Epoch [64/120    avg_loss:0.203, val_acc:0.938]
Epoch [65/120    avg_loss:0.201, val_acc:0.964]
Epoch [66/120    avg_loss:0.194, val_acc:0.952]
Epoch [67/120    avg_loss:0.224, val_acc:0.958]
Epoch [68/120    avg_loss:0.257, val_acc:0.948]
Epoch [69/120    avg_loss:0.195, val_acc:0.962]
Epoch [70/120    avg_loss:0.180, val_acc:0.980]
Epoch [71/120    avg_loss:0.176, val_acc:0.958]
Epoch [72/120    avg_loss:0.169, val_acc:0.972]
Epoch [73/120    avg_loss:0.209, val_acc:0.966]
Epoch [74/120    avg_loss:0.188, val_acc:0.962]
Epoch [75/120    avg_loss:0.199, val_acc:0.972]
Epoch [76/120    avg_loss:0.152, val_acc:0.970]
Epoch [77/120    avg_loss:0.146, val_acc:0.970]
Epoch [78/120    avg_loss:0.143, val_acc:0.966]
Epoch [79/120    avg_loss:0.137, val_acc:0.980]
Epoch [80/120    avg_loss:0.142, val_acc:0.972]
Epoch [81/120    avg_loss:0.177, val_acc:0.960]
Epoch [82/120    avg_loss:0.153, val_acc:0.966]
Epoch [83/120    avg_loss:0.142, val_acc:0.974]
Epoch [84/120    avg_loss:0.160, val_acc:0.982]
Epoch [85/120    avg_loss:0.159, val_acc:0.980]
Epoch [86/120    avg_loss:0.147, val_acc:0.978]
Epoch [87/120    avg_loss:0.154, val_acc:0.968]
Epoch [88/120    avg_loss:0.139, val_acc:0.974]
Epoch [89/120    avg_loss:0.095, val_acc:0.988]
Epoch [90/120    avg_loss:0.115, val_acc:0.976]
Epoch [91/120    avg_loss:0.112, val_acc:0.974]
Epoch [92/120    avg_loss:0.173, val_acc:0.954]
Epoch [93/120    avg_loss:0.124, val_acc:0.986]
Epoch [94/120    avg_loss:0.159, val_acc:0.962]
Epoch [95/120    avg_loss:0.142, val_acc:0.962]
Epoch [96/120    avg_loss:0.118, val_acc:0.986]
Epoch [97/120    avg_loss:0.101, val_acc:0.978]
Epoch [98/120    avg_loss:0.117, val_acc:0.986]
Epoch [99/120    avg_loss:0.142, val_acc:0.950]
Epoch [100/120    avg_loss:0.111, val_acc:0.978]
Epoch [101/120    avg_loss:0.143, val_acc:0.938]
Epoch [102/120    avg_loss:0.161, val_acc:0.982]
Epoch [103/120    avg_loss:0.106, val_acc:0.978]
Epoch [104/120    avg_loss:0.107, val_acc:0.978]
Epoch [105/120    avg_loss:0.093, val_acc:0.978]
Epoch [106/120    avg_loss:0.093, val_acc:0.978]
Epoch [107/120    avg_loss:0.081, val_acc:0.978]
Epoch [108/120    avg_loss:0.086, val_acc:0.982]
Epoch [109/120    avg_loss:0.070, val_acc:0.980]
Epoch [110/120    avg_loss:0.080, val_acc:0.982]
Epoch [111/120    avg_loss:0.074, val_acc:0.982]
Epoch [112/120    avg_loss:0.076, val_acc:0.982]
Epoch [113/120    avg_loss:0.070, val_acc:0.980]
Epoch [114/120    avg_loss:0.074, val_acc:0.980]
Epoch [115/120    avg_loss:0.065, val_acc:0.982]
Epoch [116/120    avg_loss:0.068, val_acc:0.982]
Epoch [117/120    avg_loss:0.075, val_acc:0.982]
Epoch [118/120    avg_loss:0.072, val_acc:0.982]
Epoch [119/120    avg_loss:0.072, val_acc:0.982]
Epoch [120/120    avg_loss:0.063, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   6  25 114   0   0   0   0   0   0   0   0]
 [  0   7   0   0   2   0 197   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   5   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.01705756929637

F1 scores:
[       nan 0.99491649 0.94545455 0.96491228 0.89308176 0.83516484
 0.96568627 0.87096774 1.         1.         1.         1.
 0.99445061 1.        ]

Kappa:
0.9779192867590948
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc690e7ea90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.586, val_acc:0.363]
Epoch [2/120    avg_loss:2.460, val_acc:0.429]
Epoch [3/120    avg_loss:2.355, val_acc:0.444]
Epoch [4/120    avg_loss:2.263, val_acc:0.419]
Epoch [5/120    avg_loss:2.184, val_acc:0.419]
Epoch [6/120    avg_loss:2.077, val_acc:0.456]
Epoch [7/120    avg_loss:1.995, val_acc:0.532]
Epoch [8/120    avg_loss:1.910, val_acc:0.565]
Epoch [9/120    avg_loss:1.813, val_acc:0.605]
Epoch [10/120    avg_loss:1.719, val_acc:0.653]
Epoch [11/120    avg_loss:1.582, val_acc:0.671]
Epoch [12/120    avg_loss:1.504, val_acc:0.688]
Epoch [13/120    avg_loss:1.392, val_acc:0.766]
Epoch [14/120    avg_loss:1.273, val_acc:0.762]
Epoch [15/120    avg_loss:1.165, val_acc:0.827]
Epoch [16/120    avg_loss:1.122, val_acc:0.812]
Epoch [17/120    avg_loss:1.040, val_acc:0.823]
Epoch [18/120    avg_loss:0.961, val_acc:0.812]
Epoch [19/120    avg_loss:0.883, val_acc:0.806]
Epoch [20/120    avg_loss:0.861, val_acc:0.857]
Epoch [21/120    avg_loss:0.783, val_acc:0.875]
Epoch [22/120    avg_loss:0.721, val_acc:0.863]
Epoch [23/120    avg_loss:0.667, val_acc:0.901]
Epoch [24/120    avg_loss:0.629, val_acc:0.895]
Epoch [25/120    avg_loss:0.675, val_acc:0.871]
Epoch [26/120    avg_loss:0.604, val_acc:0.903]
Epoch [27/120    avg_loss:0.524, val_acc:0.903]
Epoch [28/120    avg_loss:0.502, val_acc:0.889]
Epoch [29/120    avg_loss:0.544, val_acc:0.889]
Epoch [30/120    avg_loss:0.472, val_acc:0.915]
Epoch [31/120    avg_loss:0.450, val_acc:0.915]
Epoch [32/120    avg_loss:0.412, val_acc:0.893]
Epoch [33/120    avg_loss:0.436, val_acc:0.913]
Epoch [34/120    avg_loss:0.402, val_acc:0.913]
Epoch [35/120    avg_loss:0.402, val_acc:0.927]
Epoch [36/120    avg_loss:0.405, val_acc:0.911]
Epoch [37/120    avg_loss:0.388, val_acc:0.931]
Epoch [38/120    avg_loss:0.378, val_acc:0.913]
Epoch [39/120    avg_loss:0.335, val_acc:0.942]
Epoch [40/120    avg_loss:0.315, val_acc:0.913]
Epoch [41/120    avg_loss:0.332, val_acc:0.921]
Epoch [42/120    avg_loss:0.348, val_acc:0.903]
Epoch [43/120    avg_loss:0.298, val_acc:0.938]
Epoch [44/120    avg_loss:0.321, val_acc:0.923]
Epoch [45/120    avg_loss:0.317, val_acc:0.923]
Epoch [46/120    avg_loss:0.297, val_acc:0.942]
Epoch [47/120    avg_loss:0.254, val_acc:0.937]
Epoch [48/120    avg_loss:0.242, val_acc:0.952]
Epoch [49/120    avg_loss:0.255, val_acc:0.940]
Epoch [50/120    avg_loss:0.262, val_acc:0.938]
Epoch [51/120    avg_loss:0.228, val_acc:0.925]
Epoch [52/120    avg_loss:0.255, val_acc:0.952]
Epoch [53/120    avg_loss:0.219, val_acc:0.935]
Epoch [54/120    avg_loss:0.227, val_acc:0.940]
Epoch [55/120    avg_loss:0.308, val_acc:0.899]
Epoch [56/120    avg_loss:0.345, val_acc:0.911]
Epoch [57/120    avg_loss:0.222, val_acc:0.925]
Epoch [58/120    avg_loss:0.262, val_acc:0.940]
Epoch [59/120    avg_loss:0.239, val_acc:0.937]
Epoch [60/120    avg_loss:0.247, val_acc:0.929]
Epoch [61/120    avg_loss:0.267, val_acc:0.911]
Epoch [62/120    avg_loss:0.266, val_acc:0.940]
Epoch [63/120    avg_loss:0.232, val_acc:0.935]
Epoch [64/120    avg_loss:0.217, val_acc:0.950]
Epoch [65/120    avg_loss:0.223, val_acc:0.942]
Epoch [66/120    avg_loss:0.165, val_acc:0.958]
Epoch [67/120    avg_loss:0.149, val_acc:0.962]
Epoch [68/120    avg_loss:0.128, val_acc:0.964]
Epoch [69/120    avg_loss:0.132, val_acc:0.964]
Epoch [70/120    avg_loss:0.131, val_acc:0.970]
Epoch [71/120    avg_loss:0.134, val_acc:0.964]
Epoch [72/120    avg_loss:0.118, val_acc:0.968]
Epoch [73/120    avg_loss:0.120, val_acc:0.962]
Epoch [74/120    avg_loss:0.130, val_acc:0.966]
Epoch [75/120    avg_loss:0.139, val_acc:0.964]
Epoch [76/120    avg_loss:0.139, val_acc:0.966]
Epoch [77/120    avg_loss:0.128, val_acc:0.964]
Epoch [78/120    avg_loss:0.115, val_acc:0.968]
Epoch [79/120    avg_loss:0.115, val_acc:0.968]
Epoch [80/120    avg_loss:0.127, val_acc:0.968]
Epoch [81/120    avg_loss:0.124, val_acc:0.968]
Epoch [82/120    avg_loss:0.114, val_acc:0.972]
Epoch [83/120    avg_loss:0.116, val_acc:0.970]
Epoch [84/120    avg_loss:0.112, val_acc:0.970]
Epoch [85/120    avg_loss:0.108, val_acc:0.968]
Epoch [86/120    avg_loss:0.098, val_acc:0.968]
Epoch [87/120    avg_loss:0.102, val_acc:0.970]
Epoch [88/120    avg_loss:0.107, val_acc:0.970]
Epoch [89/120    avg_loss:0.109, val_acc:0.966]
Epoch [90/120    avg_loss:0.107, val_acc:0.970]
Epoch [91/120    avg_loss:0.098, val_acc:0.972]
Epoch [92/120    avg_loss:0.113, val_acc:0.972]
Epoch [93/120    avg_loss:0.098, val_acc:0.970]
Epoch [94/120    avg_loss:0.099, val_acc:0.972]
Epoch [95/120    avg_loss:0.099, val_acc:0.968]
Epoch [96/120    avg_loss:0.109, val_acc:0.970]
Epoch [97/120    avg_loss:0.107, val_acc:0.968]
Epoch [98/120    avg_loss:0.092, val_acc:0.968]
Epoch [99/120    avg_loss:0.094, val_acc:0.968]
Epoch [100/120    avg_loss:0.093, val_acc:0.968]
Epoch [101/120    avg_loss:0.103, val_acc:0.966]
Epoch [102/120    avg_loss:0.092, val_acc:0.974]
Epoch [103/120    avg_loss:0.093, val_acc:0.970]
Epoch [104/120    avg_loss:0.115, val_acc:0.968]
Epoch [105/120    avg_loss:0.097, val_acc:0.970]
Epoch [106/120    avg_loss:0.093, val_acc:0.968]
Epoch [107/120    avg_loss:0.087, val_acc:0.970]
Epoch [108/120    avg_loss:0.098, val_acc:0.968]
Epoch [109/120    avg_loss:0.092, val_acc:0.972]
Epoch [110/120    avg_loss:0.097, val_acc:0.976]
Epoch [111/120    avg_loss:0.088, val_acc:0.972]
Epoch [112/120    avg_loss:0.084, val_acc:0.974]
Epoch [113/120    avg_loss:0.093, val_acc:0.970]
Epoch [114/120    avg_loss:0.089, val_acc:0.972]
Epoch [115/120    avg_loss:0.082, val_acc:0.972]
Epoch [116/120    avg_loss:0.098, val_acc:0.972]
Epoch [117/120    avg_loss:0.087, val_acc:0.970]
Epoch [118/120    avg_loss:0.084, val_acc:0.970]
Epoch [119/120    avg_loss:0.080, val_acc:0.970]
Epoch [120/120    avg_loss:0.108, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   1 222   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   1 218   7   0   0   1   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0  20   0   0   0   0  74   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   3   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 0.99636364 0.94039735 0.98013245 0.92372881 0.89855072
 0.98771499 0.85057471 0.99487179 0.99893276 1.         0.99867198
 0.99557522 1.        ]

Kappa:
0.9826662872222078
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe611b9cb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.199]
Epoch [2/120    avg_loss:2.515, val_acc:0.401]
Epoch [3/120    avg_loss:2.419, val_acc:0.448]
Epoch [4/120    avg_loss:2.326, val_acc:0.571]
Epoch [5/120    avg_loss:2.237, val_acc:0.573]
Epoch [6/120    avg_loss:2.118, val_acc:0.593]
Epoch [7/120    avg_loss:2.053, val_acc:0.558]
Epoch [8/120    avg_loss:1.952, val_acc:0.609]
Epoch [9/120    avg_loss:1.861, val_acc:0.665]
Epoch [10/120    avg_loss:1.790, val_acc:0.655]
Epoch [11/120    avg_loss:1.699, val_acc:0.764]
Epoch [12/120    avg_loss:1.596, val_acc:0.756]
Epoch [13/120    avg_loss:1.478, val_acc:0.790]
Epoch [14/120    avg_loss:1.412, val_acc:0.706]
Epoch [15/120    avg_loss:1.334, val_acc:0.756]
Epoch [16/120    avg_loss:1.214, val_acc:0.750]
Epoch [17/120    avg_loss:1.127, val_acc:0.794]
Epoch [18/120    avg_loss:1.049, val_acc:0.760]
Epoch [19/120    avg_loss:0.950, val_acc:0.726]
Epoch [20/120    avg_loss:0.909, val_acc:0.770]
Epoch [21/120    avg_loss:0.873, val_acc:0.778]
Epoch [22/120    avg_loss:0.837, val_acc:0.764]
Epoch [23/120    avg_loss:0.788, val_acc:0.806]
Epoch [24/120    avg_loss:0.769, val_acc:0.853]
Epoch [25/120    avg_loss:0.666, val_acc:0.885]
Epoch [26/120    avg_loss:0.661, val_acc:0.863]
Epoch [27/120    avg_loss:0.610, val_acc:0.897]
Epoch [28/120    avg_loss:0.601, val_acc:0.895]
Epoch [29/120    avg_loss:0.549, val_acc:0.867]
Epoch [30/120    avg_loss:0.550, val_acc:0.911]
Epoch [31/120    avg_loss:0.487, val_acc:0.927]
Epoch [32/120    avg_loss:0.487, val_acc:0.869]
Epoch [33/120    avg_loss:0.538, val_acc:0.895]
Epoch [34/120    avg_loss:0.451, val_acc:0.915]
Epoch [35/120    avg_loss:0.449, val_acc:0.915]
Epoch [36/120    avg_loss:0.457, val_acc:0.907]
Epoch [37/120    avg_loss:0.453, val_acc:0.909]
Epoch [38/120    avg_loss:0.408, val_acc:0.937]
Epoch [39/120    avg_loss:0.389, val_acc:0.893]
Epoch [40/120    avg_loss:0.453, val_acc:0.929]
Epoch [41/120    avg_loss:0.369, val_acc:0.919]
Epoch [42/120    avg_loss:0.432, val_acc:0.931]
Epoch [43/120    avg_loss:0.382, val_acc:0.923]
Epoch [44/120    avg_loss:0.391, val_acc:0.931]
Epoch [45/120    avg_loss:0.410, val_acc:0.929]
Epoch [46/120    avg_loss:0.327, val_acc:0.946]
Epoch [47/120    avg_loss:0.301, val_acc:0.937]
Epoch [48/120    avg_loss:0.290, val_acc:0.935]
Epoch [49/120    avg_loss:0.274, val_acc:0.950]
Epoch [50/120    avg_loss:0.251, val_acc:0.960]
Epoch [51/120    avg_loss:0.236, val_acc:0.940]
Epoch [52/120    avg_loss:0.379, val_acc:0.923]
Epoch [53/120    avg_loss:0.315, val_acc:0.956]
Epoch [54/120    avg_loss:0.322, val_acc:0.929]
Epoch [55/120    avg_loss:0.321, val_acc:0.944]
Epoch [56/120    avg_loss:0.238, val_acc:0.948]
Epoch [57/120    avg_loss:0.274, val_acc:0.944]
Epoch [58/120    avg_loss:0.256, val_acc:0.942]
Epoch [59/120    avg_loss:0.277, val_acc:0.956]
Epoch [60/120    avg_loss:0.212, val_acc:0.962]
Epoch [61/120    avg_loss:0.221, val_acc:0.931]
Epoch [62/120    avg_loss:0.204, val_acc:0.970]
Epoch [63/120    avg_loss:0.234, val_acc:0.938]
Epoch [64/120    avg_loss:0.220, val_acc:0.958]
Epoch [65/120    avg_loss:0.236, val_acc:0.964]
Epoch [66/120    avg_loss:0.190, val_acc:0.958]
Epoch [67/120    avg_loss:0.162, val_acc:0.970]
Epoch [68/120    avg_loss:0.214, val_acc:0.923]
Epoch [69/120    avg_loss:0.192, val_acc:0.978]
Epoch [70/120    avg_loss:0.142, val_acc:0.968]
Epoch [71/120    avg_loss:0.173, val_acc:0.956]
Epoch [72/120    avg_loss:0.144, val_acc:0.978]
Epoch [73/120    avg_loss:0.174, val_acc:0.982]
Epoch [74/120    avg_loss:0.151, val_acc:0.960]
Epoch [75/120    avg_loss:0.177, val_acc:0.974]
Epoch [76/120    avg_loss:0.175, val_acc:0.962]
Epoch [77/120    avg_loss:0.218, val_acc:0.972]
Epoch [78/120    avg_loss:0.150, val_acc:0.972]
Epoch [79/120    avg_loss:0.165, val_acc:0.968]
Epoch [80/120    avg_loss:0.175, val_acc:0.968]
Epoch [81/120    avg_loss:0.146, val_acc:0.988]
Epoch [82/120    avg_loss:0.149, val_acc:0.974]
Epoch [83/120    avg_loss:0.140, val_acc:0.980]
Epoch [84/120    avg_loss:0.084, val_acc:0.984]
Epoch [85/120    avg_loss:0.133, val_acc:0.968]
Epoch [86/120    avg_loss:0.149, val_acc:0.954]
Epoch [87/120    avg_loss:0.114, val_acc:0.982]
Epoch [88/120    avg_loss:0.092, val_acc:0.986]
Epoch [89/120    avg_loss:0.079, val_acc:0.984]
Epoch [90/120    avg_loss:0.091, val_acc:0.982]
Epoch [91/120    avg_loss:0.076, val_acc:0.978]
Epoch [92/120    avg_loss:0.072, val_acc:0.988]
Epoch [93/120    avg_loss:0.087, val_acc:0.984]
Epoch [94/120    avg_loss:0.079, val_acc:0.980]
Epoch [95/120    avg_loss:0.096, val_acc:0.984]
Epoch [96/120    avg_loss:0.123, val_acc:0.935]
Epoch [97/120    avg_loss:0.174, val_acc:0.976]
Epoch [98/120    avg_loss:0.128, val_acc:0.978]
Epoch [99/120    avg_loss:0.131, val_acc:0.956]
Epoch [100/120    avg_loss:0.115, val_acc:0.960]
Epoch [101/120    avg_loss:0.107, val_acc:0.988]
Epoch [102/120    avg_loss:0.089, val_acc:0.984]
Epoch [103/120    avg_loss:0.075, val_acc:0.988]
Epoch [104/120    avg_loss:0.072, val_acc:0.978]
Epoch [105/120    avg_loss:0.066, val_acc:0.986]
Epoch [106/120    avg_loss:0.058, val_acc:0.992]
Epoch [107/120    avg_loss:0.064, val_acc:0.984]
Epoch [108/120    avg_loss:0.074, val_acc:0.978]
Epoch [109/120    avg_loss:0.079, val_acc:0.974]
Epoch [110/120    avg_loss:0.163, val_acc:0.972]
Epoch [111/120    avg_loss:0.096, val_acc:0.986]
Epoch [112/120    avg_loss:0.057, val_acc:0.984]
Epoch [113/120    avg_loss:0.053, val_acc:0.982]
Epoch [114/120    avg_loss:0.069, val_acc:0.984]
Epoch [115/120    avg_loss:0.088, val_acc:0.990]
Epoch [116/120    avg_loss:0.074, val_acc:0.978]
Epoch [117/120    avg_loss:0.119, val_acc:0.966]
Epoch [118/120    avg_loss:0.135, val_acc:0.980]
Epoch [119/120    avg_loss:0.116, val_acc:0.982]
Epoch [120/120    avg_loss:0.069, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   0   0   3   0   0   0   0   0]
 [  0   0 215   0   0   0   0   3   0   0   0   0   1   0]
 [  0   0   0 220   3   0   0   0   6   1   0   0   0   0]
 [  0   0   0  11 194  21   0   0   1   0   0   0   0   0]
 [  0   0   0   8   8 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   5   0   1   0 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.05970149253731

F1 scores:
[       nan 0.99780541 0.95343681 0.93816631 0.89814815 0.87457627
 1.         0.88505747 0.98103666 0.99893276 0.99862826 0.99734043
 0.99003322 1.        ]

Kappa:
0.9783963018947616
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61283c4a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.327]
Epoch [2/120    avg_loss:2.486, val_acc:0.371]
Epoch [3/120    avg_loss:2.401, val_acc:0.387]
Epoch [4/120    avg_loss:2.325, val_acc:0.391]
Epoch [5/120    avg_loss:2.247, val_acc:0.393]
Epoch [6/120    avg_loss:2.175, val_acc:0.446]
Epoch [7/120    avg_loss:2.083, val_acc:0.502]
Epoch [8/120    avg_loss:2.003, val_acc:0.575]
Epoch [9/120    avg_loss:1.911, val_acc:0.663]
Epoch [10/120    avg_loss:1.821, val_acc:0.629]
Epoch [11/120    avg_loss:1.714, val_acc:0.641]
Epoch [12/120    avg_loss:1.603, val_acc:0.653]
Epoch [13/120    avg_loss:1.516, val_acc:0.663]
Epoch [14/120    avg_loss:1.430, val_acc:0.690]
Epoch [15/120    avg_loss:1.334, val_acc:0.698]
Epoch [16/120    avg_loss:1.216, val_acc:0.738]
Epoch [17/120    avg_loss:1.139, val_acc:0.762]
Epoch [18/120    avg_loss:1.041, val_acc:0.843]
Epoch [19/120    avg_loss:0.963, val_acc:0.859]
Epoch [20/120    avg_loss:0.891, val_acc:0.909]
Epoch [21/120    avg_loss:0.802, val_acc:0.905]
Epoch [22/120    avg_loss:0.766, val_acc:0.899]
Epoch [23/120    avg_loss:0.714, val_acc:0.891]
Epoch [24/120    avg_loss:0.676, val_acc:0.903]
Epoch [25/120    avg_loss:0.615, val_acc:0.925]
Epoch [26/120    avg_loss:0.578, val_acc:0.913]
Epoch [27/120    avg_loss:0.536, val_acc:0.919]
Epoch [28/120    avg_loss:0.489, val_acc:0.917]
Epoch [29/120    avg_loss:0.572, val_acc:0.919]
Epoch [30/120    avg_loss:0.481, val_acc:0.933]
Epoch [31/120    avg_loss:0.447, val_acc:0.921]
Epoch [32/120    avg_loss:0.483, val_acc:0.907]
Epoch [33/120    avg_loss:0.480, val_acc:0.917]
Epoch [34/120    avg_loss:0.427, val_acc:0.927]
Epoch [35/120    avg_loss:0.405, val_acc:0.937]
Epoch [36/120    avg_loss:0.431, val_acc:0.913]
Epoch [37/120    avg_loss:0.425, val_acc:0.942]
Epoch [38/120    avg_loss:0.337, val_acc:0.937]
Epoch [39/120    avg_loss:0.381, val_acc:0.938]
Epoch [40/120    avg_loss:0.398, val_acc:0.935]
Epoch [41/120    avg_loss:0.323, val_acc:0.919]
Epoch [42/120    avg_loss:0.307, val_acc:0.942]
Epoch [43/120    avg_loss:0.297, val_acc:0.962]
Epoch [44/120    avg_loss:0.253, val_acc:0.935]
Epoch [45/120    avg_loss:0.317, val_acc:0.948]
Epoch [46/120    avg_loss:0.319, val_acc:0.933]
Epoch [47/120    avg_loss:0.311, val_acc:0.944]
Epoch [48/120    avg_loss:0.282, val_acc:0.950]
Epoch [49/120    avg_loss:0.253, val_acc:0.929]
Epoch [50/120    avg_loss:0.296, val_acc:0.944]
Epoch [51/120    avg_loss:0.291, val_acc:0.952]
Epoch [52/120    avg_loss:0.225, val_acc:0.938]
Epoch [53/120    avg_loss:0.230, val_acc:0.919]
Epoch [54/120    avg_loss:0.284, val_acc:0.950]
Epoch [55/120    avg_loss:0.273, val_acc:0.948]
Epoch [56/120    avg_loss:0.271, val_acc:0.950]
Epoch [57/120    avg_loss:0.246, val_acc:0.962]
Epoch [58/120    avg_loss:0.214, val_acc:0.960]
Epoch [59/120    avg_loss:0.193, val_acc:0.964]
Epoch [60/120    avg_loss:0.197, val_acc:0.964]
Epoch [61/120    avg_loss:0.177, val_acc:0.968]
Epoch [62/120    avg_loss:0.170, val_acc:0.966]
Epoch [63/120    avg_loss:0.168, val_acc:0.966]
Epoch [64/120    avg_loss:0.163, val_acc:0.968]
Epoch [65/120    avg_loss:0.157, val_acc:0.972]
Epoch [66/120    avg_loss:0.163, val_acc:0.970]
Epoch [67/120    avg_loss:0.160, val_acc:0.970]
Epoch [68/120    avg_loss:0.162, val_acc:0.966]
Epoch [69/120    avg_loss:0.169, val_acc:0.970]
Epoch [70/120    avg_loss:0.161, val_acc:0.968]
Epoch [71/120    avg_loss:0.151, val_acc:0.968]
Epoch [72/120    avg_loss:0.167, val_acc:0.964]
Epoch [73/120    avg_loss:0.164, val_acc:0.968]
Epoch [74/120    avg_loss:0.138, val_acc:0.968]
Epoch [75/120    avg_loss:0.160, val_acc:0.974]
Epoch [76/120    avg_loss:0.158, val_acc:0.976]
Epoch [77/120    avg_loss:0.157, val_acc:0.968]
Epoch [78/120    avg_loss:0.148, val_acc:0.966]
Epoch [79/120    avg_loss:0.162, val_acc:0.972]
Epoch [80/120    avg_loss:0.157, val_acc:0.966]
Epoch [81/120    avg_loss:0.161, val_acc:0.966]
Epoch [82/120    avg_loss:0.133, val_acc:0.970]
Epoch [83/120    avg_loss:0.136, val_acc:0.968]
Epoch [84/120    avg_loss:0.148, val_acc:0.970]
Epoch [85/120    avg_loss:0.147, val_acc:0.966]
Epoch [86/120    avg_loss:0.150, val_acc:0.970]
Epoch [87/120    avg_loss:0.153, val_acc:0.970]
Epoch [88/120    avg_loss:0.158, val_acc:0.966]
Epoch [89/120    avg_loss:0.133, val_acc:0.968]
Epoch [90/120    avg_loss:0.158, val_acc:0.968]
Epoch [91/120    avg_loss:0.149, val_acc:0.968]
Epoch [92/120    avg_loss:0.142, val_acc:0.970]
Epoch [93/120    avg_loss:0.142, val_acc:0.968]
Epoch [94/120    avg_loss:0.160, val_acc:0.968]
Epoch [95/120    avg_loss:0.129, val_acc:0.968]
Epoch [96/120    avg_loss:0.132, val_acc:0.968]
Epoch [97/120    avg_loss:0.146, val_acc:0.970]
Epoch [98/120    avg_loss:0.137, val_acc:0.968]
Epoch [99/120    avg_loss:0.133, val_acc:0.970]
Epoch [100/120    avg_loss:0.146, val_acc:0.966]
Epoch [101/120    avg_loss:0.141, val_acc:0.970]
Epoch [102/120    avg_loss:0.128, val_acc:0.970]
Epoch [103/120    avg_loss:0.141, val_acc:0.970]
Epoch [104/120    avg_loss:0.132, val_acc:0.970]
Epoch [105/120    avg_loss:0.136, val_acc:0.970]
Epoch [106/120    avg_loss:0.124, val_acc:0.970]
Epoch [107/120    avg_loss:0.137, val_acc:0.970]
Epoch [108/120    avg_loss:0.135, val_acc:0.970]
Epoch [109/120    avg_loss:0.136, val_acc:0.970]
Epoch [110/120    avg_loss:0.134, val_acc:0.970]
Epoch [111/120    avg_loss:0.135, val_acc:0.970]
Epoch [112/120    avg_loss:0.137, val_acc:0.970]
Epoch [113/120    avg_loss:0.128, val_acc:0.970]
Epoch [114/120    avg_loss:0.136, val_acc:0.970]
Epoch [115/120    avg_loss:0.130, val_acc:0.970]
Epoch [116/120    avg_loss:0.136, val_acc:0.970]
Epoch [117/120    avg_loss:0.158, val_acc:0.970]
Epoch [118/120    avg_loss:0.136, val_acc:0.970]
Epoch [119/120    avg_loss:0.138, val_acc:0.970]
Epoch [120/120    avg_loss:0.137, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   0 219   8   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 201  25   0   0   1   0   0   0   0   0]
 [  0   0   0   0  46  99   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  20   0   0   0   0  74   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.56929637526652

F1 scores:
[       nan 0.99854227 0.93777778 0.97550111 0.8340249  0.73605948
 0.99512195 0.84090909 0.99487179 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9729318461796191
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f64b6998b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.614, val_acc:0.268]
Epoch [2/120    avg_loss:2.489, val_acc:0.377]
Epoch [3/120    avg_loss:2.378, val_acc:0.415]
Epoch [4/120    avg_loss:2.281, val_acc:0.530]
Epoch [5/120    avg_loss:2.196, val_acc:0.569]
Epoch [6/120    avg_loss:2.102, val_acc:0.581]
Epoch [7/120    avg_loss:2.008, val_acc:0.593]
Epoch [8/120    avg_loss:1.911, val_acc:0.617]
Epoch [9/120    avg_loss:1.806, val_acc:0.643]
Epoch [10/120    avg_loss:1.701, val_acc:0.653]
Epoch [11/120    avg_loss:1.582, val_acc:0.685]
Epoch [12/120    avg_loss:1.472, val_acc:0.683]
Epoch [13/120    avg_loss:1.353, val_acc:0.694]
Epoch [14/120    avg_loss:1.240, val_acc:0.696]
Epoch [15/120    avg_loss:1.136, val_acc:0.730]
Epoch [16/120    avg_loss:1.069, val_acc:0.742]
Epoch [17/120    avg_loss:1.001, val_acc:0.744]
Epoch [18/120    avg_loss:0.921, val_acc:0.770]
Epoch [19/120    avg_loss:0.852, val_acc:0.800]
Epoch [20/120    avg_loss:0.794, val_acc:0.841]
Epoch [21/120    avg_loss:0.743, val_acc:0.883]
Epoch [22/120    avg_loss:0.712, val_acc:0.889]
Epoch [23/120    avg_loss:0.659, val_acc:0.879]
Epoch [24/120    avg_loss:0.655, val_acc:0.891]
Epoch [25/120    avg_loss:0.592, val_acc:0.905]
Epoch [26/120    avg_loss:0.582, val_acc:0.901]
Epoch [27/120    avg_loss:0.524, val_acc:0.873]
Epoch [28/120    avg_loss:0.553, val_acc:0.907]
Epoch [29/120    avg_loss:0.531, val_acc:0.883]
Epoch [30/120    avg_loss:0.534, val_acc:0.899]
Epoch [31/120    avg_loss:0.460, val_acc:0.895]
Epoch [32/120    avg_loss:0.475, val_acc:0.899]
Epoch [33/120    avg_loss:0.437, val_acc:0.889]
Epoch [34/120    avg_loss:0.449, val_acc:0.903]
Epoch [35/120    avg_loss:0.432, val_acc:0.905]
Epoch [36/120    avg_loss:0.393, val_acc:0.861]
Epoch [37/120    avg_loss:0.419, val_acc:0.913]
Epoch [38/120    avg_loss:0.401, val_acc:0.909]
Epoch [39/120    avg_loss:0.357, val_acc:0.925]
Epoch [40/120    avg_loss:0.296, val_acc:0.919]
Epoch [41/120    avg_loss:0.308, val_acc:0.909]
Epoch [42/120    avg_loss:0.360, val_acc:0.929]
Epoch [43/120    avg_loss:0.334, val_acc:0.905]
Epoch [44/120    avg_loss:0.309, val_acc:0.867]
Epoch [45/120    avg_loss:0.319, val_acc:0.891]
Epoch [46/120    avg_loss:0.309, val_acc:0.940]
Epoch [47/120    avg_loss:0.300, val_acc:0.931]
Epoch [48/120    avg_loss:0.271, val_acc:0.944]
Epoch [49/120    avg_loss:0.278, val_acc:0.889]
Epoch [50/120    avg_loss:0.256, val_acc:0.938]
Epoch [51/120    avg_loss:0.268, val_acc:0.929]
Epoch [52/120    avg_loss:0.298, val_acc:0.927]
Epoch [53/120    avg_loss:0.216, val_acc:0.929]
Epoch [54/120    avg_loss:0.249, val_acc:0.921]
Epoch [55/120    avg_loss:0.231, val_acc:0.938]
Epoch [56/120    avg_loss:0.209, val_acc:0.931]
Epoch [57/120    avg_loss:0.218, val_acc:0.935]
Epoch [58/120    avg_loss:0.247, val_acc:0.956]
Epoch [59/120    avg_loss:0.212, val_acc:0.921]
Epoch [60/120    avg_loss:0.282, val_acc:0.938]
Epoch [61/120    avg_loss:0.254, val_acc:0.940]
Epoch [62/120    avg_loss:0.230, val_acc:0.950]
Epoch [63/120    avg_loss:0.232, val_acc:0.948]
Epoch [64/120    avg_loss:0.230, val_acc:0.944]
Epoch [65/120    avg_loss:0.183, val_acc:0.956]
Epoch [66/120    avg_loss:0.180, val_acc:0.942]
Epoch [67/120    avg_loss:0.147, val_acc:0.954]
Epoch [68/120    avg_loss:0.170, val_acc:0.938]
Epoch [69/120    avg_loss:0.167, val_acc:0.935]
Epoch [70/120    avg_loss:0.212, val_acc:0.881]
Epoch [71/120    avg_loss:0.302, val_acc:0.950]
Epoch [72/120    avg_loss:0.260, val_acc:0.938]
Epoch [73/120    avg_loss:0.241, val_acc:0.944]
Epoch [74/120    avg_loss:0.183, val_acc:0.948]
Epoch [75/120    avg_loss:0.181, val_acc:0.948]
Epoch [76/120    avg_loss:0.154, val_acc:0.946]
Epoch [77/120    avg_loss:0.203, val_acc:0.946]
Epoch [78/120    avg_loss:0.193, val_acc:0.948]
Epoch [79/120    avg_loss:0.157, val_acc:0.954]
Epoch [80/120    avg_loss:0.134, val_acc:0.960]
Epoch [81/120    avg_loss:0.103, val_acc:0.962]
Epoch [82/120    avg_loss:0.126, val_acc:0.968]
Epoch [83/120    avg_loss:0.115, val_acc:0.970]
Epoch [84/120    avg_loss:0.101, val_acc:0.972]
Epoch [85/120    avg_loss:0.117, val_acc:0.970]
Epoch [86/120    avg_loss:0.123, val_acc:0.970]
Epoch [87/120    avg_loss:0.122, val_acc:0.968]
Epoch [88/120    avg_loss:0.122, val_acc:0.970]
Epoch [89/120    avg_loss:0.111, val_acc:0.968]
Epoch [90/120    avg_loss:0.118, val_acc:0.974]
Epoch [91/120    avg_loss:0.111, val_acc:0.970]
Epoch [92/120    avg_loss:0.110, val_acc:0.968]
Epoch [93/120    avg_loss:0.095, val_acc:0.972]
Epoch [94/120    avg_loss:0.106, val_acc:0.964]
Epoch [95/120    avg_loss:0.099, val_acc:0.968]
Epoch [96/120    avg_loss:0.099, val_acc:0.968]
Epoch [97/120    avg_loss:0.105, val_acc:0.970]
Epoch [98/120    avg_loss:0.108, val_acc:0.970]
Epoch [99/120    avg_loss:0.096, val_acc:0.970]
Epoch [100/120    avg_loss:0.107, val_acc:0.972]
Epoch [101/120    avg_loss:0.096, val_acc:0.966]
Epoch [102/120    avg_loss:0.097, val_acc:0.970]
Epoch [103/120    avg_loss:0.088, val_acc:0.972]
Epoch [104/120    avg_loss:0.086, val_acc:0.972]
Epoch [105/120    avg_loss:0.088, val_acc:0.972]
Epoch [106/120    avg_loss:0.089, val_acc:0.972]
Epoch [107/120    avg_loss:0.094, val_acc:0.970]
Epoch [108/120    avg_loss:0.086, val_acc:0.970]
Epoch [109/120    avg_loss:0.103, val_acc:0.970]
Epoch [110/120    avg_loss:0.090, val_acc:0.970]
Epoch [111/120    avg_loss:0.093, val_acc:0.970]
Epoch [112/120    avg_loss:0.086, val_acc:0.970]
Epoch [113/120    avg_loss:0.101, val_acc:0.970]
Epoch [114/120    avg_loss:0.102, val_acc:0.970]
Epoch [115/120    avg_loss:0.075, val_acc:0.970]
Epoch [116/120    avg_loss:0.102, val_acc:0.970]
Epoch [117/120    avg_loss:0.089, val_acc:0.970]
Epoch [118/120    avg_loss:0.088, val_acc:0.970]
Epoch [119/120    avg_loss:0.102, val_acc:0.970]
Epoch [120/120    avg_loss:0.087, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   1 226   0   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  30 115   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   0 199   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.96230599 0.99122807 0.90909091 0.86142322
 0.98271605 0.90909091 1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9848043289032539
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6cf04d0a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.240]
Epoch [2/120    avg_loss:2.496, val_acc:0.389]
Epoch [3/120    avg_loss:2.388, val_acc:0.407]
Epoch [4/120    avg_loss:2.305, val_acc:0.474]
Epoch [5/120    avg_loss:2.220, val_acc:0.560]
Epoch [6/120    avg_loss:2.135, val_acc:0.567]
Epoch [7/120    avg_loss:2.044, val_acc:0.575]
Epoch [8/120    avg_loss:1.943, val_acc:0.595]
Epoch [9/120    avg_loss:1.840, val_acc:0.605]
Epoch [10/120    avg_loss:1.716, val_acc:0.603]
Epoch [11/120    avg_loss:1.608, val_acc:0.633]
Epoch [12/120    avg_loss:1.529, val_acc:0.655]
Epoch [13/120    avg_loss:1.416, val_acc:0.688]
Epoch [14/120    avg_loss:1.325, val_acc:0.736]
Epoch [15/120    avg_loss:1.206, val_acc:0.752]
Epoch [16/120    avg_loss:1.135, val_acc:0.756]
Epoch [17/120    avg_loss:1.039, val_acc:0.742]
Epoch [18/120    avg_loss:0.994, val_acc:0.808]
Epoch [19/120    avg_loss:0.882, val_acc:0.859]
Epoch [20/120    avg_loss:0.779, val_acc:0.885]
Epoch [21/120    avg_loss:0.717, val_acc:0.895]
Epoch [22/120    avg_loss:0.738, val_acc:0.821]
Epoch [23/120    avg_loss:0.654, val_acc:0.883]
Epoch [24/120    avg_loss:0.611, val_acc:0.909]
Epoch [25/120    avg_loss:0.565, val_acc:0.893]
Epoch [26/120    avg_loss:0.555, val_acc:0.883]
Epoch [27/120    avg_loss:0.524, val_acc:0.909]
Epoch [28/120    avg_loss:0.491, val_acc:0.925]
Epoch [29/120    avg_loss:0.431, val_acc:0.843]
Epoch [30/120    avg_loss:0.513, val_acc:0.913]
Epoch [31/120    avg_loss:0.479, val_acc:0.901]
Epoch [32/120    avg_loss:0.494, val_acc:0.913]
Epoch [33/120    avg_loss:0.459, val_acc:0.913]
Epoch [34/120    avg_loss:0.425, val_acc:0.921]
Epoch [35/120    avg_loss:0.376, val_acc:0.917]
Epoch [36/120    avg_loss:0.368, val_acc:0.903]
Epoch [37/120    avg_loss:0.398, val_acc:0.917]
Epoch [38/120    avg_loss:0.409, val_acc:0.946]
Epoch [39/120    avg_loss:0.336, val_acc:0.901]
Epoch [40/120    avg_loss:0.365, val_acc:0.907]
Epoch [41/120    avg_loss:0.381, val_acc:0.917]
Epoch [42/120    avg_loss:0.342, val_acc:0.950]
Epoch [43/120    avg_loss:0.305, val_acc:0.940]
Epoch [44/120    avg_loss:0.380, val_acc:0.931]
Epoch [45/120    avg_loss:0.313, val_acc:0.915]
Epoch [46/120    avg_loss:0.298, val_acc:0.942]
Epoch [47/120    avg_loss:0.283, val_acc:0.950]
Epoch [48/120    avg_loss:0.283, val_acc:0.948]
Epoch [49/120    avg_loss:0.262, val_acc:0.925]
Epoch [50/120    avg_loss:0.312, val_acc:0.952]
Epoch [51/120    avg_loss:0.243, val_acc:0.952]
Epoch [52/120    avg_loss:0.245, val_acc:0.937]
Epoch [53/120    avg_loss:0.243, val_acc:0.938]
Epoch [54/120    avg_loss:0.257, val_acc:0.944]
Epoch [55/120    avg_loss:0.276, val_acc:0.956]
Epoch [56/120    avg_loss:0.243, val_acc:0.954]
Epoch [57/120    avg_loss:0.203, val_acc:0.960]
Epoch [58/120    avg_loss:0.232, val_acc:0.940]
Epoch [59/120    avg_loss:0.209, val_acc:0.952]
Epoch [60/120    avg_loss:0.199, val_acc:0.948]
Epoch [61/120    avg_loss:0.201, val_acc:0.968]
Epoch [62/120    avg_loss:0.162, val_acc:0.972]
Epoch [63/120    avg_loss:0.157, val_acc:0.946]
Epoch [64/120    avg_loss:0.190, val_acc:0.923]
Epoch [65/120    avg_loss:0.238, val_acc:0.970]
Epoch [66/120    avg_loss:0.195, val_acc:0.974]
Epoch [67/120    avg_loss:0.168, val_acc:0.956]
Epoch [68/120    avg_loss:0.177, val_acc:0.978]
Epoch [69/120    avg_loss:0.126, val_acc:0.964]
Epoch [70/120    avg_loss:0.152, val_acc:0.970]
Epoch [71/120    avg_loss:0.214, val_acc:0.940]
Epoch [72/120    avg_loss:0.193, val_acc:0.968]
Epoch [73/120    avg_loss:0.159, val_acc:0.972]
Epoch [74/120    avg_loss:0.130, val_acc:0.962]
Epoch [75/120    avg_loss:0.141, val_acc:0.960]
Epoch [76/120    avg_loss:0.187, val_acc:0.958]
Epoch [77/120    avg_loss:0.138, val_acc:0.970]
Epoch [78/120    avg_loss:0.118, val_acc:0.974]
Epoch [79/120    avg_loss:0.129, val_acc:0.966]
Epoch [80/120    avg_loss:0.125, val_acc:0.956]
Epoch [81/120    avg_loss:0.111, val_acc:0.970]
Epoch [82/120    avg_loss:0.126, val_acc:0.982]
Epoch [83/120    avg_loss:0.078, val_acc:0.984]
Epoch [84/120    avg_loss:0.079, val_acc:0.984]
Epoch [85/120    avg_loss:0.081, val_acc:0.980]
Epoch [86/120    avg_loss:0.090, val_acc:0.980]
Epoch [87/120    avg_loss:0.079, val_acc:0.982]
Epoch [88/120    avg_loss:0.094, val_acc:0.982]
Epoch [89/120    avg_loss:0.078, val_acc:0.984]
Epoch [90/120    avg_loss:0.096, val_acc:0.984]
Epoch [91/120    avg_loss:0.083, val_acc:0.982]
Epoch [92/120    avg_loss:0.072, val_acc:0.984]
Epoch [93/120    avg_loss:0.077, val_acc:0.984]
Epoch [94/120    avg_loss:0.089, val_acc:0.980]
Epoch [95/120    avg_loss:0.081, val_acc:0.980]
Epoch [96/120    avg_loss:0.085, val_acc:0.978]
Epoch [97/120    avg_loss:0.080, val_acc:0.980]
Epoch [98/120    avg_loss:0.072, val_acc:0.982]
Epoch [99/120    avg_loss:0.077, val_acc:0.980]
Epoch [100/120    avg_loss:0.067, val_acc:0.982]
Epoch [101/120    avg_loss:0.077, val_acc:0.984]
Epoch [102/120    avg_loss:0.087, val_acc:0.982]
Epoch [103/120    avg_loss:0.074, val_acc:0.984]
Epoch [104/120    avg_loss:0.068, val_acc:0.984]
Epoch [105/120    avg_loss:0.072, val_acc:0.980]
Epoch [106/120    avg_loss:0.075, val_acc:0.984]
Epoch [107/120    avg_loss:0.062, val_acc:0.984]
Epoch [108/120    avg_loss:0.077, val_acc:0.982]
Epoch [109/120    avg_loss:0.085, val_acc:0.984]
Epoch [110/120    avg_loss:0.082, val_acc:0.984]
Epoch [111/120    avg_loss:0.068, val_acc:0.984]
Epoch [112/120    avg_loss:0.068, val_acc:0.986]
Epoch [113/120    avg_loss:0.069, val_acc:0.982]
Epoch [114/120    avg_loss:0.062, val_acc:0.982]
Epoch [115/120    avg_loss:0.074, val_acc:0.984]
Epoch [116/120    avg_loss:0.067, val_acc:0.984]
Epoch [117/120    avg_loss:0.052, val_acc:0.984]
Epoch [118/120    avg_loss:0.067, val_acc:0.984]
Epoch [119/120    avg_loss:0.073, val_acc:0.986]
Epoch [120/120    avg_loss:0.072, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   1 219   5   0   0   2   0   3   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 1.         0.94305239 0.97550111 0.90549451 0.8707483
 1.         0.86315789 0.998713   0.99680511 1.         1.
 0.99889503 1.        ]

Kappa:
0.9824337518962067
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb6a1f2a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.595, val_acc:0.280]
Epoch [2/120    avg_loss:2.490, val_acc:0.288]
Epoch [3/120    avg_loss:2.381, val_acc:0.331]
Epoch [4/120    avg_loss:2.286, val_acc:0.429]
Epoch [5/120    avg_loss:2.213, val_acc:0.476]
Epoch [6/120    avg_loss:2.115, val_acc:0.542]
Epoch [7/120    avg_loss:2.019, val_acc:0.589]
Epoch [8/120    avg_loss:1.901, val_acc:0.587]
Epoch [9/120    avg_loss:1.782, val_acc:0.683]
Epoch [10/120    avg_loss:1.654, val_acc:0.758]
Epoch [11/120    avg_loss:1.522, val_acc:0.710]
Epoch [12/120    avg_loss:1.435, val_acc:0.710]
Epoch [13/120    avg_loss:1.334, val_acc:0.798]
Epoch [14/120    avg_loss:1.198, val_acc:0.837]
Epoch [15/120    avg_loss:1.126, val_acc:0.744]
Epoch [16/120    avg_loss:1.033, val_acc:0.829]
Epoch [17/120    avg_loss:0.955, val_acc:0.851]
Epoch [18/120    avg_loss:0.862, val_acc:0.879]
Epoch [19/120    avg_loss:0.800, val_acc:0.881]
Epoch [20/120    avg_loss:0.752, val_acc:0.887]
Epoch [21/120    avg_loss:0.690, val_acc:0.875]
Epoch [22/120    avg_loss:0.634, val_acc:0.893]
Epoch [23/120    avg_loss:0.583, val_acc:0.885]
Epoch [24/120    avg_loss:0.556, val_acc:0.895]
Epoch [25/120    avg_loss:0.537, val_acc:0.921]
Epoch [26/120    avg_loss:0.466, val_acc:0.901]
Epoch [27/120    avg_loss:0.464, val_acc:0.907]
Epoch [28/120    avg_loss:0.500, val_acc:0.905]
Epoch [29/120    avg_loss:0.460, val_acc:0.921]
Epoch [30/120    avg_loss:0.479, val_acc:0.901]
Epoch [31/120    avg_loss:0.440, val_acc:0.875]
Epoch [32/120    avg_loss:0.423, val_acc:0.911]
Epoch [33/120    avg_loss:0.418, val_acc:0.873]
Epoch [34/120    avg_loss:0.435, val_acc:0.917]
Epoch [35/120    avg_loss:0.356, val_acc:0.927]
Epoch [36/120    avg_loss:0.322, val_acc:0.940]
Epoch [37/120    avg_loss:0.337, val_acc:0.915]
Epoch [38/120    avg_loss:0.294, val_acc:0.952]
Epoch [39/120    avg_loss:0.283, val_acc:0.937]
Epoch [40/120    avg_loss:0.318, val_acc:0.923]
Epoch [41/120    avg_loss:0.361, val_acc:0.921]
Epoch [42/120    avg_loss:0.292, val_acc:0.919]
Epoch [43/120    avg_loss:0.334, val_acc:0.891]
Epoch [44/120    avg_loss:0.303, val_acc:0.909]
Epoch [45/120    avg_loss:0.290, val_acc:0.929]
Epoch [46/120    avg_loss:0.284, val_acc:0.952]
Epoch [47/120    avg_loss:0.233, val_acc:0.954]
Epoch [48/120    avg_loss:0.249, val_acc:0.950]
Epoch [49/120    avg_loss:0.220, val_acc:0.935]
Epoch [50/120    avg_loss:0.233, val_acc:0.958]
Epoch [51/120    avg_loss:0.237, val_acc:0.940]
Epoch [52/120    avg_loss:0.190, val_acc:0.937]
Epoch [53/120    avg_loss:0.195, val_acc:0.954]
Epoch [54/120    avg_loss:0.191, val_acc:0.958]
Epoch [55/120    avg_loss:0.204, val_acc:0.964]
Epoch [56/120    avg_loss:0.229, val_acc:0.937]
Epoch [57/120    avg_loss:0.250, val_acc:0.929]
Epoch [58/120    avg_loss:0.219, val_acc:0.968]
Epoch [59/120    avg_loss:0.168, val_acc:0.960]
Epoch [60/120    avg_loss:0.225, val_acc:0.937]
Epoch [61/120    avg_loss:0.195, val_acc:0.958]
Epoch [62/120    avg_loss:0.213, val_acc:0.933]
Epoch [63/120    avg_loss:0.238, val_acc:0.911]
Epoch [64/120    avg_loss:0.244, val_acc:0.942]
Epoch [65/120    avg_loss:0.164, val_acc:0.954]
Epoch [66/120    avg_loss:0.160, val_acc:0.954]
Epoch [67/120    avg_loss:0.182, val_acc:0.964]
Epoch [68/120    avg_loss:0.152, val_acc:0.966]
Epoch [69/120    avg_loss:0.132, val_acc:0.960]
Epoch [70/120    avg_loss:0.138, val_acc:0.960]
Epoch [71/120    avg_loss:0.123, val_acc:0.974]
Epoch [72/120    avg_loss:0.129, val_acc:0.968]
Epoch [73/120    avg_loss:0.150, val_acc:0.958]
Epoch [74/120    avg_loss:0.149, val_acc:0.972]
Epoch [75/120    avg_loss:0.121, val_acc:0.952]
Epoch [76/120    avg_loss:0.161, val_acc:0.958]
Epoch [77/120    avg_loss:0.110, val_acc:0.972]
Epoch [78/120    avg_loss:0.122, val_acc:0.968]
Epoch [79/120    avg_loss:0.155, val_acc:0.948]
Epoch [80/120    avg_loss:0.168, val_acc:0.958]
Epoch [81/120    avg_loss:0.214, val_acc:0.933]
Epoch [82/120    avg_loss:0.170, val_acc:0.915]
Epoch [83/120    avg_loss:0.143, val_acc:0.954]
Epoch [84/120    avg_loss:0.121, val_acc:0.972]
Epoch [85/120    avg_loss:0.080, val_acc:0.970]
Epoch [86/120    avg_loss:0.084, val_acc:0.972]
Epoch [87/120    avg_loss:0.095, val_acc:0.978]
Epoch [88/120    avg_loss:0.076, val_acc:0.976]
Epoch [89/120    avg_loss:0.087, val_acc:0.976]
Epoch [90/120    avg_loss:0.079, val_acc:0.976]
Epoch [91/120    avg_loss:0.057, val_acc:0.978]
Epoch [92/120    avg_loss:0.067, val_acc:0.976]
Epoch [93/120    avg_loss:0.055, val_acc:0.972]
Epoch [94/120    avg_loss:0.074, val_acc:0.978]
Epoch [95/120    avg_loss:0.071, val_acc:0.982]
Epoch [96/120    avg_loss:0.062, val_acc:0.976]
Epoch [97/120    avg_loss:0.071, val_acc:0.978]
Epoch [98/120    avg_loss:0.064, val_acc:0.974]
Epoch [99/120    avg_loss:0.064, val_acc:0.982]
Epoch [100/120    avg_loss:0.075, val_acc:0.980]
Epoch [101/120    avg_loss:0.069, val_acc:0.980]
Epoch [102/120    avg_loss:0.068, val_acc:0.976]
Epoch [103/120    avg_loss:0.071, val_acc:0.982]
Epoch [104/120    avg_loss:0.060, val_acc:0.976]
Epoch [105/120    avg_loss:0.056, val_acc:0.972]
Epoch [106/120    avg_loss:0.055, val_acc:0.978]
Epoch [107/120    avg_loss:0.060, val_acc:0.974]
Epoch [108/120    avg_loss:0.053, val_acc:0.974]
Epoch [109/120    avg_loss:0.066, val_acc:0.970]
Epoch [110/120    avg_loss:0.067, val_acc:0.974]
Epoch [111/120    avg_loss:0.051, val_acc:0.968]
Epoch [112/120    avg_loss:0.055, val_acc:0.978]
Epoch [113/120    avg_loss:0.058, val_acc:0.982]
Epoch [114/120    avg_loss:0.062, val_acc:0.970]
Epoch [115/120    avg_loss:0.056, val_acc:0.980]
Epoch [116/120    avg_loss:0.051, val_acc:0.972]
Epoch [117/120    avg_loss:0.055, val_acc:0.968]
Epoch [118/120    avg_loss:0.051, val_acc:0.980]
Epoch [119/120    avg_loss:0.061, val_acc:0.980]
Epoch [120/120    avg_loss:0.052, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 0.99854227 0.94305239 0.99782135 0.94199536 0.9201278
 0.99512195 0.87234043 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9864701603341419
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83c2fada90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.204]
Epoch [2/120    avg_loss:2.502, val_acc:0.317]
Epoch [3/120    avg_loss:2.385, val_acc:0.478]
Epoch [4/120    avg_loss:2.298, val_acc:0.544]
Epoch [5/120    avg_loss:2.202, val_acc:0.506]
Epoch [6/120    avg_loss:2.115, val_acc:0.571]
Epoch [7/120    avg_loss:2.026, val_acc:0.609]
Epoch [8/120    avg_loss:1.911, val_acc:0.655]
Epoch [9/120    avg_loss:1.791, val_acc:0.671]
Epoch [10/120    avg_loss:1.694, val_acc:0.696]
Epoch [11/120    avg_loss:1.608, val_acc:0.748]
Epoch [12/120    avg_loss:1.522, val_acc:0.754]
Epoch [13/120    avg_loss:1.375, val_acc:0.728]
Epoch [14/120    avg_loss:1.242, val_acc:0.780]
Epoch [15/120    avg_loss:1.132, val_acc:0.871]
Epoch [16/120    avg_loss:1.020, val_acc:0.887]
Epoch [17/120    avg_loss:0.917, val_acc:0.907]
Epoch [18/120    avg_loss:0.925, val_acc:0.907]
Epoch [19/120    avg_loss:0.810, val_acc:0.893]
Epoch [20/120    avg_loss:0.737, val_acc:0.923]
Epoch [21/120    avg_loss:0.654, val_acc:0.921]
Epoch [22/120    avg_loss:0.670, val_acc:0.905]
Epoch [23/120    avg_loss:0.602, val_acc:0.909]
Epoch [24/120    avg_loss:0.552, val_acc:0.917]
Epoch [25/120    avg_loss:0.545, val_acc:0.873]
Epoch [26/120    avg_loss:0.528, val_acc:0.938]
Epoch [27/120    avg_loss:0.487, val_acc:0.942]
Epoch [28/120    avg_loss:0.435, val_acc:0.921]
Epoch [29/120    avg_loss:0.460, val_acc:0.921]
Epoch [30/120    avg_loss:0.465, val_acc:0.927]
Epoch [31/120    avg_loss:0.451, val_acc:0.929]
Epoch [32/120    avg_loss:0.456, val_acc:0.911]
Epoch [33/120    avg_loss:0.446, val_acc:0.938]
Epoch [34/120    avg_loss:0.455, val_acc:0.931]
Epoch [35/120    avg_loss:0.412, val_acc:0.929]
Epoch [36/120    avg_loss:0.367, val_acc:0.946]
Epoch [37/120    avg_loss:0.357, val_acc:0.946]
Epoch [38/120    avg_loss:0.309, val_acc:0.950]
Epoch [39/120    avg_loss:0.370, val_acc:0.942]
Epoch [40/120    avg_loss:0.368, val_acc:0.950]
Epoch [41/120    avg_loss:0.294, val_acc:0.954]
Epoch [42/120    avg_loss:0.296, val_acc:0.946]
Epoch [43/120    avg_loss:0.343, val_acc:0.944]
Epoch [44/120    avg_loss:0.300, val_acc:0.956]
Epoch [45/120    avg_loss:0.293, val_acc:0.950]
Epoch [46/120    avg_loss:0.239, val_acc:0.909]
Epoch [47/120    avg_loss:0.270, val_acc:0.970]
Epoch [48/120    avg_loss:0.232, val_acc:0.968]
Epoch [49/120    avg_loss:0.213, val_acc:0.968]
Epoch [50/120    avg_loss:0.260, val_acc:0.944]
Epoch [51/120    avg_loss:0.256, val_acc:0.960]
Epoch [52/120    avg_loss:0.234, val_acc:0.962]
Epoch [53/120    avg_loss:0.251, val_acc:0.964]
Epoch [54/120    avg_loss:0.228, val_acc:0.970]
Epoch [55/120    avg_loss:0.199, val_acc:0.974]
Epoch [56/120    avg_loss:0.191, val_acc:0.960]
Epoch [57/120    avg_loss:0.198, val_acc:0.972]
Epoch [58/120    avg_loss:0.177, val_acc:0.980]
Epoch [59/120    avg_loss:0.140, val_acc:0.974]
Epoch [60/120    avg_loss:0.175, val_acc:0.968]
Epoch [61/120    avg_loss:0.225, val_acc:0.982]
Epoch [62/120    avg_loss:0.195, val_acc:0.952]
Epoch [63/120    avg_loss:0.197, val_acc:0.968]
Epoch [64/120    avg_loss:0.161, val_acc:0.960]
Epoch [65/120    avg_loss:0.157, val_acc:0.974]
Epoch [66/120    avg_loss:0.145, val_acc:0.972]
Epoch [67/120    avg_loss:0.150, val_acc:0.958]
Epoch [68/120    avg_loss:0.135, val_acc:0.964]
Epoch [69/120    avg_loss:0.133, val_acc:0.958]
Epoch [70/120    avg_loss:0.129, val_acc:0.978]
Epoch [71/120    avg_loss:0.140, val_acc:0.976]
Epoch [72/120    avg_loss:0.149, val_acc:0.948]
Epoch [73/120    avg_loss:0.144, val_acc:0.970]
Epoch [74/120    avg_loss:0.144, val_acc:0.968]
Epoch [75/120    avg_loss:0.103, val_acc:0.984]
Epoch [76/120    avg_loss:0.087, val_acc:0.984]
Epoch [77/120    avg_loss:0.085, val_acc:0.986]
Epoch [78/120    avg_loss:0.085, val_acc:0.986]
Epoch [79/120    avg_loss:0.077, val_acc:0.988]
Epoch [80/120    avg_loss:0.079, val_acc:0.988]
Epoch [81/120    avg_loss:0.074, val_acc:0.988]
Epoch [82/120    avg_loss:0.071, val_acc:0.988]
Epoch [83/120    avg_loss:0.073, val_acc:0.988]
Epoch [84/120    avg_loss:0.074, val_acc:0.986]
Epoch [85/120    avg_loss:0.073, val_acc:0.986]
Epoch [86/120    avg_loss:0.078, val_acc:0.986]
Epoch [87/120    avg_loss:0.067, val_acc:0.986]
Epoch [88/120    avg_loss:0.067, val_acc:0.988]
Epoch [89/120    avg_loss:0.071, val_acc:0.988]
Epoch [90/120    avg_loss:0.068, val_acc:0.990]
Epoch [91/120    avg_loss:0.068, val_acc:0.988]
Epoch [92/120    avg_loss:0.078, val_acc:0.986]
Epoch [93/120    avg_loss:0.055, val_acc:0.988]
Epoch [94/120    avg_loss:0.063, val_acc:0.990]
Epoch [95/120    avg_loss:0.055, val_acc:0.986]
Epoch [96/120    avg_loss:0.070, val_acc:0.988]
Epoch [97/120    avg_loss:0.064, val_acc:0.986]
Epoch [98/120    avg_loss:0.062, val_acc:0.988]
Epoch [99/120    avg_loss:0.065, val_acc:0.986]
Epoch [100/120    avg_loss:0.058, val_acc:0.986]
Epoch [101/120    avg_loss:0.055, val_acc:0.988]
Epoch [102/120    avg_loss:0.060, val_acc:0.986]
Epoch [103/120    avg_loss:0.060, val_acc:0.986]
Epoch [104/120    avg_loss:0.068, val_acc:0.988]
Epoch [105/120    avg_loss:0.059, val_acc:0.986]
Epoch [106/120    avg_loss:0.054, val_acc:0.988]
Epoch [107/120    avg_loss:0.065, val_acc:0.986]
Epoch [108/120    avg_loss:0.060, val_acc:0.986]
Epoch [109/120    avg_loss:0.057, val_acc:0.986]
Epoch [110/120    avg_loss:0.059, val_acc:0.986]
Epoch [111/120    avg_loss:0.055, val_acc:0.986]
Epoch [112/120    avg_loss:0.055, val_acc:0.988]
Epoch [113/120    avg_loss:0.049, val_acc:0.988]
Epoch [114/120    avg_loss:0.064, val_acc:0.986]
Epoch [115/120    avg_loss:0.053, val_acc:0.986]
Epoch [116/120    avg_loss:0.058, val_acc:0.988]
Epoch [117/120    avg_loss:0.062, val_acc:0.986]
Epoch [118/120    avg_loss:0.053, val_acc:0.986]
Epoch [119/120    avg_loss:0.056, val_acc:0.986]
Epoch [120/120    avg_loss:0.052, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 223   4   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.97333333 0.98454746 0.92778993 0.90034364
 1.         0.94382022 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9890797389625338
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc5e45c7a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.060]
Epoch [2/120    avg_loss:2.491, val_acc:0.282]
Epoch [3/120    avg_loss:2.392, val_acc:0.413]
Epoch [4/120    avg_loss:2.287, val_acc:0.431]
Epoch [5/120    avg_loss:2.180, val_acc:0.431]
Epoch [6/120    avg_loss:2.099, val_acc:0.486]
Epoch [7/120    avg_loss:1.981, val_acc:0.534]
Epoch [8/120    avg_loss:1.879, val_acc:0.611]
Epoch [9/120    avg_loss:1.759, val_acc:0.641]
Epoch [10/120    avg_loss:1.630, val_acc:0.641]
Epoch [11/120    avg_loss:1.526, val_acc:0.659]
Epoch [12/120    avg_loss:1.392, val_acc:0.704]
Epoch [13/120    avg_loss:1.284, val_acc:0.702]
Epoch [14/120    avg_loss:1.208, val_acc:0.760]
Epoch [15/120    avg_loss:1.070, val_acc:0.833]
Epoch [16/120    avg_loss:0.991, val_acc:0.825]
Epoch [17/120    avg_loss:0.920, val_acc:0.827]
Epoch [18/120    avg_loss:0.848, val_acc:0.887]
Epoch [19/120    avg_loss:0.793, val_acc:0.871]
Epoch [20/120    avg_loss:0.744, val_acc:0.885]
Epoch [21/120    avg_loss:0.663, val_acc:0.875]
Epoch [22/120    avg_loss:0.662, val_acc:0.885]
Epoch [23/120    avg_loss:0.643, val_acc:0.899]
Epoch [24/120    avg_loss:0.545, val_acc:0.907]
Epoch [25/120    avg_loss:0.516, val_acc:0.909]
Epoch [26/120    avg_loss:0.505, val_acc:0.903]
Epoch [27/120    avg_loss:0.503, val_acc:0.893]
Epoch [28/120    avg_loss:0.468, val_acc:0.905]
Epoch [29/120    avg_loss:0.465, val_acc:0.909]
Epoch [30/120    avg_loss:0.440, val_acc:0.917]
Epoch [31/120    avg_loss:0.398, val_acc:0.919]
Epoch [32/120    avg_loss:0.377, val_acc:0.919]
Epoch [33/120    avg_loss:0.374, val_acc:0.927]
Epoch [34/120    avg_loss:0.458, val_acc:0.919]
Epoch [35/120    avg_loss:0.467, val_acc:0.901]
Epoch [36/120    avg_loss:0.378, val_acc:0.923]
Epoch [37/120    avg_loss:0.364, val_acc:0.913]
Epoch [38/120    avg_loss:0.353, val_acc:0.933]
Epoch [39/120    avg_loss:0.279, val_acc:0.905]
Epoch [40/120    avg_loss:0.352, val_acc:0.927]
Epoch [41/120    avg_loss:0.373, val_acc:0.915]
Epoch [42/120    avg_loss:0.317, val_acc:0.942]
Epoch [43/120    avg_loss:0.281, val_acc:0.935]
Epoch [44/120    avg_loss:0.262, val_acc:0.927]
Epoch [45/120    avg_loss:0.239, val_acc:0.938]
Epoch [46/120    avg_loss:0.264, val_acc:0.935]
Epoch [47/120    avg_loss:0.283, val_acc:0.942]
Epoch [48/120    avg_loss:0.295, val_acc:0.903]
Epoch [49/120    avg_loss:0.297, val_acc:0.948]
Epoch [50/120    avg_loss:0.244, val_acc:0.944]
Epoch [51/120    avg_loss:0.246, val_acc:0.938]
Epoch [52/120    avg_loss:0.220, val_acc:0.950]
Epoch [53/120    avg_loss:0.254, val_acc:0.954]
Epoch [54/120    avg_loss:0.203, val_acc:0.952]
Epoch [55/120    avg_loss:0.174, val_acc:0.935]
Epoch [56/120    avg_loss:0.207, val_acc:0.946]
Epoch [57/120    avg_loss:0.205, val_acc:0.968]
Epoch [58/120    avg_loss:0.186, val_acc:0.964]
Epoch [59/120    avg_loss:0.162, val_acc:0.952]
Epoch [60/120    avg_loss:0.212, val_acc:0.923]
Epoch [61/120    avg_loss:0.214, val_acc:0.964]
Epoch [62/120    avg_loss:0.181, val_acc:0.954]
Epoch [63/120    avg_loss:0.176, val_acc:0.960]
Epoch [64/120    avg_loss:0.148, val_acc:0.964]
Epoch [65/120    avg_loss:0.174, val_acc:0.946]
Epoch [66/120    avg_loss:0.168, val_acc:0.919]
Epoch [67/120    avg_loss:0.231, val_acc:0.958]
Epoch [68/120    avg_loss:0.124, val_acc:0.962]
Epoch [69/120    avg_loss:0.114, val_acc:0.962]
Epoch [70/120    avg_loss:0.121, val_acc:0.970]
Epoch [71/120    avg_loss:0.153, val_acc:0.966]
Epoch [72/120    avg_loss:0.167, val_acc:0.960]
Epoch [73/120    avg_loss:0.117, val_acc:0.968]
Epoch [74/120    avg_loss:0.136, val_acc:0.954]
Epoch [75/120    avg_loss:0.121, val_acc:0.974]
Epoch [76/120    avg_loss:0.100, val_acc:0.968]
Epoch [77/120    avg_loss:0.092, val_acc:0.978]
Epoch [78/120    avg_loss:0.112, val_acc:0.980]
Epoch [79/120    avg_loss:0.078, val_acc:0.976]
Epoch [80/120    avg_loss:0.079, val_acc:0.972]
Epoch [81/120    avg_loss:0.063, val_acc:0.968]
Epoch [82/120    avg_loss:0.079, val_acc:0.978]
Epoch [83/120    avg_loss:0.087, val_acc:0.970]
Epoch [84/120    avg_loss:0.094, val_acc:0.980]
Epoch [85/120    avg_loss:0.065, val_acc:0.980]
Epoch [86/120    avg_loss:0.061, val_acc:0.978]
Epoch [87/120    avg_loss:0.105, val_acc:0.954]
Epoch [88/120    avg_loss:0.130, val_acc:0.964]
Epoch [89/120    avg_loss:0.109, val_acc:0.948]
Epoch [90/120    avg_loss:0.131, val_acc:0.966]
Epoch [91/120    avg_loss:0.129, val_acc:0.972]
Epoch [92/120    avg_loss:0.097, val_acc:0.966]
Epoch [93/120    avg_loss:0.079, val_acc:0.972]
Epoch [94/120    avg_loss:0.105, val_acc:0.972]
Epoch [95/120    avg_loss:0.100, val_acc:0.962]
Epoch [96/120    avg_loss:0.160, val_acc:0.946]
Epoch [97/120    avg_loss:0.080, val_acc:0.974]
Epoch [98/120    avg_loss:0.079, val_acc:0.978]
Epoch [99/120    avg_loss:0.068, val_acc:0.976]
Epoch [100/120    avg_loss:0.049, val_acc:0.982]
Epoch [101/120    avg_loss:0.052, val_acc:0.982]
Epoch [102/120    avg_loss:0.045, val_acc:0.984]
Epoch [103/120    avg_loss:0.039, val_acc:0.984]
Epoch [104/120    avg_loss:0.039, val_acc:0.986]
Epoch [105/120    avg_loss:0.038, val_acc:0.986]
Epoch [106/120    avg_loss:0.042, val_acc:0.986]
Epoch [107/120    avg_loss:0.043, val_acc:0.984]
Epoch [108/120    avg_loss:0.040, val_acc:0.984]
Epoch [109/120    avg_loss:0.044, val_acc:0.984]
Epoch [110/120    avg_loss:0.037, val_acc:0.986]
Epoch [111/120    avg_loss:0.037, val_acc:0.988]
Epoch [112/120    avg_loss:0.031, val_acc:0.988]
Epoch [113/120    avg_loss:0.039, val_acc:0.990]
Epoch [114/120    avg_loss:0.038, val_acc:0.988]
Epoch [115/120    avg_loss:0.034, val_acc:0.988]
Epoch [116/120    avg_loss:0.035, val_acc:0.988]
Epoch [117/120    avg_loss:0.042, val_acc:0.988]
Epoch [118/120    avg_loss:0.042, val_acc:0.986]
Epoch [119/120    avg_loss:0.037, val_acc:0.988]
Epoch [120/120    avg_loss:0.035, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.97986577 0.98678414 0.91028446 0.89932886
 0.98771499 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.988130593236493
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f7cd78ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.355]
Epoch [2/120    avg_loss:2.499, val_acc:0.423]
Epoch [3/120    avg_loss:2.404, val_acc:0.450]
Epoch [4/120    avg_loss:2.339, val_acc:0.472]
Epoch [5/120    avg_loss:2.260, val_acc:0.520]
Epoch [6/120    avg_loss:2.182, val_acc:0.591]
Epoch [7/120    avg_loss:2.101, val_acc:0.605]
Epoch [8/120    avg_loss:2.004, val_acc:0.599]
Epoch [9/120    avg_loss:1.907, val_acc:0.579]
Epoch [10/120    avg_loss:1.784, val_acc:0.587]
Epoch [11/120    avg_loss:1.657, val_acc:0.631]
Epoch [12/120    avg_loss:1.552, val_acc:0.649]
Epoch [13/120    avg_loss:1.441, val_acc:0.651]
Epoch [14/120    avg_loss:1.349, val_acc:0.688]
Epoch [15/120    avg_loss:1.268, val_acc:0.704]
Epoch [16/120    avg_loss:1.150, val_acc:0.738]
Epoch [17/120    avg_loss:1.041, val_acc:0.762]
Epoch [18/120    avg_loss:1.036, val_acc:0.768]
Epoch [19/120    avg_loss:0.942, val_acc:0.808]
Epoch [20/120    avg_loss:0.837, val_acc:0.863]
Epoch [21/120    avg_loss:0.759, val_acc:0.863]
Epoch [22/120    avg_loss:0.769, val_acc:0.831]
Epoch [23/120    avg_loss:0.843, val_acc:0.881]
Epoch [24/120    avg_loss:0.670, val_acc:0.867]
Epoch [25/120    avg_loss:0.623, val_acc:0.903]
Epoch [26/120    avg_loss:0.569, val_acc:0.887]
Epoch [27/120    avg_loss:0.587, val_acc:0.877]
Epoch [28/120    avg_loss:0.559, val_acc:0.845]
Epoch [29/120    avg_loss:0.571, val_acc:0.903]
Epoch [30/120    avg_loss:0.541, val_acc:0.891]
Epoch [31/120    avg_loss:0.529, val_acc:0.895]
Epoch [32/120    avg_loss:0.518, val_acc:0.903]
Epoch [33/120    avg_loss:0.543, val_acc:0.905]
Epoch [34/120    avg_loss:0.479, val_acc:0.915]
Epoch [35/120    avg_loss:0.422, val_acc:0.891]
Epoch [36/120    avg_loss:0.438, val_acc:0.905]
Epoch [37/120    avg_loss:0.384, val_acc:0.917]
Epoch [38/120    avg_loss:0.372, val_acc:0.937]
Epoch [39/120    avg_loss:0.378, val_acc:0.929]
Epoch [40/120    avg_loss:0.384, val_acc:0.867]
Epoch [41/120    avg_loss:0.380, val_acc:0.905]
Epoch [42/120    avg_loss:0.302, val_acc:0.915]
Epoch [43/120    avg_loss:0.309, val_acc:0.925]
Epoch [44/120    avg_loss:0.296, val_acc:0.933]
Epoch [45/120    avg_loss:0.330, val_acc:0.931]
Epoch [46/120    avg_loss:0.289, val_acc:0.917]
Epoch [47/120    avg_loss:0.262, val_acc:0.948]
Epoch [48/120    avg_loss:0.275, val_acc:0.927]
Epoch [49/120    avg_loss:0.261, val_acc:0.937]
Epoch [50/120    avg_loss:0.269, val_acc:0.933]
Epoch [51/120    avg_loss:0.289, val_acc:0.938]
Epoch [52/120    avg_loss:0.233, val_acc:0.954]
Epoch [53/120    avg_loss:0.201, val_acc:0.944]
Epoch [54/120    avg_loss:0.239, val_acc:0.942]
Epoch [55/120    avg_loss:0.213, val_acc:0.942]
Epoch [56/120    avg_loss:0.218, val_acc:0.944]
Epoch [57/120    avg_loss:0.226, val_acc:0.935]
Epoch [58/120    avg_loss:0.219, val_acc:0.946]
Epoch [59/120    avg_loss:0.208, val_acc:0.933]
Epoch [60/120    avg_loss:0.283, val_acc:0.950]
Epoch [61/120    avg_loss:0.185, val_acc:0.940]
Epoch [62/120    avg_loss:0.225, val_acc:0.952]
Epoch [63/120    avg_loss:0.227, val_acc:0.940]
Epoch [64/120    avg_loss:0.231, val_acc:0.921]
Epoch [65/120    avg_loss:0.207, val_acc:0.968]
Epoch [66/120    avg_loss:0.205, val_acc:0.954]
Epoch [67/120    avg_loss:0.188, val_acc:0.944]
Epoch [68/120    avg_loss:0.179, val_acc:0.938]
Epoch [69/120    avg_loss:0.174, val_acc:0.964]
Epoch [70/120    avg_loss:0.213, val_acc:0.964]
Epoch [71/120    avg_loss:0.152, val_acc:0.966]
Epoch [72/120    avg_loss:0.144, val_acc:0.958]
Epoch [73/120    avg_loss:0.149, val_acc:0.935]
Epoch [74/120    avg_loss:0.148, val_acc:0.972]
Epoch [75/120    avg_loss:0.155, val_acc:0.964]
Epoch [76/120    avg_loss:0.141, val_acc:0.964]
Epoch [77/120    avg_loss:0.153, val_acc:0.962]
Epoch [78/120    avg_loss:0.120, val_acc:0.970]
Epoch [79/120    avg_loss:0.139, val_acc:0.946]
Epoch [80/120    avg_loss:0.115, val_acc:0.960]
Epoch [81/120    avg_loss:0.093, val_acc:0.980]
Epoch [82/120    avg_loss:0.119, val_acc:0.962]
Epoch [83/120    avg_loss:0.141, val_acc:0.942]
Epoch [84/120    avg_loss:0.184, val_acc:0.944]
Epoch [85/120    avg_loss:0.214, val_acc:0.960]
Epoch [86/120    avg_loss:0.152, val_acc:0.970]
Epoch [87/120    avg_loss:0.163, val_acc:0.958]
Epoch [88/120    avg_loss:0.133, val_acc:0.976]
Epoch [89/120    avg_loss:0.132, val_acc:0.978]
Epoch [90/120    avg_loss:0.093, val_acc:0.974]
Epoch [91/120    avg_loss:0.087, val_acc:0.952]
Epoch [92/120    avg_loss:0.106, val_acc:0.978]
Epoch [93/120    avg_loss:0.104, val_acc:0.988]
Epoch [94/120    avg_loss:0.078, val_acc:0.962]
Epoch [95/120    avg_loss:0.093, val_acc:0.970]
Epoch [96/120    avg_loss:0.179, val_acc:0.952]
Epoch [97/120    avg_loss:0.109, val_acc:0.974]
Epoch [98/120    avg_loss:0.095, val_acc:0.980]
Epoch [99/120    avg_loss:0.112, val_acc:0.962]
Epoch [100/120    avg_loss:0.161, val_acc:0.921]
Epoch [101/120    avg_loss:0.133, val_acc:0.980]
Epoch [102/120    avg_loss:0.112, val_acc:0.976]
Epoch [103/120    avg_loss:0.105, val_acc:0.960]
Epoch [104/120    avg_loss:0.117, val_acc:0.976]
Epoch [105/120    avg_loss:0.081, val_acc:0.988]
Epoch [106/120    avg_loss:0.084, val_acc:0.982]
Epoch [107/120    avg_loss:0.112, val_acc:0.978]
Epoch [108/120    avg_loss:0.051, val_acc:0.980]
Epoch [109/120    avg_loss:0.084, val_acc:0.982]
Epoch [110/120    avg_loss:0.069, val_acc:0.968]
Epoch [111/120    avg_loss:0.077, val_acc:0.978]
Epoch [112/120    avg_loss:0.057, val_acc:0.990]
Epoch [113/120    avg_loss:0.051, val_acc:0.994]
Epoch [114/120    avg_loss:0.074, val_acc:0.980]
Epoch [115/120    avg_loss:0.055, val_acc:0.988]
Epoch [116/120    avg_loss:0.056, val_acc:0.976]
Epoch [117/120    avg_loss:0.048, val_acc:0.990]
Epoch [118/120    avg_loss:0.030, val_acc:0.986]
Epoch [119/120    avg_loss:0.044, val_acc:0.988]
Epoch [120/120    avg_loss:0.033, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 216   9   0   0   0   5   0   0   0   0   0]
 [  0   0   0   1 206  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.98398169 0.96644295 0.90549451 0.9
 0.99266504 0.96296296 0.99359795 1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9867067677149377
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb234f8dac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.538, val_acc:0.343]
Epoch [2/120    avg_loss:2.407, val_acc:0.327]
Epoch [3/120    avg_loss:2.314, val_acc:0.315]
Epoch [4/120    avg_loss:2.227, val_acc:0.339]
Epoch [5/120    avg_loss:2.145, val_acc:0.359]
Epoch [6/120    avg_loss:2.057, val_acc:0.405]
Epoch [7/120    avg_loss:1.956, val_acc:0.526]
Epoch [8/120    avg_loss:1.861, val_acc:0.587]
Epoch [9/120    avg_loss:1.777, val_acc:0.635]
Epoch [10/120    avg_loss:1.686, val_acc:0.647]
Epoch [11/120    avg_loss:1.574, val_acc:0.698]
Epoch [12/120    avg_loss:1.458, val_acc:0.754]
Epoch [13/120    avg_loss:1.356, val_acc:0.698]
Epoch [14/120    avg_loss:1.259, val_acc:0.760]
Epoch [15/120    avg_loss:1.113, val_acc:0.857]
Epoch [16/120    avg_loss:1.006, val_acc:0.881]
Epoch [17/120    avg_loss:0.932, val_acc:0.895]
Epoch [18/120    avg_loss:0.859, val_acc:0.883]
Epoch [19/120    avg_loss:0.796, val_acc:0.875]
Epoch [20/120    avg_loss:0.735, val_acc:0.873]
Epoch [21/120    avg_loss:0.670, val_acc:0.901]
Epoch [22/120    avg_loss:0.638, val_acc:0.899]
Epoch [23/120    avg_loss:0.553, val_acc:0.885]
Epoch [24/120    avg_loss:0.574, val_acc:0.919]
Epoch [25/120    avg_loss:0.544, val_acc:0.933]
Epoch [26/120    avg_loss:0.491, val_acc:0.931]
Epoch [27/120    avg_loss:0.437, val_acc:0.931]
Epoch [28/120    avg_loss:0.435, val_acc:0.899]
Epoch [29/120    avg_loss:0.502, val_acc:0.923]
Epoch [30/120    avg_loss:0.445, val_acc:0.923]
Epoch [31/120    avg_loss:0.388, val_acc:0.925]
Epoch [32/120    avg_loss:0.402, val_acc:0.923]
Epoch [33/120    avg_loss:0.368, val_acc:0.948]
Epoch [34/120    avg_loss:0.355, val_acc:0.935]
Epoch [35/120    avg_loss:0.357, val_acc:0.940]
Epoch [36/120    avg_loss:0.389, val_acc:0.938]
Epoch [37/120    avg_loss:0.371, val_acc:0.946]
Epoch [38/120    avg_loss:0.337, val_acc:0.927]
Epoch [39/120    avg_loss:0.337, val_acc:0.952]
Epoch [40/120    avg_loss:0.267, val_acc:0.952]
Epoch [41/120    avg_loss:0.275, val_acc:0.950]
Epoch [42/120    avg_loss:0.259, val_acc:0.962]
Epoch [43/120    avg_loss:0.295, val_acc:0.954]
Epoch [44/120    avg_loss:0.257, val_acc:0.964]
Epoch [45/120    avg_loss:0.239, val_acc:0.954]
Epoch [46/120    avg_loss:0.222, val_acc:0.966]
Epoch [47/120    avg_loss:0.227, val_acc:0.964]
Epoch [48/120    avg_loss:0.231, val_acc:0.968]
Epoch [49/120    avg_loss:0.225, val_acc:0.958]
Epoch [50/120    avg_loss:0.202, val_acc:0.976]
Epoch [51/120    avg_loss:0.212, val_acc:0.966]
Epoch [52/120    avg_loss:0.193, val_acc:0.956]
Epoch [53/120    avg_loss:0.216, val_acc:0.972]
Epoch [54/120    avg_loss:0.167, val_acc:0.972]
Epoch [55/120    avg_loss:0.168, val_acc:0.966]
Epoch [56/120    avg_loss:0.172, val_acc:0.980]
Epoch [57/120    avg_loss:0.219, val_acc:0.970]
Epoch [58/120    avg_loss:0.181, val_acc:0.962]
Epoch [59/120    avg_loss:0.162, val_acc:0.958]
Epoch [60/120    avg_loss:0.152, val_acc:0.976]
Epoch [61/120    avg_loss:0.150, val_acc:0.976]
Epoch [62/120    avg_loss:0.174, val_acc:0.972]
Epoch [63/120    avg_loss:0.140, val_acc:0.978]
Epoch [64/120    avg_loss:0.146, val_acc:0.972]
Epoch [65/120    avg_loss:0.147, val_acc:0.978]
Epoch [66/120    avg_loss:0.136, val_acc:0.974]
Epoch [67/120    avg_loss:0.144, val_acc:0.956]
Epoch [68/120    avg_loss:0.118, val_acc:0.978]
Epoch [69/120    avg_loss:0.147, val_acc:0.970]
Epoch [70/120    avg_loss:0.126, val_acc:0.980]
Epoch [71/120    avg_loss:0.105, val_acc:0.980]
Epoch [72/120    avg_loss:0.087, val_acc:0.984]
Epoch [73/120    avg_loss:0.084, val_acc:0.980]
Epoch [74/120    avg_loss:0.094, val_acc:0.982]
Epoch [75/120    avg_loss:0.079, val_acc:0.978]
Epoch [76/120    avg_loss:0.083, val_acc:0.984]
Epoch [77/120    avg_loss:0.079, val_acc:0.980]
Epoch [78/120    avg_loss:0.077, val_acc:0.982]
Epoch [79/120    avg_loss:0.074, val_acc:0.984]
Epoch [80/120    avg_loss:0.074, val_acc:0.984]
Epoch [81/120    avg_loss:0.066, val_acc:0.982]
Epoch [82/120    avg_loss:0.089, val_acc:0.984]
Epoch [83/120    avg_loss:0.070, val_acc:0.986]
Epoch [84/120    avg_loss:0.076, val_acc:0.986]
Epoch [85/120    avg_loss:0.066, val_acc:0.984]
Epoch [86/120    avg_loss:0.067, val_acc:0.982]
Epoch [87/120    avg_loss:0.064, val_acc:0.984]
Epoch [88/120    avg_loss:0.073, val_acc:0.986]
Epoch [89/120    avg_loss:0.061, val_acc:0.986]
Epoch [90/120    avg_loss:0.068, val_acc:0.982]
Epoch [91/120    avg_loss:0.069, val_acc:0.982]
Epoch [92/120    avg_loss:0.069, val_acc:0.984]
Epoch [93/120    avg_loss:0.069, val_acc:0.984]
Epoch [94/120    avg_loss:0.068, val_acc:0.982]
Epoch [95/120    avg_loss:0.061, val_acc:0.984]
Epoch [96/120    avg_loss:0.056, val_acc:0.982]
Epoch [97/120    avg_loss:0.071, val_acc:0.984]
Epoch [98/120    avg_loss:0.070, val_acc:0.984]
Epoch [99/120    avg_loss:0.069, val_acc:0.982]
Epoch [100/120    avg_loss:0.070, val_acc:0.984]
Epoch [101/120    avg_loss:0.059, val_acc:0.982]
Epoch [102/120    avg_loss:0.064, val_acc:0.984]
Epoch [103/120    avg_loss:0.063, val_acc:0.984]
Epoch [104/120    avg_loss:0.067, val_acc:0.984]
Epoch [105/120    avg_loss:0.055, val_acc:0.984]
Epoch [106/120    avg_loss:0.070, val_acc:0.984]
Epoch [107/120    avg_loss:0.059, val_acc:0.984]
Epoch [108/120    avg_loss:0.057, val_acc:0.984]
Epoch [109/120    avg_loss:0.053, val_acc:0.984]
Epoch [110/120    avg_loss:0.069, val_acc:0.984]
Epoch [111/120    avg_loss:0.060, val_acc:0.984]
Epoch [112/120    avg_loss:0.068, val_acc:0.984]
Epoch [113/120    avg_loss:0.051, val_acc:0.984]
Epoch [114/120    avg_loss:0.049, val_acc:0.984]
Epoch [115/120    avg_loss:0.057, val_acc:0.984]
Epoch [116/120    avg_loss:0.049, val_acc:0.984]
Epoch [117/120    avg_loss:0.056, val_acc:0.984]
Epoch [118/120    avg_loss:0.054, val_acc:0.984]
Epoch [119/120    avg_loss:0.061, val_acc:0.984]
Epoch [120/120    avg_loss:0.053, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   1 224   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.97104677 0.98678414 0.90712743 0.86713287
 1.         0.93258427 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9867055942094218
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77a18d4a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.171]
Epoch [2/120    avg_loss:2.496, val_acc:0.367]
Epoch [3/120    avg_loss:2.391, val_acc:0.437]
Epoch [4/120    avg_loss:2.317, val_acc:0.482]
Epoch [5/120    avg_loss:2.253, val_acc:0.425]
Epoch [6/120    avg_loss:2.165, val_acc:0.442]
Epoch [7/120    avg_loss:2.080, val_acc:0.492]
Epoch [8/120    avg_loss:1.990, val_acc:0.548]
Epoch [9/120    avg_loss:1.899, val_acc:0.556]
Epoch [10/120    avg_loss:1.803, val_acc:0.694]
Epoch [11/120    avg_loss:1.701, val_acc:0.613]
Epoch [12/120    avg_loss:1.603, val_acc:0.629]
Epoch [13/120    avg_loss:1.483, val_acc:0.740]
Epoch [14/120    avg_loss:1.343, val_acc:0.794]
Epoch [15/120    avg_loss:1.258, val_acc:0.825]
Epoch [16/120    avg_loss:1.167, val_acc:0.827]
Epoch [17/120    avg_loss:1.076, val_acc:0.845]
Epoch [18/120    avg_loss:0.973, val_acc:0.891]
Epoch [19/120    avg_loss:0.894, val_acc:0.871]
Epoch [20/120    avg_loss:0.849, val_acc:0.893]
Epoch [21/120    avg_loss:0.795, val_acc:0.897]
Epoch [22/120    avg_loss:0.731, val_acc:0.851]
Epoch [23/120    avg_loss:0.697, val_acc:0.889]
Epoch [24/120    avg_loss:0.663, val_acc:0.881]
Epoch [25/120    avg_loss:0.639, val_acc:0.899]
Epoch [26/120    avg_loss:0.601, val_acc:0.889]
Epoch [27/120    avg_loss:0.610, val_acc:0.893]
Epoch [28/120    avg_loss:0.605, val_acc:0.889]
Epoch [29/120    avg_loss:0.535, val_acc:0.903]
Epoch [30/120    avg_loss:0.505, val_acc:0.913]
Epoch [31/120    avg_loss:0.463, val_acc:0.903]
Epoch [32/120    avg_loss:0.529, val_acc:0.849]
Epoch [33/120    avg_loss:0.540, val_acc:0.917]
Epoch [34/120    avg_loss:0.471, val_acc:0.919]
Epoch [35/120    avg_loss:0.426, val_acc:0.925]
Epoch [36/120    avg_loss:0.398, val_acc:0.907]
Epoch [37/120    avg_loss:0.379, val_acc:0.911]
Epoch [38/120    avg_loss:0.386, val_acc:0.897]
Epoch [39/120    avg_loss:0.352, val_acc:0.917]
Epoch [40/120    avg_loss:0.400, val_acc:0.919]
Epoch [41/120    avg_loss:0.456, val_acc:0.881]
Epoch [42/120    avg_loss:0.460, val_acc:0.881]
Epoch [43/120    avg_loss:0.390, val_acc:0.923]
Epoch [44/120    avg_loss:0.334, val_acc:0.915]
Epoch [45/120    avg_loss:0.344, val_acc:0.927]
Epoch [46/120    avg_loss:0.330, val_acc:0.923]
Epoch [47/120    avg_loss:0.313, val_acc:0.909]
Epoch [48/120    avg_loss:0.271, val_acc:0.940]
Epoch [49/120    avg_loss:0.283, val_acc:0.927]
Epoch [50/120    avg_loss:0.319, val_acc:0.917]
Epoch [51/120    avg_loss:0.263, val_acc:0.944]
Epoch [52/120    avg_loss:0.266, val_acc:0.919]
Epoch [53/120    avg_loss:0.292, val_acc:0.948]
Epoch [54/120    avg_loss:0.245, val_acc:0.919]
Epoch [55/120    avg_loss:0.296, val_acc:0.940]
Epoch [56/120    avg_loss:0.294, val_acc:0.937]
Epoch [57/120    avg_loss:0.221, val_acc:0.927]
Epoch [58/120    avg_loss:0.240, val_acc:0.952]
Epoch [59/120    avg_loss:0.247, val_acc:0.950]
Epoch [60/120    avg_loss:0.227, val_acc:0.940]
Epoch [61/120    avg_loss:0.227, val_acc:0.950]
Epoch [62/120    avg_loss:0.194, val_acc:0.938]
Epoch [63/120    avg_loss:0.264, val_acc:0.956]
Epoch [64/120    avg_loss:0.204, val_acc:0.960]
Epoch [65/120    avg_loss:0.171, val_acc:0.950]
Epoch [66/120    avg_loss:0.203, val_acc:0.944]
Epoch [67/120    avg_loss:0.159, val_acc:0.958]
Epoch [68/120    avg_loss:0.194, val_acc:0.944]
Epoch [69/120    avg_loss:0.177, val_acc:0.956]
Epoch [70/120    avg_loss:0.247, val_acc:0.964]
Epoch [71/120    avg_loss:0.242, val_acc:0.935]
Epoch [72/120    avg_loss:0.202, val_acc:0.925]
Epoch [73/120    avg_loss:0.192, val_acc:0.929]
Epoch [74/120    avg_loss:0.217, val_acc:0.938]
Epoch [75/120    avg_loss:0.183, val_acc:0.958]
Epoch [76/120    avg_loss:0.159, val_acc:0.960]
Epoch [77/120    avg_loss:0.167, val_acc:0.960]
Epoch [78/120    avg_loss:0.177, val_acc:0.942]
Epoch [79/120    avg_loss:0.172, val_acc:0.944]
Epoch [80/120    avg_loss:0.153, val_acc:0.950]
Epoch [81/120    avg_loss:0.157, val_acc:0.956]
Epoch [82/120    avg_loss:0.118, val_acc:0.952]
Epoch [83/120    avg_loss:0.136, val_acc:0.950]
Epoch [84/120    avg_loss:0.121, val_acc:0.958]
Epoch [85/120    avg_loss:0.106, val_acc:0.964]
Epoch [86/120    avg_loss:0.092, val_acc:0.962]
Epoch [87/120    avg_loss:0.088, val_acc:0.964]
Epoch [88/120    avg_loss:0.101, val_acc:0.964]
Epoch [89/120    avg_loss:0.086, val_acc:0.966]
Epoch [90/120    avg_loss:0.083, val_acc:0.968]
Epoch [91/120    avg_loss:0.101, val_acc:0.964]
Epoch [92/120    avg_loss:0.085, val_acc:0.964]
Epoch [93/120    avg_loss:0.076, val_acc:0.964]
Epoch [94/120    avg_loss:0.090, val_acc:0.964]
Epoch [95/120    avg_loss:0.083, val_acc:0.966]
Epoch [96/120    avg_loss:0.083, val_acc:0.962]
Epoch [97/120    avg_loss:0.093, val_acc:0.966]
Epoch [98/120    avg_loss:0.082, val_acc:0.962]
Epoch [99/120    avg_loss:0.076, val_acc:0.964]
Epoch [100/120    avg_loss:0.078, val_acc:0.964]
Epoch [101/120    avg_loss:0.094, val_acc:0.964]
Epoch [102/120    avg_loss:0.080, val_acc:0.966]
Epoch [103/120    avg_loss:0.081, val_acc:0.970]
Epoch [104/120    avg_loss:0.075, val_acc:0.966]
Epoch [105/120    avg_loss:0.097, val_acc:0.964]
Epoch [106/120    avg_loss:0.098, val_acc:0.968]
Epoch [107/120    avg_loss:0.085, val_acc:0.968]
Epoch [108/120    avg_loss:0.071, val_acc:0.968]
Epoch [109/120    avg_loss:0.086, val_acc:0.968]
Epoch [110/120    avg_loss:0.078, val_acc:0.966]
Epoch [111/120    avg_loss:0.080, val_acc:0.966]
Epoch [112/120    avg_loss:0.089, val_acc:0.968]
Epoch [113/120    avg_loss:0.069, val_acc:0.966]
Epoch [114/120    avg_loss:0.075, val_acc:0.968]
Epoch [115/120    avg_loss:0.074, val_acc:0.972]
Epoch [116/120    avg_loss:0.082, val_acc:0.970]
Epoch [117/120    avg_loss:0.068, val_acc:0.966]
Epoch [118/120    avg_loss:0.074, val_acc:0.966]
Epoch [119/120    avg_loss:0.076, val_acc:0.968]
Epoch [120/120    avg_loss:0.086, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 205   0   0   0   0  14   0   0   0   0   0   0]
 [  0   0   0 203  26   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   2  10 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.16631130063966

F1 scores:
[       nan 1.         0.94252874 0.93333333 0.87606838 0.88666667
 1.         0.86910995 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9795862580807349
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ce554ca90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.343]
Epoch [2/120    avg_loss:2.445, val_acc:0.331]
Epoch [3/120    avg_loss:2.351, val_acc:0.325]
Epoch [4/120    avg_loss:2.272, val_acc:0.411]
Epoch [5/120    avg_loss:2.191, val_acc:0.454]
Epoch [6/120    avg_loss:2.104, val_acc:0.512]
Epoch [7/120    avg_loss:2.023, val_acc:0.532]
Epoch [8/120    avg_loss:1.930, val_acc:0.631]
Epoch [9/120    avg_loss:1.831, val_acc:0.639]
Epoch [10/120    avg_loss:1.720, val_acc:0.683]
Epoch [11/120    avg_loss:1.620, val_acc:0.750]
Epoch [12/120    avg_loss:1.505, val_acc:0.732]
Epoch [13/120    avg_loss:1.370, val_acc:0.798]
Epoch [14/120    avg_loss:1.275, val_acc:0.782]
Epoch [15/120    avg_loss:1.180, val_acc:0.881]
Epoch [16/120    avg_loss:1.089, val_acc:0.808]
Epoch [17/120    avg_loss:1.007, val_acc:0.766]
Epoch [18/120    avg_loss:0.914, val_acc:0.853]
Epoch [19/120    avg_loss:0.859, val_acc:0.889]
Epoch [20/120    avg_loss:0.760, val_acc:0.883]
Epoch [21/120    avg_loss:0.693, val_acc:0.897]
Epoch [22/120    avg_loss:0.660, val_acc:0.891]
Epoch [23/120    avg_loss:0.624, val_acc:0.917]
Epoch [24/120    avg_loss:0.532, val_acc:0.917]
Epoch [25/120    avg_loss:0.522, val_acc:0.917]
Epoch [26/120    avg_loss:0.509, val_acc:0.907]
Epoch [27/120    avg_loss:0.497, val_acc:0.931]
Epoch [28/120    avg_loss:0.452, val_acc:0.877]
Epoch [29/120    avg_loss:0.467, val_acc:0.937]
Epoch [30/120    avg_loss:0.428, val_acc:0.907]
Epoch [31/120    avg_loss:0.394, val_acc:0.913]
Epoch [32/120    avg_loss:0.373, val_acc:0.913]
Epoch [33/120    avg_loss:0.403, val_acc:0.929]
Epoch [34/120    avg_loss:0.353, val_acc:0.929]
Epoch [35/120    avg_loss:0.323, val_acc:0.938]
Epoch [36/120    avg_loss:0.304, val_acc:0.919]
Epoch [37/120    avg_loss:0.291, val_acc:0.903]
Epoch [38/120    avg_loss:0.380, val_acc:0.873]
Epoch [39/120    avg_loss:0.392, val_acc:0.915]
Epoch [40/120    avg_loss:0.348, val_acc:0.921]
Epoch [41/120    avg_loss:0.285, val_acc:0.944]
Epoch [42/120    avg_loss:0.254, val_acc:0.950]
Epoch [43/120    avg_loss:0.256, val_acc:0.929]
Epoch [44/120    avg_loss:0.312, val_acc:0.929]
Epoch [45/120    avg_loss:0.337, val_acc:0.935]
Epoch [46/120    avg_loss:0.290, val_acc:0.893]
Epoch [47/120    avg_loss:0.334, val_acc:0.946]
Epoch [48/120    avg_loss:0.249, val_acc:0.942]
Epoch [49/120    avg_loss:0.251, val_acc:0.938]
Epoch [50/120    avg_loss:0.200, val_acc:0.937]
Epoch [51/120    avg_loss:0.223, val_acc:0.944]
Epoch [52/120    avg_loss:0.186, val_acc:0.942]
Epoch [53/120    avg_loss:0.225, val_acc:0.921]
Epoch [54/120    avg_loss:0.204, val_acc:0.952]
Epoch [55/120    avg_loss:0.191, val_acc:0.950]
Epoch [56/120    avg_loss:0.180, val_acc:0.956]
Epoch [57/120    avg_loss:0.230, val_acc:0.942]
Epoch [58/120    avg_loss:0.159, val_acc:0.960]
Epoch [59/120    avg_loss:0.160, val_acc:0.927]
Epoch [60/120    avg_loss:0.172, val_acc:0.968]
Epoch [61/120    avg_loss:0.154, val_acc:0.968]
Epoch [62/120    avg_loss:0.126, val_acc:0.937]
Epoch [63/120    avg_loss:0.200, val_acc:0.958]
Epoch [64/120    avg_loss:0.144, val_acc:0.962]
Epoch [65/120    avg_loss:0.172, val_acc:0.937]
Epoch [66/120    avg_loss:0.141, val_acc:0.960]
Epoch [67/120    avg_loss:0.227, val_acc:0.968]
Epoch [68/120    avg_loss:0.158, val_acc:0.958]
Epoch [69/120    avg_loss:0.146, val_acc:0.937]
Epoch [70/120    avg_loss:0.143, val_acc:0.952]
Epoch [71/120    avg_loss:0.148, val_acc:0.968]
Epoch [72/120    avg_loss:0.149, val_acc:0.968]
Epoch [73/120    avg_loss:0.118, val_acc:0.956]
Epoch [74/120    avg_loss:0.105, val_acc:0.964]
Epoch [75/120    avg_loss:0.128, val_acc:0.962]
Epoch [76/120    avg_loss:0.122, val_acc:0.968]
Epoch [77/120    avg_loss:0.123, val_acc:0.966]
Epoch [78/120    avg_loss:0.107, val_acc:0.974]
Epoch [79/120    avg_loss:0.102, val_acc:0.980]
Epoch [80/120    avg_loss:0.086, val_acc:0.972]
Epoch [81/120    avg_loss:0.127, val_acc:0.960]
Epoch [82/120    avg_loss:0.155, val_acc:0.968]
Epoch [83/120    avg_loss:0.140, val_acc:0.958]
Epoch [84/120    avg_loss:0.094, val_acc:0.970]
Epoch [85/120    avg_loss:0.114, val_acc:0.976]
Epoch [86/120    avg_loss:0.109, val_acc:0.956]
Epoch [87/120    avg_loss:0.146, val_acc:0.958]
Epoch [88/120    avg_loss:0.125, val_acc:0.948]
Epoch [89/120    avg_loss:0.128, val_acc:0.978]
Epoch [90/120    avg_loss:0.100, val_acc:0.974]
Epoch [91/120    avg_loss:0.095, val_acc:0.970]
Epoch [92/120    avg_loss:0.091, val_acc:0.968]
Epoch [93/120    avg_loss:0.073, val_acc:0.968]
Epoch [94/120    avg_loss:0.070, val_acc:0.976]
Epoch [95/120    avg_loss:0.057, val_acc:0.978]
Epoch [96/120    avg_loss:0.055, val_acc:0.984]
Epoch [97/120    avg_loss:0.052, val_acc:0.980]
Epoch [98/120    avg_loss:0.047, val_acc:0.980]
Epoch [99/120    avg_loss:0.057, val_acc:0.980]
Epoch [100/120    avg_loss:0.060, val_acc:0.980]
Epoch [101/120    avg_loss:0.045, val_acc:0.982]
Epoch [102/120    avg_loss:0.043, val_acc:0.982]
Epoch [103/120    avg_loss:0.044, val_acc:0.982]
Epoch [104/120    avg_loss:0.046, val_acc:0.984]
Epoch [105/120    avg_loss:0.043, val_acc:0.984]
Epoch [106/120    avg_loss:0.041, val_acc:0.984]
Epoch [107/120    avg_loss:0.047, val_acc:0.984]
Epoch [108/120    avg_loss:0.044, val_acc:0.984]
Epoch [109/120    avg_loss:0.043, val_acc:0.984]
Epoch [110/120    avg_loss:0.039, val_acc:0.984]
Epoch [111/120    avg_loss:0.044, val_acc:0.984]
Epoch [112/120    avg_loss:0.040, val_acc:0.986]
Epoch [113/120    avg_loss:0.042, val_acc:0.984]
Epoch [114/120    avg_loss:0.049, val_acc:0.984]
Epoch [115/120    avg_loss:0.043, val_acc:0.988]
Epoch [116/120    avg_loss:0.053, val_acc:0.984]
Epoch [117/120    avg_loss:0.048, val_acc:0.984]
Epoch [118/120    avg_loss:0.044, val_acc:0.984]
Epoch [119/120    avg_loss:0.045, val_acc:0.984]
Epoch [120/120    avg_loss:0.041, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 226   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.99927061 0.97767857 0.99122807 0.90434783 0.85714286
 0.99756691 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9869428105733116
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f24bd980a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.570, val_acc:0.218]
Epoch [2/120    avg_loss:2.451, val_acc:0.310]
Epoch [3/120    avg_loss:2.361, val_acc:0.379]
Epoch [4/120    avg_loss:2.272, val_acc:0.415]
Epoch [5/120    avg_loss:2.175, val_acc:0.520]
Epoch [6/120    avg_loss:2.068, val_acc:0.538]
Epoch [7/120    avg_loss:1.960, val_acc:0.556]
Epoch [8/120    avg_loss:1.861, val_acc:0.571]
Epoch [9/120    avg_loss:1.755, val_acc:0.605]
Epoch [10/120    avg_loss:1.637, val_acc:0.609]
Epoch [11/120    avg_loss:1.545, val_acc:0.687]
Epoch [12/120    avg_loss:1.451, val_acc:0.710]
Epoch [13/120    avg_loss:1.360, val_acc:0.702]
Epoch [14/120    avg_loss:1.252, val_acc:0.728]
Epoch [15/120    avg_loss:1.162, val_acc:0.788]
Epoch [16/120    avg_loss:1.065, val_acc:0.808]
Epoch [17/120    avg_loss:1.032, val_acc:0.825]
Epoch [18/120    avg_loss:0.950, val_acc:0.829]
Epoch [19/120    avg_loss:0.880, val_acc:0.843]
Epoch [20/120    avg_loss:0.821, val_acc:0.855]
Epoch [21/120    avg_loss:0.751, val_acc:0.861]
Epoch [22/120    avg_loss:0.688, val_acc:0.885]
Epoch [23/120    avg_loss:0.647, val_acc:0.875]
Epoch [24/120    avg_loss:0.629, val_acc:0.867]
Epoch [25/120    avg_loss:0.570, val_acc:0.891]
Epoch [26/120    avg_loss:0.610, val_acc:0.895]
Epoch [27/120    avg_loss:0.551, val_acc:0.909]
Epoch [28/120    avg_loss:0.510, val_acc:0.887]
Epoch [29/120    avg_loss:0.520, val_acc:0.893]
Epoch [30/120    avg_loss:0.493, val_acc:0.893]
Epoch [31/120    avg_loss:0.489, val_acc:0.913]
Epoch [32/120    avg_loss:0.461, val_acc:0.931]
Epoch [33/120    avg_loss:0.443, val_acc:0.917]
Epoch [34/120    avg_loss:0.372, val_acc:0.931]
Epoch [35/120    avg_loss:0.436, val_acc:0.933]
Epoch [36/120    avg_loss:0.402, val_acc:0.929]
Epoch [37/120    avg_loss:0.381, val_acc:0.946]
Epoch [38/120    avg_loss:0.325, val_acc:0.952]
Epoch [39/120    avg_loss:0.332, val_acc:0.887]
Epoch [40/120    avg_loss:0.389, val_acc:0.917]
Epoch [41/120    avg_loss:0.299, val_acc:0.956]
Epoch [42/120    avg_loss:0.328, val_acc:0.944]
Epoch [43/120    avg_loss:0.331, val_acc:0.923]
Epoch [44/120    avg_loss:0.316, val_acc:0.942]
Epoch [45/120    avg_loss:0.311, val_acc:0.929]
Epoch [46/120    avg_loss:0.332, val_acc:0.956]
Epoch [47/120    avg_loss:0.283, val_acc:0.933]
Epoch [48/120    avg_loss:0.272, val_acc:0.960]
Epoch [49/120    avg_loss:0.237, val_acc:0.954]
Epoch [50/120    avg_loss:0.294, val_acc:0.931]
Epoch [51/120    avg_loss:0.261, val_acc:0.946]
Epoch [52/120    avg_loss:0.267, val_acc:0.933]
Epoch [53/120    avg_loss:0.261, val_acc:0.948]
Epoch [54/120    avg_loss:0.200, val_acc:0.950]
Epoch [55/120    avg_loss:0.261, val_acc:0.954]
Epoch [56/120    avg_loss:0.235, val_acc:0.962]
Epoch [57/120    avg_loss:0.238, val_acc:0.954]
Epoch [58/120    avg_loss:0.201, val_acc:0.960]
Epoch [59/120    avg_loss:0.219, val_acc:0.964]
Epoch [60/120    avg_loss:0.183, val_acc:0.970]
Epoch [61/120    avg_loss:0.192, val_acc:0.958]
Epoch [62/120    avg_loss:0.152, val_acc:0.964]
Epoch [63/120    avg_loss:0.180, val_acc:0.956]
Epoch [64/120    avg_loss:0.207, val_acc:0.970]
Epoch [65/120    avg_loss:0.186, val_acc:0.968]
Epoch [66/120    avg_loss:0.149, val_acc:0.970]
Epoch [67/120    avg_loss:0.176, val_acc:0.972]
Epoch [68/120    avg_loss:0.160, val_acc:0.966]
Epoch [69/120    avg_loss:0.144, val_acc:0.962]
Epoch [70/120    avg_loss:0.159, val_acc:0.972]
Epoch [71/120    avg_loss:0.143, val_acc:0.970]
Epoch [72/120    avg_loss:0.120, val_acc:0.964]
Epoch [73/120    avg_loss:0.122, val_acc:0.980]
Epoch [74/120    avg_loss:0.137, val_acc:0.956]
Epoch [75/120    avg_loss:0.195, val_acc:0.976]
Epoch [76/120    avg_loss:0.203, val_acc:0.966]
Epoch [77/120    avg_loss:0.165, val_acc:0.964]
Epoch [78/120    avg_loss:0.150, val_acc:0.962]
Epoch [79/120    avg_loss:0.191, val_acc:0.964]
Epoch [80/120    avg_loss:0.124, val_acc:0.984]
Epoch [81/120    avg_loss:0.111, val_acc:0.978]
Epoch [82/120    avg_loss:0.109, val_acc:0.986]
Epoch [83/120    avg_loss:0.119, val_acc:0.968]
Epoch [84/120    avg_loss:0.103, val_acc:0.970]
Epoch [85/120    avg_loss:0.100, val_acc:0.988]
Epoch [86/120    avg_loss:0.099, val_acc:0.980]
Epoch [87/120    avg_loss:0.086, val_acc:0.986]
Epoch [88/120    avg_loss:0.058, val_acc:0.990]
Epoch [89/120    avg_loss:0.063, val_acc:0.992]
Epoch [90/120    avg_loss:0.073, val_acc:0.982]
Epoch [91/120    avg_loss:0.103, val_acc:0.982]
Epoch [92/120    avg_loss:0.091, val_acc:0.970]
Epoch [93/120    avg_loss:0.100, val_acc:0.984]
Epoch [94/120    avg_loss:0.097, val_acc:0.976]
Epoch [95/120    avg_loss:0.067, val_acc:0.988]
Epoch [96/120    avg_loss:0.101, val_acc:0.990]
Epoch [97/120    avg_loss:0.084, val_acc:0.986]
Epoch [98/120    avg_loss:0.090, val_acc:0.976]
Epoch [99/120    avg_loss:0.083, val_acc:0.986]
Epoch [100/120    avg_loss:0.114, val_acc:0.950]
Epoch [101/120    avg_loss:0.145, val_acc:0.970]
Epoch [102/120    avg_loss:0.074, val_acc:0.986]
Epoch [103/120    avg_loss:0.059, val_acc:0.992]
Epoch [104/120    avg_loss:0.049, val_acc:0.994]
Epoch [105/120    avg_loss:0.043, val_acc:0.996]
Epoch [106/120    avg_loss:0.046, val_acc:0.996]
Epoch [107/120    avg_loss:0.059, val_acc:0.996]
Epoch [108/120    avg_loss:0.052, val_acc:0.996]
Epoch [109/120    avg_loss:0.037, val_acc:0.996]
Epoch [110/120    avg_loss:0.043, val_acc:0.996]
Epoch [111/120    avg_loss:0.035, val_acc:0.996]
Epoch [112/120    avg_loss:0.040, val_acc:0.996]
Epoch [113/120    avg_loss:0.038, val_acc:0.996]
Epoch [114/120    avg_loss:0.037, val_acc:0.996]
Epoch [115/120    avg_loss:0.035, val_acc:0.996]
Epoch [116/120    avg_loss:0.040, val_acc:0.996]
Epoch [117/120    avg_loss:0.036, val_acc:0.996]
Epoch [118/120    avg_loss:0.038, val_acc:0.996]
Epoch [119/120    avg_loss:0.027, val_acc:0.996]
Epoch [120/120    avg_loss:0.036, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 222   6   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.97757848 0.98230088 0.93697479 0.91240876
 1.         0.94444444 1.         0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.9900285808875565
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f6cb27ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.338]
Epoch [2/120    avg_loss:2.516, val_acc:0.395]
Epoch [3/120    avg_loss:2.422, val_acc:0.466]
Epoch [4/120    avg_loss:2.343, val_acc:0.429]
Epoch [5/120    avg_loss:2.260, val_acc:0.379]
Epoch [6/120    avg_loss:2.181, val_acc:0.397]
Epoch [7/120    avg_loss:2.096, val_acc:0.437]
Epoch [8/120    avg_loss:1.999, val_acc:0.522]
Epoch [9/120    avg_loss:1.908, val_acc:0.567]
Epoch [10/120    avg_loss:1.827, val_acc:0.595]
Epoch [11/120    avg_loss:1.740, val_acc:0.623]
Epoch [12/120    avg_loss:1.632, val_acc:0.677]
Epoch [13/120    avg_loss:1.535, val_acc:0.653]
Epoch [14/120    avg_loss:1.428, val_acc:0.685]
Epoch [15/120    avg_loss:1.310, val_acc:0.748]
Epoch [16/120    avg_loss:1.216, val_acc:0.744]
Epoch [17/120    avg_loss:1.175, val_acc:0.778]
Epoch [18/120    avg_loss:1.056, val_acc:0.839]
Epoch [19/120    avg_loss:0.932, val_acc:0.833]
Epoch [20/120    avg_loss:0.903, val_acc:0.877]
Epoch [21/120    avg_loss:0.806, val_acc:0.907]
Epoch [22/120    avg_loss:0.715, val_acc:0.921]
Epoch [23/120    avg_loss:0.667, val_acc:0.869]
Epoch [24/120    avg_loss:0.729, val_acc:0.843]
Epoch [25/120    avg_loss:0.646, val_acc:0.851]
Epoch [26/120    avg_loss:0.601, val_acc:0.891]
Epoch [27/120    avg_loss:0.576, val_acc:0.883]
Epoch [28/120    avg_loss:0.574, val_acc:0.885]
Epoch [29/120    avg_loss:0.516, val_acc:0.915]
Epoch [30/120    avg_loss:0.509, val_acc:0.907]
Epoch [31/120    avg_loss:0.452, val_acc:0.909]
Epoch [32/120    avg_loss:0.503, val_acc:0.935]
Epoch [33/120    avg_loss:0.428, val_acc:0.921]
Epoch [34/120    avg_loss:0.413, val_acc:0.903]
Epoch [35/120    avg_loss:0.394, val_acc:0.950]
Epoch [36/120    avg_loss:0.394, val_acc:0.885]
Epoch [37/120    avg_loss:0.376, val_acc:0.925]
Epoch [38/120    avg_loss:0.371, val_acc:0.929]
Epoch [39/120    avg_loss:0.336, val_acc:0.944]
Epoch [40/120    avg_loss:0.331, val_acc:0.948]
Epoch [41/120    avg_loss:0.390, val_acc:0.938]
Epoch [42/120    avg_loss:0.347, val_acc:0.946]
Epoch [43/120    avg_loss:0.324, val_acc:0.942]
Epoch [44/120    avg_loss:0.313, val_acc:0.952]
Epoch [45/120    avg_loss:0.304, val_acc:0.925]
Epoch [46/120    avg_loss:0.287, val_acc:0.948]
Epoch [47/120    avg_loss:0.279, val_acc:0.929]
Epoch [48/120    avg_loss:0.308, val_acc:0.913]
Epoch [49/120    avg_loss:0.328, val_acc:0.946]
Epoch [50/120    avg_loss:0.275, val_acc:0.952]
Epoch [51/120    avg_loss:0.237, val_acc:0.923]
Epoch [52/120    avg_loss:0.299, val_acc:0.958]
Epoch [53/120    avg_loss:0.287, val_acc:0.950]
Epoch [54/120    avg_loss:0.208, val_acc:0.966]
Epoch [55/120    avg_loss:0.241, val_acc:0.937]
Epoch [56/120    avg_loss:0.186, val_acc:0.974]
Epoch [57/120    avg_loss:0.177, val_acc:0.964]
Epoch [58/120    avg_loss:0.231, val_acc:0.972]
Epoch [59/120    avg_loss:0.173, val_acc:0.972]
Epoch [60/120    avg_loss:0.187, val_acc:0.940]
Epoch [61/120    avg_loss:0.192, val_acc:0.966]
Epoch [62/120    avg_loss:0.151, val_acc:0.982]
Epoch [63/120    avg_loss:0.135, val_acc:0.974]
Epoch [64/120    avg_loss:0.164, val_acc:0.978]
Epoch [65/120    avg_loss:0.218, val_acc:0.958]
Epoch [66/120    avg_loss:0.159, val_acc:0.956]
Epoch [67/120    avg_loss:0.188, val_acc:0.980]
Epoch [68/120    avg_loss:0.157, val_acc:0.968]
Epoch [69/120    avg_loss:0.130, val_acc:0.984]
Epoch [70/120    avg_loss:0.109, val_acc:0.980]
Epoch [71/120    avg_loss:0.098, val_acc:0.984]
Epoch [72/120    avg_loss:0.130, val_acc:0.982]
Epoch [73/120    avg_loss:0.148, val_acc:0.976]
Epoch [74/120    avg_loss:0.134, val_acc:0.986]
Epoch [75/120    avg_loss:0.175, val_acc:0.919]
Epoch [76/120    avg_loss:0.144, val_acc:0.970]
Epoch [77/120    avg_loss:0.169, val_acc:0.982]
Epoch [78/120    avg_loss:0.113, val_acc:0.984]
Epoch [79/120    avg_loss:0.083, val_acc:0.986]
Epoch [80/120    avg_loss:0.086, val_acc:0.976]
Epoch [81/120    avg_loss:0.094, val_acc:0.986]
Epoch [82/120    avg_loss:0.134, val_acc:0.980]
Epoch [83/120    avg_loss:0.074, val_acc:0.986]
Epoch [84/120    avg_loss:0.076, val_acc:0.970]
Epoch [85/120    avg_loss:0.092, val_acc:0.986]
Epoch [86/120    avg_loss:0.072, val_acc:0.982]
Epoch [87/120    avg_loss:0.083, val_acc:0.986]
Epoch [88/120    avg_loss:0.083, val_acc:0.986]
Epoch [89/120    avg_loss:0.065, val_acc:0.990]
Epoch [90/120    avg_loss:0.056, val_acc:0.988]
Epoch [91/120    avg_loss:0.067, val_acc:0.980]
Epoch [92/120    avg_loss:0.068, val_acc:0.986]
Epoch [93/120    avg_loss:0.052, val_acc:0.990]
Epoch [94/120    avg_loss:0.052, val_acc:0.994]
Epoch [95/120    avg_loss:0.068, val_acc:0.988]
Epoch [96/120    avg_loss:0.062, val_acc:0.988]
Epoch [97/120    avg_loss:0.082, val_acc:0.980]
Epoch [98/120    avg_loss:0.082, val_acc:0.988]
Epoch [99/120    avg_loss:0.062, val_acc:0.988]
Epoch [100/120    avg_loss:0.077, val_acc:0.986]
Epoch [101/120    avg_loss:0.064, val_acc:0.972]
Epoch [102/120    avg_loss:0.078, val_acc:0.980]
Epoch [103/120    avg_loss:0.057, val_acc:0.988]
Epoch [104/120    avg_loss:0.048, val_acc:0.986]
Epoch [105/120    avg_loss:0.036, val_acc:0.986]
Epoch [106/120    avg_loss:0.058, val_acc:0.984]
Epoch [107/120    avg_loss:0.068, val_acc:0.988]
Epoch [108/120    avg_loss:0.049, val_acc:0.986]
Epoch [109/120    avg_loss:0.041, val_acc:0.986]
Epoch [110/120    avg_loss:0.031, val_acc:0.986]
Epoch [111/120    avg_loss:0.033, val_acc:0.986]
Epoch [112/120    avg_loss:0.036, val_acc:0.990]
Epoch [113/120    avg_loss:0.033, val_acc:0.988]
Epoch [114/120    avg_loss:0.038, val_acc:0.990]
Epoch [115/120    avg_loss:0.035, val_acc:0.988]
Epoch [116/120    avg_loss:0.029, val_acc:0.990]
Epoch [117/120    avg_loss:0.031, val_acc:0.992]
Epoch [118/120    avg_loss:0.027, val_acc:0.992]
Epoch [119/120    avg_loss:0.026, val_acc:0.990]
Epoch [120/120    avg_loss:0.025, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.96902655 0.99563319 0.95909091 0.9442623
 1.         0.92571429 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924036469580751
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f926ad8fac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.310]
Epoch [2/120    avg_loss:2.482, val_acc:0.373]
Epoch [3/120    avg_loss:2.375, val_acc:0.377]
Epoch [4/120    avg_loss:2.294, val_acc:0.387]
Epoch [5/120    avg_loss:2.201, val_acc:0.413]
Epoch [6/120    avg_loss:2.112, val_acc:0.440]
Epoch [7/120    avg_loss:2.011, val_acc:0.478]
Epoch [8/120    avg_loss:1.925, val_acc:0.528]
Epoch [9/120    avg_loss:1.843, val_acc:0.589]
Epoch [10/120    avg_loss:1.738, val_acc:0.641]
Epoch [11/120    avg_loss:1.628, val_acc:0.675]
Epoch [12/120    avg_loss:1.572, val_acc:0.685]
Epoch [13/120    avg_loss:1.439, val_acc:0.702]
Epoch [14/120    avg_loss:1.353, val_acc:0.728]
Epoch [15/120    avg_loss:1.205, val_acc:0.815]
Epoch [16/120    avg_loss:1.073, val_acc:0.790]
Epoch [17/120    avg_loss:1.047, val_acc:0.786]
Epoch [18/120    avg_loss:1.008, val_acc:0.875]
Epoch [19/120    avg_loss:0.904, val_acc:0.873]
Epoch [20/120    avg_loss:0.796, val_acc:0.885]
Epoch [21/120    avg_loss:0.772, val_acc:0.903]
Epoch [22/120    avg_loss:0.706, val_acc:0.917]
Epoch [23/120    avg_loss:0.652, val_acc:0.851]
Epoch [24/120    avg_loss:0.685, val_acc:0.782]
Epoch [25/120    avg_loss:0.738, val_acc:0.883]
Epoch [26/120    avg_loss:0.574, val_acc:0.883]
Epoch [27/120    avg_loss:0.543, val_acc:0.911]
Epoch [28/120    avg_loss:0.515, val_acc:0.899]
Epoch [29/120    avg_loss:0.500, val_acc:0.935]
Epoch [30/120    avg_loss:0.484, val_acc:0.855]
Epoch [31/120    avg_loss:0.532, val_acc:0.879]
Epoch [32/120    avg_loss:0.442, val_acc:0.921]
Epoch [33/120    avg_loss:0.412, val_acc:0.938]
Epoch [34/120    avg_loss:0.436, val_acc:0.915]
Epoch [35/120    avg_loss:0.428, val_acc:0.903]
Epoch [36/120    avg_loss:0.361, val_acc:0.925]
Epoch [37/120    avg_loss:0.366, val_acc:0.903]
Epoch [38/120    avg_loss:0.363, val_acc:0.913]
Epoch [39/120    avg_loss:0.320, val_acc:0.915]
Epoch [40/120    avg_loss:0.347, val_acc:0.917]
Epoch [41/120    avg_loss:0.316, val_acc:0.954]
Epoch [42/120    avg_loss:0.355, val_acc:0.931]
Epoch [43/120    avg_loss:0.399, val_acc:0.923]
Epoch [44/120    avg_loss:0.339, val_acc:0.937]
Epoch [45/120    avg_loss:0.342, val_acc:0.929]
Epoch [46/120    avg_loss:0.324, val_acc:0.935]
Epoch [47/120    avg_loss:0.301, val_acc:0.929]
Epoch [48/120    avg_loss:0.257, val_acc:0.935]
Epoch [49/120    avg_loss:0.274, val_acc:0.944]
Epoch [50/120    avg_loss:0.266, val_acc:0.958]
Epoch [51/120    avg_loss:0.236, val_acc:0.929]
Epoch [52/120    avg_loss:0.246, val_acc:0.954]
Epoch [53/120    avg_loss:0.189, val_acc:0.964]
Epoch [54/120    avg_loss:0.211, val_acc:0.952]
Epoch [55/120    avg_loss:0.218, val_acc:0.958]
Epoch [56/120    avg_loss:0.220, val_acc:0.956]
Epoch [57/120    avg_loss:0.254, val_acc:0.956]
Epoch [58/120    avg_loss:0.223, val_acc:0.960]
Epoch [59/120    avg_loss:0.224, val_acc:0.960]
Epoch [60/120    avg_loss:0.223, val_acc:0.958]
Epoch [61/120    avg_loss:0.214, val_acc:0.964]
Epoch [62/120    avg_loss:0.235, val_acc:0.944]
Epoch [63/120    avg_loss:0.216, val_acc:0.948]
Epoch [64/120    avg_loss:0.231, val_acc:0.944]
Epoch [65/120    avg_loss:0.187, val_acc:0.964]
Epoch [66/120    avg_loss:0.174, val_acc:0.962]
Epoch [67/120    avg_loss:0.176, val_acc:0.962]
Epoch [68/120    avg_loss:0.170, val_acc:0.972]
Epoch [69/120    avg_loss:0.166, val_acc:0.968]
Epoch [70/120    avg_loss:0.157, val_acc:0.962]
Epoch [71/120    avg_loss:0.163, val_acc:0.956]
Epoch [72/120    avg_loss:0.209, val_acc:0.925]
Epoch [73/120    avg_loss:0.172, val_acc:0.970]
Epoch [74/120    avg_loss:0.208, val_acc:0.958]
Epoch [75/120    avg_loss:0.202, val_acc:0.960]
Epoch [76/120    avg_loss:0.180, val_acc:0.958]
Epoch [77/120    avg_loss:0.145, val_acc:0.976]
Epoch [78/120    avg_loss:0.141, val_acc:0.964]
Epoch [79/120    avg_loss:0.153, val_acc:0.960]
Epoch [80/120    avg_loss:0.122, val_acc:0.972]
Epoch [81/120    avg_loss:0.110, val_acc:0.976]
Epoch [82/120    avg_loss:0.090, val_acc:0.986]
Epoch [83/120    avg_loss:0.097, val_acc:0.976]
Epoch [84/120    avg_loss:0.195, val_acc:0.915]
Epoch [85/120    avg_loss:0.179, val_acc:0.978]
Epoch [86/120    avg_loss:0.130, val_acc:0.944]
Epoch [87/120    avg_loss:0.185, val_acc:0.931]
Epoch [88/120    avg_loss:0.151, val_acc:0.964]
Epoch [89/120    avg_loss:0.134, val_acc:0.974]
Epoch [90/120    avg_loss:0.155, val_acc:0.970]
Epoch [91/120    avg_loss:0.168, val_acc:0.950]
Epoch [92/120    avg_loss:0.121, val_acc:0.970]
Epoch [93/120    avg_loss:0.104, val_acc:0.970]
Epoch [94/120    avg_loss:0.111, val_acc:0.982]
Epoch [95/120    avg_loss:0.097, val_acc:0.966]
Epoch [96/120    avg_loss:0.101, val_acc:0.978]
Epoch [97/120    avg_loss:0.071, val_acc:0.978]
Epoch [98/120    avg_loss:0.080, val_acc:0.980]
Epoch [99/120    avg_loss:0.070, val_acc:0.978]
Epoch [100/120    avg_loss:0.091, val_acc:0.980]
Epoch [101/120    avg_loss:0.070, val_acc:0.982]
Epoch [102/120    avg_loss:0.057, val_acc:0.984]
Epoch [103/120    avg_loss:0.076, val_acc:0.980]
Epoch [104/120    avg_loss:0.081, val_acc:0.982]
Epoch [105/120    avg_loss:0.065, val_acc:0.984]
Epoch [106/120    avg_loss:0.062, val_acc:0.980]
Epoch [107/120    avg_loss:0.053, val_acc:0.984]
Epoch [108/120    avg_loss:0.064, val_acc:0.982]
Epoch [109/120    avg_loss:0.051, val_acc:0.982]
Epoch [110/120    avg_loss:0.060, val_acc:0.984]
Epoch [111/120    avg_loss:0.059, val_acc:0.982]
Epoch [112/120    avg_loss:0.063, val_acc:0.982]
Epoch [113/120    avg_loss:0.074, val_acc:0.982]
Epoch [114/120    avg_loss:0.062, val_acc:0.982]
Epoch [115/120    avg_loss:0.072, val_acc:0.984]
Epoch [116/120    avg_loss:0.057, val_acc:0.984]
Epoch [117/120    avg_loss:0.062, val_acc:0.984]
Epoch [118/120    avg_loss:0.068, val_acc:0.984]
Epoch [119/120    avg_loss:0.063, val_acc:0.984]
Epoch [120/120    avg_loss:0.046, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.96475771 0.99563319 0.92747253 0.88965517
 1.         0.9132948  1.         1.         1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9867057442735717
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99886e6668>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.611, val_acc:0.169]
Epoch [2/120    avg_loss:2.494, val_acc:0.192]
Epoch [3/120    avg_loss:2.406, val_acc:0.396]
Epoch [4/120    avg_loss:2.327, val_acc:0.415]
Epoch [5/120    avg_loss:2.250, val_acc:0.458]
Epoch [6/120    avg_loss:2.152, val_acc:0.469]
Epoch [7/120    avg_loss:2.050, val_acc:0.467]
Epoch [8/120    avg_loss:1.971, val_acc:0.540]
Epoch [9/120    avg_loss:1.857, val_acc:0.562]
Epoch [10/120    avg_loss:1.774, val_acc:0.596]
Epoch [11/120    avg_loss:1.656, val_acc:0.640]
Epoch [12/120    avg_loss:1.566, val_acc:0.692]
Epoch [13/120    avg_loss:1.469, val_acc:0.704]
Epoch [14/120    avg_loss:1.372, val_acc:0.767]
Epoch [15/120    avg_loss:1.255, val_acc:0.808]
Epoch [16/120    avg_loss:1.140, val_acc:0.869]
Epoch [17/120    avg_loss:1.054, val_acc:0.863]
Epoch [18/120    avg_loss:0.992, val_acc:0.779]
Epoch [19/120    avg_loss:0.908, val_acc:0.871]
Epoch [20/120    avg_loss:0.808, val_acc:0.887]
Epoch [21/120    avg_loss:0.761, val_acc:0.883]
Epoch [22/120    avg_loss:0.746, val_acc:0.908]
Epoch [23/120    avg_loss:0.665, val_acc:0.879]
Epoch [24/120    avg_loss:0.655, val_acc:0.906]
Epoch [25/120    avg_loss:0.566, val_acc:0.925]
Epoch [26/120    avg_loss:0.517, val_acc:0.923]
Epoch [27/120    avg_loss:0.536, val_acc:0.935]
Epoch [28/120    avg_loss:0.521, val_acc:0.912]
Epoch [29/120    avg_loss:0.504, val_acc:0.915]
Epoch [30/120    avg_loss:0.526, val_acc:0.908]
Epoch [31/120    avg_loss:0.445, val_acc:0.925]
Epoch [32/120    avg_loss:0.406, val_acc:0.935]
Epoch [33/120    avg_loss:0.363, val_acc:0.954]
Epoch [34/120    avg_loss:0.363, val_acc:0.929]
Epoch [35/120    avg_loss:0.406, val_acc:0.910]
Epoch [36/120    avg_loss:0.384, val_acc:0.933]
Epoch [37/120    avg_loss:0.357, val_acc:0.938]
Epoch [38/120    avg_loss:0.345, val_acc:0.940]
Epoch [39/120    avg_loss:0.342, val_acc:0.958]
Epoch [40/120    avg_loss:0.315, val_acc:0.956]
Epoch [41/120    avg_loss:0.260, val_acc:0.944]
Epoch [42/120    avg_loss:0.279, val_acc:0.944]
Epoch [43/120    avg_loss:0.268, val_acc:0.944]
Epoch [44/120    avg_loss:0.278, val_acc:0.938]
Epoch [45/120    avg_loss:0.304, val_acc:0.952]
Epoch [46/120    avg_loss:0.270, val_acc:0.958]
Epoch [47/120    avg_loss:0.263, val_acc:0.960]
Epoch [48/120    avg_loss:0.249, val_acc:0.960]
Epoch [49/120    avg_loss:0.210, val_acc:0.950]
Epoch [50/120    avg_loss:0.206, val_acc:0.952]
Epoch [51/120    avg_loss:0.256, val_acc:0.965]
Epoch [52/120    avg_loss:0.381, val_acc:0.917]
Epoch [53/120    avg_loss:0.263, val_acc:0.975]
Epoch [54/120    avg_loss:0.225, val_acc:0.915]
Epoch [55/120    avg_loss:0.211, val_acc:0.954]
Epoch [56/120    avg_loss:0.253, val_acc:0.927]
Epoch [57/120    avg_loss:0.259, val_acc:0.958]
Epoch [58/120    avg_loss:0.178, val_acc:0.971]
Epoch [59/120    avg_loss:0.208, val_acc:0.956]
Epoch [60/120    avg_loss:0.161, val_acc:0.975]
Epoch [61/120    avg_loss:0.156, val_acc:0.973]
Epoch [62/120    avg_loss:0.165, val_acc:0.967]
Epoch [63/120    avg_loss:0.220, val_acc:0.935]
Epoch [64/120    avg_loss:0.187, val_acc:0.977]
Epoch [65/120    avg_loss:0.155, val_acc:0.965]
Epoch [66/120    avg_loss:0.190, val_acc:0.969]
Epoch [67/120    avg_loss:0.181, val_acc:0.952]
Epoch [68/120    avg_loss:0.152, val_acc:0.952]
Epoch [69/120    avg_loss:0.150, val_acc:0.931]
Epoch [70/120    avg_loss:0.122, val_acc:0.981]
Epoch [71/120    avg_loss:0.135, val_acc:0.967]
Epoch [72/120    avg_loss:0.134, val_acc:0.977]
Epoch [73/120    avg_loss:0.110, val_acc:0.933]
Epoch [74/120    avg_loss:0.108, val_acc:0.979]
Epoch [75/120    avg_loss:0.121, val_acc:0.983]
Epoch [76/120    avg_loss:0.098, val_acc:0.977]
Epoch [77/120    avg_loss:0.105, val_acc:0.983]
Epoch [78/120    avg_loss:0.068, val_acc:0.981]
Epoch [79/120    avg_loss:0.085, val_acc:0.981]
Epoch [80/120    avg_loss:0.071, val_acc:0.977]
Epoch [81/120    avg_loss:0.071, val_acc:0.979]
Epoch [82/120    avg_loss:0.081, val_acc:0.965]
Epoch [83/120    avg_loss:0.121, val_acc:0.969]
Epoch [84/120    avg_loss:0.154, val_acc:0.973]
Epoch [85/120    avg_loss:0.112, val_acc:0.977]
Epoch [86/120    avg_loss:0.111, val_acc:0.967]
Epoch [87/120    avg_loss:0.077, val_acc:0.988]
Epoch [88/120    avg_loss:0.081, val_acc:0.981]
Epoch [89/120    avg_loss:0.103, val_acc:0.942]
Epoch [90/120    avg_loss:0.106, val_acc:0.967]
Epoch [91/120    avg_loss:0.101, val_acc:0.969]
Epoch [92/120    avg_loss:0.078, val_acc:0.983]
Epoch [93/120    avg_loss:0.054, val_acc:0.983]
Epoch [94/120    avg_loss:0.056, val_acc:0.988]
Epoch [95/120    avg_loss:0.047, val_acc:0.977]
Epoch [96/120    avg_loss:0.083, val_acc:0.973]
Epoch [97/120    avg_loss:0.051, val_acc:0.977]
Epoch [98/120    avg_loss:0.110, val_acc:0.971]
Epoch [99/120    avg_loss:0.072, val_acc:0.975]
Epoch [100/120    avg_loss:0.054, val_acc:0.983]
Epoch [101/120    avg_loss:0.061, val_acc:0.985]
Epoch [102/120    avg_loss:0.044, val_acc:0.981]
Epoch [103/120    avg_loss:0.049, val_acc:0.983]
Epoch [104/120    avg_loss:0.045, val_acc:0.985]
Epoch [105/120    avg_loss:0.034, val_acc:0.992]
Epoch [106/120    avg_loss:0.042, val_acc:0.985]
Epoch [107/120    avg_loss:0.060, val_acc:0.977]
Epoch [108/120    avg_loss:0.045, val_acc:0.990]
Epoch [109/120    avg_loss:0.044, val_acc:0.990]
Epoch [110/120    avg_loss:0.040, val_acc:0.992]
Epoch [111/120    avg_loss:0.044, val_acc:0.990]
Epoch [112/120    avg_loss:0.065, val_acc:0.979]
Epoch [113/120    avg_loss:0.057, val_acc:0.977]
Epoch [114/120    avg_loss:0.043, val_acc:0.985]
Epoch [115/120    avg_loss:0.027, val_acc:0.985]
Epoch [116/120    avg_loss:0.042, val_acc:0.988]
Epoch [117/120    avg_loss:0.047, val_acc:0.985]
Epoch [118/120    avg_loss:0.040, val_acc:0.990]
Epoch [119/120    avg_loss:0.042, val_acc:0.983]
Epoch [120/120    avg_loss:0.028, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  11   0   0   0   0   0   0   3   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   1   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.98426966 1.         0.93626374 0.90909091
 1.         0.96132597 1.         0.99893276 1.         1.
 0.99559471 1.        ]

Kappa:
0.9912160432215421
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9eb01eeac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.260]
Epoch [2/120    avg_loss:2.484, val_acc:0.335]
Epoch [3/120    avg_loss:2.391, val_acc:0.362]
Epoch [4/120    avg_loss:2.304, val_acc:0.415]
Epoch [5/120    avg_loss:2.230, val_acc:0.431]
Epoch [6/120    avg_loss:2.132, val_acc:0.456]
Epoch [7/120    avg_loss:2.056, val_acc:0.531]
Epoch [8/120    avg_loss:1.944, val_acc:0.581]
Epoch [9/120    avg_loss:1.850, val_acc:0.613]
Epoch [10/120    avg_loss:1.735, val_acc:0.623]
Epoch [11/120    avg_loss:1.631, val_acc:0.662]
Epoch [12/120    avg_loss:1.568, val_acc:0.650]
Epoch [13/120    avg_loss:1.464, val_acc:0.669]
Epoch [14/120    avg_loss:1.348, val_acc:0.667]
Epoch [15/120    avg_loss:1.258, val_acc:0.681]
Epoch [16/120    avg_loss:1.211, val_acc:0.704]
Epoch [17/120    avg_loss:1.122, val_acc:0.708]
Epoch [18/120    avg_loss:1.072, val_acc:0.706]
Epoch [19/120    avg_loss:0.975, val_acc:0.762]
Epoch [20/120    avg_loss:0.953, val_acc:0.787]
Epoch [21/120    avg_loss:0.942, val_acc:0.769]
Epoch [22/120    avg_loss:0.815, val_acc:0.875]
Epoch [23/120    avg_loss:0.736, val_acc:0.900]
Epoch [24/120    avg_loss:0.694, val_acc:0.877]
Epoch [25/120    avg_loss:0.657, val_acc:0.892]
Epoch [26/120    avg_loss:0.626, val_acc:0.923]
Epoch [27/120    avg_loss:0.615, val_acc:0.894]
Epoch [28/120    avg_loss:0.541, val_acc:0.890]
Epoch [29/120    avg_loss:0.529, val_acc:0.898]
Epoch [30/120    avg_loss:0.487, val_acc:0.879]
Epoch [31/120    avg_loss:0.465, val_acc:0.904]
Epoch [32/120    avg_loss:0.500, val_acc:0.863]
Epoch [33/120    avg_loss:0.514, val_acc:0.879]
Epoch [34/120    avg_loss:0.462, val_acc:0.900]
Epoch [35/120    avg_loss:0.405, val_acc:0.894]
Epoch [36/120    avg_loss:0.430, val_acc:0.917]
Epoch [37/120    avg_loss:0.407, val_acc:0.923]
Epoch [38/120    avg_loss:0.418, val_acc:0.929]
Epoch [39/120    avg_loss:0.403, val_acc:0.925]
Epoch [40/120    avg_loss:0.395, val_acc:0.917]
Epoch [41/120    avg_loss:0.362, val_acc:0.919]
Epoch [42/120    avg_loss:0.311, val_acc:0.940]
Epoch [43/120    avg_loss:0.333, val_acc:0.927]
Epoch [44/120    avg_loss:0.346, val_acc:0.938]
Epoch [45/120    avg_loss:0.318, val_acc:0.896]
Epoch [46/120    avg_loss:0.333, val_acc:0.923]
Epoch [47/120    avg_loss:0.292, val_acc:0.938]
Epoch [48/120    avg_loss:0.278, val_acc:0.944]
Epoch [49/120    avg_loss:0.311, val_acc:0.929]
Epoch [50/120    avg_loss:0.281, val_acc:0.923]
Epoch [51/120    avg_loss:0.268, val_acc:0.956]
Epoch [52/120    avg_loss:0.259, val_acc:0.948]
Epoch [53/120    avg_loss:0.227, val_acc:0.940]
Epoch [54/120    avg_loss:0.303, val_acc:0.929]
Epoch [55/120    avg_loss:0.280, val_acc:0.958]
Epoch [56/120    avg_loss:0.213, val_acc:0.929]
Epoch [57/120    avg_loss:0.220, val_acc:0.963]
Epoch [58/120    avg_loss:0.247, val_acc:0.942]
Epoch [59/120    avg_loss:0.215, val_acc:0.942]
Epoch [60/120    avg_loss:0.223, val_acc:0.944]
Epoch [61/120    avg_loss:0.192, val_acc:0.956]
Epoch [62/120    avg_loss:0.212, val_acc:0.942]
Epoch [63/120    avg_loss:0.189, val_acc:0.952]
Epoch [64/120    avg_loss:0.213, val_acc:0.944]
Epoch [65/120    avg_loss:0.179, val_acc:0.952]
Epoch [66/120    avg_loss:0.185, val_acc:0.965]
Epoch [67/120    avg_loss:0.219, val_acc:0.952]
Epoch [68/120    avg_loss:0.239, val_acc:0.975]
Epoch [69/120    avg_loss:0.218, val_acc:0.971]
Epoch [70/120    avg_loss:0.247, val_acc:0.958]
Epoch [71/120    avg_loss:0.163, val_acc:0.956]
Epoch [72/120    avg_loss:0.174, val_acc:0.960]
Epoch [73/120    avg_loss:0.223, val_acc:0.946]
Epoch [74/120    avg_loss:0.260, val_acc:0.931]
Epoch [75/120    avg_loss:0.225, val_acc:0.950]
Epoch [76/120    avg_loss:0.230, val_acc:0.948]
Epoch [77/120    avg_loss:0.158, val_acc:0.969]
Epoch [78/120    avg_loss:0.140, val_acc:0.931]
Epoch [79/120    avg_loss:0.160, val_acc:0.967]
Epoch [80/120    avg_loss:0.189, val_acc:0.933]
Epoch [81/120    avg_loss:0.187, val_acc:0.956]
Epoch [82/120    avg_loss:0.158, val_acc:0.963]
Epoch [83/120    avg_loss:0.129, val_acc:0.971]
Epoch [84/120    avg_loss:0.123, val_acc:0.975]
Epoch [85/120    avg_loss:0.105, val_acc:0.977]
Epoch [86/120    avg_loss:0.104, val_acc:0.979]
Epoch [87/120    avg_loss:0.089, val_acc:0.979]
Epoch [88/120    avg_loss:0.105, val_acc:0.981]
Epoch [89/120    avg_loss:0.101, val_acc:0.979]
Epoch [90/120    avg_loss:0.104, val_acc:0.983]
Epoch [91/120    avg_loss:0.101, val_acc:0.977]
Epoch [92/120    avg_loss:0.088, val_acc:0.977]
Epoch [93/120    avg_loss:0.089, val_acc:0.983]
Epoch [94/120    avg_loss:0.087, val_acc:0.983]
Epoch [95/120    avg_loss:0.088, val_acc:0.983]
Epoch [96/120    avg_loss:0.090, val_acc:0.979]
Epoch [97/120    avg_loss:0.094, val_acc:0.981]
Epoch [98/120    avg_loss:0.083, val_acc:0.983]
Epoch [99/120    avg_loss:0.090, val_acc:0.983]
Epoch [100/120    avg_loss:0.083, val_acc:0.981]
Epoch [101/120    avg_loss:0.080, val_acc:0.979]
Epoch [102/120    avg_loss:0.084, val_acc:0.981]
Epoch [103/120    avg_loss:0.098, val_acc:0.977]
Epoch [104/120    avg_loss:0.083, val_acc:0.979]
Epoch [105/120    avg_loss:0.077, val_acc:0.981]
Epoch [106/120    avg_loss:0.070, val_acc:0.979]
Epoch [107/120    avg_loss:0.068, val_acc:0.981]
Epoch [108/120    avg_loss:0.084, val_acc:0.979]
Epoch [109/120    avg_loss:0.082, val_acc:0.979]
Epoch [110/120    avg_loss:0.088, val_acc:0.975]
Epoch [111/120    avg_loss:0.075, val_acc:0.983]
Epoch [112/120    avg_loss:0.075, val_acc:0.981]
Epoch [113/120    avg_loss:0.072, val_acc:0.981]
Epoch [114/120    avg_loss:0.066, val_acc:0.981]
Epoch [115/120    avg_loss:0.068, val_acc:0.983]
Epoch [116/120    avg_loss:0.080, val_acc:0.981]
Epoch [117/120    avg_loss:0.082, val_acc:0.983]
Epoch [118/120    avg_loss:0.075, val_acc:0.983]
Epoch [119/120    avg_loss:0.063, val_acc:0.977]
Epoch [120/120    avg_loss:0.084, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.61407249466951

F1 scores:
[       nan 1.         0.9753915  0.98454746 0.88412017 0.83508772
 1.         0.93854749 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9845690229602411
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a3727fb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.584, val_acc:0.133]
Epoch [2/120    avg_loss:2.480, val_acc:0.157]
Epoch [3/120    avg_loss:2.397, val_acc:0.312]
Epoch [4/120    avg_loss:2.319, val_acc:0.351]
Epoch [5/120    avg_loss:2.249, val_acc:0.438]
Epoch [6/120    avg_loss:2.177, val_acc:0.492]
Epoch [7/120    avg_loss:2.090, val_acc:0.538]
Epoch [8/120    avg_loss:2.003, val_acc:0.554]
Epoch [9/120    avg_loss:1.906, val_acc:0.607]
Epoch [10/120    avg_loss:1.802, val_acc:0.605]
Epoch [11/120    avg_loss:1.707, val_acc:0.671]
Epoch [12/120    avg_loss:1.608, val_acc:0.687]
Epoch [13/120    avg_loss:1.473, val_acc:0.685]
Epoch [14/120    avg_loss:1.359, val_acc:0.665]
Epoch [15/120    avg_loss:1.239, val_acc:0.732]
Epoch [16/120    avg_loss:1.180, val_acc:0.800]
Epoch [17/120    avg_loss:1.065, val_acc:0.726]
Epoch [18/120    avg_loss:0.986, val_acc:0.857]
Epoch [19/120    avg_loss:0.886, val_acc:0.829]
Epoch [20/120    avg_loss:0.824, val_acc:0.885]
Epoch [21/120    avg_loss:0.778, val_acc:0.881]
Epoch [22/120    avg_loss:0.705, val_acc:0.903]
Epoch [23/120    avg_loss:0.647, val_acc:0.909]
Epoch [24/120    avg_loss:0.684, val_acc:0.909]
Epoch [25/120    avg_loss:0.589, val_acc:0.917]
Epoch [26/120    avg_loss:0.532, val_acc:0.909]
Epoch [27/120    avg_loss:0.501, val_acc:0.855]
Epoch [28/120    avg_loss:0.464, val_acc:0.931]
Epoch [29/120    avg_loss:0.429, val_acc:0.931]
Epoch [30/120    avg_loss:0.406, val_acc:0.937]
Epoch [31/120    avg_loss:0.453, val_acc:0.909]
Epoch [32/120    avg_loss:0.436, val_acc:0.883]
Epoch [33/120    avg_loss:0.404, val_acc:0.931]
Epoch [34/120    avg_loss:0.395, val_acc:0.940]
Epoch [35/120    avg_loss:0.431, val_acc:0.911]
Epoch [36/120    avg_loss:0.353, val_acc:0.911]
Epoch [37/120    avg_loss:0.345, val_acc:0.929]
Epoch [38/120    avg_loss:0.282, val_acc:0.944]
Epoch [39/120    avg_loss:0.286, val_acc:0.935]
Epoch [40/120    avg_loss:0.308, val_acc:0.952]
Epoch [41/120    avg_loss:0.306, val_acc:0.942]
Epoch [42/120    avg_loss:0.273, val_acc:0.923]
Epoch [43/120    avg_loss:0.306, val_acc:0.887]
Epoch [44/120    avg_loss:0.313, val_acc:0.931]
Epoch [45/120    avg_loss:0.270, val_acc:0.940]
Epoch [46/120    avg_loss:0.238, val_acc:0.956]
Epoch [47/120    avg_loss:0.299, val_acc:0.950]
Epoch [48/120    avg_loss:0.242, val_acc:0.958]
Epoch [49/120    avg_loss:0.235, val_acc:0.962]
Epoch [50/120    avg_loss:0.252, val_acc:0.974]
Epoch [51/120    avg_loss:0.218, val_acc:0.966]
Epoch [52/120    avg_loss:0.209, val_acc:0.966]
Epoch [53/120    avg_loss:0.161, val_acc:0.966]
Epoch [54/120    avg_loss:0.176, val_acc:0.952]
Epoch [55/120    avg_loss:0.196, val_acc:0.982]
Epoch [56/120    avg_loss:0.176, val_acc:0.968]
Epoch [57/120    avg_loss:0.301, val_acc:0.948]
Epoch [58/120    avg_loss:0.311, val_acc:0.859]
Epoch [59/120    avg_loss:0.296, val_acc:0.966]
Epoch [60/120    avg_loss:0.243, val_acc:0.893]
Epoch [61/120    avg_loss:0.192, val_acc:0.958]
Epoch [62/120    avg_loss:0.191, val_acc:0.958]
Epoch [63/120    avg_loss:0.150, val_acc:0.980]
Epoch [64/120    avg_loss:0.145, val_acc:0.962]
Epoch [65/120    avg_loss:0.181, val_acc:0.962]
Epoch [66/120    avg_loss:0.202, val_acc:0.962]
Epoch [67/120    avg_loss:0.213, val_acc:0.944]
Epoch [68/120    avg_loss:0.161, val_acc:0.958]
Epoch [69/120    avg_loss:0.169, val_acc:0.982]
Epoch [70/120    avg_loss:0.106, val_acc:0.980]
Epoch [71/120    avg_loss:0.116, val_acc:0.980]
Epoch [72/120    avg_loss:0.106, val_acc:0.982]
Epoch [73/120    avg_loss:0.097, val_acc:0.982]
Epoch [74/120    avg_loss:0.090, val_acc:0.982]
Epoch [75/120    avg_loss:0.098, val_acc:0.986]
Epoch [76/120    avg_loss:0.092, val_acc:0.988]
Epoch [77/120    avg_loss:0.098, val_acc:0.988]
Epoch [78/120    avg_loss:0.105, val_acc:0.988]
Epoch [79/120    avg_loss:0.090, val_acc:0.984]
Epoch [80/120    avg_loss:0.083, val_acc:0.988]
Epoch [81/120    avg_loss:0.080, val_acc:0.986]
Epoch [82/120    avg_loss:0.081, val_acc:0.986]
Epoch [83/120    avg_loss:0.078, val_acc:0.988]
Epoch [84/120    avg_loss:0.093, val_acc:0.986]
Epoch [85/120    avg_loss:0.085, val_acc:0.988]
Epoch [86/120    avg_loss:0.076, val_acc:0.988]
Epoch [87/120    avg_loss:0.095, val_acc:0.990]
Epoch [88/120    avg_loss:0.086, val_acc:0.988]
Epoch [89/120    avg_loss:0.077, val_acc:0.986]
Epoch [90/120    avg_loss:0.070, val_acc:0.986]
Epoch [91/120    avg_loss:0.073, val_acc:0.988]
Epoch [92/120    avg_loss:0.094, val_acc:0.988]
Epoch [93/120    avg_loss:0.080, val_acc:0.990]
Epoch [94/120    avg_loss:0.083, val_acc:0.990]
Epoch [95/120    avg_loss:0.091, val_acc:0.990]
Epoch [96/120    avg_loss:0.081, val_acc:0.990]
Epoch [97/120    avg_loss:0.079, val_acc:0.990]
Epoch [98/120    avg_loss:0.081, val_acc:0.990]
Epoch [99/120    avg_loss:0.069, val_acc:0.990]
Epoch [100/120    avg_loss:0.073, val_acc:0.990]
Epoch [101/120    avg_loss:0.080, val_acc:0.990]
Epoch [102/120    avg_loss:0.083, val_acc:0.990]
Epoch [103/120    avg_loss:0.071, val_acc:0.990]
Epoch [104/120    avg_loss:0.071, val_acc:0.990]
Epoch [105/120    avg_loss:0.077, val_acc:0.990]
Epoch [106/120    avg_loss:0.066, val_acc:0.990]
Epoch [107/120    avg_loss:0.070, val_acc:0.990]
Epoch [108/120    avg_loss:0.076, val_acc:0.990]
Epoch [109/120    avg_loss:0.070, val_acc:0.988]
Epoch [110/120    avg_loss:0.066, val_acc:0.990]
Epoch [111/120    avg_loss:0.077, val_acc:0.990]
Epoch [112/120    avg_loss:0.072, val_acc:0.990]
Epoch [113/120    avg_loss:0.067, val_acc:0.990]
Epoch [114/120    avg_loss:0.062, val_acc:0.992]
Epoch [115/120    avg_loss:0.074, val_acc:0.990]
Epoch [116/120    avg_loss:0.067, val_acc:0.990]
Epoch [117/120    avg_loss:0.079, val_acc:0.990]
Epoch [118/120    avg_loss:0.073, val_acc:0.992]
Epoch [119/120    avg_loss:0.061, val_acc:0.992]
Epoch [120/120    avg_loss:0.065, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  29 116   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 1.         0.97333333 1.         0.89892473 0.83154122
 1.         0.93181818 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9859927912305791
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0770234b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.157]
Epoch [2/120    avg_loss:2.495, val_acc:0.321]
Epoch [3/120    avg_loss:2.405, val_acc:0.403]
Epoch [4/120    avg_loss:2.324, val_acc:0.417]
Epoch [5/120    avg_loss:2.254, val_acc:0.462]
Epoch [6/120    avg_loss:2.185, val_acc:0.504]
Epoch [7/120    avg_loss:2.096, val_acc:0.532]
Epoch [8/120    avg_loss:2.012, val_acc:0.571]
Epoch [9/120    avg_loss:1.909, val_acc:0.595]
Epoch [10/120    avg_loss:1.814, val_acc:0.601]
Epoch [11/120    avg_loss:1.708, val_acc:0.706]
Epoch [12/120    avg_loss:1.566, val_acc:0.720]
Epoch [13/120    avg_loss:1.498, val_acc:0.647]
Epoch [14/120    avg_loss:1.370, val_acc:0.742]
Epoch [15/120    avg_loss:1.244, val_acc:0.740]
Epoch [16/120    avg_loss:1.114, val_acc:0.812]
Epoch [17/120    avg_loss:1.022, val_acc:0.875]
Epoch [18/120    avg_loss:0.978, val_acc:0.867]
Epoch [19/120    avg_loss:0.860, val_acc:0.887]
Epoch [20/120    avg_loss:0.784, val_acc:0.825]
Epoch [21/120    avg_loss:0.714, val_acc:0.903]
Epoch [22/120    avg_loss:0.680, val_acc:0.845]
Epoch [23/120    avg_loss:0.609, val_acc:0.883]
Epoch [24/120    avg_loss:0.676, val_acc:0.887]
Epoch [25/120    avg_loss:0.614, val_acc:0.893]
Epoch [26/120    avg_loss:0.592, val_acc:0.879]
Epoch [27/120    avg_loss:0.513, val_acc:0.901]
Epoch [28/120    avg_loss:0.498, val_acc:0.883]
Epoch [29/120    avg_loss:0.465, val_acc:0.917]
Epoch [30/120    avg_loss:0.449, val_acc:0.899]
Epoch [31/120    avg_loss:0.448, val_acc:0.927]
Epoch [32/120    avg_loss:0.409, val_acc:0.929]
Epoch [33/120    avg_loss:0.393, val_acc:0.881]
Epoch [34/120    avg_loss:0.386, val_acc:0.915]
Epoch [35/120    avg_loss:0.424, val_acc:0.921]
Epoch [36/120    avg_loss:0.356, val_acc:0.925]
Epoch [37/120    avg_loss:0.365, val_acc:0.931]
Epoch [38/120    avg_loss:0.378, val_acc:0.887]
Epoch [39/120    avg_loss:0.351, val_acc:0.937]
Epoch [40/120    avg_loss:0.353, val_acc:0.915]
Epoch [41/120    avg_loss:0.328, val_acc:0.927]
Epoch [42/120    avg_loss:0.317, val_acc:0.857]
Epoch [43/120    avg_loss:0.332, val_acc:0.929]
Epoch [44/120    avg_loss:0.276, val_acc:0.938]
Epoch [45/120    avg_loss:0.273, val_acc:0.935]
Epoch [46/120    avg_loss:0.284, val_acc:0.946]
Epoch [47/120    avg_loss:0.275, val_acc:0.901]
Epoch [48/120    avg_loss:0.284, val_acc:0.897]
Epoch [49/120    avg_loss:0.265, val_acc:0.940]
Epoch [50/120    avg_loss:0.228, val_acc:0.960]
Epoch [51/120    avg_loss:0.212, val_acc:0.935]
Epoch [52/120    avg_loss:0.227, val_acc:0.946]
Epoch [53/120    avg_loss:0.222, val_acc:0.950]
Epoch [54/120    avg_loss:0.183, val_acc:0.952]
Epoch [55/120    avg_loss:0.194, val_acc:0.966]
Epoch [56/120    avg_loss:0.202, val_acc:0.938]
Epoch [57/120    avg_loss:0.221, val_acc:0.940]
Epoch [58/120    avg_loss:0.194, val_acc:0.948]
Epoch [59/120    avg_loss:0.192, val_acc:0.950]
Epoch [60/120    avg_loss:0.201, val_acc:0.950]
Epoch [61/120    avg_loss:0.214, val_acc:0.958]
Epoch [62/120    avg_loss:0.206, val_acc:0.933]
Epoch [63/120    avg_loss:0.183, val_acc:0.964]
Epoch [64/120    avg_loss:0.201, val_acc:0.968]
Epoch [65/120    avg_loss:0.172, val_acc:0.966]
Epoch [66/120    avg_loss:0.196, val_acc:0.952]
Epoch [67/120    avg_loss:0.186, val_acc:0.946]
Epoch [68/120    avg_loss:0.154, val_acc:0.960]
Epoch [69/120    avg_loss:0.169, val_acc:0.974]
Epoch [70/120    avg_loss:0.155, val_acc:0.960]
Epoch [71/120    avg_loss:0.163, val_acc:0.958]
Epoch [72/120    avg_loss:0.165, val_acc:0.956]
Epoch [73/120    avg_loss:0.193, val_acc:0.954]
Epoch [74/120    avg_loss:0.141, val_acc:0.970]
Epoch [75/120    avg_loss:0.123, val_acc:0.966]
Epoch [76/120    avg_loss:0.121, val_acc:0.946]
Epoch [77/120    avg_loss:0.137, val_acc:0.974]
Epoch [78/120    avg_loss:0.136, val_acc:0.972]
Epoch [79/120    avg_loss:0.121, val_acc:0.964]
Epoch [80/120    avg_loss:0.129, val_acc:0.982]
Epoch [81/120    avg_loss:0.127, val_acc:0.952]
Epoch [82/120    avg_loss:0.111, val_acc:0.974]
Epoch [83/120    avg_loss:0.106, val_acc:0.976]
Epoch [84/120    avg_loss:0.137, val_acc:0.982]
Epoch [85/120    avg_loss:0.117, val_acc:0.978]
Epoch [86/120    avg_loss:0.086, val_acc:0.952]
Epoch [87/120    avg_loss:0.112, val_acc:0.976]
Epoch [88/120    avg_loss:0.102, val_acc:0.970]
Epoch [89/120    avg_loss:0.098, val_acc:0.974]
Epoch [90/120    avg_loss:0.071, val_acc:0.972]
Epoch [91/120    avg_loss:0.115, val_acc:0.933]
Epoch [92/120    avg_loss:0.188, val_acc:0.935]
Epoch [93/120    avg_loss:0.156, val_acc:0.970]
Epoch [94/120    avg_loss:0.091, val_acc:0.976]
Epoch [95/120    avg_loss:0.074, val_acc:0.986]
Epoch [96/120    avg_loss:0.096, val_acc:0.964]
Epoch [97/120    avg_loss:0.090, val_acc:0.982]
Epoch [98/120    avg_loss:0.072, val_acc:0.986]
Epoch [99/120    avg_loss:0.067, val_acc:0.964]
Epoch [100/120    avg_loss:0.068, val_acc:0.972]
Epoch [101/120    avg_loss:0.063, val_acc:0.986]
Epoch [102/120    avg_loss:0.072, val_acc:0.952]
Epoch [103/120    avg_loss:0.096, val_acc:0.950]
Epoch [104/120    avg_loss:0.110, val_acc:0.974]
Epoch [105/120    avg_loss:0.068, val_acc:0.988]
Epoch [106/120    avg_loss:0.081, val_acc:0.962]
Epoch [107/120    avg_loss:0.093, val_acc:0.984]
Epoch [108/120    avg_loss:0.077, val_acc:0.970]
Epoch [109/120    avg_loss:0.062, val_acc:0.988]
Epoch [110/120    avg_loss:0.066, val_acc:0.988]
Epoch [111/120    avg_loss:0.061, val_acc:0.988]
Epoch [112/120    avg_loss:0.060, val_acc:0.988]
Epoch [113/120    avg_loss:0.055, val_acc:0.986]
Epoch [114/120    avg_loss:0.057, val_acc:0.982]
Epoch [115/120    avg_loss:0.039, val_acc:0.984]
Epoch [116/120    avg_loss:0.045, val_acc:0.984]
Epoch [117/120    avg_loss:0.042, val_acc:0.988]
Epoch [118/120    avg_loss:0.039, val_acc:0.990]
Epoch [119/120    avg_loss:0.051, val_acc:0.984]
Epoch [120/120    avg_loss:0.057, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 225   3   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.97117517 0.98901099 0.93271462 0.91772152
 1.         0.93181818 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9897928113291241
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2f1e1c4a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.583, val_acc:0.346]
Epoch [2/120    avg_loss:2.469, val_acc:0.404]
Epoch [3/120    avg_loss:2.372, val_acc:0.415]
Epoch [4/120    avg_loss:2.293, val_acc:0.415]
Epoch [5/120    avg_loss:2.215, val_acc:0.446]
Epoch [6/120    avg_loss:2.133, val_acc:0.512]
Epoch [7/120    avg_loss:2.041, val_acc:0.548]
Epoch [8/120    avg_loss:1.939, val_acc:0.604]
Epoch [9/120    avg_loss:1.840, val_acc:0.646]
Epoch [10/120    avg_loss:1.753, val_acc:0.635]
Epoch [11/120    avg_loss:1.660, val_acc:0.650]
Epoch [12/120    avg_loss:1.573, val_acc:0.675]
Epoch [13/120    avg_loss:1.464, val_acc:0.710]
Epoch [14/120    avg_loss:1.364, val_acc:0.719]
Epoch [15/120    avg_loss:1.246, val_acc:0.748]
Epoch [16/120    avg_loss:1.193, val_acc:0.746]
Epoch [17/120    avg_loss:1.074, val_acc:0.744]
Epoch [18/120    avg_loss:1.047, val_acc:0.756]
Epoch [19/120    avg_loss:0.936, val_acc:0.779]
Epoch [20/120    avg_loss:0.834, val_acc:0.844]
Epoch [21/120    avg_loss:0.814, val_acc:0.898]
Epoch [22/120    avg_loss:0.768, val_acc:0.835]
Epoch [23/120    avg_loss:0.769, val_acc:0.917]
Epoch [24/120    avg_loss:0.646, val_acc:0.894]
Epoch [25/120    avg_loss:0.621, val_acc:0.915]
Epoch [26/120    avg_loss:0.571, val_acc:0.921]
Epoch [27/120    avg_loss:0.545, val_acc:0.860]
Epoch [28/120    avg_loss:0.568, val_acc:0.875]
Epoch [29/120    avg_loss:0.509, val_acc:0.921]
Epoch [30/120    avg_loss:0.461, val_acc:0.877]
Epoch [31/120    avg_loss:0.455, val_acc:0.927]
Epoch [32/120    avg_loss:0.454, val_acc:0.944]
Epoch [33/120    avg_loss:0.379, val_acc:0.935]
Epoch [34/120    avg_loss:0.353, val_acc:0.946]
Epoch [35/120    avg_loss:0.340, val_acc:0.904]
Epoch [36/120    avg_loss:0.359, val_acc:0.946]
Epoch [37/120    avg_loss:0.372, val_acc:0.940]
Epoch [38/120    avg_loss:0.366, val_acc:0.944]
Epoch [39/120    avg_loss:0.332, val_acc:0.892]
Epoch [40/120    avg_loss:0.332, val_acc:0.935]
Epoch [41/120    avg_loss:0.324, val_acc:0.933]
Epoch [42/120    avg_loss:0.317, val_acc:0.954]
Epoch [43/120    avg_loss:0.320, val_acc:0.933]
Epoch [44/120    avg_loss:0.238, val_acc:0.956]
Epoch [45/120    avg_loss:0.279, val_acc:0.956]
Epoch [46/120    avg_loss:0.240, val_acc:0.965]
Epoch [47/120    avg_loss:0.207, val_acc:0.933]
Epoch [48/120    avg_loss:0.244, val_acc:0.965]
Epoch [49/120    avg_loss:0.281, val_acc:0.923]
Epoch [50/120    avg_loss:0.267, val_acc:0.956]
Epoch [51/120    avg_loss:0.219, val_acc:0.965]
Epoch [52/120    avg_loss:0.201, val_acc:0.952]
Epoch [53/120    avg_loss:0.223, val_acc:0.960]
Epoch [54/120    avg_loss:0.239, val_acc:0.954]
Epoch [55/120    avg_loss:0.193, val_acc:0.975]
Epoch [56/120    avg_loss:0.251, val_acc:0.938]
Epoch [57/120    avg_loss:0.184, val_acc:0.973]
Epoch [58/120    avg_loss:0.178, val_acc:0.973]
Epoch [59/120    avg_loss:0.153, val_acc:0.977]
Epoch [60/120    avg_loss:0.163, val_acc:0.973]
Epoch [61/120    avg_loss:0.179, val_acc:0.952]
Epoch [62/120    avg_loss:0.156, val_acc:0.979]
Epoch [63/120    avg_loss:0.229, val_acc:0.931]
Epoch [64/120    avg_loss:0.176, val_acc:0.956]
Epoch [65/120    avg_loss:0.185, val_acc:0.979]
Epoch [66/120    avg_loss:0.193, val_acc:0.971]
Epoch [67/120    avg_loss:0.164, val_acc:0.971]
Epoch [68/120    avg_loss:0.135, val_acc:0.965]
Epoch [69/120    avg_loss:0.210, val_acc:0.942]
Epoch [70/120    avg_loss:0.213, val_acc:0.963]
Epoch [71/120    avg_loss:0.170, val_acc:0.935]
Epoch [72/120    avg_loss:0.166, val_acc:0.973]
Epoch [73/120    avg_loss:0.124, val_acc:0.975]
Epoch [74/120    avg_loss:0.144, val_acc:0.973]
Epoch [75/120    avg_loss:0.163, val_acc:0.969]
Epoch [76/120    avg_loss:0.122, val_acc:0.985]
Epoch [77/120    avg_loss:0.113, val_acc:0.954]
Epoch [78/120    avg_loss:0.115, val_acc:0.983]
Epoch [79/120    avg_loss:0.106, val_acc:0.965]
Epoch [80/120    avg_loss:0.096, val_acc:0.979]
Epoch [81/120    avg_loss:0.095, val_acc:0.973]
Epoch [82/120    avg_loss:0.080, val_acc:0.985]
Epoch [83/120    avg_loss:0.072, val_acc:0.983]
Epoch [84/120    avg_loss:0.117, val_acc:0.944]
Epoch [85/120    avg_loss:0.102, val_acc:0.983]
Epoch [86/120    avg_loss:0.099, val_acc:0.969]
Epoch [87/120    avg_loss:0.086, val_acc:0.988]
Epoch [88/120    avg_loss:0.112, val_acc:0.960]
Epoch [89/120    avg_loss:0.110, val_acc:0.981]
Epoch [90/120    avg_loss:0.095, val_acc:0.979]
Epoch [91/120    avg_loss:0.067, val_acc:0.975]
Epoch [92/120    avg_loss:0.077, val_acc:0.965]
Epoch [93/120    avg_loss:0.146, val_acc:0.940]
Epoch [94/120    avg_loss:0.120, val_acc:0.981]
Epoch [95/120    avg_loss:0.067, val_acc:0.979]
Epoch [96/120    avg_loss:0.060, val_acc:0.983]
Epoch [97/120    avg_loss:0.084, val_acc:0.983]
Epoch [98/120    avg_loss:0.083, val_acc:0.983]
Epoch [99/120    avg_loss:0.063, val_acc:0.983]
Epoch [100/120    avg_loss:0.110, val_acc:0.973]
Epoch [101/120    avg_loss:0.057, val_acc:0.983]
Epoch [102/120    avg_loss:0.051, val_acc:0.988]
Epoch [103/120    avg_loss:0.047, val_acc:0.988]
Epoch [104/120    avg_loss:0.043, val_acc:0.988]
Epoch [105/120    avg_loss:0.043, val_acc:0.988]
Epoch [106/120    avg_loss:0.050, val_acc:0.988]
Epoch [107/120    avg_loss:0.045, val_acc:0.988]
Epoch [108/120    avg_loss:0.046, val_acc:0.990]
Epoch [109/120    avg_loss:0.038, val_acc:0.988]
Epoch [110/120    avg_loss:0.036, val_acc:0.990]
Epoch [111/120    avg_loss:0.042, val_acc:0.988]
Epoch [112/120    avg_loss:0.038, val_acc:0.988]
Epoch [113/120    avg_loss:0.042, val_acc:0.988]
Epoch [114/120    avg_loss:0.044, val_acc:0.988]
Epoch [115/120    avg_loss:0.038, val_acc:0.988]
Epoch [116/120    avg_loss:0.044, val_acc:0.990]
Epoch [117/120    avg_loss:0.037, val_acc:0.990]
Epoch [118/120    avg_loss:0.034, val_acc:0.990]
Epoch [119/120    avg_loss:0.035, val_acc:0.988]
Epoch [120/120    avg_loss:0.047, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.97757848 0.99563319 0.9173913  0.86619718
 1.         0.96132597 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9886047794628484
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4191bd8ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.218]
Epoch [2/120    avg_loss:2.507, val_acc:0.359]
Epoch [3/120    avg_loss:2.402, val_acc:0.411]
Epoch [4/120    avg_loss:2.314, val_acc:0.472]
Epoch [5/120    avg_loss:2.227, val_acc:0.464]
Epoch [6/120    avg_loss:2.136, val_acc:0.536]
Epoch [7/120    avg_loss:2.058, val_acc:0.558]
Epoch [8/120    avg_loss:1.973, val_acc:0.629]
Epoch [9/120    avg_loss:1.877, val_acc:0.595]
Epoch [10/120    avg_loss:1.772, val_acc:0.641]
Epoch [11/120    avg_loss:1.654, val_acc:0.647]
Epoch [12/120    avg_loss:1.562, val_acc:0.690]
Epoch [13/120    avg_loss:1.426, val_acc:0.726]
Epoch [14/120    avg_loss:1.334, val_acc:0.738]
Epoch [15/120    avg_loss:1.228, val_acc:0.788]
Epoch [16/120    avg_loss:1.071, val_acc:0.782]
Epoch [17/120    avg_loss:1.047, val_acc:0.897]
Epoch [18/120    avg_loss:0.930, val_acc:0.728]
Epoch [19/120    avg_loss:0.880, val_acc:0.909]
Epoch [20/120    avg_loss:0.820, val_acc:0.903]
Epoch [21/120    avg_loss:0.720, val_acc:0.905]
Epoch [22/120    avg_loss:0.746, val_acc:0.899]
Epoch [23/120    avg_loss:0.693, val_acc:0.899]
Epoch [24/120    avg_loss:0.619, val_acc:0.923]
Epoch [25/120    avg_loss:0.543, val_acc:0.913]
Epoch [26/120    avg_loss:0.482, val_acc:0.927]
Epoch [27/120    avg_loss:0.535, val_acc:0.873]
Epoch [28/120    avg_loss:0.627, val_acc:0.909]
Epoch [29/120    avg_loss:0.497, val_acc:0.915]
Epoch [30/120    avg_loss:0.435, val_acc:0.871]
Epoch [31/120    avg_loss:0.450, val_acc:0.937]
Epoch [32/120    avg_loss:0.434, val_acc:0.944]
Epoch [33/120    avg_loss:0.365, val_acc:0.929]
Epoch [34/120    avg_loss:0.374, val_acc:0.929]
Epoch [35/120    avg_loss:0.428, val_acc:0.841]
Epoch [36/120    avg_loss:0.441, val_acc:0.923]
Epoch [37/120    avg_loss:0.399, val_acc:0.942]
Epoch [38/120    avg_loss:0.360, val_acc:0.952]
Epoch [39/120    avg_loss:0.343, val_acc:0.940]
Epoch [40/120    avg_loss:0.307, val_acc:0.960]
Epoch [41/120    avg_loss:0.276, val_acc:0.954]
Epoch [42/120    avg_loss:0.286, val_acc:0.950]
Epoch [43/120    avg_loss:0.335, val_acc:0.948]
Epoch [44/120    avg_loss:0.342, val_acc:0.958]
Epoch [45/120    avg_loss:0.267, val_acc:0.925]
Epoch [46/120    avg_loss:0.324, val_acc:0.948]
Epoch [47/120    avg_loss:0.242, val_acc:0.948]
Epoch [48/120    avg_loss:0.271, val_acc:0.960]
Epoch [49/120    avg_loss:0.221, val_acc:0.966]
Epoch [50/120    avg_loss:0.186, val_acc:0.966]
Epoch [51/120    avg_loss:0.200, val_acc:0.964]
Epoch [52/120    avg_loss:0.183, val_acc:0.974]
Epoch [53/120    avg_loss:0.176, val_acc:0.964]
Epoch [54/120    avg_loss:0.198, val_acc:0.972]
Epoch [55/120    avg_loss:0.197, val_acc:0.956]
Epoch [56/120    avg_loss:0.165, val_acc:0.974]
Epoch [57/120    avg_loss:0.176, val_acc:0.962]
Epoch [58/120    avg_loss:0.165, val_acc:0.970]
Epoch [59/120    avg_loss:0.133, val_acc:0.972]
Epoch [60/120    avg_loss:0.220, val_acc:0.960]
Epoch [61/120    avg_loss:0.259, val_acc:0.952]
Epoch [62/120    avg_loss:0.186, val_acc:0.978]
Epoch [63/120    avg_loss:0.263, val_acc:0.956]
Epoch [64/120    avg_loss:0.228, val_acc:0.948]
Epoch [65/120    avg_loss:0.166, val_acc:0.954]
Epoch [66/120    avg_loss:0.174, val_acc:0.968]
Epoch [67/120    avg_loss:0.178, val_acc:0.984]
Epoch [68/120    avg_loss:0.135, val_acc:0.980]
Epoch [69/120    avg_loss:0.154, val_acc:0.938]
Epoch [70/120    avg_loss:0.164, val_acc:0.982]
Epoch [71/120    avg_loss:0.151, val_acc:0.988]
Epoch [72/120    avg_loss:0.122, val_acc:0.978]
Epoch [73/120    avg_loss:0.115, val_acc:0.986]
Epoch [74/120    avg_loss:0.075, val_acc:0.978]
Epoch [75/120    avg_loss:0.092, val_acc:0.978]
Epoch [76/120    avg_loss:0.087, val_acc:0.980]
Epoch [77/120    avg_loss:0.122, val_acc:0.972]
Epoch [78/120    avg_loss:0.171, val_acc:0.976]
Epoch [79/120    avg_loss:0.125, val_acc:0.978]
Epoch [80/120    avg_loss:0.094, val_acc:0.986]
Epoch [81/120    avg_loss:0.121, val_acc:0.980]
Epoch [82/120    avg_loss:0.084, val_acc:0.990]
Epoch [83/120    avg_loss:0.101, val_acc:0.978]
Epoch [84/120    avg_loss:0.096, val_acc:0.982]
Epoch [85/120    avg_loss:0.080, val_acc:0.990]
Epoch [86/120    avg_loss:0.079, val_acc:0.976]
Epoch [87/120    avg_loss:0.076, val_acc:0.978]
Epoch [88/120    avg_loss:0.096, val_acc:0.986]
Epoch [89/120    avg_loss:0.063, val_acc:0.982]
Epoch [90/120    avg_loss:0.063, val_acc:0.984]
Epoch [91/120    avg_loss:0.108, val_acc:0.980]
Epoch [92/120    avg_loss:0.066, val_acc:0.982]
Epoch [93/120    avg_loss:0.080, val_acc:0.984]
Epoch [94/120    avg_loss:0.068, val_acc:0.984]
Epoch [95/120    avg_loss:0.076, val_acc:0.988]
Epoch [96/120    avg_loss:0.057, val_acc:0.984]
Epoch [97/120    avg_loss:0.071, val_acc:0.968]
Epoch [98/120    avg_loss:0.077, val_acc:0.956]
Epoch [99/120    avg_loss:0.065, val_acc:0.984]
Epoch [100/120    avg_loss:0.035, val_acc:0.992]
Epoch [101/120    avg_loss:0.037, val_acc:0.992]
Epoch [102/120    avg_loss:0.037, val_acc:0.994]
Epoch [103/120    avg_loss:0.038, val_acc:0.992]
Epoch [104/120    avg_loss:0.043, val_acc:0.992]
Epoch [105/120    avg_loss:0.031, val_acc:0.990]
Epoch [106/120    avg_loss:0.035, val_acc:0.990]
Epoch [107/120    avg_loss:0.027, val_acc:0.992]
Epoch [108/120    avg_loss:0.035, val_acc:0.990]
Epoch [109/120    avg_loss:0.031, val_acc:0.990]
Epoch [110/120    avg_loss:0.045, val_acc:0.990]
Epoch [111/120    avg_loss:0.032, val_acc:0.988]
Epoch [112/120    avg_loss:0.029, val_acc:0.990]
Epoch [113/120    avg_loss:0.030, val_acc:0.990]
Epoch [114/120    avg_loss:0.032, val_acc:0.990]
Epoch [115/120    avg_loss:0.041, val_acc:0.994]
Epoch [116/120    avg_loss:0.030, val_acc:0.990]
Epoch [117/120    avg_loss:0.033, val_acc:0.988]
Epoch [118/120    avg_loss:0.027, val_acc:0.990]
Epoch [119/120    avg_loss:0.035, val_acc:0.990]
Epoch [120/120    avg_loss:0.027, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.97550111 0.98678414 0.95768374 0.95681063
 1.         0.93785311 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9928783923489537
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb60edc89e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.389]
Epoch [2/120    avg_loss:2.450, val_acc:0.490]
Epoch [3/120    avg_loss:2.333, val_acc:0.502]
Epoch [4/120    avg_loss:2.234, val_acc:0.448]
Epoch [5/120    avg_loss:2.136, val_acc:0.452]
Epoch [6/120    avg_loss:2.039, val_acc:0.524]
Epoch [7/120    avg_loss:1.939, val_acc:0.625]
Epoch [8/120    avg_loss:1.860, val_acc:0.575]
Epoch [9/120    avg_loss:1.799, val_acc:0.714]
Epoch [10/120    avg_loss:1.705, val_acc:0.728]
Epoch [11/120    avg_loss:1.619, val_acc:0.754]
Epoch [12/120    avg_loss:1.551, val_acc:0.752]
Epoch [13/120    avg_loss:1.450, val_acc:0.772]
Epoch [14/120    avg_loss:1.372, val_acc:0.788]
Epoch [15/120    avg_loss:1.311, val_acc:0.778]
Epoch [16/120    avg_loss:1.196, val_acc:0.804]
Epoch [17/120    avg_loss:1.119, val_acc:0.817]
Epoch [18/120    avg_loss:1.010, val_acc:0.851]
Epoch [19/120    avg_loss:0.944, val_acc:0.819]
Epoch [20/120    avg_loss:0.880, val_acc:0.833]
Epoch [21/120    avg_loss:0.830, val_acc:0.829]
Epoch [22/120    avg_loss:0.770, val_acc:0.871]
Epoch [23/120    avg_loss:0.734, val_acc:0.859]
Epoch [24/120    avg_loss:0.640, val_acc:0.873]
Epoch [25/120    avg_loss:0.610, val_acc:0.871]
Epoch [26/120    avg_loss:0.587, val_acc:0.845]
Epoch [27/120    avg_loss:0.572, val_acc:0.889]
Epoch [28/120    avg_loss:0.556, val_acc:0.879]
Epoch [29/120    avg_loss:0.560, val_acc:0.855]
Epoch [30/120    avg_loss:0.517, val_acc:0.893]
Epoch [31/120    avg_loss:0.515, val_acc:0.909]
Epoch [32/120    avg_loss:0.438, val_acc:0.901]
Epoch [33/120    avg_loss:0.443, val_acc:0.909]
Epoch [34/120    avg_loss:0.408, val_acc:0.911]
Epoch [35/120    avg_loss:0.446, val_acc:0.913]
Epoch [36/120    avg_loss:0.423, val_acc:0.933]
Epoch [37/120    avg_loss:0.363, val_acc:0.935]
Epoch [38/120    avg_loss:0.360, val_acc:0.927]
Epoch [39/120    avg_loss:0.337, val_acc:0.905]
Epoch [40/120    avg_loss:0.329, val_acc:0.915]
Epoch [41/120    avg_loss:0.362, val_acc:0.913]
Epoch [42/120    avg_loss:0.352, val_acc:0.913]
Epoch [43/120    avg_loss:0.346, val_acc:0.927]
Epoch [44/120    avg_loss:0.329, val_acc:0.925]
Epoch [45/120    avg_loss:0.326, val_acc:0.927]
Epoch [46/120    avg_loss:0.291, val_acc:0.933]
Epoch [47/120    avg_loss:0.321, val_acc:0.937]
Epoch [48/120    avg_loss:0.320, val_acc:0.913]
Epoch [49/120    avg_loss:0.275, val_acc:0.948]
Epoch [50/120    avg_loss:0.329, val_acc:0.950]
Epoch [51/120    avg_loss:0.302, val_acc:0.913]
Epoch [52/120    avg_loss:0.308, val_acc:0.944]
Epoch [53/120    avg_loss:0.326, val_acc:0.923]
Epoch [54/120    avg_loss:0.285, val_acc:0.950]
Epoch [55/120    avg_loss:0.220, val_acc:0.942]
Epoch [56/120    avg_loss:0.241, val_acc:0.948]
Epoch [57/120    avg_loss:0.229, val_acc:0.954]
Epoch [58/120    avg_loss:0.195, val_acc:0.944]
Epoch [59/120    avg_loss:0.235, val_acc:0.925]
Epoch [60/120    avg_loss:0.243, val_acc:0.927]
Epoch [61/120    avg_loss:0.235, val_acc:0.944]
Epoch [62/120    avg_loss:0.211, val_acc:0.958]
Epoch [63/120    avg_loss:0.179, val_acc:0.929]
Epoch [64/120    avg_loss:0.185, val_acc:0.956]
Epoch [65/120    avg_loss:0.140, val_acc:0.958]
Epoch [66/120    avg_loss:0.209, val_acc:0.966]
Epoch [67/120    avg_loss:0.159, val_acc:0.948]
Epoch [68/120    avg_loss:0.164, val_acc:0.946]
Epoch [69/120    avg_loss:0.298, val_acc:0.942]
Epoch [70/120    avg_loss:0.166, val_acc:0.942]
Epoch [71/120    avg_loss:0.186, val_acc:0.964]
Epoch [72/120    avg_loss:0.143, val_acc:0.968]
Epoch [73/120    avg_loss:0.176, val_acc:0.935]
Epoch [74/120    avg_loss:0.170, val_acc:0.964]
Epoch [75/120    avg_loss:0.137, val_acc:0.968]
Epoch [76/120    avg_loss:0.136, val_acc:0.950]
Epoch [77/120    avg_loss:0.138, val_acc:0.966]
Epoch [78/120    avg_loss:0.146, val_acc:0.946]
Epoch [79/120    avg_loss:0.157, val_acc:0.968]
Epoch [80/120    avg_loss:0.104, val_acc:0.978]
Epoch [81/120    avg_loss:0.125, val_acc:0.948]
Epoch [82/120    avg_loss:0.110, val_acc:0.954]
Epoch [83/120    avg_loss:0.116, val_acc:0.948]
Epoch [84/120    avg_loss:0.143, val_acc:0.958]
Epoch [85/120    avg_loss:0.122, val_acc:0.964]
Epoch [86/120    avg_loss:0.103, val_acc:0.972]
Epoch [87/120    avg_loss:0.123, val_acc:0.962]
Epoch [88/120    avg_loss:0.118, val_acc:0.972]
Epoch [89/120    avg_loss:0.089, val_acc:0.968]
Epoch [90/120    avg_loss:0.093, val_acc:0.974]
Epoch [91/120    avg_loss:0.070, val_acc:0.972]
Epoch [92/120    avg_loss:0.094, val_acc:0.972]
Epoch [93/120    avg_loss:0.105, val_acc:0.970]
Epoch [94/120    avg_loss:0.066, val_acc:0.974]
Epoch [95/120    avg_loss:0.053, val_acc:0.976]
Epoch [96/120    avg_loss:0.045, val_acc:0.976]
Epoch [97/120    avg_loss:0.050, val_acc:0.978]
Epoch [98/120    avg_loss:0.053, val_acc:0.980]
Epoch [99/120    avg_loss:0.047, val_acc:0.978]
Epoch [100/120    avg_loss:0.048, val_acc:0.980]
Epoch [101/120    avg_loss:0.047, val_acc:0.978]
Epoch [102/120    avg_loss:0.038, val_acc:0.978]
Epoch [103/120    avg_loss:0.053, val_acc:0.978]
Epoch [104/120    avg_loss:0.043, val_acc:0.978]
Epoch [105/120    avg_loss:0.039, val_acc:0.978]
Epoch [106/120    avg_loss:0.047, val_acc:0.978]
Epoch [107/120    avg_loss:0.045, val_acc:0.980]
Epoch [108/120    avg_loss:0.039, val_acc:0.978]
Epoch [109/120    avg_loss:0.046, val_acc:0.978]
Epoch [110/120    avg_loss:0.041, val_acc:0.978]
Epoch [111/120    avg_loss:0.041, val_acc:0.978]
Epoch [112/120    avg_loss:0.040, val_acc:0.978]
Epoch [113/120    avg_loss:0.046, val_acc:0.978]
Epoch [114/120    avg_loss:0.044, val_acc:0.978]
Epoch [115/120    avg_loss:0.038, val_acc:0.978]
Epoch [116/120    avg_loss:0.039, val_acc:0.980]
Epoch [117/120    avg_loss:0.037, val_acc:0.978]
Epoch [118/120    avg_loss:0.044, val_acc:0.980]
Epoch [119/120    avg_loss:0.046, val_acc:0.980]
Epoch [120/120    avg_loss:0.034, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215   9   0   0   0   0   3   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 0.99927061 0.98426966 0.99563319 0.91880342 0.88
 0.99756691 0.96132597 1.         1.         0.99589603 1.
 1.         1.        ]

Kappa:
0.989078800850916
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca9e82ea20>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.628, val_acc:0.161]
Epoch [2/120    avg_loss:2.509, val_acc:0.381]
Epoch [3/120    avg_loss:2.410, val_acc:0.435]
Epoch [4/120    avg_loss:2.315, val_acc:0.508]
Epoch [5/120    avg_loss:2.239, val_acc:0.542]
Epoch [6/120    avg_loss:2.156, val_acc:0.583]
Epoch [7/120    avg_loss:2.064, val_acc:0.581]
Epoch [8/120    avg_loss:1.952, val_acc:0.659]
Epoch [9/120    avg_loss:1.866, val_acc:0.647]
Epoch [10/120    avg_loss:1.732, val_acc:0.732]
Epoch [11/120    avg_loss:1.620, val_acc:0.736]
Epoch [12/120    avg_loss:1.524, val_acc:0.720]
Epoch [13/120    avg_loss:1.403, val_acc:0.726]
Epoch [14/120    avg_loss:1.277, val_acc:0.744]
Epoch [15/120    avg_loss:1.220, val_acc:0.744]
Epoch [16/120    avg_loss:1.127, val_acc:0.786]
Epoch [17/120    avg_loss:1.013, val_acc:0.810]
Epoch [18/120    avg_loss:0.906, val_acc:0.863]
Epoch [19/120    avg_loss:0.828, val_acc:0.911]
Epoch [20/120    avg_loss:0.750, val_acc:0.885]
Epoch [21/120    avg_loss:0.705, val_acc:0.911]
Epoch [22/120    avg_loss:0.657, val_acc:0.927]
Epoch [23/120    avg_loss:0.651, val_acc:0.919]
Epoch [24/120    avg_loss:0.597, val_acc:0.859]
Epoch [25/120    avg_loss:0.553, val_acc:0.921]
Epoch [26/120    avg_loss:0.525, val_acc:0.925]
Epoch [27/120    avg_loss:0.522, val_acc:0.931]
Epoch [28/120    avg_loss:0.447, val_acc:0.919]
Epoch [29/120    avg_loss:0.440, val_acc:0.929]
Epoch [30/120    avg_loss:0.439, val_acc:0.933]
Epoch [31/120    avg_loss:0.389, val_acc:0.933]
Epoch [32/120    avg_loss:0.408, val_acc:0.927]
Epoch [33/120    avg_loss:0.545, val_acc:0.893]
Epoch [34/120    avg_loss:0.492, val_acc:0.907]
Epoch [35/120    avg_loss:0.490, val_acc:0.944]
Epoch [36/120    avg_loss:0.431, val_acc:0.923]
Epoch [37/120    avg_loss:0.362, val_acc:0.911]
Epoch [38/120    avg_loss:0.361, val_acc:0.929]
Epoch [39/120    avg_loss:0.326, val_acc:0.911]
Epoch [40/120    avg_loss:0.362, val_acc:0.942]
Epoch [41/120    avg_loss:0.324, val_acc:0.917]
Epoch [42/120    avg_loss:0.381, val_acc:0.944]
Epoch [43/120    avg_loss:0.358, val_acc:0.927]
Epoch [44/120    avg_loss:0.299, val_acc:0.954]
Epoch [45/120    avg_loss:0.275, val_acc:0.950]
Epoch [46/120    avg_loss:0.262, val_acc:0.966]
Epoch [47/120    avg_loss:0.234, val_acc:0.964]
Epoch [48/120    avg_loss:0.223, val_acc:0.960]
Epoch [49/120    avg_loss:0.215, val_acc:0.927]
Epoch [50/120    avg_loss:0.255, val_acc:0.905]
Epoch [51/120    avg_loss:0.247, val_acc:0.962]
Epoch [52/120    avg_loss:0.227, val_acc:0.946]
Epoch [53/120    avg_loss:0.212, val_acc:0.962]
Epoch [54/120    avg_loss:0.225, val_acc:0.966]
Epoch [55/120    avg_loss:0.220, val_acc:0.911]
Epoch [56/120    avg_loss:0.201, val_acc:0.952]
Epoch [57/120    avg_loss:0.192, val_acc:0.927]
Epoch [58/120    avg_loss:0.197, val_acc:0.964]
Epoch [59/120    avg_loss:0.145, val_acc:0.976]
Epoch [60/120    avg_loss:0.260, val_acc:0.935]
Epoch [61/120    avg_loss:0.192, val_acc:0.958]
Epoch [62/120    avg_loss:0.193, val_acc:0.968]
Epoch [63/120    avg_loss:0.153, val_acc:0.958]
Epoch [64/120    avg_loss:0.152, val_acc:0.970]
Epoch [65/120    avg_loss:0.197, val_acc:0.938]
Epoch [66/120    avg_loss:0.172, val_acc:0.962]
Epoch [67/120    avg_loss:0.181, val_acc:0.950]
Epoch [68/120    avg_loss:0.159, val_acc:0.970]
Epoch [69/120    avg_loss:0.119, val_acc:0.964]
Epoch [70/120    avg_loss:0.149, val_acc:0.976]
Epoch [71/120    avg_loss:0.101, val_acc:0.972]
Epoch [72/120    avg_loss:0.112, val_acc:0.960]
Epoch [73/120    avg_loss:0.177, val_acc:0.919]
Epoch [74/120    avg_loss:0.219, val_acc:0.964]
Epoch [75/120    avg_loss:0.141, val_acc:0.970]
Epoch [76/120    avg_loss:0.164, val_acc:0.942]
Epoch [77/120    avg_loss:0.206, val_acc:0.972]
Epoch [78/120    avg_loss:0.148, val_acc:0.919]
Epoch [79/120    avg_loss:0.171, val_acc:0.966]
Epoch [80/120    avg_loss:0.217, val_acc:0.958]
Epoch [81/120    avg_loss:0.214, val_acc:0.966]
Epoch [82/120    avg_loss:0.153, val_acc:0.952]
Epoch [83/120    avg_loss:0.116, val_acc:0.984]
Epoch [84/120    avg_loss:0.124, val_acc:0.956]
Epoch [85/120    avg_loss:0.121, val_acc:0.974]
Epoch [86/120    avg_loss:0.130, val_acc:0.958]
Epoch [87/120    avg_loss:0.143, val_acc:0.978]
Epoch [88/120    avg_loss:0.137, val_acc:0.978]
Epoch [89/120    avg_loss:0.133, val_acc:0.970]
Epoch [90/120    avg_loss:0.071, val_acc:0.980]
Epoch [91/120    avg_loss:0.074, val_acc:0.974]
Epoch [92/120    avg_loss:0.122, val_acc:0.978]
Epoch [93/120    avg_loss:0.147, val_acc:0.958]
Epoch [94/120    avg_loss:0.191, val_acc:0.974]
Epoch [95/120    avg_loss:0.115, val_acc:0.954]
Epoch [96/120    avg_loss:0.152, val_acc:0.948]
Epoch [97/120    avg_loss:0.127, val_acc:0.970]
Epoch [98/120    avg_loss:0.075, val_acc:0.976]
Epoch [99/120    avg_loss:0.068, val_acc:0.982]
Epoch [100/120    avg_loss:0.059, val_acc:0.980]
Epoch [101/120    avg_loss:0.063, val_acc:0.978]
Epoch [102/120    avg_loss:0.062, val_acc:0.976]
Epoch [103/120    avg_loss:0.052, val_acc:0.978]
Epoch [104/120    avg_loss:0.056, val_acc:0.976]
Epoch [105/120    avg_loss:0.066, val_acc:0.978]
Epoch [106/120    avg_loss:0.053, val_acc:0.982]
Epoch [107/120    avg_loss:0.049, val_acc:0.982]
Epoch [108/120    avg_loss:0.054, val_acc:0.982]
Epoch [109/120    avg_loss:0.054, val_acc:0.980]
Epoch [110/120    avg_loss:0.059, val_acc:0.980]
Epoch [111/120    avg_loss:0.043, val_acc:0.980]
Epoch [112/120    avg_loss:0.047, val_acc:0.980]
Epoch [113/120    avg_loss:0.067, val_acc:0.982]
Epoch [114/120    avg_loss:0.049, val_acc:0.982]
Epoch [115/120    avg_loss:0.044, val_acc:0.982]
Epoch [116/120    avg_loss:0.059, val_acc:0.982]
Epoch [117/120    avg_loss:0.048, val_acc:0.982]
Epoch [118/120    avg_loss:0.057, val_acc:0.982]
Epoch [119/120    avg_loss:0.056, val_acc:0.984]
Epoch [120/120    avg_loss:0.056, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  18   0   0   0   0   0   0   1   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   1   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 1.         0.96428571 0.99122807 0.91629956 0.89115646
 1.         0.91011236 1.         1.         1.         1.
 0.99779249 1.        ]

Kappa:
0.9871808110012392
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdff097bac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.275]
Epoch [2/120    avg_loss:2.482, val_acc:0.338]
Epoch [3/120    avg_loss:2.369, val_acc:0.362]
Epoch [4/120    avg_loss:2.283, val_acc:0.388]
Epoch [5/120    avg_loss:2.204, val_acc:0.435]
Epoch [6/120    avg_loss:2.127, val_acc:0.494]
Epoch [7/120    avg_loss:2.045, val_acc:0.525]
Epoch [8/120    avg_loss:1.945, val_acc:0.558]
Epoch [9/120    avg_loss:1.869, val_acc:0.575]
Epoch [10/120    avg_loss:1.795, val_acc:0.596]
Epoch [11/120    avg_loss:1.676, val_acc:0.637]
Epoch [12/120    avg_loss:1.598, val_acc:0.656]
Epoch [13/120    avg_loss:1.512, val_acc:0.642]
Epoch [14/120    avg_loss:1.417, val_acc:0.688]
Epoch [15/120    avg_loss:1.304, val_acc:0.717]
Epoch [16/120    avg_loss:1.222, val_acc:0.719]
Epoch [17/120    avg_loss:1.150, val_acc:0.729]
Epoch [18/120    avg_loss:1.051, val_acc:0.777]
Epoch [19/120    avg_loss:0.976, val_acc:0.821]
Epoch [20/120    avg_loss:0.915, val_acc:0.854]
Epoch [21/120    avg_loss:0.904, val_acc:0.831]
Epoch [22/120    avg_loss:0.825, val_acc:0.804]
Epoch [23/120    avg_loss:0.760, val_acc:0.819]
Epoch [24/120    avg_loss:0.690, val_acc:0.881]
Epoch [25/120    avg_loss:0.708, val_acc:0.860]
Epoch [26/120    avg_loss:0.624, val_acc:0.869]
Epoch [27/120    avg_loss:0.591, val_acc:0.896]
Epoch [28/120    avg_loss:0.603, val_acc:0.906]
Epoch [29/120    avg_loss:0.521, val_acc:0.875]
Epoch [30/120    avg_loss:0.507, val_acc:0.929]
Epoch [31/120    avg_loss:0.520, val_acc:0.927]
Epoch [32/120    avg_loss:0.469, val_acc:0.919]
Epoch [33/120    avg_loss:0.470, val_acc:0.925]
Epoch [34/120    avg_loss:0.431, val_acc:0.906]
Epoch [35/120    avg_loss:0.453, val_acc:0.877]
Epoch [36/120    avg_loss:0.485, val_acc:0.900]
Epoch [37/120    avg_loss:0.464, val_acc:0.917]
Epoch [38/120    avg_loss:0.437, val_acc:0.910]
Epoch [39/120    avg_loss:0.384, val_acc:0.925]
Epoch [40/120    avg_loss:0.434, val_acc:0.935]
Epoch [41/120    avg_loss:0.473, val_acc:0.938]
Epoch [42/120    avg_loss:0.437, val_acc:0.900]
Epoch [43/120    avg_loss:0.319, val_acc:0.948]
Epoch [44/120    avg_loss:0.373, val_acc:0.929]
Epoch [45/120    avg_loss:0.353, val_acc:0.931]
Epoch [46/120    avg_loss:0.337, val_acc:0.956]
Epoch [47/120    avg_loss:0.354, val_acc:0.950]
Epoch [48/120    avg_loss:0.312, val_acc:0.925]
Epoch [49/120    avg_loss:0.297, val_acc:0.942]
Epoch [50/120    avg_loss:0.248, val_acc:0.963]
Epoch [51/120    avg_loss:0.243, val_acc:0.950]
Epoch [52/120    avg_loss:0.244, val_acc:0.944]
Epoch [53/120    avg_loss:0.267, val_acc:0.950]
Epoch [54/120    avg_loss:0.236, val_acc:0.948]
Epoch [55/120    avg_loss:0.253, val_acc:0.935]
Epoch [56/120    avg_loss:0.248, val_acc:0.954]
Epoch [57/120    avg_loss:0.231, val_acc:0.956]
Epoch [58/120    avg_loss:0.238, val_acc:0.933]
Epoch [59/120    avg_loss:0.196, val_acc:0.963]
Epoch [60/120    avg_loss:0.182, val_acc:0.965]
Epoch [61/120    avg_loss:0.222, val_acc:0.935]
Epoch [62/120    avg_loss:0.187, val_acc:0.965]
Epoch [63/120    avg_loss:0.203, val_acc:0.963]
Epoch [64/120    avg_loss:0.168, val_acc:0.969]
Epoch [65/120    avg_loss:0.150, val_acc:0.975]
Epoch [66/120    avg_loss:0.152, val_acc:0.969]
Epoch [67/120    avg_loss:0.166, val_acc:0.967]
Epoch [68/120    avg_loss:0.182, val_acc:0.950]
Epoch [69/120    avg_loss:0.226, val_acc:0.929]
Epoch [70/120    avg_loss:0.200, val_acc:0.958]
Epoch [71/120    avg_loss:0.216, val_acc:0.950]
Epoch [72/120    avg_loss:0.240, val_acc:0.931]
Epoch [73/120    avg_loss:0.172, val_acc:0.963]
Epoch [74/120    avg_loss:0.244, val_acc:0.946]
Epoch [75/120    avg_loss:0.203, val_acc:0.963]
Epoch [76/120    avg_loss:0.165, val_acc:0.971]
Epoch [77/120    avg_loss:0.184, val_acc:0.977]
Epoch [78/120    avg_loss:0.150, val_acc:0.967]
Epoch [79/120    avg_loss:0.114, val_acc:0.977]
Epoch [80/120    avg_loss:0.106, val_acc:0.973]
Epoch [81/120    avg_loss:0.112, val_acc:0.973]
Epoch [82/120    avg_loss:0.118, val_acc:0.969]
Epoch [83/120    avg_loss:0.113, val_acc:0.946]
Epoch [84/120    avg_loss:0.097, val_acc:0.981]
Epoch [85/120    avg_loss:0.076, val_acc:0.973]
Epoch [86/120    avg_loss:0.114, val_acc:0.963]
Epoch [87/120    avg_loss:0.127, val_acc:0.971]
Epoch [88/120    avg_loss:0.160, val_acc:0.969]
Epoch [89/120    avg_loss:0.139, val_acc:0.977]
Epoch [90/120    avg_loss:0.096, val_acc:0.979]
Epoch [91/120    avg_loss:0.079, val_acc:0.990]
Epoch [92/120    avg_loss:0.065, val_acc:0.981]
Epoch [93/120    avg_loss:0.065, val_acc:0.983]
Epoch [94/120    avg_loss:0.056, val_acc:0.977]
Epoch [95/120    avg_loss:0.089, val_acc:0.979]
Epoch [96/120    avg_loss:0.065, val_acc:0.985]
Epoch [97/120    avg_loss:0.068, val_acc:0.979]
Epoch [98/120    avg_loss:0.072, val_acc:0.979]
Epoch [99/120    avg_loss:0.098, val_acc:0.954]
Epoch [100/120    avg_loss:0.194, val_acc:0.958]
Epoch [101/120    avg_loss:0.301, val_acc:0.908]
Epoch [102/120    avg_loss:0.208, val_acc:0.965]
Epoch [103/120    avg_loss:0.151, val_acc:0.979]
Epoch [104/120    avg_loss:0.100, val_acc:0.979]
Epoch [105/120    avg_loss:0.067, val_acc:0.979]
Epoch [106/120    avg_loss:0.068, val_acc:0.985]
Epoch [107/120    avg_loss:0.073, val_acc:0.988]
Epoch [108/120    avg_loss:0.063, val_acc:0.988]
Epoch [109/120    avg_loss:0.056, val_acc:0.988]
Epoch [110/120    avg_loss:0.062, val_acc:0.988]
Epoch [111/120    avg_loss:0.067, val_acc:0.988]
Epoch [112/120    avg_loss:0.066, val_acc:0.988]
Epoch [113/120    avg_loss:0.054, val_acc:0.988]
Epoch [114/120    avg_loss:0.069, val_acc:0.988]
Epoch [115/120    avg_loss:0.058, val_acc:0.988]
Epoch [116/120    avg_loss:0.056, val_acc:0.988]
Epoch [117/120    avg_loss:0.066, val_acc:0.985]
Epoch [118/120    avg_loss:0.063, val_acc:0.985]
Epoch [119/120    avg_loss:0.070, val_acc:0.985]
Epoch [120/120    avg_loss:0.060, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.99926954 0.97986577 0.99782135 0.91555556 0.8707483
 0.99757869 0.94972067 1.         0.99893276 1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9881306388955673
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7475435a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.140]
Epoch [2/120    avg_loss:2.541, val_acc:0.129]
Epoch [3/120    avg_loss:2.442, val_acc:0.306]
Epoch [4/120    avg_loss:2.366, val_acc:0.383]
Epoch [5/120    avg_loss:2.302, val_acc:0.423]
Epoch [6/120    avg_loss:2.221, val_acc:0.510]
Epoch [7/120    avg_loss:2.138, val_acc:0.537]
Epoch [8/120    avg_loss:2.047, val_acc:0.550]
Epoch [9/120    avg_loss:1.961, val_acc:0.577]
Epoch [10/120    avg_loss:1.864, val_acc:0.606]
Epoch [11/120    avg_loss:1.771, val_acc:0.646]
Epoch [12/120    avg_loss:1.661, val_acc:0.667]
Epoch [13/120    avg_loss:1.573, val_acc:0.665]
Epoch [14/120    avg_loss:1.438, val_acc:0.704]
Epoch [15/120    avg_loss:1.307, val_acc:0.754]
Epoch [16/120    avg_loss:1.202, val_acc:0.765]
Epoch [17/120    avg_loss:1.090, val_acc:0.810]
Epoch [18/120    avg_loss:1.000, val_acc:0.781]
Epoch [19/120    avg_loss:0.967, val_acc:0.765]
Epoch [20/120    avg_loss:0.944, val_acc:0.873]
Epoch [21/120    avg_loss:0.839, val_acc:0.873]
Epoch [22/120    avg_loss:0.739, val_acc:0.842]
Epoch [23/120    avg_loss:0.729, val_acc:0.852]
Epoch [24/120    avg_loss:0.698, val_acc:0.800]
Epoch [25/120    avg_loss:0.637, val_acc:0.904]
Epoch [26/120    avg_loss:0.545, val_acc:0.931]
Epoch [27/120    avg_loss:0.552, val_acc:0.915]
Epoch [28/120    avg_loss:0.572, val_acc:0.906]
Epoch [29/120    avg_loss:0.521, val_acc:0.852]
Epoch [30/120    avg_loss:0.495, val_acc:0.925]
Epoch [31/120    avg_loss:0.442, val_acc:0.923]
Epoch [32/120    avg_loss:0.460, val_acc:0.894]
Epoch [33/120    avg_loss:0.401, val_acc:0.921]
Epoch [34/120    avg_loss:0.394, val_acc:0.929]
Epoch [35/120    avg_loss:0.375, val_acc:0.938]
Epoch [36/120    avg_loss:0.324, val_acc:0.925]
Epoch [37/120    avg_loss:0.365, val_acc:0.929]
Epoch [38/120    avg_loss:0.375, val_acc:0.944]
Epoch [39/120    avg_loss:0.326, val_acc:0.944]
Epoch [40/120    avg_loss:0.303, val_acc:0.948]
Epoch [41/120    avg_loss:0.284, val_acc:0.956]
Epoch [42/120    avg_loss:0.300, val_acc:0.938]
Epoch [43/120    avg_loss:0.313, val_acc:0.944]
Epoch [44/120    avg_loss:0.318, val_acc:0.960]
Epoch [45/120    avg_loss:0.366, val_acc:0.915]
Epoch [46/120    avg_loss:0.303, val_acc:0.950]
Epoch [47/120    avg_loss:0.258, val_acc:0.958]
Epoch [48/120    avg_loss:0.329, val_acc:0.942]
Epoch [49/120    avg_loss:0.251, val_acc:0.963]
Epoch [50/120    avg_loss:0.192, val_acc:0.956]
Epoch [51/120    avg_loss:0.252, val_acc:0.963]
Epoch [52/120    avg_loss:0.202, val_acc:0.956]
Epoch [53/120    avg_loss:0.198, val_acc:0.963]
Epoch [54/120    avg_loss:0.184, val_acc:0.948]
Epoch [55/120    avg_loss:0.177, val_acc:0.954]
Epoch [56/120    avg_loss:0.175, val_acc:0.969]
Epoch [57/120    avg_loss:0.195, val_acc:0.969]
Epoch [58/120    avg_loss:0.196, val_acc:0.960]
Epoch [59/120    avg_loss:0.222, val_acc:0.948]
Epoch [60/120    avg_loss:0.183, val_acc:0.925]
Epoch [61/120    avg_loss:0.159, val_acc:0.969]
Epoch [62/120    avg_loss:0.152, val_acc:0.965]
Epoch [63/120    avg_loss:0.124, val_acc:0.967]
Epoch [64/120    avg_loss:0.142, val_acc:0.967]
Epoch [65/120    avg_loss:0.137, val_acc:0.971]
Epoch [66/120    avg_loss:0.118, val_acc:0.971]
Epoch [67/120    avg_loss:0.124, val_acc:0.981]
Epoch [68/120    avg_loss:0.100, val_acc:0.977]
Epoch [69/120    avg_loss:0.139, val_acc:0.965]
Epoch [70/120    avg_loss:0.141, val_acc:0.965]
Epoch [71/120    avg_loss:0.124, val_acc:0.975]
Epoch [72/120    avg_loss:0.128, val_acc:0.973]
Epoch [73/120    avg_loss:0.126, val_acc:0.967]
Epoch [74/120    avg_loss:0.086, val_acc:0.977]
Epoch [75/120    avg_loss:0.105, val_acc:0.960]
Epoch [76/120    avg_loss:0.089, val_acc:0.977]
Epoch [77/120    avg_loss:0.092, val_acc:0.977]
Epoch [78/120    avg_loss:0.082, val_acc:0.969]
Epoch [79/120    avg_loss:0.091, val_acc:0.981]
Epoch [80/120    avg_loss:0.067, val_acc:0.975]
Epoch [81/120    avg_loss:0.085, val_acc:0.979]
Epoch [82/120    avg_loss:0.078, val_acc:0.975]
Epoch [83/120    avg_loss:0.119, val_acc:0.981]
Epoch [84/120    avg_loss:0.095, val_acc:0.979]
Epoch [85/120    avg_loss:0.065, val_acc:0.979]
Epoch [86/120    avg_loss:0.055, val_acc:0.983]
Epoch [87/120    avg_loss:0.072, val_acc:0.977]
Epoch [88/120    avg_loss:0.090, val_acc:0.983]
Epoch [89/120    avg_loss:0.061, val_acc:0.981]
Epoch [90/120    avg_loss:0.058, val_acc:0.977]
Epoch [91/120    avg_loss:0.068, val_acc:0.979]
Epoch [92/120    avg_loss:0.067, val_acc:0.985]
Epoch [93/120    avg_loss:0.055, val_acc:0.979]
Epoch [94/120    avg_loss:0.072, val_acc:0.969]
Epoch [95/120    avg_loss:0.122, val_acc:0.969]
Epoch [96/120    avg_loss:0.084, val_acc:0.969]
Epoch [97/120    avg_loss:0.071, val_acc:0.985]
Epoch [98/120    avg_loss:0.046, val_acc:0.969]
Epoch [99/120    avg_loss:0.051, val_acc:0.981]
Epoch [100/120    avg_loss:0.037, val_acc:0.985]
Epoch [101/120    avg_loss:0.045, val_acc:0.979]
Epoch [102/120    avg_loss:0.045, val_acc:0.979]
Epoch [103/120    avg_loss:0.068, val_acc:0.977]
Epoch [104/120    avg_loss:0.074, val_acc:0.975]
Epoch [105/120    avg_loss:0.055, val_acc:0.981]
Epoch [106/120    avg_loss:0.047, val_acc:0.988]
Epoch [107/120    avg_loss:0.040, val_acc:0.977]
Epoch [108/120    avg_loss:0.038, val_acc:0.975]
Epoch [109/120    avg_loss:0.051, val_acc:0.948]
Epoch [110/120    avg_loss:0.057, val_acc:0.971]
Epoch [111/120    avg_loss:0.048, val_acc:0.979]
Epoch [112/120    avg_loss:0.042, val_acc:0.990]
Epoch [113/120    avg_loss:0.034, val_acc:0.985]
Epoch [114/120    avg_loss:0.022, val_acc:0.985]
Epoch [115/120    avg_loss:0.023, val_acc:0.985]
Epoch [116/120    avg_loss:0.043, val_acc:0.985]
Epoch [117/120    avg_loss:0.048, val_acc:0.988]
Epoch [118/120    avg_loss:0.036, val_acc:0.990]
Epoch [119/120    avg_loss:0.021, val_acc:0.988]
Epoch [120/120    avg_loss:0.017, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99707602 0.99095023 1.         0.96153846 0.93478261
 0.99277108 0.97826087 0.99870968 1.         1.         1.
 1.         1.        ]

Kappa:
0.993827923478399
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa7231fba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.225]
Epoch [2/120    avg_loss:2.508, val_acc:0.367]
Epoch [3/120    avg_loss:2.391, val_acc:0.419]
Epoch [4/120    avg_loss:2.296, val_acc:0.433]
Epoch [5/120    avg_loss:2.215, val_acc:0.460]
Epoch [6/120    avg_loss:2.111, val_acc:0.521]
Epoch [7/120    avg_loss:2.030, val_acc:0.575]
Epoch [8/120    avg_loss:1.931, val_acc:0.590]
Epoch [9/120    avg_loss:1.842, val_acc:0.642]
Epoch [10/120    avg_loss:1.724, val_acc:0.685]
Epoch [11/120    avg_loss:1.623, val_acc:0.692]
Epoch [12/120    avg_loss:1.520, val_acc:0.713]
Epoch [13/120    avg_loss:1.427, val_acc:0.735]
Epoch [14/120    avg_loss:1.348, val_acc:0.740]
Epoch [15/120    avg_loss:1.206, val_acc:0.748]
Epoch [16/120    avg_loss:1.107, val_acc:0.787]
Epoch [17/120    avg_loss:0.998, val_acc:0.875]
Epoch [18/120    avg_loss:0.902, val_acc:0.842]
Epoch [19/120    avg_loss:0.861, val_acc:0.879]
Epoch [20/120    avg_loss:0.825, val_acc:0.840]
Epoch [21/120    avg_loss:0.744, val_acc:0.898]
Epoch [22/120    avg_loss:0.674, val_acc:0.935]
Epoch [23/120    avg_loss:0.608, val_acc:0.910]
Epoch [24/120    avg_loss:0.582, val_acc:0.917]
Epoch [25/120    avg_loss:0.562, val_acc:0.850]
Epoch [26/120    avg_loss:0.590, val_acc:0.927]
Epoch [27/120    avg_loss:0.534, val_acc:0.923]
Epoch [28/120    avg_loss:0.459, val_acc:0.919]
Epoch [29/120    avg_loss:0.483, val_acc:0.923]
Epoch [30/120    avg_loss:0.473, val_acc:0.925]
Epoch [31/120    avg_loss:0.426, val_acc:0.952]
Epoch [32/120    avg_loss:0.404, val_acc:0.952]
Epoch [33/120    avg_loss:0.383, val_acc:0.925]
Epoch [34/120    avg_loss:0.438, val_acc:0.919]
Epoch [35/120    avg_loss:0.521, val_acc:0.938]
Epoch [36/120    avg_loss:0.381, val_acc:0.925]
Epoch [37/120    avg_loss:0.366, val_acc:0.940]
Epoch [38/120    avg_loss:0.316, val_acc:0.954]
Epoch [39/120    avg_loss:0.304, val_acc:0.942]
Epoch [40/120    avg_loss:0.291, val_acc:0.948]
Epoch [41/120    avg_loss:0.280, val_acc:0.946]
Epoch [42/120    avg_loss:0.326, val_acc:0.944]
Epoch [43/120    avg_loss:0.322, val_acc:0.940]
Epoch [44/120    avg_loss:0.324, val_acc:0.927]
Epoch [45/120    avg_loss:0.318, val_acc:0.954]
Epoch [46/120    avg_loss:0.293, val_acc:0.952]
Epoch [47/120    avg_loss:0.282, val_acc:0.965]
Epoch [48/120    avg_loss:0.271, val_acc:0.942]
Epoch [49/120    avg_loss:0.276, val_acc:0.969]
Epoch [50/120    avg_loss:0.269, val_acc:0.954]
Epoch [51/120    avg_loss:0.264, val_acc:0.969]
Epoch [52/120    avg_loss:0.199, val_acc:0.958]
Epoch [53/120    avg_loss:0.229, val_acc:0.931]
Epoch [54/120    avg_loss:0.281, val_acc:0.965]
Epoch [55/120    avg_loss:0.231, val_acc:0.975]
Epoch [56/120    avg_loss:0.208, val_acc:0.954]
Epoch [57/120    avg_loss:0.204, val_acc:0.925]
Epoch [58/120    avg_loss:0.179, val_acc:0.925]
Epoch [59/120    avg_loss:0.209, val_acc:0.967]
Epoch [60/120    avg_loss:0.210, val_acc:0.967]
Epoch [61/120    avg_loss:0.185, val_acc:0.971]
Epoch [62/120    avg_loss:0.197, val_acc:0.958]
Epoch [63/120    avg_loss:0.191, val_acc:0.977]
Epoch [64/120    avg_loss:0.157, val_acc:0.954]
Epoch [65/120    avg_loss:0.219, val_acc:0.931]
Epoch [66/120    avg_loss:0.194, val_acc:0.963]
Epoch [67/120    avg_loss:0.170, val_acc:0.969]
Epoch [68/120    avg_loss:0.217, val_acc:0.971]
Epoch [69/120    avg_loss:0.156, val_acc:0.979]
Epoch [70/120    avg_loss:0.159, val_acc:0.985]
Epoch [71/120    avg_loss:0.150, val_acc:0.977]
Epoch [72/120    avg_loss:0.179, val_acc:0.967]
Epoch [73/120    avg_loss:0.177, val_acc:0.975]
Epoch [74/120    avg_loss:0.151, val_acc:0.977]
Epoch [75/120    avg_loss:0.138, val_acc:0.971]
Epoch [76/120    avg_loss:0.115, val_acc:0.988]
Epoch [77/120    avg_loss:0.129, val_acc:0.983]
Epoch [78/120    avg_loss:0.121, val_acc:0.956]
Epoch [79/120    avg_loss:0.153, val_acc:0.971]
Epoch [80/120    avg_loss:0.158, val_acc:0.954]
Epoch [81/120    avg_loss:0.136, val_acc:0.969]
Epoch [82/120    avg_loss:0.111, val_acc:0.988]
Epoch [83/120    avg_loss:0.092, val_acc:0.988]
Epoch [84/120    avg_loss:0.102, val_acc:0.975]
Epoch [85/120    avg_loss:0.089, val_acc:0.992]
Epoch [86/120    avg_loss:0.075, val_acc:0.990]
Epoch [87/120    avg_loss:0.078, val_acc:0.967]
Epoch [88/120    avg_loss:0.100, val_acc:0.992]
Epoch [89/120    avg_loss:0.082, val_acc:0.981]
Epoch [90/120    avg_loss:0.068, val_acc:0.988]
Epoch [91/120    avg_loss:0.082, val_acc:0.985]
Epoch [92/120    avg_loss:0.067, val_acc:0.992]
Epoch [93/120    avg_loss:0.073, val_acc:0.983]
Epoch [94/120    avg_loss:0.082, val_acc:0.975]
Epoch [95/120    avg_loss:0.087, val_acc:0.981]
Epoch [96/120    avg_loss:0.065, val_acc:0.992]
Epoch [97/120    avg_loss:0.074, val_acc:0.988]
Epoch [98/120    avg_loss:0.064, val_acc:0.990]
Epoch [99/120    avg_loss:0.055, val_acc:0.990]
Epoch [100/120    avg_loss:0.055, val_acc:0.983]
Epoch [101/120    avg_loss:0.047, val_acc:0.988]
Epoch [102/120    avg_loss:0.056, val_acc:0.990]
Epoch [103/120    avg_loss:0.086, val_acc:0.992]
Epoch [104/120    avg_loss:0.151, val_acc:0.965]
Epoch [105/120    avg_loss:0.115, val_acc:0.981]
Epoch [106/120    avg_loss:0.080, val_acc:0.965]
Epoch [107/120    avg_loss:0.114, val_acc:0.983]
Epoch [108/120    avg_loss:0.091, val_acc:0.990]
Epoch [109/120    avg_loss:0.056, val_acc:0.990]
Epoch [110/120    avg_loss:0.060, val_acc:0.992]
Epoch [111/120    avg_loss:0.052, val_acc:0.979]
Epoch [112/120    avg_loss:0.059, val_acc:0.996]
Epoch [113/120    avg_loss:0.057, val_acc:0.988]
Epoch [114/120    avg_loss:0.062, val_acc:0.985]
Epoch [115/120    avg_loss:0.045, val_acc:0.985]
Epoch [116/120    avg_loss:0.051, val_acc:0.990]
Epoch [117/120    avg_loss:0.028, val_acc:0.992]
Epoch [118/120    avg_loss:0.026, val_acc:0.990]
Epoch [119/120    avg_loss:0.045, val_acc:0.983]
Epoch [120/120    avg_loss:0.088, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.99319728 0.99563319 0.94553377 0.9122807
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9933531466791776
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6843863a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.219]
Epoch [2/120    avg_loss:2.509, val_acc:0.298]
Epoch [3/120    avg_loss:2.417, val_acc:0.367]
Epoch [4/120    avg_loss:2.328, val_acc:0.406]
Epoch [5/120    avg_loss:2.247, val_acc:0.456]
Epoch [6/120    avg_loss:2.177, val_acc:0.481]
Epoch [7/120    avg_loss:2.089, val_acc:0.494]
Epoch [8/120    avg_loss:1.996, val_acc:0.496]
Epoch [9/120    avg_loss:1.910, val_acc:0.515]
Epoch [10/120    avg_loss:1.822, val_acc:0.594]
Epoch [11/120    avg_loss:1.753, val_acc:0.615]
Epoch [12/120    avg_loss:1.667, val_acc:0.633]
Epoch [13/120    avg_loss:1.549, val_acc:0.644]
Epoch [14/120    avg_loss:1.451, val_acc:0.648]
Epoch [15/120    avg_loss:1.343, val_acc:0.662]
Epoch [16/120    avg_loss:1.266, val_acc:0.721]
Epoch [17/120    avg_loss:1.151, val_acc:0.758]
Epoch [18/120    avg_loss:1.033, val_acc:0.777]
Epoch [19/120    avg_loss:0.942, val_acc:0.787]
Epoch [20/120    avg_loss:0.874, val_acc:0.812]
Epoch [21/120    avg_loss:0.785, val_acc:0.823]
Epoch [22/120    avg_loss:0.735, val_acc:0.860]
Epoch [23/120    avg_loss:0.674, val_acc:0.894]
Epoch [24/120    avg_loss:0.647, val_acc:0.863]
Epoch [25/120    avg_loss:0.689, val_acc:0.908]
Epoch [26/120    avg_loss:0.672, val_acc:0.902]
Epoch [27/120    avg_loss:0.603, val_acc:0.925]
Epoch [28/120    avg_loss:0.518, val_acc:0.925]
Epoch [29/120    avg_loss:0.483, val_acc:0.906]
Epoch [30/120    avg_loss:0.479, val_acc:0.927]
Epoch [31/120    avg_loss:0.418, val_acc:0.923]
Epoch [32/120    avg_loss:0.442, val_acc:0.935]
Epoch [33/120    avg_loss:0.455, val_acc:0.900]
Epoch [34/120    avg_loss:0.420, val_acc:0.923]
Epoch [35/120    avg_loss:0.419, val_acc:0.946]
Epoch [36/120    avg_loss:0.381, val_acc:0.942]
Epoch [37/120    avg_loss:0.420, val_acc:0.948]
Epoch [38/120    avg_loss:0.381, val_acc:0.952]
Epoch [39/120    avg_loss:0.312, val_acc:0.958]
Epoch [40/120    avg_loss:0.296, val_acc:0.954]
Epoch [41/120    avg_loss:0.301, val_acc:0.933]
Epoch [42/120    avg_loss:0.313, val_acc:0.963]
Epoch [43/120    avg_loss:0.344, val_acc:0.956]
Epoch [44/120    avg_loss:0.274, val_acc:0.958]
Epoch [45/120    avg_loss:0.356, val_acc:0.948]
Epoch [46/120    avg_loss:0.343, val_acc:0.956]
Epoch [47/120    avg_loss:0.326, val_acc:0.948]
Epoch [48/120    avg_loss:0.291, val_acc:0.969]
Epoch [49/120    avg_loss:0.317, val_acc:0.938]
Epoch [50/120    avg_loss:0.252, val_acc:0.952]
Epoch [51/120    avg_loss:0.237, val_acc:0.954]
Epoch [52/120    avg_loss:0.267, val_acc:0.958]
Epoch [53/120    avg_loss:0.221, val_acc:0.965]
Epoch [54/120    avg_loss:0.204, val_acc:0.965]
Epoch [55/120    avg_loss:0.191, val_acc:0.971]
Epoch [56/120    avg_loss:0.230, val_acc:0.948]
Epoch [57/120    avg_loss:0.253, val_acc:0.950]
Epoch [58/120    avg_loss:0.253, val_acc:0.946]
Epoch [59/120    avg_loss:0.199, val_acc:0.965]
Epoch [60/120    avg_loss:0.210, val_acc:0.931]
Epoch [61/120    avg_loss:0.187, val_acc:0.969]
Epoch [62/120    avg_loss:0.157, val_acc:0.967]
Epoch [63/120    avg_loss:0.187, val_acc:0.979]
Epoch [64/120    avg_loss:0.219, val_acc:0.975]
Epoch [65/120    avg_loss:0.202, val_acc:0.950]
Epoch [66/120    avg_loss:0.147, val_acc:0.973]
Epoch [67/120    avg_loss:0.187, val_acc:0.963]
Epoch [68/120    avg_loss:0.184, val_acc:0.971]
Epoch [69/120    avg_loss:0.205, val_acc:0.933]
Epoch [70/120    avg_loss:0.185, val_acc:0.963]
Epoch [71/120    avg_loss:0.180, val_acc:0.956]
Epoch [72/120    avg_loss:0.185, val_acc:0.977]
Epoch [73/120    avg_loss:0.149, val_acc:0.946]
Epoch [74/120    avg_loss:0.158, val_acc:0.983]
Epoch [75/120    avg_loss:0.151, val_acc:0.990]
Epoch [76/120    avg_loss:0.224, val_acc:0.977]
Epoch [77/120    avg_loss:0.131, val_acc:0.977]
Epoch [78/120    avg_loss:0.116, val_acc:0.985]
Epoch [79/120    avg_loss:0.101, val_acc:0.985]
Epoch [80/120    avg_loss:0.114, val_acc:0.971]
Epoch [81/120    avg_loss:0.151, val_acc:0.975]
Epoch [82/120    avg_loss:0.102, val_acc:0.983]
Epoch [83/120    avg_loss:0.112, val_acc:0.979]
Epoch [84/120    avg_loss:0.131, val_acc:0.994]
Epoch [85/120    avg_loss:0.076, val_acc:0.979]
Epoch [86/120    avg_loss:0.135, val_acc:0.969]
Epoch [87/120    avg_loss:0.085, val_acc:0.971]
Epoch [88/120    avg_loss:0.120, val_acc:0.967]
Epoch [89/120    avg_loss:0.115, val_acc:0.965]
Epoch [90/120    avg_loss:0.103, val_acc:0.992]
Epoch [91/120    avg_loss:0.106, val_acc:0.985]
Epoch [92/120    avg_loss:0.100, val_acc:0.973]
Epoch [93/120    avg_loss:0.092, val_acc:0.985]
Epoch [94/120    avg_loss:0.075, val_acc:0.990]
Epoch [95/120    avg_loss:0.081, val_acc:0.990]
Epoch [96/120    avg_loss:0.080, val_acc:0.988]
Epoch [97/120    avg_loss:0.059, val_acc:0.988]
Epoch [98/120    avg_loss:0.057, val_acc:0.990]
Epoch [99/120    avg_loss:0.046, val_acc:0.990]
Epoch [100/120    avg_loss:0.040, val_acc:0.990]
Epoch [101/120    avg_loss:0.051, val_acc:0.992]
Epoch [102/120    avg_loss:0.040, val_acc:0.992]
Epoch [103/120    avg_loss:0.047, val_acc:0.990]
Epoch [104/120    avg_loss:0.043, val_acc:0.994]
Epoch [105/120    avg_loss:0.036, val_acc:0.992]
Epoch [106/120    avg_loss:0.031, val_acc:0.992]
Epoch [107/120    avg_loss:0.037, val_acc:0.992]
Epoch [108/120    avg_loss:0.041, val_acc:0.992]
Epoch [109/120    avg_loss:0.032, val_acc:0.990]
Epoch [110/120    avg_loss:0.037, val_acc:0.994]
Epoch [111/120    avg_loss:0.034, val_acc:0.992]
Epoch [112/120    avg_loss:0.043, val_acc:0.992]
Epoch [113/120    avg_loss:0.039, val_acc:0.992]
Epoch [114/120    avg_loss:0.038, val_acc:0.990]
Epoch [115/120    avg_loss:0.032, val_acc:0.990]
Epoch [116/120    avg_loss:0.038, val_acc:0.990]
Epoch [117/120    avg_loss:0.038, val_acc:0.990]
Epoch [118/120    avg_loss:0.040, val_acc:0.992]
Epoch [119/120    avg_loss:0.046, val_acc:0.992]
Epoch [120/120    avg_loss:0.031, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.96902655 0.99122807 0.93932584 0.92409241
 1.         0.91954023 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9902670293074329
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f27b8e0ea90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.615, val_acc:0.223]
Epoch [2/120    avg_loss:2.523, val_acc:0.360]
Epoch [3/120    avg_loss:2.422, val_acc:0.362]
Epoch [4/120    avg_loss:2.344, val_acc:0.400]
Epoch [5/120    avg_loss:2.269, val_acc:0.467]
Epoch [6/120    avg_loss:2.192, val_acc:0.502]
Epoch [7/120    avg_loss:2.108, val_acc:0.504]
Epoch [8/120    avg_loss:2.028, val_acc:0.556]
Epoch [9/120    avg_loss:1.942, val_acc:0.625]
Epoch [10/120    avg_loss:1.865, val_acc:0.644]
Epoch [11/120    avg_loss:1.779, val_acc:0.631]
Epoch [12/120    avg_loss:1.707, val_acc:0.690]
Epoch [13/120    avg_loss:1.612, val_acc:0.681]
Epoch [14/120    avg_loss:1.518, val_acc:0.683]
Epoch [15/120    avg_loss:1.390, val_acc:0.706]
Epoch [16/120    avg_loss:1.307, val_acc:0.735]
Epoch [17/120    avg_loss:1.255, val_acc:0.735]
Epoch [18/120    avg_loss:1.131, val_acc:0.748]
Epoch [19/120    avg_loss:1.070, val_acc:0.833]
Epoch [20/120    avg_loss:1.026, val_acc:0.856]
Epoch [21/120    avg_loss:0.933, val_acc:0.881]
Epoch [22/120    avg_loss:0.810, val_acc:0.921]
Epoch [23/120    avg_loss:0.782, val_acc:0.912]
Epoch [24/120    avg_loss:0.722, val_acc:0.910]
Epoch [25/120    avg_loss:0.668, val_acc:0.827]
Epoch [26/120    avg_loss:0.637, val_acc:0.785]
Epoch [27/120    avg_loss:0.618, val_acc:0.938]
Epoch [28/120    avg_loss:0.566, val_acc:0.892]
Epoch [29/120    avg_loss:0.530, val_acc:0.923]
Epoch [30/120    avg_loss:0.465, val_acc:0.946]
Epoch [31/120    avg_loss:0.465, val_acc:0.946]
Epoch [32/120    avg_loss:0.476, val_acc:0.944]
Epoch [33/120    avg_loss:0.492, val_acc:0.944]
Epoch [34/120    avg_loss:0.447, val_acc:0.963]
Epoch [35/120    avg_loss:0.392, val_acc:0.933]
Epoch [36/120    avg_loss:0.462, val_acc:0.919]
Epoch [37/120    avg_loss:0.376, val_acc:0.952]
Epoch [38/120    avg_loss:0.351, val_acc:0.954]
Epoch [39/120    avg_loss:0.480, val_acc:0.948]
Epoch [40/120    avg_loss:0.403, val_acc:0.933]
Epoch [41/120    avg_loss:0.314, val_acc:0.952]
Epoch [42/120    avg_loss:0.349, val_acc:0.965]
Epoch [43/120    avg_loss:0.314, val_acc:0.946]
Epoch [44/120    avg_loss:0.332, val_acc:0.952]
Epoch [45/120    avg_loss:0.324, val_acc:0.954]
Epoch [46/120    avg_loss:0.351, val_acc:0.875]
Epoch [47/120    avg_loss:0.318, val_acc:0.956]
Epoch [48/120    avg_loss:0.328, val_acc:0.935]
Epoch [49/120    avg_loss:0.251, val_acc:0.975]
Epoch [50/120    avg_loss:0.253, val_acc:0.967]
Epoch [51/120    avg_loss:0.251, val_acc:0.929]
Epoch [52/120    avg_loss:0.227, val_acc:0.952]
Epoch [53/120    avg_loss:0.260, val_acc:0.956]
Epoch [54/120    avg_loss:0.240, val_acc:0.963]
Epoch [55/120    avg_loss:0.202, val_acc:0.956]
Epoch [56/120    avg_loss:0.197, val_acc:0.965]
Epoch [57/120    avg_loss:0.205, val_acc:0.969]
Epoch [58/120    avg_loss:0.197, val_acc:0.969]
Epoch [59/120    avg_loss:0.197, val_acc:0.983]
Epoch [60/120    avg_loss:0.211, val_acc:0.960]
Epoch [61/120    avg_loss:0.186, val_acc:0.969]
Epoch [62/120    avg_loss:0.194, val_acc:0.963]
Epoch [63/120    avg_loss:0.184, val_acc:0.940]
Epoch [64/120    avg_loss:0.192, val_acc:0.973]
Epoch [65/120    avg_loss:0.223, val_acc:0.965]
Epoch [66/120    avg_loss:0.241, val_acc:0.969]
Epoch [67/120    avg_loss:0.190, val_acc:0.952]
Epoch [68/120    avg_loss:0.188, val_acc:0.969]
Epoch [69/120    avg_loss:0.155, val_acc:0.981]
Epoch [70/120    avg_loss:0.123, val_acc:0.983]
Epoch [71/120    avg_loss:0.141, val_acc:0.971]
Epoch [72/120    avg_loss:0.169, val_acc:0.965]
Epoch [73/120    avg_loss:0.163, val_acc:0.983]
Epoch [74/120    avg_loss:0.162, val_acc:0.975]
Epoch [75/120    avg_loss:0.122, val_acc:0.979]
Epoch [76/120    avg_loss:0.114, val_acc:0.983]
Epoch [77/120    avg_loss:0.110, val_acc:0.983]
Epoch [78/120    avg_loss:0.114, val_acc:0.979]
Epoch [79/120    avg_loss:0.143, val_acc:0.981]
Epoch [80/120    avg_loss:0.106, val_acc:0.979]
Epoch [81/120    avg_loss:0.102, val_acc:0.965]
Epoch [82/120    avg_loss:0.131, val_acc:0.965]
Epoch [83/120    avg_loss:0.128, val_acc:0.965]
Epoch [84/120    avg_loss:0.128, val_acc:0.981]
Epoch [85/120    avg_loss:0.082, val_acc:0.988]
Epoch [86/120    avg_loss:0.082, val_acc:0.981]
Epoch [87/120    avg_loss:0.083, val_acc:0.985]
Epoch [88/120    avg_loss:0.093, val_acc:0.969]
Epoch [89/120    avg_loss:0.093, val_acc:0.990]
Epoch [90/120    avg_loss:0.079, val_acc:0.988]
Epoch [91/120    avg_loss:0.094, val_acc:0.985]
Epoch [92/120    avg_loss:0.067, val_acc:0.981]
Epoch [93/120    avg_loss:0.064, val_acc:0.981]
Epoch [94/120    avg_loss:0.072, val_acc:0.977]
Epoch [95/120    avg_loss:0.069, val_acc:0.992]
Epoch [96/120    avg_loss:0.060, val_acc:0.977]
Epoch [97/120    avg_loss:0.075, val_acc:0.990]
Epoch [98/120    avg_loss:0.072, val_acc:0.988]
Epoch [99/120    avg_loss:0.055, val_acc:0.979]
Epoch [100/120    avg_loss:0.056, val_acc:0.992]
Epoch [101/120    avg_loss:0.050, val_acc:0.992]
Epoch [102/120    avg_loss:0.070, val_acc:0.965]
Epoch [103/120    avg_loss:0.111, val_acc:0.981]
Epoch [104/120    avg_loss:0.091, val_acc:0.994]
Epoch [105/120    avg_loss:0.058, val_acc:0.990]
Epoch [106/120    avg_loss:0.047, val_acc:0.992]
Epoch [107/120    avg_loss:0.040, val_acc:0.992]
Epoch [108/120    avg_loss:0.035, val_acc:0.990]
Epoch [109/120    avg_loss:0.049, val_acc:0.996]
Epoch [110/120    avg_loss:0.028, val_acc:0.992]
Epoch [111/120    avg_loss:0.033, val_acc:0.990]
Epoch [112/120    avg_loss:0.045, val_acc:0.996]
Epoch [113/120    avg_loss:0.048, val_acc:0.992]
Epoch [114/120    avg_loss:0.040, val_acc:0.996]
Epoch [115/120    avg_loss:0.026, val_acc:0.994]
Epoch [116/120    avg_loss:0.026, val_acc:0.994]
Epoch [117/120    avg_loss:0.031, val_acc:0.990]
Epoch [118/120    avg_loss:0.047, val_acc:0.990]
Epoch [119/120    avg_loss:0.036, val_acc:0.988]
Epoch [120/120    avg_loss:0.046, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   2   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.99545455 0.99122807 0.95495495 0.94039735
 1.         0.98924731 1.         0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.9943029212105542
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ebd0f2b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.587, val_acc:0.335]
Epoch [2/120    avg_loss:2.461, val_acc:0.298]
Epoch [3/120    avg_loss:2.357, val_acc:0.310]
Epoch [4/120    avg_loss:2.262, val_acc:0.333]
Epoch [5/120    avg_loss:2.171, val_acc:0.398]
Epoch [6/120    avg_loss:2.074, val_acc:0.477]
Epoch [7/120    avg_loss:2.000, val_acc:0.525]
Epoch [8/120    avg_loss:1.906, val_acc:0.560]
Epoch [9/120    avg_loss:1.810, val_acc:0.571]
Epoch [10/120    avg_loss:1.748, val_acc:0.610]
Epoch [11/120    avg_loss:1.650, val_acc:0.604]
Epoch [12/120    avg_loss:1.564, val_acc:0.648]
Epoch [13/120    avg_loss:1.476, val_acc:0.690]
Epoch [14/120    avg_loss:1.419, val_acc:0.683]
Epoch [15/120    avg_loss:1.315, val_acc:0.723]
Epoch [16/120    avg_loss:1.186, val_acc:0.733]
Epoch [17/120    avg_loss:1.088, val_acc:0.762]
Epoch [18/120    avg_loss:1.076, val_acc:0.775]
Epoch [19/120    avg_loss:0.950, val_acc:0.794]
Epoch [20/120    avg_loss:0.948, val_acc:0.765]
Epoch [21/120    avg_loss:0.901, val_acc:0.777]
Epoch [22/120    avg_loss:0.831, val_acc:0.823]
Epoch [23/120    avg_loss:0.760, val_acc:0.898]
Epoch [24/120    avg_loss:0.714, val_acc:0.898]
Epoch [25/120    avg_loss:0.633, val_acc:0.877]
Epoch [26/120    avg_loss:0.630, val_acc:0.898]
Epoch [27/120    avg_loss:0.570, val_acc:0.892]
Epoch [28/120    avg_loss:0.532, val_acc:0.883]
Epoch [29/120    avg_loss:0.499, val_acc:0.931]
Epoch [30/120    avg_loss:0.490, val_acc:0.887]
Epoch [31/120    avg_loss:0.506, val_acc:0.917]
Epoch [32/120    avg_loss:0.511, val_acc:0.892]
Epoch [33/120    avg_loss:0.527, val_acc:0.906]
Epoch [34/120    avg_loss:0.530, val_acc:0.912]
Epoch [35/120    avg_loss:0.425, val_acc:0.906]
Epoch [36/120    avg_loss:0.437, val_acc:0.896]
Epoch [37/120    avg_loss:0.400, val_acc:0.921]
Epoch [38/120    avg_loss:0.393, val_acc:0.923]
Epoch [39/120    avg_loss:0.391, val_acc:0.923]
Epoch [40/120    avg_loss:0.397, val_acc:0.921]
Epoch [41/120    avg_loss:0.351, val_acc:0.869]
Epoch [42/120    avg_loss:0.386, val_acc:0.940]
Epoch [43/120    avg_loss:0.358, val_acc:0.927]
Epoch [44/120    avg_loss:0.311, val_acc:0.917]
Epoch [45/120    avg_loss:0.299, val_acc:0.942]
Epoch [46/120    avg_loss:0.314, val_acc:0.931]
Epoch [47/120    avg_loss:0.283, val_acc:0.935]
Epoch [48/120    avg_loss:0.253, val_acc:0.910]
Epoch [49/120    avg_loss:0.274, val_acc:0.931]
Epoch [50/120    avg_loss:0.281, val_acc:0.925]
Epoch [51/120    avg_loss:0.218, val_acc:0.944]
Epoch [52/120    avg_loss:0.230, val_acc:0.942]
Epoch [53/120    avg_loss:0.243, val_acc:0.948]
Epoch [54/120    avg_loss:0.224, val_acc:0.952]
Epoch [55/120    avg_loss:0.217, val_acc:0.921]
Epoch [56/120    avg_loss:0.324, val_acc:0.931]
Epoch [57/120    avg_loss:0.252, val_acc:0.917]
Epoch [58/120    avg_loss:0.251, val_acc:0.954]
Epoch [59/120    avg_loss:0.211, val_acc:0.954]
Epoch [60/120    avg_loss:0.217, val_acc:0.952]
Epoch [61/120    avg_loss:0.253, val_acc:0.929]
Epoch [62/120    avg_loss:0.311, val_acc:0.933]
Epoch [63/120    avg_loss:0.243, val_acc:0.948]
Epoch [64/120    avg_loss:0.211, val_acc:0.960]
Epoch [65/120    avg_loss:0.206, val_acc:0.925]
Epoch [66/120    avg_loss:0.189, val_acc:0.954]
Epoch [67/120    avg_loss:0.207, val_acc:0.965]
Epoch [68/120    avg_loss:0.172, val_acc:0.967]
Epoch [69/120    avg_loss:0.152, val_acc:0.975]
Epoch [70/120    avg_loss:0.157, val_acc:0.967]
Epoch [71/120    avg_loss:0.126, val_acc:0.958]
Epoch [72/120    avg_loss:0.138, val_acc:0.954]
Epoch [73/120    avg_loss:0.185, val_acc:0.948]
Epoch [74/120    avg_loss:0.152, val_acc:0.958]
Epoch [75/120    avg_loss:0.131, val_acc:0.960]
Epoch [76/120    avg_loss:0.145, val_acc:0.965]
Epoch [77/120    avg_loss:0.147, val_acc:0.944]
Epoch [78/120    avg_loss:0.148, val_acc:0.960]
Epoch [79/120    avg_loss:0.136, val_acc:0.973]
Epoch [80/120    avg_loss:0.171, val_acc:0.950]
Epoch [81/120    avg_loss:0.158, val_acc:0.969]
Epoch [82/120    avg_loss:0.209, val_acc:0.969]
Epoch [83/120    avg_loss:0.141, val_acc:0.971]
Epoch [84/120    avg_loss:0.112, val_acc:0.977]
Epoch [85/120    avg_loss:0.118, val_acc:0.975]
Epoch [86/120    avg_loss:0.104, val_acc:0.981]
Epoch [87/120    avg_loss:0.084, val_acc:0.979]
Epoch [88/120    avg_loss:0.097, val_acc:0.979]
Epoch [89/120    avg_loss:0.095, val_acc:0.981]
Epoch [90/120    avg_loss:0.090, val_acc:0.981]
Epoch [91/120    avg_loss:0.097, val_acc:0.977]
Epoch [92/120    avg_loss:0.086, val_acc:0.981]
Epoch [93/120    avg_loss:0.088, val_acc:0.979]
Epoch [94/120    avg_loss:0.092, val_acc:0.981]
Epoch [95/120    avg_loss:0.096, val_acc:0.981]
Epoch [96/120    avg_loss:0.069, val_acc:0.981]
Epoch [97/120    avg_loss:0.077, val_acc:0.979]
Epoch [98/120    avg_loss:0.077, val_acc:0.979]
Epoch [99/120    avg_loss:0.077, val_acc:0.979]
Epoch [100/120    avg_loss:0.084, val_acc:0.979]
Epoch [101/120    avg_loss:0.074, val_acc:0.981]
Epoch [102/120    avg_loss:0.087, val_acc:0.981]
Epoch [103/120    avg_loss:0.079, val_acc:0.983]
Epoch [104/120    avg_loss:0.066, val_acc:0.981]
Epoch [105/120    avg_loss:0.084, val_acc:0.985]
Epoch [106/120    avg_loss:0.077, val_acc:0.981]
Epoch [107/120    avg_loss:0.070, val_acc:0.979]
Epoch [108/120    avg_loss:0.089, val_acc:0.979]
Epoch [109/120    avg_loss:0.074, val_acc:0.979]
Epoch [110/120    avg_loss:0.078, val_acc:0.983]
Epoch [111/120    avg_loss:0.076, val_acc:0.981]
Epoch [112/120    avg_loss:0.073, val_acc:0.981]
Epoch [113/120    avg_loss:0.071, val_acc:0.985]
Epoch [114/120    avg_loss:0.082, val_acc:0.983]
Epoch [115/120    avg_loss:0.072, val_acc:0.981]
Epoch [116/120    avg_loss:0.069, val_acc:0.981]
Epoch [117/120    avg_loss:0.070, val_acc:0.981]
Epoch [118/120    avg_loss:0.065, val_acc:0.983]
Epoch [119/120    avg_loss:0.076, val_acc:0.983]
Epoch [120/120    avg_loss:0.063, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 198  29   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99926954 0.99095023 0.99782135 0.92740047 0.9022082
 0.99757869 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9914552748874608
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9ae970b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.130]
Epoch [2/120    avg_loss:2.518, val_acc:0.450]
Epoch [3/120    avg_loss:2.405, val_acc:0.471]
Epoch [4/120    avg_loss:2.317, val_acc:0.531]
Epoch [5/120    avg_loss:2.224, val_acc:0.546]
Epoch [6/120    avg_loss:2.132, val_acc:0.537]
Epoch [7/120    avg_loss:2.045, val_acc:0.544]
Epoch [8/120    avg_loss:1.951, val_acc:0.573]
Epoch [9/120    avg_loss:1.887, val_acc:0.571]
Epoch [10/120    avg_loss:1.793, val_acc:0.617]
Epoch [11/120    avg_loss:1.710, val_acc:0.631]
Epoch [12/120    avg_loss:1.600, val_acc:0.648]
Epoch [13/120    avg_loss:1.499, val_acc:0.656]
Epoch [14/120    avg_loss:1.400, val_acc:0.704]
Epoch [15/120    avg_loss:1.303, val_acc:0.717]
Epoch [16/120    avg_loss:1.202, val_acc:0.715]
Epoch [17/120    avg_loss:1.124, val_acc:0.733]
Epoch [18/120    avg_loss:1.027, val_acc:0.750]
Epoch [19/120    avg_loss:0.979, val_acc:0.808]
Epoch [20/120    avg_loss:0.917, val_acc:0.871]
Epoch [21/120    avg_loss:0.827, val_acc:0.783]
Epoch [22/120    avg_loss:0.771, val_acc:0.858]
Epoch [23/120    avg_loss:0.725, val_acc:0.806]
Epoch [24/120    avg_loss:0.658, val_acc:0.906]
Epoch [25/120    avg_loss:0.608, val_acc:0.912]
Epoch [26/120    avg_loss:0.552, val_acc:0.904]
Epoch [27/120    avg_loss:0.572, val_acc:0.923]
Epoch [28/120    avg_loss:0.583, val_acc:0.894]
Epoch [29/120    avg_loss:0.510, val_acc:0.904]
Epoch [30/120    avg_loss:0.463, val_acc:0.940]
Epoch [31/120    avg_loss:0.465, val_acc:0.929]
Epoch [32/120    avg_loss:0.433, val_acc:0.933]
Epoch [33/120    avg_loss:0.478, val_acc:0.877]
Epoch [34/120    avg_loss:0.466, val_acc:0.887]
Epoch [35/120    avg_loss:0.409, val_acc:0.944]
Epoch [36/120    avg_loss:0.346, val_acc:0.948]
Epoch [37/120    avg_loss:0.310, val_acc:0.931]
Epoch [38/120    avg_loss:0.396, val_acc:0.912]
Epoch [39/120    avg_loss:0.353, val_acc:0.950]
Epoch [40/120    avg_loss:0.326, val_acc:0.917]
Epoch [41/120    avg_loss:0.312, val_acc:0.950]
Epoch [42/120    avg_loss:0.306, val_acc:0.942]
Epoch [43/120    avg_loss:0.322, val_acc:0.915]
Epoch [44/120    avg_loss:0.345, val_acc:0.950]
Epoch [45/120    avg_loss:0.273, val_acc:0.929]
Epoch [46/120    avg_loss:0.299, val_acc:0.942]
Epoch [47/120    avg_loss:0.270, val_acc:0.952]
Epoch [48/120    avg_loss:0.267, val_acc:0.960]
Epoch [49/120    avg_loss:0.232, val_acc:0.935]
Epoch [50/120    avg_loss:0.233, val_acc:0.954]
Epoch [51/120    avg_loss:0.280, val_acc:0.952]
Epoch [52/120    avg_loss:0.235, val_acc:0.952]
Epoch [53/120    avg_loss:0.266, val_acc:0.925]
Epoch [54/120    avg_loss:0.220, val_acc:0.958]
Epoch [55/120    avg_loss:0.169, val_acc:0.956]
Epoch [56/120    avg_loss:0.161, val_acc:0.952]
Epoch [57/120    avg_loss:0.249, val_acc:0.954]
Epoch [58/120    avg_loss:0.210, val_acc:0.954]
Epoch [59/120    avg_loss:0.238, val_acc:0.973]
Epoch [60/120    avg_loss:0.292, val_acc:0.933]
Epoch [61/120    avg_loss:0.229, val_acc:0.950]
Epoch [62/120    avg_loss:0.213, val_acc:0.952]
Epoch [63/120    avg_loss:0.169, val_acc:0.965]
Epoch [64/120    avg_loss:0.151, val_acc:0.975]
Epoch [65/120    avg_loss:0.190, val_acc:0.967]
Epoch [66/120    avg_loss:0.187, val_acc:0.960]
Epoch [67/120    avg_loss:0.153, val_acc:0.965]
Epoch [68/120    avg_loss:0.133, val_acc:0.975]
Epoch [69/120    avg_loss:0.111, val_acc:0.969]
Epoch [70/120    avg_loss:0.126, val_acc:0.975]
Epoch [71/120    avg_loss:0.104, val_acc:0.981]
Epoch [72/120    avg_loss:0.096, val_acc:0.977]
Epoch [73/120    avg_loss:0.110, val_acc:0.981]
Epoch [74/120    avg_loss:0.105, val_acc:0.971]
Epoch [75/120    avg_loss:0.109, val_acc:0.963]
Epoch [76/120    avg_loss:0.171, val_acc:0.958]
Epoch [77/120    avg_loss:0.125, val_acc:0.975]
Epoch [78/120    avg_loss:0.112, val_acc:0.971]
Epoch [79/120    avg_loss:0.097, val_acc:0.969]
Epoch [80/120    avg_loss:0.082, val_acc:0.975]
Epoch [81/120    avg_loss:0.100, val_acc:0.979]
Epoch [82/120    avg_loss:0.086, val_acc:0.977]
Epoch [83/120    avg_loss:0.094, val_acc:0.963]
Epoch [84/120    avg_loss:0.095, val_acc:0.975]
Epoch [85/120    avg_loss:0.092, val_acc:0.967]
Epoch [86/120    avg_loss:0.075, val_acc:0.971]
Epoch [87/120    avg_loss:0.059, val_acc:0.975]
Epoch [88/120    avg_loss:0.057, val_acc:0.983]
Epoch [89/120    avg_loss:0.052, val_acc:0.985]
Epoch [90/120    avg_loss:0.048, val_acc:0.985]
Epoch [91/120    avg_loss:0.061, val_acc:0.988]
Epoch [92/120    avg_loss:0.040, val_acc:0.988]
Epoch [93/120    avg_loss:0.053, val_acc:0.985]
Epoch [94/120    avg_loss:0.041, val_acc:0.981]
Epoch [95/120    avg_loss:0.040, val_acc:0.981]
Epoch [96/120    avg_loss:0.053, val_acc:0.981]
Epoch [97/120    avg_loss:0.047, val_acc:0.985]
Epoch [98/120    avg_loss:0.044, val_acc:0.985]
Epoch [99/120    avg_loss:0.051, val_acc:0.981]
Epoch [100/120    avg_loss:0.044, val_acc:0.981]
Epoch [101/120    avg_loss:0.038, val_acc:0.983]
Epoch [102/120    avg_loss:0.051, val_acc:0.983]
Epoch [103/120    avg_loss:0.046, val_acc:0.985]
Epoch [104/120    avg_loss:0.041, val_acc:0.985]
Epoch [105/120    avg_loss:0.040, val_acc:0.988]
Epoch [106/120    avg_loss:0.041, val_acc:0.985]
Epoch [107/120    avg_loss:0.038, val_acc:0.983]
Epoch [108/120    avg_loss:0.040, val_acc:0.983]
Epoch [109/120    avg_loss:0.046, val_acc:0.988]
Epoch [110/120    avg_loss:0.039, val_acc:0.988]
Epoch [111/120    avg_loss:0.035, val_acc:0.981]
Epoch [112/120    avg_loss:0.036, val_acc:0.985]
Epoch [113/120    avg_loss:0.037, val_acc:0.985]
Epoch [114/120    avg_loss:0.052, val_acc:0.988]
Epoch [115/120    avg_loss:0.035, val_acc:0.985]
Epoch [116/120    avg_loss:0.032, val_acc:0.985]
Epoch [117/120    avg_loss:0.038, val_acc:0.983]
Epoch [118/120    avg_loss:0.040, val_acc:0.985]
Epoch [119/120    avg_loss:0.038, val_acc:0.985]
Epoch [120/120    avg_loss:0.041, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   5   0   0   0   0   0   0   3   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99780541 0.98871332 1.         0.92600423 0.88059701
 0.99277108 0.9726776  1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9897917345800371
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ba5fe4ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.641, val_acc:0.198]
Epoch [2/120    avg_loss:2.507, val_acc:0.431]
Epoch [3/120    avg_loss:2.391, val_acc:0.510]
Epoch [4/120    avg_loss:2.288, val_acc:0.523]
Epoch [5/120    avg_loss:2.194, val_acc:0.560]
Epoch [6/120    avg_loss:2.092, val_acc:0.540]
Epoch [7/120    avg_loss:1.981, val_acc:0.588]
Epoch [8/120    avg_loss:1.900, val_acc:0.604]
Epoch [9/120    avg_loss:1.804, val_acc:0.640]
Epoch [10/120    avg_loss:1.713, val_acc:0.652]
Epoch [11/120    avg_loss:1.604, val_acc:0.665]
Epoch [12/120    avg_loss:1.498, val_acc:0.690]
Epoch [13/120    avg_loss:1.450, val_acc:0.694]
Epoch [14/120    avg_loss:1.322, val_acc:0.708]
Epoch [15/120    avg_loss:1.236, val_acc:0.738]
Epoch [16/120    avg_loss:1.169, val_acc:0.733]
Epoch [17/120    avg_loss:1.151, val_acc:0.744]
Epoch [18/120    avg_loss:1.025, val_acc:0.787]
Epoch [19/120    avg_loss:0.919, val_acc:0.790]
Epoch [20/120    avg_loss:0.852, val_acc:0.860]
Epoch [21/120    avg_loss:0.786, val_acc:0.835]
Epoch [22/120    avg_loss:0.722, val_acc:0.912]
Epoch [23/120    avg_loss:0.679, val_acc:0.856]
Epoch [24/120    avg_loss:0.612, val_acc:0.927]
Epoch [25/120    avg_loss:0.605, val_acc:0.927]
Epoch [26/120    avg_loss:0.510, val_acc:0.948]
Epoch [27/120    avg_loss:0.481, val_acc:0.948]
Epoch [28/120    avg_loss:0.431, val_acc:0.890]
Epoch [29/120    avg_loss:0.466, val_acc:0.940]
Epoch [30/120    avg_loss:0.406, val_acc:0.923]
Epoch [31/120    avg_loss:0.450, val_acc:0.944]
Epoch [32/120    avg_loss:0.366, val_acc:0.933]
Epoch [33/120    avg_loss:0.380, val_acc:0.912]
Epoch [34/120    avg_loss:0.424, val_acc:0.925]
Epoch [35/120    avg_loss:0.369, val_acc:0.938]
Epoch [36/120    avg_loss:0.354, val_acc:0.915]
Epoch [37/120    avg_loss:0.342, val_acc:0.944]
Epoch [38/120    avg_loss:0.274, val_acc:0.921]
Epoch [39/120    avg_loss:0.284, val_acc:0.938]
Epoch [40/120    avg_loss:0.306, val_acc:0.956]
Epoch [41/120    avg_loss:0.231, val_acc:0.973]
Epoch [42/120    avg_loss:0.253, val_acc:0.965]
Epoch [43/120    avg_loss:0.227, val_acc:0.935]
Epoch [44/120    avg_loss:0.205, val_acc:0.963]
Epoch [45/120    avg_loss:0.204, val_acc:0.969]
Epoch [46/120    avg_loss:0.230, val_acc:0.965]
Epoch [47/120    avg_loss:0.230, val_acc:0.933]
Epoch [48/120    avg_loss:0.195, val_acc:0.973]
Epoch [49/120    avg_loss:0.189, val_acc:0.958]
Epoch [50/120    avg_loss:0.170, val_acc:0.960]
Epoch [51/120    avg_loss:0.163, val_acc:0.969]
Epoch [52/120    avg_loss:0.163, val_acc:0.971]
Epoch [53/120    avg_loss:0.155, val_acc:0.944]
Epoch [54/120    avg_loss:0.151, val_acc:0.963]
Epoch [55/120    avg_loss:0.162, val_acc:0.969]
Epoch [56/120    avg_loss:0.164, val_acc:0.973]
Epoch [57/120    avg_loss:0.143, val_acc:0.973]
Epoch [58/120    avg_loss:0.183, val_acc:0.963]
Epoch [59/120    avg_loss:0.147, val_acc:0.969]
Epoch [60/120    avg_loss:0.125, val_acc:0.973]
Epoch [61/120    avg_loss:0.106, val_acc:0.979]
Epoch [62/120    avg_loss:0.155, val_acc:0.971]
Epoch [63/120    avg_loss:0.134, val_acc:0.965]
Epoch [64/120    avg_loss:0.108, val_acc:0.983]
Epoch [65/120    avg_loss:0.109, val_acc:0.973]
Epoch [66/120    avg_loss:0.120, val_acc:0.979]
Epoch [67/120    avg_loss:0.108, val_acc:0.973]
Epoch [68/120    avg_loss:0.093, val_acc:0.981]
Epoch [69/120    avg_loss:0.102, val_acc:0.983]
Epoch [70/120    avg_loss:0.094, val_acc:0.969]
Epoch [71/120    avg_loss:0.084, val_acc:0.975]
Epoch [72/120    avg_loss:0.082, val_acc:0.979]
Epoch [73/120    avg_loss:0.111, val_acc:0.969]
Epoch [74/120    avg_loss:0.078, val_acc:0.971]
Epoch [75/120    avg_loss:0.101, val_acc:0.981]
Epoch [76/120    avg_loss:0.078, val_acc:0.988]
Epoch [77/120    avg_loss:0.069, val_acc:0.985]
Epoch [78/120    avg_loss:0.131, val_acc:0.983]
Epoch [79/120    avg_loss:0.095, val_acc:0.960]
Epoch [80/120    avg_loss:0.119, val_acc:0.965]
Epoch [81/120    avg_loss:0.075, val_acc:0.981]
Epoch [82/120    avg_loss:0.074, val_acc:0.981]
Epoch [83/120    avg_loss:0.075, val_acc:0.983]
Epoch [84/120    avg_loss:0.056, val_acc:0.985]
Epoch [85/120    avg_loss:0.048, val_acc:0.981]
Epoch [86/120    avg_loss:0.055, val_acc:0.983]
Epoch [87/120    avg_loss:0.082, val_acc:0.983]
Epoch [88/120    avg_loss:0.055, val_acc:0.983]
Epoch [89/120    avg_loss:0.074, val_acc:0.983]
Epoch [90/120    avg_loss:0.054, val_acc:0.983]
Epoch [91/120    avg_loss:0.033, val_acc:0.981]
Epoch [92/120    avg_loss:0.034, val_acc:0.981]
Epoch [93/120    avg_loss:0.038, val_acc:0.981]
Epoch [94/120    avg_loss:0.039, val_acc:0.981]
Epoch [95/120    avg_loss:0.032, val_acc:0.981]
Epoch [96/120    avg_loss:0.043, val_acc:0.981]
Epoch [97/120    avg_loss:0.033, val_acc:0.985]
Epoch [98/120    avg_loss:0.030, val_acc:0.983]
Epoch [99/120    avg_loss:0.031, val_acc:0.983]
Epoch [100/120    avg_loss:0.030, val_acc:0.985]
Epoch [101/120    avg_loss:0.033, val_acc:0.988]
Epoch [102/120    avg_loss:0.026, val_acc:0.985]
Epoch [103/120    avg_loss:0.035, val_acc:0.988]
Epoch [104/120    avg_loss:0.026, val_acc:0.985]
Epoch [105/120    avg_loss:0.027, val_acc:0.988]
Epoch [106/120    avg_loss:0.027, val_acc:0.985]
Epoch [107/120    avg_loss:0.030, val_acc:0.985]
Epoch [108/120    avg_loss:0.030, val_acc:0.983]
Epoch [109/120    avg_loss:0.024, val_acc:0.985]
Epoch [110/120    avg_loss:0.029, val_acc:0.985]
Epoch [111/120    avg_loss:0.030, val_acc:0.983]
Epoch [112/120    avg_loss:0.028, val_acc:0.985]
Epoch [113/120    avg_loss:0.030, val_acc:0.988]
Epoch [114/120    avg_loss:0.032, val_acc:0.985]
Epoch [115/120    avg_loss:0.024, val_acc:0.985]
Epoch [116/120    avg_loss:0.024, val_acc:0.988]
Epoch [117/120    avg_loss:0.025, val_acc:0.985]
Epoch [118/120    avg_loss:0.029, val_acc:0.985]
Epoch [119/120    avg_loss:0.029, val_acc:0.988]
Epoch [120/120    avg_loss:0.028, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   1   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   1   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99926954 0.97550111 1.         0.95909091 0.94771242
 1.         0.93785311 0.99870968 1.         1.         1.
 1.         1.        ]

Kappa:
0.993116131534171
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f2c817ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.637, val_acc:0.277]
Epoch [2/120    avg_loss:2.494, val_acc:0.329]
Epoch [3/120    avg_loss:2.390, val_acc:0.331]
Epoch [4/120    avg_loss:2.309, val_acc:0.367]
Epoch [5/120    avg_loss:2.253, val_acc:0.444]
Epoch [6/120    avg_loss:2.201, val_acc:0.537]
Epoch [7/120    avg_loss:2.125, val_acc:0.594]
Epoch [8/120    avg_loss:2.071, val_acc:0.613]
Epoch [9/120    avg_loss:1.985, val_acc:0.621]
Epoch [10/120    avg_loss:1.903, val_acc:0.577]
Epoch [11/120    avg_loss:1.804, val_acc:0.581]
Epoch [12/120    avg_loss:1.727, val_acc:0.617]
Epoch [13/120    avg_loss:1.614, val_acc:0.644]
Epoch [14/120    avg_loss:1.522, val_acc:0.681]
Epoch [15/120    avg_loss:1.398, val_acc:0.706]
Epoch [16/120    avg_loss:1.304, val_acc:0.773]
Epoch [17/120    avg_loss:1.182, val_acc:0.823]
Epoch [18/120    avg_loss:1.114, val_acc:0.783]
Epoch [19/120    avg_loss:1.036, val_acc:0.800]
Epoch [20/120    avg_loss:1.014, val_acc:0.885]
Epoch [21/120    avg_loss:0.886, val_acc:0.873]
Epoch [22/120    avg_loss:0.795, val_acc:0.812]
Epoch [23/120    avg_loss:0.766, val_acc:0.894]
Epoch [24/120    avg_loss:0.679, val_acc:0.860]
Epoch [25/120    avg_loss:0.659, val_acc:0.929]
Epoch [26/120    avg_loss:0.610, val_acc:0.873]
Epoch [27/120    avg_loss:0.606, val_acc:0.929]
Epoch [28/120    avg_loss:0.592, val_acc:0.931]
Epoch [29/120    avg_loss:0.503, val_acc:0.894]
Epoch [30/120    avg_loss:0.507, val_acc:0.917]
Epoch [31/120    avg_loss:0.475, val_acc:0.940]
Epoch [32/120    avg_loss:0.422, val_acc:0.931]
Epoch [33/120    avg_loss:0.427, val_acc:0.931]
Epoch [34/120    avg_loss:0.383, val_acc:0.929]
Epoch [35/120    avg_loss:0.435, val_acc:0.915]
Epoch [36/120    avg_loss:0.400, val_acc:0.933]
Epoch [37/120    avg_loss:0.388, val_acc:0.896]
Epoch [38/120    avg_loss:0.371, val_acc:0.942]
Epoch [39/120    avg_loss:0.327, val_acc:0.944]
Epoch [40/120    avg_loss:0.373, val_acc:0.919]
Epoch [41/120    avg_loss:0.399, val_acc:0.933]
Epoch [42/120    avg_loss:0.402, val_acc:0.908]
Epoch [43/120    avg_loss:0.384, val_acc:0.954]
Epoch [44/120    avg_loss:0.338, val_acc:0.942]
Epoch [45/120    avg_loss:0.299, val_acc:0.956]
Epoch [46/120    avg_loss:0.253, val_acc:0.965]
Epoch [47/120    avg_loss:0.269, val_acc:0.960]
Epoch [48/120    avg_loss:0.356, val_acc:0.960]
Epoch [49/120    avg_loss:0.272, val_acc:0.969]
Epoch [50/120    avg_loss:0.251, val_acc:0.933]
Epoch [51/120    avg_loss:0.302, val_acc:0.956]
Epoch [52/120    avg_loss:0.214, val_acc:0.904]
Epoch [53/120    avg_loss:0.267, val_acc:0.923]
Epoch [54/120    avg_loss:0.294, val_acc:0.946]
Epoch [55/120    avg_loss:0.258, val_acc:0.971]
Epoch [56/120    avg_loss:0.231, val_acc:0.925]
Epoch [57/120    avg_loss:0.244, val_acc:0.952]
Epoch [58/120    avg_loss:0.208, val_acc:0.967]
Epoch [59/120    avg_loss:0.191, val_acc:0.960]
Epoch [60/120    avg_loss:0.213, val_acc:0.938]
Epoch [61/120    avg_loss:0.179, val_acc:0.973]
Epoch [62/120    avg_loss:0.167, val_acc:0.973]
Epoch [63/120    avg_loss:0.201, val_acc:0.946]
Epoch [64/120    avg_loss:0.217, val_acc:0.958]
Epoch [65/120    avg_loss:0.157, val_acc:0.963]
Epoch [66/120    avg_loss:0.175, val_acc:0.969]
Epoch [67/120    avg_loss:0.175, val_acc:0.965]
Epoch [68/120    avg_loss:0.156, val_acc:0.956]
Epoch [69/120    avg_loss:0.169, val_acc:0.975]
Epoch [70/120    avg_loss:0.125, val_acc:0.963]
Epoch [71/120    avg_loss:0.133, val_acc:0.979]
Epoch [72/120    avg_loss:0.131, val_acc:0.960]
Epoch [73/120    avg_loss:0.134, val_acc:0.977]
Epoch [74/120    avg_loss:0.140, val_acc:0.967]
Epoch [75/120    avg_loss:0.157, val_acc:0.963]
Epoch [76/120    avg_loss:0.139, val_acc:0.981]
Epoch [77/120    avg_loss:0.112, val_acc:0.985]
Epoch [78/120    avg_loss:0.112, val_acc:0.973]
Epoch [79/120    avg_loss:0.121, val_acc:0.977]
Epoch [80/120    avg_loss:0.105, val_acc:0.979]
Epoch [81/120    avg_loss:0.125, val_acc:0.965]
Epoch [82/120    avg_loss:0.112, val_acc:0.981]
Epoch [83/120    avg_loss:0.097, val_acc:0.983]
Epoch [84/120    avg_loss:0.078, val_acc:0.969]
Epoch [85/120    avg_loss:0.093, val_acc:0.965]
Epoch [86/120    avg_loss:0.088, val_acc:0.983]
Epoch [87/120    avg_loss:0.082, val_acc:0.981]
Epoch [88/120    avg_loss:0.091, val_acc:0.977]
Epoch [89/120    avg_loss:0.142, val_acc:0.983]
Epoch [90/120    avg_loss:0.101, val_acc:0.988]
Epoch [91/120    avg_loss:0.091, val_acc:0.973]
Epoch [92/120    avg_loss:0.086, val_acc:0.985]
Epoch [93/120    avg_loss:0.096, val_acc:0.979]
Epoch [94/120    avg_loss:0.102, val_acc:0.981]
Epoch [95/120    avg_loss:0.090, val_acc:0.969]
Epoch [96/120    avg_loss:0.091, val_acc:0.977]
Epoch [97/120    avg_loss:0.084, val_acc:0.971]
Epoch [98/120    avg_loss:0.113, val_acc:0.977]
Epoch [99/120    avg_loss:0.053, val_acc:0.983]
Epoch [100/120    avg_loss:0.069, val_acc:0.975]
Epoch [101/120    avg_loss:0.065, val_acc:0.981]
Epoch [102/120    avg_loss:0.052, val_acc:0.979]
Epoch [103/120    avg_loss:0.069, val_acc:0.988]
Epoch [104/120    avg_loss:0.078, val_acc:0.956]
Epoch [105/120    avg_loss:0.092, val_acc:0.977]
Epoch [106/120    avg_loss:0.142, val_acc:0.973]
Epoch [107/120    avg_loss:0.106, val_acc:0.973]
Epoch [108/120    avg_loss:0.077, val_acc:0.979]
Epoch [109/120    avg_loss:0.070, val_acc:0.975]
Epoch [110/120    avg_loss:0.067, val_acc:0.983]
Epoch [111/120    avg_loss:0.068, val_acc:0.988]
Epoch [112/120    avg_loss:0.053, val_acc:0.988]
Epoch [113/120    avg_loss:0.078, val_acc:0.977]
Epoch [114/120    avg_loss:0.061, val_acc:0.979]
Epoch [115/120    avg_loss:0.061, val_acc:0.985]
Epoch [116/120    avg_loss:0.047, val_acc:0.973]
Epoch [117/120    avg_loss:0.042, val_acc:0.996]
Epoch [118/120    avg_loss:0.050, val_acc:0.988]
Epoch [119/120    avg_loss:0.086, val_acc:0.990]
Epoch [120/120    avg_loss:0.074, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   1   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  13   0   0   0   0   0   0   6   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10   0 196   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 0.99926954 0.96902655 0.99782135 0.9122807  0.92150171
 0.97512438 0.92571429 1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9871797947977484
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e21547a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.613, val_acc:0.187]
Epoch [2/120    avg_loss:2.510, val_acc:0.369]
Epoch [3/120    avg_loss:2.412, val_acc:0.454]
Epoch [4/120    avg_loss:2.330, val_acc:0.454]
Epoch [5/120    avg_loss:2.253, val_acc:0.468]
Epoch [6/120    avg_loss:2.171, val_acc:0.482]
Epoch [7/120    avg_loss:2.094, val_acc:0.488]
Epoch [8/120    avg_loss:1.997, val_acc:0.512]
Epoch [9/120    avg_loss:1.887, val_acc:0.581]
Epoch [10/120    avg_loss:1.830, val_acc:0.591]
Epoch [11/120    avg_loss:1.729, val_acc:0.617]
Epoch [12/120    avg_loss:1.629, val_acc:0.635]
Epoch [13/120    avg_loss:1.554, val_acc:0.653]
Epoch [14/120    avg_loss:1.471, val_acc:0.661]
Epoch [15/120    avg_loss:1.369, val_acc:0.665]
Epoch [16/120    avg_loss:1.285, val_acc:0.688]
Epoch [17/120    avg_loss:1.197, val_acc:0.700]
Epoch [18/120    avg_loss:1.116, val_acc:0.718]
Epoch [19/120    avg_loss:1.068, val_acc:0.704]
Epoch [20/120    avg_loss:0.979, val_acc:0.726]
Epoch [21/120    avg_loss:0.910, val_acc:0.788]
Epoch [22/120    avg_loss:0.831, val_acc:0.804]
Epoch [23/120    avg_loss:0.786, val_acc:0.841]
Epoch [24/120    avg_loss:0.728, val_acc:0.881]
Epoch [25/120    avg_loss:0.675, val_acc:0.893]
Epoch [26/120    avg_loss:0.675, val_acc:0.885]
Epoch [27/120    avg_loss:0.622, val_acc:0.913]
Epoch [28/120    avg_loss:0.575, val_acc:0.877]
Epoch [29/120    avg_loss:0.560, val_acc:0.869]
Epoch [30/120    avg_loss:0.564, val_acc:0.897]
Epoch [31/120    avg_loss:0.550, val_acc:0.873]
Epoch [32/120    avg_loss:0.506, val_acc:0.885]
Epoch [33/120    avg_loss:0.464, val_acc:0.891]
Epoch [34/120    avg_loss:0.434, val_acc:0.931]
Epoch [35/120    avg_loss:0.438, val_acc:0.889]
Epoch [36/120    avg_loss:0.430, val_acc:0.913]
Epoch [37/120    avg_loss:0.445, val_acc:0.923]
Epoch [38/120    avg_loss:0.383, val_acc:0.935]
Epoch [39/120    avg_loss:0.369, val_acc:0.944]
Epoch [40/120    avg_loss:0.386, val_acc:0.913]
Epoch [41/120    avg_loss:0.364, val_acc:0.895]
Epoch [42/120    avg_loss:0.312, val_acc:0.923]
Epoch [43/120    avg_loss:0.323, val_acc:0.960]
Epoch [44/120    avg_loss:0.287, val_acc:0.954]
Epoch [45/120    avg_loss:0.328, val_acc:0.956]
Epoch [46/120    avg_loss:0.282, val_acc:0.956]
Epoch [47/120    avg_loss:0.368, val_acc:0.950]
Epoch [48/120    avg_loss:0.310, val_acc:0.929]
Epoch [49/120    avg_loss:0.252, val_acc:0.952]
Epoch [50/120    avg_loss:0.265, val_acc:0.962]
Epoch [51/120    avg_loss:0.254, val_acc:0.944]
Epoch [52/120    avg_loss:0.212, val_acc:0.958]
Epoch [53/120    avg_loss:0.243, val_acc:0.958]
Epoch [54/120    avg_loss:0.248, val_acc:0.950]
Epoch [55/120    avg_loss:0.343, val_acc:0.897]
Epoch [56/120    avg_loss:0.312, val_acc:0.966]
Epoch [57/120    avg_loss:0.238, val_acc:0.931]
Epoch [58/120    avg_loss:0.205, val_acc:0.952]
Epoch [59/120    avg_loss:0.176, val_acc:0.962]
Epoch [60/120    avg_loss:0.225, val_acc:0.956]
Epoch [61/120    avg_loss:0.220, val_acc:0.960]
Epoch [62/120    avg_loss:0.189, val_acc:0.976]
Epoch [63/120    avg_loss:0.193, val_acc:0.935]
Epoch [64/120    avg_loss:0.192, val_acc:0.962]
Epoch [65/120    avg_loss:0.191, val_acc:0.970]
Epoch [66/120    avg_loss:0.182, val_acc:0.960]
Epoch [67/120    avg_loss:0.178, val_acc:0.970]
Epoch [68/120    avg_loss:0.183, val_acc:0.958]
Epoch [69/120    avg_loss:0.140, val_acc:0.950]
Epoch [70/120    avg_loss:0.196, val_acc:0.960]
Epoch [71/120    avg_loss:0.167, val_acc:0.960]
Epoch [72/120    avg_loss:0.167, val_acc:0.958]
Epoch [73/120    avg_loss:0.210, val_acc:0.978]
Epoch [74/120    avg_loss:0.151, val_acc:0.972]
Epoch [75/120    avg_loss:0.143, val_acc:0.964]
Epoch [76/120    avg_loss:0.170, val_acc:0.964]
Epoch [77/120    avg_loss:0.206, val_acc:0.982]
Epoch [78/120    avg_loss:0.133, val_acc:0.958]
Epoch [79/120    avg_loss:0.154, val_acc:0.980]
Epoch [80/120    avg_loss:0.165, val_acc:0.982]
Epoch [81/120    avg_loss:0.102, val_acc:0.982]
Epoch [82/120    avg_loss:0.169, val_acc:0.966]
Epoch [83/120    avg_loss:0.130, val_acc:0.968]
Epoch [84/120    avg_loss:0.152, val_acc:0.972]
Epoch [85/120    avg_loss:0.142, val_acc:0.974]
Epoch [86/120    avg_loss:0.096, val_acc:0.960]
Epoch [87/120    avg_loss:0.107, val_acc:0.990]
Epoch [88/120    avg_loss:0.264, val_acc:0.980]
Epoch [89/120    avg_loss:0.135, val_acc:0.982]
Epoch [90/120    avg_loss:0.122, val_acc:0.984]
Epoch [91/120    avg_loss:0.127, val_acc:0.974]
Epoch [92/120    avg_loss:0.103, val_acc:0.988]
Epoch [93/120    avg_loss:0.122, val_acc:0.982]
Epoch [94/120    avg_loss:0.123, val_acc:0.986]
Epoch [95/120    avg_loss:0.131, val_acc:0.976]
Epoch [96/120    avg_loss:0.142, val_acc:0.968]
Epoch [97/120    avg_loss:0.114, val_acc:0.988]
Epoch [98/120    avg_loss:0.076, val_acc:0.982]
Epoch [99/120    avg_loss:0.074, val_acc:0.976]
Epoch [100/120    avg_loss:0.092, val_acc:0.984]
Epoch [101/120    avg_loss:0.081, val_acc:0.990]
Epoch [102/120    avg_loss:0.062, val_acc:0.990]
Epoch [103/120    avg_loss:0.056, val_acc:0.988]
Epoch [104/120    avg_loss:0.069, val_acc:0.988]
Epoch [105/120    avg_loss:0.056, val_acc:0.990]
Epoch [106/120    avg_loss:0.050, val_acc:0.990]
Epoch [107/120    avg_loss:0.052, val_acc:0.990]
Epoch [108/120    avg_loss:0.054, val_acc:0.990]
Epoch [109/120    avg_loss:0.054, val_acc:0.990]
Epoch [110/120    avg_loss:0.061, val_acc:0.990]
Epoch [111/120    avg_loss:0.055, val_acc:0.990]
Epoch [112/120    avg_loss:0.042, val_acc:0.990]
Epoch [113/120    avg_loss:0.042, val_acc:0.992]
Epoch [114/120    avg_loss:0.042, val_acc:0.990]
Epoch [115/120    avg_loss:0.045, val_acc:0.990]
Epoch [116/120    avg_loss:0.052, val_acc:0.990]
Epoch [117/120    avg_loss:0.045, val_acc:0.992]
Epoch [118/120    avg_loss:0.040, val_acc:0.992]
Epoch [119/120    avg_loss:0.043, val_acc:0.990]
Epoch [120/120    avg_loss:0.047, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.97333333 1.         0.92982456 0.88888889
 1.         0.93181818 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895543362311159
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb600095ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.567, val_acc:0.373]
Epoch [2/120    avg_loss:2.463, val_acc:0.469]
Epoch [3/120    avg_loss:2.360, val_acc:0.485]
Epoch [4/120    avg_loss:2.277, val_acc:0.515]
Epoch [5/120    avg_loss:2.195, val_acc:0.590]
Epoch [6/120    avg_loss:2.102, val_acc:0.635]
Epoch [7/120    avg_loss:2.017, val_acc:0.671]
Epoch [8/120    avg_loss:1.940, val_acc:0.631]
Epoch [9/120    avg_loss:1.853, val_acc:0.637]
Epoch [10/120    avg_loss:1.738, val_acc:0.654]
Epoch [11/120    avg_loss:1.652, val_acc:0.650]
Epoch [12/120    avg_loss:1.525, val_acc:0.679]
Epoch [13/120    avg_loss:1.439, val_acc:0.702]
Epoch [14/120    avg_loss:1.355, val_acc:0.725]
Epoch [15/120    avg_loss:1.346, val_acc:0.702]
Epoch [16/120    avg_loss:1.197, val_acc:0.731]
Epoch [17/120    avg_loss:1.144, val_acc:0.742]
Epoch [18/120    avg_loss:1.040, val_acc:0.717]
Epoch [19/120    avg_loss:1.020, val_acc:0.742]
Epoch [20/120    avg_loss:0.910, val_acc:0.742]
Epoch [21/120    avg_loss:0.821, val_acc:0.779]
Epoch [22/120    avg_loss:0.801, val_acc:0.812]
Epoch [23/120    avg_loss:0.733, val_acc:0.827]
Epoch [24/120    avg_loss:0.668, val_acc:0.885]
Epoch [25/120    avg_loss:0.678, val_acc:0.821]
Epoch [26/120    avg_loss:0.650, val_acc:0.817]
Epoch [27/120    avg_loss:0.551, val_acc:0.904]
Epoch [28/120    avg_loss:0.514, val_acc:0.890]
Epoch [29/120    avg_loss:0.525, val_acc:0.912]
Epoch [30/120    avg_loss:0.485, val_acc:0.912]
Epoch [31/120    avg_loss:0.444, val_acc:0.879]
Epoch [32/120    avg_loss:0.500, val_acc:0.904]
Epoch [33/120    avg_loss:0.401, val_acc:0.894]
Epoch [34/120    avg_loss:0.426, val_acc:0.881]
Epoch [35/120    avg_loss:0.385, val_acc:0.935]
Epoch [36/120    avg_loss:0.347, val_acc:0.952]
Epoch [37/120    avg_loss:0.318, val_acc:0.896]
Epoch [38/120    avg_loss:0.323, val_acc:0.919]
Epoch [39/120    avg_loss:0.285, val_acc:0.919]
Epoch [40/120    avg_loss:0.357, val_acc:0.915]
Epoch [41/120    avg_loss:0.294, val_acc:0.925]
Epoch [42/120    avg_loss:0.297, val_acc:0.933]
Epoch [43/120    avg_loss:0.332, val_acc:0.927]
Epoch [44/120    avg_loss:0.253, val_acc:0.967]
Epoch [45/120    avg_loss:0.258, val_acc:0.940]
Epoch [46/120    avg_loss:0.264, val_acc:0.900]
Epoch [47/120    avg_loss:0.286, val_acc:0.942]
Epoch [48/120    avg_loss:0.225, val_acc:0.963]
Epoch [49/120    avg_loss:0.243, val_acc:0.917]
Epoch [50/120    avg_loss:0.230, val_acc:0.944]
Epoch [51/120    avg_loss:0.170, val_acc:0.933]
Epoch [52/120    avg_loss:0.170, val_acc:0.952]
Epoch [53/120    avg_loss:0.197, val_acc:0.952]
Epoch [54/120    avg_loss:0.156, val_acc:0.979]
Epoch [55/120    avg_loss:0.164, val_acc:0.954]
Epoch [56/120    avg_loss:0.285, val_acc:0.946]
Epoch [57/120    avg_loss:0.203, val_acc:0.944]
Epoch [58/120    avg_loss:0.183, val_acc:0.965]
Epoch [59/120    avg_loss:0.146, val_acc:0.954]
Epoch [60/120    avg_loss:0.110, val_acc:0.977]
Epoch [61/120    avg_loss:0.122, val_acc:0.965]
Epoch [62/120    avg_loss:0.112, val_acc:0.977]
Epoch [63/120    avg_loss:0.127, val_acc:0.977]
Epoch [64/120    avg_loss:0.130, val_acc:0.973]
Epoch [65/120    avg_loss:0.119, val_acc:0.967]
Epoch [66/120    avg_loss:0.101, val_acc:0.944]
Epoch [67/120    avg_loss:0.117, val_acc:0.983]
Epoch [68/120    avg_loss:0.079, val_acc:0.985]
Epoch [69/120    avg_loss:0.080, val_acc:0.988]
Epoch [70/120    avg_loss:0.113, val_acc:0.965]
Epoch [71/120    avg_loss:0.115, val_acc:0.977]
Epoch [72/120    avg_loss:0.083, val_acc:0.983]
Epoch [73/120    avg_loss:0.128, val_acc:0.992]
Epoch [74/120    avg_loss:0.235, val_acc:0.925]
Epoch [75/120    avg_loss:0.193, val_acc:0.977]
Epoch [76/120    avg_loss:0.099, val_acc:0.983]
Epoch [77/120    avg_loss:0.090, val_acc:0.990]
Epoch [78/120    avg_loss:0.064, val_acc:0.979]
Epoch [79/120    avg_loss:0.062, val_acc:0.977]
Epoch [80/120    avg_loss:0.065, val_acc:0.994]
Epoch [81/120    avg_loss:0.076, val_acc:0.992]
Epoch [82/120    avg_loss:0.078, val_acc:0.985]
Epoch [83/120    avg_loss:0.100, val_acc:0.988]
Epoch [84/120    avg_loss:0.103, val_acc:0.994]
Epoch [85/120    avg_loss:0.102, val_acc:0.983]
Epoch [86/120    avg_loss:0.059, val_acc:0.988]
Epoch [87/120    avg_loss:0.072, val_acc:0.992]
Epoch [88/120    avg_loss:0.058, val_acc:0.985]
Epoch [89/120    avg_loss:0.063, val_acc:0.996]
Epoch [90/120    avg_loss:0.068, val_acc:0.996]
Epoch [91/120    avg_loss:0.082, val_acc:0.992]
Epoch [92/120    avg_loss:0.118, val_acc:0.910]
Epoch [93/120    avg_loss:0.120, val_acc:0.967]
Epoch [94/120    avg_loss:0.106, val_acc:0.965]
Epoch [95/120    avg_loss:0.088, val_acc:0.988]
Epoch [96/120    avg_loss:0.052, val_acc:0.992]
Epoch [97/120    avg_loss:0.049, val_acc:0.983]
Epoch [98/120    avg_loss:0.046, val_acc:0.996]
Epoch [99/120    avg_loss:0.043, val_acc:0.983]
Epoch [100/120    avg_loss:0.048, val_acc:0.990]
Epoch [101/120    avg_loss:0.044, val_acc:0.996]
Epoch [102/120    avg_loss:0.032, val_acc:0.990]
Epoch [103/120    avg_loss:0.034, val_acc:0.983]
Epoch [104/120    avg_loss:0.035, val_acc:0.996]
Epoch [105/120    avg_loss:0.035, val_acc:0.985]
Epoch [106/120    avg_loss:0.030, val_acc:0.990]
Epoch [107/120    avg_loss:0.033, val_acc:0.996]
Epoch [108/120    avg_loss:0.026, val_acc:0.992]
Epoch [109/120    avg_loss:0.024, val_acc:0.992]
Epoch [110/120    avg_loss:0.028, val_acc:0.996]
Epoch [111/120    avg_loss:0.036, val_acc:0.996]
Epoch [112/120    avg_loss:0.065, val_acc:0.981]
Epoch [113/120    avg_loss:0.044, val_acc:0.988]
Epoch [114/120    avg_loss:0.068, val_acc:0.946]
Epoch [115/120    avg_loss:0.086, val_acc:0.981]
Epoch [116/120    avg_loss:0.050, val_acc:0.979]
Epoch [117/120    avg_loss:0.050, val_acc:0.994]
Epoch [118/120    avg_loss:0.046, val_acc:0.981]
Epoch [119/120    avg_loss:0.040, val_acc:0.994]
Epoch [120/120    avg_loss:0.031, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   8   0   0   1   0   0   0   1   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99412628 0.98648649 1.         0.96230599 0.94845361
 0.98095238 0.96703297 0.998713   1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9926422090842376
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbf60ebdac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.306]
Epoch [2/120    avg_loss:2.491, val_acc:0.327]
Epoch [3/120    avg_loss:2.391, val_acc:0.329]
Epoch [4/120    avg_loss:2.330, val_acc:0.342]
Epoch [5/120    avg_loss:2.262, val_acc:0.356]
Epoch [6/120    avg_loss:2.192, val_acc:0.394]
Epoch [7/120    avg_loss:2.139, val_acc:0.438]
Epoch [8/120    avg_loss:2.041, val_acc:0.452]
Epoch [9/120    avg_loss:1.968, val_acc:0.485]
Epoch [10/120    avg_loss:1.905, val_acc:0.517]
Epoch [11/120    avg_loss:1.821, val_acc:0.531]
Epoch [12/120    avg_loss:1.736, val_acc:0.546]
Epoch [13/120    avg_loss:1.653, val_acc:0.596]
Epoch [14/120    avg_loss:1.567, val_acc:0.662]
Epoch [15/120    avg_loss:1.496, val_acc:0.673]
Epoch [16/120    avg_loss:1.431, val_acc:0.704]
Epoch [17/120    avg_loss:1.313, val_acc:0.715]
Epoch [18/120    avg_loss:1.222, val_acc:0.723]
Epoch [19/120    avg_loss:1.155, val_acc:0.750]
Epoch [20/120    avg_loss:1.094, val_acc:0.742]
Epoch [21/120    avg_loss:1.011, val_acc:0.765]
Epoch [22/120    avg_loss:0.932, val_acc:0.777]
Epoch [23/120    avg_loss:0.857, val_acc:0.760]
Epoch [24/120    avg_loss:0.782, val_acc:0.785]
Epoch [25/120    avg_loss:0.700, val_acc:0.783]
Epoch [26/120    avg_loss:0.694, val_acc:0.840]
Epoch [27/120    avg_loss:0.659, val_acc:0.848]
Epoch [28/120    avg_loss:0.633, val_acc:0.825]
Epoch [29/120    avg_loss:0.605, val_acc:0.854]
Epoch [30/120    avg_loss:0.593, val_acc:0.846]
Epoch [31/120    avg_loss:0.555, val_acc:0.823]
Epoch [32/120    avg_loss:0.607, val_acc:0.856]
Epoch [33/120    avg_loss:0.592, val_acc:0.871]
Epoch [34/120    avg_loss:0.451, val_acc:0.900]
Epoch [35/120    avg_loss:0.421, val_acc:0.908]
Epoch [36/120    avg_loss:0.478, val_acc:0.917]
Epoch [37/120    avg_loss:0.404, val_acc:0.915]
Epoch [38/120    avg_loss:0.404, val_acc:0.908]
Epoch [39/120    avg_loss:0.372, val_acc:0.935]
Epoch [40/120    avg_loss:0.351, val_acc:0.931]
Epoch [41/120    avg_loss:0.329, val_acc:0.912]
Epoch [42/120    avg_loss:0.344, val_acc:0.921]
Epoch [43/120    avg_loss:0.306, val_acc:0.933]
Epoch [44/120    avg_loss:0.354, val_acc:0.910]
Epoch [45/120    avg_loss:0.305, val_acc:0.908]
Epoch [46/120    avg_loss:0.273, val_acc:0.931]
Epoch [47/120    avg_loss:0.267, val_acc:0.950]
Epoch [48/120    avg_loss:0.244, val_acc:0.960]
Epoch [49/120    avg_loss:0.295, val_acc:0.935]
Epoch [50/120    avg_loss:0.279, val_acc:0.929]
Epoch [51/120    avg_loss:0.290, val_acc:0.942]
Epoch [52/120    avg_loss:0.246, val_acc:0.948]
Epoch [53/120    avg_loss:0.225, val_acc:0.927]
Epoch [54/120    avg_loss:0.209, val_acc:0.958]
Epoch [55/120    avg_loss:0.184, val_acc:0.973]
Epoch [56/120    avg_loss:0.192, val_acc:0.933]
Epoch [57/120    avg_loss:0.222, val_acc:0.958]
Epoch [58/120    avg_loss:0.274, val_acc:0.940]
Epoch [59/120    avg_loss:0.210, val_acc:0.944]
Epoch [60/120    avg_loss:0.183, val_acc:0.954]
Epoch [61/120    avg_loss:0.204, val_acc:0.923]
Epoch [62/120    avg_loss:0.223, val_acc:0.952]
Epoch [63/120    avg_loss:0.221, val_acc:0.958]
Epoch [64/120    avg_loss:0.232, val_acc:0.940]
Epoch [65/120    avg_loss:0.171, val_acc:0.942]
Epoch [66/120    avg_loss:0.154, val_acc:0.967]
Epoch [67/120    avg_loss:0.161, val_acc:0.956]
Epoch [68/120    avg_loss:0.189, val_acc:0.958]
Epoch [69/120    avg_loss:0.132, val_acc:0.971]
Epoch [70/120    avg_loss:0.112, val_acc:0.983]
Epoch [71/120    avg_loss:0.107, val_acc:0.977]
Epoch [72/120    avg_loss:0.103, val_acc:0.981]
Epoch [73/120    avg_loss:0.103, val_acc:0.981]
Epoch [74/120    avg_loss:0.107, val_acc:0.983]
Epoch [75/120    avg_loss:0.093, val_acc:0.981]
Epoch [76/120    avg_loss:0.118, val_acc:0.983]
Epoch [77/120    avg_loss:0.088, val_acc:0.981]
Epoch [78/120    avg_loss:0.093, val_acc:0.985]
Epoch [79/120    avg_loss:0.100, val_acc:0.981]
Epoch [80/120    avg_loss:0.093, val_acc:0.988]
Epoch [81/120    avg_loss:0.100, val_acc:0.985]
Epoch [82/120    avg_loss:0.091, val_acc:0.983]
Epoch [83/120    avg_loss:0.089, val_acc:0.981]
Epoch [84/120    avg_loss:0.095, val_acc:0.983]
Epoch [85/120    avg_loss:0.091, val_acc:0.985]
Epoch [86/120    avg_loss:0.095, val_acc:0.981]
Epoch [87/120    avg_loss:0.090, val_acc:0.985]
Epoch [88/120    avg_loss:0.081, val_acc:0.985]
Epoch [89/120    avg_loss:0.095, val_acc:0.985]
Epoch [90/120    avg_loss:0.087, val_acc:0.988]
Epoch [91/120    avg_loss:0.081, val_acc:0.985]
Epoch [92/120    avg_loss:0.083, val_acc:0.983]
Epoch [93/120    avg_loss:0.075, val_acc:0.981]
Epoch [94/120    avg_loss:0.097, val_acc:0.985]
Epoch [95/120    avg_loss:0.092, val_acc:0.983]
Epoch [96/120    avg_loss:0.081, val_acc:0.983]
Epoch [97/120    avg_loss:0.088, val_acc:0.985]
Epoch [98/120    avg_loss:0.078, val_acc:0.988]
Epoch [99/120    avg_loss:0.084, val_acc:0.983]
Epoch [100/120    avg_loss:0.073, val_acc:0.983]
Epoch [101/120    avg_loss:0.079, val_acc:0.985]
Epoch [102/120    avg_loss:0.087, val_acc:0.988]
Epoch [103/120    avg_loss:0.088, val_acc:0.985]
Epoch [104/120    avg_loss:0.070, val_acc:0.988]
Epoch [105/120    avg_loss:0.075, val_acc:0.983]
Epoch [106/120    avg_loss:0.079, val_acc:0.985]
Epoch [107/120    avg_loss:0.080, val_acc:0.988]
Epoch [108/120    avg_loss:0.087, val_acc:0.985]
Epoch [109/120    avg_loss:0.086, val_acc:0.981]
Epoch [110/120    avg_loss:0.083, val_acc:0.983]
Epoch [111/120    avg_loss:0.084, val_acc:0.985]
Epoch [112/120    avg_loss:0.079, val_acc:0.985]
Epoch [113/120    avg_loss:0.078, val_acc:0.983]
Epoch [114/120    avg_loss:0.076, val_acc:0.985]
Epoch [115/120    avg_loss:0.082, val_acc:0.985]
Epoch [116/120    avg_loss:0.082, val_acc:0.988]
Epoch [117/120    avg_loss:0.069, val_acc:0.985]
Epoch [118/120    avg_loss:0.075, val_acc:0.985]
Epoch [119/120    avg_loss:0.065, val_acc:0.988]
Epoch [120/120    avg_loss:0.081, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210   9   0   0   0   0   0   0   8   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   4   0   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99195318 0.99095023 1.         0.94382022 0.94158076
 0.98329356 0.97826087 0.99481865 1.         1.         1.
 0.99124726 1.        ]

Kappa:
0.9905047124919567
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd3fad62b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.641, val_acc:0.133]
Epoch [2/120    avg_loss:2.543, val_acc:0.260]
Epoch [3/120    avg_loss:2.443, val_acc:0.388]
Epoch [4/120    avg_loss:2.361, val_acc:0.371]
Epoch [5/120    avg_loss:2.275, val_acc:0.425]
Epoch [6/120    avg_loss:2.173, val_acc:0.467]
Epoch [7/120    avg_loss:2.095, val_acc:0.477]
Epoch [8/120    avg_loss:1.986, val_acc:0.490]
Epoch [9/120    avg_loss:1.889, val_acc:0.537]
Epoch [10/120    avg_loss:1.814, val_acc:0.625]
Epoch [11/120    avg_loss:1.720, val_acc:0.656]
Epoch [12/120    avg_loss:1.634, val_acc:0.648]
Epoch [13/120    avg_loss:1.549, val_acc:0.688]
Epoch [14/120    avg_loss:1.475, val_acc:0.704]
Epoch [15/120    avg_loss:1.396, val_acc:0.733]
Epoch [16/120    avg_loss:1.304, val_acc:0.733]
Epoch [17/120    avg_loss:1.236, val_acc:0.742]
Epoch [18/120    avg_loss:1.171, val_acc:0.735]
Epoch [19/120    avg_loss:1.067, val_acc:0.765]
Epoch [20/120    avg_loss:0.962, val_acc:0.769]
Epoch [21/120    avg_loss:0.885, val_acc:0.867]
Epoch [22/120    avg_loss:0.782, val_acc:0.819]
Epoch [23/120    avg_loss:0.744, val_acc:0.804]
Epoch [24/120    avg_loss:0.714, val_acc:0.898]
Epoch [25/120    avg_loss:0.625, val_acc:0.908]
Epoch [26/120    avg_loss:0.559, val_acc:0.933]
Epoch [27/120    avg_loss:0.523, val_acc:0.883]
Epoch [28/120    avg_loss:0.588, val_acc:0.925]
Epoch [29/120    avg_loss:0.573, val_acc:0.877]
Epoch [30/120    avg_loss:0.519, val_acc:0.927]
Epoch [31/120    avg_loss:0.486, val_acc:0.815]
Epoch [32/120    avg_loss:0.413, val_acc:0.933]
Epoch [33/120    avg_loss:0.385, val_acc:0.948]
Epoch [34/120    avg_loss:0.356, val_acc:0.956]
Epoch [35/120    avg_loss:0.403, val_acc:0.927]
Epoch [36/120    avg_loss:0.364, val_acc:0.952]
Epoch [37/120    avg_loss:0.343, val_acc:0.952]
Epoch [38/120    avg_loss:0.308, val_acc:0.931]
Epoch [39/120    avg_loss:0.222, val_acc:0.950]
Epoch [40/120    avg_loss:0.259, val_acc:0.950]
Epoch [41/120    avg_loss:0.261, val_acc:0.952]
Epoch [42/120    avg_loss:0.309, val_acc:0.935]
Epoch [43/120    avg_loss:0.274, val_acc:0.967]
Epoch [44/120    avg_loss:0.231, val_acc:0.979]
Epoch [45/120    avg_loss:0.217, val_acc:0.975]
Epoch [46/120    avg_loss:0.191, val_acc:0.979]
Epoch [47/120    avg_loss:0.207, val_acc:0.979]
Epoch [48/120    avg_loss:0.170, val_acc:0.975]
Epoch [49/120    avg_loss:0.198, val_acc:0.975]
Epoch [50/120    avg_loss:0.173, val_acc:0.969]
Epoch [51/120    avg_loss:0.186, val_acc:0.975]
Epoch [52/120    avg_loss:0.207, val_acc:0.975]
Epoch [53/120    avg_loss:0.183, val_acc:0.960]
Epoch [54/120    avg_loss:0.178, val_acc:0.977]
Epoch [55/120    avg_loss:0.168, val_acc:0.977]
Epoch [56/120    avg_loss:0.129, val_acc:0.983]
Epoch [57/120    avg_loss:0.118, val_acc:0.996]
Epoch [58/120    avg_loss:0.126, val_acc:0.944]
Epoch [59/120    avg_loss:0.281, val_acc:0.956]
Epoch [60/120    avg_loss:0.187, val_acc:0.971]
Epoch [61/120    avg_loss:0.137, val_acc:0.983]
Epoch [62/120    avg_loss:0.093, val_acc:0.994]
Epoch [63/120    avg_loss:0.148, val_acc:0.933]
Epoch [64/120    avg_loss:0.194, val_acc:0.971]
Epoch [65/120    avg_loss:0.145, val_acc:0.992]
Epoch [66/120    avg_loss:0.086, val_acc:0.977]
Epoch [67/120    avg_loss:0.102, val_acc:0.983]
Epoch [68/120    avg_loss:0.109, val_acc:0.971]
Epoch [69/120    avg_loss:0.138, val_acc:0.979]
Epoch [70/120    avg_loss:0.107, val_acc:0.985]
Epoch [71/120    avg_loss:0.087, val_acc:0.990]
Epoch [72/120    avg_loss:0.070, val_acc:0.990]
Epoch [73/120    avg_loss:0.068, val_acc:0.992]
Epoch [74/120    avg_loss:0.059, val_acc:0.992]
Epoch [75/120    avg_loss:0.058, val_acc:0.992]
Epoch [76/120    avg_loss:0.070, val_acc:0.990]
Epoch [77/120    avg_loss:0.060, val_acc:0.996]
Epoch [78/120    avg_loss:0.056, val_acc:0.994]
Epoch [79/120    avg_loss:0.069, val_acc:0.994]
Epoch [80/120    avg_loss:0.052, val_acc:0.994]
Epoch [81/120    avg_loss:0.052, val_acc:0.994]
Epoch [82/120    avg_loss:0.054, val_acc:0.994]
Epoch [83/120    avg_loss:0.062, val_acc:0.994]
Epoch [84/120    avg_loss:0.055, val_acc:0.994]
Epoch [85/120    avg_loss:0.047, val_acc:0.994]
Epoch [86/120    avg_loss:0.052, val_acc:0.994]
Epoch [87/120    avg_loss:0.054, val_acc:0.994]
Epoch [88/120    avg_loss:0.057, val_acc:0.996]
Epoch [89/120    avg_loss:0.060, val_acc:0.994]
Epoch [90/120    avg_loss:0.049, val_acc:0.996]
Epoch [91/120    avg_loss:0.055, val_acc:0.996]
Epoch [92/120    avg_loss:0.048, val_acc:0.996]
Epoch [93/120    avg_loss:0.044, val_acc:0.996]
Epoch [94/120    avg_loss:0.042, val_acc:0.996]
Epoch [95/120    avg_loss:0.046, val_acc:0.996]
Epoch [96/120    avg_loss:0.043, val_acc:0.994]
Epoch [97/120    avg_loss:0.051, val_acc:0.994]
Epoch [98/120    avg_loss:0.045, val_acc:0.994]
Epoch [99/120    avg_loss:0.047, val_acc:0.994]
Epoch [100/120    avg_loss:0.045, val_acc:0.994]
Epoch [101/120    avg_loss:0.045, val_acc:0.994]
Epoch [102/120    avg_loss:0.038, val_acc:0.996]
Epoch [103/120    avg_loss:0.058, val_acc:0.994]
Epoch [104/120    avg_loss:0.049, val_acc:0.994]
Epoch [105/120    avg_loss:0.042, val_acc:0.996]
Epoch [106/120    avg_loss:0.045, val_acc:0.996]
Epoch [107/120    avg_loss:0.046, val_acc:0.996]
Epoch [108/120    avg_loss:0.051, val_acc:0.996]
Epoch [109/120    avg_loss:0.046, val_acc:0.994]
Epoch [110/120    avg_loss:0.042, val_acc:0.996]
Epoch [111/120    avg_loss:0.043, val_acc:0.996]
Epoch [112/120    avg_loss:0.042, val_acc:0.996]
Epoch [113/120    avg_loss:0.043, val_acc:0.998]
Epoch [114/120    avg_loss:0.041, val_acc:0.996]
Epoch [115/120    avg_loss:0.036, val_acc:0.996]
Epoch [116/120    avg_loss:0.036, val_acc:0.996]
Epoch [117/120    avg_loss:0.038, val_acc:0.996]
Epoch [118/120    avg_loss:0.037, val_acc:0.996]
Epoch [119/120    avg_loss:0.042, val_acc:0.996]
Epoch [120/120    avg_loss:0.040, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 0.99926954 0.99545455 1.         0.97052154 0.95709571
 0.99757869 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9962021456561156
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff67ac9aa20>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.087]
Epoch [2/120    avg_loss:2.501, val_acc:0.283]
Epoch [3/120    avg_loss:2.414, val_acc:0.312]
Epoch [4/120    avg_loss:2.345, val_acc:0.325]
Epoch [5/120    avg_loss:2.269, val_acc:0.362]
Epoch [6/120    avg_loss:2.208, val_acc:0.512]
Epoch [7/120    avg_loss:2.146, val_acc:0.573]
Epoch [8/120    avg_loss:2.068, val_acc:0.604]
Epoch [9/120    avg_loss:2.008, val_acc:0.585]
Epoch [10/120    avg_loss:1.923, val_acc:0.631]
Epoch [11/120    avg_loss:1.826, val_acc:0.604]
Epoch [12/120    avg_loss:1.719, val_acc:0.619]
Epoch [13/120    avg_loss:1.639, val_acc:0.677]
Epoch [14/120    avg_loss:1.549, val_acc:0.665]
Epoch [15/120    avg_loss:1.440, val_acc:0.706]
Epoch [16/120    avg_loss:1.343, val_acc:0.731]
Epoch [17/120    avg_loss:1.238, val_acc:0.746]
Epoch [18/120    avg_loss:1.174, val_acc:0.762]
Epoch [19/120    avg_loss:1.079, val_acc:0.769]
Epoch [20/120    avg_loss:0.988, val_acc:0.781]
Epoch [21/120    avg_loss:0.901, val_acc:0.827]
Epoch [22/120    avg_loss:0.789, val_acc:0.831]
Epoch [23/120    avg_loss:0.740, val_acc:0.844]
Epoch [24/120    avg_loss:0.670, val_acc:0.921]
Epoch [25/120    avg_loss:0.636, val_acc:0.892]
Epoch [26/120    avg_loss:0.647, val_acc:0.850]
Epoch [27/120    avg_loss:0.613, val_acc:0.898]
Epoch [28/120    avg_loss:0.529, val_acc:0.915]
Epoch [29/120    avg_loss:0.536, val_acc:0.923]
Epoch [30/120    avg_loss:0.471, val_acc:0.935]
Epoch [31/120    avg_loss:0.445, val_acc:0.927]
Epoch [32/120    avg_loss:0.453, val_acc:0.927]
Epoch [33/120    avg_loss:0.411, val_acc:0.940]
Epoch [34/120    avg_loss:0.392, val_acc:0.950]
Epoch [35/120    avg_loss:0.362, val_acc:0.935]
Epoch [36/120    avg_loss:0.370, val_acc:0.927]
Epoch [37/120    avg_loss:0.337, val_acc:0.927]
Epoch [38/120    avg_loss:0.348, val_acc:0.944]
Epoch [39/120    avg_loss:0.366, val_acc:0.906]
Epoch [40/120    avg_loss:0.316, val_acc:0.946]
Epoch [41/120    avg_loss:0.308, val_acc:0.950]
Epoch [42/120    avg_loss:0.340, val_acc:0.917]
Epoch [43/120    avg_loss:0.308, val_acc:0.954]
Epoch [44/120    avg_loss:0.261, val_acc:0.965]
Epoch [45/120    avg_loss:0.232, val_acc:0.946]
Epoch [46/120    avg_loss:0.247, val_acc:0.967]
Epoch [47/120    avg_loss:0.228, val_acc:0.969]
Epoch [48/120    avg_loss:0.240, val_acc:0.927]
Epoch [49/120    avg_loss:0.228, val_acc:0.931]
Epoch [50/120    avg_loss:0.232, val_acc:0.956]
Epoch [51/120    avg_loss:0.231, val_acc:0.977]
Epoch [52/120    avg_loss:0.260, val_acc:0.900]
Epoch [53/120    avg_loss:0.231, val_acc:0.965]
Epoch [54/120    avg_loss:0.184, val_acc:0.971]
Epoch [55/120    avg_loss:0.212, val_acc:0.965]
Epoch [56/120    avg_loss:0.149, val_acc:0.971]
Epoch [57/120    avg_loss:0.180, val_acc:0.971]
Epoch [58/120    avg_loss:0.194, val_acc:0.965]
Epoch [59/120    avg_loss:0.214, val_acc:0.950]
Epoch [60/120    avg_loss:0.234, val_acc:0.960]
Epoch [61/120    avg_loss:0.157, val_acc:0.983]
Epoch [62/120    avg_loss:0.201, val_acc:0.963]
Epoch [63/120    avg_loss:0.209, val_acc:0.967]
Epoch [64/120    avg_loss:0.177, val_acc:0.965]
Epoch [65/120    avg_loss:0.178, val_acc:0.963]
Epoch [66/120    avg_loss:0.203, val_acc:0.960]
Epoch [67/120    avg_loss:0.142, val_acc:0.967]
Epoch [68/120    avg_loss:0.206, val_acc:0.952]
Epoch [69/120    avg_loss:0.205, val_acc:0.979]
Epoch [70/120    avg_loss:0.184, val_acc:0.967]
Epoch [71/120    avg_loss:0.141, val_acc:0.969]
Epoch [72/120    avg_loss:0.133, val_acc:0.963]
Epoch [73/120    avg_loss:0.105, val_acc:0.981]
Epoch [74/120    avg_loss:0.084, val_acc:0.981]
Epoch [75/120    avg_loss:0.083, val_acc:0.983]
Epoch [76/120    avg_loss:0.077, val_acc:0.983]
Epoch [77/120    avg_loss:0.070, val_acc:0.983]
Epoch [78/120    avg_loss:0.072, val_acc:0.983]
Epoch [79/120    avg_loss:0.072, val_acc:0.983]
Epoch [80/120    avg_loss:0.068, val_acc:0.981]
Epoch [81/120    avg_loss:0.072, val_acc:0.981]
Epoch [82/120    avg_loss:0.062, val_acc:0.983]
Epoch [83/120    avg_loss:0.075, val_acc:0.983]
Epoch [84/120    avg_loss:0.055, val_acc:0.983]
Epoch [85/120    avg_loss:0.063, val_acc:0.983]
Epoch [86/120    avg_loss:0.058, val_acc:0.983]
Epoch [87/120    avg_loss:0.062, val_acc:0.983]
Epoch [88/120    avg_loss:0.067, val_acc:0.983]
Epoch [89/120    avg_loss:0.060, val_acc:0.983]
Epoch [90/120    avg_loss:0.051, val_acc:0.981]
Epoch [91/120    avg_loss:0.066, val_acc:0.981]
Epoch [92/120    avg_loss:0.054, val_acc:0.981]
Epoch [93/120    avg_loss:0.060, val_acc:0.981]
Epoch [94/120    avg_loss:0.060, val_acc:0.983]
Epoch [95/120    avg_loss:0.055, val_acc:0.983]
Epoch [96/120    avg_loss:0.058, val_acc:0.983]
Epoch [97/120    avg_loss:0.051, val_acc:0.983]
Epoch [98/120    avg_loss:0.050, val_acc:0.983]
Epoch [99/120    avg_loss:0.058, val_acc:0.983]
Epoch [100/120    avg_loss:0.061, val_acc:0.983]
Epoch [101/120    avg_loss:0.065, val_acc:0.981]
Epoch [102/120    avg_loss:0.055, val_acc:0.981]
Epoch [103/120    avg_loss:0.050, val_acc:0.981]
Epoch [104/120    avg_loss:0.061, val_acc:0.981]
Epoch [105/120    avg_loss:0.053, val_acc:0.981]
Epoch [106/120    avg_loss:0.070, val_acc:0.981]
Epoch [107/120    avg_loss:0.060, val_acc:0.981]
Epoch [108/120    avg_loss:0.049, val_acc:0.981]
Epoch [109/120    avg_loss:0.055, val_acc:0.981]
Epoch [110/120    avg_loss:0.046, val_acc:0.981]
Epoch [111/120    avg_loss:0.053, val_acc:0.983]
Epoch [112/120    avg_loss:0.059, val_acc:0.981]
Epoch [113/120    avg_loss:0.053, val_acc:0.983]
Epoch [114/120    avg_loss:0.046, val_acc:0.983]
Epoch [115/120    avg_loss:0.059, val_acc:0.983]
Epoch [116/120    avg_loss:0.047, val_acc:0.983]
Epoch [117/120    avg_loss:0.054, val_acc:0.983]
Epoch [118/120    avg_loss:0.046, val_acc:0.985]
Epoch [119/120    avg_loss:0.048, val_acc:0.985]
Epoch [120/120    avg_loss:0.046, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  16   0   0   0   0   0   0   1   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99707174 0.98426966 0.99782135 0.9375     0.91582492
 0.98795181 0.96132597 1.         1.         1.         0.99470899
 0.99447514 1.        ]

Kappa:
0.9897933842282456
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86648e2b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.175]
Epoch [2/120    avg_loss:2.483, val_acc:0.277]
Epoch [3/120    avg_loss:2.370, val_acc:0.279]
Epoch [4/120    avg_loss:2.273, val_acc:0.458]
Epoch [5/120    avg_loss:2.188, val_acc:0.479]
Epoch [6/120    avg_loss:2.107, val_acc:0.546]
Epoch [7/120    avg_loss:2.014, val_acc:0.573]
Epoch [8/120    avg_loss:1.925, val_acc:0.571]
Epoch [9/120    avg_loss:1.824, val_acc:0.617]
Epoch [10/120    avg_loss:1.716, val_acc:0.615]
Epoch [11/120    avg_loss:1.628, val_acc:0.615]
Epoch [12/120    avg_loss:1.531, val_acc:0.633]
Epoch [13/120    avg_loss:1.449, val_acc:0.637]
Epoch [14/120    avg_loss:1.358, val_acc:0.669]
Epoch [15/120    avg_loss:1.296, val_acc:0.690]
Epoch [16/120    avg_loss:1.211, val_acc:0.715]
Epoch [17/120    avg_loss:1.163, val_acc:0.731]
Epoch [18/120    avg_loss:1.085, val_acc:0.742]
Epoch [19/120    avg_loss:1.010, val_acc:0.781]
Epoch [20/120    avg_loss:0.946, val_acc:0.779]
Epoch [21/120    avg_loss:0.919, val_acc:0.771]
Epoch [22/120    avg_loss:0.842, val_acc:0.785]
Epoch [23/120    avg_loss:0.777, val_acc:0.794]
Epoch [24/120    avg_loss:0.751, val_acc:0.792]
Epoch [25/120    avg_loss:0.680, val_acc:0.838]
Epoch [26/120    avg_loss:0.707, val_acc:0.775]
Epoch [27/120    avg_loss:0.635, val_acc:0.792]
Epoch [28/120    avg_loss:0.584, val_acc:0.881]
Epoch [29/120    avg_loss:0.580, val_acc:0.827]
Epoch [30/120    avg_loss:0.522, val_acc:0.919]
Epoch [31/120    avg_loss:0.489, val_acc:0.935]
Epoch [32/120    avg_loss:0.499, val_acc:0.904]
Epoch [33/120    avg_loss:0.472, val_acc:0.929]
Epoch [34/120    avg_loss:0.528, val_acc:0.838]
Epoch [35/120    avg_loss:0.527, val_acc:0.915]
Epoch [36/120    avg_loss:0.434, val_acc:0.923]
Epoch [37/120    avg_loss:0.431, val_acc:0.933]
Epoch [38/120    avg_loss:0.371, val_acc:0.933]
Epoch [39/120    avg_loss:0.336, val_acc:0.919]
Epoch [40/120    avg_loss:0.368, val_acc:0.929]
Epoch [41/120    avg_loss:0.368, val_acc:0.927]
Epoch [42/120    avg_loss:0.342, val_acc:0.925]
Epoch [43/120    avg_loss:0.326, val_acc:0.938]
Epoch [44/120    avg_loss:0.288, val_acc:0.946]
Epoch [45/120    avg_loss:0.250, val_acc:0.952]
Epoch [46/120    avg_loss:0.293, val_acc:0.956]
Epoch [47/120    avg_loss:0.277, val_acc:0.960]
Epoch [48/120    avg_loss:0.266, val_acc:0.940]
Epoch [49/120    avg_loss:0.256, val_acc:0.944]
Epoch [50/120    avg_loss:0.276, val_acc:0.960]
Epoch [51/120    avg_loss:0.236, val_acc:0.963]
Epoch [52/120    avg_loss:0.245, val_acc:0.965]
Epoch [53/120    avg_loss:0.235, val_acc:0.958]
Epoch [54/120    avg_loss:0.203, val_acc:0.960]
Epoch [55/120    avg_loss:0.220, val_acc:0.923]
Epoch [56/120    avg_loss:0.197, val_acc:0.971]
Epoch [57/120    avg_loss:0.248, val_acc:0.948]
Epoch [58/120    avg_loss:0.217, val_acc:0.975]
Epoch [59/120    avg_loss:0.192, val_acc:0.967]
Epoch [60/120    avg_loss:0.202, val_acc:0.963]
Epoch [61/120    avg_loss:0.199, val_acc:0.967]
Epoch [62/120    avg_loss:0.163, val_acc:0.973]
Epoch [63/120    avg_loss:0.175, val_acc:0.965]
Epoch [64/120    avg_loss:0.211, val_acc:0.975]
Epoch [65/120    avg_loss:0.189, val_acc:0.963]
Epoch [66/120    avg_loss:0.202, val_acc:0.971]
Epoch [67/120    avg_loss:0.150, val_acc:0.969]
Epoch [68/120    avg_loss:0.175, val_acc:0.981]
Epoch [69/120    avg_loss:0.186, val_acc:0.971]
Epoch [70/120    avg_loss:0.134, val_acc:0.977]
Epoch [71/120    avg_loss:0.143, val_acc:0.973]
Epoch [72/120    avg_loss:0.150, val_acc:0.973]
Epoch [73/120    avg_loss:0.165, val_acc:0.958]
Epoch [74/120    avg_loss:0.118, val_acc:0.981]
Epoch [75/120    avg_loss:0.143, val_acc:0.958]
Epoch [76/120    avg_loss:0.126, val_acc:0.985]
Epoch [77/120    avg_loss:0.192, val_acc:0.977]
Epoch [78/120    avg_loss:0.150, val_acc:0.973]
Epoch [79/120    avg_loss:0.116, val_acc:0.979]
Epoch [80/120    avg_loss:0.171, val_acc:0.948]
Epoch [81/120    avg_loss:0.189, val_acc:0.923]
Epoch [82/120    avg_loss:0.121, val_acc:0.992]
Epoch [83/120    avg_loss:0.117, val_acc:0.990]
Epoch [84/120    avg_loss:0.139, val_acc:0.935]
Epoch [85/120    avg_loss:0.161, val_acc:0.988]
Epoch [86/120    avg_loss:0.137, val_acc:0.981]
Epoch [87/120    avg_loss:0.144, val_acc:0.977]
Epoch [88/120    avg_loss:0.141, val_acc:0.979]
Epoch [89/120    avg_loss:0.127, val_acc:0.958]
Epoch [90/120    avg_loss:0.109, val_acc:0.988]
Epoch [91/120    avg_loss:0.088, val_acc:0.981]
Epoch [92/120    avg_loss:0.095, val_acc:0.969]
Epoch [93/120    avg_loss:0.099, val_acc:0.983]
Epoch [94/120    avg_loss:0.075, val_acc:0.983]
Epoch [95/120    avg_loss:0.075, val_acc:0.992]
Epoch [96/120    avg_loss:0.096, val_acc:0.981]
Epoch [97/120    avg_loss:0.082, val_acc:0.992]
Epoch [98/120    avg_loss:0.069, val_acc:0.998]
Epoch [99/120    avg_loss:0.079, val_acc:0.990]
Epoch [100/120    avg_loss:0.089, val_acc:0.988]
Epoch [101/120    avg_loss:0.124, val_acc:0.994]
Epoch [102/120    avg_loss:0.069, val_acc:1.000]
Epoch [103/120    avg_loss:0.063, val_acc:0.992]
Epoch [104/120    avg_loss:0.048, val_acc:0.998]
Epoch [105/120    avg_loss:0.052, val_acc:0.990]
Epoch [106/120    avg_loss:0.069, val_acc:0.988]
Epoch [107/120    avg_loss:0.100, val_acc:0.985]
Epoch [108/120    avg_loss:0.070, val_acc:1.000]
Epoch [109/120    avg_loss:0.062, val_acc:0.998]
Epoch [110/120    avg_loss:0.046, val_acc:0.992]
Epoch [111/120    avg_loss:0.043, val_acc:1.000]
Epoch [112/120    avg_loss:0.032, val_acc:0.996]
Epoch [113/120    avg_loss:0.064, val_acc:0.992]
Epoch [114/120    avg_loss:0.040, val_acc:0.985]
Epoch [115/120    avg_loss:0.081, val_acc:0.975]
Epoch [116/120    avg_loss:0.053, val_acc:0.990]
Epoch [117/120    avg_loss:0.091, val_acc:0.988]
Epoch [118/120    avg_loss:0.077, val_acc:0.992]
Epoch [119/120    avg_loss:0.063, val_acc:0.979]
Epoch [120/120    avg_loss:0.059, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 224   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99264706 0.98871332 0.9978308  0.95726496 0.93090909
 0.97630332 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.991692881870098
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2608c88a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.144]
Epoch [2/120    avg_loss:2.535, val_acc:0.304]
Epoch [3/120    avg_loss:2.448, val_acc:0.323]
Epoch [4/120    avg_loss:2.367, val_acc:0.342]
Epoch [5/120    avg_loss:2.299, val_acc:0.379]
Epoch [6/120    avg_loss:2.242, val_acc:0.448]
Epoch [7/120    avg_loss:2.181, val_acc:0.527]
Epoch [8/120    avg_loss:2.116, val_acc:0.592]
Epoch [9/120    avg_loss:2.046, val_acc:0.625]
Epoch [10/120    avg_loss:1.980, val_acc:0.604]
Epoch [11/120    avg_loss:1.915, val_acc:0.665]
Epoch [12/120    avg_loss:1.789, val_acc:0.660]
Epoch [13/120    avg_loss:1.711, val_acc:0.677]
Epoch [14/120    avg_loss:1.605, val_acc:0.681]
Epoch [15/120    avg_loss:1.472, val_acc:0.710]
Epoch [16/120    avg_loss:1.371, val_acc:0.748]
Epoch [17/120    avg_loss:1.281, val_acc:0.758]
Epoch [18/120    avg_loss:1.185, val_acc:0.758]
Epoch [19/120    avg_loss:1.107, val_acc:0.781]
Epoch [20/120    avg_loss:1.082, val_acc:0.777]
Epoch [21/120    avg_loss:0.973, val_acc:0.779]
Epoch [22/120    avg_loss:0.902, val_acc:0.794]
Epoch [23/120    avg_loss:0.813, val_acc:0.785]
Epoch [24/120    avg_loss:0.740, val_acc:0.823]
Epoch [25/120    avg_loss:0.707, val_acc:0.817]
Epoch [26/120    avg_loss:0.653, val_acc:0.815]
Epoch [27/120    avg_loss:0.607, val_acc:0.838]
Epoch [28/120    avg_loss:0.601, val_acc:0.871]
Epoch [29/120    avg_loss:0.550, val_acc:0.848]
Epoch [30/120    avg_loss:0.520, val_acc:0.900]
Epoch [31/120    avg_loss:0.483, val_acc:0.877]
Epoch [32/120    avg_loss:0.445, val_acc:0.846]
Epoch [33/120    avg_loss:0.409, val_acc:0.929]
Epoch [34/120    avg_loss:0.406, val_acc:0.917]
Epoch [35/120    avg_loss:0.381, val_acc:0.942]
Epoch [36/120    avg_loss:0.336, val_acc:0.952]
Epoch [37/120    avg_loss:0.381, val_acc:0.919]
Epoch [38/120    avg_loss:0.430, val_acc:0.935]
Epoch [39/120    avg_loss:0.339, val_acc:0.925]
Epoch [40/120    avg_loss:0.316, val_acc:0.892]
Epoch [41/120    avg_loss:0.326, val_acc:0.902]
Epoch [42/120    avg_loss:0.339, val_acc:0.906]
Epoch [43/120    avg_loss:0.348, val_acc:0.923]
Epoch [44/120    avg_loss:0.273, val_acc:0.942]
Epoch [45/120    avg_loss:0.305, val_acc:0.956]
Epoch [46/120    avg_loss:0.292, val_acc:0.956]
Epoch [47/120    avg_loss:0.247, val_acc:0.954]
Epoch [48/120    avg_loss:0.241, val_acc:0.960]
Epoch [49/120    avg_loss:0.265, val_acc:0.938]
Epoch [50/120    avg_loss:0.230, val_acc:0.967]
Epoch [51/120    avg_loss:0.173, val_acc:0.963]
Epoch [52/120    avg_loss:0.157, val_acc:0.967]
Epoch [53/120    avg_loss:0.213, val_acc:0.954]
Epoch [54/120    avg_loss:0.193, val_acc:0.960]
Epoch [55/120    avg_loss:0.166, val_acc:0.969]
Epoch [56/120    avg_loss:0.151, val_acc:0.967]
Epoch [57/120    avg_loss:0.307, val_acc:0.956]
Epoch [58/120    avg_loss:0.258, val_acc:0.948]
Epoch [59/120    avg_loss:0.187, val_acc:0.958]
Epoch [60/120    avg_loss:0.198, val_acc:0.954]
Epoch [61/120    avg_loss:0.203, val_acc:0.954]
Epoch [62/120    avg_loss:0.145, val_acc:0.933]
Epoch [63/120    avg_loss:0.154, val_acc:0.975]
Epoch [64/120    avg_loss:0.140, val_acc:0.973]
Epoch [65/120    avg_loss:0.110, val_acc:0.975]
Epoch [66/120    avg_loss:0.150, val_acc:0.981]
Epoch [67/120    avg_loss:0.118, val_acc:0.963]
Epoch [68/120    avg_loss:0.106, val_acc:0.985]
Epoch [69/120    avg_loss:0.096, val_acc:0.973]
Epoch [70/120    avg_loss:0.117, val_acc:0.956]
Epoch [71/120    avg_loss:0.173, val_acc:0.963]
Epoch [72/120    avg_loss:0.141, val_acc:0.967]
Epoch [73/120    avg_loss:0.110, val_acc:0.973]
Epoch [74/120    avg_loss:0.100, val_acc:0.979]
Epoch [75/120    avg_loss:0.068, val_acc:0.979]
Epoch [76/120    avg_loss:0.089, val_acc:0.981]
Epoch [77/120    avg_loss:0.083, val_acc:0.969]
Epoch [78/120    avg_loss:0.122, val_acc:0.969]
Epoch [79/120    avg_loss:0.083, val_acc:0.988]
Epoch [80/120    avg_loss:0.051, val_acc:0.985]
Epoch [81/120    avg_loss:0.070, val_acc:0.988]
Epoch [82/120    avg_loss:0.057, val_acc:0.965]
Epoch [83/120    avg_loss:0.083, val_acc:0.963]
Epoch [84/120    avg_loss:0.053, val_acc:0.981]
Epoch [85/120    avg_loss:0.056, val_acc:0.971]
Epoch [86/120    avg_loss:0.075, val_acc:0.979]
Epoch [87/120    avg_loss:0.045, val_acc:0.985]
Epoch [88/120    avg_loss:0.036, val_acc:0.985]
Epoch [89/120    avg_loss:0.055, val_acc:0.965]
Epoch [90/120    avg_loss:0.064, val_acc:0.979]
Epoch [91/120    avg_loss:0.060, val_acc:0.979]
Epoch [92/120    avg_loss:0.073, val_acc:0.983]
Epoch [93/120    avg_loss:0.055, val_acc:0.992]
Epoch [94/120    avg_loss:0.052, val_acc:0.985]
Epoch [95/120    avg_loss:0.074, val_acc:0.981]
Epoch [96/120    avg_loss:0.098, val_acc:0.975]
Epoch [97/120    avg_loss:0.079, val_acc:0.992]
Epoch [98/120    avg_loss:0.060, val_acc:0.973]
Epoch [99/120    avg_loss:0.075, val_acc:0.967]
Epoch [100/120    avg_loss:0.098, val_acc:0.985]
Epoch [101/120    avg_loss:0.055, val_acc:0.975]
Epoch [102/120    avg_loss:0.060, val_acc:0.988]
Epoch [103/120    avg_loss:0.041, val_acc:0.985]
Epoch [104/120    avg_loss:0.049, val_acc:0.990]
Epoch [105/120    avg_loss:0.041, val_acc:0.985]
Epoch [106/120    avg_loss:0.036, val_acc:0.983]
Epoch [107/120    avg_loss:0.023, val_acc:0.983]
Epoch [108/120    avg_loss:0.030, val_acc:0.985]
Epoch [109/120    avg_loss:0.020, val_acc:0.988]
Epoch [110/120    avg_loss:0.018, val_acc:0.988]
Epoch [111/120    avg_loss:0.022, val_acc:0.990]
Epoch [112/120    avg_loss:0.018, val_acc:0.990]
Epoch [113/120    avg_loss:0.024, val_acc:0.988]
Epoch [114/120    avg_loss:0.020, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.988]
Epoch [116/120    avg_loss:0.020, val_acc:0.988]
Epoch [117/120    avg_loss:0.018, val_acc:0.988]
Epoch [118/120    avg_loss:0.018, val_acc:0.988]
Epoch [119/120    avg_loss:0.015, val_acc:0.988]
Epoch [120/120    avg_loss:0.018, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99412628 0.99545455 1.         0.96103896 0.93617021
 0.98095238 0.98924731 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9931169307006855
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f70c13b8ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.573, val_acc:0.344]
Epoch [2/120    avg_loss:2.441, val_acc:0.383]
Epoch [3/120    avg_loss:2.348, val_acc:0.394]
Epoch [4/120    avg_loss:2.257, val_acc:0.383]
Epoch [5/120    avg_loss:2.181, val_acc:0.385]
Epoch [6/120    avg_loss:2.124, val_acc:0.412]
Epoch [7/120    avg_loss:2.047, val_acc:0.460]
Epoch [8/120    avg_loss:1.987, val_acc:0.533]
Epoch [9/120    avg_loss:1.925, val_acc:0.581]
Epoch [10/120    avg_loss:1.843, val_acc:0.613]
Epoch [11/120    avg_loss:1.773, val_acc:0.588]
Epoch [12/120    avg_loss:1.664, val_acc:0.625]
Epoch [13/120    avg_loss:1.603, val_acc:0.652]
Epoch [14/120    avg_loss:1.527, val_acc:0.656]
Epoch [15/120    avg_loss:1.430, val_acc:0.662]
Epoch [16/120    avg_loss:1.348, val_acc:0.673]
Epoch [17/120    avg_loss:1.283, val_acc:0.696]
Epoch [18/120    avg_loss:1.210, val_acc:0.713]
Epoch [19/120    avg_loss:1.122, val_acc:0.706]
Epoch [20/120    avg_loss:1.045, val_acc:0.762]
Epoch [21/120    avg_loss:0.988, val_acc:0.723]
Epoch [22/120    avg_loss:0.957, val_acc:0.775]
Epoch [23/120    avg_loss:0.862, val_acc:0.773]
Epoch [24/120    avg_loss:0.800, val_acc:0.804]
Epoch [25/120    avg_loss:0.755, val_acc:0.792]
Epoch [26/120    avg_loss:0.742, val_acc:0.827]
Epoch [27/120    avg_loss:0.707, val_acc:0.760]
Epoch [28/120    avg_loss:0.701, val_acc:0.779]
Epoch [29/120    avg_loss:0.649, val_acc:0.804]
Epoch [30/120    avg_loss:0.570, val_acc:0.808]
Epoch [31/120    avg_loss:0.561, val_acc:0.848]
Epoch [32/120    avg_loss:0.527, val_acc:0.896]
Epoch [33/120    avg_loss:0.487, val_acc:0.938]
Epoch [34/120    avg_loss:0.463, val_acc:0.863]
Epoch [35/120    avg_loss:0.447, val_acc:0.831]
Epoch [36/120    avg_loss:0.453, val_acc:0.933]
Epoch [37/120    avg_loss:0.406, val_acc:0.921]
Epoch [38/120    avg_loss:0.435, val_acc:0.912]
Epoch [39/120    avg_loss:0.463, val_acc:0.877]
Epoch [40/120    avg_loss:0.408, val_acc:0.931]
Epoch [41/120    avg_loss:0.320, val_acc:0.910]
Epoch [42/120    avg_loss:0.331, val_acc:0.946]
Epoch [43/120    avg_loss:0.478, val_acc:0.929]
Epoch [44/120    avg_loss:0.300, val_acc:0.935]
Epoch [45/120    avg_loss:0.316, val_acc:0.965]
Epoch [46/120    avg_loss:0.291, val_acc:0.946]
Epoch [47/120    avg_loss:0.283, val_acc:0.956]
Epoch [48/120    avg_loss:0.264, val_acc:0.965]
Epoch [49/120    avg_loss:0.207, val_acc:0.960]
Epoch [50/120    avg_loss:0.265, val_acc:0.948]
Epoch [51/120    avg_loss:0.228, val_acc:0.963]
Epoch [52/120    avg_loss:0.227, val_acc:0.954]
Epoch [53/120    avg_loss:0.243, val_acc:0.971]
Epoch [54/120    avg_loss:0.215, val_acc:0.963]
Epoch [55/120    avg_loss:0.194, val_acc:0.965]
Epoch [56/120    avg_loss:0.170, val_acc:0.965]
Epoch [57/120    avg_loss:0.173, val_acc:0.958]
Epoch [58/120    avg_loss:0.165, val_acc:0.977]
Epoch [59/120    avg_loss:0.180, val_acc:0.979]
Epoch [60/120    avg_loss:0.136, val_acc:0.985]
Epoch [61/120    avg_loss:0.121, val_acc:0.975]
Epoch [62/120    avg_loss:0.117, val_acc:0.985]
Epoch [63/120    avg_loss:0.113, val_acc:0.977]
Epoch [64/120    avg_loss:0.166, val_acc:0.963]
Epoch [65/120    avg_loss:0.164, val_acc:0.931]
Epoch [66/120    avg_loss:0.128, val_acc:0.975]
Epoch [67/120    avg_loss:0.109, val_acc:0.977]
Epoch [68/120    avg_loss:0.125, val_acc:0.977]
Epoch [69/120    avg_loss:0.129, val_acc:0.960]
Epoch [70/120    avg_loss:0.119, val_acc:0.985]
Epoch [71/120    avg_loss:0.102, val_acc:0.988]
Epoch [72/120    avg_loss:0.079, val_acc:0.981]
Epoch [73/120    avg_loss:0.105, val_acc:0.985]
Epoch [74/120    avg_loss:0.084, val_acc:0.973]
Epoch [75/120    avg_loss:0.198, val_acc:0.973]
Epoch [76/120    avg_loss:0.208, val_acc:0.983]
Epoch [77/120    avg_loss:0.143, val_acc:0.979]
Epoch [78/120    avg_loss:0.121, val_acc:0.979]
Epoch [79/120    avg_loss:0.150, val_acc:0.979]
Epoch [80/120    avg_loss:0.135, val_acc:0.985]
Epoch [81/120    avg_loss:0.074, val_acc:0.981]
Epoch [82/120    avg_loss:0.075, val_acc:0.988]
Epoch [83/120    avg_loss:0.089, val_acc:0.950]
Epoch [84/120    avg_loss:0.097, val_acc:0.990]
Epoch [85/120    avg_loss:0.050, val_acc:0.990]
Epoch [86/120    avg_loss:0.061, val_acc:0.990]
Epoch [87/120    avg_loss:0.061, val_acc:0.985]
Epoch [88/120    avg_loss:0.047, val_acc:0.990]
Epoch [89/120    avg_loss:0.053, val_acc:0.975]
Epoch [90/120    avg_loss:0.107, val_acc:0.992]
Epoch [91/120    avg_loss:0.096, val_acc:0.956]
Epoch [92/120    avg_loss:0.104, val_acc:0.975]
Epoch [93/120    avg_loss:0.090, val_acc:0.992]
Epoch [94/120    avg_loss:0.076, val_acc:0.983]
Epoch [95/120    avg_loss:0.082, val_acc:0.992]
Epoch [96/120    avg_loss:0.067, val_acc:0.990]
Epoch [97/120    avg_loss:0.076, val_acc:0.985]
Epoch [98/120    avg_loss:0.052, val_acc:0.985]
Epoch [99/120    avg_loss:0.045, val_acc:0.994]
Epoch [100/120    avg_loss:0.036, val_acc:0.990]
Epoch [101/120    avg_loss:0.032, val_acc:0.994]
Epoch [102/120    avg_loss:0.034, val_acc:0.990]
Epoch [103/120    avg_loss:0.037, val_acc:0.992]
Epoch [104/120    avg_loss:0.078, val_acc:0.896]
Epoch [105/120    avg_loss:0.098, val_acc:0.988]
Epoch [106/120    avg_loss:0.060, val_acc:0.992]
Epoch [107/120    avg_loss:0.037, val_acc:0.992]
Epoch [108/120    avg_loss:0.038, val_acc:0.990]
Epoch [109/120    avg_loss:0.027, val_acc:0.994]
Epoch [110/120    avg_loss:0.028, val_acc:0.992]
Epoch [111/120    avg_loss:0.021, val_acc:0.994]
Epoch [112/120    avg_loss:0.027, val_acc:0.992]
Epoch [113/120    avg_loss:0.025, val_acc:0.994]
Epoch [114/120    avg_loss:0.052, val_acc:0.967]
Epoch [115/120    avg_loss:0.123, val_acc:0.983]
Epoch [116/120    avg_loss:0.053, val_acc:0.988]
Epoch [117/120    avg_loss:0.060, val_acc:0.990]
Epoch [118/120    avg_loss:0.032, val_acc:0.994]
Epoch [119/120    avg_loss:0.025, val_acc:0.994]
Epoch [120/120    avg_loss:0.033, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 196  31   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99266862 1.         0.99343545 0.92018779 0.90342679
 0.98095238 1.         0.99741602 1.         1.         1.
 1.         1.        ]

Kappa:
0.9895582754841252
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff5869caa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.615, val_acc:0.215]
Epoch [2/120    avg_loss:2.502, val_acc:0.354]
Epoch [3/120    avg_loss:2.398, val_acc:0.410]
Epoch [4/120    avg_loss:2.323, val_acc:0.492]
Epoch [5/120    avg_loss:2.236, val_acc:0.500]
Epoch [6/120    avg_loss:2.165, val_acc:0.523]
Epoch [7/120    avg_loss:2.100, val_acc:0.540]
Epoch [8/120    avg_loss:2.020, val_acc:0.575]
Epoch [9/120    avg_loss:1.920, val_acc:0.623]
Epoch [10/120    avg_loss:1.834, val_acc:0.642]
Epoch [11/120    avg_loss:1.728, val_acc:0.667]
Epoch [12/120    avg_loss:1.620, val_acc:0.685]
Epoch [13/120    avg_loss:1.503, val_acc:0.713]
Epoch [14/120    avg_loss:1.397, val_acc:0.717]
Epoch [15/120    avg_loss:1.310, val_acc:0.738]
Epoch [16/120    avg_loss:1.193, val_acc:0.756]
Epoch [17/120    avg_loss:1.111, val_acc:0.740]
Epoch [18/120    avg_loss:1.029, val_acc:0.758]
Epoch [19/120    avg_loss:0.944, val_acc:0.765]
Epoch [20/120    avg_loss:0.845, val_acc:0.802]
Epoch [21/120    avg_loss:0.781, val_acc:0.819]
Epoch [22/120    avg_loss:0.750, val_acc:0.850]
Epoch [23/120    avg_loss:0.704, val_acc:0.860]
Epoch [24/120    avg_loss:0.670, val_acc:0.875]
Epoch [25/120    avg_loss:0.608, val_acc:0.887]
Epoch [26/120    avg_loss:0.575, val_acc:0.835]
Epoch [27/120    avg_loss:0.568, val_acc:0.873]
Epoch [28/120    avg_loss:0.511, val_acc:0.929]
Epoch [29/120    avg_loss:0.487, val_acc:0.915]
Epoch [30/120    avg_loss:0.453, val_acc:0.923]
Epoch [31/120    avg_loss:0.442, val_acc:0.931]
Epoch [32/120    avg_loss:0.438, val_acc:0.917]
Epoch [33/120    avg_loss:0.575, val_acc:0.860]
Epoch [34/120    avg_loss:0.527, val_acc:0.879]
Epoch [35/120    avg_loss:0.493, val_acc:0.929]
Epoch [36/120    avg_loss:0.401, val_acc:0.933]
Epoch [37/120    avg_loss:0.383, val_acc:0.933]
Epoch [38/120    avg_loss:0.374, val_acc:0.933]
Epoch [39/120    avg_loss:0.349, val_acc:0.935]
Epoch [40/120    avg_loss:0.340, val_acc:0.935]
Epoch [41/120    avg_loss:0.335, val_acc:0.923]
Epoch [42/120    avg_loss:0.371, val_acc:0.933]
Epoch [43/120    avg_loss:0.282, val_acc:0.938]
Epoch [44/120    avg_loss:0.330, val_acc:0.942]
Epoch [45/120    avg_loss:0.282, val_acc:0.950]
Epoch [46/120    avg_loss:0.269, val_acc:0.940]
Epoch [47/120    avg_loss:0.264, val_acc:0.950]
Epoch [48/120    avg_loss:0.248, val_acc:0.908]
Epoch [49/120    avg_loss:0.234, val_acc:0.948]
Epoch [50/120    avg_loss:0.302, val_acc:0.956]
Epoch [51/120    avg_loss:0.241, val_acc:0.944]
Epoch [52/120    avg_loss:0.232, val_acc:0.956]
Epoch [53/120    avg_loss:0.227, val_acc:0.942]
Epoch [54/120    avg_loss:0.209, val_acc:0.952]
Epoch [55/120    avg_loss:0.187, val_acc:0.960]
Epoch [56/120    avg_loss:0.162, val_acc:0.967]
Epoch [57/120    avg_loss:0.199, val_acc:0.935]
Epoch [58/120    avg_loss:0.185, val_acc:0.960]
Epoch [59/120    avg_loss:0.185, val_acc:0.960]
Epoch [60/120    avg_loss:0.163, val_acc:0.960]
Epoch [61/120    avg_loss:0.168, val_acc:0.960]
Epoch [62/120    avg_loss:0.233, val_acc:0.952]
Epoch [63/120    avg_loss:0.210, val_acc:0.952]
Epoch [64/120    avg_loss:0.206, val_acc:0.958]
Epoch [65/120    avg_loss:0.190, val_acc:0.965]
Epoch [66/120    avg_loss:0.179, val_acc:0.940]
Epoch [67/120    avg_loss:0.178, val_acc:0.952]
Epoch [68/120    avg_loss:0.173, val_acc:0.975]
Epoch [69/120    avg_loss:0.180, val_acc:0.958]
Epoch [70/120    avg_loss:0.214, val_acc:0.963]
Epoch [71/120    avg_loss:0.137, val_acc:0.975]
Epoch [72/120    avg_loss:0.147, val_acc:0.935]
Epoch [73/120    avg_loss:0.118, val_acc:0.971]
Epoch [74/120    avg_loss:0.138, val_acc:0.967]
Epoch [75/120    avg_loss:0.118, val_acc:0.963]
Epoch [76/120    avg_loss:0.132, val_acc:0.971]
Epoch [77/120    avg_loss:0.121, val_acc:0.963]
Epoch [78/120    avg_loss:0.112, val_acc:0.979]
Epoch [79/120    avg_loss:0.123, val_acc:0.971]
Epoch [80/120    avg_loss:0.103, val_acc:0.967]
Epoch [81/120    avg_loss:0.123, val_acc:0.977]
Epoch [82/120    avg_loss:0.098, val_acc:0.983]
Epoch [83/120    avg_loss:0.093, val_acc:0.979]
Epoch [84/120    avg_loss:0.090, val_acc:0.977]
Epoch [85/120    avg_loss:0.104, val_acc:0.973]
Epoch [86/120    avg_loss:0.084, val_acc:0.977]
Epoch [87/120    avg_loss:0.081, val_acc:0.973]
Epoch [88/120    avg_loss:0.087, val_acc:0.981]
Epoch [89/120    avg_loss:0.103, val_acc:0.965]
Epoch [90/120    avg_loss:0.106, val_acc:0.983]
Epoch [91/120    avg_loss:0.107, val_acc:0.969]
Epoch [92/120    avg_loss:0.119, val_acc:0.979]
Epoch [93/120    avg_loss:0.083, val_acc:0.983]
Epoch [94/120    avg_loss:0.066, val_acc:0.975]
Epoch [95/120    avg_loss:0.076, val_acc:0.975]
Epoch [96/120    avg_loss:0.125, val_acc:0.985]
Epoch [97/120    avg_loss:0.075, val_acc:0.981]
Epoch [98/120    avg_loss:0.097, val_acc:0.960]
Epoch [99/120    avg_loss:0.109, val_acc:0.967]
Epoch [100/120    avg_loss:0.153, val_acc:0.979]
Epoch [101/120    avg_loss:0.100, val_acc:0.969]
Epoch [102/120    avg_loss:0.078, val_acc:0.988]
Epoch [103/120    avg_loss:0.073, val_acc:0.990]
Epoch [104/120    avg_loss:0.099, val_acc:0.967]
Epoch [105/120    avg_loss:0.085, val_acc:0.988]
Epoch [106/120    avg_loss:0.075, val_acc:0.988]
Epoch [107/120    avg_loss:0.061, val_acc:0.988]
Epoch [108/120    avg_loss:0.063, val_acc:0.988]
Epoch [109/120    avg_loss:0.102, val_acc:0.990]
Epoch [110/120    avg_loss:0.082, val_acc:0.988]
Epoch [111/120    avg_loss:0.081, val_acc:0.981]
Epoch [112/120    avg_loss:0.088, val_acc:0.981]
Epoch [113/120    avg_loss:0.068, val_acc:0.988]
Epoch [114/120    avg_loss:0.101, val_acc:0.979]
Epoch [115/120    avg_loss:0.130, val_acc:0.973]
Epoch [116/120    avg_loss:0.105, val_acc:0.985]
Epoch [117/120    avg_loss:0.078, val_acc:0.979]
Epoch [118/120    avg_loss:0.058, val_acc:0.975]
Epoch [119/120    avg_loss:0.047, val_acc:0.979]
Epoch [120/120    avg_loss:0.059, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 669   0   0   0   0  16   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0  13 214   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 0.98818316 0.99095023 0.97251586 0.93043478 0.9298893
 0.96261682 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9876596414971018
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb83852aa20>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.626, val_acc:0.129]
Epoch [2/120    avg_loss:2.520, val_acc:0.335]
Epoch [3/120    avg_loss:2.421, val_acc:0.396]
Epoch [4/120    avg_loss:2.341, val_acc:0.358]
Epoch [5/120    avg_loss:2.253, val_acc:0.356]
Epoch [6/120    avg_loss:2.178, val_acc:0.423]
Epoch [7/120    avg_loss:2.094, val_acc:0.496]
Epoch [8/120    avg_loss:1.998, val_acc:0.548]
Epoch [9/120    avg_loss:1.928, val_acc:0.621]
Epoch [10/120    avg_loss:1.824, val_acc:0.688]
Epoch [11/120    avg_loss:1.739, val_acc:0.692]
Epoch [12/120    avg_loss:1.655, val_acc:0.683]
Epoch [13/120    avg_loss:1.543, val_acc:0.702]
Epoch [14/120    avg_loss:1.461, val_acc:0.696]
Epoch [15/120    avg_loss:1.344, val_acc:0.708]
Epoch [16/120    avg_loss:1.278, val_acc:0.733]
Epoch [17/120    avg_loss:1.197, val_acc:0.754]
Epoch [18/120    avg_loss:1.073, val_acc:0.781]
Epoch [19/120    avg_loss:0.979, val_acc:0.773]
Epoch [20/120    avg_loss:0.910, val_acc:0.777]
Epoch [21/120    avg_loss:0.820, val_acc:0.775]
Epoch [22/120    avg_loss:0.745, val_acc:0.779]
Epoch [23/120    avg_loss:0.767, val_acc:0.771]
Epoch [24/120    avg_loss:0.676, val_acc:0.787]
Epoch [25/120    avg_loss:0.638, val_acc:0.796]
Epoch [26/120    avg_loss:0.587, val_acc:0.825]
Epoch [27/120    avg_loss:0.583, val_acc:0.854]
Epoch [28/120    avg_loss:0.595, val_acc:0.787]
Epoch [29/120    avg_loss:0.521, val_acc:0.835]
Epoch [30/120    avg_loss:0.487, val_acc:0.806]
Epoch [31/120    avg_loss:0.514, val_acc:0.896]
Epoch [32/120    avg_loss:0.492, val_acc:0.923]
Epoch [33/120    avg_loss:0.431, val_acc:0.919]
Epoch [34/120    avg_loss:0.425, val_acc:0.892]
Epoch [35/120    avg_loss:0.498, val_acc:0.923]
Epoch [36/120    avg_loss:0.406, val_acc:0.902]
Epoch [37/120    avg_loss:0.375, val_acc:0.942]
Epoch [38/120    avg_loss:0.384, val_acc:0.850]
Epoch [39/120    avg_loss:0.355, val_acc:0.938]
Epoch [40/120    avg_loss:0.384, val_acc:0.940]
Epoch [41/120    avg_loss:0.330, val_acc:0.908]
Epoch [42/120    avg_loss:0.271, val_acc:0.923]
Epoch [43/120    avg_loss:0.320, val_acc:0.948]
Epoch [44/120    avg_loss:0.276, val_acc:0.952]
Epoch [45/120    avg_loss:0.255, val_acc:0.933]
Epoch [46/120    avg_loss:0.259, val_acc:0.958]
Epoch [47/120    avg_loss:0.279, val_acc:0.915]
Epoch [48/120    avg_loss:0.246, val_acc:0.912]
Epoch [49/120    avg_loss:0.241, val_acc:0.950]
Epoch [50/120    avg_loss:0.258, val_acc:0.960]
Epoch [51/120    avg_loss:0.234, val_acc:0.963]
Epoch [52/120    avg_loss:0.247, val_acc:0.952]
Epoch [53/120    avg_loss:0.261, val_acc:0.946]
Epoch [54/120    avg_loss:0.227, val_acc:0.921]
Epoch [55/120    avg_loss:0.205, val_acc:0.929]
Epoch [56/120    avg_loss:0.295, val_acc:0.917]
Epoch [57/120    avg_loss:0.220, val_acc:0.944]
Epoch [58/120    avg_loss:0.202, val_acc:0.958]
Epoch [59/120    avg_loss:0.151, val_acc:0.944]
Epoch [60/120    avg_loss:0.185, val_acc:0.960]
Epoch [61/120    avg_loss:0.178, val_acc:0.952]
Epoch [62/120    avg_loss:0.184, val_acc:0.954]
Epoch [63/120    avg_loss:0.176, val_acc:0.948]
Epoch [64/120    avg_loss:0.194, val_acc:0.954]
Epoch [65/120    avg_loss:0.141, val_acc:0.965]
Epoch [66/120    avg_loss:0.125, val_acc:0.960]
Epoch [67/120    avg_loss:0.108, val_acc:0.965]
Epoch [68/120    avg_loss:0.112, val_acc:0.965]
Epoch [69/120    avg_loss:0.111, val_acc:0.967]
Epoch [70/120    avg_loss:0.124, val_acc:0.971]
Epoch [71/120    avg_loss:0.095, val_acc:0.965]
Epoch [72/120    avg_loss:0.116, val_acc:0.971]
Epoch [73/120    avg_loss:0.108, val_acc:0.969]
Epoch [74/120    avg_loss:0.101, val_acc:0.969]
Epoch [75/120    avg_loss:0.102, val_acc:0.971]
Epoch [76/120    avg_loss:0.119, val_acc:0.971]
Epoch [77/120    avg_loss:0.098, val_acc:0.971]
Epoch [78/120    avg_loss:0.114, val_acc:0.969]
Epoch [79/120    avg_loss:0.106, val_acc:0.969]
Epoch [80/120    avg_loss:0.098, val_acc:0.973]
Epoch [81/120    avg_loss:0.117, val_acc:0.973]
Epoch [82/120    avg_loss:0.088, val_acc:0.973]
Epoch [83/120    avg_loss:0.108, val_acc:0.973]
Epoch [84/120    avg_loss:0.085, val_acc:0.975]
Epoch [85/120    avg_loss:0.110, val_acc:0.973]
Epoch [86/120    avg_loss:0.092, val_acc:0.973]
Epoch [87/120    avg_loss:0.084, val_acc:0.973]
Epoch [88/120    avg_loss:0.089, val_acc:0.973]
Epoch [89/120    avg_loss:0.108, val_acc:0.973]
Epoch [90/120    avg_loss:0.086, val_acc:0.975]
Epoch [91/120    avg_loss:0.091, val_acc:0.973]
Epoch [92/120    avg_loss:0.101, val_acc:0.973]
Epoch [93/120    avg_loss:0.106, val_acc:0.973]
Epoch [94/120    avg_loss:0.093, val_acc:0.979]
Epoch [95/120    avg_loss:0.094, val_acc:0.975]
Epoch [96/120    avg_loss:0.107, val_acc:0.973]
Epoch [97/120    avg_loss:0.100, val_acc:0.977]
Epoch [98/120    avg_loss:0.089, val_acc:0.975]
Epoch [99/120    avg_loss:0.102, val_acc:0.979]
Epoch [100/120    avg_loss:0.092, val_acc:0.979]
Epoch [101/120    avg_loss:0.088, val_acc:0.975]
Epoch [102/120    avg_loss:0.078, val_acc:0.975]
Epoch [103/120    avg_loss:0.096, val_acc:0.973]
Epoch [104/120    avg_loss:0.079, val_acc:0.973]
Epoch [105/120    avg_loss:0.071, val_acc:0.975]
Epoch [106/120    avg_loss:0.085, val_acc:0.975]
Epoch [107/120    avg_loss:0.082, val_acc:0.975]
Epoch [108/120    avg_loss:0.079, val_acc:0.973]
Epoch [109/120    avg_loss:0.092, val_acc:0.973]
Epoch [110/120    avg_loss:0.070, val_acc:0.977]
Epoch [111/120    avg_loss:0.089, val_acc:0.973]
Epoch [112/120    avg_loss:0.088, val_acc:0.975]
Epoch [113/120    avg_loss:0.086, val_acc:0.975]
Epoch [114/120    avg_loss:0.081, val_acc:0.975]
Epoch [115/120    avg_loss:0.074, val_acc:0.975]
Epoch [116/120    avg_loss:0.072, val_acc:0.975]
Epoch [117/120    avg_loss:0.090, val_acc:0.975]
Epoch [118/120    avg_loss:0.077, val_acc:0.975]
Epoch [119/120    avg_loss:0.084, val_acc:0.975]
Epoch [120/120    avg_loss:0.076, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  19   0   0   1   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 0.99412628 0.98871332 1.         0.93243243 0.90301003
 0.98095238 0.9726776  0.998713   1.         1.         0.98562092
 0.9877095  1.        ]

Kappa:
0.9871843982218264
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff20236ca58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.645, val_acc:0.115]
Epoch [2/120    avg_loss:2.529, val_acc:0.275]
Epoch [3/120    avg_loss:2.419, val_acc:0.338]
Epoch [4/120    avg_loss:2.314, val_acc:0.388]
Epoch [5/120    avg_loss:2.212, val_acc:0.423]
Epoch [6/120    avg_loss:2.119, val_acc:0.450]
Epoch [7/120    avg_loss:2.017, val_acc:0.512]
Epoch [8/120    avg_loss:1.936, val_acc:0.571]
Epoch [9/120    avg_loss:1.844, val_acc:0.594]
Epoch [10/120    avg_loss:1.719, val_acc:0.654]
Epoch [11/120    avg_loss:1.617, val_acc:0.654]
Epoch [12/120    avg_loss:1.542, val_acc:0.669]
Epoch [13/120    avg_loss:1.419, val_acc:0.704]
Epoch [14/120    avg_loss:1.368, val_acc:0.719]
Epoch [15/120    avg_loss:1.272, val_acc:0.723]
Epoch [16/120    avg_loss:1.154, val_acc:0.713]
Epoch [17/120    avg_loss:1.133, val_acc:0.729]
Epoch [18/120    avg_loss:1.026, val_acc:0.725]
Epoch [19/120    avg_loss:0.929, val_acc:0.760]
Epoch [20/120    avg_loss:0.866, val_acc:0.804]
Epoch [21/120    avg_loss:0.806, val_acc:0.860]
Epoch [22/120    avg_loss:0.757, val_acc:0.790]
Epoch [23/120    avg_loss:0.746, val_acc:0.825]
Epoch [24/120    avg_loss:0.655, val_acc:0.896]
Epoch [25/120    avg_loss:0.603, val_acc:0.915]
Epoch [26/120    avg_loss:0.566, val_acc:0.912]
Epoch [27/120    avg_loss:0.515, val_acc:0.904]
Epoch [28/120    avg_loss:0.515, val_acc:0.915]
Epoch [29/120    avg_loss:0.492, val_acc:0.915]
Epoch [30/120    avg_loss:0.445, val_acc:0.921]
Epoch [31/120    avg_loss:0.430, val_acc:0.863]
Epoch [32/120    avg_loss:0.508, val_acc:0.915]
Epoch [33/120    avg_loss:0.380, val_acc:0.944]
Epoch [34/120    avg_loss:0.394, val_acc:0.921]
Epoch [35/120    avg_loss:0.374, val_acc:0.946]
Epoch [36/120    avg_loss:0.349, val_acc:0.902]
Epoch [37/120    avg_loss:0.358, val_acc:0.917]
Epoch [38/120    avg_loss:0.335, val_acc:0.942]
Epoch [39/120    avg_loss:0.303, val_acc:0.942]
Epoch [40/120    avg_loss:0.332, val_acc:0.925]
Epoch [41/120    avg_loss:0.322, val_acc:0.944]
Epoch [42/120    avg_loss:0.303, val_acc:0.938]
Epoch [43/120    avg_loss:0.301, val_acc:0.915]
Epoch [44/120    avg_loss:0.336, val_acc:0.940]
Epoch [45/120    avg_loss:0.382, val_acc:0.956]
Epoch [46/120    avg_loss:0.300, val_acc:0.960]
Epoch [47/120    avg_loss:0.263, val_acc:0.946]
Epoch [48/120    avg_loss:0.246, val_acc:0.952]
Epoch [49/120    avg_loss:0.291, val_acc:0.940]
Epoch [50/120    avg_loss:0.255, val_acc:0.948]
Epoch [51/120    avg_loss:0.260, val_acc:0.952]
Epoch [52/120    avg_loss:0.233, val_acc:0.956]
Epoch [53/120    avg_loss:0.210, val_acc:0.965]
Epoch [54/120    avg_loss:0.198, val_acc:0.938]
Epoch [55/120    avg_loss:0.257, val_acc:0.960]
Epoch [56/120    avg_loss:0.189, val_acc:0.956]
Epoch [57/120    avg_loss:0.184, val_acc:0.890]
Epoch [58/120    avg_loss:0.215, val_acc:0.963]
Epoch [59/120    avg_loss:0.196, val_acc:0.952]
Epoch [60/120    avg_loss:0.171, val_acc:0.975]
Epoch [61/120    avg_loss:0.142, val_acc:0.973]
Epoch [62/120    avg_loss:0.158, val_acc:0.969]
Epoch [63/120    avg_loss:0.154, val_acc:0.960]
Epoch [64/120    avg_loss:0.184, val_acc:0.965]
Epoch [65/120    avg_loss:0.179, val_acc:0.938]
Epoch [66/120    avg_loss:0.160, val_acc:0.973]
Epoch [67/120    avg_loss:0.138, val_acc:0.981]
Epoch [68/120    avg_loss:0.162, val_acc:0.946]
Epoch [69/120    avg_loss:0.181, val_acc:0.969]
Epoch [70/120    avg_loss:0.160, val_acc:0.960]
Epoch [71/120    avg_loss:0.127, val_acc:0.979]
Epoch [72/120    avg_loss:0.109, val_acc:0.981]
Epoch [73/120    avg_loss:0.109, val_acc:0.977]
Epoch [74/120    avg_loss:0.168, val_acc:0.981]
Epoch [75/120    avg_loss:0.141, val_acc:0.983]
Epoch [76/120    avg_loss:0.110, val_acc:0.979]
Epoch [77/120    avg_loss:0.106, val_acc:0.948]
Epoch [78/120    avg_loss:0.104, val_acc:0.975]
Epoch [79/120    avg_loss:0.132, val_acc:0.981]
Epoch [80/120    avg_loss:0.107, val_acc:0.985]
Epoch [81/120    avg_loss:0.089, val_acc:0.979]
Epoch [82/120    avg_loss:0.116, val_acc:0.983]
Epoch [83/120    avg_loss:0.087, val_acc:0.981]
Epoch [84/120    avg_loss:0.103, val_acc:0.977]
Epoch [85/120    avg_loss:0.083, val_acc:0.969]
Epoch [86/120    avg_loss:0.095, val_acc:0.977]
Epoch [87/120    avg_loss:0.102, val_acc:0.971]
Epoch [88/120    avg_loss:0.184, val_acc:0.892]
Epoch [89/120    avg_loss:0.161, val_acc:0.948]
Epoch [90/120    avg_loss:0.110, val_acc:0.971]
Epoch [91/120    avg_loss:0.122, val_acc:0.981]
Epoch [92/120    avg_loss:0.093, val_acc:0.960]
Epoch [93/120    avg_loss:0.118, val_acc:0.975]
Epoch [94/120    avg_loss:0.103, val_acc:0.983]
Epoch [95/120    avg_loss:0.062, val_acc:0.983]
Epoch [96/120    avg_loss:0.057, val_acc:0.983]
Epoch [97/120    avg_loss:0.055, val_acc:0.985]
Epoch [98/120    avg_loss:0.061, val_acc:0.985]
Epoch [99/120    avg_loss:0.055, val_acc:0.985]
Epoch [100/120    avg_loss:0.053, val_acc:0.985]
Epoch [101/120    avg_loss:0.053, val_acc:0.985]
Epoch [102/120    avg_loss:0.069, val_acc:0.988]
Epoch [103/120    avg_loss:0.057, val_acc:0.988]
Epoch [104/120    avg_loss:0.051, val_acc:0.992]
Epoch [105/120    avg_loss:0.049, val_acc:0.992]
Epoch [106/120    avg_loss:0.054, val_acc:0.992]
Epoch [107/120    avg_loss:0.056, val_acc:0.990]
Epoch [108/120    avg_loss:0.048, val_acc:0.990]
Epoch [109/120    avg_loss:0.052, val_acc:0.990]
Epoch [110/120    avg_loss:0.053, val_acc:0.990]
Epoch [111/120    avg_loss:0.046, val_acc:0.992]
Epoch [112/120    avg_loss:0.050, val_acc:0.992]
Epoch [113/120    avg_loss:0.045, val_acc:0.992]
Epoch [114/120    avg_loss:0.045, val_acc:0.992]
Epoch [115/120    avg_loss:0.046, val_acc:0.992]
Epoch [116/120    avg_loss:0.046, val_acc:0.992]
Epoch [117/120    avg_loss:0.048, val_acc:0.992]
Epoch [118/120    avg_loss:0.042, val_acc:0.992]
Epoch [119/120    avg_loss:0.057, val_acc:0.992]
Epoch [120/120    avg_loss:0.045, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99412628 0.98871332 1.         0.97747748 0.96666667
 0.98095238 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9945413391465938
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb1273d5a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.114]
Epoch [2/120    avg_loss:2.539, val_acc:0.221]
Epoch [3/120    avg_loss:2.457, val_acc:0.453]
Epoch [4/120    avg_loss:2.377, val_acc:0.451]
Epoch [5/120    avg_loss:2.298, val_acc:0.449]
Epoch [6/120    avg_loss:2.223, val_acc:0.453]
Epoch [7/120    avg_loss:2.149, val_acc:0.475]
Epoch [8/120    avg_loss:2.079, val_acc:0.514]
Epoch [9/120    avg_loss:1.997, val_acc:0.547]
Epoch [10/120    avg_loss:1.910, val_acc:0.594]
Epoch [11/120    avg_loss:1.834, val_acc:0.635]
Epoch [12/120    avg_loss:1.753, val_acc:0.633]
Epoch [13/120    avg_loss:1.667, val_acc:0.656]
Epoch [14/120    avg_loss:1.579, val_acc:0.688]
Epoch [15/120    avg_loss:1.488, val_acc:0.705]
Epoch [16/120    avg_loss:1.399, val_acc:0.713]
Epoch [17/120    avg_loss:1.325, val_acc:0.713]
Epoch [18/120    avg_loss:1.228, val_acc:0.746]
Epoch [19/120    avg_loss:1.166, val_acc:0.811]
Epoch [20/120    avg_loss:1.080, val_acc:0.744]
Epoch [21/120    avg_loss:1.020, val_acc:0.762]
Epoch [22/120    avg_loss:0.940, val_acc:0.777]
Epoch [23/120    avg_loss:0.907, val_acc:0.803]
Epoch [24/120    avg_loss:0.843, val_acc:0.865]
Epoch [25/120    avg_loss:0.810, val_acc:0.871]
Epoch [26/120    avg_loss:0.755, val_acc:0.877]
Epoch [27/120    avg_loss:0.686, val_acc:0.881]
Epoch [28/120    avg_loss:0.643, val_acc:0.885]
Epoch [29/120    avg_loss:0.640, val_acc:0.877]
Epoch [30/120    avg_loss:0.643, val_acc:0.891]
Epoch [31/120    avg_loss:0.578, val_acc:0.893]
Epoch [32/120    avg_loss:0.594, val_acc:0.898]
Epoch [33/120    avg_loss:0.607, val_acc:0.848]
Epoch [34/120    avg_loss:0.499, val_acc:0.904]
Epoch [35/120    avg_loss:0.462, val_acc:0.928]
Epoch [36/120    avg_loss:0.437, val_acc:0.914]
Epoch [37/120    avg_loss:0.413, val_acc:0.924]
Epoch [38/120    avg_loss:0.487, val_acc:0.838]
Epoch [39/120    avg_loss:0.454, val_acc:0.926]
Epoch [40/120    avg_loss:0.377, val_acc:0.918]
Epoch [41/120    avg_loss:0.378, val_acc:0.932]
Epoch [42/120    avg_loss:0.373, val_acc:0.918]
Epoch [43/120    avg_loss:0.356, val_acc:0.945]
Epoch [44/120    avg_loss:0.341, val_acc:0.941]
Epoch [45/120    avg_loss:0.387, val_acc:0.920]
Epoch [46/120    avg_loss:0.322, val_acc:0.941]
Epoch [47/120    avg_loss:0.321, val_acc:0.941]
Epoch [48/120    avg_loss:0.302, val_acc:0.930]
Epoch [49/120    avg_loss:0.283, val_acc:0.945]
Epoch [50/120    avg_loss:0.295, val_acc:0.926]
Epoch [51/120    avg_loss:0.311, val_acc:0.945]
Epoch [52/120    avg_loss:0.250, val_acc:0.945]
Epoch [53/120    avg_loss:0.271, val_acc:0.957]
Epoch [54/120    avg_loss:0.262, val_acc:0.953]
Epoch [55/120    avg_loss:0.304, val_acc:0.938]
Epoch [56/120    avg_loss:0.240, val_acc:0.910]
Epoch [57/120    avg_loss:0.291, val_acc:0.945]
Epoch [58/120    avg_loss:0.251, val_acc:0.934]
Epoch [59/120    avg_loss:0.221, val_acc:0.955]
Epoch [60/120    avg_loss:0.273, val_acc:0.938]
Epoch [61/120    avg_loss:0.222, val_acc:0.951]
Epoch [62/120    avg_loss:0.245, val_acc:0.963]
Epoch [63/120    avg_loss:0.255, val_acc:0.951]
Epoch [64/120    avg_loss:0.220, val_acc:0.959]
Epoch [65/120    avg_loss:0.211, val_acc:0.949]
Epoch [66/120    avg_loss:0.191, val_acc:0.963]
Epoch [67/120    avg_loss:0.153, val_acc:0.965]
Epoch [68/120    avg_loss:0.189, val_acc:0.934]
Epoch [69/120    avg_loss:0.267, val_acc:0.957]
Epoch [70/120    avg_loss:0.237, val_acc:0.938]
Epoch [71/120    avg_loss:0.196, val_acc:0.969]
Epoch [72/120    avg_loss:0.175, val_acc:0.953]
Epoch [73/120    avg_loss:0.196, val_acc:0.963]
Epoch [74/120    avg_loss:0.201, val_acc:0.951]
Epoch [75/120    avg_loss:0.211, val_acc:0.941]
Epoch [76/120    avg_loss:0.221, val_acc:0.953]
Epoch [77/120    avg_loss:0.165, val_acc:0.959]
Epoch [78/120    avg_loss:0.177, val_acc:0.967]
Epoch [79/120    avg_loss:0.175, val_acc:0.951]
Epoch [80/120    avg_loss:0.194, val_acc:0.936]
Epoch [81/120    avg_loss:0.164, val_acc:0.965]
Epoch [82/120    avg_loss:0.164, val_acc:0.959]
Epoch [83/120    avg_loss:0.170, val_acc:0.971]
Epoch [84/120    avg_loss:0.177, val_acc:0.943]
Epoch [85/120    avg_loss:0.202, val_acc:0.961]
Epoch [86/120    avg_loss:0.151, val_acc:0.932]
Epoch [87/120    avg_loss:0.205, val_acc:0.951]
Epoch [88/120    avg_loss:0.134, val_acc:0.957]
Epoch [89/120    avg_loss:0.121, val_acc:0.957]
Epoch [90/120    avg_loss:0.092, val_acc:0.973]
Epoch [91/120    avg_loss:0.090, val_acc:0.969]
Epoch [92/120    avg_loss:0.140, val_acc:0.945]
Epoch [93/120    avg_loss:0.130, val_acc:0.961]
Epoch [94/120    avg_loss:0.101, val_acc:0.973]
Epoch [95/120    avg_loss:0.102, val_acc:0.959]
Epoch [96/120    avg_loss:0.093, val_acc:0.967]
Epoch [97/120    avg_loss:0.067, val_acc:0.977]
Epoch [98/120    avg_loss:0.073, val_acc:0.977]
Epoch [99/120    avg_loss:0.076, val_acc:0.979]
Epoch [100/120    avg_loss:0.123, val_acc:0.916]
Epoch [101/120    avg_loss:0.126, val_acc:0.971]
Epoch [102/120    avg_loss:0.170, val_acc:0.930]
Epoch [103/120    avg_loss:0.190, val_acc:0.951]
Epoch [104/120    avg_loss:0.153, val_acc:0.957]
Epoch [105/120    avg_loss:0.123, val_acc:0.957]
Epoch [106/120    avg_loss:0.098, val_acc:0.975]
Epoch [107/120    avg_loss:0.131, val_acc:0.973]
Epoch [108/120    avg_loss:0.117, val_acc:0.955]
Epoch [109/120    avg_loss:0.096, val_acc:0.973]
Epoch [110/120    avg_loss:0.122, val_acc:0.953]
Epoch [111/120    avg_loss:0.144, val_acc:0.979]
Epoch [112/120    avg_loss:0.115, val_acc:0.973]
Epoch [113/120    avg_loss:0.081, val_acc:0.969]
Epoch [114/120    avg_loss:0.093, val_acc:0.967]
Epoch [115/120    avg_loss:0.078, val_acc:0.965]
Epoch [116/120    avg_loss:0.083, val_acc:0.977]
Epoch [117/120    avg_loss:0.079, val_acc:0.979]
Epoch [118/120    avg_loss:0.069, val_acc:0.971]
Epoch [119/120    avg_loss:0.066, val_acc:0.977]
Epoch [120/120    avg_loss:0.071, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 200   0   0   0   0  19   0   0   0   0   0   0]
 [  0   0   0 201  22   0   0   0   7   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  39 106   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   2  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.88912579957356

F1 scores:
[       nan 1.         0.93896714 0.93271462 0.875      0.83464567
 0.99516908 0.85858586 0.99106003 1.         1.         1.
 1.         1.        ]

Kappa:
0.976496004913105
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8670fcaa20>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.635, val_acc:0.146]
Epoch [2/120    avg_loss:2.527, val_acc:0.223]
Epoch [3/120    avg_loss:2.443, val_acc:0.301]
Epoch [4/120    avg_loss:2.364, val_acc:0.314]
Epoch [5/120    avg_loss:2.303, val_acc:0.352]
Epoch [6/120    avg_loss:2.225, val_acc:0.371]
Epoch [7/120    avg_loss:2.154, val_acc:0.402]
Epoch [8/120    avg_loss:2.082, val_acc:0.434]
Epoch [9/120    avg_loss:2.012, val_acc:0.488]
Epoch [10/120    avg_loss:1.951, val_acc:0.568]
Epoch [11/120    avg_loss:1.876, val_acc:0.574]
Epoch [12/120    avg_loss:1.794, val_acc:0.645]
Epoch [13/120    avg_loss:1.728, val_acc:0.660]
Epoch [14/120    avg_loss:1.655, val_acc:0.680]
Epoch [15/120    avg_loss:1.589, val_acc:0.688]
Epoch [16/120    avg_loss:1.480, val_acc:0.691]
Epoch [17/120    avg_loss:1.402, val_acc:0.713]
Epoch [18/120    avg_loss:1.313, val_acc:0.775]
Epoch [19/120    avg_loss:1.249, val_acc:0.814]
Epoch [20/120    avg_loss:1.179, val_acc:0.816]
Epoch [21/120    avg_loss:1.134, val_acc:0.834]
Epoch [22/120    avg_loss:1.085, val_acc:0.871]
Epoch [23/120    avg_loss:0.975, val_acc:0.873]
Epoch [24/120    avg_loss:0.904, val_acc:0.867]
Epoch [25/120    avg_loss:0.892, val_acc:0.830]
Epoch [26/120    avg_loss:0.832, val_acc:0.895]
Epoch [27/120    avg_loss:0.822, val_acc:0.881]
Epoch [28/120    avg_loss:0.769, val_acc:0.812]
Epoch [29/120    avg_loss:0.768, val_acc:0.832]
Epoch [30/120    avg_loss:0.731, val_acc:0.889]
Epoch [31/120    avg_loss:0.656, val_acc:0.873]
Epoch [32/120    avg_loss:0.618, val_acc:0.885]
Epoch [33/120    avg_loss:0.591, val_acc:0.908]
Epoch [34/120    avg_loss:0.533, val_acc:0.908]
Epoch [35/120    avg_loss:0.519, val_acc:0.916]
Epoch [36/120    avg_loss:0.473, val_acc:0.883]
Epoch [37/120    avg_loss:0.497, val_acc:0.906]
Epoch [38/120    avg_loss:0.455, val_acc:0.932]
Epoch [39/120    avg_loss:0.432, val_acc:0.918]
Epoch [40/120    avg_loss:0.422, val_acc:0.906]
Epoch [41/120    avg_loss:0.439, val_acc:0.881]
Epoch [42/120    avg_loss:0.468, val_acc:0.916]
Epoch [43/120    avg_loss:0.501, val_acc:0.910]
Epoch [44/120    avg_loss:0.419, val_acc:0.928]
Epoch [45/120    avg_loss:0.391, val_acc:0.918]
Epoch [46/120    avg_loss:0.421, val_acc:0.904]
Epoch [47/120    avg_loss:0.406, val_acc:0.920]
Epoch [48/120    avg_loss:0.331, val_acc:0.936]
Epoch [49/120    avg_loss:0.342, val_acc:0.926]
Epoch [50/120    avg_loss:0.388, val_acc:0.908]
Epoch [51/120    avg_loss:0.398, val_acc:0.850]
Epoch [52/120    avg_loss:0.407, val_acc:0.930]
Epoch [53/120    avg_loss:0.342, val_acc:0.932]
Epoch [54/120    avg_loss:0.316, val_acc:0.922]
Epoch [55/120    avg_loss:0.367, val_acc:0.908]
Epoch [56/120    avg_loss:0.291, val_acc:0.934]
Epoch [57/120    avg_loss:0.328, val_acc:0.930]
Epoch [58/120    avg_loss:0.287, val_acc:0.920]
Epoch [59/120    avg_loss:0.328, val_acc:0.943]
Epoch [60/120    avg_loss:0.321, val_acc:0.936]
Epoch [61/120    avg_loss:0.270, val_acc:0.932]
Epoch [62/120    avg_loss:0.308, val_acc:0.926]
Epoch [63/120    avg_loss:0.359, val_acc:0.914]
Epoch [64/120    avg_loss:0.286, val_acc:0.941]
Epoch [65/120    avg_loss:0.260, val_acc:0.932]
Epoch [66/120    avg_loss:0.267, val_acc:0.910]
Epoch [67/120    avg_loss:0.266, val_acc:0.939]
Epoch [68/120    avg_loss:0.240, val_acc:0.949]
Epoch [69/120    avg_loss:0.216, val_acc:0.941]
Epoch [70/120    avg_loss:0.212, val_acc:0.953]
Epoch [71/120    avg_loss:0.211, val_acc:0.918]
Epoch [72/120    avg_loss:0.183, val_acc:0.955]
Epoch [73/120    avg_loss:0.195, val_acc:0.949]
Epoch [74/120    avg_loss:0.217, val_acc:0.953]
Epoch [75/120    avg_loss:0.226, val_acc:0.941]
Epoch [76/120    avg_loss:0.212, val_acc:0.953]
Epoch [77/120    avg_loss:0.179, val_acc:0.951]
Epoch [78/120    avg_loss:0.182, val_acc:0.959]
Epoch [79/120    avg_loss:0.165, val_acc:0.949]
Epoch [80/120    avg_loss:0.198, val_acc:0.941]
Epoch [81/120    avg_loss:0.200, val_acc:0.959]
Epoch [82/120    avg_loss:0.203, val_acc:0.936]
Epoch [83/120    avg_loss:0.193, val_acc:0.941]
Epoch [84/120    avg_loss:0.191, val_acc:0.947]
Epoch [85/120    avg_loss:0.185, val_acc:0.938]
Epoch [86/120    avg_loss:0.183, val_acc:0.914]
Epoch [87/120    avg_loss:0.178, val_acc:0.900]
Epoch [88/120    avg_loss:0.260, val_acc:0.951]
Epoch [89/120    avg_loss:0.170, val_acc:0.957]
Epoch [90/120    avg_loss:0.158, val_acc:0.971]
Epoch [91/120    avg_loss:0.147, val_acc:0.959]
Epoch [92/120    avg_loss:0.156, val_acc:0.959]
Epoch [93/120    avg_loss:0.173, val_acc:0.957]
Epoch [94/120    avg_loss:0.125, val_acc:0.971]
Epoch [95/120    avg_loss:0.153, val_acc:0.943]
Epoch [96/120    avg_loss:0.186, val_acc:0.955]
Epoch [97/120    avg_loss:0.150, val_acc:0.955]
Epoch [98/120    avg_loss:0.141, val_acc:0.963]
Epoch [99/120    avg_loss:0.144, val_acc:0.965]
Epoch [100/120    avg_loss:0.158, val_acc:0.945]
Epoch [101/120    avg_loss:0.121, val_acc:0.973]
Epoch [102/120    avg_loss:0.103, val_acc:0.973]
Epoch [103/120    avg_loss:0.091, val_acc:0.973]
Epoch [104/120    avg_loss:0.112, val_acc:0.957]
Epoch [105/120    avg_loss:0.244, val_acc:0.939]
Epoch [106/120    avg_loss:0.164, val_acc:0.959]
Epoch [107/120    avg_loss:0.138, val_acc:0.969]
Epoch [108/120    avg_loss:0.116, val_acc:0.967]
Epoch [109/120    avg_loss:0.103, val_acc:0.973]
Epoch [110/120    avg_loss:0.082, val_acc:0.969]
Epoch [111/120    avg_loss:0.093, val_acc:0.975]
Epoch [112/120    avg_loss:0.113, val_acc:0.961]
Epoch [113/120    avg_loss:0.114, val_acc:0.963]
Epoch [114/120    avg_loss:0.108, val_acc:0.973]
Epoch [115/120    avg_loss:0.098, val_acc:0.969]
Epoch [116/120    avg_loss:0.120, val_acc:0.961]
Epoch [117/120    avg_loss:0.108, val_acc:0.973]
Epoch [118/120    avg_loss:0.095, val_acc:0.973]
Epoch [119/120    avg_loss:0.084, val_acc:0.977]
Epoch [120/120    avg_loss:0.105, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 195   0   0   0   0  24   0   0   0   0   0   0]
 [  0   0   1 219   3   0   0   2   2   3   0   0   0   0]
 [  0   0   0   1 161  65   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   0   0   0   0   0   0   4   0   2   0 447   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0   0 832]]

Accuracy:
97.16417910447761

F1 scores:
[       nan 1.         0.90277778 0.97333333 0.81108312 0.7965616
 1.         0.80597015 0.98974359 0.99680511 0.99726027 0.99600533
 0.99003322 0.99879952]

Kappa:
0.9684398476458779
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f283134fa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.305]
Epoch [2/120    avg_loss:2.519, val_acc:0.445]
Epoch [3/120    avg_loss:2.418, val_acc:0.422]
Epoch [4/120    avg_loss:2.345, val_acc:0.430]
Epoch [5/120    avg_loss:2.277, val_acc:0.465]
Epoch [6/120    avg_loss:2.209, val_acc:0.479]
Epoch [7/120    avg_loss:2.143, val_acc:0.506]
Epoch [8/120    avg_loss:2.065, val_acc:0.518]
Epoch [9/120    avg_loss:2.005, val_acc:0.541]
Epoch [10/120    avg_loss:1.905, val_acc:0.582]
Epoch [11/120    avg_loss:1.816, val_acc:0.605]
Epoch [12/120    avg_loss:1.736, val_acc:0.656]
Epoch [13/120    avg_loss:1.642, val_acc:0.711]
Epoch [14/120    avg_loss:1.552, val_acc:0.750]
Epoch [15/120    avg_loss:1.474, val_acc:0.773]
Epoch [16/120    avg_loss:1.378, val_acc:0.795]
Epoch [17/120    avg_loss:1.297, val_acc:0.779]
Epoch [18/120    avg_loss:1.231, val_acc:0.836]
Epoch [19/120    avg_loss:1.201, val_acc:0.852]
Epoch [20/120    avg_loss:1.113, val_acc:0.887]
Epoch [21/120    avg_loss:1.053, val_acc:0.865]
Epoch [22/120    avg_loss:0.980, val_acc:0.844]
Epoch [23/120    avg_loss:0.884, val_acc:0.887]
Epoch [24/120    avg_loss:0.841, val_acc:0.887]
Epoch [25/120    avg_loss:0.775, val_acc:0.898]
Epoch [26/120    avg_loss:0.739, val_acc:0.908]
Epoch [27/120    avg_loss:0.690, val_acc:0.879]
Epoch [28/120    avg_loss:0.684, val_acc:0.896]
Epoch [29/120    avg_loss:0.667, val_acc:0.906]
Epoch [30/120    avg_loss:0.596, val_acc:0.906]
Epoch [31/120    avg_loss:0.592, val_acc:0.820]
Epoch [32/120    avg_loss:0.571, val_acc:0.912]
Epoch [33/120    avg_loss:0.518, val_acc:0.908]
Epoch [34/120    avg_loss:0.489, val_acc:0.916]
Epoch [35/120    avg_loss:0.547, val_acc:0.910]
Epoch [36/120    avg_loss:0.488, val_acc:0.826]
Epoch [37/120    avg_loss:0.503, val_acc:0.924]
Epoch [38/120    avg_loss:0.464, val_acc:0.875]
Epoch [39/120    avg_loss:0.432, val_acc:0.873]
Epoch [40/120    avg_loss:0.459, val_acc:0.865]
Epoch [41/120    avg_loss:0.424, val_acc:0.896]
Epoch [42/120    avg_loss:0.404, val_acc:0.924]
Epoch [43/120    avg_loss:0.454, val_acc:0.922]
Epoch [44/120    avg_loss:0.446, val_acc:0.881]
Epoch [45/120    avg_loss:0.427, val_acc:0.922]
Epoch [46/120    avg_loss:0.380, val_acc:0.941]
Epoch [47/120    avg_loss:0.345, val_acc:0.938]
Epoch [48/120    avg_loss:0.354, val_acc:0.939]
Epoch [49/120    avg_loss:0.320, val_acc:0.930]
Epoch [50/120    avg_loss:0.323, val_acc:0.938]
Epoch [51/120    avg_loss:0.333, val_acc:0.936]
Epoch [52/120    avg_loss:0.323, val_acc:0.881]
Epoch [53/120    avg_loss:0.332, val_acc:0.904]
Epoch [54/120    avg_loss:0.341, val_acc:0.930]
Epoch [55/120    avg_loss:0.323, val_acc:0.920]
Epoch [56/120    avg_loss:0.330, val_acc:0.910]
Epoch [57/120    avg_loss:0.276, val_acc:0.938]
Epoch [58/120    avg_loss:0.263, val_acc:0.936]
Epoch [59/120    avg_loss:0.252, val_acc:0.930]
Epoch [60/120    avg_loss:0.259, val_acc:0.951]
Epoch [61/120    avg_loss:0.204, val_acc:0.949]
Epoch [62/120    avg_loss:0.206, val_acc:0.953]
Epoch [63/120    avg_loss:0.207, val_acc:0.955]
Epoch [64/120    avg_loss:0.183, val_acc:0.959]
Epoch [65/120    avg_loss:0.196, val_acc:0.957]
Epoch [66/120    avg_loss:0.203, val_acc:0.961]
Epoch [67/120    avg_loss:0.181, val_acc:0.957]
Epoch [68/120    avg_loss:0.181, val_acc:0.961]
Epoch [69/120    avg_loss:0.179, val_acc:0.965]
Epoch [70/120    avg_loss:0.170, val_acc:0.963]
Epoch [71/120    avg_loss:0.174, val_acc:0.961]
Epoch [72/120    avg_loss:0.163, val_acc:0.963]
Epoch [73/120    avg_loss:0.170, val_acc:0.963]
Epoch [74/120    avg_loss:0.165, val_acc:0.963]
Epoch [75/120    avg_loss:0.163, val_acc:0.963]
Epoch [76/120    avg_loss:0.163, val_acc:0.963]
Epoch [77/120    avg_loss:0.167, val_acc:0.961]
Epoch [78/120    avg_loss:0.174, val_acc:0.965]
Epoch [79/120    avg_loss:0.164, val_acc:0.967]
Epoch [80/120    avg_loss:0.190, val_acc:0.963]
Epoch [81/120    avg_loss:0.179, val_acc:0.961]
Epoch [82/120    avg_loss:0.163, val_acc:0.967]
Epoch [83/120    avg_loss:0.154, val_acc:0.969]
Epoch [84/120    avg_loss:0.155, val_acc:0.965]
Epoch [85/120    avg_loss:0.146, val_acc:0.967]
Epoch [86/120    avg_loss:0.157, val_acc:0.967]
Epoch [87/120    avg_loss:0.144, val_acc:0.961]
Epoch [88/120    avg_loss:0.145, val_acc:0.967]
Epoch [89/120    avg_loss:0.150, val_acc:0.963]
Epoch [90/120    avg_loss:0.165, val_acc:0.967]
Epoch [91/120    avg_loss:0.144, val_acc:0.967]
Epoch [92/120    avg_loss:0.147, val_acc:0.967]
Epoch [93/120    avg_loss:0.164, val_acc:0.967]
Epoch [94/120    avg_loss:0.146, val_acc:0.971]
Epoch [95/120    avg_loss:0.141, val_acc:0.969]
Epoch [96/120    avg_loss:0.139, val_acc:0.969]
Epoch [97/120    avg_loss:0.138, val_acc:0.971]
Epoch [98/120    avg_loss:0.142, val_acc:0.973]
Epoch [99/120    avg_loss:0.133, val_acc:0.967]
Epoch [100/120    avg_loss:0.130, val_acc:0.967]
Epoch [101/120    avg_loss:0.137, val_acc:0.967]
Epoch [102/120    avg_loss:0.154, val_acc:0.969]
Epoch [103/120    avg_loss:0.120, val_acc:0.973]
Epoch [104/120    avg_loss:0.134, val_acc:0.965]
Epoch [105/120    avg_loss:0.132, val_acc:0.963]
Epoch [106/120    avg_loss:0.134, val_acc:0.973]
Epoch [107/120    avg_loss:0.130, val_acc:0.969]
Epoch [108/120    avg_loss:0.131, val_acc:0.963]
Epoch [109/120    avg_loss:0.119, val_acc:0.969]
Epoch [110/120    avg_loss:0.117, val_acc:0.967]
Epoch [111/120    avg_loss:0.148, val_acc:0.965]
Epoch [112/120    avg_loss:0.132, val_acc:0.971]
Epoch [113/120    avg_loss:0.126, val_acc:0.967]
Epoch [114/120    avg_loss:0.133, val_acc:0.971]
Epoch [115/120    avg_loss:0.123, val_acc:0.963]
Epoch [116/120    avg_loss:0.119, val_acc:0.973]
Epoch [117/120    avg_loss:0.153, val_acc:0.975]
Epoch [118/120    avg_loss:0.127, val_acc:0.967]
Epoch [119/120    avg_loss:0.106, val_acc:0.967]
Epoch [120/120    avg_loss:0.127, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 214   7   0   0   0   8   1   0   0   0   0]
 [  0   0   0   2 205  20   0   0   0   0   0   0   0   0]
 [  0   0   0  14  18 113   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   1  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 373   4   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.01705756929637

F1 scores:
[       nan 1.         0.96017699 0.93043478 0.89715536 0.81294964
 0.99757869 0.89017341 0.98979592 0.99893276 1.         0.99466667
 0.9956044  1.        ]

Kappa:
0.9779182952115906
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fae3e8c89b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.168]
Epoch [2/120    avg_loss:2.534, val_acc:0.355]
Epoch [3/120    avg_loss:2.439, val_acc:0.383]
Epoch [4/120    avg_loss:2.358, val_acc:0.365]
Epoch [5/120    avg_loss:2.292, val_acc:0.375]
Epoch [6/120    avg_loss:2.216, val_acc:0.426]
Epoch [7/120    avg_loss:2.162, val_acc:0.486]
Epoch [8/120    avg_loss:2.088, val_acc:0.512]
Epoch [9/120    avg_loss:2.016, val_acc:0.533]
Epoch [10/120    avg_loss:1.946, val_acc:0.562]
Epoch [11/120    avg_loss:1.882, val_acc:0.615]
Epoch [12/120    avg_loss:1.813, val_acc:0.650]
Epoch [13/120    avg_loss:1.754, val_acc:0.656]
Epoch [14/120    avg_loss:1.661, val_acc:0.652]
Epoch [15/120    avg_loss:1.601, val_acc:0.711]
Epoch [16/120    avg_loss:1.548, val_acc:0.684]
Epoch [17/120    avg_loss:1.446, val_acc:0.752]
Epoch [18/120    avg_loss:1.380, val_acc:0.740]
Epoch [19/120    avg_loss:1.306, val_acc:0.779]
Epoch [20/120    avg_loss:1.234, val_acc:0.799]
Epoch [21/120    avg_loss:1.152, val_acc:0.689]
Epoch [22/120    avg_loss:1.086, val_acc:0.854]
Epoch [23/120    avg_loss:1.030, val_acc:0.857]
Epoch [24/120    avg_loss:0.987, val_acc:0.871]
Epoch [25/120    avg_loss:0.909, val_acc:0.871]
Epoch [26/120    avg_loss:0.861, val_acc:0.887]
Epoch [27/120    avg_loss:0.832, val_acc:0.887]
Epoch [28/120    avg_loss:0.753, val_acc:0.900]
Epoch [29/120    avg_loss:0.761, val_acc:0.896]
Epoch [30/120    avg_loss:0.700, val_acc:0.875]
Epoch [31/120    avg_loss:0.693, val_acc:0.896]
Epoch [32/120    avg_loss:0.640, val_acc:0.898]
Epoch [33/120    avg_loss:0.664, val_acc:0.900]
Epoch [34/120    avg_loss:0.588, val_acc:0.900]
Epoch [35/120    avg_loss:0.556, val_acc:0.912]
Epoch [36/120    avg_loss:0.579, val_acc:0.910]
Epoch [37/120    avg_loss:0.538, val_acc:0.879]
Epoch [38/120    avg_loss:0.493, val_acc:0.914]
Epoch [39/120    avg_loss:0.439, val_acc:0.908]
Epoch [40/120    avg_loss:0.465, val_acc:0.896]
Epoch [41/120    avg_loss:0.431, val_acc:0.924]
Epoch [42/120    avg_loss:0.405, val_acc:0.902]
Epoch [43/120    avg_loss:0.418, val_acc:0.908]
Epoch [44/120    avg_loss:0.416, val_acc:0.922]
Epoch [45/120    avg_loss:0.363, val_acc:0.916]
Epoch [46/120    avg_loss:0.398, val_acc:0.914]
Epoch [47/120    avg_loss:0.326, val_acc:0.924]
Epoch [48/120    avg_loss:0.404, val_acc:0.902]
Epoch [49/120    avg_loss:0.385, val_acc:0.900]
Epoch [50/120    avg_loss:0.329, val_acc:0.930]
Epoch [51/120    avg_loss:0.322, val_acc:0.945]
Epoch [52/120    avg_loss:0.334, val_acc:0.904]
Epoch [53/120    avg_loss:0.370, val_acc:0.936]
Epoch [54/120    avg_loss:0.375, val_acc:0.916]
Epoch [55/120    avg_loss:0.314, val_acc:0.930]
Epoch [56/120    avg_loss:0.314, val_acc:0.934]
Epoch [57/120    avg_loss:0.326, val_acc:0.896]
Epoch [58/120    avg_loss:0.363, val_acc:0.885]
Epoch [59/120    avg_loss:0.296, val_acc:0.928]
Epoch [60/120    avg_loss:0.248, val_acc:0.938]
Epoch [61/120    avg_loss:0.271, val_acc:0.914]
Epoch [62/120    avg_loss:0.263, val_acc:0.939]
Epoch [63/120    avg_loss:0.230, val_acc:0.941]
Epoch [64/120    avg_loss:0.238, val_acc:0.938]
Epoch [65/120    avg_loss:0.207, val_acc:0.949]
Epoch [66/120    avg_loss:0.182, val_acc:0.953]
Epoch [67/120    avg_loss:0.172, val_acc:0.953]
Epoch [68/120    avg_loss:0.167, val_acc:0.953]
Epoch [69/120    avg_loss:0.168, val_acc:0.953]
Epoch [70/120    avg_loss:0.165, val_acc:0.953]
Epoch [71/120    avg_loss:0.160, val_acc:0.953]
Epoch [72/120    avg_loss:0.159, val_acc:0.959]
Epoch [73/120    avg_loss:0.161, val_acc:0.959]
Epoch [74/120    avg_loss:0.147, val_acc:0.959]
Epoch [75/120    avg_loss:0.161, val_acc:0.957]
Epoch [76/120    avg_loss:0.170, val_acc:0.959]
Epoch [77/120    avg_loss:0.167, val_acc:0.955]
Epoch [78/120    avg_loss:0.147, val_acc:0.957]
Epoch [79/120    avg_loss:0.158, val_acc:0.957]
Epoch [80/120    avg_loss:0.139, val_acc:0.959]
Epoch [81/120    avg_loss:0.142, val_acc:0.957]
Epoch [82/120    avg_loss:0.144, val_acc:0.953]
Epoch [83/120    avg_loss:0.145, val_acc:0.959]
Epoch [84/120    avg_loss:0.161, val_acc:0.959]
Epoch [85/120    avg_loss:0.151, val_acc:0.961]
Epoch [86/120    avg_loss:0.151, val_acc:0.959]
Epoch [87/120    avg_loss:0.150, val_acc:0.957]
Epoch [88/120    avg_loss:0.137, val_acc:0.961]
Epoch [89/120    avg_loss:0.145, val_acc:0.959]
Epoch [90/120    avg_loss:0.150, val_acc:0.963]
Epoch [91/120    avg_loss:0.144, val_acc:0.963]
Epoch [92/120    avg_loss:0.131, val_acc:0.961]
Epoch [93/120    avg_loss:0.149, val_acc:0.959]
Epoch [94/120    avg_loss:0.156, val_acc:0.967]
Epoch [95/120    avg_loss:0.147, val_acc:0.965]
Epoch [96/120    avg_loss:0.124, val_acc:0.967]
Epoch [97/120    avg_loss:0.135, val_acc:0.959]
Epoch [98/120    avg_loss:0.158, val_acc:0.959]
Epoch [99/120    avg_loss:0.138, val_acc:0.965]
Epoch [100/120    avg_loss:0.128, val_acc:0.961]
Epoch [101/120    avg_loss:0.148, val_acc:0.963]
Epoch [102/120    avg_loss:0.128, val_acc:0.965]
Epoch [103/120    avg_loss:0.133, val_acc:0.963]
Epoch [104/120    avg_loss:0.134, val_acc:0.963]
Epoch [105/120    avg_loss:0.119, val_acc:0.961]
Epoch [106/120    avg_loss:0.118, val_acc:0.969]
Epoch [107/120    avg_loss:0.125, val_acc:0.959]
Epoch [108/120    avg_loss:0.133, val_acc:0.961]
Epoch [109/120    avg_loss:0.114, val_acc:0.967]
Epoch [110/120    avg_loss:0.119, val_acc:0.965]
Epoch [111/120    avg_loss:0.123, val_acc:0.967]
Epoch [112/120    avg_loss:0.129, val_acc:0.965]
Epoch [113/120    avg_loss:0.119, val_acc:0.967]
Epoch [114/120    avg_loss:0.117, val_acc:0.963]
Epoch [115/120    avg_loss:0.114, val_acc:0.959]
Epoch [116/120    avg_loss:0.117, val_acc:0.963]
Epoch [117/120    avg_loss:0.114, val_acc:0.961]
Epoch [118/120    avg_loss:0.128, val_acc:0.963]
Epoch [119/120    avg_loss:0.115, val_acc:0.967]
Epoch [120/120    avg_loss:0.104, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 203   0   0   0   0  16   0   0   0   0   0   0]
 [  0   0   0 216   3   0   0   2   9   0   0   0   0   0]
 [  0   0   0   1 203  22   0   0   1   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.80383795309169

F1 scores:
[       nan 1.         0.91855204 0.96644295 0.8826087  0.83216783
 0.99756691 0.81481481 0.98339719 1.         1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9755479477779849
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4023402ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.084]
Epoch [2/120    avg_loss:2.524, val_acc:0.234]
Epoch [3/120    avg_loss:2.450, val_acc:0.271]
Epoch [4/120    avg_loss:2.392, val_acc:0.363]
Epoch [5/120    avg_loss:2.334, val_acc:0.463]
Epoch [6/120    avg_loss:2.279, val_acc:0.484]
Epoch [7/120    avg_loss:2.210, val_acc:0.516]
Epoch [8/120    avg_loss:2.150, val_acc:0.525]
Epoch [9/120    avg_loss:2.070, val_acc:0.537]
Epoch [10/120    avg_loss:1.998, val_acc:0.527]
Epoch [11/120    avg_loss:1.913, val_acc:0.541]
Epoch [12/120    avg_loss:1.811, val_acc:0.551]
Epoch [13/120    avg_loss:1.710, val_acc:0.555]
Epoch [14/120    avg_loss:1.632, val_acc:0.559]
Epoch [15/120    avg_loss:1.578, val_acc:0.572]
Epoch [16/120    avg_loss:1.481, val_acc:0.615]
Epoch [17/120    avg_loss:1.405, val_acc:0.637]
Epoch [18/120    avg_loss:1.354, val_acc:0.672]
Epoch [19/120    avg_loss:1.317, val_acc:0.662]
Epoch [20/120    avg_loss:1.262, val_acc:0.670]
Epoch [21/120    avg_loss:1.194, val_acc:0.719]
Epoch [22/120    avg_loss:1.112, val_acc:0.715]
Epoch [23/120    avg_loss:1.091, val_acc:0.754]
Epoch [24/120    avg_loss:1.031, val_acc:0.748]
Epoch [25/120    avg_loss:1.021, val_acc:0.748]
Epoch [26/120    avg_loss:0.989, val_acc:0.775]
Epoch [27/120    avg_loss:0.917, val_acc:0.803]
Epoch [28/120    avg_loss:0.906, val_acc:0.773]
Epoch [29/120    avg_loss:0.834, val_acc:0.846]
Epoch [30/120    avg_loss:0.805, val_acc:0.859]
Epoch [31/120    avg_loss:0.805, val_acc:0.801]
Epoch [32/120    avg_loss:0.767, val_acc:0.875]
Epoch [33/120    avg_loss:0.717, val_acc:0.840]
Epoch [34/120    avg_loss:0.705, val_acc:0.879]
Epoch [35/120    avg_loss:0.662, val_acc:0.850]
Epoch [36/120    avg_loss:0.656, val_acc:0.844]
Epoch [37/120    avg_loss:0.637, val_acc:0.883]
Epoch [38/120    avg_loss:0.598, val_acc:0.893]
Epoch [39/120    avg_loss:0.568, val_acc:0.850]
Epoch [40/120    avg_loss:0.554, val_acc:0.887]
Epoch [41/120    avg_loss:0.571, val_acc:0.885]
Epoch [42/120    avg_loss:0.503, val_acc:0.906]
Epoch [43/120    avg_loss:0.547, val_acc:0.891]
Epoch [44/120    avg_loss:0.510, val_acc:0.875]
Epoch [45/120    avg_loss:0.507, val_acc:0.910]
Epoch [46/120    avg_loss:0.437, val_acc:0.904]
Epoch [47/120    avg_loss:0.456, val_acc:0.904]
Epoch [48/120    avg_loss:0.453, val_acc:0.898]
Epoch [49/120    avg_loss:0.435, val_acc:0.926]
Epoch [50/120    avg_loss:0.407, val_acc:0.928]
Epoch [51/120    avg_loss:0.393, val_acc:0.914]
Epoch [52/120    avg_loss:0.370, val_acc:0.918]
Epoch [53/120    avg_loss:0.373, val_acc:0.900]
Epoch [54/120    avg_loss:0.409, val_acc:0.922]
Epoch [55/120    avg_loss:0.389, val_acc:0.922]
Epoch [56/120    avg_loss:0.353, val_acc:0.922]
Epoch [57/120    avg_loss:0.353, val_acc:0.916]
Epoch [58/120    avg_loss:0.335, val_acc:0.920]
Epoch [59/120    avg_loss:0.340, val_acc:0.926]
Epoch [60/120    avg_loss:0.290, val_acc:0.930]
Epoch [61/120    avg_loss:0.357, val_acc:0.922]
Epoch [62/120    avg_loss:0.310, val_acc:0.928]
Epoch [63/120    avg_loss:0.299, val_acc:0.928]
Epoch [64/120    avg_loss:0.307, val_acc:0.900]
Epoch [65/120    avg_loss:0.352, val_acc:0.906]
Epoch [66/120    avg_loss:0.307, val_acc:0.938]
Epoch [67/120    avg_loss:0.335, val_acc:0.932]
Epoch [68/120    avg_loss:0.314, val_acc:0.932]
Epoch [69/120    avg_loss:0.283, val_acc:0.934]
Epoch [70/120    avg_loss:0.292, val_acc:0.936]
Epoch [71/120    avg_loss:0.345, val_acc:0.918]
Epoch [72/120    avg_loss:0.309, val_acc:0.945]
Epoch [73/120    avg_loss:0.297, val_acc:0.930]
Epoch [74/120    avg_loss:0.248, val_acc:0.939]
Epoch [75/120    avg_loss:0.237, val_acc:0.928]
Epoch [76/120    avg_loss:0.219, val_acc:0.938]
Epoch [77/120    avg_loss:0.229, val_acc:0.951]
Epoch [78/120    avg_loss:0.193, val_acc:0.939]
Epoch [79/120    avg_loss:0.196, val_acc:0.941]
Epoch [80/120    avg_loss:0.235, val_acc:0.924]
Epoch [81/120    avg_loss:0.223, val_acc:0.943]
Epoch [82/120    avg_loss:0.205, val_acc:0.932]
Epoch [83/120    avg_loss:0.274, val_acc:0.918]
Epoch [84/120    avg_loss:0.266, val_acc:0.945]
Epoch [85/120    avg_loss:0.227, val_acc:0.951]
Epoch [86/120    avg_loss:0.220, val_acc:0.953]
Epoch [87/120    avg_loss:0.205, val_acc:0.938]
Epoch [88/120    avg_loss:0.201, val_acc:0.953]
Epoch [89/120    avg_loss:0.171, val_acc:0.957]
Epoch [90/120    avg_loss:0.153, val_acc:0.955]
Epoch [91/120    avg_loss:0.216, val_acc:0.932]
Epoch [92/120    avg_loss:0.243, val_acc:0.914]
Epoch [93/120    avg_loss:0.247, val_acc:0.955]
Epoch [94/120    avg_loss:0.174, val_acc:0.949]
Epoch [95/120    avg_loss:0.233, val_acc:0.938]
Epoch [96/120    avg_loss:0.208, val_acc:0.939]
Epoch [97/120    avg_loss:0.190, val_acc:0.957]
Epoch [98/120    avg_loss:0.180, val_acc:0.951]
Epoch [99/120    avg_loss:0.170, val_acc:0.967]
Epoch [100/120    avg_loss:0.202, val_acc:0.875]
Epoch [101/120    avg_loss:0.190, val_acc:0.943]
Epoch [102/120    avg_loss:0.172, val_acc:0.951]
Epoch [103/120    avg_loss:0.180, val_acc:0.959]
Epoch [104/120    avg_loss:0.149, val_acc:0.967]
Epoch [105/120    avg_loss:0.118, val_acc:0.967]
Epoch [106/120    avg_loss:0.134, val_acc:0.959]
Epoch [107/120    avg_loss:0.136, val_acc:0.967]
Epoch [108/120    avg_loss:0.126, val_acc:0.959]
Epoch [109/120    avg_loss:0.099, val_acc:0.969]
Epoch [110/120    avg_loss:0.137, val_acc:0.943]
Epoch [111/120    avg_loss:0.164, val_acc:0.963]
Epoch [112/120    avg_loss:0.156, val_acc:0.973]
Epoch [113/120    avg_loss:0.189, val_acc:0.943]
Epoch [114/120    avg_loss:0.173, val_acc:0.949]
Epoch [115/120    avg_loss:0.196, val_acc:0.957]
Epoch [116/120    avg_loss:0.163, val_acc:0.961]
Epoch [117/120    avg_loss:0.132, val_acc:0.963]
Epoch [118/120    avg_loss:0.128, val_acc:0.961]
Epoch [119/120    avg_loss:0.173, val_acc:0.955]
Epoch [120/120    avg_loss:0.116, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 217   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 200   9   0   0   0  19   0   0   0   0   0]
 [  0   0   2   0 136  88   0   0   1   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0  81   0   0  22  26  77   0   0   0   0   0   0   0]
 [  0   0  31   0   0   0   0  63   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0   0 372   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
93.39019189765459

F1 scores:
[       nan 0.94287681 0.89117043 0.93023256 0.67160494 0.68193384
 0.54416961 0.80254777 0.95384615 1.         1.         1.
 1.         1.        ]

Kappa:
0.92627531684018
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4888b89b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.148]
Epoch [2/120    avg_loss:2.547, val_acc:0.144]
Epoch [3/120    avg_loss:2.471, val_acc:0.325]
Epoch [4/120    avg_loss:2.395, val_acc:0.356]
Epoch [5/120    avg_loss:2.329, val_acc:0.392]
Epoch [6/120    avg_loss:2.269, val_acc:0.463]
Epoch [7/120    avg_loss:2.205, val_acc:0.510]
Epoch [8/120    avg_loss:2.133, val_acc:0.560]
Epoch [9/120    avg_loss:2.051, val_acc:0.625]
Epoch [10/120    avg_loss:1.967, val_acc:0.669]
Epoch [11/120    avg_loss:1.878, val_acc:0.769]
Epoch [12/120    avg_loss:1.801, val_acc:0.787]
Epoch [13/120    avg_loss:1.694, val_acc:0.781]
Epoch [14/120    avg_loss:1.596, val_acc:0.812]
Epoch [15/120    avg_loss:1.494, val_acc:0.790]
Epoch [16/120    avg_loss:1.410, val_acc:0.812]
Epoch [17/120    avg_loss:1.328, val_acc:0.815]
Epoch [18/120    avg_loss:1.284, val_acc:0.810]
Epoch [19/120    avg_loss:1.162, val_acc:0.833]
Epoch [20/120    avg_loss:1.065, val_acc:0.848]
Epoch [21/120    avg_loss:0.975, val_acc:0.852]
Epoch [22/120    avg_loss:0.925, val_acc:0.873]
Epoch [23/120    avg_loss:0.911, val_acc:0.887]
Epoch [24/120    avg_loss:0.841, val_acc:0.885]
Epoch [25/120    avg_loss:0.749, val_acc:0.887]
Epoch [26/120    avg_loss:0.733, val_acc:0.904]
Epoch [27/120    avg_loss:0.682, val_acc:0.910]
Epoch [28/120    avg_loss:0.684, val_acc:0.910]
Epoch [29/120    avg_loss:0.651, val_acc:0.908]
Epoch [30/120    avg_loss:0.620, val_acc:0.883]
Epoch [31/120    avg_loss:0.634, val_acc:0.838]
Epoch [32/120    avg_loss:0.597, val_acc:0.915]
Epoch [33/120    avg_loss:0.530, val_acc:0.900]
Epoch [34/120    avg_loss:0.517, val_acc:0.900]
Epoch [35/120    avg_loss:0.497, val_acc:0.900]
Epoch [36/120    avg_loss:0.467, val_acc:0.915]
Epoch [37/120    avg_loss:0.446, val_acc:0.910]
Epoch [38/120    avg_loss:0.419, val_acc:0.925]
Epoch [39/120    avg_loss:0.406, val_acc:0.927]
Epoch [40/120    avg_loss:0.411, val_acc:0.931]
Epoch [41/120    avg_loss:0.427, val_acc:0.942]
Epoch [42/120    avg_loss:0.358, val_acc:0.917]
Epoch [43/120    avg_loss:0.367, val_acc:0.944]
Epoch [44/120    avg_loss:0.376, val_acc:0.940]
Epoch [45/120    avg_loss:0.386, val_acc:0.925]
Epoch [46/120    avg_loss:0.370, val_acc:0.954]
Epoch [47/120    avg_loss:0.362, val_acc:0.931]
Epoch [48/120    avg_loss:0.340, val_acc:0.950]
Epoch [49/120    avg_loss:0.333, val_acc:0.931]
Epoch [50/120    avg_loss:0.306, val_acc:0.908]
Epoch [51/120    avg_loss:0.343, val_acc:0.954]
Epoch [52/120    avg_loss:0.287, val_acc:0.938]
Epoch [53/120    avg_loss:0.323, val_acc:0.952]
Epoch [54/120    avg_loss:0.298, val_acc:0.952]
Epoch [55/120    avg_loss:0.235, val_acc:0.942]
Epoch [56/120    avg_loss:0.249, val_acc:0.908]
Epoch [57/120    avg_loss:0.321, val_acc:0.946]
Epoch [58/120    avg_loss:0.258, val_acc:0.952]
Epoch [59/120    avg_loss:0.245, val_acc:0.948]
Epoch [60/120    avg_loss:0.262, val_acc:0.956]
Epoch [61/120    avg_loss:0.228, val_acc:0.956]
Epoch [62/120    avg_loss:0.224, val_acc:0.938]
Epoch [63/120    avg_loss:0.234, val_acc:0.956]
Epoch [64/120    avg_loss:0.184, val_acc:0.965]
Epoch [65/120    avg_loss:0.196, val_acc:0.965]
Epoch [66/120    avg_loss:0.194, val_acc:0.956]
Epoch [67/120    avg_loss:0.180, val_acc:0.950]
Epoch [68/120    avg_loss:0.186, val_acc:0.958]
Epoch [69/120    avg_loss:0.227, val_acc:0.956]
Epoch [70/120    avg_loss:0.218, val_acc:0.954]
Epoch [71/120    avg_loss:0.227, val_acc:0.938]
Epoch [72/120    avg_loss:0.291, val_acc:0.952]
Epoch [73/120    avg_loss:0.180, val_acc:0.969]
Epoch [74/120    avg_loss:0.199, val_acc:0.963]
Epoch [75/120    avg_loss:0.174, val_acc:0.971]
Epoch [76/120    avg_loss:0.154, val_acc:0.960]
Epoch [77/120    avg_loss:0.142, val_acc:0.971]
Epoch [78/120    avg_loss:0.155, val_acc:0.952]
Epoch [79/120    avg_loss:0.203, val_acc:0.935]
Epoch [80/120    avg_loss:0.207, val_acc:0.960]
Epoch [81/120    avg_loss:0.170, val_acc:0.956]
Epoch [82/120    avg_loss:0.163, val_acc:0.965]
Epoch [83/120    avg_loss:0.155, val_acc:0.958]
Epoch [84/120    avg_loss:0.143, val_acc:0.952]
Epoch [85/120    avg_loss:0.144, val_acc:0.965]
Epoch [86/120    avg_loss:0.124, val_acc:0.975]
Epoch [87/120    avg_loss:0.108, val_acc:0.958]
Epoch [88/120    avg_loss:0.115, val_acc:0.960]
Epoch [89/120    avg_loss:0.110, val_acc:0.969]
Epoch [90/120    avg_loss:0.113, val_acc:0.963]
Epoch [91/120    avg_loss:0.122, val_acc:0.969]
Epoch [92/120    avg_loss:0.119, val_acc:0.963]
Epoch [93/120    avg_loss:0.112, val_acc:0.979]
Epoch [94/120    avg_loss:0.093, val_acc:0.975]
Epoch [95/120    avg_loss:0.081, val_acc:0.977]
Epoch [96/120    avg_loss:0.102, val_acc:0.977]
Epoch [97/120    avg_loss:0.147, val_acc:0.938]
Epoch [98/120    avg_loss:0.180, val_acc:0.954]
Epoch [99/120    avg_loss:0.158, val_acc:0.960]
Epoch [100/120    avg_loss:0.133, val_acc:0.965]
Epoch [101/120    avg_loss:0.239, val_acc:0.906]
Epoch [102/120    avg_loss:0.193, val_acc:0.965]
Epoch [103/120    avg_loss:0.164, val_acc:0.969]
Epoch [104/120    avg_loss:0.123, val_acc:0.981]
Epoch [105/120    avg_loss:0.105, val_acc:0.977]
Epoch [106/120    avg_loss:0.091, val_acc:0.963]
Epoch [107/120    avg_loss:0.086, val_acc:0.975]
Epoch [108/120    avg_loss:0.086, val_acc:0.975]
Epoch [109/120    avg_loss:0.078, val_acc:0.973]
Epoch [110/120    avg_loss:0.117, val_acc:0.958]
Epoch [111/120    avg_loss:0.098, val_acc:0.952]
Epoch [112/120    avg_loss:0.101, val_acc:0.975]
Epoch [113/120    avg_loss:0.091, val_acc:0.965]
Epoch [114/120    avg_loss:0.096, val_acc:0.971]
Epoch [115/120    avg_loss:0.085, val_acc:0.967]
Epoch [116/120    avg_loss:0.138, val_acc:0.973]
Epoch [117/120    avg_loss:0.103, val_acc:0.967]
Epoch [118/120    avg_loss:0.078, val_acc:0.979]
Epoch [119/120    avg_loss:0.063, val_acc:0.977]
Epoch [120/120    avg_loss:0.057, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 200   0   0   0   0  19   0   0   0   0   0   0]
 [  0   0   0 215   8   0   0   0   4   3   0   0   0   0]
 [  0   0   1   1 210  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  30 115   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.99573560767591

F1 scores:
[       nan 1.         0.92807425 0.96412556 0.88235294 0.83636364
 0.99756691 0.84693878 0.99487179 0.99680511 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9776842863582988
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65676f6ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.655, val_acc:0.152]
Epoch [2/120    avg_loss:2.546, val_acc:0.391]
Epoch [3/120    avg_loss:2.445, val_acc:0.445]
Epoch [4/120    avg_loss:2.362, val_acc:0.449]
Epoch [5/120    avg_loss:2.293, val_acc:0.457]
Epoch [6/120    avg_loss:2.228, val_acc:0.482]
Epoch [7/120    avg_loss:2.180, val_acc:0.504]
Epoch [8/120    avg_loss:2.130, val_acc:0.537]
Epoch [9/120    avg_loss:2.076, val_acc:0.582]
Epoch [10/120    avg_loss:2.011, val_acc:0.611]
Epoch [11/120    avg_loss:1.953, val_acc:0.639]
Epoch [12/120    avg_loss:1.885, val_acc:0.693]
Epoch [13/120    avg_loss:1.808, val_acc:0.670]
Epoch [14/120    avg_loss:1.743, val_acc:0.688]
Epoch [15/120    avg_loss:1.674, val_acc:0.748]
Epoch [16/120    avg_loss:1.584, val_acc:0.764]
Epoch [17/120    avg_loss:1.502, val_acc:0.766]
Epoch [18/120    avg_loss:1.426, val_acc:0.812]
Epoch [19/120    avg_loss:1.356, val_acc:0.803]
Epoch [20/120    avg_loss:1.278, val_acc:0.814]
Epoch [21/120    avg_loss:1.203, val_acc:0.816]
Epoch [22/120    avg_loss:1.137, val_acc:0.832]
Epoch [23/120    avg_loss:1.076, val_acc:0.840]
Epoch [24/120    avg_loss:0.996, val_acc:0.852]
Epoch [25/120    avg_loss:0.924, val_acc:0.787]
Epoch [26/120    avg_loss:0.908, val_acc:0.873]
Epoch [27/120    avg_loss:0.789, val_acc:0.877]
Epoch [28/120    avg_loss:0.717, val_acc:0.887]
Epoch [29/120    avg_loss:0.720, val_acc:0.889]
Epoch [30/120    avg_loss:0.685, val_acc:0.863]
Epoch [31/120    avg_loss:0.671, val_acc:0.908]
Epoch [32/120    avg_loss:0.599, val_acc:0.826]
Epoch [33/120    avg_loss:0.583, val_acc:0.902]
Epoch [34/120    avg_loss:0.535, val_acc:0.908]
Epoch [35/120    avg_loss:0.513, val_acc:0.918]
Epoch [36/120    avg_loss:0.466, val_acc:0.918]
Epoch [37/120    avg_loss:0.490, val_acc:0.928]
Epoch [38/120    avg_loss:0.493, val_acc:0.896]
Epoch [39/120    avg_loss:0.486, val_acc:0.926]
Epoch [40/120    avg_loss:0.448, val_acc:0.900]
Epoch [41/120    avg_loss:0.410, val_acc:0.916]
Epoch [42/120    avg_loss:0.388, val_acc:0.928]
Epoch [43/120    avg_loss:0.343, val_acc:0.938]
Epoch [44/120    avg_loss:0.373, val_acc:0.936]
Epoch [45/120    avg_loss:0.378, val_acc:0.920]
Epoch [46/120    avg_loss:0.350, val_acc:0.939]
Epoch [47/120    avg_loss:0.337, val_acc:0.936]
Epoch [48/120    avg_loss:0.320, val_acc:0.932]
Epoch [49/120    avg_loss:0.325, val_acc:0.938]
Epoch [50/120    avg_loss:0.277, val_acc:0.939]
Epoch [51/120    avg_loss:0.265, val_acc:0.932]
Epoch [52/120    avg_loss:0.348, val_acc:0.902]
Epoch [53/120    avg_loss:0.472, val_acc:0.920]
Epoch [54/120    avg_loss:0.359, val_acc:0.906]
Epoch [55/120    avg_loss:0.339, val_acc:0.932]
Epoch [56/120    avg_loss:0.293, val_acc:0.934]
Epoch [57/120    avg_loss:0.278, val_acc:0.939]
Epoch [58/120    avg_loss:0.258, val_acc:0.918]
Epoch [59/120    avg_loss:0.303, val_acc:0.900]
Epoch [60/120    avg_loss:0.263, val_acc:0.936]
Epoch [61/120    avg_loss:0.230, val_acc:0.945]
Epoch [62/120    avg_loss:0.254, val_acc:0.934]
Epoch [63/120    avg_loss:0.230, val_acc:0.930]
Epoch [64/120    avg_loss:0.244, val_acc:0.938]
Epoch [65/120    avg_loss:0.242, val_acc:0.928]
Epoch [66/120    avg_loss:0.206, val_acc:0.926]
Epoch [67/120    avg_loss:0.210, val_acc:0.945]
Epoch [68/120    avg_loss:0.267, val_acc:0.920]
Epoch [69/120    avg_loss:0.275, val_acc:0.939]
Epoch [70/120    avg_loss:0.277, val_acc:0.949]
Epoch [71/120    avg_loss:0.237, val_acc:0.906]
Epoch [72/120    avg_loss:0.309, val_acc:0.928]
Epoch [73/120    avg_loss:0.233, val_acc:0.947]
Epoch [74/120    avg_loss:0.237, val_acc:0.910]
Epoch [75/120    avg_loss:0.210, val_acc:0.949]
Epoch [76/120    avg_loss:0.174, val_acc:0.969]
Epoch [77/120    avg_loss:0.189, val_acc:0.955]
Epoch [78/120    avg_loss:0.218, val_acc:0.938]
Epoch [79/120    avg_loss:0.247, val_acc:0.932]
Epoch [80/120    avg_loss:0.202, val_acc:0.941]
Epoch [81/120    avg_loss:0.187, val_acc:0.951]
Epoch [82/120    avg_loss:0.190, val_acc:0.955]
Epoch [83/120    avg_loss:0.147, val_acc:0.959]
Epoch [84/120    avg_loss:0.144, val_acc:0.953]
Epoch [85/120    avg_loss:0.151, val_acc:0.965]
Epoch [86/120    avg_loss:0.130, val_acc:0.957]
Epoch [87/120    avg_loss:0.172, val_acc:0.961]
Epoch [88/120    avg_loss:0.227, val_acc:0.932]
Epoch [89/120    avg_loss:0.199, val_acc:0.963]
Epoch [90/120    avg_loss:0.142, val_acc:0.971]
Epoch [91/120    avg_loss:0.145, val_acc:0.963]
Epoch [92/120    avg_loss:0.128, val_acc:0.967]
Epoch [93/120    avg_loss:0.118, val_acc:0.969]
Epoch [94/120    avg_loss:0.107, val_acc:0.969]
Epoch [95/120    avg_loss:0.099, val_acc:0.969]
Epoch [96/120    avg_loss:0.110, val_acc:0.967]
Epoch [97/120    avg_loss:0.107, val_acc:0.969]
Epoch [98/120    avg_loss:0.118, val_acc:0.971]
Epoch [99/120    avg_loss:0.104, val_acc:0.969]
Epoch [100/120    avg_loss:0.099, val_acc:0.973]
Epoch [101/120    avg_loss:0.095, val_acc:0.971]
Epoch [102/120    avg_loss:0.103, val_acc:0.973]
Epoch [103/120    avg_loss:0.122, val_acc:0.971]
Epoch [104/120    avg_loss:0.094, val_acc:0.973]
Epoch [105/120    avg_loss:0.103, val_acc:0.973]
Epoch [106/120    avg_loss:0.097, val_acc:0.973]
Epoch [107/120    avg_loss:0.104, val_acc:0.971]
Epoch [108/120    avg_loss:0.108, val_acc:0.973]
Epoch [109/120    avg_loss:0.096, val_acc:0.975]
Epoch [110/120    avg_loss:0.093, val_acc:0.975]
Epoch [111/120    avg_loss:0.104, val_acc:0.973]
Epoch [112/120    avg_loss:0.105, val_acc:0.973]
Epoch [113/120    avg_loss:0.086, val_acc:0.975]
Epoch [114/120    avg_loss:0.094, val_acc:0.971]
Epoch [115/120    avg_loss:0.086, val_acc:0.971]
Epoch [116/120    avg_loss:0.088, val_acc:0.975]
Epoch [117/120    avg_loss:0.095, val_acc:0.973]
Epoch [118/120    avg_loss:0.085, val_acc:0.973]
Epoch [119/120    avg_loss:0.082, val_acc:0.975]
Epoch [120/120    avg_loss:0.108, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 217  10   0   0   0   2   0   0   0   0   0]
 [  0   0   1   0 221   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  37 108   0   0   0   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0  24   0   0   0   0  70   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   4   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.05970149253731

F1 scores:
[       nan 0.99636364 0.94396552 0.97091723 0.89292929 0.8372093
 0.98771499 0.85365854 0.99232737 1.         1.         0.99734043
 0.99336283 1.        ]

Kappa:
0.9783893424352723
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd86a10da20>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.596, val_acc:0.248]
Epoch [2/120    avg_loss:2.519, val_acc:0.307]
Epoch [3/120    avg_loss:2.459, val_acc:0.309]
Epoch [4/120    avg_loss:2.403, val_acc:0.320]
Epoch [5/120    avg_loss:2.344, val_acc:0.342]
Epoch [6/120    avg_loss:2.296, val_acc:0.453]
Epoch [7/120    avg_loss:2.241, val_acc:0.498]
Epoch [8/120    avg_loss:2.178, val_acc:0.500]
Epoch [9/120    avg_loss:2.098, val_acc:0.500]
Epoch [10/120    avg_loss:2.026, val_acc:0.518]
Epoch [11/120    avg_loss:1.935, val_acc:0.559]
Epoch [12/120    avg_loss:1.893, val_acc:0.611]
Epoch [13/120    avg_loss:1.801, val_acc:0.625]
Epoch [14/120    avg_loss:1.723, val_acc:0.686]
Epoch [15/120    avg_loss:1.666, val_acc:0.670]
Epoch [16/120    avg_loss:1.572, val_acc:0.783]
Epoch [17/120    avg_loss:1.486, val_acc:0.732]
Epoch [18/120    avg_loss:1.410, val_acc:0.818]
Epoch [19/120    avg_loss:1.342, val_acc:0.756]
Epoch [20/120    avg_loss:1.230, val_acc:0.773]
Epoch [21/120    avg_loss:1.165, val_acc:0.828]
Epoch [22/120    avg_loss:1.098, val_acc:0.852]
Epoch [23/120    avg_loss:1.004, val_acc:0.859]
Epoch [24/120    avg_loss:0.932, val_acc:0.857]
Epoch [25/120    avg_loss:0.880, val_acc:0.906]
Epoch [26/120    avg_loss:0.831, val_acc:0.857]
Epoch [27/120    avg_loss:0.819, val_acc:0.893]
Epoch [28/120    avg_loss:0.736, val_acc:0.900]
Epoch [29/120    avg_loss:0.698, val_acc:0.898]
Epoch [30/120    avg_loss:0.686, val_acc:0.926]
Epoch [31/120    avg_loss:0.722, val_acc:0.904]
Epoch [32/120    avg_loss:0.608, val_acc:0.898]
Epoch [33/120    avg_loss:0.610, val_acc:0.914]
Epoch [34/120    avg_loss:0.579, val_acc:0.918]
Epoch [35/120    avg_loss:0.537, val_acc:0.910]
Epoch [36/120    avg_loss:0.500, val_acc:0.928]
Epoch [37/120    avg_loss:0.505, val_acc:0.920]
Epoch [38/120    avg_loss:0.508, val_acc:0.924]
Epoch [39/120    avg_loss:0.474, val_acc:0.934]
Epoch [40/120    avg_loss:0.455, val_acc:0.938]
Epoch [41/120    avg_loss:0.457, val_acc:0.939]
Epoch [42/120    avg_loss:0.444, val_acc:0.922]
Epoch [43/120    avg_loss:0.464, val_acc:0.922]
Epoch [44/120    avg_loss:0.438, val_acc:0.920]
Epoch [45/120    avg_loss:0.456, val_acc:0.932]
Epoch [46/120    avg_loss:0.462, val_acc:0.896]
Epoch [47/120    avg_loss:0.418, val_acc:0.920]
Epoch [48/120    avg_loss:0.427, val_acc:0.932]
Epoch [49/120    avg_loss:0.407, val_acc:0.939]
Epoch [50/120    avg_loss:0.382, val_acc:0.928]
Epoch [51/120    avg_loss:0.355, val_acc:0.926]
Epoch [52/120    avg_loss:0.331, val_acc:0.943]
Epoch [53/120    avg_loss:0.357, val_acc:0.943]
Epoch [54/120    avg_loss:0.341, val_acc:0.939]
Epoch [55/120    avg_loss:0.306, val_acc:0.938]
Epoch [56/120    avg_loss:0.338, val_acc:0.936]
Epoch [57/120    avg_loss:0.278, val_acc:0.943]
Epoch [58/120    avg_loss:0.283, val_acc:0.945]
Epoch [59/120    avg_loss:0.307, val_acc:0.930]
Epoch [60/120    avg_loss:0.295, val_acc:0.953]
Epoch [61/120    avg_loss:0.332, val_acc:0.941]
Epoch [62/120    avg_loss:0.315, val_acc:0.955]
Epoch [63/120    avg_loss:0.314, val_acc:0.930]
Epoch [64/120    avg_loss:0.283, val_acc:0.961]
Epoch [65/120    avg_loss:0.268, val_acc:0.947]
Epoch [66/120    avg_loss:0.265, val_acc:0.938]
Epoch [67/120    avg_loss:0.303, val_acc:0.951]
Epoch [68/120    avg_loss:0.325, val_acc:0.930]
Epoch [69/120    avg_loss:0.281, val_acc:0.955]
Epoch [70/120    avg_loss:0.218, val_acc:0.955]
Epoch [71/120    avg_loss:0.213, val_acc:0.949]
Epoch [72/120    avg_loss:0.200, val_acc:0.965]
Epoch [73/120    avg_loss:0.208, val_acc:0.963]
Epoch [74/120    avg_loss:0.210, val_acc:0.965]
Epoch [75/120    avg_loss:0.209, val_acc:0.967]
Epoch [76/120    avg_loss:0.240, val_acc:0.932]
Epoch [77/120    avg_loss:0.195, val_acc:0.963]
Epoch [78/120    avg_loss:0.200, val_acc:0.975]
Epoch [79/120    avg_loss:0.186, val_acc:0.973]
Epoch [80/120    avg_loss:0.194, val_acc:0.957]
Epoch [81/120    avg_loss:0.161, val_acc:0.975]
Epoch [82/120    avg_loss:0.150, val_acc:0.975]
Epoch [83/120    avg_loss:0.165, val_acc:0.953]
Epoch [84/120    avg_loss:0.202, val_acc:0.949]
Epoch [85/120    avg_loss:0.179, val_acc:0.938]
Epoch [86/120    avg_loss:0.205, val_acc:0.969]
Epoch [87/120    avg_loss:0.163, val_acc:0.963]
Epoch [88/120    avg_loss:0.170, val_acc:0.965]
Epoch [89/120    avg_loss:0.134, val_acc:0.975]
Epoch [90/120    avg_loss:0.178, val_acc:0.885]
Epoch [91/120    avg_loss:0.182, val_acc:0.973]
Epoch [92/120    avg_loss:0.184, val_acc:0.957]
Epoch [93/120    avg_loss:0.146, val_acc:0.963]
Epoch [94/120    avg_loss:0.122, val_acc:0.971]
Epoch [95/120    avg_loss:0.111, val_acc:0.967]
Epoch [96/120    avg_loss:0.117, val_acc:0.967]
Epoch [97/120    avg_loss:0.121, val_acc:0.975]
Epoch [98/120    avg_loss:0.135, val_acc:0.969]
Epoch [99/120    avg_loss:0.151, val_acc:0.959]
Epoch [100/120    avg_loss:0.107, val_acc:0.957]
Epoch [101/120    avg_loss:0.121, val_acc:0.951]
Epoch [102/120    avg_loss:0.128, val_acc:0.971]
Epoch [103/120    avg_loss:0.126, val_acc:0.973]
Epoch [104/120    avg_loss:0.110, val_acc:0.973]
Epoch [105/120    avg_loss:0.100, val_acc:0.973]
Epoch [106/120    avg_loss:0.105, val_acc:0.975]
Epoch [107/120    avg_loss:0.089, val_acc:0.961]
Epoch [108/120    avg_loss:0.120, val_acc:0.943]
Epoch [109/120    avg_loss:0.118, val_acc:0.951]
Epoch [110/120    avg_loss:0.174, val_acc:0.959]
Epoch [111/120    avg_loss:0.116, val_acc:0.963]
Epoch [112/120    avg_loss:0.111, val_acc:0.980]
Epoch [113/120    avg_loss:0.106, val_acc:0.971]
Epoch [114/120    avg_loss:0.100, val_acc:0.969]
Epoch [115/120    avg_loss:0.124, val_acc:0.973]
Epoch [116/120    avg_loss:0.110, val_acc:0.963]
Epoch [117/120    avg_loss:0.085, val_acc:0.988]
Epoch [118/120    avg_loss:0.087, val_acc:0.975]
Epoch [119/120    avg_loss:0.067, val_acc:0.979]
Epoch [120/120    avg_loss:0.051, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 203   0   0   0   0  16   0   0   0   0   0   0]
 [  0   0   0 215  10   1   0   1   3   0   0   0   0   0]
 [  0   0   0   0 207  18   0   0   0   0   1   0   1   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0   0 379   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.91044776119402

F1 scores:
[       nan 1.         0.91647856 0.96629213 0.88650964 0.85314685
 1.         0.84974093 0.98441558 1.         0.99862826 0.99603699
 0.99557522 1.        ]

Kappa:
0.9767380926966913
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f98fc6d9a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.582, val_acc:0.133]
Epoch [2/120    avg_loss:2.504, val_acc:0.172]
Epoch [3/120    avg_loss:2.436, val_acc:0.350]
Epoch [4/120    avg_loss:2.381, val_acc:0.396]
Epoch [5/120    avg_loss:2.336, val_acc:0.418]
Epoch [6/120    avg_loss:2.285, val_acc:0.439]
Epoch [7/120    avg_loss:2.238, val_acc:0.471]
Epoch [8/120    avg_loss:2.178, val_acc:0.496]
Epoch [9/120    avg_loss:2.120, val_acc:0.498]
Epoch [10/120    avg_loss:2.058, val_acc:0.525]
Epoch [11/120    avg_loss:1.991, val_acc:0.541]
Epoch [12/120    avg_loss:1.925, val_acc:0.545]
Epoch [13/120    avg_loss:1.865, val_acc:0.562]
Epoch [14/120    avg_loss:1.788, val_acc:0.580]
Epoch [15/120    avg_loss:1.725, val_acc:0.588]
Epoch [16/120    avg_loss:1.645, val_acc:0.629]
Epoch [17/120    avg_loss:1.582, val_acc:0.646]
Epoch [18/120    avg_loss:1.486, val_acc:0.674]
Epoch [19/120    avg_loss:1.417, val_acc:0.705]
Epoch [20/120    avg_loss:1.351, val_acc:0.678]
Epoch [21/120    avg_loss:1.255, val_acc:0.717]
Epoch [22/120    avg_loss:1.213, val_acc:0.793]
Epoch [23/120    avg_loss:1.114, val_acc:0.727]
Epoch [24/120    avg_loss:1.045, val_acc:0.859]
Epoch [25/120    avg_loss:0.947, val_acc:0.842]
Epoch [26/120    avg_loss:0.904, val_acc:0.855]
Epoch [27/120    avg_loss:0.856, val_acc:0.863]
Epoch [28/120    avg_loss:0.769, val_acc:0.879]
Epoch [29/120    avg_loss:0.711, val_acc:0.879]
Epoch [30/120    avg_loss:0.685, val_acc:0.891]
Epoch [31/120    avg_loss:0.667, val_acc:0.904]
Epoch [32/120    avg_loss:0.619, val_acc:0.906]
Epoch [33/120    avg_loss:0.604, val_acc:0.902]
Epoch [34/120    avg_loss:0.590, val_acc:0.893]
Epoch [35/120    avg_loss:0.626, val_acc:0.902]
Epoch [36/120    avg_loss:0.547, val_acc:0.898]
Epoch [37/120    avg_loss:0.534, val_acc:0.906]
Epoch [38/120    avg_loss:0.474, val_acc:0.896]
Epoch [39/120    avg_loss:0.447, val_acc:0.906]
Epoch [40/120    avg_loss:0.436, val_acc:0.900]
Epoch [41/120    avg_loss:0.415, val_acc:0.912]
Epoch [42/120    avg_loss:0.411, val_acc:0.912]
Epoch [43/120    avg_loss:0.400, val_acc:0.914]
Epoch [44/120    avg_loss:0.456, val_acc:0.893]
Epoch [45/120    avg_loss:0.457, val_acc:0.875]
Epoch [46/120    avg_loss:0.411, val_acc:0.916]
Epoch [47/120    avg_loss:0.359, val_acc:0.916]
Epoch [48/120    avg_loss:0.422, val_acc:0.916]
Epoch [49/120    avg_loss:0.352, val_acc:0.916]
Epoch [50/120    avg_loss:0.297, val_acc:0.930]
Epoch [51/120    avg_loss:0.315, val_acc:0.930]
Epoch [52/120    avg_loss:0.319, val_acc:0.934]
Epoch [53/120    avg_loss:0.342, val_acc:0.920]
Epoch [54/120    avg_loss:0.312, val_acc:0.936]
Epoch [55/120    avg_loss:0.293, val_acc:0.939]
Epoch [56/120    avg_loss:0.303, val_acc:0.928]
Epoch [57/120    avg_loss:0.296, val_acc:0.928]
Epoch [58/120    avg_loss:0.250, val_acc:0.918]
Epoch [59/120    avg_loss:0.307, val_acc:0.916]
Epoch [60/120    avg_loss:0.289, val_acc:0.943]
Epoch [61/120    avg_loss:0.265, val_acc:0.951]
Epoch [62/120    avg_loss:0.264, val_acc:0.924]
Epoch [63/120    avg_loss:0.261, val_acc:0.916]
Epoch [64/120    avg_loss:0.277, val_acc:0.930]
Epoch [65/120    avg_loss:0.239, val_acc:0.943]
Epoch [66/120    avg_loss:0.224, val_acc:0.939]
Epoch [67/120    avg_loss:0.196, val_acc:0.936]
Epoch [68/120    avg_loss:0.220, val_acc:0.930]
Epoch [69/120    avg_loss:0.229, val_acc:0.943]
Epoch [70/120    avg_loss:0.315, val_acc:0.920]
Epoch [71/120    avg_loss:0.231, val_acc:0.943]
Epoch [72/120    avg_loss:0.276, val_acc:0.939]
Epoch [73/120    avg_loss:0.254, val_acc:0.939]
Epoch [74/120    avg_loss:0.211, val_acc:0.932]
Epoch [75/120    avg_loss:0.212, val_acc:0.941]
Epoch [76/120    avg_loss:0.195, val_acc:0.947]
Epoch [77/120    avg_loss:0.164, val_acc:0.951]
Epoch [78/120    avg_loss:0.162, val_acc:0.953]
Epoch [79/120    avg_loss:0.163, val_acc:0.953]
Epoch [80/120    avg_loss:0.143, val_acc:0.957]
Epoch [81/120    avg_loss:0.161, val_acc:0.959]
Epoch [82/120    avg_loss:0.160, val_acc:0.953]
Epoch [83/120    avg_loss:0.181, val_acc:0.957]
Epoch [84/120    avg_loss:0.159, val_acc:0.961]
Epoch [85/120    avg_loss:0.165, val_acc:0.959]
Epoch [86/120    avg_loss:0.145, val_acc:0.959]
Epoch [87/120    avg_loss:0.175, val_acc:0.961]
Epoch [88/120    avg_loss:0.145, val_acc:0.961]
Epoch [89/120    avg_loss:0.146, val_acc:0.961]
Epoch [90/120    avg_loss:0.144, val_acc:0.961]
Epoch [91/120    avg_loss:0.135, val_acc:0.959]
Epoch [92/120    avg_loss:0.136, val_acc:0.955]
Epoch [93/120    avg_loss:0.139, val_acc:0.961]
Epoch [94/120    avg_loss:0.141, val_acc:0.957]
Epoch [95/120    avg_loss:0.143, val_acc:0.955]
Epoch [96/120    avg_loss:0.133, val_acc:0.959]
Epoch [97/120    avg_loss:0.131, val_acc:0.959]
Epoch [98/120    avg_loss:0.139, val_acc:0.963]
Epoch [99/120    avg_loss:0.141, val_acc:0.961]
Epoch [100/120    avg_loss:0.131, val_acc:0.961]
Epoch [101/120    avg_loss:0.136, val_acc:0.957]
Epoch [102/120    avg_loss:0.139, val_acc:0.959]
Epoch [103/120    avg_loss:0.133, val_acc:0.963]
Epoch [104/120    avg_loss:0.143, val_acc:0.967]
Epoch [105/120    avg_loss:0.127, val_acc:0.967]
Epoch [106/120    avg_loss:0.126, val_acc:0.961]
Epoch [107/120    avg_loss:0.127, val_acc:0.961]
Epoch [108/120    avg_loss:0.137, val_acc:0.967]
Epoch [109/120    avg_loss:0.124, val_acc:0.963]
Epoch [110/120    avg_loss:0.128, val_acc:0.969]
Epoch [111/120    avg_loss:0.147, val_acc:0.961]
Epoch [112/120    avg_loss:0.114, val_acc:0.961]
Epoch [113/120    avg_loss:0.118, val_acc:0.967]
Epoch [114/120    avg_loss:0.121, val_acc:0.969]
Epoch [115/120    avg_loss:0.130, val_acc:0.963]
Epoch [116/120    avg_loss:0.124, val_acc:0.969]
Epoch [117/120    avg_loss:0.140, val_acc:0.965]
Epoch [118/120    avg_loss:0.115, val_acc:0.965]
Epoch [119/120    avg_loss:0.122, val_acc:0.963]
Epoch [120/120    avg_loss:0.119, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 204   0   0   0   0  15   0   0   0   0   0   0]
 [  0   0   7 216   6   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0  31 114   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8   0 198   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.63326226012794

F1 scores:
[       nan 1.         0.91071429 0.96860987 0.85232068 0.8028169
 0.98019802 0.8172043  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9736495862156445
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fee73d34b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.617, val_acc:0.205]
Epoch [2/120    avg_loss:2.542, val_acc:0.277]
Epoch [3/120    avg_loss:2.464, val_acc:0.279]
Epoch [4/120    avg_loss:2.380, val_acc:0.281]
Epoch [5/120    avg_loss:2.293, val_acc:0.311]
Epoch [6/120    avg_loss:2.204, val_acc:0.375]
Epoch [7/120    avg_loss:2.127, val_acc:0.393]
Epoch [8/120    avg_loss:2.049, val_acc:0.514]
Epoch [9/120    avg_loss:1.993, val_acc:0.564]
Epoch [10/120    avg_loss:1.939, val_acc:0.557]
Epoch [11/120    avg_loss:1.857, val_acc:0.637]
Epoch [12/120    avg_loss:1.782, val_acc:0.668]
Epoch [13/120    avg_loss:1.716, val_acc:0.688]
Epoch [14/120    avg_loss:1.659, val_acc:0.664]
Epoch [15/120    avg_loss:1.569, val_acc:0.682]
Epoch [16/120    avg_loss:1.463, val_acc:0.689]
Epoch [17/120    avg_loss:1.396, val_acc:0.711]
Epoch [18/120    avg_loss:1.304, val_acc:0.668]
Epoch [19/120    avg_loss:1.246, val_acc:0.775]
Epoch [20/120    avg_loss:1.193, val_acc:0.723]
Epoch [21/120    avg_loss:1.147, val_acc:0.695]
Epoch [22/120    avg_loss:1.057, val_acc:0.744]
Epoch [23/120    avg_loss:1.001, val_acc:0.807]
Epoch [24/120    avg_loss:0.956, val_acc:0.842]
Epoch [25/120    avg_loss:0.898, val_acc:0.861]
Epoch [26/120    avg_loss:0.825, val_acc:0.893]
Epoch [27/120    avg_loss:0.775, val_acc:0.895]
Epoch [28/120    avg_loss:0.720, val_acc:0.885]
Epoch [29/120    avg_loss:0.706, val_acc:0.902]
Epoch [30/120    avg_loss:0.653, val_acc:0.914]
Epoch [31/120    avg_loss:0.589, val_acc:0.898]
Epoch [32/120    avg_loss:0.552, val_acc:0.910]
Epoch [33/120    avg_loss:0.583, val_acc:0.875]
Epoch [34/120    avg_loss:0.554, val_acc:0.916]
Epoch [35/120    avg_loss:0.528, val_acc:0.857]
Epoch [36/120    avg_loss:0.523, val_acc:0.912]
Epoch [37/120    avg_loss:0.460, val_acc:0.879]
Epoch [38/120    avg_loss:0.485, val_acc:0.908]
Epoch [39/120    avg_loss:0.413, val_acc:0.887]
Epoch [40/120    avg_loss:0.420, val_acc:0.936]
Epoch [41/120    avg_loss:0.402, val_acc:0.938]
Epoch [42/120    avg_loss:0.371, val_acc:0.939]
Epoch [43/120    avg_loss:0.359, val_acc:0.930]
Epoch [44/120    avg_loss:0.385, val_acc:0.914]
Epoch [45/120    avg_loss:0.357, val_acc:0.928]
Epoch [46/120    avg_loss:0.399, val_acc:0.887]
Epoch [47/120    avg_loss:0.346, val_acc:0.928]
Epoch [48/120    avg_loss:0.315, val_acc:0.918]
Epoch [49/120    avg_loss:0.360, val_acc:0.938]
Epoch [50/120    avg_loss:0.369, val_acc:0.924]
Epoch [51/120    avg_loss:0.415, val_acc:0.908]
Epoch [52/120    avg_loss:0.319, val_acc:0.906]
Epoch [53/120    avg_loss:0.303, val_acc:0.918]
Epoch [54/120    avg_loss:0.293, val_acc:0.912]
Epoch [55/120    avg_loss:0.343, val_acc:0.936]
Epoch [56/120    avg_loss:0.259, val_acc:0.945]
Epoch [57/120    avg_loss:0.224, val_acc:0.953]
Epoch [58/120    avg_loss:0.218, val_acc:0.953]
Epoch [59/120    avg_loss:0.196, val_acc:0.953]
Epoch [60/120    avg_loss:0.200, val_acc:0.951]
Epoch [61/120    avg_loss:0.196, val_acc:0.953]
Epoch [62/120    avg_loss:0.199, val_acc:0.951]
Epoch [63/120    avg_loss:0.194, val_acc:0.957]
Epoch [64/120    avg_loss:0.189, val_acc:0.959]
Epoch [65/120    avg_loss:0.179, val_acc:0.955]
Epoch [66/120    avg_loss:0.192, val_acc:0.953]
Epoch [67/120    avg_loss:0.184, val_acc:0.969]
Epoch [68/120    avg_loss:0.185, val_acc:0.959]
Epoch [69/120    avg_loss:0.176, val_acc:0.957]
Epoch [70/120    avg_loss:0.169, val_acc:0.963]
Epoch [71/120    avg_loss:0.171, val_acc:0.955]
Epoch [72/120    avg_loss:0.178, val_acc:0.967]
Epoch [73/120    avg_loss:0.164, val_acc:0.965]
Epoch [74/120    avg_loss:0.169, val_acc:0.955]
Epoch [75/120    avg_loss:0.164, val_acc:0.963]
Epoch [76/120    avg_loss:0.152, val_acc:0.959]
Epoch [77/120    avg_loss:0.173, val_acc:0.963]
Epoch [78/120    avg_loss:0.152, val_acc:0.953]
Epoch [79/120    avg_loss:0.162, val_acc:0.971]
Epoch [80/120    avg_loss:0.162, val_acc:0.969]
Epoch [81/120    avg_loss:0.164, val_acc:0.971]
Epoch [82/120    avg_loss:0.152, val_acc:0.965]
Epoch [83/120    avg_loss:0.153, val_acc:0.971]
Epoch [84/120    avg_loss:0.150, val_acc:0.961]
Epoch [85/120    avg_loss:0.161, val_acc:0.957]
Epoch [86/120    avg_loss:0.169, val_acc:0.969]
Epoch [87/120    avg_loss:0.148, val_acc:0.963]
Epoch [88/120    avg_loss:0.149, val_acc:0.961]
Epoch [89/120    avg_loss:0.145, val_acc:0.971]
Epoch [90/120    avg_loss:0.145, val_acc:0.969]
Epoch [91/120    avg_loss:0.148, val_acc:0.969]
Epoch [92/120    avg_loss:0.141, val_acc:0.965]
Epoch [93/120    avg_loss:0.125, val_acc:0.971]
Epoch [94/120    avg_loss:0.162, val_acc:0.957]
Epoch [95/120    avg_loss:0.139, val_acc:0.971]
Epoch [96/120    avg_loss:0.144, val_acc:0.967]
Epoch [97/120    avg_loss:0.146, val_acc:0.963]
Epoch [98/120    avg_loss:0.137, val_acc:0.969]
Epoch [99/120    avg_loss:0.136, val_acc:0.973]
Epoch [100/120    avg_loss:0.148, val_acc:0.971]
Epoch [101/120    avg_loss:0.113, val_acc:0.967]
Epoch [102/120    avg_loss:0.133, val_acc:0.969]
Epoch [103/120    avg_loss:0.148, val_acc:0.973]
Epoch [104/120    avg_loss:0.144, val_acc:0.967]
Epoch [105/120    avg_loss:0.128, val_acc:0.973]
Epoch [106/120    avg_loss:0.127, val_acc:0.971]
Epoch [107/120    avg_loss:0.129, val_acc:0.971]
Epoch [108/120    avg_loss:0.129, val_acc:0.969]
Epoch [109/120    avg_loss:0.126, val_acc:0.969]
Epoch [110/120    avg_loss:0.127, val_acc:0.967]
Epoch [111/120    avg_loss:0.121, val_acc:0.971]
Epoch [112/120    avg_loss:0.117, val_acc:0.975]
Epoch [113/120    avg_loss:0.125, val_acc:0.975]
Epoch [114/120    avg_loss:0.121, val_acc:0.965]
Epoch [115/120    avg_loss:0.155, val_acc:0.967]
Epoch [116/120    avg_loss:0.120, val_acc:0.975]
Epoch [117/120    avg_loss:0.123, val_acc:0.971]
Epoch [118/120    avg_loss:0.122, val_acc:0.973]
Epoch [119/120    avg_loss:0.131, val_acc:0.977]
Epoch [120/120    avg_loss:0.124, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 199  14   0   0   0  13   4   0   0   0   0]
 [  0   0   0   0 213  13   0   0   1   0   0   0   0   0]
 [  0   0   0   0  34 110   0   0   1   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  23   0   0   0   0  71   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   2   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.50533049040511

F1 scores:
[       nan 1.         0.92444444 0.92773893 0.86761711 0.82089552
 0.99756691 0.80681818 0.98103666 0.99574468 1.         1.
 0.99778761 1.        ]

Kappa:
0.9722177091054962
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb9db1a1a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.051]
Epoch [2/120    avg_loss:2.579, val_acc:0.320]
Epoch [3/120    avg_loss:2.503, val_acc:0.330]
Epoch [4/120    avg_loss:2.436, val_acc:0.330]
Epoch [5/120    avg_loss:2.377, val_acc:0.326]
Epoch [6/120    avg_loss:2.318, val_acc:0.338]
Epoch [7/120    avg_loss:2.257, val_acc:0.342]
Epoch [8/120    avg_loss:2.209, val_acc:0.402]
Epoch [9/120    avg_loss:2.152, val_acc:0.504]
Epoch [10/120    avg_loss:2.099, val_acc:0.559]
Epoch [11/120    avg_loss:2.044, val_acc:0.568]
Epoch [12/120    avg_loss:1.956, val_acc:0.576]
Epoch [13/120    avg_loss:1.900, val_acc:0.602]
Epoch [14/120    avg_loss:1.816, val_acc:0.611]
Epoch [15/120    avg_loss:1.739, val_acc:0.604]
Epoch [16/120    avg_loss:1.607, val_acc:0.641]
Epoch [17/120    avg_loss:1.527, val_acc:0.664]
Epoch [18/120    avg_loss:1.445, val_acc:0.682]
Epoch [19/120    avg_loss:1.332, val_acc:0.682]
Epoch [20/120    avg_loss:1.234, val_acc:0.713]
Epoch [21/120    avg_loss:1.175, val_acc:0.738]
Epoch [22/120    avg_loss:1.171, val_acc:0.760]
Epoch [23/120    avg_loss:1.089, val_acc:0.783]
Epoch [24/120    avg_loss:0.984, val_acc:0.783]
Epoch [25/120    avg_loss:0.947, val_acc:0.764]
Epoch [26/120    avg_loss:0.917, val_acc:0.771]
Epoch [27/120    avg_loss:0.785, val_acc:0.838]
Epoch [28/120    avg_loss:0.750, val_acc:0.879]
Epoch [29/120    avg_loss:0.711, val_acc:0.881]
Epoch [30/120    avg_loss:0.715, val_acc:0.852]
Epoch [31/120    avg_loss:0.675, val_acc:0.906]
Epoch [32/120    avg_loss:0.590, val_acc:0.840]
Epoch [33/120    avg_loss:0.598, val_acc:0.891]
Epoch [34/120    avg_loss:0.555, val_acc:0.902]
Epoch [35/120    avg_loss:0.571, val_acc:0.898]
Epoch [36/120    avg_loss:0.558, val_acc:0.893]
Epoch [37/120    avg_loss:0.548, val_acc:0.924]
Epoch [38/120    avg_loss:0.558, val_acc:0.906]
Epoch [39/120    avg_loss:0.575, val_acc:0.859]
Epoch [40/120    avg_loss:0.609, val_acc:0.920]
Epoch [41/120    avg_loss:0.489, val_acc:0.906]
Epoch [42/120    avg_loss:0.484, val_acc:0.906]
Epoch [43/120    avg_loss:0.446, val_acc:0.910]
Epoch [44/120    avg_loss:0.436, val_acc:0.902]
Epoch [45/120    avg_loss:0.430, val_acc:0.918]
Epoch [46/120    avg_loss:0.424, val_acc:0.922]
Epoch [47/120    avg_loss:0.401, val_acc:0.918]
Epoch [48/120    avg_loss:0.383, val_acc:0.924]
Epoch [49/120    avg_loss:0.358, val_acc:0.938]
Epoch [50/120    avg_loss:0.355, val_acc:0.920]
Epoch [51/120    avg_loss:0.347, val_acc:0.904]
Epoch [52/120    avg_loss:0.352, val_acc:0.941]
Epoch [53/120    avg_loss:0.310, val_acc:0.951]
Epoch [54/120    avg_loss:0.330, val_acc:0.941]
Epoch [55/120    avg_loss:0.330, val_acc:0.928]
Epoch [56/120    avg_loss:0.274, val_acc:0.928]
Epoch [57/120    avg_loss:0.366, val_acc:0.916]
Epoch [58/120    avg_loss:0.302, val_acc:0.863]
Epoch [59/120    avg_loss:0.319, val_acc:0.951]
Epoch [60/120    avg_loss:0.297, val_acc:0.934]
Epoch [61/120    avg_loss:0.302, val_acc:0.941]
Epoch [62/120    avg_loss:0.265, val_acc:0.959]
Epoch [63/120    avg_loss:0.274, val_acc:0.926]
Epoch [64/120    avg_loss:0.274, val_acc:0.949]
Epoch [65/120    avg_loss:0.228, val_acc:0.969]
Epoch [66/120    avg_loss:0.228, val_acc:0.963]
Epoch [67/120    avg_loss:0.250, val_acc:0.949]
Epoch [68/120    avg_loss:0.207, val_acc:0.949]
Epoch [69/120    avg_loss:0.229, val_acc:0.965]
Epoch [70/120    avg_loss:0.237, val_acc:0.951]
Epoch [71/120    avg_loss:0.198, val_acc:0.963]
Epoch [72/120    avg_loss:0.164, val_acc:0.965]
Epoch [73/120    avg_loss:0.187, val_acc:0.951]
Epoch [74/120    avg_loss:0.215, val_acc:0.945]
Epoch [75/120    avg_loss:0.266, val_acc:0.912]
Epoch [76/120    avg_loss:0.261, val_acc:0.953]
Epoch [77/120    avg_loss:0.198, val_acc:0.971]
Epoch [78/120    avg_loss:0.250, val_acc:0.949]
Epoch [79/120    avg_loss:0.246, val_acc:0.953]
Epoch [80/120    avg_loss:0.177, val_acc:0.938]
Epoch [81/120    avg_loss:0.176, val_acc:0.959]
Epoch [82/120    avg_loss:0.169, val_acc:0.961]
Epoch [83/120    avg_loss:0.150, val_acc:0.965]
Epoch [84/120    avg_loss:0.148, val_acc:0.967]
Epoch [85/120    avg_loss:0.117, val_acc:0.971]
Epoch [86/120    avg_loss:0.105, val_acc:0.967]
Epoch [87/120    avg_loss:0.137, val_acc:0.951]
Epoch [88/120    avg_loss:0.184, val_acc:0.951]
Epoch [89/120    avg_loss:0.125, val_acc:0.975]
Epoch [90/120    avg_loss:0.125, val_acc:0.971]
Epoch [91/120    avg_loss:0.109, val_acc:0.975]
Epoch [92/120    avg_loss:0.128, val_acc:0.979]
Epoch [93/120    avg_loss:0.095, val_acc:0.971]
Epoch [94/120    avg_loss:0.130, val_acc:0.957]
Epoch [95/120    avg_loss:0.142, val_acc:0.971]
Epoch [96/120    avg_loss:0.120, val_acc:0.973]
Epoch [97/120    avg_loss:0.108, val_acc:0.967]
Epoch [98/120    avg_loss:0.109, val_acc:0.977]
Epoch [99/120    avg_loss:0.117, val_acc:0.975]
Epoch [100/120    avg_loss:0.103, val_acc:0.982]
Epoch [101/120    avg_loss:0.150, val_acc:0.971]
Epoch [102/120    avg_loss:0.114, val_acc:0.986]
Epoch [103/120    avg_loss:0.089, val_acc:0.986]
Epoch [104/120    avg_loss:0.096, val_acc:0.979]
Epoch [105/120    avg_loss:0.110, val_acc:0.977]
Epoch [106/120    avg_loss:0.112, val_acc:0.951]
Epoch [107/120    avg_loss:0.119, val_acc:0.953]
Epoch [108/120    avg_loss:0.165, val_acc:0.969]
Epoch [109/120    avg_loss:0.096, val_acc:0.969]
Epoch [110/120    avg_loss:0.088, val_acc:0.980]
Epoch [111/120    avg_loss:0.091, val_acc:0.982]
Epoch [112/120    avg_loss:0.103, val_acc:0.971]
Epoch [113/120    avg_loss:0.147, val_acc:0.951]
Epoch [114/120    avg_loss:0.122, val_acc:0.977]
Epoch [115/120    avg_loss:0.093, val_acc:0.979]
Epoch [116/120    avg_loss:0.056, val_acc:0.979]
Epoch [117/120    avg_loss:0.052, val_acc:0.984]
Epoch [118/120    avg_loss:0.061, val_acc:0.988]
Epoch [119/120    avg_loss:0.058, val_acc:0.988]
Epoch [120/120    avg_loss:0.057, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   3 210  15   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 1.         0.95842451 0.95454545 0.88602151 0.8707483
 1.         0.91954023 0.998713   0.99893276 1.         1.
 0.99778761 1.        ]

Kappa:
0.982432641332807
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4b84bda58>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.613, val_acc:0.176]
Epoch [2/120    avg_loss:2.521, val_acc:0.363]
Epoch [3/120    avg_loss:2.433, val_acc:0.406]
Epoch [4/120    avg_loss:2.362, val_acc:0.395]
Epoch [5/120    avg_loss:2.302, val_acc:0.436]
Epoch [6/120    avg_loss:2.250, val_acc:0.455]
Epoch [7/120    avg_loss:2.177, val_acc:0.469]
Epoch [8/120    avg_loss:2.116, val_acc:0.480]
Epoch [9/120    avg_loss:2.044, val_acc:0.504]
Epoch [10/120    avg_loss:1.988, val_acc:0.520]
Epoch [11/120    avg_loss:1.919, val_acc:0.535]
Epoch [12/120    avg_loss:1.847, val_acc:0.549]
Epoch [13/120    avg_loss:1.773, val_acc:0.561]
Epoch [14/120    avg_loss:1.702, val_acc:0.598]
Epoch [15/120    avg_loss:1.615, val_acc:0.627]
Epoch [16/120    avg_loss:1.562, val_acc:0.639]
Epoch [17/120    avg_loss:1.475, val_acc:0.678]
Epoch [18/120    avg_loss:1.417, val_acc:0.699]
Epoch [19/120    avg_loss:1.328, val_acc:0.711]
Epoch [20/120    avg_loss:1.286, val_acc:0.701]
Epoch [21/120    avg_loss:1.210, val_acc:0.742]
Epoch [22/120    avg_loss:1.175, val_acc:0.764]
Epoch [23/120    avg_loss:1.094, val_acc:0.752]
Epoch [24/120    avg_loss:1.054, val_acc:0.803]
Epoch [25/120    avg_loss:0.951, val_acc:0.770]
Epoch [26/120    avg_loss:0.908, val_acc:0.811]
Epoch [27/120    avg_loss:0.852, val_acc:0.822]
Epoch [28/120    avg_loss:0.782, val_acc:0.812]
Epoch [29/120    avg_loss:0.757, val_acc:0.816]
Epoch [30/120    avg_loss:0.709, val_acc:0.822]
Epoch [31/120    avg_loss:0.723, val_acc:0.805]
Epoch [32/120    avg_loss:0.743, val_acc:0.830]
Epoch [33/120    avg_loss:0.644, val_acc:0.807]
Epoch [34/120    avg_loss:0.609, val_acc:0.887]
Epoch [35/120    avg_loss:0.618, val_acc:0.842]
Epoch [36/120    avg_loss:0.583, val_acc:0.867]
Epoch [37/120    avg_loss:0.547, val_acc:0.926]
Epoch [38/120    avg_loss:0.486, val_acc:0.926]
Epoch [39/120    avg_loss:0.478, val_acc:0.924]
Epoch [40/120    avg_loss:0.509, val_acc:0.938]
Epoch [41/120    avg_loss:0.474, val_acc:0.928]
Epoch [42/120    avg_loss:0.463, val_acc:0.943]
Epoch [43/120    avg_loss:0.419, val_acc:0.936]
Epoch [44/120    avg_loss:0.397, val_acc:0.926]
Epoch [45/120    avg_loss:0.393, val_acc:0.941]
Epoch [46/120    avg_loss:0.434, val_acc:0.941]
Epoch [47/120    avg_loss:0.382, val_acc:0.928]
Epoch [48/120    avg_loss:0.348, val_acc:0.943]
Epoch [49/120    avg_loss:0.319, val_acc:0.934]
Epoch [50/120    avg_loss:0.341, val_acc:0.896]
Epoch [51/120    avg_loss:0.343, val_acc:0.930]
Epoch [52/120    avg_loss:0.303, val_acc:0.953]
Epoch [53/120    avg_loss:0.330, val_acc:0.926]
Epoch [54/120    avg_loss:0.304, val_acc:0.926]
Epoch [55/120    avg_loss:0.316, val_acc:0.920]
Epoch [56/120    avg_loss:0.379, val_acc:0.941]
Epoch [57/120    avg_loss:0.329, val_acc:0.926]
Epoch [58/120    avg_loss:0.303, val_acc:0.939]
Epoch [59/120    avg_loss:0.302, val_acc:0.930]
Epoch [60/120    avg_loss:0.259, val_acc:0.949]
Epoch [61/120    avg_loss:0.273, val_acc:0.947]
Epoch [62/120    avg_loss:0.259, val_acc:0.947]
Epoch [63/120    avg_loss:0.232, val_acc:0.963]
Epoch [64/120    avg_loss:0.267, val_acc:0.906]
Epoch [65/120    avg_loss:0.265, val_acc:0.959]
Epoch [66/120    avg_loss:0.213, val_acc:0.967]
Epoch [67/120    avg_loss:0.222, val_acc:0.965]
Epoch [68/120    avg_loss:0.235, val_acc:0.973]
Epoch [69/120    avg_loss:0.224, val_acc:0.965]
Epoch [70/120    avg_loss:0.244, val_acc:0.955]
Epoch [71/120    avg_loss:0.211, val_acc:0.959]
Epoch [72/120    avg_loss:0.178, val_acc:0.971]
Epoch [73/120    avg_loss:0.188, val_acc:0.961]
Epoch [74/120    avg_loss:0.314, val_acc:0.930]
Epoch [75/120    avg_loss:0.233, val_acc:0.965]
Epoch [76/120    avg_loss:0.217, val_acc:0.867]
Epoch [77/120    avg_loss:0.250, val_acc:0.967]
Epoch [78/120    avg_loss:0.170, val_acc:0.975]
Epoch [79/120    avg_loss:0.201, val_acc:0.959]
Epoch [80/120    avg_loss:0.221, val_acc:0.959]
Epoch [81/120    avg_loss:0.205, val_acc:0.971]
Epoch [82/120    avg_loss:0.148, val_acc:0.965]
Epoch [83/120    avg_loss:0.200, val_acc:0.973]
Epoch [84/120    avg_loss:0.161, val_acc:0.973]
Epoch [85/120    avg_loss:0.167, val_acc:0.961]
Epoch [86/120    avg_loss:0.129, val_acc:0.961]
Epoch [87/120    avg_loss:0.173, val_acc:0.961]
Epoch [88/120    avg_loss:0.245, val_acc:0.967]
Epoch [89/120    avg_loss:0.177, val_acc:0.973]
Epoch [90/120    avg_loss:0.121, val_acc:0.980]
Epoch [91/120    avg_loss:0.109, val_acc:0.984]
Epoch [92/120    avg_loss:0.110, val_acc:0.979]
Epoch [93/120    avg_loss:0.113, val_acc:0.980]
Epoch [94/120    avg_loss:0.086, val_acc:0.963]
Epoch [95/120    avg_loss:0.118, val_acc:0.979]
Epoch [96/120    avg_loss:0.119, val_acc:0.965]
Epoch [97/120    avg_loss:0.127, val_acc:0.979]
Epoch [98/120    avg_loss:0.110, val_acc:0.975]
Epoch [99/120    avg_loss:0.144, val_acc:0.975]
Epoch [100/120    avg_loss:0.112, val_acc:0.967]
Epoch [101/120    avg_loss:0.104, val_acc:0.971]
Epoch [102/120    avg_loss:0.104, val_acc:0.977]
Epoch [103/120    avg_loss:0.116, val_acc:0.979]
Epoch [104/120    avg_loss:0.113, val_acc:0.973]
Epoch [105/120    avg_loss:0.079, val_acc:0.982]
Epoch [106/120    avg_loss:0.073, val_acc:0.984]
Epoch [107/120    avg_loss:0.062, val_acc:0.982]
Epoch [108/120    avg_loss:0.060, val_acc:0.984]
Epoch [109/120    avg_loss:0.064, val_acc:0.984]
Epoch [110/120    avg_loss:0.070, val_acc:0.982]
Epoch [111/120    avg_loss:0.069, val_acc:0.982]
Epoch [112/120    avg_loss:0.057, val_acc:0.984]
Epoch [113/120    avg_loss:0.070, val_acc:0.984]
Epoch [114/120    avg_loss:0.062, val_acc:0.982]
Epoch [115/120    avg_loss:0.059, val_acc:0.984]
Epoch [116/120    avg_loss:0.063, val_acc:0.982]
Epoch [117/120    avg_loss:0.066, val_acc:0.986]
Epoch [118/120    avg_loss:0.073, val_acc:0.984]
Epoch [119/120    avg_loss:0.052, val_acc:0.980]
Epoch [120/120    avg_loss:0.059, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 218   7   0   0   0   1   4   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   1   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.96902655 0.97104677 0.90265487 0.87625418
 1.         0.91954023 0.998713   0.99574468 1.         1.
 0.99889503 1.        ]

Kappa:
0.9848061426594678
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0bcc06a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.148]
Epoch [2/120    avg_loss:2.523, val_acc:0.129]
Epoch [3/120    avg_loss:2.444, val_acc:0.246]
Epoch [4/120    avg_loss:2.356, val_acc:0.318]
Epoch [5/120    avg_loss:2.302, val_acc:0.342]
Epoch [6/120    avg_loss:2.219, val_acc:0.391]
Epoch [7/120    avg_loss:2.167, val_acc:0.426]
Epoch [8/120    avg_loss:2.072, val_acc:0.420]
Epoch [9/120    avg_loss:2.014, val_acc:0.453]
Epoch [10/120    avg_loss:1.938, val_acc:0.477]
Epoch [11/120    avg_loss:1.867, val_acc:0.482]
Epoch [12/120    avg_loss:1.786, val_acc:0.527]
Epoch [13/120    avg_loss:1.739, val_acc:0.547]
Epoch [14/120    avg_loss:1.658, val_acc:0.609]
Epoch [15/120    avg_loss:1.603, val_acc:0.658]
Epoch [16/120    avg_loss:1.543, val_acc:0.703]
Epoch [17/120    avg_loss:1.458, val_acc:0.701]
Epoch [18/120    avg_loss:1.423, val_acc:0.717]
Epoch [19/120    avg_loss:1.335, val_acc:0.742]
Epoch [20/120    avg_loss:1.262, val_acc:0.738]
Epoch [21/120    avg_loss:1.164, val_acc:0.740]
Epoch [22/120    avg_loss:1.077, val_acc:0.779]
Epoch [23/120    avg_loss:1.010, val_acc:0.875]
Epoch [24/120    avg_loss:0.939, val_acc:0.822]
Epoch [25/120    avg_loss:0.853, val_acc:0.834]
Epoch [26/120    avg_loss:0.808, val_acc:0.834]
Epoch [27/120    avg_loss:0.763, val_acc:0.902]
Epoch [28/120    avg_loss:0.732, val_acc:0.896]
Epoch [29/120    avg_loss:0.678, val_acc:0.887]
Epoch [30/120    avg_loss:0.691, val_acc:0.875]
Epoch [31/120    avg_loss:0.633, val_acc:0.918]
Epoch [32/120    avg_loss:0.597, val_acc:0.920]
Epoch [33/120    avg_loss:0.563, val_acc:0.902]
Epoch [34/120    avg_loss:0.577, val_acc:0.916]
Epoch [35/120    avg_loss:0.522, val_acc:0.898]
Epoch [36/120    avg_loss:0.525, val_acc:0.920]
Epoch [37/120    avg_loss:0.468, val_acc:0.918]
Epoch [38/120    avg_loss:0.452, val_acc:0.926]
Epoch [39/120    avg_loss:0.439, val_acc:0.930]
Epoch [40/120    avg_loss:0.419, val_acc:0.922]
Epoch [41/120    avg_loss:0.397, val_acc:0.930]
Epoch [42/120    avg_loss:0.390, val_acc:0.930]
Epoch [43/120    avg_loss:0.370, val_acc:0.908]
Epoch [44/120    avg_loss:0.394, val_acc:0.900]
Epoch [45/120    avg_loss:0.429, val_acc:0.938]
Epoch [46/120    avg_loss:0.342, val_acc:0.934]
Epoch [47/120    avg_loss:0.367, val_acc:0.906]
Epoch [48/120    avg_loss:0.375, val_acc:0.928]
Epoch [49/120    avg_loss:0.349, val_acc:0.941]
Epoch [50/120    avg_loss:0.337, val_acc:0.932]
Epoch [51/120    avg_loss:0.315, val_acc:0.938]
Epoch [52/120    avg_loss:0.337, val_acc:0.916]
Epoch [53/120    avg_loss:0.386, val_acc:0.916]
Epoch [54/120    avg_loss:0.322, val_acc:0.934]
Epoch [55/120    avg_loss:0.333, val_acc:0.934]
Epoch [56/120    avg_loss:0.354, val_acc:0.938]
Epoch [57/120    avg_loss:0.289, val_acc:0.953]
Epoch [58/120    avg_loss:0.240, val_acc:0.936]
Epoch [59/120    avg_loss:0.271, val_acc:0.957]
Epoch [60/120    avg_loss:0.250, val_acc:0.963]
Epoch [61/120    avg_loss:0.258, val_acc:0.963]
Epoch [62/120    avg_loss:0.282, val_acc:0.932]
Epoch [63/120    avg_loss:0.293, val_acc:0.941]
Epoch [64/120    avg_loss:0.255, val_acc:0.949]
Epoch [65/120    avg_loss:0.270, val_acc:0.939]
Epoch [66/120    avg_loss:0.229, val_acc:0.965]
Epoch [67/120    avg_loss:0.208, val_acc:0.957]
Epoch [68/120    avg_loss:0.207, val_acc:0.959]
Epoch [69/120    avg_loss:0.242, val_acc:0.957]
Epoch [70/120    avg_loss:0.227, val_acc:0.953]
Epoch [71/120    avg_loss:0.258, val_acc:0.938]
Epoch [72/120    avg_loss:0.247, val_acc:0.969]
Epoch [73/120    avg_loss:0.238, val_acc:0.959]
Epoch [74/120    avg_loss:0.192, val_acc:0.963]
Epoch [75/120    avg_loss:0.190, val_acc:0.969]
Epoch [76/120    avg_loss:0.162, val_acc:0.969]
Epoch [77/120    avg_loss:0.154, val_acc:0.957]
Epoch [78/120    avg_loss:0.176, val_acc:0.941]
Epoch [79/120    avg_loss:0.156, val_acc:0.965]
Epoch [80/120    avg_loss:0.161, val_acc:0.971]
Epoch [81/120    avg_loss:0.125, val_acc:0.963]
Epoch [82/120    avg_loss:0.118, val_acc:0.963]
Epoch [83/120    avg_loss:0.142, val_acc:0.971]
Epoch [84/120    avg_loss:0.133, val_acc:0.969]
Epoch [85/120    avg_loss:0.146, val_acc:0.975]
Epoch [86/120    avg_loss:0.110, val_acc:0.969]
Epoch [87/120    avg_loss:0.124, val_acc:0.951]
Epoch [88/120    avg_loss:0.160, val_acc:0.971]
Epoch [89/120    avg_loss:0.168, val_acc:0.973]
Epoch [90/120    avg_loss:0.155, val_acc:0.969]
Epoch [91/120    avg_loss:0.135, val_acc:0.957]
Epoch [92/120    avg_loss:0.167, val_acc:0.977]
Epoch [93/120    avg_loss:0.131, val_acc:0.977]
Epoch [94/120    avg_loss:0.101, val_acc:0.990]
Epoch [95/120    avg_loss:0.082, val_acc:0.977]
Epoch [96/120    avg_loss:0.108, val_acc:0.982]
Epoch [97/120    avg_loss:0.095, val_acc:0.979]
Epoch [98/120    avg_loss:0.108, val_acc:0.955]
Epoch [99/120    avg_loss:0.134, val_acc:0.969]
Epoch [100/120    avg_loss:0.106, val_acc:0.969]
Epoch [101/120    avg_loss:0.133, val_acc:0.961]
Epoch [102/120    avg_loss:0.124, val_acc:0.973]
Epoch [103/120    avg_loss:0.135, val_acc:0.977]
Epoch [104/120    avg_loss:0.110, val_acc:0.982]
Epoch [105/120    avg_loss:0.090, val_acc:0.975]
Epoch [106/120    avg_loss:0.117, val_acc:0.961]
Epoch [107/120    avg_loss:0.115, val_acc:0.973]
Epoch [108/120    avg_loss:0.100, val_acc:0.982]
Epoch [109/120    avg_loss:0.092, val_acc:0.977]
Epoch [110/120    avg_loss:0.084, val_acc:0.979]
Epoch [111/120    avg_loss:0.067, val_acc:0.984]
Epoch [112/120    avg_loss:0.067, val_acc:0.979]
Epoch [113/120    avg_loss:0.085, val_acc:0.975]
Epoch [114/120    avg_loss:0.065, val_acc:0.980]
Epoch [115/120    avg_loss:0.059, val_acc:0.986]
Epoch [116/120    avg_loss:0.057, val_acc:0.986]
Epoch [117/120    avg_loss:0.058, val_acc:0.984]
Epoch [118/120    avg_loss:0.062, val_acc:0.984]
Epoch [119/120    avg_loss:0.059, val_acc:0.984]
Epoch [120/120    avg_loss:0.053, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   2 224   3   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   3   0   0   1   0 202   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.997815   0.96       0.98678414 0.93607306 0.92258065
 0.99019608 0.91011236 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9881302093259098
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc25875470>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.078]
Epoch [2/120    avg_loss:2.561, val_acc:0.346]
Epoch [3/120    avg_loss:2.493, val_acc:0.404]
Epoch [4/120    avg_loss:2.419, val_acc:0.424]
Epoch [5/120    avg_loss:2.347, val_acc:0.422]
Epoch [6/120    avg_loss:2.282, val_acc:0.422]
Epoch [7/120    avg_loss:2.220, val_acc:0.443]
Epoch [8/120    avg_loss:2.148, val_acc:0.467]
Epoch [9/120    avg_loss:2.092, val_acc:0.484]
Epoch [10/120    avg_loss:2.038, val_acc:0.504]
Epoch [11/120    avg_loss:1.979, val_acc:0.523]
Epoch [12/120    avg_loss:1.942, val_acc:0.547]
Epoch [13/120    avg_loss:1.867, val_acc:0.566]
Epoch [14/120    avg_loss:1.809, val_acc:0.617]
Epoch [15/120    avg_loss:1.768, val_acc:0.623]
Epoch [16/120    avg_loss:1.693, val_acc:0.643]
Epoch [17/120    avg_loss:1.630, val_acc:0.656]
Epoch [18/120    avg_loss:1.579, val_acc:0.658]
Epoch [19/120    avg_loss:1.491, val_acc:0.680]
Epoch [20/120    avg_loss:1.400, val_acc:0.684]
Epoch [21/120    avg_loss:1.361, val_acc:0.680]
Epoch [22/120    avg_loss:1.279, val_acc:0.662]
Epoch [23/120    avg_loss:1.192, val_acc:0.705]
Epoch [24/120    avg_loss:1.154, val_acc:0.725]
Epoch [25/120    avg_loss:1.037, val_acc:0.768]
Epoch [26/120    avg_loss:0.954, val_acc:0.762]
Epoch [27/120    avg_loss:0.916, val_acc:0.771]
Epoch [28/120    avg_loss:0.866, val_acc:0.760]
Epoch [29/120    avg_loss:0.813, val_acc:0.846]
Epoch [30/120    avg_loss:0.762, val_acc:0.857]
Epoch [31/120    avg_loss:0.779, val_acc:0.820]
Epoch [32/120    avg_loss:0.748, val_acc:0.834]
Epoch [33/120    avg_loss:0.664, val_acc:0.883]
Epoch [34/120    avg_loss:0.621, val_acc:0.924]
Epoch [35/120    avg_loss:0.575, val_acc:0.928]
Epoch [36/120    avg_loss:0.578, val_acc:0.918]
Epoch [37/120    avg_loss:0.558, val_acc:0.922]
Epoch [38/120    avg_loss:0.554, val_acc:0.922]
Epoch [39/120    avg_loss:0.516, val_acc:0.924]
Epoch [40/120    avg_loss:0.451, val_acc:0.924]
Epoch [41/120    avg_loss:0.410, val_acc:0.934]
Epoch [42/120    avg_loss:0.418, val_acc:0.939]
Epoch [43/120    avg_loss:0.395, val_acc:0.916]
Epoch [44/120    avg_loss:0.434, val_acc:0.941]
Epoch [45/120    avg_loss:0.425, val_acc:0.918]
Epoch [46/120    avg_loss:0.354, val_acc:0.943]
Epoch [47/120    avg_loss:0.407, val_acc:0.916]
Epoch [48/120    avg_loss:0.432, val_acc:0.912]
Epoch [49/120    avg_loss:0.432, val_acc:0.928]
Epoch [50/120    avg_loss:0.329, val_acc:0.943]
Epoch [51/120    avg_loss:0.313, val_acc:0.930]
Epoch [52/120    avg_loss:0.372, val_acc:0.928]
Epoch [53/120    avg_loss:0.310, val_acc:0.955]
Epoch [54/120    avg_loss:0.288, val_acc:0.939]
Epoch [55/120    avg_loss:0.341, val_acc:0.941]
Epoch [56/120    avg_loss:0.296, val_acc:0.949]
Epoch [57/120    avg_loss:0.302, val_acc:0.941]
Epoch [58/120    avg_loss:0.278, val_acc:0.947]
Epoch [59/120    avg_loss:0.239, val_acc:0.951]
Epoch [60/120    avg_loss:0.309, val_acc:0.955]
Epoch [61/120    avg_loss:0.324, val_acc:0.926]
Epoch [62/120    avg_loss:0.316, val_acc:0.959]
Epoch [63/120    avg_loss:0.244, val_acc:0.965]
Epoch [64/120    avg_loss:0.238, val_acc:0.953]
Epoch [65/120    avg_loss:0.230, val_acc:0.934]
Epoch [66/120    avg_loss:0.206, val_acc:0.973]
Epoch [67/120    avg_loss:0.176, val_acc:0.961]
Epoch [68/120    avg_loss:0.194, val_acc:0.949]
Epoch [69/120    avg_loss:0.173, val_acc:0.969]
Epoch [70/120    avg_loss:0.194, val_acc:0.951]
Epoch [71/120    avg_loss:0.189, val_acc:0.967]
Epoch [72/120    avg_loss:0.178, val_acc:0.975]
Epoch [73/120    avg_loss:0.149, val_acc:0.975]
Epoch [74/120    avg_loss:0.150, val_acc:0.967]
Epoch [75/120    avg_loss:0.157, val_acc:0.965]
Epoch [76/120    avg_loss:0.194, val_acc:0.979]
Epoch [77/120    avg_loss:0.134, val_acc:0.967]
Epoch [78/120    avg_loss:0.147, val_acc:0.975]
Epoch [79/120    avg_loss:0.122, val_acc:0.977]
Epoch [80/120    avg_loss:0.127, val_acc:0.977]
Epoch [81/120    avg_loss:0.119, val_acc:0.979]
Epoch [82/120    avg_loss:0.126, val_acc:0.975]
Epoch [83/120    avg_loss:0.145, val_acc:0.971]
Epoch [84/120    avg_loss:0.169, val_acc:0.973]
Epoch [85/120    avg_loss:0.135, val_acc:0.965]
Epoch [86/120    avg_loss:0.129, val_acc:0.977]
Epoch [87/120    avg_loss:0.102, val_acc:0.979]
Epoch [88/120    avg_loss:0.105, val_acc:0.982]
Epoch [89/120    avg_loss:0.109, val_acc:0.973]
Epoch [90/120    avg_loss:0.102, val_acc:0.975]
Epoch [91/120    avg_loss:0.105, val_acc:0.971]
Epoch [92/120    avg_loss:0.106, val_acc:0.975]
Epoch [93/120    avg_loss:0.117, val_acc:0.975]
Epoch [94/120    avg_loss:0.115, val_acc:0.980]
Epoch [95/120    avg_loss:0.116, val_acc:0.953]
Epoch [96/120    avg_loss:0.183, val_acc:0.979]
Epoch [97/120    avg_loss:0.161, val_acc:0.969]
Epoch [98/120    avg_loss:0.166, val_acc:0.973]
Epoch [99/120    avg_loss:0.147, val_acc:0.971]
Epoch [100/120    avg_loss:0.120, val_acc:0.955]
Epoch [101/120    avg_loss:0.143, val_acc:0.955]
Epoch [102/120    avg_loss:0.116, val_acc:0.971]
Epoch [103/120    avg_loss:0.090, val_acc:0.980]
Epoch [104/120    avg_loss:0.087, val_acc:0.984]
Epoch [105/120    avg_loss:0.078, val_acc:0.984]
Epoch [106/120    avg_loss:0.072, val_acc:0.984]
Epoch [107/120    avg_loss:0.075, val_acc:0.984]
Epoch [108/120    avg_loss:0.068, val_acc:0.982]
Epoch [109/120    avg_loss:0.081, val_acc:0.982]
Epoch [110/120    avg_loss:0.070, val_acc:0.982]
Epoch [111/120    avg_loss:0.086, val_acc:0.984]
Epoch [112/120    avg_loss:0.080, val_acc:0.984]
Epoch [113/120    avg_loss:0.069, val_acc:0.982]
Epoch [114/120    avg_loss:0.070, val_acc:0.984]
Epoch [115/120    avg_loss:0.064, val_acc:0.984]
Epoch [116/120    avg_loss:0.067, val_acc:0.984]
Epoch [117/120    avg_loss:0.060, val_acc:0.984]
Epoch [118/120    avg_loss:0.061, val_acc:0.982]
Epoch [119/120    avg_loss:0.072, val_acc:0.984]
Epoch [120/120    avg_loss:0.062, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 218  10   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 1.         0.94570136 0.97321429 0.90868597 0.89836066
 1.         0.86956522 0.99742931 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9838583617300037
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fed4ccb9b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.335]
Epoch [2/120    avg_loss:2.521, val_acc:0.346]
Epoch [3/120    avg_loss:2.434, val_acc:0.390]
Epoch [4/120    avg_loss:2.382, val_acc:0.373]
Epoch [5/120    avg_loss:2.327, val_acc:0.385]
Epoch [6/120    avg_loss:2.260, val_acc:0.398]
Epoch [7/120    avg_loss:2.215, val_acc:0.450]
Epoch [8/120    avg_loss:2.190, val_acc:0.490]
Epoch [9/120    avg_loss:2.131, val_acc:0.515]
Epoch [10/120    avg_loss:2.072, val_acc:0.556]
Epoch [11/120    avg_loss:1.995, val_acc:0.548]
Epoch [12/120    avg_loss:1.930, val_acc:0.554]
Epoch [13/120    avg_loss:1.864, val_acc:0.567]
Epoch [14/120    avg_loss:1.793, val_acc:0.575]
Epoch [15/120    avg_loss:1.744, val_acc:0.696]
Epoch [16/120    avg_loss:1.636, val_acc:0.696]
Epoch [17/120    avg_loss:1.568, val_acc:0.675]
Epoch [18/120    avg_loss:1.508, val_acc:0.735]
Epoch [19/120    avg_loss:1.430, val_acc:0.781]
Epoch [20/120    avg_loss:1.374, val_acc:0.802]
Epoch [21/120    avg_loss:1.296, val_acc:0.852]
Epoch [22/120    avg_loss:1.196, val_acc:0.844]
Epoch [23/120    avg_loss:1.136, val_acc:0.860]
Epoch [24/120    avg_loss:1.065, val_acc:0.869]
Epoch [25/120    avg_loss:1.034, val_acc:0.848]
Epoch [26/120    avg_loss:0.957, val_acc:0.883]
Epoch [27/120    avg_loss:0.898, val_acc:0.873]
Epoch [28/120    avg_loss:0.815, val_acc:0.892]
Epoch [29/120    avg_loss:0.797, val_acc:0.892]
Epoch [30/120    avg_loss:0.718, val_acc:0.902]
Epoch [31/120    avg_loss:0.675, val_acc:0.854]
Epoch [32/120    avg_loss:0.673, val_acc:0.885]
Epoch [33/120    avg_loss:0.659, val_acc:0.919]
Epoch [34/120    avg_loss:0.540, val_acc:0.894]
Epoch [35/120    avg_loss:0.585, val_acc:0.912]
Epoch [36/120    avg_loss:0.576, val_acc:0.908]
Epoch [37/120    avg_loss:0.551, val_acc:0.919]
Epoch [38/120    avg_loss:0.488, val_acc:0.923]
Epoch [39/120    avg_loss:0.465, val_acc:0.902]
Epoch [40/120    avg_loss:0.471, val_acc:0.902]
Epoch [41/120    avg_loss:0.436, val_acc:0.933]
Epoch [42/120    avg_loss:0.481, val_acc:0.821]
Epoch [43/120    avg_loss:0.439, val_acc:0.912]
Epoch [44/120    avg_loss:0.421, val_acc:0.935]
Epoch [45/120    avg_loss:0.388, val_acc:0.929]
Epoch [46/120    avg_loss:0.419, val_acc:0.915]
Epoch [47/120    avg_loss:0.417, val_acc:0.904]
Epoch [48/120    avg_loss:0.368, val_acc:0.931]
Epoch [49/120    avg_loss:0.357, val_acc:0.923]
Epoch [50/120    avg_loss:0.361, val_acc:0.929]
Epoch [51/120    avg_loss:0.328, val_acc:0.940]
Epoch [52/120    avg_loss:0.341, val_acc:0.919]
Epoch [53/120    avg_loss:0.314, val_acc:0.927]
Epoch [54/120    avg_loss:0.308, val_acc:0.950]
Epoch [55/120    avg_loss:0.320, val_acc:0.948]
Epoch [56/120    avg_loss:0.251, val_acc:0.931]
Epoch [57/120    avg_loss:0.299, val_acc:0.960]
Epoch [58/120    avg_loss:0.260, val_acc:0.954]
Epoch [59/120    avg_loss:0.252, val_acc:0.927]
Epoch [60/120    avg_loss:0.340, val_acc:0.954]
Epoch [61/120    avg_loss:0.311, val_acc:0.938]
Epoch [62/120    avg_loss:0.262, val_acc:0.954]
Epoch [63/120    avg_loss:0.265, val_acc:0.923]
Epoch [64/120    avg_loss:0.259, val_acc:0.946]
Epoch [65/120    avg_loss:0.245, val_acc:0.954]
Epoch [66/120    avg_loss:0.287, val_acc:0.958]
Epoch [67/120    avg_loss:0.227, val_acc:0.927]
Epoch [68/120    avg_loss:0.226, val_acc:0.950]
Epoch [69/120    avg_loss:0.214, val_acc:0.948]
Epoch [70/120    avg_loss:0.187, val_acc:0.960]
Epoch [71/120    avg_loss:0.182, val_acc:0.956]
Epoch [72/120    avg_loss:0.200, val_acc:0.960]
Epoch [73/120    avg_loss:0.329, val_acc:0.940]
Epoch [74/120    avg_loss:0.275, val_acc:0.960]
Epoch [75/120    avg_loss:0.204, val_acc:0.971]
Epoch [76/120    avg_loss:0.178, val_acc:0.969]
Epoch [77/120    avg_loss:0.213, val_acc:0.952]
Epoch [78/120    avg_loss:0.246, val_acc:0.965]
Epoch [79/120    avg_loss:0.207, val_acc:0.973]
Epoch [80/120    avg_loss:0.209, val_acc:0.958]
Epoch [81/120    avg_loss:0.170, val_acc:0.960]
Epoch [82/120    avg_loss:0.175, val_acc:0.952]
Epoch [83/120    avg_loss:0.176, val_acc:0.950]
Epoch [84/120    avg_loss:0.179, val_acc:0.969]
Epoch [85/120    avg_loss:0.298, val_acc:0.927]
Epoch [86/120    avg_loss:0.184, val_acc:0.967]
Epoch [87/120    avg_loss:0.204, val_acc:0.973]
Epoch [88/120    avg_loss:0.194, val_acc:0.956]
Epoch [89/120    avg_loss:0.207, val_acc:0.977]
Epoch [90/120    avg_loss:0.167, val_acc:0.956]
Epoch [91/120    avg_loss:0.191, val_acc:0.967]
Epoch [92/120    avg_loss:0.157, val_acc:0.965]
Epoch [93/120    avg_loss:0.118, val_acc:0.977]
Epoch [94/120    avg_loss:0.111, val_acc:0.950]
Epoch [95/120    avg_loss:0.127, val_acc:0.963]
Epoch [96/120    avg_loss:0.253, val_acc:0.967]
Epoch [97/120    avg_loss:0.178, val_acc:0.965]
Epoch [98/120    avg_loss:0.166, val_acc:0.967]
Epoch [99/120    avg_loss:0.168, val_acc:0.958]
Epoch [100/120    avg_loss:0.149, val_acc:0.950]
Epoch [101/120    avg_loss:0.143, val_acc:0.965]
Epoch [102/120    avg_loss:0.117, val_acc:0.977]
Epoch [103/120    avg_loss:0.106, val_acc:0.977]
Epoch [104/120    avg_loss:0.118, val_acc:0.977]
Epoch [105/120    avg_loss:0.112, val_acc:0.981]
Epoch [106/120    avg_loss:0.099, val_acc:0.985]
Epoch [107/120    avg_loss:0.147, val_acc:0.960]
Epoch [108/120    avg_loss:0.123, val_acc:0.960]
Epoch [109/120    avg_loss:0.085, val_acc:0.990]
Epoch [110/120    avg_loss:0.076, val_acc:0.973]
Epoch [111/120    avg_loss:0.153, val_acc:0.931]
Epoch [112/120    avg_loss:0.197, val_acc:0.952]
Epoch [113/120    avg_loss:0.125, val_acc:0.958]
Epoch [114/120    avg_loss:0.150, val_acc:0.952]
Epoch [115/120    avg_loss:0.225, val_acc:0.940]
Epoch [116/120    avg_loss:0.219, val_acc:0.960]
Epoch [117/120    avg_loss:0.174, val_acc:0.965]
Epoch [118/120    avg_loss:0.127, val_acc:0.973]
Epoch [119/120    avg_loss:0.119, val_acc:0.979]
Epoch [120/120    avg_loss:0.099, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 202   0   0   0   0  15   0   0   0   0   0   0]
 [  0   0   0 215  12   2   0   0   0   1   0   0   0   0]
 [  0   0   0   1 207  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.16631130063966

F1 scores:
[       nan 0.99854227 0.93953488 0.96412556 0.87898089 0.85121107
 0.99266504 0.8989899  0.99481865 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9795858664998472
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f855bd81b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.127]
Epoch [2/120    avg_loss:2.550, val_acc:0.312]
Epoch [3/120    avg_loss:2.469, val_acc:0.425]
Epoch [4/120    avg_loss:2.390, val_acc:0.469]
Epoch [5/120    avg_loss:2.323, val_acc:0.490]
Epoch [6/120    avg_loss:2.257, val_acc:0.502]
Epoch [7/120    avg_loss:2.196, val_acc:0.521]
Epoch [8/120    avg_loss:2.119, val_acc:0.521]
Epoch [9/120    avg_loss:2.049, val_acc:0.544]
Epoch [10/120    avg_loss:1.980, val_acc:0.571]
Epoch [11/120    avg_loss:1.908, val_acc:0.588]
Epoch [12/120    avg_loss:1.856, val_acc:0.621]
Epoch [13/120    avg_loss:1.788, val_acc:0.654]
Epoch [14/120    avg_loss:1.704, val_acc:0.683]
Epoch [15/120    avg_loss:1.647, val_acc:0.688]
Epoch [16/120    avg_loss:1.577, val_acc:0.702]
Epoch [17/120    avg_loss:1.475, val_acc:0.696]
Epoch [18/120    avg_loss:1.417, val_acc:0.704]
Epoch [19/120    avg_loss:1.346, val_acc:0.733]
Epoch [20/120    avg_loss:1.254, val_acc:0.740]
Epoch [21/120    avg_loss:1.214, val_acc:0.758]
Epoch [22/120    avg_loss:1.152, val_acc:0.769]
Epoch [23/120    avg_loss:1.089, val_acc:0.798]
Epoch [24/120    avg_loss:1.008, val_acc:0.804]
Epoch [25/120    avg_loss:0.950, val_acc:0.798]
Epoch [26/120    avg_loss:0.909, val_acc:0.875]
Epoch [27/120    avg_loss:0.853, val_acc:0.879]
Epoch [28/120    avg_loss:0.777, val_acc:0.917]
Epoch [29/120    avg_loss:0.742, val_acc:0.938]
Epoch [30/120    avg_loss:0.738, val_acc:0.925]
Epoch [31/120    avg_loss:0.682, val_acc:0.931]
Epoch [32/120    avg_loss:0.632, val_acc:0.929]
Epoch [33/120    avg_loss:0.590, val_acc:0.938]
Epoch [34/120    avg_loss:0.544, val_acc:0.933]
Epoch [35/120    avg_loss:0.513, val_acc:0.942]
Epoch [36/120    avg_loss:0.492, val_acc:0.917]
Epoch [37/120    avg_loss:0.513, val_acc:0.946]
Epoch [38/120    avg_loss:0.505, val_acc:0.940]
Epoch [39/120    avg_loss:0.450, val_acc:0.946]
Epoch [40/120    avg_loss:0.482, val_acc:0.910]
Epoch [41/120    avg_loss:0.487, val_acc:0.942]
Epoch [42/120    avg_loss:0.417, val_acc:0.938]
Epoch [43/120    avg_loss:0.401, val_acc:0.950]
Epoch [44/120    avg_loss:0.387, val_acc:0.944]
Epoch [45/120    avg_loss:0.420, val_acc:0.879]
Epoch [46/120    avg_loss:0.461, val_acc:0.933]
Epoch [47/120    avg_loss:0.418, val_acc:0.925]
Epoch [48/120    avg_loss:0.379, val_acc:0.940]
Epoch [49/120    avg_loss:0.386, val_acc:0.944]
Epoch [50/120    avg_loss:0.349, val_acc:0.952]
Epoch [51/120    avg_loss:0.371, val_acc:0.946]
Epoch [52/120    avg_loss:0.344, val_acc:0.960]
Epoch [53/120    avg_loss:0.321, val_acc:0.933]
Epoch [54/120    avg_loss:0.320, val_acc:0.969]
Epoch [55/120    avg_loss:0.292, val_acc:0.942]
Epoch [56/120    avg_loss:0.261, val_acc:0.971]
Epoch [57/120    avg_loss:0.235, val_acc:0.950]
Epoch [58/120    avg_loss:0.265, val_acc:0.963]
Epoch [59/120    avg_loss:0.301, val_acc:0.948]
Epoch [60/120    avg_loss:0.259, val_acc:0.975]
Epoch [61/120    avg_loss:0.274, val_acc:0.958]
Epoch [62/120    avg_loss:0.280, val_acc:0.971]
Epoch [63/120    avg_loss:0.297, val_acc:0.952]
Epoch [64/120    avg_loss:0.290, val_acc:0.967]
Epoch [65/120    avg_loss:0.234, val_acc:0.977]
Epoch [66/120    avg_loss:0.209, val_acc:0.988]
Epoch [67/120    avg_loss:0.202, val_acc:0.973]
Epoch [68/120    avg_loss:0.178, val_acc:0.985]
Epoch [69/120    avg_loss:0.171, val_acc:0.981]
Epoch [70/120    avg_loss:0.145, val_acc:0.981]
Epoch [71/120    avg_loss:0.172, val_acc:0.956]
Epoch [72/120    avg_loss:0.181, val_acc:0.985]
Epoch [73/120    avg_loss:0.171, val_acc:0.988]
Epoch [74/120    avg_loss:0.166, val_acc:0.981]
Epoch [75/120    avg_loss:0.158, val_acc:0.979]
Epoch [76/120    avg_loss:0.141, val_acc:0.988]
Epoch [77/120    avg_loss:0.195, val_acc:0.973]
Epoch [78/120    avg_loss:0.182, val_acc:0.985]
Epoch [79/120    avg_loss:0.205, val_acc:0.979]
Epoch [80/120    avg_loss:0.184, val_acc:0.990]
Epoch [81/120    avg_loss:0.190, val_acc:0.960]
Epoch [82/120    avg_loss:0.200, val_acc:0.967]
Epoch [83/120    avg_loss:0.241, val_acc:0.960]
Epoch [84/120    avg_loss:0.228, val_acc:0.981]
Epoch [85/120    avg_loss:0.149, val_acc:0.988]
Epoch [86/120    avg_loss:0.138, val_acc:0.975]
Epoch [87/120    avg_loss:0.149, val_acc:0.977]
Epoch [88/120    avg_loss:0.160, val_acc:0.979]
Epoch [89/120    avg_loss:0.160, val_acc:0.983]
Epoch [90/120    avg_loss:0.143, val_acc:0.992]
Epoch [91/120    avg_loss:0.098, val_acc:0.983]
Epoch [92/120    avg_loss:0.093, val_acc:0.981]
Epoch [93/120    avg_loss:0.083, val_acc:0.992]
Epoch [94/120    avg_loss:0.170, val_acc:0.990]
Epoch [95/120    avg_loss:0.149, val_acc:0.983]
Epoch [96/120    avg_loss:0.160, val_acc:0.981]
Epoch [97/120    avg_loss:0.151, val_acc:0.988]
Epoch [98/120    avg_loss:0.123, val_acc:0.990]
Epoch [99/120    avg_loss:0.103, val_acc:0.985]
Epoch [100/120    avg_loss:0.084, val_acc:0.994]
Epoch [101/120    avg_loss:0.100, val_acc:0.998]
Epoch [102/120    avg_loss:0.086, val_acc:1.000]
Epoch [103/120    avg_loss:0.078, val_acc:1.000]
Epoch [104/120    avg_loss:0.086, val_acc:0.994]
Epoch [105/120    avg_loss:0.093, val_acc:0.998]
Epoch [106/120    avg_loss:0.074, val_acc:0.992]
Epoch [107/120    avg_loss:0.059, val_acc:0.996]
Epoch [108/120    avg_loss:0.064, val_acc:0.996]
Epoch [109/120    avg_loss:0.069, val_acc:0.992]
Epoch [110/120    avg_loss:0.064, val_acc:0.985]
Epoch [111/120    avg_loss:0.074, val_acc:0.996]
Epoch [112/120    avg_loss:0.085, val_acc:0.977]
Epoch [113/120    avg_loss:0.077, val_acc:0.985]
Epoch [114/120    avg_loss:0.079, val_acc:0.985]
Epoch [115/120    avg_loss:0.076, val_acc:0.979]
Epoch [116/120    avg_loss:0.065, val_acc:0.994]
Epoch [117/120    avg_loss:0.061, val_acc:0.998]
Epoch [118/120    avg_loss:0.054, val_acc:0.998]
Epoch [119/120    avg_loss:0.051, val_acc:0.996]
Epoch [120/120    avg_loss:0.045, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.9569161  0.99563319 0.94091904 0.90592334
 1.         0.90322581 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9888426633311244
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb443b88b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.092]
Epoch [2/120    avg_loss:2.520, val_acc:0.302]
Epoch [3/120    avg_loss:2.446, val_acc:0.317]
Epoch [4/120    avg_loss:2.376, val_acc:0.306]
Epoch [5/120    avg_loss:2.318, val_acc:0.333]
Epoch [6/120    avg_loss:2.266, val_acc:0.421]
Epoch [7/120    avg_loss:2.207, val_acc:0.477]
Epoch [8/120    avg_loss:2.152, val_acc:0.540]
Epoch [9/120    avg_loss:2.104, val_acc:0.567]
Epoch [10/120    avg_loss:2.056, val_acc:0.585]
Epoch [11/120    avg_loss:1.971, val_acc:0.600]
Epoch [12/120    avg_loss:1.904, val_acc:0.606]
Epoch [13/120    avg_loss:1.809, val_acc:0.631]
Epoch [14/120    avg_loss:1.740, val_acc:0.627]
Epoch [15/120    avg_loss:1.697, val_acc:0.644]
Epoch [16/120    avg_loss:1.614, val_acc:0.679]
Epoch [17/120    avg_loss:1.537, val_acc:0.667]
Epoch [18/120    avg_loss:1.456, val_acc:0.658]
Epoch [19/120    avg_loss:1.395, val_acc:0.685]
Epoch [20/120    avg_loss:1.322, val_acc:0.690]
Epoch [21/120    avg_loss:1.255, val_acc:0.713]
Epoch [22/120    avg_loss:1.183, val_acc:0.713]
Epoch [23/120    avg_loss:1.102, val_acc:0.831]
Epoch [24/120    avg_loss:1.083, val_acc:0.833]
Epoch [25/120    avg_loss:1.007, val_acc:0.869]
Epoch [26/120    avg_loss:0.926, val_acc:0.892]
Epoch [27/120    avg_loss:0.895, val_acc:0.879]
Epoch [28/120    avg_loss:0.842, val_acc:0.910]
Epoch [29/120    avg_loss:0.751, val_acc:0.912]
Epoch [30/120    avg_loss:0.709, val_acc:0.858]
Epoch [31/120    avg_loss:0.713, val_acc:0.904]
Epoch [32/120    avg_loss:0.692, val_acc:0.919]
Epoch [33/120    avg_loss:0.639, val_acc:0.894]
Epoch [34/120    avg_loss:0.661, val_acc:0.906]
Epoch [35/120    avg_loss:0.598, val_acc:0.919]
Epoch [36/120    avg_loss:0.520, val_acc:0.915]
Epoch [37/120    avg_loss:0.499, val_acc:0.906]
Epoch [38/120    avg_loss:0.460, val_acc:0.912]
Epoch [39/120    avg_loss:0.464, val_acc:0.933]
Epoch [40/120    avg_loss:0.425, val_acc:0.923]
Epoch [41/120    avg_loss:0.417, val_acc:0.873]
Epoch [42/120    avg_loss:0.459, val_acc:0.906]
Epoch [43/120    avg_loss:0.442, val_acc:0.908]
Epoch [44/120    avg_loss:0.410, val_acc:0.912]
Epoch [45/120    avg_loss:0.377, val_acc:0.923]
Epoch [46/120    avg_loss:0.355, val_acc:0.919]
Epoch [47/120    avg_loss:0.373, val_acc:0.919]
Epoch [48/120    avg_loss:0.389, val_acc:0.933]
Epoch [49/120    avg_loss:0.373, val_acc:0.942]
Epoch [50/120    avg_loss:0.341, val_acc:0.942]
Epoch [51/120    avg_loss:0.344, val_acc:0.931]
Epoch [52/120    avg_loss:0.347, val_acc:0.912]
Epoch [53/120    avg_loss:0.367, val_acc:0.948]
Epoch [54/120    avg_loss:0.279, val_acc:0.938]
Epoch [55/120    avg_loss:0.343, val_acc:0.944]
Epoch [56/120    avg_loss:0.336, val_acc:0.946]
Epoch [57/120    avg_loss:0.289, val_acc:0.944]
Epoch [58/120    avg_loss:0.264, val_acc:0.940]
Epoch [59/120    avg_loss:0.248, val_acc:0.940]
Epoch [60/120    avg_loss:0.261, val_acc:0.935]
Epoch [61/120    avg_loss:0.365, val_acc:0.938]
Epoch [62/120    avg_loss:0.307, val_acc:0.952]
Epoch [63/120    avg_loss:0.269, val_acc:0.935]
Epoch [64/120    avg_loss:0.264, val_acc:0.956]
Epoch [65/120    avg_loss:0.254, val_acc:0.946]
Epoch [66/120    avg_loss:0.237, val_acc:0.929]
Epoch [67/120    avg_loss:0.296, val_acc:0.952]
Epoch [68/120    avg_loss:0.238, val_acc:0.929]
Epoch [69/120    avg_loss:0.268, val_acc:0.950]
Epoch [70/120    avg_loss:0.260, val_acc:0.952]
Epoch [71/120    avg_loss:0.281, val_acc:0.921]
Epoch [72/120    avg_loss:0.292, val_acc:0.946]
Epoch [73/120    avg_loss:0.273, val_acc:0.908]
Epoch [74/120    avg_loss:0.294, val_acc:0.915]
Epoch [75/120    avg_loss:0.222, val_acc:0.952]
Epoch [76/120    avg_loss:0.247, val_acc:0.950]
Epoch [77/120    avg_loss:0.187, val_acc:0.960]
Epoch [78/120    avg_loss:0.189, val_acc:0.956]
Epoch [79/120    avg_loss:0.200, val_acc:0.954]
Epoch [80/120    avg_loss:0.187, val_acc:0.958]
Epoch [81/120    avg_loss:0.174, val_acc:0.965]
Epoch [82/120    avg_loss:0.178, val_acc:0.973]
Epoch [83/120    avg_loss:0.202, val_acc:0.960]
Epoch [84/120    avg_loss:0.191, val_acc:0.963]
Epoch [85/120    avg_loss:0.200, val_acc:0.910]
Epoch [86/120    avg_loss:0.220, val_acc:0.956]
Epoch [87/120    avg_loss:0.186, val_acc:0.963]
Epoch [88/120    avg_loss:0.140, val_acc:0.954]
Epoch [89/120    avg_loss:0.138, val_acc:0.956]
Epoch [90/120    avg_loss:0.245, val_acc:0.940]
Epoch [91/120    avg_loss:0.226, val_acc:0.958]
Epoch [92/120    avg_loss:0.151, val_acc:0.929]
Epoch [93/120    avg_loss:0.193, val_acc:0.954]
Epoch [94/120    avg_loss:0.145, val_acc:0.958]
Epoch [95/120    avg_loss:0.145, val_acc:0.965]
Epoch [96/120    avg_loss:0.132, val_acc:0.965]
Epoch [97/120    avg_loss:0.102, val_acc:0.969]
Epoch [98/120    avg_loss:0.090, val_acc:0.967]
Epoch [99/120    avg_loss:0.100, val_acc:0.971]
Epoch [100/120    avg_loss:0.092, val_acc:0.967]
Epoch [101/120    avg_loss:0.103, val_acc:0.965]
Epoch [102/120    avg_loss:0.085, val_acc:0.965]
Epoch [103/120    avg_loss:0.094, val_acc:0.965]
Epoch [104/120    avg_loss:0.095, val_acc:0.967]
Epoch [105/120    avg_loss:0.084, val_acc:0.969]
Epoch [106/120    avg_loss:0.092, val_acc:0.965]
Epoch [107/120    avg_loss:0.092, val_acc:0.965]
Epoch [108/120    avg_loss:0.107, val_acc:0.967]
Epoch [109/120    avg_loss:0.090, val_acc:0.967]
Epoch [110/120    avg_loss:0.080, val_acc:0.965]
Epoch [111/120    avg_loss:0.095, val_acc:0.965]
Epoch [112/120    avg_loss:0.089, val_acc:0.965]
Epoch [113/120    avg_loss:0.083, val_acc:0.965]
Epoch [114/120    avg_loss:0.090, val_acc:0.965]
Epoch [115/120    avg_loss:0.104, val_acc:0.965]
Epoch [116/120    avg_loss:0.083, val_acc:0.965]
Epoch [117/120    avg_loss:0.083, val_acc:0.965]
Epoch [118/120    avg_loss:0.097, val_acc:0.965]
Epoch [119/120    avg_loss:0.073, val_acc:0.965]
Epoch [120/120    avg_loss:0.094, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   2 224   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 201  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.4861407249467

F1 scores:
[       nan 1.         0.94854586 0.98461538 0.89333333 0.85521886
 1.         0.8839779  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9831456863809007
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdbb7d7da90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.196]
Epoch [2/120    avg_loss:2.537, val_acc:0.327]
Epoch [3/120    avg_loss:2.465, val_acc:0.383]
Epoch [4/120    avg_loss:2.400, val_acc:0.381]
Epoch [5/120    avg_loss:2.341, val_acc:0.390]
Epoch [6/120    avg_loss:2.290, val_acc:0.415]
Epoch [7/120    avg_loss:2.240, val_acc:0.448]
Epoch [8/120    avg_loss:2.199, val_acc:0.479]
Epoch [9/120    avg_loss:2.138, val_acc:0.510]
Epoch [10/120    avg_loss:2.097, val_acc:0.550]
Epoch [11/120    avg_loss:2.021, val_acc:0.550]
Epoch [12/120    avg_loss:1.983, val_acc:0.562]
Epoch [13/120    avg_loss:1.918, val_acc:0.600]
Epoch [14/120    avg_loss:1.857, val_acc:0.629]
Epoch [15/120    avg_loss:1.803, val_acc:0.658]
Epoch [16/120    avg_loss:1.720, val_acc:0.667]
Epoch [17/120    avg_loss:1.646, val_acc:0.677]
Epoch [18/120    avg_loss:1.568, val_acc:0.692]
Epoch [19/120    avg_loss:1.501, val_acc:0.685]
Epoch [20/120    avg_loss:1.442, val_acc:0.673]
Epoch [21/120    avg_loss:1.352, val_acc:0.719]
Epoch [22/120    avg_loss:1.282, val_acc:0.725]
Epoch [23/120    avg_loss:1.193, val_acc:0.733]
Epoch [24/120    avg_loss:1.136, val_acc:0.831]
Epoch [25/120    avg_loss:1.053, val_acc:0.760]
Epoch [26/120    avg_loss:0.970, val_acc:0.877]
Epoch [27/120    avg_loss:0.884, val_acc:0.831]
Epoch [28/120    avg_loss:0.861, val_acc:0.850]
Epoch [29/120    avg_loss:0.765, val_acc:0.871]
Epoch [30/120    avg_loss:0.773, val_acc:0.865]
Epoch [31/120    avg_loss:0.711, val_acc:0.865]
Epoch [32/120    avg_loss:0.704, val_acc:0.898]
Epoch [33/120    avg_loss:0.658, val_acc:0.900]
Epoch [34/120    avg_loss:0.673, val_acc:0.890]
Epoch [35/120    avg_loss:0.582, val_acc:0.925]
Epoch [36/120    avg_loss:0.592, val_acc:0.910]
Epoch [37/120    avg_loss:0.535, val_acc:0.942]
Epoch [38/120    avg_loss:0.483, val_acc:0.925]
Epoch [39/120    avg_loss:0.475, val_acc:0.915]
Epoch [40/120    avg_loss:0.529, val_acc:0.927]
Epoch [41/120    avg_loss:0.457, val_acc:0.908]
Epoch [42/120    avg_loss:0.483, val_acc:0.900]
Epoch [43/120    avg_loss:0.474, val_acc:0.912]
Epoch [44/120    avg_loss:0.411, val_acc:0.944]
Epoch [45/120    avg_loss:0.397, val_acc:0.935]
Epoch [46/120    avg_loss:0.394, val_acc:0.942]
Epoch [47/120    avg_loss:0.377, val_acc:0.944]
Epoch [48/120    avg_loss:0.364, val_acc:0.954]
Epoch [49/120    avg_loss:0.333, val_acc:0.950]
Epoch [50/120    avg_loss:0.348, val_acc:0.960]
Epoch [51/120    avg_loss:0.278, val_acc:0.927]
Epoch [52/120    avg_loss:0.305, val_acc:0.950]
Epoch [53/120    avg_loss:0.288, val_acc:0.956]
Epoch [54/120    avg_loss:0.308, val_acc:0.952]
Epoch [55/120    avg_loss:0.322, val_acc:0.942]
Epoch [56/120    avg_loss:0.276, val_acc:0.946]
Epoch [57/120    avg_loss:0.256, val_acc:0.958]
Epoch [58/120    avg_loss:0.291, val_acc:0.946]
Epoch [59/120    avg_loss:0.277, val_acc:0.954]
Epoch [60/120    avg_loss:0.274, val_acc:0.960]
Epoch [61/120    avg_loss:0.298, val_acc:0.919]
Epoch [62/120    avg_loss:0.296, val_acc:0.952]
Epoch [63/120    avg_loss:0.230, val_acc:0.963]
Epoch [64/120    avg_loss:0.234, val_acc:0.948]
Epoch [65/120    avg_loss:0.288, val_acc:0.948]
Epoch [66/120    avg_loss:0.275, val_acc:0.952]
Epoch [67/120    avg_loss:0.225, val_acc:0.971]
Epoch [68/120    avg_loss:0.201, val_acc:0.956]
Epoch [69/120    avg_loss:0.169, val_acc:0.985]
Epoch [70/120    avg_loss:0.152, val_acc:0.979]
Epoch [71/120    avg_loss:0.169, val_acc:0.946]
Epoch [72/120    avg_loss:0.237, val_acc:0.948]
Epoch [73/120    avg_loss:0.163, val_acc:0.977]
Epoch [74/120    avg_loss:0.152, val_acc:0.963]
Epoch [75/120    avg_loss:0.204, val_acc:0.971]
Epoch [76/120    avg_loss:0.215, val_acc:0.973]
Epoch [77/120    avg_loss:0.225, val_acc:0.973]
Epoch [78/120    avg_loss:0.201, val_acc:0.969]
Epoch [79/120    avg_loss:0.165, val_acc:0.988]
Epoch [80/120    avg_loss:0.129, val_acc:0.977]
Epoch [81/120    avg_loss:0.157, val_acc:0.967]
Epoch [82/120    avg_loss:0.153, val_acc:0.975]
Epoch [83/120    avg_loss:0.129, val_acc:0.988]
Epoch [84/120    avg_loss:0.116, val_acc:0.979]
Epoch [85/120    avg_loss:0.096, val_acc:0.983]
Epoch [86/120    avg_loss:0.110, val_acc:0.990]
Epoch [87/120    avg_loss:0.104, val_acc:0.988]
Epoch [88/120    avg_loss:0.102, val_acc:0.977]
Epoch [89/120    avg_loss:0.108, val_acc:0.983]
Epoch [90/120    avg_loss:0.098, val_acc:0.985]
Epoch [91/120    avg_loss:0.113, val_acc:0.983]
Epoch [92/120    avg_loss:0.094, val_acc:0.985]
Epoch [93/120    avg_loss:0.103, val_acc:0.977]
Epoch [94/120    avg_loss:0.111, val_acc:0.979]
Epoch [95/120    avg_loss:0.194, val_acc:0.958]
Epoch [96/120    avg_loss:0.153, val_acc:0.946]
Epoch [97/120    avg_loss:0.155, val_acc:0.967]
Epoch [98/120    avg_loss:0.137, val_acc:0.977]
Epoch [99/120    avg_loss:0.116, val_acc:0.963]
Epoch [100/120    avg_loss:0.097, val_acc:0.971]
Epoch [101/120    avg_loss:0.092, val_acc:0.985]
Epoch [102/120    avg_loss:0.067, val_acc:0.988]
Epoch [103/120    avg_loss:0.068, val_acc:0.988]
Epoch [104/120    avg_loss:0.066, val_acc:0.988]
Epoch [105/120    avg_loss:0.060, val_acc:0.985]
Epoch [106/120    avg_loss:0.068, val_acc:0.990]
Epoch [107/120    avg_loss:0.068, val_acc:0.985]
Epoch [108/120    avg_loss:0.059, val_acc:0.990]
Epoch [109/120    avg_loss:0.072, val_acc:0.990]
Epoch [110/120    avg_loss:0.058, val_acc:0.990]
Epoch [111/120    avg_loss:0.059, val_acc:0.990]
Epoch [112/120    avg_loss:0.068, val_acc:0.990]
Epoch [113/120    avg_loss:0.068, val_acc:0.990]
Epoch [114/120    avg_loss:0.047, val_acc:0.992]
Epoch [115/120    avg_loss:0.053, val_acc:0.992]
Epoch [116/120    avg_loss:0.062, val_acc:0.992]
Epoch [117/120    avg_loss:0.051, val_acc:0.992]
Epoch [118/120    avg_loss:0.056, val_acc:0.990]
Epoch [119/120    avg_loss:0.072, val_acc:0.994]
Epoch [120/120    avg_loss:0.063, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 224   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.96688742 0.98678414 0.92372881 0.9
 0.99266504 0.93181818 1.         1.         1.         1.
 0.99778761 1.        ]

Kappa:
0.9878923881901102
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83413e2ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.107]
Epoch [2/120    avg_loss:2.542, val_acc:0.177]
Epoch [3/120    avg_loss:2.455, val_acc:0.385]
Epoch [4/120    avg_loss:2.390, val_acc:0.388]
Epoch [5/120    avg_loss:2.330, val_acc:0.440]
Epoch [6/120    avg_loss:2.265, val_acc:0.458]
Epoch [7/120    avg_loss:2.195, val_acc:0.477]
Epoch [8/120    avg_loss:2.126, val_acc:0.492]
Epoch [9/120    avg_loss:2.038, val_acc:0.492]
Epoch [10/120    avg_loss:1.951, val_acc:0.510]
Epoch [11/120    avg_loss:1.884, val_acc:0.540]
Epoch [12/120    avg_loss:1.813, val_acc:0.569]
Epoch [13/120    avg_loss:1.709, val_acc:0.615]
Epoch [14/120    avg_loss:1.610, val_acc:0.629]
Epoch [15/120    avg_loss:1.542, val_acc:0.662]
Epoch [16/120    avg_loss:1.432, val_acc:0.679]
Epoch [17/120    avg_loss:1.364, val_acc:0.702]
Epoch [18/120    avg_loss:1.316, val_acc:0.715]
Epoch [19/120    avg_loss:1.204, val_acc:0.719]
Epoch [20/120    avg_loss:1.123, val_acc:0.740]
Epoch [21/120    avg_loss:1.083, val_acc:0.746]
Epoch [22/120    avg_loss:1.083, val_acc:0.821]
Epoch [23/120    avg_loss:0.978, val_acc:0.877]
Epoch [24/120    avg_loss:0.935, val_acc:0.887]
Epoch [25/120    avg_loss:0.851, val_acc:0.906]
Epoch [26/120    avg_loss:0.812, val_acc:0.890]
Epoch [27/120    avg_loss:0.764, val_acc:0.912]
Epoch [28/120    avg_loss:0.699, val_acc:0.908]
Epoch [29/120    avg_loss:0.628, val_acc:0.923]
Epoch [30/120    avg_loss:0.632, val_acc:0.910]
Epoch [31/120    avg_loss:0.615, val_acc:0.919]
Epoch [32/120    avg_loss:0.557, val_acc:0.910]
Epoch [33/120    avg_loss:0.565, val_acc:0.892]
Epoch [34/120    avg_loss:0.550, val_acc:0.925]
Epoch [35/120    avg_loss:0.584, val_acc:0.925]
Epoch [36/120    avg_loss:0.493, val_acc:0.919]
Epoch [37/120    avg_loss:0.440, val_acc:0.935]
Epoch [38/120    avg_loss:0.439, val_acc:0.844]
Epoch [39/120    avg_loss:0.484, val_acc:0.896]
Epoch [40/120    avg_loss:0.451, val_acc:0.944]
Epoch [41/120    avg_loss:0.431, val_acc:0.910]
Epoch [42/120    avg_loss:0.398, val_acc:0.906]
Epoch [43/120    avg_loss:0.394, val_acc:0.900]
Epoch [44/120    avg_loss:0.391, val_acc:0.912]
Epoch [45/120    avg_loss:0.356, val_acc:0.925]
Epoch [46/120    avg_loss:0.367, val_acc:0.917]
Epoch [47/120    avg_loss:0.337, val_acc:0.952]
Epoch [48/120    avg_loss:0.318, val_acc:0.933]
Epoch [49/120    avg_loss:0.371, val_acc:0.940]
Epoch [50/120    avg_loss:0.378, val_acc:0.933]
Epoch [51/120    avg_loss:0.399, val_acc:0.933]
Epoch [52/120    avg_loss:0.315, val_acc:0.929]
Epoch [53/120    avg_loss:0.360, val_acc:0.944]
Epoch [54/120    avg_loss:0.353, val_acc:0.935]
Epoch [55/120    avg_loss:0.330, val_acc:0.944]
Epoch [56/120    avg_loss:0.266, val_acc:0.950]
Epoch [57/120    avg_loss:0.256, val_acc:0.940]
Epoch [58/120    avg_loss:0.243, val_acc:0.921]
Epoch [59/120    avg_loss:0.238, val_acc:0.927]
Epoch [60/120    avg_loss:0.210, val_acc:0.960]
Epoch [61/120    avg_loss:0.220, val_acc:0.958]
Epoch [62/120    avg_loss:0.231, val_acc:0.946]
Epoch [63/120    avg_loss:0.234, val_acc:0.948]
Epoch [64/120    avg_loss:0.238, val_acc:0.946]
Epoch [65/120    avg_loss:0.255, val_acc:0.950]
Epoch [66/120    avg_loss:0.211, val_acc:0.940]
Epoch [67/120    avg_loss:0.216, val_acc:0.952]
Epoch [68/120    avg_loss:0.194, val_acc:0.950]
Epoch [69/120    avg_loss:0.246, val_acc:0.946]
Epoch [70/120    avg_loss:0.263, val_acc:0.956]
Epoch [71/120    avg_loss:0.206, val_acc:0.944]
Epoch [72/120    avg_loss:0.155, val_acc:0.967]
Epoch [73/120    avg_loss:0.173, val_acc:0.971]
Epoch [74/120    avg_loss:0.141, val_acc:0.975]
Epoch [75/120    avg_loss:0.159, val_acc:0.946]
Epoch [76/120    avg_loss:0.181, val_acc:0.969]
Epoch [77/120    avg_loss:0.197, val_acc:0.938]
Epoch [78/120    avg_loss:0.177, val_acc:0.960]
Epoch [79/120    avg_loss:0.143, val_acc:0.965]
Epoch [80/120    avg_loss:0.113, val_acc:0.967]
Epoch [81/120    avg_loss:0.104, val_acc:0.960]
Epoch [82/120    avg_loss:0.129, val_acc:0.965]
Epoch [83/120    avg_loss:0.147, val_acc:0.954]
Epoch [84/120    avg_loss:0.148, val_acc:0.946]
Epoch [85/120    avg_loss:0.144, val_acc:0.977]
Epoch [86/120    avg_loss:0.138, val_acc:0.958]
Epoch [87/120    avg_loss:0.137, val_acc:0.977]
Epoch [88/120    avg_loss:0.112, val_acc:0.954]
Epoch [89/120    avg_loss:0.116, val_acc:0.977]
Epoch [90/120    avg_loss:0.108, val_acc:0.977]
Epoch [91/120    avg_loss:0.112, val_acc:0.973]
Epoch [92/120    avg_loss:0.142, val_acc:0.860]
Epoch [93/120    avg_loss:0.150, val_acc:0.944]
Epoch [94/120    avg_loss:0.131, val_acc:0.979]
Epoch [95/120    avg_loss:0.130, val_acc:0.973]
Epoch [96/120    avg_loss:0.077, val_acc:0.967]
Epoch [97/120    avg_loss:0.097, val_acc:0.975]
Epoch [98/120    avg_loss:0.178, val_acc:0.954]
Epoch [99/120    avg_loss:0.197, val_acc:0.960]
Epoch [100/120    avg_loss:0.164, val_acc:0.960]
Epoch [101/120    avg_loss:0.101, val_acc:0.975]
Epoch [102/120    avg_loss:0.100, val_acc:0.977]
Epoch [103/120    avg_loss:0.089, val_acc:0.975]
Epoch [104/120    avg_loss:0.093, val_acc:0.977]
Epoch [105/120    avg_loss:0.075, val_acc:0.975]
Epoch [106/120    avg_loss:0.055, val_acc:0.981]
Epoch [107/120    avg_loss:0.072, val_acc:0.979]
Epoch [108/120    avg_loss:0.071, val_acc:0.981]
Epoch [109/120    avg_loss:0.066, val_acc:0.983]
Epoch [110/120    avg_loss:0.058, val_acc:0.983]
Epoch [111/120    avg_loss:0.055, val_acc:0.981]
Epoch [112/120    avg_loss:0.058, val_acc:0.985]
Epoch [113/120    avg_loss:0.061, val_acc:0.977]
Epoch [114/120    avg_loss:0.102, val_acc:0.969]
Epoch [115/120    avg_loss:0.176, val_acc:0.965]
Epoch [116/120    avg_loss:0.113, val_acc:0.967]
Epoch [117/120    avg_loss:0.087, val_acc:0.977]
Epoch [118/120    avg_loss:0.060, val_acc:0.985]
Epoch [119/120    avg_loss:0.051, val_acc:0.971]
Epoch [120/120    avg_loss:0.043, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219   7   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 1.         0.95842451 0.97550111 0.91612903 0.89965398
 0.99266504 0.88757396 0.99487179 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9850425404485963
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7cc05b6a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.597, val_acc:0.240]
Epoch [2/120    avg_loss:2.509, val_acc:0.308]
Epoch [3/120    avg_loss:2.442, val_acc:0.325]
Epoch [4/120    avg_loss:2.378, val_acc:0.342]
Epoch [5/120    avg_loss:2.319, val_acc:0.367]
Epoch [6/120    avg_loss:2.261, val_acc:0.423]
Epoch [7/120    avg_loss:2.222, val_acc:0.460]
Epoch [8/120    avg_loss:2.155, val_acc:0.487]
Epoch [9/120    avg_loss:2.122, val_acc:0.515]
Epoch [10/120    avg_loss:2.055, val_acc:0.533]
Epoch [11/120    avg_loss:1.990, val_acc:0.571]
Epoch [12/120    avg_loss:1.928, val_acc:0.621]
Epoch [13/120    avg_loss:1.840, val_acc:0.625]
Epoch [14/120    avg_loss:1.763, val_acc:0.637]
Epoch [15/120    avg_loss:1.693, val_acc:0.662]
Epoch [16/120    avg_loss:1.633, val_acc:0.692]
Epoch [17/120    avg_loss:1.561, val_acc:0.698]
Epoch [18/120    avg_loss:1.468, val_acc:0.702]
Epoch [19/120    avg_loss:1.407, val_acc:0.710]
Epoch [20/120    avg_loss:1.306, val_acc:0.731]
Epoch [21/120    avg_loss:1.221, val_acc:0.738]
Epoch [22/120    avg_loss:1.174, val_acc:0.731]
Epoch [23/120    avg_loss:1.099, val_acc:0.750]
Epoch [24/120    avg_loss:1.033, val_acc:0.748]
Epoch [25/120    avg_loss:0.985, val_acc:0.796]
Epoch [26/120    avg_loss:0.902, val_acc:0.800]
Epoch [27/120    avg_loss:0.807, val_acc:0.898]
Epoch [28/120    avg_loss:0.804, val_acc:0.879]
Epoch [29/120    avg_loss:0.766, val_acc:0.879]
Epoch [30/120    avg_loss:0.706, val_acc:0.923]
Epoch [31/120    avg_loss:0.645, val_acc:0.921]
Epoch [32/120    avg_loss:0.565, val_acc:0.912]
Epoch [33/120    avg_loss:0.632, val_acc:0.883]
Epoch [34/120    avg_loss:0.580, val_acc:0.900]
Epoch [35/120    avg_loss:0.543, val_acc:0.896]
Epoch [36/120    avg_loss:0.527, val_acc:0.917]
Epoch [37/120    avg_loss:0.484, val_acc:0.873]
Epoch [38/120    avg_loss:0.463, val_acc:0.944]
Epoch [39/120    avg_loss:0.436, val_acc:0.887]
Epoch [40/120    avg_loss:0.479, val_acc:0.931]
Epoch [41/120    avg_loss:0.441, val_acc:0.925]
Epoch [42/120    avg_loss:0.422, val_acc:0.938]
Epoch [43/120    avg_loss:0.391, val_acc:0.910]
Epoch [44/120    avg_loss:0.408, val_acc:0.892]
Epoch [45/120    avg_loss:0.446, val_acc:0.938]
Epoch [46/120    avg_loss:0.333, val_acc:0.944]
Epoch [47/120    avg_loss:0.342, val_acc:0.935]
Epoch [48/120    avg_loss:0.343, val_acc:0.927]
Epoch [49/120    avg_loss:0.356, val_acc:0.948]
Epoch [50/120    avg_loss:0.285, val_acc:0.935]
Epoch [51/120    avg_loss:0.307, val_acc:0.963]
Epoch [52/120    avg_loss:0.271, val_acc:0.940]
Epoch [53/120    avg_loss:0.387, val_acc:0.935]
Epoch [54/120    avg_loss:0.318, val_acc:0.960]
Epoch [55/120    avg_loss:0.253, val_acc:0.933]
Epoch [56/120    avg_loss:0.341, val_acc:0.965]
Epoch [57/120    avg_loss:0.285, val_acc:0.956]
Epoch [58/120    avg_loss:0.266, val_acc:0.942]
Epoch [59/120    avg_loss:0.301, val_acc:0.967]
Epoch [60/120    avg_loss:0.226, val_acc:0.960]
Epoch [61/120    avg_loss:0.215, val_acc:0.969]
Epoch [62/120    avg_loss:0.199, val_acc:0.948]
Epoch [63/120    avg_loss:0.206, val_acc:0.960]
Epoch [64/120    avg_loss:0.207, val_acc:0.958]
Epoch [65/120    avg_loss:0.213, val_acc:0.965]
Epoch [66/120    avg_loss:0.173, val_acc:0.931]
Epoch [67/120    avg_loss:0.225, val_acc:0.958]
Epoch [68/120    avg_loss:0.163, val_acc:0.975]
Epoch [69/120    avg_loss:0.206, val_acc:0.933]
Epoch [70/120    avg_loss:0.214, val_acc:0.954]
Epoch [71/120    avg_loss:0.245, val_acc:0.952]
Epoch [72/120    avg_loss:0.250, val_acc:0.960]
Epoch [73/120    avg_loss:0.184, val_acc:0.954]
Epoch [74/120    avg_loss:0.203, val_acc:0.965]
Epoch [75/120    avg_loss:0.174, val_acc:0.965]
Epoch [76/120    avg_loss:0.160, val_acc:0.963]
Epoch [77/120    avg_loss:0.233, val_acc:0.954]
Epoch [78/120    avg_loss:0.242, val_acc:0.969]
Epoch [79/120    avg_loss:0.223, val_acc:0.960]
Epoch [80/120    avg_loss:0.185, val_acc:0.963]
Epoch [81/120    avg_loss:0.191, val_acc:0.963]
Epoch [82/120    avg_loss:0.169, val_acc:0.971]
Epoch [83/120    avg_loss:0.114, val_acc:0.973]
Epoch [84/120    avg_loss:0.114, val_acc:0.975]
Epoch [85/120    avg_loss:0.113, val_acc:0.975]
Epoch [86/120    avg_loss:0.104, val_acc:0.975]
Epoch [87/120    avg_loss:0.114, val_acc:0.977]
Epoch [88/120    avg_loss:0.115, val_acc:0.979]
Epoch [89/120    avg_loss:0.089, val_acc:0.979]
Epoch [90/120    avg_loss:0.106, val_acc:0.979]
Epoch [91/120    avg_loss:0.104, val_acc:0.979]
Epoch [92/120    avg_loss:0.106, val_acc:0.979]
Epoch [93/120    avg_loss:0.109, val_acc:0.977]
Epoch [94/120    avg_loss:0.087, val_acc:0.979]
Epoch [95/120    avg_loss:0.095, val_acc:0.979]
Epoch [96/120    avg_loss:0.093, val_acc:0.981]
Epoch [97/120    avg_loss:0.089, val_acc:0.979]
Epoch [98/120    avg_loss:0.080, val_acc:0.979]
Epoch [99/120    avg_loss:0.093, val_acc:0.979]
Epoch [100/120    avg_loss:0.084, val_acc:0.981]
Epoch [101/120    avg_loss:0.071, val_acc:0.979]
Epoch [102/120    avg_loss:0.094, val_acc:0.979]
Epoch [103/120    avg_loss:0.090, val_acc:0.979]
Epoch [104/120    avg_loss:0.082, val_acc:0.977]
Epoch [105/120    avg_loss:0.101, val_acc:0.981]
Epoch [106/120    avg_loss:0.079, val_acc:0.983]
Epoch [107/120    avg_loss:0.082, val_acc:0.979]
Epoch [108/120    avg_loss:0.091, val_acc:0.981]
Epoch [109/120    avg_loss:0.084, val_acc:0.979]
Epoch [110/120    avg_loss:0.085, val_acc:0.981]
Epoch [111/120    avg_loss:0.087, val_acc:0.979]
Epoch [112/120    avg_loss:0.089, val_acc:0.979]
Epoch [113/120    avg_loss:0.075, val_acc:0.979]
Epoch [114/120    avg_loss:0.090, val_acc:0.983]
Epoch [115/120    avg_loss:0.086, val_acc:0.979]
Epoch [116/120    avg_loss:0.076, val_acc:0.979]
Epoch [117/120    avg_loss:0.074, val_acc:0.981]
Epoch [118/120    avg_loss:0.084, val_acc:0.983]
Epoch [119/120    avg_loss:0.081, val_acc:0.985]
Epoch [120/120    avg_loss:0.079, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   2 221   3   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.95824176 0.98004435 0.91028446 0.88435374
 0.99019608 0.9017341  0.99614891 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9848058088623804
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f770ed99a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.614, val_acc:0.163]
Epoch [2/120    avg_loss:2.537, val_acc:0.350]
Epoch [3/120    avg_loss:2.458, val_acc:0.358]
Epoch [4/120    avg_loss:2.380, val_acc:0.373]
Epoch [5/120    avg_loss:2.344, val_acc:0.415]
Epoch [6/120    avg_loss:2.278, val_acc:0.448]
Epoch [7/120    avg_loss:2.226, val_acc:0.475]
Epoch [8/120    avg_loss:2.166, val_acc:0.512]
Epoch [9/120    avg_loss:2.101, val_acc:0.517]
Epoch [10/120    avg_loss:2.053, val_acc:0.533]
Epoch [11/120    avg_loss:1.979, val_acc:0.594]
Epoch [12/120    avg_loss:1.918, val_acc:0.610]
Epoch [13/120    avg_loss:1.829, val_acc:0.654]
Epoch [14/120    avg_loss:1.739, val_acc:0.658]
Epoch [15/120    avg_loss:1.670, val_acc:0.698]
Epoch [16/120    avg_loss:1.584, val_acc:0.694]
Epoch [17/120    avg_loss:1.517, val_acc:0.713]
Epoch [18/120    avg_loss:1.403, val_acc:0.708]
Epoch [19/120    avg_loss:1.321, val_acc:0.729]
Epoch [20/120    avg_loss:1.227, val_acc:0.744]
Epoch [21/120    avg_loss:1.193, val_acc:0.750]
Epoch [22/120    avg_loss:1.090, val_acc:0.771]
Epoch [23/120    avg_loss:1.042, val_acc:0.777]
Epoch [24/120    avg_loss:0.950, val_acc:0.794]
Epoch [25/120    avg_loss:0.873, val_acc:0.806]
Epoch [26/120    avg_loss:0.819, val_acc:0.846]
Epoch [27/120    avg_loss:0.794, val_acc:0.846]
Epoch [28/120    avg_loss:0.702, val_acc:0.825]
Epoch [29/120    avg_loss:0.685, val_acc:0.844]
Epoch [30/120    avg_loss:0.641, val_acc:0.844]
Epoch [31/120    avg_loss:0.615, val_acc:0.921]
Epoch [32/120    avg_loss:0.587, val_acc:0.915]
Epoch [33/120    avg_loss:0.593, val_acc:0.935]
Epoch [34/120    avg_loss:0.532, val_acc:0.917]
Epoch [35/120    avg_loss:0.519, val_acc:0.919]
Epoch [36/120    avg_loss:0.471, val_acc:0.940]
Epoch [37/120    avg_loss:0.472, val_acc:0.942]
Epoch [38/120    avg_loss:0.428, val_acc:0.927]
Epoch [39/120    avg_loss:0.431, val_acc:0.954]
Epoch [40/120    avg_loss:0.444, val_acc:0.940]
Epoch [41/120    avg_loss:0.443, val_acc:0.929]
Epoch [42/120    avg_loss:0.371, val_acc:0.929]
Epoch [43/120    avg_loss:0.438, val_acc:0.940]
Epoch [44/120    avg_loss:0.396, val_acc:0.933]
Epoch [45/120    avg_loss:0.345, val_acc:0.944]
Epoch [46/120    avg_loss:0.336, val_acc:0.938]
Epoch [47/120    avg_loss:0.367, val_acc:0.942]
Epoch [48/120    avg_loss:0.394, val_acc:0.917]
Epoch [49/120    avg_loss:0.352, val_acc:0.958]
Epoch [50/120    avg_loss:0.403, val_acc:0.933]
Epoch [51/120    avg_loss:0.328, val_acc:0.935]
Epoch [52/120    avg_loss:0.340, val_acc:0.952]
Epoch [53/120    avg_loss:0.304, val_acc:0.958]
Epoch [54/120    avg_loss:0.250, val_acc:0.942]
Epoch [55/120    avg_loss:0.268, val_acc:0.946]
Epoch [56/120    avg_loss:0.306, val_acc:0.908]
Epoch [57/120    avg_loss:0.273, val_acc:0.929]
Epoch [58/120    avg_loss:0.271, val_acc:0.956]
Epoch [59/120    avg_loss:0.237, val_acc:0.958]
Epoch [60/120    avg_loss:0.236, val_acc:0.963]
Epoch [61/120    avg_loss:0.204, val_acc:0.960]
Epoch [62/120    avg_loss:0.197, val_acc:0.954]
Epoch [63/120    avg_loss:0.222, val_acc:0.963]
Epoch [64/120    avg_loss:0.215, val_acc:0.948]
Epoch [65/120    avg_loss:0.193, val_acc:0.963]
Epoch [66/120    avg_loss:0.174, val_acc:0.929]
Epoch [67/120    avg_loss:0.226, val_acc:0.954]
Epoch [68/120    avg_loss:0.191, val_acc:0.954]
Epoch [69/120    avg_loss:0.185, val_acc:0.975]
Epoch [70/120    avg_loss:0.176, val_acc:0.971]
Epoch [71/120    avg_loss:0.176, val_acc:0.965]
Epoch [72/120    avg_loss:0.182, val_acc:0.960]
Epoch [73/120    avg_loss:0.162, val_acc:0.954]
Epoch [74/120    avg_loss:0.190, val_acc:0.956]
Epoch [75/120    avg_loss:0.190, val_acc:0.971]
Epoch [76/120    avg_loss:0.144, val_acc:0.975]
Epoch [77/120    avg_loss:0.119, val_acc:0.950]
Epoch [78/120    avg_loss:0.108, val_acc:0.969]
Epoch [79/120    avg_loss:0.141, val_acc:0.973]
Epoch [80/120    avg_loss:0.136, val_acc:0.954]
Epoch [81/120    avg_loss:0.131, val_acc:0.969]
Epoch [82/120    avg_loss:0.147, val_acc:0.975]
Epoch [83/120    avg_loss:0.107, val_acc:0.973]
Epoch [84/120    avg_loss:0.102, val_acc:0.973]
Epoch [85/120    avg_loss:0.097, val_acc:0.967]
Epoch [86/120    avg_loss:0.190, val_acc:0.954]
Epoch [87/120    avg_loss:0.160, val_acc:0.956]
Epoch [88/120    avg_loss:0.157, val_acc:0.946]
Epoch [89/120    avg_loss:0.189, val_acc:0.938]
Epoch [90/120    avg_loss:0.150, val_acc:0.979]
Epoch [91/120    avg_loss:0.159, val_acc:0.965]
Epoch [92/120    avg_loss:0.245, val_acc:0.948]
Epoch [93/120    avg_loss:0.183, val_acc:0.963]
Epoch [94/120    avg_loss:0.120, val_acc:0.969]
Epoch [95/120    avg_loss:0.107, val_acc:0.975]
Epoch [96/120    avg_loss:0.080, val_acc:0.973]
Epoch [97/120    avg_loss:0.094, val_acc:0.977]
Epoch [98/120    avg_loss:0.096, val_acc:0.971]
Epoch [99/120    avg_loss:0.090, val_acc:0.975]
Epoch [100/120    avg_loss:0.099, val_acc:0.985]
Epoch [101/120    avg_loss:0.093, val_acc:0.973]
Epoch [102/120    avg_loss:0.083, val_acc:0.967]
Epoch [103/120    avg_loss:0.122, val_acc:0.967]
Epoch [104/120    avg_loss:0.093, val_acc:0.971]
Epoch [105/120    avg_loss:0.069, val_acc:0.975]
Epoch [106/120    avg_loss:0.069, val_acc:0.985]
Epoch [107/120    avg_loss:0.083, val_acc:0.956]
Epoch [108/120    avg_loss:0.081, val_acc:0.975]
Epoch [109/120    avg_loss:0.060, val_acc:0.975]
Epoch [110/120    avg_loss:0.057, val_acc:0.979]
Epoch [111/120    avg_loss:0.068, val_acc:0.975]
Epoch [112/120    avg_loss:0.083, val_acc:0.977]
Epoch [113/120    avg_loss:0.106, val_acc:0.973]
Epoch [114/120    avg_loss:0.098, val_acc:0.971]
Epoch [115/120    avg_loss:0.071, val_acc:0.973]
Epoch [116/120    avg_loss:0.073, val_acc:0.973]
Epoch [117/120    avg_loss:0.073, val_acc:0.979]
Epoch [118/120    avg_loss:0.086, val_acc:0.981]
Epoch [119/120    avg_loss:0.074, val_acc:0.983]
Epoch [120/120    avg_loss:0.050, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  17   0   0   0   0   0   0   2   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.96902655 1.         0.92650334 0.89419795
 1.         0.91954023 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9888419669603166
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7bcc276ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.606, val_acc:0.152]
Epoch [2/120    avg_loss:2.524, val_acc:0.258]
Epoch [3/120    avg_loss:2.450, val_acc:0.327]
Epoch [4/120    avg_loss:2.380, val_acc:0.356]
Epoch [5/120    avg_loss:2.326, val_acc:0.404]
Epoch [6/120    avg_loss:2.268, val_acc:0.396]
Epoch [7/120    avg_loss:2.204, val_acc:0.427]
Epoch [8/120    avg_loss:2.148, val_acc:0.498]
Epoch [9/120    avg_loss:2.075, val_acc:0.525]
Epoch [10/120    avg_loss:1.982, val_acc:0.544]
Epoch [11/120    avg_loss:1.931, val_acc:0.579]
Epoch [12/120    avg_loss:1.887, val_acc:0.615]
Epoch [13/120    avg_loss:1.796, val_acc:0.631]
Epoch [14/120    avg_loss:1.733, val_acc:0.625]
Epoch [15/120    avg_loss:1.665, val_acc:0.675]
Epoch [16/120    avg_loss:1.590, val_acc:0.694]
Epoch [17/120    avg_loss:1.506, val_acc:0.713]
Epoch [18/120    avg_loss:1.411, val_acc:0.713]
Epoch [19/120    avg_loss:1.350, val_acc:0.708]
Epoch [20/120    avg_loss:1.243, val_acc:0.767]
Epoch [21/120    avg_loss:1.161, val_acc:0.858]
Epoch [22/120    avg_loss:1.122, val_acc:0.744]
Epoch [23/120    avg_loss:1.051, val_acc:0.858]
Epoch [24/120    avg_loss:1.008, val_acc:0.879]
Epoch [25/120    avg_loss:0.905, val_acc:0.852]
Epoch [26/120    avg_loss:0.853, val_acc:0.910]
Epoch [27/120    avg_loss:0.792, val_acc:0.873]
Epoch [28/120    avg_loss:0.745, val_acc:0.902]
Epoch [29/120    avg_loss:0.704, val_acc:0.879]
Epoch [30/120    avg_loss:0.664, val_acc:0.927]
Epoch [31/120    avg_loss:0.654, val_acc:0.902]
Epoch [32/120    avg_loss:0.636, val_acc:0.923]
Epoch [33/120    avg_loss:0.564, val_acc:0.938]
Epoch [34/120    avg_loss:0.558, val_acc:0.933]
Epoch [35/120    avg_loss:0.542, val_acc:0.881]
Epoch [36/120    avg_loss:0.539, val_acc:0.902]
Epoch [37/120    avg_loss:0.547, val_acc:0.933]
Epoch [38/120    avg_loss:0.506, val_acc:0.942]
Epoch [39/120    avg_loss:0.509, val_acc:0.908]
Epoch [40/120    avg_loss:0.488, val_acc:0.935]
Epoch [41/120    avg_loss:0.420, val_acc:0.938]
Epoch [42/120    avg_loss:0.382, val_acc:0.938]
Epoch [43/120    avg_loss:0.397, val_acc:0.969]
Epoch [44/120    avg_loss:0.360, val_acc:0.929]
Epoch [45/120    avg_loss:0.387, val_acc:0.950]
Epoch [46/120    avg_loss:0.326, val_acc:0.956]
Epoch [47/120    avg_loss:0.333, val_acc:0.948]
Epoch [48/120    avg_loss:0.348, val_acc:0.956]
Epoch [49/120    avg_loss:0.344, val_acc:0.956]
Epoch [50/120    avg_loss:0.342, val_acc:0.950]
Epoch [51/120    avg_loss:0.297, val_acc:0.973]
Epoch [52/120    avg_loss:0.320, val_acc:0.967]
Epoch [53/120    avg_loss:0.300, val_acc:0.952]
Epoch [54/120    avg_loss:0.354, val_acc:0.963]
Epoch [55/120    avg_loss:0.374, val_acc:0.967]
Epoch [56/120    avg_loss:0.353, val_acc:0.931]
Epoch [57/120    avg_loss:0.334, val_acc:0.969]
Epoch [58/120    avg_loss:0.259, val_acc:0.963]
Epoch [59/120    avg_loss:0.303, val_acc:0.973]
Epoch [60/120    avg_loss:0.261, val_acc:0.931]
Epoch [61/120    avg_loss:0.245, val_acc:0.975]
Epoch [62/120    avg_loss:0.229, val_acc:0.967]
Epoch [63/120    avg_loss:0.212, val_acc:0.958]
Epoch [64/120    avg_loss:0.191, val_acc:0.977]
Epoch [65/120    avg_loss:0.184, val_acc:0.985]
Epoch [66/120    avg_loss:0.202, val_acc:0.973]
Epoch [67/120    avg_loss:0.176, val_acc:0.981]
Epoch [68/120    avg_loss:0.222, val_acc:0.977]
Epoch [69/120    avg_loss:0.187, val_acc:0.981]
Epoch [70/120    avg_loss:0.175, val_acc:0.981]
Epoch [71/120    avg_loss:0.166, val_acc:0.969]
Epoch [72/120    avg_loss:0.165, val_acc:0.988]
Epoch [73/120    avg_loss:0.156, val_acc:0.985]
Epoch [74/120    avg_loss:0.150, val_acc:0.979]
Epoch [75/120    avg_loss:0.153, val_acc:0.967]
Epoch [76/120    avg_loss:0.201, val_acc:0.983]
Epoch [77/120    avg_loss:0.134, val_acc:0.981]
Epoch [78/120    avg_loss:0.151, val_acc:0.983]
Epoch [79/120    avg_loss:0.161, val_acc:0.952]
Epoch [80/120    avg_loss:0.217, val_acc:0.969]
Epoch [81/120    avg_loss:0.165, val_acc:0.973]
Epoch [82/120    avg_loss:0.148, val_acc:0.985]
Epoch [83/120    avg_loss:0.140, val_acc:0.988]
Epoch [84/120    avg_loss:0.124, val_acc:0.983]
Epoch [85/120    avg_loss:0.134, val_acc:0.983]
Epoch [86/120    avg_loss:0.140, val_acc:0.973]
Epoch [87/120    avg_loss:0.165, val_acc:0.958]
Epoch [88/120    avg_loss:0.155, val_acc:0.973]
Epoch [89/120    avg_loss:0.124, val_acc:0.979]
Epoch [90/120    avg_loss:0.113, val_acc:0.981]
Epoch [91/120    avg_loss:0.132, val_acc:0.973]
Epoch [92/120    avg_loss:0.097, val_acc:0.977]
Epoch [93/120    avg_loss:0.145, val_acc:0.965]
Epoch [94/120    avg_loss:0.146, val_acc:0.975]
Epoch [95/120    avg_loss:0.141, val_acc:0.979]
Epoch [96/120    avg_loss:0.105, val_acc:0.979]
Epoch [97/120    avg_loss:0.118, val_acc:0.985]
Epoch [98/120    avg_loss:0.085, val_acc:0.988]
Epoch [99/120    avg_loss:0.065, val_acc:0.988]
Epoch [100/120    avg_loss:0.086, val_acc:0.990]
Epoch [101/120    avg_loss:0.064, val_acc:0.990]
Epoch [102/120    avg_loss:0.063, val_acc:0.988]
Epoch [103/120    avg_loss:0.059, val_acc:0.985]
Epoch [104/120    avg_loss:0.070, val_acc:0.988]
Epoch [105/120    avg_loss:0.057, val_acc:0.985]
Epoch [106/120    avg_loss:0.063, val_acc:0.988]
Epoch [107/120    avg_loss:0.063, val_acc:0.988]
Epoch [108/120    avg_loss:0.064, val_acc:0.985]
Epoch [109/120    avg_loss:0.063, val_acc:0.988]
Epoch [110/120    avg_loss:0.063, val_acc:0.990]
Epoch [111/120    avg_loss:0.057, val_acc:0.990]
Epoch [112/120    avg_loss:0.077, val_acc:0.988]
Epoch [113/120    avg_loss:0.056, val_acc:0.988]
Epoch [114/120    avg_loss:0.065, val_acc:0.985]
Epoch [115/120    avg_loss:0.063, val_acc:0.988]
Epoch [116/120    avg_loss:0.055, val_acc:0.990]
Epoch [117/120    avg_loss:0.055, val_acc:0.990]
Epoch [118/120    avg_loss:0.069, val_acc:0.990]
Epoch [119/120    avg_loss:0.058, val_acc:0.990]
Epoch [120/120    avg_loss:0.052, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 227   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.96902655 0.99343545 0.94382022 0.93069307
 0.99512195 0.92571429 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9907418496034195
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ce2b96a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.177]
Epoch [2/120    avg_loss:2.542, val_acc:0.167]
Epoch [3/120    avg_loss:2.477, val_acc:0.329]
Epoch [4/120    avg_loss:2.418, val_acc:0.371]
Epoch [5/120    avg_loss:2.368, val_acc:0.390]
Epoch [6/120    avg_loss:2.312, val_acc:0.438]
Epoch [7/120    avg_loss:2.252, val_acc:0.479]
Epoch [8/120    avg_loss:2.212, val_acc:0.571]
Epoch [9/120    avg_loss:2.143, val_acc:0.608]
Epoch [10/120    avg_loss:2.091, val_acc:0.623]
Epoch [11/120    avg_loss:2.044, val_acc:0.644]
Epoch [12/120    avg_loss:1.992, val_acc:0.654]
Epoch [13/120    avg_loss:1.920, val_acc:0.656]
Epoch [14/120    avg_loss:1.862, val_acc:0.669]
Epoch [15/120    avg_loss:1.784, val_acc:0.671]
Epoch [16/120    avg_loss:1.703, val_acc:0.675]
Epoch [17/120    avg_loss:1.607, val_acc:0.673]
Epoch [18/120    avg_loss:1.532, val_acc:0.698]
Epoch [19/120    avg_loss:1.437, val_acc:0.715]
Epoch [20/120    avg_loss:1.329, val_acc:0.719]
Epoch [21/120    avg_loss:1.261, val_acc:0.731]
Epoch [22/120    avg_loss:1.188, val_acc:0.735]
Epoch [23/120    avg_loss:1.140, val_acc:0.742]
Epoch [24/120    avg_loss:1.022, val_acc:0.771]
Epoch [25/120    avg_loss:0.987, val_acc:0.783]
Epoch [26/120    avg_loss:0.888, val_acc:0.810]
Epoch [27/120    avg_loss:0.839, val_acc:0.821]
Epoch [28/120    avg_loss:0.774, val_acc:0.881]
Epoch [29/120    avg_loss:0.741, val_acc:0.892]
Epoch [30/120    avg_loss:0.679, val_acc:0.894]
Epoch [31/120    avg_loss:0.704, val_acc:0.898]
Epoch [32/120    avg_loss:0.642, val_acc:0.902]
Epoch [33/120    avg_loss:0.614, val_acc:0.915]
Epoch [34/120    avg_loss:0.539, val_acc:0.912]
Epoch [35/120    avg_loss:0.568, val_acc:0.908]
Epoch [36/120    avg_loss:0.558, val_acc:0.902]
Epoch [37/120    avg_loss:0.523, val_acc:0.921]
Epoch [38/120    avg_loss:0.483, val_acc:0.912]
Epoch [39/120    avg_loss:0.509, val_acc:0.938]
Epoch [40/120    avg_loss:0.444, val_acc:0.927]
Epoch [41/120    avg_loss:0.405, val_acc:0.935]
Epoch [42/120    avg_loss:0.406, val_acc:0.912]
Epoch [43/120    avg_loss:0.431, val_acc:0.908]
Epoch [44/120    avg_loss:0.385, val_acc:0.917]
Epoch [45/120    avg_loss:0.375, val_acc:0.850]
Epoch [46/120    avg_loss:0.373, val_acc:0.938]
Epoch [47/120    avg_loss:0.342, val_acc:0.921]
Epoch [48/120    avg_loss:0.364, val_acc:0.942]
Epoch [49/120    avg_loss:0.374, val_acc:0.912]
Epoch [50/120    avg_loss:0.346, val_acc:0.940]
Epoch [51/120    avg_loss:0.416, val_acc:0.923]
Epoch [52/120    avg_loss:0.392, val_acc:0.931]
Epoch [53/120    avg_loss:0.349, val_acc:0.898]
Epoch [54/120    avg_loss:0.357, val_acc:0.919]
Epoch [55/120    avg_loss:0.326, val_acc:0.938]
Epoch [56/120    avg_loss:0.297, val_acc:0.919]
Epoch [57/120    avg_loss:0.267, val_acc:0.944]
Epoch [58/120    avg_loss:0.254, val_acc:0.940]
Epoch [59/120    avg_loss:0.312, val_acc:0.952]
Epoch [60/120    avg_loss:0.285, val_acc:0.929]
Epoch [61/120    avg_loss:0.273, val_acc:0.952]
Epoch [62/120    avg_loss:0.219, val_acc:0.958]
Epoch [63/120    avg_loss:0.224, val_acc:0.950]
Epoch [64/120    avg_loss:0.236, val_acc:0.935]
Epoch [65/120    avg_loss:0.208, val_acc:0.952]
Epoch [66/120    avg_loss:0.228, val_acc:0.965]
Epoch [67/120    avg_loss:0.201, val_acc:0.981]
Epoch [68/120    avg_loss:0.196, val_acc:0.977]
Epoch [69/120    avg_loss:0.249, val_acc:0.931]
Epoch [70/120    avg_loss:0.211, val_acc:0.965]
Epoch [71/120    avg_loss:0.197, val_acc:0.967]
Epoch [72/120    avg_loss:0.164, val_acc:0.971]
Epoch [73/120    avg_loss:0.145, val_acc:0.981]
Epoch [74/120    avg_loss:0.192, val_acc:0.963]
Epoch [75/120    avg_loss:0.225, val_acc:0.975]
Epoch [76/120    avg_loss:0.293, val_acc:0.952]
Epoch [77/120    avg_loss:0.207, val_acc:0.960]
Epoch [78/120    avg_loss:0.189, val_acc:0.973]
Epoch [79/120    avg_loss:0.140, val_acc:0.965]
Epoch [80/120    avg_loss:0.155, val_acc:0.977]
Epoch [81/120    avg_loss:0.168, val_acc:0.975]
Epoch [82/120    avg_loss:0.168, val_acc:0.971]
Epoch [83/120    avg_loss:0.129, val_acc:0.981]
Epoch [84/120    avg_loss:0.135, val_acc:0.944]
Epoch [85/120    avg_loss:0.160, val_acc:0.981]
Epoch [86/120    avg_loss:0.145, val_acc:0.969]
Epoch [87/120    avg_loss:0.121, val_acc:0.981]
Epoch [88/120    avg_loss:0.114, val_acc:0.983]
Epoch [89/120    avg_loss:0.127, val_acc:0.973]
Epoch [90/120    avg_loss:0.094, val_acc:0.990]
Epoch [91/120    avg_loss:0.083, val_acc:0.983]
Epoch [92/120    avg_loss:0.084, val_acc:0.979]
Epoch [93/120    avg_loss:0.101, val_acc:0.960]
Epoch [94/120    avg_loss:0.139, val_acc:0.971]
Epoch [95/120    avg_loss:0.126, val_acc:0.990]
Epoch [96/120    avg_loss:0.119, val_acc:0.977]
Epoch [97/120    avg_loss:0.140, val_acc:0.954]
Epoch [98/120    avg_loss:0.152, val_acc:0.965]
Epoch [99/120    avg_loss:0.112, val_acc:0.969]
Epoch [100/120    avg_loss:0.100, val_acc:0.988]
Epoch [101/120    avg_loss:0.086, val_acc:0.990]
Epoch [102/120    avg_loss:0.078, val_acc:0.988]
Epoch [103/120    avg_loss:0.067, val_acc:0.990]
Epoch [104/120    avg_loss:0.082, val_acc:0.985]
Epoch [105/120    avg_loss:0.088, val_acc:0.988]
Epoch [106/120    avg_loss:0.071, val_acc:0.985]
Epoch [107/120    avg_loss:0.073, val_acc:0.985]
Epoch [108/120    avg_loss:0.103, val_acc:0.971]
Epoch [109/120    avg_loss:0.089, val_acc:0.981]
Epoch [110/120    avg_loss:0.079, val_acc:0.983]
Epoch [111/120    avg_loss:0.098, val_acc:0.983]
Epoch [112/120    avg_loss:0.087, val_acc:0.983]
Epoch [113/120    avg_loss:0.081, val_acc:0.981]
Epoch [114/120    avg_loss:0.068, val_acc:0.988]
Epoch [115/120    avg_loss:0.073, val_acc:0.979]
Epoch [116/120    avg_loss:0.082, val_acc:0.979]
Epoch [117/120    avg_loss:0.078, val_acc:0.988]
Epoch [118/120    avg_loss:0.057, val_acc:0.985]
Epoch [119/120    avg_loss:0.058, val_acc:0.985]
Epoch [120/120    avg_loss:0.046, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 226   2   0   0   1   0   1   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.97078652 0.99122807 0.93333333 0.90540541
 1.         0.92307692 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9893175755467081
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f56c98dcac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.279]
Epoch [2/120    avg_loss:2.525, val_acc:0.344]
Epoch [3/120    avg_loss:2.454, val_acc:0.338]
Epoch [4/120    avg_loss:2.391, val_acc:0.352]
Epoch [5/120    avg_loss:2.330, val_acc:0.394]
Epoch [6/120    avg_loss:2.285, val_acc:0.417]
Epoch [7/120    avg_loss:2.236, val_acc:0.492]
Epoch [8/120    avg_loss:2.197, val_acc:0.508]
Epoch [9/120    avg_loss:2.127, val_acc:0.529]
Epoch [10/120    avg_loss:2.061, val_acc:0.571]
Epoch [11/120    avg_loss:1.997, val_acc:0.585]
Epoch [12/120    avg_loss:1.936, val_acc:0.575]
Epoch [13/120    avg_loss:1.857, val_acc:0.600]
Epoch [14/120    avg_loss:1.795, val_acc:0.592]
Epoch [15/120    avg_loss:1.744, val_acc:0.633]
Epoch [16/120    avg_loss:1.669, val_acc:0.635]
Epoch [17/120    avg_loss:1.601, val_acc:0.673]
Epoch [18/120    avg_loss:1.517, val_acc:0.665]
Epoch [19/120    avg_loss:1.463, val_acc:0.677]
Epoch [20/120    avg_loss:1.368, val_acc:0.690]
Epoch [21/120    avg_loss:1.264, val_acc:0.733]
Epoch [22/120    avg_loss:1.202, val_acc:0.696]
Epoch [23/120    avg_loss:1.140, val_acc:0.817]
Epoch [24/120    avg_loss:1.038, val_acc:0.779]
Epoch [25/120    avg_loss:1.003, val_acc:0.821]
Epoch [26/120    avg_loss:0.913, val_acc:0.898]
Epoch [27/120    avg_loss:0.878, val_acc:0.885]
Epoch [28/120    avg_loss:0.833, val_acc:0.898]
Epoch [29/120    avg_loss:0.775, val_acc:0.881]
Epoch [30/120    avg_loss:0.726, val_acc:0.879]
Epoch [31/120    avg_loss:0.661, val_acc:0.910]
Epoch [32/120    avg_loss:0.664, val_acc:0.906]
Epoch [33/120    avg_loss:0.606, val_acc:0.912]
Epoch [34/120    avg_loss:0.650, val_acc:0.890]
Epoch [35/120    avg_loss:0.644, val_acc:0.900]
Epoch [36/120    avg_loss:0.609, val_acc:0.904]
Epoch [37/120    avg_loss:0.549, val_acc:0.900]
Epoch [38/120    avg_loss:0.486, val_acc:0.921]
Epoch [39/120    avg_loss:0.466, val_acc:0.910]
Epoch [40/120    avg_loss:0.559, val_acc:0.879]
Epoch [41/120    avg_loss:0.482, val_acc:0.938]
Epoch [42/120    avg_loss:0.451, val_acc:0.919]
Epoch [43/120    avg_loss:0.450, val_acc:0.831]
Epoch [44/120    avg_loss:0.464, val_acc:0.885]
Epoch [45/120    avg_loss:0.439, val_acc:0.935]
Epoch [46/120    avg_loss:0.393, val_acc:0.921]
Epoch [47/120    avg_loss:0.394, val_acc:0.938]
Epoch [48/120    avg_loss:0.386, val_acc:0.919]
Epoch [49/120    avg_loss:0.355, val_acc:0.940]
Epoch [50/120    avg_loss:0.334, val_acc:0.946]
Epoch [51/120    avg_loss:0.329, val_acc:0.942]
Epoch [52/120    avg_loss:0.339, val_acc:0.950]
Epoch [53/120    avg_loss:0.289, val_acc:0.940]
Epoch [54/120    avg_loss:0.315, val_acc:0.935]
Epoch [55/120    avg_loss:0.284, val_acc:0.946]
Epoch [56/120    avg_loss:0.303, val_acc:0.950]
Epoch [57/120    avg_loss:0.327, val_acc:0.910]
Epoch [58/120    avg_loss:0.315, val_acc:0.950]
Epoch [59/120    avg_loss:0.309, val_acc:0.942]
Epoch [60/120    avg_loss:0.302, val_acc:0.952]
Epoch [61/120    avg_loss:0.294, val_acc:0.902]
Epoch [62/120    avg_loss:0.319, val_acc:0.946]
Epoch [63/120    avg_loss:0.252, val_acc:0.960]
Epoch [64/120    avg_loss:0.303, val_acc:0.865]
Epoch [65/120    avg_loss:0.346, val_acc:0.948]
Epoch [66/120    avg_loss:0.285, val_acc:0.971]
Epoch [67/120    avg_loss:0.204, val_acc:0.952]
Epoch [68/120    avg_loss:0.246, val_acc:0.963]
Epoch [69/120    avg_loss:0.221, val_acc:0.940]
Epoch [70/120    avg_loss:0.238, val_acc:0.956]
Epoch [71/120    avg_loss:0.264, val_acc:0.963]
Epoch [72/120    avg_loss:0.267, val_acc:0.954]
Epoch [73/120    avg_loss:0.268, val_acc:0.952]
Epoch [74/120    avg_loss:0.250, val_acc:0.960]
Epoch [75/120    avg_loss:0.207, val_acc:0.969]
Epoch [76/120    avg_loss:0.201, val_acc:0.977]
Epoch [77/120    avg_loss:0.205, val_acc:0.910]
Epoch [78/120    avg_loss:0.248, val_acc:0.965]
Epoch [79/120    avg_loss:0.197, val_acc:0.973]
Epoch [80/120    avg_loss:0.176, val_acc:0.969]
Epoch [81/120    avg_loss:0.164, val_acc:0.973]
Epoch [82/120    avg_loss:0.172, val_acc:0.956]
Epoch [83/120    avg_loss:0.199, val_acc:0.958]
Epoch [84/120    avg_loss:0.199, val_acc:0.958]
Epoch [85/120    avg_loss:0.178, val_acc:0.975]
Epoch [86/120    avg_loss:0.184, val_acc:0.971]
Epoch [87/120    avg_loss:0.185, val_acc:0.969]
Epoch [88/120    avg_loss:0.197, val_acc:0.971]
Epoch [89/120    avg_loss:0.190, val_acc:0.969]
Epoch [90/120    avg_loss:0.154, val_acc:0.977]
Epoch [91/120    avg_loss:0.114, val_acc:0.977]
Epoch [92/120    avg_loss:0.118, val_acc:0.977]
Epoch [93/120    avg_loss:0.120, val_acc:0.979]
Epoch [94/120    avg_loss:0.121, val_acc:0.973]
Epoch [95/120    avg_loss:0.114, val_acc:0.973]
Epoch [96/120    avg_loss:0.134, val_acc:0.975]
Epoch [97/120    avg_loss:0.112, val_acc:0.973]
Epoch [98/120    avg_loss:0.117, val_acc:0.981]
Epoch [99/120    avg_loss:0.102, val_acc:0.981]
Epoch [100/120    avg_loss:0.110, val_acc:0.973]
Epoch [101/120    avg_loss:0.093, val_acc:0.977]
Epoch [102/120    avg_loss:0.094, val_acc:0.977]
Epoch [103/120    avg_loss:0.097, val_acc:0.975]
Epoch [104/120    avg_loss:0.092, val_acc:0.979]
Epoch [105/120    avg_loss:0.086, val_acc:0.979]
Epoch [106/120    avg_loss:0.095, val_acc:0.981]
Epoch [107/120    avg_loss:0.099, val_acc:0.977]
Epoch [108/120    avg_loss:0.095, val_acc:0.977]
Epoch [109/120    avg_loss:0.098, val_acc:0.981]
Epoch [110/120    avg_loss:0.116, val_acc:0.979]
Epoch [111/120    avg_loss:0.098, val_acc:0.977]
Epoch [112/120    avg_loss:0.109, val_acc:0.977]
Epoch [113/120    avg_loss:0.111, val_acc:0.981]
Epoch [114/120    avg_loss:0.112, val_acc:0.979]
Epoch [115/120    avg_loss:0.090, val_acc:0.979]
Epoch [116/120    avg_loss:0.085, val_acc:0.979]
Epoch [117/120    avg_loss:0.095, val_acc:0.979]
Epoch [118/120    avg_loss:0.079, val_acc:0.981]
Epoch [119/120    avg_loss:0.080, val_acc:0.979]
Epoch [120/120    avg_loss:0.083, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 217  13   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 1.         0.97117517 0.97091723 0.875      0.84641638
 1.         0.92571429 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.982907310222702
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a498cfa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.647, val_acc:0.185]
Epoch [2/120    avg_loss:2.573, val_acc:0.248]
Epoch [3/120    avg_loss:2.500, val_acc:0.267]
Epoch [4/120    avg_loss:2.418, val_acc:0.331]
Epoch [5/120    avg_loss:2.346, val_acc:0.500]
Epoch [6/120    avg_loss:2.291, val_acc:0.471]
Epoch [7/120    avg_loss:2.224, val_acc:0.469]
Epoch [8/120    avg_loss:2.171, val_acc:0.483]
Epoch [9/120    avg_loss:2.105, val_acc:0.504]
Epoch [10/120    avg_loss:2.051, val_acc:0.531]
Epoch [11/120    avg_loss:1.986, val_acc:0.550]
Epoch [12/120    avg_loss:1.911, val_acc:0.565]
Epoch [13/120    avg_loss:1.856, val_acc:0.581]
Epoch [14/120    avg_loss:1.798, val_acc:0.592]
Epoch [15/120    avg_loss:1.701, val_acc:0.621]
Epoch [16/120    avg_loss:1.653, val_acc:0.637]
Epoch [17/120    avg_loss:1.581, val_acc:0.665]
Epoch [18/120    avg_loss:1.494, val_acc:0.685]
Epoch [19/120    avg_loss:1.386, val_acc:0.698]
Epoch [20/120    avg_loss:1.358, val_acc:0.706]
Epoch [21/120    avg_loss:1.283, val_acc:0.723]
Epoch [22/120    avg_loss:1.200, val_acc:0.790]
Epoch [23/120    avg_loss:1.084, val_acc:0.827]
Epoch [24/120    avg_loss:1.037, val_acc:0.852]
Epoch [25/120    avg_loss:0.972, val_acc:0.883]
Epoch [26/120    avg_loss:0.888, val_acc:0.854]
Epoch [27/120    avg_loss:0.854, val_acc:0.906]
Epoch [28/120    avg_loss:0.787, val_acc:0.904]
Epoch [29/120    avg_loss:0.711, val_acc:0.846]
Epoch [30/120    avg_loss:0.694, val_acc:0.898]
Epoch [31/120    avg_loss:0.666, val_acc:0.900]
Epoch [32/120    avg_loss:0.706, val_acc:0.912]
Epoch [33/120    avg_loss:0.604, val_acc:0.915]
Epoch [34/120    avg_loss:0.559, val_acc:0.917]
Epoch [35/120    avg_loss:0.512, val_acc:0.912]
Epoch [36/120    avg_loss:0.493, val_acc:0.904]
Epoch [37/120    avg_loss:0.488, val_acc:0.925]
Epoch [38/120    avg_loss:0.470, val_acc:0.915]
Epoch [39/120    avg_loss:0.463, val_acc:0.900]
Epoch [40/120    avg_loss:0.406, val_acc:0.935]
Epoch [41/120    avg_loss:0.429, val_acc:0.917]
Epoch [42/120    avg_loss:0.414, val_acc:0.940]
Epoch [43/120    avg_loss:0.372, val_acc:0.929]
Epoch [44/120    avg_loss:0.362, val_acc:0.912]
Epoch [45/120    avg_loss:0.342, val_acc:0.942]
Epoch [46/120    avg_loss:0.315, val_acc:0.923]
Epoch [47/120    avg_loss:0.350, val_acc:0.929]
Epoch [48/120    avg_loss:0.319, val_acc:0.948]
Epoch [49/120    avg_loss:0.285, val_acc:0.942]
Epoch [50/120    avg_loss:0.277, val_acc:0.950]
Epoch [51/120    avg_loss:0.297, val_acc:0.908]
Epoch [52/120    avg_loss:0.334, val_acc:0.933]
Epoch [53/120    avg_loss:0.286, val_acc:0.960]
Epoch [54/120    avg_loss:0.229, val_acc:0.938]
Epoch [55/120    avg_loss:0.279, val_acc:0.944]
Epoch [56/120    avg_loss:0.280, val_acc:0.952]
Epoch [57/120    avg_loss:0.287, val_acc:0.933]
Epoch [58/120    avg_loss:0.238, val_acc:0.933]
Epoch [59/120    avg_loss:0.232, val_acc:0.948]
Epoch [60/120    avg_loss:0.333, val_acc:0.917]
Epoch [61/120    avg_loss:0.269, val_acc:0.952]
Epoch [62/120    avg_loss:0.275, val_acc:0.948]
Epoch [63/120    avg_loss:0.275, val_acc:0.958]
Epoch [64/120    avg_loss:0.208, val_acc:0.958]
Epoch [65/120    avg_loss:0.178, val_acc:0.952]
Epoch [66/120    avg_loss:0.241, val_acc:0.944]
Epoch [67/120    avg_loss:0.199, val_acc:0.956]
Epoch [68/120    avg_loss:0.156, val_acc:0.956]
Epoch [69/120    avg_loss:0.171, val_acc:0.958]
Epoch [70/120    avg_loss:0.166, val_acc:0.960]
Epoch [71/120    avg_loss:0.139, val_acc:0.963]
Epoch [72/120    avg_loss:0.144, val_acc:0.960]
Epoch [73/120    avg_loss:0.137, val_acc:0.960]
Epoch [74/120    avg_loss:0.154, val_acc:0.963]
Epoch [75/120    avg_loss:0.132, val_acc:0.963]
Epoch [76/120    avg_loss:0.145, val_acc:0.963]
Epoch [77/120    avg_loss:0.142, val_acc:0.960]
Epoch [78/120    avg_loss:0.153, val_acc:0.960]
Epoch [79/120    avg_loss:0.142, val_acc:0.960]
Epoch [80/120    avg_loss:0.135, val_acc:0.960]
Epoch [81/120    avg_loss:0.133, val_acc:0.963]
Epoch [82/120    avg_loss:0.146, val_acc:0.960]
Epoch [83/120    avg_loss:0.124, val_acc:0.965]
Epoch [84/120    avg_loss:0.125, val_acc:0.963]
Epoch [85/120    avg_loss:0.123, val_acc:0.960]
Epoch [86/120    avg_loss:0.149, val_acc:0.963]
Epoch [87/120    avg_loss:0.135, val_acc:0.969]
Epoch [88/120    avg_loss:0.120, val_acc:0.969]
Epoch [89/120    avg_loss:0.141, val_acc:0.965]
Epoch [90/120    avg_loss:0.129, val_acc:0.960]
Epoch [91/120    avg_loss:0.136, val_acc:0.965]
Epoch [92/120    avg_loss:0.127, val_acc:0.965]
Epoch [93/120    avg_loss:0.133, val_acc:0.967]
Epoch [94/120    avg_loss:0.129, val_acc:0.965]
Epoch [95/120    avg_loss:0.115, val_acc:0.963]
Epoch [96/120    avg_loss:0.123, val_acc:0.967]
Epoch [97/120    avg_loss:0.124, val_acc:0.963]
Epoch [98/120    avg_loss:0.117, val_acc:0.965]
Epoch [99/120    avg_loss:0.130, val_acc:0.967]
Epoch [100/120    avg_loss:0.120, val_acc:0.965]
Epoch [101/120    avg_loss:0.112, val_acc:0.969]
Epoch [102/120    avg_loss:0.118, val_acc:0.967]
Epoch [103/120    avg_loss:0.108, val_acc:0.965]
Epoch [104/120    avg_loss:0.127, val_acc:0.965]
Epoch [105/120    avg_loss:0.111, val_acc:0.963]
Epoch [106/120    avg_loss:0.132, val_acc:0.963]
Epoch [107/120    avg_loss:0.113, val_acc:0.963]
Epoch [108/120    avg_loss:0.121, val_acc:0.963]
Epoch [109/120    avg_loss:0.117, val_acc:0.967]
Epoch [110/120    avg_loss:0.126, val_acc:0.967]
Epoch [111/120    avg_loss:0.122, val_acc:0.971]
Epoch [112/120    avg_loss:0.118, val_acc:0.971]
Epoch [113/120    avg_loss:0.132, val_acc:0.971]
Epoch [114/120    avg_loss:0.107, val_acc:0.969]
Epoch [115/120    avg_loss:0.111, val_acc:0.967]
Epoch [116/120    avg_loss:0.107, val_acc:0.969]
Epoch [117/120    avg_loss:0.117, val_acc:0.967]
Epoch [118/120    avg_loss:0.110, val_acc:0.965]
Epoch [119/120    avg_loss:0.133, val_acc:0.967]
Epoch [120/120    avg_loss:0.104, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  29 116   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 1.         0.97333333 1.         0.89177489 0.82269504
 1.         0.93181818 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9852807435699811
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9582858b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.598, val_acc:0.152]
Epoch [2/120    avg_loss:2.525, val_acc:0.321]
Epoch [3/120    avg_loss:2.449, val_acc:0.465]
Epoch [4/120    avg_loss:2.385, val_acc:0.494]
Epoch [5/120    avg_loss:2.315, val_acc:0.506]
Epoch [6/120    avg_loss:2.253, val_acc:0.490]
Epoch [7/120    avg_loss:2.199, val_acc:0.487]
Epoch [8/120    avg_loss:2.136, val_acc:0.494]
Epoch [9/120    avg_loss:2.065, val_acc:0.496]
Epoch [10/120    avg_loss:2.022, val_acc:0.512]
Epoch [11/120    avg_loss:1.948, val_acc:0.519]
Epoch [12/120    avg_loss:1.889, val_acc:0.529]
Epoch [13/120    avg_loss:1.834, val_acc:0.544]
Epoch [14/120    avg_loss:1.765, val_acc:0.550]
Epoch [15/120    avg_loss:1.714, val_acc:0.560]
Epoch [16/120    avg_loss:1.644, val_acc:0.585]
Epoch [17/120    avg_loss:1.580, val_acc:0.621]
Epoch [18/120    avg_loss:1.523, val_acc:0.654]
Epoch [19/120    avg_loss:1.478, val_acc:0.662]
Epoch [20/120    avg_loss:1.399, val_acc:0.675]
Epoch [21/120    avg_loss:1.352, val_acc:0.685]
Epoch [22/120    avg_loss:1.294, val_acc:0.738]
Epoch [23/120    avg_loss:1.183, val_acc:0.756]
Epoch [24/120    avg_loss:1.141, val_acc:0.754]
Epoch [25/120    avg_loss:1.092, val_acc:0.748]
Epoch [26/120    avg_loss:1.055, val_acc:0.746]
Epoch [27/120    avg_loss:0.969, val_acc:0.802]
Epoch [28/120    avg_loss:0.889, val_acc:0.771]
Epoch [29/120    avg_loss:0.851, val_acc:0.806]
Epoch [30/120    avg_loss:0.800, val_acc:0.858]
Epoch [31/120    avg_loss:0.788, val_acc:0.821]
Epoch [32/120    avg_loss:0.775, val_acc:0.792]
Epoch [33/120    avg_loss:0.713, val_acc:0.850]
Epoch [34/120    avg_loss:0.704, val_acc:0.873]
Epoch [35/120    avg_loss:0.709, val_acc:0.910]
Epoch [36/120    avg_loss:0.656, val_acc:0.896]
Epoch [37/120    avg_loss:0.653, val_acc:0.900]
Epoch [38/120    avg_loss:0.588, val_acc:0.944]
Epoch [39/120    avg_loss:0.569, val_acc:0.915]
Epoch [40/120    avg_loss:0.551, val_acc:0.915]
Epoch [41/120    avg_loss:0.488, val_acc:0.917]
Epoch [42/120    avg_loss:0.492, val_acc:0.929]
Epoch [43/120    avg_loss:0.495, val_acc:0.919]
Epoch [44/120    avg_loss:0.466, val_acc:0.933]
Epoch [45/120    avg_loss:0.438, val_acc:0.954]
Epoch [46/120    avg_loss:0.393, val_acc:0.954]
Epoch [47/120    avg_loss:0.425, val_acc:0.917]
Epoch [48/120    avg_loss:0.418, val_acc:0.942]
Epoch [49/120    avg_loss:0.449, val_acc:0.921]
Epoch [50/120    avg_loss:0.417, val_acc:0.946]
Epoch [51/120    avg_loss:0.329, val_acc:0.948]
Epoch [52/120    avg_loss:0.459, val_acc:0.944]
Epoch [53/120    avg_loss:0.394, val_acc:0.952]
Epoch [54/120    avg_loss:0.335, val_acc:0.958]
Epoch [55/120    avg_loss:0.349, val_acc:0.938]
Epoch [56/120    avg_loss:0.301, val_acc:0.950]
Epoch [57/120    avg_loss:0.296, val_acc:0.956]
Epoch [58/120    avg_loss:0.289, val_acc:0.946]
Epoch [59/120    avg_loss:0.281, val_acc:0.940]
Epoch [60/120    avg_loss:0.292, val_acc:0.967]
Epoch [61/120    avg_loss:0.276, val_acc:0.963]
Epoch [62/120    avg_loss:0.286, val_acc:0.927]
Epoch [63/120    avg_loss:0.229, val_acc:0.954]
Epoch [64/120    avg_loss:0.226, val_acc:0.956]
Epoch [65/120    avg_loss:0.256, val_acc:0.917]
Epoch [66/120    avg_loss:0.247, val_acc:0.938]
Epoch [67/120    avg_loss:0.221, val_acc:0.971]
Epoch [68/120    avg_loss:0.274, val_acc:0.965]
Epoch [69/120    avg_loss:0.298, val_acc:0.956]
Epoch [70/120    avg_loss:0.287, val_acc:0.958]
Epoch [71/120    avg_loss:0.244, val_acc:0.958]
Epoch [72/120    avg_loss:0.260, val_acc:0.967]
Epoch [73/120    avg_loss:0.272, val_acc:0.965]
Epoch [74/120    avg_loss:0.257, val_acc:0.971]
Epoch [75/120    avg_loss:0.218, val_acc:0.975]
Epoch [76/120    avg_loss:0.211, val_acc:0.977]
Epoch [77/120    avg_loss:0.170, val_acc:0.977]
Epoch [78/120    avg_loss:0.144, val_acc:0.977]
Epoch [79/120    avg_loss:0.149, val_acc:0.969]
Epoch [80/120    avg_loss:0.157, val_acc:0.979]
Epoch [81/120    avg_loss:0.167, val_acc:0.973]
Epoch [82/120    avg_loss:0.173, val_acc:0.981]
Epoch [83/120    avg_loss:0.138, val_acc:0.983]
Epoch [84/120    avg_loss:0.115, val_acc:0.975]
Epoch [85/120    avg_loss:0.192, val_acc:0.971]
Epoch [86/120    avg_loss:0.156, val_acc:0.979]
Epoch [87/120    avg_loss:0.188, val_acc:0.965]
Epoch [88/120    avg_loss:0.164, val_acc:0.979]
Epoch [89/120    avg_loss:0.204, val_acc:0.942]
Epoch [90/120    avg_loss:0.246, val_acc:0.965]
Epoch [91/120    avg_loss:0.193, val_acc:0.969]
Epoch [92/120    avg_loss:0.141, val_acc:0.988]
Epoch [93/120    avg_loss:0.137, val_acc:0.988]
Epoch [94/120    avg_loss:0.112, val_acc:0.983]
Epoch [95/120    avg_loss:0.167, val_acc:0.967]
Epoch [96/120    avg_loss:0.190, val_acc:0.958]
Epoch [97/120    avg_loss:0.132, val_acc:0.981]
Epoch [98/120    avg_loss:0.108, val_acc:0.975]
Epoch [99/120    avg_loss:0.101, val_acc:0.977]
Epoch [100/120    avg_loss:0.092, val_acc:0.983]
Epoch [101/120    avg_loss:0.089, val_acc:0.983]
Epoch [102/120    avg_loss:0.110, val_acc:0.988]
Epoch [103/120    avg_loss:0.102, val_acc:0.981]
Epoch [104/120    avg_loss:0.105, val_acc:0.973]
Epoch [105/120    avg_loss:0.079, val_acc:0.979]
Epoch [106/120    avg_loss:0.095, val_acc:0.988]
Epoch [107/120    avg_loss:0.084, val_acc:0.990]
Epoch [108/120    avg_loss:0.083, val_acc:0.990]
Epoch [109/120    avg_loss:0.086, val_acc:0.988]
Epoch [110/120    avg_loss:0.210, val_acc:0.969]
Epoch [111/120    avg_loss:0.196, val_acc:0.977]
Epoch [112/120    avg_loss:0.121, val_acc:0.975]
Epoch [113/120    avg_loss:0.169, val_acc:0.952]
Epoch [114/120    avg_loss:0.168, val_acc:0.973]
Epoch [115/120    avg_loss:0.104, val_acc:0.981]
Epoch [116/120    avg_loss:0.098, val_acc:0.988]
Epoch [117/120    avg_loss:0.077, val_acc:0.985]
Epoch [118/120    avg_loss:0.081, val_acc:0.990]
Epoch [119/120    avg_loss:0.061, val_acc:0.990]
Epoch [120/120    avg_loss:0.051, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   0   0   2   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0  60 143  24   0   0   0   0   0   0   0   0]
 [  0   0   0   9   0 136   0   0   0   0   0   0   0   0]
 [  0   0   0  15   0   0 191   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.22814498933901

F1 scores:
[       nan 0.99853801 0.95842451 0.84346225 0.77297297 0.89180328
 0.96221662 0.88757396 0.99742931 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9691382927427052
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c1dbe2ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.632, val_acc:0.094]
Epoch [2/120    avg_loss:2.531, val_acc:0.204]
Epoch [3/120    avg_loss:2.443, val_acc:0.265]
Epoch [4/120    avg_loss:2.368, val_acc:0.294]
Epoch [5/120    avg_loss:2.316, val_acc:0.333]
Epoch [6/120    avg_loss:2.250, val_acc:0.362]
Epoch [7/120    avg_loss:2.190, val_acc:0.412]
Epoch [8/120    avg_loss:2.147, val_acc:0.404]
Epoch [9/120    avg_loss:2.082, val_acc:0.450]
Epoch [10/120    avg_loss:2.011, val_acc:0.517]
Epoch [11/120    avg_loss:1.941, val_acc:0.546]
Epoch [12/120    avg_loss:1.885, val_acc:0.552]
Epoch [13/120    avg_loss:1.825, val_acc:0.569]
Epoch [14/120    avg_loss:1.761, val_acc:0.592]
Epoch [15/120    avg_loss:1.671, val_acc:0.613]
Epoch [16/120    avg_loss:1.598, val_acc:0.627]
Epoch [17/120    avg_loss:1.546, val_acc:0.652]
Epoch [18/120    avg_loss:1.448, val_acc:0.665]
Epoch [19/120    avg_loss:1.388, val_acc:0.696]
Epoch [20/120    avg_loss:1.301, val_acc:0.685]
Epoch [21/120    avg_loss:1.262, val_acc:0.710]
Epoch [22/120    avg_loss:1.157, val_acc:0.746]
Epoch [23/120    avg_loss:1.058, val_acc:0.785]
Epoch [24/120    avg_loss:1.007, val_acc:0.785]
Epoch [25/120    avg_loss:0.936, val_acc:0.787]
Epoch [26/120    avg_loss:0.880, val_acc:0.860]
Epoch [27/120    avg_loss:0.802, val_acc:0.881]
Epoch [28/120    avg_loss:0.797, val_acc:0.867]
Epoch [29/120    avg_loss:0.753, val_acc:0.892]
Epoch [30/120    avg_loss:0.662, val_acc:0.894]
Epoch [31/120    avg_loss:0.685, val_acc:0.817]
Epoch [32/120    avg_loss:0.639, val_acc:0.931]
Epoch [33/120    avg_loss:0.646, val_acc:0.898]
Epoch [34/120    avg_loss:0.567, val_acc:0.944]
Epoch [35/120    avg_loss:0.535, val_acc:0.921]
Epoch [36/120    avg_loss:0.568, val_acc:0.900]
Epoch [37/120    avg_loss:0.518, val_acc:0.908]
Epoch [38/120    avg_loss:0.512, val_acc:0.923]
Epoch [39/120    avg_loss:0.517, val_acc:0.898]
Epoch [40/120    avg_loss:0.436, val_acc:0.954]
Epoch [41/120    avg_loss:0.414, val_acc:0.927]
Epoch [42/120    avg_loss:0.371, val_acc:0.952]
Epoch [43/120    avg_loss:0.352, val_acc:0.946]
Epoch [44/120    avg_loss:0.376, val_acc:0.960]
Epoch [45/120    avg_loss:0.368, val_acc:0.929]
Epoch [46/120    avg_loss:0.348, val_acc:0.938]
Epoch [47/120    avg_loss:0.319, val_acc:0.963]
Epoch [48/120    avg_loss:0.365, val_acc:0.950]
Epoch [49/120    avg_loss:0.354, val_acc:0.954]
Epoch [50/120    avg_loss:0.315, val_acc:0.965]
Epoch [51/120    avg_loss:0.258, val_acc:0.960]
Epoch [52/120    avg_loss:0.301, val_acc:0.938]
Epoch [53/120    avg_loss:0.352, val_acc:0.950]
Epoch [54/120    avg_loss:0.284, val_acc:0.969]
Epoch [55/120    avg_loss:0.269, val_acc:0.958]
Epoch [56/120    avg_loss:0.265, val_acc:0.956]
Epoch [57/120    avg_loss:0.260, val_acc:0.952]
Epoch [58/120    avg_loss:0.294, val_acc:0.969]
Epoch [59/120    avg_loss:0.262, val_acc:0.948]
Epoch [60/120    avg_loss:0.285, val_acc:0.977]
Epoch [61/120    avg_loss:0.242, val_acc:0.952]
Epoch [62/120    avg_loss:0.233, val_acc:0.956]
Epoch [63/120    avg_loss:0.241, val_acc:0.963]
Epoch [64/120    avg_loss:0.270, val_acc:0.902]
Epoch [65/120    avg_loss:0.262, val_acc:0.946]
Epoch [66/120    avg_loss:0.262, val_acc:0.965]
Epoch [67/120    avg_loss:0.248, val_acc:0.963]
Epoch [68/120    avg_loss:0.169, val_acc:0.965]
Epoch [69/120    avg_loss:0.220, val_acc:0.963]
Epoch [70/120    avg_loss:0.176, val_acc:0.950]
Epoch [71/120    avg_loss:0.185, val_acc:0.965]
Epoch [72/120    avg_loss:0.223, val_acc:0.965]
Epoch [73/120    avg_loss:0.180, val_acc:0.958]
Epoch [74/120    avg_loss:0.151, val_acc:0.971]
Epoch [75/120    avg_loss:0.153, val_acc:0.979]
Epoch [76/120    avg_loss:0.141, val_acc:0.973]
Epoch [77/120    avg_loss:0.135, val_acc:0.977]
Epoch [78/120    avg_loss:0.121, val_acc:0.977]
Epoch [79/120    avg_loss:0.129, val_acc:0.977]
Epoch [80/120    avg_loss:0.110, val_acc:0.979]
Epoch [81/120    avg_loss:0.123, val_acc:0.977]
Epoch [82/120    avg_loss:0.136, val_acc:0.979]
Epoch [83/120    avg_loss:0.124, val_acc:0.977]
Epoch [84/120    avg_loss:0.123, val_acc:0.977]
Epoch [85/120    avg_loss:0.124, val_acc:0.977]
Epoch [86/120    avg_loss:0.099, val_acc:0.977]
Epoch [87/120    avg_loss:0.118, val_acc:0.977]
Epoch [88/120    avg_loss:0.121, val_acc:0.981]
Epoch [89/120    avg_loss:0.106, val_acc:0.977]
Epoch [90/120    avg_loss:0.102, val_acc:0.977]
Epoch [91/120    avg_loss:0.119, val_acc:0.977]
Epoch [92/120    avg_loss:0.122, val_acc:0.975]
Epoch [93/120    avg_loss:0.107, val_acc:0.977]
Epoch [94/120    avg_loss:0.098, val_acc:0.981]
Epoch [95/120    avg_loss:0.109, val_acc:0.973]
Epoch [96/120    avg_loss:0.117, val_acc:0.979]
Epoch [97/120    avg_loss:0.122, val_acc:0.977]
Epoch [98/120    avg_loss:0.143, val_acc:0.977]
Epoch [99/120    avg_loss:0.105, val_acc:0.973]
Epoch [100/120    avg_loss:0.106, val_acc:0.979]
Epoch [101/120    avg_loss:0.104, val_acc:0.983]
Epoch [102/120    avg_loss:0.100, val_acc:0.977]
Epoch [103/120    avg_loss:0.105, val_acc:0.979]
Epoch [104/120    avg_loss:0.101, val_acc:0.977]
Epoch [105/120    avg_loss:0.111, val_acc:0.977]
Epoch [106/120    avg_loss:0.089, val_acc:0.977]
Epoch [107/120    avg_loss:0.116, val_acc:0.981]
Epoch [108/120    avg_loss:0.102, val_acc:0.981]
Epoch [109/120    avg_loss:0.102, val_acc:0.981]
Epoch [110/120    avg_loss:0.108, val_acc:0.977]
Epoch [111/120    avg_loss:0.095, val_acc:0.977]
Epoch [112/120    avg_loss:0.105, val_acc:0.979]
Epoch [113/120    avg_loss:0.105, val_acc:0.975]
Epoch [114/120    avg_loss:0.110, val_acc:0.981]
Epoch [115/120    avg_loss:0.088, val_acc:0.981]
Epoch [116/120    avg_loss:0.099, val_acc:0.981]
Epoch [117/120    avg_loss:0.094, val_acc:0.981]
Epoch [118/120    avg_loss:0.086, val_acc:0.979]
Epoch [119/120    avg_loss:0.095, val_acc:0.979]
Epoch [120/120    avg_loss:0.092, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 227   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 200  27   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.96263736 0.99343545 0.91954023 0.89032258
 0.99756691 0.9132948  1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9874184823238711
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9aff2f9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.179]
Epoch [2/120    avg_loss:2.543, val_acc:0.388]
Epoch [3/120    avg_loss:2.450, val_acc:0.496]
Epoch [4/120    avg_loss:2.363, val_acc:0.504]
Epoch [5/120    avg_loss:2.284, val_acc:0.517]
Epoch [6/120    avg_loss:2.217, val_acc:0.542]
Epoch [7/120    avg_loss:2.137, val_acc:0.596]
Epoch [8/120    avg_loss:2.069, val_acc:0.598]
Epoch [9/120    avg_loss:1.999, val_acc:0.596]
Epoch [10/120    avg_loss:1.908, val_acc:0.617]
Epoch [11/120    avg_loss:1.840, val_acc:0.635]
Epoch [12/120    avg_loss:1.764, val_acc:0.627]
Epoch [13/120    avg_loss:1.661, val_acc:0.637]
Epoch [14/120    avg_loss:1.569, val_acc:0.654]
Epoch [15/120    avg_loss:1.498, val_acc:0.677]
Epoch [16/120    avg_loss:1.422, val_acc:0.696]
Epoch [17/120    avg_loss:1.341, val_acc:0.708]
Epoch [18/120    avg_loss:1.303, val_acc:0.769]
Epoch [19/120    avg_loss:1.191, val_acc:0.717]
Epoch [20/120    avg_loss:1.131, val_acc:0.725]
Epoch [21/120    avg_loss:1.137, val_acc:0.804]
Epoch [22/120    avg_loss:0.972, val_acc:0.781]
Epoch [23/120    avg_loss:0.910, val_acc:0.842]
Epoch [24/120    avg_loss:0.888, val_acc:0.873]
Epoch [25/120    avg_loss:0.807, val_acc:0.856]
Epoch [26/120    avg_loss:0.776, val_acc:0.840]
Epoch [27/120    avg_loss:0.770, val_acc:0.812]
Epoch [28/120    avg_loss:0.720, val_acc:0.904]
Epoch [29/120    avg_loss:0.641, val_acc:0.896]
Epoch [30/120    avg_loss:0.653, val_acc:0.887]
Epoch [31/120    avg_loss:0.585, val_acc:0.898]
Epoch [32/120    avg_loss:0.596, val_acc:0.892]
Epoch [33/120    avg_loss:0.694, val_acc:0.838]
Epoch [34/120    avg_loss:0.638, val_acc:0.875]
Epoch [35/120    avg_loss:0.548, val_acc:0.894]
Epoch [36/120    avg_loss:0.532, val_acc:0.860]
Epoch [37/120    avg_loss:0.510, val_acc:0.898]
Epoch [38/120    avg_loss:0.470, val_acc:0.904]
Epoch [39/120    avg_loss:0.422, val_acc:0.906]
Epoch [40/120    avg_loss:0.420, val_acc:0.902]
Epoch [41/120    avg_loss:0.453, val_acc:0.917]
Epoch [42/120    avg_loss:0.426, val_acc:0.902]
Epoch [43/120    avg_loss:0.419, val_acc:0.906]
Epoch [44/120    avg_loss:0.376, val_acc:0.900]
Epoch [45/120    avg_loss:0.371, val_acc:0.910]
Epoch [46/120    avg_loss:0.368, val_acc:0.917]
Epoch [47/120    avg_loss:0.323, val_acc:0.944]
Epoch [48/120    avg_loss:0.386, val_acc:0.825]
Epoch [49/120    avg_loss:0.399, val_acc:0.940]
Epoch [50/120    avg_loss:0.345, val_acc:0.879]
Epoch [51/120    avg_loss:0.348, val_acc:0.942]
Epoch [52/120    avg_loss:0.323, val_acc:0.906]
Epoch [53/120    avg_loss:0.296, val_acc:0.944]
Epoch [54/120    avg_loss:0.281, val_acc:0.940]
Epoch [55/120    avg_loss:0.261, val_acc:0.921]
Epoch [56/120    avg_loss:0.242, val_acc:0.940]
Epoch [57/120    avg_loss:0.273, val_acc:0.952]
Epoch [58/120    avg_loss:0.245, val_acc:0.958]
Epoch [59/120    avg_loss:0.219, val_acc:0.940]
Epoch [60/120    avg_loss:0.223, val_acc:0.948]
Epoch [61/120    avg_loss:0.241, val_acc:0.933]
Epoch [62/120    avg_loss:0.212, val_acc:0.956]
Epoch [63/120    avg_loss:0.217, val_acc:0.954]
Epoch [64/120    avg_loss:0.226, val_acc:0.956]
Epoch [65/120    avg_loss:0.207, val_acc:0.935]
Epoch [66/120    avg_loss:0.223, val_acc:0.946]
Epoch [67/120    avg_loss:0.218, val_acc:0.950]
Epoch [68/120    avg_loss:0.213, val_acc:0.944]
Epoch [69/120    avg_loss:0.185, val_acc:0.967]
Epoch [70/120    avg_loss:0.194, val_acc:0.931]
Epoch [71/120    avg_loss:0.191, val_acc:0.948]
Epoch [72/120    avg_loss:0.155, val_acc:0.969]
Epoch [73/120    avg_loss:0.160, val_acc:0.965]
Epoch [74/120    avg_loss:0.180, val_acc:0.960]
Epoch [75/120    avg_loss:0.195, val_acc:0.923]
Epoch [76/120    avg_loss:0.213, val_acc:0.942]
Epoch [77/120    avg_loss:0.188, val_acc:0.958]
Epoch [78/120    avg_loss:0.149, val_acc:0.965]
Epoch [79/120    avg_loss:0.152, val_acc:0.963]
Epoch [80/120    avg_loss:0.153, val_acc:0.917]
Epoch [81/120    avg_loss:0.179, val_acc:0.931]
Epoch [82/120    avg_loss:0.173, val_acc:0.977]
Epoch [83/120    avg_loss:0.143, val_acc:0.973]
Epoch [84/120    avg_loss:0.168, val_acc:0.963]
Epoch [85/120    avg_loss:0.154, val_acc:0.963]
Epoch [86/120    avg_loss:0.159, val_acc:0.965]
Epoch [87/120    avg_loss:0.143, val_acc:0.975]
Epoch [88/120    avg_loss:0.136, val_acc:0.948]
Epoch [89/120    avg_loss:0.179, val_acc:0.963]
Epoch [90/120    avg_loss:0.168, val_acc:0.958]
Epoch [91/120    avg_loss:0.169, val_acc:0.965]
Epoch [92/120    avg_loss:0.111, val_acc:0.981]
Epoch [93/120    avg_loss:0.118, val_acc:0.983]
Epoch [94/120    avg_loss:0.103, val_acc:0.977]
Epoch [95/120    avg_loss:0.075, val_acc:0.977]
Epoch [96/120    avg_loss:0.102, val_acc:0.971]
Epoch [97/120    avg_loss:0.105, val_acc:0.977]
Epoch [98/120    avg_loss:0.089, val_acc:0.963]
Epoch [99/120    avg_loss:0.105, val_acc:0.967]
Epoch [100/120    avg_loss:0.090, val_acc:0.981]
Epoch [101/120    avg_loss:0.062, val_acc:0.979]
Epoch [102/120    avg_loss:0.089, val_acc:0.977]
Epoch [103/120    avg_loss:0.121, val_acc:0.977]
Epoch [104/120    avg_loss:0.129, val_acc:0.958]
Epoch [105/120    avg_loss:0.155, val_acc:0.956]
Epoch [106/120    avg_loss:0.096, val_acc:0.973]
Epoch [107/120    avg_loss:0.081, val_acc:0.977]
Epoch [108/120    avg_loss:0.067, val_acc:0.981]
Epoch [109/120    avg_loss:0.069, val_acc:0.983]
Epoch [110/120    avg_loss:0.068, val_acc:0.983]
Epoch [111/120    avg_loss:0.062, val_acc:0.981]
Epoch [112/120    avg_loss:0.055, val_acc:0.981]
Epoch [113/120    avg_loss:0.058, val_acc:0.983]
Epoch [114/120    avg_loss:0.049, val_acc:0.983]
Epoch [115/120    avg_loss:0.056, val_acc:0.983]
Epoch [116/120    avg_loss:0.050, val_acc:0.985]
Epoch [117/120    avg_loss:0.045, val_acc:0.983]
Epoch [118/120    avg_loss:0.048, val_acc:0.983]
Epoch [119/120    avg_loss:0.054, val_acc:0.983]
Epoch [120/120    avg_loss:0.051, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.97117517 1.         0.94930876 0.92903226
 1.         0.92571429 1.         1.         1.         0.98691099
 0.98883929 1.        ]

Kappa:
0.9893182535382494
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff201effac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.605, val_acc:0.221]
Epoch [2/120    avg_loss:2.511, val_acc:0.335]
Epoch [3/120    avg_loss:2.426, val_acc:0.325]
Epoch [4/120    avg_loss:2.357, val_acc:0.335]
Epoch [5/120    avg_loss:2.305, val_acc:0.410]
Epoch [6/120    avg_loss:2.237, val_acc:0.483]
Epoch [7/120    avg_loss:2.190, val_acc:0.477]
Epoch [8/120    avg_loss:2.115, val_acc:0.531]
Epoch [9/120    avg_loss:2.059, val_acc:0.542]
Epoch [10/120    avg_loss:1.979, val_acc:0.573]
Epoch [11/120    avg_loss:1.898, val_acc:0.627]
Epoch [12/120    avg_loss:1.831, val_acc:0.669]
Epoch [13/120    avg_loss:1.720, val_acc:0.690]
Epoch [14/120    avg_loss:1.656, val_acc:0.723]
Epoch [15/120    avg_loss:1.558, val_acc:0.735]
Epoch [16/120    avg_loss:1.458, val_acc:0.769]
Epoch [17/120    avg_loss:1.388, val_acc:0.781]
Epoch [18/120    avg_loss:1.273, val_acc:0.796]
Epoch [19/120    avg_loss:1.196, val_acc:0.800]
Epoch [20/120    avg_loss:1.113, val_acc:0.773]
Epoch [21/120    avg_loss:1.007, val_acc:0.819]
Epoch [22/120    avg_loss:0.922, val_acc:0.844]
Epoch [23/120    avg_loss:0.846, val_acc:0.796]
Epoch [24/120    avg_loss:0.831, val_acc:0.877]
Epoch [25/120    avg_loss:0.742, val_acc:0.894]
Epoch [26/120    avg_loss:0.721, val_acc:0.835]
Epoch [27/120    avg_loss:0.674, val_acc:0.894]
Epoch [28/120    avg_loss:0.622, val_acc:0.896]
Epoch [29/120    avg_loss:0.597, val_acc:0.894]
Epoch [30/120    avg_loss:0.603, val_acc:0.917]
Epoch [31/120    avg_loss:0.539, val_acc:0.931]
Epoch [32/120    avg_loss:0.490, val_acc:0.892]
Epoch [33/120    avg_loss:0.465, val_acc:0.921]
Epoch [34/120    avg_loss:0.470, val_acc:0.902]
Epoch [35/120    avg_loss:0.473, val_acc:0.904]
Epoch [36/120    avg_loss:0.442, val_acc:0.915]
Epoch [37/120    avg_loss:0.387, val_acc:0.919]
Epoch [38/120    avg_loss:0.420, val_acc:0.923]
Epoch [39/120    avg_loss:0.370, val_acc:0.929]
Epoch [40/120    avg_loss:0.372, val_acc:0.915]
Epoch [41/120    avg_loss:0.359, val_acc:0.940]
Epoch [42/120    avg_loss:0.362, val_acc:0.944]
Epoch [43/120    avg_loss:0.339, val_acc:0.938]
Epoch [44/120    avg_loss:0.317, val_acc:0.933]
Epoch [45/120    avg_loss:0.344, val_acc:0.923]
Epoch [46/120    avg_loss:0.320, val_acc:0.942]
Epoch [47/120    avg_loss:0.325, val_acc:0.933]
Epoch [48/120    avg_loss:0.247, val_acc:0.927]
Epoch [49/120    avg_loss:0.258, val_acc:0.950]
Epoch [50/120    avg_loss:0.296, val_acc:0.944]
Epoch [51/120    avg_loss:0.296, val_acc:0.938]
Epoch [52/120    avg_loss:0.279, val_acc:0.946]
Epoch [53/120    avg_loss:0.254, val_acc:0.954]
Epoch [54/120    avg_loss:0.231, val_acc:0.915]
Epoch [55/120    avg_loss:0.249, val_acc:0.917]
Epoch [56/120    avg_loss:0.262, val_acc:0.931]
Epoch [57/120    avg_loss:0.242, val_acc:0.912]
Epoch [58/120    avg_loss:0.251, val_acc:0.902]
Epoch [59/120    avg_loss:0.258, val_acc:0.965]
Epoch [60/120    avg_loss:0.220, val_acc:0.956]
Epoch [61/120    avg_loss:0.212, val_acc:0.883]
Epoch [62/120    avg_loss:0.213, val_acc:0.963]
Epoch [63/120    avg_loss:0.216, val_acc:0.958]
Epoch [64/120    avg_loss:0.200, val_acc:0.942]
Epoch [65/120    avg_loss:0.237, val_acc:0.960]
Epoch [66/120    avg_loss:0.186, val_acc:0.958]
Epoch [67/120    avg_loss:0.169, val_acc:0.967]
Epoch [68/120    avg_loss:0.181, val_acc:0.967]
Epoch [69/120    avg_loss:0.147, val_acc:0.977]
Epoch [70/120    avg_loss:0.143, val_acc:0.975]
Epoch [71/120    avg_loss:0.143, val_acc:0.969]
Epoch [72/120    avg_loss:0.163, val_acc:0.969]
Epoch [73/120    avg_loss:0.152, val_acc:0.971]
Epoch [74/120    avg_loss:0.143, val_acc:0.969]
Epoch [75/120    avg_loss:0.105, val_acc:0.971]
Epoch [76/120    avg_loss:0.148, val_acc:0.971]
Epoch [77/120    avg_loss:0.154, val_acc:0.969]
Epoch [78/120    avg_loss:0.144, val_acc:0.971]
Epoch [79/120    avg_loss:0.139, val_acc:0.960]
Epoch [80/120    avg_loss:0.172, val_acc:0.969]
Epoch [81/120    avg_loss:0.137, val_acc:0.971]
Epoch [82/120    avg_loss:0.128, val_acc:0.973]
Epoch [83/120    avg_loss:0.129, val_acc:0.990]
Epoch [84/120    avg_loss:0.106, val_acc:0.983]
Epoch [85/120    avg_loss:0.075, val_acc:0.983]
Epoch [86/120    avg_loss:0.098, val_acc:0.983]
Epoch [87/120    avg_loss:0.093, val_acc:0.983]
Epoch [88/120    avg_loss:0.075, val_acc:0.985]
Epoch [89/120    avg_loss:0.080, val_acc:0.988]
Epoch [90/120    avg_loss:0.075, val_acc:0.990]
Epoch [91/120    avg_loss:0.075, val_acc:0.988]
Epoch [92/120    avg_loss:0.078, val_acc:0.988]
Epoch [93/120    avg_loss:0.083, val_acc:0.990]
Epoch [94/120    avg_loss:0.067, val_acc:0.990]
Epoch [95/120    avg_loss:0.091, val_acc:0.988]
Epoch [96/120    avg_loss:0.070, val_acc:0.992]
Epoch [97/120    avg_loss:0.074, val_acc:0.990]
Epoch [98/120    avg_loss:0.068, val_acc:0.992]
Epoch [99/120    avg_loss:0.075, val_acc:0.992]
Epoch [100/120    avg_loss:0.074, val_acc:0.990]
Epoch [101/120    avg_loss:0.070, val_acc:0.990]
Epoch [102/120    avg_loss:0.072, val_acc:0.988]
Epoch [103/120    avg_loss:0.078, val_acc:0.992]
Epoch [104/120    avg_loss:0.068, val_acc:0.992]
Epoch [105/120    avg_loss:0.063, val_acc:0.994]
Epoch [106/120    avg_loss:0.063, val_acc:0.992]
Epoch [107/120    avg_loss:0.075, val_acc:0.988]
Epoch [108/120    avg_loss:0.067, val_acc:0.994]
Epoch [109/120    avg_loss:0.060, val_acc:0.994]
Epoch [110/120    avg_loss:0.071, val_acc:0.992]
Epoch [111/120    avg_loss:0.079, val_acc:0.992]
Epoch [112/120    avg_loss:0.066, val_acc:0.990]
Epoch [113/120    avg_loss:0.064, val_acc:0.992]
Epoch [114/120    avg_loss:0.068, val_acc:0.994]
Epoch [115/120    avg_loss:0.056, val_acc:0.992]
Epoch [116/120    avg_loss:0.070, val_acc:0.994]
Epoch [117/120    avg_loss:0.064, val_acc:0.992]
Epoch [118/120    avg_loss:0.058, val_acc:0.994]
Epoch [119/120    avg_loss:0.057, val_acc:0.988]
Epoch [120/120    avg_loss:0.053, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 228   0   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.97968397 0.99563319 0.93777778 0.9047619
 1.         0.95081967 1.         0.9978678  1.         0.99602649
 0.99668508 1.        ]

Kappa:
0.9900296271618102
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0bc4795a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.277]
Epoch [2/120    avg_loss:2.528, val_acc:0.283]
Epoch [3/120    avg_loss:2.449, val_acc:0.298]
Epoch [4/120    avg_loss:2.384, val_acc:0.317]
Epoch [5/120    avg_loss:2.326, val_acc:0.335]
Epoch [6/120    avg_loss:2.273, val_acc:0.350]
Epoch [7/120    avg_loss:2.225, val_acc:0.412]
Epoch [8/120    avg_loss:2.167, val_acc:0.440]
Epoch [9/120    avg_loss:2.107, val_acc:0.463]
Epoch [10/120    avg_loss:2.031, val_acc:0.483]
Epoch [11/120    avg_loss:1.971, val_acc:0.502]
Epoch [12/120    avg_loss:1.890, val_acc:0.531]
Epoch [13/120    avg_loss:1.816, val_acc:0.544]
Epoch [14/120    avg_loss:1.749, val_acc:0.619]
Epoch [15/120    avg_loss:1.657, val_acc:0.635]
Epoch [16/120    avg_loss:1.550, val_acc:0.637]
Epoch [17/120    avg_loss:1.479, val_acc:0.650]
Epoch [18/120    avg_loss:1.400, val_acc:0.675]
Epoch [19/120    avg_loss:1.306, val_acc:0.692]
Epoch [20/120    avg_loss:1.207, val_acc:0.710]
Epoch [21/120    avg_loss:1.142, val_acc:0.721]
Epoch [22/120    avg_loss:1.062, val_acc:0.756]
Epoch [23/120    avg_loss:1.004, val_acc:0.802]
Epoch [24/120    avg_loss:0.930, val_acc:0.856]
Epoch [25/120    avg_loss:0.884, val_acc:0.823]
Epoch [26/120    avg_loss:0.799, val_acc:0.881]
Epoch [27/120    avg_loss:0.814, val_acc:0.815]
Epoch [28/120    avg_loss:0.705, val_acc:0.894]
Epoch [29/120    avg_loss:0.678, val_acc:0.829]
Epoch [30/120    avg_loss:0.621, val_acc:0.860]
Epoch [31/120    avg_loss:0.633, val_acc:0.917]
Epoch [32/120    avg_loss:0.587, val_acc:0.892]
Epoch [33/120    avg_loss:0.556, val_acc:0.906]
Epoch [34/120    avg_loss:0.505, val_acc:0.906]
Epoch [35/120    avg_loss:0.464, val_acc:0.923]
Epoch [36/120    avg_loss:0.450, val_acc:0.923]
Epoch [37/120    avg_loss:0.489, val_acc:0.915]
Epoch [38/120    avg_loss:0.439, val_acc:0.910]
Epoch [39/120    avg_loss:0.445, val_acc:0.915]
Epoch [40/120    avg_loss:0.414, val_acc:0.904]
Epoch [41/120    avg_loss:0.437, val_acc:0.923]
Epoch [42/120    avg_loss:0.391, val_acc:0.931]
Epoch [43/120    avg_loss:0.348, val_acc:0.933]
Epoch [44/120    avg_loss:0.340, val_acc:0.948]
Epoch [45/120    avg_loss:0.345, val_acc:0.938]
Epoch [46/120    avg_loss:0.340, val_acc:0.938]
Epoch [47/120    avg_loss:0.334, val_acc:0.921]
Epoch [48/120    avg_loss:0.347, val_acc:0.929]
Epoch [49/120    avg_loss:0.320, val_acc:0.950]
Epoch [50/120    avg_loss:0.291, val_acc:0.944]
Epoch [51/120    avg_loss:0.294, val_acc:0.942]
Epoch [52/120    avg_loss:0.290, val_acc:0.929]
Epoch [53/120    avg_loss:0.320, val_acc:0.927]
Epoch [54/120    avg_loss:0.296, val_acc:0.869]
Epoch [55/120    avg_loss:0.337, val_acc:0.915]
Epoch [56/120    avg_loss:0.292, val_acc:0.954]
Epoch [57/120    avg_loss:0.261, val_acc:0.912]
Epoch [58/120    avg_loss:0.261, val_acc:0.952]
Epoch [59/120    avg_loss:0.249, val_acc:0.944]
Epoch [60/120    avg_loss:0.226, val_acc:0.933]
Epoch [61/120    avg_loss:0.224, val_acc:0.942]
Epoch [62/120    avg_loss:0.218, val_acc:0.952]
Epoch [63/120    avg_loss:0.241, val_acc:0.921]
Epoch [64/120    avg_loss:0.269, val_acc:0.940]
Epoch [65/120    avg_loss:0.324, val_acc:0.892]
Epoch [66/120    avg_loss:0.330, val_acc:0.942]
Epoch [67/120    avg_loss:0.272, val_acc:0.958]
Epoch [68/120    avg_loss:0.220, val_acc:0.952]
Epoch [69/120    avg_loss:0.217, val_acc:0.963]
Epoch [70/120    avg_loss:0.205, val_acc:0.954]
Epoch [71/120    avg_loss:0.201, val_acc:0.958]
Epoch [72/120    avg_loss:0.168, val_acc:0.948]
Epoch [73/120    avg_loss:0.195, val_acc:0.929]
Epoch [74/120    avg_loss:0.203, val_acc:0.940]
Epoch [75/120    avg_loss:0.199, val_acc:0.948]
Epoch [76/120    avg_loss:0.166, val_acc:0.933]
Epoch [77/120    avg_loss:0.165, val_acc:0.938]
Epoch [78/120    avg_loss:0.202, val_acc:0.960]
Epoch [79/120    avg_loss:0.158, val_acc:0.925]
Epoch [80/120    avg_loss:0.170, val_acc:0.965]
Epoch [81/120    avg_loss:0.168, val_acc:0.963]
Epoch [82/120    avg_loss:0.163, val_acc:0.923]
Epoch [83/120    avg_loss:0.156, val_acc:0.952]
Epoch [84/120    avg_loss:0.121, val_acc:0.965]
Epoch [85/120    avg_loss:0.136, val_acc:0.938]
Epoch [86/120    avg_loss:0.159, val_acc:0.940]
Epoch [87/120    avg_loss:0.174, val_acc:0.965]
Epoch [88/120    avg_loss:0.167, val_acc:0.973]
Epoch [89/120    avg_loss:0.193, val_acc:0.952]
Epoch [90/120    avg_loss:0.169, val_acc:0.958]
Epoch [91/120    avg_loss:0.151, val_acc:0.969]
Epoch [92/120    avg_loss:0.120, val_acc:0.956]
Epoch [93/120    avg_loss:0.116, val_acc:0.967]
Epoch [94/120    avg_loss:0.118, val_acc:0.948]
Epoch [95/120    avg_loss:0.135, val_acc:0.969]
Epoch [96/120    avg_loss:0.132, val_acc:0.965]
Epoch [97/120    avg_loss:0.143, val_acc:0.965]
Epoch [98/120    avg_loss:0.153, val_acc:0.944]
Epoch [99/120    avg_loss:0.168, val_acc:0.969]
Epoch [100/120    avg_loss:0.119, val_acc:0.965]
Epoch [101/120    avg_loss:0.130, val_acc:0.967]
Epoch [102/120    avg_loss:0.124, val_acc:0.971]
Epoch [103/120    avg_loss:0.084, val_acc:0.969]
Epoch [104/120    avg_loss:0.102, val_acc:0.975]
Epoch [105/120    avg_loss:0.080, val_acc:0.973]
Epoch [106/120    avg_loss:0.087, val_acc:0.973]
Epoch [107/120    avg_loss:0.081, val_acc:0.969]
Epoch [108/120    avg_loss:0.061, val_acc:0.971]
Epoch [109/120    avg_loss:0.067, val_acc:0.971]
Epoch [110/120    avg_loss:0.077, val_acc:0.971]
Epoch [111/120    avg_loss:0.066, val_acc:0.971]
Epoch [112/120    avg_loss:0.069, val_acc:0.971]
Epoch [113/120    avg_loss:0.069, val_acc:0.971]
Epoch [114/120    avg_loss:0.083, val_acc:0.973]
Epoch [115/120    avg_loss:0.071, val_acc:0.971]
Epoch [116/120    avg_loss:0.058, val_acc:0.971]
Epoch [117/120    avg_loss:0.058, val_acc:0.971]
Epoch [118/120    avg_loss:0.060, val_acc:0.971]
Epoch [119/120    avg_loss:0.060, val_acc:0.971]
Epoch [120/120    avg_loss:0.062, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 224   4   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.97117517 0.98678414 0.93243243 0.91803279
 0.99756691 0.92655367 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895551462061888
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ea7920b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.204]
Epoch [2/120    avg_loss:2.517, val_acc:0.333]
Epoch [3/120    avg_loss:2.424, val_acc:0.344]
Epoch [4/120    avg_loss:2.352, val_acc:0.388]
Epoch [5/120    avg_loss:2.287, val_acc:0.425]
Epoch [6/120    avg_loss:2.229, val_acc:0.481]
Epoch [7/120    avg_loss:2.172, val_acc:0.548]
Epoch [8/120    avg_loss:2.092, val_acc:0.569]
Epoch [9/120    avg_loss:2.041, val_acc:0.598]
Epoch [10/120    avg_loss:1.963, val_acc:0.623]
Epoch [11/120    avg_loss:1.882, val_acc:0.615]
Epoch [12/120    avg_loss:1.812, val_acc:0.604]
Epoch [13/120    avg_loss:1.729, val_acc:0.640]
Epoch [14/120    avg_loss:1.683, val_acc:0.656]
Epoch [15/120    avg_loss:1.571, val_acc:0.671]
Epoch [16/120    avg_loss:1.511, val_acc:0.675]
Epoch [17/120    avg_loss:1.443, val_acc:0.688]
Epoch [18/120    avg_loss:1.336, val_acc:0.702]
Epoch [19/120    avg_loss:1.271, val_acc:0.704]
Epoch [20/120    avg_loss:1.225, val_acc:0.715]
Epoch [21/120    avg_loss:1.156, val_acc:0.725]
Epoch [22/120    avg_loss:1.164, val_acc:0.725]
Epoch [23/120    avg_loss:1.025, val_acc:0.762]
Epoch [24/120    avg_loss:0.999, val_acc:0.725]
Epoch [25/120    avg_loss:0.903, val_acc:0.767]
Epoch [26/120    avg_loss:0.867, val_acc:0.787]
Epoch [27/120    avg_loss:0.844, val_acc:0.771]
Epoch [28/120    avg_loss:0.774, val_acc:0.790]
Epoch [29/120    avg_loss:0.703, val_acc:0.798]
Epoch [30/120    avg_loss:0.678, val_acc:0.796]
Epoch [31/120    avg_loss:0.623, val_acc:0.810]
Epoch [32/120    avg_loss:0.678, val_acc:0.810]
Epoch [33/120    avg_loss:0.599, val_acc:0.823]
Epoch [34/120    avg_loss:0.564, val_acc:0.871]
Epoch [35/120    avg_loss:0.579, val_acc:0.846]
Epoch [36/120    avg_loss:0.564, val_acc:0.942]
Epoch [37/120    avg_loss:0.476, val_acc:0.848]
Epoch [38/120    avg_loss:0.551, val_acc:0.854]
Epoch [39/120    avg_loss:0.581, val_acc:0.919]
Epoch [40/120    avg_loss:0.518, val_acc:0.871]
Epoch [41/120    avg_loss:0.420, val_acc:0.950]
Epoch [42/120    avg_loss:0.448, val_acc:0.950]
Epoch [43/120    avg_loss:0.439, val_acc:0.942]
Epoch [44/120    avg_loss:0.421, val_acc:0.925]
Epoch [45/120    avg_loss:0.409, val_acc:0.944]
Epoch [46/120    avg_loss:0.385, val_acc:0.942]
Epoch [47/120    avg_loss:0.378, val_acc:0.950]
Epoch [48/120    avg_loss:0.383, val_acc:0.958]
Epoch [49/120    avg_loss:0.341, val_acc:0.958]
Epoch [50/120    avg_loss:0.307, val_acc:0.975]
Epoch [51/120    avg_loss:0.312, val_acc:0.963]
Epoch [52/120    avg_loss:0.332, val_acc:0.960]
Epoch [53/120    avg_loss:0.298, val_acc:0.977]
Epoch [54/120    avg_loss:0.278, val_acc:0.958]
Epoch [55/120    avg_loss:0.280, val_acc:0.971]
Epoch [56/120    avg_loss:0.253, val_acc:0.988]
Epoch [57/120    avg_loss:0.229, val_acc:0.963]
Epoch [58/120    avg_loss:0.259, val_acc:0.919]
Epoch [59/120    avg_loss:0.291, val_acc:0.985]
Epoch [60/120    avg_loss:0.225, val_acc:0.956]
Epoch [61/120    avg_loss:0.244, val_acc:0.977]
Epoch [62/120    avg_loss:0.198, val_acc:0.950]
Epoch [63/120    avg_loss:0.259, val_acc:0.979]
Epoch [64/120    avg_loss:0.209, val_acc:0.983]
Epoch [65/120    avg_loss:0.201, val_acc:0.975]
Epoch [66/120    avg_loss:0.211, val_acc:0.973]
Epoch [67/120    avg_loss:0.162, val_acc:0.988]
Epoch [68/120    avg_loss:0.168, val_acc:0.983]
Epoch [69/120    avg_loss:0.155, val_acc:0.985]
Epoch [70/120    avg_loss:0.178, val_acc:0.985]
Epoch [71/120    avg_loss:0.160, val_acc:0.988]
Epoch [72/120    avg_loss:0.154, val_acc:0.988]
Epoch [73/120    avg_loss:0.165, val_acc:0.985]
Epoch [74/120    avg_loss:0.164, val_acc:0.975]
Epoch [75/120    avg_loss:0.169, val_acc:0.983]
Epoch [76/120    avg_loss:0.144, val_acc:0.985]
Epoch [77/120    avg_loss:0.122, val_acc:0.985]
Epoch [78/120    avg_loss:0.136, val_acc:0.979]
Epoch [79/120    avg_loss:0.143, val_acc:0.988]
Epoch [80/120    avg_loss:0.146, val_acc:0.985]
Epoch [81/120    avg_loss:0.145, val_acc:0.981]
Epoch [82/120    avg_loss:0.128, val_acc:0.967]
Epoch [83/120    avg_loss:0.166, val_acc:0.988]
Epoch [84/120    avg_loss:0.152, val_acc:0.981]
Epoch [85/120    avg_loss:0.131, val_acc:0.975]
Epoch [86/120    avg_loss:0.136, val_acc:0.965]
Epoch [87/120    avg_loss:0.098, val_acc:0.992]
Epoch [88/120    avg_loss:0.108, val_acc:0.981]
Epoch [89/120    avg_loss:0.124, val_acc:0.983]
Epoch [90/120    avg_loss:0.102, val_acc:0.996]
Epoch [91/120    avg_loss:0.089, val_acc:0.990]
Epoch [92/120    avg_loss:0.104, val_acc:0.981]
Epoch [93/120    avg_loss:0.102, val_acc:0.988]
Epoch [94/120    avg_loss:0.088, val_acc:0.990]
Epoch [95/120    avg_loss:0.075, val_acc:0.994]
Epoch [96/120    avg_loss:0.094, val_acc:0.990]
Epoch [97/120    avg_loss:0.095, val_acc:0.988]
Epoch [98/120    avg_loss:0.087, val_acc:0.985]
Epoch [99/120    avg_loss:0.119, val_acc:0.981]
Epoch [100/120    avg_loss:0.097, val_acc:0.992]
Epoch [101/120    avg_loss:0.080, val_acc:0.983]
Epoch [102/120    avg_loss:0.073, val_acc:0.985]
Epoch [103/120    avg_loss:0.077, val_acc:0.988]
Epoch [104/120    avg_loss:0.076, val_acc:0.992]
Epoch [105/120    avg_loss:0.064, val_acc:0.992]
Epoch [106/120    avg_loss:0.055, val_acc:0.994]
Epoch [107/120    avg_loss:0.047, val_acc:0.994]
Epoch [108/120    avg_loss:0.061, val_acc:0.994]
Epoch [109/120    avg_loss:0.055, val_acc:0.994]
Epoch [110/120    avg_loss:0.050, val_acc:0.994]
Epoch [111/120    avg_loss:0.048, val_acc:0.994]
Epoch [112/120    avg_loss:0.044, val_acc:0.994]
Epoch [113/120    avg_loss:0.044, val_acc:0.994]
Epoch [114/120    avg_loss:0.045, val_acc:0.996]
Epoch [115/120    avg_loss:0.042, val_acc:0.996]
Epoch [116/120    avg_loss:0.045, val_acc:0.996]
Epoch [117/120    avg_loss:0.047, val_acc:0.996]
Epoch [118/120    avg_loss:0.041, val_acc:0.996]
Epoch [119/120    avg_loss:0.042, val_acc:0.996]
Epoch [120/120    avg_loss:0.039, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   5   0   0   0   0   0   0   1   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99926954 0.99319728 1.         0.95258621 0.92473118
 0.99757869 0.98378378 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9938277622737888
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7be746b6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.648, val_acc:0.129]
Epoch [2/120    avg_loss:2.560, val_acc:0.398]
Epoch [3/120    avg_loss:2.467, val_acc:0.383]
Epoch [4/120    avg_loss:2.387, val_acc:0.375]
Epoch [5/120    avg_loss:2.316, val_acc:0.396]
Epoch [6/120    avg_loss:2.250, val_acc:0.404]
Epoch [7/120    avg_loss:2.183, val_acc:0.442]
Epoch [8/120    avg_loss:2.108, val_acc:0.490]
Epoch [9/120    avg_loss:2.035, val_acc:0.540]
Epoch [10/120    avg_loss:1.976, val_acc:0.579]
Epoch [11/120    avg_loss:1.938, val_acc:0.606]
Epoch [12/120    avg_loss:1.858, val_acc:0.652]
Epoch [13/120    avg_loss:1.807, val_acc:0.640]
Epoch [14/120    avg_loss:1.745, val_acc:0.642]
Epoch [15/120    avg_loss:1.674, val_acc:0.648]
Epoch [16/120    avg_loss:1.618, val_acc:0.677]
Epoch [17/120    avg_loss:1.558, val_acc:0.696]
Epoch [18/120    avg_loss:1.512, val_acc:0.706]
Epoch [19/120    avg_loss:1.397, val_acc:0.715]
Epoch [20/120    avg_loss:1.332, val_acc:0.738]
Epoch [21/120    avg_loss:1.291, val_acc:0.723]
Epoch [22/120    avg_loss:1.225, val_acc:0.742]
Epoch [23/120    avg_loss:1.132, val_acc:0.731]
Epoch [24/120    avg_loss:1.074, val_acc:0.754]
Epoch [25/120    avg_loss:1.006, val_acc:0.765]
Epoch [26/120    avg_loss:0.955, val_acc:0.754]
Epoch [27/120    avg_loss:0.886, val_acc:0.787]
Epoch [28/120    avg_loss:0.838, val_acc:0.775]
Epoch [29/120    avg_loss:0.853, val_acc:0.829]
Epoch [30/120    avg_loss:0.790, val_acc:0.848]
Epoch [31/120    avg_loss:0.730, val_acc:0.860]
Epoch [32/120    avg_loss:0.722, val_acc:0.852]
Epoch [33/120    avg_loss:0.685, val_acc:0.938]
Epoch [34/120    avg_loss:0.647, val_acc:0.915]
Epoch [35/120    avg_loss:0.609, val_acc:0.935]
Epoch [36/120    avg_loss:0.544, val_acc:0.887]
Epoch [37/120    avg_loss:0.552, val_acc:0.912]
Epoch [38/120    avg_loss:0.600, val_acc:0.827]
Epoch [39/120    avg_loss:0.568, val_acc:0.933]
Epoch [40/120    avg_loss:0.478, val_acc:0.938]
Epoch [41/120    avg_loss:0.447, val_acc:0.938]
Epoch [42/120    avg_loss:0.455, val_acc:0.923]
Epoch [43/120    avg_loss:0.448, val_acc:0.942]
Epoch [44/120    avg_loss:0.416, val_acc:0.944]
Epoch [45/120    avg_loss:0.422, val_acc:0.919]
Epoch [46/120    avg_loss:0.400, val_acc:0.938]
Epoch [47/120    avg_loss:0.381, val_acc:0.938]
Epoch [48/120    avg_loss:0.368, val_acc:0.935]
Epoch [49/120    avg_loss:0.420, val_acc:0.956]
Epoch [50/120    avg_loss:0.349, val_acc:0.946]
Epoch [51/120    avg_loss:0.380, val_acc:0.917]
Epoch [52/120    avg_loss:0.369, val_acc:0.927]
Epoch [53/120    avg_loss:0.331, val_acc:0.940]
Epoch [54/120    avg_loss:0.318, val_acc:0.944]
Epoch [55/120    avg_loss:0.318, val_acc:0.948]
Epoch [56/120    avg_loss:0.330, val_acc:0.942]
Epoch [57/120    avg_loss:0.314, val_acc:0.938]
Epoch [58/120    avg_loss:0.280, val_acc:0.963]
Epoch [59/120    avg_loss:0.261, val_acc:0.946]
Epoch [60/120    avg_loss:0.272, val_acc:0.963]
Epoch [61/120    avg_loss:0.285, val_acc:0.942]
Epoch [62/120    avg_loss:0.264, val_acc:0.950]
Epoch [63/120    avg_loss:0.263, val_acc:0.965]
Epoch [64/120    avg_loss:0.272, val_acc:0.944]
Epoch [65/120    avg_loss:0.236, val_acc:0.965]
Epoch [66/120    avg_loss:0.241, val_acc:0.958]
Epoch [67/120    avg_loss:0.247, val_acc:0.956]
Epoch [68/120    avg_loss:0.209, val_acc:0.967]
Epoch [69/120    avg_loss:0.186, val_acc:0.960]
Epoch [70/120    avg_loss:0.203, val_acc:0.954]
Epoch [71/120    avg_loss:0.187, val_acc:0.956]
Epoch [72/120    avg_loss:0.182, val_acc:0.952]
Epoch [73/120    avg_loss:0.181, val_acc:0.950]
Epoch [74/120    avg_loss:0.226, val_acc:0.963]
Epoch [75/120    avg_loss:0.196, val_acc:0.890]
Epoch [76/120    avg_loss:0.203, val_acc:0.967]
Epoch [77/120    avg_loss:0.330, val_acc:0.933]
Epoch [78/120    avg_loss:0.223, val_acc:0.960]
Epoch [79/120    avg_loss:0.193, val_acc:0.952]
Epoch [80/120    avg_loss:0.186, val_acc:0.942]
Epoch [81/120    avg_loss:0.172, val_acc:0.977]
Epoch [82/120    avg_loss:0.122, val_acc:0.975]
Epoch [83/120    avg_loss:0.138, val_acc:0.979]
Epoch [84/120    avg_loss:0.136, val_acc:0.981]
Epoch [85/120    avg_loss:0.123, val_acc:0.985]
Epoch [86/120    avg_loss:0.119, val_acc:0.977]
Epoch [87/120    avg_loss:0.124, val_acc:0.967]
Epoch [88/120    avg_loss:0.112, val_acc:0.967]
Epoch [89/120    avg_loss:0.125, val_acc:0.975]
Epoch [90/120    avg_loss:0.150, val_acc:0.963]
Epoch [91/120    avg_loss:0.142, val_acc:0.973]
Epoch [92/120    avg_loss:0.138, val_acc:0.965]
Epoch [93/120    avg_loss:0.127, val_acc:0.977]
Epoch [94/120    avg_loss:0.111, val_acc:0.983]
Epoch [95/120    avg_loss:0.115, val_acc:0.975]
Epoch [96/120    avg_loss:0.137, val_acc:0.950]
Epoch [97/120    avg_loss:0.123, val_acc:0.979]
Epoch [98/120    avg_loss:0.086, val_acc:0.983]
Epoch [99/120    avg_loss:0.093, val_acc:0.977]
Epoch [100/120    avg_loss:0.079, val_acc:0.977]
Epoch [101/120    avg_loss:0.078, val_acc:0.977]
Epoch [102/120    avg_loss:0.073, val_acc:0.983]
Epoch [103/120    avg_loss:0.063, val_acc:0.979]
Epoch [104/120    avg_loss:0.073, val_acc:0.981]
Epoch [105/120    avg_loss:0.065, val_acc:0.979]
Epoch [106/120    avg_loss:0.066, val_acc:0.981]
Epoch [107/120    avg_loss:0.067, val_acc:0.981]
Epoch [108/120    avg_loss:0.066, val_acc:0.981]
Epoch [109/120    avg_loss:0.065, val_acc:0.977]
Epoch [110/120    avg_loss:0.062, val_acc:0.981]
Epoch [111/120    avg_loss:0.069, val_acc:0.981]
Epoch [112/120    avg_loss:0.059, val_acc:0.981]
Epoch [113/120    avg_loss:0.057, val_acc:0.979]
Epoch [114/120    avg_loss:0.082, val_acc:0.979]
Epoch [115/120    avg_loss:0.058, val_acc:0.979]
Epoch [116/120    avg_loss:0.059, val_acc:0.979]
Epoch [117/120    avg_loss:0.057, val_acc:0.979]
Epoch [118/120    avg_loss:0.068, val_acc:0.979]
Epoch [119/120    avg_loss:0.059, val_acc:0.979]
Epoch [120/120    avg_loss:0.074, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 0.99707174 0.98206278 1.         0.92608696 0.88028169
 0.99038462 0.95555556 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9890806874135968
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b74c2eb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.109]
Epoch [2/120    avg_loss:2.568, val_acc:0.344]
Epoch [3/120    avg_loss:2.490, val_acc:0.338]
Epoch [4/120    avg_loss:2.414, val_acc:0.340]
Epoch [5/120    avg_loss:2.363, val_acc:0.367]
Epoch [6/120    avg_loss:2.306, val_acc:0.381]
Epoch [7/120    avg_loss:2.254, val_acc:0.410]
Epoch [8/120    avg_loss:2.197, val_acc:0.419]
Epoch [9/120    avg_loss:2.150, val_acc:0.429]
Epoch [10/120    avg_loss:2.083, val_acc:0.458]
Epoch [11/120    avg_loss:2.008, val_acc:0.517]
Epoch [12/120    avg_loss:1.928, val_acc:0.525]
Epoch [13/120    avg_loss:1.850, val_acc:0.542]
Epoch [14/120    avg_loss:1.800, val_acc:0.560]
Epoch [15/120    avg_loss:1.714, val_acc:0.581]
Epoch [16/120    avg_loss:1.642, val_acc:0.621]
Epoch [17/120    avg_loss:1.575, val_acc:0.656]
Epoch [18/120    avg_loss:1.533, val_acc:0.613]
Epoch [19/120    avg_loss:1.458, val_acc:0.656]
Epoch [20/120    avg_loss:1.371, val_acc:0.679]
Epoch [21/120    avg_loss:1.321, val_acc:0.679]
Epoch [22/120    avg_loss:1.229, val_acc:0.706]
Epoch [23/120    avg_loss:1.185, val_acc:0.713]
Epoch [24/120    avg_loss:1.126, val_acc:0.727]
Epoch [25/120    avg_loss:1.079, val_acc:0.721]
Epoch [26/120    avg_loss:0.995, val_acc:0.765]
Epoch [27/120    avg_loss:0.955, val_acc:0.804]
Epoch [28/120    avg_loss:0.888, val_acc:0.802]
Epoch [29/120    avg_loss:0.834, val_acc:0.823]
Epoch [30/120    avg_loss:0.806, val_acc:0.819]
Epoch [31/120    avg_loss:0.731, val_acc:0.831]
Epoch [32/120    avg_loss:0.726, val_acc:0.873]
Epoch [33/120    avg_loss:0.637, val_acc:0.887]
Epoch [34/120    avg_loss:0.634, val_acc:0.912]
Epoch [35/120    avg_loss:0.553, val_acc:0.890]
Epoch [36/120    avg_loss:0.602, val_acc:0.869]
Epoch [37/120    avg_loss:0.618, val_acc:0.883]
Epoch [38/120    avg_loss:0.596, val_acc:0.919]
Epoch [39/120    avg_loss:0.566, val_acc:0.883]
Epoch [40/120    avg_loss:0.476, val_acc:0.908]
Epoch [41/120    avg_loss:0.441, val_acc:0.940]
Epoch [42/120    avg_loss:0.452, val_acc:0.923]
Epoch [43/120    avg_loss:0.455, val_acc:0.917]
Epoch [44/120    avg_loss:0.391, val_acc:0.933]
Epoch [45/120    avg_loss:0.418, val_acc:0.935]
Epoch [46/120    avg_loss:0.391, val_acc:0.956]
Epoch [47/120    avg_loss:0.385, val_acc:0.954]
Epoch [48/120    avg_loss:0.353, val_acc:0.944]
Epoch [49/120    avg_loss:0.367, val_acc:0.931]
Epoch [50/120    avg_loss:0.355, val_acc:0.942]
Epoch [51/120    avg_loss:0.362, val_acc:0.942]
Epoch [52/120    avg_loss:0.337, val_acc:0.950]
Epoch [53/120    avg_loss:0.299, val_acc:0.952]
Epoch [54/120    avg_loss:0.283, val_acc:0.958]
Epoch [55/120    avg_loss:0.290, val_acc:0.956]
Epoch [56/120    avg_loss:0.274, val_acc:0.948]
Epoch [57/120    avg_loss:0.305, val_acc:0.940]
Epoch [58/120    avg_loss:0.262, val_acc:0.963]
Epoch [59/120    avg_loss:0.229, val_acc:0.952]
Epoch [60/120    avg_loss:0.259, val_acc:0.952]
Epoch [61/120    avg_loss:0.262, val_acc:0.938]
Epoch [62/120    avg_loss:0.276, val_acc:0.908]
Epoch [63/120    avg_loss:0.289, val_acc:0.950]
Epoch [64/120    avg_loss:0.237, val_acc:0.963]
Epoch [65/120    avg_loss:0.214, val_acc:0.965]
Epoch [66/120    avg_loss:0.238, val_acc:0.954]
Epoch [67/120    avg_loss:0.224, val_acc:0.967]
Epoch [68/120    avg_loss:0.182, val_acc:0.969]
Epoch [69/120    avg_loss:0.211, val_acc:0.908]
Epoch [70/120    avg_loss:0.186, val_acc:0.952]
Epoch [71/120    avg_loss:0.209, val_acc:0.950]
Epoch [72/120    avg_loss:0.241, val_acc:0.963]
Epoch [73/120    avg_loss:0.184, val_acc:0.960]
Epoch [74/120    avg_loss:0.177, val_acc:0.977]
Epoch [75/120    avg_loss:0.225, val_acc:0.919]
Epoch [76/120    avg_loss:0.201, val_acc:0.898]
Epoch [77/120    avg_loss:0.259, val_acc:0.946]
Epoch [78/120    avg_loss:0.163, val_acc:0.969]
Epoch [79/120    avg_loss:0.180, val_acc:0.960]
Epoch [80/120    avg_loss:0.171, val_acc:0.979]
Epoch [81/120    avg_loss:0.134, val_acc:0.967]
Epoch [82/120    avg_loss:0.182, val_acc:0.965]
Epoch [83/120    avg_loss:0.141, val_acc:0.990]
Epoch [84/120    avg_loss:0.138, val_acc:0.977]
Epoch [85/120    avg_loss:0.135, val_acc:0.960]
Epoch [86/120    avg_loss:0.153, val_acc:0.969]
Epoch [87/120    avg_loss:0.124, val_acc:0.983]
Epoch [88/120    avg_loss:0.116, val_acc:0.981]
Epoch [89/120    avg_loss:0.104, val_acc:0.981]
Epoch [90/120    avg_loss:0.085, val_acc:0.992]
Epoch [91/120    avg_loss:0.086, val_acc:0.981]
Epoch [92/120    avg_loss:0.092, val_acc:0.975]
Epoch [93/120    avg_loss:0.092, val_acc:0.988]
Epoch [94/120    avg_loss:0.092, val_acc:0.971]
Epoch [95/120    avg_loss:0.079, val_acc:0.975]
Epoch [96/120    avg_loss:0.073, val_acc:0.981]
Epoch [97/120    avg_loss:0.079, val_acc:0.983]
Epoch [98/120    avg_loss:0.125, val_acc:0.958]
Epoch [99/120    avg_loss:0.128, val_acc:0.988]
Epoch [100/120    avg_loss:0.123, val_acc:0.975]
Epoch [101/120    avg_loss:0.124, val_acc:0.983]
Epoch [102/120    avg_loss:0.085, val_acc:0.983]
Epoch [103/120    avg_loss:0.067, val_acc:0.990]
Epoch [104/120    avg_loss:0.054, val_acc:0.990]
Epoch [105/120    avg_loss:0.045, val_acc:0.992]
Epoch [106/120    avg_loss:0.052, val_acc:0.992]
Epoch [107/120    avg_loss:0.052, val_acc:0.988]
Epoch [108/120    avg_loss:0.047, val_acc:0.988]
Epoch [109/120    avg_loss:0.046, val_acc:0.988]
Epoch [110/120    avg_loss:0.062, val_acc:0.990]
Epoch [111/120    avg_loss:0.046, val_acc:0.988]
Epoch [112/120    avg_loss:0.042, val_acc:0.988]
Epoch [113/120    avg_loss:0.042, val_acc:0.992]
Epoch [114/120    avg_loss:0.041, val_acc:0.992]
Epoch [115/120    avg_loss:0.044, val_acc:0.988]
Epoch [116/120    avg_loss:0.040, val_acc:0.990]
Epoch [117/120    avg_loss:0.053, val_acc:0.992]
Epoch [118/120    avg_loss:0.045, val_acc:0.992]
Epoch [119/120    avg_loss:0.048, val_acc:0.994]
Epoch [120/120    avg_loss:0.041, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99780541 0.98206278 0.99782135 0.96613995 0.95364238
 0.99033816 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9938285619658637
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d40c87b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.173]
Epoch [2/120    avg_loss:2.531, val_acc:0.279]
Epoch [3/120    avg_loss:2.448, val_acc:0.431]
Epoch [4/120    avg_loss:2.371, val_acc:0.542]
Epoch [5/120    avg_loss:2.306, val_acc:0.550]
Epoch [6/120    avg_loss:2.249, val_acc:0.508]
Epoch [7/120    avg_loss:2.169, val_acc:0.498]
Epoch [8/120    avg_loss:2.107, val_acc:0.498]
Epoch [9/120    avg_loss:2.038, val_acc:0.481]
Epoch [10/120    avg_loss:1.972, val_acc:0.521]
Epoch [11/120    avg_loss:1.900, val_acc:0.512]
Epoch [12/120    avg_loss:1.849, val_acc:0.537]
Epoch [13/120    avg_loss:1.759, val_acc:0.550]
Epoch [14/120    avg_loss:1.695, val_acc:0.688]
Epoch [15/120    avg_loss:1.661, val_acc:0.677]
Epoch [16/120    avg_loss:1.563, val_acc:0.675]
Epoch [17/120    avg_loss:1.508, val_acc:0.704]
Epoch [18/120    avg_loss:1.402, val_acc:0.767]
Epoch [19/120    avg_loss:1.316, val_acc:0.756]
Epoch [20/120    avg_loss:1.229, val_acc:0.738]
Epoch [21/120    avg_loss:1.163, val_acc:0.808]
Epoch [22/120    avg_loss:1.113, val_acc:0.804]
Epoch [23/120    avg_loss:1.026, val_acc:0.848]
Epoch [24/120    avg_loss:0.949, val_acc:0.815]
Epoch [25/120    avg_loss:0.942, val_acc:0.875]
Epoch [26/120    avg_loss:0.836, val_acc:0.883]
Epoch [27/120    avg_loss:0.760, val_acc:0.885]
Epoch [28/120    avg_loss:0.720, val_acc:0.854]
Epoch [29/120    avg_loss:0.715, val_acc:0.908]
Epoch [30/120    avg_loss:0.691, val_acc:0.896]
Epoch [31/120    avg_loss:0.650, val_acc:0.881]
Epoch [32/120    avg_loss:0.577, val_acc:0.908]
Epoch [33/120    avg_loss:0.528, val_acc:0.912]
Epoch [34/120    avg_loss:0.517, val_acc:0.898]
Epoch [35/120    avg_loss:0.525, val_acc:0.929]
Epoch [36/120    avg_loss:0.512, val_acc:0.904]
Epoch [37/120    avg_loss:0.461, val_acc:0.896]
Epoch [38/120    avg_loss:0.472, val_acc:0.933]
Epoch [39/120    avg_loss:0.428, val_acc:0.904]
Epoch [40/120    avg_loss:0.376, val_acc:0.940]
Epoch [41/120    avg_loss:0.364, val_acc:0.946]
Epoch [42/120    avg_loss:0.388, val_acc:0.938]
Epoch [43/120    avg_loss:0.358, val_acc:0.935]
Epoch [44/120    avg_loss:0.340, val_acc:0.925]
Epoch [45/120    avg_loss:0.322, val_acc:0.938]
Epoch [46/120    avg_loss:0.345, val_acc:0.935]
Epoch [47/120    avg_loss:0.384, val_acc:0.940]
Epoch [48/120    avg_loss:0.323, val_acc:0.940]
Epoch [49/120    avg_loss:0.328, val_acc:0.938]
Epoch [50/120    avg_loss:0.369, val_acc:0.946]
Epoch [51/120    avg_loss:0.312, val_acc:0.960]
Epoch [52/120    avg_loss:0.258, val_acc:0.952]
Epoch [53/120    avg_loss:0.265, val_acc:0.942]
Epoch [54/120    avg_loss:0.327, val_acc:0.931]
Epoch [55/120    avg_loss:0.318, val_acc:0.956]
Epoch [56/120    avg_loss:0.267, val_acc:0.946]
Epoch [57/120    avg_loss:0.256, val_acc:0.942]
Epoch [58/120    avg_loss:0.251, val_acc:0.963]
Epoch [59/120    avg_loss:0.180, val_acc:0.960]
Epoch [60/120    avg_loss:0.259, val_acc:0.948]
Epoch [61/120    avg_loss:0.220, val_acc:0.954]
Epoch [62/120    avg_loss:0.202, val_acc:0.965]
Epoch [63/120    avg_loss:0.185, val_acc:0.963]
Epoch [64/120    avg_loss:0.174, val_acc:0.958]
Epoch [65/120    avg_loss:0.170, val_acc:0.969]
Epoch [66/120    avg_loss:0.310, val_acc:0.912]
Epoch [67/120    avg_loss:0.294, val_acc:0.917]
Epoch [68/120    avg_loss:0.236, val_acc:0.969]
Epoch [69/120    avg_loss:0.164, val_acc:0.965]
Epoch [70/120    avg_loss:0.187, val_acc:0.963]
Epoch [71/120    avg_loss:0.172, val_acc:0.958]
Epoch [72/120    avg_loss:0.213, val_acc:0.963]
Epoch [73/120    avg_loss:0.183, val_acc:0.960]
Epoch [74/120    avg_loss:0.244, val_acc:0.952]
Epoch [75/120    avg_loss:0.238, val_acc:0.952]
Epoch [76/120    avg_loss:0.239, val_acc:0.960]
Epoch [77/120    avg_loss:0.136, val_acc:0.965]
Epoch [78/120    avg_loss:0.120, val_acc:0.979]
Epoch [79/120    avg_loss:0.116, val_acc:0.973]
Epoch [80/120    avg_loss:0.111, val_acc:0.979]
Epoch [81/120    avg_loss:0.164, val_acc:0.969]
Epoch [82/120    avg_loss:0.142, val_acc:0.971]
Epoch [83/120    avg_loss:0.138, val_acc:0.956]
Epoch [84/120    avg_loss:0.132, val_acc:0.971]
Epoch [85/120    avg_loss:0.123, val_acc:0.975]
Epoch [86/120    avg_loss:0.117, val_acc:0.973]
Epoch [87/120    avg_loss:0.104, val_acc:0.969]
Epoch [88/120    avg_loss:0.118, val_acc:0.977]
Epoch [89/120    avg_loss:0.145, val_acc:0.977]
Epoch [90/120    avg_loss:0.111, val_acc:0.977]
Epoch [91/120    avg_loss:0.101, val_acc:0.975]
Epoch [92/120    avg_loss:0.109, val_acc:0.969]
Epoch [93/120    avg_loss:0.073, val_acc:0.979]
Epoch [94/120    avg_loss:0.061, val_acc:0.973]
Epoch [95/120    avg_loss:0.086, val_acc:0.983]
Epoch [96/120    avg_loss:0.084, val_acc:0.975]
Epoch [97/120    avg_loss:0.092, val_acc:0.979]
Epoch [98/120    avg_loss:0.059, val_acc:0.988]
Epoch [99/120    avg_loss:0.076, val_acc:0.981]
Epoch [100/120    avg_loss:0.120, val_acc:0.971]
Epoch [101/120    avg_loss:0.101, val_acc:0.981]
Epoch [102/120    avg_loss:0.102, val_acc:0.977]
Epoch [103/120    avg_loss:0.161, val_acc:0.904]
Epoch [104/120    avg_loss:0.177, val_acc:0.963]
Epoch [105/120    avg_loss:0.090, val_acc:0.983]
Epoch [106/120    avg_loss:0.090, val_acc:0.931]
Epoch [107/120    avg_loss:0.109, val_acc:0.975]
Epoch [108/120    avg_loss:0.124, val_acc:0.971]
Epoch [109/120    avg_loss:0.104, val_acc:0.985]
Epoch [110/120    avg_loss:0.087, val_acc:0.969]
Epoch [111/120    avg_loss:0.082, val_acc:0.975]
Epoch [112/120    avg_loss:0.079, val_acc:0.975]
Epoch [113/120    avg_loss:0.072, val_acc:0.985]
Epoch [114/120    avg_loss:0.061, val_acc:0.985]
Epoch [115/120    avg_loss:0.056, val_acc:0.988]
Epoch [116/120    avg_loss:0.048, val_acc:0.988]
Epoch [117/120    avg_loss:0.044, val_acc:0.988]
Epoch [118/120    avg_loss:0.048, val_acc:0.988]
Epoch [119/120    avg_loss:0.047, val_acc:0.988]
Epoch [120/120    avg_loss:0.045, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.97550111 0.97777778 0.91517857 0.90849673
 1.         0.93785311 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9883682892871898
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f84848dda58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.583, val_acc:0.342]
Epoch [2/120    avg_loss:2.474, val_acc:0.458]
Epoch [3/120    avg_loss:2.378, val_acc:0.479]
Epoch [4/120    avg_loss:2.318, val_acc:0.492]
Epoch [5/120    avg_loss:2.257, val_acc:0.498]
Epoch [6/120    avg_loss:2.194, val_acc:0.498]
Epoch [7/120    avg_loss:2.143, val_acc:0.487]
Epoch [8/120    avg_loss:2.085, val_acc:0.517]
Epoch [9/120    avg_loss:2.022, val_acc:0.548]
Epoch [10/120    avg_loss:1.932, val_acc:0.540]
Epoch [11/120    avg_loss:1.882, val_acc:0.583]
Epoch [12/120    avg_loss:1.824, val_acc:0.598]
Epoch [13/120    avg_loss:1.762, val_acc:0.615]
Epoch [14/120    avg_loss:1.678, val_acc:0.642]
Epoch [15/120    avg_loss:1.602, val_acc:0.660]
Epoch [16/120    avg_loss:1.533, val_acc:0.677]
Epoch [17/120    avg_loss:1.477, val_acc:0.671]
Epoch [18/120    avg_loss:1.388, val_acc:0.706]
Epoch [19/120    avg_loss:1.323, val_acc:0.713]
Epoch [20/120    avg_loss:1.254, val_acc:0.723]
Epoch [21/120    avg_loss:1.176, val_acc:0.727]
Epoch [22/120    avg_loss:1.115, val_acc:0.758]
Epoch [23/120    avg_loss:1.043, val_acc:0.773]
Epoch [24/120    avg_loss:0.979, val_acc:0.794]
Epoch [25/120    avg_loss:0.922, val_acc:0.781]
Epoch [26/120    avg_loss:0.888, val_acc:0.794]
Epoch [27/120    avg_loss:0.807, val_acc:0.800]
Epoch [28/120    avg_loss:0.772, val_acc:0.796]
Epoch [29/120    avg_loss:0.699, val_acc:0.817]
Epoch [30/120    avg_loss:0.643, val_acc:0.775]
Epoch [31/120    avg_loss:0.641, val_acc:0.808]
Epoch [32/120    avg_loss:0.603, val_acc:0.821]
Epoch [33/120    avg_loss:0.583, val_acc:0.869]
Epoch [34/120    avg_loss:0.591, val_acc:0.879]
Epoch [35/120    avg_loss:0.512, val_acc:0.927]
Epoch [36/120    avg_loss:0.473, val_acc:0.915]
Epoch [37/120    avg_loss:0.440, val_acc:0.950]
Epoch [38/120    avg_loss:0.452, val_acc:0.933]
Epoch [39/120    avg_loss:0.447, val_acc:0.846]
Epoch [40/120    avg_loss:0.421, val_acc:0.933]
Epoch [41/120    avg_loss:0.417, val_acc:0.902]
Epoch [42/120    avg_loss:0.424, val_acc:0.875]
Epoch [43/120    avg_loss:0.411, val_acc:0.933]
Epoch [44/120    avg_loss:0.346, val_acc:0.890]
Epoch [45/120    avg_loss:0.311, val_acc:0.940]
Epoch [46/120    avg_loss:0.361, val_acc:0.933]
Epoch [47/120    avg_loss:0.307, val_acc:0.948]
Epoch [48/120    avg_loss:0.279, val_acc:0.960]
Epoch [49/120    avg_loss:0.296, val_acc:0.948]
Epoch [50/120    avg_loss:0.258, val_acc:0.929]
Epoch [51/120    avg_loss:0.284, val_acc:0.954]
Epoch [52/120    avg_loss:0.271, val_acc:0.956]
Epoch [53/120    avg_loss:0.256, val_acc:0.963]
Epoch [54/120    avg_loss:0.244, val_acc:0.950]
Epoch [55/120    avg_loss:0.292, val_acc:0.958]
Epoch [56/120    avg_loss:0.235, val_acc:0.948]
Epoch [57/120    avg_loss:0.263, val_acc:0.948]
Epoch [58/120    avg_loss:0.235, val_acc:0.954]
Epoch [59/120    avg_loss:0.215, val_acc:0.952]
Epoch [60/120    avg_loss:0.216, val_acc:0.971]
Epoch [61/120    avg_loss:0.237, val_acc:0.958]
Epoch [62/120    avg_loss:0.215, val_acc:0.969]
Epoch [63/120    avg_loss:0.255, val_acc:0.963]
Epoch [64/120    avg_loss:0.231, val_acc:0.931]
Epoch [65/120    avg_loss:0.245, val_acc:0.950]
Epoch [66/120    avg_loss:0.230, val_acc:0.958]
Epoch [67/120    avg_loss:0.221, val_acc:0.971]
Epoch [68/120    avg_loss:0.197, val_acc:0.956]
Epoch [69/120    avg_loss:0.174, val_acc:0.967]
Epoch [70/120    avg_loss:0.158, val_acc:0.969]
Epoch [71/120    avg_loss:0.190, val_acc:0.981]
Epoch [72/120    avg_loss:0.172, val_acc:0.963]
Epoch [73/120    avg_loss:0.182, val_acc:0.960]
Epoch [74/120    avg_loss:0.146, val_acc:0.973]
Epoch [75/120    avg_loss:0.159, val_acc:0.973]
Epoch [76/120    avg_loss:0.147, val_acc:0.983]
Epoch [77/120    avg_loss:0.130, val_acc:0.965]
Epoch [78/120    avg_loss:0.137, val_acc:0.983]
Epoch [79/120    avg_loss:0.167, val_acc:0.967]
Epoch [80/120    avg_loss:0.130, val_acc:0.977]
Epoch [81/120    avg_loss:0.154, val_acc:0.967]
Epoch [82/120    avg_loss:0.159, val_acc:0.971]
Epoch [83/120    avg_loss:0.190, val_acc:0.958]
Epoch [84/120    avg_loss:0.188, val_acc:0.904]
Epoch [85/120    avg_loss:0.160, val_acc:0.973]
Epoch [86/120    avg_loss:0.157, val_acc:0.983]
Epoch [87/120    avg_loss:0.142, val_acc:0.971]
Epoch [88/120    avg_loss:0.113, val_acc:0.990]
Epoch [89/120    avg_loss:0.104, val_acc:0.988]
Epoch [90/120    avg_loss:0.135, val_acc:0.981]
Epoch [91/120    avg_loss:0.123, val_acc:0.981]
Epoch [92/120    avg_loss:0.105, val_acc:0.994]
Epoch [93/120    avg_loss:0.089, val_acc:0.988]
Epoch [94/120    avg_loss:0.124, val_acc:0.938]
Epoch [95/120    avg_loss:0.148, val_acc:0.950]
Epoch [96/120    avg_loss:0.197, val_acc:0.969]
Epoch [97/120    avg_loss:0.169, val_acc:0.983]
Epoch [98/120    avg_loss:0.090, val_acc:0.983]
Epoch [99/120    avg_loss:0.114, val_acc:0.940]
Epoch [100/120    avg_loss:0.123, val_acc:0.983]
Epoch [101/120    avg_loss:0.090, val_acc:0.985]
Epoch [102/120    avg_loss:0.115, val_acc:0.973]
Epoch [103/120    avg_loss:0.149, val_acc:0.988]
Epoch [104/120    avg_loss:0.117, val_acc:0.988]
Epoch [105/120    avg_loss:0.101, val_acc:0.990]
Epoch [106/120    avg_loss:0.081, val_acc:0.985]
Epoch [107/120    avg_loss:0.071, val_acc:0.992]
Epoch [108/120    avg_loss:0.072, val_acc:0.992]
Epoch [109/120    avg_loss:0.072, val_acc:0.996]
Epoch [110/120    avg_loss:0.065, val_acc:0.994]
Epoch [111/120    avg_loss:0.063, val_acc:0.994]
Epoch [112/120    avg_loss:0.060, val_acc:0.996]
Epoch [113/120    avg_loss:0.058, val_acc:0.996]
Epoch [114/120    avg_loss:0.063, val_acc:0.996]
Epoch [115/120    avg_loss:0.056, val_acc:0.996]
Epoch [116/120    avg_loss:0.062, val_acc:0.996]
Epoch [117/120    avg_loss:0.058, val_acc:0.996]
Epoch [118/120    avg_loss:0.050, val_acc:0.996]
Epoch [119/120    avg_loss:0.051, val_acc:0.996]
Epoch [120/120    avg_loss:0.061, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 0.99780541 0.99095023 1.         0.94036697 0.91558442
 0.99277108 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.99216736861086
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a6e1bea20>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.115]
Epoch [2/120    avg_loss:2.552, val_acc:0.352]
Epoch [3/120    avg_loss:2.469, val_acc:0.373]
Epoch [4/120    avg_loss:2.400, val_acc:0.385]
Epoch [5/120    avg_loss:2.340, val_acc:0.427]
Epoch [6/120    avg_loss:2.269, val_acc:0.525]
Epoch [7/120    avg_loss:2.209, val_acc:0.598]
Epoch [8/120    avg_loss:2.141, val_acc:0.623]
Epoch [9/120    avg_loss:2.067, val_acc:0.606]
Epoch [10/120    avg_loss:1.977, val_acc:0.598]
Epoch [11/120    avg_loss:1.916, val_acc:0.623]
Epoch [12/120    avg_loss:1.824, val_acc:0.633]
Epoch [13/120    avg_loss:1.732, val_acc:0.669]
Epoch [14/120    avg_loss:1.642, val_acc:0.665]
Epoch [15/120    avg_loss:1.528, val_acc:0.700]
Epoch [16/120    avg_loss:1.435, val_acc:0.717]
Epoch [17/120    avg_loss:1.387, val_acc:0.698]
Epoch [18/120    avg_loss:1.298, val_acc:0.708]
Epoch [19/120    avg_loss:1.218, val_acc:0.725]
Epoch [20/120    avg_loss:1.124, val_acc:0.752]
Epoch [21/120    avg_loss:1.097, val_acc:0.756]
Epoch [22/120    avg_loss:1.014, val_acc:0.796]
Epoch [23/120    avg_loss:0.959, val_acc:0.794]
Epoch [24/120    avg_loss:0.912, val_acc:0.800]
Epoch [25/120    avg_loss:0.837, val_acc:0.831]
Epoch [26/120    avg_loss:0.780, val_acc:0.875]
Epoch [27/120    avg_loss:0.697, val_acc:0.944]
Epoch [28/120    avg_loss:0.726, val_acc:0.875]
Epoch [29/120    avg_loss:0.703, val_acc:0.831]
Epoch [30/120    avg_loss:0.609, val_acc:0.935]
Epoch [31/120    avg_loss:0.587, val_acc:0.940]
Epoch [32/120    avg_loss:0.521, val_acc:0.938]
Epoch [33/120    avg_loss:0.517, val_acc:0.948]
Epoch [34/120    avg_loss:0.493, val_acc:0.933]
Epoch [35/120    avg_loss:0.443, val_acc:0.946]
Epoch [36/120    avg_loss:0.440, val_acc:0.915]
Epoch [37/120    avg_loss:0.417, val_acc:0.942]
Epoch [38/120    avg_loss:0.370, val_acc:0.944]
Epoch [39/120    avg_loss:0.441, val_acc:0.890]
Epoch [40/120    avg_loss:0.421, val_acc:0.938]
Epoch [41/120    avg_loss:0.386, val_acc:0.935]
Epoch [42/120    avg_loss:0.332, val_acc:0.938]
Epoch [43/120    avg_loss:0.328, val_acc:0.912]
Epoch [44/120    avg_loss:0.324, val_acc:0.877]
Epoch [45/120    avg_loss:0.403, val_acc:0.919]
Epoch [46/120    avg_loss:0.345, val_acc:0.950]
Epoch [47/120    avg_loss:0.308, val_acc:0.958]
Epoch [48/120    avg_loss:0.282, val_acc:0.938]
Epoch [49/120    avg_loss:0.313, val_acc:0.958]
Epoch [50/120    avg_loss:0.297, val_acc:0.963]
Epoch [51/120    avg_loss:0.254, val_acc:0.954]
Epoch [52/120    avg_loss:0.261, val_acc:0.915]
Epoch [53/120    avg_loss:0.270, val_acc:0.940]
Epoch [54/120    avg_loss:0.303, val_acc:0.956]
Epoch [55/120    avg_loss:0.274, val_acc:0.946]
Epoch [56/120    avg_loss:0.283, val_acc:0.967]
Epoch [57/120    avg_loss:0.236, val_acc:0.931]
Epoch [58/120    avg_loss:0.259, val_acc:0.969]
Epoch [59/120    avg_loss:0.203, val_acc:0.967]
Epoch [60/120    avg_loss:0.209, val_acc:0.975]
Epoch [61/120    avg_loss:0.286, val_acc:0.971]
Epoch [62/120    avg_loss:0.250, val_acc:0.960]
Epoch [63/120    avg_loss:0.239, val_acc:0.950]
Epoch [64/120    avg_loss:0.204, val_acc:0.971]
Epoch [65/120    avg_loss:0.197, val_acc:0.965]
Epoch [66/120    avg_loss:0.223, val_acc:0.956]
Epoch [67/120    avg_loss:0.198, val_acc:0.973]
Epoch [68/120    avg_loss:0.158, val_acc:0.973]
Epoch [69/120    avg_loss:0.168, val_acc:0.960]
Epoch [70/120    avg_loss:0.205, val_acc:0.973]
Epoch [71/120    avg_loss:0.186, val_acc:0.983]
Epoch [72/120    avg_loss:0.143, val_acc:0.981]
Epoch [73/120    avg_loss:0.153, val_acc:0.977]
Epoch [74/120    avg_loss:0.162, val_acc:0.973]
Epoch [75/120    avg_loss:0.132, val_acc:0.975]
Epoch [76/120    avg_loss:0.156, val_acc:0.975]
Epoch [77/120    avg_loss:0.164, val_acc:0.938]
Epoch [78/120    avg_loss:0.152, val_acc:0.983]
Epoch [79/120    avg_loss:0.165, val_acc:0.975]
Epoch [80/120    avg_loss:0.131, val_acc:0.948]
Epoch [81/120    avg_loss:0.122, val_acc:0.975]
Epoch [82/120    avg_loss:0.138, val_acc:0.958]
Epoch [83/120    avg_loss:0.130, val_acc:0.988]
Epoch [84/120    avg_loss:0.141, val_acc:0.973]
Epoch [85/120    avg_loss:0.125, val_acc:0.979]
Epoch [86/120    avg_loss:0.114, val_acc:0.967]
Epoch [87/120    avg_loss:0.133, val_acc:0.977]
Epoch [88/120    avg_loss:0.100, val_acc:0.981]
Epoch [89/120    avg_loss:0.118, val_acc:0.971]
Epoch [90/120    avg_loss:0.120, val_acc:0.960]
Epoch [91/120    avg_loss:0.168, val_acc:0.965]
Epoch [92/120    avg_loss:0.139, val_acc:0.967]
Epoch [93/120    avg_loss:0.123, val_acc:0.981]
Epoch [94/120    avg_loss:0.098, val_acc:0.977]
Epoch [95/120    avg_loss:0.109, val_acc:0.985]
Epoch [96/120    avg_loss:0.096, val_acc:0.981]
Epoch [97/120    avg_loss:0.103, val_acc:0.988]
Epoch [98/120    avg_loss:0.080, val_acc:0.983]
Epoch [99/120    avg_loss:0.075, val_acc:0.983]
Epoch [100/120    avg_loss:0.062, val_acc:0.988]
Epoch [101/120    avg_loss:0.061, val_acc:0.988]
Epoch [102/120    avg_loss:0.072, val_acc:0.990]
Epoch [103/120    avg_loss:0.060, val_acc:0.990]
Epoch [104/120    avg_loss:0.062, val_acc:0.988]
Epoch [105/120    avg_loss:0.068, val_acc:0.990]
Epoch [106/120    avg_loss:0.062, val_acc:0.990]
Epoch [107/120    avg_loss:0.071, val_acc:0.988]
Epoch [108/120    avg_loss:0.068, val_acc:0.988]
Epoch [109/120    avg_loss:0.050, val_acc:0.988]
Epoch [110/120    avg_loss:0.061, val_acc:0.985]
Epoch [111/120    avg_loss:0.065, val_acc:0.985]
Epoch [112/120    avg_loss:0.072, val_acc:0.988]
Epoch [113/120    avg_loss:0.058, val_acc:0.985]
Epoch [114/120    avg_loss:0.055, val_acc:0.990]
Epoch [115/120    avg_loss:0.059, val_acc:0.990]
Epoch [116/120    avg_loss:0.056, val_acc:0.988]
Epoch [117/120    avg_loss:0.054, val_acc:0.990]
Epoch [118/120    avg_loss:0.055, val_acc:0.988]
Epoch [119/120    avg_loss:0.062, val_acc:0.988]
Epoch [120/120    avg_loss:0.059, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.97986577 1.         0.94065934 0.90657439
 1.         0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9914537455235803
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab0c7ff9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.132]
Epoch [2/120    avg_loss:2.562, val_acc:0.133]
Epoch [3/120    avg_loss:2.496, val_acc:0.319]
Epoch [4/120    avg_loss:2.435, val_acc:0.348]
Epoch [5/120    avg_loss:2.377, val_acc:0.373]
Epoch [6/120    avg_loss:2.319, val_acc:0.415]
Epoch [7/120    avg_loss:2.270, val_acc:0.438]
Epoch [8/120    avg_loss:2.213, val_acc:0.463]
Epoch [9/120    avg_loss:2.151, val_acc:0.552]
Epoch [10/120    avg_loss:2.074, val_acc:0.575]
Epoch [11/120    avg_loss:1.983, val_acc:0.598]
Epoch [12/120    avg_loss:1.918, val_acc:0.594]
Epoch [13/120    avg_loss:1.822, val_acc:0.621]
Epoch [14/120    avg_loss:1.736, val_acc:0.625]
Epoch [15/120    avg_loss:1.641, val_acc:0.658]
Epoch [16/120    avg_loss:1.573, val_acc:0.675]
Epoch [17/120    avg_loss:1.472, val_acc:0.696]
Epoch [18/120    avg_loss:1.417, val_acc:0.694]
Epoch [19/120    avg_loss:1.344, val_acc:0.696]
Epoch [20/120    avg_loss:1.263, val_acc:0.721]
Epoch [21/120    avg_loss:1.175, val_acc:0.767]
Epoch [22/120    avg_loss:1.106, val_acc:0.746]
Epoch [23/120    avg_loss:1.035, val_acc:0.833]
Epoch [24/120    avg_loss:0.946, val_acc:0.777]
Epoch [25/120    avg_loss:0.906, val_acc:0.898]
Epoch [26/120    avg_loss:0.841, val_acc:0.912]
Epoch [27/120    avg_loss:0.821, val_acc:0.879]
Epoch [28/120    avg_loss:0.735, val_acc:0.910]
Epoch [29/120    avg_loss:0.710, val_acc:0.927]
Epoch [30/120    avg_loss:0.662, val_acc:0.933]
Epoch [31/120    avg_loss:0.599, val_acc:0.927]
Epoch [32/120    avg_loss:0.561, val_acc:0.910]
Epoch [33/120    avg_loss:0.532, val_acc:0.919]
Epoch [34/120    avg_loss:0.508, val_acc:0.935]
Epoch [35/120    avg_loss:0.447, val_acc:0.925]
Epoch [36/120    avg_loss:0.428, val_acc:0.890]
Epoch [37/120    avg_loss:0.471, val_acc:0.910]
Epoch [38/120    avg_loss:0.513, val_acc:0.933]
Epoch [39/120    avg_loss:0.461, val_acc:0.940]
Epoch [40/120    avg_loss:0.390, val_acc:0.942]
Epoch [41/120    avg_loss:0.431, val_acc:0.935]
Epoch [42/120    avg_loss:0.386, val_acc:0.910]
Epoch [43/120    avg_loss:0.343, val_acc:0.952]
Epoch [44/120    avg_loss:0.322, val_acc:0.946]
Epoch [45/120    avg_loss:0.326, val_acc:0.940]
Epoch [46/120    avg_loss:0.312, val_acc:0.952]
Epoch [47/120    avg_loss:0.326, val_acc:0.898]
Epoch [48/120    avg_loss:0.330, val_acc:0.894]
Epoch [49/120    avg_loss:0.418, val_acc:0.940]
Epoch [50/120    avg_loss:0.324, val_acc:0.915]
Epoch [51/120    avg_loss:0.378, val_acc:0.929]
Epoch [52/120    avg_loss:0.302, val_acc:0.946]
Epoch [53/120    avg_loss:0.270, val_acc:0.944]
Epoch [54/120    avg_loss:0.261, val_acc:0.940]
Epoch [55/120    avg_loss:0.285, val_acc:0.956]
Epoch [56/120    avg_loss:0.232, val_acc:0.954]
Epoch [57/120    avg_loss:0.232, val_acc:0.946]
Epoch [58/120    avg_loss:0.365, val_acc:0.958]
Epoch [59/120    avg_loss:0.258, val_acc:0.954]
Epoch [60/120    avg_loss:0.231, val_acc:0.950]
Epoch [61/120    avg_loss:0.223, val_acc:0.969]
Epoch [62/120    avg_loss:0.232, val_acc:0.946]
Epoch [63/120    avg_loss:0.202, val_acc:0.952]
Epoch [64/120    avg_loss:0.208, val_acc:0.952]
Epoch [65/120    avg_loss:0.174, val_acc:0.969]
Epoch [66/120    avg_loss:0.196, val_acc:0.967]
Epoch [67/120    avg_loss:0.245, val_acc:0.960]
Epoch [68/120    avg_loss:0.239, val_acc:0.958]
Epoch [69/120    avg_loss:0.214, val_acc:0.912]
Epoch [70/120    avg_loss:0.258, val_acc:0.927]
Epoch [71/120    avg_loss:0.220, val_acc:0.952]
Epoch [72/120    avg_loss:0.225, val_acc:0.960]
Epoch [73/120    avg_loss:0.180, val_acc:0.954]
Epoch [74/120    avg_loss:0.169, val_acc:0.967]
Epoch [75/120    avg_loss:0.136, val_acc:0.969]
Epoch [76/120    avg_loss:0.140, val_acc:0.971]
Epoch [77/120    avg_loss:0.135, val_acc:0.942]
Epoch [78/120    avg_loss:0.128, val_acc:0.973]
Epoch [79/120    avg_loss:0.129, val_acc:0.971]
Epoch [80/120    avg_loss:0.144, val_acc:0.963]
Epoch [81/120    avg_loss:0.155, val_acc:0.950]
Epoch [82/120    avg_loss:0.115, val_acc:0.971]
Epoch [83/120    avg_loss:0.133, val_acc:0.931]
Epoch [84/120    avg_loss:0.124, val_acc:0.969]
Epoch [85/120    avg_loss:0.146, val_acc:0.977]
Epoch [86/120    avg_loss:0.111, val_acc:0.973]
Epoch [87/120    avg_loss:0.136, val_acc:0.975]
Epoch [88/120    avg_loss:0.130, val_acc:0.979]
Epoch [89/120    avg_loss:0.137, val_acc:0.975]
Epoch [90/120    avg_loss:0.108, val_acc:0.975]
Epoch [91/120    avg_loss:0.065, val_acc:0.979]
Epoch [92/120    avg_loss:0.093, val_acc:0.967]
Epoch [93/120    avg_loss:0.096, val_acc:0.988]
Epoch [94/120    avg_loss:0.101, val_acc:0.977]
Epoch [95/120    avg_loss:0.133, val_acc:0.973]
Epoch [96/120    avg_loss:0.117, val_acc:0.975]
Epoch [97/120    avg_loss:0.087, val_acc:0.967]
Epoch [98/120    avg_loss:0.112, val_acc:0.965]
Epoch [99/120    avg_loss:0.103, val_acc:0.981]
Epoch [100/120    avg_loss:0.068, val_acc:0.985]
Epoch [101/120    avg_loss:0.073, val_acc:0.969]
Epoch [102/120    avg_loss:0.083, val_acc:0.981]
Epoch [103/120    avg_loss:0.057, val_acc:0.981]
Epoch [104/120    avg_loss:0.053, val_acc:0.981]
Epoch [105/120    avg_loss:0.054, val_acc:0.988]
Epoch [106/120    avg_loss:0.057, val_acc:0.977]
Epoch [107/120    avg_loss:0.061, val_acc:0.977]
Epoch [108/120    avg_loss:0.135, val_acc:0.979]
Epoch [109/120    avg_loss:0.067, val_acc:0.969]
Epoch [110/120    avg_loss:0.049, val_acc:0.988]
Epoch [111/120    avg_loss:0.050, val_acc:0.975]
Epoch [112/120    avg_loss:0.046, val_acc:0.985]
Epoch [113/120    avg_loss:0.035, val_acc:0.985]
Epoch [114/120    avg_loss:0.048, val_acc:0.981]
Epoch [115/120    avg_loss:0.056, val_acc:0.967]
Epoch [116/120    avg_loss:0.050, val_acc:0.981]
Epoch [117/120    avg_loss:0.059, val_acc:0.977]
Epoch [118/120    avg_loss:0.053, val_acc:0.981]
Epoch [119/120    avg_loss:0.047, val_acc:0.985]
Epoch [120/120    avg_loss:0.054, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 212  18   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 0.99853801 0.97986577 0.95927602 0.89727463 0.89122807
 0.99516908 0.94972067 1.         1.         1.         0.98950131
 0.99109131 1.        ]

Kappa:
0.9838581305202655
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96b6ab3b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.062]
Epoch [2/120    avg_loss:2.567, val_acc:0.371]
Epoch [3/120    avg_loss:2.484, val_acc:0.362]
Epoch [4/120    avg_loss:2.416, val_acc:0.356]
Epoch [5/120    avg_loss:2.346, val_acc:0.335]
Epoch [6/120    avg_loss:2.291, val_acc:0.348]
Epoch [7/120    avg_loss:2.236, val_acc:0.350]
Epoch [8/120    avg_loss:2.166, val_acc:0.358]
Epoch [9/120    avg_loss:2.118, val_acc:0.404]
Epoch [10/120    avg_loss:2.061, val_acc:0.433]
Epoch [11/120    avg_loss:1.996, val_acc:0.463]
Epoch [12/120    avg_loss:1.933, val_acc:0.540]
Epoch [13/120    avg_loss:1.873, val_acc:0.567]
Epoch [14/120    avg_loss:1.811, val_acc:0.577]
Epoch [15/120    avg_loss:1.742, val_acc:0.615]
Epoch [16/120    avg_loss:1.692, val_acc:0.648]
Epoch [17/120    avg_loss:1.594, val_acc:0.675]
Epoch [18/120    avg_loss:1.538, val_acc:0.685]
Epoch [19/120    avg_loss:1.480, val_acc:0.690]
Epoch [20/120    avg_loss:1.385, val_acc:0.704]
Epoch [21/120    avg_loss:1.307, val_acc:0.717]
Epoch [22/120    avg_loss:1.256, val_acc:0.735]
Epoch [23/120    avg_loss:1.177, val_acc:0.738]
Epoch [24/120    avg_loss:1.156, val_acc:0.746]
Epoch [25/120    avg_loss:1.087, val_acc:0.752]
Epoch [26/120    avg_loss:0.981, val_acc:0.877]
Epoch [27/120    avg_loss:0.923, val_acc:0.787]
Epoch [28/120    avg_loss:0.865, val_acc:0.900]
Epoch [29/120    avg_loss:0.832, val_acc:0.881]
Epoch [30/120    avg_loss:0.763, val_acc:0.794]
Epoch [31/120    avg_loss:0.694, val_acc:0.921]
Epoch [32/120    avg_loss:0.724, val_acc:0.865]
Epoch [33/120    avg_loss:0.699, val_acc:0.921]
Epoch [34/120    avg_loss:0.650, val_acc:0.910]
Epoch [35/120    avg_loss:0.611, val_acc:0.908]
Epoch [36/120    avg_loss:0.554, val_acc:0.925]
Epoch [37/120    avg_loss:0.510, val_acc:0.917]
Epoch [38/120    avg_loss:0.518, val_acc:0.915]
Epoch [39/120    avg_loss:0.466, val_acc:0.900]
Epoch [40/120    avg_loss:0.506, val_acc:0.915]
Epoch [41/120    avg_loss:0.476, val_acc:0.952]
Epoch [42/120    avg_loss:0.402, val_acc:0.944]
Epoch [43/120    avg_loss:0.412, val_acc:0.938]
Epoch [44/120    avg_loss:0.400, val_acc:0.919]
Epoch [45/120    avg_loss:0.379, val_acc:0.950]
Epoch [46/120    avg_loss:0.414, val_acc:0.940]
Epoch [47/120    avg_loss:0.363, val_acc:0.956]
Epoch [48/120    avg_loss:0.375, val_acc:0.929]
Epoch [49/120    avg_loss:0.377, val_acc:0.944]
Epoch [50/120    avg_loss:0.352, val_acc:0.950]
Epoch [51/120    avg_loss:0.333, val_acc:0.954]
Epoch [52/120    avg_loss:0.316, val_acc:0.969]
Epoch [53/120    avg_loss:0.300, val_acc:0.938]
Epoch [54/120    avg_loss:0.314, val_acc:0.950]
Epoch [55/120    avg_loss:0.322, val_acc:0.975]
Epoch [56/120    avg_loss:0.298, val_acc:0.948]
Epoch [57/120    avg_loss:0.295, val_acc:0.950]
Epoch [58/120    avg_loss:0.239, val_acc:0.935]
Epoch [59/120    avg_loss:0.271, val_acc:0.915]
Epoch [60/120    avg_loss:0.279, val_acc:0.967]
Epoch [61/120    avg_loss:0.248, val_acc:0.973]
Epoch [62/120    avg_loss:0.398, val_acc:0.925]
Epoch [63/120    avg_loss:0.276, val_acc:0.950]
Epoch [64/120    avg_loss:0.243, val_acc:0.969]
Epoch [65/120    avg_loss:0.229, val_acc:0.967]
Epoch [66/120    avg_loss:0.204, val_acc:0.954]
Epoch [67/120    avg_loss:0.203, val_acc:0.952]
Epoch [68/120    avg_loss:0.235, val_acc:0.960]
Epoch [69/120    avg_loss:0.183, val_acc:0.969]
Epoch [70/120    avg_loss:0.183, val_acc:0.975]
Epoch [71/120    avg_loss:0.168, val_acc:0.973]
Epoch [72/120    avg_loss:0.149, val_acc:0.967]
Epoch [73/120    avg_loss:0.144, val_acc:0.977]
Epoch [74/120    avg_loss:0.154, val_acc:0.983]
Epoch [75/120    avg_loss:0.141, val_acc:0.985]
Epoch [76/120    avg_loss:0.158, val_acc:0.975]
Epoch [77/120    avg_loss:0.140, val_acc:0.973]
Epoch [78/120    avg_loss:0.143, val_acc:0.981]
Epoch [79/120    avg_loss:0.157, val_acc:0.981]
Epoch [80/120    avg_loss:0.149, val_acc:0.977]
Epoch [81/120    avg_loss:0.125, val_acc:0.975]
Epoch [82/120    avg_loss:0.134, val_acc:0.979]
Epoch [83/120    avg_loss:0.140, val_acc:0.975]
Epoch [84/120    avg_loss:0.144, val_acc:0.973]
Epoch [85/120    avg_loss:0.132, val_acc:0.981]
Epoch [86/120    avg_loss:0.134, val_acc:0.979]
Epoch [87/120    avg_loss:0.131, val_acc:0.975]
Epoch [88/120    avg_loss:0.124, val_acc:0.975]
Epoch [89/120    avg_loss:0.133, val_acc:0.977]
Epoch [90/120    avg_loss:0.133, val_acc:0.977]
Epoch [91/120    avg_loss:0.137, val_acc:0.973]
Epoch [92/120    avg_loss:0.156, val_acc:0.975]
Epoch [93/120    avg_loss:0.129, val_acc:0.979]
Epoch [94/120    avg_loss:0.157, val_acc:0.981]
Epoch [95/120    avg_loss:0.135, val_acc:0.985]
Epoch [96/120    avg_loss:0.113, val_acc:0.985]
Epoch [97/120    avg_loss:0.131, val_acc:0.985]
Epoch [98/120    avg_loss:0.131, val_acc:0.985]
Epoch [99/120    avg_loss:0.136, val_acc:0.985]
Epoch [100/120    avg_loss:0.134, val_acc:0.983]
Epoch [101/120    avg_loss:0.130, val_acc:0.983]
Epoch [102/120    avg_loss:0.143, val_acc:0.983]
Epoch [103/120    avg_loss:0.119, val_acc:0.985]
Epoch [104/120    avg_loss:0.113, val_acc:0.985]
Epoch [105/120    avg_loss:0.133, val_acc:0.985]
Epoch [106/120    avg_loss:0.135, val_acc:0.983]
Epoch [107/120    avg_loss:0.137, val_acc:0.983]
Epoch [108/120    avg_loss:0.115, val_acc:0.981]
Epoch [109/120    avg_loss:0.128, val_acc:0.981]
Epoch [110/120    avg_loss:0.122, val_acc:0.979]
Epoch [111/120    avg_loss:0.123, val_acc:0.981]
Epoch [112/120    avg_loss:0.129, val_acc:0.979]
Epoch [113/120    avg_loss:0.127, val_acc:0.979]
Epoch [114/120    avg_loss:0.125, val_acc:0.983]
Epoch [115/120    avg_loss:0.131, val_acc:0.985]
Epoch [116/120    avg_loss:0.132, val_acc:0.983]
Epoch [117/120    avg_loss:0.148, val_acc:0.985]
Epoch [118/120    avg_loss:0.154, val_acc:0.983]
Epoch [119/120    avg_loss:0.133, val_acc:0.985]
Epoch [120/120    avg_loss:0.130, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 200  27   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99927061 0.97550111 0.99782135 0.91743119 0.88311688
 0.99756691 0.94382022 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9886055506031588
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f34f64b3a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.113]
Epoch [2/120    avg_loss:2.493, val_acc:0.294]
Epoch [3/120    avg_loss:2.416, val_acc:0.298]
Epoch [4/120    avg_loss:2.349, val_acc:0.342]
Epoch [5/120    avg_loss:2.298, val_acc:0.404]
Epoch [6/120    avg_loss:2.245, val_acc:0.400]
Epoch [7/120    avg_loss:2.196, val_acc:0.435]
Epoch [8/120    avg_loss:2.152, val_acc:0.446]
Epoch [9/120    avg_loss:2.085, val_acc:0.463]
Epoch [10/120    avg_loss:2.035, val_acc:0.471]
Epoch [11/120    avg_loss:1.970, val_acc:0.485]
Epoch [12/120    avg_loss:1.918, val_acc:0.537]
Epoch [13/120    avg_loss:1.836, val_acc:0.556]
Epoch [14/120    avg_loss:1.760, val_acc:0.581]
Epoch [15/120    avg_loss:1.716, val_acc:0.588]
Epoch [16/120    avg_loss:1.635, val_acc:0.610]
Epoch [17/120    avg_loss:1.547, val_acc:0.683]
Epoch [18/120    avg_loss:1.475, val_acc:0.690]
Epoch [19/120    avg_loss:1.397, val_acc:0.735]
Epoch [20/120    avg_loss:1.334, val_acc:0.729]
Epoch [21/120    avg_loss:1.298, val_acc:0.765]
Epoch [22/120    avg_loss:1.182, val_acc:0.762]
Epoch [23/120    avg_loss:1.137, val_acc:0.806]
Epoch [24/120    avg_loss:1.024, val_acc:0.819]
Epoch [25/120    avg_loss:0.955, val_acc:0.894]
Epoch [26/120    avg_loss:0.861, val_acc:0.902]
Epoch [27/120    avg_loss:0.807, val_acc:0.881]
Epoch [28/120    avg_loss:0.819, val_acc:0.887]
Epoch [29/120    avg_loss:0.720, val_acc:0.908]
Epoch [30/120    avg_loss:0.653, val_acc:0.904]
Epoch [31/120    avg_loss:0.649, val_acc:0.892]
Epoch [32/120    avg_loss:0.612, val_acc:0.902]
Epoch [33/120    avg_loss:0.594, val_acc:0.860]
Epoch [34/120    avg_loss:0.534, val_acc:0.919]
Epoch [35/120    avg_loss:0.491, val_acc:0.915]
Epoch [36/120    avg_loss:0.491, val_acc:0.904]
Epoch [37/120    avg_loss:0.476, val_acc:0.921]
Epoch [38/120    avg_loss:0.413, val_acc:0.917]
Epoch [39/120    avg_loss:0.428, val_acc:0.921]
Epoch [40/120    avg_loss:0.433, val_acc:0.925]
Epoch [41/120    avg_loss:0.461, val_acc:0.917]
Epoch [42/120    avg_loss:0.406, val_acc:0.921]
Epoch [43/120    avg_loss:0.372, val_acc:0.929]
Epoch [44/120    avg_loss:0.316, val_acc:0.948]
Epoch [45/120    avg_loss:0.350, val_acc:0.950]
Epoch [46/120    avg_loss:0.329, val_acc:0.938]
Epoch [47/120    avg_loss:0.380, val_acc:0.927]
Epoch [48/120    avg_loss:0.360, val_acc:0.950]
Epoch [49/120    avg_loss:0.290, val_acc:0.952]
Epoch [50/120    avg_loss:0.259, val_acc:0.954]
Epoch [51/120    avg_loss:0.281, val_acc:0.952]
Epoch [52/120    avg_loss:0.286, val_acc:0.950]
Epoch [53/120    avg_loss:0.284, val_acc:0.975]
Epoch [54/120    avg_loss:0.235, val_acc:0.952]
Epoch [55/120    avg_loss:0.232, val_acc:0.969]
Epoch [56/120    avg_loss:0.204, val_acc:0.946]
Epoch [57/120    avg_loss:0.217, val_acc:0.967]
Epoch [58/120    avg_loss:0.207, val_acc:0.969]
Epoch [59/120    avg_loss:0.215, val_acc:0.935]
Epoch [60/120    avg_loss:0.199, val_acc:0.963]
Epoch [61/120    avg_loss:0.203, val_acc:0.988]
Epoch [62/120    avg_loss:0.266, val_acc:0.952]
Epoch [63/120    avg_loss:0.223, val_acc:0.975]
Epoch [64/120    avg_loss:0.210, val_acc:0.954]
Epoch [65/120    avg_loss:0.188, val_acc:0.973]
Epoch [66/120    avg_loss:0.171, val_acc:0.979]
Epoch [67/120    avg_loss:0.151, val_acc:0.969]
Epoch [68/120    avg_loss:0.136, val_acc:0.981]
Epoch [69/120    avg_loss:0.133, val_acc:0.985]
Epoch [70/120    avg_loss:0.184, val_acc:0.967]
Epoch [71/120    avg_loss:0.274, val_acc:0.971]
Epoch [72/120    avg_loss:0.172, val_acc:0.954]
Epoch [73/120    avg_loss:0.178, val_acc:0.975]
Epoch [74/120    avg_loss:0.181, val_acc:0.967]
Epoch [75/120    avg_loss:0.155, val_acc:0.979]
Epoch [76/120    avg_loss:0.126, val_acc:0.983]
Epoch [77/120    avg_loss:0.110, val_acc:0.981]
Epoch [78/120    avg_loss:0.099, val_acc:0.983]
Epoch [79/120    avg_loss:0.108, val_acc:0.979]
Epoch [80/120    avg_loss:0.108, val_acc:0.983]
Epoch [81/120    avg_loss:0.108, val_acc:0.981]
Epoch [82/120    avg_loss:0.115, val_acc:0.979]
Epoch [83/120    avg_loss:0.090, val_acc:0.981]
Epoch [84/120    avg_loss:0.102, val_acc:0.983]
Epoch [85/120    avg_loss:0.095, val_acc:0.983]
Epoch [86/120    avg_loss:0.098, val_acc:0.981]
Epoch [87/120    avg_loss:0.099, val_acc:0.981]
Epoch [88/120    avg_loss:0.099, val_acc:0.981]
Epoch [89/120    avg_loss:0.101, val_acc:0.981]
Epoch [90/120    avg_loss:0.097, val_acc:0.981]
Epoch [91/120    avg_loss:0.103, val_acc:0.981]
Epoch [92/120    avg_loss:0.101, val_acc:0.981]
Epoch [93/120    avg_loss:0.106, val_acc:0.981]
Epoch [94/120    avg_loss:0.103, val_acc:0.981]
Epoch [95/120    avg_loss:0.089, val_acc:0.983]
Epoch [96/120    avg_loss:0.093, val_acc:0.983]
Epoch [97/120    avg_loss:0.094, val_acc:0.983]
Epoch [98/120    avg_loss:0.100, val_acc:0.983]
Epoch [99/120    avg_loss:0.102, val_acc:0.983]
Epoch [100/120    avg_loss:0.102, val_acc:0.983]
Epoch [101/120    avg_loss:0.090, val_acc:0.983]
Epoch [102/120    avg_loss:0.089, val_acc:0.983]
Epoch [103/120    avg_loss:0.093, val_acc:0.983]
Epoch [104/120    avg_loss:0.092, val_acc:0.983]
Epoch [105/120    avg_loss:0.100, val_acc:0.983]
Epoch [106/120    avg_loss:0.089, val_acc:0.983]
Epoch [107/120    avg_loss:0.100, val_acc:0.983]
Epoch [108/120    avg_loss:0.105, val_acc:0.983]
Epoch [109/120    avg_loss:0.083, val_acc:0.983]
Epoch [110/120    avg_loss:0.103, val_acc:0.983]
Epoch [111/120    avg_loss:0.094, val_acc:0.983]
Epoch [112/120    avg_loss:0.091, val_acc:0.983]
Epoch [113/120    avg_loss:0.080, val_acc:0.983]
Epoch [114/120    avg_loss:0.085, val_acc:0.983]
Epoch [115/120    avg_loss:0.100, val_acc:0.983]
Epoch [116/120    avg_loss:0.096, val_acc:0.983]
Epoch [117/120    avg_loss:0.092, val_acc:0.983]
Epoch [118/120    avg_loss:0.089, val_acc:0.983]
Epoch [119/120    avg_loss:0.088, val_acc:0.983]
Epoch [120/120    avg_loss:0.096, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 198  29   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.98426966 1.         0.90617849 0.87012987
 0.99756691 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9886060246555667
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbed22cda58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.260]
Epoch [2/120    avg_loss:2.530, val_acc:0.362]
Epoch [3/120    avg_loss:2.438, val_acc:0.348]
Epoch [4/120    avg_loss:2.366, val_acc:0.346]
Epoch [5/120    avg_loss:2.295, val_acc:0.348]
Epoch [6/120    avg_loss:2.227, val_acc:0.360]
Epoch [7/120    avg_loss:2.175, val_acc:0.369]
Epoch [8/120    avg_loss:2.110, val_acc:0.427]
Epoch [9/120    avg_loss:2.060, val_acc:0.473]
Epoch [10/120    avg_loss:1.986, val_acc:0.508]
Epoch [11/120    avg_loss:1.891, val_acc:0.577]
Epoch [12/120    avg_loss:1.851, val_acc:0.594]
Epoch [13/120    avg_loss:1.784, val_acc:0.606]
Epoch [14/120    avg_loss:1.710, val_acc:0.640]
Epoch [15/120    avg_loss:1.639, val_acc:0.629]
Epoch [16/120    avg_loss:1.550, val_acc:0.669]
Epoch [17/120    avg_loss:1.449, val_acc:0.725]
Epoch [18/120    avg_loss:1.368, val_acc:0.723]
Epoch [19/120    avg_loss:1.302, val_acc:0.748]
Epoch [20/120    avg_loss:1.220, val_acc:0.731]
Epoch [21/120    avg_loss:1.145, val_acc:0.733]
Epoch [22/120    avg_loss:1.091, val_acc:0.756]
Epoch [23/120    avg_loss:0.994, val_acc:0.773]
Epoch [24/120    avg_loss:0.955, val_acc:0.785]
Epoch [25/120    avg_loss:0.854, val_acc:0.831]
Epoch [26/120    avg_loss:0.811, val_acc:0.858]
Epoch [27/120    avg_loss:0.743, val_acc:0.794]
Epoch [28/120    avg_loss:0.764, val_acc:0.865]
Epoch [29/120    avg_loss:0.634, val_acc:0.933]
Epoch [30/120    avg_loss:0.611, val_acc:0.885]
Epoch [31/120    avg_loss:0.659, val_acc:0.890]
Epoch [32/120    avg_loss:0.562, val_acc:0.931]
Epoch [33/120    avg_loss:0.519, val_acc:0.908]
Epoch [34/120    avg_loss:0.532, val_acc:0.921]
Epoch [35/120    avg_loss:0.493, val_acc:0.919]
Epoch [36/120    avg_loss:0.529, val_acc:0.933]
Epoch [37/120    avg_loss:0.484, val_acc:0.917]
Epoch [38/120    avg_loss:0.468, val_acc:0.900]
Epoch [39/120    avg_loss:0.432, val_acc:0.894]
Epoch [40/120    avg_loss:0.384, val_acc:0.929]
Epoch [41/120    avg_loss:0.369, val_acc:0.942]
Epoch [42/120    avg_loss:0.332, val_acc:0.938]
Epoch [43/120    avg_loss:0.324, val_acc:0.923]
Epoch [44/120    avg_loss:0.342, val_acc:0.929]
Epoch [45/120    avg_loss:0.289, val_acc:0.963]
Epoch [46/120    avg_loss:0.259, val_acc:0.958]
Epoch [47/120    avg_loss:0.289, val_acc:0.971]
Epoch [48/120    avg_loss:0.244, val_acc:0.938]
Epoch [49/120    avg_loss:0.271, val_acc:0.950]
Epoch [50/120    avg_loss:0.276, val_acc:0.969]
Epoch [51/120    avg_loss:0.287, val_acc:0.942]
Epoch [52/120    avg_loss:0.261, val_acc:0.956]
Epoch [53/120    avg_loss:0.285, val_acc:0.940]
Epoch [54/120    avg_loss:0.215, val_acc:0.946]
Epoch [55/120    avg_loss:0.211, val_acc:0.975]
Epoch [56/120    avg_loss:0.182, val_acc:0.973]
Epoch [57/120    avg_loss:0.191, val_acc:0.954]
Epoch [58/120    avg_loss:0.216, val_acc:0.983]
Epoch [59/120    avg_loss:0.172, val_acc:0.979]
Epoch [60/120    avg_loss:0.164, val_acc:0.979]
Epoch [61/120    avg_loss:0.188, val_acc:0.979]
Epoch [62/120    avg_loss:0.195, val_acc:0.981]
Epoch [63/120    avg_loss:0.178, val_acc:0.952]
Epoch [64/120    avg_loss:0.163, val_acc:0.979]
Epoch [65/120    avg_loss:0.142, val_acc:0.971]
Epoch [66/120    avg_loss:0.164, val_acc:0.912]
Epoch [67/120    avg_loss:0.262, val_acc:0.975]
Epoch [68/120    avg_loss:0.186, val_acc:0.971]
Epoch [69/120    avg_loss:0.177, val_acc:0.967]
Epoch [70/120    avg_loss:0.182, val_acc:0.969]
Epoch [71/120    avg_loss:0.125, val_acc:0.973]
Epoch [72/120    avg_loss:0.136, val_acc:0.985]
Epoch [73/120    avg_loss:0.109, val_acc:0.985]
Epoch [74/120    avg_loss:0.092, val_acc:0.988]
Epoch [75/120    avg_loss:0.102, val_acc:0.990]
Epoch [76/120    avg_loss:0.098, val_acc:0.990]
Epoch [77/120    avg_loss:0.092, val_acc:0.990]
Epoch [78/120    avg_loss:0.090, val_acc:0.990]
Epoch [79/120    avg_loss:0.090, val_acc:0.990]
Epoch [80/120    avg_loss:0.101, val_acc:0.988]
Epoch [81/120    avg_loss:0.075, val_acc:0.988]
Epoch [82/120    avg_loss:0.093, val_acc:0.988]
Epoch [83/120    avg_loss:0.098, val_acc:0.990]
Epoch [84/120    avg_loss:0.088, val_acc:0.990]
Epoch [85/120    avg_loss:0.099, val_acc:0.992]
Epoch [86/120    avg_loss:0.091, val_acc:0.992]
Epoch [87/120    avg_loss:0.091, val_acc:0.992]
Epoch [88/120    avg_loss:0.079, val_acc:0.990]
Epoch [89/120    avg_loss:0.083, val_acc:0.988]
Epoch [90/120    avg_loss:0.086, val_acc:0.988]
Epoch [91/120    avg_loss:0.084, val_acc:0.990]
Epoch [92/120    avg_loss:0.094, val_acc:0.990]
Epoch [93/120    avg_loss:0.091, val_acc:0.990]
Epoch [94/120    avg_loss:0.088, val_acc:0.992]
Epoch [95/120    avg_loss:0.074, val_acc:0.992]
Epoch [96/120    avg_loss:0.071, val_acc:0.992]
Epoch [97/120    avg_loss:0.073, val_acc:0.992]
Epoch [98/120    avg_loss:0.086, val_acc:0.992]
Epoch [99/120    avg_loss:0.080, val_acc:0.990]
Epoch [100/120    avg_loss:0.072, val_acc:0.990]
Epoch [101/120    avg_loss:0.077, val_acc:0.994]
Epoch [102/120    avg_loss:0.071, val_acc:0.992]
Epoch [103/120    avg_loss:0.086, val_acc:0.992]
Epoch [104/120    avg_loss:0.079, val_acc:0.992]
Epoch [105/120    avg_loss:0.073, val_acc:0.994]
Epoch [106/120    avg_loss:0.080, val_acc:0.990]
Epoch [107/120    avg_loss:0.081, val_acc:0.990]
Epoch [108/120    avg_loss:0.089, val_acc:0.990]
Epoch [109/120    avg_loss:0.079, val_acc:0.990]
Epoch [110/120    avg_loss:0.072, val_acc:0.992]
Epoch [111/120    avg_loss:0.079, val_acc:0.992]
Epoch [112/120    avg_loss:0.076, val_acc:0.992]
Epoch [113/120    avg_loss:0.063, val_acc:0.992]
Epoch [114/120    avg_loss:0.067, val_acc:0.992]
Epoch [115/120    avg_loss:0.069, val_acc:0.992]
Epoch [116/120    avg_loss:0.057, val_acc:0.992]
Epoch [117/120    avg_loss:0.068, val_acc:0.992]
Epoch [118/120    avg_loss:0.069, val_acc:0.990]
Epoch [119/120    avg_loss:0.061, val_acc:0.990]
Epoch [120/120    avg_loss:0.067, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  16   0   0   0   0   0   0   2   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.997815   0.98426966 1.         0.93095768 0.90102389
 0.99266504 0.96132597 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.990266119799671
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ec8997a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.310]
Epoch [2/120    avg_loss:2.532, val_acc:0.348]
Epoch [3/120    avg_loss:2.453, val_acc:0.348]
Epoch [4/120    avg_loss:2.382, val_acc:0.375]
Epoch [5/120    avg_loss:2.318, val_acc:0.388]
Epoch [6/120    avg_loss:2.259, val_acc:0.427]
Epoch [7/120    avg_loss:2.206, val_acc:0.504]
Epoch [8/120    avg_loss:2.145, val_acc:0.498]
Epoch [9/120    avg_loss:2.080, val_acc:0.525]
Epoch [10/120    avg_loss:2.011, val_acc:0.537]
Epoch [11/120    avg_loss:1.920, val_acc:0.535]
Epoch [12/120    avg_loss:1.855, val_acc:0.581]
Epoch [13/120    avg_loss:1.775, val_acc:0.577]
Epoch [14/120    avg_loss:1.692, val_acc:0.600]
Epoch [15/120    avg_loss:1.604, val_acc:0.583]
Epoch [16/120    avg_loss:1.527, val_acc:0.613]
Epoch [17/120    avg_loss:1.447, val_acc:0.642]
Epoch [18/120    avg_loss:1.384, val_acc:0.654]
Epoch [19/120    avg_loss:1.312, val_acc:0.685]
Epoch [20/120    avg_loss:1.248, val_acc:0.683]
Epoch [21/120    avg_loss:1.215, val_acc:0.700]
Epoch [22/120    avg_loss:1.195, val_acc:0.744]
Epoch [23/120    avg_loss:1.063, val_acc:0.715]
Epoch [24/120    avg_loss:0.997, val_acc:0.748]
Epoch [25/120    avg_loss:0.903, val_acc:0.783]
Epoch [26/120    avg_loss:0.847, val_acc:0.779]
Epoch [27/120    avg_loss:0.814, val_acc:0.794]
Epoch [28/120    avg_loss:0.725, val_acc:0.896]
Epoch [29/120    avg_loss:0.732, val_acc:0.827]
Epoch [30/120    avg_loss:0.718, val_acc:0.823]
Epoch [31/120    avg_loss:0.656, val_acc:0.863]
Epoch [32/120    avg_loss:0.613, val_acc:0.877]
Epoch [33/120    avg_loss:0.594, val_acc:0.827]
Epoch [34/120    avg_loss:0.566, val_acc:0.919]
Epoch [35/120    avg_loss:0.565, val_acc:0.933]
Epoch [36/120    avg_loss:0.492, val_acc:0.915]
Epoch [37/120    avg_loss:0.501, val_acc:0.935]
Epoch [38/120    avg_loss:0.428, val_acc:0.919]
Epoch [39/120    avg_loss:0.381, val_acc:0.944]
Epoch [40/120    avg_loss:0.406, val_acc:0.929]
Epoch [41/120    avg_loss:0.413, val_acc:0.946]
Epoch [42/120    avg_loss:0.415, val_acc:0.944]
Epoch [43/120    avg_loss:0.366, val_acc:0.940]
Epoch [44/120    avg_loss:0.423, val_acc:0.912]
Epoch [45/120    avg_loss:0.372, val_acc:0.929]
Epoch [46/120    avg_loss:0.328, val_acc:0.958]
Epoch [47/120    avg_loss:0.322, val_acc:0.919]
Epoch [48/120    avg_loss:0.313, val_acc:0.948]
Epoch [49/120    avg_loss:0.241, val_acc:0.933]
Epoch [50/120    avg_loss:0.247, val_acc:0.965]
Epoch [51/120    avg_loss:0.226, val_acc:0.938]
Epoch [52/120    avg_loss:0.229, val_acc:0.963]
Epoch [53/120    avg_loss:0.242, val_acc:0.921]
Epoch [54/120    avg_loss:0.225, val_acc:0.944]
Epoch [55/120    avg_loss:0.252, val_acc:0.935]
Epoch [56/120    avg_loss:0.249, val_acc:0.967]
Epoch [57/120    avg_loss:0.171, val_acc:0.975]
Epoch [58/120    avg_loss:0.156, val_acc:0.969]
Epoch [59/120    avg_loss:0.217, val_acc:0.944]
Epoch [60/120    avg_loss:0.272, val_acc:0.944]
Epoch [61/120    avg_loss:0.238, val_acc:0.960]
Epoch [62/120    avg_loss:0.184, val_acc:0.954]
Epoch [63/120    avg_loss:0.229, val_acc:0.960]
Epoch [64/120    avg_loss:0.238, val_acc:0.967]
Epoch [65/120    avg_loss:0.167, val_acc:0.956]
Epoch [66/120    avg_loss:0.199, val_acc:0.948]
Epoch [67/120    avg_loss:0.172, val_acc:0.965]
Epoch [68/120    avg_loss:0.138, val_acc:0.971]
Epoch [69/120    avg_loss:0.111, val_acc:0.981]
Epoch [70/120    avg_loss:0.098, val_acc:0.985]
Epoch [71/120    avg_loss:0.120, val_acc:0.969]
Epoch [72/120    avg_loss:0.171, val_acc:0.971]
Epoch [73/120    avg_loss:0.132, val_acc:0.981]
Epoch [74/120    avg_loss:0.101, val_acc:0.977]
Epoch [75/120    avg_loss:0.126, val_acc:0.990]
Epoch [76/120    avg_loss:0.096, val_acc:0.977]
Epoch [77/120    avg_loss:0.101, val_acc:0.975]
Epoch [78/120    avg_loss:0.117, val_acc:0.958]
Epoch [79/120    avg_loss:0.119, val_acc:0.967]
Epoch [80/120    avg_loss:0.090, val_acc:0.985]
Epoch [81/120    avg_loss:0.083, val_acc:0.965]
Epoch [82/120    avg_loss:0.078, val_acc:0.994]
Epoch [83/120    avg_loss:0.073, val_acc:0.977]
Epoch [84/120    avg_loss:0.079, val_acc:0.988]
Epoch [85/120    avg_loss:0.086, val_acc:0.988]
Epoch [86/120    avg_loss:0.071, val_acc:0.990]
Epoch [87/120    avg_loss:0.060, val_acc:0.977]
Epoch [88/120    avg_loss:0.077, val_acc:0.981]
Epoch [89/120    avg_loss:0.057, val_acc:0.985]
Epoch [90/120    avg_loss:0.040, val_acc:0.979]
Epoch [91/120    avg_loss:0.059, val_acc:0.988]
Epoch [92/120    avg_loss:0.116, val_acc:0.981]
Epoch [93/120    avg_loss:0.113, val_acc:0.992]
Epoch [94/120    avg_loss:0.067, val_acc:0.988]
Epoch [95/120    avg_loss:0.076, val_acc:0.975]
Epoch [96/120    avg_loss:0.092, val_acc:0.985]
Epoch [97/120    avg_loss:0.059, val_acc:0.990]
Epoch [98/120    avg_loss:0.040, val_acc:0.992]
Epoch [99/120    avg_loss:0.049, val_acc:0.992]
Epoch [100/120    avg_loss:0.040, val_acc:0.992]
Epoch [101/120    avg_loss:0.039, val_acc:0.988]
Epoch [102/120    avg_loss:0.033, val_acc:0.990]
Epoch [103/120    avg_loss:0.054, val_acc:0.994]
Epoch [104/120    avg_loss:0.037, val_acc:0.994]
Epoch [105/120    avg_loss:0.040, val_acc:0.994]
Epoch [106/120    avg_loss:0.041, val_acc:0.994]
Epoch [107/120    avg_loss:0.036, val_acc:0.994]
Epoch [108/120    avg_loss:0.037, val_acc:0.994]
Epoch [109/120    avg_loss:0.037, val_acc:0.994]
Epoch [110/120    avg_loss:0.042, val_acc:0.992]
Epoch [111/120    avg_loss:0.044, val_acc:0.996]
Epoch [112/120    avg_loss:0.035, val_acc:0.996]
Epoch [113/120    avg_loss:0.032, val_acc:0.996]
Epoch [114/120    avg_loss:0.033, val_acc:0.994]
Epoch [115/120    avg_loss:0.034, val_acc:0.994]
Epoch [116/120    avg_loss:0.026, val_acc:0.996]
Epoch [117/120    avg_loss:0.033, val_acc:0.998]
Epoch [118/120    avg_loss:0.034, val_acc:0.996]
Epoch [119/120    avg_loss:0.043, val_acc:0.994]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.96491228 0.94444444
 0.99038462 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.995015399929257
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8803c70470>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.622, val_acc:0.152]
Epoch [2/120    avg_loss:2.532, val_acc:0.404]
Epoch [3/120    avg_loss:2.444, val_acc:0.473]
Epoch [4/120    avg_loss:2.370, val_acc:0.465]
Epoch [5/120    avg_loss:2.311, val_acc:0.444]
Epoch [6/120    avg_loss:2.249, val_acc:0.448]
Epoch [7/120    avg_loss:2.190, val_acc:0.479]
Epoch [8/120    avg_loss:2.137, val_acc:0.542]
Epoch [9/120    avg_loss:2.059, val_acc:0.581]
Epoch [10/120    avg_loss:1.987, val_acc:0.608]
Epoch [11/120    avg_loss:1.925, val_acc:0.610]
Epoch [12/120    avg_loss:1.834, val_acc:0.617]
Epoch [13/120    avg_loss:1.755, val_acc:0.633]
Epoch [14/120    avg_loss:1.670, val_acc:0.640]
Epoch [15/120    avg_loss:1.633, val_acc:0.677]
Epoch [16/120    avg_loss:1.541, val_acc:0.662]
Epoch [17/120    avg_loss:1.450, val_acc:0.690]
Epoch [18/120    avg_loss:1.374, val_acc:0.717]
Epoch [19/120    avg_loss:1.294, val_acc:0.702]
Epoch [20/120    avg_loss:1.213, val_acc:0.717]
Epoch [21/120    avg_loss:1.166, val_acc:0.740]
Epoch [22/120    avg_loss:1.093, val_acc:0.748]
Epoch [23/120    avg_loss:0.993, val_acc:0.781]
Epoch [24/120    avg_loss:0.944, val_acc:0.773]
Epoch [25/120    avg_loss:0.884, val_acc:0.785]
Epoch [26/120    avg_loss:0.939, val_acc:0.779]
Epoch [27/120    avg_loss:0.820, val_acc:0.794]
Epoch [28/120    avg_loss:0.786, val_acc:0.781]
Epoch [29/120    avg_loss:0.798, val_acc:0.796]
Epoch [30/120    avg_loss:0.731, val_acc:0.777]
Epoch [31/120    avg_loss:0.663, val_acc:0.808]
Epoch [32/120    avg_loss:0.643, val_acc:0.802]
Epoch [33/120    avg_loss:0.602, val_acc:0.838]
Epoch [34/120    avg_loss:0.565, val_acc:0.854]
Epoch [35/120    avg_loss:0.563, val_acc:0.840]
Epoch [36/120    avg_loss:0.540, val_acc:0.860]
Epoch [37/120    avg_loss:0.496, val_acc:0.904]
Epoch [38/120    avg_loss:0.476, val_acc:0.921]
Epoch [39/120    avg_loss:0.475, val_acc:0.856]
Epoch [40/120    avg_loss:0.467, val_acc:0.938]
Epoch [41/120    avg_loss:0.422, val_acc:0.919]
Epoch [42/120    avg_loss:0.418, val_acc:0.933]
Epoch [43/120    avg_loss:0.374, val_acc:0.938]
Epoch [44/120    avg_loss:0.390, val_acc:0.921]
Epoch [45/120    avg_loss:0.358, val_acc:0.944]
Epoch [46/120    avg_loss:0.369, val_acc:0.935]
Epoch [47/120    avg_loss:0.344, val_acc:0.923]
Epoch [48/120    avg_loss:0.292, val_acc:0.940]
Epoch [49/120    avg_loss:0.323, val_acc:0.942]
Epoch [50/120    avg_loss:0.401, val_acc:0.929]
Epoch [51/120    avg_loss:0.322, val_acc:0.942]
Epoch [52/120    avg_loss:0.284, val_acc:0.948]
Epoch [53/120    avg_loss:0.273, val_acc:0.944]
Epoch [54/120    avg_loss:0.288, val_acc:0.969]
Epoch [55/120    avg_loss:0.299, val_acc:0.948]
Epoch [56/120    avg_loss:0.293, val_acc:0.956]
Epoch [57/120    avg_loss:0.336, val_acc:0.950]
Epoch [58/120    avg_loss:0.301, val_acc:0.925]
Epoch [59/120    avg_loss:0.304, val_acc:0.965]
Epoch [60/120    avg_loss:0.253, val_acc:0.933]
Epoch [61/120    avg_loss:0.270, val_acc:0.933]
Epoch [62/120    avg_loss:0.280, val_acc:0.965]
Epoch [63/120    avg_loss:0.248, val_acc:0.933]
Epoch [64/120    avg_loss:0.258, val_acc:0.956]
Epoch [65/120    avg_loss:0.216, val_acc:0.963]
Epoch [66/120    avg_loss:0.189, val_acc:0.971]
Epoch [67/120    avg_loss:0.179, val_acc:0.977]
Epoch [68/120    avg_loss:0.179, val_acc:0.958]
Epoch [69/120    avg_loss:0.239, val_acc:0.969]
Epoch [70/120    avg_loss:0.197, val_acc:0.963]
Epoch [71/120    avg_loss:0.166, val_acc:0.954]
Epoch [72/120    avg_loss:0.178, val_acc:0.969]
Epoch [73/120    avg_loss:0.187, val_acc:0.975]
Epoch [74/120    avg_loss:0.201, val_acc:0.967]
Epoch [75/120    avg_loss:0.120, val_acc:0.973]
Epoch [76/120    avg_loss:0.138, val_acc:0.965]
Epoch [77/120    avg_loss:0.149, val_acc:0.973]
Epoch [78/120    avg_loss:0.116, val_acc:0.969]
Epoch [79/120    avg_loss:0.125, val_acc:0.975]
Epoch [80/120    avg_loss:0.116, val_acc:0.979]
Epoch [81/120    avg_loss:0.153, val_acc:0.967]
Epoch [82/120    avg_loss:0.127, val_acc:0.971]
Epoch [83/120    avg_loss:0.098, val_acc:0.988]
Epoch [84/120    avg_loss:0.084, val_acc:0.985]
Epoch [85/120    avg_loss:0.083, val_acc:0.975]
Epoch [86/120    avg_loss:0.097, val_acc:0.977]
Epoch [87/120    avg_loss:0.083, val_acc:0.969]
Epoch [88/120    avg_loss:0.080, val_acc:0.975]
Epoch [89/120    avg_loss:0.139, val_acc:0.946]
Epoch [90/120    avg_loss:0.120, val_acc:0.973]
Epoch [91/120    avg_loss:0.165, val_acc:0.938]
Epoch [92/120    avg_loss:0.155, val_acc:0.973]
Epoch [93/120    avg_loss:0.099, val_acc:0.975]
Epoch [94/120    avg_loss:0.100, val_acc:0.988]
Epoch [95/120    avg_loss:0.073, val_acc:0.983]
Epoch [96/120    avg_loss:0.069, val_acc:0.988]
Epoch [97/120    avg_loss:0.074, val_acc:0.979]
Epoch [98/120    avg_loss:0.066, val_acc:0.985]
Epoch [99/120    avg_loss:0.075, val_acc:0.990]
Epoch [100/120    avg_loss:0.063, val_acc:0.983]
Epoch [101/120    avg_loss:0.076, val_acc:0.973]
Epoch [102/120    avg_loss:0.092, val_acc:0.981]
Epoch [103/120    avg_loss:0.072, val_acc:0.983]
Epoch [104/120    avg_loss:0.059, val_acc:0.975]
Epoch [105/120    avg_loss:0.060, val_acc:0.990]
Epoch [106/120    avg_loss:0.060, val_acc:0.992]
Epoch [107/120    avg_loss:0.058, val_acc:0.988]
Epoch [108/120    avg_loss:0.070, val_acc:0.988]
Epoch [109/120    avg_loss:0.049, val_acc:0.988]
Epoch [110/120    avg_loss:0.037, val_acc:0.992]
Epoch [111/120    avg_loss:0.044, val_acc:0.981]
Epoch [112/120    avg_loss:0.037, val_acc:0.985]
Epoch [113/120    avg_loss:0.037, val_acc:0.992]
Epoch [114/120    avg_loss:0.040, val_acc:0.988]
Epoch [115/120    avg_loss:0.037, val_acc:0.990]
Epoch [116/120    avg_loss:0.057, val_acc:0.981]
Epoch [117/120    avg_loss:0.119, val_acc:0.975]
Epoch [118/120    avg_loss:0.120, val_acc:0.979]
Epoch [119/120    avg_loss:0.066, val_acc:0.981]
Epoch [120/120    avg_loss:0.089, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 0.99486427 0.97117517 0.98678414 0.9475891  0.93040293
 0.98329356 0.92571429 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9893180248473725
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19a733eb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.640, val_acc:0.077]
Epoch [2/120    avg_loss:2.542, val_acc:0.144]
Epoch [3/120    avg_loss:2.447, val_acc:0.338]
Epoch [4/120    avg_loss:2.362, val_acc:0.352]
Epoch [5/120    avg_loss:2.286, val_acc:0.365]
Epoch [6/120    avg_loss:2.206, val_acc:0.392]
Epoch [7/120    avg_loss:2.132, val_acc:0.448]
Epoch [8/120    avg_loss:2.049, val_acc:0.492]
Epoch [9/120    avg_loss:1.995, val_acc:0.556]
Epoch [10/120    avg_loss:1.910, val_acc:0.554]
Epoch [11/120    avg_loss:1.854, val_acc:0.558]
Epoch [12/120    avg_loss:1.786, val_acc:0.579]
Epoch [13/120    avg_loss:1.701, val_acc:0.581]
Epoch [14/120    avg_loss:1.624, val_acc:0.606]
Epoch [15/120    avg_loss:1.569, val_acc:0.596]
Epoch [16/120    avg_loss:1.488, val_acc:0.648]
Epoch [17/120    avg_loss:1.420, val_acc:0.675]
Epoch [18/120    avg_loss:1.371, val_acc:0.677]
Epoch [19/120    avg_loss:1.325, val_acc:0.694]
Epoch [20/120    avg_loss:1.229, val_acc:0.717]
Epoch [21/120    avg_loss:1.169, val_acc:0.735]
Epoch [22/120    avg_loss:1.139, val_acc:0.729]
Epoch [23/120    avg_loss:1.095, val_acc:0.742]
Epoch [24/120    avg_loss:1.022, val_acc:0.738]
Epoch [25/120    avg_loss:1.027, val_acc:0.748]
Epoch [26/120    avg_loss:0.952, val_acc:0.771]
Epoch [27/120    avg_loss:0.912, val_acc:0.806]
Epoch [28/120    avg_loss:0.868, val_acc:0.790]
Epoch [29/120    avg_loss:0.796, val_acc:0.783]
Epoch [30/120    avg_loss:0.783, val_acc:0.790]
Epoch [31/120    avg_loss:0.744, val_acc:0.794]
Epoch [32/120    avg_loss:0.723, val_acc:0.777]
Epoch [33/120    avg_loss:0.677, val_acc:0.794]
Epoch [34/120    avg_loss:0.646, val_acc:0.810]
Epoch [35/120    avg_loss:0.630, val_acc:0.810]
Epoch [36/120    avg_loss:0.571, val_acc:0.817]
Epoch [37/120    avg_loss:0.549, val_acc:0.833]
Epoch [38/120    avg_loss:0.510, val_acc:0.856]
Epoch [39/120    avg_loss:0.462, val_acc:0.831]
Epoch [40/120    avg_loss:0.478, val_acc:0.933]
Epoch [41/120    avg_loss:0.423, val_acc:0.867]
Epoch [42/120    avg_loss:0.388, val_acc:0.894]
Epoch [43/120    avg_loss:0.405, val_acc:0.892]
Epoch [44/120    avg_loss:0.401, val_acc:0.915]
Epoch [45/120    avg_loss:0.392, val_acc:0.912]
Epoch [46/120    avg_loss:0.364, val_acc:0.956]
Epoch [47/120    avg_loss:0.427, val_acc:0.944]
Epoch [48/120    avg_loss:0.357, val_acc:0.929]
Epoch [49/120    avg_loss:0.361, val_acc:0.896]
Epoch [50/120    avg_loss:0.349, val_acc:0.935]
Epoch [51/120    avg_loss:0.305, val_acc:0.954]
Epoch [52/120    avg_loss:0.293, val_acc:0.933]
Epoch [53/120    avg_loss:0.251, val_acc:0.944]
Epoch [54/120    avg_loss:0.267, val_acc:0.958]
Epoch [55/120    avg_loss:0.302, val_acc:0.950]
Epoch [56/120    avg_loss:0.261, val_acc:0.944]
Epoch [57/120    avg_loss:0.315, val_acc:0.925]
Epoch [58/120    avg_loss:0.257, val_acc:0.954]
Epoch [59/120    avg_loss:0.214, val_acc:0.958]
Epoch [60/120    avg_loss:0.205, val_acc:0.950]
Epoch [61/120    avg_loss:0.208, val_acc:0.954]
Epoch [62/120    avg_loss:0.228, val_acc:0.963]
Epoch [63/120    avg_loss:0.240, val_acc:0.927]
Epoch [64/120    avg_loss:0.286, val_acc:0.965]
Epoch [65/120    avg_loss:0.242, val_acc:0.894]
Epoch [66/120    avg_loss:0.267, val_acc:0.954]
Epoch [67/120    avg_loss:0.194, val_acc:0.969]
Epoch [68/120    avg_loss:0.222, val_acc:0.948]
Epoch [69/120    avg_loss:0.164, val_acc:0.973]
Epoch [70/120    avg_loss:0.202, val_acc:0.971]
Epoch [71/120    avg_loss:0.170, val_acc:0.963]
Epoch [72/120    avg_loss:0.157, val_acc:0.967]
Epoch [73/120    avg_loss:0.164, val_acc:0.973]
Epoch [74/120    avg_loss:0.173, val_acc:0.969]
Epoch [75/120    avg_loss:0.179, val_acc:0.944]
Epoch [76/120    avg_loss:0.186, val_acc:0.929]
Epoch [77/120    avg_loss:0.167, val_acc:0.971]
Epoch [78/120    avg_loss:0.125, val_acc:0.971]
Epoch [79/120    avg_loss:0.115, val_acc:0.967]
Epoch [80/120    avg_loss:0.129, val_acc:0.971]
Epoch [81/120    avg_loss:0.098, val_acc:0.971]
Epoch [82/120    avg_loss:0.105, val_acc:0.975]
Epoch [83/120    avg_loss:0.101, val_acc:0.975]
Epoch [84/120    avg_loss:0.109, val_acc:0.979]
Epoch [85/120    avg_loss:0.126, val_acc:0.971]
Epoch [86/120    avg_loss:0.113, val_acc:0.981]
Epoch [87/120    avg_loss:0.100, val_acc:0.981]
Epoch [88/120    avg_loss:0.087, val_acc:0.975]
Epoch [89/120    avg_loss:0.087, val_acc:0.983]
Epoch [90/120    avg_loss:0.116, val_acc:0.973]
Epoch [91/120    avg_loss:0.136, val_acc:0.952]
Epoch [92/120    avg_loss:0.135, val_acc:0.973]
Epoch [93/120    avg_loss:0.116, val_acc:0.944]
Epoch [94/120    avg_loss:0.115, val_acc:0.967]
Epoch [95/120    avg_loss:0.113, val_acc:0.981]
Epoch [96/120    avg_loss:0.063, val_acc:0.971]
Epoch [97/120    avg_loss:0.080, val_acc:0.981]
Epoch [98/120    avg_loss:0.077, val_acc:0.983]
Epoch [99/120    avg_loss:0.080, val_acc:0.979]
Epoch [100/120    avg_loss:0.131, val_acc:0.983]
Epoch [101/120    avg_loss:0.072, val_acc:0.985]
Epoch [102/120    avg_loss:0.077, val_acc:0.985]
Epoch [103/120    avg_loss:0.073, val_acc:0.983]
Epoch [104/120    avg_loss:0.069, val_acc:0.979]
Epoch [105/120    avg_loss:0.075, val_acc:0.988]
Epoch [106/120    avg_loss:0.072, val_acc:0.988]
Epoch [107/120    avg_loss:0.059, val_acc:0.985]
Epoch [108/120    avg_loss:0.054, val_acc:0.983]
Epoch [109/120    avg_loss:0.038, val_acc:0.981]
Epoch [110/120    avg_loss:0.053, val_acc:0.977]
Epoch [111/120    avg_loss:0.048, val_acc:0.973]
Epoch [112/120    avg_loss:0.095, val_acc:0.969]
Epoch [113/120    avg_loss:0.091, val_acc:0.971]
Epoch [114/120    avg_loss:0.110, val_acc:0.983]
Epoch [115/120    avg_loss:0.097, val_acc:0.969]
Epoch [116/120    avg_loss:0.119, val_acc:0.958]
Epoch [117/120    avg_loss:0.086, val_acc:0.988]
Epoch [118/120    avg_loss:0.062, val_acc:0.983]
Epoch [119/120    avg_loss:0.052, val_acc:0.975]
Epoch [120/120    avg_loss:0.057, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219   8   0   0   3   0   0   0   0   0   0]
 [  0   0   0   0 187  40   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   6   0   0   0   0   0   0 382   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.4861407249467

F1 scores:
[       nan 0.99124088 0.98206278 0.97550111 0.88625592 0.87878788
 0.98564593 0.93989071 0.99220779 1.         1.         1.
 1.         1.        ]

Kappa:
0.9831491611218319
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa01e7e7b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.617, val_acc:0.085]
Epoch [2/120    avg_loss:2.550, val_acc:0.273]
Epoch [3/120    avg_loss:2.471, val_acc:0.273]
Epoch [4/120    avg_loss:2.398, val_acc:0.275]
Epoch [5/120    avg_loss:2.342, val_acc:0.300]
Epoch [6/120    avg_loss:2.271, val_acc:0.365]
Epoch [7/120    avg_loss:2.214, val_acc:0.415]
Epoch [8/120    avg_loss:2.151, val_acc:0.446]
Epoch [9/120    avg_loss:2.086, val_acc:0.508]
Epoch [10/120    avg_loss:2.016, val_acc:0.540]
Epoch [11/120    avg_loss:1.945, val_acc:0.560]
Epoch [12/120    avg_loss:1.849, val_acc:0.565]
Epoch [13/120    avg_loss:1.779, val_acc:0.585]
Epoch [14/120    avg_loss:1.700, val_acc:0.588]
Epoch [15/120    avg_loss:1.604, val_acc:0.627]
Epoch [16/120    avg_loss:1.500, val_acc:0.629]
Epoch [17/120    avg_loss:1.433, val_acc:0.652]
Epoch [18/120    avg_loss:1.343, val_acc:0.669]
Epoch [19/120    avg_loss:1.282, val_acc:0.669]
Epoch [20/120    avg_loss:1.192, val_acc:0.702]
Epoch [21/120    avg_loss:1.127, val_acc:0.698]
Epoch [22/120    avg_loss:1.077, val_acc:0.685]
Epoch [23/120    avg_loss:1.033, val_acc:0.729]
Epoch [24/120    avg_loss:0.946, val_acc:0.721]
Epoch [25/120    avg_loss:0.915, val_acc:0.750]
Epoch [26/120    avg_loss:0.827, val_acc:0.754]
Epoch [27/120    avg_loss:0.778, val_acc:0.794]
Epoch [28/120    avg_loss:0.779, val_acc:0.760]
Epoch [29/120    avg_loss:0.744, val_acc:0.821]
Epoch [30/120    avg_loss:0.692, val_acc:0.829]
Epoch [31/120    avg_loss:0.658, val_acc:0.883]
Epoch [32/120    avg_loss:0.639, val_acc:0.879]
Epoch [33/120    avg_loss:0.645, val_acc:0.831]
Epoch [34/120    avg_loss:0.546, val_acc:0.898]
Epoch [35/120    avg_loss:0.535, val_acc:0.927]
Epoch [36/120    avg_loss:0.495, val_acc:0.933]
Epoch [37/120    avg_loss:0.437, val_acc:0.935]
Epoch [38/120    avg_loss:0.441, val_acc:0.940]
Epoch [39/120    avg_loss:0.394, val_acc:0.917]
Epoch [40/120    avg_loss:0.408, val_acc:0.917]
Epoch [41/120    avg_loss:0.376, val_acc:0.935]
Epoch [42/120    avg_loss:0.362, val_acc:0.929]
Epoch [43/120    avg_loss:0.391, val_acc:0.960]
Epoch [44/120    avg_loss:0.341, val_acc:0.946]
Epoch [45/120    avg_loss:0.298, val_acc:0.944]
Epoch [46/120    avg_loss:0.314, val_acc:0.954]
Epoch [47/120    avg_loss:0.309, val_acc:0.931]
Epoch [48/120    avg_loss:0.356, val_acc:0.915]
Epoch [49/120    avg_loss:0.366, val_acc:0.938]
Epoch [50/120    avg_loss:0.339, val_acc:0.931]
Epoch [51/120    avg_loss:0.375, val_acc:0.917]
Epoch [52/120    avg_loss:0.348, val_acc:0.919]
Epoch [53/120    avg_loss:0.299, val_acc:0.948]
Epoch [54/120    avg_loss:0.294, val_acc:0.931]
Epoch [55/120    avg_loss:0.274, val_acc:0.952]
Epoch [56/120    avg_loss:0.263, val_acc:0.946]
Epoch [57/120    avg_loss:0.234, val_acc:0.971]
Epoch [58/120    avg_loss:0.194, val_acc:0.969]
Epoch [59/120    avg_loss:0.190, val_acc:0.960]
Epoch [60/120    avg_loss:0.196, val_acc:0.967]
Epoch [61/120    avg_loss:0.196, val_acc:0.967]
Epoch [62/120    avg_loss:0.185, val_acc:0.967]
Epoch [63/120    avg_loss:0.188, val_acc:0.969]
Epoch [64/120    avg_loss:0.177, val_acc:0.969]
Epoch [65/120    avg_loss:0.173, val_acc:0.971]
Epoch [66/120    avg_loss:0.175, val_acc:0.969]
Epoch [67/120    avg_loss:0.181, val_acc:0.971]
Epoch [68/120    avg_loss:0.188, val_acc:0.971]
Epoch [69/120    avg_loss:0.175, val_acc:0.971]
Epoch [70/120    avg_loss:0.174, val_acc:0.973]
Epoch [71/120    avg_loss:0.178, val_acc:0.977]
Epoch [72/120    avg_loss:0.188, val_acc:0.973]
Epoch [73/120    avg_loss:0.184, val_acc:0.975]
Epoch [74/120    avg_loss:0.163, val_acc:0.971]
Epoch [75/120    avg_loss:0.182, val_acc:0.973]
Epoch [76/120    avg_loss:0.181, val_acc:0.971]
Epoch [77/120    avg_loss:0.171, val_acc:0.971]
Epoch [78/120    avg_loss:0.155, val_acc:0.971]
Epoch [79/120    avg_loss:0.187, val_acc:0.971]
Epoch [80/120    avg_loss:0.156, val_acc:0.973]
Epoch [81/120    avg_loss:0.158, val_acc:0.971]
Epoch [82/120    avg_loss:0.163, val_acc:0.973]
Epoch [83/120    avg_loss:0.152, val_acc:0.975]
Epoch [84/120    avg_loss:0.171, val_acc:0.971]
Epoch [85/120    avg_loss:0.154, val_acc:0.971]
Epoch [86/120    avg_loss:0.151, val_acc:0.973]
Epoch [87/120    avg_loss:0.142, val_acc:0.975]
Epoch [88/120    avg_loss:0.165, val_acc:0.975]
Epoch [89/120    avg_loss:0.146, val_acc:0.975]
Epoch [90/120    avg_loss:0.166, val_acc:0.975]
Epoch [91/120    avg_loss:0.147, val_acc:0.975]
Epoch [92/120    avg_loss:0.147, val_acc:0.975]
Epoch [93/120    avg_loss:0.163, val_acc:0.975]
Epoch [94/120    avg_loss:0.156, val_acc:0.975]
Epoch [95/120    avg_loss:0.152, val_acc:0.975]
Epoch [96/120    avg_loss:0.158, val_acc:0.975]
Epoch [97/120    avg_loss:0.159, val_acc:0.975]
Epoch [98/120    avg_loss:0.153, val_acc:0.975]
Epoch [99/120    avg_loss:0.160, val_acc:0.975]
Epoch [100/120    avg_loss:0.145, val_acc:0.975]
Epoch [101/120    avg_loss:0.144, val_acc:0.975]
Epoch [102/120    avg_loss:0.172, val_acc:0.975]
Epoch [103/120    avg_loss:0.155, val_acc:0.975]
Epoch [104/120    avg_loss:0.158, val_acc:0.975]
Epoch [105/120    avg_loss:0.150, val_acc:0.975]
Epoch [106/120    avg_loss:0.151, val_acc:0.975]
Epoch [107/120    avg_loss:0.148, val_acc:0.975]
Epoch [108/120    avg_loss:0.148, val_acc:0.975]
Epoch [109/120    avg_loss:0.151, val_acc:0.975]
Epoch [110/120    avg_loss:0.165, val_acc:0.975]
Epoch [111/120    avg_loss:0.148, val_acc:0.975]
Epoch [112/120    avg_loss:0.147, val_acc:0.975]
Epoch [113/120    avg_loss:0.144, val_acc:0.975]
Epoch [114/120    avg_loss:0.149, val_acc:0.975]
Epoch [115/120    avg_loss:0.158, val_acc:0.975]
Epoch [116/120    avg_loss:0.145, val_acc:0.975]
Epoch [117/120    avg_loss:0.143, val_acc:0.975]
Epoch [118/120    avg_loss:0.173, val_acc:0.975]
Epoch [119/120    avg_loss:0.153, val_acc:0.975]
Epoch [120/120    avg_loss:0.166, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 199  27   0   0   0   0   0   0   1   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.9941349  0.97333333 0.99782135 0.92343387 0.90095847
 0.98086124 0.93181818 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9874207165318161
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6d66c19b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.140]
Epoch [2/120    avg_loss:2.546, val_acc:0.323]
Epoch [3/120    avg_loss:2.468, val_acc:0.325]
Epoch [4/120    avg_loss:2.396, val_acc:0.325]
Epoch [5/120    avg_loss:2.339, val_acc:0.325]
Epoch [6/120    avg_loss:2.275, val_acc:0.327]
Epoch [7/120    avg_loss:2.217, val_acc:0.335]
Epoch [8/120    avg_loss:2.158, val_acc:0.356]
Epoch [9/120    avg_loss:2.091, val_acc:0.408]
Epoch [10/120    avg_loss:2.021, val_acc:0.500]
Epoch [11/120    avg_loss:1.942, val_acc:0.535]
Epoch [12/120    avg_loss:1.888, val_acc:0.550]
Epoch [13/120    avg_loss:1.824, val_acc:0.573]
Epoch [14/120    avg_loss:1.752, val_acc:0.585]
Epoch [15/120    avg_loss:1.692, val_acc:0.619]
Epoch [16/120    avg_loss:1.624, val_acc:0.635]
Epoch [17/120    avg_loss:1.555, val_acc:0.644]
Epoch [18/120    avg_loss:1.486, val_acc:0.635]
Epoch [19/120    avg_loss:1.413, val_acc:0.704]
Epoch [20/120    avg_loss:1.302, val_acc:0.729]
Epoch [21/120    avg_loss:1.216, val_acc:0.715]
Epoch [22/120    avg_loss:1.137, val_acc:0.760]
Epoch [23/120    avg_loss:1.091, val_acc:0.760]
Epoch [24/120    avg_loss:1.044, val_acc:0.762]
Epoch [25/120    avg_loss:0.931, val_acc:0.812]
Epoch [26/120    avg_loss:0.859, val_acc:0.796]
Epoch [27/120    avg_loss:0.881, val_acc:0.765]
Epoch [28/120    avg_loss:0.770, val_acc:0.885]
Epoch [29/120    avg_loss:0.705, val_acc:0.898]
Epoch [30/120    avg_loss:0.654, val_acc:0.800]
Epoch [31/120    avg_loss:0.593, val_acc:0.923]
Epoch [32/120    avg_loss:0.603, val_acc:0.900]
Epoch [33/120    avg_loss:0.533, val_acc:0.863]
Epoch [34/120    avg_loss:0.512, val_acc:0.898]
Epoch [35/120    avg_loss:0.476, val_acc:0.921]
Epoch [36/120    avg_loss:0.466, val_acc:0.867]
Epoch [37/120    avg_loss:0.484, val_acc:0.887]
Epoch [38/120    avg_loss:0.447, val_acc:0.931]
Epoch [39/120    avg_loss:0.431, val_acc:0.935]
Epoch [40/120    avg_loss:0.358, val_acc:0.919]
Epoch [41/120    avg_loss:0.358, val_acc:0.929]
Epoch [42/120    avg_loss:0.318, val_acc:0.927]
Epoch [43/120    avg_loss:0.379, val_acc:0.923]
Epoch [44/120    avg_loss:0.334, val_acc:0.929]
Epoch [45/120    avg_loss:0.299, val_acc:0.960]
Epoch [46/120    avg_loss:0.331, val_acc:0.940]
Epoch [47/120    avg_loss:0.304, val_acc:0.948]
Epoch [48/120    avg_loss:0.311, val_acc:0.917]
Epoch [49/120    avg_loss:0.324, val_acc:0.940]
Epoch [50/120    avg_loss:0.310, val_acc:0.900]
Epoch [51/120    avg_loss:0.280, val_acc:0.927]
Epoch [52/120    avg_loss:0.238, val_acc:0.929]
Epoch [53/120    avg_loss:0.270, val_acc:0.921]
Epoch [54/120    avg_loss:0.273, val_acc:0.946]
Epoch [55/120    avg_loss:0.279, val_acc:0.921]
Epoch [56/120    avg_loss:0.315, val_acc:0.910]
Epoch [57/120    avg_loss:0.282, val_acc:0.940]
Epoch [58/120    avg_loss:0.317, val_acc:0.921]
Epoch [59/120    avg_loss:0.274, val_acc:0.952]
Epoch [60/120    avg_loss:0.209, val_acc:0.963]
Epoch [61/120    avg_loss:0.182, val_acc:0.963]
Epoch [62/120    avg_loss:0.172, val_acc:0.963]
Epoch [63/120    avg_loss:0.178, val_acc:0.958]
Epoch [64/120    avg_loss:0.167, val_acc:0.965]
Epoch [65/120    avg_loss:0.159, val_acc:0.965]
Epoch [66/120    avg_loss:0.160, val_acc:0.967]
Epoch [67/120    avg_loss:0.148, val_acc:0.965]
Epoch [68/120    avg_loss:0.149, val_acc:0.965]
Epoch [69/120    avg_loss:0.166, val_acc:0.969]
Epoch [70/120    avg_loss:0.152, val_acc:0.969]
Epoch [71/120    avg_loss:0.148, val_acc:0.969]
Epoch [72/120    avg_loss:0.166, val_acc:0.969]
Epoch [73/120    avg_loss:0.157, val_acc:0.965]
Epoch [74/120    avg_loss:0.150, val_acc:0.967]
Epoch [75/120    avg_loss:0.148, val_acc:0.969]
Epoch [76/120    avg_loss:0.153, val_acc:0.969]
Epoch [77/120    avg_loss:0.136, val_acc:0.971]
Epoch [78/120    avg_loss:0.136, val_acc:0.969]
Epoch [79/120    avg_loss:0.148, val_acc:0.969]
Epoch [80/120    avg_loss:0.140, val_acc:0.969]
Epoch [81/120    avg_loss:0.165, val_acc:0.969]
Epoch [82/120    avg_loss:0.161, val_acc:0.965]
Epoch [83/120    avg_loss:0.147, val_acc:0.969]
Epoch [84/120    avg_loss:0.142, val_acc:0.967]
Epoch [85/120    avg_loss:0.129, val_acc:0.975]
Epoch [86/120    avg_loss:0.141, val_acc:0.975]
Epoch [87/120    avg_loss:0.146, val_acc:0.973]
Epoch [88/120    avg_loss:0.156, val_acc:0.965]
Epoch [89/120    avg_loss:0.155, val_acc:0.973]
Epoch [90/120    avg_loss:0.136, val_acc:0.973]
Epoch [91/120    avg_loss:0.141, val_acc:0.973]
Epoch [92/120    avg_loss:0.130, val_acc:0.973]
Epoch [93/120    avg_loss:0.125, val_acc:0.977]
Epoch [94/120    avg_loss:0.119, val_acc:0.977]
Epoch [95/120    avg_loss:0.124, val_acc:0.975]
Epoch [96/120    avg_loss:0.138, val_acc:0.975]
Epoch [97/120    avg_loss:0.140, val_acc:0.971]
Epoch [98/120    avg_loss:0.127, val_acc:0.973]
Epoch [99/120    avg_loss:0.116, val_acc:0.975]
Epoch [100/120    avg_loss:0.131, val_acc:0.973]
Epoch [101/120    avg_loss:0.135, val_acc:0.971]
Epoch [102/120    avg_loss:0.146, val_acc:0.979]
Epoch [103/120    avg_loss:0.132, val_acc:0.973]
Epoch [104/120    avg_loss:0.151, val_acc:0.975]
Epoch [105/120    avg_loss:0.131, val_acc:0.975]
Epoch [106/120    avg_loss:0.121, val_acc:0.971]
Epoch [107/120    avg_loss:0.129, val_acc:0.977]
Epoch [108/120    avg_loss:0.138, val_acc:0.977]
Epoch [109/120    avg_loss:0.116, val_acc:0.975]
Epoch [110/120    avg_loss:0.112, val_acc:0.977]
Epoch [111/120    avg_loss:0.133, val_acc:0.975]
Epoch [112/120    avg_loss:0.136, val_acc:0.971]
Epoch [113/120    avg_loss:0.130, val_acc:0.975]
Epoch [114/120    avg_loss:0.109, val_acc:0.975]
Epoch [115/120    avg_loss:0.125, val_acc:0.979]
Epoch [116/120    avg_loss:0.123, val_acc:0.977]
Epoch [117/120    avg_loss:0.113, val_acc:0.977]
Epoch [118/120    avg_loss:0.125, val_acc:0.977]
Epoch [119/120    avg_loss:0.111, val_acc:0.979]
Epoch [120/120    avg_loss:0.110, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 672   0   0   0   0  13   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 191  36   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.99042004 0.99095023 1.         0.90952381 0.88271605
 0.96941176 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9869496444480084
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f877d212b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.163]
Epoch [2/120    avg_loss:2.522, val_acc:0.175]
Epoch [3/120    avg_loss:2.438, val_acc:0.373]
Epoch [4/120    avg_loss:2.370, val_acc:0.440]
Epoch [5/120    avg_loss:2.312, val_acc:0.504]
Epoch [6/120    avg_loss:2.253, val_acc:0.527]
Epoch [7/120    avg_loss:2.184, val_acc:0.548]
Epoch [8/120    avg_loss:2.111, val_acc:0.546]
Epoch [9/120    avg_loss:2.055, val_acc:0.554]
Epoch [10/120    avg_loss:1.978, val_acc:0.552]
Epoch [11/120    avg_loss:1.906, val_acc:0.581]
Epoch [12/120    avg_loss:1.837, val_acc:0.608]
Epoch [13/120    avg_loss:1.785, val_acc:0.640]
Epoch [14/120    avg_loss:1.702, val_acc:0.665]
Epoch [15/120    avg_loss:1.621, val_acc:0.683]
Epoch [16/120    avg_loss:1.556, val_acc:0.679]
Epoch [17/120    avg_loss:1.479, val_acc:0.706]
Epoch [18/120    avg_loss:1.409, val_acc:0.706]
Epoch [19/120    avg_loss:1.309, val_acc:0.698]
Epoch [20/120    avg_loss:1.256, val_acc:0.654]
Epoch [21/120    avg_loss:1.178, val_acc:0.713]
Epoch [22/120    avg_loss:1.101, val_acc:0.727]
Epoch [23/120    avg_loss:1.004, val_acc:0.754]
Epoch [24/120    avg_loss:0.998, val_acc:0.735]
Epoch [25/120    avg_loss:0.893, val_acc:0.771]
Epoch [26/120    avg_loss:0.918, val_acc:0.775]
Epoch [27/120    avg_loss:0.874, val_acc:0.762]
Epoch [28/120    avg_loss:0.828, val_acc:0.790]
Epoch [29/120    avg_loss:0.757, val_acc:0.823]
Epoch [30/120    avg_loss:0.693, val_acc:0.806]
Epoch [31/120    avg_loss:0.654, val_acc:0.838]
Epoch [32/120    avg_loss:0.634, val_acc:0.856]
Epoch [33/120    avg_loss:0.608, val_acc:0.865]
Epoch [34/120    avg_loss:0.573, val_acc:0.858]
Epoch [35/120    avg_loss:0.552, val_acc:0.877]
Epoch [36/120    avg_loss:0.541, val_acc:0.929]
Epoch [37/120    avg_loss:0.509, val_acc:0.819]
Epoch [38/120    avg_loss:0.540, val_acc:0.850]
Epoch [39/120    avg_loss:0.487, val_acc:0.912]
Epoch [40/120    avg_loss:0.456, val_acc:0.871]
Epoch [41/120    avg_loss:0.481, val_acc:0.915]
Epoch [42/120    avg_loss:0.420, val_acc:0.933]
Epoch [43/120    avg_loss:0.382, val_acc:0.944]
Epoch [44/120    avg_loss:0.352, val_acc:0.963]
Epoch [45/120    avg_loss:0.346, val_acc:0.921]
Epoch [46/120    avg_loss:0.396, val_acc:0.925]
Epoch [47/120    avg_loss:0.335, val_acc:0.942]
Epoch [48/120    avg_loss:0.328, val_acc:0.938]
Epoch [49/120    avg_loss:0.314, val_acc:0.950]
Epoch [50/120    avg_loss:0.350, val_acc:0.950]
Epoch [51/120    avg_loss:0.275, val_acc:0.938]
Epoch [52/120    avg_loss:0.275, val_acc:0.963]
Epoch [53/120    avg_loss:0.255, val_acc:0.942]
Epoch [54/120    avg_loss:0.263, val_acc:0.938]
Epoch [55/120    avg_loss:0.295, val_acc:0.931]
Epoch [56/120    avg_loss:0.270, val_acc:0.954]
Epoch [57/120    avg_loss:0.237, val_acc:0.950]
Epoch [58/120    avg_loss:0.249, val_acc:0.952]
Epoch [59/120    avg_loss:0.210, val_acc:0.948]
Epoch [60/120    avg_loss:0.216, val_acc:0.960]
Epoch [61/120    avg_loss:0.245, val_acc:0.973]
Epoch [62/120    avg_loss:0.200, val_acc:0.979]
Epoch [63/120    avg_loss:0.203, val_acc:0.977]
Epoch [64/120    avg_loss:0.166, val_acc:0.973]
Epoch [65/120    avg_loss:0.172, val_acc:0.990]
Epoch [66/120    avg_loss:0.161, val_acc:0.973]
Epoch [67/120    avg_loss:0.175, val_acc:0.973]
Epoch [68/120    avg_loss:0.150, val_acc:0.960]
Epoch [69/120    avg_loss:0.195, val_acc:0.940]
Epoch [70/120    avg_loss:0.271, val_acc:0.973]
Epoch [71/120    avg_loss:0.254, val_acc:0.938]
Epoch [72/120    avg_loss:0.278, val_acc:0.960]
Epoch [73/120    avg_loss:0.179, val_acc:0.983]
Epoch [74/120    avg_loss:0.162, val_acc:0.977]
Epoch [75/120    avg_loss:0.184, val_acc:0.933]
Epoch [76/120    avg_loss:0.187, val_acc:0.960]
Epoch [77/120    avg_loss:0.234, val_acc:0.940]
Epoch [78/120    avg_loss:0.197, val_acc:0.971]
Epoch [79/120    avg_loss:0.137, val_acc:0.975]
Epoch [80/120    avg_loss:0.154, val_acc:0.977]
Epoch [81/120    avg_loss:0.130, val_acc:0.977]
Epoch [82/120    avg_loss:0.123, val_acc:0.979]
Epoch [83/120    avg_loss:0.124, val_acc:0.979]
Epoch [84/120    avg_loss:0.110, val_acc:0.979]
Epoch [85/120    avg_loss:0.124, val_acc:0.979]
Epoch [86/120    avg_loss:0.103, val_acc:0.979]
Epoch [87/120    avg_loss:0.101, val_acc:0.981]
Epoch [88/120    avg_loss:0.113, val_acc:0.988]
Epoch [89/120    avg_loss:0.098, val_acc:0.988]
Epoch [90/120    avg_loss:0.104, val_acc:0.985]
Epoch [91/120    avg_loss:0.090, val_acc:0.985]
Epoch [92/120    avg_loss:0.103, val_acc:0.985]
Epoch [93/120    avg_loss:0.111, val_acc:0.988]
Epoch [94/120    avg_loss:0.100, val_acc:0.988]
Epoch [95/120    avg_loss:0.103, val_acc:0.988]
Epoch [96/120    avg_loss:0.108, val_acc:0.990]
Epoch [97/120    avg_loss:0.101, val_acc:0.990]
Epoch [98/120    avg_loss:0.098, val_acc:0.988]
Epoch [99/120    avg_loss:0.104, val_acc:0.990]
Epoch [100/120    avg_loss:0.096, val_acc:0.988]
Epoch [101/120    avg_loss:0.093, val_acc:0.988]
Epoch [102/120    avg_loss:0.104, val_acc:0.988]
Epoch [103/120    avg_loss:0.092, val_acc:0.988]
Epoch [104/120    avg_loss:0.100, val_acc:0.988]
Epoch [105/120    avg_loss:0.094, val_acc:0.988]
Epoch [106/120    avg_loss:0.090, val_acc:0.988]
Epoch [107/120    avg_loss:0.100, val_acc:0.988]
Epoch [108/120    avg_loss:0.091, val_acc:0.988]
Epoch [109/120    avg_loss:0.101, val_acc:0.988]
Epoch [110/120    avg_loss:0.100, val_acc:0.988]
Epoch [111/120    avg_loss:0.096, val_acc:0.988]
Epoch [112/120    avg_loss:0.110, val_acc:0.988]
Epoch [113/120    avg_loss:0.099, val_acc:0.988]
Epoch [114/120    avg_loss:0.098, val_acc:0.988]
Epoch [115/120    avg_loss:0.096, val_acc:0.988]
Epoch [116/120    avg_loss:0.098, val_acc:0.988]
Epoch [117/120    avg_loss:0.103, val_acc:0.988]
Epoch [118/120    avg_loss:0.092, val_acc:0.988]
Epoch [119/120    avg_loss:0.091, val_acc:0.988]
Epoch [120/120    avg_loss:0.100, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 200  26   0   0   0   0   0   0   1   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 0.99486427 0.98206278 1.         0.93023256 0.90734824
 0.98329356 0.95555556 1.         1.         1.         0.9986755
 0.99779249 1.        ]

Kappa:
0.9890827125456555
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8bcffeab38>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.127]
Epoch [2/120    avg_loss:2.539, val_acc:0.287]
Epoch [3/120    avg_loss:2.462, val_acc:0.329]
Epoch [4/120    avg_loss:2.389, val_acc:0.344]
Epoch [5/120    avg_loss:2.332, val_acc:0.402]
Epoch [6/120    avg_loss:2.270, val_acc:0.469]
Epoch [7/120    avg_loss:2.215, val_acc:0.515]
Epoch [8/120    avg_loss:2.154, val_acc:0.512]
Epoch [9/120    avg_loss:2.090, val_acc:0.487]
Epoch [10/120    avg_loss:2.028, val_acc:0.512]
Epoch [11/120    avg_loss:1.950, val_acc:0.517]
Epoch [12/120    avg_loss:1.892, val_acc:0.529]
Epoch [13/120    avg_loss:1.787, val_acc:0.548]
Epoch [14/120    avg_loss:1.716, val_acc:0.562]
Epoch [15/120    avg_loss:1.623, val_acc:0.573]
Epoch [16/120    avg_loss:1.543, val_acc:0.594]
Epoch [17/120    avg_loss:1.475, val_acc:0.646]
Epoch [18/120    avg_loss:1.409, val_acc:0.667]
Epoch [19/120    avg_loss:1.315, val_acc:0.721]
Epoch [20/120    avg_loss:1.254, val_acc:0.694]
Epoch [21/120    avg_loss:1.180, val_acc:0.715]
Epoch [22/120    avg_loss:1.106, val_acc:0.769]
Epoch [23/120    avg_loss:0.999, val_acc:0.771]
Epoch [24/120    avg_loss:0.984, val_acc:0.762]
Epoch [25/120    avg_loss:0.871, val_acc:0.808]
Epoch [26/120    avg_loss:0.830, val_acc:0.852]
Epoch [27/120    avg_loss:0.787, val_acc:0.838]
Epoch [28/120    avg_loss:0.746, val_acc:0.823]
Epoch [29/120    avg_loss:0.700, val_acc:0.877]
Epoch [30/120    avg_loss:0.626, val_acc:0.858]
Epoch [31/120    avg_loss:0.588, val_acc:0.910]
Epoch [32/120    avg_loss:0.591, val_acc:0.842]
Epoch [33/120    avg_loss:0.569, val_acc:0.871]
Epoch [34/120    avg_loss:0.510, val_acc:0.917]
Epoch [35/120    avg_loss:0.506, val_acc:0.927]
Epoch [36/120    avg_loss:0.498, val_acc:0.927]
Epoch [37/120    avg_loss:0.455, val_acc:0.873]
Epoch [38/120    avg_loss:0.405, val_acc:0.933]
Epoch [39/120    avg_loss:0.380, val_acc:0.952]
Epoch [40/120    avg_loss:0.353, val_acc:0.935]
Epoch [41/120    avg_loss:0.374, val_acc:0.956]
Epoch [42/120    avg_loss:0.344, val_acc:0.946]
Epoch [43/120    avg_loss:0.353, val_acc:0.960]
Epoch [44/120    avg_loss:0.294, val_acc:0.960]
Epoch [45/120    avg_loss:0.295, val_acc:0.965]
Epoch [46/120    avg_loss:0.312, val_acc:0.954]
Epoch [47/120    avg_loss:0.287, val_acc:0.967]
Epoch [48/120    avg_loss:0.286, val_acc:0.950]
Epoch [49/120    avg_loss:0.304, val_acc:0.960]
Epoch [50/120    avg_loss:0.297, val_acc:0.952]
Epoch [51/120    avg_loss:0.287, val_acc:0.971]
Epoch [52/120    avg_loss:0.218, val_acc:0.944]
Epoch [53/120    avg_loss:0.238, val_acc:0.971]
Epoch [54/120    avg_loss:0.223, val_acc:0.938]
Epoch [55/120    avg_loss:0.211, val_acc:0.985]
Epoch [56/120    avg_loss:0.187, val_acc:0.985]
Epoch [57/120    avg_loss:0.192, val_acc:0.977]
Epoch [58/120    avg_loss:0.178, val_acc:0.973]
Epoch [59/120    avg_loss:0.198, val_acc:0.977]
Epoch [60/120    avg_loss:0.188, val_acc:0.977]
Epoch [61/120    avg_loss:0.189, val_acc:0.981]
Epoch [62/120    avg_loss:0.168, val_acc:0.977]
Epoch [63/120    avg_loss:0.176, val_acc:0.971]
Epoch [64/120    avg_loss:0.228, val_acc:0.981]
Epoch [65/120    avg_loss:0.201, val_acc:0.965]
Epoch [66/120    avg_loss:0.161, val_acc:0.981]
Epoch [67/120    avg_loss:0.173, val_acc:0.979]
Epoch [68/120    avg_loss:0.153, val_acc:0.985]
Epoch [69/120    avg_loss:0.119, val_acc:0.981]
Epoch [70/120    avg_loss:0.128, val_acc:0.990]
Epoch [71/120    avg_loss:0.151, val_acc:0.971]
Epoch [72/120    avg_loss:0.127, val_acc:0.983]
Epoch [73/120    avg_loss:0.107, val_acc:0.992]
Epoch [74/120    avg_loss:0.106, val_acc:0.985]
Epoch [75/120    avg_loss:0.118, val_acc:0.985]
Epoch [76/120    avg_loss:0.128, val_acc:0.985]
Epoch [77/120    avg_loss:0.114, val_acc:0.988]
Epoch [78/120    avg_loss:0.111, val_acc:0.975]
Epoch [79/120    avg_loss:0.101, val_acc:0.975]
Epoch [80/120    avg_loss:0.132, val_acc:0.988]
Epoch [81/120    avg_loss:0.074, val_acc:0.990]
Epoch [82/120    avg_loss:0.086, val_acc:0.996]
Epoch [83/120    avg_loss:0.072, val_acc:0.996]
Epoch [84/120    avg_loss:0.064, val_acc:0.994]
Epoch [85/120    avg_loss:0.069, val_acc:0.988]
Epoch [86/120    avg_loss:0.052, val_acc:0.992]
Epoch [87/120    avg_loss:0.065, val_acc:1.000]
Epoch [88/120    avg_loss:0.064, val_acc:0.996]
Epoch [89/120    avg_loss:0.053, val_acc:0.998]
Epoch [90/120    avg_loss:0.049, val_acc:0.996]
Epoch [91/120    avg_loss:0.047, val_acc:0.996]
Epoch [92/120    avg_loss:0.063, val_acc:1.000]
Epoch [93/120    avg_loss:0.061, val_acc:0.994]
Epoch [94/120    avg_loss:0.060, val_acc:1.000]
Epoch [95/120    avg_loss:0.049, val_acc:0.985]
Epoch [96/120    avg_loss:0.042, val_acc:0.998]
Epoch [97/120    avg_loss:0.042, val_acc:0.994]
Epoch [98/120    avg_loss:0.067, val_acc:0.973]
Epoch [99/120    avg_loss:0.164, val_acc:0.971]
Epoch [100/120    avg_loss:0.132, val_acc:0.985]
Epoch [101/120    avg_loss:0.106, val_acc:1.000]
Epoch [102/120    avg_loss:0.064, val_acc:0.998]
Epoch [103/120    avg_loss:0.064, val_acc:0.998]
Epoch [104/120    avg_loss:0.067, val_acc:0.990]
Epoch [105/120    avg_loss:0.062, val_acc:0.998]
Epoch [106/120    avg_loss:0.064, val_acc:0.992]
Epoch [107/120    avg_loss:0.065, val_acc:0.994]
Epoch [108/120    avg_loss:0.051, val_acc:0.992]
Epoch [109/120    avg_loss:0.053, val_acc:0.996]
Epoch [110/120    avg_loss:0.049, val_acc:0.996]
Epoch [111/120    avg_loss:0.038, val_acc:0.996]
Epoch [112/120    avg_loss:0.031, val_acc:0.996]
Epoch [113/120    avg_loss:0.033, val_acc:1.000]
Epoch [114/120    avg_loss:0.027, val_acc:1.000]
Epoch [115/120    avg_loss:0.024, val_acc:1.000]
Epoch [116/120    avg_loss:0.027, val_acc:1.000]
Epoch [117/120    avg_loss:0.030, val_acc:0.998]
Epoch [118/120    avg_loss:0.044, val_acc:0.994]
Epoch [119/120    avg_loss:0.056, val_acc:0.996]
Epoch [120/120    avg_loss:0.065, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.82942430703625

F1 scores:
[       nan 0.996337   1.         1.         0.99334812 0.98976109
 0.98800959 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.998101202178901
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f37b7c95b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.652, val_acc:0.060]
Epoch [2/120    avg_loss:2.561, val_acc:0.142]
Epoch [3/120    avg_loss:2.459, val_acc:0.350]
Epoch [4/120    avg_loss:2.368, val_acc:0.346]
Epoch [5/120    avg_loss:2.295, val_acc:0.333]
Epoch [6/120    avg_loss:2.229, val_acc:0.394]
Epoch [7/120    avg_loss:2.150, val_acc:0.458]
Epoch [8/120    avg_loss:2.074, val_acc:0.506]
Epoch [9/120    avg_loss:1.994, val_acc:0.567]
Epoch [10/120    avg_loss:1.916, val_acc:0.610]
Epoch [11/120    avg_loss:1.849, val_acc:0.604]
Epoch [12/120    avg_loss:1.755, val_acc:0.635]
Epoch [13/120    avg_loss:1.688, val_acc:0.658]
Epoch [14/120    avg_loss:1.587, val_acc:0.667]
Epoch [15/120    avg_loss:1.537, val_acc:0.681]
Epoch [16/120    avg_loss:1.454, val_acc:0.690]
Epoch [17/120    avg_loss:1.395, val_acc:0.694]
Epoch [18/120    avg_loss:1.298, val_acc:0.710]
Epoch [19/120    avg_loss:1.229, val_acc:0.713]
Epoch [20/120    avg_loss:1.186, val_acc:0.706]
Epoch [21/120    avg_loss:1.162, val_acc:0.723]
Epoch [22/120    avg_loss:1.063, val_acc:0.752]
Epoch [23/120    avg_loss:1.006, val_acc:0.762]
Epoch [24/120    avg_loss:0.940, val_acc:0.742]
Epoch [25/120    avg_loss:0.904, val_acc:0.787]
Epoch [26/120    avg_loss:0.838, val_acc:0.810]
Epoch [27/120    avg_loss:0.784, val_acc:0.812]
Epoch [28/120    avg_loss:0.711, val_acc:0.827]
Epoch [29/120    avg_loss:0.700, val_acc:0.838]
Epoch [30/120    avg_loss:0.690, val_acc:0.898]
Epoch [31/120    avg_loss:0.649, val_acc:0.833]
Epoch [32/120    avg_loss:0.625, val_acc:0.923]
Epoch [33/120    avg_loss:0.612, val_acc:0.929]
Epoch [34/120    avg_loss:0.536, val_acc:0.927]
Epoch [35/120    avg_loss:0.524, val_acc:0.935]
Epoch [36/120    avg_loss:0.452, val_acc:0.940]
Epoch [37/120    avg_loss:0.474, val_acc:0.915]
Epoch [38/120    avg_loss:0.506, val_acc:0.931]
Epoch [39/120    avg_loss:0.484, val_acc:0.940]
Epoch [40/120    avg_loss:0.434, val_acc:0.917]
Epoch [41/120    avg_loss:0.395, val_acc:0.927]
Epoch [42/120    avg_loss:0.433, val_acc:0.948]
Epoch [43/120    avg_loss:0.423, val_acc:0.904]
Epoch [44/120    avg_loss:0.379, val_acc:0.938]
Epoch [45/120    avg_loss:0.315, val_acc:0.933]
Epoch [46/120    avg_loss:0.342, val_acc:0.954]
Epoch [47/120    avg_loss:0.319, val_acc:0.956]
Epoch [48/120    avg_loss:0.296, val_acc:0.958]
Epoch [49/120    avg_loss:0.267, val_acc:0.938]
Epoch [50/120    avg_loss:0.305, val_acc:0.950]
Epoch [51/120    avg_loss:0.289, val_acc:0.969]
Epoch [52/120    avg_loss:0.238, val_acc:0.958]
Epoch [53/120    avg_loss:0.233, val_acc:0.967]
Epoch [54/120    avg_loss:0.215, val_acc:0.967]
Epoch [55/120    avg_loss:0.212, val_acc:0.944]
Epoch [56/120    avg_loss:0.219, val_acc:0.967]
Epoch [57/120    avg_loss:0.189, val_acc:0.973]
Epoch [58/120    avg_loss:0.200, val_acc:0.971]
Epoch [59/120    avg_loss:0.220, val_acc:0.967]
Epoch [60/120    avg_loss:0.212, val_acc:0.965]
Epoch [61/120    avg_loss:0.287, val_acc:0.938]
Epoch [62/120    avg_loss:0.235, val_acc:0.971]
Epoch [63/120    avg_loss:0.214, val_acc:0.967]
Epoch [64/120    avg_loss:0.208, val_acc:0.981]
Epoch [65/120    avg_loss:0.171, val_acc:0.985]
Epoch [66/120    avg_loss:0.143, val_acc:0.971]
Epoch [67/120    avg_loss:0.183, val_acc:0.940]
Epoch [68/120    avg_loss:0.167, val_acc:0.971]
Epoch [69/120    avg_loss:0.166, val_acc:0.977]
Epoch [70/120    avg_loss:0.120, val_acc:0.981]
Epoch [71/120    avg_loss:0.113, val_acc:0.985]
Epoch [72/120    avg_loss:0.138, val_acc:0.973]
Epoch [73/120    avg_loss:0.235, val_acc:0.908]
Epoch [74/120    avg_loss:0.212, val_acc:0.960]
Epoch [75/120    avg_loss:0.174, val_acc:0.960]
Epoch [76/120    avg_loss:0.172, val_acc:0.971]
Epoch [77/120    avg_loss:0.152, val_acc:0.958]
Epoch [78/120    avg_loss:0.115, val_acc:0.975]
Epoch [79/120    avg_loss:0.105, val_acc:0.977]
Epoch [80/120    avg_loss:0.114, val_acc:0.981]
Epoch [81/120    avg_loss:0.124, val_acc:0.967]
Epoch [82/120    avg_loss:0.136, val_acc:0.973]
Epoch [83/120    avg_loss:0.133, val_acc:0.969]
Epoch [84/120    avg_loss:0.106, val_acc:0.975]
Epoch [85/120    avg_loss:0.109, val_acc:0.979]
Epoch [86/120    avg_loss:0.079, val_acc:0.983]
Epoch [87/120    avg_loss:0.074, val_acc:0.983]
Epoch [88/120    avg_loss:0.089, val_acc:0.983]
Epoch [89/120    avg_loss:0.062, val_acc:0.988]
Epoch [90/120    avg_loss:0.067, val_acc:0.988]
Epoch [91/120    avg_loss:0.075, val_acc:0.985]
Epoch [92/120    avg_loss:0.071, val_acc:0.990]
Epoch [93/120    avg_loss:0.077, val_acc:0.990]
Epoch [94/120    avg_loss:0.063, val_acc:0.988]
Epoch [95/120    avg_loss:0.073, val_acc:0.985]
Epoch [96/120    avg_loss:0.057, val_acc:0.988]
Epoch [97/120    avg_loss:0.067, val_acc:0.990]
Epoch [98/120    avg_loss:0.057, val_acc:0.988]
Epoch [99/120    avg_loss:0.063, val_acc:0.990]
Epoch [100/120    avg_loss:0.057, val_acc:0.990]
Epoch [101/120    avg_loss:0.069, val_acc:0.988]
Epoch [102/120    avg_loss:0.069, val_acc:0.983]
Epoch [103/120    avg_loss:0.056, val_acc:0.985]
Epoch [104/120    avg_loss:0.055, val_acc:0.988]
Epoch [105/120    avg_loss:0.059, val_acc:0.988]
Epoch [106/120    avg_loss:0.059, val_acc:0.988]
Epoch [107/120    avg_loss:0.068, val_acc:0.988]
Epoch [108/120    avg_loss:0.059, val_acc:0.985]
Epoch [109/120    avg_loss:0.057, val_acc:0.988]
Epoch [110/120    avg_loss:0.056, val_acc:0.988]
Epoch [111/120    avg_loss:0.053, val_acc:0.985]
Epoch [112/120    avg_loss:0.057, val_acc:0.988]
Epoch [113/120    avg_loss:0.051, val_acc:0.985]
Epoch [114/120    avg_loss:0.046, val_acc:0.985]
Epoch [115/120    avg_loss:0.058, val_acc:0.985]
Epoch [116/120    avg_loss:0.062, val_acc:0.988]
Epoch [117/120    avg_loss:0.062, val_acc:0.988]
Epoch [118/120    avg_loss:0.064, val_acc:0.988]
Epoch [119/120    avg_loss:0.053, val_acc:0.988]
Epoch [120/120    avg_loss:0.056, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 674   0   0   0   0  11   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.99190581 0.99095023 1.         0.94170404 0.91275168
 0.97399527 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9902700320376994
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fddb814db70>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.668, val_acc:0.048]
Epoch [2/120    avg_loss:2.584, val_acc:0.154]
Epoch [3/120    avg_loss:2.503, val_acc:0.371]
Epoch [4/120    avg_loss:2.432, val_acc:0.388]
Epoch [5/120    avg_loss:2.363, val_acc:0.463]
Epoch [6/120    avg_loss:2.300, val_acc:0.504]
Epoch [7/120    avg_loss:2.234, val_acc:0.490]
Epoch [8/120    avg_loss:2.176, val_acc:0.512]
Epoch [9/120    avg_loss:2.111, val_acc:0.548]
Epoch [10/120    avg_loss:2.041, val_acc:0.556]
Epoch [11/120    avg_loss:1.972, val_acc:0.573]
Epoch [12/120    avg_loss:1.899, val_acc:0.558]
Epoch [13/120    avg_loss:1.839, val_acc:0.581]
Epoch [14/120    avg_loss:1.753, val_acc:0.596]
Epoch [15/120    avg_loss:1.690, val_acc:0.596]
Epoch [16/120    avg_loss:1.596, val_acc:0.637]
Epoch [17/120    avg_loss:1.522, val_acc:0.660]
Epoch [18/120    avg_loss:1.460, val_acc:0.681]
Epoch [19/120    avg_loss:1.407, val_acc:0.673]
Epoch [20/120    avg_loss:1.318, val_acc:0.725]
Epoch [21/120    avg_loss:1.237, val_acc:0.750]
Epoch [22/120    avg_loss:1.147, val_acc:0.742]
Epoch [23/120    avg_loss:1.084, val_acc:0.731]
Epoch [24/120    avg_loss:1.057, val_acc:0.765]
Epoch [25/120    avg_loss:0.984, val_acc:0.742]
Epoch [26/120    avg_loss:0.943, val_acc:0.756]
Epoch [27/120    avg_loss:0.844, val_acc:0.848]
Epoch [28/120    avg_loss:0.772, val_acc:0.871]
Epoch [29/120    avg_loss:0.751, val_acc:0.863]
Epoch [30/120    avg_loss:0.716, val_acc:0.773]
Epoch [31/120    avg_loss:0.682, val_acc:0.885]
Epoch [32/120    avg_loss:0.654, val_acc:0.904]
Epoch [33/120    avg_loss:0.570, val_acc:0.917]
Epoch [34/120    avg_loss:0.574, val_acc:0.887]
Epoch [35/120    avg_loss:0.541, val_acc:0.879]
Epoch [36/120    avg_loss:0.496, val_acc:0.910]
Epoch [37/120    avg_loss:0.471, val_acc:0.817]
Epoch [38/120    avg_loss:0.464, val_acc:0.900]
Epoch [39/120    avg_loss:0.459, val_acc:0.896]
Epoch [40/120    avg_loss:0.419, val_acc:0.938]
Epoch [41/120    avg_loss:0.381, val_acc:0.915]
Epoch [42/120    avg_loss:0.359, val_acc:0.929]
Epoch [43/120    avg_loss:0.354, val_acc:0.921]
Epoch [44/120    avg_loss:0.345, val_acc:0.923]
Epoch [45/120    avg_loss:0.365, val_acc:0.923]
Epoch [46/120    avg_loss:0.453, val_acc:0.854]
Epoch [47/120    avg_loss:0.451, val_acc:0.902]
Epoch [48/120    avg_loss:0.386, val_acc:0.942]
Epoch [49/120    avg_loss:0.336, val_acc:0.944]
Epoch [50/120    avg_loss:0.306, val_acc:0.900]
Epoch [51/120    avg_loss:0.317, val_acc:0.915]
Epoch [52/120    avg_loss:0.319, val_acc:0.929]
Epoch [53/120    avg_loss:0.294, val_acc:0.938]
Epoch [54/120    avg_loss:0.279, val_acc:0.954]
Epoch [55/120    avg_loss:0.290, val_acc:0.942]
Epoch [56/120    avg_loss:0.318, val_acc:0.948]
Epoch [57/120    avg_loss:0.233, val_acc:0.925]
Epoch [58/120    avg_loss:0.243, val_acc:0.933]
Epoch [59/120    avg_loss:0.293, val_acc:0.944]
Epoch [60/120    avg_loss:0.237, val_acc:0.956]
Epoch [61/120    avg_loss:0.219, val_acc:0.965]
Epoch [62/120    avg_loss:0.199, val_acc:0.965]
Epoch [63/120    avg_loss:0.221, val_acc:0.956]
Epoch [64/120    avg_loss:0.244, val_acc:0.963]
Epoch [65/120    avg_loss:0.205, val_acc:0.917]
Epoch [66/120    avg_loss:0.239, val_acc:0.967]
Epoch [67/120    avg_loss:0.194, val_acc:0.958]
Epoch [68/120    avg_loss:0.189, val_acc:0.971]
Epoch [69/120    avg_loss:0.213, val_acc:0.969]
Epoch [70/120    avg_loss:0.168, val_acc:0.975]
Epoch [71/120    avg_loss:0.154, val_acc:0.963]
Epoch [72/120    avg_loss:0.140, val_acc:0.967]
Epoch [73/120    avg_loss:0.152, val_acc:0.960]
Epoch [74/120    avg_loss:0.143, val_acc:0.977]
Epoch [75/120    avg_loss:0.140, val_acc:0.977]
Epoch [76/120    avg_loss:0.133, val_acc:0.975]
Epoch [77/120    avg_loss:0.127, val_acc:0.973]
Epoch [78/120    avg_loss:0.137, val_acc:0.975]
Epoch [79/120    avg_loss:0.140, val_acc:0.956]
Epoch [80/120    avg_loss:0.144, val_acc:0.965]
Epoch [81/120    avg_loss:0.135, val_acc:0.975]
Epoch [82/120    avg_loss:0.146, val_acc:0.969]
Epoch [83/120    avg_loss:0.147, val_acc:0.971]
Epoch [84/120    avg_loss:0.122, val_acc:0.971]
Epoch [85/120    avg_loss:0.105, val_acc:0.979]
Epoch [86/120    avg_loss:0.148, val_acc:0.975]
Epoch [87/120    avg_loss:0.138, val_acc:0.973]
Epoch [88/120    avg_loss:0.114, val_acc:0.975]
Epoch [89/120    avg_loss:0.108, val_acc:0.983]
Epoch [90/120    avg_loss:0.115, val_acc:0.979]
Epoch [91/120    avg_loss:0.104, val_acc:0.935]
Epoch [92/120    avg_loss:0.102, val_acc:0.977]
Epoch [93/120    avg_loss:0.100, val_acc:0.985]
Epoch [94/120    avg_loss:0.092, val_acc:0.983]
Epoch [95/120    avg_loss:0.087, val_acc:0.973]
Epoch [96/120    avg_loss:0.100, val_acc:0.988]
Epoch [97/120    avg_loss:0.082, val_acc:0.981]
Epoch [98/120    avg_loss:0.080, val_acc:0.979]
Epoch [99/120    avg_loss:0.105, val_acc:0.960]
Epoch [100/120    avg_loss:0.103, val_acc:0.979]
Epoch [101/120    avg_loss:0.088, val_acc:0.981]
Epoch [102/120    avg_loss:0.066, val_acc:0.956]
Epoch [103/120    avg_loss:0.070, val_acc:0.979]
Epoch [104/120    avg_loss:0.125, val_acc:0.971]
Epoch [105/120    avg_loss:0.099, val_acc:0.988]
Epoch [106/120    avg_loss:0.079, val_acc:0.983]
Epoch [107/120    avg_loss:0.104, val_acc:0.979]
Epoch [108/120    avg_loss:0.083, val_acc:0.988]
Epoch [109/120    avg_loss:0.059, val_acc:0.988]
Epoch [110/120    avg_loss:0.189, val_acc:0.979]
Epoch [111/120    avg_loss:0.119, val_acc:0.975]
Epoch [112/120    avg_loss:0.070, val_acc:0.994]
Epoch [113/120    avg_loss:0.058, val_acc:0.988]
Epoch [114/120    avg_loss:0.081, val_acc:0.983]
Epoch [115/120    avg_loss:0.104, val_acc:0.985]
Epoch [116/120    avg_loss:0.077, val_acc:0.990]
Epoch [117/120    avg_loss:0.052, val_acc:0.990]
Epoch [118/120    avg_loss:0.057, val_acc:0.988]
Epoch [119/120    avg_loss:0.047, val_acc:0.983]
Epoch [120/120    avg_loss:0.046, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 674   0   0   0   0  11   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  11   0   0   0   0   0   0   2   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99190581 1.         1.         0.96396396 0.95302013
 0.97399527 1.         1.         1.         1.         0.98300654
 0.98327759 1.        ]

Kappa:
0.9905077729246771
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd9390f0b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.263]
Epoch [2/120    avg_loss:2.493, val_acc:0.285]
Epoch [3/120    avg_loss:2.409, val_acc:0.327]
Epoch [4/120    avg_loss:2.343, val_acc:0.338]
Epoch [5/120    avg_loss:2.281, val_acc:0.348]
Epoch [6/120    avg_loss:2.242, val_acc:0.371]
Epoch [7/120    avg_loss:2.186, val_acc:0.427]
Epoch [8/120    avg_loss:2.126, val_acc:0.440]
Epoch [9/120    avg_loss:2.053, val_acc:0.473]
Epoch [10/120    avg_loss:1.987, val_acc:0.510]
Epoch [11/120    avg_loss:1.930, val_acc:0.525]
Epoch [12/120    avg_loss:1.834, val_acc:0.562]
Epoch [13/120    avg_loss:1.746, val_acc:0.608]
Epoch [14/120    avg_loss:1.681, val_acc:0.627]
Epoch [15/120    avg_loss:1.586, val_acc:0.662]
Epoch [16/120    avg_loss:1.524, val_acc:0.690]
Epoch [17/120    avg_loss:1.433, val_acc:0.696]
Epoch [18/120    avg_loss:1.364, val_acc:0.750]
Epoch [19/120    avg_loss:1.261, val_acc:0.748]
Epoch [20/120    avg_loss:1.155, val_acc:0.750]
Epoch [21/120    avg_loss:1.097, val_acc:0.762]
Epoch [22/120    avg_loss:1.033, val_acc:0.777]
Epoch [23/120    avg_loss:0.932, val_acc:0.787]
Epoch [24/120    avg_loss:0.867, val_acc:0.771]
Epoch [25/120    avg_loss:0.852, val_acc:0.823]
Epoch [26/120    avg_loss:0.799, val_acc:0.802]
Epoch [27/120    avg_loss:0.706, val_acc:0.827]
Epoch [28/120    avg_loss:0.652, val_acc:0.887]
Epoch [29/120    avg_loss:0.630, val_acc:0.838]
Epoch [30/120    avg_loss:0.649, val_acc:0.838]
Epoch [31/120    avg_loss:0.602, val_acc:0.833]
Epoch [32/120    avg_loss:0.526, val_acc:0.896]
Epoch [33/120    avg_loss:0.524, val_acc:0.938]
Epoch [34/120    avg_loss:0.496, val_acc:0.915]
Epoch [35/120    avg_loss:0.511, val_acc:0.875]
Epoch [36/120    avg_loss:0.470, val_acc:0.931]
Epoch [37/120    avg_loss:0.432, val_acc:0.910]
Epoch [38/120    avg_loss:0.465, val_acc:0.902]
Epoch [39/120    avg_loss:0.433, val_acc:0.950]
Epoch [40/120    avg_loss:0.454, val_acc:0.865]
Epoch [41/120    avg_loss:0.391, val_acc:0.896]
Epoch [42/120    avg_loss:0.409, val_acc:0.954]
Epoch [43/120    avg_loss:0.358, val_acc:0.963]
Epoch [44/120    avg_loss:0.364, val_acc:0.933]
Epoch [45/120    avg_loss:0.354, val_acc:0.965]
Epoch [46/120    avg_loss:0.352, val_acc:0.950]
Epoch [47/120    avg_loss:0.339, val_acc:0.935]
Epoch [48/120    avg_loss:0.297, val_acc:0.935]
Epoch [49/120    avg_loss:0.296, val_acc:0.946]
Epoch [50/120    avg_loss:0.247, val_acc:0.956]
Epoch [51/120    avg_loss:0.251, val_acc:0.960]
Epoch [52/120    avg_loss:0.260, val_acc:0.960]
Epoch [53/120    avg_loss:0.269, val_acc:0.927]
Epoch [54/120    avg_loss:0.284, val_acc:0.933]
Epoch [55/120    avg_loss:0.291, val_acc:0.956]
Epoch [56/120    avg_loss:0.265, val_acc:0.971]
Epoch [57/120    avg_loss:0.248, val_acc:0.963]
Epoch [58/120    avg_loss:0.226, val_acc:0.967]
Epoch [59/120    avg_loss:0.247, val_acc:0.963]
Epoch [60/120    avg_loss:0.270, val_acc:0.979]
Epoch [61/120    avg_loss:0.206, val_acc:0.938]
Epoch [62/120    avg_loss:0.251, val_acc:0.963]
Epoch [63/120    avg_loss:0.226, val_acc:0.960]
Epoch [64/120    avg_loss:0.207, val_acc:0.933]
Epoch [65/120    avg_loss:0.239, val_acc:0.979]
Epoch [66/120    avg_loss:0.189, val_acc:0.967]
Epoch [67/120    avg_loss:0.169, val_acc:0.983]
Epoch [68/120    avg_loss:0.154, val_acc:0.977]
Epoch [69/120    avg_loss:0.155, val_acc:0.975]
Epoch [70/120    avg_loss:0.152, val_acc:0.975]
Epoch [71/120    avg_loss:0.127, val_acc:0.979]
Epoch [72/120    avg_loss:0.140, val_acc:0.977]
Epoch [73/120    avg_loss:0.153, val_acc:0.977]
Epoch [74/120    avg_loss:0.144, val_acc:0.975]
Epoch [75/120    avg_loss:0.136, val_acc:0.975]
Epoch [76/120    avg_loss:0.178, val_acc:0.967]
Epoch [77/120    avg_loss:0.150, val_acc:0.985]
Epoch [78/120    avg_loss:0.129, val_acc:0.981]
Epoch [79/120    avg_loss:0.136, val_acc:0.973]
Epoch [80/120    avg_loss:0.117, val_acc:0.979]
Epoch [81/120    avg_loss:0.134, val_acc:0.975]
Epoch [82/120    avg_loss:0.143, val_acc:0.983]
Epoch [83/120    avg_loss:0.150, val_acc:0.988]
Epoch [84/120    avg_loss:0.149, val_acc:0.988]
Epoch [85/120    avg_loss:0.134, val_acc:0.981]
Epoch [86/120    avg_loss:0.099, val_acc:0.973]
Epoch [87/120    avg_loss:0.099, val_acc:0.977]
Epoch [88/120    avg_loss:0.106, val_acc:0.990]
Epoch [89/120    avg_loss:0.078, val_acc:0.990]
Epoch [90/120    avg_loss:0.120, val_acc:0.979]
Epoch [91/120    avg_loss:0.122, val_acc:0.990]
Epoch [92/120    avg_loss:0.114, val_acc:0.977]
Epoch [93/120    avg_loss:0.113, val_acc:0.990]
Epoch [94/120    avg_loss:0.104, val_acc:0.994]
Epoch [95/120    avg_loss:0.090, val_acc:0.994]
Epoch [96/120    avg_loss:0.092, val_acc:0.992]
Epoch [97/120    avg_loss:0.137, val_acc:0.988]
Epoch [98/120    avg_loss:0.093, val_acc:0.985]
Epoch [99/120    avg_loss:0.091, val_acc:0.985]
Epoch [100/120    avg_loss:0.086, val_acc:0.988]
Epoch [101/120    avg_loss:0.064, val_acc:0.994]
Epoch [102/120    avg_loss:0.065, val_acc:0.988]
Epoch [103/120    avg_loss:0.078, val_acc:0.985]
Epoch [104/120    avg_loss:0.059, val_acc:0.992]
Epoch [105/120    avg_loss:0.045, val_acc:0.996]
Epoch [106/120    avg_loss:0.044, val_acc:0.994]
Epoch [107/120    avg_loss:0.053, val_acc:0.990]
Epoch [108/120    avg_loss:0.050, val_acc:0.992]
Epoch [109/120    avg_loss:0.068, val_acc:0.994]
Epoch [110/120    avg_loss:0.060, val_acc:0.990]
Epoch [111/120    avg_loss:0.055, val_acc:0.992]
Epoch [112/120    avg_loss:0.054, val_acc:0.998]
Epoch [113/120    avg_loss:0.057, val_acc:0.985]
Epoch [114/120    avg_loss:0.095, val_acc:0.979]
Epoch [115/120    avg_loss:0.122, val_acc:0.979]
Epoch [116/120    avg_loss:0.106, val_acc:0.973]
Epoch [117/120    avg_loss:0.111, val_acc:0.988]
Epoch [118/120    avg_loss:0.166, val_acc:0.960]
Epoch [119/120    avg_loss:0.102, val_acc:0.988]
Epoch [120/120    avg_loss:0.070, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 673   0   0   0   0  12   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  15 438   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99116348 1.         1.         0.94273128 0.91034483
 0.97169811 1.         1.         1.         1.         0.98049415
 0.98316498 1.        ]

Kappa:
0.9874231675807642
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d02e1a9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.680, val_acc:0.046]
Epoch [2/120    avg_loss:2.599, val_acc:0.246]
Epoch [3/120    avg_loss:2.514, val_acc:0.371]
Epoch [4/120    avg_loss:2.431, val_acc:0.406]
Epoch [5/120    avg_loss:2.359, val_acc:0.402]
Epoch [6/120    avg_loss:2.310, val_acc:0.410]
Epoch [7/120    avg_loss:2.252, val_acc:0.431]
Epoch [8/120    avg_loss:2.199, val_acc:0.452]
Epoch [9/120    avg_loss:2.149, val_acc:0.471]
Epoch [10/120    avg_loss:2.094, val_acc:0.504]
Epoch [11/120    avg_loss:2.046, val_acc:0.556]
Epoch [12/120    avg_loss:1.997, val_acc:0.640]
Epoch [13/120    avg_loss:1.930, val_acc:0.600]
Epoch [14/120    avg_loss:1.864, val_acc:0.692]
Epoch [15/120    avg_loss:1.801, val_acc:0.706]
Epoch [16/120    avg_loss:1.742, val_acc:0.717]
Epoch [17/120    avg_loss:1.738, val_acc:0.688]
Epoch [18/120    avg_loss:1.636, val_acc:0.752]
Epoch [19/120    avg_loss:1.573, val_acc:0.746]
Epoch [20/120    avg_loss:1.514, val_acc:0.781]
Epoch [21/120    avg_loss:1.433, val_acc:0.760]
Epoch [22/120    avg_loss:1.381, val_acc:0.762]
Epoch [23/120    avg_loss:1.331, val_acc:0.777]
Epoch [24/120    avg_loss:1.245, val_acc:0.779]
Epoch [25/120    avg_loss:1.200, val_acc:0.796]
Epoch [26/120    avg_loss:1.136, val_acc:0.792]
Epoch [27/120    avg_loss:1.062, val_acc:0.812]
Epoch [28/120    avg_loss:1.041, val_acc:0.767]
Epoch [29/120    avg_loss:0.955, val_acc:0.854]
Epoch [30/120    avg_loss:0.918, val_acc:0.829]
Epoch [31/120    avg_loss:0.899, val_acc:0.817]
Epoch [32/120    avg_loss:0.867, val_acc:0.854]
Epoch [33/120    avg_loss:0.819, val_acc:0.885]
Epoch [34/120    avg_loss:0.735, val_acc:0.881]
Epoch [35/120    avg_loss:0.729, val_acc:0.890]
Epoch [36/120    avg_loss:0.678, val_acc:0.908]
Epoch [37/120    avg_loss:0.658, val_acc:0.904]
Epoch [38/120    avg_loss:0.600, val_acc:0.925]
Epoch [39/120    avg_loss:0.615, val_acc:0.869]
Epoch [40/120    avg_loss:0.737, val_acc:0.906]
Epoch [41/120    avg_loss:0.591, val_acc:0.904]
Epoch [42/120    avg_loss:0.604, val_acc:0.921]
Epoch [43/120    avg_loss:0.514, val_acc:0.908]
Epoch [44/120    avg_loss:0.528, val_acc:0.927]
Epoch [45/120    avg_loss:0.503, val_acc:0.898]
Epoch [46/120    avg_loss:0.531, val_acc:0.890]
Epoch [47/120    avg_loss:0.517, val_acc:0.867]
Epoch [48/120    avg_loss:0.474, val_acc:0.940]
Epoch [49/120    avg_loss:0.445, val_acc:0.921]
Epoch [50/120    avg_loss:0.428, val_acc:0.921]
Epoch [51/120    avg_loss:0.376, val_acc:0.938]
Epoch [52/120    avg_loss:0.378, val_acc:0.933]
Epoch [53/120    avg_loss:0.410, val_acc:0.927]
Epoch [54/120    avg_loss:0.377, val_acc:0.917]
Epoch [55/120    avg_loss:0.376, val_acc:0.942]
Epoch [56/120    avg_loss:0.336, val_acc:0.921]
Epoch [57/120    avg_loss:0.355, val_acc:0.954]
Epoch [58/120    avg_loss:0.308, val_acc:0.948]
Epoch [59/120    avg_loss:0.342, val_acc:0.906]
Epoch [60/120    avg_loss:0.444, val_acc:0.944]
Epoch [61/120    avg_loss:0.383, val_acc:0.938]
Epoch [62/120    avg_loss:0.378, val_acc:0.944]
Epoch [63/120    avg_loss:0.347, val_acc:0.946]
Epoch [64/120    avg_loss:0.311, val_acc:0.958]
Epoch [65/120    avg_loss:0.320, val_acc:0.948]
Epoch [66/120    avg_loss:0.305, val_acc:0.948]
Epoch [67/120    avg_loss:0.299, val_acc:0.965]
Epoch [68/120    avg_loss:0.275, val_acc:0.929]
Epoch [69/120    avg_loss:0.248, val_acc:0.958]
Epoch [70/120    avg_loss:0.245, val_acc:0.969]
Epoch [71/120    avg_loss:0.250, val_acc:0.881]
Epoch [72/120    avg_loss:0.292, val_acc:0.940]
Epoch [73/120    avg_loss:0.291, val_acc:0.956]
Epoch [74/120    avg_loss:0.252, val_acc:0.960]
Epoch [75/120    avg_loss:0.270, val_acc:0.954]
Epoch [76/120    avg_loss:0.229, val_acc:0.967]
Epoch [77/120    avg_loss:0.199, val_acc:0.969]
Epoch [78/120    avg_loss:0.230, val_acc:0.954]
Epoch [79/120    avg_loss:0.222, val_acc:0.969]
Epoch [80/120    avg_loss:0.194, val_acc:0.952]
Epoch [81/120    avg_loss:0.201, val_acc:0.973]
Epoch [82/120    avg_loss:0.194, val_acc:0.969]
Epoch [83/120    avg_loss:0.191, val_acc:0.967]
Epoch [84/120    avg_loss:0.182, val_acc:0.969]
Epoch [85/120    avg_loss:0.183, val_acc:0.948]
Epoch [86/120    avg_loss:0.162, val_acc:0.971]
Epoch [87/120    avg_loss:0.166, val_acc:0.952]
Epoch [88/120    avg_loss:0.176, val_acc:0.969]
Epoch [89/120    avg_loss:0.134, val_acc:0.969]
Epoch [90/120    avg_loss:0.192, val_acc:0.950]
Epoch [91/120    avg_loss:0.136, val_acc:0.969]
Epoch [92/120    avg_loss:0.153, val_acc:0.973]
Epoch [93/120    avg_loss:0.203, val_acc:0.948]
Epoch [94/120    avg_loss:0.199, val_acc:0.956]
Epoch [95/120    avg_loss:0.169, val_acc:0.967]
Epoch [96/120    avg_loss:0.143, val_acc:0.971]
Epoch [97/120    avg_loss:0.131, val_acc:0.967]
Epoch [98/120    avg_loss:0.125, val_acc:0.969]
Epoch [99/120    avg_loss:0.137, val_acc:0.969]
Epoch [100/120    avg_loss:0.159, val_acc:0.975]
Epoch [101/120    avg_loss:0.180, val_acc:0.965]
Epoch [102/120    avg_loss:0.232, val_acc:0.946]
Epoch [103/120    avg_loss:0.193, val_acc:0.948]
Epoch [104/120    avg_loss:0.190, val_acc:0.963]
Epoch [105/120    avg_loss:0.160, val_acc:0.969]
Epoch [106/120    avg_loss:0.138, val_acc:0.948]
Epoch [107/120    avg_loss:0.138, val_acc:0.967]
Epoch [108/120    avg_loss:0.128, val_acc:0.975]
Epoch [109/120    avg_loss:0.093, val_acc:0.977]
Epoch [110/120    avg_loss:0.110, val_acc:0.969]
Epoch [111/120    avg_loss:0.115, val_acc:0.969]
Epoch [112/120    avg_loss:0.137, val_acc:0.971]
Epoch [113/120    avg_loss:0.124, val_acc:0.969]
Epoch [114/120    avg_loss:0.141, val_acc:0.975]
Epoch [115/120    avg_loss:0.115, val_acc:0.977]
Epoch [116/120    avg_loss:0.096, val_acc:0.979]
Epoch [117/120    avg_loss:0.082, val_acc:0.977]
Epoch [118/120    avg_loss:0.082, val_acc:0.981]
Epoch [119/120    avg_loss:0.105, val_acc:0.977]
Epoch [120/120    avg_loss:0.098, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 189   0   0   0   0  30   0   0   0   0   0   0]
 [  0   0   2 226   0   0   0   0   1   1   0   0   0   0]
 [  0   0   0   4 210  13   0   0   0   0   0   0   0   0]
 [  0   0   0   5  16 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 372   5   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.12366737739872

F1 scores:
[       nan 1.         0.89786223 0.97204301 0.92715232 0.87943262
 1.         0.80769231 0.99742268 0.99893276 1.         0.99332443
 0.99451153 1.        ]

Kappa:
0.979111734506461
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdbeb6ffa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.638, val_acc:0.096]
Epoch [2/120    avg_loss:2.564, val_acc:0.256]
Epoch [3/120    avg_loss:2.496, val_acc:0.271]
Epoch [4/120    avg_loss:2.435, val_acc:0.283]
Epoch [5/120    avg_loss:2.398, val_acc:0.296]
Epoch [6/120    avg_loss:2.350, val_acc:0.296]
Epoch [7/120    avg_loss:2.319, val_acc:0.315]
Epoch [8/120    avg_loss:2.251, val_acc:0.335]
Epoch [9/120    avg_loss:2.196, val_acc:0.369]
Epoch [10/120    avg_loss:2.152, val_acc:0.429]
Epoch [11/120    avg_loss:2.106, val_acc:0.463]
Epoch [12/120    avg_loss:2.045, val_acc:0.506]
Epoch [13/120    avg_loss:1.979, val_acc:0.515]
Epoch [14/120    avg_loss:1.918, val_acc:0.537]
Epoch [15/120    avg_loss:1.851, val_acc:0.552]
Epoch [16/120    avg_loss:1.784, val_acc:0.577]
Epoch [17/120    avg_loss:1.751, val_acc:0.608]
Epoch [18/120    avg_loss:1.690, val_acc:0.606]
Epoch [19/120    avg_loss:1.666, val_acc:0.617]
Epoch [20/120    avg_loss:1.596, val_acc:0.662]
Epoch [21/120    avg_loss:1.548, val_acc:0.642]
Epoch [22/120    avg_loss:1.469, val_acc:0.690]
Epoch [23/120    avg_loss:1.429, val_acc:0.677]
Epoch [24/120    avg_loss:1.369, val_acc:0.713]
Epoch [25/120    avg_loss:1.324, val_acc:0.735]
Epoch [26/120    avg_loss:1.261, val_acc:0.735]
Epoch [27/120    avg_loss:1.226, val_acc:0.792]
Epoch [28/120    avg_loss:1.161, val_acc:0.817]
Epoch [29/120    avg_loss:1.110, val_acc:0.804]
Epoch [30/120    avg_loss:1.054, val_acc:0.840]
Epoch [31/120    avg_loss:1.025, val_acc:0.819]
Epoch [32/120    avg_loss:0.988, val_acc:0.833]
Epoch [33/120    avg_loss:0.909, val_acc:0.894]
Epoch [34/120    avg_loss:0.855, val_acc:0.892]
Epoch [35/120    avg_loss:0.778, val_acc:0.890]
Epoch [36/120    avg_loss:0.740, val_acc:0.867]
Epoch [37/120    avg_loss:0.761, val_acc:0.808]
Epoch [38/120    avg_loss:0.739, val_acc:0.873]
Epoch [39/120    avg_loss:0.724, val_acc:0.865]
Epoch [40/120    avg_loss:0.698, val_acc:0.890]
Epoch [41/120    avg_loss:0.663, val_acc:0.894]
Epoch [42/120    avg_loss:0.595, val_acc:0.908]
Epoch [43/120    avg_loss:0.548, val_acc:0.919]
Epoch [44/120    avg_loss:0.539, val_acc:0.917]
Epoch [45/120    avg_loss:0.556, val_acc:0.892]
Epoch [46/120    avg_loss:0.539, val_acc:0.873]
Epoch [47/120    avg_loss:0.520, val_acc:0.917]
Epoch [48/120    avg_loss:0.539, val_acc:0.906]
Epoch [49/120    avg_loss:0.501, val_acc:0.919]
Epoch [50/120    avg_loss:0.474, val_acc:0.912]
Epoch [51/120    avg_loss:0.425, val_acc:0.919]
Epoch [52/120    avg_loss:0.445, val_acc:0.908]
Epoch [53/120    avg_loss:0.421, val_acc:0.925]
Epoch [54/120    avg_loss:0.413, val_acc:0.935]
Epoch [55/120    avg_loss:0.380, val_acc:0.929]
Epoch [56/120    avg_loss:0.434, val_acc:0.844]
Epoch [57/120    avg_loss:0.471, val_acc:0.910]
Epoch [58/120    avg_loss:0.394, val_acc:0.921]
Epoch [59/120    avg_loss:0.358, val_acc:0.923]
Epoch [60/120    avg_loss:0.362, val_acc:0.925]
Epoch [61/120    avg_loss:0.354, val_acc:0.925]
Epoch [62/120    avg_loss:0.354, val_acc:0.938]
Epoch [63/120    avg_loss:0.315, val_acc:0.946]
Epoch [64/120    avg_loss:0.279, val_acc:0.960]
Epoch [65/120    avg_loss:0.333, val_acc:0.958]
Epoch [66/120    avg_loss:0.305, val_acc:0.944]
Epoch [67/120    avg_loss:0.288, val_acc:0.942]
Epoch [68/120    avg_loss:0.280, val_acc:0.969]
Epoch [69/120    avg_loss:0.270, val_acc:0.931]
Epoch [70/120    avg_loss:0.307, val_acc:0.944]
Epoch [71/120    avg_loss:0.331, val_acc:0.952]
Epoch [72/120    avg_loss:0.289, val_acc:0.915]
Epoch [73/120    avg_loss:0.292, val_acc:0.940]
Epoch [74/120    avg_loss:0.323, val_acc:0.898]
Epoch [75/120    avg_loss:0.280, val_acc:0.925]
Epoch [76/120    avg_loss:0.239, val_acc:0.954]
Epoch [77/120    avg_loss:0.239, val_acc:0.944]
Epoch [78/120    avg_loss:0.261, val_acc:0.917]
Epoch [79/120    avg_loss:0.233, val_acc:0.960]
Epoch [80/120    avg_loss:0.240, val_acc:0.935]
Epoch [81/120    avg_loss:0.208, val_acc:0.973]
Epoch [82/120    avg_loss:0.194, val_acc:0.948]
Epoch [83/120    avg_loss:0.203, val_acc:0.917]
Epoch [84/120    avg_loss:0.240, val_acc:0.935]
Epoch [85/120    avg_loss:0.216, val_acc:0.942]
Epoch [86/120    avg_loss:0.237, val_acc:0.946]
Epoch [87/120    avg_loss:0.189, val_acc:0.954]
Epoch [88/120    avg_loss:0.188, val_acc:0.967]
Epoch [89/120    avg_loss:0.160, val_acc:0.967]
Epoch [90/120    avg_loss:0.204, val_acc:0.940]
Epoch [91/120    avg_loss:0.184, val_acc:0.967]
Epoch [92/120    avg_loss:0.146, val_acc:0.981]
Epoch [93/120    avg_loss:0.144, val_acc:0.965]
Epoch [94/120    avg_loss:0.166, val_acc:0.969]
Epoch [95/120    avg_loss:0.219, val_acc:0.925]
Epoch [96/120    avg_loss:0.201, val_acc:0.971]
Epoch [97/120    avg_loss:0.182, val_acc:0.975]
Epoch [98/120    avg_loss:0.144, val_acc:0.969]
Epoch [99/120    avg_loss:0.156, val_acc:0.960]
Epoch [100/120    avg_loss:0.198, val_acc:0.967]
Epoch [101/120    avg_loss:0.159, val_acc:0.969]
Epoch [102/120    avg_loss:0.169, val_acc:0.969]
Epoch [103/120    avg_loss:0.120, val_acc:0.973]
Epoch [104/120    avg_loss:0.143, val_acc:0.963]
Epoch [105/120    avg_loss:0.182, val_acc:0.967]
Epoch [106/120    avg_loss:0.112, val_acc:0.975]
Epoch [107/120    avg_loss:0.108, val_acc:0.973]
Epoch [108/120    avg_loss:0.093, val_acc:0.973]
Epoch [109/120    avg_loss:0.094, val_acc:0.977]
Epoch [110/120    avg_loss:0.093, val_acc:0.977]
Epoch [111/120    avg_loss:0.082, val_acc:0.979]
Epoch [112/120    avg_loss:0.087, val_acc:0.979]
Epoch [113/120    avg_loss:0.079, val_acc:0.983]
Epoch [114/120    avg_loss:0.099, val_acc:0.981]
Epoch [115/120    avg_loss:0.101, val_acc:0.981]
Epoch [116/120    avg_loss:0.087, val_acc:0.981]
Epoch [117/120    avg_loss:0.090, val_acc:0.981]
Epoch [118/120    avg_loss:0.080, val_acc:0.979]
Epoch [119/120    avg_loss:0.082, val_acc:0.983]
Epoch [120/120    avg_loss:0.083, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   1 217   9   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 194  33   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.05970149253731

F1 scores:
[       nan 0.99636364 0.93721973 0.97091723 0.8758465  0.8516129
 0.98771499 0.85082873 0.99742931 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.978395941014475
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa1268ce9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.605, val_acc:0.127]
Epoch [2/120    avg_loss:2.539, val_acc:0.377]
Epoch [3/120    avg_loss:2.474, val_acc:0.417]
Epoch [4/120    avg_loss:2.418, val_acc:0.408]
Epoch [5/120    avg_loss:2.358, val_acc:0.431]
Epoch [6/120    avg_loss:2.296, val_acc:0.431]
Epoch [7/120    avg_loss:2.242, val_acc:0.425]
Epoch [8/120    avg_loss:2.177, val_acc:0.425]
Epoch [9/120    avg_loss:2.124, val_acc:0.421]
Epoch [10/120    avg_loss:2.065, val_acc:0.435]
Epoch [11/120    avg_loss:2.006, val_acc:0.463]
Epoch [12/120    avg_loss:1.958, val_acc:0.463]
Epoch [13/120    avg_loss:1.902, val_acc:0.508]
Epoch [14/120    avg_loss:1.833, val_acc:0.537]
Epoch [15/120    avg_loss:1.774, val_acc:0.588]
Epoch [16/120    avg_loss:1.725, val_acc:0.600]
Epoch [17/120    avg_loss:1.658, val_acc:0.613]
Epoch [18/120    avg_loss:1.615, val_acc:0.656]
Epoch [19/120    avg_loss:1.535, val_acc:0.658]
Epoch [20/120    avg_loss:1.447, val_acc:0.644]
Epoch [21/120    avg_loss:1.397, val_acc:0.646]
Epoch [22/120    avg_loss:1.312, val_acc:0.677]
Epoch [23/120    avg_loss:1.265, val_acc:0.679]
Epoch [24/120    avg_loss:1.218, val_acc:0.675]
Epoch [25/120    avg_loss:1.199, val_acc:0.683]
Epoch [26/120    avg_loss:1.119, val_acc:0.725]
Epoch [27/120    avg_loss:1.086, val_acc:0.815]
Epoch [28/120    avg_loss:0.977, val_acc:0.819]
Epoch [29/120    avg_loss:0.921, val_acc:0.842]
Epoch [30/120    avg_loss:0.863, val_acc:0.752]
Epoch [31/120    avg_loss:0.882, val_acc:0.867]
Epoch [32/120    avg_loss:0.793, val_acc:0.844]
Epoch [33/120    avg_loss:0.893, val_acc:0.785]
Epoch [34/120    avg_loss:0.785, val_acc:0.877]
Epoch [35/120    avg_loss:0.688, val_acc:0.906]
Epoch [36/120    avg_loss:0.611, val_acc:0.900]
Epoch [37/120    avg_loss:0.610, val_acc:0.915]
Epoch [38/120    avg_loss:0.579, val_acc:0.877]
Epoch [39/120    avg_loss:0.700, val_acc:0.875]
Epoch [40/120    avg_loss:0.653, val_acc:0.908]
Epoch [41/120    avg_loss:0.570, val_acc:0.910]
Epoch [42/120    avg_loss:0.565, val_acc:0.925]
Epoch [43/120    avg_loss:0.497, val_acc:0.908]
Epoch [44/120    avg_loss:0.467, val_acc:0.910]
Epoch [45/120    avg_loss:0.430, val_acc:0.933]
Epoch [46/120    avg_loss:0.389, val_acc:0.917]
Epoch [47/120    avg_loss:0.436, val_acc:0.923]
Epoch [48/120    avg_loss:0.423, val_acc:0.944]
Epoch [49/120    avg_loss:0.391, val_acc:0.933]
Epoch [50/120    avg_loss:0.378, val_acc:0.908]
Epoch [51/120    avg_loss:0.387, val_acc:0.935]
Epoch [52/120    avg_loss:0.386, val_acc:0.940]
Epoch [53/120    avg_loss:0.402, val_acc:0.931]
Epoch [54/120    avg_loss:0.373, val_acc:0.912]
Epoch [55/120    avg_loss:0.363, val_acc:0.925]
Epoch [56/120    avg_loss:0.351, val_acc:0.940]
Epoch [57/120    avg_loss:0.348, val_acc:0.946]
Epoch [58/120    avg_loss:0.378, val_acc:0.938]
Epoch [59/120    avg_loss:0.333, val_acc:0.938]
Epoch [60/120    avg_loss:0.328, val_acc:0.946]
Epoch [61/120    avg_loss:0.341, val_acc:0.940]
Epoch [62/120    avg_loss:0.313, val_acc:0.950]
Epoch [63/120    avg_loss:0.303, val_acc:0.938]
Epoch [64/120    avg_loss:0.268, val_acc:0.954]
Epoch [65/120    avg_loss:0.254, val_acc:0.956]
Epoch [66/120    avg_loss:0.239, val_acc:0.946]
Epoch [67/120    avg_loss:0.285, val_acc:0.952]
Epoch [68/120    avg_loss:0.255, val_acc:0.956]
Epoch [69/120    avg_loss:0.226, val_acc:0.946]
Epoch [70/120    avg_loss:0.246, val_acc:0.946]
Epoch [71/120    avg_loss:0.226, val_acc:0.965]
Epoch [72/120    avg_loss:0.217, val_acc:0.973]
Epoch [73/120    avg_loss:0.202, val_acc:0.956]
Epoch [74/120    avg_loss:0.193, val_acc:0.956]
Epoch [75/120    avg_loss:0.207, val_acc:0.956]
Epoch [76/120    avg_loss:0.209, val_acc:0.973]
Epoch [77/120    avg_loss:0.167, val_acc:0.965]
Epoch [78/120    avg_loss:0.209, val_acc:0.960]
Epoch [79/120    avg_loss:0.173, val_acc:0.948]
Epoch [80/120    avg_loss:0.187, val_acc:0.958]
Epoch [81/120    avg_loss:0.174, val_acc:0.969]
Epoch [82/120    avg_loss:0.222, val_acc:0.958]
Epoch [83/120    avg_loss:0.252, val_acc:0.954]
Epoch [84/120    avg_loss:0.233, val_acc:0.963]
Epoch [85/120    avg_loss:0.176, val_acc:0.973]
Epoch [86/120    avg_loss:0.188, val_acc:0.948]
Epoch [87/120    avg_loss:0.161, val_acc:0.950]
Epoch [88/120    avg_loss:0.212, val_acc:0.979]
Epoch [89/120    avg_loss:0.209, val_acc:0.919]
Epoch [90/120    avg_loss:0.223, val_acc:0.965]
Epoch [91/120    avg_loss:0.165, val_acc:0.969]
Epoch [92/120    avg_loss:0.142, val_acc:0.969]
Epoch [93/120    avg_loss:0.129, val_acc:0.977]
Epoch [94/120    avg_loss:0.117, val_acc:0.975]
Epoch [95/120    avg_loss:0.115, val_acc:0.981]
Epoch [96/120    avg_loss:0.130, val_acc:0.973]
Epoch [97/120    avg_loss:0.143, val_acc:0.979]
Epoch [98/120    avg_loss:0.142, val_acc:0.969]
Epoch [99/120    avg_loss:0.166, val_acc:0.973]
Epoch [100/120    avg_loss:0.146, val_acc:0.973]
Epoch [101/120    avg_loss:0.167, val_acc:0.965]
Epoch [102/120    avg_loss:0.136, val_acc:0.973]
Epoch [103/120    avg_loss:0.119, val_acc:0.977]
Epoch [104/120    avg_loss:0.112, val_acc:0.965]
Epoch [105/120    avg_loss:0.093, val_acc:0.977]
Epoch [106/120    avg_loss:0.096, val_acc:0.983]
Epoch [107/120    avg_loss:0.079, val_acc:0.977]
Epoch [108/120    avg_loss:0.096, val_acc:0.971]
Epoch [109/120    avg_loss:0.103, val_acc:0.985]
Epoch [110/120    avg_loss:0.101, val_acc:0.975]
Epoch [111/120    avg_loss:0.108, val_acc:0.973]
Epoch [112/120    avg_loss:0.109, val_acc:0.952]
Epoch [113/120    avg_loss:0.149, val_acc:0.967]
Epoch [114/120    avg_loss:0.129, val_acc:0.950]
Epoch [115/120    avg_loss:0.104, val_acc:0.977]
Epoch [116/120    avg_loss:0.125, val_acc:0.958]
Epoch [117/120    avg_loss:0.113, val_acc:0.981]
Epoch [118/120    avg_loss:0.083, val_acc:0.979]
Epoch [119/120    avg_loss:0.094, val_acc:0.973]
Epoch [120/120    avg_loss:0.079, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 193   0   0   0   0  26   0   0   0   0   0   0]
 [  0   0   0 227   2   0   0   0   0   1   0   0   0   0]
 [  0   0   0   8 217   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  29 116   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.14498933901919

F1 scores:
[       nan 0.99707602 0.90610329 0.97634409 0.91368421 0.88212928
 0.99033816 0.8        0.998713   0.99893276 1.         1.
 0.99889503 1.        ]

Kappa:
0.979347868833989
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e6bc14a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.023]
Epoch [2/120    avg_loss:2.557, val_acc:0.225]
Epoch [3/120    avg_loss:2.477, val_acc:0.385]
Epoch [4/120    avg_loss:2.408, val_acc:0.402]
Epoch [5/120    avg_loss:2.345, val_acc:0.423]
Epoch [6/120    avg_loss:2.291, val_acc:0.456]
Epoch [7/120    avg_loss:2.234, val_acc:0.479]
Epoch [8/120    avg_loss:2.168, val_acc:0.481]
Epoch [9/120    avg_loss:2.109, val_acc:0.496]
Epoch [10/120    avg_loss:2.056, val_acc:0.510]
Epoch [11/120    avg_loss:2.012, val_acc:0.519]
Epoch [12/120    avg_loss:1.949, val_acc:0.533]
Epoch [13/120    avg_loss:1.896, val_acc:0.535]
Epoch [14/120    avg_loss:1.858, val_acc:0.585]
Epoch [15/120    avg_loss:1.809, val_acc:0.565]
Epoch [16/120    avg_loss:1.751, val_acc:0.577]
Epoch [17/120    avg_loss:1.693, val_acc:0.604]
Epoch [18/120    avg_loss:1.626, val_acc:0.671]
Epoch [19/120    avg_loss:1.594, val_acc:0.715]
Epoch [20/120    avg_loss:1.518, val_acc:0.725]
Epoch [21/120    avg_loss:1.464, val_acc:0.771]
Epoch [22/120    avg_loss:1.397, val_acc:0.808]
Epoch [23/120    avg_loss:1.319, val_acc:0.802]
Epoch [24/120    avg_loss:1.276, val_acc:0.829]
Epoch [25/120    avg_loss:1.254, val_acc:0.735]
Epoch [26/120    avg_loss:1.174, val_acc:0.858]
Epoch [27/120    avg_loss:1.114, val_acc:0.883]
Epoch [28/120    avg_loss:1.038, val_acc:0.848]
Epoch [29/120    avg_loss:1.029, val_acc:0.860]
Epoch [30/120    avg_loss:0.978, val_acc:0.890]
Epoch [31/120    avg_loss:0.868, val_acc:0.885]
Epoch [32/120    avg_loss:0.851, val_acc:0.902]
Epoch [33/120    avg_loss:0.791, val_acc:0.906]
Epoch [34/120    avg_loss:0.761, val_acc:0.881]
Epoch [35/120    avg_loss:0.744, val_acc:0.915]
Epoch [36/120    avg_loss:0.686, val_acc:0.925]
Epoch [37/120    avg_loss:0.672, val_acc:0.896]
Epoch [38/120    avg_loss:0.627, val_acc:0.904]
Epoch [39/120    avg_loss:0.603, val_acc:0.902]
Epoch [40/120    avg_loss:0.531, val_acc:0.927]
Epoch [41/120    avg_loss:0.564, val_acc:0.915]
Epoch [42/120    avg_loss:0.498, val_acc:0.908]
Epoch [43/120    avg_loss:0.505, val_acc:0.923]
Epoch [44/120    avg_loss:0.456, val_acc:0.940]
Epoch [45/120    avg_loss:0.440, val_acc:0.925]
Epoch [46/120    avg_loss:0.441, val_acc:0.923]
Epoch [47/120    avg_loss:0.432, val_acc:0.873]
Epoch [48/120    avg_loss:0.445, val_acc:0.923]
Epoch [49/120    avg_loss:0.429, val_acc:0.921]
Epoch [50/120    avg_loss:0.456, val_acc:0.933]
Epoch [51/120    avg_loss:0.443, val_acc:0.946]
Epoch [52/120    avg_loss:0.456, val_acc:0.917]
Epoch [53/120    avg_loss:0.516, val_acc:0.904]
Epoch [54/120    avg_loss:0.443, val_acc:0.927]
Epoch [55/120    avg_loss:0.399, val_acc:0.948]
Epoch [56/120    avg_loss:0.356, val_acc:0.929]
Epoch [57/120    avg_loss:0.321, val_acc:0.938]
Epoch [58/120    avg_loss:0.359, val_acc:0.915]
Epoch [59/120    avg_loss:0.318, val_acc:0.942]
Epoch [60/120    avg_loss:0.293, val_acc:0.929]
Epoch [61/120    avg_loss:0.261, val_acc:0.929]
Epoch [62/120    avg_loss:0.264, val_acc:0.952]
Epoch [63/120    avg_loss:0.306, val_acc:0.944]
Epoch [64/120    avg_loss:0.267, val_acc:0.946]
Epoch [65/120    avg_loss:0.261, val_acc:0.925]
Epoch [66/120    avg_loss:0.292, val_acc:0.925]
Epoch [67/120    avg_loss:0.278, val_acc:0.942]
Epoch [68/120    avg_loss:0.254, val_acc:0.954]
Epoch [69/120    avg_loss:0.225, val_acc:0.952]
Epoch [70/120    avg_loss:0.214, val_acc:0.946]
Epoch [71/120    avg_loss:0.246, val_acc:0.948]
Epoch [72/120    avg_loss:0.275, val_acc:0.931]
Epoch [73/120    avg_loss:0.263, val_acc:0.940]
Epoch [74/120    avg_loss:0.305, val_acc:0.912]
Epoch [75/120    avg_loss:0.407, val_acc:0.931]
Epoch [76/120    avg_loss:0.257, val_acc:0.944]
Epoch [77/120    avg_loss:0.231, val_acc:0.952]
Epoch [78/120    avg_loss:0.174, val_acc:0.956]
Epoch [79/120    avg_loss:0.232, val_acc:0.942]
Epoch [80/120    avg_loss:0.273, val_acc:0.950]
Epoch [81/120    avg_loss:0.269, val_acc:0.960]
Epoch [82/120    avg_loss:0.187, val_acc:0.965]
Epoch [83/120    avg_loss:0.181, val_acc:0.963]
Epoch [84/120    avg_loss:0.231, val_acc:0.942]
Epoch [85/120    avg_loss:0.222, val_acc:0.956]
Epoch [86/120    avg_loss:0.204, val_acc:0.954]
Epoch [87/120    avg_loss:0.145, val_acc:0.963]
Epoch [88/120    avg_loss:0.162, val_acc:0.963]
Epoch [89/120    avg_loss:0.153, val_acc:0.963]
Epoch [90/120    avg_loss:0.144, val_acc:0.965]
Epoch [91/120    avg_loss:0.125, val_acc:0.965]
Epoch [92/120    avg_loss:0.152, val_acc:0.971]
Epoch [93/120    avg_loss:0.194, val_acc:0.948]
Epoch [94/120    avg_loss:0.175, val_acc:0.963]
Epoch [95/120    avg_loss:0.143, val_acc:0.954]
Epoch [96/120    avg_loss:0.149, val_acc:0.958]
Epoch [97/120    avg_loss:0.157, val_acc:0.946]
Epoch [98/120    avg_loss:0.144, val_acc:0.956]
Epoch [99/120    avg_loss:0.132, val_acc:0.965]
Epoch [100/120    avg_loss:0.131, val_acc:0.950]
Epoch [101/120    avg_loss:0.133, val_acc:0.965]
Epoch [102/120    avg_loss:0.114, val_acc:0.977]
Epoch [103/120    avg_loss:0.113, val_acc:0.973]
Epoch [104/120    avg_loss:0.132, val_acc:0.963]
Epoch [105/120    avg_loss:0.128, val_acc:0.954]
Epoch [106/120    avg_loss:0.111, val_acc:0.969]
Epoch [107/120    avg_loss:0.096, val_acc:0.950]
Epoch [108/120    avg_loss:0.092, val_acc:0.973]
Epoch [109/120    avg_loss:0.095, val_acc:0.977]
Epoch [110/120    avg_loss:0.097, val_acc:0.971]
Epoch [111/120    avg_loss:0.124, val_acc:0.950]
Epoch [112/120    avg_loss:0.168, val_acc:0.952]
Epoch [113/120    avg_loss:0.179, val_acc:0.956]
Epoch [114/120    avg_loss:0.162, val_acc:0.902]
Epoch [115/120    avg_loss:0.186, val_acc:0.948]
Epoch [116/120    avg_loss:0.128, val_acc:0.958]
Epoch [117/120    avg_loss:0.136, val_acc:0.960]
Epoch [118/120    avg_loss:0.102, val_acc:0.963]
Epoch [119/120    avg_loss:0.109, val_acc:0.969]
Epoch [120/120    avg_loss:0.092, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 208  18   0   0   0   4   0   0   0   0   0]
 [  0   0   1   0 211  14   0   0   1   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0  58   0   0   6   3 139   0   0   0   0   0   0   0]
 [  0   0  27   0   0   0   0  67   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 448   5]
 [  0   0   1   0   0   0   0   0   0   0   0   0   0 833]]

Accuracy:
96.4818763326226

F1 scores:
[       nan 0.95938375 0.93562232 0.94977169 0.8647541  0.84697509
 0.8057971  0.82716049 0.99359795 1.         1.         1.
 0.99445061 0.99641148]

Kappa:
0.9607645798740065
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4ac311da20>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.645, val_acc:0.081]
Epoch [2/120    avg_loss:2.583, val_acc:0.163]
Epoch [3/120    avg_loss:2.516, val_acc:0.342]
Epoch [4/120    avg_loss:2.459, val_acc:0.354]
Epoch [5/120    avg_loss:2.406, val_acc:0.381]
Epoch [6/120    avg_loss:2.361, val_acc:0.440]
Epoch [7/120    avg_loss:2.312, val_acc:0.471]
Epoch [8/120    avg_loss:2.279, val_acc:0.488]
Epoch [9/120    avg_loss:2.220, val_acc:0.512]
Epoch [10/120    avg_loss:2.152, val_acc:0.535]
Epoch [11/120    avg_loss:2.103, val_acc:0.527]
Epoch [12/120    avg_loss:2.044, val_acc:0.529]
Epoch [13/120    avg_loss:1.980, val_acc:0.544]
Epoch [14/120    avg_loss:1.929, val_acc:0.573]
Epoch [15/120    avg_loss:1.848, val_acc:0.600]
Epoch [16/120    avg_loss:1.768, val_acc:0.608]
Epoch [17/120    avg_loss:1.714, val_acc:0.612]
Epoch [18/120    avg_loss:1.635, val_acc:0.640]
Epoch [19/120    avg_loss:1.544, val_acc:0.637]
Epoch [20/120    avg_loss:1.498, val_acc:0.646]
Epoch [21/120    avg_loss:1.416, val_acc:0.665]
Epoch [22/120    avg_loss:1.365, val_acc:0.642]
Epoch [23/120    avg_loss:1.311, val_acc:0.675]
Epoch [24/120    avg_loss:1.252, val_acc:0.673]
Epoch [25/120    avg_loss:1.187, val_acc:0.742]
Epoch [26/120    avg_loss:1.130, val_acc:0.754]
Epoch [27/120    avg_loss:1.074, val_acc:0.738]
Epoch [28/120    avg_loss:1.022, val_acc:0.844]
Epoch [29/120    avg_loss:0.992, val_acc:0.842]
Epoch [30/120    avg_loss:0.932, val_acc:0.854]
Epoch [31/120    avg_loss:0.921, val_acc:0.877]
Epoch [32/120    avg_loss:0.849, val_acc:0.867]
Epoch [33/120    avg_loss:0.817, val_acc:0.896]
Epoch [34/120    avg_loss:0.782, val_acc:0.915]
Epoch [35/120    avg_loss:0.751, val_acc:0.898]
Epoch [36/120    avg_loss:0.701, val_acc:0.900]
Epoch [37/120    avg_loss:0.665, val_acc:0.925]
Epoch [38/120    avg_loss:0.603, val_acc:0.938]
Epoch [39/120    avg_loss:0.587, val_acc:0.929]
Epoch [40/120    avg_loss:0.596, val_acc:0.915]
Epoch [41/120    avg_loss:0.604, val_acc:0.929]
Epoch [42/120    avg_loss:0.553, val_acc:0.908]
Epoch [43/120    avg_loss:0.547, val_acc:0.933]
Epoch [44/120    avg_loss:0.498, val_acc:0.912]
Epoch [45/120    avg_loss:0.521, val_acc:0.906]
Epoch [46/120    avg_loss:0.525, val_acc:0.925]
Epoch [47/120    avg_loss:0.483, val_acc:0.935]
Epoch [48/120    avg_loss:0.473, val_acc:0.898]
Epoch [49/120    avg_loss:0.472, val_acc:0.923]
Epoch [50/120    avg_loss:0.411, val_acc:0.931]
Epoch [51/120    avg_loss:0.425, val_acc:0.944]
Epoch [52/120    avg_loss:0.402, val_acc:0.931]
Epoch [53/120    avg_loss:0.423, val_acc:0.940]
Epoch [54/120    avg_loss:0.390, val_acc:0.910]
Epoch [55/120    avg_loss:0.443, val_acc:0.944]
Epoch [56/120    avg_loss:0.372, val_acc:0.910]
Epoch [57/120    avg_loss:0.447, val_acc:0.944]
Epoch [58/120    avg_loss:0.388, val_acc:0.910]
Epoch [59/120    avg_loss:0.385, val_acc:0.948]
Epoch [60/120    avg_loss:0.330, val_acc:0.954]
Epoch [61/120    avg_loss:0.317, val_acc:0.942]
Epoch [62/120    avg_loss:0.333, val_acc:0.919]
Epoch [63/120    avg_loss:0.355, val_acc:0.948]
Epoch [64/120    avg_loss:0.351, val_acc:0.950]
Epoch [65/120    avg_loss:0.284, val_acc:0.946]
Epoch [66/120    avg_loss:0.297, val_acc:0.960]
Epoch [67/120    avg_loss:0.284, val_acc:0.962]
Epoch [68/120    avg_loss:0.236, val_acc:0.952]
Epoch [69/120    avg_loss:0.277, val_acc:0.942]
Epoch [70/120    avg_loss:0.315, val_acc:0.967]
Epoch [71/120    avg_loss:0.276, val_acc:0.962]
Epoch [72/120    avg_loss:0.236, val_acc:0.963]
Epoch [73/120    avg_loss:0.278, val_acc:0.927]
Epoch [74/120    avg_loss:0.276, val_acc:0.954]
Epoch [75/120    avg_loss:0.253, val_acc:0.963]
Epoch [76/120    avg_loss:0.206, val_acc:0.962]
Epoch [77/120    avg_loss:0.230, val_acc:0.973]
Epoch [78/120    avg_loss:0.238, val_acc:0.944]
Epoch [79/120    avg_loss:0.239, val_acc:0.960]
Epoch [80/120    avg_loss:0.259, val_acc:0.954]
Epoch [81/120    avg_loss:0.247, val_acc:0.942]
Epoch [82/120    avg_loss:0.215, val_acc:0.948]
Epoch [83/120    avg_loss:0.216, val_acc:0.967]
Epoch [84/120    avg_loss:0.200, val_acc:0.965]
Epoch [85/120    avg_loss:0.187, val_acc:0.977]
Epoch [86/120    avg_loss:0.210, val_acc:0.942]
Epoch [87/120    avg_loss:0.293, val_acc:0.948]
Epoch [88/120    avg_loss:0.254, val_acc:0.967]
Epoch [89/120    avg_loss:0.249, val_acc:0.962]
Epoch [90/120    avg_loss:0.240, val_acc:0.952]
Epoch [91/120    avg_loss:0.193, val_acc:0.971]
Epoch [92/120    avg_loss:0.192, val_acc:0.969]
Epoch [93/120    avg_loss:0.155, val_acc:0.975]
Epoch [94/120    avg_loss:0.226, val_acc:0.969]
Epoch [95/120    avg_loss:0.201, val_acc:0.960]
Epoch [96/120    avg_loss:0.178, val_acc:0.975]
Epoch [97/120    avg_loss:0.213, val_acc:0.923]
Epoch [98/120    avg_loss:0.232, val_acc:0.967]
Epoch [99/120    avg_loss:0.173, val_acc:0.965]
Epoch [100/120    avg_loss:0.134, val_acc:0.973]
Epoch [101/120    avg_loss:0.131, val_acc:0.973]
Epoch [102/120    avg_loss:0.141, val_acc:0.979]
Epoch [103/120    avg_loss:0.146, val_acc:0.981]
Epoch [104/120    avg_loss:0.125, val_acc:0.979]
Epoch [105/120    avg_loss:0.131, val_acc:0.979]
Epoch [106/120    avg_loss:0.148, val_acc:0.981]
Epoch [107/120    avg_loss:0.128, val_acc:0.979]
Epoch [108/120    avg_loss:0.151, val_acc:0.977]
Epoch [109/120    avg_loss:0.119, val_acc:0.977]
Epoch [110/120    avg_loss:0.128, val_acc:0.977]
Epoch [111/120    avg_loss:0.123, val_acc:0.977]
Epoch [112/120    avg_loss:0.123, val_acc:0.977]
Epoch [113/120    avg_loss:0.125, val_acc:0.977]
Epoch [114/120    avg_loss:0.121, val_acc:0.979]
Epoch [115/120    avg_loss:0.119, val_acc:0.979]
Epoch [116/120    avg_loss:0.118, val_acc:0.979]
Epoch [117/120    avg_loss:0.124, val_acc:0.977]
Epoch [118/120    avg_loss:0.130, val_acc:0.979]
Epoch [119/120    avg_loss:0.107, val_acc:0.979]
Epoch [120/120    avg_loss:0.112, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   1 216   3   5   0   0   5   0   0   0   0   0]
 [  0   0   0   1 194  32   0   0   0   0   0   0   0   0]
 [  0   0   0   6  25 114   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  21   0   0   0   0  73   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.59061833688699

F1 scores:
[       nan 0.99708879 0.92888889 0.95364238 0.86414254 0.77027027
 0.99019608 0.82485876 0.99359795 1.         1.         1.
 1.         1.        ]

Kappa:
0.9731710410048167
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e19205a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.204]
Epoch [2/120    avg_loss:2.553, val_acc:0.321]
Epoch [3/120    avg_loss:2.479, val_acc:0.327]
Epoch [4/120    avg_loss:2.412, val_acc:0.340]
Epoch [5/120    avg_loss:2.349, val_acc:0.323]
Epoch [6/120    avg_loss:2.305, val_acc:0.306]
Epoch [7/120    avg_loss:2.249, val_acc:0.329]
Epoch [8/120    avg_loss:2.196, val_acc:0.365]
Epoch [9/120    avg_loss:2.151, val_acc:0.383]
Epoch [10/120    avg_loss:2.082, val_acc:0.431]
Epoch [11/120    avg_loss:2.035, val_acc:0.440]
Epoch [12/120    avg_loss:1.981, val_acc:0.465]
Epoch [13/120    avg_loss:1.945, val_acc:0.510]
Epoch [14/120    avg_loss:1.882, val_acc:0.554]
Epoch [15/120    avg_loss:1.846, val_acc:0.577]
Epoch [16/120    avg_loss:1.794, val_acc:0.613]
Epoch [17/120    avg_loss:1.756, val_acc:0.646]
Epoch [18/120    avg_loss:1.708, val_acc:0.669]
Epoch [19/120    avg_loss:1.649, val_acc:0.660]
Epoch [20/120    avg_loss:1.606, val_acc:0.685]
Epoch [21/120    avg_loss:1.555, val_acc:0.715]
Epoch [22/120    avg_loss:1.461, val_acc:0.698]
Epoch [23/120    avg_loss:1.399, val_acc:0.750]
Epoch [24/120    avg_loss:1.345, val_acc:0.771]
Epoch [25/120    avg_loss:1.288, val_acc:0.798]
Epoch [26/120    avg_loss:1.227, val_acc:0.783]
Epoch [27/120    avg_loss:1.154, val_acc:0.875]
Epoch [28/120    avg_loss:1.116, val_acc:0.773]
Epoch [29/120    avg_loss:1.054, val_acc:0.819]
Epoch [30/120    avg_loss:0.963, val_acc:0.900]
Epoch [31/120    avg_loss:0.898, val_acc:0.887]
Epoch [32/120    avg_loss:0.858, val_acc:0.760]
Epoch [33/120    avg_loss:0.802, val_acc:0.902]
Epoch [34/120    avg_loss:0.774, val_acc:0.894]
Epoch [35/120    avg_loss:0.720, val_acc:0.825]
Epoch [36/120    avg_loss:0.671, val_acc:0.887]
Epoch [37/120    avg_loss:0.624, val_acc:0.902]
Epoch [38/120    avg_loss:0.608, val_acc:0.892]
Epoch [39/120    avg_loss:0.600, val_acc:0.875]
Epoch [40/120    avg_loss:0.551, val_acc:0.906]
Epoch [41/120    avg_loss:0.594, val_acc:0.883]
Epoch [42/120    avg_loss:0.555, val_acc:0.840]
Epoch [43/120    avg_loss:0.621, val_acc:0.919]
Epoch [44/120    avg_loss:0.534, val_acc:0.917]
Epoch [45/120    avg_loss:0.462, val_acc:0.925]
Epoch [46/120    avg_loss:0.498, val_acc:0.917]
Epoch [47/120    avg_loss:0.430, val_acc:0.923]
Epoch [48/120    avg_loss:0.466, val_acc:0.921]
Epoch [49/120    avg_loss:0.461, val_acc:0.946]
Epoch [50/120    avg_loss:0.417, val_acc:0.935]
Epoch [51/120    avg_loss:0.405, val_acc:0.912]
Epoch [52/120    avg_loss:0.366, val_acc:0.942]
Epoch [53/120    avg_loss:0.378, val_acc:0.935]
Epoch [54/120    avg_loss:0.324, val_acc:0.944]
Epoch [55/120    avg_loss:0.320, val_acc:0.933]
Epoch [56/120    avg_loss:0.326, val_acc:0.942]
Epoch [57/120    avg_loss:0.301, val_acc:0.950]
Epoch [58/120    avg_loss:0.295, val_acc:0.923]
Epoch [59/120    avg_loss:0.323, val_acc:0.938]
Epoch [60/120    avg_loss:0.276, val_acc:0.944]
Epoch [61/120    avg_loss:0.282, val_acc:0.921]
Epoch [62/120    avg_loss:0.346, val_acc:0.919]
Epoch [63/120    avg_loss:0.267, val_acc:0.954]
Epoch [64/120    avg_loss:0.268, val_acc:0.963]
Epoch [65/120    avg_loss:0.281, val_acc:0.952]
Epoch [66/120    avg_loss:0.288, val_acc:0.960]
Epoch [67/120    avg_loss:0.222, val_acc:0.956]
Epoch [68/120    avg_loss:0.239, val_acc:0.954]
Epoch [69/120    avg_loss:0.275, val_acc:0.948]
Epoch [70/120    avg_loss:0.306, val_acc:0.867]
Epoch [71/120    avg_loss:0.232, val_acc:0.954]
Epoch [72/120    avg_loss:0.252, val_acc:0.935]
Epoch [73/120    avg_loss:0.262, val_acc:0.942]
Epoch [74/120    avg_loss:0.325, val_acc:0.963]
Epoch [75/120    avg_loss:0.229, val_acc:0.971]
Epoch [76/120    avg_loss:0.215, val_acc:0.958]
Epoch [77/120    avg_loss:0.235, val_acc:0.967]
Epoch [78/120    avg_loss:0.246, val_acc:0.935]
Epoch [79/120    avg_loss:0.215, val_acc:0.956]
Epoch [80/120    avg_loss:0.256, val_acc:0.952]
Epoch [81/120    avg_loss:0.291, val_acc:0.958]
Epoch [82/120    avg_loss:0.182, val_acc:0.967]
Epoch [83/120    avg_loss:0.171, val_acc:0.969]
Epoch [84/120    avg_loss:0.148, val_acc:0.973]
Epoch [85/120    avg_loss:0.154, val_acc:0.969]
Epoch [86/120    avg_loss:0.187, val_acc:0.956]
Epoch [87/120    avg_loss:0.138, val_acc:0.971]
Epoch [88/120    avg_loss:0.182, val_acc:0.952]
Epoch [89/120    avg_loss:0.205, val_acc:0.946]
Epoch [90/120    avg_loss:0.211, val_acc:0.981]
Epoch [91/120    avg_loss:0.143, val_acc:0.981]
Epoch [92/120    avg_loss:0.140, val_acc:0.975]
Epoch [93/120    avg_loss:0.122, val_acc:0.981]
Epoch [94/120    avg_loss:0.132, val_acc:0.979]
Epoch [95/120    avg_loss:0.141, val_acc:0.990]
Epoch [96/120    avg_loss:0.118, val_acc:0.983]
Epoch [97/120    avg_loss:0.129, val_acc:0.971]
Epoch [98/120    avg_loss:0.161, val_acc:0.977]
Epoch [99/120    avg_loss:0.142, val_acc:0.954]
Epoch [100/120    avg_loss:0.148, val_acc:0.971]
Epoch [101/120    avg_loss:0.116, val_acc:0.971]
Epoch [102/120    avg_loss:0.123, val_acc:0.956]
Epoch [103/120    avg_loss:0.146, val_acc:0.975]
Epoch [104/120    avg_loss:0.142, val_acc:0.985]
Epoch [105/120    avg_loss:0.126, val_acc:0.981]
Epoch [106/120    avg_loss:0.087, val_acc:0.985]
Epoch [107/120    avg_loss:0.109, val_acc:0.963]
Epoch [108/120    avg_loss:0.130, val_acc:0.967]
Epoch [109/120    avg_loss:0.129, val_acc:0.973]
Epoch [110/120    avg_loss:0.086, val_acc:0.981]
Epoch [111/120    avg_loss:0.094, val_acc:0.988]
Epoch [112/120    avg_loss:0.080, val_acc:0.992]
Epoch [113/120    avg_loss:0.078, val_acc:0.985]
Epoch [114/120    avg_loss:0.074, val_acc:0.990]
Epoch [115/120    avg_loss:0.085, val_acc:0.992]
Epoch [116/120    avg_loss:0.072, val_acc:0.990]
Epoch [117/120    avg_loss:0.068, val_acc:0.988]
Epoch [118/120    avg_loss:0.065, val_acc:0.990]
Epoch [119/120    avg_loss:0.090, val_acc:0.985]
Epoch [120/120    avg_loss:0.076, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   1   0   0   1   0   0   0   0   0   0]
 [  0   0   0 220   9   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0   0 382   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 452   1]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.272921108742

F1 scores:
[       nan 0.997815   0.94347826 0.97777778 0.88791209 0.86287625
 0.99266504 0.88888889 0.99092088 1.         1.         1.
 0.99889503 0.99940084]

Kappa:
0.9807698343195865
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f24e8f37ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.630, val_acc:0.140]
Epoch [2/120    avg_loss:2.558, val_acc:0.231]
Epoch [3/120    avg_loss:2.485, val_acc:0.375]
Epoch [4/120    avg_loss:2.414, val_acc:0.463]
Epoch [5/120    avg_loss:2.355, val_acc:0.500]
Epoch [6/120    avg_loss:2.302, val_acc:0.519]
Epoch [7/120    avg_loss:2.243, val_acc:0.515]
Epoch [8/120    avg_loss:2.184, val_acc:0.506]
Epoch [9/120    avg_loss:2.121, val_acc:0.504]
Epoch [10/120    avg_loss:2.061, val_acc:0.542]
Epoch [11/120    avg_loss:1.976, val_acc:0.577]
Epoch [12/120    avg_loss:1.922, val_acc:0.592]
Epoch [13/120    avg_loss:1.852, val_acc:0.656]
Epoch [14/120    avg_loss:1.781, val_acc:0.694]
Epoch [15/120    avg_loss:1.707, val_acc:0.746]
Epoch [16/120    avg_loss:1.637, val_acc:0.777]
Epoch [17/120    avg_loss:1.583, val_acc:0.760]
Epoch [18/120    avg_loss:1.532, val_acc:0.806]
Epoch [19/120    avg_loss:1.449, val_acc:0.765]
Epoch [20/120    avg_loss:1.387, val_acc:0.827]
Epoch [21/120    avg_loss:1.311, val_acc:0.840]
Epoch [22/120    avg_loss:1.250, val_acc:0.856]
Epoch [23/120    avg_loss:1.139, val_acc:0.856]
Epoch [24/120    avg_loss:1.095, val_acc:0.865]
Epoch [25/120    avg_loss:1.035, val_acc:0.896]
Epoch [26/120    avg_loss:0.989, val_acc:0.898]
Epoch [27/120    avg_loss:0.909, val_acc:0.912]
Epoch [28/120    avg_loss:0.851, val_acc:0.906]
Epoch [29/120    avg_loss:0.786, val_acc:0.919]
Epoch [30/120    avg_loss:0.729, val_acc:0.896]
Epoch [31/120    avg_loss:0.720, val_acc:0.902]
Epoch [32/120    avg_loss:0.657, val_acc:0.910]
Epoch [33/120    avg_loss:0.610, val_acc:0.927]
Epoch [34/120    avg_loss:0.580, val_acc:0.881]
Epoch [35/120    avg_loss:0.575, val_acc:0.942]
Epoch [36/120    avg_loss:0.545, val_acc:0.940]
Epoch [37/120    avg_loss:0.549, val_acc:0.927]
Epoch [38/120    avg_loss:0.533, val_acc:0.915]
Epoch [39/120    avg_loss:0.498, val_acc:0.919]
Epoch [40/120    avg_loss:0.449, val_acc:0.942]
Epoch [41/120    avg_loss:0.431, val_acc:0.948]
Epoch [42/120    avg_loss:0.436, val_acc:0.933]
Epoch [43/120    avg_loss:0.413, val_acc:0.942]
Epoch [44/120    avg_loss:0.374, val_acc:0.948]
Epoch [45/120    avg_loss:0.420, val_acc:0.908]
Epoch [46/120    avg_loss:0.417, val_acc:0.938]
Epoch [47/120    avg_loss:0.370, val_acc:0.929]
Epoch [48/120    avg_loss:0.373, val_acc:0.938]
Epoch [49/120    avg_loss:0.339, val_acc:0.944]
Epoch [50/120    avg_loss:0.310, val_acc:0.940]
Epoch [51/120    avg_loss:0.314, val_acc:0.940]
Epoch [52/120    avg_loss:0.308, val_acc:0.948]
Epoch [53/120    avg_loss:0.316, val_acc:0.948]
Epoch [54/120    avg_loss:0.271, val_acc:0.946]
Epoch [55/120    avg_loss:0.278, val_acc:0.942]
Epoch [56/120    avg_loss:0.315, val_acc:0.956]
Epoch [57/120    avg_loss:0.297, val_acc:0.954]
Epoch [58/120    avg_loss:0.267, val_acc:0.946]
Epoch [59/120    avg_loss:0.344, val_acc:0.948]
Epoch [60/120    avg_loss:0.309, val_acc:0.923]
Epoch [61/120    avg_loss:0.301, val_acc:0.942]
Epoch [62/120    avg_loss:0.265, val_acc:0.938]
Epoch [63/120    avg_loss:0.270, val_acc:0.946]
Epoch [64/120    avg_loss:0.223, val_acc:0.954]
Epoch [65/120    avg_loss:0.257, val_acc:0.829]
Epoch [66/120    avg_loss:0.320, val_acc:0.923]
Epoch [67/120    avg_loss:0.303, val_acc:0.948]
Epoch [68/120    avg_loss:0.292, val_acc:0.938]
Epoch [69/120    avg_loss:0.287, val_acc:0.958]
Epoch [70/120    avg_loss:0.270, val_acc:0.950]
Epoch [71/120    avg_loss:0.233, val_acc:0.963]
Epoch [72/120    avg_loss:0.195, val_acc:0.958]
Epoch [73/120    avg_loss:0.214, val_acc:0.954]
Epoch [74/120    avg_loss:0.250, val_acc:0.935]
Epoch [75/120    avg_loss:0.289, val_acc:0.952]
Epoch [76/120    avg_loss:0.326, val_acc:0.946]
Epoch [77/120    avg_loss:0.305, val_acc:0.950]
Epoch [78/120    avg_loss:0.253, val_acc:0.944]
Epoch [79/120    avg_loss:0.215, val_acc:0.956]
Epoch [80/120    avg_loss:0.205, val_acc:0.963]
Epoch [81/120    avg_loss:0.207, val_acc:0.940]
Epoch [82/120    avg_loss:0.243, val_acc:0.948]
Epoch [83/120    avg_loss:0.178, val_acc:0.965]
Epoch [84/120    avg_loss:0.171, val_acc:0.954]
Epoch [85/120    avg_loss:0.183, val_acc:0.958]
Epoch [86/120    avg_loss:0.164, val_acc:0.963]
Epoch [87/120    avg_loss:0.138, val_acc:0.977]
Epoch [88/120    avg_loss:0.154, val_acc:0.965]
Epoch [89/120    avg_loss:0.220, val_acc:0.942]
Epoch [90/120    avg_loss:0.214, val_acc:0.948]
Epoch [91/120    avg_loss:0.206, val_acc:0.954]
Epoch [92/120    avg_loss:0.202, val_acc:0.958]
Epoch [93/120    avg_loss:0.193, val_acc:0.956]
Epoch [94/120    avg_loss:0.175, val_acc:0.967]
Epoch [95/120    avg_loss:0.197, val_acc:0.954]
Epoch [96/120    avg_loss:0.207, val_acc:0.956]
Epoch [97/120    avg_loss:0.226, val_acc:0.956]
Epoch [98/120    avg_loss:0.195, val_acc:0.963]
Epoch [99/120    avg_loss:0.163, val_acc:0.967]
Epoch [100/120    avg_loss:0.195, val_acc:0.958]
Epoch [101/120    avg_loss:0.160, val_acc:0.971]
Epoch [102/120    avg_loss:0.123, val_acc:0.975]
Epoch [103/120    avg_loss:0.132, val_acc:0.977]
Epoch [104/120    avg_loss:0.107, val_acc:0.981]
Epoch [105/120    avg_loss:0.114, val_acc:0.983]
Epoch [106/120    avg_loss:0.104, val_acc:0.981]
Epoch [107/120    avg_loss:0.097, val_acc:0.981]
Epoch [108/120    avg_loss:0.104, val_acc:0.983]
Epoch [109/120    avg_loss:0.103, val_acc:0.983]
Epoch [110/120    avg_loss:0.098, val_acc:0.983]
Epoch [111/120    avg_loss:0.091, val_acc:0.983]
Epoch [112/120    avg_loss:0.092, val_acc:0.981]
Epoch [113/120    avg_loss:0.106, val_acc:0.983]
Epoch [114/120    avg_loss:0.095, val_acc:0.985]
Epoch [115/120    avg_loss:0.093, val_acc:0.981]
Epoch [116/120    avg_loss:0.091, val_acc:0.981]
Epoch [117/120    avg_loss:0.092, val_acc:0.981]
Epoch [118/120    avg_loss:0.099, val_acc:0.981]
Epoch [119/120    avg_loss:0.096, val_acc:0.979]
Epoch [120/120    avg_loss:0.098, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   2   0   0   5   0   0   0   0   0   0]
 [  0   0   2 220   6   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  29 116   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  23   0   0   0   0  71   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.10234541577825

F1 scores:
[       nan 1.         0.92982456 0.97777778 0.88185654 0.83154122
 0.99756691 0.83529412 0.99742931 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.978869263356328
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0b6953a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.607, val_acc:0.279]
Epoch [2/120    avg_loss:2.526, val_acc:0.348]
Epoch [3/120    avg_loss:2.449, val_acc:0.367]
Epoch [4/120    avg_loss:2.397, val_acc:0.408]
Epoch [5/120    avg_loss:2.339, val_acc:0.415]
Epoch [6/120    avg_loss:2.289, val_acc:0.417]
Epoch [7/120    avg_loss:2.259, val_acc:0.402]
Epoch [8/120    avg_loss:2.228, val_acc:0.400]
Epoch [9/120    avg_loss:2.169, val_acc:0.348]
Epoch [10/120    avg_loss:2.121, val_acc:0.321]
Epoch [11/120    avg_loss:2.080, val_acc:0.317]
Epoch [12/120    avg_loss:2.043, val_acc:0.417]
Epoch [13/120    avg_loss:1.975, val_acc:0.469]
Epoch [14/120    avg_loss:1.930, val_acc:0.548]
Epoch [15/120    avg_loss:1.865, val_acc:0.544]
Epoch [16/120    avg_loss:1.829, val_acc:0.554]
Epoch [17/120    avg_loss:1.793, val_acc:0.575]
Epoch [18/120    avg_loss:1.708, val_acc:0.608]
Epoch [19/120    avg_loss:1.659, val_acc:0.635]
Epoch [20/120    avg_loss:1.617, val_acc:0.623]
Epoch [21/120    avg_loss:1.567, val_acc:0.637]
Epoch [22/120    avg_loss:1.525, val_acc:0.675]
Epoch [23/120    avg_loss:1.437, val_acc:0.677]
Epoch [24/120    avg_loss:1.383, val_acc:0.673]
Epoch [25/120    avg_loss:1.339, val_acc:0.725]
Epoch [26/120    avg_loss:1.284, val_acc:0.733]
Epoch [27/120    avg_loss:1.245, val_acc:0.738]
Epoch [28/120    avg_loss:1.174, val_acc:0.742]
Epoch [29/120    avg_loss:1.129, val_acc:0.767]
Epoch [30/120    avg_loss:1.109, val_acc:0.719]
Epoch [31/120    avg_loss:1.045, val_acc:0.758]
Epoch [32/120    avg_loss:0.996, val_acc:0.740]
Epoch [33/120    avg_loss:0.955, val_acc:0.835]
Epoch [34/120    avg_loss:0.929, val_acc:0.750]
Epoch [35/120    avg_loss:0.908, val_acc:0.752]
Epoch [36/120    avg_loss:0.899, val_acc:0.781]
Epoch [37/120    avg_loss:0.818, val_acc:0.844]
Epoch [38/120    avg_loss:0.777, val_acc:0.883]
Epoch [39/120    avg_loss:0.757, val_acc:0.848]
Epoch [40/120    avg_loss:0.667, val_acc:0.919]
Epoch [41/120    avg_loss:0.684, val_acc:0.842]
Epoch [42/120    avg_loss:0.639, val_acc:0.912]
Epoch [43/120    avg_loss:0.609, val_acc:0.929]
Epoch [44/120    avg_loss:0.551, val_acc:0.844]
Epoch [45/120    avg_loss:0.571, val_acc:0.931]
Epoch [46/120    avg_loss:0.529, val_acc:0.854]
Epoch [47/120    avg_loss:0.562, val_acc:0.910]
Epoch [48/120    avg_loss:0.530, val_acc:0.854]
Epoch [49/120    avg_loss:0.536, val_acc:0.931]
Epoch [50/120    avg_loss:0.502, val_acc:0.898]
Epoch [51/120    avg_loss:0.468, val_acc:0.915]
Epoch [52/120    avg_loss:0.473, val_acc:0.929]
Epoch [53/120    avg_loss:0.434, val_acc:0.906]
Epoch [54/120    avg_loss:0.468, val_acc:0.892]
Epoch [55/120    avg_loss:0.467, val_acc:0.933]
Epoch [56/120    avg_loss:0.408, val_acc:0.942]
Epoch [57/120    avg_loss:0.395, val_acc:0.944]
Epoch [58/120    avg_loss:0.407, val_acc:0.948]
Epoch [59/120    avg_loss:0.442, val_acc:0.869]
Epoch [60/120    avg_loss:0.464, val_acc:0.942]
Epoch [61/120    avg_loss:0.432, val_acc:0.921]
Epoch [62/120    avg_loss:0.422, val_acc:0.958]
Epoch [63/120    avg_loss:0.426, val_acc:0.940]
Epoch [64/120    avg_loss:0.421, val_acc:0.931]
Epoch [65/120    avg_loss:0.332, val_acc:0.952]
Epoch [66/120    avg_loss:0.305, val_acc:0.954]
Epoch [67/120    avg_loss:0.285, val_acc:0.952]
Epoch [68/120    avg_loss:0.293, val_acc:0.950]
Epoch [69/120    avg_loss:0.305, val_acc:0.950]
Epoch [70/120    avg_loss:0.288, val_acc:0.954]
Epoch [71/120    avg_loss:0.279, val_acc:0.954]
Epoch [72/120    avg_loss:0.274, val_acc:0.963]
Epoch [73/120    avg_loss:0.244, val_acc:0.956]
Epoch [74/120    avg_loss:0.241, val_acc:0.954]
Epoch [75/120    avg_loss:0.318, val_acc:0.933]
Epoch [76/120    avg_loss:0.302, val_acc:0.948]
Epoch [77/120    avg_loss:0.273, val_acc:0.960]
Epoch [78/120    avg_loss:0.257, val_acc:0.960]
Epoch [79/120    avg_loss:0.248, val_acc:0.952]
Epoch [80/120    avg_loss:0.223, val_acc:0.967]
Epoch [81/120    avg_loss:0.212, val_acc:0.950]
Epoch [82/120    avg_loss:0.210, val_acc:0.935]
Epoch [83/120    avg_loss:0.196, val_acc:0.954]
Epoch [84/120    avg_loss:0.181, val_acc:0.931]
Epoch [85/120    avg_loss:0.194, val_acc:0.971]
Epoch [86/120    avg_loss:0.193, val_acc:0.946]
Epoch [87/120    avg_loss:0.302, val_acc:0.921]
Epoch [88/120    avg_loss:0.298, val_acc:0.952]
Epoch [89/120    avg_loss:0.231, val_acc:0.963]
Epoch [90/120    avg_loss:0.259, val_acc:0.952]
Epoch [91/120    avg_loss:0.237, val_acc:0.958]
Epoch [92/120    avg_loss:0.292, val_acc:0.954]
Epoch [93/120    avg_loss:0.272, val_acc:0.950]
Epoch [94/120    avg_loss:0.216, val_acc:0.967]
Epoch [95/120    avg_loss:0.171, val_acc:0.977]
Epoch [96/120    avg_loss:0.201, val_acc:0.969]
Epoch [97/120    avg_loss:0.191, val_acc:0.958]
Epoch [98/120    avg_loss:0.169, val_acc:0.969]
Epoch [99/120    avg_loss:0.157, val_acc:0.965]
Epoch [100/120    avg_loss:0.144, val_acc:0.973]
Epoch [101/120    avg_loss:0.158, val_acc:0.973]
Epoch [102/120    avg_loss:0.150, val_acc:0.973]
Epoch [103/120    avg_loss:0.223, val_acc:0.910]
Epoch [104/120    avg_loss:0.203, val_acc:0.963]
Epoch [105/120    avg_loss:0.163, val_acc:0.971]
Epoch [106/120    avg_loss:0.177, val_acc:0.956]
Epoch [107/120    avg_loss:0.186, val_acc:0.927]
Epoch [108/120    avg_loss:0.267, val_acc:0.935]
Epoch [109/120    avg_loss:0.233, val_acc:0.958]
Epoch [110/120    avg_loss:0.156, val_acc:0.965]
Epoch [111/120    avg_loss:0.142, val_acc:0.975]
Epoch [112/120    avg_loss:0.127, val_acc:0.977]
Epoch [113/120    avg_loss:0.109, val_acc:0.977]
Epoch [114/120    avg_loss:0.121, val_acc:0.977]
Epoch [115/120    avg_loss:0.110, val_acc:0.973]
Epoch [116/120    avg_loss:0.110, val_acc:0.971]
Epoch [117/120    avg_loss:0.107, val_acc:0.971]
Epoch [118/120    avg_loss:0.110, val_acc:0.973]
Epoch [119/120    avg_loss:0.113, val_acc:0.977]
Epoch [120/120    avg_loss:0.108, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 222   6   0   0   1   1   0   0   0   0   0]
 [  0   0   0   0 200  27   0   0   0   0   0   0   0   0]
 [  0   0   0   0  39 106   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   0   0   0   0   0   0   4   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.69722814498934

F1 scores:
[       nan 0.997815   0.94642857 0.98230088 0.84745763 0.76258993
 0.99266504 0.8603352  0.99359795 1.         1.         0.99600533
 0.99226519 1.        ]

Kappa:
0.9743580748144628
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5be9bbfac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.639, val_acc:0.113]
Epoch [2/120    avg_loss:2.568, val_acc:0.285]
Epoch [3/120    avg_loss:2.495, val_acc:0.329]
Epoch [4/120    avg_loss:2.417, val_acc:0.329]
Epoch [5/120    avg_loss:2.363, val_acc:0.338]
Epoch [6/120    avg_loss:2.290, val_acc:0.348]
Epoch [7/120    avg_loss:2.235, val_acc:0.356]
Epoch [8/120    avg_loss:2.171, val_acc:0.365]
Epoch [9/120    avg_loss:2.134, val_acc:0.375]
Epoch [10/120    avg_loss:2.089, val_acc:0.385]
Epoch [11/120    avg_loss:2.037, val_acc:0.438]
Epoch [12/120    avg_loss:1.979, val_acc:0.452]
Epoch [13/120    avg_loss:1.928, val_acc:0.479]
Epoch [14/120    avg_loss:1.865, val_acc:0.490]
Epoch [15/120    avg_loss:1.833, val_acc:0.510]
Epoch [16/120    avg_loss:1.771, val_acc:0.562]
Epoch [17/120    avg_loss:1.704, val_acc:0.592]
Epoch [18/120    avg_loss:1.665, val_acc:0.642]
Epoch [19/120    avg_loss:1.612, val_acc:0.648]
Epoch [20/120    avg_loss:1.569, val_acc:0.658]
Epoch [21/120    avg_loss:1.487, val_acc:0.665]
Epoch [22/120    avg_loss:1.448, val_acc:0.710]
Epoch [23/120    avg_loss:1.407, val_acc:0.735]
Epoch [24/120    avg_loss:1.349, val_acc:0.727]
Epoch [25/120    avg_loss:1.294, val_acc:0.825]
Epoch [26/120    avg_loss:1.242, val_acc:0.819]
Epoch [27/120    avg_loss:1.154, val_acc:0.865]
Epoch [28/120    avg_loss:1.146, val_acc:0.869]
Epoch [29/120    avg_loss:1.051, val_acc:0.858]
Epoch [30/120    avg_loss:1.005, val_acc:0.881]
Epoch [31/120    avg_loss:0.935, val_acc:0.877]
Epoch [32/120    avg_loss:0.903, val_acc:0.881]
Epoch [33/120    avg_loss:0.864, val_acc:0.885]
Epoch [34/120    avg_loss:0.809, val_acc:0.885]
Epoch [35/120    avg_loss:0.793, val_acc:0.892]
Epoch [36/120    avg_loss:0.766, val_acc:0.887]
Epoch [37/120    avg_loss:0.713, val_acc:0.892]
Epoch [38/120    avg_loss:0.649, val_acc:0.902]
Epoch [39/120    avg_loss:0.629, val_acc:0.892]
Epoch [40/120    avg_loss:0.601, val_acc:0.900]
Epoch [41/120    avg_loss:0.585, val_acc:0.929]
Epoch [42/120    avg_loss:0.557, val_acc:0.915]
Epoch [43/120    avg_loss:0.508, val_acc:0.927]
Epoch [44/120    avg_loss:0.531, val_acc:0.900]
Epoch [45/120    avg_loss:0.525, val_acc:0.923]
Epoch [46/120    avg_loss:0.526, val_acc:0.919]
Epoch [47/120    avg_loss:0.482, val_acc:0.917]
Epoch [48/120    avg_loss:0.444, val_acc:0.931]
Epoch [49/120    avg_loss:0.460, val_acc:0.933]
Epoch [50/120    avg_loss:0.486, val_acc:0.904]
Epoch [51/120    avg_loss:0.454, val_acc:0.929]
Epoch [52/120    avg_loss:0.441, val_acc:0.931]
Epoch [53/120    avg_loss:0.374, val_acc:0.942]
Epoch [54/120    avg_loss:0.390, val_acc:0.931]
Epoch [55/120    avg_loss:0.480, val_acc:0.923]
Epoch [56/120    avg_loss:0.450, val_acc:0.956]
Epoch [57/120    avg_loss:0.378, val_acc:0.927]
Epoch [58/120    avg_loss:0.376, val_acc:0.944]
Epoch [59/120    avg_loss:0.329, val_acc:0.946]
Epoch [60/120    avg_loss:0.335, val_acc:0.912]
Epoch [61/120    avg_loss:0.430, val_acc:0.919]
Epoch [62/120    avg_loss:0.383, val_acc:0.940]
Epoch [63/120    avg_loss:0.352, val_acc:0.940]
Epoch [64/120    avg_loss:0.355, val_acc:0.938]
Epoch [65/120    avg_loss:0.342, val_acc:0.942]
Epoch [66/120    avg_loss:0.297, val_acc:0.960]
Epoch [67/120    avg_loss:0.307, val_acc:0.950]
Epoch [68/120    avg_loss:0.243, val_acc:0.950]
Epoch [69/120    avg_loss:0.249, val_acc:0.956]
Epoch [70/120    avg_loss:0.246, val_acc:0.948]
Epoch [71/120    avg_loss:0.259, val_acc:0.940]
Epoch [72/120    avg_loss:0.248, val_acc:0.950]
Epoch [73/120    avg_loss:0.206, val_acc:0.963]
Epoch [74/120    avg_loss:0.249, val_acc:0.956]
Epoch [75/120    avg_loss:0.258, val_acc:0.954]
Epoch [76/120    avg_loss:0.268, val_acc:0.965]
Epoch [77/120    avg_loss:0.264, val_acc:0.940]
Epoch [78/120    avg_loss:0.283, val_acc:0.885]
Epoch [79/120    avg_loss:0.233, val_acc:0.944]
Epoch [80/120    avg_loss:0.330, val_acc:0.902]
Epoch [81/120    avg_loss:0.275, val_acc:0.940]
Epoch [82/120    avg_loss:0.212, val_acc:0.954]
Epoch [83/120    avg_loss:0.219, val_acc:0.938]
Epoch [84/120    avg_loss:0.207, val_acc:0.948]
Epoch [85/120    avg_loss:0.243, val_acc:0.948]
Epoch [86/120    avg_loss:0.202, val_acc:0.948]
Epoch [87/120    avg_loss:0.217, val_acc:0.946]
Epoch [88/120    avg_loss:0.154, val_acc:0.969]
Epoch [89/120    avg_loss:0.166, val_acc:0.967]
Epoch [90/120    avg_loss:0.157, val_acc:0.973]
Epoch [91/120    avg_loss:0.171, val_acc:0.956]
Epoch [92/120    avg_loss:0.202, val_acc:0.965]
Epoch [93/120    avg_loss:0.241, val_acc:0.954]
Epoch [94/120    avg_loss:0.215, val_acc:0.969]
Epoch [95/120    avg_loss:0.285, val_acc:0.933]
Epoch [96/120    avg_loss:0.316, val_acc:0.929]
Epoch [97/120    avg_loss:0.199, val_acc:0.952]
Epoch [98/120    avg_loss:0.188, val_acc:0.952]
Epoch [99/120    avg_loss:0.154, val_acc:0.973]
Epoch [100/120    avg_loss:0.141, val_acc:0.969]
Epoch [101/120    avg_loss:0.151, val_acc:0.973]
Epoch [102/120    avg_loss:0.266, val_acc:0.942]
Epoch [103/120    avg_loss:0.257, val_acc:0.952]
Epoch [104/120    avg_loss:0.193, val_acc:0.979]
Epoch [105/120    avg_loss:0.169, val_acc:0.967]
Epoch [106/120    avg_loss:0.171, val_acc:0.958]
Epoch [107/120    avg_loss:0.137, val_acc:0.965]
Epoch [108/120    avg_loss:0.134, val_acc:0.956]
Epoch [109/120    avg_loss:0.123, val_acc:0.971]
Epoch [110/120    avg_loss:0.112, val_acc:0.952]
Epoch [111/120    avg_loss:0.131, val_acc:0.952]
Epoch [112/120    avg_loss:0.140, val_acc:0.969]
Epoch [113/120    avg_loss:0.195, val_acc:0.929]
Epoch [114/120    avg_loss:0.145, val_acc:0.975]
Epoch [115/120    avg_loss:0.213, val_acc:0.952]
Epoch [116/120    avg_loss:0.240, val_acc:0.958]
Epoch [117/120    avg_loss:0.178, val_acc:0.956]
Epoch [118/120    avg_loss:0.145, val_acc:0.971]
Epoch [119/120    avg_loss:0.115, val_acc:0.971]
Epoch [120/120    avg_loss:0.141, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 203   0   4   0   0  12   0   0   0   0   0   0]
 [  0   0   5 218   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   2 204  19   0   0   0   0   0   0   2   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  22   0   0   0   0  72   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.91044776119402

F1 scores:
[       nan 1.         0.90222222 0.96888889 0.87931034 0.85314685
 1.         0.80898876 0.99870968 0.99893276 1.         0.99867198
 0.99669967 1.        ]

Kappa:
0.9767340973895843
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdadd3b3b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 27787==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.119]
Epoch [2/120    avg_loss:2.578, val_acc:0.375]
Epoch [3/120    avg_loss:2.509, val_acc:0.365]
Epoch [4/120    avg_loss:2.452, val_acc:0.356]
Epoch [5/120    avg_loss:2.393, val_acc:0.354]
Epoch [6/120    avg_loss:2.358, val_acc:0.360]
Epoch [7/120    avg_loss:2.316, val_acc:0.381]
Epoch [8/120    avg_loss:2.269, val_acc:0.390]
Epoch [9/120    avg_loss:2.225, val_acc:0.379]
Epoch [10/120    avg_loss:2.171, val_acc:0.369]
Epoch [11/120    avg_loss:2.106, val_acc:0.377]
Epoch [12/120    avg_loss:2.061, val_acc:0.421]
Epoch [13/120    avg_loss:2.006, val_acc:0.490]
Epoch [14/120    avg_loss:1.955, val_acc:0.519]
Epoch [15/120    avg_loss:1.909, val_acc:0.583]
Epoch [16/120    avg_loss:1.877, val_acc:0.615]
Epoch [17/120    avg_loss:1.799, val_acc:0.629]
Epoch [18/120    avg_loss:1.750, val_acc:0.667]
Epoch [19/120    avg_loss:1.705, val_acc:0.723]
Epoch [20/120    avg_loss:1.667, val_acc:0.729]
Epoch [21/120    avg_loss:1.601, val_acc:0.794]
Epoch [22/120    avg_loss:1.573, val_acc:0.748]
Epoch [23/120    avg_loss:1.501, val_acc:0.779]
Epoch [24/120    avg_loss:1.446, val_acc:0.787]
Epoch [25/120    avg_loss:1.391, val_acc:0.792]
Epoch [26/120    avg_loss:1.323, val_acc:0.821]
Epoch [27/120    avg_loss:1.273, val_acc:0.838]
Epoch [28/120    avg_loss:1.213, val_acc:0.850]
Epoch [29/120    avg_loss:1.123, val_acc:0.854]
Epoch [30/120    avg_loss:1.077, val_acc:0.838]
Epoch [31/120    avg_loss:1.022, val_acc:0.854]
Epoch [32/120    avg_loss:0.972, val_acc:0.871]
Epoch [33/120    avg_loss:0.936, val_acc:0.856]
Epoch [34/120    avg_loss:0.910, val_acc:0.875]
Epoch [35/120    avg_loss:0.833, val_acc:0.887]
Epoch [36/120    avg_loss:0.770, val_acc:0.887]
Epoch [37/120    avg_loss:0.743, val_acc:0.885]
Epoch [38/120    avg_loss:0.717, val_acc:0.900]
Epoch [39/120    avg_loss:0.681, val_acc:0.890]
Epoch [40/120    avg_loss:0.630, val_acc:0.896]
Epoch [41/120    avg_loss:0.603, val_acc:0.915]
Epoch [42/120    avg_loss:0.594, val_acc:0.888]
Epoch [43/120    avg_loss:0.551, val_acc:0.913]
Epoch [44/120    avg_loss:0.530, val_acc:0.898]
Epoch [45/120    avg_loss:0.522, val_acc:0.915]
Epoch [46/120    avg_loss:0.558, val_acc:0.898]
Epoch [47/120    avg_loss:0.491, val_acc:0.927]
Epoch [48/120    avg_loss:0.461, val_acc:0.898]
Epoch [49/120    avg_loss:0.528, val_acc:0.912]
Epoch [50/120    avg_loss:0.463, val_acc:0.921]
Epoch [51/120    avg_loss:0.448, val_acc:0.925]
Epoch [52/120    avg_loss:0.444, val_acc:0.913]
Epoch [53/120    avg_loss:0.426, val_acc:0.923]
Epoch [54/120    avg_loss:0.353, val_acc:0.929]
Epoch [55/120    avg_loss:0.362, val_acc:0.925]
Epoch [56/120    avg_loss:0.402, val_acc:0.942]
Epoch [57/120    avg_loss:0.362, val_acc:0.921]
Epoch [58/120    avg_loss:0.368, val_acc:0.919]
Epoch [59/120    avg_loss:0.363, val_acc:0.912]
Epoch [60/120    avg_loss:0.362, val_acc:0.912]
Epoch [61/120    avg_loss:0.339, val_acc:0.925]
Epoch [62/120    avg_loss:0.302, val_acc:0.940]
Epoch [63/120    avg_loss:0.324, val_acc:0.900]
Epoch [64/120    avg_loss:0.292, val_acc:0.933]
Epoch [65/120    avg_loss:0.267, val_acc:0.937]
Epoch [66/120    avg_loss:0.305, val_acc:0.942]
Epoch [67/120    avg_loss:0.255, val_acc:0.946]
Epoch [68/120    avg_loss:0.250, val_acc:0.950]
Epoch [69/120    avg_loss:0.327, val_acc:0.935]
Epoch [70/120    avg_loss:0.238, val_acc:0.956]
Epoch [71/120    avg_loss:0.237, val_acc:0.954]
Epoch [72/120    avg_loss:0.252, val_acc:0.954]
Epoch [73/120    avg_loss:0.201, val_acc:0.962]
Epoch [74/120    avg_loss:0.198, val_acc:0.960]
Epoch [75/120    avg_loss:0.201, val_acc:0.954]
Epoch [76/120    avg_loss:0.302, val_acc:0.956]
Epoch [77/120    avg_loss:0.265, val_acc:0.956]
Epoch [78/120    avg_loss:0.285, val_acc:0.952]
Epoch [79/120    avg_loss:0.215, val_acc:0.950]
Epoch [80/120    avg_loss:0.220, val_acc:0.965]
Epoch [81/120    avg_loss:0.155, val_acc:0.956]
Epoch [82/120    avg_loss:0.173, val_acc:0.967]
Epoch [83/120    avg_loss:0.166, val_acc:0.963]
Epoch [84/120    avg_loss:0.190, val_acc:0.958]
Epoch [85/120    avg_loss:0.293, val_acc:0.938]
Epoch [86/120    avg_loss:0.199, val_acc:0.933]
Epoch [87/120    avg_loss:0.159, val_acc:0.962]
Epoch [88/120    avg_loss:0.150, val_acc:0.950]
Epoch [89/120    avg_loss:0.190, val_acc:0.965]
Epoch [90/120    avg_loss:0.168, val_acc:0.967]
Epoch [91/120    avg_loss:0.146, val_acc:0.962]
Epoch [92/120    avg_loss:0.136, val_acc:0.969]
Epoch [93/120    avg_loss:0.161, val_acc:0.958]
Epoch [94/120    avg_loss:0.149, val_acc:0.963]
Epoch [95/120    avg_loss:0.147, val_acc:0.975]
Epoch [96/120    avg_loss:0.135, val_acc:0.967]
Epoch [97/120    avg_loss:0.159, val_acc:0.925]
Epoch [98/120    avg_loss:0.165, val_acc:0.960]
Epoch [99/120    avg_loss:0.132, val_acc:0.965]
Epoch [100/120    avg_loss:0.122, val_acc:0.965]
Epoch [101/120    avg_loss:0.120, val_acc:0.956]
Epoch [102/120    avg_loss:0.100, val_acc:0.962]
Epoch [103/120    avg_loss:0.142, val_acc:0.950]
Epoch [104/120    avg_loss:0.201, val_acc:0.950]
Epoch [105/120    avg_loss:0.158, val_acc:0.960]
Epoch [106/120    avg_loss:0.098, val_acc:0.969]
Epoch [107/120    avg_loss:0.132, val_acc:0.979]
Epoch [108/120    avg_loss:0.097, val_acc:0.977]
Epoch [109/120    avg_loss:0.103, val_acc:0.969]
Epoch [110/120    avg_loss:0.142, val_acc:0.960]
Epoch [111/120    avg_loss:0.135, val_acc:0.969]
Epoch [112/120    avg_loss:0.113, val_acc:0.969]
Epoch [113/120    avg_loss:0.119, val_acc:0.967]
Epoch [114/120    avg_loss:0.104, val_acc:0.971]
Epoch [115/120    avg_loss:0.099, val_acc:0.956]
Epoch [116/120    avg_loss:0.168, val_acc:0.977]
Epoch [117/120    avg_loss:0.120, val_acc:0.962]
Epoch [118/120    avg_loss:0.083, val_acc:0.975]
Epoch [119/120    avg_loss:0.095, val_acc:0.963]
Epoch [120/120    avg_loss:0.152, val_acc:0.929]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 422   0   0   0   0 263   0   0   0   0   0   0   0]
 [  0   0 200   0   0   0   0  19   0   0   0   0   0   0]
 [  0   0   0 213   5   0   0   2   6   4   0   0   0   0]
 [  0   0   0   6 214   3   4   0   0   0   0   0   0   0]
 [  0   0   0   0  31  57  57   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   7   0   0   0   0   0   0   0   0   0 446   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0   0 831]]

Accuracy:
90.87420042643923

F1 scores:
[       nan 0.76242096 0.89485459 0.94877506 0.89727463 0.55609756
 0.55978261 0.80208333 0.99103713 0.99574468 1.         1.
 0.99221357 0.9981982 ]

Kappa:
0.8990270973197955
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f720f5baa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.131]
Epoch [2/120    avg_loss:2.556, val_acc:0.281]
Epoch [3/120    avg_loss:2.483, val_acc:0.315]
Epoch [4/120    avg_loss:2.398, val_acc:0.319]
Epoch [5/120    avg_loss:2.328, val_acc:0.340]
Epoch [6/120    avg_loss:2.269, val_acc:0.383]
Epoch [7/120    avg_loss:2.214, val_acc:0.415]
Epoch [8/120    avg_loss:2.167, val_acc:0.444]
Epoch [9/120    avg_loss:2.105, val_acc:0.483]
Epoch [10/120    avg_loss:2.057, val_acc:0.504]
Epoch [11/120    avg_loss:1.989, val_acc:0.519]
Epoch [12/120    avg_loss:1.950, val_acc:0.548]
Epoch [13/120    avg_loss:1.888, val_acc:0.579]
Epoch [14/120    avg_loss:1.814, val_acc:0.590]
Epoch [15/120    avg_loss:1.773, val_acc:0.627]
Epoch [16/120    avg_loss:1.686, val_acc:0.671]
Epoch [17/120    avg_loss:1.649, val_acc:0.660]
Epoch [18/120    avg_loss:1.580, val_acc:0.696]
Epoch [19/120    avg_loss:1.510, val_acc:0.702]
Epoch [20/120    avg_loss:1.438, val_acc:0.727]
Epoch [21/120    avg_loss:1.382, val_acc:0.800]
Epoch [22/120    avg_loss:1.287, val_acc:0.804]
Epoch [23/120    avg_loss:1.228, val_acc:0.821]
Epoch [24/120    avg_loss:1.162, val_acc:0.785]
Epoch [25/120    avg_loss:1.126, val_acc:0.808]
Epoch [26/120    avg_loss:1.052, val_acc:0.873]
Epoch [27/120    avg_loss:1.006, val_acc:0.890]
Epoch [28/120    avg_loss:0.937, val_acc:0.871]
Epoch [29/120    avg_loss:0.905, val_acc:0.881]
Epoch [30/120    avg_loss:0.839, val_acc:0.815]
Epoch [31/120    avg_loss:0.784, val_acc:0.883]
Epoch [32/120    avg_loss:0.742, val_acc:0.908]
Epoch [33/120    avg_loss:0.711, val_acc:0.925]
Epoch [34/120    avg_loss:0.686, val_acc:0.912]
Epoch [35/120    avg_loss:0.671, val_acc:0.912]
Epoch [36/120    avg_loss:0.671, val_acc:0.910]
Epoch [37/120    avg_loss:0.595, val_acc:0.917]
Epoch [38/120    avg_loss:0.560, val_acc:0.915]
Epoch [39/120    avg_loss:0.516, val_acc:0.923]
Epoch [40/120    avg_loss:0.518, val_acc:0.919]
Epoch [41/120    avg_loss:0.484, val_acc:0.904]
Epoch [42/120    avg_loss:0.510, val_acc:0.915]
Epoch [43/120    avg_loss:0.450, val_acc:0.919]
Epoch [44/120    avg_loss:0.416, val_acc:0.927]
Epoch [45/120    avg_loss:0.396, val_acc:0.948]
Epoch [46/120    avg_loss:0.382, val_acc:0.929]
Epoch [47/120    avg_loss:0.350, val_acc:0.921]
Epoch [48/120    avg_loss:0.365, val_acc:0.933]
Epoch [49/120    avg_loss:0.357, val_acc:0.929]
Epoch [50/120    avg_loss:0.340, val_acc:0.940]
Epoch [51/120    avg_loss:0.310, val_acc:0.944]
Epoch [52/120    avg_loss:0.283, val_acc:0.952]
Epoch [53/120    avg_loss:0.284, val_acc:0.952]
Epoch [54/120    avg_loss:0.259, val_acc:0.963]
Epoch [55/120    avg_loss:0.278, val_acc:0.958]
Epoch [56/120    avg_loss:0.300, val_acc:0.942]
Epoch [57/120    avg_loss:0.278, val_acc:0.958]
Epoch [58/120    avg_loss:0.247, val_acc:0.938]
Epoch [59/120    avg_loss:0.281, val_acc:0.935]
Epoch [60/120    avg_loss:0.273, val_acc:0.950]
Epoch [61/120    avg_loss:0.263, val_acc:0.954]
Epoch [62/120    avg_loss:0.274, val_acc:0.954]
Epoch [63/120    avg_loss:0.320, val_acc:0.942]
Epoch [64/120    avg_loss:0.263, val_acc:0.958]
Epoch [65/120    avg_loss:0.230, val_acc:0.963]
Epoch [66/120    avg_loss:0.223, val_acc:0.954]
Epoch [67/120    avg_loss:0.276, val_acc:0.958]
Epoch [68/120    avg_loss:0.219, val_acc:0.963]
Epoch [69/120    avg_loss:0.195, val_acc:0.971]
Epoch [70/120    avg_loss:0.227, val_acc:0.958]
Epoch [71/120    avg_loss:0.187, val_acc:0.967]
Epoch [72/120    avg_loss:0.194, val_acc:0.963]
Epoch [73/120    avg_loss:0.200, val_acc:0.965]
Epoch [74/120    avg_loss:0.168, val_acc:0.979]
Epoch [75/120    avg_loss:0.204, val_acc:0.963]
Epoch [76/120    avg_loss:0.157, val_acc:0.975]
Epoch [77/120    avg_loss:0.178, val_acc:0.954]
Epoch [78/120    avg_loss:0.155, val_acc:0.967]
Epoch [79/120    avg_loss:0.146, val_acc:0.969]
Epoch [80/120    avg_loss:0.129, val_acc:0.956]
Epoch [81/120    avg_loss:0.128, val_acc:0.960]
Epoch [82/120    avg_loss:0.201, val_acc:0.954]
Epoch [83/120    avg_loss:0.148, val_acc:0.948]
Epoch [84/120    avg_loss:0.140, val_acc:0.971]
Epoch [85/120    avg_loss:0.146, val_acc:0.958]
Epoch [86/120    avg_loss:0.147, val_acc:0.956]
Epoch [87/120    avg_loss:0.169, val_acc:0.973]
Epoch [88/120    avg_loss:0.136, val_acc:0.977]
Epoch [89/120    avg_loss:0.097, val_acc:0.967]
Epoch [90/120    avg_loss:0.100, val_acc:0.965]
Epoch [91/120    avg_loss:0.111, val_acc:0.975]
Epoch [92/120    avg_loss:0.091, val_acc:0.973]
Epoch [93/120    avg_loss:0.100, val_acc:0.973]
Epoch [94/120    avg_loss:0.095, val_acc:0.977]
Epoch [95/120    avg_loss:0.085, val_acc:0.977]
Epoch [96/120    avg_loss:0.089, val_acc:0.977]
Epoch [97/120    avg_loss:0.092, val_acc:0.979]
Epoch [98/120    avg_loss:0.085, val_acc:0.979]
Epoch [99/120    avg_loss:0.086, val_acc:0.975]
Epoch [100/120    avg_loss:0.088, val_acc:0.973]
Epoch [101/120    avg_loss:0.082, val_acc:0.969]
Epoch [102/120    avg_loss:0.074, val_acc:0.973]
Epoch [103/120    avg_loss:0.080, val_acc:0.975]
Epoch [104/120    avg_loss:0.083, val_acc:0.979]
Epoch [105/120    avg_loss:0.082, val_acc:0.969]
Epoch [106/120    avg_loss:0.086, val_acc:0.971]
Epoch [107/120    avg_loss:0.095, val_acc:0.983]
Epoch [108/120    avg_loss:0.081, val_acc:0.977]
Epoch [109/120    avg_loss:0.076, val_acc:0.971]
Epoch [110/120    avg_loss:0.079, val_acc:0.969]
Epoch [111/120    avg_loss:0.085, val_acc:0.977]
Epoch [112/120    avg_loss:0.075, val_acc:0.981]
Epoch [113/120    avg_loss:0.085, val_acc:0.971]
Epoch [114/120    avg_loss:0.076, val_acc:0.969]
Epoch [115/120    avg_loss:0.072, val_acc:0.979]
Epoch [116/120    avg_loss:0.077, val_acc:0.983]
Epoch [117/120    avg_loss:0.067, val_acc:0.971]
Epoch [118/120    avg_loss:0.069, val_acc:0.973]
Epoch [119/120    avg_loss:0.066, val_acc:0.973]
Epoch [120/120    avg_loss:0.071, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 217   4   0   0   0   4   3   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8   0 198   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.23027718550107

F1 scores:
[       nan 1.         0.96475771 0.97091723 0.87234043 0.83216783
 0.98019802 0.91954023 0.99487179 0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9802937904405824
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f41ea95db70>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.073]
Epoch [2/120    avg_loss:2.577, val_acc:0.256]
Epoch [3/120    avg_loss:2.526, val_acc:0.362]
Epoch [4/120    avg_loss:2.470, val_acc:0.417]
Epoch [5/120    avg_loss:2.420, val_acc:0.425]
Epoch [6/120    avg_loss:2.376, val_acc:0.448]
Epoch [7/120    avg_loss:2.326, val_acc:0.456]
Epoch [8/120    avg_loss:2.288, val_acc:0.450]
Epoch [9/120    avg_loss:2.250, val_acc:0.460]
Epoch [10/120    avg_loss:2.209, val_acc:0.544]
Epoch [11/120    avg_loss:2.170, val_acc:0.585]
Epoch [12/120    avg_loss:2.112, val_acc:0.571]
Epoch [13/120    avg_loss:2.067, val_acc:0.573]
Epoch [14/120    avg_loss:2.034, val_acc:0.567]
Epoch [15/120    avg_loss:1.975, val_acc:0.581]
Epoch [16/120    avg_loss:1.914, val_acc:0.583]
Epoch [17/120    avg_loss:1.850, val_acc:0.594]
Epoch [18/120    avg_loss:1.795, val_acc:0.602]
Epoch [19/120    avg_loss:1.742, val_acc:0.613]
Epoch [20/120    avg_loss:1.672, val_acc:0.635]
Epoch [21/120    avg_loss:1.608, val_acc:0.644]
Epoch [22/120    avg_loss:1.555, val_acc:0.648]
Epoch [23/120    avg_loss:1.470, val_acc:0.669]
Epoch [24/120    avg_loss:1.403, val_acc:0.700]
Epoch [25/120    avg_loss:1.346, val_acc:0.717]
Epoch [26/120    avg_loss:1.276, val_acc:0.729]
Epoch [27/120    avg_loss:1.234, val_acc:0.729]
Epoch [28/120    avg_loss:1.155, val_acc:0.762]
Epoch [29/120    avg_loss:1.091, val_acc:0.827]
Epoch [30/120    avg_loss:1.054, val_acc:0.744]
Epoch [31/120    avg_loss:1.005, val_acc:0.817]
Epoch [32/120    avg_loss:0.989, val_acc:0.806]
Epoch [33/120    avg_loss:0.924, val_acc:0.842]
Epoch [34/120    avg_loss:0.841, val_acc:0.867]
Epoch [35/120    avg_loss:0.794, val_acc:0.846]
Epoch [36/120    avg_loss:0.785, val_acc:0.883]
Epoch [37/120    avg_loss:0.730, val_acc:0.800]
Epoch [38/120    avg_loss:0.732, val_acc:0.800]
Epoch [39/120    avg_loss:0.677, val_acc:0.885]
Epoch [40/120    avg_loss:0.654, val_acc:0.873]
Epoch [41/120    avg_loss:0.629, val_acc:0.865]
Epoch [42/120    avg_loss:0.667, val_acc:0.871]
Epoch [43/120    avg_loss:0.600, val_acc:0.856]
Epoch [44/120    avg_loss:0.539, val_acc:0.904]
Epoch [45/120    avg_loss:0.492, val_acc:0.919]
Epoch [46/120    avg_loss:0.489, val_acc:0.890]
Epoch [47/120    avg_loss:0.496, val_acc:0.912]
Epoch [48/120    avg_loss:0.518, val_acc:0.896]
Epoch [49/120    avg_loss:0.474, val_acc:0.919]
Epoch [50/120    avg_loss:0.521, val_acc:0.923]
Epoch [51/120    avg_loss:0.486, val_acc:0.871]
Epoch [52/120    avg_loss:0.458, val_acc:0.908]
Epoch [53/120    avg_loss:0.494, val_acc:0.919]
Epoch [54/120    avg_loss:0.495, val_acc:0.833]
Epoch [55/120    avg_loss:0.456, val_acc:0.912]
Epoch [56/120    avg_loss:0.388, val_acc:0.919]
Epoch [57/120    avg_loss:0.455, val_acc:0.912]
Epoch [58/120    avg_loss:0.437, val_acc:0.910]
Epoch [59/120    avg_loss:0.424, val_acc:0.902]
Epoch [60/120    avg_loss:0.402, val_acc:0.915]
Epoch [61/120    avg_loss:0.365, val_acc:0.912]
Epoch [62/120    avg_loss:0.386, val_acc:0.921]
Epoch [63/120    avg_loss:0.326, val_acc:0.935]
Epoch [64/120    avg_loss:0.333, val_acc:0.912]
Epoch [65/120    avg_loss:0.332, val_acc:0.933]
Epoch [66/120    avg_loss:0.349, val_acc:0.931]
Epoch [67/120    avg_loss:0.321, val_acc:0.927]
Epoch [68/120    avg_loss:0.283, val_acc:0.938]
Epoch [69/120    avg_loss:0.288, val_acc:0.942]
Epoch [70/120    avg_loss:0.306, val_acc:0.881]
Epoch [71/120    avg_loss:0.339, val_acc:0.931]
Epoch [72/120    avg_loss:0.291, val_acc:0.944]
Epoch [73/120    avg_loss:0.321, val_acc:0.915]
Epoch [74/120    avg_loss:0.293, val_acc:0.938]
Epoch [75/120    avg_loss:0.241, val_acc:0.954]
Epoch [76/120    avg_loss:0.240, val_acc:0.944]
Epoch [77/120    avg_loss:0.266, val_acc:0.944]
Epoch [78/120    avg_loss:0.266, val_acc:0.933]
Epoch [79/120    avg_loss:0.256, val_acc:0.950]
Epoch [80/120    avg_loss:0.204, val_acc:0.958]
Epoch [81/120    avg_loss:0.216, val_acc:0.935]
Epoch [82/120    avg_loss:0.251, val_acc:0.954]
Epoch [83/120    avg_loss:0.215, val_acc:0.960]
Epoch [84/120    avg_loss:0.254, val_acc:0.948]
Epoch [85/120    avg_loss:0.262, val_acc:0.912]
Epoch [86/120    avg_loss:0.220, val_acc:0.948]
Epoch [87/120    avg_loss:0.186, val_acc:0.946]
Epoch [88/120    avg_loss:0.202, val_acc:0.960]
Epoch [89/120    avg_loss:0.169, val_acc:0.969]
Epoch [90/120    avg_loss:0.177, val_acc:0.954]
Epoch [91/120    avg_loss:0.167, val_acc:0.954]
Epoch [92/120    avg_loss:0.179, val_acc:0.956]
Epoch [93/120    avg_loss:0.205, val_acc:0.954]
Epoch [94/120    avg_loss:0.190, val_acc:0.946]
Epoch [95/120    avg_loss:0.146, val_acc:0.973]
Epoch [96/120    avg_loss:0.149, val_acc:0.965]
Epoch [97/120    avg_loss:0.170, val_acc:0.958]
Epoch [98/120    avg_loss:0.198, val_acc:0.963]
Epoch [99/120    avg_loss:0.145, val_acc:0.960]
Epoch [100/120    avg_loss:0.149, val_acc:0.942]
Epoch [101/120    avg_loss:0.226, val_acc:0.946]
Epoch [102/120    avg_loss:0.174, val_acc:0.973]
Epoch [103/120    avg_loss:0.162, val_acc:0.958]
Epoch [104/120    avg_loss:0.156, val_acc:0.956]
Epoch [105/120    avg_loss:0.129, val_acc:0.973]
Epoch [106/120    avg_loss:0.134, val_acc:0.975]
Epoch [107/120    avg_loss:0.170, val_acc:0.973]
Epoch [108/120    avg_loss:0.135, val_acc:0.975]
Epoch [109/120    avg_loss:0.117, val_acc:0.971]
Epoch [110/120    avg_loss:0.134, val_acc:0.971]
Epoch [111/120    avg_loss:0.105, val_acc:0.956]
Epoch [112/120    avg_loss:0.155, val_acc:0.969]
Epoch [113/120    avg_loss:0.105, val_acc:0.965]
Epoch [114/120    avg_loss:0.119, val_acc:0.965]
Epoch [115/120    avg_loss:0.112, val_acc:0.965]
Epoch [116/120    avg_loss:0.117, val_acc:0.977]
Epoch [117/120    avg_loss:0.098, val_acc:0.965]
Epoch [118/120    avg_loss:0.110, val_acc:0.973]
Epoch [119/120    avg_loss:0.121, val_acc:0.971]
Epoch [120/120    avg_loss:0.121, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   4 222   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.98198198 0.98230088 0.90423163 0.88448845
 0.99266504 0.98924731 0.99870968 1.         1.         0.99600533
 0.99669967 1.        ]

Kappa:
0.9874192553770195
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe0bb9f9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.100]
Epoch [2/120    avg_loss:2.543, val_acc:0.219]
Epoch [3/120    avg_loss:2.477, val_acc:0.310]
Epoch [4/120    avg_loss:2.422, val_acc:0.356]
Epoch [5/120    avg_loss:2.379, val_acc:0.333]
Epoch [6/120    avg_loss:2.326, val_acc:0.352]
Epoch [7/120    avg_loss:2.277, val_acc:0.360]
Epoch [8/120    avg_loss:2.238, val_acc:0.367]
Epoch [9/120    avg_loss:2.181, val_acc:0.406]
Epoch [10/120    avg_loss:2.119, val_acc:0.465]
Epoch [11/120    avg_loss:2.066, val_acc:0.479]
Epoch [12/120    avg_loss:2.019, val_acc:0.510]
Epoch [13/120    avg_loss:1.944, val_acc:0.519]
Epoch [14/120    avg_loss:1.886, val_acc:0.565]
Epoch [15/120    avg_loss:1.852, val_acc:0.565]
Epoch [16/120    avg_loss:1.782, val_acc:0.602]
Epoch [17/120    avg_loss:1.732, val_acc:0.600]
Epoch [18/120    avg_loss:1.676, val_acc:0.669]
Epoch [19/120    avg_loss:1.616, val_acc:0.675]
Epoch [20/120    avg_loss:1.548, val_acc:0.681]
Epoch [21/120    avg_loss:1.513, val_acc:0.683]
Epoch [22/120    avg_loss:1.426, val_acc:0.731]
Epoch [23/120    avg_loss:1.394, val_acc:0.744]
Epoch [24/120    avg_loss:1.333, val_acc:0.702]
Epoch [25/120    avg_loss:1.252, val_acc:0.773]
Epoch [26/120    avg_loss:1.206, val_acc:0.815]
Epoch [27/120    avg_loss:1.170, val_acc:0.775]
Epoch [28/120    avg_loss:1.074, val_acc:0.856]
Epoch [29/120    avg_loss:1.049, val_acc:0.867]
Epoch [30/120    avg_loss:0.987, val_acc:0.760]
Epoch [31/120    avg_loss:0.954, val_acc:0.779]
Epoch [32/120    avg_loss:0.894, val_acc:0.881]
Epoch [33/120    avg_loss:0.856, val_acc:0.910]
Epoch [34/120    avg_loss:0.814, val_acc:0.906]
Epoch [35/120    avg_loss:0.758, val_acc:0.892]
Epoch [36/120    avg_loss:0.753, val_acc:0.902]
Epoch [37/120    avg_loss:0.665, val_acc:0.921]
Epoch [38/120    avg_loss:0.686, val_acc:0.917]
Epoch [39/120    avg_loss:0.638, val_acc:0.887]
Epoch [40/120    avg_loss:0.641, val_acc:0.898]
Epoch [41/120    avg_loss:0.567, val_acc:0.927]
Epoch [42/120    avg_loss:0.551, val_acc:0.927]
Epoch [43/120    avg_loss:0.508, val_acc:0.912]
Epoch [44/120    avg_loss:0.506, val_acc:0.925]
Epoch [45/120    avg_loss:0.485, val_acc:0.923]
Epoch [46/120    avg_loss:0.454, val_acc:0.935]
Epoch [47/120    avg_loss:0.456, val_acc:0.933]
Epoch [48/120    avg_loss:0.454, val_acc:0.931]
Epoch [49/120    avg_loss:0.431, val_acc:0.898]
Epoch [50/120    avg_loss:0.473, val_acc:0.929]
Epoch [51/120    avg_loss:0.425, val_acc:0.927]
Epoch [52/120    avg_loss:0.409, val_acc:0.942]
Epoch [53/120    avg_loss:0.372, val_acc:0.925]
Epoch [54/120    avg_loss:0.352, val_acc:0.944]
Epoch [55/120    avg_loss:0.350, val_acc:0.946]
Epoch [56/120    avg_loss:0.336, val_acc:0.931]
Epoch [57/120    avg_loss:0.338, val_acc:0.935]
Epoch [58/120    avg_loss:0.363, val_acc:0.944]
Epoch [59/120    avg_loss:0.349, val_acc:0.944]
Epoch [60/120    avg_loss:0.338, val_acc:0.931]
Epoch [61/120    avg_loss:0.342, val_acc:0.963]
Epoch [62/120    avg_loss:0.309, val_acc:0.965]
Epoch [63/120    avg_loss:0.277, val_acc:0.967]
Epoch [64/120    avg_loss:0.285, val_acc:0.948]
Epoch [65/120    avg_loss:0.272, val_acc:0.948]
Epoch [66/120    avg_loss:0.282, val_acc:0.948]
Epoch [67/120    avg_loss:0.274, val_acc:0.954]
Epoch [68/120    avg_loss:0.230, val_acc:0.956]
Epoch [69/120    avg_loss:0.235, val_acc:0.971]
Epoch [70/120    avg_loss:0.232, val_acc:0.983]
Epoch [71/120    avg_loss:0.229, val_acc:0.950]
Epoch [72/120    avg_loss:0.264, val_acc:0.967]
Epoch [73/120    avg_loss:0.239, val_acc:0.940]
Epoch [74/120    avg_loss:0.271, val_acc:0.927]
Epoch [75/120    avg_loss:0.256, val_acc:0.971]
Epoch [76/120    avg_loss:0.225, val_acc:0.950]
Epoch [77/120    avg_loss:0.229, val_acc:0.952]
Epoch [78/120    avg_loss:0.212, val_acc:0.979]
Epoch [79/120    avg_loss:0.191, val_acc:0.975]
Epoch [80/120    avg_loss:0.197, val_acc:0.940]
Epoch [81/120    avg_loss:0.239, val_acc:0.971]
Epoch [82/120    avg_loss:0.196, val_acc:0.975]
Epoch [83/120    avg_loss:0.210, val_acc:0.981]
Epoch [84/120    avg_loss:0.163, val_acc:0.979]
Epoch [85/120    avg_loss:0.148, val_acc:0.985]
Epoch [86/120    avg_loss:0.132, val_acc:0.985]
Epoch [87/120    avg_loss:0.150, val_acc:0.988]
Epoch [88/120    avg_loss:0.152, val_acc:0.990]
Epoch [89/120    avg_loss:0.134, val_acc:0.983]
Epoch [90/120    avg_loss:0.138, val_acc:0.988]
Epoch [91/120    avg_loss:0.130, val_acc:0.992]
Epoch [92/120    avg_loss:0.136, val_acc:0.990]
Epoch [93/120    avg_loss:0.146, val_acc:0.985]
Epoch [94/120    avg_loss:0.124, val_acc:0.988]
Epoch [95/120    avg_loss:0.132, val_acc:0.985]
Epoch [96/120    avg_loss:0.122, val_acc:0.990]
Epoch [97/120    avg_loss:0.121, val_acc:0.988]
Epoch [98/120    avg_loss:0.137, val_acc:0.990]
Epoch [99/120    avg_loss:0.122, val_acc:0.988]
Epoch [100/120    avg_loss:0.114, val_acc:0.983]
Epoch [101/120    avg_loss:0.127, val_acc:0.985]
Epoch [102/120    avg_loss:0.118, val_acc:0.985]
Epoch [103/120    avg_loss:0.117, val_acc:0.985]
Epoch [104/120    avg_loss:0.127, val_acc:0.988]
Epoch [105/120    avg_loss:0.110, val_acc:0.988]
Epoch [106/120    avg_loss:0.139, val_acc:0.988]
Epoch [107/120    avg_loss:0.116, val_acc:0.985]
Epoch [108/120    avg_loss:0.108, val_acc:0.985]
Epoch [109/120    avg_loss:0.118, val_acc:0.985]
Epoch [110/120    avg_loss:0.113, val_acc:0.985]
Epoch [111/120    avg_loss:0.129, val_acc:0.985]
Epoch [112/120    avg_loss:0.107, val_acc:0.985]
Epoch [113/120    avg_loss:0.104, val_acc:0.985]
Epoch [114/120    avg_loss:0.104, val_acc:0.985]
Epoch [115/120    avg_loss:0.115, val_acc:0.985]
Epoch [116/120    avg_loss:0.113, val_acc:0.985]
Epoch [117/120    avg_loss:0.103, val_acc:0.985]
Epoch [118/120    avg_loss:0.121, val_acc:0.985]
Epoch [119/120    avg_loss:0.120, val_acc:0.985]
Epoch [120/120    avg_loss:0.109, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   3 226   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 192  35   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.4861407249467

F1 scores:
[       nan 1.         0.95111111 0.99122807 0.89095128 0.84984026
 1.         0.89385475 0.998713   1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9831463944094962
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4949409b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.158]
Epoch [2/120    avg_loss:2.521, val_acc:0.260]
Epoch [3/120    avg_loss:2.442, val_acc:0.371]
Epoch [4/120    avg_loss:2.372, val_acc:0.350]
Epoch [5/120    avg_loss:2.304, val_acc:0.392]
Epoch [6/120    avg_loss:2.230, val_acc:0.452]
Epoch [7/120    avg_loss:2.170, val_acc:0.475]
Epoch [8/120    avg_loss:2.090, val_acc:0.500]
Epoch [9/120    avg_loss:2.040, val_acc:0.508]
Epoch [10/120    avg_loss:1.974, val_acc:0.510]
Epoch [11/120    avg_loss:1.942, val_acc:0.512]
Epoch [12/120    avg_loss:1.885, val_acc:0.527]
Epoch [13/120    avg_loss:1.823, val_acc:0.533]
Epoch [14/120    avg_loss:1.776, val_acc:0.540]
Epoch [15/120    avg_loss:1.702, val_acc:0.604]
Epoch [16/120    avg_loss:1.627, val_acc:0.592]
Epoch [17/120    avg_loss:1.568, val_acc:0.679]
Epoch [18/120    avg_loss:1.519, val_acc:0.748]
Epoch [19/120    avg_loss:1.436, val_acc:0.771]
Epoch [20/120    avg_loss:1.381, val_acc:0.650]
Epoch [21/120    avg_loss:1.334, val_acc:0.798]
Epoch [22/120    avg_loss:1.265, val_acc:0.792]
Epoch [23/120    avg_loss:1.215, val_acc:0.696]
Epoch [24/120    avg_loss:1.198, val_acc:0.852]
Epoch [25/120    avg_loss:1.109, val_acc:0.854]
Epoch [26/120    avg_loss:1.044, val_acc:0.858]
Epoch [27/120    avg_loss:1.012, val_acc:0.887]
Epoch [28/120    avg_loss:0.945, val_acc:0.885]
Epoch [29/120    avg_loss:0.882, val_acc:0.885]
Epoch [30/120    avg_loss:0.860, val_acc:0.873]
Epoch [31/120    avg_loss:0.840, val_acc:0.863]
Epoch [32/120    avg_loss:0.834, val_acc:0.838]
Epoch [33/120    avg_loss:0.790, val_acc:0.900]
Epoch [34/120    avg_loss:0.751, val_acc:0.844]
Epoch [35/120    avg_loss:0.749, val_acc:0.908]
Epoch [36/120    avg_loss:0.705, val_acc:0.867]
Epoch [37/120    avg_loss:0.677, val_acc:0.883]
Epoch [38/120    avg_loss:0.616, val_acc:0.896]
Epoch [39/120    avg_loss:0.627, val_acc:0.887]
Epoch [40/120    avg_loss:0.628, val_acc:0.879]
Epoch [41/120    avg_loss:0.585, val_acc:0.892]
Epoch [42/120    avg_loss:0.607, val_acc:0.925]
Epoch [43/120    avg_loss:0.552, val_acc:0.925]
Epoch [44/120    avg_loss:0.505, val_acc:0.919]
Epoch [45/120    avg_loss:0.524, val_acc:0.890]
Epoch [46/120    avg_loss:0.479, val_acc:0.915]
Epoch [47/120    avg_loss:0.446, val_acc:0.908]
Epoch [48/120    avg_loss:0.420, val_acc:0.933]
Epoch [49/120    avg_loss:0.443, val_acc:0.917]
Epoch [50/120    avg_loss:0.431, val_acc:0.942]
Epoch [51/120    avg_loss:0.470, val_acc:0.902]
Epoch [52/120    avg_loss:0.479, val_acc:0.894]
Epoch [53/120    avg_loss:0.447, val_acc:0.931]
Epoch [54/120    avg_loss:0.374, val_acc:0.917]
Epoch [55/120    avg_loss:0.372, val_acc:0.929]
Epoch [56/120    avg_loss:0.359, val_acc:0.946]
Epoch [57/120    avg_loss:0.399, val_acc:0.946]
Epoch [58/120    avg_loss:0.379, val_acc:0.940]
Epoch [59/120    avg_loss:0.344, val_acc:0.944]
Epoch [60/120    avg_loss:0.338, val_acc:0.940]
Epoch [61/120    avg_loss:0.347, val_acc:0.942]
Epoch [62/120    avg_loss:0.321, val_acc:0.925]
Epoch [63/120    avg_loss:0.325, val_acc:0.923]
Epoch [64/120    avg_loss:0.348, val_acc:0.963]
Epoch [65/120    avg_loss:0.308, val_acc:0.956]
Epoch [66/120    avg_loss:0.303, val_acc:0.950]
Epoch [67/120    avg_loss:0.257, val_acc:0.960]
Epoch [68/120    avg_loss:0.294, val_acc:0.940]
Epoch [69/120    avg_loss:0.297, val_acc:0.948]
Epoch [70/120    avg_loss:0.302, val_acc:0.946]
Epoch [71/120    avg_loss:0.286, val_acc:0.950]
Epoch [72/120    avg_loss:0.285, val_acc:0.946]
Epoch [73/120    avg_loss:0.271, val_acc:0.944]
Epoch [74/120    avg_loss:0.229, val_acc:0.958]
Epoch [75/120    avg_loss:0.239, val_acc:0.935]
Epoch [76/120    avg_loss:0.218, val_acc:0.967]
Epoch [77/120    avg_loss:0.232, val_acc:0.956]
Epoch [78/120    avg_loss:0.255, val_acc:0.969]
Epoch [79/120    avg_loss:0.221, val_acc:0.938]
Epoch [80/120    avg_loss:0.208, val_acc:0.958]
Epoch [81/120    avg_loss:0.194, val_acc:0.973]
Epoch [82/120    avg_loss:0.215, val_acc:0.965]
Epoch [83/120    avg_loss:0.202, val_acc:0.960]
Epoch [84/120    avg_loss:0.173, val_acc:0.960]
Epoch [85/120    avg_loss:0.211, val_acc:0.967]
Epoch [86/120    avg_loss:0.185, val_acc:0.973]
Epoch [87/120    avg_loss:0.158, val_acc:0.971]
Epoch [88/120    avg_loss:0.174, val_acc:0.983]
Epoch [89/120    avg_loss:0.198, val_acc:0.971]
Epoch [90/120    avg_loss:0.198, val_acc:0.950]
Epoch [91/120    avg_loss:0.213, val_acc:0.960]
Epoch [92/120    avg_loss:0.178, val_acc:0.975]
Epoch [93/120    avg_loss:0.266, val_acc:0.948]
Epoch [94/120    avg_loss:0.201, val_acc:0.958]
Epoch [95/120    avg_loss:0.211, val_acc:0.954]
Epoch [96/120    avg_loss:0.232, val_acc:0.969]
Epoch [97/120    avg_loss:0.217, val_acc:0.954]
Epoch [98/120    avg_loss:0.235, val_acc:0.963]
Epoch [99/120    avg_loss:0.172, val_acc:0.971]
Epoch [100/120    avg_loss:0.145, val_acc:0.975]
Epoch [101/120    avg_loss:0.123, val_acc:0.988]
Epoch [102/120    avg_loss:0.150, val_acc:0.983]
Epoch [103/120    avg_loss:0.130, val_acc:0.981]
Epoch [104/120    avg_loss:0.132, val_acc:0.979]
Epoch [105/120    avg_loss:0.144, val_acc:0.971]
Epoch [106/120    avg_loss:0.133, val_acc:0.956]
Epoch [107/120    avg_loss:0.117, val_acc:0.975]
Epoch [108/120    avg_loss:0.120, val_acc:0.963]
Epoch [109/120    avg_loss:0.137, val_acc:0.977]
Epoch [110/120    avg_loss:0.093, val_acc:0.988]
Epoch [111/120    avg_loss:0.112, val_acc:0.979]
Epoch [112/120    avg_loss:0.097, val_acc:0.973]
Epoch [113/120    avg_loss:0.109, val_acc:0.983]
Epoch [114/120    avg_loss:0.098, val_acc:0.956]
Epoch [115/120    avg_loss:0.150, val_acc:0.979]
Epoch [116/120    avg_loss:0.130, val_acc:0.973]
Epoch [117/120    avg_loss:0.117, val_acc:0.975]
Epoch [118/120    avg_loss:0.110, val_acc:0.977]
Epoch [119/120    avg_loss:0.105, val_acc:0.981]
Epoch [120/120    avg_loss:0.106, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   1   0   3   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   6 203   7  13   0   0   0   1   0   0   0   0]
 [  0   0   0   0 154  73   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1  15 190   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.12153518123667

F1 scores:
[       nan 0.99707174 0.95515695 0.93764434 0.78772379 0.73846154
 0.95238095 0.92473118 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9679687384195594
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d2f8e7a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.046]
Epoch [2/120    avg_loss:2.575, val_acc:0.296]
Epoch [3/120    avg_loss:2.505, val_acc:0.304]
Epoch [4/120    avg_loss:2.444, val_acc:0.308]
Epoch [5/120    avg_loss:2.388, val_acc:0.321]
Epoch [6/120    avg_loss:2.353, val_acc:0.333]
Epoch [7/120    avg_loss:2.302, val_acc:0.358]
Epoch [8/120    avg_loss:2.274, val_acc:0.406]
Epoch [9/120    avg_loss:2.233, val_acc:0.452]
Epoch [10/120    avg_loss:2.196, val_acc:0.487]
Epoch [11/120    avg_loss:2.145, val_acc:0.496]
Epoch [12/120    avg_loss:2.097, val_acc:0.485]
Epoch [13/120    avg_loss:2.047, val_acc:0.487]
Epoch [14/120    avg_loss:2.001, val_acc:0.527]
Epoch [15/120    avg_loss:1.949, val_acc:0.550]
Epoch [16/120    avg_loss:1.888, val_acc:0.577]
Epoch [17/120    avg_loss:1.839, val_acc:0.602]
Epoch [18/120    avg_loss:1.770, val_acc:0.610]
Epoch [19/120    avg_loss:1.701, val_acc:0.617]
Epoch [20/120    avg_loss:1.629, val_acc:0.658]
Epoch [21/120    avg_loss:1.555, val_acc:0.671]
Epoch [22/120    avg_loss:1.500, val_acc:0.665]
Epoch [23/120    avg_loss:1.423, val_acc:0.692]
Epoch [24/120    avg_loss:1.359, val_acc:0.690]
Epoch [25/120    avg_loss:1.277, val_acc:0.715]
Epoch [26/120    avg_loss:1.216, val_acc:0.683]
Epoch [27/120    avg_loss:1.152, val_acc:0.735]
Epoch [28/120    avg_loss:1.099, val_acc:0.829]
Epoch [29/120    avg_loss:1.038, val_acc:0.750]
Epoch [30/120    avg_loss:0.997, val_acc:0.815]
Epoch [31/120    avg_loss:0.942, val_acc:0.838]
Epoch [32/120    avg_loss:0.862, val_acc:0.871]
Epoch [33/120    avg_loss:0.806, val_acc:0.856]
Epoch [34/120    avg_loss:0.768, val_acc:0.842]
Epoch [35/120    avg_loss:0.712, val_acc:0.846]
Epoch [36/120    avg_loss:0.710, val_acc:0.790]
Epoch [37/120    avg_loss:0.699, val_acc:0.887]
Epoch [38/120    avg_loss:0.618, val_acc:0.877]
Epoch [39/120    avg_loss:0.603, val_acc:0.912]
Epoch [40/120    avg_loss:0.564, val_acc:0.835]
Epoch [41/120    avg_loss:0.571, val_acc:0.892]
Epoch [42/120    avg_loss:0.580, val_acc:0.910]
Epoch [43/120    avg_loss:0.522, val_acc:0.881]
Epoch [44/120    avg_loss:0.502, val_acc:0.919]
Epoch [45/120    avg_loss:0.473, val_acc:0.915]
Epoch [46/120    avg_loss:0.433, val_acc:0.919]
Epoch [47/120    avg_loss:0.441, val_acc:0.892]
Epoch [48/120    avg_loss:0.464, val_acc:0.906]
Epoch [49/120    avg_loss:0.397, val_acc:0.894]
Epoch [50/120    avg_loss:0.409, val_acc:0.919]
Epoch [51/120    avg_loss:0.437, val_acc:0.921]
Epoch [52/120    avg_loss:0.374, val_acc:0.921]
Epoch [53/120    avg_loss:0.398, val_acc:0.902]
Epoch [54/120    avg_loss:0.417, val_acc:0.927]
Epoch [55/120    avg_loss:0.384, val_acc:0.938]
Epoch [56/120    avg_loss:0.353, val_acc:0.910]
Epoch [57/120    avg_loss:0.347, val_acc:0.898]
Epoch [58/120    avg_loss:0.339, val_acc:0.946]
Epoch [59/120    avg_loss:0.307, val_acc:0.933]
Epoch [60/120    avg_loss:0.304, val_acc:0.938]
Epoch [61/120    avg_loss:0.278, val_acc:0.946]
Epoch [62/120    avg_loss:0.311, val_acc:0.923]
Epoch [63/120    avg_loss:0.287, val_acc:0.942]
Epoch [64/120    avg_loss:0.240, val_acc:0.935]
Epoch [65/120    avg_loss:0.229, val_acc:0.944]
Epoch [66/120    avg_loss:0.321, val_acc:0.925]
Epoch [67/120    avg_loss:0.281, val_acc:0.946]
Epoch [68/120    avg_loss:0.268, val_acc:0.946]
Epoch [69/120    avg_loss:0.229, val_acc:0.927]
Epoch [70/120    avg_loss:0.272, val_acc:0.904]
Epoch [71/120    avg_loss:0.241, val_acc:0.948]
Epoch [72/120    avg_loss:0.220, val_acc:0.946]
Epoch [73/120    avg_loss:0.207, val_acc:0.950]
Epoch [74/120    avg_loss:0.177, val_acc:0.954]
Epoch [75/120    avg_loss:0.171, val_acc:0.954]
Epoch [76/120    avg_loss:0.179, val_acc:0.958]
Epoch [77/120    avg_loss:0.199, val_acc:0.969]
Epoch [78/120    avg_loss:0.221, val_acc:0.944]
Epoch [79/120    avg_loss:0.334, val_acc:0.954]
Epoch [80/120    avg_loss:0.286, val_acc:0.954]
Epoch [81/120    avg_loss:0.215, val_acc:0.954]
Epoch [82/120    avg_loss:0.194, val_acc:0.960]
Epoch [83/120    avg_loss:0.174, val_acc:0.950]
Epoch [84/120    avg_loss:0.173, val_acc:0.956]
Epoch [85/120    avg_loss:0.159, val_acc:0.971]
Epoch [86/120    avg_loss:0.147, val_acc:0.963]
Epoch [87/120    avg_loss:0.167, val_acc:0.960]
Epoch [88/120    avg_loss:0.170, val_acc:0.977]
Epoch [89/120    avg_loss:0.146, val_acc:0.958]
Epoch [90/120    avg_loss:0.144, val_acc:0.973]
Epoch [91/120    avg_loss:0.138, val_acc:0.965]
Epoch [92/120    avg_loss:0.143, val_acc:0.975]
Epoch [93/120    avg_loss:0.134, val_acc:0.971]
Epoch [94/120    avg_loss:0.127, val_acc:0.973]
Epoch [95/120    avg_loss:0.129, val_acc:0.971]
Epoch [96/120    avg_loss:0.141, val_acc:0.977]
Epoch [97/120    avg_loss:0.115, val_acc:0.977]
Epoch [98/120    avg_loss:0.122, val_acc:0.975]
Epoch [99/120    avg_loss:0.125, val_acc:0.954]
Epoch [100/120    avg_loss:0.123, val_acc:0.960]
Epoch [101/120    avg_loss:0.158, val_acc:0.979]
Epoch [102/120    avg_loss:0.195, val_acc:0.963]
Epoch [103/120    avg_loss:0.194, val_acc:0.956]
Epoch [104/120    avg_loss:0.158, val_acc:0.960]
Epoch [105/120    avg_loss:0.132, val_acc:0.969]
Epoch [106/120    avg_loss:0.109, val_acc:0.952]
Epoch [107/120    avg_loss:0.106, val_acc:0.977]
Epoch [108/120    avg_loss:0.110, val_acc:0.971]
Epoch [109/120    avg_loss:0.110, val_acc:0.973]
Epoch [110/120    avg_loss:0.133, val_acc:0.973]
Epoch [111/120    avg_loss:0.105, val_acc:0.965]
Epoch [112/120    avg_loss:0.081, val_acc:0.971]
Epoch [113/120    avg_loss:0.105, val_acc:0.981]
Epoch [114/120    avg_loss:0.117, val_acc:0.977]
Epoch [115/120    avg_loss:0.090, val_acc:0.981]
Epoch [116/120    avg_loss:0.119, val_acc:0.948]
Epoch [117/120    avg_loss:0.138, val_acc:0.956]
Epoch [118/120    avg_loss:0.139, val_acc:0.958]
Epoch [119/120    avg_loss:0.097, val_acc:0.977]
Epoch [120/120    avg_loss:0.098, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 668   0   0   0   0  14   0   3   0   0   0   0   0]
 [  0   0 183   0   1   0   0  35   0   0   0   0   0   0]
 [  0   0   0 228   0   0   0   0   1   1   0   0   0   0]
 [  0   0   0 115 112   0   0   0   0   0   0   0   0   0]
 [  0   0   0  31  19  95   0   0   0   0   0   0   0   0]
 [  0   0   0   1   8  12 181   4   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
94.77611940298507

F1 scores:
[       nan 0.98743533 0.91044776 0.75371901 0.61035422 0.75396825
 0.90274314 0.82819383 0.99487179 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9418651362229302
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe179749a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.073]
Epoch [2/120    avg_loss:2.584, val_acc:0.285]
Epoch [3/120    avg_loss:2.511, val_acc:0.287]
Epoch [4/120    avg_loss:2.450, val_acc:0.287]
Epoch [5/120    avg_loss:2.398, val_acc:0.287]
Epoch [6/120    avg_loss:2.355, val_acc:0.287]
Epoch [7/120    avg_loss:2.315, val_acc:0.287]
Epoch [8/120    avg_loss:2.269, val_acc:0.331]
Epoch [9/120    avg_loss:2.210, val_acc:0.388]
Epoch [10/120    avg_loss:2.163, val_acc:0.408]
Epoch [11/120    avg_loss:2.116, val_acc:0.429]
Epoch [12/120    avg_loss:2.073, val_acc:0.421]
Epoch [13/120    avg_loss:2.055, val_acc:0.475]
Epoch [14/120    avg_loss:1.969, val_acc:0.529]
Epoch [15/120    avg_loss:1.887, val_acc:0.604]
Epoch [16/120    avg_loss:1.852, val_acc:0.606]
Epoch [17/120    avg_loss:1.807, val_acc:0.652]
Epoch [18/120    avg_loss:1.765, val_acc:0.671]
Epoch [19/120    avg_loss:1.692, val_acc:0.673]
Epoch [20/120    avg_loss:1.668, val_acc:0.662]
Epoch [21/120    avg_loss:1.574, val_acc:0.710]
Epoch [22/120    avg_loss:1.512, val_acc:0.748]
Epoch [23/120    avg_loss:1.445, val_acc:0.727]
Epoch [24/120    avg_loss:1.423, val_acc:0.746]
Epoch [25/120    avg_loss:1.339, val_acc:0.710]
Epoch [26/120    avg_loss:1.272, val_acc:0.777]
Epoch [27/120    avg_loss:1.178, val_acc:0.815]
Epoch [28/120    avg_loss:1.147, val_acc:0.808]
Epoch [29/120    avg_loss:1.071, val_acc:0.838]
Epoch [30/120    avg_loss:1.003, val_acc:0.827]
Epoch [31/120    avg_loss:0.957, val_acc:0.883]
Epoch [32/120    avg_loss:0.929, val_acc:0.850]
Epoch [33/120    avg_loss:0.867, val_acc:0.892]
Epoch [34/120    avg_loss:0.822, val_acc:0.894]
Epoch [35/120    avg_loss:0.768, val_acc:0.921]
Epoch [36/120    avg_loss:0.713, val_acc:0.923]
Epoch [37/120    avg_loss:0.675, val_acc:0.929]
Epoch [38/120    avg_loss:0.690, val_acc:0.910]
Epoch [39/120    avg_loss:0.697, val_acc:0.885]
Epoch [40/120    avg_loss:0.672, val_acc:0.896]
Epoch [41/120    avg_loss:0.608, val_acc:0.921]
Epoch [42/120    avg_loss:0.558, val_acc:0.931]
Epoch [43/120    avg_loss:0.532, val_acc:0.935]
Epoch [44/120    avg_loss:0.502, val_acc:0.942]
Epoch [45/120    avg_loss:0.575, val_acc:0.944]
Epoch [46/120    avg_loss:0.506, val_acc:0.935]
Epoch [47/120    avg_loss:0.471, val_acc:0.938]
Epoch [48/120    avg_loss:0.550, val_acc:0.931]
Epoch [49/120    avg_loss:0.434, val_acc:0.946]
Epoch [50/120    avg_loss:0.433, val_acc:0.944]
Epoch [51/120    avg_loss:0.545, val_acc:0.910]
Epoch [52/120    avg_loss:0.472, val_acc:0.931]
Epoch [53/120    avg_loss:0.451, val_acc:0.950]
Epoch [54/120    avg_loss:0.427, val_acc:0.946]
Epoch [55/120    avg_loss:0.438, val_acc:0.952]
Epoch [56/120    avg_loss:0.392, val_acc:0.952]
Epoch [57/120    avg_loss:0.367, val_acc:0.906]
Epoch [58/120    avg_loss:0.366, val_acc:0.948]
Epoch [59/120    avg_loss:0.327, val_acc:0.954]
Epoch [60/120    avg_loss:0.366, val_acc:0.958]
Epoch [61/120    avg_loss:0.341, val_acc:0.958]
Epoch [62/120    avg_loss:0.329, val_acc:0.965]
Epoch [63/120    avg_loss:0.347, val_acc:0.935]
Epoch [64/120    avg_loss:0.325, val_acc:0.952]
Epoch [65/120    avg_loss:0.343, val_acc:0.954]
Epoch [66/120    avg_loss:0.314, val_acc:0.942]
Epoch [67/120    avg_loss:0.287, val_acc:0.950]
Epoch [68/120    avg_loss:0.276, val_acc:0.942]
Epoch [69/120    avg_loss:0.327, val_acc:0.915]
Epoch [70/120    avg_loss:0.327, val_acc:0.960]
Epoch [71/120    avg_loss:0.276, val_acc:0.950]
Epoch [72/120    avg_loss:0.291, val_acc:0.960]
Epoch [73/120    avg_loss:0.227, val_acc:0.960]
Epoch [74/120    avg_loss:0.297, val_acc:0.956]
Epoch [75/120    avg_loss:0.294, val_acc:0.971]
Epoch [76/120    avg_loss:0.284, val_acc:0.967]
Epoch [77/120    avg_loss:0.229, val_acc:0.973]
Epoch [78/120    avg_loss:0.272, val_acc:0.942]
Epoch [79/120    avg_loss:0.304, val_acc:0.956]
Epoch [80/120    avg_loss:0.229, val_acc:0.971]
Epoch [81/120    avg_loss:0.200, val_acc:0.975]
Epoch [82/120    avg_loss:0.195, val_acc:0.971]
Epoch [83/120    avg_loss:0.199, val_acc:0.983]
Epoch [84/120    avg_loss:0.208, val_acc:0.956]
Epoch [85/120    avg_loss:0.274, val_acc:0.963]
Epoch [86/120    avg_loss:0.259, val_acc:0.975]
Epoch [87/120    avg_loss:0.178, val_acc:0.967]
Epoch [88/120    avg_loss:0.200, val_acc:0.967]
Epoch [89/120    avg_loss:0.190, val_acc:0.990]
Epoch [90/120    avg_loss:0.184, val_acc:0.954]
Epoch [91/120    avg_loss:0.195, val_acc:0.956]
Epoch [92/120    avg_loss:0.205, val_acc:0.963]
Epoch [93/120    avg_loss:0.191, val_acc:0.958]
Epoch [94/120    avg_loss:0.242, val_acc:0.971]
Epoch [95/120    avg_loss:0.223, val_acc:0.940]
Epoch [96/120    avg_loss:0.188, val_acc:0.954]
Epoch [97/120    avg_loss:0.172, val_acc:0.981]
Epoch [98/120    avg_loss:0.192, val_acc:0.950]
Epoch [99/120    avg_loss:0.174, val_acc:0.960]
Epoch [100/120    avg_loss:0.167, val_acc:0.971]
Epoch [101/120    avg_loss:0.131, val_acc:0.981]
Epoch [102/120    avg_loss:0.117, val_acc:0.973]
Epoch [103/120    avg_loss:0.117, val_acc:0.979]
Epoch [104/120    avg_loss:0.102, val_acc:0.988]
Epoch [105/120    avg_loss:0.099, val_acc:0.990]
Epoch [106/120    avg_loss:0.091, val_acc:0.990]
Epoch [107/120    avg_loss:0.087, val_acc:0.988]
Epoch [108/120    avg_loss:0.094, val_acc:0.985]
Epoch [109/120    avg_loss:0.111, val_acc:0.990]
Epoch [110/120    avg_loss:0.092, val_acc:0.990]
Epoch [111/120    avg_loss:0.093, val_acc:0.990]
Epoch [112/120    avg_loss:0.090, val_acc:0.990]
Epoch [113/120    avg_loss:0.070, val_acc:0.990]
Epoch [114/120    avg_loss:0.091, val_acc:0.990]
Epoch [115/120    avg_loss:0.077, val_acc:0.988]
Epoch [116/120    avg_loss:0.074, val_acc:0.990]
Epoch [117/120    avg_loss:0.095, val_acc:0.988]
Epoch [118/120    avg_loss:0.087, val_acc:0.988]
Epoch [119/120    avg_loss:0.109, val_acc:0.990]
Epoch [120/120    avg_loss:0.092, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 205   0   0   0   0  14   0   0   0   0   0   0]
 [  0   0   0 216  11   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.61407249466951

F1 scores:
[       nan 1.         0.94470046 0.96860987 0.91479821 0.91262136
 1.         0.875      1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9845712807588106
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f204c41ca20>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.639, val_acc:0.115]
Epoch [2/120    avg_loss:2.557, val_acc:0.115]
Epoch [3/120    avg_loss:2.485, val_acc:0.237]
Epoch [4/120    avg_loss:2.420, val_acc:0.340]
Epoch [5/120    avg_loss:2.378, val_acc:0.367]
Epoch [6/120    avg_loss:2.322, val_acc:0.412]
Epoch [7/120    avg_loss:2.266, val_acc:0.477]
Epoch [8/120    avg_loss:2.231, val_acc:0.498]
Epoch [9/120    avg_loss:2.167, val_acc:0.523]
Epoch [10/120    avg_loss:2.130, val_acc:0.542]
Epoch [11/120    avg_loss:2.068, val_acc:0.606]
Epoch [12/120    avg_loss:2.006, val_acc:0.619]
Epoch [13/120    avg_loss:1.936, val_acc:0.610]
Epoch [14/120    avg_loss:1.873, val_acc:0.637]
Epoch [15/120    avg_loss:1.808, val_acc:0.627]
Epoch [16/120    avg_loss:1.735, val_acc:0.646]
Epoch [17/120    avg_loss:1.665, val_acc:0.660]
Epoch [18/120    avg_loss:1.621, val_acc:0.633]
Epoch [19/120    avg_loss:1.547, val_acc:0.673]
Epoch [20/120    avg_loss:1.451, val_acc:0.696]
Epoch [21/120    avg_loss:1.387, val_acc:0.706]
Epoch [22/120    avg_loss:1.294, val_acc:0.733]
Epoch [23/120    avg_loss:1.253, val_acc:0.750]
Epoch [24/120    avg_loss:1.190, val_acc:0.760]
Epoch [25/120    avg_loss:1.145, val_acc:0.775]
Epoch [26/120    avg_loss:1.083, val_acc:0.815]
Epoch [27/120    avg_loss:1.002, val_acc:0.881]
Epoch [28/120    avg_loss:0.983, val_acc:0.821]
Epoch [29/120    avg_loss:0.922, val_acc:0.879]
Epoch [30/120    avg_loss:0.877, val_acc:0.879]
Epoch [31/120    avg_loss:0.808, val_acc:0.894]
Epoch [32/120    avg_loss:0.780, val_acc:0.917]
Epoch [33/120    avg_loss:0.801, val_acc:0.875]
Epoch [34/120    avg_loss:0.823, val_acc:0.892]
Epoch [35/120    avg_loss:0.778, val_acc:0.840]
Epoch [36/120    avg_loss:0.674, val_acc:0.908]
Epoch [37/120    avg_loss:0.661, val_acc:0.919]
Epoch [38/120    avg_loss:0.616, val_acc:0.906]
Epoch [39/120    avg_loss:0.589, val_acc:0.915]
Epoch [40/120    avg_loss:0.613, val_acc:0.906]
Epoch [41/120    avg_loss:0.530, val_acc:0.933]
Epoch [42/120    avg_loss:0.562, val_acc:0.940]
Epoch [43/120    avg_loss:0.464, val_acc:0.894]
Epoch [44/120    avg_loss:0.438, val_acc:0.921]
Epoch [45/120    avg_loss:0.447, val_acc:0.931]
Epoch [46/120    avg_loss:0.467, val_acc:0.935]
Epoch [47/120    avg_loss:0.461, val_acc:0.915]
Epoch [48/120    avg_loss:0.481, val_acc:0.923]
Epoch [49/120    avg_loss:0.435, val_acc:0.917]
Epoch [50/120    avg_loss:0.422, val_acc:0.929]
Epoch [51/120    avg_loss:0.423, val_acc:0.927]
Epoch [52/120    avg_loss:0.369, val_acc:0.938]
Epoch [53/120    avg_loss:0.418, val_acc:0.902]
Epoch [54/120    avg_loss:0.424, val_acc:0.938]
Epoch [55/120    avg_loss:0.401, val_acc:0.944]
Epoch [56/120    avg_loss:0.379, val_acc:0.940]
Epoch [57/120    avg_loss:0.352, val_acc:0.940]
Epoch [58/120    avg_loss:0.365, val_acc:0.921]
Epoch [59/120    avg_loss:0.336, val_acc:0.956]
Epoch [60/120    avg_loss:0.312, val_acc:0.942]
Epoch [61/120    avg_loss:0.356, val_acc:0.902]
Epoch [62/120    avg_loss:0.383, val_acc:0.938]
Epoch [63/120    avg_loss:0.291, val_acc:0.944]
Epoch [64/120    avg_loss:0.279, val_acc:0.958]
Epoch [65/120    avg_loss:0.323, val_acc:0.935]
Epoch [66/120    avg_loss:0.278, val_acc:0.952]
Epoch [67/120    avg_loss:0.260, val_acc:0.940]
Epoch [68/120    avg_loss:0.229, val_acc:0.956]
Epoch [69/120    avg_loss:0.216, val_acc:0.954]
Epoch [70/120    avg_loss:0.218, val_acc:0.952]
Epoch [71/120    avg_loss:0.232, val_acc:0.952]
Epoch [72/120    avg_loss:0.199, val_acc:0.963]
Epoch [73/120    avg_loss:0.222, val_acc:0.958]
Epoch [74/120    avg_loss:0.203, val_acc:0.929]
Epoch [75/120    avg_loss:0.284, val_acc:0.933]
Epoch [76/120    avg_loss:0.230, val_acc:0.967]
Epoch [77/120    avg_loss:0.195, val_acc:0.965]
Epoch [78/120    avg_loss:0.202, val_acc:0.967]
Epoch [79/120    avg_loss:0.210, val_acc:0.929]
Epoch [80/120    avg_loss:0.184, val_acc:0.965]
Epoch [81/120    avg_loss:0.248, val_acc:0.938]
Epoch [82/120    avg_loss:0.254, val_acc:0.925]
Epoch [83/120    avg_loss:0.273, val_acc:0.946]
Epoch [84/120    avg_loss:0.243, val_acc:0.950]
Epoch [85/120    avg_loss:0.269, val_acc:0.971]
Epoch [86/120    avg_loss:0.198, val_acc:0.960]
Epoch [87/120    avg_loss:0.187, val_acc:0.977]
Epoch [88/120    avg_loss:0.193, val_acc:0.975]
Epoch [89/120    avg_loss:0.150, val_acc:0.971]
Epoch [90/120    avg_loss:0.176, val_acc:0.958]
Epoch [91/120    avg_loss:0.154, val_acc:0.965]
Epoch [92/120    avg_loss:0.121, val_acc:0.977]
Epoch [93/120    avg_loss:0.115, val_acc:0.967]
Epoch [94/120    avg_loss:0.108, val_acc:0.971]
Epoch [95/120    avg_loss:0.141, val_acc:0.969]
Epoch [96/120    avg_loss:0.119, val_acc:0.967]
Epoch [97/120    avg_loss:0.168, val_acc:0.969]
Epoch [98/120    avg_loss:0.180, val_acc:0.950]
Epoch [99/120    avg_loss:0.255, val_acc:0.954]
Epoch [100/120    avg_loss:0.187, val_acc:0.958]
Epoch [101/120    avg_loss:0.168, val_acc:0.965]
Epoch [102/120    avg_loss:0.113, val_acc:0.981]
Epoch [103/120    avg_loss:0.141, val_acc:0.973]
Epoch [104/120    avg_loss:0.137, val_acc:0.975]
Epoch [105/120    avg_loss:0.152, val_acc:0.965]
Epoch [106/120    avg_loss:0.166, val_acc:0.971]
Epoch [107/120    avg_loss:0.130, val_acc:0.967]
Epoch [108/120    avg_loss:0.120, val_acc:0.975]
Epoch [109/120    avg_loss:0.100, val_acc:0.963]
Epoch [110/120    avg_loss:0.106, val_acc:0.969]
Epoch [111/120    avg_loss:0.131, val_acc:0.973]
Epoch [112/120    avg_loss:0.118, val_acc:0.973]
Epoch [113/120    avg_loss:0.134, val_acc:0.981]
Epoch [114/120    avg_loss:0.105, val_acc:0.967]
Epoch [115/120    avg_loss:0.101, val_acc:0.983]
Epoch [116/120    avg_loss:0.121, val_acc:0.981]
Epoch [117/120    avg_loss:0.102, val_acc:0.954]
Epoch [118/120    avg_loss:0.140, val_acc:0.963]
Epoch [119/120    avg_loss:0.108, val_acc:0.971]
Epoch [120/120    avg_loss:0.072, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 225   0   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   5   3 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17   0 189   0   0   0   0   0   0   0]
 [  0   0  25   0   0   0   0  69   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.29424307036247

F1 scores:
[       nan 1.         0.94193548 0.97826087 0.89977728 0.89250814
 0.95696203 0.84662577 0.99614891 1.         1.         1.
 1.         1.        ]

Kappa:
0.9810070274504643
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f44c75a8ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.206]
Epoch [2/120    avg_loss:2.562, val_acc:0.358]
Epoch [3/120    avg_loss:2.498, val_acc:0.371]
Epoch [4/120    avg_loss:2.443, val_acc:0.367]
Epoch [5/120    avg_loss:2.392, val_acc:0.379]
Epoch [6/120    avg_loss:2.340, val_acc:0.410]
Epoch [7/120    avg_loss:2.283, val_acc:0.429]
Epoch [8/120    avg_loss:2.229, val_acc:0.433]
Epoch [9/120    avg_loss:2.145, val_acc:0.446]
Epoch [10/120    avg_loss:2.086, val_acc:0.450]
Epoch [11/120    avg_loss:2.040, val_acc:0.473]
Epoch [12/120    avg_loss:1.984, val_acc:0.498]
Epoch [13/120    avg_loss:1.918, val_acc:0.546]
Epoch [14/120    avg_loss:1.872, val_acc:0.556]
Epoch [15/120    avg_loss:1.834, val_acc:0.577]
Epoch [16/120    avg_loss:1.746, val_acc:0.588]
Epoch [17/120    avg_loss:1.686, val_acc:0.615]
Epoch [18/120    avg_loss:1.621, val_acc:0.629]
Epoch [19/120    avg_loss:1.576, val_acc:0.629]
Epoch [20/120    avg_loss:1.505, val_acc:0.650]
Epoch [21/120    avg_loss:1.416, val_acc:0.652]
Epoch [22/120    avg_loss:1.353, val_acc:0.665]
Epoch [23/120    avg_loss:1.302, val_acc:0.710]
Epoch [24/120    avg_loss:1.259, val_acc:0.677]
Epoch [25/120    avg_loss:1.192, val_acc:0.702]
Epoch [26/120    avg_loss:1.131, val_acc:0.725]
Epoch [27/120    avg_loss:1.074, val_acc:0.762]
Epoch [28/120    avg_loss:1.038, val_acc:0.815]
Epoch [29/120    avg_loss:0.952, val_acc:0.804]
Epoch [30/120    avg_loss:0.924, val_acc:0.833]
Epoch [31/120    avg_loss:0.872, val_acc:0.850]
Epoch [32/120    avg_loss:0.830, val_acc:0.846]
Epoch [33/120    avg_loss:0.860, val_acc:0.852]
Epoch [34/120    avg_loss:0.822, val_acc:0.863]
Epoch [35/120    avg_loss:0.730, val_acc:0.806]
Epoch [36/120    avg_loss:0.716, val_acc:0.906]
Epoch [37/120    avg_loss:0.672, val_acc:0.900]
Epoch [38/120    avg_loss:0.658, val_acc:0.873]
Epoch [39/120    avg_loss:0.616, val_acc:0.877]
Epoch [40/120    avg_loss:0.589, val_acc:0.902]
Epoch [41/120    avg_loss:0.560, val_acc:0.900]
Epoch [42/120    avg_loss:0.711, val_acc:0.900]
Epoch [43/120    avg_loss:0.633, val_acc:0.879]
Epoch [44/120    avg_loss:0.597, val_acc:0.898]
Epoch [45/120    avg_loss:0.552, val_acc:0.910]
Epoch [46/120    avg_loss:0.519, val_acc:0.908]
Epoch [47/120    avg_loss:0.509, val_acc:0.898]
Epoch [48/120    avg_loss:0.511, val_acc:0.898]
Epoch [49/120    avg_loss:0.500, val_acc:0.877]
Epoch [50/120    avg_loss:0.446, val_acc:0.921]
Epoch [51/120    avg_loss:0.423, val_acc:0.929]
Epoch [52/120    avg_loss:0.411, val_acc:0.915]
Epoch [53/120    avg_loss:0.442, val_acc:0.923]
Epoch [54/120    avg_loss:0.441, val_acc:0.908]
Epoch [55/120    avg_loss:0.412, val_acc:0.910]
Epoch [56/120    avg_loss:0.397, val_acc:0.925]
Epoch [57/120    avg_loss:0.369, val_acc:0.912]
Epoch [58/120    avg_loss:0.402, val_acc:0.935]
Epoch [59/120    avg_loss:0.374, val_acc:0.935]
Epoch [60/120    avg_loss:0.352, val_acc:0.938]
Epoch [61/120    avg_loss:0.337, val_acc:0.935]
Epoch [62/120    avg_loss:0.340, val_acc:0.919]
Epoch [63/120    avg_loss:0.368, val_acc:0.919]
Epoch [64/120    avg_loss:0.341, val_acc:0.931]
Epoch [65/120    avg_loss:0.301, val_acc:0.942]
Epoch [66/120    avg_loss:0.286, val_acc:0.950]
Epoch [67/120    avg_loss:0.299, val_acc:0.908]
Epoch [68/120    avg_loss:0.344, val_acc:0.927]
Epoch [69/120    avg_loss:0.321, val_acc:0.923]
Epoch [70/120    avg_loss:0.268, val_acc:0.942]
Epoch [71/120    avg_loss:0.279, val_acc:0.927]
Epoch [72/120    avg_loss:0.332, val_acc:0.923]
Epoch [73/120    avg_loss:0.267, val_acc:0.923]
Epoch [74/120    avg_loss:0.238, val_acc:0.940]
Epoch [75/120    avg_loss:0.278, val_acc:0.925]
Epoch [76/120    avg_loss:0.274, val_acc:0.908]
Epoch [77/120    avg_loss:0.303, val_acc:0.925]
Epoch [78/120    avg_loss:0.270, val_acc:0.950]
Epoch [79/120    avg_loss:0.222, val_acc:0.940]
Epoch [80/120    avg_loss:0.223, val_acc:0.948]
Epoch [81/120    avg_loss:0.205, val_acc:0.942]
Epoch [82/120    avg_loss:0.218, val_acc:0.954]
Epoch [83/120    avg_loss:0.199, val_acc:0.952]
Epoch [84/120    avg_loss:0.182, val_acc:0.942]
Epoch [85/120    avg_loss:0.228, val_acc:0.950]
Epoch [86/120    avg_loss:0.192, val_acc:0.958]
Epoch [87/120    avg_loss:0.227, val_acc:0.944]
Epoch [88/120    avg_loss:0.207, val_acc:0.948]
Epoch [89/120    avg_loss:0.219, val_acc:0.950]
Epoch [90/120    avg_loss:0.176, val_acc:0.950]
Epoch [91/120    avg_loss:0.159, val_acc:0.956]
Epoch [92/120    avg_loss:0.193, val_acc:0.963]
Epoch [93/120    avg_loss:0.196, val_acc:0.952]
Epoch [94/120    avg_loss:0.199, val_acc:0.946]
Epoch [95/120    avg_loss:0.175, val_acc:0.954]
Epoch [96/120    avg_loss:0.143, val_acc:0.956]
Epoch [97/120    avg_loss:0.168, val_acc:0.944]
Epoch [98/120    avg_loss:0.222, val_acc:0.935]
Epoch [99/120    avg_loss:0.180, val_acc:0.940]
Epoch [100/120    avg_loss:0.168, val_acc:0.948]
Epoch [101/120    avg_loss:0.163, val_acc:0.950]
Epoch [102/120    avg_loss:0.195, val_acc:0.958]
Epoch [103/120    avg_loss:0.168, val_acc:0.958]
Epoch [104/120    avg_loss:0.156, val_acc:0.967]
Epoch [105/120    avg_loss:0.175, val_acc:0.948]
Epoch [106/120    avg_loss:0.172, val_acc:0.946]
Epoch [107/120    avg_loss:0.145, val_acc:0.956]
Epoch [108/120    avg_loss:0.151, val_acc:0.950]
Epoch [109/120    avg_loss:0.136, val_acc:0.938]
Epoch [110/120    avg_loss:0.143, val_acc:0.956]
Epoch [111/120    avg_loss:0.116, val_acc:0.954]
Epoch [112/120    avg_loss:0.134, val_acc:0.967]
Epoch [113/120    avg_loss:0.114, val_acc:0.965]
Epoch [114/120    avg_loss:0.142, val_acc:0.963]
Epoch [115/120    avg_loss:0.164, val_acc:0.958]
Epoch [116/120    avg_loss:0.145, val_acc:0.965]
Epoch [117/120    avg_loss:0.145, val_acc:0.948]
Epoch [118/120    avg_loss:0.128, val_acc:0.946]
Epoch [119/120    avg_loss:0.136, val_acc:0.958]
Epoch [120/120    avg_loss:0.132, val_acc:0.954]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 227   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   4 196  27   0   0   0   0   0   0   0   0]
 [  0   0   0  11   3 131   0   0   0   0   0   0   0   0]
 [  0  43   0   0   3   0 160   0   0   0   0   0   0   0]
 [  0   0  33   0   0   0   0  61   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.29211087420042

F1 scores:
[       nan 0.96956829 0.92600423 0.96186441 0.91162791 0.86468647
 0.87431694 0.78709677 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9698162884610736
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba5d85eb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.650, val_acc:0.065]
Epoch [2/120    avg_loss:2.573, val_acc:0.219]
Epoch [3/120    avg_loss:2.507, val_acc:0.317]
Epoch [4/120    avg_loss:2.447, val_acc:0.325]
Epoch [5/120    avg_loss:2.398, val_acc:0.354]
Epoch [6/120    avg_loss:2.367, val_acc:0.377]
Epoch [7/120    avg_loss:2.323, val_acc:0.371]
Epoch [8/120    avg_loss:2.289, val_acc:0.419]
Epoch [9/120    avg_loss:2.233, val_acc:0.427]
Epoch [10/120    avg_loss:2.210, val_acc:0.448]
Epoch [11/120    avg_loss:2.157, val_acc:0.479]
Epoch [12/120    avg_loss:2.115, val_acc:0.494]
Epoch [13/120    avg_loss:2.074, val_acc:0.531]
Epoch [14/120    avg_loss:2.024, val_acc:0.542]
Epoch [15/120    avg_loss:1.971, val_acc:0.556]
Epoch [16/120    avg_loss:1.904, val_acc:0.573]
Epoch [17/120    avg_loss:1.850, val_acc:0.590]
Epoch [18/120    avg_loss:1.776, val_acc:0.585]
Epoch [19/120    avg_loss:1.708, val_acc:0.596]
Epoch [20/120    avg_loss:1.656, val_acc:0.590]
Epoch [21/120    avg_loss:1.604, val_acc:0.615]
Epoch [22/120    avg_loss:1.523, val_acc:0.625]
Epoch [23/120    avg_loss:1.459, val_acc:0.673]
Epoch [24/120    avg_loss:1.392, val_acc:0.669]
Epoch [25/120    avg_loss:1.318, val_acc:0.675]
Epoch [26/120    avg_loss:1.269, val_acc:0.702]
Epoch [27/120    avg_loss:1.232, val_acc:0.704]
Epoch [28/120    avg_loss:1.129, val_acc:0.719]
Epoch [29/120    avg_loss:1.090, val_acc:0.740]
Epoch [30/120    avg_loss:1.075, val_acc:0.767]
Epoch [31/120    avg_loss:1.051, val_acc:0.783]
Epoch [32/120    avg_loss:0.976, val_acc:0.794]
Epoch [33/120    avg_loss:0.928, val_acc:0.796]
Epoch [34/120    avg_loss:0.915, val_acc:0.787]
Epoch [35/120    avg_loss:0.892, val_acc:0.804]
Epoch [36/120    avg_loss:0.886, val_acc:0.817]
Epoch [37/120    avg_loss:0.815, val_acc:0.848]
Epoch [38/120    avg_loss:0.761, val_acc:0.875]
Epoch [39/120    avg_loss:0.714, val_acc:0.887]
Epoch [40/120    avg_loss:0.660, val_acc:0.858]
Epoch [41/120    avg_loss:0.665, val_acc:0.902]
Epoch [42/120    avg_loss:0.610, val_acc:0.890]
Epoch [43/120    avg_loss:0.624, val_acc:0.898]
Epoch [44/120    avg_loss:0.571, val_acc:0.910]
Epoch [45/120    avg_loss:0.544, val_acc:0.921]
Epoch [46/120    avg_loss:0.579, val_acc:0.917]
Epoch [47/120    avg_loss:0.541, val_acc:0.898]
Epoch [48/120    avg_loss:0.501, val_acc:0.915]
Epoch [49/120    avg_loss:0.490, val_acc:0.921]
Epoch [50/120    avg_loss:0.488, val_acc:0.915]
Epoch [51/120    avg_loss:0.447, val_acc:0.919]
Epoch [52/120    avg_loss:0.454, val_acc:0.919]
Epoch [53/120    avg_loss:0.433, val_acc:0.925]
Epoch [54/120    avg_loss:0.392, val_acc:0.883]
Epoch [55/120    avg_loss:0.421, val_acc:0.863]
Epoch [56/120    avg_loss:0.428, val_acc:0.912]
Epoch [57/120    avg_loss:0.371, val_acc:0.931]
Epoch [58/120    avg_loss:0.379, val_acc:0.919]
Epoch [59/120    avg_loss:0.321, val_acc:0.912]
Epoch [60/120    avg_loss:0.298, val_acc:0.921]
Epoch [61/120    avg_loss:0.361, val_acc:0.923]
Epoch [62/120    avg_loss:0.329, val_acc:0.923]
Epoch [63/120    avg_loss:0.314, val_acc:0.900]
Epoch [64/120    avg_loss:0.303, val_acc:0.927]
Epoch [65/120    avg_loss:0.321, val_acc:0.942]
Epoch [66/120    avg_loss:0.288, val_acc:0.925]
Epoch [67/120    avg_loss:0.280, val_acc:0.942]
Epoch [68/120    avg_loss:0.289, val_acc:0.944]
Epoch [69/120    avg_loss:0.220, val_acc:0.944]
Epoch [70/120    avg_loss:0.242, val_acc:0.931]
Epoch [71/120    avg_loss:0.335, val_acc:0.933]
Epoch [72/120    avg_loss:0.291, val_acc:0.938]
Epoch [73/120    avg_loss:0.286, val_acc:0.933]
Epoch [74/120    avg_loss:0.309, val_acc:0.935]
Epoch [75/120    avg_loss:0.232, val_acc:0.935]
Epoch [76/120    avg_loss:0.196, val_acc:0.952]
Epoch [77/120    avg_loss:0.215, val_acc:0.879]
Epoch [78/120    avg_loss:0.227, val_acc:0.940]
Epoch [79/120    avg_loss:0.243, val_acc:0.944]
Epoch [80/120    avg_loss:0.247, val_acc:0.894]
Epoch [81/120    avg_loss:0.232, val_acc:0.938]
Epoch [82/120    avg_loss:0.173, val_acc:0.960]
Epoch [83/120    avg_loss:0.194, val_acc:0.946]
Epoch [84/120    avg_loss:0.180, val_acc:0.948]
Epoch [85/120    avg_loss:0.162, val_acc:0.963]
Epoch [86/120    avg_loss:0.148, val_acc:0.946]
Epoch [87/120    avg_loss:0.184, val_acc:0.952]
Epoch [88/120    avg_loss:0.142, val_acc:0.963]
Epoch [89/120    avg_loss:0.134, val_acc:0.952]
Epoch [90/120    avg_loss:0.112, val_acc:0.958]
Epoch [91/120    avg_loss:0.121, val_acc:0.952]
Epoch [92/120    avg_loss:0.107, val_acc:0.950]
Epoch [93/120    avg_loss:0.144, val_acc:0.952]
Epoch [94/120    avg_loss:0.123, val_acc:0.956]
Epoch [95/120    avg_loss:0.141, val_acc:0.958]
Epoch [96/120    avg_loss:0.115, val_acc:0.927]
Epoch [97/120    avg_loss:0.138, val_acc:0.940]
Epoch [98/120    avg_loss:0.127, val_acc:0.960]
Epoch [99/120    avg_loss:0.140, val_acc:0.973]
Epoch [100/120    avg_loss:0.173, val_acc:0.969]
Epoch [101/120    avg_loss:0.137, val_acc:0.952]
Epoch [102/120    avg_loss:0.150, val_acc:0.944]
Epoch [103/120    avg_loss:0.148, val_acc:0.960]
Epoch [104/120    avg_loss:0.147, val_acc:0.965]
Epoch [105/120    avg_loss:0.105, val_acc:0.946]
Epoch [106/120    avg_loss:0.114, val_acc:0.967]
Epoch [107/120    avg_loss:0.116, val_acc:0.940]
Epoch [108/120    avg_loss:0.155, val_acc:0.965]
Epoch [109/120    avg_loss:0.125, val_acc:0.950]
Epoch [110/120    avg_loss:0.106, val_acc:0.958]
Epoch [111/120    avg_loss:0.098, val_acc:0.950]
Epoch [112/120    avg_loss:0.081, val_acc:0.952]
Epoch [113/120    avg_loss:0.118, val_acc:0.965]
Epoch [114/120    avg_loss:0.083, val_acc:0.973]
Epoch [115/120    avg_loss:0.068, val_acc:0.971]
Epoch [116/120    avg_loss:0.058, val_acc:0.973]
Epoch [117/120    avg_loss:0.056, val_acc:0.973]
Epoch [118/120    avg_loss:0.075, val_acc:0.973]
Epoch [119/120    avg_loss:0.066, val_acc:0.977]
Epoch [120/120    avg_loss:0.052, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  18   0   0   0   0   0   0   1   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.69936034115139

F1 scores:
[       nan 1.         0.96263736 0.97777778 0.90434783 0.88737201
 1.         0.90697674 0.99870968 1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9855184258277299
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1f4183ca90>
supervision:full
center_pixel:True
Network :
Number of parameter: 29387==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.632, val_acc:0.094]
Epoch [2/120    avg_loss:2.557, val_acc:0.188]
Epoch [3/120    avg_loss:2.488, val_acc:0.317]
Epoch [4/120    avg_loss:2.435, val_acc:0.383]
Epoch [5/120    avg_loss:2.388, val_acc:0.410]
Epoch [6/120    avg_loss:2.341, val_acc:0.442]
Epoch [7/120    avg_loss:2.290, val_acc:0.473]
Epoch [8/120    avg_loss:2.247, val_acc:0.527]
Epoch [9/120    avg_loss:2.189, val_acc:0.527]
Epoch [10/120    avg_loss:2.136, val_acc:0.552]
Epoch [11/120    avg_loss:2.066, val_acc:0.550]
Epoch [12/120    avg_loss:1.991, val_acc:0.554]
Epoch [13/120    avg_loss:1.920, val_acc:0.546]
Epoch [14/120    avg_loss:1.852, val_acc:0.594]
Epoch [15/120    avg_loss:1.778, val_acc:0.610]
Epoch [16/120    avg_loss:1.691, val_acc:0.640]
Epoch [17/120    avg_loss:1.623, val_acc:0.688]
Epoch [18/120    avg_loss:1.532, val_acc:0.683]
Epoch [19/120    avg_loss:1.464, val_acc:0.721]
Epoch [20/120    avg_loss:1.400, val_acc:0.729]
Epoch [21/120    avg_loss:1.322, val_acc:0.706]
Epoch [22/120    avg_loss:1.264, val_acc:0.738]
Epoch [23/120    avg_loss:1.168, val_acc:0.748]
Epoch [24/120    avg_loss:1.095, val_acc:0.750]
Epoch [25/120    avg_loss:1.030, val_acc:0.787]
Epoch [26/120    avg_loss:1.003, val_acc:0.767]
Epoch [27/120    avg_loss:0.957, val_acc:0.802]
Epoch [28/120    avg_loss:0.900, val_acc:0.756]
Epoch [29/120    avg_loss:0.843, val_acc:0.802]
Epoch [30/120    avg_loss:0.767, val_acc:0.829]
Epoch [31/120    avg_loss:0.738, val_acc:0.825]
Epoch [32/120    avg_loss:0.700, val_acc:0.798]
Epoch [33/120    avg_loss:0.710, val_acc:0.856]
Epoch [34/120    avg_loss:0.651, val_acc:0.881]
Epoch [35/120    avg_loss:0.604, val_acc:0.917]
Epoch [36/120    avg_loss:0.615, val_acc:0.887]
Epoch [37/120    avg_loss:0.571, val_acc:0.894]
Epoch [38/120    avg_loss:0.539, val_acc:0.921]
Epoch [39/120    avg_loss:0.534, val_acc:0.923]
Epoch [40/120    avg_loss:0.503, val_acc:0.925]
Epoch [41/120    avg_loss:0.501, val_acc:0.879]
Epoch [42/120    avg_loss:0.505, val_acc:0.887]
Epoch [43/120    avg_loss:0.516, val_acc:0.858]
Epoch [44/120    avg_loss:0.456, val_acc:0.923]
Epoch [45/120    avg_loss:0.452, val_acc:0.921]
Epoch [46/120    avg_loss:0.409, val_acc:0.923]
Epoch [47/120    avg_loss:0.398, val_acc:0.944]
Epoch [48/120    avg_loss:0.416, val_acc:0.942]
Epoch [49/120    avg_loss:0.422, val_acc:0.852]
Epoch [50/120    avg_loss:0.438, val_acc:0.946]
Epoch [51/120    avg_loss:0.354, val_acc:0.915]
Epoch [52/120    avg_loss:0.397, val_acc:0.931]
Epoch [53/120    avg_loss:0.367, val_acc:0.935]
Epoch [54/120    avg_loss:0.343, val_acc:0.960]
Epoch [55/120    avg_loss:0.332, val_acc:0.946]
Epoch [56/120    avg_loss:0.386, val_acc:0.935]
Epoch [57/120    avg_loss:0.320, val_acc:0.923]
Epoch [58/120    avg_loss:0.380, val_acc:0.923]
Epoch [59/120    avg_loss:0.299, val_acc:0.950]
Epoch [60/120    avg_loss:0.334, val_acc:0.948]
Epoch [61/120    avg_loss:0.365, val_acc:0.923]
Epoch [62/120    avg_loss:0.319, val_acc:0.942]
Epoch [63/120    avg_loss:0.317, val_acc:0.938]
Epoch [64/120    avg_loss:0.320, val_acc:0.912]
Epoch [65/120    avg_loss:0.284, val_acc:0.956]
Epoch [66/120    avg_loss:0.282, val_acc:0.967]
Epoch [67/120    avg_loss:0.259, val_acc:0.892]
Epoch [68/120    avg_loss:0.274, val_acc:0.952]
Epoch [69/120    avg_loss:0.256, val_acc:0.960]
Epoch [70/120    avg_loss:0.289, val_acc:0.942]
Epoch [71/120    avg_loss:0.258, val_acc:0.935]
Epoch [72/120    avg_loss:0.245, val_acc:0.963]
Epoch [73/120    avg_loss:0.220, val_acc:0.956]
Epoch [74/120    avg_loss:0.232, val_acc:0.946]
Epoch [75/120    avg_loss:0.209, val_acc:0.967]
Epoch [76/120    avg_loss:0.187, val_acc:0.960]
Epoch [77/120    avg_loss:0.192, val_acc:0.960]
Epoch [78/120    avg_loss:0.175, val_acc:0.956]
Epoch [79/120    avg_loss:0.165, val_acc:0.981]
Epoch [80/120    avg_loss:0.182, val_acc:0.969]
Epoch [81/120    avg_loss:0.200, val_acc:0.975]
Epoch [82/120    avg_loss:0.192, val_acc:0.950]
Epoch [83/120    avg_loss:0.170, val_acc:0.975]
Epoch [84/120    avg_loss:0.195, val_acc:0.954]
Epoch [85/120    avg_loss:0.183, val_acc:0.969]
Epoch [86/120    avg_loss:0.198, val_acc:0.954]
Epoch [87/120    avg_loss:0.187, val_acc:0.965]
Epoch [88/120    avg_loss:0.178, val_acc:0.942]
Epoch [89/120    avg_loss:0.195, val_acc:0.954]
Epoch [90/120    avg_loss:0.194, val_acc:0.944]
Epoch [91/120    avg_loss:0.198, val_acc:0.975]
Epoch [92/120    avg_loss:0.163, val_acc:0.956]
Epoch [93/120    avg_loss:0.148, val_acc:0.969]
Epoch [94/120    avg_loss:0.138, val_acc:0.971]
Epoch [95/120    avg_loss:0.125, val_acc:0.977]
Epoch [96/120    avg_loss:0.117, val_acc:0.983]
Epoch [97/120    avg_loss:0.137, val_acc:0.981]
Epoch [98/120    avg_loss:0.125, val_acc:0.977]
Epoch [99/120    avg_loss:0.121, val_acc:0.981]
Epoch [100/120    avg_loss:0.114, val_acc:0.983]
Epoch [101/120    avg_loss:0.128, val_acc:0.977]
Epoch [102/120    avg_loss:0.105, val_acc:0.979]
Epoch [103/120    avg_loss:0.118, val_acc:0.990]
Epoch [104/120    avg_loss:0.107, val_acc:0.983]
Epoch [105/120    avg_loss:0.112, val_acc:0.981]
Epoch [106/120    avg_loss:0.108, val_acc:0.983]
Epoch [107/120    avg_loss:0.132, val_acc:0.981]
Epoch [108/120    avg_loss:0.108, val_acc:0.981]
Epoch [109/120    avg_loss:0.122, val_acc:0.985]
Epoch [110/120    avg_loss:0.106, val_acc:0.988]
Epoch [111/120    avg_loss:0.109, val_acc:0.983]
Epoch [112/120    avg_loss:0.115, val_acc:0.988]
Epoch [113/120    avg_loss:0.122, val_acc:0.981]
Epoch [114/120    avg_loss:0.115, val_acc:0.990]
Epoch [115/120    avg_loss:0.096, val_acc:0.990]
Epoch [116/120    avg_loss:0.103, val_acc:0.988]
Epoch [117/120    avg_loss:0.102, val_acc:0.983]
Epoch [118/120    avg_loss:0.105, val_acc:0.983]
Epoch [119/120    avg_loss:0.102, val_acc:0.983]
Epoch [120/120    avg_loss:0.099, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   1 224   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  28 117   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.272921108742

F1 scores:
[       nan 1.         0.94594595 0.98678414 0.87931034 0.82105263
 1.         0.87431694 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9807710461732286
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48956aba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.610, val_acc:0.100]
Epoch [2/120    avg_loss:2.543, val_acc:0.135]
Epoch [3/120    avg_loss:2.478, val_acc:0.344]
Epoch [4/120    avg_loss:2.407, val_acc:0.362]
Epoch [5/120    avg_loss:2.350, val_acc:0.338]
Epoch [6/120    avg_loss:2.278, val_acc:0.396]
Epoch [7/120    avg_loss:2.194, val_acc:0.419]
Epoch [8/120    avg_loss:2.131, val_acc:0.452]
Epoch [9/120    avg_loss:2.057, val_acc:0.433]
Epoch [10/120    avg_loss:2.035, val_acc:0.487]
Epoch [11/120    avg_loss:1.980, val_acc:0.496]
Epoch [12/120    avg_loss:1.919, val_acc:0.515]
Epoch [13/120    avg_loss:1.868, val_acc:0.592]
Epoch [14/120    avg_loss:1.779, val_acc:0.615]
Epoch [15/120    avg_loss:1.766, val_acc:0.646]
Epoch [16/120    avg_loss:1.695, val_acc:0.658]
Epoch [17/120    avg_loss:1.628, val_acc:0.662]
Epoch [18/120    avg_loss:1.574, val_acc:0.667]
Epoch [19/120    avg_loss:1.514, val_acc:0.667]
Epoch [20/120    avg_loss:1.442, val_acc:0.683]
Epoch [21/120    avg_loss:1.394, val_acc:0.704]
Epoch [22/120    avg_loss:1.310, val_acc:0.688]
Epoch [23/120    avg_loss:1.263, val_acc:0.713]
Epoch [24/120    avg_loss:1.187, val_acc:0.681]
Epoch [25/120    avg_loss:1.156, val_acc:0.688]
Epoch [26/120    avg_loss:1.087, val_acc:0.742]
Epoch [27/120    avg_loss:1.001, val_acc:0.742]
Epoch [28/120    avg_loss:0.969, val_acc:0.798]
Epoch [29/120    avg_loss:0.985, val_acc:0.729]
Epoch [30/120    avg_loss:0.909, val_acc:0.815]
Epoch [31/120    avg_loss:0.840, val_acc:0.879]
Epoch [32/120    avg_loss:0.790, val_acc:0.863]
Epoch [33/120    avg_loss:0.719, val_acc:0.904]
Epoch [34/120    avg_loss:0.710, val_acc:0.894]
Epoch [35/120    avg_loss:0.671, val_acc:0.917]
Epoch [36/120    avg_loss:0.661, val_acc:0.877]
Epoch [37/120    avg_loss:0.599, val_acc:0.896]
Epoch [38/120    avg_loss:0.552, val_acc:0.917]
Epoch [39/120    avg_loss:0.520, val_acc:0.925]
Epoch [40/120    avg_loss:0.512, val_acc:0.925]
Epoch [41/120    avg_loss:0.510, val_acc:0.906]
Epoch [42/120    avg_loss:0.490, val_acc:0.904]
Epoch [43/120    avg_loss:0.500, val_acc:0.908]
Epoch [44/120    avg_loss:0.477, val_acc:0.927]
Epoch [45/120    avg_loss:0.477, val_acc:0.921]
Epoch [46/120    avg_loss:0.441, val_acc:0.944]
Epoch [47/120    avg_loss:0.397, val_acc:0.923]
Epoch [48/120    avg_loss:0.384, val_acc:0.883]
Epoch [49/120    avg_loss:0.364, val_acc:0.923]
Epoch [50/120    avg_loss:0.354, val_acc:0.935]
Epoch [51/120    avg_loss:0.362, val_acc:0.931]
Epoch [52/120    avg_loss:0.380, val_acc:0.946]
Epoch [53/120    avg_loss:0.348, val_acc:0.921]
Epoch [54/120    avg_loss:0.356, val_acc:0.927]
Epoch [55/120    avg_loss:0.341, val_acc:0.944]
Epoch [56/120    avg_loss:0.286, val_acc:0.935]
Epoch [57/120    avg_loss:0.354, val_acc:0.942]
Epoch [58/120    avg_loss:0.338, val_acc:0.950]
Epoch [59/120    avg_loss:0.285, val_acc:0.938]
Epoch [60/120    avg_loss:0.316, val_acc:0.946]
Epoch [61/120    avg_loss:0.306, val_acc:0.938]
Epoch [62/120    avg_loss:0.260, val_acc:0.954]
Epoch [63/120    avg_loss:0.254, val_acc:0.963]
Epoch [64/120    avg_loss:0.234, val_acc:0.958]
Epoch [65/120    avg_loss:0.252, val_acc:0.963]
Epoch [66/120    avg_loss:0.237, val_acc:0.952]
Epoch [67/120    avg_loss:0.225, val_acc:0.948]
Epoch [68/120    avg_loss:0.236, val_acc:0.948]
Epoch [69/120    avg_loss:0.215, val_acc:0.965]
Epoch [70/120    avg_loss:0.200, val_acc:0.942]
Epoch [71/120    avg_loss:0.215, val_acc:0.954]
Epoch [72/120    avg_loss:0.181, val_acc:0.960]
Epoch [73/120    avg_loss:0.215, val_acc:0.938]
Epoch [74/120    avg_loss:0.219, val_acc:0.954]
Epoch [75/120    avg_loss:0.185, val_acc:0.956]
Epoch [76/120    avg_loss:0.191, val_acc:0.960]
Epoch [77/120    avg_loss:0.177, val_acc:0.954]
Epoch [78/120    avg_loss:0.169, val_acc:0.958]
Epoch [79/120    avg_loss:0.151, val_acc:0.963]
Epoch [80/120    avg_loss:0.125, val_acc:0.973]
Epoch [81/120    avg_loss:0.150, val_acc:0.967]
Epoch [82/120    avg_loss:0.195, val_acc:0.950]
Epoch [83/120    avg_loss:0.208, val_acc:0.938]
Epoch [84/120    avg_loss:0.213, val_acc:0.958]
Epoch [85/120    avg_loss:0.179, val_acc:0.960]
Epoch [86/120    avg_loss:0.137, val_acc:0.975]
Epoch [87/120    avg_loss:0.140, val_acc:0.969]
Epoch [88/120    avg_loss:0.152, val_acc:0.973]
Epoch [89/120    avg_loss:0.131, val_acc:0.975]
Epoch [90/120    avg_loss:0.136, val_acc:0.977]
Epoch [91/120    avg_loss:0.157, val_acc:0.965]
Epoch [92/120    avg_loss:0.169, val_acc:0.965]
Epoch [93/120    avg_loss:0.150, val_acc:0.956]
Epoch [94/120    avg_loss:0.138, val_acc:0.975]
Epoch [95/120    avg_loss:0.121, val_acc:0.981]
Epoch [96/120    avg_loss:0.118, val_acc:0.979]
Epoch [97/120    avg_loss:0.108, val_acc:0.971]
Epoch [98/120    avg_loss:0.093, val_acc:0.977]
Epoch [99/120    avg_loss:0.093, val_acc:0.979]
Epoch [100/120    avg_loss:0.107, val_acc:0.988]
Epoch [101/120    avg_loss:0.112, val_acc:0.956]
Epoch [102/120    avg_loss:0.131, val_acc:0.969]
Epoch [103/120    avg_loss:0.111, val_acc:0.977]
Epoch [104/120    avg_loss:0.100, val_acc:0.969]
Epoch [105/120    avg_loss:0.090, val_acc:0.988]
Epoch [106/120    avg_loss:0.098, val_acc:0.973]
Epoch [107/120    avg_loss:0.091, val_acc:0.973]
Epoch [108/120    avg_loss:0.101, val_acc:0.981]
Epoch [109/120    avg_loss:0.073, val_acc:0.971]
Epoch [110/120    avg_loss:0.082, val_acc:0.981]
Epoch [111/120    avg_loss:0.079, val_acc:0.971]
Epoch [112/120    avg_loss:0.084, val_acc:0.990]
Epoch [113/120    avg_loss:0.084, val_acc:0.977]
Epoch [114/120    avg_loss:0.085, val_acc:0.971]
Epoch [115/120    avg_loss:0.084, val_acc:0.975]
Epoch [116/120    avg_loss:0.066, val_acc:0.981]
Epoch [117/120    avg_loss:0.067, val_acc:0.983]
Epoch [118/120    avg_loss:0.055, val_acc:0.981]
Epoch [119/120    avg_loss:0.073, val_acc:0.983]
Epoch [120/120    avg_loss:0.071, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   0   0   4   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   9 194  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   3   0   0   7   0 196   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 0.99488678 0.96475771 0.98081023 0.89814815 0.90967742
 0.97512438 0.90697674 0.99487179 1.         1.         1.
 1.         1.        ]

Kappa:
0.9840948121896614
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a33d66a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.131]
Epoch [2/120    avg_loss:2.542, val_acc:0.183]
Epoch [3/120    avg_loss:2.484, val_acc:0.317]
Epoch [4/120    avg_loss:2.424, val_acc:0.323]
Epoch [5/120    avg_loss:2.370, val_acc:0.350]
Epoch [6/120    avg_loss:2.332, val_acc:0.381]
Epoch [7/120    avg_loss:2.287, val_acc:0.438]
Epoch [8/120    avg_loss:2.242, val_acc:0.458]
Epoch [9/120    avg_loss:2.197, val_acc:0.490]
Epoch [10/120    avg_loss:2.146, val_acc:0.504]
Epoch [11/120    avg_loss:2.094, val_acc:0.519]
Epoch [12/120    avg_loss:2.050, val_acc:0.540]
Epoch [13/120    avg_loss:1.989, val_acc:0.562]
Epoch [14/120    avg_loss:1.925, val_acc:0.581]
Epoch [15/120    avg_loss:1.858, val_acc:0.594]
Epoch [16/120    avg_loss:1.802, val_acc:0.613]
Epoch [17/120    avg_loss:1.735, val_acc:0.613]
Epoch [18/120    avg_loss:1.652, val_acc:0.629]
Epoch [19/120    avg_loss:1.567, val_acc:0.648]
Epoch [20/120    avg_loss:1.505, val_acc:0.669]
Epoch [21/120    avg_loss:1.423, val_acc:0.708]
Epoch [22/120    avg_loss:1.349, val_acc:0.725]
Epoch [23/120    avg_loss:1.267, val_acc:0.713]
Epoch [24/120    avg_loss:1.214, val_acc:0.708]
Epoch [25/120    avg_loss:1.175, val_acc:0.777]
Epoch [26/120    avg_loss:1.115, val_acc:0.752]
Epoch [27/120    avg_loss:1.027, val_acc:0.825]
Epoch [28/120    avg_loss:1.000, val_acc:0.769]
Epoch [29/120    avg_loss:0.999, val_acc:0.852]
Epoch [30/120    avg_loss:0.938, val_acc:0.781]
Epoch [31/120    avg_loss:0.874, val_acc:0.844]
Epoch [32/120    avg_loss:0.822, val_acc:0.867]
Epoch [33/120    avg_loss:0.778, val_acc:0.881]
Epoch [34/120    avg_loss:0.747, val_acc:0.875]
Epoch [35/120    avg_loss:0.743, val_acc:0.890]
Epoch [36/120    avg_loss:0.679, val_acc:0.894]
Epoch [37/120    avg_loss:0.648, val_acc:0.783]
Epoch [38/120    avg_loss:0.664, val_acc:0.792]
Epoch [39/120    avg_loss:0.662, val_acc:0.854]
Epoch [40/120    avg_loss:0.653, val_acc:0.879]
Epoch [41/120    avg_loss:0.567, val_acc:0.881]
Epoch [42/120    avg_loss:0.559, val_acc:0.896]
Epoch [43/120    avg_loss:0.530, val_acc:0.883]
Epoch [44/120    avg_loss:0.503, val_acc:0.877]
Epoch [45/120    avg_loss:0.476, val_acc:0.875]
Epoch [46/120    avg_loss:0.442, val_acc:0.902]
Epoch [47/120    avg_loss:0.433, val_acc:0.919]
Epoch [48/120    avg_loss:0.432, val_acc:0.900]
Epoch [49/120    avg_loss:0.473, val_acc:0.915]
Epoch [50/120    avg_loss:0.408, val_acc:0.910]
Epoch [51/120    avg_loss:0.371, val_acc:0.927]
Epoch [52/120    avg_loss:0.352, val_acc:0.925]
Epoch [53/120    avg_loss:0.311, val_acc:0.925]
Epoch [54/120    avg_loss:0.338, val_acc:0.933]
Epoch [55/120    avg_loss:0.307, val_acc:0.940]
Epoch [56/120    avg_loss:0.301, val_acc:0.929]
Epoch [57/120    avg_loss:0.314, val_acc:0.917]
Epoch [58/120    avg_loss:0.280, val_acc:0.933]
Epoch [59/120    avg_loss:0.288, val_acc:0.946]
Epoch [60/120    avg_loss:0.289, val_acc:0.942]
Epoch [61/120    avg_loss:0.262, val_acc:0.950]
Epoch [62/120    avg_loss:0.258, val_acc:0.956]
Epoch [63/120    avg_loss:0.272, val_acc:0.887]
Epoch [64/120    avg_loss:0.383, val_acc:0.890]
Epoch [65/120    avg_loss:0.331, val_acc:0.925]
Epoch [66/120    avg_loss:0.258, val_acc:0.935]
Epoch [67/120    avg_loss:0.272, val_acc:0.948]
Epoch [68/120    avg_loss:0.283, val_acc:0.935]
Epoch [69/120    avg_loss:0.256, val_acc:0.927]
Epoch [70/120    avg_loss:0.233, val_acc:0.965]
Epoch [71/120    avg_loss:0.232, val_acc:0.938]
Epoch [72/120    avg_loss:0.213, val_acc:0.958]
Epoch [73/120    avg_loss:0.191, val_acc:0.950]
Epoch [74/120    avg_loss:0.250, val_acc:0.960]
Epoch [75/120    avg_loss:0.197, val_acc:0.969]
Epoch [76/120    avg_loss:0.216, val_acc:0.965]
Epoch [77/120    avg_loss:0.175, val_acc:0.960]
Epoch [78/120    avg_loss:0.179, val_acc:0.963]
Epoch [79/120    avg_loss:0.189, val_acc:0.956]
Epoch [80/120    avg_loss:0.320, val_acc:0.921]
Epoch [81/120    avg_loss:0.287, val_acc:0.950]
Epoch [82/120    avg_loss:0.199, val_acc:0.954]
Epoch [83/120    avg_loss:0.160, val_acc:0.960]
Epoch [84/120    avg_loss:0.176, val_acc:0.965]
Epoch [85/120    avg_loss:0.167, val_acc:0.963]
Epoch [86/120    avg_loss:0.136, val_acc:0.969]
Epoch [87/120    avg_loss:0.121, val_acc:0.971]
Epoch [88/120    avg_loss:0.135, val_acc:0.948]
Epoch [89/120    avg_loss:0.149, val_acc:0.969]
Epoch [90/120    avg_loss:0.129, val_acc:0.977]
Epoch [91/120    avg_loss:0.145, val_acc:0.960]
Epoch [92/120    avg_loss:0.159, val_acc:0.956]
Epoch [93/120    avg_loss:0.149, val_acc:0.973]
Epoch [94/120    avg_loss:0.155, val_acc:0.971]
Epoch [95/120    avg_loss:0.147, val_acc:0.969]
Epoch [96/120    avg_loss:0.154, val_acc:0.944]
Epoch [97/120    avg_loss:0.139, val_acc:0.973]
Epoch [98/120    avg_loss:0.096, val_acc:0.975]
Epoch [99/120    avg_loss:0.090, val_acc:0.983]
Epoch [100/120    avg_loss:0.102, val_acc:0.977]
Epoch [101/120    avg_loss:0.100, val_acc:0.975]
Epoch [102/120    avg_loss:0.113, val_acc:0.983]
Epoch [103/120    avg_loss:0.106, val_acc:0.979]
Epoch [104/120    avg_loss:0.104, val_acc:0.983]
Epoch [105/120    avg_loss:0.101, val_acc:0.981]
Epoch [106/120    avg_loss:0.108, val_acc:0.979]
Epoch [107/120    avg_loss:0.097, val_acc:0.967]
Epoch [108/120    avg_loss:0.117, val_acc:0.981]
Epoch [109/120    avg_loss:0.074, val_acc:0.983]
Epoch [110/120    avg_loss:0.083, val_acc:0.977]
Epoch [111/120    avg_loss:0.066, val_acc:0.983]
Epoch [112/120    avg_loss:0.075, val_acc:0.963]
Epoch [113/120    avg_loss:0.092, val_acc:0.969]
Epoch [114/120    avg_loss:0.100, val_acc:0.985]
Epoch [115/120    avg_loss:0.092, val_acc:0.963]
Epoch [116/120    avg_loss:0.128, val_acc:0.969]
Epoch [117/120    avg_loss:0.126, val_acc:0.983]
Epoch [118/120    avg_loss:0.093, val_acc:0.979]
Epoch [119/120    avg_loss:0.074, val_acc:0.988]
Epoch [120/120    avg_loss:0.060, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 223   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.99853801 0.97117517 0.98454746 0.94247788 0.93288591
 0.99516908 0.93181818 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9902674307965943
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd26d20e9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.682, val_acc:0.081]
Epoch [2/120    avg_loss:2.615, val_acc:0.127]
Epoch [3/120    avg_loss:2.563, val_acc:0.371]
Epoch [4/120    avg_loss:2.510, val_acc:0.373]
Epoch [5/120    avg_loss:2.458, val_acc:0.383]
Epoch [6/120    avg_loss:2.406, val_acc:0.440]
Epoch [7/120    avg_loss:2.348, val_acc:0.475]
Epoch [8/120    avg_loss:2.288, val_acc:0.494]
Epoch [9/120    avg_loss:2.241, val_acc:0.500]
Epoch [10/120    avg_loss:2.176, val_acc:0.525]
Epoch [11/120    avg_loss:2.107, val_acc:0.498]
Epoch [12/120    avg_loss:2.043, val_acc:0.548]
Epoch [13/120    avg_loss:1.963, val_acc:0.562]
Epoch [14/120    avg_loss:1.895, val_acc:0.625]
Epoch [15/120    avg_loss:1.842, val_acc:0.590]
Epoch [16/120    avg_loss:1.796, val_acc:0.635]
Epoch [17/120    avg_loss:1.728, val_acc:0.629]
Epoch [18/120    avg_loss:1.663, val_acc:0.656]
Epoch [19/120    avg_loss:1.555, val_acc:0.660]
Epoch [20/120    avg_loss:1.514, val_acc:0.675]
Epoch [21/120    avg_loss:1.453, val_acc:0.688]
Epoch [22/120    avg_loss:1.367, val_acc:0.750]
Epoch [23/120    avg_loss:1.310, val_acc:0.765]
Epoch [24/120    avg_loss:1.219, val_acc:0.758]
Epoch [25/120    avg_loss:1.153, val_acc:0.790]
Epoch [26/120    avg_loss:1.083, val_acc:0.848]
Epoch [27/120    avg_loss:0.988, val_acc:0.792]
Epoch [28/120    avg_loss:0.986, val_acc:0.806]
Epoch [29/120    avg_loss:0.920, val_acc:0.883]
Epoch [30/120    avg_loss:0.848, val_acc:0.900]
Epoch [31/120    avg_loss:0.809, val_acc:0.894]
Epoch [32/120    avg_loss:0.778, val_acc:0.904]
Epoch [33/120    avg_loss:0.716, val_acc:0.917]
Epoch [34/120    avg_loss:0.704, val_acc:0.879]
Epoch [35/120    avg_loss:0.645, val_acc:0.908]
Epoch [36/120    avg_loss:0.645, val_acc:0.931]
Epoch [37/120    avg_loss:0.622, val_acc:0.908]
Epoch [38/120    avg_loss:0.573, val_acc:0.923]
Epoch [39/120    avg_loss:0.594, val_acc:0.933]
Epoch [40/120    avg_loss:0.565, val_acc:0.935]
Epoch [41/120    avg_loss:0.504, val_acc:0.923]
Epoch [42/120    avg_loss:0.460, val_acc:0.919]
Epoch [43/120    avg_loss:0.453, val_acc:0.935]
Epoch [44/120    avg_loss:0.456, val_acc:0.904]
Epoch [45/120    avg_loss:0.469, val_acc:0.929]
Epoch [46/120    avg_loss:0.430, val_acc:0.929]
Epoch [47/120    avg_loss:0.411, val_acc:0.908]
Epoch [48/120    avg_loss:0.406, val_acc:0.958]
Epoch [49/120    avg_loss:0.389, val_acc:0.958]
Epoch [50/120    avg_loss:0.346, val_acc:0.946]
Epoch [51/120    avg_loss:0.383, val_acc:0.944]
Epoch [52/120    avg_loss:0.347, val_acc:0.965]
Epoch [53/120    avg_loss:0.357, val_acc:0.933]
Epoch [54/120    avg_loss:0.317, val_acc:0.958]
Epoch [55/120    avg_loss:0.340, val_acc:0.971]
Epoch [56/120    avg_loss:0.324, val_acc:0.927]
Epoch [57/120    avg_loss:0.331, val_acc:0.963]
Epoch [58/120    avg_loss:0.262, val_acc:0.973]
Epoch [59/120    avg_loss:0.265, val_acc:0.965]
Epoch [60/120    avg_loss:0.296, val_acc:0.973]
Epoch [61/120    avg_loss:0.253, val_acc:0.969]
Epoch [62/120    avg_loss:0.250, val_acc:0.967]
Epoch [63/120    avg_loss:0.211, val_acc:0.946]
Epoch [64/120    avg_loss:0.275, val_acc:0.950]
Epoch [65/120    avg_loss:0.260, val_acc:0.969]
Epoch [66/120    avg_loss:0.233, val_acc:0.975]
Epoch [67/120    avg_loss:0.205, val_acc:0.975]
Epoch [68/120    avg_loss:0.198, val_acc:0.981]
Epoch [69/120    avg_loss:0.197, val_acc:0.954]
Epoch [70/120    avg_loss:0.271, val_acc:0.954]
Epoch [71/120    avg_loss:0.288, val_acc:0.971]
Epoch [72/120    avg_loss:0.220, val_acc:0.967]
Epoch [73/120    avg_loss:0.218, val_acc:0.979]
Epoch [74/120    avg_loss:0.171, val_acc:0.979]
Epoch [75/120    avg_loss:0.173, val_acc:0.967]
Epoch [76/120    avg_loss:0.159, val_acc:0.969]
Epoch [77/120    avg_loss:0.153, val_acc:0.977]
Epoch [78/120    avg_loss:0.165, val_acc:0.971]
Epoch [79/120    avg_loss:0.170, val_acc:0.973]
Epoch [80/120    avg_loss:0.176, val_acc:0.981]
Epoch [81/120    avg_loss:0.158, val_acc:0.981]
Epoch [82/120    avg_loss:0.132, val_acc:0.977]
Epoch [83/120    avg_loss:0.173, val_acc:0.965]
Epoch [84/120    avg_loss:0.184, val_acc:0.988]
Epoch [85/120    avg_loss:0.161, val_acc:0.973]
Epoch [86/120    avg_loss:0.175, val_acc:0.981]
Epoch [87/120    avg_loss:0.189, val_acc:0.983]
Epoch [88/120    avg_loss:0.217, val_acc:0.985]
Epoch [89/120    avg_loss:0.204, val_acc:0.971]
Epoch [90/120    avg_loss:0.182, val_acc:0.981]
Epoch [91/120    avg_loss:0.120, val_acc:0.983]
Epoch [92/120    avg_loss:0.124, val_acc:0.985]
Epoch [93/120    avg_loss:0.125, val_acc:0.973]
Epoch [94/120    avg_loss:0.208, val_acc:0.971]
Epoch [95/120    avg_loss:0.204, val_acc:0.969]
Epoch [96/120    avg_loss:0.162, val_acc:0.967]
Epoch [97/120    avg_loss:0.111, val_acc:0.977]
Epoch [98/120    avg_loss:0.104, val_acc:0.977]
Epoch [99/120    avg_loss:0.100, val_acc:0.981]
Epoch [100/120    avg_loss:0.103, val_acc:0.981]
Epoch [101/120    avg_loss:0.091, val_acc:0.979]
Epoch [102/120    avg_loss:0.085, val_acc:0.979]
Epoch [103/120    avg_loss:0.080, val_acc:0.981]
Epoch [104/120    avg_loss:0.082, val_acc:0.981]
Epoch [105/120    avg_loss:0.078, val_acc:0.983]
Epoch [106/120    avg_loss:0.097, val_acc:0.985]
Epoch [107/120    avg_loss:0.096, val_acc:0.988]
Epoch [108/120    avg_loss:0.085, val_acc:0.983]
Epoch [109/120    avg_loss:0.085, val_acc:0.983]
Epoch [110/120    avg_loss:0.077, val_acc:0.981]
Epoch [111/120    avg_loss:0.078, val_acc:0.981]
Epoch [112/120    avg_loss:0.087, val_acc:0.981]
Epoch [113/120    avg_loss:0.091, val_acc:0.981]
Epoch [114/120    avg_loss:0.071, val_acc:0.981]
Epoch [115/120    avg_loss:0.099, val_acc:0.985]
Epoch [116/120    avg_loss:0.094, val_acc:0.981]
Epoch [117/120    avg_loss:0.080, val_acc:0.979]
Epoch [118/120    avg_loss:0.083, val_acc:0.981]
Epoch [119/120    avg_loss:0.084, val_acc:0.985]
Epoch [120/120    avg_loss:0.076, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   1   0   0   2   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99926954 0.97550111 0.99343545 0.93181818 0.92258065
 0.98529412 0.9273743  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895556991587874
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feadc0dca90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.250]
Epoch [2/120    avg_loss:2.566, val_acc:0.317]
Epoch [3/120    avg_loss:2.503, val_acc:0.323]
Epoch [4/120    avg_loss:2.448, val_acc:0.365]
Epoch [5/120    avg_loss:2.396, val_acc:0.400]
Epoch [6/120    avg_loss:2.344, val_acc:0.417]
Epoch [7/120    avg_loss:2.310, val_acc:0.452]
Epoch [8/120    avg_loss:2.264, val_acc:0.492]
Epoch [9/120    avg_loss:2.214, val_acc:0.523]
Epoch [10/120    avg_loss:2.162, val_acc:0.548]
Epoch [11/120    avg_loss:2.107, val_acc:0.565]
Epoch [12/120    avg_loss:2.043, val_acc:0.590]
Epoch [13/120    avg_loss:1.985, val_acc:0.621]
Epoch [14/120    avg_loss:1.919, val_acc:0.615]
Epoch [15/120    avg_loss:1.843, val_acc:0.656]
Epoch [16/120    avg_loss:1.787, val_acc:0.673]
Epoch [17/120    avg_loss:1.725, val_acc:0.692]
Epoch [18/120    avg_loss:1.662, val_acc:0.692]
Epoch [19/120    avg_loss:1.615, val_acc:0.704]
Epoch [20/120    avg_loss:1.548, val_acc:0.733]
Epoch [21/120    avg_loss:1.474, val_acc:0.746]
Epoch [22/120    avg_loss:1.402, val_acc:0.760]
Epoch [23/120    avg_loss:1.323, val_acc:0.767]
Epoch [24/120    avg_loss:1.276, val_acc:0.810]
Epoch [25/120    avg_loss:1.227, val_acc:0.867]
Epoch [26/120    avg_loss:1.165, val_acc:0.769]
Epoch [27/120    avg_loss:1.105, val_acc:0.808]
Epoch [28/120    avg_loss:1.057, val_acc:0.883]
Epoch [29/120    avg_loss:0.993, val_acc:0.885]
Epoch [30/120    avg_loss:0.918, val_acc:0.881]
Epoch [31/120    avg_loss:0.882, val_acc:0.900]
Epoch [32/120    avg_loss:0.849, val_acc:0.906]
Epoch [33/120    avg_loss:0.767, val_acc:0.915]
Epoch [34/120    avg_loss:0.731, val_acc:0.931]
Epoch [35/120    avg_loss:0.806, val_acc:0.877]
Epoch [36/120    avg_loss:0.783, val_acc:0.912]
Epoch [37/120    avg_loss:0.732, val_acc:0.906]
Epoch [38/120    avg_loss:0.666, val_acc:0.944]
Epoch [39/120    avg_loss:0.624, val_acc:0.915]
Epoch [40/120    avg_loss:0.571, val_acc:0.929]
Epoch [41/120    avg_loss:0.532, val_acc:0.946]
Epoch [42/120    avg_loss:0.560, val_acc:0.815]
Epoch [43/120    avg_loss:0.549, val_acc:0.921]
Epoch [44/120    avg_loss:0.551, val_acc:0.948]
Epoch [45/120    avg_loss:0.504, val_acc:0.948]
Epoch [46/120    avg_loss:0.443, val_acc:0.929]
Epoch [47/120    avg_loss:0.417, val_acc:0.856]
Epoch [48/120    avg_loss:0.394, val_acc:0.935]
Epoch [49/120    avg_loss:0.370, val_acc:0.956]
Epoch [50/120    avg_loss:0.344, val_acc:0.950]
Epoch [51/120    avg_loss:0.379, val_acc:0.946]
Epoch [52/120    avg_loss:0.352, val_acc:0.950]
Epoch [53/120    avg_loss:0.315, val_acc:0.954]
Epoch [54/120    avg_loss:0.303, val_acc:0.956]
Epoch [55/120    avg_loss:0.308, val_acc:0.965]
Epoch [56/120    avg_loss:0.275, val_acc:0.958]
Epoch [57/120    avg_loss:0.343, val_acc:0.971]
Epoch [58/120    avg_loss:0.311, val_acc:0.952]
Epoch [59/120    avg_loss:0.295, val_acc:0.963]
Epoch [60/120    avg_loss:0.278, val_acc:0.969]
Epoch [61/120    avg_loss:0.236, val_acc:0.960]
Epoch [62/120    avg_loss:0.291, val_acc:0.954]
Epoch [63/120    avg_loss:0.271, val_acc:0.960]
Epoch [64/120    avg_loss:0.289, val_acc:0.956]
Epoch [65/120    avg_loss:0.260, val_acc:0.963]
Epoch [66/120    avg_loss:0.230, val_acc:0.952]
Epoch [67/120    avg_loss:0.218, val_acc:0.942]
Epoch [68/120    avg_loss:0.225, val_acc:0.948]
Epoch [69/120    avg_loss:0.237, val_acc:0.960]
Epoch [70/120    avg_loss:0.221, val_acc:0.950]
Epoch [71/120    avg_loss:0.176, val_acc:0.965]
Epoch [72/120    avg_loss:0.173, val_acc:0.977]
Epoch [73/120    avg_loss:0.158, val_acc:0.979]
Epoch [74/120    avg_loss:0.159, val_acc:0.979]
Epoch [75/120    avg_loss:0.140, val_acc:0.979]
Epoch [76/120    avg_loss:0.143, val_acc:0.977]
Epoch [77/120    avg_loss:0.153, val_acc:0.977]
Epoch [78/120    avg_loss:0.143, val_acc:0.975]
Epoch [79/120    avg_loss:0.139, val_acc:0.975]
Epoch [80/120    avg_loss:0.140, val_acc:0.977]
Epoch [81/120    avg_loss:0.140, val_acc:0.975]
Epoch [82/120    avg_loss:0.129, val_acc:0.979]
Epoch [83/120    avg_loss:0.151, val_acc:0.983]
Epoch [84/120    avg_loss:0.139, val_acc:0.977]
Epoch [85/120    avg_loss:0.128, val_acc:0.979]
Epoch [86/120    avg_loss:0.140, val_acc:0.979]
Epoch [87/120    avg_loss:0.135, val_acc:0.977]
Epoch [88/120    avg_loss:0.115, val_acc:0.985]
Epoch [89/120    avg_loss:0.129, val_acc:0.983]
Epoch [90/120    avg_loss:0.132, val_acc:0.983]
Epoch [91/120    avg_loss:0.128, val_acc:0.979]
Epoch [92/120    avg_loss:0.130, val_acc:0.981]
Epoch [93/120    avg_loss:0.118, val_acc:0.981]
Epoch [94/120    avg_loss:0.141, val_acc:0.981]
Epoch [95/120    avg_loss:0.114, val_acc:0.983]
Epoch [96/120    avg_loss:0.133, val_acc:0.983]
Epoch [97/120    avg_loss:0.123, val_acc:0.981]
Epoch [98/120    avg_loss:0.129, val_acc:0.983]
Epoch [99/120    avg_loss:0.126, val_acc:0.983]
Epoch [100/120    avg_loss:0.124, val_acc:0.981]
Epoch [101/120    avg_loss:0.127, val_acc:0.983]
Epoch [102/120    avg_loss:0.126, val_acc:0.981]
Epoch [103/120    avg_loss:0.130, val_acc:0.981]
Epoch [104/120    avg_loss:0.119, val_acc:0.981]
Epoch [105/120    avg_loss:0.106, val_acc:0.981]
Epoch [106/120    avg_loss:0.126, val_acc:0.981]
Epoch [107/120    avg_loss:0.125, val_acc:0.981]
Epoch [108/120    avg_loss:0.113, val_acc:0.981]
Epoch [109/120    avg_loss:0.115, val_acc:0.981]
Epoch [110/120    avg_loss:0.114, val_acc:0.981]
Epoch [111/120    avg_loss:0.117, val_acc:0.981]
Epoch [112/120    avg_loss:0.115, val_acc:0.981]
Epoch [113/120    avg_loss:0.126, val_acc:0.983]
Epoch [114/120    avg_loss:0.119, val_acc:0.983]
Epoch [115/120    avg_loss:0.117, val_acc:0.983]
Epoch [116/120    avg_loss:0.122, val_acc:0.983]
Epoch [117/120    avg_loss:0.124, val_acc:0.983]
Epoch [118/120    avg_loss:0.114, val_acc:0.983]
Epoch [119/120    avg_loss:0.127, val_acc:0.983]
Epoch [120/120    avg_loss:0.114, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.97333333 0.98901099 0.92694064 0.91318328
 1.         0.93785311 0.99870968 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9893180805342326
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd382678b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.677, val_acc:0.167]
Epoch [2/120    avg_loss:2.603, val_acc:0.350]
Epoch [3/120    avg_loss:2.527, val_acc:0.381]
Epoch [4/120    avg_loss:2.461, val_acc:0.442]
Epoch [5/120    avg_loss:2.389, val_acc:0.454]
Epoch [6/120    avg_loss:2.328, val_acc:0.469]
Epoch [7/120    avg_loss:2.263, val_acc:0.498]
Epoch [8/120    avg_loss:2.204, val_acc:0.533]
Epoch [9/120    avg_loss:2.143, val_acc:0.562]
Epoch [10/120    avg_loss:2.072, val_acc:0.579]
Epoch [11/120    avg_loss:2.011, val_acc:0.596]
Epoch [12/120    avg_loss:1.947, val_acc:0.610]
Epoch [13/120    avg_loss:1.859, val_acc:0.637]
Epoch [14/120    avg_loss:1.810, val_acc:0.669]
Epoch [15/120    avg_loss:1.715, val_acc:0.669]
Epoch [16/120    avg_loss:1.668, val_acc:0.719]
Epoch [17/120    avg_loss:1.577, val_acc:0.721]
Epoch [18/120    avg_loss:1.512, val_acc:0.729]
Epoch [19/120    avg_loss:1.456, val_acc:0.775]
Epoch [20/120    avg_loss:1.402, val_acc:0.758]
Epoch [21/120    avg_loss:1.355, val_acc:0.777]
Epoch [22/120    avg_loss:1.291, val_acc:0.744]
Epoch [23/120    avg_loss:1.189, val_acc:0.754]
Epoch [24/120    avg_loss:1.142, val_acc:0.762]
Epoch [25/120    avg_loss:1.064, val_acc:0.792]
Epoch [26/120    avg_loss:1.008, val_acc:0.831]
Epoch [27/120    avg_loss:0.970, val_acc:0.796]
Epoch [28/120    avg_loss:0.947, val_acc:0.810]
Epoch [29/120    avg_loss:0.889, val_acc:0.794]
Epoch [30/120    avg_loss:0.834, val_acc:0.898]
Epoch [31/120    avg_loss:0.814, val_acc:0.848]
Epoch [32/120    avg_loss:0.760, val_acc:0.894]
Epoch [33/120    avg_loss:0.708, val_acc:0.900]
Epoch [34/120    avg_loss:0.647, val_acc:0.812]
Epoch [35/120    avg_loss:0.715, val_acc:0.875]
Epoch [36/120    avg_loss:0.604, val_acc:0.904]
Epoch [37/120    avg_loss:0.600, val_acc:0.871]
Epoch [38/120    avg_loss:0.617, val_acc:0.902]
Epoch [39/120    avg_loss:0.551, val_acc:0.915]
Epoch [40/120    avg_loss:0.537, val_acc:0.921]
Epoch [41/120    avg_loss:0.530, val_acc:0.898]
Epoch [42/120    avg_loss:0.479, val_acc:0.915]
Epoch [43/120    avg_loss:0.433, val_acc:0.915]
Epoch [44/120    avg_loss:0.417, val_acc:0.931]
Epoch [45/120    avg_loss:0.445, val_acc:0.927]
Epoch [46/120    avg_loss:0.431, val_acc:0.929]
Epoch [47/120    avg_loss:0.430, val_acc:0.950]
Epoch [48/120    avg_loss:0.391, val_acc:0.940]
Epoch [49/120    avg_loss:0.464, val_acc:0.906]
Epoch [50/120    avg_loss:0.410, val_acc:0.946]
Epoch [51/120    avg_loss:0.372, val_acc:0.938]
Epoch [52/120    avg_loss:0.331, val_acc:0.933]
Epoch [53/120    avg_loss:0.339, val_acc:0.935]
Epoch [54/120    avg_loss:0.323, val_acc:0.925]
Epoch [55/120    avg_loss:0.294, val_acc:0.946]
Epoch [56/120    avg_loss:0.350, val_acc:0.933]
Epoch [57/120    avg_loss:0.324, val_acc:0.929]
Epoch [58/120    avg_loss:0.355, val_acc:0.938]
Epoch [59/120    avg_loss:0.286, val_acc:0.933]
Epoch [60/120    avg_loss:0.301, val_acc:0.931]
Epoch [61/120    avg_loss:0.249, val_acc:0.948]
Epoch [62/120    avg_loss:0.239, val_acc:0.954]
Epoch [63/120    avg_loss:0.218, val_acc:0.960]
Epoch [64/120    avg_loss:0.200, val_acc:0.960]
Epoch [65/120    avg_loss:0.215, val_acc:0.967]
Epoch [66/120    avg_loss:0.204, val_acc:0.963]
Epoch [67/120    avg_loss:0.216, val_acc:0.965]
Epoch [68/120    avg_loss:0.206, val_acc:0.967]
Epoch [69/120    avg_loss:0.210, val_acc:0.967]
Epoch [70/120    avg_loss:0.207, val_acc:0.973]
Epoch [71/120    avg_loss:0.196, val_acc:0.967]
Epoch [72/120    avg_loss:0.201, val_acc:0.971]
Epoch [73/120    avg_loss:0.204, val_acc:0.969]
Epoch [74/120    avg_loss:0.201, val_acc:0.973]
Epoch [75/120    avg_loss:0.184, val_acc:0.973]
Epoch [76/120    avg_loss:0.204, val_acc:0.971]
Epoch [77/120    avg_loss:0.189, val_acc:0.969]
Epoch [78/120    avg_loss:0.201, val_acc:0.965]
Epoch [79/120    avg_loss:0.180, val_acc:0.971]
Epoch [80/120    avg_loss:0.185, val_acc:0.969]
Epoch [81/120    avg_loss:0.193, val_acc:0.969]
Epoch [82/120    avg_loss:0.171, val_acc:0.973]
Epoch [83/120    avg_loss:0.185, val_acc:0.973]
Epoch [84/120    avg_loss:0.189, val_acc:0.973]
Epoch [85/120    avg_loss:0.175, val_acc:0.975]
Epoch [86/120    avg_loss:0.180, val_acc:0.973]
Epoch [87/120    avg_loss:0.158, val_acc:0.971]
Epoch [88/120    avg_loss:0.185, val_acc:0.971]
Epoch [89/120    avg_loss:0.166, val_acc:0.971]
Epoch [90/120    avg_loss:0.172, val_acc:0.969]
Epoch [91/120    avg_loss:0.163, val_acc:0.973]
Epoch [92/120    avg_loss:0.180, val_acc:0.971]
Epoch [93/120    avg_loss:0.177, val_acc:0.969]
Epoch [94/120    avg_loss:0.172, val_acc:0.969]
Epoch [95/120    avg_loss:0.165, val_acc:0.973]
Epoch [96/120    avg_loss:0.179, val_acc:0.971]
Epoch [97/120    avg_loss:0.164, val_acc:0.969]
Epoch [98/120    avg_loss:0.161, val_acc:0.971]
Epoch [99/120    avg_loss:0.165, val_acc:0.971]
Epoch [100/120    avg_loss:0.176, val_acc:0.971]
Epoch [101/120    avg_loss:0.158, val_acc:0.969]
Epoch [102/120    avg_loss:0.168, val_acc:0.969]
Epoch [103/120    avg_loss:0.175, val_acc:0.969]
Epoch [104/120    avg_loss:0.165, val_acc:0.969]
Epoch [105/120    avg_loss:0.176, val_acc:0.969]
Epoch [106/120    avg_loss:0.161, val_acc:0.969]
Epoch [107/120    avg_loss:0.147, val_acc:0.971]
Epoch [108/120    avg_loss:0.158, val_acc:0.975]
Epoch [109/120    avg_loss:0.156, val_acc:0.975]
Epoch [110/120    avg_loss:0.161, val_acc:0.973]
Epoch [111/120    avg_loss:0.156, val_acc:0.973]
Epoch [112/120    avg_loss:0.156, val_acc:0.971]
Epoch [113/120    avg_loss:0.160, val_acc:0.969]
Epoch [114/120    avg_loss:0.150, val_acc:0.971]
Epoch [115/120    avg_loss:0.169, val_acc:0.971]
Epoch [116/120    avg_loss:0.163, val_acc:0.969]
Epoch [117/120    avg_loss:0.154, val_acc:0.971]
Epoch [118/120    avg_loss:0.162, val_acc:0.967]
Epoch [119/120    avg_loss:0.145, val_acc:0.967]
Epoch [120/120    avg_loss:0.157, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   4 218   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 182  45   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   1   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.03837953091684

F1 scores:
[       nan 1.         0.95633188 0.97321429 0.86052009 0.84848485
 1.         0.90697674 0.99870968 1.         1.         0.98305085
 0.98544233 1.        ]

Kappa:
0.9781636659251416
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3d8852ab00>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.209]
Epoch [2/120    avg_loss:2.586, val_acc:0.315]
Epoch [3/120    avg_loss:2.510, val_acc:0.327]
Epoch [4/120    avg_loss:2.453, val_acc:0.315]
Epoch [5/120    avg_loss:2.404, val_acc:0.329]
Epoch [6/120    avg_loss:2.349, val_acc:0.335]
Epoch [7/120    avg_loss:2.315, val_acc:0.348]
Epoch [8/120    avg_loss:2.264, val_acc:0.390]
Epoch [9/120    avg_loss:2.243, val_acc:0.421]
Epoch [10/120    avg_loss:2.195, val_acc:0.473]
Epoch [11/120    avg_loss:2.175, val_acc:0.523]
Epoch [12/120    avg_loss:2.131, val_acc:0.581]
Epoch [13/120    avg_loss:2.081, val_acc:0.600]
Epoch [14/120    avg_loss:2.039, val_acc:0.613]
Epoch [15/120    avg_loss:1.979, val_acc:0.617]
Epoch [16/120    avg_loss:1.920, val_acc:0.631]
Epoch [17/120    avg_loss:1.856, val_acc:0.640]
Epoch [18/120    avg_loss:1.785, val_acc:0.646]
Epoch [19/120    avg_loss:1.718, val_acc:0.658]
Epoch [20/120    avg_loss:1.680, val_acc:0.667]
Epoch [21/120    avg_loss:1.632, val_acc:0.679]
Epoch [22/120    avg_loss:1.532, val_acc:0.708]
Epoch [23/120    avg_loss:1.507, val_acc:0.702]
Epoch [24/120    avg_loss:1.440, val_acc:0.694]
Epoch [25/120    avg_loss:1.349, val_acc:0.725]
Epoch [26/120    avg_loss:1.305, val_acc:0.729]
Epoch [27/120    avg_loss:1.200, val_acc:0.758]
Epoch [28/120    avg_loss:1.150, val_acc:0.769]
Epoch [29/120    avg_loss:1.087, val_acc:0.779]
Epoch [30/120    avg_loss:1.070, val_acc:0.771]
Epoch [31/120    avg_loss:1.001, val_acc:0.785]
Epoch [32/120    avg_loss:0.986, val_acc:0.754]
Epoch [33/120    avg_loss:0.896, val_acc:0.806]
Epoch [34/120    avg_loss:0.880, val_acc:0.781]
Epoch [35/120    avg_loss:0.810, val_acc:0.796]
Epoch [36/120    avg_loss:0.749, val_acc:0.867]
Epoch [37/120    avg_loss:0.733, val_acc:0.808]
Epoch [38/120    avg_loss:0.683, val_acc:0.856]
Epoch [39/120    avg_loss:0.653, val_acc:0.873]
Epoch [40/120    avg_loss:0.634, val_acc:0.902]
Epoch [41/120    avg_loss:0.581, val_acc:0.900]
Epoch [42/120    avg_loss:0.535, val_acc:0.912]
Epoch [43/120    avg_loss:0.501, val_acc:0.912]
Epoch [44/120    avg_loss:0.544, val_acc:0.869]
Epoch [45/120    avg_loss:0.528, val_acc:0.881]
Epoch [46/120    avg_loss:0.513, val_acc:0.904]
Epoch [47/120    avg_loss:0.523, val_acc:0.906]
Epoch [48/120    avg_loss:0.544, val_acc:0.819]
Epoch [49/120    avg_loss:0.514, val_acc:0.908]
Epoch [50/120    avg_loss:0.461, val_acc:0.885]
Epoch [51/120    avg_loss:0.452, val_acc:0.846]
Epoch [52/120    avg_loss:0.462, val_acc:0.881]
Epoch [53/120    avg_loss:0.430, val_acc:0.935]
Epoch [54/120    avg_loss:0.398, val_acc:0.925]
Epoch [55/120    avg_loss:0.370, val_acc:0.940]
Epoch [56/120    avg_loss:0.371, val_acc:0.929]
Epoch [57/120    avg_loss:0.399, val_acc:0.940]
Epoch [58/120    avg_loss:0.351, val_acc:0.940]
Epoch [59/120    avg_loss:0.353, val_acc:0.910]
Epoch [60/120    avg_loss:0.423, val_acc:0.896]
Epoch [61/120    avg_loss:0.417, val_acc:0.894]
Epoch [62/120    avg_loss:0.351, val_acc:0.929]
Epoch [63/120    avg_loss:0.327, val_acc:0.948]
Epoch [64/120    avg_loss:0.293, val_acc:0.948]
Epoch [65/120    avg_loss:0.311, val_acc:0.946]
Epoch [66/120    avg_loss:0.264, val_acc:0.956]
Epoch [67/120    avg_loss:0.283, val_acc:0.950]
Epoch [68/120    avg_loss:0.255, val_acc:0.963]
Epoch [69/120    avg_loss:0.306, val_acc:0.940]
Epoch [70/120    avg_loss:0.335, val_acc:0.952]
Epoch [71/120    avg_loss:0.265, val_acc:0.944]
Epoch [72/120    avg_loss:0.240, val_acc:0.963]
Epoch [73/120    avg_loss:0.235, val_acc:0.929]
Epoch [74/120    avg_loss:0.282, val_acc:0.923]
Epoch [75/120    avg_loss:0.223, val_acc:0.963]
Epoch [76/120    avg_loss:0.229, val_acc:0.958]
Epoch [77/120    avg_loss:0.277, val_acc:0.923]
Epoch [78/120    avg_loss:0.228, val_acc:0.965]
Epoch [79/120    avg_loss:0.202, val_acc:0.969]
Epoch [80/120    avg_loss:0.212, val_acc:0.956]
Epoch [81/120    avg_loss:0.179, val_acc:0.952]
Epoch [82/120    avg_loss:0.205, val_acc:0.963]
Epoch [83/120    avg_loss:0.190, val_acc:0.950]
Epoch [84/120    avg_loss:0.188, val_acc:0.965]
Epoch [85/120    avg_loss:0.167, val_acc:0.971]
Epoch [86/120    avg_loss:0.172, val_acc:0.977]
Epoch [87/120    avg_loss:0.171, val_acc:0.956]
Epoch [88/120    avg_loss:0.158, val_acc:0.960]
Epoch [89/120    avg_loss:0.174, val_acc:0.969]
Epoch [90/120    avg_loss:0.153, val_acc:0.971]
Epoch [91/120    avg_loss:0.140, val_acc:0.977]
Epoch [92/120    avg_loss:0.148, val_acc:0.975]
Epoch [93/120    avg_loss:0.155, val_acc:0.973]
Epoch [94/120    avg_loss:0.155, val_acc:0.969]
Epoch [95/120    avg_loss:0.138, val_acc:0.977]
Epoch [96/120    avg_loss:0.153, val_acc:0.973]
Epoch [97/120    avg_loss:0.126, val_acc:0.975]
Epoch [98/120    avg_loss:0.143, val_acc:0.958]
Epoch [99/120    avg_loss:0.139, val_acc:0.969]
Epoch [100/120    avg_loss:0.137, val_acc:0.960]
Epoch [101/120    avg_loss:0.161, val_acc:0.981]
Epoch [102/120    avg_loss:0.147, val_acc:0.979]
Epoch [103/120    avg_loss:0.141, val_acc:0.956]
Epoch [104/120    avg_loss:0.150, val_acc:0.977]
Epoch [105/120    avg_loss:0.141, val_acc:0.983]
Epoch [106/120    avg_loss:0.137, val_acc:0.969]
Epoch [107/120    avg_loss:0.132, val_acc:0.973]
Epoch [108/120    avg_loss:0.123, val_acc:0.956]
Epoch [109/120    avg_loss:0.101, val_acc:0.983]
Epoch [110/120    avg_loss:0.163, val_acc:0.969]
Epoch [111/120    avg_loss:0.148, val_acc:0.973]
Epoch [112/120    avg_loss:0.128, val_acc:0.940]
Epoch [113/120    avg_loss:0.191, val_acc:0.983]
Epoch [114/120    avg_loss:0.169, val_acc:0.979]
Epoch [115/120    avg_loss:0.119, val_acc:0.958]
Epoch [116/120    avg_loss:0.138, val_acc:0.973]
Epoch [117/120    avg_loss:0.099, val_acc:0.938]
Epoch [118/120    avg_loss:0.134, val_acc:0.971]
Epoch [119/120    avg_loss:0.146, val_acc:0.973]
Epoch [120/120    avg_loss:0.115, val_acc:0.956]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   1   0  29 423   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.272921108742

F1 scores:
[       nan 1.         0.96475771 0.99563319 0.91990847 0.88599349
 1.         0.93181818 0.99741602 0.99893276 1.         0.96296296
 0.96575342 1.        ]

Kappa:
0.9807744794157272
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f347e7ed9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.229]
Epoch [2/120    avg_loss:2.510, val_acc:0.321]
Epoch [3/120    avg_loss:2.454, val_acc:0.338]
Epoch [4/120    avg_loss:2.395, val_acc:0.362]
Epoch [5/120    avg_loss:2.342, val_acc:0.392]
Epoch [6/120    avg_loss:2.308, val_acc:0.400]
Epoch [7/120    avg_loss:2.248, val_acc:0.406]
Epoch [8/120    avg_loss:2.211, val_acc:0.427]
Epoch [9/120    avg_loss:2.157, val_acc:0.467]
Epoch [10/120    avg_loss:2.117, val_acc:0.485]
Epoch [11/120    avg_loss:2.045, val_acc:0.496]
Epoch [12/120    avg_loss:2.019, val_acc:0.515]
Epoch [13/120    avg_loss:1.950, val_acc:0.527]
Epoch [14/120    avg_loss:1.910, val_acc:0.550]
Epoch [15/120    avg_loss:1.844, val_acc:0.565]
Epoch [16/120    avg_loss:1.799, val_acc:0.562]
Epoch [17/120    avg_loss:1.762, val_acc:0.554]
Epoch [18/120    avg_loss:1.730, val_acc:0.613]
Epoch [19/120    avg_loss:1.645, val_acc:0.633]
Epoch [20/120    avg_loss:1.580, val_acc:0.646]
Epoch [21/120    avg_loss:1.526, val_acc:0.654]
Epoch [22/120    avg_loss:1.478, val_acc:0.650]
Epoch [23/120    avg_loss:1.411, val_acc:0.665]
Epoch [24/120    avg_loss:1.362, val_acc:0.667]
Epoch [25/120    avg_loss:1.309, val_acc:0.675]
Epoch [26/120    avg_loss:1.209, val_acc:0.694]
Epoch [27/120    avg_loss:1.178, val_acc:0.690]
Epoch [28/120    avg_loss:1.137, val_acc:0.723]
Epoch [29/120    avg_loss:1.094, val_acc:0.729]
Epoch [30/120    avg_loss:1.030, val_acc:0.742]
Epoch [31/120    avg_loss:0.975, val_acc:0.754]
Epoch [32/120    avg_loss:0.919, val_acc:0.754]
Epoch [33/120    avg_loss:0.890, val_acc:0.827]
Epoch [34/120    avg_loss:0.809, val_acc:0.777]
Epoch [35/120    avg_loss:0.767, val_acc:0.869]
Epoch [36/120    avg_loss:0.734, val_acc:0.831]
Epoch [37/120    avg_loss:0.718, val_acc:0.879]
Epoch [38/120    avg_loss:0.649, val_acc:0.885]
Epoch [39/120    avg_loss:0.679, val_acc:0.902]
Epoch [40/120    avg_loss:0.681, val_acc:0.900]
Epoch [41/120    avg_loss:0.581, val_acc:0.883]
Epoch [42/120    avg_loss:0.576, val_acc:0.885]
Epoch [43/120    avg_loss:0.530, val_acc:0.885]
Epoch [44/120    avg_loss:0.510, val_acc:0.921]
Epoch [45/120    avg_loss:0.485, val_acc:0.919]
Epoch [46/120    avg_loss:0.457, val_acc:0.925]
Epoch [47/120    avg_loss:0.478, val_acc:0.917]
Epoch [48/120    avg_loss:0.439, val_acc:0.927]
Epoch [49/120    avg_loss:0.430, val_acc:0.906]
Epoch [50/120    avg_loss:0.412, val_acc:0.927]
Epoch [51/120    avg_loss:0.388, val_acc:0.929]
Epoch [52/120    avg_loss:0.376, val_acc:0.931]
Epoch [53/120    avg_loss:0.362, val_acc:0.912]
Epoch [54/120    avg_loss:0.363, val_acc:0.938]
Epoch [55/120    avg_loss:0.346, val_acc:0.881]
Epoch [56/120    avg_loss:0.369, val_acc:0.921]
Epoch [57/120    avg_loss:0.394, val_acc:0.929]
Epoch [58/120    avg_loss:0.332, val_acc:0.933]
Epoch [59/120    avg_loss:0.339, val_acc:0.921]
Epoch [60/120    avg_loss:0.394, val_acc:0.919]
Epoch [61/120    avg_loss:0.312, val_acc:0.929]
Epoch [62/120    avg_loss:0.306, val_acc:0.933]
Epoch [63/120    avg_loss:0.297, val_acc:0.944]
Epoch [64/120    avg_loss:0.284, val_acc:0.942]
Epoch [65/120    avg_loss:0.285, val_acc:0.950]
Epoch [66/120    avg_loss:0.274, val_acc:0.944]
Epoch [67/120    avg_loss:0.277, val_acc:0.965]
Epoch [68/120    avg_loss:0.267, val_acc:0.942]
Epoch [69/120    avg_loss:0.256, val_acc:0.950]
Epoch [70/120    avg_loss:0.253, val_acc:0.958]
Epoch [71/120    avg_loss:0.255, val_acc:0.938]
Epoch [72/120    avg_loss:0.236, val_acc:0.954]
Epoch [73/120    avg_loss:0.237, val_acc:0.950]
Epoch [74/120    avg_loss:0.244, val_acc:0.935]
Epoch [75/120    avg_loss:0.218, val_acc:0.965]
Epoch [76/120    avg_loss:0.226, val_acc:0.956]
Epoch [77/120    avg_loss:0.190, val_acc:0.956]
Epoch [78/120    avg_loss:0.166, val_acc:0.965]
Epoch [79/120    avg_loss:0.201, val_acc:0.971]
Epoch [80/120    avg_loss:0.219, val_acc:0.956]
Epoch [81/120    avg_loss:0.177, val_acc:0.965]
Epoch [82/120    avg_loss:0.181, val_acc:0.969]
Epoch [83/120    avg_loss:0.140, val_acc:0.975]
Epoch [84/120    avg_loss:0.225, val_acc:0.958]
Epoch [85/120    avg_loss:0.225, val_acc:0.950]
Epoch [86/120    avg_loss:0.235, val_acc:0.935]
Epoch [87/120    avg_loss:0.213, val_acc:0.971]
Epoch [88/120    avg_loss:0.203, val_acc:0.965]
Epoch [89/120    avg_loss:0.236, val_acc:0.942]
Epoch [90/120    avg_loss:0.264, val_acc:0.958]
Epoch [91/120    avg_loss:0.218, val_acc:0.963]
Epoch [92/120    avg_loss:0.184, val_acc:0.952]
Epoch [93/120    avg_loss:0.180, val_acc:0.965]
Epoch [94/120    avg_loss:0.164, val_acc:0.971]
Epoch [95/120    avg_loss:0.154, val_acc:0.975]
Epoch [96/120    avg_loss:0.113, val_acc:0.971]
Epoch [97/120    avg_loss:0.145, val_acc:0.981]
Epoch [98/120    avg_loss:0.103, val_acc:0.981]
Epoch [99/120    avg_loss:0.110, val_acc:0.977]
Epoch [100/120    avg_loss:0.086, val_acc:0.981]
Epoch [101/120    avg_loss:0.123, val_acc:0.971]
Epoch [102/120    avg_loss:0.167, val_acc:0.956]
Epoch [103/120    avg_loss:0.156, val_acc:0.975]
Epoch [104/120    avg_loss:0.119, val_acc:0.979]
Epoch [105/120    avg_loss:0.105, val_acc:0.975]
Epoch [106/120    avg_loss:0.094, val_acc:0.985]
Epoch [107/120    avg_loss:0.108, val_acc:0.983]
Epoch [108/120    avg_loss:0.099, val_acc:0.983]
Epoch [109/120    avg_loss:0.111, val_acc:0.977]
Epoch [110/120    avg_loss:0.104, val_acc:0.977]
Epoch [111/120    avg_loss:0.124, val_acc:0.981]
Epoch [112/120    avg_loss:0.096, val_acc:0.973]
Epoch [113/120    avg_loss:0.089, val_acc:0.985]
Epoch [114/120    avg_loss:0.082, val_acc:0.985]
Epoch [115/120    avg_loss:0.082, val_acc:0.981]
Epoch [116/120    avg_loss:0.083, val_acc:0.975]
Epoch [117/120    avg_loss:0.107, val_acc:0.979]
Epoch [118/120    avg_loss:0.068, val_acc:0.981]
Epoch [119/120    avg_loss:0.074, val_acc:0.973]
Epoch [120/120    avg_loss:0.130, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 213  17   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  32 113   0   0   0   0   0   0   0   0]
 [  0   9   0   0   0   0 197   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.18763326226012

F1 scores:
[       nan 0.99347353 0.98871332 0.96162528 0.85239085 0.80714286
 0.97766749 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9798167483270279
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe001ad7b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.637, val_acc:0.081]
Epoch [2/120    avg_loss:2.571, val_acc:0.312]
Epoch [3/120    avg_loss:2.517, val_acc:0.360]
Epoch [4/120    avg_loss:2.461, val_acc:0.429]
Epoch [5/120    avg_loss:2.401, val_acc:0.456]
Epoch [6/120    avg_loss:2.344, val_acc:0.492]
Epoch [7/120    avg_loss:2.288, val_acc:0.525]
Epoch [8/120    avg_loss:2.229, val_acc:0.552]
Epoch [9/120    avg_loss:2.169, val_acc:0.562]
Epoch [10/120    avg_loss:2.110, val_acc:0.575]
Epoch [11/120    avg_loss:2.064, val_acc:0.585]
Epoch [12/120    avg_loss:1.989, val_acc:0.596]
Epoch [13/120    avg_loss:1.933, val_acc:0.621]
Epoch [14/120    avg_loss:1.868, val_acc:0.631]
Epoch [15/120    avg_loss:1.809, val_acc:0.635]
Epoch [16/120    avg_loss:1.729, val_acc:0.635]
Epoch [17/120    avg_loss:1.658, val_acc:0.656]
Epoch [18/120    avg_loss:1.620, val_acc:0.667]
Epoch [19/120    avg_loss:1.535, val_acc:0.662]
Epoch [20/120    avg_loss:1.476, val_acc:0.667]
Epoch [21/120    avg_loss:1.396, val_acc:0.679]
Epoch [22/120    avg_loss:1.328, val_acc:0.685]
Epoch [23/120    avg_loss:1.270, val_acc:0.692]
Epoch [24/120    avg_loss:1.172, val_acc:0.702]
Epoch [25/120    avg_loss:1.179, val_acc:0.725]
Epoch [26/120    avg_loss:1.094, val_acc:0.725]
Epoch [27/120    avg_loss:0.999, val_acc:0.729]
Epoch [28/120    avg_loss:0.962, val_acc:0.733]
Epoch [29/120    avg_loss:0.914, val_acc:0.769]
Epoch [30/120    avg_loss:0.833, val_acc:0.771]
Epoch [31/120    avg_loss:0.825, val_acc:0.771]
Epoch [32/120    avg_loss:0.814, val_acc:0.810]
Epoch [33/120    avg_loss:0.743, val_acc:0.827]
Epoch [34/120    avg_loss:0.699, val_acc:0.877]
Epoch [35/120    avg_loss:0.671, val_acc:0.885]
Epoch [36/120    avg_loss:0.676, val_acc:0.833]
Epoch [37/120    avg_loss:0.594, val_acc:0.927]
Epoch [38/120    avg_loss:0.570, val_acc:0.921]
Epoch [39/120    avg_loss:0.535, val_acc:0.917]
Epoch [40/120    avg_loss:0.518, val_acc:0.929]
Epoch [41/120    avg_loss:0.511, val_acc:0.921]
Epoch [42/120    avg_loss:0.510, val_acc:0.927]
Epoch [43/120    avg_loss:0.474, val_acc:0.900]
Epoch [44/120    avg_loss:0.474, val_acc:0.933]
Epoch [45/120    avg_loss:0.492, val_acc:0.919]
Epoch [46/120    avg_loss:0.447, val_acc:0.917]
Epoch [47/120    avg_loss:0.420, val_acc:0.915]
Epoch [48/120    avg_loss:0.474, val_acc:0.879]
Epoch [49/120    avg_loss:0.411, val_acc:0.927]
Epoch [50/120    avg_loss:0.379, val_acc:0.946]
Epoch [51/120    avg_loss:0.359, val_acc:0.954]
Epoch [52/120    avg_loss:0.373, val_acc:0.950]
Epoch [53/120    avg_loss:0.371, val_acc:0.927]
Epoch [54/120    avg_loss:0.366, val_acc:0.925]
Epoch [55/120    avg_loss:0.341, val_acc:0.940]
Epoch [56/120    avg_loss:0.319, val_acc:0.954]
Epoch [57/120    avg_loss:0.275, val_acc:0.921]
Epoch [58/120    avg_loss:0.306, val_acc:0.958]
Epoch [59/120    avg_loss:0.306, val_acc:0.923]
Epoch [60/120    avg_loss:0.329, val_acc:0.952]
Epoch [61/120    avg_loss:0.290, val_acc:0.950]
Epoch [62/120    avg_loss:0.259, val_acc:0.954]
Epoch [63/120    avg_loss:0.249, val_acc:0.938]
Epoch [64/120    avg_loss:0.333, val_acc:0.938]
Epoch [65/120    avg_loss:0.298, val_acc:0.950]
Epoch [66/120    avg_loss:0.271, val_acc:0.942]
Epoch [67/120    avg_loss:0.285, val_acc:0.935]
Epoch [68/120    avg_loss:0.262, val_acc:0.971]
Epoch [69/120    avg_loss:0.291, val_acc:0.967]
Epoch [70/120    avg_loss:0.249, val_acc:0.960]
Epoch [71/120    avg_loss:0.205, val_acc:0.985]
Epoch [72/120    avg_loss:0.224, val_acc:0.975]
Epoch [73/120    avg_loss:0.195, val_acc:0.950]
Epoch [74/120    avg_loss:0.210, val_acc:0.979]
Epoch [75/120    avg_loss:0.160, val_acc:0.983]
Epoch [76/120    avg_loss:0.163, val_acc:0.963]
Epoch [77/120    avg_loss:0.148, val_acc:0.985]
Epoch [78/120    avg_loss:0.163, val_acc:0.973]
Epoch [79/120    avg_loss:0.168, val_acc:0.977]
Epoch [80/120    avg_loss:0.165, val_acc:0.975]
Epoch [81/120    avg_loss:0.155, val_acc:0.983]
Epoch [82/120    avg_loss:0.153, val_acc:0.977]
Epoch [83/120    avg_loss:0.141, val_acc:0.983]
Epoch [84/120    avg_loss:0.150, val_acc:0.935]
Epoch [85/120    avg_loss:0.163, val_acc:0.981]
Epoch [86/120    avg_loss:0.135, val_acc:0.983]
Epoch [87/120    avg_loss:0.121, val_acc:0.981]
Epoch [88/120    avg_loss:0.130, val_acc:0.981]
Epoch [89/120    avg_loss:0.107, val_acc:0.967]
Epoch [90/120    avg_loss:0.169, val_acc:0.963]
Epoch [91/120    avg_loss:0.132, val_acc:0.977]
Epoch [92/120    avg_loss:0.121, val_acc:0.979]
Epoch [93/120    avg_loss:0.108, val_acc:0.981]
Epoch [94/120    avg_loss:0.100, val_acc:0.985]
Epoch [95/120    avg_loss:0.099, val_acc:0.988]
Epoch [96/120    avg_loss:0.089, val_acc:0.988]
Epoch [97/120    avg_loss:0.096, val_acc:0.985]
Epoch [98/120    avg_loss:0.081, val_acc:0.985]
Epoch [99/120    avg_loss:0.090, val_acc:0.988]
Epoch [100/120    avg_loss:0.080, val_acc:0.985]
Epoch [101/120    avg_loss:0.090, val_acc:0.983]
Epoch [102/120    avg_loss:0.077, val_acc:0.983]
Epoch [103/120    avg_loss:0.075, val_acc:0.985]
Epoch [104/120    avg_loss:0.071, val_acc:0.985]
Epoch [105/120    avg_loss:0.079, val_acc:0.988]
Epoch [106/120    avg_loss:0.087, val_acc:0.988]
Epoch [107/120    avg_loss:0.078, val_acc:0.985]
Epoch [108/120    avg_loss:0.075, val_acc:0.985]
Epoch [109/120    avg_loss:0.065, val_acc:0.985]
Epoch [110/120    avg_loss:0.071, val_acc:0.988]
Epoch [111/120    avg_loss:0.079, val_acc:0.985]
Epoch [112/120    avg_loss:0.073, val_acc:0.985]
Epoch [113/120    avg_loss:0.077, val_acc:0.985]
Epoch [114/120    avg_loss:0.068, val_acc:0.985]
Epoch [115/120    avg_loss:0.081, val_acc:0.988]
Epoch [116/120    avg_loss:0.070, val_acc:0.988]
Epoch [117/120    avg_loss:0.071, val_acc:0.985]
Epoch [118/120    avg_loss:0.069, val_acc:0.985]
Epoch [119/120    avg_loss:0.077, val_acc:0.985]
Epoch [120/120    avg_loss:0.068, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.97986577 0.99782135 0.94199536 0.92356688
 1.         0.94972067 1.         1.         1.         0.99210526
 0.99333333 1.        ]

Kappa:
0.9905053675040231
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe993cd0a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.638, val_acc:0.160]
Epoch [2/120    avg_loss:2.578, val_acc:0.202]
Epoch [3/120    avg_loss:2.510, val_acc:0.263]
Epoch [4/120    avg_loss:2.454, val_acc:0.379]
Epoch [5/120    avg_loss:2.408, val_acc:0.379]
Epoch [6/120    avg_loss:2.357, val_acc:0.383]
Epoch [7/120    avg_loss:2.306, val_acc:0.404]
Epoch [8/120    avg_loss:2.241, val_acc:0.429]
Epoch [9/120    avg_loss:2.191, val_acc:0.458]
Epoch [10/120    avg_loss:2.124, val_acc:0.465]
Epoch [11/120    avg_loss:2.066, val_acc:0.492]
Epoch [12/120    avg_loss:2.005, val_acc:0.542]
Epoch [13/120    avg_loss:1.944, val_acc:0.569]
Epoch [14/120    avg_loss:1.879, val_acc:0.596]
Epoch [15/120    avg_loss:1.806, val_acc:0.596]
Epoch [16/120    avg_loss:1.721, val_acc:0.625]
Epoch [17/120    avg_loss:1.685, val_acc:0.642]
Epoch [18/120    avg_loss:1.589, val_acc:0.654]
Epoch [19/120    avg_loss:1.532, val_acc:0.669]
Epoch [20/120    avg_loss:1.475, val_acc:0.677]
Epoch [21/120    avg_loss:1.414, val_acc:0.696]
Epoch [22/120    avg_loss:1.365, val_acc:0.683]
Epoch [23/120    avg_loss:1.315, val_acc:0.685]
Epoch [24/120    avg_loss:1.239, val_acc:0.721]
Epoch [25/120    avg_loss:1.199, val_acc:0.706]
Epoch [26/120    avg_loss:1.123, val_acc:0.746]
Epoch [27/120    avg_loss:1.180, val_acc:0.719]
Epoch [28/120    avg_loss:1.108, val_acc:0.785]
Epoch [29/120    avg_loss:1.001, val_acc:0.827]
Epoch [30/120    avg_loss:0.948, val_acc:0.873]
Epoch [31/120    avg_loss:0.902, val_acc:0.829]
Epoch [32/120    avg_loss:0.866, val_acc:0.912]
Epoch [33/120    avg_loss:0.805, val_acc:0.925]
Epoch [34/120    avg_loss:0.751, val_acc:0.906]
Epoch [35/120    avg_loss:0.757, val_acc:0.921]
Epoch [36/120    avg_loss:0.682, val_acc:0.910]
Epoch [37/120    avg_loss:0.640, val_acc:0.919]
Epoch [38/120    avg_loss:0.626, val_acc:0.938]
Epoch [39/120    avg_loss:0.567, val_acc:0.944]
Epoch [40/120    avg_loss:0.531, val_acc:0.927]
Epoch [41/120    avg_loss:0.542, val_acc:0.935]
Epoch [42/120    avg_loss:0.590, val_acc:0.923]
Epoch [43/120    avg_loss:0.509, val_acc:0.950]
Epoch [44/120    avg_loss:0.488, val_acc:0.931]
Epoch [45/120    avg_loss:0.481, val_acc:0.929]
Epoch [46/120    avg_loss:0.453, val_acc:0.940]
Epoch [47/120    avg_loss:0.486, val_acc:0.954]
Epoch [48/120    avg_loss:0.406, val_acc:0.946]
Epoch [49/120    avg_loss:0.382, val_acc:0.938]
Epoch [50/120    avg_loss:0.418, val_acc:0.954]
Epoch [51/120    avg_loss:0.353, val_acc:0.954]
Epoch [52/120    avg_loss:0.347, val_acc:0.956]
Epoch [53/120    avg_loss:0.380, val_acc:0.938]
Epoch [54/120    avg_loss:0.379, val_acc:0.950]
Epoch [55/120    avg_loss:0.327, val_acc:0.944]
Epoch [56/120    avg_loss:0.335, val_acc:0.967]
Epoch [57/120    avg_loss:0.331, val_acc:0.960]
Epoch [58/120    avg_loss:0.355, val_acc:0.948]
Epoch [59/120    avg_loss:0.322, val_acc:0.963]
Epoch [60/120    avg_loss:0.295, val_acc:0.977]
Epoch [61/120    avg_loss:0.273, val_acc:0.975]
Epoch [62/120    avg_loss:0.277, val_acc:0.952]
Epoch [63/120    avg_loss:0.293, val_acc:0.967]
Epoch [64/120    avg_loss:0.275, val_acc:0.960]
Epoch [65/120    avg_loss:0.269, val_acc:0.960]
Epoch [66/120    avg_loss:0.258, val_acc:0.954]
Epoch [67/120    avg_loss:0.258, val_acc:0.971]
Epoch [68/120    avg_loss:0.237, val_acc:0.950]
Epoch [69/120    avg_loss:0.238, val_acc:0.971]
Epoch [70/120    avg_loss:0.224, val_acc:0.975]
Epoch [71/120    avg_loss:0.206, val_acc:0.973]
Epoch [72/120    avg_loss:0.207, val_acc:0.973]
Epoch [73/120    avg_loss:0.229, val_acc:0.977]
Epoch [74/120    avg_loss:0.244, val_acc:0.971]
Epoch [75/120    avg_loss:0.193, val_acc:0.981]
Epoch [76/120    avg_loss:0.172, val_acc:0.975]
Epoch [77/120    avg_loss:0.161, val_acc:0.960]
Epoch [78/120    avg_loss:0.181, val_acc:0.983]
Epoch [79/120    avg_loss:0.193, val_acc:0.985]
Epoch [80/120    avg_loss:0.171, val_acc:0.985]
Epoch [81/120    avg_loss:0.156, val_acc:0.981]
Epoch [82/120    avg_loss:0.158, val_acc:0.979]
Epoch [83/120    avg_loss:0.148, val_acc:0.988]
Epoch [84/120    avg_loss:0.205, val_acc:0.969]
Epoch [85/120    avg_loss:0.151, val_acc:0.975]
Epoch [86/120    avg_loss:0.135, val_acc:0.977]
Epoch [87/120    avg_loss:0.124, val_acc:0.958]
Epoch [88/120    avg_loss:0.131, val_acc:0.990]
Epoch [89/120    avg_loss:0.108, val_acc:0.977]
Epoch [90/120    avg_loss:0.118, val_acc:0.983]
Epoch [91/120    avg_loss:0.112, val_acc:0.977]
Epoch [92/120    avg_loss:0.099, val_acc:0.983]
Epoch [93/120    avg_loss:0.109, val_acc:0.990]
Epoch [94/120    avg_loss:0.113, val_acc:0.981]
Epoch [95/120    avg_loss:0.107, val_acc:0.992]
Epoch [96/120    avg_loss:0.117, val_acc:0.979]
Epoch [97/120    avg_loss:0.107, val_acc:0.990]
Epoch [98/120    avg_loss:0.109, val_acc:0.977]
Epoch [99/120    avg_loss:0.124, val_acc:0.967]
Epoch [100/120    avg_loss:0.182, val_acc:0.975]
Epoch [101/120    avg_loss:0.143, val_acc:0.977]
Epoch [102/120    avg_loss:0.153, val_acc:0.988]
Epoch [103/120    avg_loss:0.112, val_acc:0.988]
Epoch [104/120    avg_loss:0.169, val_acc:0.981]
Epoch [105/120    avg_loss:0.112, val_acc:0.985]
Epoch [106/120    avg_loss:0.095, val_acc:0.990]
Epoch [107/120    avg_loss:0.103, val_acc:0.975]
Epoch [108/120    avg_loss:0.102, val_acc:0.992]
Epoch [109/120    avg_loss:0.097, val_acc:0.985]
Epoch [110/120    avg_loss:0.091, val_acc:0.990]
Epoch [111/120    avg_loss:0.081, val_acc:0.990]
Epoch [112/120    avg_loss:0.065, val_acc:0.988]
Epoch [113/120    avg_loss:0.062, val_acc:0.988]
Epoch [114/120    avg_loss:0.064, val_acc:0.990]
Epoch [115/120    avg_loss:0.051, val_acc:0.990]
Epoch [116/120    avg_loss:0.052, val_acc:0.992]
Epoch [117/120    avg_loss:0.048, val_acc:0.998]
Epoch [118/120    avg_loss:0.062, val_acc:0.992]
Epoch [119/120    avg_loss:0.081, val_acc:0.988]
Epoch [120/120    avg_loss:0.090, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 217  13   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  36 109   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.98206278 0.97091723 0.88709677 0.83524904
 1.         0.95555556 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9848050189200701
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe519d179e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 31307==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.632, val_acc:0.168]
Epoch [2/120    avg_loss:2.568, val_acc:0.317]
Epoch [3/120    avg_loss:2.493, val_acc:0.342]
Epoch [4/120    avg_loss:2.424, val_acc:0.358]
Epoch [5/120    avg_loss:2.359, val_acc:0.348]
Epoch [6/120    avg_loss:2.309, val_acc:0.371]
Epoch [7/120    avg_loss:2.251, val_acc:0.375]
Epoch [8/120    avg_loss:2.190, val_acc:0.412]
Epoch [9/120    avg_loss:2.153, val_acc:0.440]
Epoch [10/120    avg_loss:2.097, val_acc:0.463]
Epoch [11/120    avg_loss:2.035, val_acc:0.492]
Epoch [12/120    avg_loss:1.967, val_acc:0.523]
Epoch [13/120    avg_loss:1.919, val_acc:0.556]
Epoch [14/120    avg_loss:1.838, val_acc:0.567]
Epoch [15/120    avg_loss:1.757, val_acc:0.585]
Epoch [16/120    avg_loss:1.702, val_acc:0.596]
Epoch [17/120    avg_loss:1.653, val_acc:0.602]
Epoch [18/120    avg_loss:1.566, val_acc:0.617]
Epoch [19/120    avg_loss:1.529, val_acc:0.617]
Epoch [20/120    avg_loss:1.442, val_acc:0.648]
Epoch [21/120    avg_loss:1.427, val_acc:0.706]
Epoch [22/120    avg_loss:1.366, val_acc:0.658]
Epoch [23/120    avg_loss:1.269, val_acc:0.677]
Epoch [24/120    avg_loss:1.186, val_acc:0.790]
Epoch [25/120    avg_loss:1.148, val_acc:0.825]
Epoch [26/120    avg_loss:1.088, val_acc:0.842]
Epoch [27/120    avg_loss:1.036, val_acc:0.854]
Epoch [28/120    avg_loss:0.967, val_acc:0.867]
Epoch [29/120    avg_loss:0.893, val_acc:0.863]
Epoch [30/120    avg_loss:0.868, val_acc:0.869]
Epoch [31/120    avg_loss:0.798, val_acc:0.850]
Epoch [32/120    avg_loss:0.761, val_acc:0.871]
Epoch [33/120    avg_loss:0.734, val_acc:0.877]
Epoch [34/120    avg_loss:0.713, val_acc:0.779]
Epoch [35/120    avg_loss:0.743, val_acc:0.898]
Epoch [36/120    avg_loss:0.663, val_acc:0.894]
Epoch [37/120    avg_loss:0.641, val_acc:0.896]
Epoch [38/120    avg_loss:0.634, val_acc:0.910]
Epoch [39/120    avg_loss:0.604, val_acc:0.879]
Epoch [40/120    avg_loss:0.591, val_acc:0.885]
Epoch [41/120    avg_loss:0.588, val_acc:0.944]
Epoch [42/120    avg_loss:0.527, val_acc:0.917]
Epoch [43/120    avg_loss:0.490, val_acc:0.935]
Epoch [44/120    avg_loss:0.466, val_acc:0.923]
Epoch [45/120    avg_loss:0.448, val_acc:0.946]
Epoch [46/120    avg_loss:0.445, val_acc:0.933]
Epoch [47/120    avg_loss:0.440, val_acc:0.944]
Epoch [48/120    avg_loss:0.403, val_acc:0.933]
Epoch [49/120    avg_loss:0.417, val_acc:0.933]
Epoch [50/120    avg_loss:0.394, val_acc:0.958]
Epoch [51/120    avg_loss:0.383, val_acc:0.946]
Epoch [52/120    avg_loss:0.403, val_acc:0.935]
Epoch [53/120    avg_loss:0.347, val_acc:0.910]
Epoch [54/120    avg_loss:0.358, val_acc:0.896]
Epoch [55/120    avg_loss:0.382, val_acc:0.948]
Epoch [56/120    avg_loss:0.374, val_acc:0.942]
Epoch [57/120    avg_loss:0.321, val_acc:0.946]
Epoch [58/120    avg_loss:0.319, val_acc:0.960]
Epoch [59/120    avg_loss:0.313, val_acc:0.921]
Epoch [60/120    avg_loss:0.323, val_acc:0.958]
Epoch [61/120    avg_loss:0.296, val_acc:0.971]
Epoch [62/120    avg_loss:0.268, val_acc:0.967]
Epoch [63/120    avg_loss:0.336, val_acc:0.900]
Epoch [64/120    avg_loss:0.320, val_acc:0.944]
Epoch [65/120    avg_loss:0.294, val_acc:0.969]
Epoch [66/120    avg_loss:0.266, val_acc:0.967]
Epoch [67/120    avg_loss:0.306, val_acc:0.952]
Epoch [68/120    avg_loss:0.304, val_acc:0.965]
Epoch [69/120    avg_loss:0.255, val_acc:0.956]
Epoch [70/120    avg_loss:0.213, val_acc:0.948]
Epoch [71/120    avg_loss:0.245, val_acc:0.954]
Epoch [72/120    avg_loss:0.239, val_acc:0.940]
Epoch [73/120    avg_loss:0.191, val_acc:0.956]
Epoch [74/120    avg_loss:0.249, val_acc:0.933]
Epoch [75/120    avg_loss:0.233, val_acc:0.960]
Epoch [76/120    avg_loss:0.180, val_acc:0.973]
Epoch [77/120    avg_loss:0.178, val_acc:0.975]
Epoch [78/120    avg_loss:0.192, val_acc:0.975]
Epoch [79/120    avg_loss:0.151, val_acc:0.975]
Epoch [80/120    avg_loss:0.169, val_acc:0.977]
Epoch [81/120    avg_loss:0.177, val_acc:0.975]
Epoch [82/120    avg_loss:0.149, val_acc:0.977]
Epoch [83/120    avg_loss:0.166, val_acc:0.975]
Epoch [84/120    avg_loss:0.151, val_acc:0.979]
Epoch [85/120    avg_loss:0.147, val_acc:0.977]
Epoch [86/120    avg_loss:0.165, val_acc:0.977]
Epoch [87/120    avg_loss:0.183, val_acc:0.973]
Epoch [88/120    avg_loss:0.173, val_acc:0.973]
Epoch [89/120    avg_loss:0.150, val_acc:0.981]
Epoch [90/120    avg_loss:0.146, val_acc:0.977]
Epoch [91/120    avg_loss:0.154, val_acc:0.975]
Epoch [92/120    avg_loss:0.161, val_acc:0.977]
Epoch [93/120    avg_loss:0.126, val_acc:0.975]
Epoch [94/120    avg_loss:0.138, val_acc:0.979]
Epoch [95/120    avg_loss:0.129, val_acc:0.979]
Epoch [96/120    avg_loss:0.160, val_acc:0.979]
Epoch [97/120    avg_loss:0.138, val_acc:0.979]
Epoch [98/120    avg_loss:0.143, val_acc:0.979]
Epoch [99/120    avg_loss:0.124, val_acc:0.979]
Epoch [100/120    avg_loss:0.130, val_acc:0.979]
Epoch [101/120    avg_loss:0.147, val_acc:0.981]
Epoch [102/120    avg_loss:0.131, val_acc:0.979]
Epoch [103/120    avg_loss:0.132, val_acc:0.977]
Epoch [104/120    avg_loss:0.132, val_acc:0.979]
Epoch [105/120    avg_loss:0.116, val_acc:0.979]
Epoch [106/120    avg_loss:0.145, val_acc:0.981]
Epoch [107/120    avg_loss:0.123, val_acc:0.975]
Epoch [108/120    avg_loss:0.132, val_acc:0.981]
Epoch [109/120    avg_loss:0.123, val_acc:0.979]
Epoch [110/120    avg_loss:0.128, val_acc:0.975]
Epoch [111/120    avg_loss:0.145, val_acc:0.979]
Epoch [112/120    avg_loss:0.127, val_acc:0.977]
Epoch [113/120    avg_loss:0.126, val_acc:0.975]
Epoch [114/120    avg_loss:0.134, val_acc:0.975]
Epoch [115/120    avg_loss:0.106, val_acc:0.977]
Epoch [116/120    avg_loss:0.126, val_acc:0.975]
Epoch [117/120    avg_loss:0.122, val_acc:0.979]
Epoch [118/120    avg_loss:0.124, val_acc:0.975]
Epoch [119/120    avg_loss:0.112, val_acc:0.977]
Epoch [120/120    avg_loss:0.138, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 222   5   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   9   0   0   2   0 195   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 0.99347353 0.95633188 0.98230088 0.91685393 0.8961039
 0.97256858 0.88757396 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9838537951327485
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc282301a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.279]
Epoch [2/120    avg_loss:2.570, val_acc:0.417]
Epoch [3/120    avg_loss:2.513, val_acc:0.438]
Epoch [4/120    avg_loss:2.460, val_acc:0.467]
Epoch [5/120    avg_loss:2.397, val_acc:0.465]
Epoch [6/120    avg_loss:2.351, val_acc:0.454]
Epoch [7/120    avg_loss:2.299, val_acc:0.469]
Epoch [8/120    avg_loss:2.245, val_acc:0.475]
Epoch [9/120    avg_loss:2.190, val_acc:0.485]
Epoch [10/120    avg_loss:2.138, val_acc:0.504]
Epoch [11/120    avg_loss:2.084, val_acc:0.531]
Epoch [12/120    avg_loss:2.029, val_acc:0.556]
Epoch [13/120    avg_loss:1.954, val_acc:0.579]
Epoch [14/120    avg_loss:1.911, val_acc:0.573]
Epoch [15/120    avg_loss:1.830, val_acc:0.592]
Epoch [16/120    avg_loss:1.767, val_acc:0.590]
Epoch [17/120    avg_loss:1.700, val_acc:0.604]
Epoch [18/120    avg_loss:1.634, val_acc:0.610]
Epoch [19/120    avg_loss:1.564, val_acc:0.621]
Epoch [20/120    avg_loss:1.485, val_acc:0.631]
Epoch [21/120    avg_loss:1.418, val_acc:0.658]
Epoch [22/120    avg_loss:1.347, val_acc:0.671]
Epoch [23/120    avg_loss:1.271, val_acc:0.694]
Epoch [24/120    avg_loss:1.200, val_acc:0.729]
Epoch [25/120    avg_loss:1.143, val_acc:0.731]
Epoch [26/120    avg_loss:1.167, val_acc:0.735]
Epoch [27/120    avg_loss:1.068, val_acc:0.756]
Epoch [28/120    avg_loss:0.982, val_acc:0.823]
Epoch [29/120    avg_loss:0.962, val_acc:0.852]
Epoch [30/120    avg_loss:0.945, val_acc:0.846]
Epoch [31/120    avg_loss:0.855, val_acc:0.890]
Epoch [32/120    avg_loss:0.801, val_acc:0.865]
Epoch [33/120    avg_loss:0.756, val_acc:0.896]
Epoch [34/120    avg_loss:0.709, val_acc:0.900]
Epoch [35/120    avg_loss:0.716, val_acc:0.792]
Epoch [36/120    avg_loss:0.711, val_acc:0.898]
Epoch [37/120    avg_loss:0.633, val_acc:0.887]
Epoch [38/120    avg_loss:0.583, val_acc:0.919]
Epoch [39/120    avg_loss:0.571, val_acc:0.902]
Epoch [40/120    avg_loss:0.577, val_acc:0.869]
Epoch [41/120    avg_loss:0.515, val_acc:0.921]
Epoch [42/120    avg_loss:0.471, val_acc:0.921]
Epoch [43/120    avg_loss:0.474, val_acc:0.948]
Epoch [44/120    avg_loss:0.451, val_acc:0.931]
Epoch [45/120    avg_loss:0.411, val_acc:0.940]
Epoch [46/120    avg_loss:0.432, val_acc:0.929]
Epoch [47/120    avg_loss:0.423, val_acc:0.906]
Epoch [48/120    avg_loss:0.403, val_acc:0.944]
Epoch [49/120    avg_loss:0.414, val_acc:0.933]
Epoch [50/120    avg_loss:0.366, val_acc:0.950]
Epoch [51/120    avg_loss:0.344, val_acc:0.948]
Epoch [52/120    avg_loss:0.323, val_acc:0.935]
Epoch [53/120    avg_loss:0.380, val_acc:0.935]
Epoch [54/120    avg_loss:0.361, val_acc:0.925]
Epoch [55/120    avg_loss:0.324, val_acc:0.969]
Epoch [56/120    avg_loss:0.314, val_acc:0.935]
Epoch [57/120    avg_loss:0.333, val_acc:0.952]
Epoch [58/120    avg_loss:0.321, val_acc:0.940]
Epoch [59/120    avg_loss:0.335, val_acc:0.965]
Epoch [60/120    avg_loss:0.277, val_acc:0.952]
Epoch [61/120    avg_loss:0.250, val_acc:0.973]
Epoch [62/120    avg_loss:0.304, val_acc:0.950]
Epoch [63/120    avg_loss:0.267, val_acc:0.954]
Epoch [64/120    avg_loss:0.255, val_acc:0.967]
Epoch [65/120    avg_loss:0.250, val_acc:0.921]
Epoch [66/120    avg_loss:0.315, val_acc:0.971]
Epoch [67/120    avg_loss:0.254, val_acc:0.925]
Epoch [68/120    avg_loss:0.270, val_acc:0.952]
Epoch [69/120    avg_loss:0.260, val_acc:0.954]
Epoch [70/120    avg_loss:0.223, val_acc:0.967]
Epoch [71/120    avg_loss:0.210, val_acc:0.940]
Epoch [72/120    avg_loss:0.205, val_acc:0.965]
Epoch [73/120    avg_loss:0.198, val_acc:0.956]
Epoch [74/120    avg_loss:0.202, val_acc:0.906]
Epoch [75/120    avg_loss:0.195, val_acc:0.956]
Epoch [76/120    avg_loss:0.167, val_acc:0.977]
Epoch [77/120    avg_loss:0.149, val_acc:0.979]
Epoch [78/120    avg_loss:0.143, val_acc:0.977]
Epoch [79/120    avg_loss:0.135, val_acc:0.977]
Epoch [80/120    avg_loss:0.148, val_acc:0.973]
Epoch [81/120    avg_loss:0.145, val_acc:0.973]
Epoch [82/120    avg_loss:0.137, val_acc:0.971]
Epoch [83/120    avg_loss:0.142, val_acc:0.973]
Epoch [84/120    avg_loss:0.141, val_acc:0.977]
Epoch [85/120    avg_loss:0.156, val_acc:0.975]
Epoch [86/120    avg_loss:0.121, val_acc:0.977]
Epoch [87/120    avg_loss:0.150, val_acc:0.979]
Epoch [88/120    avg_loss:0.147, val_acc:0.981]
Epoch [89/120    avg_loss:0.119, val_acc:0.973]
Epoch [90/120    avg_loss:0.139, val_acc:0.975]
Epoch [91/120    avg_loss:0.133, val_acc:0.977]
Epoch [92/120    avg_loss:0.131, val_acc:0.979]
Epoch [93/120    avg_loss:0.144, val_acc:0.977]
Epoch [94/120    avg_loss:0.127, val_acc:0.975]
Epoch [95/120    avg_loss:0.124, val_acc:0.973]
Epoch [96/120    avg_loss:0.124, val_acc:0.977]
Epoch [97/120    avg_loss:0.127, val_acc:0.975]
Epoch [98/120    avg_loss:0.133, val_acc:0.977]
Epoch [99/120    avg_loss:0.131, val_acc:0.979]
Epoch [100/120    avg_loss:0.137, val_acc:0.973]
Epoch [101/120    avg_loss:0.120, val_acc:0.979]
Epoch [102/120    avg_loss:0.124, val_acc:0.981]
Epoch [103/120    avg_loss:0.121, val_acc:0.981]
Epoch [104/120    avg_loss:0.136, val_acc:0.981]
Epoch [105/120    avg_loss:0.129, val_acc:0.981]
Epoch [106/120    avg_loss:0.112, val_acc:0.981]
Epoch [107/120    avg_loss:0.129, val_acc:0.981]
Epoch [108/120    avg_loss:0.115, val_acc:0.981]
Epoch [109/120    avg_loss:0.124, val_acc:0.981]
Epoch [110/120    avg_loss:0.123, val_acc:0.981]
Epoch [111/120    avg_loss:0.124, val_acc:0.981]
Epoch [112/120    avg_loss:0.126, val_acc:0.979]
Epoch [113/120    avg_loss:0.117, val_acc:0.979]
Epoch [114/120    avg_loss:0.123, val_acc:0.979]
Epoch [115/120    avg_loss:0.127, val_acc:0.979]
Epoch [116/120    avg_loss:0.126, val_acc:0.979]
Epoch [117/120    avg_loss:0.134, val_acc:0.979]
Epoch [118/120    avg_loss:0.123, val_acc:0.979]
Epoch [119/120    avg_loss:0.123, val_acc:0.979]
Epoch [120/120    avg_loss:0.129, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 190  35   0   0   0   0   0   0   2   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.61407249466951

F1 scores:
[       nan 0.99707174 0.98426966 0.97777778 0.87557604 0.86792453
 0.99038462 0.96132597 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9845724816290569
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d4acb6b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.618, val_acc:0.110]
Epoch [2/120    avg_loss:2.553, val_acc:0.306]
Epoch [3/120    avg_loss:2.488, val_acc:0.381]
Epoch [4/120    avg_loss:2.424, val_acc:0.381]
Epoch [5/120    avg_loss:2.370, val_acc:0.410]
Epoch [6/120    avg_loss:2.337, val_acc:0.452]
Epoch [7/120    avg_loss:2.300, val_acc:0.460]
Epoch [8/120    avg_loss:2.247, val_acc:0.471]
Epoch [9/120    avg_loss:2.204, val_acc:0.463]
Epoch [10/120    avg_loss:2.164, val_acc:0.475]
Epoch [11/120    avg_loss:2.116, val_acc:0.473]
Epoch [12/120    avg_loss:2.059, val_acc:0.475]
Epoch [13/120    avg_loss:2.021, val_acc:0.510]
Epoch [14/120    avg_loss:1.941, val_acc:0.537]
Epoch [15/120    avg_loss:1.899, val_acc:0.554]
Epoch [16/120    avg_loss:1.838, val_acc:0.594]
Epoch [17/120    avg_loss:1.806, val_acc:0.615]
Epoch [18/120    avg_loss:1.740, val_acc:0.621]
Epoch [19/120    avg_loss:1.705, val_acc:0.623]
Epoch [20/120    avg_loss:1.644, val_acc:0.621]
Epoch [21/120    avg_loss:1.592, val_acc:0.652]
Epoch [22/120    avg_loss:1.555, val_acc:0.640]
Epoch [23/120    avg_loss:1.481, val_acc:0.675]
Epoch [24/120    avg_loss:1.416, val_acc:0.698]
Epoch [25/120    avg_loss:1.328, val_acc:0.713]
Epoch [26/120    avg_loss:1.265, val_acc:0.717]
Epoch [27/120    avg_loss:1.200, val_acc:0.740]
Epoch [28/120    avg_loss:1.147, val_acc:0.752]
Epoch [29/120    avg_loss:1.089, val_acc:0.769]
Epoch [30/120    avg_loss:0.994, val_acc:0.767]
Epoch [31/120    avg_loss:0.963, val_acc:0.767]
Epoch [32/120    avg_loss:0.957, val_acc:0.777]
Epoch [33/120    avg_loss:0.905, val_acc:0.783]
Epoch [34/120    avg_loss:0.868, val_acc:0.779]
Epoch [35/120    avg_loss:0.849, val_acc:0.798]
Epoch [36/120    avg_loss:0.809, val_acc:0.792]
Epoch [37/120    avg_loss:0.745, val_acc:0.806]
Epoch [38/120    avg_loss:0.698, val_acc:0.856]
Epoch [39/120    avg_loss:0.685, val_acc:0.875]
Epoch [40/120    avg_loss:0.648, val_acc:0.871]
Epoch [41/120    avg_loss:0.619, val_acc:0.927]
Epoch [42/120    avg_loss:0.573, val_acc:0.894]
Epoch [43/120    avg_loss:0.557, val_acc:0.885]
Epoch [44/120    avg_loss:0.531, val_acc:0.931]
Epoch [45/120    avg_loss:0.503, val_acc:0.931]
Epoch [46/120    avg_loss:0.503, val_acc:0.923]
Epoch [47/120    avg_loss:0.523, val_acc:0.898]
Epoch [48/120    avg_loss:0.473, val_acc:0.927]
Epoch [49/120    avg_loss:0.513, val_acc:0.896]
Epoch [50/120    avg_loss:0.484, val_acc:0.929]
Epoch [51/120    avg_loss:0.430, val_acc:0.938]
Epoch [52/120    avg_loss:0.396, val_acc:0.894]
Epoch [53/120    avg_loss:0.451, val_acc:0.919]
Epoch [54/120    avg_loss:0.415, val_acc:0.927]
Epoch [55/120    avg_loss:0.384, val_acc:0.944]
Epoch [56/120    avg_loss:0.351, val_acc:0.935]
Epoch [57/120    avg_loss:0.433, val_acc:0.917]
Epoch [58/120    avg_loss:0.393, val_acc:0.938]
Epoch [59/120    avg_loss:0.327, val_acc:0.935]
Epoch [60/120    avg_loss:0.339, val_acc:0.915]
Epoch [61/120    avg_loss:0.399, val_acc:0.919]
Epoch [62/120    avg_loss:0.348, val_acc:0.933]
Epoch [63/120    avg_loss:0.281, val_acc:0.944]
Epoch [64/120    avg_loss:0.316, val_acc:0.952]
Epoch [65/120    avg_loss:0.278, val_acc:0.935]
Epoch [66/120    avg_loss:0.305, val_acc:0.938]
Epoch [67/120    avg_loss:0.292, val_acc:0.935]
Epoch [68/120    avg_loss:0.267, val_acc:0.942]
Epoch [69/120    avg_loss:0.338, val_acc:0.931]
Epoch [70/120    avg_loss:0.324, val_acc:0.906]
Epoch [71/120    avg_loss:0.413, val_acc:0.927]
Epoch [72/120    avg_loss:0.307, val_acc:0.946]
Epoch [73/120    avg_loss:0.272, val_acc:0.948]
Epoch [74/120    avg_loss:0.253, val_acc:0.946]
Epoch [75/120    avg_loss:0.238, val_acc:0.948]
Epoch [76/120    avg_loss:0.216, val_acc:0.960]
Epoch [77/120    avg_loss:0.247, val_acc:0.938]
Epoch [78/120    avg_loss:0.212, val_acc:0.948]
Epoch [79/120    avg_loss:0.206, val_acc:0.954]
Epoch [80/120    avg_loss:0.216, val_acc:0.952]
Epoch [81/120    avg_loss:0.204, val_acc:0.946]
Epoch [82/120    avg_loss:0.176, val_acc:0.965]
Epoch [83/120    avg_loss:0.170, val_acc:0.938]
Epoch [84/120    avg_loss:0.186, val_acc:0.942]
Epoch [85/120    avg_loss:0.190, val_acc:0.954]
Epoch [86/120    avg_loss:0.190, val_acc:0.944]
Epoch [87/120    avg_loss:0.192, val_acc:0.960]
Epoch [88/120    avg_loss:0.154, val_acc:0.948]
Epoch [89/120    avg_loss:0.155, val_acc:0.960]
Epoch [90/120    avg_loss:0.197, val_acc:0.950]
Epoch [91/120    avg_loss:0.187, val_acc:0.938]
Epoch [92/120    avg_loss:0.156, val_acc:0.948]
Epoch [93/120    avg_loss:0.195, val_acc:0.952]
Epoch [94/120    avg_loss:0.173, val_acc:0.956]
Epoch [95/120    avg_loss:0.158, val_acc:0.971]
Epoch [96/120    avg_loss:0.153, val_acc:0.948]
Epoch [97/120    avg_loss:0.157, val_acc:0.956]
Epoch [98/120    avg_loss:0.131, val_acc:0.965]
Epoch [99/120    avg_loss:0.147, val_acc:0.948]
Epoch [100/120    avg_loss:0.202, val_acc:0.977]
Epoch [101/120    avg_loss:0.213, val_acc:0.958]
Epoch [102/120    avg_loss:0.140, val_acc:0.975]
Epoch [103/120    avg_loss:0.132, val_acc:0.971]
Epoch [104/120    avg_loss:0.142, val_acc:0.963]
Epoch [105/120    avg_loss:0.184, val_acc:0.960]
Epoch [106/120    avg_loss:0.140, val_acc:0.963]
Epoch [107/120    avg_loss:0.157, val_acc:0.967]
Epoch [108/120    avg_loss:0.140, val_acc:0.960]
Epoch [109/120    avg_loss:0.111, val_acc:0.975]
Epoch [110/120    avg_loss:0.109, val_acc:0.975]
Epoch [111/120    avg_loss:0.086, val_acc:0.977]
Epoch [112/120    avg_loss:0.077, val_acc:0.981]
Epoch [113/120    avg_loss:0.096, val_acc:0.981]
Epoch [114/120    avg_loss:0.104, val_acc:0.973]
Epoch [115/120    avg_loss:0.099, val_acc:0.979]
Epoch [116/120    avg_loss:0.096, val_acc:0.963]
Epoch [117/120    avg_loss:0.109, val_acc:0.971]
Epoch [118/120    avg_loss:0.085, val_acc:0.975]
Epoch [119/120    avg_loss:0.090, val_acc:0.975]
Epoch [120/120    avg_loss:0.094, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   3 227   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 198  26   0   0   0   0   0   0   3   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0  12   3   0   2   0 189   0   0   0   0   0   0   0]
 [  0   0  33   0   0   0   0  61   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.12366737739872

F1 scores:
[       nan 0.99058653 0.91823899 0.99343545 0.91666667 0.90032154
 0.95454545 0.78709677 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9791015488916002
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3078cd2ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.625, val_acc:0.046]
Epoch [2/120    avg_loss:2.550, val_acc:0.125]
Epoch [3/120    avg_loss:2.493, val_acc:0.263]
Epoch [4/120    avg_loss:2.430, val_acc:0.338]
Epoch [5/120    avg_loss:2.376, val_acc:0.335]
Epoch [6/120    avg_loss:2.324, val_acc:0.367]
Epoch [7/120    avg_loss:2.275, val_acc:0.412]
Epoch [8/120    avg_loss:2.217, val_acc:0.471]
Epoch [9/120    avg_loss:2.165, val_acc:0.492]
Epoch [10/120    avg_loss:2.117, val_acc:0.548]
Epoch [11/120    avg_loss:2.044, val_acc:0.583]
Epoch [12/120    avg_loss:1.985, val_acc:0.579]
Epoch [13/120    avg_loss:1.920, val_acc:0.613]
Epoch [14/120    avg_loss:1.844, val_acc:0.631]
Epoch [15/120    avg_loss:1.781, val_acc:0.656]
Epoch [16/120    avg_loss:1.687, val_acc:0.665]
Epoch [17/120    avg_loss:1.616, val_acc:0.683]
Epoch [18/120    avg_loss:1.542, val_acc:0.685]
Epoch [19/120    avg_loss:1.466, val_acc:0.690]
Epoch [20/120    avg_loss:1.406, val_acc:0.706]
Epoch [21/120    avg_loss:1.332, val_acc:0.715]
Epoch [22/120    avg_loss:1.285, val_acc:0.721]
Epoch [23/120    avg_loss:1.204, val_acc:0.738]
Epoch [24/120    avg_loss:1.166, val_acc:0.756]
Epoch [25/120    avg_loss:1.096, val_acc:0.765]
Epoch [26/120    avg_loss:1.016, val_acc:0.746]
Epoch [27/120    avg_loss:0.995, val_acc:0.781]
Epoch [28/120    avg_loss:0.895, val_acc:0.798]
Epoch [29/120    avg_loss:0.886, val_acc:0.852]
Epoch [30/120    avg_loss:0.832, val_acc:0.860]
Epoch [31/120    avg_loss:0.764, val_acc:0.912]
Epoch [32/120    avg_loss:0.718, val_acc:0.887]
Epoch [33/120    avg_loss:0.659, val_acc:0.910]
Epoch [34/120    avg_loss:0.631, val_acc:0.925]
Epoch [35/120    avg_loss:0.610, val_acc:0.910]
Epoch [36/120    avg_loss:0.579, val_acc:0.896]
Epoch [37/120    avg_loss:0.570, val_acc:0.854]
Epoch [38/120    avg_loss:0.517, val_acc:0.965]
Epoch [39/120    avg_loss:0.478, val_acc:0.931]
Epoch [40/120    avg_loss:0.453, val_acc:0.960]
Epoch [41/120    avg_loss:0.434, val_acc:0.942]
Epoch [42/120    avg_loss:0.426, val_acc:0.950]
Epoch [43/120    avg_loss:0.446, val_acc:0.927]
Epoch [44/120    avg_loss:0.388, val_acc:0.958]
Epoch [45/120    avg_loss:0.408, val_acc:0.950]
Epoch [46/120    avg_loss:0.401, val_acc:0.931]
Epoch [47/120    avg_loss:0.404, val_acc:0.952]
Epoch [48/120    avg_loss:0.375, val_acc:0.954]
Epoch [49/120    avg_loss:0.398, val_acc:0.938]
Epoch [50/120    avg_loss:0.372, val_acc:0.952]
Epoch [51/120    avg_loss:0.318, val_acc:0.969]
Epoch [52/120    avg_loss:0.296, val_acc:0.965]
Epoch [53/120    avg_loss:0.240, val_acc:0.969]
Epoch [54/120    avg_loss:0.315, val_acc:0.965]
Epoch [55/120    avg_loss:0.283, val_acc:0.963]
Epoch [56/120    avg_loss:0.249, val_acc:0.938]
Epoch [57/120    avg_loss:0.273, val_acc:0.950]
Epoch [58/120    avg_loss:0.293, val_acc:0.958]
Epoch [59/120    avg_loss:0.280, val_acc:0.960]
Epoch [60/120    avg_loss:0.224, val_acc:0.967]
Epoch [61/120    avg_loss:0.252, val_acc:0.952]
Epoch [62/120    avg_loss:0.229, val_acc:0.967]
Epoch [63/120    avg_loss:0.270, val_acc:0.960]
Epoch [64/120    avg_loss:0.242, val_acc:0.946]
Epoch [65/120    avg_loss:0.234, val_acc:0.967]
Epoch [66/120    avg_loss:0.238, val_acc:0.973]
Epoch [67/120    avg_loss:0.186, val_acc:0.958]
Epoch [68/120    avg_loss:0.181, val_acc:0.981]
Epoch [69/120    avg_loss:0.177, val_acc:0.983]
Epoch [70/120    avg_loss:0.182, val_acc:0.971]
Epoch [71/120    avg_loss:0.176, val_acc:0.971]
Epoch [72/120    avg_loss:0.163, val_acc:0.983]
Epoch [73/120    avg_loss:0.144, val_acc:0.965]
Epoch [74/120    avg_loss:0.155, val_acc:0.981]
Epoch [75/120    avg_loss:0.122, val_acc:0.969]
Epoch [76/120    avg_loss:0.142, val_acc:0.985]
Epoch [77/120    avg_loss:0.133, val_acc:0.967]
Epoch [78/120    avg_loss:0.168, val_acc:0.981]
Epoch [79/120    avg_loss:0.128, val_acc:0.977]
Epoch [80/120    avg_loss:0.156, val_acc:0.969]
Epoch [81/120    avg_loss:0.164, val_acc:0.973]
Epoch [82/120    avg_loss:0.143, val_acc:0.983]
Epoch [83/120    avg_loss:0.115, val_acc:0.981]
Epoch [84/120    avg_loss:0.116, val_acc:0.988]
Epoch [85/120    avg_loss:0.101, val_acc:0.988]
Epoch [86/120    avg_loss:0.098, val_acc:0.985]
Epoch [87/120    avg_loss:0.093, val_acc:0.963]
Epoch [88/120    avg_loss:0.116, val_acc:0.979]
Epoch [89/120    avg_loss:0.100, val_acc:0.983]
Epoch [90/120    avg_loss:0.102, val_acc:0.981]
Epoch [91/120    avg_loss:0.122, val_acc:0.990]
Epoch [92/120    avg_loss:0.130, val_acc:0.977]
Epoch [93/120    avg_loss:0.119, val_acc:0.963]
Epoch [94/120    avg_loss:0.106, val_acc:0.981]
Epoch [95/120    avg_loss:0.091, val_acc:0.988]
Epoch [96/120    avg_loss:0.061, val_acc:0.994]
Epoch [97/120    avg_loss:0.061, val_acc:0.983]
Epoch [98/120    avg_loss:0.083, val_acc:0.990]
Epoch [99/120    avg_loss:0.081, val_acc:0.985]
Epoch [100/120    avg_loss:0.062, val_acc:0.990]
Epoch [101/120    avg_loss:0.065, val_acc:0.988]
Epoch [102/120    avg_loss:0.049, val_acc:0.990]
Epoch [103/120    avg_loss:0.060, val_acc:0.988]
Epoch [104/120    avg_loss:0.078, val_acc:0.990]
Epoch [105/120    avg_loss:0.081, val_acc:0.983]
Epoch [106/120    avg_loss:0.075, val_acc:0.988]
Epoch [107/120    avg_loss:0.061, val_acc:0.981]
Epoch [108/120    avg_loss:0.059, val_acc:0.983]
Epoch [109/120    avg_loss:0.097, val_acc:0.971]
Epoch [110/120    avg_loss:0.071, val_acc:0.981]
Epoch [111/120    avg_loss:0.056, val_acc:0.996]
Epoch [112/120    avg_loss:0.042, val_acc:0.994]
Epoch [113/120    avg_loss:0.036, val_acc:0.994]
Epoch [114/120    avg_loss:0.048, val_acc:0.992]
Epoch [115/120    avg_loss:0.037, val_acc:0.990]
Epoch [116/120    avg_loss:0.033, val_acc:0.990]
Epoch [117/120    avg_loss:0.040, val_acc:0.990]
Epoch [118/120    avg_loss:0.039, val_acc:0.992]
Epoch [119/120    avg_loss:0.035, val_acc:0.992]
Epoch [120/120    avg_loss:0.043, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99926954 0.99545455 1.         0.95280899 0.92976589
 0.99757869 0.98924731 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9931164071177798
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f351a56ca58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.655, val_acc:0.123]
Epoch [2/120    avg_loss:2.582, val_acc:0.346]
Epoch [3/120    avg_loss:2.508, val_acc:0.390]
Epoch [4/120    avg_loss:2.439, val_acc:0.388]
Epoch [5/120    avg_loss:2.391, val_acc:0.404]
Epoch [6/120    avg_loss:2.341, val_acc:0.469]
Epoch [7/120    avg_loss:2.290, val_acc:0.485]
Epoch [8/120    avg_loss:2.238, val_acc:0.504]
Epoch [9/120    avg_loss:2.185, val_acc:0.515]
Epoch [10/120    avg_loss:2.131, val_acc:0.554]
Epoch [11/120    avg_loss:2.066, val_acc:0.562]
Epoch [12/120    avg_loss:1.990, val_acc:0.565]
Epoch [13/120    avg_loss:1.922, val_acc:0.565]
Epoch [14/120    avg_loss:1.882, val_acc:0.569]
Epoch [15/120    avg_loss:1.813, val_acc:0.588]
Epoch [16/120    avg_loss:1.758, val_acc:0.592]
Epoch [17/120    avg_loss:1.710, val_acc:0.619]
Epoch [18/120    avg_loss:1.663, val_acc:0.619]
Epoch [19/120    avg_loss:1.618, val_acc:0.610]
Epoch [20/120    avg_loss:1.571, val_acc:0.633]
Epoch [21/120    avg_loss:1.517, val_acc:0.675]
Epoch [22/120    avg_loss:1.433, val_acc:0.667]
Epoch [23/120    avg_loss:1.415, val_acc:0.669]
Epoch [24/120    avg_loss:1.344, val_acc:0.669]
Epoch [25/120    avg_loss:1.288, val_acc:0.671]
Epoch [26/120    avg_loss:1.247, val_acc:0.685]
Epoch [27/120    avg_loss:1.204, val_acc:0.702]
Epoch [28/120    avg_loss:1.132, val_acc:0.740]
Epoch [29/120    avg_loss:1.076, val_acc:0.723]
Epoch [30/120    avg_loss:1.083, val_acc:0.752]
Epoch [31/120    avg_loss:1.017, val_acc:0.721]
Epoch [32/120    avg_loss:0.923, val_acc:0.777]
Epoch [33/120    avg_loss:0.928, val_acc:0.775]
Epoch [34/120    avg_loss:0.877, val_acc:0.825]
Epoch [35/120    avg_loss:0.836, val_acc:0.804]
Epoch [36/120    avg_loss:0.773, val_acc:0.898]
Epoch [37/120    avg_loss:0.734, val_acc:0.829]
Epoch [38/120    avg_loss:0.692, val_acc:0.921]
Epoch [39/120    avg_loss:0.670, val_acc:0.827]
Epoch [40/120    avg_loss:0.648, val_acc:0.856]
Epoch [41/120    avg_loss:0.609, val_acc:0.933]
Epoch [42/120    avg_loss:0.609, val_acc:0.935]
Epoch [43/120    avg_loss:0.562, val_acc:0.933]
Epoch [44/120    avg_loss:0.549, val_acc:0.952]
Epoch [45/120    avg_loss:0.507, val_acc:0.946]
Epoch [46/120    avg_loss:0.500, val_acc:0.944]
Epoch [47/120    avg_loss:0.497, val_acc:0.935]
Epoch [48/120    avg_loss:0.460, val_acc:0.946]
Epoch [49/120    avg_loss:0.473, val_acc:0.942]
Epoch [50/120    avg_loss:0.475, val_acc:0.871]
Epoch [51/120    avg_loss:0.443, val_acc:0.927]
Epoch [52/120    avg_loss:0.456, val_acc:0.950]
Epoch [53/120    avg_loss:0.431, val_acc:0.946]
Epoch [54/120    avg_loss:0.418, val_acc:0.940]
Epoch [55/120    avg_loss:0.368, val_acc:0.950]
Epoch [56/120    avg_loss:0.335, val_acc:0.935]
Epoch [57/120    avg_loss:0.339, val_acc:0.954]
Epoch [58/120    avg_loss:0.344, val_acc:0.948]
Epoch [59/120    avg_loss:0.340, val_acc:0.954]
Epoch [60/120    avg_loss:0.305, val_acc:0.952]
Epoch [61/120    avg_loss:0.311, val_acc:0.956]
Epoch [62/120    avg_loss:0.282, val_acc:0.965]
Epoch [63/120    avg_loss:0.269, val_acc:0.963]
Epoch [64/120    avg_loss:0.264, val_acc:0.946]
Epoch [65/120    avg_loss:0.252, val_acc:0.967]
Epoch [66/120    avg_loss:0.252, val_acc:0.950]
Epoch [67/120    avg_loss:0.248, val_acc:0.946]
Epoch [68/120    avg_loss:0.295, val_acc:0.956]
Epoch [69/120    avg_loss:0.274, val_acc:0.935]
Epoch [70/120    avg_loss:0.262, val_acc:0.963]
Epoch [71/120    avg_loss:0.300, val_acc:0.952]
Epoch [72/120    avg_loss:0.243, val_acc:0.954]
Epoch [73/120    avg_loss:0.209, val_acc:0.965]
Epoch [74/120    avg_loss:0.188, val_acc:0.958]
Epoch [75/120    avg_loss:0.200, val_acc:0.965]
Epoch [76/120    avg_loss:0.210, val_acc:0.967]
Epoch [77/120    avg_loss:0.199, val_acc:0.963]
Epoch [78/120    avg_loss:0.180, val_acc:0.979]
Epoch [79/120    avg_loss:0.174, val_acc:0.971]
Epoch [80/120    avg_loss:0.193, val_acc:0.958]
Epoch [81/120    avg_loss:0.235, val_acc:0.960]
Epoch [82/120    avg_loss:0.204, val_acc:0.948]
Epoch [83/120    avg_loss:0.247, val_acc:0.956]
Epoch [84/120    avg_loss:0.259, val_acc:0.971]
Epoch [85/120    avg_loss:0.227, val_acc:0.942]
Epoch [86/120    avg_loss:0.180, val_acc:0.965]
Epoch [87/120    avg_loss:0.197, val_acc:0.969]
Epoch [88/120    avg_loss:0.162, val_acc:0.965]
Epoch [89/120    avg_loss:0.158, val_acc:0.973]
Epoch [90/120    avg_loss:0.161, val_acc:0.975]
Epoch [91/120    avg_loss:0.162, val_acc:0.973]
Epoch [92/120    avg_loss:0.123, val_acc:0.973]
Epoch [93/120    avg_loss:0.132, val_acc:0.979]
Epoch [94/120    avg_loss:0.121, val_acc:0.981]
Epoch [95/120    avg_loss:0.128, val_acc:0.981]
Epoch [96/120    avg_loss:0.113, val_acc:0.983]
Epoch [97/120    avg_loss:0.130, val_acc:0.983]
Epoch [98/120    avg_loss:0.109, val_acc:0.981]
Epoch [99/120    avg_loss:0.119, val_acc:0.985]
Epoch [100/120    avg_loss:0.140, val_acc:0.985]
Epoch [101/120    avg_loss:0.112, val_acc:0.985]
Epoch [102/120    avg_loss:0.101, val_acc:0.985]
Epoch [103/120    avg_loss:0.102, val_acc:0.985]
Epoch [104/120    avg_loss:0.120, val_acc:0.981]
Epoch [105/120    avg_loss:0.107, val_acc:0.985]
Epoch [106/120    avg_loss:0.110, val_acc:0.988]
Epoch [107/120    avg_loss:0.097, val_acc:0.988]
Epoch [108/120    avg_loss:0.092, val_acc:0.988]
Epoch [109/120    avg_loss:0.087, val_acc:0.988]
Epoch [110/120    avg_loss:0.108, val_acc:0.988]
Epoch [111/120    avg_loss:0.091, val_acc:0.988]
Epoch [112/120    avg_loss:0.089, val_acc:0.988]
Epoch [113/120    avg_loss:0.101, val_acc:0.988]
Epoch [114/120    avg_loss:0.117, val_acc:0.990]
Epoch [115/120    avg_loss:0.091, val_acc:0.985]
Epoch [116/120    avg_loss:0.095, val_acc:0.988]
Epoch [117/120    avg_loss:0.088, val_acc:0.988]
Epoch [118/120    avg_loss:0.104, val_acc:0.985]
Epoch [119/120    avg_loss:0.088, val_acc:0.990]
Epoch [120/120    avg_loss:0.099, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.97986577 0.99782135 0.94712644 0.92903226
 1.         0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924039926007762
creating ./logs/logs-2022-01-17KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f36b3a7da58>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.267]
Epoch [2/120    avg_loss:2.517, val_acc:0.267]
Epoch [3/120    avg_loss:2.465, val_acc:0.269]
Epoch [4/120    avg_loss:2.406, val_acc:0.285]
Epoch [5/120    avg_loss:2.363, val_acc:0.300]
Epoch [6/120    avg_loss:2.328, val_acc:0.308]
Epoch [7/120    avg_loss:2.294, val_acc:0.354]
Epoch [8/120    avg_loss:2.250, val_acc:0.440]
Epoch [9/120    avg_loss:2.197, val_acc:0.479]
Epoch [10/120    avg_loss:2.162, val_acc:0.494]
Epoch [11/120    avg_loss:2.106, val_acc:0.540]
Epoch [12/120    avg_loss:2.075, val_acc:0.552]
Epoch [13/120    avg_loss:2.019, val_acc:0.554]
Epoch [14/120    avg_loss:1.979, val_acc:0.567]
Epoch [15/120    avg_loss:1.904, val_acc:0.565]
Epoch [16/120    avg_loss:1.847, val_acc:0.588]
Epoch [17/120    avg_loss:1.781, val_acc:0.604]
Epoch [18/120    avg_loss:1.715, val_acc:0.623]
Epoch [19/120    avg_loss:1.670, val_acc:0.648]
Epoch [20/120    avg_loss:1.592, val_acc:0.665]
Epoch [21/120    avg_loss:1.540, val_acc:0.692]
Epoch [22/120    avg_loss:1.433, val_acc:0.706]
Epoch [23/120    avg_loss:1.365, val_acc:0.719]
Epoch [24/120    avg_loss:1.291, val_acc:0.742]
Epoch [25/120    avg_loss:1.220, val_acc:0.723]
Epoch [26/120    avg_loss:1.136, val_acc:0.719]
Epoch [27/120    avg_loss:1.092, val_acc:0.746]
Epoch [28/120    avg_loss:1.029, val_acc:0.731]
Epoch [29/120    avg_loss:0.961, val_acc:0.779]
Epoch [30/120    avg_loss:0.915, val_acc:0.796]
Epoch [31/120    avg_loss:0.874, val_acc:0.767]
Epoch [32/120    avg_loss:0.834, val_acc:0.806]
Epoch [33/120    avg_loss:0.785, val_acc:0.823]
Epoch [34/120    avg_loss:0.764, val_acc:0.912]
Epoch [35/120    avg_loss:0.694, val_acc:0.915]
Epoch [36/120    avg_loss:0.674, val_acc:0.902]
Epoch [37/120    avg_loss:0.669, val_acc:0.921]
Epoch [38/120    avg_loss:0.628, val_acc:0.929]
Epoch [39/120    avg_loss:0.591, val_acc:0.921]
Epoch [40/120    avg_loss:0.530, val_acc:0.933]
Epoch [41/120    avg_loss:0.515, val_acc:0.938]
Epoch [42/120    avg_loss:0.491, val_acc:0.925]
Epoch [43/120    avg_loss:0.529, val_acc:0.929]
Epoch [44/120    avg_loss:0.470, val_acc:0.910]
Epoch [45/120    avg_loss:0.462, val_acc:0.938]
Epoch [46/120    avg_loss:0.453, val_acc:0.860]
Epoch [47/120    avg_loss:0.456, val_acc:0.940]
Epoch [48/120    avg_loss:0.395, val_acc:0.940]
Epoch [49/120    avg_loss:0.374, val_acc:0.944]
Epoch [50/120    avg_loss:0.359, val_acc:0.948]
Epoch [51/120    avg_loss:0.343, val_acc:0.940]
Epoch [52/120    avg_loss:0.360, val_acc:0.938]
Epoch [53/120    avg_loss:0.344, val_acc:0.948]
Epoch [54/120    avg_loss:0.340, val_acc:0.942]
Epoch [55/120    avg_loss:0.351, val_acc:0.931]
Epoch [56/120    avg_loss:0.381, val_acc:0.892]
Epoch [57/120    avg_loss:0.370, val_acc:0.944]
Epoch [58/120    avg_loss:0.304, val_acc:0.952]
Epoch [59/120    avg_loss:0.282, val_acc:0.938]
Epoch [60/120    avg_loss:0.304, val_acc:0.940]
Epoch [61/120    avg_loss:0.303, val_acc:0.956]
Epoch [62/120    avg_loss:0.282, val_acc:0.960]
Epoch [63/120    avg_loss:0.265, val_acc:0.948]
Epoch [64/120    avg_loss:0.261, val_acc:0.963]
Epoch [65/120    avg_loss:0.257, val_acc:0.958]
Epoch [66/120    avg_loss:0.254, val_acc:0.960]
Epoch [67/120    avg_loss:0.241, val_acc:0.967]
Epoch [68/120    avg_loss:0.223, val_acc:0.958]
Epoch [69/120    avg_loss:0.215, val_acc:0.956]
Epoch [70/120    avg_loss:0.207, val_acc:0.960]
Epoch [71/120    avg_loss:0.214, val_acc:0.958]
Epoch [72/120    avg_loss:0.218, val_acc:0.950]
Epoch [73/120    avg_loss:0.230, val_acc:0.967]
Epoch [74/120    avg_loss:0.203, val_acc:0.956]
Epoch [75/120    avg_loss:0.198, val_acc:0.973]
Epoch [76/120    avg_loss:0.204, val_acc:0.958]
Epoch [77/120    avg_loss:0.179, val_acc:0.969]
Epoch [78/120    avg_loss:0.203, val_acc:0.954]
Epoch [79/120    avg_loss:0.225, val_acc:0.965]
Epoch [80/120    avg_loss:0.202, val_acc:0.965]
Epoch [81/120    avg_loss:0.210, val_acc:0.952]
Epoch [82/120    avg_loss:0.197, val_acc:0.969]
Epoch [83/120    avg_loss:0.186, val_acc:0.983]
Epoch [84/120    avg_loss:0.159, val_acc:0.973]
Epoch [85/120    avg_loss:0.180, val_acc:0.967]
Epoch [86/120    avg_loss:0.161, val_acc:0.975]
Epoch [87/120    avg_loss:0.179, val_acc:0.971]
Epoch [88/120    avg_loss:0.155, val_acc:0.977]
Epoch [89/120    avg_loss:0.131, val_acc:0.988]
Epoch [90/120    avg_loss:0.157, val_acc:0.973]
Epoch [91/120    avg_loss:0.144, val_acc:0.981]
Epoch [92/120    avg_loss:0.183, val_acc:0.948]
Epoch [93/120    avg_loss:0.148, val_acc:0.965]
Epoch [94/120    avg_loss:0.135, val_acc:0.988]
Epoch [95/120    avg_loss:0.119, val_acc:0.969]
Epoch [96/120    avg_loss:0.152, val_acc:0.983]
Epoch [97/120    avg_loss:0.132, val_acc:0.981]
Epoch [98/120    avg_loss:0.159, val_acc:0.975]
Epoch [99/120    avg_loss:0.110, val_acc:0.983]
Epoch [100/120    avg_loss:0.110, val_acc:0.977]
Epoch [101/120    avg_loss:0.122, val_acc:0.971]
Epoch [102/120    avg_loss:0.118, val_acc:0.975]
Epoch [103/120    avg_loss:0.124, val_acc:0.983]
Epoch [104/120    avg_loss:0.114, val_acc:0.950]
Epoch [105/120    avg_loss:0.164, val_acc:0.958]
Epoch [106/120    avg_loss:0.133, val_acc:0.973]
Epoch [107/120    avg_loss:0.155, val_acc:0.979]
Epoch [108/120    avg_loss:0.129, val_acc:0.983]
Epoch [109/120    avg_loss:0.110, val_acc:0.985]
Epoch [110/120    avg_loss:0.093, val_acc:0.985]
Epoch [111/120    avg_loss:0.091, val_acc:0.985]
Epoch [112/120    avg_loss:0.085, val_acc:0.985]
Epoch [113/120    avg_loss:0.082, val_acc:0.988]
Epoch [114/120    avg_loss:0.074, val_acc:0.988]
Epoch [115/120    avg_loss:0.084, val_acc:0.988]
Epoch [116/120    avg_loss:0.078, val_acc:0.988]
Epoch [117/120    avg_loss:0.075, val_acc:0.988]
Epoch [118/120    avg_loss:0.079, val_acc:0.988]
Epoch [119/120    avg_loss:0.083, val_acc:0.988]
Epoch [120/120    avg_loss:0.071, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 194  29   0   0   0   0   0   0   4   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 0.996337   0.98206278 1.         0.91943128 0.90566038
 0.98800959 0.95555556 1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9888446452247486
